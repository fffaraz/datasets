There's SGDB (Java) as well. It seems to embed an adjacency list into each node which improves performance of activation procedures. http://ups.savba.sk/~marek/sgdb.html
Retrospectively improving the structure of widely used libraries, whilst retaining backwards compatibility with existing code, is the raison d'etre of the various superclass proposals.
Other alternatives are plain, old pattern matching as well as uncons :: b -&gt; (a -&gt; b) -&gt; [a] -&gt; b uncons x _ [] = x uncons _ f (x:_) = f x , so that head = uncons (error "Prelude.head: Empty list") id listToMaybe = uncons Nothing Just 
That's just `case` in disguise :-) Admittedly a nice, curriable disguise.
Going into the details of how the whole thing actually works is (IMO) more confusing than glossing over the details here. (That's also why I linked to the monad-control chapter, which describes the mechanism more fully.) It's easy enough to write try for reader version of try manually: tryReader :: ReaderT r IO a -&gt; ReaderT r IO (Either SomeException a) tryReader (ReaderT f) = ReaderT $ \r -&gt; try $ f r The problem is creating a general interface that works for many monads and many types of functions. Even if the types are not at all correct (as I was trying to make clear), the sentiment is correct: you want to hide away the reader-specific stuff from the outer monad so you can use the standard `try` function.
I'm not sure I like the alternatives given there. I'd just go with case analysis, or a first-class version of case analysis (e.g., * `maybe :: b -&gt; (a -&gt; b) -&gt; Maybe a -&gt; b`, * `either :: (a -&gt; c) -&gt; (b -&gt; c) -&gt; Either a b -&gt; c`, * `uncurry :: (a -&gt; b -&gt; c) -&gt; (a, b) -&gt; c`, * [`list :: (a -&gt; [a] -&gt; b) -&gt; b -&gt; [a] -&gt; b`](http://hackage.haskell.org/packages/archive/list-extras/0.4.0.1/doc/html/Data-List-Extras.html), etc.). There's some partiality that we can't get rid of, or are unwilling to do so with the current type system (e.g., division by zero), but that doesn't mean we should allow all these partial functions for case analysis. So I agree with this call to stop using flagrantly partial functions. Down with `head`! Down with `tail`! Down with `fromJust`!
You know, sometimes one just *has* to abstract for the sake of it... class Nullable c where empty :: c null :: c -&gt; Bool class HasElement c where type Element c :: * class (Nullable c, HasElement c) =&gt; Decons c where decons :: a -&gt; (Element c -&gt; c -&gt; a) -&gt; c -&gt; a decons x f xs = if null xs then x else f (head xs) (tail xs) head :: c -&gt; Element c head = decons (error "head: No head available") const tail :: c -&gt; c tail = decons (error "tail: No tail available") (const id) instance Nullable [a] where empty = [] null = P.null instance HasElement [a] where type Element [a] = a instance Decons [a] where decons x _ [] = x decons _ f (x:xs) = f x xs instance Nullable (Maybe a) where empty = Nothing null = isNothing instance HasElement (Maybe a) where type Element (Maybe a) = a instance Decons (Maybe a) where decons x f = maybe x (flip f Nothing) -- durr hurr instance Nullable Bool where empty = False null = not -- durr hurr hurr instance HasElement Bool where type Element Bool = () -- durr hurr hurr hurr instance Decons Bool where decons x f b = if b then f () empty else x 
Is it working on Windows now? Last-time I tried to install snap (with the snap-chat course) I could not make it run on Windows, though on Linux it worked fine.
I don't understand. What if my data type *is* a list *and* I know that *head* will not throw an exception? foo [] = 0 foo xs = bar $ head xs 
Another alternative: (:~&gt;) lenses from fclabels.
Then use pattern matching properly. foo [] = 0 foo (x:xs) = bar $ x 
 foo [] = 0 foo (x:_) = bar x Then you get a compile warning if you miss a case.
Oscar St. `fromJust` will have your `head` for your treasonous words, citoyen!
Augh.
Then you should use a data type that enforces that constraint. One possibility is a smart constructor: module List1 ( List1, list1 ) newtype List1 a = List1 [a] list1 :: [a] -&gt; Maybe (List1 a) list1 [] = Nothing list1 l = Just (List1 l) Then you could have head :: List1 a -&gt; a head (List1 (x:_)) = x headMaybe :: [a] -&gt; Maybe a headMaybe = fmap head . list1 That's one way to do it. There's also a way to enforce that at the data level: newtype List1 a = List1 (a, [a]) list1 :: [a] -&gt; Maybe (List1 a) list1 [] = Nothing list1 (x:xs) = Just (List1 (x, xs)) head :: List1 a -&gt; a head (x, _) = x -- headMaybe is same as above 
You take that code and burn it right now lest someone thinks it's a good idea!
No, the grandparent should definitely use case analysis here.
I didn't see that part of your comment, sorry. Looking at your MState, it seems to use a single TVar for every single bit of state, which defeats roughly all of the point of STM. Edit: I mean, it's fine when you only have one piece of state, but you rarely, if ever, only have one piece of state.
You seem to have missed that this function is in fact perfectly safe and *that's why it can be expressed without head*: the second line is just a pattern match! The `[]` case is already handled on the previous line.
I've found the `safe` package really useful for avoiding some classes of partial functions (such as `head`): http://hackage.haskell.org/package/safe 
Thanks, that's my point.
I know it will probably never happen, but I dream of a prelude where all functions are total: head :: [a] -&gt; Maybe a read :: Read a =&gt; String -&gt; Maybe a Etcetera. Perhaps the current partial versions could be added as `unsafeHead` and such. But *if* this were to happen at all, it would take at least a couple of revisions of base: deprecate the old versions (e.g. remove head) and add the new (e.g. add safeHead), remove the old ones (remove head), replace them by the new ones (add head as the safe version) and deprecate the 'old' new version (e.g. safeHead), finally remove the 'old' new version (e.g. safeHead). Hey, I can dream, right?
Then I don't understand your point. Why use `head` there? It's completely unidiomatic and confusing.
[Parsec's `Stream`](http://hackage.haskell.org/packages/archive/parsec/3.1.2/doc/html/Text-Parsec-Prim.html#t:Stream). It actually makes sense as it allows you to use the same code for, say, lists and `Text`, as well as more custom data types. I didn't get around needing more instances than the initial batch of Prelude types, yet, and, frankly, `Bool` being sensibly deconsable isn't my fault. The whole thing would of course be nicer with superclass instances. I rest my case. 
Sure, I agree completely with everything you said. My point is that you shouldn't call out foo [] = 0 foo xs = bar $ head xs for being fragile without explicitly mentioning that *you should just pattern-match* in this case; it's not obvious to me, reading your post, that you're suggesting pattern-matching, as opposed to one of the more complicated solutions, as a fix for this definition.
What makes a partial function?
Returning bottom when given non-bottom input.
Hear, hear! I never understood why Haskell's de facto iterators threw exceptions instead of returning a Maybe type. It's A) dangerous, unidiomatic, and crufty. For example, I hate it when a programming language throws an exception when a regex doesn't match a string. This forces the programmer to use cumbersome try/catch expressions when a simple `match.nil?` or `case ... Nothing -&gt;` would suffice.
A function `f` is partial if there exists a value `x` such that `f x` is bottom and `x` is not bottom. (correct me if I'm wrong)
His point was to pick the simplest case where we know that the list is non-empty (and thus safe to use `head` on) but it's not obvious to the compiler, and using `safeHead`/`listToMaybe` would be annoying. Telling him to use pattern matching there misses his actual point, which is when there's non-obvious knowledge about the value implied by other code.
Do you have a real world example of this?
What does bottom and not bottom mean?
Didn't the original article cover this, by advising to use `fromMaybe (error "...") . listToMaybe`? Then if your invariants go wrong, at least you get a helpful message at runtime.
I was speaking more generally to answer the question "What if I know that my list has at least one element." Prove it with the data type. Assert it earlier, avoid mistakes later. If you only needed the case-by-case (e.g. reasonable default value), then it would be improved by something like: fromHead :: b -&gt; (a -&gt; b) -&gt; [a] -&gt; b fromHead x f = maybe x f . headMaybe So you could say: foo = fromHead 0 bar Which lets you make a one liner. Like if you wanted to do something like this: something list1 list2 = fromHead 0 bar1 list1 + fromHead 1 bar2 list2 Instead of: something list1 list2 = foo1 list1 + foo2 list2 where foo1 [] = 0 foo1 (x:xs) = bar1 x foo2 [] = 1 foo2 (x:xs) = bar2 x 
A function is partial when it is not defined for all inputs - `head` is non-partial because it is an error to call it with an empty list, in spite of its type signature specifying that it operates on lists.
[Bottom](http://www.haskell.org/haskellwiki/Bottom) is a wrap-up term for all values that aren't actually real values, the most important being non-termination: A non-terminating function can be of any type, so any Haskell type includes bottom among its possible values. As writing head (x:_) = x head [] = head [] would be rather sense-free, exceptions are yet another way to generate bottom values: error :: String -&gt; a see, they are of truly any type. (Exceptions are catchable, though, and compiled ghc code would throw one for that second head case... but when thinking in that way we're quite far indeed from pure, denotational thought and deeply in the dark and stormy topic of operational semantics and getting-stuff-done) Conversely, "non-bottom", in the Haskell sense, means "Terminating and not an exception".
I know I should probably just jump on the bandwagon and shout agreement, but I'd suggest caution. There are three possible responses to a partial function: 1. Redesign the types so that invalid inputs are unrepresentable. This is a very good thing, when you can do it. It's also very hard in the general case, sometimes even requiring something like dependent types, and there are plenty of situations where code needs to just be written with the tools you have. 2. Generalize the function in a natural way into a total function on the input type with well-specified semantics. Like the first point, this is also a good thing, when it's possible; and very hard in general. 3. Turn your dynamically detected runtime exceptions into logic errors. This is a very, very bad thing! I've worked with far too much code written by programmers who are prone to say things like "I'll just return zero; this will never happen anyway, so it doesn't matter". Avoiding partial functions entirely is a wonderful idea if you're willing and able -- and have the programming language like Agda that gives you the tools -- to do it. But in practice, avoiding partial functions is far too likely to lead you to the third answer. Now you not only get no compile-time check, but you also get broken semantics! It occurs to me that in many cases where changing the types or generalizing the functions in a well-defined way are not possible, the complaint about `Prelude.head` is really just about poor error messages, and in particular, missing stack traces. A reasonable complaint, yes, but one that should be *fixed*, not worked around by hobbling `Prelude`. I'm encouraged that Simon Marlow mentioned on Google+ recently that he has a decent idea of how to get stack traces in Haskell, at least for interpreted code.
Did you know that you didn't need to declare a new class just to make a type function? Instead of class HasElement a where type Element a :: * instance HasElement [a] where type Element [a] = a consider just type family Element a :: * type instance Element [a] = a
I think most of the time people use these partial functions is because they think the error will never happen. Actually it's like assertions we used in other languages for impossible cases. I think assertions themselves is OK, the problem with haskell's partial function is that error message don't tell where the impossible case occurs. I think we should improve the error message first.
Yes, but then contexts would have equality constraints in them. ...which is, admittedly, a rationale I just made up because the code is too old to make recalling the actual rationale easy. It's quite possible refactoring is the reason the code is as it is, `HasElement` first having had a `singleton :: Element c -&gt; c` function or something along those lines.
Would they? I think there would just be no context. e.g. class (Nullable c, Element c) =&gt; Decons c where would just become this: class Nullable c =&gt; Decons c where ...no equality in sight.
Barring checking the whole thing with an actual compiler, yeah, I think you're right. It wouldn't even occur in method contexts. I guess I never stumbled over it because it's not an interface user code uses directly, and most library code just uses Decons in its contexts. 
&gt; Other languages also have static type checking. He is asking what Haskell gives over other commonly used languages. *Good* static type checking, including the ability to type effects and thus better manage them. This is a major source of bugs in other languages.
I laughed at the unintentional pun &gt; that second *head case* :)
It sounds like you're suffering from a reaction which has much less to do with Haskell and much more to do with the decisions a manager made. The fact that wrapping and using imperative GUI libraries in Haskell is ugly is less a reflection on Haskell than on the history of GUI libraries, which is littered with historical cruft composed of the accumulation of short-term decisions. Haskell can't wave a wand and make pre-existing bad design decisions disappear. A big part of Haskell's power comes from making it possible to avoid that sort of programming in the first place. It's too bad the manager(s) you mentioned didn't recognize that. If they had, they might have decided to go with another UI approach, and you'd be free to focus on the expressive power and safety by design that Haskell offers, which seems to be something you're missing at the moment, in your negative reaction to a management decision. FWIW, I've found web interfaces make for a good way to provide UIs to Haskell programs. The web has an inherently functional model which is easy to handle on the back end without the imperative litter you mentioned. 
His net looks pretty full!
Perhaps we should merge this effort with efforts such as [The Other Prelude](http://www.haskell.org/haskellwiki/The_Other_Prelude) and [Numeric Prelude](http://www.haskell.org/haskellwiki/Numeric_Prelude)? It would be nice to have a "people's prelude" with a general collection of these cleanups. I suppose if I can't find anything else to do during the [upcoming VirtuaHac](http://haskell.org/haskellwiki/VirtuaHac), I'll spend some time putting together the OtherSafeNumericAwesomePrelude or something like that. :)
this is a good post, its not a troll or a flame i definitely understand his point about haskell in a team. i spent years getting to an intermediate stage with haskell. i can't sell that as a solution to a team. there are lots of smart productive valuable coders who simply aren't motivated to take a very deep dive into something that may not pay off for a year or more. i would only use haskell in industry if a team already used it, or if i was sure that no one but me ever needed to see the code i've also often felt like i was serving the type system and not the other way around. haskell encourages library writers to think creatively about types, but often its not clear to others why or how it all works i've also found that way too many contributions to hackage presume type signatures are substantial documentation. most hackage packages are terribly underdocumented. and while haskell is clearly on the forefront of parallelism and concurrency, its almost impossible to keep the options in your head. compare this with Go - you have goroutines and a few mechanisms for making them easy to use like the author, i have been doing a lot of Go coding. i'm disappointed that they didn't try to use basic functional techniques, but the language has support for a powerful-enough type system and good support for concurrency, and i can sell it. i would have no problem advocating Go to moderately skilled coders with an expectation of a week or so ramp-up time for basic proficiency. Go also gets one thing right that haskell has gotten very wrong - strings. i don't think hackage will ever be sanitary with respect to strings, years from now we'll still see various approaches and libraries used. i can see haskell turning into a lisp....an old-guard of staunch supporters that swear it is better, shrinking slowly over the years, punctuated by various calls-to-action how to fix this? make a haskell for dummies. keep the parts that everyone loves, chuck some of the advanced features that tend to frustrate more than reward (laziness), and clean up the core types (strings). big-brains can continue to use regular haskell...little-brains and industry people can incorporate features over time. it will soon be time for this anyway...all of those GHC hints are starting to become a bunch of hacky crutches, and they seem to get more numerous with each release
Here are a couple hopefully not too contrived examples: -- Find the smallest power of 2 larger than the argument. nextPowerOfTwo :: Integer -&gt; Integer nextPowerOfTwo n = head . filter (&gt; n) $ iterate (2 *) 1 -- Compute the minimum width of a closed interval containing every element of the argument. width :: [Integer] -&gt; Integer width [] = 0 width xs = maximum xs - minimum xs (Note that the two examples are safe for different reasons: in the first example, `head` can never cause a pattern match failure because `filter f $ iterate g x` is always a nonterminating list, i.e., never contains `[]`; while in the second example `xs` is just nonempty.)
Typo? &gt; That basically means any transformer stack built around IO, **as well as** ContT and Iteratee are not involved. Should this be "as long as"?
Question to the author: If, instead of "Prelude.head: empty list", a pattern match failure in `head` provided a useful traceback which pinpointed the source of the error, would you still advocate the deprecation of `head`?
I find that it's a lot harder for me to get going somewhere when not programming in Haskell or any similar language, so I don't think that it's an acceptable critique of a language to say you find it hard to get started.
LYAH is kind of a Haskell for dummies isn't it? It has colorful pictures! :)
Yes, it's a typo. Thanks for the heads-up, it's fix now.
If anyone has some other ideas of examples they'd like to see, please post (either here or on the blog itself). I definitely think that the best way of explaining XML parsing/rendering is with a large number of real-life use cases.
&gt; it's *fixed* now Not sure whether to :) or ಠ_ಠ. Let's go with :)
Good point. Anyway, my thinking was not that head is unavoidable, but that it might not be necessarily bad.
2 cents. 99% of my code lives in IO + exception + occasional state realm. It is trivial to write such code in any mainstream language, but requires serious mind bending and quite some awkwardness in monad transformer land. I find it easier to write "pass-the-world" code with appropriate accessors (record syntax, where are thou?) than use a three level deep stack of monad transformers. This is the main barrier I have with haskell. Getting anything done with a modicum of professionalism (hello, error handling and logging) is just not there out-of-the-box. This results in me writing functional code in the mainstream language of the day (Java/C++), instead of me writing code in Haskell. And I love functional programming and strong type systems. Apologies if the awkward monad stack is there and I just missed the appropriate tutorial. 
My experience is quite different. Since I got over the learning curve and "came out the other side" with Haskell I can't go back to mainstream languages. Haskell removes so much unnecessary work, and I don't miss it one bit. The only problem is that I haven't yet been able to eliminate those other languages from my life.
no i mean a language that is haskell-for-dummies haskell' never really arrived, one could conceive of it resurfacing as a vast resimplification of what haskell has become
Yes, I certainly intended that, hahaha&lt;/nervous-laugh&gt;.
Hah, Haskell for dummies? That'll never happen. As SPJ likes to say about Haskell, "Avoid success at all costs."
I agree that it's not a troll or flame, but I also felt like the author didn't deliver on what he promised in his opening sentence. Why isn't Haskell good for his tasks? Why is Go better? I'm not even saying he's wrong, just that I can't see why he's right.
These are good examples! I can't see what to do about the first. The second one we can eliminate all partialness, if maximum and minimum were redefined: maximum :: Ord a =&gt; [a] -&gt; Maybe a minimum :: Ord a =&gt; [a] -&gt; Maybe a width :: [Integer] -&gt; Integer width xs = maybe 0 $ (-) &lt;$&gt; maximum xs &lt;*&gt; minimum xs This encodes the "defaultness" of the zero width for an empty list nicely.
I think this is where a framework like Yesod can excel- we pick out a good monad stack and try to make it convenient to interact with. I hope the Haskell community can settle on a standard monad stack and make it highly accessible.
Strange. If you look through my comment history, I had mentioned some of the author's points but had gotten myself voted down to oblivion (mostly by fanboys probably).
But that has been my point! That simply writing in Haskell does not make good code. Bad programmers in Haskell can produce equally bad code, e.g. this stupid, ugly GUI library. Good programmers will produce good code in whatever language. Does Haskell make it easier to write good code? Maybe, but only if you have top notch programmers being given infinite time resource. Yes, infinite time resource. Constructing good Haskell takes a lot of effort. You already cited the current GUI libraries as an example when it is done in rush.
As an aside, I'm curious what research has gone into changing the way dependency analysis occurs. With the strong type system, it seems like Cabal ought to be able to look at the *types* exposed by various packages. It seems like there is very little communication between developers on when the interfaces will change and what changes might be breaking. This information might be best encapsulated in the type system. Has there been research into this?
I share your pain, and more resource regarding the building some 'native' packages like process &amp; network on windows for GHC 7.2 would be appreciated (even better if possible : binary version)
I've been doing some freelance web-dev and FFI work in it, but not used it in a bigger setting. Yesod's been really nice in catching errors.
 &gt; A resulting correct Haskell program is likely more reliable, maintainable, and perhaps faster, but getting to that point (framing your problem the right way, knowing how to profile or deal with space leaks, understanding when unsafePerformIO is actually the right thing) is just too much effort. This is just a matter of training, it becomes effortless after a while. It's no different from learning to play the guitar or piano. When learning Haskell, people with prior exposure to imperative languages are actually at a disadvantage, because they have to *unlearn* certain patterns in order to write idiomatic Haskell code. In particular, I don't understand why you need to know when to *not* use `unsafePerformIO`, because that the answer is "don't use it ever, except in a few well-known special circumstances".
Looking through your comment history, it's several pages until any hit negative scores from r/haskell, plenty of positive scores on reasonable criticism, and negative scores almost entirely on comments where you call people liars or fanboys. Not terribly surprising.
Hey, it's you again! You don't have to look through my comment history; you lived through it. ^^ Anyway, what do you think of the article?
By "the packages" you basically mean any package on hackage? They explicitly require GHC 7.2? Not sure exactly what your trouble is; 1) no HP (or precompiled bundle) with GHC 7.2 or later? 2) packages not working on Windows? If (1) then somebody should step up and provide such a binary installer, for testing and development purposes. No HP will include GHC 7.2 as it's only a "technology preview". Last stable release is still GHC 7.0.4 (or .3 as in HP 2011.2.0.1). I'm sure nobody breaks a package for Windows intentionally without a good reason. If (2) then surely they would be happy to accept patches. And indeed, the Haskell Platform lacks some docs; http://trac.haskell.org/haskell-platform/wiki/ReleaseTimetable (release dates not updated).
I agree. I can't build leksah on windows because of haddock, and other gtk packages failing to build
I think the problem is that many packages are being updated to work with the new `base-4.4.0.0` library, but this makes life difficult for people who still have `base-4.3.1.0` installed. Basically, you have to recompile everything whenever you upgrade your `base` library, even GHC itself for those packages that use the GHC API, like `haddock`.
To call that a fix, I dunno. For one thing it's noisier, and harder to read what the intended behaviour is, in my opinion. It also provides the default value 0 (through the sum) where 0 isn't even a possible return value. Why force the programmer to deal with cases they know are impossible?
This was posted on the Haskell-reddit recently: http://skilpat.tumblr.com/post/9411500320/a-modular-package-language-for-haskell EDIT: Here is the reddit submission: http://www.reddit.com/r/haskell/comments/jvlrw/mmmodularity_a_modular_package_language_for/
But hopefully, those packages should still work with base-4.3.1.0 as well, right? It seems good practice to test with at least the last two GHC versions before you release a package.
Stacks, in my understanding, are actually just development aids, and code using it shouldn't depend on their structure. That is, if you use `lift` you're probably doing it wrong (unless it's `liftIO`, which should merely be constrained to known places). If you don't scatter `lift`s all over your place, it's quite trivial to flatten the stack after you know what pieces you actually need. A lenses package helps, too, especially with the `Reader` and `State` parts.
Yes, it works on Windows. You can install the Haskell Platform, `cabal install snap`, and proceed to use the `snap` tool. I can happily say that it works brilliantly. There was [an issue](https://github.com/snapframework/snap-core/issues/102) with Snap 0.5.4 on Windows, but it has been fixed in 0.5.5.
Secretly writing a parser and DSL to replace some very fragile CPP trickery with something better for the job. Parsec love.
As a freelancer, building an interactive teaching application combining some open source Haskell natural language generation software with components written in other languages. Types and abstractions are helpful. I don't think I could wrap my head around my problem without them. Having a relatively small community can be an issue if you want to do things like create a 64 bit Windows DLL
I wonder if Hack-Nix handles dependencies better. Nix itself is supposed to be much better at it, so you would think so: http://nixos.org/nix/ http://www.haskell.org/haskellwiki/Hack-Nix
Hopefully. However, when one package doesn't, you are often forced to upgrade all of them.
The plan was to do a HP release with GHC 7.0.4 and then wait for 7.4, thus skipping 7.2. There are some unsolved problems with GHC 7.0.4 on OS X, though, so that's why the release was delayed. 
We use a lot of functional programming to build quantitative tools for financial analysis. One part of this is a small lazy functional programming language that we use for reporting, with row types, rank n types, etc. that runs on the JVM. While the language implementation itself is in scala, we often prototype new features for it in Haskell and Agda. Another part of our infrastructure uses functional techniques such as iteratees, monoidal reducers, etc. to fuse together lots of passes over datasets. (Shameless Plug: edwardkmett is hiring for more programmers if any Haskell guys want to work in Boston at a well-established company. PM me for details). ;)
7.0.4 wouldn't fix the issue of many packages requiring 7.2. :/
I have a buildbot that makes sure that all my packages, including network, builds on the last 3 versions of GHC.
I argue that those packages should be fixed to support 7.0.4 again. Packages need to support at least the 2 latest GHC releases, or they'll make upgrading tough.
Well hackage doesn't enforce this, it doesn't even bother testing whether these packages have build errors on anything but the latest version. Pre-edit: I take that back, packages that were around during the update to a new version of GHC get both build logs. 
We are using Haskell for almost everything: web frontend, database, backend work. Happy so far, although it had a lot of surprising consequences. (Shameless plug: Scrive is hiring for more programmers if any Haskell guys want to work in Sweden at a startup. PM me for details.)
How about having 'end of lifetime' approach for ghc? For example, ghc-6.12, ghc-7.0 are now before end-of-lifetime, but ghc-6.10 passed end-of-lifetime. So new hackage server warns at least packages that breaks building before end-of-life. We need to have some principled way of maintaining hackage. 
Make it / its source code public? That would be great, I think.
I agree. Microsoft style "Support version N-2 and newer" would be fine even for the short release cycles of GHC, and would do a lot to mitigate issues.
Pingwell is using Haskell for computer vision on mobile devices. 
Haskell' was never going to be a new language, but an *update* to the language. It hasn't "arrived" but it hasn't disappeared either: the Haskell' effort resulted in Haskell2010, the first update to the language spec since Haskell98, and is ongoing. It may never yield a spec entitled Haskell', but it will continue to standardize extensions and release new specs (which I would guess will all be named after the year).
I have not used it yet but yes in theory it should. You should not have to worry about what to install or not to install, but you might need to switch to different [profiles](http://hydra.nixos.org/build/1450948/download/1/manual/#sec-profiles) to get different programs with conflicting dependencies to run.
Here at [Janrain](http://www.janrain.com), we use Haskell for several projects. Our application backend for Capture is implemented using Haskell and [Snap](http://snapframework.com). The application provides a RESTful API to the database. We have lots of complaints, but we're generally happy. We can develop quickly, and the application is very stable and very fast. (oh yeah, and we're always hiring, too)
$1600 &gt;___&gt; Too much for hobbyists. Haskell isn't exactly coporatized enough for that price.
We use my log visualization tools http://jkff.info/software/timeplotters/ almost daily for analyzing behavior (both correctness and performance) of a large computational cluster (which we built with .NET, however).
also, for the packages that use bleeding-edge-new functionality of the newest release of ghc, we need to mark those packages as 'super-experimental' ;-)
The original code was something I'd probably create right now (being a beginner in Haskell and "unlearning" imperative things). Then it all gets turned into a two line, beautiful, version by Phil. That's why I'm learning Haskell :)
I use Haskell for basically everything which is not in Matlab. This can be as simple as scripting, but things on the horizon include for example JIT-ed DSL(s), interactive visualization using OpenGL, or using Snap as user interface / reporting tool (all of which has some proof-of-concept prototypes). This is at a rather small finance shop.
Of course you're not forcing anyone, I was being a bit hyperbolic there :). It's just that discouragement shouldn't inconvenience those who know what they're doing. Perhaps including it as a GHC warning or in a tool like HLint is the more appropriate approach. It lets those who want it continue to have it (and not needlessly break old code if it were removed from the Prelude), but it still layers on a bit of guilt for those that use it without thinking.
What kind of surprising consequences?
I just used Haskell to build a game contest server for university students. The advantages I was looking for was easy specification of the games, strong security by default, and the good and safe multithreading of playing multiple games simultaneously without me jumping through hoops. I got the second two without trying, the first is coming a bit harder. My first pass at the server is going to end up being "the one to throw away", as I learned so much about the problem it's hardly even worth trying to refactor the mess on that side I ended up with. But I know what went wrong now. However, part of why I could use Haskell anyhow was that this was a one-off anyhow and nobody else had to support it. I still wouldn't try to push it into anything production critical for where I work, because very few people would be able to work on it.
Parsec is one of the "killer libs" of Haskell.
I've got to second that parsec love. Easy to write, easy to read, easy to understand. However, alot of people have warned me against it, saying its not efficient. I guess efficiency isn't always the most important factor for a parser lib though, unless its for a high throughput search engine or something.
I'm interested in this, you're using OpenCV bindings? How do you run haskell on mobile devices?
Seconded. The current Haskell Platform does not run well in Mac OS X 10.7 Lion.
At [Silk](http://silkapp.com) we currently use Haskell for almost all our backend work. We have several HTTP based services working together on an AWS infrastructure. Most of our tools are more or less REST based, frontend/view work is offloaded to both Ruby and JavaScript. We also use Haskell for some of our tooling, like deployment, monitoring, data import scripts, worker queues, data migrations etc. Biggest issues we have are related to performance and memory consumption. When a Haskell process suddenly uses 90% of the available CPU time on your machine and doubles in size within a few seconds it is very hard to know what's actually going on behind the scenes. Splitting our codebase appears to be the solution. Initially we started out with a monolithic Haskell server, but we now growing towards a more modular setup with lots of separate tools. This gives us more control over what happens 'in our cloud' and allows us to use tools in written different languages in the same setup. What I miss the most: more insight in what threads are running inside a server and when exactly the garbage collector kicks in. As I've said before, development in Haskell is extremely easy and satisfying, but maintenance can be a big pain.
It'd be nice to have an online course with a more accessible price tag.
Do mean as the server is running? For the offline case (ie. run the server, collecting some data, analyse data post-mortem), are you getting good mileage out of ThreadScope?
Thanks, this is exactly the type of answer I was hoping for. The types of things Silk is doing must use a ton of resources, too!
Note that there's a whole lot more to DTDs than what's covered here; I didn't include full DTD support in xml-types because of how horrible they are to work with. If anyone needs full DTD support, Yitzchak Gale has you covered with his [dtd-types](http://hackage.haskell.org/package/dtd-types) package.
Well, it's easy to see how to turn `head` itself into a total function resulting in a `Maybe` type. But then what about the function that was using it? That's the place you ought to be worried. The code that was using `head` is now... what? a. Using `fromJust`, or throwing an `undefined` or `error` in the `Nothing` case of a pattern match? Then you've just moved the problem, and made the code longer just for kicks. b. Returning some garbage value when it comes back as `Nothing`? See above. Of course, the "right" answer is probably (c), change the logic to using a non-empty list type instead, but then we're well down the rabbit hole to dependent typing; I'd hazard a guess that anyone using a type that means "list with at least n members" for any value of n is going to be wishing for Agda soon enough. That the runtime exception, in the end, comes from `Prelude.head` tells you *nothing* *at* *all* about how difficult it would have been to avoid a partial function.
Could I ask the silly question of what makes this different from, say, function! SetToCabalBuild() if glob("*.cabal") != '' set makeprg=cabal\ build endif endfunction autocmd BufEnter *.hs,*.lhs :call SetToCabalBuild() (One thing I hate about my setup is that if I run :make when I have the .cabal file as buffer, I get lots of error messages in quickfix)
Your solution would run a full cabal build on :mak which is enough if all you want is to position the cursor to the first warning/error, my version, in concert with haskellmode, does the same thing for the current file and additionally slurps all available type info from ghci so you can query it, or insert missing signatures with two keypresses, and other goodies. For that to work, however, haskellmode needs to know what options are needed to compile the current file, which is where all that hackery comes into play, because most files from cabal projects don't compile without comandline options. I'm absolutely certain, though, that my vimscript-foo sucks, so there's certainly more elegant ways to do the evil deeds that are necessary.
Thanks, I'm also using haskellmode although I suspect I'm not getting nearly as much out of it like I should. Inserting missing signatures for example sounds like a great thing. Being able to just plug in missing imports too.
I third it. [hledger](http://hledger.org) got off the ground thanks to parsec.
We use easyVision for prototyping. Alberto's blog has a nice example. http://covector.blogspot.com/2011/05/metric-rectification-from-circles.html. Discussing toolchain development to target iOS with some partners (Well-Typed). Lots of plans for easyVision evolution (arrow notation, gpus etc). We think Haskell is going to provide a great platform for this type of app. Still interested? I can be reached via http://www.pingwell.com/contact-us.html 
Yes, why not turn on 7.0 builds again in Hackage, at least until 7.4?
While I generally really like hackage and the idea of systems like cabal I've had some serious issues with cabal bugs - particularly [#371](http://hackage.haskell.org/trac/hackage/ticket/371). They've always been resolvable but still highly annoying. (I think my last one was Text depending on deepseq but iterating between 1.1.0.2 and 1.2.0.1 depending on what was building. This left me with half my packages working at any time. This wasn't resolved until I added a spurious requirement of deepseq-1.1.0.2 to a package that didn't care about deepseq at all.)
Brindle Waye is using Haskell for a fairly large body of custom reporting and monitoring tasks. Sadly, the user-facing applications are all still Java at this point, but an ever-increasing amount of our direct customer support works involves Haskell. Also, we use a Snap web server in a client-side HTML-based eLearning authoring tool, so that the user can preview some client-side JavaScript stuff without running into browser security restrictions that arise from local-scheme URLs. That's all of 20 lines of Haskell, but Haskell turned out to be one of the easiest languages to package up and ship an executable that contains basically a vanilla stand-alone web server with some minimal logic, and the value per line is very high!
You must be the other member of the Commercial Users Of Agda group then :-)
I'm writing an automation tool that interacts with Ixia, a performance testing application. We're using Yesod for the front end. It's making me a better programmer. Another current tool I'm working on is a Yesod application that schedules and tracks testing jobs that my team does. TL;DR Toolmaker Uses Haskell To Make Tools.
So much for anonymity ;)
At the startup where I work, I prototype tricky logic (things like routing cores for messaging systems) in Haskell. Once I get the properties right, the logic gets translated into Erlang and Python for production. I hang onto the Haskell models, however: they help me reason about and modify the logic when we need to make changes.
We have intranet web app written with Yesod. And an accounting service interfacing with ADP Tax server. More to follow.
 unsafePerformIO is the goto of Haskell.
Not a company, but I'm a Haskell programmer in a lab (PhD student at UCSF). I might be applying to you guys out when I graduate. I use Haskell to do structural bioinformatics queries to guide protein design.
It is of course [on hackage](http://hackage.haskell.org/package/random-shuffle-0.0.2), for the convenience of the reader. 
haha. -1.
&gt; Bad programmers in Haskell can produce equally bad code, e.g. this stupid, ugly GUI library. I think you missed my point, which was that the ugliness of the GUI library is at least in part, and possibly mostly, due to the ugliness of the underlying non-Haskell GUI API being used. Imperative GUI code is some of the worst design in computing today, and that's got nothing to do with Haskell. Your best bet is to be strategic and interface to those systems in a saner way, such as the web interfaces I mentioned. &gt; Good programmers will produce good code in whatever language. Perhaps, but there's still a measurable cost for using less productive languages - it's the reason, for example, that almost no-one writes business systems in C++. &gt; Does Haskell make it easier to write good code? Maybe, but only if you have top notch programmers being given infinite time resource. That's an exaggeration. One reason for this perception is that most people have learned to program imperatively, and have to unlearn a lot to become proficient at Haskell. Incorrect preconceptions are one of the biggest barriers to learning, and are the main reason that programmers with experience in other languages find Haskell to take "a lot of effort". But I think another thing you're suffering from is pioneer syndrome. It's difficult to meaningfully compare Haskell to mainstream languages with decades of industry use behind them. Anyone using Haskell professionally today represents a small fraction of one percent of all professional programmers. There are inevitable costs for using such a niche language. If you feel forced to do so, rather than finding it a useful tradeoff for the benefits the language provides, then you're going to end up bitter and negative about it, as you seem to have done. 
Are you capable of actual, contributing conversations?
-1 ಠ_ಠ
Thanks for that confirmation.
np. -1
Hopefully not the *unforeseen consequences* mentioned in some other domain...
Warning: Oleg
Will the base libraries be changed to use this? It appears they could without breaking backwards compatibility. On the other hand, it would make the type classes harder to understand, especially for newcomers, which may be reason not to do it. I can think of problems I've faced in the past where this would have made for an elegant solution.
So we now need to slip functor in above monad, and maybe change functor to endofunctor, and...
I'll answer for 95% of companies. Nothing at all. :(
We wrote a blog post some months ago about what we had done with haskell at bump: http://devblog.bu.mp/haskell-at-bump We have an update coming soon. Emboldened by our early experiences, we just rewrote a key system using multicore STM. It's been in production successfully for a few months now. 
In C you can write x = x+1 In Haskell you can't. However, to use C it is clear that the equals sign does not mean the same thing as the equals sign in math class. In Haskell, they are the same. This means lots of ideas from math can be used to help you write Haskell programs. This turns out to be a very powerful reuse of knowledge. 
But there are (in my opinion better) alternatives to assignment.
Is Oleg a person?
This is a very common question in this sub-reddit. See cdsmith's recent comment to another thread for some relevant reading material: http://www.reddit.com/r/haskell/comments/l7wtd/why_learn_haskell/c2qi03c There are a multitude of reasons why people like Haskell over other languages. I think in the end you'll need to read up, get your feet wet and see for yourself. Edit: Here's another link that may be helpful: http://amtal.github.com/2011/08/25/why-haskell-is-kinda-cool.html
No mere "person".
["Why Functional Programming Matters"](http://www.cse.chalmers.se/~rjmh/Papers/whyfp.html)
Well, you can't write exactly that. But it is still doable. do x &lt;- newIORef 3 modifyIORef x succ
Of course, you can't do this outside the IO monad... (please, nobody bring up the "unsafe" functions here...)
The problem for backwards compatability is that functions which are parameterised over a (Functor f) constraint will also need to be abstracted over some set of (SubCat f x) constraints, for potentially quite a number of different x. So many existing type signatures will have to change. This is particularly tricky if a function wants to use fmap at a polytype because you can't abstract over a constraint (forall x. SubCat f x) in Haskell.. yet.
Haskell's type system is like no other. "side" effects are encoded in the type system, allowing function type signatures to make much stronger guarantees. getArgs :: IO [String] For example, `getArgs` is an IO action that produces the list of program arguments as a String. You can't access the program arguments just anywhere. plusOne :: Int -&gt; Int plusOne x = x + 1 Because the contract of `plusOne` does not mention anywhere that it is an IO action, it therefore cannot execute IO actions itself. Haskell's type system encourages "pure functional" programming, where you write functions in a referentially transparent way, meaning that the function is influenced only by its direct inputs, and influences only its direct outputs. Pure functional programming is what mainly sets Haskell apart from other languages. But there's tons of other reasons why people love Haskell. See also: [Why Learn Haskell slides](http://ugcs.net/~keegan/talks/why-learn-haskell/talk.pdf)
and Haskellers consider the inability to perform IO outside the IO monad a Good Thing^TM
In simple terms, Haskell is mostly about correctness and elegance. Correctness is an important property of software which has often been neglected in the past. For instance, if you write a program in C, then you can only do a lot of testing to ensure that it is bug free. However, you cannot prove it to be bug free, no matter how much testing you do. In Haskell, however, we can prove most functions to be correct because Haskell is very similar to Maths. In terms of elegance, Haskell syntax is arguably much cleaner than the syntax of a C like language. Additionally, modern language features such as pattern matching and lazy evaluation allow us to write neater programs and solve complex problems in fewer lines of code which are ultimately easier to understand. If you come from an imperative background (especially with poor understanding of Maths / Computer Science theory) then it can be a bit difficult to get used to. A good book for beginners is Programming in Haskell by Graham Hutton which explains most of the fundamental concepts in Haskell in a very simple and intuitive way. There are also various other resources online such as [this book](http://learnyouahaskell.com/) which goes into more detail in its later chapters.
Read [Learn You A Haskell](http://learnyouahaskell.com/) for a hands-on introduction to Haskell. Why use Haskell? Haskell is type-safe. Where other languages let you write code that mistakenly passes a `string` to a function that takes an `int`, Haskell refuses to compile such code. There's a joke in the Haskell community that when your code finally does compile, it's guaranteed to be bug-free. There's a grain of truth in that, because most bugs are the result of poorly-typed code. Although Haskell is strictly typed, its type system is also rich. You can easily write your own custom data types, called `records`, and Haskell can automatically figure out how to print, compare, and sort instances of your types if you write `deriving (Eq, Ord, Show)`. Haskell's algebraic type system lets you construct data structures of arbitrary complexity, by building onto types such things as `[Thing]`, a list of `Thing`s, or `[[Thing]]`, a matrix of `Thing`s, or `Tree Thing`, a tree of `Thing`s. Again, Haskell automatically figures out how to print, compare, and sort these abstract types, too. Haskell is pure. Code for computations, `x = 2 + 2`, is separate from code for side effects, `putStrLn (show x)`. For newbies, this one is an inconvenience, but once you get used to it, it's quite powerful. Haskell is parallel. Because Haskell is pure, parallelization is trivial. As long as code has no side effects and doesn't share variables, it can be run in multicore mode. See [YelloSoft](http://www.yellosoft.us/parallel-processing-with-haskell). Haskell is lazy. Because Haskell is pure, Haskell can intelligently decide when to perform calculations. It's normal to write a Haskell function like `primes :: [Int]` that returns the infinite list of prime numbers. How do you use such a thing? You only take a finite chunk of numbers from it at a time. `take 5 primes` -&gt; `[2, 3, 5, 7, 11]` Haskell is declarative. Haskell's pattern matching allows you to write clear, concise functions. fib 0 = 0 fib 1 = 1 fib n = fib (n-1) + fib (n-2) It's just like a partial function in mathematics. Where other languages use try/catch blocks to handle errors, Haskell code mostly uses the `Maybe a` type, where `a` is a type of your choice. If you're parsing an integer, you'll could use `maybeRead :: Maybe Int`, which either returns `Just x`, where x is the successfully parsed integer, or `Nothing`, to signify that the parse failed. The type system itself is the error handling mechanism.
[in multiple places](http://hackage.haskell.org/packages/archive/random-extras/0.18.1/doc/html/Data-Random-Extras.html#v:shuffle) edit: and random-extras does it better, because you can get the resulting random generator back out after shuffling the list
[Why Haskell just works](http://www.haskell.org/haskellwiki/Why_Haskell_just_works)
Ah, I see. Thanks for the explanation! That makes this a little less exciting; I was hoping to be able to sneak in a few extra constraints in instances of standard type classes...
We haven't really tried ThreadScope yet, as our (live) problems are hard and time-consuming to reproduce on development machines. We'll take another stab at it soon, though.
I think by "layperson" he meant a programming layperson...
I think rimmjob_'s [comment](http://www.reddit.com/r/haskell/comments/lho6g/please_explain_haskell_for_a_layperson/c2stjmi) is the kind of answer that matters: because it's fun! But it's more than that. In industry-standard languages, you often run into things you just can't solve elegantly. Any and every solution has to be a hack. In Haskell and related languages (because Haskell is *not* the only language that can do this), you get the feeling that there *is* an elegant solution to every problem. Maybe we haven't figured it out yet, maybe initial shortcomings in the language design make them tough, but they're out there. That is a great feeling.
Like no other except others with System Fω type systems. Lots of languages have strictly typed functions like Haskell has, but few of those have parametric polymorphism. Lots of languages have something *almost* like parametric types (in the form of generics). I don't know of any other that has type inference, however, but that part is kind of useless for me, since I really like to type everything so I can see what's going on.
You don't have to go that far. let x = x + 1 in ... works perfectly fine.
Within the do block, isn't _ &lt;- mask $ blah blah the same as mask $ blah blah ?
One word: composition. The bane of modern software engineering is that software is complex, and no one can sanely hold all the details of a large project in their mind at the same time. The key is to find ways to express the result in smaller pieces which are individually understandable. The problem then becomes that when you take two pieces and put them together, they end up doing something you didn't expect or want. Haskell, more through _culture_ than anything else, manages this. Various people here have pointed to technical merits, but at the end, it's really the culture that these technical merits have encouraged/forced/enabled which allows for this.
Does it? ghci&gt; let x = 3 in ( let x = x + 1 in x ) *hangs* Since Haskell allows recursive definitions, `let x = x + 1` will not use `x=3` in the expression `x + 1`. Instead, it will keep expanding `x = x + 1` indefinitely in order to evaluate `x`. There is, of course, no problem at all with ghci&gt; let x = 3 in ( let x' = x + 1 in x' ) 4
One of the coolest examples of Haskell's type system being used for a *practical* application is Simon Peyton Jone's article Beautiful Concurrency, which discusses the design of a module for "Software Transactional Memory", or STM. STM allows you to run parts of your code in a transaction. Inside a transaction, you can read and write to mutable variables (called STRef's). However, until you commit, your reads and writes are not actually visible to the outside world. If something doesn't go right inside the transaction, you have the ability to abort, in which case, the world never sees your changes. In a traditional language, like Java, this almost works. Accept that *any* function may invoke irreversible side effects. The colorful example SPJ loves to use is "launch missiles". Inside a Haskell STM transaction, the ONLY actions you are allowed to perform are READ from an STRef or WRITE to an STRef. You can't launch missiles or do anything else that STM doesn't know how to "undo" in case you abort. PDF: http://research.microsoft.com/en-us/um/people/simonpj/papers/stm/beautiful.pdf
Whoops. Odd, my understanding of the Haskell variable system was that the x+1 would be evaluated in a different environment than the low x, but now that I think about it that wouldn't work, given the ability to do things like `let x = f x in x`. Hm. Downvote my original comment please.
I have seen type inference in ML and Scala, as well as a few toy languages. I've never seen a (non-toy) language besides Haskell whose type system makes guarantees about effects.
It's not useless to you, type interference will do more than that, it resolves types for polymorphic functions across a whole chain of them etc.
It works fine. Haskell solves the equation and sees that x = ⊥
Well, effects in Haskell are just a type, so if you force all of your IO primitives to have that type, you get guarantees for effects, iinm.
Well, when I hear type inference to me it means you can leave something untyped and Haskell will find the type for you, whereas what you describe is, to me, just the consequence of type checking: the thing won't type check unless this and that have the relevant types. If that's included in type inference tho, then any language with overloading has some sort of type inference.
I'm with barsoap here. monad transformers are usually better used to define your own monad with the stuff you need, like newtype MyAwesomeMonad = MyAwesomeMonad (StateT statebit (... IO)) deriving (MonadState statebit, MonadIO, ...)
That's a bit of a tautology, isn't it?
More specifics would be nice. I can see the extra effort, but for me it's often worth it. It seems to be almost as hard to figure out what kinds of values you want to use in your program in a dynamically typed language, and once you capture part of the design in types the compiler take care of checking it for you, so you don't have to keep as much in your head as you work on the rest of the program.
unsafePerformIO is far worse than goto. Goto does not sweep stuff under the rug like unsafePerformIO does.
The learning curve of Haskell is high. The rewards are pretty high too in my opinion, but it's not for everyone or every project either. Go is immediately accessible, pretty sane all in all. I tend to disagree with the claim that it is ugly. Ignoring programming language research? To an extent I'd say yes, but it doesn't ignore programming experience. The people involved have written some of the cleanest C code I've ever seen in my life, had a hand in designing two of the cleanest OS architectures I've ever seen, both of which are incredibly portable. Plan 9 runs on everything from storage appliances to Blue Gene to guruplugs, and Inferno, based on Plan 9 with an ancestor to Go as the application programming language can run hosted on another OS or natively on the Nintendo DS. Clearly the people behind Go know something important, and that's "how to get work done." That's not to say Go is the perfect language for every scenario either... 
For the "lay person"...."hard".
ST monad is not unsafe &gt;.&gt;
I don't understand the price tag. That much money for a couple of days in which somebody tells you things that you can find online for free?
I totally agree with that. It's just that I see other statically languages do achieve what you said to some degree--maybe not as flexible as Haskell--but with much less effort. But again, we are speaking without some actual code. So, this is bound to become just another rhetoric. Let's not start another fight.
If you understand why in physics it is valuable to be able to count with units, in addition to the numbers, and reason with them, you already understand something about types and values in Haskell.
Disregard all these guys in this thread. They are too good to remember the simple beauties of Haskell. I'll tell about my little, lovely experience (an almost one year old experience): * Destructuring: Decompose data structured "on the fly". * Pattern matching: gracious `swith` syntax replacement * Guards: You'll no longer write nasty `if`'s in function bodies. * Partial function application * [Data.Data](http://www.haskell.org/ghc/docs/latest/html/libraries/base/Data-Data.html): can't explain it but I see as something awesome that I still can't uncrack * AND A SHITLOAD OF NEW EXPERIMENTAL IDEAS.
None of that was invented in/for Haskell (it's all common stuff to do in Lisp, for instance), so although cool, I don't think it captures Haskell per se.
Yeah, but it differs. You know how it goes &gt; Simple is great &gt; Expressive it's excellent.
I was excited about [this one](http://www.reddit.com/r/uorfip/) (free) but the teacher seems to have kinda forgotten about it :(
Yep, it's certainly different.
A minor point: STRefs are used in the ST monad, not the STM monad. You're thinking of TVars.
A lay-programmer?
Can you elaborate on how Haskell (or any functional language) does composition better than functions (the procedural kind) and classes in C++ or Java (or any procedural languages)?
In Haskell, more errors are caught by the compiler than in most other languages. Haskell allows you to write relatively short programs without sacrificing readability.
There's also no problem with &gt; let x = x + 1 in 2 =&gt; 2 Laziness. Sitting on your [bottom](http://en.wikipedia.org/wiki/Bottom_type) is OK.
Perhaps that's a little too brief. ["_Mostly_ hard"](http://en.wikipedia.org/wiki/Mostly_Harmless#Title)
Clean.
Ok, I guess you are right about that. Note also that haskell functions can be polymorphic in the return type.
Here, in Mother Russia, I use Java, C# and sh scripts to fix bugs in financial software at outstaffing company. But after job I’m heavily using Haskell to troll programming forums and individuals. Cool stuff from Utrecht University, such as uu-parsinglib, helps me much in my duties and Haskell’s type system advanced features, such as rank-n types, allows me to troll even hobbyists language developers. I also read “Categories for the Working Mathematician” to pas the time on my way to job and back home, I’m sure it’s pretty cool practical application of the category theory. Finally I have multimedia internet site, written eternally in Haskell, which I’ve created on weekend, when I was not drinking vodka and playing balalajka.
In a functional language, a function returns a value, nothing more. No no modifying inputs, no mutating state, no different results next time, no side-effects. This means that if you compose two functions, the composite will behave entirely predictably. Additionally, the functional languages tend to have far more powerful and more clean type systems and metaprogramming capabilities.
Why can't I do the same in C++? Say, I separate logic that has not side effects with those that do, and my C++ functions always return a new value instead of changing any global or input arguments. Type system is not an unique feature of functional languages. Lisps does not even have types.
Really cool! Have you thought about how to handle mixed content? (f.e. a link in the middle of a sentence.) This is often clumsy in data formats designed for programming, like f.e. JSON.
Honestly, I'm really not sure what the use case is here. Templating is essentially a solved problem at this point. As a guy in the trenchs, how would this help me solve the _hard_ problems, things like making it less painful to maintain state across requests, make interactive applications, etc.
Very nice work! simple and elegant. As for event processing, how about considering this approach http://jaspervdj.be/posts/2011-10-16-type-safe-events.html ? Although FRP is great, it may be quite difficult in implementation, I guess. 
Indeed. Thanks.
I must disagree. Templating as done by your average templating language (e.g. Python's Cheetah) has lots of undesirable properties, including being non-composable, not dealing with security issues (like XHR attacks), and not guaranteeing well-formed output. The state of the art (http://groups.inf.ed.ac.uk/links/formlets/) is quite a bit better, but not widely used and still haven't addresses issues like asynchronous requests from JavaScript or XHR. Although the latter should be easier to address using formlets than it is in e.g. Cheetah.
Yes, that's something you don't find in hardly any statically typed language. But even that is just type checking, not inference, to me. I'm not saying my definition is "correct" tho, I'm just saying this is what I think of when I hear these terms.
State of the art for templating I would consider something more like mako, which supports inheritance, automatic escaping, etc. Haml is really nice too, and does guarantee well formed ness.
The basic problem no templating languages I've seen (except formelts based ones) compose e.g. given a function that generates a date widget for a form you cannot call the function twice to create a form with two date widgets (think a flight booking form). The problem is that templating languages builds forms like this: def draw_date_widget() &lt;input name="year"&gt; &lt;input name="month"&gt; &lt;input name="day"&gt; so calling it twice like so: def draw_flight_booking_form() &lt;form method="POST" action=".."&gt; draw_date_widget() draw_date_widget() &lt;/form&gt; doesn't work as a any given 'name' can only occur once in a form. There's no good way to fix this in commonly used templating languages. Bad solutions include: def draw_date_widget($year_var, $month_var, $day_var) &lt;input name="$year_var"&gt; &lt;input name="$month_var"&gt; &lt;input name="$day_var"&gt; which forces you to perform non-local changes to your program in order to thread around names. The problem applies to CSS class names and IDs. Programming in templating languages is like programming in a programming language where all variables are global. We've just gotten to used to it to see how much it blows. 
Hachicode (http://hachicode.com or http://8cd.me) uses Haskell for nearly everything. Our entire web backend is written in Haskell except for our OCR binary which uses tesseract and some custom image processing in Leptonica. We'd like to move the IP to REPA/DevIL at some point as it seems like a very natural application but Leptonica just has so many algorithms built in its hard to say no. Also, our client app is almost entirely sencha/phonegap, **but** its sencha javascript that is generated by [yesod](http://www.yesodweb.com) and its julius and hamlet templating language. Its pretty cool that we can update 99% of the logic and user interface of our app over the air without a trip to the app store with Apple's blessing. I'm not sure why [yesod](http://www.yesodweb.com) scares people away. Template Haskell is nothing to fear and it allows for a ridiculously awesome amount of static typechecking in a webapp. Type-safe URLs have saved us so much effort. Hamlet files, which any web designer can understand in about 3 seconds give us a great way to let our designers work on our app w/out a developer watching. 
I completely fail to see how this resembles an actual problem. def draw_date_widget($id): &lt;input name="{$id}_year"&gt; &lt;input name="{$id}_month"&gt; &lt;input name="{$id}_date"&gt; This is a drop in the bucket compared to the hard part of writing a web app. If you're really concerned about things like this, I would suggest the proper angle of attack would be get rid of templating together and start looking at a much higher level of abstraction. 
I'm mostly doing the following: 1. Bought "Haskell: The Craft of Functional Programming" (3rd) 2. Subscribed to haskell-beginners and haskell-cafe 3. Trying to keep an eye on #haskell at Freenode 4. Reading lots of code
Good question! Initially it _was_ just a basic templating library in Haskell. The problem is turning Haskell code into javascript when it depends on user input (i.e. if something needs to be recomputed). I could have just stuffed everything in a monad and compiled the monad to JS, but I felt like this was a fairly clunky solution. It also tied me to a large conceptual framework that is largely unnecessary for what I want to do.
a sidenote, I just happen to be working on a project that uses Cheetah and it supports inheritance (sort of) the idea is that you define the placeholders in the base template **compile it** and then use `#extends` on the children, those in turn will define the placeholder contents as methods using `#def` it's far from pretty or convenient but it works.
Right now it would be something like this: text $ "Check out this " ++ link "http://www.elm-lang.org" "site" ++ "." My more long term thought would be for text-based content to be specified primarily in a separate markdown language (much like the one used for reddit comments). This separates content from design in a nice way. Text just becomes data that fits into a larger layout framework. Contrast this with HTML/CSS/JS in which text and content are an fundamental part of the layout. If you want to change the formatting you have to actually mess with the content directly. Does that approach make sense? edit: Is that what you meant? In the case of structured data (similar to JSON but well-typed), you could define a render function that converts your record or tuple or list or whatever into an element. (e.g. render :: { firstName :: String, lastName :: String, age :: Int } -&gt; Element)
Sorry to jump in so late, but getting rid of templating is exactly my goal! As I see it, the fundamental abstractions in HTML do not really make sense for a GUI language. It was designed for text and is totally adequate for that task, but once you start trying to have complicated graphical elements (e.g. drop down menus) the abstractions of &lt;div&gt; or &lt;ul&gt; are not really what you want anymore. I am pretty much on the same page as tibbe: &gt; Programming in templating languages is like programming in a programming language where all variables are global. We've just gotten to used to it to see how much it blows. Great quote by the way :) I'll share some of my ideas of how to deal with dynamic content in a second. I think they can address security issues by making certain classes of mistakes impossible.
You can do exactly the same with ghc. gcc -c prog.c -o prog.o gcc prog.o libfoo.a -o prog and lo, with ghc it's the same... ghc -c prog.hs -o prog.o ghc prog.o libfoo.a -o prog 
Thank you! :) I need to take a closer look at that link, but I think this idea and FRP are different ways of addressing the same issue: the world changes and we shouldn't need to manage these changes manually. Just today, my thesis adviser told me to take a look at Concurrent ML which deals with many similar issues from an entirely different perspective.
You can do the same in C++, it's just that (1) C++ requires a ton of boilerplate that is death to code readability/maintainability, and (2) C++ doesn't provide you very good tools for taking advantage of the composability. I've heard that C++11 has made some strides toward remedying the latter feature. I'd love to hear a summary of these features from someone who's looked into them in depth. 
From practical point of view, I also suggest to implement very restricted version of IO and IORef for addressing event processing although not elegant at all. With IORef, event call-back function can have one-to-one correspondence with javascript closure. So the implementation is relatively easy compared with more elegant approach. Later, we will be able to wrap those IO kitchen sink with FRP abstraction. (as we do, for example, when making FRP library using gtk2hs / wxHaskell) 
I think tibbe is putting things a better than I can, but here is one example program (that doesn't compile right now) that gets at why my approach would be nicer than templating. toCelsius f = 5 * (f - 32) / 9 main = let (fahrenheit_text_box, fahrenheit_input_string) = input celsius = asText . toCelsius $ read fahrenheit_input_string in flowRight [ fahrenheit_text_box , text $ " in Fahrenheit = " ++ celsius ++ " in Celsius" input gives back a tuple of a text box and a string that actively updates to match the contents of the text box. You can just use it as you can any normal value, but things will be recomputed as needed. I should note that the use of read is not really safe here because the user input could be malformed. This can be handled pretty easily though. Because graphical elements are embedded in the host language, input validation can be part of the definition of an element rather than a property that you maybe enforce later in JS. edit: added another example in the comments.
BTW, this approach reminds me of Ur/Web, on which, though excellent, one of my personal complaints is that the grammar is somewhat different than haskell : http://www.impredicative.com/ur/
The guy who designed Ur/Web was actually at my school until pretty recently. In making this I have been realizing how many extremely smart people have been working on very similar issues with very similar approaches. Ur/Web is quite a bit fancier than I'd like to go. I think this will make everything conceptually simpler in my language. The real reason is not entirely readability and clarity; it is also because I am not knowledgeable enough to understand and use the machinery behind Ur/Web (I am not a seasoned dependent-types wizard like Adam Chlipala :P).
Agda below Pascal on "has a strong type system"?!
I _really_ like the idea of web-based compilation (like an in-browser REPL/editor). Imagine having no installation or config for a compiler/libraries/profiling/testing! That is what I was going for with my example framework, but it could be way more robust. I thought about just using ghcjs, but I think there are many parts of Haskell that you explicitly _do not_ want for GUIs and, more specifically, the web. * I didn't want to deal with explicitly effectful computations from the beginning (i.e. the IO monad). Many useful parts of IO can be rephrased as Streams of data or Signals (in the FRP lingo) in which the data you want (mouse position, key presses, etc.) can be streamed in a way that feel functional. * Laziness is also trouble. Consider a long-running computation on mouse movements that only gets displayed every minute or so. With lazy evaluation, this becomes a GIANT thunk that gets evaluated all at once. I think this sort of thing happens frequently enough to make a call-by-value more desirable. That said, I would love to have a well-developed language like Haskell up and running for direct client-side programming! I messed around with GHC briefly and was pretty overwhelmed, so I don't think it can be me! Thank you! With so many smart people already working on this, I'll need the luck :) I need to take another look at Ur too; thanks for reminding me of that project.
I am not sure if I understand the problem exactly, as I haven't done an extreme amount of templating, but I think my language solves the general problem by making elements more modular. I think my reply to the same comment you are responding to gets at this issue.
I didn't include this sort of example because they don't compile yet, but here is one that gets at the issue: -- checkboxes' :: [(String, a)] -&gt; [a] transforms = [ ( "Italic" , italic ) , ( "Bold" , bold ) , ( "Underlined", underline ) , ( "Red" , textColor "red" ) ] message = "Hello, World!" main = let (boxes, ts) = checkboxes' transforms in flowDown [ text "Choose some text transforms:" , boxes , text $ "Transformed text: " ++ foldl ($) message ts ] In this example, checkboxes' returns a two things: * a checkbox element in which each check is labeled with a string. In this case "Italic", "Bold", etc. * a time-varying list of the values associated with the boxes that have been checked. If someone checks "Italic" the list becomes [italic]. If they then check "Bold", the list becomes [italic,bold]. The computation in main would be recomputed as this list changes. I posted another example elsewhere in this thread that might also clarify.
Right now I am planning on punting on IO for as long as possible. I took a look at FranTk (FRP with TK), but I thought that accepting the underlying imperative framework made the end result much more complicated than it inherently needed to be. edit on the topic of implementation difficulty: FRP is not exactly a pretty picture when it comes to implementation. That is definitely a big concern. Right now I am focusing on the theoretical side of the question 'how should one write GUIs or websites?'. I think you have to handle effectful code (IO and IORef type stuff) no matter what. It is more a question of who gets to manage and maintain it: the coder or the compiler.
I don't think this is a templating language by any estimation. It's a functional language for generating webpages.
Some more technical questions... Did you write your own parser, or is this on top of haskell-src-exts? Are your semantics lazy or strict? What do you plan to do regarding custom data types? How strongly typed do you inted to be? Also, it looks pretty neat so far. However, the FRP/interactive stuff is where many a project has foundered on the rocks of efficiency and subtle dependency/concurrency semantics. I look forward to seeing your approach.
On the topic of creating dynamic GUIs and web pages: I didn't include these examples on the site because they don't compile yet. Here is one that presents users with a series of checkboxes corresponding to text manipulations (italic, bold, etc.). As you check the boxes, the manipulations will be applied to some example text (message). -- checkboxes' :: [(String, a)] -&gt; [a] transforms = [ ( "Italic" , italic ) , ( "Bold" , bold ) , ( "Underlined", underline ) , ( "Red" , textColor "red" ) ] message = "Hello, World!" main = let (boxes, ts) = checkboxes' transforms in flowDown [ text "Choose some text transforms:" , boxes , text $ "Transformed text: " ++ foldl ($) message ts ] In this example, checkboxes' returns a two things: * a checkbox element in which each check is labeled with a string. In this case "Italic", "Bold", etc. * a time-varying list of the values associated with the boxes that have been checked. If someone checks "Italic" the list becomes [italic]. If they then check "Bold", the list becomes [italic,bold]. The computation in main would be recomputed as this list changes. I posted this and another example elsewhere if you want to see some other ideas.
tl;dr: it's all in Haskell and it has been a lot of fun wrestling with Haskell for a decent sized project. I wrote my own compiler in Haskell. In fact everything is done in Haskell (the compiler and the web server). * I use parsec for lexing. * Once I have a tokenized representation of a program, I use a custom library for parsing the tokens. Having my own parser/combinators is not ideal, but I like having this degree of freedom. I also had extreme difficulty using just parsec for the parsing phase. * Translation to JS is just a function on the AST that is built up during parsing. * The site done using HAppStack which (although not trendy) has been really great so far. I looked at using GHC for all of this, but there is just *so much* complicated code! I didn't want to get bogged down trying to understand almost 20 years of extremely advanced/clever tricks and optimization. It's not that I don't want to be clever (I do!). I just can't possibly understand all of the detail that is there. 
campo-espacio
* I wrote my own parser. * The semantics are almost certainly going to be strict (specifically call-by-value). This solves some space leak problems with FRP (see [this space leak](http://blog.edwardamsden.com/2011/03/demonstrating-time-leak-in-arrowized.html)). * I plan on having at least basic ADTs (such that type inference is decidable). Is that what you mean? * Strongly typed in the Haskell or ML sense of the term. For things like JSON this means required that records have certain fields of certain types. I'd like to have nice extensible records a la [this paper](http://research.microsoft.com/pubs/65409/scopedlabels.pdf). So far, FRP has been really challenging. I thought I would just cut away FRP features that were non-essential to GUIs, but it is proving to be a much more nuanced problem than I thought. I am now in the process of being overwhelmed by the volume of relevant research on these questions :) Thank you for your interest; it is really encouraging! I'll definitely post again when I've got a prototype working :)
Let me ask a question, and please don't take offense at this. How much actual webdev work have you done in the real world? I'd like to share my observations from ~5 years in the trenchs (and several years before that doing web apps for internal issues) * Designers don't code. They do HTML. If it looks mostly like HTML they can handle it. But doesn't expect too much. * Designers don't always build templates, but it's really nice for them to be able to it. * Modularity of the sort you seem to think of isn't that big of a win. Parent/child template inheritance is all in practice I've ever needed. Again, this seems like solution in search of a problem. 
my reply was only about template inheritance in Cheetah
I agree with this however there's something you're ommiting, "designers" exist but also "frontend devs" exist, the latter *will* code, they'll make the js-heavy parts and they use turing-complete templating systems whenever available, that's why I dislike templates that try to hide programming, they're not for designers but designers do most of their work in photoshop not even HTML.
in general, you do not create the page isolated from the code who will receive the request data and it's that code the one that defines the names and avoids collisions, because it's the one who expects those same names back.
No offense taken at all. I thank you for considering my language enough to even ask :) This is a weakness that I am well aware of. Since I am still in University, I have only done web-dev stuff during internships or for personal projects. This greatly skews my view of what it is actually like to use this stuff in the real wold. Nonetheless, I think I _do_ have a valid complaint. My complaint is with HTML/CSS/JS as an abstraction for GUIs. That's not to say that it doesn't get the job done. I just think it could be cleaner and more abstract, and thus, more reusable. I definitely see where you are coming from though. Furthermore, it is usually a bad idea to start from scratch. I think it's a good idea in this case, but so does everyone that does stuff like this :P Can you be more explicit about the complaint that "Designers don't code"? This may be the case _right now_, but I don't think that is inherent to designing a web page or designers abilities. HTML/CSS/JS is a pretty complicated to wrap your head around even for a programmer because there are so many different pieces. If a non-programmer restricted themselves to the 'Elements and Layout' portion of the language, they would have the same expressive power as HTML. I argue that it's even a bit nicer to use because you can abstract away complicated layout code. Some dev writes it once, and any designer can use it just like 'bold' or 'centerX'. As a non-programmer improves, they could incorporate more advanced language features into their designs without needing to learn a whole new language. It seems like a more natural way to do things. I'll link you to my thoughts on modularity in a little while. It is relevant to your objections. Also, I hope you'll give the language a chance once it has developed more! edit: thoughts on [Modules](http://elm-lang.org/Modules.pdf). The Style module would allow you to snap in a style to an entire GUI and get reasonable results. Widget libraries could take a style module as an argument. This means that a drop down menu (or image viewer or API wrapper, etc.) could be restyled just by passing in your particular style module. This would be really convenient.
&gt; I argue that it's even a bit nicer to use because you can abstract away &gt; complicated layout code. Some dev writes it once, and any designer &gt; can use it just like 'bold' or 'centerX'. It's say that's another of these great-in-theory, doesn't really work in practice kind of things. If something is complex enough that that approach is worthwhile, then it will in practice be pretty picky about where you can stick it. Again also I'm not seeing why this is better than partials/template inheritance. Or for that matter CSS inheritence. The general feeling I'm getting from this is that works very well on paper, but could get really, really painful when you need to tweak something on a specific page in a very specific way.
I wouldn't categorize this as a template language issue. In Yesod, for instance, the Handler and Form monads both provide the ability to generate unique IDs. The former is used for id attributes, the latter for name attributes, since an id must be unique throughout the entire page while a name must be unique throughout just the form. Hamlet itself doesn't provide any functionality to help out with this. And yes, the idea was definitely ripped off directly from formlets.
Better phrased than my own response :) Thanks! My experience with frontend stuff has been all for a company that does tons of JS heavy frontends. I was doing map/reduces and such, but I got to use AppEngine+JS in this setting a little bit. Using templates in python was doable, but it certainly wasn't beautiful.
Exactly! I think of it as a language which compiles to a graphical element. In fact, main won't type check unless it evaluates to an Element :) This approach makes potentially dry things like Pascal's triangle much more visual and engaging :)
Again I see your concerns, but they are equally (if not more) applicable to HTML/CSS/JS. It would be a nightmare to do what we are talking about with existing languages. If my approach is slightly less nightmare-ish, I think it is worthwhile :) At the end of the day, I think we just disagree. It will be clearer who is right as my work progresses though. Maybe it's you. For the sake of my thesis, I hope it's not though :P Keep in mind that this _is_ an academic thesis project. Failure is a likely option. Part of the fun is that the theory is pretty cool. edit: I just realized we may be talking about different things. As the language is now, it is definitely not a suitable replacement for HTML/CSS/JS. Totally agree. Static web pages are not enough. Nothing but the simplest of web pages could be made with Elm today. It is the language extensions that I am working on now that will make it a viable alternative. [This](http://conal.net/papers/icfp97/) may clarify. I think my thesis is to address the "_hard_ problems" to which you refer :) Maybe we do agree after all?
Thanks, guys! You guys are the reason I still hang out on Reddit.
You must have looked at Wadler's Links: http://groups.inf.ed.ac.uk/links/ since you mention formlets (where they came from). How is you project different from Links ? (apart from being active -- Links is very much in limbo AFAIK) In any case, good luck with your project ! I am still waiting for a language that will significantly ease the pain of web development. PS: other projects to look at: Ur/Web, OPA-lang, Lift/Scala, Mobl (http://www.mobl-lang.org/), Flapjax. They all offer some form of reactive programming.
I've actually never seen Links. Thanks for the pointer! It looks pretty cool, but it appears to keep the underlying HTML/CSS abstractions in place. I am trying to do GUIs from scratch (no tags, ids, or classes). I'll take a closer look tonight though. Having server and client code in one place seems nice though (as with Opa). I toyed with the idea of being both server-side and client-side, but it was just too much to handle. I couldn't tackle all of that complexity at once.
Yeah, that's a complete absurd @_@
Polls like this are bound to have problems due to people subjectively exaggerating things, and due to the fact that everyone's favorite language is not the one he/she uses for work (or school). Looking at this poll, Objective C supposedly has more user responses in common with Fortran than with Smalltalk (which was a primary influence on its design) and than with C (of which it is a superset): http://therighttool.hammerprinciple.com/items/objective-c Objective C is not my favorite language either, but c'mon, Fortran? Fruit hatred must run deep among the respondees.
To add to your literature survey... You might want to have a look at the FRP implementation I wrote in Agda, together with an Agda-to-JS compiler, with links to the DOM model. It's agda-frp-js on github.
Looks interesting, shall try.
Haha, thanks! I could always use more :P I'll definitely take a look though. This sounds more relevant than most other things I've seen.
You can write C++ that way, but C++ is not designed to make that easy for you. And there's no guarantee when I look at some other programmer's 'Shape* mkCircle(int);' that it doesn't have side effects. But in Haskell, when I see a function 'circle :: Int -&gt; Shape' in Haskell, I know that it won't have side effects (modulo unsafePerformIO and friends)
its a programming language
Also, while you discount those with industry experience at your peril, remember that people "in the trenches" become used to the tools they have and are resistant to change. When ALGOL and other structured programming languages first came on the scene, FORTRAN and assembly programmers "in the trenches" were resistant, feeling that abstractions like loops and conditional blocks weren't worth it "in practice." They said the same thing about object oriented languages, and now they're saying the same thing about functional languages. You need to be careful about taking industry experience too seriously.
The paper [Why FP matters](http://www.cs.utexas.edu/~shmat/courses/cs345/whyfp.pdf) explains that pretty well.
To support the functional style, you need more than the ability to define functions. You also need: * Syntactic support to make it practical (For example, defining a generic composition operator of functions in C++ is going to be syntactically very heavy) * Idiomatic support. If you write functional code in a language that doesn't support the functional idioms you're going to cast yourself away from the language's community and libraries. And you're likely to render the compiler unable to optimize your code and compile it properly. * Lexical scoping * Library support (or you'd have to re-implement the entire Haskell prelude to make it practical) * Most Lisps are not much of functional languages. Common Lisp is not any more functional than Python, for example. A Type System gives far more benefit in a purely functional environment, because the lack of side-effects means your types tell a lot more of the story. When the types tell much of the story (what the functions do), type-checking checks a lot more. This is why a pure-functional program that type-checks so rarely has bugs pass through. 
No
Yes, but since we're on it, let's ask for 7.2 ;-) GHC 6 served us well in the last 2 challenges, but the possibility of any speed gains are worth it, specially when not writing reactive agents.
I would guess because that's what's in Ubuntu's universe repository.
It took me long enough to get them to go with 6.12 (not that my efforts had anything to do with them using 6.12 now, anyway). They were still on 6.8 last time.
This is an interresting work. If I had to make my own perfect system to deal with web applications here is what I would want in the end. 1. Never ever use JS/HTML/CSS again. 2. The ability to control where computation occurs: Client/Server/Persistent Server 3. Completely think again the web layout system. The best system I ever used was the one used by Cappuccino which is similar to the one used by Cocoa (on mac). Here is a [link to a tutorial](http://cappuccino.org/learn/tutorials/automatic-layout/). 4. Also related to 3, a designer should be able to make a very nice photoshop image, with an hyperstrange layout (path, circles, text in diagonal, etc...). It should work nicely. 5. Find a way to make non trivial rendering: text on path, fractals, animations. A good example of rendering language is [metapost](http://tex.loria.fr/prod-graph/zoonekynd/metapost/metapost.html). 6. Make wonderful design (animation, shadows, special filter effects, etc...) might not be able to be done only in a browser or by programming it. Remember designer aren't programmers. It shouldn't be too difficult to create an UI Builder to create, visualize and manipulate the application like webpages. 7. Related to 6, It also shouldn't be too difficult to create a program which generate, nice design/animation. Typically, from a photoshop image with a description of the behaviour it should be as easy as possible to create a website. Remember, designer in a project should be able to be creative without thinking too much about how they could make it possible. These firsts seven point were (mostly) only about the "display on the client" problem. Now a complete web application is something very difficult to do. Particularly, passing informations from one point to another of a "flow". Display data and save them in a persistent store. I recently written a [post about yesod](http://www.reddit.com/tb/lj2br). And the type safety property of yesod should clearly be a necessity in a system that want to hide JS/HTML/CSS and also (I believe) SQL. Also another thought. In the end the ideal system should be create a: full js client side rendering &lt;-&gt; RESTful API server &lt;-&gt; Persistent Store Layer Having the ability to deal with only one langage to generate all this would be awesome. I have another remark. GHC is incredible. Making a system for which you state cannot generate anything in JS but the server side compile using haskell might be better for me thant a completely new language without all incredible optimization made by GHC. I wish you good luck!
I'm pretty sure ALGOL and FORTRAN were both in their early development at roughly the same time, and were both structured programming languages...
The latest Ubuntu version (11.10) was released about a week ago with GHC 7.0.3.
Very nice. When I get a little time, I'd like to compute a minimum spanning tree for the same points to see how far off the solution is from the lower bound.
So link is of type String -&gt; String -&gt; String? I would expect the type String -&gt; String -&gt; Element. How about an inline image?
How would you "compile the monad" to JS? The monad allows you to use previous results. Monads pretty much force the language to be hosted in Haskell (unless the monad is just a preprocessor language as in e.g: Atom). If you want to compile to JS, you need something non-monadic. Category+Applicative + potentially more combinators.
It would be great if the Haskell interface used structures similar to those in other languages. In the past, interface which used lists was a big handicap for Haskell. Other languages used arrays.
I was on the contest server side, rather than the client, but I recently had to switch from 6.12 to 7.0(.4) due to some bad behavior in System.Timeout (in a nutshell, it hangs your program on average after only a few dozen uses on 6.12), and I could see how that could be a problem if you're trying to program time-limited bots too. It's not just performance, 6.12 has at least one significant correctness issue.
You'd have to thread `$id` all the way from the servlet to the (template) library function that draws the widget as 1) the servlet needs to refer to it when it receives the form submission and 2) `$id` needs to be globally unique within the page. It'll look like this: # Servlet code def response(): ID = 'some unique id' if valid_date(form_data, ID): business_logic() # Display dictionary used to render template disp['id'] = ID render_template('some.tmpl', disp) # Main template for this page def render_page(): # This function really takes O(100) arguments, through the display dictionary ... draw_flight_booking_form($id) ... # Library code for draw flight booking forms def draw_flight_booking_form($id): ... draw_date_widget($id) ... You might be tempted to not pass `$id` around but if you don't you have now introduced an invisible data flow between any servlet that eventually end up calling `draw_date_widget` (during template rendering). Aside: Using an `$id` prefix is not quite enough; you might still have collisions. If you want to be sure you don't you have to have one placeholder per name. Aside 2: The same applies to all CSS classes so if you had: &lt;label class="required-field"&gt; &lt;input name="first-name"&gt; &lt;/label&gt; you ought to also parametrize the function on `required-field` as well as `first-name`. This is not a theoretical problem. If you allow me to use an argument from authority, I would say that this problem appears a lot in the YouTube code base (i.e. at work). It's tempting to look at toy programs and examples and say this is easy to deal with. But remember: it also easy to reason about e.g. global variables in a small program. It is at scale it gets bad. The same applies to this particular problem. &gt; This is a drop in the bucket compared to the hard part of writing a web app. Even if it is, that's no reason to add more incidental complexity to our applications. We already have plenty of it. That being said, I don't think it's a small part of it. At least where I work servlet code typically looks like this: def response(): # Lots of form and GET data validation. # Fragile because it refers to form field names directly like so: form['foo'] # A little bit of business logic. # Stuff a hundred variables in the display dictionary. &gt; If you're really concerned about things like this, I would suggest the proper angle of attack would be get rid of templating together and start looking at a much higher level of abstraction. I agree. :) 
Yeow. That was clear back in the era before type and data families.
That means throwing reuse out of the window. Lets take youtube.com as an example. If you browse around the site you notice that all over the site we have small video thumbnails with a little bit of metadata attached to them. Drawing these thumbnails is a cross-page concern. There's no one page that owns that piece of code.
Layering the ID handling on top of the more basic HTML generation is fine, as long as the programming model presented to the user doesn't have undesirable properties.
&gt; How much actual webdev work have you done in the real world? I know this wasn't directed at me but I'll chime in anyway. The last three years I've worked on youtube.com, which I guess is the one of the biggest Cheetah users in the world (or at least was). &gt; Designers don't code. They do HTML. If it looks mostly like HTML they can handle it. But doesn't expect too much. This is one of the touted benefits of templating languages; they are accesible to designers as they're mostly HTML with a few placeholders and control structures stuck in there. This is not true, at least not at YouTube/Google. Templates (where we use them) are typically lots of dynamic stuff with a little bit of HTML stuck in between. As webapps grow and become more dynamic (AJAXy if you wish), there's less and less HTML, more dynamically generate HTML, and more JavaScript. &gt; Modularity of the sort you seem to think of isn't that big of a win. Parent/child template inheritance is all in practice I've ever needed. If it's the only hammer you have...
Yeah, I'm guessing that they aren't using Ubuntu 11.10 =)
that's not true, you call the same geenrator on the pages that generate them and then you reuse; you don't create the page FROM the html, it gets created from the server which generates the html itself.
Nice :)
FORTRAN was originally a GOTO language, lacking structure. Later versions added the structure we take for granted today. EDIT: It appears I was wrong, there was a DO loop structure in its initial release. Ignore what I said then.
I am not sure how to do inline images right now. I suspect that there is not really a nice solution, and I am reasonably content to leave them out if it leads to nicer abstractions over all. Here's why I didn't take that route though: I think the logical conclusion of your expectation would be for all string manipulations to return an Element. This runs into problems because you'd need to put strings together with flowRight, but what if one string is many lines long. We would like it to flow across lines in a nice way and mix well with other strings, but that means that these Element are not really rectangles anymore. Suddenly Elements constructed from strings don't have a well defined shape. This breaks things like 'width' or 'size' which now needs to be aware of what _sort_ of Element it is dealing with, thus breaking the abstraction of Elements. Your point is really important though. People make links out of images all the time! This is a pretty big oversight on my part! I could add a function (link :: String -&gt; Element -&gt; Element) that allows you to turn an arbitrary Elements into hyper-links. I think this would respect the shape abstractions of elements, but I have a weird feeling about it. In any case, in addition to having images as links, I think this would roughly allow the kind of behavior you are thinking of :)
You are totally right. I gave a temporally confused answer. If you just consider the language as it is today (no dynamic behaviors), I believe it could be done in a monad. All the layout computation would be done in Haskell and then converted into HTML/CSS. This was generally how I was doing things until midsummer. When you try to add dynamic behaviors, this is no longer possible because any Haskell computation is stuck in Haskell (e.g. (+) cannot be translated to JS). This is why I started compiling things myself. It may be possible to do something like this purely in Haskell, but I felt like the conceptual overhead would not be worth it. One shouldn't need to understand Arrows or Monads or Functors or whatever to draw a square. I am noticing that I tend to talk about this project in temporally homogeneous way. For me, Elm has a continuous trajectory from when I started to a couple months from now, so I keep giving weird responses. I discussed my general approach at length with TylerEaves (elsewhere in here) before I realized he was talking about the present language and I was talking about the future one. tl;dr: You are right. My original answer didn't make sense. 
I'm afraid I don't understand this sentence. Perhaps you could give an example?
I don't think you can dismiss mixed content as a feature that can be left out. F.e. from a type-safe markup language I would expect to be unable to generate invalid html, but now `tail (link "bla" "bla")` does exactly that. Look at how the W3C solves this in the DOM, using text nodes.
And ignoring non-termination, laziness (have you really received that Shape yet?) and various performance aspects.
Fair enough. That said, I find it much simpler to program in 'total haskell' than I do to program in 'pure C++'.
Honestly, I thought this was an in-joke. Scala seems to support it as well, in its own bizarre way.
There do seem to be modifications to GHC that let you do let ?x = ?x + 1 in ?x but what Haskell normally does with this is creates a new environment env' that extends the environment of the expression as a whole, and then evaluates both `x + 1` and `...` in the new environment, so that you can get cyclic structures like in let ones = 1 : ones in ones
nobody forces you to use their starter package, write your own.
I do not understand what handicap you believe Haskell suffered from in previous years. Are you suggesting that there was a problem with performance? FWIW, I was reasonably successful in last year's contest (using Haskell) and at no point did I bother with any performance optimisation at all; my experience was that there was simply no issue here. Besides which, as zlonax hints: if you want to use some other data structure - just do it!
Slides: http://www.ozonehouse.com/mark/haskell-amuse-bouche/slides.html Code: https://github.com/mtnviewmark/haskell-amuse-bouche
&gt; O'CAML Where'd that apostrophe come from? I have never seen it that way.
Slides don't work on Android. 
My go to web framework is Ruby on Rails. Part of what RoR so good are the gems. If I want to let my users login via facebook/twitter/google accounts I can just add the gem to my gemlist. If I want to add permissions to my user accounts I just add the gem to my gemlist. Etc... How does Yesod deal with this? I've not heard of any 'Gems for Yesod', about people complaining about the lack of it, or people exclaiming that Yesod eliminates the need for it, so it's a bit of a foggy topic for me. The core parts of a web app aside (the basic MVC stuff), how does Yesod allow for the reuse of web app code as a community? Please let me know if my question is missing the point, also.
Works pretty well in [Firefox for Mobile](https://www.mozilla.org/en-US/mobile/)
Try looking at the [slide source on github](https://github.com/mtnviewmark/haskell-amuse-bouche/blob/master/slides.md). Github does a decent job of formatting the markdown format, and I'd bet that'll render on Android.
Could be an artifact of the ranking algorithm. If there aren't many users comparing pascal to agda directly it won't always do a good job ordering them. Rank aggregation (especially of partial lists) is hard. This is also an older less advanced version of the code I have for rank aggregation. I need to integrate the new stuff at some point, but a) It still lacks some features of the old one (I need it to be able to handle ties to do the individual language descriptions) and b) Time and effort. (Appropriately enough the new one is written in Haskell)
There is a lot to the story of surprises with Haskell, most of them very positive. I should really write a coherent blog post of this, but just to answer your question off the top of my head: * I come from OOP background, FP programming in Haskell seems (to me) as 'dual' to OOP * Haskell web frameworks are surprisingly mature and complete (we are using Happstack) * refactoring in Haskell (guided by types) is surprisingly easy * speed of GHC compiled code is very good * space leaks do not bite that hard as I expected them * monads are easy, monad stacks are big architectural decision and probably many other things. (Everything here is just MHO).
Good point, I didn't compare Pascal to Agda either. It's Arrow's theorem and worse, isn't it? 
The problem is more lack of information than arrow most of the time. The issue is that unlike most partial list voting leaving something off the list is not a vote against it. This means that we tend to have very little info about uncommon languages, and even less about comparisons between unrelated uncommon languages. My guess is that what's happening here is that agda does badly in the initial sorting round due to lack of information and the the algorithm can only make local changes after that so can't move it high enough up. This sort of problem may be better in the next version but is unlikely to ever go away completely
There's definitely a social distance from drmaciver bias in the results, though the poll has been running long enough that that should be mitigated. The similar to results should also be taken with a pinch of salt. They're just a cute hack - I'd definitely not claim they were statistically valid
Gems is a package manager for Ruby. The Haskell equivalent would be [Cabal](http://www.haskell.org/cabal/). Note, that Cabal is not specific to Yesod (just like RubyGems is not specific to Ruby on Rails).
Cheers, that worked. 
Hmm.. Actually I made a mistake, you can't really compile anything even involving Functor. fmap makes opaque black boxes. Need Category + other non-opaque combinators.
Just go to [Hackage](http://hackage.haskell.org/packages/archive/pkg-list.html) and look for "yesod". You'll see a large number of packages. The line between framework and additional is actually very vague in this breakdown. yesod-auth, for example, provides all the login stuff you are describing, and is maintained by the Yesod team, but you can use Yesod without touching it.
Sweet. I generally like Bump as a company too (although I think you guys might have fallen a little out of vogue in recent months)... Nice to hear you guys use haskell too!
Interesting. Our CTO has a similar background (although BS level, not PhD) but has worked on bioinformatics before. Definitely stay in touch, jobs@thoughtleadr.com :)
&gt; the value per line is very high Agreed :)
That last sentence implies that `(return $! x) &gt;&gt;= return` has different behavior from `return $! x` -- but we all know the monad law that says `m &gt;&gt;= return = m`! Discovered via IRC, when debugging the complaints in this hpaste: http://hpaste.org/52973 (Using the Haskell Platform 2010.2.0.0 on Windows XP.)
This has been known for a while; presenting it as something about `evaluate` obscures it, it's as simple as: (undefined :: IO ()) `seq` () ≡ undefined ((undefined :: IO ()) &gt;&gt;= return) `seq` () ≡ () The standard counterargument seems to be that the monad laws only apply in the context of "executing" that monad, not as the monadic values considered independently. I think this is nonsense.
Yow, I didn't know that one. It's nasty, too! Can you elaborate on the counterargument a bit?
By failing these cases, how might code using the IO "monad" be broken? Does this behavior break assumptions outside the scope of where `seq` is used?
See the hpaste in my top-level comment for an example of a seemingly innocent transformation that breaks behavior significantly (without a single `seq` in sight!). Even hlint suggests doing the innocent transformation that breaks the behavior.
It's the same one as the one this link is about: `return $! undefined` obviously ≡ `undefined`, so (return $! undefined) `seq` () ≡ undefined but if you say `(return $! undefined) &gt;&gt;= return`, that wraps it up in a lambda inside the IO newtype, and so ((return $! undefined) &gt;&gt;= return) `seq` () ≡ () I don't understand the counterargument myself (I believe monochrom on IRC made it), but specialised to IO it's something like "the actions only have to be equivalent upon *execution*; they can be distinguishable as values". But I don't see how that generalises to monads that aren't "special" like IO is. There were murmurs of some standard monads violating the laws in this manner too at the time, but I don't have any details or evidence.
The counter argument is that laws (monad, monoid, applicative, free theorems) only apply to total values.
`lineParse` smells of `unsafePerformIO`. Anyhow, I skeptical that those two programs can produce observably different behavior given a proper implementation of `lineParse`.
I posted an annotation, wherein I attempt to make the effect demonstrable. Am I missing something? [http://hpaste.org/52973](http://hpaste.org/52973) 
It's not about IO. In presence of strictness even Reader isn't a monad. ret = return :: a -&gt; Reader r a x = seq (ret $! undefined) 0 -- _|_ y = seq ((ret $! undefined) &gt;&gt;= ret) 0 -- 0 (By the way "evaluate" should be defined for any monad, not only IO) Edit: ehird's comment makes it even simpler: undef = undefined :: Reader r a x = seq undef 0 -- _|_ y = seq (undef &gt;&gt;= return) 0 -- 0 
`lineParse` is just a pure function. Note that `lineFetch` calls `lineParse`, not the other way around (as perhaps you're worried about with your `unsafePerformIO` comment).
I wrote the starter bot a few months back. I wasn't too concerned with performance other than making sure there weren't any huge memory leaks. I was also writing it as a learning exercise so I wasn't trying to be too crafty.
Yes, standard monads break the laws too (see my comment below)
Looks like they are still using the starter bot I wrote a few months back. I don't remember what version they have but you can get the latest at [github](https://github.com/qacek/ants-haskell). I don't have the time to compete in the contest so I don't think I'll be working on the starter bot any more. Maybe someone can use it as a starting point to finish the implementation, hills weren't in the spec when I wrote it for example. I can't say I worked too much on performance but I think I got rid of all the memory leaks at the very least.
&gt; lineParse is just a pure function. I'm skeptical of this claim.
That is a more reasonable argument than the one I heard.
Ah, g__'s comment shows that the standard monads break the laws too.
Yep; because `Reader` is a function underneath. We should make functions unlifted to solve this and make eta-reduction sound. ;-) (Unlifted is the relevant thing here, right?)
Haskell types and functions don't form a category.
Maybe not, but IO is close enough to a monad that modeling it with &gt;&gt;= and return makes a lot of sense. :P
I'm looking into it. It needs more love than just adding hill support: e.g. it only supports 3 enemies, although the contest allows 10 concurrent players in a map. When I finish my work on it, I'll make sure it gets pushed upstream.
The latest code on github supports an arbitrary number of enemies. I would start with that code.
Could you please review my pull request with hill support? https://github.com/qacek/ants-haskell/pull/1
Sorry a mistake on my part, since I'm not unfamiliar with cabal I should have realised that. I guess a better question is how is cabal's coverage of the functionality you are likely to need?
I certainly will. Thanks.
b-b-but what about Hask!
Seems like you guys beat me to it. Ah well, as long as there's a good starter package, I'm happy! :)
Yep, starter package very good indeed. :)
TL;DR seq breaks useful laws. 
The best fix would be to make the value of (a `seq` b) be independent of a, even for the case where a is undefined - that would be less surprising, and mean the monad laws hold in the example you gave.
It doesn't look as though a Tile can correctly handle the case where an ant is on a hill - or am I misunderstanding?
You should use tuples(i.e. pairs). doubleUs x y = (x*2, y*2) doubleUs 5 5 &gt; (10,10)
Thanks 
indenting
Or you could put the numbers in a list, and use the map function. If you want a list or a tuple depends on your use case. If you always want to double two numbers at a time, the tuple version of ProJedi is better. The list version is convenient if you want to be able to double any number of numbers: doubleUs = map (*2) doubleUs [5,5] &gt; [10,10] doubleUs [1,2,3,4] &gt; [2,4,6,8]
The neat way to do it would be to define an algebraic type and then put it in the Show class: data Doubled a = D a a instance (Show a) =&gt; Show (Doubled a) where show (D x y) = show x ++ " " ++ show y doubleUs :: (Num a) =&gt; Doubled a -&gt; Doubled a doubleUs (D x y) = D (2*x) (2*y)
Uhh, I got nothing really. I could also step up my game.
Hoping someone has a way for haskell mode to play nice with cabal-dev, as well.
ah, [scion](https://github.com/nominolo/scion) mentioned in the emacs thread is available for vim as well. Looks like just what I need.
Here are the two haskell specific pluins I use: * ConqueTerm (http://code.google.com/p/conque/) to put GHCi inside a buffer. * vim-tagbar + lushtags (http://hackage.haskell.org/package/lushtags-0.0.1) for nice haskell ctags. Note that there is a haskell-mode for vim featuring haddock and GHC integration: http://projects.haskell.org/haskellmode-vim/ 
I use bare-bones vim for most of my home editing and emacs with haskell-mode for work. Haskell-mode is really nice but honestly Haskell isn't really one of those languages calling out for an IDE if you ask me. Bare-bones vim is fine.
I prefer to use the following configuration: full-screen terminal window with bare-bones vim and drop-down terminal(yakuake, guake) for ghci session
Here's a big thing for editing Haskell code (any code, but Haskell more so than others) with Vim: Use block selection (CTRL-V) — why? Because Haskell code (style) relies more on horizontal justification (vertical alignment) than others, and block selection preserves all whitespace.
Note that the vim frontend seems to be marked defunct. The last commit to it originated in 2009.
This is freaking cool. I'll definitely try it to see if it can replace my current usage of haskell-mode. I tried ghc-mode some time ago, but returned back to haskell-mode.
As someone who is familiar with both haskell-mode and your haskell-emacs, what would you say i miss, if i'd switch from haskell-mode to your haskell-emacs ?
In the absence of additional requirements, I would suggest not even talking about this when tuples will do just fine. As much fun as it may be to show the 43 ways you can accomplish a given task in a given language, it tends not to "impress" beginners so much as scare them. (I say "given" because by no means is this a Haskell problem.)
But then what would be the point of `seq`?
http://urchin.earth.li/~ian/style/vim.html
Did you try Emacs with vimpulse? Edit: I get downvoted vor suggesting vimpulse to a guy who uses vim and Emacs? Come on...
kazu's ghc-mod is excellent.
Simple binding to allow adding type signatures to your top-level functions: function! HaskellType() w execute "normal {j^YP" execute (".!ghc -XNoMonomorphismRestriction -w % -e \":t " . expand("&lt;cword&gt;") . "\"") redraw! endfunction function Haskell() map &lt;buffer&gt; &lt;silent&gt; tt :call HaskellType()&lt;Cr&gt; " more haskell stuff here endfunction autocmd BufRead,BufNewFile *.{ag,hs,lhs,ghs} call Haskell() edit: more random stuff here: https://github.com/sebastiaanvisser/dotrc/blob/master/vim/haskell.vim
haskellmode with [this](http://hpaste.org/51363)[1] as cabal integration hack. You *really* want to have automagic type signature insertion. I've got xmonad, so I don't need any fancy ghci-in-a-buffer thing, but NERDTree comes handy navigating projects. [1] I recommend adding -XNoMonomorphismRestriction to the options, no matter what the cabal file says.
I really love vim's ability to C-n/C-p complete across module boundaries, when you've started your vim from the namespace root.
We're changing the author? Not again! ~amstan Contest Organizer
We're using 10.04 ~amstan Contest Organizer
Right now the haskell version is dictated by what is available in the Ubuntu 10.04 main repository. Compiling it from source is not an option, since it takes too long for worker deployment. If you really want to go with 7.x you'll need to find an easy way to get it in ubuntu. Ideally a ppa or debs from somewhere. It would also be nice if it was secure. ~amstan Contest Organizer
Pretty good autocompletion via the ghc-mod package: https://github.com/ujihisa/neco-ghc
How do you set up lushtags to work with tagbar? It looks like each new language needs to be added manually.
I like my vi like my coffee. Black and white and with a terribad undo.
Someone already sent me a patch to add support for hills, so it seems like the starter package is more or less complete again. To be fair though, I haven't looked at the spec in a while.
I pretty much just use indenting and highlighting. As long as I can still grep my import lists, I don't worry too much about them. The snippets look nice, though - I might give those a try. Using something like Scion to do continuous compilation would be nice, but when I have continuous compilation in F# in Visual Studio it does irritate me lot when it's type inference is flipping out when I'm halfway through writing a function and I haven't gotten down to the proper return value. I feel like I'm in a more peaceful state of mind with Haskell (without continuous compilation), even if I may be less productive.
I assume this is something like viper-mode? Actually, I don't have a preference for vi-style modal editing. I started out on Emacs and began using vim around 2003 or so mainly because Emacs couldn't deal with unicode properly in those days, and at the time I lived in China and that was a feature I couldn't live with out. Then Emacs came up to speed but by then I was pretty used to just firing up vim from the command line to make quick edits, including not so quick ones. And in the very early days I did all my coding with ed (not by choice, I was on a line terminal in those days, old guy here) so one thing I really appreciated in vi was the ability to use colon commands that were more or less just the ed commands from my youth. I typically do "serious" coding in Emacs, and the keystroke commands are wired into my brain. I'm one of the few people I know that truly doesn't have much of a preference. So I guess what I'm saying is, I wouldn't use a vi emulation mode in Emacs for comfort, although if you're a vi affecionado who needs more power, I would recommend it, because Emacs is customizable in a way no other editor can ever be. This is both its great strength and its great weakness. As for why you were downvoted, I don't know.
Could someone help me understand something in this? Recently, I read [Julienne Walker's](http://eternallyconfuzzled.com/arts/jsw_art_rand.aspx) blog in which she writes, &gt; For example, to shrink the range into [0..N), one might do this: &gt; 1 int r = rand() % N; &gt; Anyone who does this will be rewarded with a seemingly random sequence and be thrilled that their clever solution worked. Unfotunately, this does not work. The solution only works when N evenly divides into RAND_MAX. [...] The reason is because forcing the range in this way eliminates any chance of having a uniform distribution. But Oleg writes, &gt; We can obtain r[i] as rng[i] mod (n-i), [...] &gt; Finally, rng can be a suitable pseudo-random &gt; number generator (PRNG). Note, PRNG are specifically tested that (rng &gt; mod k) is uniformly distributed. The latter phrase does not mean that &gt; the histogram of (rng mod k) is perfectly flat. Even the "real" random &gt; numbers generally don't have the perfectly flat histogram, for any &gt; finite number of samples. It is important that the histogram is &gt; "statistically" flat, in the sense of chi-squared or &gt; Kolmogorov-Smirnov criteria. A volume of Knuth discusses PRNG tests in &gt; great detail. This seems to contradict Julienne's statement about "eliminates any chance of having a uniform distribution." Is (rng mod k) uniform in general? Or is this true only for ones that pass the testing he aludes to? Anyone got a good reference for this that doesn't cost [53$](http://www.amazon.com/Art-Computer-Programming-Seminumerical-Algorithms)? or maybe this is the excuse to buy his books I've been waiting for...
Heavy vim user, Haskell user and vim plugin writer, I use my own scripts in combination to other plugins : * [Haskell Conceal](https://github.com/Twinside/vim-haskellConceal), display unicode operator while editing normal ASCII, require vim 7.3, really fancy * [Haskell fold](https://github.com/Twinside/vim-haskellFold) Provide a nice folding for Haskell, stripping out comment and displaying type signature in the fold. [screenshot](http://twinside.github.com/vim-haskellFold/) * [Hoogle](https://github.com/Twinside/vim-hoogle) Helper script to query locally installed hoogle from within vim. You can also throw a .cabal definition (in standard in vim 7.3), a recent haskell syntax file (search vim.org, I don't remember the last one) PS: I'll add some screenshot tonight.
Vimpulse is an addon to viper, but much more advanced. I use emacs in daemon mode that starts as a service and supports multiple windows. You can even use the same Emacs instance both in a terminal and a gtk window. I "abuse" Emacs as a Vim with better script support ;) 
Note that you cannot just pass that into a function, that is, this does not work doubleUs x y = (x*2, y*2) doubleUs (doubleUs 5 5) That would be a type error.
Cool, but how can I enable that for Haskell?
Emacs is a great editor, and, ironically enough, very lightweight by today's standards. Eight megabytes and constantly swapping, they used to say. Ah, back when 8M was a lot...
[evil](http://gitorious.org/evil) is the new vimpulse.
This may not be easy to implement, but I consider basic indenting support the 2nd most important feature after syntax highlighting. As failing to indent Haskell properly is a syntax error, this is even more important.
Vimpulse is being phased out, apparently. The author [encourages](https://lists.ourproject.org/pipermail/implementations-list/2011-April/000756.html) users to migrate to [evil](https://gitorious.org/evil).
The solution to that is uncurry, by the way: uncurry :: (a -&gt; b -&gt; c) -&gt; (a, b) -&gt; c That is, it takes a function expecting two (separate) arguments, and returns a function taking these arguments at once, as a tuple. So the call would become: uncurry doubleUs (doubleUs 5 5)
I use [haskellmode](http://projects.haskell.org/haskellmode-vim/), and [hothasktags](http://hackage.haskell.org/package/hothasktags) (which I find invaluable to navigate code).
Thanks, the last time I researched this topic vimpulse was the new stuff ;)
Thanks for the hint to you, too!
Why do I need javascript for this blog?
Funny pics :P 
Is it OK if I give you .tar.gz (with ghc 7.0.3 + haskell platform) so you just unpack it upon deployment to some directory and add it to PATH?
Why does this even matter? If Apple insists on shipping an ancient GCC with their OS, the Haskell platform should just bundle a more recent build (the same way it does on Windows).
I have this snippet in .vim/after/syntax/haskell.vim &gt; setlocal include=^import &gt; setlocal includeexpr=substitute(substitute(v:fname,'\\.','/','g'),'$','.hs','')
I can confirm. After years and many unsuccessful tries to break through a high wall, I was finally able to use haskell in production thanks to Yesod.
You could also avoid the collisions by using a bloom filter to approximate entries seen so far when generating them for the sort based implementation. You would be ensured of no collisions, and under the assumption of uniform distribution of hashes you would introduce no bias. 
This article was highly needed! While all information presented can be gathered from different sources [Control.Concurrent](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Concurrent.html), the [FFI spec.](http://www.haskell.org/onlinereport/haskell2010/haskellch8.html) and its [implementation in GHC](http://www.haskell.org/ghc/docs/latest/html/users_guide/ffi-ghc.html#ffi-threads), combining it in a short article is valuable. Regarding the name of `forkOS`, wouldn't it be better to rename that to something like `forkBound`? (And maybe rename `forkIO` to just `fork`?) I think such a simple change will already clear up a lot of confusion.
Apple no longer ship GCC (except in its llvm-gcc form) at all. Personally I think this is a good thing, but others may disagree.
Yes, this is pretty standard. Using your terminology, you're computing the fixed point of a new version of the "pseudo-recursive" function that memoizes its results based on all its arguments except for the first.
I haven't played much with syntax before, but that last trick for highlighting bad whitespace is awesome.
&gt; edit: more random stuff here: https://github.com/sebastiaanvisser/dotrc/blob/master/vim/haskell.vim 404?
I disagree, Company adoption drives language adoption. If say Google were to create a top haskell web application framework and server, in no time, Haskell would see increased usage. You see this with top technology like Java, C++ (through Microsoft's early Win32 platforms), .NET, Apple and Cocoa, Android has increased Linux deployments If they build it, people will come. ... I would love to see this experiment. Let's pretend Google creates a fork of the darcs project. Now gdarcs is a google project. And maybe they build haskell oriented hooks into the software. I bet you will see increased use for this particular technology and haskell solely because google was involved in a major haskell based application.
I think GWT proved that's *not* the case. Go and Dart aren't exactly taking the world by storm either. Greg *did* give a good explanation for the cases you mention: it's the *only* way to develop for a specific platform. Apple decides all iOS apps must be programmed in Objective C, so you have a bunch of people learning it. But this hasn't led to GNUStep taking off. Likewise with .NET: Mono hasn't displaced Java in any meaningful way on non-Windows platforms.
GWT is being used. It could be THE next framework, especially with Google Chrome adoption. 
Simply copy the `tagbar-haskell.vim` from the util folder in lushtags source repo to your vim's pulgin folder, e.g. `~./vim/plugin/` This works for me
GNUStep is also being used. It could be the next desktop framework. But that hasn't happened, and there's no compelling reason to say that it will. Rails had a much more disruptive influence on the industry than GWT did, by far. In other words: the name "Google" certainly helped, but not very much. Is there something I'm missing about the GWT and Google Chrome connection?
You will find the technique explained in (Section 2 of) http://dl.acm.org/citation.cfm?doid=1739230.1739244 Also, more specifically about the application to memoization, the technical report cited as [3] in the above paper.
Yea that is one framework and technology: On GWT, it could be one of the most popular Java frameworks right now. Now with HTML5 support. As Chrome starts adding HTML5 features, so goes GWT. It didn't replace rails but it is certainly an influence in the Java community. Android had a disruptive influence. Android and their development environment. Google Chrome had a disruptive influence. Google AppEngine (also allows GWT use) 
I like it. Anyway, you can add ?v=0 to the URL to get a non-javascript version.
[these](http://www.reddit.com/r/haskell/comments/lg2jv/status_of_haskell_platform_cabalhackage/) [other](http://www.reddit.com/r/haskell/comments/kk9hl/did_haskell_platform_miss_the_july_release_its/) [posts](http://www.reddit.com/r/haskell/comments/jaa8t/eta_on_the_haskell_platform_2011400_release/) all ask this question, with varying amounts of feedback... Not entirely helpful, but that's what's out there right now.
i use it regularly
How many people develop in AppleScript (Apple), Fortress (Sun/Oracle) or PostScript (Adobe)? How does that compare to the number of people writing code in Perl (no company) or Python (no company)? How many people were programming in Objective-C (Apple) before the iPhone and iPad came along? They've added a framework to programm iOS devices and it's thriving since. Company adoption can lead to a language's success, but it is neither a pre-condition nor a guarantee.
&gt; I've been working with Posix regex for a few weeks and I'm coming to the conclusion that for my task I need PCRE. &gt; Is it possible to implement multi-line matching? Most people with this "conclusion" didn't realize that they actually want a simple parser.
http://trac.haskell.org/haskell-platform
Installation instructions are here: https://github.com/bitc/lushtags , works well for me...
Are you sure that this is a quasicrystal? I would like to see some kind of proof.
&gt; Buy this Article Why on earth are publications not available for free? Makes me rage.
&gt; monads are easy, **monad stacks are big architectural decision** Can you elaborate on that? Are you talking about nested monad transformers? Why is it a big decision?
Isn't it known that there isn't a 5-fold plane tiling?
gtk2hs has tight integration with cairo, which is perhaps one of the nicest 2d vector graphics libraries on the market today.
You could have a look at the diagrams package: http://projects.haskell.org/diagrams/ and try and wrap that up in an FRP library for the UI interaction. (caveat I'm probably even newer than you to haskell so I probably don't know what I'm talking about)
Yes, that's true. However, the atoms in a [quasicrystal][1] will conspire to produce an X-ray scattering image that shows a 5-fold symmetry. It's a 5-fold symmetry "on average", so to speak. [1]: http://en.wikipedia.org/wiki/Quasicrystal
That is a possibility, but I'd really really like to avoid that if at all possible. It's a giant pain on Windows to bundle our own tools. On the face of it it seems simple, but when people try to mix objects and libraries created with our bundled tools with those created with other tools, all kinds of weird issues tend to arise. I imagine on OS X this would be a common issue - Apple has a tendency to make changes to its binary formats and linking semantics between releases. On the other hand, we could make it so that GHC will *work* (albeit slowly) when built with XCode, meanwhile we can build the official distributions using gcc so you'll get good performance from the standard install.
Could be useful for me if it didn't contain the words "run this as root". So I went through the RPITA build sequence described [here](http://heisenbug.blogspot.com/2011/09/ghc-704-on-centos.html).
Agreed. Parsec and Attoparsec are both quite beautiful parsing libraries. Attoparsec in particular is extremely fast (though somewhat pared-down).
if I understand you correctly the problem with template widgets was that -on a template- you can't invoke them without parametrization of sorts. what I'm saying is that you *don't* use widgets on the template, you define objects that have the definition of the form and they generate the widgets and also prefill the contents and are supposed to parse the request that comes back on submit. of course that isn't Haskell, but that's how the rest of the world does it. an example in Django (Python) here: https://docs.djangoproject.com/en/1.3/topics/forms/
Ported to HTML5 canvas: http://www.jasondavies.com/animated-quasicrystals/
Noted: at some stage I will think about user-level installs. But first I have to get the system releases straight for RHEL/CentOS 5 &amp; 6.
It won't be easier, BTW. But I you try, I recomend you use gtk with cairo. I used it with Haskell own project and I satisfied with that combo.
We are currently seeking two new Lecturers (Assistant Professors) in Computer Science at the University of Nottingham, UK. These posts are available from 1st January 2012 on a fixed-term basis for three years. Applications from within the area of functional programming would be most welcome! The deadline for applications is 9th November 2011.
Mercury. Disciple, I think.
I have written up a [blog post](http://justhaskell.org/2011/10/25/enterprise-linux-for-haskell/) that says a little about the rationale of the proposed structure of the distribution.
[google scholar is your friend](http://scholar.google.com/scholar?q=EffectiveAdvice%3A+disciplined+advice+with+explicit+effects&amp;hl=en&amp;btnG=Search&amp;as_sdt=1%2C45&amp;as_sdtp=on)
1. With a bit of Google you can certainly find the paper for free. I gave (and always give) the DOI link, because that's the most authorative information you can get in terms of title, abstract, bibliographic details, etc. From there on, instead of raging, you are free to use that information to find the paper elsewhere. 2. Why are publications not available for free? In this case, because ACM provides a service to the conference. Actually, a number of services without which the conference might not even exist. One obvious thing is the preparation of printed proceedings (though to be honest, I don't know whether AOSD still has printed proceedings). 3. I do publish with ACM. Nevertheless you can have free access, with ACM's blessing, to all my papers, in their "official ACM Digital Library form", by following the "ACM Author-ize" links from my homepage. See http://www.acm.org/publications/acm-author-izer-service 4. I am a member of the ACM, but I wouldn't mind either (actually would be happy about it) if their publications were all open access. They aren't, but there are better and more useful ways to react to this fact than raging.
You're right; that [3] reference ([Monadic Memoization Mixins](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.3218&amp;rep=rep1&amp;type=pdf)) indeed concluded exactly the same thing: &gt; For memoization, the memoized Fibonacci function will have the form `fix (memo . gFib)` for an appropriate `memo` and generator `gFib`. This has the effect of binding self-reference in `gFib` to the memoized version of the function. What I called "pseudo-recursion", that article called "open recursion".
&gt; They aren't, but there are better and more useful ways to react to this fact than raging. Namely? Do you mean something like write a letter to ACM, or rather to shut up and search for a free copy, which might or might not exist?
Well, which of the two was more productive: * using the information I provided to find the article * raging ?
I've always thought the terms "safe" and "unsafe" in the FFI backward. It feels like you're declaring something about the function - and so I'd expect a "safe" one to be that didn't block, didn't call back to Haskell, etc... But really you are declaring something about the state the Haskell RTS needs to be in before calling the function. 
I'm sorry, but i do not think you have a complete picture of what's going on in industry. For example we did evaluate at one point one of the GWT based frameworks (vaadin). We decided against it. The reason: it ties the server side to java platform, whereas other pure javascript UI libraries (ExtJS, Dojo, YUI etc) allow you to have a server agnostic client. Modularity is the key. And GWT solutions are too heavy, too tightly coupled and at the same time fragile. It's the same thing as with RPC/CORBA vs plain html/REST. That's the main reason GWT did not (and will not) take the web programming by storm. it's simply a misguided idea that looks cool on the surface.
Hmm.. I can see things have changed on the starter packages page, but it looks like the previous version of the code was repackaged and released again? (the zip was different, but ants.hs unchanged)
This is similar to the Ruby tool RVM (Ruby Version Manager) which works on any unix system. RVM has been in common use for a year or two and enabled the community transition to the newer version of Ruby (1.9), in large part because it is still easy to work with the old version (1.8) when needed - there is no downside to starting a new project on the latest version. It makes it dead simple for me to work on different projects, some of them even using jRuby (Java implementation). Recently a [a less obtrusive alternative](https://github.com/sstephenson/rbenv) called rbenv was created. Given that RVM/rbenv are just shell scripts they should not be hard to port for use with other programming languages.
Cool. I'm wondering though, why is JavaScript version faster than Haskell's one? Different algorithm?
The basic algorithm looks very similar. The canvas is slightly smaller and it uses different `scale` though, they might affect performance.
The Repa solution computes 800x800 where the JS is doing 500x500, which is 2.5 times the work. But I think that still leaves some performance questions unanswered.
OK, ExtJS is maintained by Sencha corporation. My point is that, top frameworks 'can' have backing by companies or top companies. 
It is used by gmail after all. I wonder if gmail is used by anyone.
Yes, I think "open recursion" is a more standard term. It's also useful for data types: data ListF a r = Nil | Cons a r newtype Fix f = In (f (Fix f)) type List a = Fix (ListF a) fromList [] = In Nil fromList (x:xs) = In (Cons x (fromList xs)) toList (In Nil) = [] toList (In (Cons x xs)) = x : toList xs Data types written in this way can be extended/modified in the same way as you did with the open function to add extra functionality, e.g. metavariables, annotations etc.
I suspect it to be "easier" in that it should may be better suited for the problems I face. Expect it to sort out the concepts faster. Maybe not.
And Paul Graham's custom build LISP ecommerce site was used by millions of Yahoo users. How does it prove that it is widely adopted by programming industry ? 
I have a hard time understanding where you are going with this. No one is debating this obvious fact. It is a common knowledge that big corporations push their own web frameworks. 2 of the most widely used ones are ASP from Microsoft and JSP from Sun/Oracle. Also remember ColdFusion ? Where is it now ? What exactly does it prove, or how it invalidates anything OP says in his article ? 
I like cairo. Although it seems a bit unwieldy for prototyping. Maybe it's *reasonably* easy to work with in haskell...
You just need to think about how 'fix f' works. I'm going to use the un-tied version of fix for demonstration purposes: fix :: (a -&gt; a) -&gt; a fix f = f (fix f) Now, in this version we actually create a chain of f's, which is evaluated based on how 'recursive' f is. For example: f :: (Bool -&gt; [Bool]) -&gt; (Bool -&gt; [Bool]) f rec v = v : (rec $! (not v)) f _|_ False = False : (_|_ $! (not False)) = False : (_|_ True) = False : _|_ f (f _|_) False = False : (f _|_ $! (not False)) = False : (f _|_ True) = False : True : (_|_ $! (not True)) = False : True : (_|_ False) = Fase : True : _|_ etc. So, "fix f" continually expands this \_|\_ further and further inside of f. Now, we have a memoizer (specialized to Bool): (Bool -&gt; a) -&gt; (Bool -&gt; a) memo :: (Bool -&gt; a) -&gt; (Bool -&gt; a) memo =obs id Internally, memo creates a table of some kind and lazily initializes it as f is called: memo f = (\b -&gt; if b then fTrue else fFalse) where fTrue = f True fFalse = f False g :: Bool -&gt; [Bool] g = fix (memo . f) = (memo . f) (fix (memo . f)) = memo (f (fix (memo . f)) fFalse and fTrue inside of g are memoized; although in this case each internal call gets its own table. That's where we tie the knot, by redefining fix: fix f = w where w = f w g = fix (memo . f) = w where w = (memo . f) w = memo (f w) = (\b -&gt; if b then fTrue else fFalse) where fTrue = f w True fFalse = f w False Simplifying: g = w where fTrue = f w True fFalse = f w False w b = if b then fTrue else fFalse Now, lets try evaluating this at a concrete value: g False = w False = fFalse fFalse = f w False = False : (w $! (not False)) = False : (w True) = False : fTrue fTrue = f w True = True : (w $! (not True)) = True : (w False) = True : fFalse So we now have only two cons cells allocated, pointing at each other, and fFalse and fTrue pointing to the two cons cells. g simply returns the cell which its argument asks for without doing any additional computation or allocation.
Thanks for input and honesty. A cairo frontend. Interesting.
Also, you can simplify your type signature: recMemo :: (a -&gt; a) -&gt; (a -&gt; a) -&gt; a recMemo memo f = fix (memo . f) This might help you understand why it works just as well in the memo2 case :)
Replacing the image output with a call to R.sumAll speeds it up from around 15s to under 5. Looks like the image library is pretty slow. Even 5s is still quite slow, though. A C transliteration with the angles inlined and unrolled runs in around 100ms, close to what you would expect if performance is dominated by cosines at a cost somewhere in the neighborhood of Agner Fog's 40-100 cycles per FCOS. Time isn't significantly changed by replacing Repa with Vec or by manually unrolling the loop in quasicrystal (the second at least is good, I guess). 
The author's argument is on frameworks drive language adoption. I argue that company investment in a particular framework and company investment in a particular language can drive language adoption at a much faster rate. Company investment in framework and language can contribute to a software technologies. You have given a couple of example to disprove my theory. But there are so many examples that prove my theory. ... Pick your favorite company, if they were to invest in haskell or even take over parts of the language changes are just associating with haskell libraries, I guarantee the rate of adoption of that software will increase ten fold. ... I don't necessarily like it. But companies still tend to drive technology. It could be Haskell technology or Scala or HTML5 frameworks. The idea that you can just get a couple of open source hackers in a room and hope that their tech takes off, just didn't happen. Plus, why is this a bad thing? It is a good. What is wrong with say Google, taking a strong in Haskell? And advertising that they do?
I do not see author twisting arms of commercial companies and forcing them NOT to create or support a haskell web framework. On the other hand, successful open source projects DO get commercial backing (Linux, MySql, ROR, PHP). So yes, couple of open source hackers make things happen all the time. The idea that you can somehow rally commercial support just by asking for it is wrong. No one is going to waste their money on you. You have to make those companies notice your success, and want to be part of it, want to profit from it. And open source framework is the right start.
Or it is possible that the companies create the technology. 
yesod is actually what made me love TH and QQ. i've gone from thinking it's black magic best left to oleg-types to actively looking for places to remove repetition.
I agree that TH and QQ is something to be used carefully and judiciously and that it doesn't come for free... but repetition is the single greatest sin in software engineering. If that's the last option I've got left, I'll take it.
This is really fantastic. Install went super smooth on a Centos 5.3 box with sudo permissions. I'm pretty lucky I have the exact config you are targeting ! Thanks ! Oh .. forgot to mention. I'm behind a firewall and because of some proxy issues I had to download the rpm first and then install rather than be able to do it in one. Yum downloaded things fine so I'm not sure what was up.
Just one nitpick here: &gt; By the way "evaluate" should be defined for any monad, not only IO The "evaluate" function is very particular to IO; it enforces order of operations even in the presence of side effects. "seq a b" doesn't necessarily guarantee that 'a' is evaluated before 'b'; in particular, (seq (throw foo) (throw bar)) is allowed to throw either foo or bar; this frees up the compiler to do more optimizations. See the [imprecise exceptions paper](http://research.microsoft.com/en-us/um/people/simonpj/papers/imprecise-exn.htm). "evaluate" is supposed to guarantee that a particular exception is thrown even in the presence of other side effects; it's tied to IO. For pure values, such as the reader monad, (seq x (return ())) works fine.
Note that haskell-platform for RHEL 6 and derivatives is available in Fedora [EPEL](http://fedoraproject.org/wiki/EPEL). Currently it is based on ghc-6.12.3 but I'm thinking to rebase to ghc-7.0.4 after ghc-7.4 is released.
And by 21-23, he means 20-22 :)
After a profiling, it seems that divMod' is incredibly slow here. Rolling a less-generic version that only divides/mods by 1 brings the execution time from 15s down to 2s. Then the computation is dominated by wave.
That function is called (flip const).
By the way, let me throw something else out there: if someone comes up with a way to get the features Greg mentioned, without TH/QQ and without boilerplate, **please** let us know! We have no obsession with TH/QQ, and would be happier to have a simpler strategy. But so far, we haven't found another approach which we felt was overall better. Case in point: Boris came up with Groundhog, which was basically a simplified Persistent, and we jumped at it. Give us a better idea of type-safe URLs, and we'll do the same.
Running in real time with pre-release Repa 3 and Gloss 1.5. http://www.youtube.com/watch?v=v_0Yyl19fiI
Function that is needed here is properFraction, element of RealFrac class. It is fast, but need to be corrected for negatives: &gt; aux n = case properFraction n of &gt; kv@(k, v) | v &lt; 0 -&gt; (k-1,1+v) &gt; | otherwise -&gt; kv 
Shameless offtopic/advertisement: Indeed, Groundhog had operator-based query format which looks simpler. However I would not call all library simplified. It is more advanced than Persistent in some other aspects. For example, it supports data with multiple constructors and polymorphic fields. Also you don't need to use TH to define your data. TH is used separately to generate the auxiliary code. Typically it takes just one line. An example of Groundhog picking up data Test defined elsewhere and a counting it with a query: deriveEntity ''Test Nothing count $ (toArith OneField + 1) * 2 &gt;. AnotherField ||. ThirdField ==. "abc"
As for inspiration, [Zwaluw](http://hackage.haskell.org/package/Zwaluw) is nice attempt to type-safe URI routing using combinators.
The problem with off-by-one errors is that they still type check.
Good point about the install and download needing to be separated. I will fix up the instructions. 
When I see this: parseInt postId &gt;&gt;= return . getPostR The hlint within me screams: `fmap`!
Nice find. I wasn't as suspicious of that as I should have been. Hah, using another cos to pick the color might be cheaper. Odd it went all the way down to 2s, when I saw larger savings from dropping the image output. Maybe repa is smarter about taking advantage of the reduction than I expected. 2s is still quite a bit slower than dominated by the trig functions.
That looks almost *exactly* the same as web-routes-boomerang (which I believe took Zwaluw as inspiration). Equally neat with the reversible parsing. If you setup a convention for Handler names (as we have in Yesod) the dispatcher could be automatically created by TH. But even then it still requires the extra step of declaring an external data structure and then using derived references to it in the parser. As opposed to Yesod's QQ approach in which the user is never exposed to derived references.
I'm also writing about kind polymorphism, [here](http://students.mimuw.edu.pl/~kg262935/kpol/Author.pdf) is a rough draft and [here](http://students.mimuw.edu.pl/~kg262935/kpol/prog.hs) is source code.
I like the concept of being able to use Template Haskell without Quasi-Quoting, and would love to accept patches to make that happen for Persistent. However, I don't think we are going to undertake the effort ourselves until the record namespacing issue is solved in GHC - the quasi-quoting helps everyone deal with that in a consistent and easy to use way right now (whereas Groundhog is still risking field name conflicts). Glad to see you are still hacking on Groundhog! 
Not if you have [sufficiently](http://www.cse.chalmers.se/~nad/listings/lib/Data.Fin.html#1) [advanced](http://www.cse.chalmers.se/~nad/listings/lib/Data.List.Any.html#1063) [types](http://wiki.portal.chalmers.se/agda/pmwiki.php) :)
&lt;downvotes self&gt;
Wow, with this and [Constraint kinds](http://blog.omega-prime.co.uk/?p=127), Haskell will gain so much more power and convenience.
I dislike (over)use of TH, even though I've written some template haskell myself. I think my dislike comes from a couple of things. First, the TH code is not first class. It is much harder to reuse, mix and match, etc. than normal haskell functions. Second, it adds a stage to compilation that can fail in non-obvious ways. We had very annoying linker errors with TH when mixed with foreign function interfaces to C++. Third, quasi-quoting adds new custom syntax. Haskell has very nice syntax; custom syntax a library writer comes up with tends to be not as nice. A lot of the problems TH is used for can be solved with simple generic programming, such as that used by [regular](hackage.haskell.org/package/regular) or the upcoming generics in GHC 7.2. For some cases, like compile time templates, TH can be great. But my stance is to avoid TH if you can and stay in normal Haskell if possible. In Yesod (and this is looking from the outside, so I might be wrong) it seems as though the default reaction seems to be to grab TH more often than I like.
Well, I'm not a Haskeller but I'm an extreme VIM user, so I'd like to suggest a generic plug-in. This is one of the most powerful plug-ins I know: Vundle (www.vim.org/scripts/script.php?script_id=3458) Vundle is similar to pathogen (http://www.vim.org/scripts/script.php?script_id=2332) but it goes far beyond simply organizing the plug-ins folder, with vundle you can search for plugin-ins, install, and keep them up to date (without leaving VIM's window). It's almost like an apt-get (for Debian/Ubuntu users). It's reeeally nice!
I'm so excited by this! I can't wait for the release. I can see a torrent of EDSLs coming :)
The discussion of how to pronounce things like `f :: A -&gt; B` comes up fairly often. My new answer is that `f :: A -&gt; B` is pronounced "f takes an ***A*** ... and turns it into a ***B***" but you have to say it in your best Patrick voice.
The Christmas presents just keep piling up.
Interesting stuff, but I gotta say one thing. That prefix single quote (for disambiguating promoted data constructors) is really gonna screw up syntax highlighting in most editors. Edit: Scratch that. Since single quotes are used only for single character literals, it should be fine to stop searching for the matching single quote on a word boundary.
I'm biased, of course. But YAY!
any summary of what this enables for a haskell newb?
Given the footnote: &gt; Truth in advertising: at the time of writing the branch still has bugs and we have not yet released it. But we have done enough work to make us confident that there are no unexpected obstacles, and expect to release well before the end of 2011. Does this mean it'll make it for GHC 7.4? =)
The first part of the paper explains exactly that, the technical parts come afterwards.
Finally, this is amazing.
Moved -- http://inutile.club/estatis/password-security-checker/
Moved -- http://www.inutile.ens.fr/estatis/falso/
It already messes up a lot of them when used in TH splices. The same word-boundary trick should work there, too.
Agda, here we come!
Perhaps you should give Yesod a whirl and see if you still feel that way. I'm no expert, but from my limited toy experience with Yesod, the use of TH feels very natural in the small portions where it is required; the quasi-quoting custom syntaxes for routing, templates, and persistent really are quite nice.
I TOLD CHONGLI TO PUT IN A REQUIRED COMMA AND HE DID I AM GOD. In related news, I'm currently [learning me a haskell](http://learnyouahaskell.com/) and didn't know this place existed. I think this will be quite useful!
So, what other SHE-goodies should we be looking forward to in future versions of Haskell?
Take a look at Gloss which promises that you will "Get something cool on the screen in under 10 minutes": http://hackage.haskell.org/package/gloss http://hackage.haskell.org/package/gloss-examples You can also play around with it [here](http://dac4.designacourse.com:8000/).
In Debian sid (unstable) appears as 2011.3.0.0~pre.4: http://packages.debian.org/sid/haskell-platform :-) 
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.154.8165
what's wrong with f has type a to b?
Nothing, as long as you say it in your best Patrick voice.
It's a definite possibility for 7.4, but I can't make any promises.
We are also contemplating adding support for "typecase" including autogeneration of singletons. Personally I'd love to see some sort of syntax sugar for Applicative make its way into GHC as well, but that may or may not happen.
Haskell is the new Haskell.
Hm, the restriction to only one level of promotion feels somewhat annoying. It would be nice to have sorts of kinds, and kinds of sorts of kinds, and sorts of kinds of sorts of kinds.. but then you're getting seriously into Agda territory.
It's types all the way down.
boomerang took more from Zwaluw than just inspiration. A significant portion of code was lifted (with permission) directly from Zwaluw and adapted to be used with web-routes. (Mostly fixing some issues with unicode and url escaping). 
The use of TH in routing can reduce boilerplate. But it can also increase boilerplate. It lacks the extensibility and reuseability of combinators. I am not 100% certain about the details of the yesod routing stuff, so perhaps some of what I say belong is wrong. I do intend to investigate this for real in the next couple weeks. [In fact, it turns out that I completely missed the SinglePiece class which can be used to work around many of the issues I describe. That adds a different set of trade offs that I will describe later when I have actually done my research.] As far as I know, the variable parts of a route in yesod can only be String values or Ints or newtype wrappers around Ints. There is no way for a user to add additional types. Perhaps on our site we have a lot of dates embedded in the urls like: .../year/month/day/... with boomerang we could make a parser for that: day :: Router r (Day :- r) day = rGregorian . integer &lt;/&gt; int &lt;/&gt; int and then reuse that in the routing table: rFoo . "foo" &lt;/&gt; day &lt;/&gt; "bar" &lt;&gt; rBaz . "baz" &lt;/&gt; day In yesod, we would have to repeat the pattern over and over in the route table. /foo/#Integer/#Int/#Int Foo GET /bar/#Integer/#Int/#Int Bar GET Additionally, instead of the route Type containing something like .... | Foo Day | Bar Day .... It would be like: | Foo Integer Int Int | Bar Integer Int Int The constructors in the boomerang example contain more useful semantic information. They are not just numbers -- but rather a Day. In some sense, by only allowing Ints and Strings in URLs, the typed urls in yesod are actually not very type-safe. Additionally we would have to keep adding boilerplate to convert those values to Days in our code, if Days are what we actually want to work with. Now let's say that we want to change from: ../year/month/day/.. to ../year-month-day/.. In the boomerang example, we need only modify the 'day' combinator. day = rGregorian . integer . "-" . int . "-" . int In the yesod version we have to change every route. Additionally, in yesod, the order of the arguments to the constructors is linked to the order of the parameters in the corresponding url constructor. One could argue that this fails to properly separate concerns. If we want to change from: ../year/month/day/.. to: ../day/month/year/.. That will mean instead of yesod generating: | Foo Integer int Int we will get the type: | Foo Int Int Integer And we must now update every place that calls Foo to use the new calling convention. Here we are fortunate that the type actually changed. So, if we miss a place, we will get a type error. But if the type had been, Foo Int Int Int, then we would get mysterious runtime errors. (Actually I am not sure #Integer is actually valid in yesod. So perhaps we do have all Ints). In the boomerang example, we can keep the route type exactly the same. We need only modify the 'day' combinator to reflect the desired order. So, with web-routes-boomerang we can define the url type. Then we can define the mapping of type-safe urls to handlers. Then later we can define how to map that type-safe url to a url string and back with out it having any affect on the code we have already written. In yesod, changing your mapping from the url string to the url type can cause you to have to update code all over your application. I will write this up in more detail after I have finished my investigations. I am not against the use of TH or QQ in general. For example, I think jmacro is a fantastic library. I think that trying to create a combinator library for javascript is an exceedingly difficult prospect, and that jmacro is almost certainly the better solution for generating javascript from Haskell. To be clear.. creating a combinator library for a little DSL that gets compiled to javascript could be fine. But trying to represent the entirety of javascript as a Haskell combinator library would be ... tough to do well. 
I'd rather not see singletons. I mean: they might be necessary ancillary constructions to enable Pi and Sigma types, but that doesn't mean we have to *see* them. If a job's worth doing, it's worth doing well. It's also be nice to see the gap between kind and type narrow, so we can do proper telescopic constructions. How do we have a type indexed by a vector indexed by its length? The heterogeneous equality approach is not as difficult as it is frightening. Of course you can (and people do) mess it up. But it's not compulsory to mess it up. Other SHE-things in the pipeline: default superclass instances, perhaps, and there was a rumour about the long overdue pattern synonyms. Meanwhile, I'm thinking about what I'm going to shove next into the Strathclyde end of that pipe...
The new haskeller is unlikely to engage in type level programming directly, but she would no doubt benefit from libraries with better functionality or improved interfaces due to the advanced type wizardry they contain, things like the heterogeneous lists discussed in the paper.
&lt;$&gt;
Or Smalltalk: #| Expression -&gt; Output ------------------------------------------------ 1 | (Object new) class -&gt; Object 2 | Object class -&gt; Object class 3 | Object class class -&gt; Metaclass 4 | Object class class class -&gt; Metaclass class 5 | Object class class class class -&gt; Metaclass 5'| Metaclass class class -&gt; Metaclass Note: 5 and 5' are the same. The trick is in the 5th expression. We should define MetaMetaclass for the 5th output. For avoiding this infinite recursion, Smalltalk overlap 'Metaclass' and 'Metaclass class'.
I (can't really say "we", since most programmers here use different tools) use Haskell for bioinformatics: analyzing (large) data related to genes and molecular biology. See http://biohaskell.org and the "Bioinformatics" category on Hackage.
I really should, but we already use Happstack and Snap at work, and free time is very limited, sadly.
Looks promising!
This is not the same as Agda's mechanism. x : tau : Set : Set2 : Set3 : Set 4 : Set 5 ....
is this a puzzle: &gt;RWWJQXUQJQXHWDFQGZRTWNWFJATNBJUSYYMQYNTNMDTJWTWITNUJUXUSUTJNPQYJNUQRTW XWJDDTSJLKSGJUHTWTIWFFQZFUHWRGAFWGNWSJXUJXLXSFXXTTHXFJFSJYYTPFYHSMTHXW FQGSUIDUXRYQFQJFQNRPZSTJQXQXFXYFIXKBUNXWQSHSTNSFRTZYUWJJXWJDNJRFTPUTXW JJWFHJKNLZUNYJYDYTIJZTXXFZFNIZHUHJEFHWMJRBWYYYGJFNXAJNWYYYGNEKBTQHYNST FTNTU
I was wondering the same thing. hmmm.
It's not rot13.
I enjoyed the web server article.
You can actually still implement heterogeneous lists using nested binary tuples in Haskell98. The only difference is a lack of syntactic sugar to express the type A list with a Char, Int, Bool and Double would be represented as: (Char, (Int, (Bool, (Double, ())))) Really all you need to do is modify GHC to make syntactic sugar so that the above type can be rewritten as: [Char, Int, Bool, Double] or whatever other notation you prefer. Your head and tail functions just become fst and snd.
You can use any type you would like in Yesod's routes. You just have to define an instance for it. This [blog post gives a neat example](http://chplib.wordpress.com/2011/06/17/pieces-of-yesod-inverting-a-haskell-function/) of a case where you can actually do so in a reversable way. So this entire example is invalid- you can have re-usable custom route types in Yesod also. Perhaps you could edit your comment with a statement to that effect. Although it points out one limitation - I think you aren't allowed to have a slash in a data type - you would need to use a dash or some other delimiter. But this has little to do with combinators - it is a matter of convenience - forcing the user to indicate slash consumption with a combinator has the potential for errors, whereas the DSL makes sure the 99% use case of slash separated types occurs without error. If we ever have a user that cares we could come up with a way to keep the good default of no slash consumption but still allow for slash consumption for the rare edge case.
Here are the frequency counts. There is no C, O, or V. 4 3 A 4 B 7 D 2 E 23 F 7 G 12 H 6 I 30 J 4 K 3 L 4 M 20 N 4 P 16 Q 9 R 14 S 26 T 18 U 23 W 22 X 20 Y 8 Z 
It would also be nice if we could use cabal. I'm sick of copy-pasting utility functions from cabal packages when I could be just making it a dependency.
Ah yes. That would invalidate much of that particular example. Though maybe not all? For example, if you had: /foo/#Bar/#Baz Foo GET And you change the route to: /foo/#Baz/#Bar Foo GET That would still affect the order of the arguments to Foo and require you to update any place that uses the Foo constructor? What is the potential for errors you refer to with regards to the explicit / operator? As a side note.. it might be nice to use boomerang to assist in writing SinglePiece instances...
Everything gets easier when Patrick explains it.
I would assume that it's a big decision because the stack you use can be a very visible component of a library's API.
So you can modify how the parsed data is returned such that it is different from the order of the parsing. I think if you showed the code for that it would help demonstrate why that is a bad idea. That places complex logic in the routing that we first have to understand. Then we can no longer simply look at wherever we use url constructors and quickly understand what the url looks like - we have to also have knowledge of what is going on in the routing. To me this is a non-feature that demonstrates the benefits of the constraints of Yesod's routing. I would much rather change every use of the url constructor in the application (no matter what the size of an application is, there are going to be very few instances of any *one* url) then start down such a path. I think web-routes would be better if it took a list rather using a &lt;&gt; combinator which could be mis-typed for &lt;/&gt;. (this is already possible somehow I am sure, but it is what I would show in documentation). We would gladly accept patches to make Yesod's routing compile down to something like web-routes boomerang. There are issues with doing that though, including performance. But we shouldn't spend effort on this ourselves because none of our users care. As jgreene just said on IRC: why would you want to have to think about the routing at all, the routing dsl in yesod is dead simple and it's the one thing I didn't have to read over more than once
&gt; We already use Happstack and Snap at work I'm jealous :) &gt; free time is very limited Not so jealous :( I hope you either find yourself with some extra free time, or an opportunity to try Yesod at work. You don't have to *use* it, but it seems a pity that you can't even *try* it.
Don't forget to drink your Ovaltine?
How does looking at the URL constructor and knowing what the url might look like benefit you? Personally, I like the fact that I don't have to think about what urls look like at all (except when writing the code that decides what the urls look like). &lt;&gt; is just an alias for mappend. So you can use a list and mconcat instead if you prefer. Also, 'compiling yesod down to boomerang' is sort of the wrong direction. The idea would be to create a combinator using boomerang and then automatically extract the SinglePiece methods from that combinator. Something a bit like: rGregorian = xmap3 toGregorian (Just . fromGregorian) $(deriveSinglePiece ''Day [| rGregorian $ integer . "-" . int . "-" . int |]) Of course, one could suggest that it would be simpler at that point to just use the combinators everywhere. How would you actually define, instance SinglePiece Day, which can parse "2011-10-27"? Based on what I current know I would write (untested): import Data.Text.Read instance SinglePiece Day where fromSinglePiece t = case decimal t of Nothing -&gt; Nothing (Just (y, t1)) -&gt; case uncons t1 of Nothing -&gt; Nothing (Just (c, t2)) | c /= '-' -&gt; Nothing |otherwise -&gt; case decimal t2 of Nothing -&gt; Nothing (Just (m, t3)) -&gt; case uncons t3 of Nothing -&gt; Nothing (Just (c, t4)) | c /= '-' -&gt; Nothing |otherwise -&gt; case decimal t4 of Nothing -&gt; Nothing (Just (d, t5)) -&gt; Just $ fromGregorian y m d toSinglePiece day = Text.pack (formatTime defaultTimeLocale "%Y-%m-%d" day) But, perhaps it is easier than I think. An obvious improvement would be to use some sort of parser library instead of just parsing the Text by hand. If we are going to use a parser library, then why not use one like boomerang, which also generates the printer for us in a safe way? 
This doesn't look like a substitution cipher, because the only substitution that makes "READER" appear in the text also creates a lot of weird letter patterns and I'd expect a reference to the Monad.Reader somewhere. (For reference: ...READEREA....E........R....RD....E.......R......DRDAD.D.R..E.R.DE...A.R....R....RD ........E..D...........RADRA.A..AA...A.R..R...........A..E..D..DA..E.ER.E......REAEA.A.. .A..D.A.E...........D.RRA.R..R....D.A.RR...R....D..R.....R..AA.......D.R.....R.......R..A .R...........E.........D ) There's also no substitution that can generate "YAMAMOTO", yet I'd expect a mention of this issue's contributors.
I prefer to say it in my best Simon Peyton-Jones voice.
[Indent object plugin](https://github.com/michaeljsmith/vim-indent-object) lets you select and manipulate blocks by indentation level. 
I think that the main reason is the fact that Cabal wasn't designed to be a package manager but a build system. See [Repeat after me: “Cabal is not a Package Manager”](http://ivanmiljenovic.wordpress.com/2010/03/15/repeat-after-me-cabal-is-not-a-package-manager/).
The separation of cabal and cabal-install makes sense, so I guess my beef is with cabal-install, but if it lets you install packages then it should let you reverse the process. Any idea how to do that short of removing the files manually?
As near as I can tell, the only use for cabal is to install cabal-dev, which should then be used for everything on a per-project basis. Not only should you never install anything globally, you probably shouldn't even install it for your local user. Long term, I sort of worry about what happens if everybody does that, because it may remove the incentive to fix the situation we have now, but right now, it's the only effective defense, especially if you're depending on a lot of packages. (Happily, if used correctly it also lets you download a package, apply a small fix, then use that instead, if necessary.)
This is a very common thought for newcomers actually. I still think that it should be done too but I am more interested in solving the package dependencies hardships ATM. Therefore maybe this is a good item to add to the Haskell Wiki in an FAQ section in the meantime. 
We are probably going to have to agree to disagree about the (negative) value of making the ordering of constructor arguments for the url different from the actual form of the url. yes, you can use any library you would like to generate a SinglePiece instance - I am not sure what the point of the code dump here is. Personally I would re-use an existing library that parses it in with a single function call. I like the idea of using a boomerang-like parser for the ToSinglePiece instance. Why do we need Template Haskell though? can't we just have the equivalent of toSinlePiece = parseIn boomerangParser and fromSinglePiece = parseOut boomerangParser ? We have to keep in mind performance though. We are doing some routing optimizations now that we need to benchmark. After that it will be easier to try out different parsing techniques or routing approaches in general.
I see where you are coming from, but the problem is, several uses of "cabal install" are for installing executables: happy, hlint, yesod, djinn, pl, agda, even cabal-dev which you mentioned. It would be nice if there was a clean way to uninstall such executables (and related baggage) that were installed via "cabal install".
**No, ghc-pkg unregister does not actually remove the installed files. Yes, you do need to delete them manually**
I prefer: 'f' is a function from 'A' to 'B'
What's wrong with template haskell!?! You can do it by hand if you want. It's just more boilerplate. Regarding the code dump.. mostly I wanted to make sure I wasn't missing any more features of the yesod stuff. Given that example, it seems like yesod is lacking in conciseness and safety when you get down to the specifics of parsing within an individual path segment. You have to repeat the syntax description twice, once to create the parser and again to create the printer. And you get no real assurance that they are even inverses. In my web-routes experiences.. I often screwed it up. But you do have a more concise syntax for describe things in terms of sequencing the path segments, and you don't have to declare your type explicitly, because it is derived implicitly. In practice, maybe that is a bigger win. In general, I think people keep their urls pretty simple. 
It's a mystery. A program that can install should also be able to uninstall.
in order to keep it simple?
&gt; Cabal wasn't designed to be a package manager but a build system On behalf of my fellow system administrators: bless you, sir! 
Lots of people want this, and its been an outstanding request for years. Mainly it hasn't been done yet because "its not something you'd want to get wrong." http://hackage.haskell.org/trac/hackage/ticket/234
I think you are missing the point that it is up to the Yesod user to use boomerang or any other parser they want. We could add documentation or recommend that users try boomerang, but I wouldn't want to couple Yesod to invertible parsing. There is already plenty of great parsing code available that is not invertible, and we have not yet benchmarked boomerang. Having to write 4 words of boilerplate to use Boomerang doesn't really matter - as you said, users keep urls pretty simple. I just looked over an application with 50+ routes - they used one custom type that they had to write a parser for, and one of the definitions was: toSinglePiece = pack . show . fromEnum 
Cabal-dev is basically a filesystem-namespaced cabal. It builds executables, too. To run them, change your $PATH. I don't disagree that some uninstall support would be nice. I intend this as a very practical workaround rather than a "solution". That said, unlike most things called "workarounds" this actually has other benefits as well and in some cases is just plain "the right answer".
My point is that it shouldn't take two parsers to do one job... The yesod th parser, and then another parser for the individual segments. Given the difficulty of parsing custom types, I am not surprised that an application with 50+ routes would avoid using them :)
When I can get away with it, which is most of the time, I find it simpler and more efficient to clean or reset just the main set of packages (with [ghc-pkg-clean &amp; ghc-pkg-reset helper scripts](https://gist.github.com/1185421)).
Thanks for the link. It cleared a lot of doubts I had and made me realize I was expecting cabal/cabal-install to do things which they were not designed for. A side effect is that now I understand why Fedora has everything packages in RPMs for Haskell while yesterday I was thinking "why waste time when _cabal_ can do all of this?" 
Thanks for that pointer. I think I'll take a look at this issue. 
How hard would this be to implement?
...the other main reason is that it'd have to rebuild all libraries depending on the uninstalled one. `cabal upgrade` was removed, too, for basically the same reason.
What converted me to this way of thinking was trying to develop a Yesod app in one directory while working on an entirely separate project in another. Yesod's dependencies had a nasty run of going through phases where they conflicted with _themselves_ (which I don't blame Yesod for, it's just exposing a problem in the underlying data it is working with), let alone trying to get a working non-Yesod install with its own quirky requirements (which got caught up in the mtl/transformers switch in fun and exciting ways back in the day). I had to copy cabal-dev directories between machines a couple of times because the exact same cabal file for my app worked one day, but not the next. Prior to that I had also occasionally just gone in with a flamethrower, but bootstrapping up to Yesodability can take a bit of time.
We use cabal-debian to debianize every cabal package we use. It doesn't make sense to build a packaging system only for haskell packages, what if your program needs packages written in other languages?
Simon Peyton-Jones is patrick?!?!
Yet curiously, it's rather common that most build-system/package-manager hybrids do not support uininstallation. From looking at implementations, the reason seems to be that there is no natural way to store (let alone record) the set of installed files.
\+1. I've been playing with Yesod recently and the massive dependency chain is only manageable from a development POV with `cabal-dev`, as far as I've seen. Mostly the massive dependency chain has its natural set of problems, but also, because it's huge and GHC is slow, building takes a long time - so blowing away `~/.ghc` for example hurts a lot when something randomly trips out. And because of the massive dependency chain, you can weird it out pretty easily yes, by just working on something else. If you need to install things like `happy` etc or do deployment with a specific requirement, sure, `cabal` is fine probably. But if you're *actually* doing *active* development, there is literally no substitute or reason to not use `cabal-dev` these days as far as I can see. Yes, it papers over problems that exist in the underlying model, but frankly I'm not happy to fix those every several days at this point. It just makes life easy, and it also lets you make sure *your* dependencies and build are sane.
Article typo: Ganesh announced the call for nominations, not Ian. :)
Just solved it. Hint: It's using both a substitution cipher and a transposition cipher.
I'm really excited by this. Thank you. :-) Probably a typo: beginning of §3.4, "we do promote GADTs" -&gt; "we do NOT promote GADTs" 
There is a natural place where Cabal will register the package metadata so that ghc-pkg can find it. Alongside this would be a natural place to record the list of installed files.
I (and others) have run into all sorts of compatibility problems with GLUT in a Windows 7 (64 bit) environment. GLUT is no longer maintained, at least for Windows. Have you tested under Windows 7 and if so what version/build of the GLUT DLL are you using? Thanks.
Is anyone else seeing a compilation error when compiling snap-core 0.6? [ 7 of 19] Compiling Snap.Internal.Http.Types ( src\Snap\Internal\Http\Types.hs, dist\build\Snap\Internal\Http\Types.o ) src\Snap\Internal\Http\Types.hs:181:48: Illegal symbol '.' in type Perhaps you intended -XRankNTypes or similar flag to enable explicit-forall syntax: forall &lt;tvs&gt;. &lt;type&gt; cabal: Error: some packages failed to install: snap-core-0.6.0 failed during the building phase. The exception was: ExitFailure 1 **EDIT:** Checking out the [master sources on GitHub](https://github.com/snapframework/snap-core) and building from that (v0.6.0.1) seems to do the trick.
`gtirioni` you might also read the much more precise wisdom of `monochrom` , see especially the section on `unsafeInterleaveInstall` http://www.vex.net/~trebla/haskell/sicp.xhtml#unsafeInterleave (I don't see that anyone has yet propounded advice on this topic that is suited to all platforms and, more importantly, all relations the user might bear to the Haskell language. Different arrangements are appropriate for a non-Haskelling `xmonad` or `pandoc` user, an industrial user, and an average Haskell 'hacker'. `monochrom` is clear on this point and addresses himself to the last of these.)
I would prefer to deprecate GLUT. I didn't test it because I only released one minor change to it that someone else tested for me (as it was a fix to a bug they were having). I do have a windows 7 (64bit) machine I could test on. Do you have test programs you could share with me? Have you tried the GLFW-b library? That's what I use instead of GLUT and I've been happy with it. Here are my example programs that use it: * https://github.com/dagit/nehe-tuts/ * http://hackage.haskell.org/package/nehe-tuts I also created a matrix of the alternatives to GLUT: http://blog.codersbase.com/2011/03/picking-gui-library-to-use-with-opengl.html
I'm interested in both OpenGL and Haskell, but I have yet to use the Haskell bindings. I would be willing to give a hand where needed.
Thanks for the offer! I have some example programs to help you get started: * https://github.com/dagit/nehe-tuts/ * those are based on the tutorials here: http://nehe.gamedev.net/ (just look on the right side of the screen for "Legacy Tutorials", the lesson numbers should match up)
Great! I'll get back to you when I have worked through some of the NeHe tutorials.
What are your issues with GLUT? I used it on Win7 64 bit without problems (apart from the usual platform-independent issues, like lack of handling of extra mouse buttons). In fact, I tried to switch away (to GLFW[-b]) a few times, but I always returned since I had more serious issues with those. GLUT is an ugly aging dinosaur, but it still seems to be the most reliable player...
I vaguely remember reading at one point that the plan was to have a flattened type/kind level: types would classify types (would classify types would classify types...). I'd be curious about the reason it changed.
+1 for pattern synonyms
What issues did you have with GLFW-b?
My memory is very bad, but iirc, last time it was the lack the display callback (i couldn't manage to update the screen at specific events), and maybe some threading issues. Before that, maybe some cross-compatibility issues, but that was a while ago...
If you happen to try it again or remember the details, please file a bug report :) https://github.com/bsl/GLFW-b/issues I sometimes looks at the issues listed there and send Brian patches. He's been very easy to work with.
Very cool with the snaplets. For the curious, the equivalent in Yesod is subsites (although if you are working at the WAI-HTTP level and don't need to know about the application you should use a WAI middleware or application). Yesod authentication is done through a subsite and two people have now developed an admin subsite. I don't know how to make a comparison between the two yet, in part because I haven't even looked at a Yesod subsite yet. From my very limited understanding, this actually looks exactly the same as Yesod subsites except it defines a file system layout. Most likely i am missing something though.
It's actually quite similar, yes. We realized this after coming up with our version (we hadn't looked at the Yesod subsite stuff), which is a good sign that we've both settled on a sensible encoding of this idea into Haskell datatypes. There are differences between the two -- the biggest one from my perspective is the lens stuff which gives you a MonadState instance on the subtype, which makes a huge difference in how you would actually use them. I'm guessing Yesod could probably implement this idea without trouble. It's a good idea IMO, we were happy when we figured it out: it makes programming this way very convenient, you get simple, easy, and typesafe request-local state. Ours also seems to have fewer types and typeclasses, is less complicated, is likely less flexible and possibly does less.
would be better to point new haskellers to the Haskell Platform
I just released 0.6.0.1 to hackage, so it should work now.
Really glad to see work being done here. In the past I have relied upon a derivative of https://github.com/scan/GLUtil for buffer objects, textures, etc... Is there any plan to have a pure programmable pipeline version available integrated into these base libraries?
I recently spent some time translating [chapter 4 of duriansoftware's OpenGL tutorial](http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-4:-Rendering-a-Dynamic-3D-Scene-with-Phong-Shading.html) to Haskell. The only real snag was that he uses glUniformMatrix4fv to feed matrices to the shaders, and that binding is still not exposed. It looks like this is a known issue (#14 in the github project), and it's easy enough to work around it by passing a vector-of-vectors and creating the matrix in the shaders. So... yeah. I guess that's my way of saying thanks for taking over maintainership, and I will keep my eye out.
Matrix handling is offered in my [GLUtil](https://github.com/acowley/GLUtil/blob/master/src/Graphics/GLUtil/Shaders.hs) library and any forks of it floating around on github.
I hope I can help with development moving forward! I've become too comfortable FFI'ing into little C stubs to get around trouble spots. I'd also like to see more effort put into middle-ware style libraries that provide a *small* amount of abstraction over the nasty imperative bits without totally discarding any and all connection to the world's collective OpenGL wisdom.
The usage of lenses here is really spectacular.
Yes, the page is located at "monads.php". PHP! :P I spent very, very little time creating this website, and someday it will be running a web server written in Haskell, serving .lhs files as beautiful HTML, and all that good stuff. Someday. For now I just use some quick-n-dirty PHP to slap the simple header and footer around a plain old HTML page. And yes, this is *another* Monad tutorial; but I think you'll like it. Please give it a glance, and let me know if anything: * is not "Dead Simple", * is Misleading, or * is Just Plain Wrong
While I personally understand monads, the most common issue people have with most monad tutorials is that they don't show enough examples on how monads are useful.
Well, I took the complete opposite approach. :) I feel that monad tutorials get convoluted and confusing when you try to mix up explaining "what is a monad?" with "how do I use monads?" and "why should I use monads?"
The biggest problem is that you can't show how to use a monad without showing what a monad is, and you can't show what a monad is (in a way that the average programmer understands) before you show how to use it.
I would advise against describing monads as pointed. A point is a distinguished element, like the XPlus type classes have -- list has [], nat has 0, etc. A monad instead has an injection that will inject any value into the monad. This is a very different thing. The only way it could be true to describe a monad as pointed is if you talk about the category of endofunctors, where the injection is a natural transformation return : 1 -&gt; m, in which case it's only pointed-al (since monads are monoidal, not true monoids).
Well a good introduction can be seen at this video here, [link to video](http://www.youtube.com/watch?v=b9FagOVqxmI&amp;feature=share)
If cabal is not a package manager then why can it install packages?
For a moment I was hoping you were going for the minimal approach with fmap, pure, and join. As it is the tutorial gives the impression that you have to have all these other things as well to be a monad.
I was patterning my explanation to closely mirror the typeclassopedia. I agree, though, that "injection" makes a lot more sense.
&gt; monads are monoidal, not true monoids Can you expand on this? Is it a claim about mathematical monads or just Haskell monads? What does "monoidal" mean when used like this? I thought that monoidal categories were those with a chosen bifunctor and 1 object, and monoids were objects in monoidal categories with particular homs and commutative diagrams. And monads are just monoids in the monoidal category of endofunctors over a given category.
Sorry to disappoint. The only "other" things I mentioned were bind and ap, (and that &gt;=&gt; can be derived). I did exlicitly mention that the minimal definition of a monad is either {fmap, return, and join} or {return and bind}. I also felt it important to explain that Monad is a typeclass, where the types have kind star-to-star; it seems to me that most monad explanations take this information for granted. But I don't think I mentioned any part of a Monad that is not a necessary part. Monads (in theory) *must* be Applicative Functors, which *must* be Functors, and also Pointed, which *must* have kind star-to-star.
&gt; you can't show what a monad is (in a way that the average programmer understands) before you show how to use it Well this is exactly what I tried to do. If the "average programmer" doesn't understand, then, well, so be it. However, I am seriously considering writing companion tutorials that demonstrate "how to use monads". I don't believe that anyone is going to click through the link with 0% knowledge of monads, and after reading the article be 100% comfortable with monads, but I think it is a step in the right direction.
Well never happen! understanding CPS will (oops somehow the sentence recursed and knotted itself).
One thing Haskel community has not yet discovered is examples and real world use cases. Edit: I really like the tutorial. Thanks OP. It just needs some real world examples.
Maybe, state, reader, writer, exceptions, parsers, compositions of all of the above, etc It's pretty nice to see a monad instance and know how to use a library without having to read a whole bunch of api specific combinators.
I still remember the first time I was learning how to make instances of the Arbitrary typeclass for my own types, and I realized that I could use fmap, or &lt;$&gt; and &lt;*&gt;, to define it very simply in terms of `arbitrary` for the subtypes.
Thanks for putting this break down in one place. When learning this material for the first time it had to be scraped from several locations. I do not think i have run into a document which covers this issues in a concise manner. Do not listen to this people calling for more examples. While examples are great many monad tutorials out there do not provide a coherent abstract structure to tie everything together. While this does. 
I think it's not possible to distill typeclassopedia further without loosing pedagogical efficiency. If you want to understand monads by taking the theory road, typeclassopedia is the best thing in the west. (And yes, do the exercises!) For a practical introduction (seeing monad without knowing they are monads), the parser chapter of Graham Hutton's "Programming in Haskell" does a good job. This brings you some intuition about how to use monads. The IO monad is too special to use it as a first example. I also think that monads should be learned after functors, monoids and applicative functors.
this is good stuff. i wish had read this before all the other tutorials had confused the hell out of me. question: could someone explain the difference between a "type" and a "kind" for me? they both look like function signaures to me.
Types classify values, and kinds classify types, in haskell
He probably just wanted to point out that monads aren't monoids in the usual algebraic sense, but in a more obscure (but related) categorical sense
We need something similar to the Wadsworth constant but for monad tutorials. Or any tutorial, for that matter. This tutorial is a perfect example of it. You want to write a tutorial? Don't waste your time on introductions, meta information or disclaimers: grab your readers by showing some code and dive right in. 
Here is some simple css, that will majorly add to readability: body { font-family:arial; text-align:justify; font-weight:normal; padding:10px 10%; line-spacing: 3px; } 
I considered using separate notation for "type foo has kind bar", but I'm not aware of anything used besides `foo :: bar` to express such a thing. Anyways, you should be able to tell the difference because kind signatures use asterisks, while type signatures don't. In a way it's good that they look similar, though, per godofpumpkins' comment.
A very nice companion to the Typeclassopedia, but if you want something to scan over quickly. I found this a good way to confirm my knowledge so far, thanks!
Thanks. I was unaware of that project. The plan at the moment is to have OpenGLRaw support all of OpenGL. Then the next step is to have the Haskell OpenGL bindings expose everything that OpenGLRaw supports.
Thanks, I updated the whole site to use exactly that! :) I also added h1, h2 { text-align: left; } Even though my site has basically no content, I try for a minimalist layout that still looks good on mobile devices, and in the absence of other css, the headers were getting justified, which looks a bit wonky on my phone. The 10% padding works great everywhere. Someday I'll probably redo the entire site's look, but for now that little tip worked wonders. :) Any other css tips? Not guaranteeing that I'd implement them, but I'm always open to suggestions.
Yeah. It's tricky to do the middle-ware bits well. I think the first step is to get a solid foundation. My philosophy is that if you do that right, others can easily build on it and you sort of let the community of users innovate and come up with the best solution.
I guess a white background is fine. You can always dabble in a bit of very light colour shades. Ie, light grey background, then everything inside a content div with a white background. Just don't overdo it and end up with something ugly. Also, get your headers sorted out (this will force you to write standards compliant html, plus it looks more professional), &lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"&gt; &lt;html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"&gt; I guess you don't really need to worry about this yet, if this is just a placeholder. But remember that readability is key.
Every time i use hlint it breaks on quasi-quoting. :( Considering that my haskell application is written in yesod, that means that i practically cannot hlint any of my modules. Does anyone knows about some workarounds for this problem ? 
A monoid, in the strictest sense, is a set A with two arrows e : 1 -&gt; A, m : A*A -&gt; A satisfying certain laws (where * is cartesian product). A monoidal object A looks pretty much the same, except it's not a set, and we use a more general notion of product. For endofunctors, the "product" is horizontal composition, so a monoidal endofunctor is an object F in the category of endofunctors for your chosen base category, with arrows (ie natural transformations) e : 1 -&gt; F, and m : FF -&gt; F, where adjacency is composition. Usually people will just describe monoidal objects as monoids but the distinction is pretty important because the category of monoids, for instance, is monoidal sets, not monoidal-objects-in-general.
You can keep the pointed explanation, but you should couch it in an explanation that it's a point in a particular category. As it stands, an (family of) arrow(s) return : forall a. a -&gt; m a is not technically a point, nor even a point-oid, in the category of interest.
It is a little hard to see what `Maybe` classifies, unless it's all the `Maybe` values in whatever type. That is, I can complete the sequences Left 1 :: Either Int Char :: * 'a' :: Char :: * getChar :: IO Char :: * but how do I start these sequences?: ... :: Maybe :: * -&gt; * ... :: IO :: * -&gt; * or these? ... :: Either :: * -&gt; * -&gt; * ... :: StateT :: * -&gt; (* -&gt; *) -&gt; * -&gt; * If you use the word 'type' for anyhow-kinded things, as you do, then it looks like a lot of them don't classify so-called values. Things of kind * classify values (and e.g. kind #); if types are what classify values then only things of kind * (# etc) are types. The facts are plain enough but drb226 is looking for the best way of communicating them. `drb226`'s metaphor (ahem) of completeness and incompleteness has something going for it. 
Tell that to a category theorist. :)
But Maybe isn't a type, so why would godofpumpkins' statement be problematic?
Because the claims "Maybe isn't a type", "Maybe has kind * -&gt; *" and "kinds classify types", taken together, are a contradiction? 
~~~That's not a contradiction at all. Maybe *isn't* a type: type's have kind `*`. Maybe has kind `* -&gt; *`, making it a type operator. But yes, if kinds classify types, that leads to contradiction.~~~ Retracted due to conversations on irc.
EDIT: Nevermind, it's already submitted: https://code.google.com/p/ndmitchell/issues/detail?id=473 [Submit an enhancement request](https://code.google.com/p/ndmitchell/issues/list?q=proj:HLint) or maybe even a patch? I've never looked inside of HLint so I don't know how hard it would be to understand and modify the code to take quasiquoting into account. Maybe easier would be to implement something like a `-- NO_HLINT` comment that tells HLint to ignore a certain section of your code.
The typing colon is a relation relating values to types. Nothing says that all types are inhabited. All values have types, and all types have kinds. Not all types have values, and not all kinds have types. I think the distinction we need to make here is what people call a "sort" in Agda, which is what we allow to be on the right of a colon. Typically, people only allow "typey" things to be there, so `*` in Haskell, and in Agda things like `Set` or `Set 1`. You can't define new sorts in Agda, but you can in Haskell when the autolifting stuff makes it into GHC. I'm not saying it'd be meaningful to have something like `x : Maybe`, but it does make talking about this stuff a bit more consistent. You just make sure there is no such `x`, so your logic doesn't let you prove anything you couldn't prove before. I may be completely wrong here, but it doesn't seem all that ridiculous :)
Just for our convenience, I guess.
What exactly do you mean? The classic OpenGL bindings supports all this (ok, with the notable exception of framebuffer objects); GLUtil looks to me only a thin convenience wrapper around it.
Jones: &gt; The biologists have invented what they call the taxonomical hierarchy; they use it to classify people. Smith: &gt; Well, actually people are just one of the headings in the hierarchy, it is clearer to say that it classifies all terrestrial organisms. Jones: &gt; No, I was of of course assuming that most species and genera are unpeopled. It is a prejudice to think that all headings in a people-classification must be peopled. I may be completely wrong here, but that seems kind of ridiculous :)
What would you call things that go on the left of the kinding relation (the colon)?
...does anyone happen to know the story about why it was called "Monad" originally?
btw, 7 of those 10 packages happen to be ghc-bundled packages... maybe that's part of the reason they are "popular"?
Kinded things? Or, for a given kind, things of that kind? The whole point is that we don't know what to say, when speaking in English, and in a monad tutorial. Where `*` is on the right, *types* go on the left. This is the only case where we have something to say -- when speaking English, and in a monad tutorial. I was considering the thought that is best not to ruin this, the one piece of intelligible information, by declaring types to be whatever goes on the left of a kinding statement. Then there is no way into the kind system.
What is the downside to the simple `value : type : kind` approach I mentioned?
Its cause of a different kind of [monad](http://en.wikipedia.org/wiki/Monadology). Explained at the very start of [this](http://channel9.msdn.com/shows/Going+Deep/Expert-to-Expert-Erik-Meijer-and-Jeffrey-Snover-Inside-PowerShell/) Channel 9 video.
Yes! Vundle is really, really nice. &gt; Bundle 'lukerandall/haskellmode-vim' :)
It certainly screws up the Github syntax highlighting: https://github.com/ia0/GhcKindsExamples/blob/master/NatRecord.hs
I misplaced the &gt; (in a way that the average programmer understands)
`text-align: justify` is usually detrimental to readability (it makes it harder to find the next line if lines get long). Browsers are also usually don't know how to break up words, so they introduce huge spaces which hurts readability even more.
The introductory meta information / disclaimers were so the reader knows exactly what to expect from my "monad tutorial". Perhaps using the word "tutorial" was misleading, which is why I felt the need to explain that I would not be "show[ing] some code and div[ing] right in". I also wanted to make sure that readers were comfortable both with currying and with type and kind annotations, the latter of which I made heavy use of throughout the tutorial.
I think it just uses haskell-src-exts for parsing...
I will mention this to my friend. Thanks for the heads up!
What kinds of functions would you write to operate on such lists?
One example: (x, y) !! 0 = x (x, y) !! n = y !! (n - 1) Edit: Oops, that would not type-check. I had a use for these a while ago. Give me a second to dig back up those functions.
You can't write those. Try :) Even if you had a story for being able to distinguish between nil and cons.
It seems you're looking for a HList? ([Hackage](http://hackage.haskell.org/package/HList). [Paper](http://homepages.cwi.nl/~ralf/HList/)).
Ok, so I went back through my old notes. My original motivating use for this was making it easier to positionally access large tuples. In other words, instead of using the tuple: (a, b, c, d, e) You'd use the nested tuples: (a, (b, (c, (d, (e, ())))) This made it very easy to write functions to access elements by position within the tuple. For example, to access the fourth element: third = fst . snd . snd The type of which is: (a, (b, (c, d))) -&gt; c So it will work on heterogeneous lists of at least 3 elements and won't even type-check on smaller lists, raising an error at compile-time. The only disadvantage is that you cannot write a general accessor function like (!!) as I failed to do below. It will never type-check, except against infinitely-nested tuples, which haskell98 won't allow. I'll give some more examples as I go through my notes.
Why would you want to do this instead of declaring your own data type and using record syntax to access it?
Yeah, I just caught that, but see my parallel comment below.
No function abstracted as a catamorphism will work on this, unless you want to write a separate algebra for every tail's type. Even `length` can't be trivially implemented, exactly because of this. Anamorphisms are worse, and probably impossible without dependently typed algebras. And like many heterogeneous lists, this solution also doesn't offer a functor interface. So what *can* you do with it?
I agree completely! I really appreciate you taking the lead on the effort to maintain API-level translations of bindings like this. This approach is critical to the development of useful abstraction layers.
 o another example. Let's say that you have some tabular data organized as tuples and you don't have a database (which was my situation at the time I came up with this). In my case I had 5 columns of heterogeneous data that a colleague had given me to work on his computer. I could organize the data like 5-tuples like so: [(a, b, c, d, e)] and write 5 accessor functions: get1 (a, _, _, _, _) = a get2 (_, b, _, _, _) = b ... or I could organize it as my heterogeneous list: [(a, (b, (c, (d, (e, ()))))] and then the accessor functions are: get1 = fst get2 = first . snd get3 = second . snd ... Besides some slight notational convenience, there is still no advantage to my approach at this point because you still have to write out an accessor function for each position. The advantage of my approach was that once you had written out primitive functions for each index in the list it was very easy to compose them, unlike functions designed for n-tuples. So let's define some primitive list manipulations. I found that one set of minimal functions that I had to write out by hand were: import Control.Arrow drop1 = snd drop2 = snd . drop1 drop3 = snd . drop2 ... with1 f = second f with2 f = second (with1 f) with3 f = second (with2 f) .... So if I want to get the fourth element, I would just use: fst . drop3 If I wanted to remove the element at position 4, I would use: with3 snd If I wanted to take the first 4 elements of the list, I would use: with4 (const ()) If I want to substitute the 2nd element with a new element: with1 (first $ const y) ... and so on. And the best part is that these functions will all work and type-check on any heterogeneous lists that satisfy their minimum length requirement. Providing a list that is too small generates a compile-time error.
See [this](http://www.reddit.com/r/haskell/comments/lt9w2/heterogeneous_lists/c2vfkf7) followup which explains that once you write out the primitive positional functions you can then compose them very easily.
I forgot to mention that another advantage is that you only have to write one set of primitives and it will work for all heterogeneous lists, whereas for n-tuples and data types you have to write new primitives for each new length. With my approach, if you want to handle heterogeneous lists up to 99 elements, you only have to write 99 primitives. With n-tuples and records, you have to define 99 * (99-1) / 2 primitive functions to handle all lengths up to 99 and they are not composable.
Why not use the TypeOperators language extension? The syntactic sugar can easily be made, unfortunately not using a comma. Type level syntactic sugar: infixr :&gt; type a :&gt; b = (a, b) Value level syntactic sugar: infixr ! a ! b = (a, b) Example HList. myHList :: Int :&gt; Char :&gt; String :&gt; () :&gt; Char :&gt; Bool :&gt; () myHList = 10 ! 'a' ! "hallo" ! () ! 'b' ! True ! () Btw, using nested tuples for heterogeneous lists is not uncommon. 
Thanks for the TypeOperators tip.
As said before, indexing with value level numbers will not work. You can use type-level numbers though. This requires the TypeFamlies extension: class Index a b where type Idx a b :: * (!!) :: b -&gt; a -&gt; Idx a b instance Index Z (a, b) where type Idx Z (a, b) = a (a, _) !! Z = a instance Index n b =&gt; Index (S n) (a, b) where type Idx (S n) (a, b) = Idx n b (_, b) !! S n = b !! n main :: IO () main = do print $ myHList !! (S . S . S . S . S) Z print $ myHList !! Z print $ myHList !! (S . S . S) Z putStrLn $ myHList !! (S . S) Z 
You can write such an accessor, but you need something GADT-like. For example, here's something from some HList experiments I wrote a few days ago to play with constraint kinds (I'll omit that stuff as it's irrelevant here): data HList as where Nil :: HList () Cons :: a -&gt; HList as -&gt; HList (a, as) data Index as a where Here :: Index (a, as) a There :: Index as a -&gt; Index (t, as) a (!) :: HList as -&gt; Index as a -&gt; a Cons a as ! Here = a Cons a as ! There i = as ! i The GADT representation of the HList isn't crucial if you have an index, but makes other operations that don't have an index easier, because your nested tuple representation doesn't allow me to "typecase", to determine if a polymorphic list is a cons or a nil. Another approach is to write a closed typeclass and make (,) and () instances of it, but this GADT stuff is a lot more convenient.
Exactly. This is the pretty much the same thing, after renaming (,) to HCons and () to HNil. Which is a good thing, as you would not want to confuse heterogeneous lists with actual pairs.
Thanks for the excellent response. I'm still new to GADTs, but let me see if I got this correct. The type "Index a b" represents instructions for how to extract an element of type "b" from an object of type "a". "Here" is the base case that says that if we have a tuple of type (a, as), then we can trivially extract "a" from the first element of the tuple. "There" is just recursive induction. So, for example, if I wanted to instead make a heterogeneous tree I would start with the GADT: data Tree a Leaf :: Tree () Node :: Tree l -&gt; x -&gt; Tree r -&gt; Tree (l, x, r) The index type would then be: Index a b where Here :: Index (Tree (l, b, r)) b Right :: Index a b -&gt; Index (Tree (l, m, a)) b Left :: Index a b -&gt; Index (Tree (a, m, r)) b ... and my accessor function would be: (!) :: Tree a -&gt; Index a b -&gt; b Node l m r ! Here = m Node l m r ! Right i = r ! i Node l m r ! Left i = l ! i Did I get that correct?
nice! database stuff in haskell seems to get light attention in tutorials, this is appreciated!
I understand, but what is gained by doing this over writing your own data type?
snaplets look awesome. seems to be a server-side take on html5 manifests. good work! happy to see the snap people innovating, this is shaping up to be a killer stack
The ability to compose heterogeneous list functions. Read the other long comment I posted right below that one.
If I have no idea what type of data is in the list, what meaningful function could I apply to any element in that list?
Accession, rearrangement, views (on the whole list). Obviously you can't do folds or maps.
Good stuff. Is there anything in haskell land to do multi touch or gesture recognition?
My stab at a correction - remove mentioning the type of the container: Index a b where Here :: Index (l, b, r) b Right :: Index a b -&gt; Index (l, m, a) b Left :: Index a b -&gt; Index (a, m, r) b
Why not consolidate it: hetGet n = foldr (.) fst . replicate n snd hetWith n = foldr (.) id . replicate n second (change the rhs "n"s to "pred n"s for your base-1 numbering). 
Ever run the command 'make install'? I think that cabal is supposed to be the same. 
There seems to be a Haskell wrapper for OpenCV in hackage.
Good catch. Thanks.
I think you mean: foldr (.) fst (replicate n snd) foldr (.) id (replicate n second) ... however those will won't type-check because they requires a type of infinitely nested tuples.
At the type-level, maybe **is** the value. More specifically, at the type-level maybe is a function that takes a type and returns a new type. In the same sense that functions like "id" and "head" are also values at the value level, maybe is both a function and value at the type level. Thus the appropriate analogy for what you wrote would be: Just :: a -&gt; Maybe a -- Function at value level Maybe :: * -&gt; * -- Function at type level Edit: So, for example, since Maybe is a value at the type-level it can be passed to other functions at the type-level, like monad transformers.
Is there any write-up on how the lenses are used? I'm always interested in seeing how two very similar concepts are implemented.
Yesod includes database pool by default. 
It's in the docs.
I have run 'make install' and I have also run 'make uninstall'. Make can be programmed to have whatever target you want. 
Yes, that's the point. Note that you are dogmatically affirming that the concept 'type (at the value level)' is not the same concept as 'value (at the type level)', where pumpkin presupposed their identity. They can indeed be forcibly identified, as I said, so that 'type = kinded thing', or 'type = value at the type level' . Then we will note that some types, interestingly enough, are, as you might say, types *of* things; in particular, types of kinds `*` and `#` have this curious property -- this use of the word "type" is natural where a kind system is up and running and you are communicating with fellow initiates. The consequence is that you only really know what a type is (in this sense) after you know what a kind is. But the original question (see above) was "what is a kind?"; if "type" now means: *kinded thing* then this is basically the same as the question "what is a type?", i.e. "what is a thing-that-has-a-kind"?
[response scrapped for superior nonsense above]
Good news :) If someone is curious, let's take a look to this video to watch the power of the Kinect in action :) http://www.youtube.com/watch?v=ho8KVOe_y08
&gt; Note that you are dogmatically affirming that the concept 'type (at the value level)' is not the same concept as 'value (at the type level)' No, on the contrary I said exactly that. A value at the type level is a type at the value level. Read my first sentence. Perhaps I was not clear because I was trying to also make the point that type functions are first-class values at the type level and value functions are first-class values at the value level, therefore a type-function is also a type-value and a value-function is also a normal value. Now, Maybe is a function at the type-level (and thus also a value at the type-level), but maybe is NOT a value at the value level. You cannot pass a Maybe as a value to a function at the value level. Thus, something like this is meaningless at the value level: f :: Maybe -&gt; Int ... simply because functions only accept values at the type-level of kind (*). So to answer your question of "What is a kind?", the answer would be "The type of a type".
Lots of discussion about this function [here](http://stackoverflow.com/questions/3911060/library-function-to-compose-a-function-with-itself-n-times) on Stack Overflow.
Because it's a less-powerful version of iterate? apply n f = foldr (.) id $ replicate n f apply' n f = (!! n) . iterate f
Try running it on a large n.
I upvoted you because I like your implementation better, but this answer is still unsatisfying. You can also write replicate as: replicate n = take n . repeat but we still have a replicate function.
I think you're thinking of logical programming, ala prolog. Procedural programming is about moving, modifying, and combining data in various ways. Functional programming is about moving, modifying, and combining functions (although many functional programming languages happily provide a fairly rich set of type/object declarations). While it is possible to program haskell to give you the convergence properties of this system, it will not give it to you "for free", like Prolog would. That being said, Haskell is also a lot faster than Prolog, and more flexible.
No, that example you've given isn't the kind of declarative programming Haskell is talking about. What you're describing is more of a [Prolog](http://en.wikipedia.org/wiki/Prolog) sort of problem (except I don't think Prolog could handle that sort of numerical logic). Anyway, Haskell's declarative style is more in terms of "this **is** that." For example, you could define the head of a list as the following: &gt; The head of a list constructed by *x* followed by *xs* **is** *x*. Which directly translates into Haskell: head (x:xs) = x That's also why Haskell likes recursion instead of loops. In English, you'd define factorial like the following: &gt; The factorial of 0 **is** 1. &gt; The factorial of any other number *n* **is** *n* times the factorial of *n* minus 1. Which isomorphically translates into Haskell: factorial 0 = 1 factorial n = n * factorial (n - 1) The declarative style makes it possible to translate many other concepts, like the convergent system you described. For example, you could make a monad: mySystem = do a &lt;- newConstant b &lt;- newConstant c &lt;- newConstant d &lt;- newConstant a `isGreaterThan` b c `isGreaterThan` b main = print (convergentProperties mySystem) If you're curious, I might be able to show you how that monad would be implemented. The reason why you'd need *isGreaterThan* instead of the &gt; operator is because &gt; is just a black box and Haskell doesn't know its properties (without searching the whole integer domain, that is).
Damnit I wish I had bought one when I had the chance! I see something interesting in the near future combining this, Repa, Gloss and OpenCV.
How does Haskell resolve factorial n = n * factorial (n - 1) ? &gt; &amp;gt; is just a black box and Haskell doesn't know its properties (without searching the whole integer domain, that is) Can you elaborate on that? &gt; system you described Well I was purposely leaving the system undefined. I'm thinking about simulating an economy in Haskell, or just a large dynamical system. &gt; I might be able to show you If we're doing a tutorial example, let's use something more classical -- like a Mandelbrot set. To my way of thinking, it "should" take an infinite amount of computational time to brutishly figure out whether a point belongs or not -- unless there is some theorem that gives you a shortcut.
I believe the process for evaluating factorial is just "left-hand reduction." For example, when you evaluate: factorial 3 It looks to the definition of *factorial* to decide what to do. It tries the first case, *factorial 0*, but that is different. But *factorial n* matches, where *n* is 3. So it substitutes 3 for *n* and keeps going: 3 * factorial (3 - 1) 3 * factorial 2 3 * (2 * factorial (2 - 1)) 3 * (2 * factorial 1) 3 * (2 * (1 * factorial (1 - 1))) 3 * (2 * (1 * factorial 0)) 3 * (2 * (1 * 1)) 3 * (2 * 1) 3 * 2 6 &gt; is just a black box and Haskell doesn't know its properties (without searching the whole integer domain, that is) What I meant was that Haskell doesn't know anything about &gt; except that it compares numbers. For example, if you asked Haskell, "Which integers are &gt; 5 and &lt; 10?" It would have to check every single integer to give you an answer. The black box is a [mathematical analogy](http://en.wikipedia.org/wiki/Black_box) for a function. Haskell doesn't know it can just start at 5 and count up to 10; as far as it's aware, the set of numbers might be {5, 6, 7, 8, 9, 10, 623908}. Which is why you couldn't make a dynamical system very easily on it. So, yes, because it's a black box you'd be forced to do a brute-force search on ℝ, which would be O(∞). You'd need other means to provide Haskell the extra information.
Why the Jumbo distinction? Why not have a simple getter/putter of Word64 that uses the encoding that they use? Does retaining the information about the way the frame length was encoded matter to anything?
GLUT does not reliably transmit key bindings to the application. When I switched to a different keyboard layout (Hebrew) it did not send any keystrokes to my application at all. Key combos such as "Shift+Tab" did not get sent either.
I call this function `church` :)
I have to admit that the possible issues with "bizarre" keyboard layouts never occurred to me :) I'm not even sure if I tried it with my native language's layout, but I think that should work. Do the alternatives handle these issues? (I would guess the answer probably depends on which API they use to interact with the OS...) btw, does this issue occur on all platforms? 
I was taking it as read that let foo :: Maybe; foo = undefined is a 'parse error' (see ghci) and thus that no one can affirm that Maybe is a 'type at the value level' -- to continue with this way of speaking. This, which you don't deny, contradicts the proposition that "A value at the type level is a type at the value level", which you now want to affirm. For Maybe is a 'value at the type level' as :k Maybe teaches us (*this* was your first sentence, which I did read). The converse (to continue with this jargon of levels) is true: "A type at the value level is a value at the type level". In particular it's a value of * or # or some other basic kind, at the type level. If "x is the type of a type" means "x is the type of something that can be supposed, with out a parser error, to be the type of something" then your answer to the question, "what is a kind?", like pumpkin's, is "\*", since that is the type (kind) of types. But this answer to the question 'what is a kind?' is of especially little use to someone who wants to understand * -&gt; * -- or rather, to comprehend a monad tutorial where this is a way of formulating what the principal conceptual difficulty is. 
Excellent point Peaker. The Small/Jumbo distinction is no longer necessary now that I use Attoparsec-Binary instead of shift/masking a ByteString as I thought I would. This will make my code much shorter. The code was also an attempt at putting a monad value inside an algebaric datatype... but unnecessary! Are there are any performance considerations for passing the Word64 in to AP.take? I wouldn't want the coercion to Integer to be a big penalty. I know I'm paying it right now, but maybe there's a better solution? I'll update the blog post to reflect these modifications.
I haven't encountered any issues with GLFW-b yet, but I've only started using it.
The difference between `apply` and `replicate`, however, is that `replicate` is more generally useful. I don't find a lot of circumstances where I want to apply a function n times. I'm not saying `apply` isn't useful, but it's not a common need, and Prelude *does* need to draw the line *somewhere*.
&gt; Haskell doesn't know it can just start at 5 and count up to 10; as far as it's aware, the set of numbers might be {5, 6, 7, 8, 9, 10, 623908}. What about the succ function? I want to use the word "filtration" here. &gt; The black box is a mathematical analogy for a function. OK, so **&gt;**: {anything} &amp;times; {anything} &amp;rarr; {0,1}. But what does that have to do with defining *isGreaterThan* which would also be a relation with the same domain and codomain?
Allow me to refine your definitions of "functional programming" and "procedural programming". In "procedural programming" you define "procedures". A procedure is a list of instructions to be executed in order. You achieve your desired result by executing procedures. In "functional programming" you define "functions". A function is a (referentially transparent) mapping from input to output. You achieve your desired result by applying functions to inputs, and inspecting their outputs. In "logical programming" you define "constraints". A constraint is either a premise ("assume A") or an implication ("given A, assume B", or in other words, "A implies B"). You achieve your desired result by observing whether all of the constraints can be harmonized, and if so, in which ways. You seem to be confusing "logical programming" with "functional programming", although if you wish to *implement* your own logical programming language, then using functional programming to implement it isn't a bad idea.
Hello. I *think* I'm the target audience of your tutorial since: - I just started learning Haskell (using the Write yourself a Scheme in 48 hours) - I know Scala (3 year usage) - I think I understand currying from Scala and even how to use it (the Ahhh moment occured a week ago when I saw currying in Haskell). - I already understand a bit how to use monads (from the "WYS48H"). I didn't finish it yet dough. Yet you lost me right at the beginning. The reason is the usage of the **typeclass** word in: &gt; Monad is a **typeclass**. &gt; Functor is a **typeclass**. &gt; Pointed is a **typeclass**. &gt; Applicative is a **typeclass**, whose types are both Pointed and Functors. Only later do I found out that maybe I should read typeclassopedia to understand typeclasses. The point is that, either you leave the word "typeclass" out or you define it properly. The way it is, it is hard for your target audience since a new concept used in your Monad tutorial isn't explained anywhere.
Oleg can.
The most common example I come across is simulation where you have some iteration function you want to apply n steps and see what happens and you aren't interested in the previous iterations. If it's a pure simulation function, you use the 'apply' I defined. For monadic simulations, you could use the generalized version of apply: import Control.Category import Prelude hiding ((.), id) apply n f = foldr (.) id (replicate n f) Then you could also use it to compose multiple monadic steps which are functions in the Kleisli category. This doesn't have to be in the Prelude, but I think it at least belongs in some common library like Data.List or Control.Category.
Ok, now I see where you are coming from. You mean to say that Maybe is distinguished from a type like Int in that only Int can be used to type a value, therefore we must distinguish functions on the type level (i.e. Type constructors) from values on the type level, since they are not interchangeable **for the purpose of typing values**. I get you now.
I found what might be a good explanation [here](http://haskell.org/haskellwiki/Hitchhikers_guide_to_Haskell#Chapter_4:_REALLY_packing_the_knapsack_this_time). &gt; What is a typeclass? A typeclass is a Haskell way of dealing with the following situation: suppose that you are writing a library of useful functions and you don't know in advance how exactly they will be used, so you want to make them generic. Now, on one hand you don't want to restrict your users to certain type (e.g. String). On the other hand, you want to enforce the convention that arguments for your function must satisfy a certain set of constraints. That is where typeclass comes in handy. &gt;Think of typeclass as a contract (or "interface", in Java terms) that your type must fulfill in order to be admitted as an argument to certain functions. As I said, I'm trying to understand the concept *as we speak*. So take my suggestion with LOTS of *grains of salt*.
I wonder if "safely" and "unsafely" would be better keywords. Then it's clear that you're declaring how to import the function rather than describing a property of the function.
How do we determine if something should be in Prelude or not?
It's because isGreaterThan would NOT have the same result type. The type of isGreaterThan as he gave it would be: (Monad m) =&gt; a -&gt; a -&gt; m () ... where you could think of m () as being some action that registers the inequality somewhere else to be consulted later when you call the solver.
Just out of curiosity, how would you implement newConstant or reference creation functions in general? This is something that interests me. How would you implement uniqueness, fast reference creation, fast reference lookup, and (optionally) fast reference destruction?
Out of curiosity, I benchmarked various implementations (applied to a=Int, n=100000, f=succ. I also used Int for the first argument.): http://hpaste.org/53349 Numbers in brackets are with -fllvm. * Your definition: 825µs [799µs] * Via iterate: 16.2ms (!) [16ms] * Explicit tail recursion: 215µs [78µs] * Explicit tail recursion, strict accumulator: 310µs [78µs] * Explicit tail recursion, 4-fold manual loop unrolling: 145µs [58µs] * Explicit tail recursion, 4-fold manual loop unrolling, strict accumulator: 126µs [59µs] * Using the ST monad: 9.6ms [same] I used GHC 7.2.1 with -O2.
I'm surprised that explicit tail recursion was so much faster than iterate since it's so obvious how you would fuse those two recursive functions. That's one thing I dislike about GHC. It's optimization process is so opague and unpredictable.
everything. read the HList paper.
Well, a correction then: with type classes, it is possible to use catamorphisms if you never really look at the contained values. But is there a way to, for instance, calculate the sum of all the contained Num values in the list? Also, is there any way to produce an arbitrary HList at runtime? And lookups in the paper are implemented with type-level naturals (I can only find `HLookupByHNat` in the package as well)... so can you actually give some Integral and convert that to an index?
&gt; it is possible to use catamorphisms if you never really look at the contained values sure it is. look for HFoldr or HMap type classes to see how HOF work at the type-level (they are defunctionalized) &gt; But is there a way to, for instance, calculate the sum of all the contained Num values in the list? all Ints? sure. all Integers? yes. all Nums (where you provide at least one instance per type (e.g. Int, Integer, Double)) yes. generically? no. iirc, there's no backtracking when you match an instance like "Num a =&gt; Sum a". actually Sum might be possible, thanks to how defaulting works for Num class, but it's a hack (so no Concat class). &gt; is there any way to produce an arbitrary HList at runtime? why would you do that? HList is a generic tool to help write other type-level code. there's no point to use it at the value level. &gt; so can you actually give some Integral and convert that to an index? No. frominteger 2 and fromInteger 3 would have to be of different types.
&gt; why would you do that? Deserialization. &gt; HList is a generic tool to help write other type-level code. So you can't do everything with it. Are there any concrete examples that demonstrate its usefulness?
&gt; Deserialization. if you know the shape of the serialized data (e.g. list of 2 chars, one int and a list of Bools), there's no problem with it. &gt; So you can't do everything with it. there's a static/dynamic trade-off. with a [Int], you can read a variable number of ints from a string. but you cannot guarantee that there will be exactly 3 numbers in such a list. you cannot protect against head [] errorrs. &gt; Are there any concrete examples that demonstrate its usefulness? I wrote a statically typed, extensible and checked exception library using [ideas from] HList. http://www.patch-tag.com/r/Paczesiowa/pure-exception/home
I've certainly never thought about dynamically replacing the callback function like that while using GLUT in C, definitly an approach that makes sense in a functional setting. I'll be using this trick in the future I think, thanks!
I like it. =D {-# LANGUAGE TypeFamilies #-} class Church v where type EncodingOf v a church :: v -&gt; EncodingOf v a instance Church Int where type EncodingOf Int a = (a -&gt; a) -&gt; (a -&gt; a) church n f = (!!n) . iterate f instance Church Bool where type EncodingOf Bool a = a -&gt; a -&gt; a church b t f = if b then t else f instance Church (v1, v2) where type EncodingOf (v1, v2) a = (v1 -&gt; v2 -&gt; a) -&gt; a church (v1, v2) f = f v1 v2 instance Church (Either v1 v2) where type EncodingOf (Either v1 v2) a = (v1 -&gt; a) -&gt; (v2 -&gt; a) -&gt; a church v left right = either left right v instance Church [v] where type EncodingOf [v] a = a -&gt; (v -&gt; a -&gt; a) -&gt; a church vs nil cons = foldr cons nil vs
People who liked this should also check out the following paper, which uses the continuation monad to create a lazy data structure describing your program's control flow. You can then feed this data structure to a scheduler that you program yourself (and it's trivial to write). It's actually more generalizable than the paper suggests. [A Language-based Approach to Unifying Events and Threads](http://www.cis.upenn.edu/~stevez/papers/LZ06b.pdf)
So are we going to see built-in 'constructors' for type-level integers? Please?
oh this is a video of the actual talk, together with the slides. fantastic!
I'm a Java programmer and what is this.
I am curious, you encountered some resistance against using attoparsec on Haskell IRC. I have also noticed this sentiment throughout the community. However, I have been successfully using it for image parsing. Out of curiosity do you know why the anti-attoparsec sentiment exists? What are the better alternatives? I know of Data.Binary but that doesn't seem as powerful.
So I've been comfortable with higher order functions and recursion for a long time now, so I can't really remember what it is, if anything, that seems so confusing about them at first blush. Does anyone have any insight as to _why_ iteration seems easier at first? Does it really or is this an epiphenomenon due to the fact that most people learn languages where iteration is preferred first? What about higher ordered functions? They seem straightforward to me too. I guess there is the potential to confuse a function for the value it calculates?
This is the Haskell reddit. Welcome! [edit] More seriously, it is a way of turning an event loop "inside-out" by using continuations instead of a traditional callback method, which can (e.g.) avoid the complications of maintaining an explicit state machine in your event processor.
Let me take the definition of framework vs. library as "A library is something you call, a framework is something that calls you". Have you ever been using a framework but encountered a situation where you wished you could manage execution flow of the callback the framework is using in a more sophisticated manner? Ever wanted to call something like `framework_do_your_thing_here()`, so that you have the effect of all your call state being preserved on the next call? This lets you do that. So that's where the "reinversion of control" statement comes from; you break out of "you are a function being called and you can't keep your own call state between calls" to be something that can keep call state around. This can simplify a lot of things. By-hand reimplementations generally take the form of something like (in random pseudo code) states = enum(BEGIN, CONNECTING, CONNECTED, DISCONNECTING, etc.) my_state = BEGIN; callback = fun () { if (my_state == BEGIN) { ... my_state = CONNECTING; } elseif (my_state == CONNECTING) { ... } elseif (my_state == CONNECTED) { ... } } where perhaps callback is handling all socket events for some socket. With continuations, you could write the internals of the callback in much more conventional looking code, and you can also get things like exception handlers that can be oblivious of the fact that the execution is actually jumping in and out of their context freely.
Here's one simple way. -- a real implementation would use a newtype and do hiding via the module system type System = State ([Integer], [(Integer, Integer)]) runSystem :: System () -&gt; [(Integer, Integer)] newConstant :: System Integer isGreaterThan :: Integer -&gt; Integer -&gt; System () runSystem = snd . execState ([0..], empty) newConstant = gets (head . fst) &lt;* modify (first tail) a `isGreaterThan` b = modify (second ((a,b):))
Type classes are a fairly basic Haskell feature which, in a broader Haskell tutorial, would typically be covered before monads. For example in the [Gentle Intro](http://www.haskell.org/tutorial/index.html), type classes are in section 5 and monads are in section 9. Or, in [RWH](http://book.realworldhaskell.org/read/index.html), Using Typeclasses is chapter 6, and Monads is chapter 14. One of the perils of learning by blog post is that there's no such overall organization, so you have to get that guidance from somewhere else.
The following is for GHC only. If you expect the `Word64` that's sent in most messages to fit in a machine word, then the coercion to `Integer` should be very cheap for those messages, since it will use the `S#` constructor: data Integer = S# Int# | J# ByteArray# Then again, if it doesn't fit in a machine word, you've probably got a damn big message coming down your pipe, and fretting about the time it takes to allocate and fill a `ByteArray` is a mistake.
My own experience seems to mirror this: The novice programmer is happy when her program actually runs at all, having no idea whether her program runs correctly. She is unconcerned about making it work correctly until the program runs afoul of a user, and she gets into trouble. The mature programmer is happy when her program runs correctly, and she looks for resources that help make that possible. She is planning ahead many steps and abstracting away anything that has a nasty habit of making trouble. Programming in Haskell may be a choice she makes toward that end. In my experience Haskell often requires more planning and a sense of a bigger picture. This is mentally expensive at times, especially for the beginner. However the benefit is (hopefully) a program that runs correctly, which I value more than a program that simply runs. Haskell is unlikely to ever be ranked among the 'easy to learn' languages, but then it doesn't strive to be that. Haskell can be a gateway to programming maturity, and growing up isn't always easy.
I just experienced an a-ha with continuations. Thanks. 
Very interesting indeed! And the speaker is one of the authors of "Real World Haskell"! Impressive!
I'd be curious too, since I wrote attoparsec.
These kinds of articles are dead useful.until the arguments become natural efforts by FPers to make FP accessible will fail.
Many programmers think in terms of what they want the computer to do. This is by definition imperative. Switching to what you want to calculate instead is difficult. If this is because of what they learned first, or because it is inherently more simple, I don't know. With higher order functions, I think there are many things that make it hard. Many people have come to see functions as something *special*, so when you can just pass them around, it takes some time to get used to it. In OO, they are also attached to objects, making them more special (since there is one special argument). Things also get abstract pretty quickly with higher order functions, since you are almost never able to say something concrete about the function passed in.
Fixed. Thanks for spotting that!
&gt; You shouldn't necessarily dismiss it [mysql] out of hand and go "oh boy I'm gonna use cassandra in my little 2-person startup. Stick with something that works until you have proven the other parts of your business model. Is he being ironic? He is doing the opposite of that, using Haskell and all.
The GHC team had been closing bugs for about 20 years by the time Cassandra appeared on the scene. It's even older than MySQL!
One theory I've heard (that makes some sense to me) for why people traditionally have trouble with recursion, is that they are taught to understand code by executing it in their head. Iteration is easy to understand this way, because it only involves keeping track of some finite amount of current state. If you want to execute recursion, then in general you need a stack. And keeping track of the whole stack is harder than what is involved in iteration. The corresponding solution is to not teach people to understand code this way. Recursion is easier to understand by treating recursive calls as subproblems that are automatically solved in each case. So you no longer think about the overall execution of the function, but how to break the problem into subproblems. And then recursion is quite natural. Of course, effects may complicate the picture. If you don't have some sort of invariant on the state, you could well end up having to execute the sub-calls to figure out what exactly they did.But that's arguably poor design.
I'm actually not learning from blog post, this was a reddit related deviation. :) I started Haskell with [**Write Yourself a Scheme in 48 Hours**](http://jonathan.tang.name/files/scheme_in_48/tutorial/overview.html) to get an hands on feel of what programming in Haskell really is. Then I was intending to go with the **Real World Haskell** book that a friend of mine pointed me to. Do you think the **Gentle Intro** should go somewhere in my plan? Thanks.
I guess I overestimated the obviousness of it, but in the section "Monad is a typeclass" I indirectly described what a typeclass is: &gt; Certain types [pertain to a given typeclass]. We say these types are instances of [that particular] typeclass. Values are not [instances of a typeclass], nor are functions. I've made some modifications to quickly introduce what a typeclass is, and linked a section of LYAH. I really appreciate your comment; I'm not entirely sure whom my target audience is, but I'm pretty sure you're in it. :)
Real World Haskell is much more recent and has a more pragmatic emphasis, so is probably a good next step. The pace can be a little slow depending on where you're at in the learning curve, but you can always skim &amp; skip. Gentle Intro is pretty old (circa 2000), and has often been accused of not being at all gentle. Because of its age, it covers basic Haskell 98, with no references to newer libraries and idioms that have since become standard. As a tutorial for that core language, I think one virtue it has is that it's fairly concise, but that also makes it quite dense, hence the ungentle reputation. Whether you find it useful may depend on your learning style. Edit: [Learn You A Haskell](http://learnyouahaskell.com/) may also be worth a look - it's lighter than RWH, and could act as a good overview/intro.
I second that - thanks very much, Hari, for writing it. I hope you stick with it and find learning Haskell as rewarding as I did. The "functional way" comes to seem very simple and natural in time. It works like this IMO: FP allows you to take the organizational and logical parts of your program and make them declarative ("what the program is, not what it does"). This makes your code cleaner and simpler, at the cost of being more abstracted. The cost comes in the form of time and effort for a person to acquire understanding of the abstraction. "What the program is" is an artefact of the human mind and has nothing to do with the machine. By abstracting we're moving away from "reality", in exchange for a powerful organizing principle to help you avoid drowing in a sea of implementation detail. That is, away from the domain of implementation and towards the domain of human thought, but then a new programmer comes along and we've departed from the common frame of reference - the implementation. An imperative programmer in this situation has to "reverse engineer" the implementation and convert it back into thought. By contrast, a functional programmer has to learn the thought process of the previous programmer. In theory this is better, for these reasons: * The abstraction is already closer to human thought, so it should be easier to digest. * As abstractions become better known in the programming community, the process of picking up another person's code gets easier. Functional abstractions work well in this regard, because they tend to be very lightweight and general. Learning what an "Applicative" is doesn't just give you a way to talk to other programmers. It will save you actual work. * The program should be simpler overall than its imperative equivalent. If this is not so, then FP has failed. The really powerful bit is that functional-style abstraction has the property of "composability", which essentially means that when you fit the pieces together, you don't get any surprises. However, you are always left with an imperative bit. Ultimately your program must actually *do* something, and so those imperative programming skills remain important. The idea is that the remaining imperative part of the program should be much simpler than it would have been in an imperative approach to programming. So there's my zealotry. I still think it's important, though. FP is only any use if it's actually better. It's no good if it's just different.
I like to learn by reading other people's code and, for me, all the Haskell extensions and "crazy" syntax they enable has been a roadblock. I read somewhere that when you're finally understanding Haskell syntax, something new comes up. I feel like that and it's been frustrating sometimes. But I'm not giving up, on the contrary.. it's been very exciting to learn Haskell and if I've my way.. I'll probably be writing most of my code (which isn't that much) using Haskell in the near future.
Haskell and cassandra's benefits are in a completely different areas. Cassandra is about scaling to handle millions of users and hundreds of terrabytes of data. Whereas haskell is about empowering a 2-person-startup to be able to do the work of a 20-person team. 
On one of the slides, it is mentioned that you can handle 90000+ http requests/sec thanks to the GHC I/O manager... but what kind of hardware does this require?
I'm currently using Attoparsec (along with attoparsec-binary) to write an implementation of CurveCP. I mostly chose attoparsec out of familiarity off hand. I have not seen many anti-attoparsec sentiments, but I am not looking. The only reasonable contender I know of to attoparsec for this sort of work is the 'cereal' library which is also strict and has a continuation-based interface. That said, I am thinking of switching to cereal mostly because I am already using the serialization interface, and I may not actually need most of the power offered by parser combinators for parsing this network protocol. Doing so will keep the code just as simple and eliminate a dependency from my point of view. With `attoparsec-binary` I think attoparsec itself is pretty good for this sort of work. Perhaps one thing I can recommend if you intend on it to be used for this sort of stuff is to merge those package (a few simple combinators for parsing different `Word` sizes.) Cereal already has such things, and it comes up often enough.
While I don't entirely disagree, my own experience as a Teacher's Assistant for an introductory programming course in C++ leads me to conclude that Haskell definitely has at least *some* novice-friendly advantages. Consider the C code: #include &lt;stdio.h&gt; int main() { printf("%f\n", 3); } What does this print? I can tell you that newbies are absolutely baffled when this prints out `0.000000`. They are even more baffled when they forget a parameter to printf (e.g. `printf("%d")`) and their code segfaults. Newbies are typically horrible at isolating the error in their code, so to try and figure out where they went wrong, and they scratch their heads for hours tracing the execution of their program, trying to find a place where the printed variable got set to 0. In comparison, Haskell's printf is a breath of fresh air. ghci&gt; import Text.Printf ghci&gt; printf "%f\n" 3 *** Exception: Printf.printf: bad argument ghci&gt; printf "%f\n" *** Exception: Printf.printf: argument list ended prematurely ghci&gt; printf "%f\n" 3.0 3.0 Also, null pointer exceptions are not novice-friendly for the same reason: it is very difficult for a novice to track down the place where the reference got set to null, and/or failed to get set to something other than null. Creating new data types, and even generics, are a *lot* more noob-friendly in Haskell. Try doing the following in imperative languages, and (as a noob) you'll get all kinds of headaches, either dealing with null pointers, or loss of type safety. data Maybe a = Just a | Nothing Pattern matching is fairly easy for a novice to pick up; it's a shame we don't have this feature in more languages. Along these same lines, see the [Quick and dirty reinversion of control](http://www.reddit.com/r/haskell/comments/luyf4/quick_and_dirty_reinversion_of_control/) /r/haskell posting.
While I certainly agree that teaching the concept of breaking a problem into subproblems, I wouldn't go so far as to say we *shouldn't* teach people to mentally execute imperative code. If you are writing imperative code, then a debugger that steps through your code one line at a time is absolutely priceless. The only problem with this approach is when your code doesn't work that way (e.g. Haskell code, which is both lazy and declarative).
Sorry, my brain's English compiler doesn't support -XIncoherentSentences
I guess we shouldn't be too surprised that a coauthor of RWH is running a startup on Haskell. :)
For me, the hard part of haskell is its information density: one line of haskell can make what I do in ten lines of python. That requires a lot more of attention reading haskell code. And if you are a newbie, and you can't parse it correctly (what's the difference between **$** and **.** again?) the consequences are bigger. Since haskell favors function composition, a seasoned programmer will combine a pair of curried functions, will lift the result, map it and filter inline whenever he needs it. A python programer will define a function to curry and compose both functions, and another one for the lift. So, haskell expresivity and haskell power is newbie-unfriendly.
Tell me about it! That's was exactly the way I learned recursion. The worst part is not the analysis, but the design.I basically started with a draft, then executed it in paper. After failing, go back, make random changes and started again. I finally learned recursion the right way reading Paul Graham lisp's book. It's a lot easier when you make the association between recursive data structures and its associated recursive functions.
Thanks for answering; I was getting to it but I was a bit busy. What you have is about what I'd have said.
Noob warning- don't expect a full answer. From the lurking I have done with haskell, the closest solution I saw for your problem was to have 2 implementations, a plain simple\slow but Correct, and a fast one that you intend to use(and test). Then you use the slower one to confirm the other. But you'll need two feasible implementations, which for complex algorithms may not be the case.
The line is drawn, yes, but it's a lazy line.
&gt; Does anyone have any insight as to why iteration seems easier at first? IMHO, it's because it's easier to reason about less powerful languages. While the complete language is equally powerful in both cases, the language fragment which a student understands is less powerful until they get to the end. Imperative languages usually start by teaching sequences, then conditionals, then for or foreach loops, then finally while loops. The corresponding concepts from functional programming are, respectively, composition, pattern matching, structural recursion, then general recursion. A particular weakness of functional languages in this regard is that structural recursion is syntactically a special case of general recursion, but general recursion induces a more powerful (and thus harder to understand) language. Because of this, there's no way to teach the easier special case first like imperative languages can.
For him, Haskell's a proven tool. He hold the knowledge to use it, and what to expect when it fails. It'll work. Now on something he doesn't know (cassandra), he has no knowledge he won't be able to see it's shortcomings, and how to manage that.
Here's an example that's a little more complex, using monadic QC, presented at the Haskell Symposium a couple of years ago (although it's not quite like the problem you describe). It may help give you some ideas: https://www.cs.indiana.edu/~lepike/pub_pages/qc-biphase.html
There's more to them than just this one use case, and you can tie yourself up in knots a bit if you're not careful, but this is a good first encounter, I think. Continuations are like closures, only even moreso, in that once you learn to see where you'd use one, you _really_ want them in your language, and attempts to manually hack them in to a language that doesn't support them are at best unsatisfactory and at worst almost impossible, to say nothing of the mangled code that results.
Addition, haskell specific: Nasty perlish use of hieroglyphics to represent complicated abstractions instead of words.
From skimming down chapter 8 of **Learn You a Haskell**, the book seems so easy to read, It almost takes away the fun of learning something the hard way. :D But since I want to be productive ASAP, I think I will use it to fill some holes after finishing **Write Yourself a Scheme in 48 Hours** Thanks a lot for all the great suggestions.
Sure. Executing step-wise actually works pretty well for iteration and similar imperative code. Even in an imperative language, I think the subproblem approach is important for understanding recursion, though. I think that approach tends to get mentioned in conjunction with recursion, but I don't know that it's always stressed that it's frequently the more appropriate way to understand recursive algorithms, and that mental execution tends to not work so well. For instance, towers of hanoi is often described imperatively, and using mutation, but mentally executing the code is (if you ask me) unlikely to give anywhere near the understanding that the inductive approach does.
Thanks a lot. This is really helpful.
There is one sentiment expressed in the comments that resonated a lot with me, namely that Haskell seems more enjoyable for experienced programmers. I think it's because it's hard to understand how useful Haskell is until you've had to grapple with all the defects of mainstream programming languages.
If you performed some optimization on your function then you can do the sort of testing you hint at: write tests that compare the answer your function gives with the right answer, where the right answer is computed by a slower but obviously correct function. For instance, to test a merge sort, you can compare it to, say, an selection sort (which presumably is easier to write correctly). But the tests you write don't have to check that the answer is completely correct, instead you can express properties your function is expected to satisfy. For instance, for a sorting function you can test that (1) the output is sorted and (2) of the same length as the input (without checking that the elements of the output all appeared in the input the same number of times). Or for a matrix multiplication, you could write tests that say that it is associative, that the identity matrix times M is M, etc. For a function that computes an average you can write tests that say that the average is between the min and the max, etc. If your function has a specification you should write tests that verify the function satisfies the specification, of course. (And if you don't have a specification, i.e., you can't explain exactly what the function is supposed to compute in every case, you might consider figuring that out and then writing tests for that.)
FYI, the github link doesn't seem to be working; I think it's meant to point [here](https://github.com/Julianporter/Distributed-Haskell).
I confirm that testing by comparison with a naive algorithm is a terrific idea.
Try the following: * Validating the results for simpler cases of your datastructure where you can compute the result analytically (e.g. if they're trees - try degenerate linear trees, or complete binary trees, etc.) * Formulating properties in terms of relations between several datastructures (e.g. "answer for tree A-x-B should be between answers for A and B")
&gt; What does this print? I can tell you that newbies are absolutely baffled when this prints out `0.000000`. Hell, I've been programming for almost twenty years now and that baffled me for a solid minute or two, _even knowing_ it had to be a problem with casting and that `3` is an int literal, not a float literal.
Huh? None of these examples have anything to do with imperative vs. functional programming, just C/C++ vs. pretty much any more modern language...
* Null pointers * Automagical type coercions you might not like * Mutable state These novice-killers are widespread amongst practically all modern langagues. I realize it is a bit of a cheap shot to include "mutable state", but it's the truth. Referentially transparent code is typically easier for a novice to reason about than code that uses mutable state.
&gt; "how to test properties or expectations that aren't algebraically simple" Do you have any particular "complex" properties or expectations in mind? Just because a property is simple doesn't mean it is trivial. prop_idempotent :: (a -&gt; a) -&gt; a -&gt; Bool prop_idempotent f x = f x == f (f x) ghci&gt; quickCheck (prop_idempotent (abs :: Int -&gt; Int)) ++++ OK, Passed 100 tests. ghci&gt; quickCheck (prop_idempotent (negate :: Int -&gt; Int)) *** Failed! Falsifiable (after 2 tests): 1 Even though I've given simple examples (abs and negate), idempotency is an important concept for many `a -&gt; a` functions. Suppose we want to check that performing two garbage collections in a row will result in the same amount of allocated space afterwards. garbageCollect :: ProgramState -&gt; ProgramState prop_collect origState = freeSpace (garbageCollect origState) == freeSpace (garbagecollect (garbageCollect origState)) (You would need to make ProgramState an instance of Arbitrary.)
FYI, clang would have caught that error at compile time. Also, Haskell's type error messages can be quite cryptic even to experienced Haskell programmers.
&gt; clang would have caught that error at compile time Really? Clang inspects the format string and checks that the rest of the arguments have the correct types at compile time? &gt; Haskell's type error messages can be quite cryptic even to experienced Haskell programmers. [citation needed]. I'm not saying I haven't run into cryptic error messages, but 99% of the time it's simply Expected Type: Foo Inferred Type: Bar in expression: baz quux There is also [Helium](http://en.wikipedia.org/wiki/Helium_\(Haskell\)) for beginners, though it seems a little out of date. On a related note, [Racket's beginner languages](http://docs.racket-lang.org/drracket/htdp-langs.html) are also designed specifically to be novice-friendly.
&gt;What does this print? I can tell you that newbies are absolutely baffled when this prints out 0.000000. I have to ask - why does it do this? My only guess is that it's reading the memory bit by bit. Since floats/doubles are stored sort of like scientific notation, the first few bits would be all 0s, meaning "0.00000", with some more 0s and then a 11 for the 3, meaning "2^x". Net result is "0.0000 * 2^x", which comes out as 0. Edit: Tried it out, and I get a number *near* 0, so I'm off on my guess. :(
&gt;&gt; clang would have caught that error at compile time &gt; Really? Clang inspects the format string and checks that the rest of the arguments have the correct types at compile time? Yes. Clang is to gnucc as Helium is to Haskell, except Clang supports the full C++ specification while Helium does not support the full Haskell specification.
&gt; Really? Clang inspects the format string and checks that the rest of the arguments have the correct types at compile time? gcc does that too, for all functions that have the [format arg](http://ohse.de/uwe/articles/gcc-attributes.html#func-format_arg) attribute. 
That code passes an int type to code that expects a float type argument. This can cause anything from reading the wrong representation to using the wrong argument-passing convention. For example, integer arguments may be passed in one set of registers, whereas float arguments may be passed in another set of registers.
&gt; For me, the hard part of haskell is its information density: one line of haskell can make what I do in ten lines of python. Can you give an example? Python can generally express the same notions as Haskell. Some uses of type-classes require a bit more Python code, but I don't think there's ever a factor of 10. In my experience, Python is about as concise as Haskell (sometimes slightly more, sometimes less). That is a huge testament to Haskell's strength, though, because Python gives up so much safety, performance and other trade-offs to be as concise as that.
The code you want to read is not some open source code you find in the wild, but the following: * the [Prelude](http://www.haskell.org/onlinereport/standard-prelude.html) * [Functional Pearls](http://www.haskell.org/haskellwiki/Research_papers/Functional_pearls)
It probably makes more sense to compare to the fastest available alternatives like nginx. We are planning on doing that benchmark. Warp shouldn't be that far off.
&gt; haskell is about empowering a 2-person-startup to be able to do the work of a 20-person team ^^ I so much love that sentence. Really good presentation.
Well, the figure of ten lines of python for one of haskell doesn't apply in every circumstance. But if you use **(+ 5)** in a haskell, you'll probably use **def plus_five(x)** in python instead of **(lambda x: x+5)** to make your code more readable. That's the python way. Add two more haskell *sections* in one expression and you'll end with nine lines of python. And in python, you have to write the arguments of the functions. In haskell you write **(f . g)** when in python you'll have to do **f(g(x))**. So, if you have a lot of application, composition and currying in a haskell expression, you'll probably end writing the equivalent code in several lines of python, just because python is more verbose and you want to clear things a bit. Take for example [Write Yourself a Scheme in 48 Hours](http://jonathan.tang.name/files/scheme_in_48/tutorial/overview.html). In haskell you have algebraic data types. That allows succinct and elegant code: data LispVal = Atom String | List [LispVal] | DottedList [LispVal] LispVal | Number Integer | String String | Bool Bool In Python, I should have to create six or seven classes. Don't get me wrong. I find OOP beautiful, and it have its own set of advantages (In OOP I could add a new type of LispVal without modifying the preexistent code). Or look at this code: parseNumber :: Parser LispVal parseNumber = liftM (Number . read) $ many1 digit I can't read that code without gasp ;) It gives you a parser that returns a LispVal's Number based only in previous functions. You don't have to write new code, just combine existent ones. If I had to program parseNumber in python, I won't do it composing basic functions like digit or many1, but make my own version from scratch. I love python and its clear syntax, and I'm more fluent in it than in Haskell. But even with my poor haskell knowledge, a lot of times I find myself thinking "I wish I could do this haskell thing in python". And of course, these "faults" are not exclusive of python. You can say the same thing about perl, C or java. 
I actually did something along these lines with *digest-pure* - test by comparing the output for arbitrary inputs with what the (zlib-based) *digest* package would give. Ironically, I found a bug in *digest*, which used an incorrect seed for adler32.
It's either that or use the words and watch people get all anxious over monoids, applicative functors etc. There's no winning.
Indeed. Then the git:// at the front was a bit of a give-away! Anyway, fixed now.
Very nice! Thanks!
I agree idiomatic Haskell is much more concise than idiomatic Python, especially with Parsec, Applicative, etc. However, even that is not a factor of 10. Also, Python *can* represent most of these idioms, it would just be very "unpythonic". Also, Python makes all sorts of metaprogramming much cheaper, which can sometimes make it more concise than Haskell. That said, I also find myself wishing for Haskell features in Python all the time.
Koen Claessen and John Hughes talk about this, among other things, in their very nice paper [Testing Monadic Code with QuickCheck](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=AC50131A201DBC7D53484553CB4C85B2?doi=10.1.1.19.9275&amp;rep=rep1&amp;type=pdf). I think this paper does a great job opening one's eyes to other ways of thinking about how to use QuickCheck: * sometimes it is possible to describe the behavior of a component *completely* using algebraic properties--you can think of this as, essentially, giving a declarative model implementation, which QuickCheck then tests your given implementation against * sometimes it is easier to give a simple model implementation and check that yours works identically (as MdxBhmt suggests) Anyway, a very nice paper that the OP probably would enjoy. 
This is a good opportunity to mention Neil Mitchell's http://neilmitchell.blogspot.com/2010/01/haskell-io-without-monads.html . It has IMHO a great value to a newbie, so I hope someone will find it useful. I personally have found it too late during my Haskell path :-( . &lt;small rant about monads&gt;Do not try to understand monads. Try to understand a Maybe monad. Then try to understand a List monad. Then try to understand a State monad. And then ... you might start getting a general understanding of what a monad "IS".&lt;/small rant about monads&gt;
Yesod and Warp are awesome :) We are eager to see those benchmarks :)
Yes yes, and double yes!
&gt; Can you give an example? Maybe something like this? It was discussed [here](http://www.reddit.com/r/programming/comments/225f0/beautiful_haskell_implementation_of_maths_power) before. import Control.Monad powerset :: [a] -&gt; [[a]] powerset = filterM (const [True, False]) I don't how many lines of Python it would take to write it, though. 
Well, you can implement filterM in Python, you'd just need to manually pass the dictionary/etc, which will be less concise, but not 10 times less concise. Even implementing the naive recursion in Python: def powerset(xs): if not xs: yield [] return for o in [[xs[0]], []]: for r in powerset(xs[1:]): yield o + r is about 8 lines (which you don't have to implement). Also filterM is a pretty atypically concise example.
Bryan is a very good speaker!
Well, a single word name wouldn't tell you what it is anyways, since it is a complicated abstraction. I mean, would replacing &gt;&gt;= with `bind` really help understand some monadic code if you didn't know what a monad was? No. And you couldn't really get more specific, like calling it cartesian_product since that's only true for the list monad, but something sufficiently general, like take_it_out_of_the_box_and_do_something_that_puts_it_in_a_new_box would not be readable. 
Can it scale into the cloud?
This implementation almost certainly won't, because of the very simple persistence store. A server backed by a DB (which could then be multithreaded) probably will.
Comments on reddit are threaded, so click reply on someone's comment or enter yours in the box at the top to make a top level post :)
I think I may be in the minority, but I always thought the focus on understanding the derivations of the monads and the monad laws was misplaced. Instead of understanding the monad, just understand the **semantics** of a monad. How the state monad works isn't nearly as important as just learning how get and put work. 
no! no! no! those functional pearls are the absolute wrong place to start. yes they are beautiful but most of them are purely academic and all are not helpful to a new haskeller. unfortunately, haskell induces people to try to be especially clever and play code-gold left and right. the result is that most "good" haskell code is absolutely beautiful to the experienced haskeller (think the oneliner for powerset) but useless to a neophyte. its better to just start some toy projects and ask for help in #haskell and stackoverflow. if you want to do a web app, I strongly suggest checking out [yesod](http://www.yesodweb.com) the scaffolded site gives you a great start.
http://hackage.haskell.org/package/Printf-TH (for catching printf errors at compile time)
 &gt; Those functional pearls are the absolute wrong place to start. yes they are beautiful but most of them are purely academic and all are not helpful to a new haskeller. No, I think they are precisely the right place to start, in particular the [Sudoku][1] pearl by Richard Bird. They teach you the way of thinking that gave birth to Haskell: programs from calculation. Sure, they don't teach you how to write to a file or other tasks that are commonly used as beginner tasks in imperative languages. But they give you a strong background in thinking purely functionally, which is precisely what you need to use Haskell effectively. [1]: http://www.cs.tufts.edu/~nr/comp150fp/archive/richard-bird/sudoku.pdf 
When I watched the ghc status report (same event) on youtube SPJ said something about safe haskell from this talk being one of the things people would look back on in 10 years as being extremely influential. Although I don't doubt SPJ since he's been involved in many things like this over similar time scales I don't actually see why safe haskell would be so influential. If I didn't mishear (I think at next event microphones ought to be attached to the speakers if possible) would somebody be able to clarify just why safe haskell is so valuable? When cdsmith was doing haskell for kids I think he mentioned using that for his server running code so I guess that's a possible application?
Thanks so much for your considered response. I am already finding the "thinking" part much easier after a while. I think the part about abstraction is interesting. Because we're so used to thinking like the computer does, we start to inherit (slightly) its limitations when trying to tell it what to do.
This year's GHC status report packed some serious awesome! Thanks for the videos, too bad the sound quality's so terrible.
Is there any chance the ICFP 2011 videos will come online any time soon?
Yep, room microphones are awful.
First of all, who cares? Second of all, who cares? I started typing details here, but who cares? You sound like you're offended. Stop that. (Also, this is a "primitive" function in Haskell; reduce is fold.)
And they ARE easy to implement. 
&gt;You can write reduce() in a few lines of Python. Not so in a functional language." That's a load of crap.
Guido doesn't get functional programming.
Dude who doesn't understand functional programming in saying things reflecting a lack of understanding of functional programming shocker.
I'm curious what he actually meant, since the straightforward interpretation is nonsense as far as I'm concerned. For the sake of demonstration, here's the implementation for a left fold, taken from the Haskell '98 prelude (I believe most people would agree Haskell is a functional language): foldl f z [] = z foldl f z (x:xs) = foldl f (f z x) xs
I agree that one should not be offended by Guido's remarks, and I didn't intend to sound offended, more like puzzled. I was curious to see if anyone had a guess as to why Guido thinks fold is unimplementable in a functional language since I honestly can't imagine why someone would think that. I don't understand what you mean by &gt; (Also, this is a "primitive" function in Haskell; reduce is fold.) I know that reduce is called foldr in Haskell, but I'm not quite sure what you mean by '"primitive"', I'm pretty sure foldr is not built-in to any Haskell compiler or interpreter, but rather has a short definition, in Haskell, in the standard library.
He's right, we can't write them in a few lines. We can write them in no lines. Sometimes fewer.
This is what I'm curious about too. What makes Guido think foldr shouldn't be implementable in a functional language? I'd like to know exactly what the misconception is, and how common it might be.
i think what he means is that reduce, aka fold, in FP, for example haskell, is a built in primitive. Primitives are implemented at a compiler level, not user level. So it is not implemented in "a few lines". The converse can also be said that objects can be implemented in a few lines in FP, but not so in OOP. His point is that whether a language is functional only depends on the type of primitives, so python is not a functional language.
&gt; i think what he means is that reduce, aka fold, in FP, for example haskell, is a built in primitive. Perhaps that's what he means but it's simply not a true statement, hence the confusion.
Maybe he's referring to the built-in tail call optimization?
Exactly. I've spoken to Guido van Rossum myself about functional programming and he really genuinely doesn't understand it. That's not to say he's dumb or not a nice guy or anything. It's just not his area.
that's pretty cool :&gt;
[MPTCs not needed in the end](http://www.haskell.org/pipermail/haskell-cafe/2011-November/096573.html). Turns out the story was that GHC's already quite-good rewrite rules for `realToFrac` weren't firing because of the (in my opinion, somewhat strange additional) definition cFloatConv = realToFrac and the subsequent use of `cFloatConv` everywhere instead of `realToFrac`.
Check out some of the [tests for Heist](https://github.com/snapframework/heist/blob/master/test/suite/Text/Templating/Heist/Tests.hs), specifically the tests for bind and apply. I can't say that the Heist tests as a whole are a pristine example of good test code, but we are doing some nontrivial stuff for bind and apply.
Source link: http://findfunaax.com/notes/file/77. Scroll down to "Why did you choose to support multiple paradigms?".
Shouldn't that be something that GHC should be able to figure out?
I'm not 100% sure I know what you mean by "something that GHC should be able to figure out". At a guess, what you're complaining about is that GHC doesn't always choose to inline bindings that are merely additional names (as this one is). If so, then I think I might agree with you, though I can't be sure I see exactly what that would mean for recursive bindings like foo = bar bar = foo
It would be nice if Cairo worked on Windows at all.
Warning: Oleg.
Hopefully yes. This is included in the work Iavor Diatchki has been doing; see http://hackage.haskell.org/trac/ghc/wiki/TypeNats .
Simply because having a flattened type/kind level seriously complicates the equational theory. So we decided to go with something simpler for the moment, but it doesn't rule out collapsing the levels in the future.
I intentionally left "real world examples" out. I've thought about writing companion essays elaborating on the uses of particular monads, and actually started writing one for Maybe. But I wanted to keep this one as straightforward as possible, and delving into details about specific monads (without dedicating a significant amount of prose) often leads the reader to believe that "monads are for state/failure/nondeterminism/IO/whatever", which may lead to erroneous preconceptions when the reader is presented with a brand new instance of Monad.
That is solved by choosing a "loop breaker" -- a binding which is not inlined. E.g. foo is replaced by bar, but bar is not replaced back by foo since it's a loop breaker.
And some other very bright minds that do have the ability to clearly communicate their ideas. This is, or was, a thorough analyses and a good read.
I also enjoyed the article on Mighttp 2. As a non-academic, I always enjoy that style of article where the author is incrementally improving or troubleshooting performance issues. It's like a mystery novel, I wanted to keep reading until I discovered who murdered nginx and warp!
It does. I'm using it on Windows all the time.
I got it "working" as well, but it crashes in interactive mode. Compiled programs are fine.
Hm, could be. I never used it in interactive mode.
Argh, two columns again, with no alternate version for e-readers. Why does progress have to come one death at a time?
[http://www.reddit.com/r/haskell/comments/lpw4a/giving_haskell_a_promotion_typed_typelevel/](http://www.reddit.com/r/haskell/comments/lpw4a/giving_haskell_a_promotion_typed_typelevel/)
Has anyone checked if this is ghc-version dependent? Someone pointed out a similar performance problem to me a while ago and it turned out that it was fast with ghc-6.12 and 7.2, but really really slow with 7.0.
I also like to think that it is taking something that is real problem domain — a web server — and doing it in Haskell. I know there is a version control system and window manager written in Haskell, and I have been known to like darcs before. But this is much closer to my 'real', and they did it in Haskell. I admit to limited real exposure to Haskell in the Large. I am waiting for a UI paradigm to come along that regular people can grasp without blowing a few neurons. FRP is really cool, and I think I have a game idea that could probably make use of it - once I get free time to experiment again...
While I generally prefer Snap, Persistent (the db pool you mention) is very nice to work with. It's my go-to solution for databases anymore.
Sent!
yes, the warning should be interpreted as "beware, your mind may be blown repeatedly" rather than "beware, hard to understand".
Thank you for working on this. I will send a log later today.
Sent. :)
Nice article, but the thing with the "difficult recursion" exhibits a poor education, IMHO. Never read "Algorithms and Data Structures"? Never worked with recursive data structures? Lists, Trees? Really? I have used recursion long before I started with functional programming. It is useful (and for some problems, even indispensable) in every programming language. 
I adjusted the script slightly to automatically remove username and password from the .cabal/config file: https://gist.github.com/4a66a713df259b6e8aed
Great news! Thanks to all involved.
&gt; I have the feeling, though, that at least the first constraint should be possible to be fulfilled during construction, but I don't really see how. Honestly, don't sweat it. First attain basic fluency in Haskell before trying to get fancy. &gt; Actually I think that this problem is quite perfect for haskell, as it should be possible to formulate the constraint as rules somehow and let the system figure everything else out, but maybe I'm completely wrong? Probably, yes, it sounds like you may be mistaking Haskell for a logic language. Haskell will let you write concise algorithmic code for manipulating your graphs, but it has no ability to convert from rules to execution for you like a logic language\*. It seems to me you are trying to use a language you're hazy on to implement an algorithm you're hazy on (at least, I'm not getting a solid sense of problem definition here). That's a difficult combo. Haskell will be trying to tell you various things about the implementation of your algorithm, but you don't understand either Haskell or your algorithm enough to hear what it will try to tell you. (This isn't really a good idea in any language, but Haskell can make it much harder to just sort of skid along for a while than many other languages.) My suggestion would be to either learn Haskell by practicing on a problem domain you're more familiar with, or learn your algorithm better by at least sketching it out in your favorite language, then doing a second pass in Haskell. \* To the extent one might be inclined to argue this statement, please remember the context we are in. There's a difference between _being_ a constraint solver out of the box, and making it easy to build one if you already know how and are fluent in the language.
Thanks! I didn't think to scrub those as I don't have them in mine!
I've been struggling with how to use quickcheck too, since everything I want to check is nontrivial, and the examples are all trivial. In addition quickcheck2 is apparently undocumented so there's lots of guessing from the old paper and looking for examples. I finally decided I could indeed quickcheck by testing only one aspect of the output at a time, e.g. that the output timestamps are in order, not that they have any particular value. Then I can write a simple reference version of the (very large, complicated) function under test that only puts things in the right order but with simple monotonically increasing timestamps, and assert that that aspect of the output is equal. The generators are far from trivial as well, e.g. list of non-overlapping ranges, another list of random values with the same length, a tree with ID references that must not be dangling. Rather than trying to write a Gen for the whole thing, I think I have to write a Gen for something simpler without complicated invariants like references and coordinated lists, and then a set of functions to convert that to the real input. After all that I'm not sure if it'll be worth testing those particular properties, but at the least I'll have a library of Gen functions and can more easily build tests for other properties I may be interested in. If successful, maybe I'll write a little bit about the experience since I definitely think quickcheck2 documentation is sparse and scattered.
Here's a complex property I've wanted to test: MIDI has 16 channels, and a pitch bend affects the pitch of all sounding notes on one channel. Given a set of pitch curves, assert that the MIDI output allocates them among channels so that none of them conflict with each other until the number of simultaneous incompatible pitch curves (parallel curves separated by an intergral number of semitones can also share a channel) exceeds 16, at which point channels may be reused. I think the key is to not try to test that all at once, but to say randomly generate inputs with the precondition that they are all compatible and assert that only one channel is ever used. But of course to test anything you also need to write a synthesizer emulator, to turn the stateful set of timed messages into a series of states, and assert things about those, and of course the emulator can't be buggy, and must account for the fact that real hardware takes a little while to react, etc. etc. But by the time you get all the support stuff written it may be a lot larger (and hence have its own bugs) than just enumerating a set of inputs and asserting things about them manually. You might object that the test is too high level, that individual functions inside the process should be tested, but in my experience the bugs are not in the individual functions but in the interactions, e.g. the function to slice a pitch signal interacts with the way the MIDI generator selects ranges of pitch to check for compatibility in a way to miss a bit and thus screw up channel allocation under certain circumstances. This is the sort of thing randomized inputs could do well at.
I wonder if I could get ghc to run on an embedded system. I have an arm cortex m3, which should work with the llvm and now clang. Would be a fun exercise. 
Thank you to everybody who sent me submissions. I'm still analyzing the details and will report what I find.
You might want to watch [this video](http://entirelysubjective.com/programming/data-driven-programming-haskell-1/) which shows the way at least one Haskeller approaches a problem.
If you are writing the code to allocate curves to channels, why would you need to simulate a synthesizer to assert that it is allocating curves in the way you intend? A synthesizer sounds like something you might want if you are not sure the output you want your code to have is actually enough to get the hardware to sound like you want. That's a bigger problem than QuickCheck is designed for. I've used the Gen stuff to make randomized input for functional tests, but it's on unit tests where it really works nicely.
Have you tried profiling it? If you're using ghc: * compile with the `-prof` flag; * run with the `+RTS -p` flag; * look at the generated `*.prof` file. [Profiling (ghc manual).](http://www.haskell.org/ghc/docs/latest/html/users_guide/profiling.html)
Actually try first profiling with -prof -auto-all. And you need to install profiled versions of all referenced packages by cabal install -p.
I think you'll get better help if you post the code you've written. I suggest that you paste your code onto http://hpaste.org/ and give a link to it. I would also direct you to the existing, pure fft package on hackage. Just click "Source" and browse around for a good reference implementation. http://hackage.haskell.org/packages/archive/pure-fft/0.2.0/doc/html/src/Numeric-FFT.html#fft
You're not compiling with optimisation. Also you're not using unboxed vectors. To fix the former you only need to add -O to the ghc invocation (ghc might not be truly magical, but it's more magical than you think!). The latter requires changing the Data.Vector import to Data.Vector.Unboxed and adding a couple of type annotations (I had to specify the type of numvec, and the type of round used in main): http://hpaste.org/53543 These changes brought it down to 0.057s on the test image on my system :) (e: To clarify what's going on, as I understand it code using Data.Vector is hugely dependent on optimisation for performance: Without it, every time you use a vector function you're actually constructing a whole new vector. Which means a hell of a lot of copying data around. I think it's actually not so much ghc that's incredibly magical here but the inlining and fusion rules in the Data.Vector library. They basically do the equivalent of replacing *map f . map g* (where in order to compute the result you first need to compute an intermediate vector) with *map (f . g)*, except in a far more general way. In this case it seems that ghc -O also picks up on the *(fft_CT xse)* subexpression in fft_CT being reused, and other similar cases, so that it can avoid recomputing those values. Surprisingly [you can't always depend on that happening](http://www.haskell.org/haskellwiki/GHC_optimisations#Common_subexpression_elimination), though. So if you want to make sure a computation's result is shared, bind it explicitly. As for switching to unboxed vectors, that simply means that you're using arrays which actually directly contain their values, rather than pointers to heap objects containing the values. Unboxed vectors are faster than boxed vectors but can't be defined in terms of themselves (for instance, *let v = cons 0 (generate 10 (\i -&gt; 1 + (v ! i))) in v* == *fromList [0,1,2,3,4,5,6,7,8,9,10]* can be evaluated with boxed vectors but not unboxed vectors).)
You have a mistake in your algorithm. Look at line 43-44: let xtop = Data.Vector.zipWith3 fft_top (fft_CT xse) (fft_CT xso) (Data.Vector.enumFromStepN 0 1 m) xbottom = Data.Vector.zipWith3 fft_bottom (fft_CT xse) (fft_CT xso) (Data.Vector.enumFromStepN 0 1 m) Here, you compute `fft_CT xse` and `fft_CT xso` twice, once on each line. This means you make four recursive calls to `fft_CT` instead of two. Define `fft_CT xse` and `fft_CT xso` in a `let` instead. The same thing happens in `ifft_CT`. Minor problem: you only call `dft` when the input length is 1. This is a bit wasteful: `dft xs == xs` if `xs` has length 1, so you could remove `dft` altogether and it would lead to more efficient (and simpler) code. Really make sure you're using `-O` too, otherwise performance will be bad. Style hints: * You can do `import Prelude hiding (map, length, (!), (++), blahblahblah)`; that way you will be able to write just `++` instead of `Data.Vector.++`, for example. * Use guards instead of if-then-else: see http://en.wikibooks.org/wiki/Haskell/Control_structures#Guards. * Try to avoid duplicate code. For example, `fft_CT` and `ifft_CT` are the same apart from a scale factor and a sign change in `fft_top` and `fft_bottom`; add the scale factor and sign as parameters and then you only need one function. EDIT: Ari_Rahikkala's suggestion to use unboxed vectors is also important.
I posted the code in the links ? Are the links not working for you ?
Thanks. I will take a look at this. It looks interesting.
My God! what a huge amount of performance difference it makes when I use -O2. My next step was to use unboxed vectors. I wanted to write code using boxed vectors and then convert to unboxed vectors to find out what the difference would be. Thank you for the detailed reply and explanation. 
thanks for mentioning my video. I am not sure it's a best practice video or even representative of how I program. Usually I develop bottom up, so I solve one low level problem at a time. In the video I tried a top-down approach, but certainly it (sometimes) cant hurt to look at one approach ;) Jonas
Just as a small nitpick, you really want to use -O2 rather than -O, especially when using vector.
Cool, I actually didn't know about that. It used to be the case for years that the performance of -O was practically indistinguishable from -O2. What does -O2 do (or accomplish better) these days that -O doesn't?
Fixed imports so that it is more readable, so for convenience, here it is: http://hpaste.org/53549
This looks very pretty. In comparison my code looks positively ugly. But I come from the C++ world, so even the old code was good looking to my brain.
Thanks for the catch. I changed that and the run-time came down by a bit. But using the -O2 was the important thing. I will make the style changes too, just so my code can look better.
No I did not on this code, unfortunately. But I have some older code (this same fft I wrote a version earlier) and that ran slow and when I did profile it, it said it was spending the most time in main, which was not very helpful. On hindsight, I should've profiled this too before I asked on here.
-O doesn't turn on the SpecConstr transformation which is crucial for stream fusion (used at least by vector and text).
Thanks for your feedback. I request you the article again. I didn't say recursion was difficult to understand. Recursion is trivially simple if presented as a solution and it is easy if you know how to apply it. All I said was that it takes time for a programmer thinking in terms of iterative processes to suddenly adjust his/her mindset to solve non-trivial, non-obvious problems recursively and this could pose a barrier for functional programming which does away with for and while loops. Yes, everybody knows lists and trees but it's possible to walk lists and trees with iteration and it is, in some cases, less resource intensive in traditional programming languages to iterate over data structures.
Maybe his point is that in FP, you define all sorts of things recursively which you would never do recursively in an imperative language. Example: *length*. This takes a bit of getting used to.
Yeah, it looks as though ghc did common subexpression elimination on the recursive calls. It doesn't often do that though (CSE can cause space leaks in the presence of lazy evaluation), so it's best to do it by hand when it's important.
Can you help [homebrew](https://github.com/mxcl/homebrew) have official, up-to-date formulae (packages)? * [Haskell Platform](https://github.com/mxcl/homebrew/blob/master/Library/Formula/haskell-platform.rb) * [GHC](https://github.com/mxcl/homebrew/blob/master/Library/Formula/ghc.rb) * [Cabal](https://github.com/mxcl/homebrew/blob/master/Library/Formula/cabal-install.rb)
I think replacing Prelude functions with others like that is very unidiomatic; it should only be done for things that ought to be in the Prelude anyway (e.g. Control.Category, Control.Exception).
I don't use homebrew, so it would be hard for me to develop and maintain formulae for it. Briefly looking at it, I see two ways to go with this: * Maintain the GHC/HP/Cabal layout that HP uses. In this case the formula would be nothing more than download the .pkg and install it, hence, trivial and not much advantage over downloading and clicking. * Place GHC, HP, and Cabal components in homebrew's "Cellar" as it sets up. This would be a major effort in re-thinking the (haskell) package layout and location schemes, followed by possible many formulae. (This seems to be the Achilles' Heel of package management systems: The don't anticipate packages with their own package management, such as programming languages!) The main Haskell Platform repo has the Makefile I use to generate the installation package. All steps are automated, so you can see how it constructs the layout, where it places things, and how it builds the components. Darcs repo: http://code.haskell.org/haskell-platform/ Makefile: http://code.haskell.org/haskell-platform/src/macos/Makefile 
&gt; It seems to me you are trying to use a language you're hazy on to implement an algorithm you're hazy on Sure, I have null experience with Haskell but I think I understand the algorithm quite well, at least in imperative language style (if this distinction exists). I actually have an implementation of this thing in C++ but I don't want to simply translate it one to one, but to try it more in the functional style. In my other implementation, for instance, I create a graph with parts of the above constraints applied, traverse it, check if the graph has the correct properties and then keep/discard it. In Haskell, as I understand it, the first step would be to create an enormous list of graphs and then only keep 10^(-5) or so of those graphs. It's not about performance, I just think that there must be a better way ... &gt; Probably, yes, it sounds like you may be mistaking Haskell for a logic language. Haskell will let you write concise algorithmic code for manipulating your graphs, but it has no ability to convert from rules to execution for you like a logic language*. Yes, I understand that Haskell is not a logic language. What I meant with rules was something like the following type Node = Int type Order = Int -- the order of a vertex == number of adjacent edges data Vertex = Vertex Node Order validVertex :: Vertex -&gt; Maybe Vertex validVertex (Vertex n o) | o &lt; 5 = Just (Vertex n o) | otherwise Nothing and then during creation, the graph collapses to Nothing if there is a Vertex of the wrong order. The difference to my other implementation would be that Vertex now can be treated as an object and functions applied to Vertex give me its properties etc., while in C++ this is overkill and I just keep a list of pairs of nodes and orders in the Graph class, iterate over them in for loops, etc. Now my line of thought goes like this: Obviously, one has to construct the whole graph before being able to set the Order of the Vertex. Is Vertex as it stands above then a bad choice? It is not possible to define a function to create a single graph and then the next one and so on, as one needs all permutations, so should one still do this and pass some State along? Or should we create two types of graphs, create a simple Data.Graph first, run some analysis and then create another type of Graph with Vertex as vertices? At least then filtering would be easier. And so on. It's not so much about that it is impossible for me to do this *somehow*, I'm pretty sure I'm able to. But I'd like to use what Haskell offers and learn. And to be honest, working through all the examples in the tutorials didn't put me in a situation where I feel I'd be able to. Which, I guess, is also the power of haskell ...
Why? if you use none of the Prelude functions and all of their replacements from a different module, I don't see why you shouldn't just hide the Prelude versions.
Thank you for the link! bookmarked
Independent of the bugs in your code, I think that the radix-2 DIT FFT (which is what this is) is one of those algorithms where the in-place version is both way more efficient and easier to follow. So you can see what I mean, here's an implementation of that algorithm that I wrote a few weeks ago: https://github.com/bos/statistics/blob/master/Statistics/Transform.hs
Thanks, I will take a look at it and compare my timings with yours.
Every time you forget to use -O2, all GHC developers simultaneously shed a single tear.
The first approach is the way to go, as homebrew doesn't repackage individual langage-specific packages, such as [Ruby gems or Python eggs](https://github.com/mxcl/homebrew/wiki/Gems%2C-Eggs-and-Perl-Modules). I agree this should be trivial, but it turns out that certain things are difficult. For instance, in August, [I was trying to get GHC 7.2.1 to work](https://github.com/mxcl/homebrew/pull/6979), and ran into incompatibilities with cabal-install-0.10.2. I eventually gave up, and I'm using GHC 7.0.4. How would you build a Haskell system including GHC 7.2.1 and cabal-install on a Mac today?
i blame haskell. seriously. if tuning a hundred lines of code like this is such a hassle, what happens in code ten times that size? for anyone sane, you are going to burn up a week trying to finish this in haskell, then realize you could have churned out five times as much acceptably-fast code with another tool. and if i want to spend a week tuning code, i'll tune c and get better time and space optimizations anyway i constantly get downvoted for saying haskell is actually as fast as python for most realistic scenarios where you are going to dedicate a realistic amount of time to writing and debugging code. to me, this proves it. the author of these code slices is not an idiot and put a reasonable amount of labor into making things right, and still had the solution blow up in his face. and that unboxed vectors are the answer strikes me as the same class of solution that also contains strictness annotations...to make haskell fast, make it unhaskelly 
Well, because the output is going to be timestamped MIDI messages. It's easy to assert time insensitive stuff like 'all ((==0) . channel) msgs' but much harder to say assert each note starts with a correct pitch, if that pitch is determined by PitchBend chan val .... NoteOn chan key... If you have a synth simulation it can turn this series of "mutate this, mutate that" msgs back into a nice functional list of states. As far as being a bigger problem, I'd say this kind of problem is the most interesting! Subtle interactions between complicated systems are where the bugs are, and that's also where the number of combinations grows to beyond where you can just think them all up and hardcode them in a unit test.
I instinctively associate map, length, (++), etc. with their list versions; having to keep the special import list the module uses in mind constantly just to understand small pieces of code does not sound pleasant to me.
 * Are you actually implementing the same algorithm? * Are you using `seq` correctly? * Are you using `runhaskell` (interpreted) or `ghc` (compiled)?
Why isn't `-O2` on by default for every compiler?
Generally due to longer compilation times.
I adding -O2 to the compiler flags so hard?
You blame Haskell because the OP forgot to compile with optimisations? Really? As to unboxed vectors, they are about as unhaskelly as classes without virtual destructors are un-C++'y. That is, not at all.
If that is your output, then isn't there some point where you pitch bends have been assigned to different channels of output? That's where you could use QuickCheck to assert that the distribution behaves the way it is supposed to. Then you might have other properties to check that a set of pitch bends applies correctly to a stream of nodes, maybe asserting that a pair tested to be compatible can actually be applied together in a nice way. Do you know how to write code that describes correct behavior? Maybe with a sufficient set of tests, maybe with a reference implementation. "Subtle interaction" sounds to me like one part depends on an undocumented property of the output of another part. You can test the whole thing as a black box, but the bigger the system the less likely a set of randomized cases is going to give decent coverage. QuickCheck is a much sharper tool if you can break those cases down into clearly stated properties of the individual components which should be sufficient for them to get along. Then you'll have separate tests that each component behaves like it's supposed to, and the higher level tests just need to worry whether the properties fit together correctly.
Advice is all good, I don't really have anything to add but a request. After you're done optimizing, can you update your post with the results?
That unsafeCoerce hack for monad/applicative is deliciously evil.
There's another problem with using "foreign unsafe" - it leaves that haskell thread in an inconsistent state while the call is in progress, such that you can't call back into haskell (via foreign export, say) on that same OS thread. Without unsafe, you can. I didn't know that a long-running unsafe foreign call could jam the works in any other way, though. I'm going to need to test that.
&gt; I actually have an implementation of this thing in C++ but I don't want to simply translate it one to one, but to try it more in the functional style. I had a similar problem some time ago. I implemented an imperative graph algorithm and rewrote it gradually into a functional one. I used Ocaml and Ocamlgraph. Not Haskell, but functional and effective - might be an alternative in this case.
I wouldn't! That is, I only package what is in the Haskell-Platform spec, which is still at 7.0.2, and about to move to 7.0.4. As you've learned, getting the delicate dance between GHC, Cabal, Cabal-install, and the other HP packages is non-trivial. It almost never is as simple as just upgrading one of those components. Hence, I wouldn't package them for homebrew as if they were. I'd package Haskell Platform. And if I was doing that, 7.2.1 wouldn't be in the mix! Now, if you know a cohesive set of package versions for GHC, Cabal, Cabal-install, and the other packages, then you could use the HP Makefile I wrote (see link above) to build them into a single installable package.
Right, if I'm understanding you correctly, that's what I'm planning to do. Pull out individual properties and test them individually. I had assumed that since randomized input produces all combinations of properties I would need to test them together, but there are ways around that. WRT subtle interactions, I could say "one part depends on an unrealized property of another." For example, 'Signal.within start end' chops a signal to [start, end). So another function tests the samples within the range and is happy. But wait, within has to include the preceding sample or add a leading sample at start or the value before the first one is undefined (e.g. [(0, 42)] sliced to (4, 8) should be a constant 42). So change 'within' but whoops... forgot the full implication for its caller who is now getting out of range samples! And actually in reality that full implication was mediated through another function that resamples two signals for comparison. Testing any of the three individually (which I did!) won't turn up that problem. Another variation on this theme: it turns out the rules during the decay of a note are different, which falls naturally out of how a synthesizer handles them, but is definitely not obvious at the comparing-pitch-curves level. So the low level is doing what is obviously correct and the high level is also obviously correct, but it's not obvious that they're different! So it's not so much relying on undocumented properties but rather that there are properties I didn't realize existed... and debugging is the act of finding out about those properties. Since I didn't realize they existed I didn't write tests for them, but randomized input would likely have hit them.
And why does hackage upload warn that you should only use -O2 if absolutely necessary?
&gt; I have the feeling, though, that at least the first constraint should be possible to be fulfilled during construction, but I don't really see how It looks like you'll want to start with a list of vertices, delete a vertex from that list to get a list of candidate reachable nodes for the deleted node, generate a list of subsets of the reachable nodes, and then filter the subsets to exclude those with the wrong number of nodes. The standard library functions for those things are delete, subsequences, and filter respectively. Once you have the filtered subsets for each vertex, pick one of each of them using sequence and zip it with the original vertex list to get a list of the adjacency list representations the constructor expects. &gt; Actually I think that this problem is quite perfect for haskell, as it should be possible to formulate the constraint as rules somehow and let the system figure everything else out, but maybe I'm completely wrong? Only slightly wrong. Haskell's laziness can be used to generate a large number of candidate solutions and then rule them out without having to allocate memory for all the possibilities at the same time, but it still requires you to write some generation code in addition to the constraints. &gt; Apart from some pointers on how to get to a solution, I'd be very interested in general hints of how you experienced haskellers attack problems (start simple, evolve, abstract away, etc.) The first question you need to ask yourself when writing a Haskell program is "What IO is the program going to do?". If the program involves interaction with the user, there will be more data that needs to live in the IO monad than for a batch program, and whether you want data to be strict usually depends on the same question. After that, you can usually think of a Haskell program as refining its input by throwing out erroneous input data until it's in the form you can use for computation. The process of throwing out erroneous input can also be thought of as strengthening the type of the input data (e.g. from the very weak type String to a strong type representing a syntax tree). Another helpful way of thinking orthogonal to the one above is to regard your functions as existing on a continuum from general, polymorphic, and abstract functions to narrow, monomorphic, and concrete functions. Abstract functions are more flexible but difficult to reason about, while concrete functions are the opposite. Because of this, it's helpful to have both by implementing concrete functions in terms of more abstract ones. Even if your concrete functions are just trivial renamings or type restrictions of abstract functions, I think it's still useful to have them. Likewise, even if your abstract functions are only used once, I think it's still useful to make them in order to separate programming techniques and data representations from domain logic.
I think that warning is silly. Lots of packages benefit from -O2.
Well, I don't know for sure about ghc, but optimizations make it a lot harder to debug gcc-compiled code.
Why isn't -O2 on by default?
* Yes, I am implementing the same algorithm. * My code does not use seq ? * Thanks, I didn't even know about runhaskell :-). I am using ghc and ghci. PS: How did you get the font to change for seq in your post ?
runhaskell is fantastic for when you're testing your code. GHC is good for when you move onto the optimization phase. Because Haskell uses lazy evaluation, sometimes you need to forcibly evaluate things. Otherwise, you can get a stack overflow due to recursion. seq and deepseq are helpful for preventing that kind of error. I used backticks (`). When you edit a post or comment on Reddit, you can click "formatting help" by the bottom-right of the text area for a table of syntax keys.
[Here](http://hpaste.org/53585) is the modified code with some style changes. I am updating the post too.
This is nice information to know. One more question are ghci and runhaskell the same ? Appreciate the formatting tips, but my 'formatting help' link does not contain the backticks tip!
GHCi is the official Haskell interpreter, it's just like the Python interpreter but for Haskell code. When you run code from the command line, you often do it like this: hello.hs: main :: IO () main = putStrLn "Hello World!" Example (compiled): $ ghc -o hello hello.hs $ ./hello Hello World! But you can also do it the scripting way. Example (scripting): $ runhaskell hello.hs Hello World! In Unix-based operating systems like Linux and Mac OS X, you can use shebangs to shorten your commands. hello.hs: #!/usr/bin/env runhaskell main :: IO () main = putStrLn "Hello World!" Example (dot-slash): $ chmod a+x hello.hs $ ./hello.hs Hello World! The chmod command only needs to be used once, to mark the hello.hs file as executable. After that, the terminal treats it as if it were an ordinary binary. runhaskell is particularly useful because not all Haskell programmers use GHC. Some use Hugs, for example. Where a GHC user would use the shebang line `#!/usr/bin/env runghc`, a Hugs user would use `#!/usr/bin/env runhugs`. Of course, these shebangs create a hassle for coders who want their Haskell code to work on any platform, no matter if they use GHC Haskell or Hugs Haskell. So runhaskell actually uses the particular Haskell on your system.
This is amazing information. I have been thinking of using Haskell as a replacement for shell scripts (which are not used by anyone else and on any other system other than which it is written on, so portability is not a concern) in my day job. This will go a long way in achieving that, thank you.
Happy to help, I love scripting. :) You might like this [Rosetta Code](http://rosettacode.org/wiki/Scripted_Main) article, it helps you write APIs and CLIs all in one code file.
Haskell is not the be all and end all of languages. C has its place and so does Python or Haskell or any of the other languages. Now let me tell you why I am learning Haskell. Because it is *fun*, and for the first time I am learning amazingly new things. Things that I never knew existed in programming, new ways of doing things, safer ways of doing things, easier ways of doing things and the most important, lego way of doing things. I can put together complex code using smaller building blocks so easily. I see that maths and algorithms, in papers and text books, translate so fluently to Haskell code. Haskell's strength may not be outright brute performance, for that we have C and assembly. Lastly the coolest part of Haskell is the community. Where else do you have PhD candidates doing research in the language, researchers and generally very well informed people answer questions for you and so patiently and nicely too. I guess Haskell not only teaches one good programming practices and a great language, but also teaches humility. 
Bookmarked! Thank you.
Can we have the new benches?
I will edit the OP and update the new benches.
thank you :D
Because some optimisations can actually slow down code. I don't think that's likely with -O2, but it's a better idea to let people decide to use it or not.
It seems to me that this is not a datastore, but just the identity monad. Basically, the equations for `get` and `put` tell you that `S a` is isomorphic to `a`; i.e. you can store precisely one value. The two functions need different types and weaker laws if you want an interesting monad.
Hei Jonas, but what about video #3? You know, I'm looking forward to see a real application of your program and a real text recognition with it :)
&gt; How would you build a Haskell system including GHC 7.2.1 and cabal-install on a Mac today? I have [some instructions](https://gist.github.com/1169332) which work quite well, although I need to update them to make it use an older cabal-install.
&gt; If I was doing that, 7.2.1 wouldn't be in the mix! Which is totally reasonable, but all of those of us who have new Macs or upgraded to Lion for some other reason can't get the Haskell Platform running on it. Unfortunately, given the time between HP releases, it's currently less than straightforward getting GHC and cabal-install running on OS X.
I would even argue that part of the point of putting code into a library is in order to spend more time building up front and spread the cost of that time over all the library's clients.
Because it can sometimes make compilation very slow. And in most cases it makes no significant difference in performance. EDIT: Axman6 pointed out that in some cases it can even *slow* performance.
I agree with this. Whenever I need to use functions with the same name as functions in the Prelude or other very common libraries, I import the library qualified with a very short alias. It loses almost nothing in brevity, and it makes the code much easier to read.
yes, i am aware of its unfinished status. whenever work leaves me time ill finish the screencast. hopefully in the next weeks ;)
Really? I am using a Macbook Pro with OSX 10.7.2 and have successfully installed the Haskell Platform directly from Haskell.org and through Homebrew, and also both the 32 and 64bit versions. There were some compiler warnings that I had to mute, but otherwise it works fine. What is the problem you're having?
Monads, is there anything they can't do?