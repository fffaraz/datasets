Hahahaha.
from a technical standpoint i likes the haskell articles in "open source software architecture" (about ghc and yesod i believe) -- non of which are pure commercial initiatives. there are pretty technical slides online somewhere about silk i believe...
Yeah, the `5.0` syntax is how C does it. Then again, we do have many numeric types including `Int`, `Integer`, `Word8`, `Word16`, etc, and even C overloads numeric literals for these.
More like I don't know enough advantages to see a good trade off. But there could be advantages I didn't know about yet, hence my comment.
I wouldn't call it a "sum" type though, it's a union type. Contrast `Bool + Bool = 4` with `Bool U Bool = 2`.
the curry would make it so much better though
Would have been cuter with conduits. Meh, not impressed. I'm kidding \^\^, that's cool!
Just the `do` you say? main = putStr =&lt;&lt; readFile "valentinesday.txt"
You probably mean that the browser itself is a monster? That's certainly true, but everyone has got one installed and it's easy to communicate with. So, it's quite minimal when looking from the Haskell side.
 x &amp;lt;- [1..4]
This morning I found this fairly [recent paper](http://www.accursoft.com/Haskell-GUI/Graphical%20User%20Interfaces%20in%20Haskell.pdf) providing a detailed comparison of many Haskell GUI libraries, and the difficulties involved in using each to implement a tic tac toe style game. Edit: From a Windows perspective, I might add.
It's a tagged union type (you can ask if a value `is_a?String`, etc). The user does not get to define the type, so there is no way to add Bool to it twice (and none of the languages choose to include such in the definition), but in languages with user-defined "types" those *do* automatically get added to the sum.
that's pretty creepy somehow. the girl is, i mean.
Ahh, yes. I agree that would be useful. However it doesn't eliminate the need to manually manage package repositories/sandboxes.
Funnily enough, I'm using ekg for very similar purposes, and I see it's commented out in your .cabal file. Were you using it to plot telemetry or just for RTS monitoring? If the former, why did you not pursue adapting ekg to your needs rather than rolling your own?
he could give her a "Proda" hand bag, surely one letter wouldn't ruin her vd? anyway, I don't really care, it seemed funny - that's it.
I was using it for RTS monitoring to make sure my log didn't grow too big. I commented it out because it adds lots of dependencies. I never considered using EKG to actually plot the protobufs. What are you using it for?
Well it is the thought that counts, and nothing says I wanted to get you something but you're not worth the best, like poorly counterfeited designer goods. You have to consider the grand scheme of geeky Valentines gifts. Pity the poor souls who got PHP love messages last Thursday.
It's for a robot that is doing some recognition and tracking. I use ekg to plot performance measures as well as running estimates of things I'm tracking. For example, one useful plot is a time history of tracked velocity. It's very easy to get that going in ekg, and the interface composes nicely with other logging/monitoring systems. I have a monoid of loggers, so I can send data to any combination of csv, ekg, log file, and console output.
I posted it on the article, but will repeat it here: http://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/
Thanks for the feedback, folks. Got it.
that's dedication man. 
Hey there... I'm pretty creepy.
By parametricity, that is equivalent to a list of Ints.
[Someone already did that.](http://www.reddit.com/r/learnprogramming/comments/186950/girlfriend_of_programmer_seeking_valentines_day/c8bzp24)
I've seen this article a few times before. I haven't really dug deep into the consequences of this anti-pattern. However, I have an inkling it is more about Haskell's poor handling of existential types and the second-class nature of typeclasses. Perhaps my belief is that existential types deserve more credit than they get :)
Should've gone for hex.
It depends on what you consider a "type" to be. `int?` and `natural?` might both be true of the value `0`. So while values in dynamically-typed programs *are* usually tagged, I hesitate to (for example) call The Scheme Type a tagged union. I think of it as a supertype instead. (In fact, *the* supertype of all types: "top" or "any").
Well let me put it that way - I don't see it as a mistake - yes this seems to be an anti-pattern, and I was not aware of this - is this trivial or a mistake? On the other hand your comment is somewhat rude and just the opposite of constructive so I take you just as one of the counterexamples to the well-acclaimed kind Haskell community. But thank you for your time - but in the future try to let your great intellect shine by giving some good advice with your bad behaviour at least.
thank you - I was not aware of this - sorry to waste your time. I won't remove the entry here and update my blog just that others might find your comment - as a warning. Thank you.
Maybe you care to give the Wiki-Administrators at http://www.haskell.org/haskellwiki/Existential_types#Problem_statement a request to update this as well? I guess I as a beginner am not worthy to such a deed :D Maybe there should be a hint - not that TrollandSteward gets another bad day...
&gt; sorry to waste your time Not much of a waste. You went out of your way to try to help others, and I just thought I'd return the favour :) Keep up the exploring!
She's already a better programmer than most.
Why wouldn't you just use a tuple or something for this?
Hmmm, this gets me thinking of my pet project on the backburner, `racket-interfaces`. First, consider the new-ish generics that have already been introduced into Racket. This is quite close to the C# code. #lang racket (require racket/generic) (define-generics animal (number-of-limbs animal)) (define-struct dog (name) #:methods gen:animal [(define (number-of-limbs d) 4)]) (define-struct spider () #:methods gen:animal [(define (number-of-limbs s) 8)]) (define animals (list (dog "Mozart") (spider))) (for ([a animals]) (printf "this one has ~a limbs~n" (number-of-limbs a))) I don't believe Typed Racket supports this generics library yet, but it's easy to imagine how it could work. The downside to this technique is that instances must be attached to the struct definition; if the author didn't add the instance then you must create a new struct on top of the existing one that adds the instance, which means you have to wrap all of the data you want to use inside of your new structs. Yuck. In Haskell, the instance declaration exists separate from the data declaration. This has its pros and cons as well. We are still limited to one instance per data type (and must use wrapping in order to use a different instance). I am exploring a different approach, where the instance usage is explicit. This is not too different from how it works in Scala, which has implicit parameters to simulate the "default" type class instance, but allows you to explicitly pass a different instance if desired. #lang racket (require my/awesome/generics) (define-interface animal (number-of-limbs)) (define-struct dog (name)) (define-struct spider ()) (define-instance animal-dog animal dog (define (number-of-limbs d) 4)) (define-instance animal-spider animal spider (define (number-of-limbs s) 8)) This technique gives names to each instance, so you can refer to it directly, rather than forcing you to depend on the instance implicitly bound to your data. We could then theoretically write stuff like this: (with-instance monad maybe-monad (do (&lt;- x (just 3)) (return x))) However, in this particular case, having the instances *not* intrinsically tied with the data makes it obnoxious to deal with such mixed data. ;; option 1: a custom instance (define animals (list (dog "Mozart") (spider))) (define-instance animal my-animal-instance (define (number-of-limbs a) (cond [(dog? a) (with-instance animal animal-dog (number-of-limbs a))] [(spider? a) (with-instance animal animal-spider (number-of-limbs a))])) (with-instance animal my-animal-instance (for ([a animals]) (printf "this one has ~a limbs~n" (number-of-limbs a)))) ;; option 2, bundle instance with data (define-struct ani (a inst)) (define animals (list (ani (dog "Mozart") animal-dog) (ani (spider) animal-spider)) (for (wrapped-a animals) (with-instance animal (ani-inst wrapped-a) (printf "This animal has ~a limbs~n) (ani-a wrapped-a)))) Tradeoffs, tradeoffs, everywhere.
For the existing generics, see http://docs.racket-lang.org/reference/struct-generics.html For my incomplete pet project, see https://github.com/DanBurton/racket-interfaces
I modified the summary to read: "Management of finite resources is an important part of any modern build system, only properly available in Shake and Ninja." 
I (as the author of Glome) don't understand your objection to existential types. Your argument seems to be that it's possible to accomplish the same thing without existential types, so therefore you shouldn't use them. My main objection to the record-of-closures approach that you're advocating is that it's going to use a lot of memory. Suppose you have a ray tracer that defines a sphere primitive. A sphere could be defined as a center position (3 floats) and a radius, so 4 floats in all. For polymorphism, you'll need some kind of type identifier. I'm assuming that the existential typeclass approach stores one set of methods that are shared by every instance of a particular type (i.e. all spheres share a single copy of the sphere methods) -- that could be a mistaken assumption, but you could at least imagine an implementation of typeclasses could share methods. In your approach, every sphere would require 4 floats, plus a big pile of unshareable closures, for each primitive object. The closures themselves might not be very big, but it should be easy to imagine a scenerio where most of the memory used by a ray tracer is just storing millions (or billions) of closures. Existential typeclasses have their drawbacks, but so do records of closures. I don't think it's appropriate to label something as an antipattern when it just represents different tradeoffs.
As far as I know, GHC *always* erases types: this means that there isn't "some kind of type identifier" (note that a Haskell program can have an unbounded number of types). So it must be storing the existential as a record of closures.
I love Foldable, but it's not as general as this one, as it requires a parametric type. So the `Text` example is impossible using `Foldable` (but it's a bit useless anyway, since we already have OverloadedStrings).
I'd love to, but I can't find out who is/are building this... there's no info on the wiki page.
This *might* be true, but there's no reason it has to be (the closures can certainly be made just as lightweight as any dynamic dispatch mechanism would be), and without data from the actual GHC implementation there is not much to discuss.
Looks to me like it's from your now-a-programmer girlfriend. 
\#3 makes me wish Haskell records were more pleasant to use :(
GHC makes for each type a record containing the methods for that type, and passes those records around as extra arguments to any function with class constraints. Look up "dictionary passing". (Instances with a context of their own turn into functions rather than records, and may give less sharing). There's no reason you would get any less or more sharing if you write things by hand. You can see it all if you look at the generated core.
Here's an example. Suppose we have a simple ray tracer, and we define Solids as anything that has these methods: data Solid = Solid { rayIntersection :: Ray -&gt; Intersection shadow :: Ray -&gt; Bool ... } Spheres are one possible instance: Data Sphere = Sphere (Float, Float, Float) Float sphereIntersection :: Sphere -&gt; Ray -&gt; Intersection sphereShadow :: Sphere -&gt; Ray -&gt; Bool ... And let's say we create a list of spheres: sphere1 = Sphere (0,0,0) 1 sphere2 = Sphere (0,1,0) 0.5 sphereSolid1 :: Solid sphereSolid1 = Solid (sphereIntersection sphere1) (sphereShadow sphere1) sphereSolid2 :: Solid sphereSolid2 = Solid (sphereIntersection sphere2) (sphereShadow sphere2) spheres :: [Solid] spheres [sphere1methods, sphere2methods] The problem here is that "(sphereIntersection sphere1)" etc.. are all closures and each one takes up a little bit of memory. It may not be very much -- maybe 2 words (a pointer to the sphere data and another pointer to the function), but there is no possible way of sharing that data between spheres because the sphere data is different for each sphere. The cost goes up as you add methods. If you have a dozen methods, that's 24 words of extra, unsharable overhead for each primitive. It's hard to justify that when an unboxed representation of sphere might be 4 words. It is, of course, possible that some of the memory overhead is mitigated by lazy evaluation: those closures may not be allocated until the "spheres" list is actually evaluated, but in a ray tracer it isn't safe to rely on that -- you might need to sort the entire scene into a bounding volume heirarchy, which would require evaluation of many of the thunks before you even start rendering. I'm not claiming that existential types as currently implemented in GHC are any better than this, but one could at least imagine a hypothetical implementation that avoids allocating millions of closures by storing the methods in a form that isn't partially applied, and thus is shareable between multiple instances of the same type. For what it's worth, I'm not terribly thrilled about existential types either, it's just that they seemed like they might be the least bad solution. There may be a better way to do the "list of methods" approach that doesn't partially evaluate anything, but I'm not sure how to pull it off. (I suspect there will be an unsafeCoerce in there somewhere.) What I have in mind is something like: data SolidOps a = SolidOps { rayIntersection :: a -&gt; Ray -&gt; Intersection shadow :: a -&gt; Ray -&gt; Bool ... } SphereOps = SolidOps sphereIntersection sphereShadow data Solid a = (a, SolidOps) sphere1 = (Sphere (0,0,0) 1, SphereOps) sphere2 = (Sphere (0,1,0) 0.5, SphereOps) spheres :: [Solid] spheres = [sphere1, sphere2] This gets the per-object overhead down -- we're only wasting a word for the SphereOps reference (and the tuple boxing overhead), and it doesn't matter how many methods are in SphereOps -- only one of them need ever be constructed. But, of course, the typechecker won't allow this. (Maybe there's a type extension somewhere that would help?) 
Ah I see. The commit message mentions it: commit 3234a4ade7204c4206831b4c1dc4a8b23624cc6b Author: Simon Peyton Jones &lt;simonpj@microsoft.com&gt; Date: Thu Feb 14 13:04:14 2013 +0000 Add OverloadedLists, allowing list syntax to be overloaded This work was all done by Achim Krause &lt;achim.t.krause@gmail.com&gt; George Giorgidze &lt;giorgidze@gmail.com&gt; Weijers Jeroen &lt;jeroen.weijers@uni-tuebingen.de&gt; It allows list syntax, such as [a,b], [a..b] and so on, to be overloaded so that it works for a variety of types. The design is described here: http://hackage.haskell.org/trac/ghc/wiki/OverloadedLists Eg. you can use it for maps, so that [(1,"foo"), (4,"bar")] :: Map Int String The main changes * The ExplicitList constructor of HsExpr gets witness field * Ditto ArithSeq constructor * Ditto the ListPat constructor of HsPat Everything else flows from this.
If you existentially quantify the values in the list, that means that the only operations you can apply are those in the type class. This means you could equivalently just store the limbs in the list and forget the storing animals entirely. More generally, this is equivalent to partially applying your record of functions to each animal and storing the partially applied records in the list. For more sophisticated examples you may need the existentially quantified type class, but it is gratuitous in this case: limbs :: [Int] limbs = [nrLimbs (Dog "Mozart"), nrLimbs (Spider "Spider"")] That `limbs` list has the exact same amount of information as your animals list, but rather than wrap the animals in a constructor we just wrap them in the function you were going to use on them anyway. Thanks to laziness, Haskell won't actually compute the limbs until you need them.
Have no fear, `lens` is here.
Does lens also solve the overloading issues that force everything to have an unique name? I though it was more about adding support for setters and things like that.
I interpreted the comment as saying "Haskell makes it impossible to make trivial mistakes", not as a criticism of you.
&gt; You forgot to put your code in code blocks... Here's an example. Suppose we have a simple ray tracer, and we define Solids as anything that has these methods: data Solid = Solid { rayIntersection :: Ray -&gt; Intersection shadow :: Ray -&gt; Bool ... } Spheres are one possible instance: Data Sphere = Sphere (Float, Float, Float) Float sphereIntersection :: Sphere -&gt; Ray -&gt; Intersection sphereShadow :: Sphere -&gt; Ray -&gt; Bool ... And let's say we create a list of spheres: sphere1 = Sphere (0,0,0) 1 sphere2 = Sphere (0,1,0) 0.5 sphereSolid1 :: Solid sphereSolid1 = Solid (sphereIntersection sphere1) (sphereShadow sphere1) sphereSolid2 :: Solid sphereSolid2 = Solid (sphereIntersection sphere2) (sphereShadow sphere2) spheres :: [Solid] spheres [sphere1methods, sphere2methods] The problem here is that "(sphereIntersection sphere1)" etc.. are all closures and each one takes up a little bit of memory. It may not be very much -- maybe 2 words (a pointer to the sphere data and another pointer to the function), but there is no possible way of sharing that data between spheres because the sphere data is different for each sphere. The cost goes up as you add methods. If you have a dozen methods, that's 24 words of extra, unsharable overhead for each primitive. It's hard to justify that when an unboxed representation of sphere might be 4 words. It is, of course, possible that some of the memory overhead is mitigated by lazy evaluation: those closures may not be allocated until the "spheres" list is actually evaluated, but in a ray tracer it isn't safe to rely on that -- you might need to sort the entire scene into a bounding volume heirarchy, which would require evaluation of many of the thunks before you even start rendering. I'm not claiming that existential types as currently implemented in GHC are any better than this, but one could at least imagine a hypothetical implementation that avoids allocating millions of closures by storing the methods in a form that isn't partially applied, and thus is shareable between multiple instances of the same type. For what it's worth, I'm not terribly thrilled about existential types either, it's just that they seemed like they might be the least bad solution. There may be a better way to do the "list of methods" approach that doesn't partially evaluate anything, but I'm not sure how to pull it off. (I suspect there will be an unsafeCoerce in there somewhere.) What I have in mind is something like: data SolidOps a = SolidOps { rayIntersection :: a -&gt; Ray -&gt; Intersection shadow :: a -&gt; Ray -&gt; Bool ... } SphereOps = SolidOps sphereIntersection sphereShadow data Solid a = (a, SolidOps) sphere1 = (Sphere (0,0,0) 1, SphereOps) sphere2 = (Sphere (0,1,0) 0.5, SphereOps) spheres :: [Solid] spheres = [sphere1, sphere2] This gets the per-object overhead down -- we're only wasting a word for the SphereOps reference (and the tuple boxing overhead), and it doesn't matter how many methods are in SphereOps -- only one of them need ever be constructed. But, of course, the typechecker won't allow this. (Maybe there's a type extension somewhere that would help?) 
Well just look at his profile on reddit and the name - I think he saw the common resonance to my posting and just had to give some kind of trollish comment - could be wrong as I'm no native english speaker and sometimes miss subtle things - if so I apologize.
Thank you as I try to explain in the blog the whole thing started when I wanted to implement a simple raytracer and there I want my scene-object to be open so ADT are not an option for me. 2 seems to be the consensus here and I can see the reason behind that and indeed I tryed something like that (starting with a function that given a ray will yield an hittest-option but as soon as I started to add kd-tree support to optimize the tracing I had need for more then one function obvious and I guess I just was to stupid to see 3 at once - so I went out and looked what others did in the same situation. As I do mainly OOPish thinks I of course looked for something simulating base-classes and after quite some googling found the linked haskell-wiki article that just said "listen stupid - this is just what you want" - my raytracing goal was even one of the samples there. Having spent quite some time searching and wanting to try it I tought it a good idea to write a bit about it but alas I stepped into something I only know as "architecture fail"
Just asked in #haskell-lens (on freenode). You can use the [`makeFields`](http://hackage.haskell.org/packages/archive/lens/3.8.7.3/doc/html/Control-Lens-TH.html#v:makeFields) function in `Control.Lens.TH` to generate classes for your records which essentially allows them to be overloaded. EDIT: apparently this isn't recommended, but I don't know another way around it yet :)
...got a typo in the title, please apply `s/2013/2012/` while reading :-/
"The problem here is that "(sphereIntersection sphere1)" etc.. are all closures and each one takes up a little bit of memory. It may not be very much -- maybe 2 words (a pointer to the sphere data and another pointer to the function), but there is no possible way of sharing that data between spheres because the sphere data is different for each sphere. The cost goes up as you add methods. If you have a dozen methods, that's 24 words of extra, unsharable overhead for each primitive. It's hard to justify that when an unboxed representation of sphere might be 4 words." I think that thinking in terms of words for a high level language like Haskell without knowing the internals of the compiler is pretty much pointless. Perhaps you could reduce the number of closures by something like: data Solid = Solid { rayIntersection :: Ray -&gt; Intersection, shadow :: Ray -&gt; Bool } newSphere center radius = Solid { rayIntersection = \ray -&gt; ..., shadow = \ray -&gt; ... } 
I suspect it's probably because of the ancient GHC 6.12.1 version that this is not included in base by default.
Why is it not recommended?
The only way I know of to do this is to abuse type classes and sometimes type families. I do not normally recommend this. It appears this is basically what lens does, though.
While working on incremental updates, I realized that I _already_ have low level building blocks for PDF generating. And it is not so hard to implement. The hard part is to design clean API for that. So, now PDF generation tools are in my todo list. Thank you for the idea!
Yes, the space issue is a fair point. The record of closures will indeed take one word per function in the record, plus the closure. The existential with typeclass will take exactly the amount that is needed for the data constructor plus one extra word for the type class dictionary. Yes, this is how GHC represents constructors with existential type with type classes, it stores a pointer to the dictionary for each class constraint. So that's actually much more like a classic object representation in an OOP language (class-based rather than prototype-based).
Since you can't have two functions with the same name (that's the problem we're solving, after all) and the convention for record accessors is `_[data name][field name]`, the template function knows that it needs to clip the underscore and the data name - leaving only the field name as a type class function, which can be "overloaded". http://hackage.haskell.org/packages/archive/lens/3.8.5/doc/html/Control-Lens-TH.html There are some examples in the documentation. This pattern is used for several templates. You don't need to understand TH or QQ to get the picture. 
Template Haskell generates code that the static checker can't check, and declarations are order dependent. Only use it for projects when you know what you're doing and keep it at a minimum. 
Look at this: http://elm-lang.org/learn/Records.elm It's a language inspired by Haskell, but they decided to make records first class.
No I think OP means between (char '[') (char ']') ((++) &lt;$&gt; (many1 letter) &lt;*&gt; (show . pred . read &lt;&amp;&gt; count 4 digit)) Damn that's ugly. :-)
&gt; A startup [...] ditching some dynamic language for some other dynamic language is expected and unsurprising As you say, it seems OP felt conventional wisdom suggests this would be true. However - if my experience is anything to go by - this is an assumption that doesn't stand up to scrutiny. For a shop that uses Ruby, what does switching to Python or Node (or etc.) buy you? Greater performance? More sexy language constructs or syntactic sugar? A better library ecosystem? For all of these, I'd argue that the advantages of one dynamic language over the other are slight enough to make the cost of switching not worth it. To ask a dev team to switch to a new language (and the associated cost of learning new frameworks, libraries, etc) only makes sense when the new language offers something of greater utility than a more agreeable lambda syntax or the like. I've worked on biggish Ruby projects and felt the pain of writing thousands of lines of tests to try and replicate what a static type system would give for free. I think Haskell's type system is a compelling enough feature to make people want to switch*. I see experiences like that as being the motivator for people to switch to Haskell, so you'll see people coming from languages like Ruby and Python, and not so much *ML and Scala, where Haskell's advantages aren't as pronounced. * or perhaps I am extrapolating too much from a population size of one :)
Ah thanks, that answered it! I had always thought the TH just stripped off the _ so I was expecting something like class HasVec2 ... where vec2X = ...
I assume this only works if fields with the same name have the same type as well?
`makeLenses` and `makeClassy` just strip off _. `makeFields` is a separate beast. It makes classes per field rather than one class for the entire type, and needs the extra flexibility. I tend to use `makeClassy` to make nested related functionality, because I personally find `makeFields`, just one step too ad hoc, but I try not to let personal value judgments like that lock functionality that others find useful out of the API. 
Nope. =) class HasFoo c e | c -&gt; e where foo :: Functor f =&gt; (e -&gt; f e) -&gt; c -&gt; f c The functional dependency there is enough to let them vary per container. In `lens HEAD`, you can even make it so they can support typechanging assignment like the ones from `makeLenses`.
I still don't understand you. Perhaps using Nix has coloured my vision here. In nix I don't think of packages as installed or uninstalled. Packages may be cached or not-yet-cached in the nix-store, but since the cache is transparent I don't think about it much. In Nix, an application or library is specified by by a nix-expression, which is effectively a purely-functional make language. In theory, one can simply edit one's source files and run nix-build (instead of make) which will build your software artefact according to the expression in default.nix (analogous to a Makefile). Prebuilt components that already exist in the nix-store (the cache) can be used when available, and the cached components are specified to whatever granularity you specify in your nix-expression. In particular you could cache down to the level of an object file. The point is that, in this way, you don't need to install or uninstall anything. You do not need to set up a build environment (beyond being in a particular subdirectory) because the default.nix file specifies the entire build-environment needed (usually by importing a version of the nixpkgs module). That said, I've never used nix as a make program like this, so I'm blowing a bit of smoke here.
There are plenty of Haskell web frameworks, small and large. They are not usually referred to as GUI frameworks though. 
I do mean GUI in the sense of (ab)using the browser's graphical capabilities. In contrast, web frameworks are mainly concerned about the backend.
The usual problem with Windows and open source projects, not limited to Haskell at all, is that the platform has the worst contributor to user ratio and at the same time is the least compatible with other platforms (i.e. the most work). 
GUI libraries are usually huge. There are very few of them and almost no languages have native ones. Most use bindings to other languages' GUI libraries, as does Haskell. Wrapping C++ libraries in idiomatic code in particular is painful because you can not use half of its features via an FFI. 
Because shachaf told me so. Seriously though, I would hesitate to use it often because you'll end up littering your code with typeclasses- one for each overloaded field. I haven't actually run into any case where I need to use it, so my advice would be to use it sparingly.
For that you will most likely need a Javascript library. Of course in the future projects like Fay might include Haskell bindings to those though the more obscure your use of the browser is the less likely they are to be in there anytime soon.
The first question is why is Shake not suited to what you want to do? Do you think some features you don't use will introduce overhead? Does it miss some feature you need? As someone who has implemented three different build/dependency algorithms in Haskell, it's not trivial to do, and even harder to do efficiently! If you can reuse Shake, I would. Note that Kansas Lava already uses Shake as a testing system, with good results. The logic you are looking for is in Shake.Development.Database.build. I do not expect anyone to understand this code, but it corresponds to Figure 5 of the paper. Things block by entering the waiting state. You might be expecting a thread, and an MVar to do the blocking, but it doesn't work that way - instead I use the Pending field of Waiting to keep a list of actions that "awaken" the dependent actions. Things do not run in build, but instead get moved in the the thread pool to run separately. The Diagnostic output should tell you every state transition that happens, which is the interesting bits. I wrote that code over a couple of months, it's incredibly imperative, and highly sensitive - if you do decide to do things yourself, I suggest you do something much simpler.
&gt; Oops! The server couldn't find the requested information (/presentations/Type-Functional-Design). 
I think they are great. I am still on the fence wrt using Haskell in a startup. One part is the language, and as a programmer I have a good feel for that part. The other part is how organizations train or hire Haskell programmers, how fast they could turn around, how hard it is to recruit etc. Those are the wildcards. I am pretty confident regarding the technical sophistication of the ghc compiler for example.
I haven't used nix, but my experience with using cabal to develop large multi-package projects suggests that you absolutely need to install and uninstall things. (Install/uninstall here are equivalent to adding/removing from a package cache.) The package cache can't be transparent (i.e. you will care about removing things from the cache) because without the ability to remove things from the cache, you'll eventually have more than one incompatible instance of same version cached and you'll have to decide which one you want. In my typical use, one of those instances will be "old" and not wanted any more, in which case I want to manually remove it. So my point is this: purely functional nix-style package systems may solve a number of problems, but they don't eliminate the need to uninstall things manually.
All I really remember from "Write Yourself a Lisp..." was how to parse in Haskell, which blew my mind. So I begin reading "Write Yourself a Haskell..." and it begins by ducking the parse. Oh my, I remember now. That's why I switched from Lisp to Haskell. This isn't really fair. Bigloo Scheme has a really nice input parser, if not as conceptually slick as Haskell parsing. I used it to eliminate most parentheses in Lisp. (Is this like asking a work elephant to hand you the ankus?)
The `Rule` monad isn't even a proper monad, it's just a monoid (ab)using the monad syntax. Things are encoded in the `Action` monad, and the whole thing then gets interpreted by a rather intricate and evil thing ndmitchell already described, with help of an intricate matching scheme tying, in this case, filenames to filename patterns (it's actually completely general), all the info needed for that gets encoded in the `Rule` monoid. If you want to customise anything, don't go further than the matching scheme, for beyond that be dragons. The actual magic that happens in that example, which is what sets shake apart from every other build system I know of, is that you can fire rules depending on `contents`: Because shake dependencies form a monad, not just an applicative functor.
I know of two other build systems that also form monads, Redo and Ninja (http://neilmitchell.blogspot.co.uk/2012/02/four-interesting-build-tools.html)
I've been using Idris recently, and I must say it is much easier to do ordinary programming with than other dependently typed languages. I like it a lot. I also think having partial functions is a good thing, since partiality is unique in that type systems that prevent it must exclude some correct programs (with no universal way to recover them). My bigger concern with Idris and totality is that * the totality checker doesn't seem integrated in to the type system. I want an effect system for termination. I don't really want a totality checker separate from the type system. * the termination checker is easy to fool. That is, it is not really very hard to get programs to loop forever to pass as total. Non termination in dependently typed language has a long history. NuPRL has partiality types, and these turn out to be beneficial. 
Well its good to know there is a bit of black magic in the backend, and not just my own inability to read the code. I've only been obsessing over Haskell for a couple months, so a mature codebase can be pretty intense at first parse. Its good advice to use Shake -- The more I read the code the more it seems I don't want to reinvent that wheel. It should be relatively easy to customize, so I'll keep it in mind. Thanks!
Currently you can't 'cross a chasm in two jumps' to change the type of something like data Foo a = Foo { __x, __y :: a } makeLenses ''Foo so Foo 1 2 &amp; _x .~ "hello" &amp; _y .~ "world" will fail. [We're adding something to fix it soon, though!](https://github.com/ekmett/lens/issues/197) Then you can use makeExposed ''Foo and you'll get the ability to do &gt;&gt;&gt; Foo 1 2 &amp; exposed %~ \v -&gt; v &amp; _x .~ "hello" &amp; _y .~ "world" Foo "hello" "world" This will likely be in in the 4.0 release as `glguy` already has [a working implementation](http://hpaste.org/81965).
Does anyone else have trouble understanding this talk? I'm not sure the example he gives is good, the slides are very uninformative by themselves, and I really can't follow what he's trying to accomplish. And I say that as someone who has seen Haskell before.
I had searched for this a few months back and then just switched to using Ruby instead. I'd like a decent library but don't need it badly enough to undertake the big project of building one myself.
Why are you implementing `map` inline inside `describeAnimals`? 
Bugger, I hoped to cash in makeFields for glorious imaginary internet points, but you beat me! (I implemented it) Good to see people like it though.
Methinks you want dependant types.
From the code: &gt;However, we can encode several interesting properties on the type-level by promoting the Piece type to a type-kind and thus, X and O into types. This is done by the language extension DataKinds which was introduced in GHC 7.4.1. From the docs: &gt;With -XDataKinds, GHC automatically promotes every suitable datatype to be a kind, and its (value) constructors to be type constructors. Haskell is so slippery. Every time I think I can pick up a piece of code and start reading it I get hit with something that I haven't seen or learned before. It seems as though others accept and welcome all these constant new additions more easily. Do I have the wrong kind of brain for Haskell that I find this very frustrating ?
It is working now.
Shake is 828 patches, 3.7K lines of code and 1.1K lines of tests, which I've been developing for about 5 years. It's not black magic (it doesn't do anything funky, unsafe or with advanced language features), but it is intricate and has been refined over a long period of time. Good luck with using Shake, do let me know if you get stuck (or ideally, ask on http://stackoverflow.com/questions/tagged/shake-build-system). And do post something if you get anything working!
You can be extremely productive in Haskell using only libraries that use only a few modest extensions. You can even be productive sticking to Haskell98 although it can get a little clunky. I would not use a library that depends extensively on DataKinds to do anything. When *I* try to use something more complicated than multiparameter type classes or rank-N types, I usually end up refactoring my code to not use the extra extensions anyway. The most complicated extension I am comfortable using is data families.
s/Haskell/GHC
The problem that I have is that this kind of crazy thing in Haskell seems just accessible enough that it seems like I should understand, instead of like with C++, when I see some crazy template meta-programming, where it's obvious that it's beyond the comprehension of mere mortals.
Fixed and uploaded as smallcheck-1.0.1. It now builds with GHC 7.4.
But Lisps (Scheme, Dylan, Common Lisp, Clojure) are completly different: are strictly evaluated, feature macros, s-expressions, not purely functional... You can also say that C++ is a Fortran because Fortran -&gt; ALGOL -&gt; Simula -&gt; C++
And if you group species based on common descent, describing groups by what the shared ancestor was like, humans are a variety of fish, like all other vertebrates.
You're welcome!
At this point, the ML lineage is clearly its own thing. But if one was thinking about Haskell in e.g. 1994, placing it in the broader lisp family would have seemed way less weird. And while we have working definitions of species and semi-working definitions of genus, and on the other side, kingdom is pretty clear, things like class and order, as i recall, are somewhat more arbitrary.
MLs are strictly evaluated too. Arguably core scheme isn't about macros (though it has them!). S-expresions are just syntax (or lack thereof). MLs similarly aren't purely functional. ML is really close to a subset of scheme + types, actually. And Haskell is certainly in the ML lineage. So getting all hung up on how different they seem now tends to obscure how close they all used to seem, and how close the are to one another relative to e.g. prolog or BCPL-lineage languages. These days I can use racket to toss on optional type signatures and add a lazy extension to the language and come _way_ closer to Haskell than 90% of the languages out there.
that was a nice look at a real-life use of phantom types
Which it's a cool book, does it really deserves to be burned or moved to the thrash can? 
I hear [xmonad](http://www.haskell.org/haskellwiki/Xmonad/Guided_tour_of_the_xmonad_source) is something nice to look at. 
"Type Classes: An Exploration of the Design Space" introduces motivating examples for MPTCs including the state and reader monads. As I recall, while they have fundeps in the current `mtl`, they don't really need them for a wide variety of use cases. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.5674 This classic Mark Jones paper is cited as the origin of these examples: http://web.cecs.pdx.edu/~mpj/pubs/springschool95.pdf 
The problem is that existential types allow you to create, say, a list of items of different types but all belonging to the same typeclass. If you call a method on a member of that list, you need to know the concrete type, but you don't know it at compile time (because it's a heterogeneous list), so you need to tag the entry somehow to remember what the type is (or at least keep around a reference to the appropriate method table).
Well, it's not completely pointless. We know that floating point numbers take memory to store. We know that closures with free variables take memory to store. We know that a record of N closures will take proportionally more memory. If we can avoid allocating N closures for each list item, we can save that much memory. (Granted, most of the time we don't care, and issues of maintenance, readability, etc.. should take precedence, but sometimes it does matter how much memory we're using.) We might not know, without intimate knowledge of GHC internals, exactly how much memory we might save by storing our spheres one way or the other, but at least we can say that the best we can hope for with a list of records of N closures is whatever it takes to store N closures in addition to the free variables (i.e. the center and radius of the sphere), and that doesn't sound very good to me.
I will definitely check it out, thanks!
I think with haskell libs and novel GHC extensions you get a different, and more useful, kind of complexity; rather than something that's simply a confusing amount of indirection, you are increasing or adding new kinds of abstraction which make your language more expressive, but make your brain hurt at first.
Awesome!!!!
&gt; However, we can encode several interesting properties on the type-level by promoting the Piece type to a type-kind Is it possible to write out what that would look like without the DataKinds extension? Something like data X = X data O = O I would guess? Edit: Actually, reading ahead a bit more, it sounds like it would be more like data X data O and yet the original `Piece` definition still exists. So `X` and `O` are types (when they appear in type signatures, etc.) and values (when they appear in other code). Am I close to making any sense at all?
Oh bugger. I wrote (v ~ HasX Vec2 Double) =&gt; Vec2 in the file I ran through GHCI. I'm just going to change the example to read differently. I'm not sure what you mean by `HasX Vec2 Double v =&gt; v`, though. Isn't that adding an extra type parameter to the class? I guess I could do `(t ~ Vec2, HasX t Double) =&gt; t` (which is sort of what I was intending to show with the original), but instead I think i'll just use a function as an example :p
template meta-programming gets talked up, but really it isn't so bad. It just has terrible syntax. If you think of templates as (pure) functions from types to code, with specialization used as a form of pattern matching, it really makes more sense. Write your template code in Haskell first/translate other peoples template code to Haskell to understand it. The code only undergoes limited syntactic checks prior to template instantiation, so your type errors happen late in the process. Templates are a lot like macros. C++ has a lot of complexity, and template meta-programs often run into this complexity, but mostly that is not from templates per say, but just that templates reveal it. For example, function overloading is super complicated (name lookup rules?) in C++, and templates often generate code that depends on these complex name lookup rules. But, this is because overloading is complicated, not because templates are complicated.
`CyclicTurn` is a constraint on turn. It says that `turn` can be reified, that `Other turn` can be reified, and that `Other (Other turn)` is just the same thing as `turn`. It means essentially the same thing as class (ReifyPiece turn , ReifyPiece (Other turn) , Other (Other turn) ~ turn ) =&gt; CyclicTurn turn instance (ReifyPiece turn , ReifyPiece (Other turn) , Other (Other turn) ~ turn ) =&gt; CyclicTurn turn
yes. DataKinds is not necessary and you can always replace it using this technique. But, doing it this way uses the kind `*` as a sort of grab bag, much as if the language only had a single type and we just added constructors willy nilly. DataKinds makes type level programs more strongly typed.
That book was the first haskell book I read and I became to love haskell since. Great book! 
Really? Thought it was Japanese ... also, all the domains are .jp...
Well yes, that was my point. A classification that's unarguably accurate in a broad enough historical context is not necessarily useful (or sensible) for classifying things as they currently exist. It also remains true that the Lisp and ML families of languages are, at some level, dialects of lambda calculus, and are more similar to each other than to many other languages; likewise, compared to a cuttlefish, goldfish have a great deal in common with weird terrestrial lungfish like you and me.
&gt; I'm not sure what you mean by HasX Vec2 Double v =&gt; v, though. Isn't that adding an extra type parameter to the class? Clearly what I meant was that I was sleep deprived when I wrote it. :-) At the time I thought that you were saying that v was an instance of the type HasX Vec2 Double (though obviously in retrospect Vec2 is the type of v). I know that this is not what ~ means, but I thought you were using it incorrectly because I couldn't figure out why else you would be equating v to a constraint; incidentally, what is the point of the v ~ in the clause?
The core engine for nikki and the robots was developed open-source: http://www.joyridelabs.de/game/code/ just visiting that page, I notice they've stopped working on the game. Sad to see :-( 
Thanks! Too bad it's not actively maintained, but it seems like a valuable resource all the same. 
&gt; In fact, since you mentioned a non-essential equality constraint, I'm going to make a wild guess that you're using the good old post-hoc unification trick to avoid the aforementioned ambiguity headaches. :] You're correct :)
It is Japanese. 
As you might have guessed, I'm part of the crowd that gets really excited by new language features. :) That said, I'd be a lot more conservative with language extensions if I was writing code for production use, but when I'm just playing around it's fun to experiment with all the shiny, new toys!
Someone should make a "24 days of ghc extensions" series :-) 
&gt; I also think having partial functions is a good thing, since partiality is unique in that type systems that prevent it must exclude some correct programs (with no universal way to recover them). Could you elaborate on that? I'm not familiar with Idris (or PRL)'s support for partiality, but I naturally see myself using a "Wait more" non-terminating monad (like Conor presents them for example). One may be tempted to have a different monad that says either "Result foo" or "Loops", and I understand what you're saying as a remark that the, contrarily to state or exceptions, the `run` of this monad cannot be internalized, implemented in the language itself. But the "Wait more" monad can, that's the fuel hack. (That feels related to Aaron Stump's remark that reasoning on non-termination really likes the excluded middle.) (Hint: a great opportunity for a blog post!)
In the case of Haskell, I would not actually recommend this approach. After all, Haskell is not your standard imperative programming language where everything follows more or less the same patterns. To learn both the idioms and the huge range of possibilities, I would recommend to focus on code specifically made for presentation instead. Here is the code where I learned Haskell from: * The [Prelude](http://ww2.cs.mu.oz.au/172/Haskell/tourofprelude.html) * [Evolution of a Haskell programmer](http://www.willamette.edu/~fruehr/haskell/evolution.html) * [Examples packaged with the (former and now very dead) Hugs interpreter](http://cvs.haskell.org/Hugs/pages/downloading.htm) (Sorry, I don't know a direct link) * [Functional Pearls](http://www.haskell.org/haskellwiki/Research_papers/Functional_pearls) A huge collection of Haskell "tricks", one more impressive than the next. For the "boring tasks" you mention, I think that they are very well covered in Real World Haskell. 
You might want to read [this Stack Overflow answer of mine](http://stackoverflow.com/questions/14617815/different-types-in-case-expression-result-in-haskell/14619425#14619425). The key idea is that a lot of "OOP" is best expressed in Haskell by defining a record type that contains function members. This record type definition is analogous to an OOP interface; values of the record type are analogous to "objects" that implement the interface. This doesn't have implementation inheritance like your example does, but implementation inheritance is evil. It also tends to force all your "objects" to share the same uniform interfaces, but that's goodâ€”it basically forces you to compose instead of inherit.
I think [hpaste's source](https://github.com/chrisdone/hpaste) is pretty easy to grok and the application is perfectly ordinary and boring.
Too bad it's not the 16th century, and I can't burn *you*, because you seem to take programming languages too seriously.
This is very old.
This looks really impressive and well thought out. Wouldn't it make sense to rename `smallCheck` to `smallCheckWithDepth`, or something, so that it's self documenting, and you don't need to add comments at use sites about what the argument means? You lose the nice touch of having an eponymous function, but... well. I noticed that the `Serial` and `CoSerial` instances for `[a]` don't have a `Monad m` constraint, while the instances for `Maybe a`, `Either a b`, and so forth, do. A superficial reading of the source code doesn't suggest a reason for this. Is there one?
Also from types to types.
Would there be a reason to sometimes use `&gt;&gt;=` and `&lt;*&gt;` for `Series` instead of the fair `&gt;&gt;-` and `&lt;~&gt;`? If not, maybe it would be an idea to not derive `Monad` and `Applicative`, and manually write instances for `Series` that use `&gt;&gt;-` and `&lt;~&gt;`?
&gt; This looks really impressive and well thought out. Thanks! &gt; Wouldn't it make sense to rename smallCheck to smallCheckWithDepth, or something, so that it's self documenting, and you don't need to add comments at use sites about what the argument means? You probably won't put `smallCheck` into your code â€” you'd use a testing framework for that. It is mainly for playing in ghci, and short names are preferred there. I thought about making `smallCheck` use some default depth just like QuickCheck does, and having `smallCheckWithDepth` as you suggest. The downsides are: 1. You often want to try different depth â€” so if you started with `smallCheck`, you then need to go to the beginning of the line and change it to `smallCheckWithDepth`, which might be a bit annoying. 2. It's hard to come up with a "one size fits all" depth. For Integers, 5 is a modest depth (which covers numbers from -5 to 5), but for lists, trees or functions it may be infeasible. &gt; I noticed that the Serial and CoSerial instances for [a] don't have a Monad m constraint, while the instances for Maybe a, Either a b, and so forth, do. Good point. I've just removed Monad constraints where they are superfluous. It doesn't make any practical difference, however, because in all those cases the constraint is implied by other constraints anyway.
That was my first idea as well. Alas, such instances do not obey the Monad/Applicative laws. And many people have a strong opinion about that :)
I guess that is because the order of the results change? You could make the order unobservable by returning a set. But then you need the order, which you could fix by returning only the elements for the actual depth, i.e. for Integers for depth 5 you'd get a Set containing only -5 and 5. But that's probably quite a big change.
&gt; I guess that is because the order of the results change? Yes. &gt; You could make the order unobservable It's hard, because the order affects which counterexample will be printed. Regarding the set idea (if you literally mean Data.Set or some other sorting mechanism): 1. What if the elements are incomparable? 2. We no longer can generate examples on demand. What if there are a large number of examples? Then it harms both the time and space efficiency.
The default smallCheck could just have a limit on the number of testcases and report the depth searched: Î»&gt; smallCheck prop_abc Ok, stopped after 1000 tests, tested all values up to depth 3.
Awhile back I wrote a [StackOverflow answer](http://stackoverflow.com/questions/4369962/what-are-some-good-example-haskell-projects/4370489#4370489) with a number of my recommendations.
I once needed a feature in [pandoc](http://johnmacfarlane.net/pandoc/), the wonderful text converter. I had just learned the basics of Haskell, but I still managed to add a unit test and code the feature (extending the markdown parser). The project is active, so the main author incorporated my patch after cleaning it severly. I learned greatly from this episode.
If a series would return only the elements for the actual depth, would you still need fairness?
This is not trivial to achieve in general. For example, a list of depth `n` would be either a list of length `n`, or a list of smaller length where at least one element is of depth `n`. Generating values of depth `&lt;= n` is much more natural for most of the types.
The mind of a Japanese! And I'm sitting here, unable to parse a textfile!
Sounds reasonable... I've recorded it as https://github.com/feuerbach/smallcheck/issues/13
We have a couple of these in `lens`. In particular class Conjoined p =&gt; Indexable i p where -- | Build a function from an 'indexed' function. indexed :: p a b -&gt; i -&gt; a -&gt; b With instance Indexable i (-&gt;) where indexed = const That can be instantiated entirely with 98+MPTCs. Now, the raison d'Ãªtre for the class is that I can then put constraints on `i` in the instances. instance i ~ j =&gt; Indexable i (Indexed j) where ... which gives us a 'conditional' functional dependency.
As cool as this video is, the name is sort of misleading. It's a 2 minute long video showing the final product, and that's basically it. I think it's awesome someone was able to make this in Haskell but I don't think it should be titled like a documentary-type video about making it, because it isn't.
Also, the code is 1600 undocumented and uncommented lines... Even though it's cool, it cannot be called instructive...
&gt; It seems as though others accept and welcome all these constant new additions more easily. Do I have the wrong kind of brain for Haskell that I find this very frustrating ? No, Haskell 98 is a wonderfully powerful language, and you can enjoy it very easily without having to read any document other than the official language spec. You might have "the wrong kind of brain" for Haskell-y blog posts, though, which do tend to geek out over the new GHC extensions.
&gt; We're adding something to fix it soon, though! Oh wow, nice! Needs moar blog post.
This is really cool. One of the few things I like about Haskell is the lack of extensible records. This is a huge step in that direction while maintaining type safety and all that jazz.
Why can't you parse a text file?
Those new GHC extensions aren't meant for everyone. Its often experimental stuff that is on the bleeding edge of types and language research. It would be nice if they had a better way to tell what extensions are finally mature enough to be "safe to use" though, specially in a "this is useful and easy to use" sense.
&gt; you'll eventually have more than one incompatible instance of same version cached and you'll have to decide which one you want. You decide what you want by specifying precisely what you want in some language (nix for example), not by virtue of happen to having it in a cache. I get the impression you are criticizing "purely functional package management (encoding dependencies in a hash)" without ever experiencing it. (Not that I think nix solves all problems or is without fault.)
I enjoyed watching this live coding session on YouTube in which the author creates a simple Sokoban game: http://www.youtube.com/watch?v=mtvoOIsN-GU . It's actually pretty relaxing to watch ;-)
It depends on my application, but I often use `import qualified Data.List as List`, or `import qualified Foo.Bar as FB`. In fact, I use it often enough that I have a snippet set up to autocomplete it for me. I would say that your rant is more against haskellers, not against haskell itself.
&gt; Haskell basically makes the ill-advised option 4) the default behavior That behavior requires the fewest syntax tokens to specify. Thatâ€™s not the same as a default. You donâ€™t have to crack open a config file or specify a compiler directive to get the behavior you want. You are perfectly free to qualify all your imports if you wish. Another thing you can do is to list the functions you want to import explicitly. Like this: import Data.List(nub, intersperse) Then you know where they come from and you only import those functions you are going to use, relieving namespace pollution. Although Iâ€™m really tempted to suggest that if you are worried about namespace pollution you may be writing modules that do too many things at once i.e. import a lot of stuff i.e. arenâ€™t very modular. Or maybe youâ€™re just used to a language where importing lots of things is the norm. The thing I donâ€™t understand is why qualified imports without the "as" clause are even necessary to state at all. Couldnâ€™t the mere use of, say Data.List.nub, automatically import that function. There may be an explicitness or deliberateness argument why this isnâ€™t already done. The canâ€™t say the same for qualified-as and and unqualified imports.
I'm very happy with namespace pollution as the alternative Just.Looks.ugly Especially.For.Operators.Like.++.
&gt; The thing I donâ€™t understand is why qualified imports without the "as" clause are even necessary to state at all. Different packages can use the same hierarchal path so you could install two different packages which both have Control.Foo. You can specify the which package you want on the command line -package or you can use [package qualified imports](http://www.haskell.org/ghc/docs/latest/html/users_guide/syntax-extns.html#package-imports) which I think is in ghc 7.6 and up. Now maybe there should be an official body that approves module hierarchical names. I am not sure what the correct choice is no this yet.
The alternative is not fully qualified names for everything but explicitly saying that you are importing (++) from "Just.Looks.ugly Especially.For.Operators.Like".
True but most of what I import does not tend to be operators.
&gt; import Foo.Bar (baz, wumpus, zebra), though it pointlessly requires you to add parentheses -- and is also uncommon to see. I believe that this is the best solution in most cases, and I don't think this is uncommon at all. This is how I always do it, and my editor almost automates these selective imports. Why do you prefer qualified imports to selective imports by default in Haskell code? In my experience, nearly all situations where I want to use two things with the same name from different modules, the things are part of the same type-class anyways and unaffected by import syntax. (&lt;|&gt;) from Control.Alternative and Parsec is a note-worthy case. Both versions work on the same types but have different semantics. For that reason, I wouldn't import it qualified from Alternative anyways.
I have made it my coding convention (and evangelise this about) to always import *and* export with an explicit list, unless doing a qualified import. With the exception of modules from the Haskell report.
amen
Is there any project in particular that you would like to name and shame for poor import practices?
&gt; That behavior requires the fewest syntax tokens to specify. I hate typing, and would hate to give *everything* a namespace. Hell even Perl lets you bring stuff in unqualified.
I completely disagree and I quite like Haskell's import syntax. The only thing I would change is make import qualified Data.Char as C into import Data.Char as C
"the whole thing"
Throw a rock in the air, you'll hit one. I don't want to shame this one since it obviously took a lot of work and is impressive, but it *is* the one that finally prompted me to write this: the Mario Bros one that [came up on this board](http://www.reddit.com/r/haskell/comments/18v4ep/making_super_nario_bros_in_haskell/). Take a look at [Main.hs](http://svn.coderepos.org/share/lang/haskell/nario/Main.hs). Lots of imports, all "naked".
I just read your post again and I don't see where you mention to change the syntax like that. You just mention "more cumbersome syntax" but you aren't specific about that change.
code usually isn't written to make first time reading easier. you are essentially asking for all readers to tolerate extra name qualification so that someone unfamiliar with the code has navigate it easier. i favor making very productive people more productive, not enhancing discovery for less productive people. i dont want to type extra name all the time so that someday somebody will be able to avoid hoogle. 
The second allows the prefix to be optional, which some people will want. It's not like you type qualified with a high frequency.
I think a program that automatically generates the imports could easily be written. If something is ambiguous you have to qualify it anyway. If it's unambiguous it's easy to for the program to know where the function comes from. Heuristics could handle common abbreviations such as knowing 'List.' requires Data.List to be imported qualified as List. Eclipse for java has ctrl-shift-o to automatically add imports for a function and if a program for generating imports existed then it would be easy to do like eclipse does with whatever editor you use.
&gt;code usually isn't written to make first time reading easier. I'm just describing the Python convention. Is your claim that Python's entire import system is optimized for newer readers of the code at the (excessive) expense of people who already know the code? If so, would you say that most of the files you read (and most of the learning curve for projects) are files you're reading for the 1st/second time, or files you're already familiar with? Do you typically just work with the same code all the time?
I have been using this approach for a long time, but recently I've made a small shift. I now always write imports like this: import Data.Map as Map (insert, insertWith, toList) import Data.Set as Set (insert, union, singleton) This way I can always find where symbols come from, and I can use symbols unqualified as long as they don't collide with anything. And if they do collide I have a short prefix to apply.
In fact, actually two things: 1. No one ever intentionally uses `import Foo as F`. In basically all cases where this is done, the author really meant `import qualified Foo as F` and just forgot to say `qualified`. 2. Hardly anyone uses `import qualified Foo` either, and the few cases where it's desired could be fixed using `as` syntax. Or just automatically trying the import when a name is qualified by a constructor name that's not in scope. I think it would be nice if Haskell released the word `qualified`, and just automatically qualified the first form, and didn't require an explicit declaration for the second. Of course, the change would require years... first add an extension, then add a warning for users of syntax that's going to change meaning, then make the switch, with at least a year or more of migration time between steps. Maybe a few warts can be lived with. :)
&gt; Edit2: Ah, one of those days: Make a non-responsive comment that ignores virtually the entire post? Get voted up. Point out that the commenter did this? Get voted down. This has probably less with that you pointed it out and more with how it came across when you pointed it out.
In general, I think there's a higher standard for universality of operators for precisely this reason. They can't be used qualified (at least by sane people; technically it's possible of course...) so when defining an operator, it's worth asking whether someone who isn't familiar with that operator ought to be stopped and sent off immediately to find the context they are missing. That's fine for widely used stuff, but it's one of the issues I have with the lens package, for instance.
How does Control.Alternative.(&lt;|&gt;) have different semantics from the one in Parsec, when the one in Parsec defines the one in Alternative as being equal to the one in Parsec?
That is incredibly short sighted. Half the time you will come back to your own code 6 months later and not know what is going on. Congratulations, you've just turned the 3 seconds it would have taken you to explicitly name the import into half an hour of stuffing around trying to discover/understand your code again for what might be a simple modification that would have only taken a couple minutes. I'm not even mentioning making your code readable to other people is a worthy goal in itself.
I don't think this is related. Requiring an explicit import statement doesn't help with the problem of multiple packages that define a Foo.Bar module. The qualified import statement doesn't identify which package was meant either. To do that, you have to resort to the PackageImports GHC language extension as you said. Then, yes, you'd need to state the import explicitly, but only because it's NOT just a simple `import qualified Foo.Bar`.
Which package contains Language.Haskell.Liquid.Prelude?
&gt;Why do you prefer qualified imports to selective imports by default in Haskell code? For the reasons I gave related to namespace pollution and ease of tracking the source of new/unknown functions.
You are right, I did not recognized how orthogonal two issues were when writing my comment. Thanks for the correction.
&gt; ...so you need to tag the entry somehow My point is: the instances themselves can serve as the "tag". There is no need to require type literals to serve as the tag; moreover, if you did use the type as the tag, then you'd have to perform instance lookup repeatedly (whereas using the instances as the tag allows you to share the lookup among all use sites). &gt; (or at least keep around a reference to the appropriate method table) Which is precisely what I suggested as the replacement for passing type literals around.
Another good example is giving the proper type to [realToFrac](http://hackage.haskell.org/packages/archive/logfloat/0.12.1/doc/html/Data-Number-RealToFrac.html): class (Real a, Fractional b) =&gt; RealToFrac a b where realToFrac :: a -&gt; b This is not contrived at all, because we do occasionally wish to convert between `Float` and `Double` in a way that preserves their semantics wrt infinities and NaN. To say nothing of the more general case.
&gt; For the reasons I gave related to namespace pollution and ease of tracking the source of new/unknown functions. But if you use a selective import, then you can easily track the source of the function, and there is no namespace pollution because you only add the names that you mentioned explicitly.
&gt; Edit3: Butthurt alert! Someone's run through and voted down everything I've said here... Perhaps it has something to do with the abrasive and sometimes obstinate nature of your replies? I largely agreed with your original post, but I have nonetheless been downvoting many of your replies because they have frankly been obnoxious. If you were merely politely and reasonably disagreeing with the people here you would not be getting such a negative response to your replies.
I misinterpreted his comment as a criticism. My bad.
Now I don't know what you're asking about. What I proposed was Haskell adopting python's conventions and defaults (for module imports). That means you have the option *either* to bring functions into your namespace prefix-free (from X import Y), OR to bring in the whole module but use the prefix (import X). Yes, both have their advantages. Yes, if you do prefix-free you should be explicit about what you're importing (rather than use Haskell's `import X` or Python's `from x import *`. I *thought* you were asking about why the minimal import statement should default to qualified (like in Python), but now you're just repeating back things I agree with. So, what's your question, and where do you think you disagree with me?
It happens. :-)
There are three things you can do in Python. First, you can important a module either fully qualified, import fully.qualified.module or partially qualified, import partially.qualified.module as module. You can do the same thing in Haskell via respectively import qualified Fully.Qualified.Module or import qualified Partially.Qualified.Module as Module Second, you can import selected names from a module, from module import x, y, x which you can also do in Haskell via, import Module (x,y,z) Finally, you can import all names in a module, from module import * which you can also do in Haskell via, import Module So in short, everything you can do in Python you can also do in Haskell, so I am not sure why you seem to be saying otherwise unless I am misunderstanding your claim.
You would probably have learned the answer in my original post, where I explicitly said all of that (that you can do the Python way in Haskell) and then went on to articulate the problem. Here, let me give you the excerpts that would have answered your question before you joined the discussion, had you read them. &gt;&gt;**it seems to be standard practice (and is a language default)** to simply say: `import Foo.Bar` ... This is REALLY annoying **when trying to read** someone else's code &gt;&gt;[...] &gt;&gt;Haskell basically makes the ill-advised option [of dumping an entire module into your namespace] the default behavior. ... I don't know if the more cumbersome syntax ... is the real cause, but in practice, **I do see most Haskell code using the ill-advised "import Foo.Bar".**[bold added] Notice how those points -- about Haskell making the good option unnecessarily cumbersome, about Haskell users mostly using the bad import option, about this decreasing readability possibly *because* it's cumbersome -- apply *even though* it's possible to do the Python way in Haskell? Do you see how your confusion would have been extinguished if only you had actually read the post you were replying to? This tactic generalizes nicely as a way of enhancing your understanding.
It seems to me that the second form was just added in the name of orthogonality, rather than because anyone sat down and decided it was useful. While it's nice that the meaning of `as` and `qualified` can be understood independently of each other... honestly, when was the last time you wanted to *both* declare a short and concise qualified name for things *and* dump them all into the global namespace? I think it would be an improvement to have GHC warn that you probably meant qualified, in case you accidentally use the latter form.
So what does the current import Data.Char as C become? I don't do that by itself, but recently I started saying import Data.Char as C (ord, chr) 
Fixity/associativity :-(
This response really makes you sound like a jerk. I don't know if that was intended. I'd understand if you're frustrated since so many people (myself included) are responding along the lines of "we really don't like qualified imports because it's more typing". In Python you need the word from. In Haskell you add parenthesis. And in general Haskell programmers don't like to type. So that's why we don't do qualified imports.
Here's an idea! Many parts of GHC are exposed as a library. You could probably write a program which finds unqualified imports and changes them to the import that uses explicit parenthesis, or if you're really ambitious, a qualified import while updating all of the code. Using that, go submit a bunch of pull requests to the authors of the libraries.
Regrettably, the liquidhaskell package itself. (One can specify an include directory) the default is something like this: https://github.com/ucsd-progsys/liquidhaskell/tree/master/include/ (with specs for various basic, prelude, libraries, e.g. GHC.List, Data.Vector, etc.) We need to figure out a cleaner way to integrate with cabal and such...
&gt;This response really makes you sound like a jerk. I don't know if that was intended. I'd understand if you're frustrated since so many people (myself included) are responding along the lines of "we really don't like qualified imports because it's more typing". No, you wouldn't be understanding anything, because no one seems to be responding that way, and if they were, it would actually be responsive! My frustration is that I say, "hey, in Haskell, you can do it this way, but it..." And then everyone responds with, "Wrong. In Haskell you can do it that way. What's the problem?" Again, if people aren't going to read what they're responding to, why respond at all?
Really? Was the writing not clear, or -- be honest -- did you just not read the parts I excerpted before responding? When you read those parts (you didn't, but let's play along with your excuse for a minute), what about it was unclear? If it was unclear, please articulate for me a *clear* way of writing that so as to head of these misunderstandings. Then again, I can only fix "the writing was unclear". I can't do much about "I didn't read 95% of the post", which seems to be the case here. Ya know?
&gt; The instances of the IsList class should satisfy the following property: &gt; fromList . toList = id ['a', 'a', 'b', 'c'] :: Set Char -- Would this not break that property?
I think everyone hates a lot of Haskell's syntax quirks. About a year ago, I raged hard on this subject and came up with a good list of things I'd change: Kill record syntax. It's so terrible I would probably take a step in the direction of Agda and make records a separate form. Haskell syntax allows you to mix records with variants, but I don't think that's a useful idea. Plus, it introduces partial projections. (Life is better when functions are total). I'd kill the module system. (Aka, I'd add a real module system) A module can span multiple files. There's no need to write "module Foo where" at the top of every file. Export/internal/abstract flags should appear right above the appropriate top-level definition. No fucking export lists. I would combine type declarations together under a single keyword. Not "data", "type", and "newtype". I would probably use "new datatype" for newtype, "datatype" for "data", and ax "type" all together. Now grep works to find type declarations. I would probably swap some keywords and other things just to irk Haskellers. "where" and "data" are really juicy identifiers... why make them keywords? Again, there's no need for "module" to be at the top of your damn files. Typed Directed name resolution I'd think long and hard about including some kind of type-directed name resolution. It's one thing OOP does really, really well that FP absolutely sucks at. Especially when you start talking about nuanced datatypes, you quickly run into problems with needing recycled names. (The problem is even worse in dependently-typed languages... Agda solves this by letting you use Unicode in identifiers). (This is more an issue of not allowing ambiguous names... think about this more). Type class cleanups Type class instances would only be declarable in either the module the instance was declared or in the module the datatype was declared. No overlapping instances. No adding semantics unless you were the one to declare the class or the type. Possibly, too, you could pass local custom instances. All top-level terms must be declared with a type signature. Yes, Hindley-Milner is great. But just because total type reconstruction of your program is decidable does NOT mean I want to spend my time deciding it. I don't want to have to load your Git Hub source code into my REPL to get the types of shit. I want it there in front of me. Maybe this means the language comes with a tool that will annotate it all for you. Maybe you just need to man up and do it. It's not that hard. Infix Operators I'm not sure exactly how I'd do it, but I'd neuter infix operators. Operator overload can quickly lead to abuse, and declaring fixity levels is non-modular. I'd clean up the standard library. This isn't a syntax thing, and every Haskeller knows the Prelude is borked. And I wouldn't call it the Prelude. I would call it something manly, like System or Core. Also, for the rest of the modules, I would set precedence that module packaging should not be obnoxious. I don't want to ever see anything like System.Timeout -- a module with only one function in it.... Also not a syntax thing, but for the love of god, that function Monad m =&gt; forall a. a -&gt; m a.... I'm not calling that return. In fact, I'd make "return" a keyword. Unification Unification visualization tools. Unification is a double-edged sword -- extremely useful for saving you the trouble of writing out type-level arguments, but very aggravating when you can't figure out why its trying to unify two seemingly unrelated terms. I feel these are the "missing semicolons" of strongly typed languages -- beginner programmers are paralyzed until they learn to cope with these.
There is a good hackish workaround for when youâ€™re working with unfamiliar code. Change an identifier from `foo` to `fooX`. The compiler will helpfully inform you â€œNot in scope: `fooX`. Perhaps you meant `foo` (imported from `Data.Foo`)?â€
&gt; No one ever intentionally uses import Foo as F. In basically all cases where this is done, the author really meant import qualified Foo as F and just forgot to say qualified. Actually, I use it in combination with import lists. import Data.Map as Map (Map, lookup, fromList) import Data.Set as Set (Set, union, fromList) This way, i can use unqualified names, but distinguish with qualified names for the `fromList` easily.
The think that I don't like about that convention is that you have to include everything you are using from Data.Map, even if you qualify it with Map --- i.e., if you are going to use something like `Map.name` then you still have to list it in the import list even though the qualified name `Map.name` already tells you where it is coming from. That is why my preference is for the (slightly more verbose) import qualified Data.Map as Map import Data.Map (Map) and then just use `Map.X` for any `X` that is specific to Maps. (Of course, to some extent this comes down to personal preference, rather than there necessarily being a globally superior way to handle imports.) Edit: Formatting tweaks; added a parenthetical.
are you saying qualifying names would be the solution to "stuffing around" my code? that's what comments are for. how did anyone get anything done in C before C++ added namespaces? i expect each module to be documented with a top level purpose, important dependencies, and interface documentation. important functions are annotated with pre/post conditions. how do qualified name on imports improve code clarity. are you suggesting that the best way to understand some code is to do a depth first search through the call graph? so you need qualified names to make the search faster? no i'm not saying qualified names are useless, i qualify lots of imports, but certainly not all and probably not more than half. import qualified Data.Monoid why would you do that. but i would qualify Attoparsec, or containers imports so i haven't been convinced by the rule: "every imported function should be qualified"
Welcome to learning Haskell and this subreddit! However, it's not very good etiquette to post on here about things that don't drive discussion about Haskell itself or related concepts.
Hoogle can search any package, not just the Haskell platform. You can also generate hoogle indexes for your own projects. [See for yourself](http://www.haskell.org/hoogle/?hoogle=%2Bansi-terminal+setSGR).
Same here. Is there something about Python that causes this sentiment?!
It's clear to me that SilasX knows that Haskell's imports provide the same functionality as Python's. He's simply suggesting that Haskellers should use one particular form of import in preference to the others.
Code clarity is one of my primary aims when writing code. I love Haskell because it allows me to write more clearly than in any other language. I feel that unqualified imports, however, subvert this goal.
Thank you for the reply; I was hoping for such an explanation for the downvotes. Honestly I am suffering from sleep problems right now so it is entirely possible that my reply to him was off target but it is hard to tell through the fog. :-) In fairness I did deliberately end it with "unless I am misunderstanding your claim" since I considered it entirely possible in this case that I was the one who was misunderstanding him rather than vice versa. :-)
I think it's quite clear that SilasX knows how to write non-namespace-polluting code. What he's trying to suggest is that *others* do so too! (And I strongly support that)
&gt; What would count as "reasonably disagreeing" with someone who ignores 95% of your post and repeats it back to you as if it's a refutation? gcross provided an example of your communication style which is not well received: &gt; Edit3: Butthurt alert! Someone's run through and voted down everything I've said here... I do not understand what specifically you meant by this. What I perceived in the communication was a vague negative implication about person(s) down voting your comments. So what most people take away is the negativeness and little else. &gt; (Oh sorry ... &gt; Did you bother to read &gt; ... which I already noted in my observations all three comments above are often used in a baiting fashion or to imply something negative about that which they are replying to. If there are used that way a high percentage of the time then people are likely to interpret them that way. Likely, comments like these are not read as reasonably disagreeing because noticeable percentage of the time they are found in conversations the readers did not find productive. 
No, `toList` on that gives `['a', 'b', 'c']`, and then `fromList` gives you back the same set. It does break `toList . fromList = id`, which is probably why that way around wasn't included.
`toList x = [x]`? :)
Try `-fno-warn-deprecated-flags`.
I've been wanting overloaded whitespace, actually! Or at the very least something where you're if trying to apply something that isn't a function, but an instance of a typeclass 'FunctionLike', a call of 'toFunction' or somesuch gets inserted. I always want to use instances of the following types as if they were functions: data Iso a b = Iso { iso :: a -&gt; b, osi :: b -&gt; a } newtype IntPermutation = IP (Data.IntMap.IntMap Int) data Mapping g h where Mapping :: (Group g, Group h) =&gt; g -&gt; h -&gt; Data.Map.Map (E g) (E h) -&gt; Mapping g h
More polymorphism often translates to more safety, though, as the generality restricts incorrect uses of the code. It is much more likely to get this function correct: (a -&gt; b -&gt; c) -&gt; Map k a -&gt; Map k b -&gt; Map k c Than this one: (Int -&gt; Int -&gt; String) -&gt; Map Char Int -&gt; Map Char Int -&gt; Map Char String So even if there is only one use of that function that needs only the latter type, it is still better to write it in polymorphic form.
It might be nice to be able to change a function to receive a Set rather than a List, and not have to fix any of the callers that used literal list syntax.
You mean: Set.fromList, Map.fromList, or such, right? That could get redundant.
OP explicitly says that you can do this. His question is one of defaults / simpler syntax for the less-nice case.
The solution is to never allow incompatible, ambiguously named and versioned artefacts. If you take the version numbering out of control of the user you can have globally unique versions. As for typing in a long hash name? That's easily solved the way git does it: require the user to specify only the minimal unambiguous prefix of the hash (often as short as 4 characters). As for providing human-readable sequencing? That can be done with tagging.
I can't tell if you are being sarcastic or not. If not, could you explain? I can't seem to find any documentation of `NoTraditionalRecordSyntax` in google.
It's the [first hit on Google](http://hackage.haskell.org/trac/ghc/ticket/3356). And I'm not sure if a vacuous truth counts as sarcasm: `NoTraditionalRecordSyntax` disables the Haskell 98 (and Haskell 2010) record syntax, which means you can't declare records or use any related notation (but I guess you can still import them, and use the field names as "getters"). Without the traditional record syntax, all related warts disappear from the language. Personally, I wish `NoTraditionalRecordSyntax` and `GADTSyntax` (and `NoTraditionalDataSyntax`, if it will ever be introduced) will be enabled by default in future revisions. And for good measure, switch the lexemes `:` and `::`. That, or I should just start using Agda.
What kind of sick mind writes a letter for Valentines Day? /s
&gt; Was the writing not clear You really weren't as clear as you seem to think. In the original post you say that Python's "import conventions make a LOT more sense", but you go on to make a list of *language-level features* described in terms of what one is *allowed* or not to do. If someone misunderstands you, well, that's unfortunate. But rather than taking the effort to clarify, you take even more effort to behave like a whiny, pissy little brat who can't abide the mildest disagreement. That's why people have down-voted you here, BTW: engaging with a petulant infant isn't worth the time.
&gt;You really weren't as clear as you seem to think. In the original post you say that Python's "import conventions make a LOT more sense", but you go on to make a list of language-level features described in terms of what one is allowed or not to do. And *then* I went on to say how, even though the Haskell language *allows* you to use good practices, most people *don't*. And look, I even made sure the title reflected the fact that it's not just a syntax issue -- see the word "conventions". (I can't added the title after the fact, FWIW.) So please don't tell me the clarity wasn't there. Accept the blame for misunderstanding it yourself, or tell me what I should have done differently. So again, what was the problem? What could I have done differently? How should I have responded to the ill-founded criticisms better, such that you, in your eminent fairness, wouldn't characterize my replies as those of a petulant infant?
&gt;gcross provided an example of your communication style which is not well received: Which was long after people started complaining about how my "you didn't read the post" criticisms were being non-well received. &gt;in my observations all three comments above are often used in a baiting fashion or to imply something negative about that which they are replying to. If there are used that way a high percentage of the time then people are likely to interpret them that way. Likely, comments like these are not read as reasonably disagreeing because noticeable percentage of the time they are found in conversations the readers did not find productive. So what's the proper way to tell someone that they responded without actually knowing what they were responding to, in a way that avoids hurting their feelings? Examples, please.
Would it be enough to link to the numerous python projects that use this style? Because I would get to count all all of those very, very useful projects toward the merit of my preferred style. (Edit: of course, you could exclude all Python projects that use the Haskell-preferred style of `from foo import *`.) For my part, I've worked on a lot of Django projects for work, but that's unfortunately proprietary. On your end, you could acknowledge that an entire community does it the way I described, and so all of those would count as evidence of the superiority of my suggestion. Want to go that route?
&gt;i'm not familiar with python so i won't make claims about what its like to read/write python. what i'm saying is: its not my goal to improve discovery along the minor axis, "where is this function imported from" at the expense of, "type these extra characters over and over" It's a one-time thing at the top of the file, not doing something "over and over". With qualified imports, you can truncate the name of the package. And that's a major axis for a minor axis. People find new projects *all the time* and have to make sense of them. In Python projects, such as those using Django, it's trivial to get involved in a big project specifically *because* you can so quickly find where everything comes from. I should know -- I got a job at a Django shop when I had *never used it before* specifically because python *and its userbase* has the convention of making it easier to chase down the source of anything I don't already know.
&gt; Accept the blame for misunderstanding it yourself, or tell me what I should have done differently. My own comprehension of your post wasn't at issue. Rather, I meant only that I could understand and empathize with those who *were* confused about your point â€” something that you seem quite diligently unwilling (or unable) to do. What *I* think you could have done differently involves not the composition of your original post (I'm not your writing instructor, after all) but the character of your responses. Encourages discussion: "No, actually *that's* not what I meant; *this* is what I meant." Makes you look like a total asshole and scares everyone off: "Why is no one reading my post!?! Because clearly no one is â€” there's no way I was anything but totally 100% clear! Look, I said the word "CONVENTION"! Didn't you read it? HUH? WHY IS EVERYONE DOWN-VOTING ME WAAH WAAAAAAHHHHHH MOMMY". Etc.
&gt;My own comprehension of your post wasn't at issue. Rather, I meant only that I could understand and empathize with those who were confused about your point â€” something that you seem quite diligently unwilling (or unable) to do. What I think you could have done differently involves not the composition of your original post (I'm not your writing instructor, after all) but the character of your responses. So you *don't* actually know how I could have headed off this misunderstanding and yet still want to blame my writing for the misunderstanding. Lovely! &gt;Makes you look like a total asshole and scares everyone off: "Why is no one reading my post!?! Because clearly no one is â€” there's no way I was anything but totally 100% clear! Look, I said the word "CONVENTION"! Didn't you read it? HUH? WHY IS EVERYONE DOWN-VOTING ME WAAH WAAAAAAHHHHHH MOMMY". Etc. Pretty sure I didn't do anything like that. I already gave a fairer characterization of the exchange: My post: "Haskell does X, but that's bad because ..." Responses: "Haskell actually *does* do X ... so what's the problem?" How should I respond to that without noting that they obviously didn't read critical parts? Indeed, why should I respond to something like that at all, unless (as was the case here) people are voting it up as some kind of insightful comment?
Can't you just install it on a flash drive at home and bring that in? The Haskell Platform has a portable option that should let you do this.
GHC's Windows binaries don't need to be installed. They can be downloaded at http://www.haskell.org/ghc/download. 
Okay, I see where you're coming from.
Very good, but .tar.bz2? :)
http://www.haskell.org/pipermail/wikibook/2012-July/000083.html
Works for me. Can't unpack it right now because I don't know of a self-contained bz2 decompressor, however I can unpack it at home and bring it tomorrow and copy it from a usb stick. Thanks guys, you've been helpful.
I prefer: replace src dest = intercalate dest . splitOn src replace "3" "2"
I just don't see that happening. I wasn't objecting to long hash names, I was objecting to specifying dependencies with anything that is globally unique. If you're using tags to specify a range in a sequence, then you're back to not being globally unique, and now you have incompatible ambiguous names. You can't have it both ways.
No, I haven't used purely functional package management, but I can still criticize it based on properties that I know it has to have. My point here is that globally unique IDs might be attractive for dependency resolution, but they're not attractive for dependency specification. I just don't see how you're going to get people to specify dependencies in terms of globally unique IDs in their cabal files. See my response to chonglibloodsport below.
OK, I'm totally confused now. You want non-globally unique dependency resolution? Isn't that what we have right now?
If you're currently working with bash scripts it might be worth having a look at the "Shelly" package for your Haskell transition.
I'm a Haskell noob, so please forgive me â€” Can you explain why I'm hearing about lenses all of a sudden? Are they a library? What's going on with them? (You don't need to explain how lenses work; that might be asking too much of you)
You can just skip the registerization steps when installing the HP.
Yep. "lens" libraries have been around for a while, but one particular library has really taken off in popularity as of late. The same could be said of iteratees/pipes/conduits. See also: http://lens.github.com
Is there some central place I can read up on WTF those all are and why Haskell is loving them so much now? I feel bad asking these questions here in random threads.
Accessing data in complicated structures is a pain in the ass because you have to pattern match on everything. One of the things lenses do is give you a way to "focus" in to a particular part of a larger structure and work with it. Join #haskell on freenode and ask for a lens demo. Or more likely, ask them to stop golfing with lenses for a few minutes and explain what's going on.
7-zip, it's your friend.
Its awesomeness. Not that Haskell isn't awesome -- they each have their strengths, but imports is one thing Python gets right and Haskell gets wrong. I would say Haskell is better at getting you to think about programming with new eyes, though.
You'd be surprised how many replies I got that seemed to miss that.
Or, say, any instance of `Applicative`, a.k.a. idiom bracket style. So yes, it actually would be useful, despite the worrying potential for being too "clever".
&gt; Haskellâ€™s laziness means that we donâ€™t have to worry about it (or other ghost variables) imposing any run-time overhead at all. What about memory? Won't the ghost's thunks start piling up?
I don't think there is any good universal resource on them right now. Part of their appeal is they are so "new", it is almost a hipster effect. "Oh it's this obscure library, you've probably never heard of it." For lenses, there's that link, which I think has some very good explanations. For the rest, you could probably get some good responses by asking on #haskell irc. But really, you shouldn't feel any urgent need to go find out what they are. There's nothing wrong with asking for details on threads like these.
I'm liking the idea of "mainstream Haskell". It allows me to be hipster without being hipster.
Oh, probably. The bigger questions are how difficult it would be to do it right without breaking anything else, and whether you can convince enough people that it's not a horrible idea that will only result in obfuscation.
That makes sense. Thanks for pointing that out.
There's actually one other thing you can do in Python: you can use as to rename names as you import them from module import foo, bar as baz now you can use baz. I would like this in Haskell. It could be used to rename functions where you would otherwise need to use hiding (plus maybe another qualified import to get access to both versions of a clashing name). In general I agree with SilasX: I would prefer if more Haskellers used a convention of named imports, if the lighter syntax was for qualified imports, if there was specific syntax to import all names unqualified. That said import syntax is not my biggest wart in Haskell; import statements sit out of the way at the top of the file.
Both you and chonglibloodsport seem to think that using a purely functional package manager implies that you have specify dependencies by writing out GUIDs by hand. I assure you this is not the case and if you had actually used nix you wouldn't be spreading such FUD. Purely functional package managers require you to specify GUIDs by hand no more than programming with pointers requires you to specify memory addresses by hand.
&gt; (Oh sorry ... I mean the one with the fewest syntax tokens. An important distinction.) You correct to be sarcastic there. On reflection that point of mine was completely redundant. Consider it retracted. As for the rest of what you are saying here. I did read your post but I missed the fact you were looking for solutions to the problem of other peopleâ€™s code. Sorry. I guess the reason I missed that is because there isnâ€™t much you can do about it.
Thanks. When I replied to him it had sounded to me like he was saying something else, but I will readily admit that I may have been confused.
And in Haskell, zipWith7 too. Sorry, no zipWith8...
I wrote one, it's fix-imports on hackage.
[PortableApps](http://portableapps.com/) are your best friends. They were originally designed to run from USB stick but they can be installed in any folder and are self-contained.
thank you - really need a "compile / syntax / spelling" phase/check for my posts
I already am actually, although I end up using it less than I thought I would. I've been trying to stay "native", i.e., use native Haskell functions for creating directories, copying files, etc, instead of using bash via `Shelly`. One thing I really like about Shelly is that I can change directories in a thread-safe way, which will prove very useful in the near future.
The flag is `-ddump-minimal-imports`
Very good interview. I think that Duncan is able to expose important technical advantages of Haskell/FP to any outsiders in an enlightening way -- a great communicator.
Here are [the slides](https://github.com/evancz/elm-at-mloc-js) if you'd like to follow along and look at the links in the presentation.
its not a qualified import if you don't have to qualify the name. aliasing can make it shorter, but you can't eliminate the overhead. 
I've been using Haskell for years and I still completely agree with the rant and disagree with the bullets from the style guide: &gt; They slow down development: almost every change is accompanied by an import list change. Development speed is much more about *reading* code than writing it. Optimizing for writability at the expense of readability is almost always the wrong choice. &gt; They cause spurious conflicts between developers. Not if you use qualified imports. And even if you import specific names, they're trivial conflicts that are easy to solve, even by trial&amp;error. &gt; They lead to useless warnings about unused imports, and time wasted trying to keep the import declarations "minimal". Again, not if you use qualified imports, but even if you don't, handling these warnings costs a tiny fraction of the time it costs to figure out where names are coming from, or doing an email correspondence with maintainers to fix their packages when their open imports caused a build error. &gt; TAGS is a good way to find out where an identifier is defined (use make tags in ghc/compiler, and hit M-. in emacs). TAGS assume names are globally unique, which they aren't. It's also not trivial to set up TAGS that cover all the packages you're importing. About Data.IORef, I agree that it is useless to import the names explicitly, or qualified, because they did the silly thing of qualifying each name in the name. It is silly, because we already have module qualification support. It would be much nicer to have: import qualified Data.IORef as IORef x &lt;- IORef.new 5 IORef.modify x (+1) Double qualifications in names are a problem, not a solution, because they encourage the bad import style. I somewhat agree about basic Prelude-ish names that come from very well known modules (Control.{Applicative, Monad, Concurrent}). But it is *not* reasonable to assume the reader has to familiarize themselves with every export list of every dependency in order to *read* your code effectively. 
&gt; TL;DR Please use qualified imports or explicitly list the parts of the module you're importing 
After many years in Haskell development, I can attest that in my experience, open unqualified imports are not worth the tiny benefits they supposedly bring. I do end up with [long import lists](https://github.com/Peaker/lamdu/blob/master/src/Lamdu/Main.hs) but that's a tiny price to pay.
That certainly helps, but it is not a solution. Using "proper" imports guarantees there are no name collisions as a result of libraries just adding new names. Using tiny vocbularies with powerful abstractions will still break your package when innocent changes to export lists (e.g: adding names, re-exporting with deprecation wrapper) are made. It will just be more rare. I think a single package breakage that requires hours of email correspondence with authors to fix, during which time all of the dependents are not buildable wastes more effort and time than many thousands of slightly shorter unqualified names save. Not to mention the ability to read random code on github without first setting up hoogle to index all of its dependencies.
Ha, that's a nice one. It even typechecks! Might not be totally intuitive though...
fair enough
Optimizing for "less typing" (writability) is a really bad idea when it costs you in readability. Code is read many more times than it is written, and development speed is more a function of readability than writability. Also, I've spent more time than I'd like doing email correspondence with authors of my package dependencies, as their package broke because of innocent export list changes due to unqualified imports. When that happens, your package becomes unbuildable (by a simple "cabal install") until the dependency is fixed. Not all package authors are willing to accept trivial fixes for problems like that, and I'll never get the time wasted there back (I ended up having to fork the dependency to fix the unqualified import build problems).
I had a really bad time with the [bitmap](http://hackage.haskell.org/packages/archive/bitmap/0.0.1/doc/html/src/Data-Bitmap-Pure.html) package. At some point, `Foreign` stopped re-exporting `unsafePerformIO` directly, and instead exported a deprecation wrapper of it. Then, bitmap got ambiguous name errors from `unsafePerformIO` and stopped building. This meant that `cabal install` on every single package that had a direct or indirect dependency on the `bitmap` package failed! This costed many hours of correspondence with the bitmap author, hours of work and frustration -- that I'm very very sure outweigh any benefits that author might have had from his use of unqualified imports in his projects. I ended up having to fork the `bitmap` package because the author refused to upload a trivial fix to the import issues, and instead made vague promises of releasing 0.0.2, which he had eventually released a long time after he said he would. During all that time, if I hadn't forked the package, my package would be much harder to install. I've encountered more build breakage from unqualified imports, but this one is the main one that comes to mind. I've wondered many times where a name comes from -- and using TAGS or Hoogle to find it means that to read random code I'd have to set up TAGS or Hoogle *for all the dependencies of the code I am reading*. This is an insane amount of overhead to just read code. All this, just to save some qualifications when writing code? I agree with the OP that on this issue, the Haskell community is just insane.
Some people will want it, but the OP's point is that it is bad practice. It means that adding a name to Data.Char breaks dependencies. Changing a re-export into a wrapper-export also breaks dependencies. These are innocent changes that should not break things. It also means that I cannot read random Haskell code, see a name, and know where it comes from so I can read the documentation or see its type. I have to go ahead and set up TAGS, hoogle databases, or what not, of *each and every imported dependency* just to be able to do this. The miniscule benefit of less qualification is not worth these huge downsides.
Namespace pollution vs. looking ugly. The former is a deep problem that may break your package down the road. The latter is a superficial aesthetic problem. Why would you trade a superficial problem for a deep one?
Then use closed unqualified imports if you prefer: import Control.Foo.Bar (this, that, (~)) But don't just throw all the names from tomorrow's `Control.Foo.Bar` to collide with the names from tomorrow's `Control.A.B`. Because tomorrow, they might have names they don't have today, and you really have no idea what those could be.
Haskell does make the "right" way more cumbersome and encourages the "wrong" way, so it is a fault with Haskell itself as well.
Having to search a name through a modest number of modules (say 8) is still 8 times more expensive than searching it only in 1. It's IMO an unacceptable price to pay for the miniscule benefit of unqualifying names.
By making it the simplest and shortest syntax, it is effectively encouraging it. What you are saying is akin to saying: "Laziness in Haskell is not a default, it's just less tokens than typing ! for strictness". The short way that you can opt out of by adding more tokens *is* a default.
I agree with your original point. I do think you could have been clearer, but your post was clear to me. However, you should expect people to be defensive and emotional when it comes to the way they program, which they see as a part of them. I agree that most of the responses to you did not make much sense if they read your post and understood it -- but all of this is still not justification to respond to them in the way you did. Be more diplomatic, it's more effective at converting people. Don't disparage people for misreading, or just re-quote the texts you wrote. Just explain again, in different words. If many make the same mistake, you can link to your own comment re-explaining the issues in different (hopefully kind) words.
It's nice how Elm evolved from its infancy to be the subject of a presentation. I still remember your first posts like "check it out this small Haskell subset I'm writing as my senior thesis" :)
I love you and I'm so proud of you.
http://youtu.be/RkeOAh7hAjY
Many of those things go well beyond syntax! It's a long list, but in quite a few cases, you seem to have an outlook that the language ought to require good large-scale software engineering practices at the expense of experimental coding. It would be a real shame to lose something like the ability to infer top-level types, just because it's not appropriate for long-lived code in a large project. Or custom infix operators. The misogynistic language is unfortunate too...
Aww, thanks! :) The internet tends to be a harsh place, so your comment is really encouraging!
Thank you!!! That code is about 6 trillion times as readable (plus or minus) as most Haskell code with long import lists.
&gt; if I see a function I don't recognize, where did it come from? Foo.Bar? Data.List? Who knows? GHC does. GHCi has the :info command. For example: Prelude&gt; import Data.Map Prelude Data.Map&gt; :info fromList fromList :: Ord k =&gt; [(k, a)] -&gt; Map k a -- Defined in `Data.Map' Simply create a keyboard shortcut in your text editor (personally, I use Ctrl+F1, as F1 searches Hayoo) to call ghc -e ":info &lt;insert identifier at cursor here&gt;" &lt;filename&gt; This also has the upside of working with modules not available on hoogle/hayoo.
That's helpful, and I'll probably adopt that, but it doesn't do anything for: - code I read on the web - functions with the same name across different modules - functions from user-created modules (in some contexts) - the short-sightedness of avoidably shifting the code-reading difficulty to the reader
Plus, from my understanding, it helped you landing your Google job, which is "not bad" I think :D
hscope, it's cscope but for Haskell. Okay... wtf is cscope? A little more elaboration in the readme would be nice.
[cscope](http://cscope.sourceforge.net/) is a symbol database for C/C++ program. Think it as ctags on steroid. Ctags allows you to find where a symbol is defined, cscope allows you to find where a symbol is referenced. (and much more, but that's the only feature I use.) It's well integrated with vim. In fact, cscope integration is one of the major reason I use vim for work.
&gt;when was the last time you wanted to both declare a short and concise qualified name for things and dump them all into the global namespace? Yesterday. And not just in a "well, co-incidentally I needed to do that yesterday for the first time in years", but I do it all the time. I want all of Big.Long.Module.Name.X and Big.Long.Module.Name.Y, but they contain one or two functions with conflicting names.
&gt; Why would you trade a superficial problem for a deep one? Because I've only had namespace problems once or twice in 10 years of coding Haskell and one of them was only because I couldn't wait for (&lt;&gt;) to be defined in Data.Monoid so I defined it myself. Naughty I know, but I was using it so much that `mappend` just wasn't an option. Also I'm very stringent about the packages I depend on (not everyone writes web-apps!) so even though some of my software has grown quite big, rippling changes tend to be my fault - changing my design - rather than dependencies changing their APIs. 
The trouble is that comonads are not objects. It is more like, "many objects have an associated comonad". But every object also has an associated monad, with the same underlying functor. Objectness is a functorial property of context -- context which determines, for example, which class's method to use when called on an object; the state of the "world", etc. An object "knows" whether a value is a null pointer in the same way a computation in (Maybe a) "knows" whether it failed or succeeded -- by looking it up. The article commits the monad tutorial fallacy at a fundamental level.
&gt; I think it would be nice if Haskell released the word qualified It's actually not a reserved word at all, since its import use occurs in a syntactic position where no value identifier is allowed. Same with as and hiding.
I've already encountered multiple namespace problems in others' packages. Manually patching dependencies everywhere is a hell of a price to pay for some qualifications. Not to mention the difficulty of reading the code, having no idea where to find the type or documentation of the names you see. 
I'm using Elm for a school project. I may write about it.
I'm terrified!
I have been too lazy too learn about FRP for a while now. Lo and behold, I just learned quite a bit about it without even trying. Quite intuitive, really. Elm looks awesome - kudos for designing such a straightforward implementation and double kudos for giving it a real world interface (the browser) to work with on day one. 
Lists are perfect control structures rather than data structures because of their laziness. In fact that is just a generally good rule of thumb for any type: Lazy (i.e. Haskell's default) = Control Structure Strict (i.e. strictness annotations) = Data Structure I find that Data.Sequence from containers always outperforms or ties lists in performance as a data structure, and you still get to keep the nice pattern matching properties of lists. If you don't need pattern matching, then use vectors, preferably unboxed vectors. Edit: I went back and checked: Seq is twice as slow for pure prepending and inspection, so I was mistaken. The disadvantage of Seq and vectors is they require an additional dependency, so usually people only use them if the performance really matters. For example, if all I need are simple error messages I will just use String because the performance impact is negligible and a `text` dependency is overkill. If I am doing heavy duty text processing then I will use Text.
Can somebody briefly explain what Lisp macros are?
Sure, the best way to do that is with a little code: let's say you want to write a piece of code that does something like this: (fun1 (fun2 (fun3 x))) That is, fun3 applied to x, fun2 applied to the result, etc. You see how you have to read it inside out? That's a little silly. It'd be nicer if you could write something like this: (thread x (fun3) (fun2) (fun1)) Now you read it left to right (take x, apply fun3 to it, fun2 to that, etc). We're still doing the same operation, but it reads differently. In this example, thread is a macro. When the lisp interpreter sees (thread ...) anywhere, it passes the rest of the expression (as unevaluated code) to the macro function. That function then returns the original code (fun1 (fun2 (fun3 x))) which the lisp interpreter then runs as normal. So, lisp macros are functions that operate on CODE and return CODE. Lisp makes this easy because code is just lisp data structures in the first place.
I sometimes find Haskell more difficult to read for the reasons SilasX points out. For imports only used once, I kind of wish one could skip the import and use a fully qualified name, like one can do in Java. I'm mildly curious why that isn't allowed.
Several other people have made excellent arguments for linked lists, so I will just say I agree, lists in Haskell are lazy control structures, not data types. Unlike C or even the more high-level C++, Haskell objects in memory in a compiled program have been optimized into something that scarcely resembles the code. Laziness provides all kinds of freedom to the optimizer and code is optimized extremely well, so you never have to worry about details like memory allocation and access times, you just worry about writing good, correct code. My own rebuttal to you is this: I write compilers, which are notoriously difficult to make work in parallel. There must always be some kind of preprocessing, you can't just break a string into arbitrary substrings and compile each substring into a part of an abstract syntax tree. So parallelization at the parsing level is not an option, and this is where lists really come in handy. Lists let me store chunks of data, and Haskell's laziness takes care of the details of how to allocate nodes in memory. My code may be written in such a way that a ten-thousand line program might be parsed into a single list, but because of laziness, in physical memory it is more likely that only one line of code is being allocated at any given time. All the intermediate data structures are also probably being allocated in handfuls, and then once they are converted into to the AST node they are deleted from memory in a very short time. Because of laziness, only the final AST, which never goes out of scope, stays in memory the entire time. Since parsing, or more generally, graph reduction algorithms, are so pervasive, lazy linked lists are extremely important, and that is why they are so commonly used in Haskell. 
It has nothing to do with lists being primitive. The standard libraries just provide some reasonable rewrite rules so that certain functions can be fused together. Other libraries can do the same thing.
&gt; I find that Data.Sequence from containers always outperforms or ties lists in performance as a data structure My experiences differ in ways that I think make sense. Adding or removing elements from one side of a sequence is slower than adding or removing elements from the head of a linked list. The main reason I can think of to use sequence is for the two-sidedness or the logarithmic time indexing. The former tends to be very unusual, at least for me, and the latter is often better served by an array. Also, I think linked lists make perfectly fine data structures, too. That they are lazy doesn't really affect this.
&gt; The general consensus in the Haskell community seems to be that, while Lisp macros are really cool, the Haskell type system makes Lisp-style macros unnecessary. I think you misunderstand or at least misstated the consensus. The type system has little to do with macros being unnecessary. If anything, it's just that Haskell allows you to program at a very high level of abstraction, and GHC is unusually good at compiling layers of abstraction away. Most of the times you would use a macro in Lisp are just unnecessary because of this. I doubt many of us would claim there are no uses, though. I don't think macros are always a smell. Sometimes they are, such as when they are being used out of ignorance by somebody who doesn't realize what I said above. However, sometimes they just indicate that the core language is inconvenient in some way.
My understanding is that many of the optimizations are made using rewrite rules, so any library could implement them. 
This is an extremely C-centric argument. Perhaps linked lists do not have much use in C, but they're extremely convenient for functional programming, and functional programming has always been more about developer productivity and correctness/maintainability than performance. 
I have a confession: I do not do much haskell, and I do even less theory. I'm kind of a country bumpkin compared to most of this subreddit. I only have a vague idea of what you just said. However, I have the feeling that you may be taking my example a little too literally. Lisp macros take code as arguments and reduce to code that is then evaluated. In my example, it was unimportant what fun3 fun2 and fun1 actually do.
Imagine you have some kind of procedure (called macro) that lets you: * choose how and when to evaluate pieces of code that you pass like arguments to that function * return code In Haskell not every construct in the language can be coded as a function definition (typeclass and data declarations for example). So you need something like Template Haskell to process things like that. Lisp is homoiconic, that is, all the code (every statement) reflects the abstract syntax tree of the language itself. So with macros you can transform code manipulating directly the structure of the AST. So with Lisp macros you can encode things like: * new binding constructs (like let) * new control structures (Lisp has strict evaluation order, so you need macros) * functions with arbitrary number of arguments
Hmm, good point. I really don't know how they would be used in C--I'm going through a very anti-C phase right now :P. (If I had to write C for embedded programming, I would probably resort to generating it from a Haskell/OCaml DSL right now, which is how little I want to use or think about the language itself.)
Also, functionality being available for special cases doesnâ€™t mean it is bad, nor that it should be removed, even when it is a bad choice for by far the most cases. Itâ€™s a slippery slope of some programmers being too dumb and misusing it and complaining about it, language designers listening to them and dumbing things down, and programmers becoming even dumber because of the new limitations. Until it ends in the realm of dumbed-down nightmares like MS Clippy, Visual Basic, and anything Apple or Windows 8 or even Gnome 3. Luckily, the Haskell community is exceptionally intelligent, and thankfully has always steered clear of that fallacy.
Really great introduction to FRP and very entertaining presentation! How does Elm hold up performance wise? It seems almost trivial to keep throwing in signals but there must be some performance issues with too many signals?
Briefly? If you know Template Haskell, they're easy to compare: Basic quotation: * Lisp: 'â€¦ * Template Haskell: 'â€¦ (values) * Template Haskell: ''â€¦ (types) Quasi quotation and splicing (inserting non-quoted values `foo` and `bar` into a quoted value): * Lisp: `(â€¦ ,foo â€¦ ,bar) * Template Haskell: [| â€¦ $(foo) â€¦ $(bar) |] That's where the similarities end, then: 1. In Lisp, macros are don't require a special invocation syntax any different to normal functions. 2. All arguments to macros are quoted. In Haskell: 1. Something similar to macros can only be achieved via splicing (as if the whole Haskell module is already quoted). One difference, but it's probably superficial in terms of the end result, is that [| â€¦ |] quotation produces a monadic `Q Exp`, whereas in Lisp `(â€¦) produces a normal list of syntactical tokens. Implementation and invocation comparison: So, (defmacro aif (test then else) `(let ((it ,test)) (if it ,then ,else))) can be invoked as: (aif (&gt; a b) (print it)) (This is an interesting macro in Lisp because `if` treats non-nil things as "true"-ish.) In Haskell, {-# LANGUAGE QuasiQuotes, TemplateHaskell #-} import Language.Haskell.TH import Language.Haskell.TH.Syntax aif :: Bool -&gt; Q Exp -&gt; Q Exp -&gt; Q Exp aif pred pthen pelse = [| let it = $(lift pred) in if it then $(pthen) else $(pelse) |] (forgive me if I got some of this wrong, TH is a bit more finicky than Lisp's macros) can be invoked (in an expression) as: $(aif (a &gt; b) [| print it |]) So you can see that in the Haskell version I had to (1) invoke a splice with the `$(â€¦)` syntax, and (2) quote the parameters. In Lisp there is simply a different namespace for macros, allowing the compiler to check at read time whether something in call-position is a macro or not. At least, this *would* work, except TH's `[| â€¦ |]` demands that names be in scope.Â¹ So the usefulness of TH is basically restricted to boilerplate code, and not really syntactic abstraction, like it is in Lisp. They're often used in Lisp where simple laziness would do, but they're also often used to get things like lambda-case for free, or whatever, without having to wait for there to be a patch to their compiler of choice or lobby it for years in the Haskell proposal community. 1: Well, you could use a different quasiquoter rather than TH's built-in one (like HSE to just parse the tokens into a tree), which would allow the above, but that's not the default of TH.
The `vector` library that my second link talks about is not a Haskell primitive; it's just a third-party library. And the only way Haskell lists differ from user-defined types is that the language has syntactic sugar for them. As geezusfreek and tikhonjelvis point out, the trick is [a compiler that supports user-defined rewrite rules](http://www.haskell.org/ghc/docs/latest/html/users_guide/rewrite-rules.html). But the deeper reasons rewrite rules are possible are: 1. The language's insistence on referential transparency (expressions that denote the same value should be mutually substitutable in all contexts); 2. Thinking of types in terms of *equational laws* that its basic operations must respect, and not in terms of particular implementations. Rewrite rules is the use of equational laws to transform programs very aggressively while still preserving their meaning.
A large class of macros are not necessary in Haskell, because laziness allows us to create various control structures directly (LOOP is an excellent example here). A further class of macros are not necessary because they involve type-based introspection and dispatch (or perhaps specialization). These are the macros that typeclasses, and type families, tend to render unnecessary. Vector is a very good example of where we can have a high-level interface to extremely specialized performance-sensitive code through judicious use of typeclasses. Still other macros are necessary. These tend to be, I'd argue, "hacks" to provide, e.g., deriving of classes not in the Haskell spec. For such hacks, the new Generic mechanism can alleviate a great deal of pain. Still other macros provide a condensed syntax for things we can't currently do in other regards. But as the language has grown more powerful, the need for such macros has been reduced. Some other macros let us generate large amounts of code involving introducing top-level names, or top level instance declarations, from a small amount of code. Getting correct, condensed, names for type-level naturals, for example. Here, macros are indispensable. However, if we continue to extend the language with fully-featured type-level naturals (as we have been), then the need for that particular case, for example, goes away. Another set of macros are those involving quasiquotation to embed the concrete syntax of another language _within_ Haskell. Here we not only generate code, but locally override the lexer. I don't think the need for some such facility will go away. Another set of macros involve specializations that need to be made on an ad-hoc and parameterized basis for efficient code generation. Furthermore, such transformations might need to take account of a great deal of local context. See the shonan challenge, for example: http://okmij.org/ftp/meta-programming/Shonan-challenge.pdf For things such as this, I can't imagine the need for macros or otherwise generative programming will ever vanish either. I'm sure there's some cases I've missed.
Why isn't TH friendly? I think its poorly documented, and there are various improvement proposals that I think are quite good. But the core seems good to me. On the documentation side, I think the problem is that the haddocks emphasize using the concrete AST constructors, but in fact one can use various types of auto-liftings and embedded splices to write in a much more elegant style.
Cool! Please do! It is really helpful to get formal feedback on the project :)
I think TH is awesome - at least something is there for this stuff! I actually would prefer the AST constructor approach. The AST quotes are far too limited - they need splicings in many more places. It's quite frustrating when trying to do TH, because initially you're like "This is neat! I can use the syntax of the language to build syntax trees!". And then pretty soon, such as when giving something a programmatic name (which you almost always do), you have to convert parts of your splice to explicit construction. Pretty soon you're never using AST quotes. I also think that having all of the aliases that lift the constructors into the Q monad is a serious API smell. When I first saw TH I was like "whoah, that's complicated!". When really it's quite a simple API. I've got a WIP / temporarily abandoned project / repo of TH experiments here: https://github.com/mgsloan/quasi-extras . These quasi-quoters allow you to splice anything in - https://github.com/mgsloan/quasi-extras/blob/master/src/Language/Quasi/Ast/TH.hs (except listlike things, such as lists of declarations / cases, that's till TODO)
Bear in mind that those style guidelines were written when we used to use explicit import lists, and removing the import lists was a *huge* boost to productivity. I would have no problem with using qualified imports (for some things), but we already qualify as part of the name in many cases, e.g. idName would become Id.name. However, Panic.panic would be somewhat redundant, so it's not clearly a win. Basically, you need to figure out how to grep the whole project using your editor. For me it's 2-3 keystrokes to grep the project for the identifier under the cursor. Using qualified imports would make it *harder* to do this. I also advocate using explicit import lists for imports in other packages, because (a) you can easily M-x rgrep in the current project, but not across all packages, and (b) it helps to insulate you from breakages due to API additions.
Thank you! :) I have not had any problems with performance, nor have I heard about any so far. One of the novel things in Elm (at least in FRP literature) is armortizing recomputation costs. If an input changes, only the values that depend on that input must be recomputed. That means Elm will only do work if it is absolutely necessary. This is possible because Elm has a discrete semantics, so a mouse sitting still cannot cause any computation (as it could in arrowized FRP, real-time FRP, and classical FRP). edit: see [my thesis](http://www.testblogpleaseignore.com/wp-content/uploads/2012/04/thesis.pdf) for a better overview. Nonetheless, I am in the process of making some performance improvements for the next release, so this should only get better!
Thanks! :) I'm glad the video was helpful!
Look at [an answer I wrote on SO last night](http://stackoverflow.com/questions/15031190/what-is-the-optimal-way-of-representing-a-floating-point-number-in-a-range-from/15035023#15035023). While writing it I realised (again) that just because `Foo` was in scope at the macro definition, that didn't mean I could use it in the generated code: for that I needed `Foo` to be in scope wherever the macro was invoked, and the whole point was to not export `Foo`. So TH doesn't support encapsulation as well as ordinary functions do, and it's counter-intuitive (or at least counter my intuition) in not allowing me to use something merely because it's in scope. I never got very far with Lisp, so I don't know whether this is a problem with Lisp macros as well (I can see that it's a harder problem in Haskell because the compiler has to explicitly keep track of the type) but it's certainly a problem I experience with TH. (Or perhaps TH already supports this but it is not well documented?)
&gt; And the only way Haskell lists differ from user-defined types is that the language has syntactic sugar for them. Which isn't even privileged in GHC HEAD, cf. [OverloadedLists](http://hackage.haskell.org/trac/ghc/wiki/OverloadedLists).
It's not just the existence of rules. It's also that the definitions of common functions are set up to cause those rules to fire more often (e.g., if you want list fusion for your functions, then you should define them using `build`, `foldr`, and `augment`).
Another point: the criticism seems to assume that hardware is some god send given, and we must change our programming habits and languages to please the machine. But we can safely assume that if functional programming rises more and more, so will hardware architectures evolve and try to please **us**. For example, it couldn't be that hard to recognize a list node (two adjacent pointers) and speculatively pre-load the data they point to?! Apart from that: Do we really care about a few microseconds. I for my part don't care about "TLB" or some such for the simple reason that I do not even know what this is. Nor do I think I am supposed to.
Well, the concept of Lisp macros is quite simple. Lisp code is simply a bunch of nested lists of symbols, right? So defining a macro is pretty much defining a function from list to list that gets called at compile-time on the syntactic arguments of the macro. There are facilities such as backquotes to ease the building of the resulting list, but the general idea is just that: list transformation at compile-time. So you can see why Lisp macros can't really work as well in any other language. They don't encumber the syntax because Lisp, in a way, doesn't have a syntax: the code is directly the AST in text form.
That sounds like a reasonable compromise. I also avoid things like Panic.panic, Map.Map, etc. I use explicit import lists for those and for operators. But I don't really see how removing explicit import lists could be a *huge* boost to productivity. How much of your time is spent managing the import lists even if you explicitly list symbols? What about an editor key to add the import lists? Also, the negative effects on productivity, which is that you need to interactively grep each name to figure out where it's from are harder to measure and will only appear later. 
Yes, TH claims it has quosi-quotation support, but it's never true, very limited splice support and no nested quosi-quotation. Simply put, TH's quosi-quote is far from Lisp's quosi-quote
TLB is the translation lookaside buffer. It basically caches recently accessed virtual pages. Jumping around to new pages all the time means that you get a page fault which means the TLB must be restocked with new information. This can be quite slow, particularly seeing as a page fault means a context switch.
I see, that sounds cool.
&gt; Heuristics could handle common abbreviations such as knowing 'List.' requires Data.List to be imported qualified as List. The actual fix here would be to sanitize module names, rather than to get used to heuristical aliases.
It's very powerful; but it's also very easy to fuck up :)
&gt; and you still get to keep the nice pattern matching properties of lists You do? How?
Is Liskell still an active thing, and how do its macros compare to TH? Anyone?
No, I'm not saying that. I'm saying that if you don't specify dependencies by writing (partial) GUIDs, then the cache of built packages will not be transparent, and if you are developing systems where several packages are being developed in concert, you will end up wanting to be able to manually remove things from the cache.
Having globally unique IDs for packages certainly has its uses. I'm saying that they're not what you want to use to specify dependencies. Yes, that is the way our current dependency system works. But the system's tooling is inadequate.
For me, the ability to substitute one's own literate preprocessor in Haskell trumps Template Haskell. I've tried both for the same problems. Haskell is always "in charge" when one uses Template Haskell. Haskell can share control with TeX or any other interested party, with a custom literate preprocessor. Lisp half a century ago had the idea of stealing a bit from words for garbage collection. Haskell uses indentation, but doesn't "steal a bit" for comments. I prefer flush comments, indented code, avoiding any need for comment markers that make code look like a 1980's email message. This is a few lines to write, as a literate preprocessor. More interesting is here docs with variable interpolation. One can do this in Template Haskell but it's not pretty. With a preprocessor it can be pretty. Fundamentally, Lisp macros are a preprocessor facility. Yes, we have that in Haskell. 
Isn't the problem that they're not really globally unique; that duplicates are possible? If they were truly globally unique duplicates would be impossible (i.e. cryptographic hashes).
How are you supposed to predict which future dependency version will break your package? Package version ranges just don't work very well in practice. I'm not proposing GUIDs be the sole determiner in dependency resolution. I'd like to see a system whereby the type signatures also play a part. If none of the public type signatures have changed from one GUID to another you can make a safe bet that they are compatible. With this sort of system in mind, you can make a package policy that requires breaking changes to modify at least one type signature.
In that example, you wouldn't use a monad. You'd use a pure function: (#) = flip ($) (&gt;&gt;&gt;) = flip (.) -- Defined in Control.Category 4 # (*2) &gt;&gt;&gt; (+1) &gt;&gt;&gt; sqrt -- result: 3.0
Actually Seq is lazy in the spine. This is necessary to provide the O(1) cons in a persistent context. This doesn't help you from an 'infinite Seq' perspective though because you're shuffling the whole structure as you go down the tree, so it looks like a strict container from the outside in that regard.
To some extent: dot notation. If you need a class from a library, you only have one fully qualified name to worry about. After that, dot puts you in the right namespace to use its methods without further qualification, etc. In my experience this is a pretty significant benefit for namespace management. Java also has this advantage.
&gt; How are you supposed to predict which future dependency version will break your package? That's what the PVP is for. There may be some shortcomings, but for the most part it works well for me. It's certainly way better than no convention at all.
The PVP is of small reassurance to someone with a broken system which has backed itself into a corner with its dependency graph.
Well, I have a foot in both worlds, so maybe I can help you understand here. Both the Lisp and Haskell communities have the concept of code as data. However, the dominant such concept is very different in each community. The Lisp concept of code as data is very **syntactic**, and embodied by homoiconicity, macros and eval. The Haskell one is very **semantic**, and embodied by monads, free monads and `do`-notation. One of my favorite ways to understand the Haskell concept is the [operational monad](http://apfelmus.nfshost.com/articles/operational-monad.html), which sadly is not the most beginner friendly, but the 10,000 feet view may be instructive nonetheless. The operational monad is a library for implementing DSLs in Haskell. This library models a "program" as a *dynamically generated sequence of primitive instructions, possibly terminating*: 1. Primitive instructions are defined by the library's user. Each one produces a result of a certain type. 2. The program is not a fixed sequence of instructions known ahead of time; it has the power to examine the result value of each instruction and choose different continuations based on it. So instead of modeling programs as expressions and translation from expressions to expressions, here we are modeling programs as "current instruction" and "continuation." I don't actually expect you to get all of this at first, but maybe [my heavily commented example of implementing a "pausable" language in Haskell](http://pastebin.com/gLukWauL) would be of some help. (This was my "teach myself the `operational` library" exercise.)
Macros in Lisp are part of the compilation step. Imagine for a moment that haskell no longer had "do" notation for monads. That would be incredibly inconvenient. What if you could write a function that took an abstract syntax tree that looked like "do" notation and returned a new AST that looked like regular function calls. That would be a macro, and your compiler would call the macro during the compilation phase. There would be no difference between calling a macro and writing the code the macro produced... to the compiler. To you it might be a godsend though, since you don't have to write all those &gt;&gt;= calls anymore. The compiler would handle it. I think you too may have taken my example a bit too literally.
Aye, Haskell can do my example without Macros. I thought I would pick an example of a very useful macro from a real lisp as an example. I probably could have picked a better one.
Do you know a good article about the "build" function? All I found was [this](http://www.cs.ioc.ee/~tarmo/misc/builds-subm.pdf), which was over my head.
the real power lies in that Lisp macros entitle users have the ability to do multiple-staging. You have to embed ghci or ghc to simulate such behavior...
Through viewl and viewr.
Whoops, you are right. I thought there was a strictness annotation on the recursion. Also, you are right that there is no way it do O(1) prepends if it did have one.
You are mistaken, at least for relatively modern GHC. See this example: https://github.com/Peaker/testTH If your macro uses direct quotation for names (i.e: 'foo or ''Foo) rather than strings (i.e: mkName "foo") then you get early binding and encapsulated access to the names. You don't need them imported in the client code and they need not be importable at all. I consider TH that uses string names unnecessarily a bug, because it exposes the macro implementation details and requires the user to import unnecessary stuff.
This post is based on a very common ignorance of the concept of intrusive data structures. Regardless of the advantages lists provide for pure functional programming, even in C lists actually have important properties that arrays lack. The author here is assuming that to insert or remove an item in the middle of the list we actually have to traverse the list all the way to that point. That is false. With intrusive doubly linked lists, we have very cheap O(1) deletion of an element when we have a pointer to that element, and that is extremely common. We also have O(1) insertion in the middle, when we know what we want to insert after, which is less common, but not that rare either. When a data structure needs to be in multiple structures simultaneously, and no random access is needed, putting it in arrays and scanning them when that data structure is deleted is usually going to be a dumb choice. Putting it in multiple intrusive lists and using O(1) delete is the right choice. Also, the amortized techniques of arrays give reasonable throughput to compete with linked lists, but they screw latency up. Thinking that arrays completely replace lists betrays a basic ignorance of computer science.
C is actually a relatively nice language if used in a certain way (intrusive data structures, avoiding dynamic allocations and unnecessary indirections) that makes it easy to produce well-performing code. Intrusive structures also form a nice lesser known pattern for generic reusability of code without lots of (void*) indirections. Sure, it really lacks a module system, sum types and pattern matching, parametric and ad-hoc polymorphism, RAII and proper consts, but on the other hand, it also lacks the awful features added by other contenders that added some of these useful ones. 
And some of them are just outright false, like insertion/deletion from the middle costing O(N).
So the actual link to the class is where? Didn't see it on the page, perhaps because of viewing from a smartphone but it should be a little more prominent on the page. 
Perhaps it would be better if I say that laziness is more appropriate for control structures.
The links is here, and nowhere to be found within that website. As they say, 90 % of everything is crap. Anyway, the link is http://shuklan.com/haskell/ (the lectures are regularly posted here when they come out by the way, so you can't miss new ones) :-)
I also donâ€™t consider lisp macros to be â€œproperâ€ either. It sounds a bit better than TH though. But not much. Type-safe rank-n polymorphism with no monomorphism restriction is more like what I meant.
so far I am mostly looking a scheme interpreter which is also making me learn scheme and the Parsec stuff. I haven't thought of a real word application for it yet. Other than maybe writing a web page with it. 
you can hoogle an operator name. Custom operators are fantastic from a software engineering perspective because they allow EDSLs. Haskell lacks tools for programming in the large (but so do most language) but I don't think custom operators are a bad thing. Custom operators =/= overloading. 
Yes, you can Hoogle. But you can't Google. Hoogle is great for searching documentation. But in actual software, you end up spending just as much time searching through user forums, Stack Overflow, and wherever else trying to hunt down usage issues or looking for known bugs. You're right that custom operators is not the same as overloading, but there's so much overlap in how they work in many languages. I have a personal conjecture that in Haskell, the predominant use case for infix operators is as methods for typeclasses. The terseness of operators is undeniable. Any functional language should include them. However, I don't feel any language has done them quite right yet. 
The link is in the first paragraph and the link is indeed found on the website. It's a UReddit class and is therefore listed in the computer science section of the catalog at http://ureddit.com homepage.
Oh, I'd seen that before, but it looked like just a series of lectures, not a "class". What do I get from enrolling? I don't care about credentials, but I do care about getting the right assignments to test my skill, feedback on my work on them, and a forum that's doing the same thing with me. Unfortunately, I'm already enrolled in another related course at Coursera, "Programming Languages", which discusses PL design and the tradeoffs therein, with an emphasis on functional programming and using Standard ML, Racket, and Ruby as the working languages. I'm way behind in it too and only enrolled a week late. Hopefully I'll at least learn how I "would have done" on the HWs.
Depends what you're looking for. If you just want to use it, all you need is it's type: build :: forall a. (forall b. (a -&gt; b -&gt; b) -&gt; b -&gt; b) -&gt; [a] Okay, maybe it'd be more helpful to get a link to the [documentation](http://hackage.haskell.org/packages/archive/base/latest/doc/html/GHC-Exts.html#v:build) which explains that: build g = g (:) [] That is, the generator you pass to build will receive two arguments. Intuitively we can think of the arguments as being the two constructors for lists (aka the "initial list-algebra"); however, in truth, the rewrite rules make it so that we may end up getting something else (i.e., some other list-algebra)--- which is the whole point. But, so long as we treat them as if they were the constructors then we're golden. So, if you're building a list and all you really need are the constructors, then you can enable list fusion by abstracting over the constructors and wrapping the whole thing in `build`. If you want to understand a bit more about the theory behind it, I recommend [A Short Cut to Deforestation](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.51.646) as a good starting place. It comes at things from the calculational perspective (i.e., rewrite rules), rather than getting into the recursion theory behind it all. After that, it might be good to take a look at [Shortcut Deforestation in Calculational Form](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.36.4082) or [Warm Fusion: Deriving build-catas from recursive definitions](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.39.5086); which are, again, more calculational rather than recursion theoretic. Related is [Rewriting Haskell Strings](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.90.3166) which was the paper introducing `ByteString`, which also relies on these tricks. Some other classic papers in this area are [The Concatenate Vanishes](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.118.455) and [Stream Fusion: From lists to streams to nothing at all](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.104.7401). While googling for the urls above, I just came across [Exploiting algebra/coalgebra duality for program fusion extensions](http://www.fing.edu.uy/inco/proyectos/fusion/papers/ldta11.pdf); haven't read it, but from a quick glance it looks like a pretty good introduction. The calculational perspective is a great way to get started, but as some point you will want to delve into the recursion theory in order to really get a handle on how/why it all works and how it can be extended to other data types. It's definitely good to get the calculational perspective under your belt beforehand though. Once you want to get into the recursion theory, papers by Johann &amp; Ghani tend to be pretty easy to read. Once you're more comfortable with the recursion theory, Uustalu &amp; Vene are an excellent read--- but they're definitely not a good place to start before reading the other links above.
Sure, it's library-level. I was just pointing out that the magic isn't just in the rules, it's also in being sure that definitions are phrased in such a way that the rules will fire. Prior to the introduction of rewrite rules in GHC, there was a lot of work on how to do list fusion--- work which *did* rely on the fact that lists are (or could be) a primitive type. It's only through people grinding out all of those details that we finally came to the point where we understand the optimizations well enough to be able to implement them at the library level rather than as built-in primitives. The use of functions like `build` and `augment` is crucial to this theory, and without them we could not implement this at the library level. Rewrite rules are only a small part of the story, they're the final capstone to library-level implementation.
If you sign up and add it to your account, you'll get the announcements of new lectures and so on that the teacher sends out. Past that, you can get in touch with the teacher if you have questions or want feedback on your code. UReddit lets teachers conduct their classes however they like, so it's up to /u/CarbonFire.
I had noticed I forgot to put the link so I edited it in and came back a few hours later to find the post open and that I had failed to actually press "update" so yes, it has been edited in. Sorry about that.
No problem, thanks for the fix and owning up to it! (And, er, reassuring me I'm not going senile.)
I don't have a very sophisticated answer to this, but having tried a little bit of th vs a title bit of Racket macros, the latter feels *much* more approachable. 
&gt; However, if we continue to extend the language with (blah), then the need for that particular case, for example, goes away. You have mentioned a few special cases of macros that have been baked into the Haskell language. This reminds me of the old Scheme saying. &gt; Programming languages should be designed not by piling feature on top of feature, but by removing the weaknesses and restrictions that make additional features appear necessary.
But the baked-in stuff is better than we could get with even the best macros! Type-level nats done right are much more potentially performant than just using peanos, no matter the sugar on the latter. Similarly, a proper generic deriving mechanism is much more typesafe, sound, and extensible than the equivalent available from macros, which is necessarily somewhat ad-hoc.
Well, in the end Lisp macros are similar to TH except they don't need a new syntax to be invoked, so they look "cleaner".
&gt; Haskell is a relatively new programming language that, though gaining in popularity, is not nearly as widespread as C or Java. Just wondering: Is Java regarded being a "relatively new programming language" as well?
I wouldn't call that OO...
well it's maybe a bit far-stretched to call it OO but we then have some sort of "methods" that are stateful only on a specific portion of data, and ways of composing them.
Exactly! There's been a lot of confusion recently about what OOP is. Go grab your copy of [TAPL](http://www.cis.upenn.edu/~bcpierce/tapl/) and learn yourself some OOP.
See [`Control.Lens.Zoom.zoom`](http://hackage.haskell.org/packages/archive/lens/latest/doc/html/Control-Lens-Zoom.html#v:zoom).
Which other contenders and awful features are you thinking of, besides C++?
Oh, right. Yeah, you're preaching to the choir here :) I thought you might have some other interesting (bad) languages in mind I hadn't considered or heard about. I plug it oftener than healthy, but if you want a C-like language with &gt; a module system, sum types and pattern matching, parametric and ad-hoc polymorphism, RAII and proper consts [take a look at Rust][1]. I can barely wait for it to stabilize (probably some time this year). [1]: http://static.rust-lang.org/doc/tutorial.html
Just fyi (I am pretty biased in this anyway), but given that list of requirements, XMPP would have been a much, much better fit.
I've never understood thing line of work. I come out of OO, and I would never say I miss its syntax in Haskell :P
Sure, I didn't mean to imply that it's just a matter of blasting out a bunch of list functions and then special casing all possible compositions of them. However, the approach is more general than just lists, too.
I claim that non-intrusive linked lists are *still* useful. With an appropriately fast allocator you can get an efficient, dynamically growing and shrinking stack without relying on any sort of amortization. Also, while it takes some time to walk from the head toward the tail, once you have done the work you can keep a pointer to where you were, continue modifying the head (so long as you are sure you would remove enough elements to reach your pointer), and pick back up where you were again in constant time. Also, as I said before, if it's doubly-linked you get some other fun abilities, like inserting and removing in the middle, something like a mutable zipper. Of course they don't always win against arrays or whatever else is in your toolbox, but that's not the point.
Have you tried implementing XMPP? IRC is simple to implement to the point of being underspecified, but I'll take that any day over XMPP's hairy bloated mess.
My opinion is that "type-level nats done right" and "a proper generic deriving mechanism" should be implementable as a library via macros, rather than having to hack the compiler to support them. In my eyes, if a macro system fails to satisfy this goal, then this doesn't mean macros are not good enough, it means that particular macro system isn't good enough.
Check out http://hackage.haskell.org/packages/archive/mtl/2.0.0.0/doc/html/Control-Monad-RWS-Lazy.html#t:RWST
Why? Do we have a need for super fast IRC servers? Did he say this was designed for speed?
&gt; Both the Lisp and Haskell communities have the concept of code as data. However, the dominant such concept is very different in each community. The Lisp concept of code as data is very **syntactic**, and embodied by homoiconicity, macros and eval. The Haskell one is very **semantic**, and embodied by monads, free monads and do-notation. This might be the most interesting thing I've read in a while in proportion to the number of characters. I hadn't realised that but it makes so much sense. Thanks for enlightening me a little.
Side-note: The post is more than 2 years old and the repository has seen no activity for 9 months.
If you have the time, I would really recommend you to learn Lisp at least to the point where you have gotten a glimpse of what Lisp macros mean. I mean this in the same way as I do when I say that I recommend people to learn Haskell until they get the whole purity and type system thing. Both are concepts that will enlighten you a bit.
It is entirely possible, though, that these benefits of non-intrusive lists will be swallowed by the huge constants involved in pointer-chasing. Arrays removal via large memmove's and such are likely to beat list removal in the middle if you do in fact have to chase N pointers to do so.
&gt;I'm curious how the author is going to do the automatic backlogging he describes, because IRC has no way to say "this message was originally sent at 12:34". Indeed. BNC clients do this by recording message arrival time serverside, fetching messages that were received when the client was detached, and dumping them all down to the client, but this results in the client just seeing a bunch of messages all suddenly appearing with the same timestamp. Of course, this is really ugly. I would prefer to run irssi in a screen session or use a "distributed IRC client" like [Quassel](https://www.smuxi.org/main/) or [Smuxi](https://www.smuxi.org/main/), which don't have this problem.
Thank you Simon. I will carve out some time to do some reading and commenting.
But my point is that there are cases where you don't have to do this kind of pointer chasing because you already have the pointer to the list node that you need.
That was great. Thank you so much! [pdf version](http://www.c3.lanl.gov/~kei/deforestation-short-cut.pdf)
In STL speak, for example, that's a list iterator. A pointer to your list node (data) does not allow you to do any operations on it, STL-wise. If I recall STL correctly, plenty of operations invalidate the list iterators, meaning that keeping hold of list iterators is more dangerous and less useful.
I'm learning things already. I found it a really good approach to explaining the difference between WHNF and NF, as well as the details of `seq`, `deepseq`, `rpar`, and `rseq`, something that most tutorials I've read online do a poor job of. As someone who is still pretty new to Haskell, it was quite informative, and I look forward to seeing the rest of the book.
I've been looking forward to this book for a long time now, happy to see (parts of) how it turned out!
But I wasn't talking about whatever STL does. I'm talking about linked lists.
Non intrusive linked lists can in theory have a pointer from the data back to the node and then they just add indirection overhead, error conditions, and extra penalty. But all the correct asymptotics are there. I was talking about STL as a posterboy of bad non intrusive lists. 
Well, riffing on winterkoninkje's answer, this is the type of `build`: build :: forall a. (forall b. (a -&gt; b -&gt; b) -&gt; b -&gt; b) -&gt; [a] But let's single out the type of `build`'s argument: -- | The Church encoding of lists. type ChurchList a = forall b. (a -&gt; b -&gt; b) -&gt; b -&gt; b -- | Folding a ChurchList is just applying it to the fold arguments. foldrChurch f z xs = xs f z [Church encoding](http://en.wikipedia.org/wiki/Church_encoding) is a technique that represents data structures as functions. In the case of lists, you can think of their Church encoding as the partial application of `foldr` to that list, so that you cannot pattern match on the list and the only way you can access it is to pass in the two other `foldr` arguments. So: -- | Turn a concrete list into a Church-encoded one. toChurchList :: [a] -&gt; ChurchList a toChurchList = \f z -&gt; foldr f z But we don't need to resort to the `[a]` type to build Church-encoded lists. We can express the constructors directly: empty :: ChurchList a empty = \f z -&gt; z cons :: a -&gt; ChurchList a -&gt; ChurchList a cons x xs = \f z -&gt; f x (xs f z) In summary, a Church-encoded list is a list that exposes `foldr` as the only way of accessing it. Such a list can be represented as no more than a function. So now, back to `build`. We can rewrite its type as follows: build :: ChurchList a -&gt; [a] build churchList = churchList (:) [] So `build` just turns a `ChurchList` into a concrete list. Now one of the keys to `foldr/build` fusion is this equation: foldr f z (build churchList) == churchList f z Since the `ChurchList` knows how to fold itself already, we don't need to construct its cons cells to fold it.
I actually can't wait for my commute tomorrow now! The part on concurrency is a lot of material that I really ought to know but don't, and a cursory glance shows that it's already shaping up to be a fantastic book.
I'm looking forward to more content, so far it's a lot of fun. 
I think the real problem is academia needs to separate "Computer Science" and "Software Engineering."
But efficient type-level nats are about changing the typechecker, not generating syntax!
Thanks for making this available for review comments. I have read through a few chapters and posted a bunch of comments. Sorry if some are too nitpicky, e.g., about British spelling, Oxford comma, etc. I hope to get through the other chapters in the next few days. This looks like it will be a great addition to available books on Haskell.
This is generally considered an anti-pattern; google for "existential type class anti-pattern" and you'll find stuff.
I think was PthariensFlame was pointing at is mentioned here (http://www.reddit.com/r/haskell/comments/18rcc6/easy_in_oop_harder_in_fp_list_of_animals/ - yeah I'm the stupid one bloging a anti-pattern) and the article he mentiones is here: http://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/ There are some nice "escapes" mentioned in the reddit comments to my post so I don't think I have to copy them here. In any case - maybe I'm stupid again but I kind of like the way you use it here - even if it might be better to use a simple algebraic type if you don't need the extensibility of Value
What do you want to with a "Value" value ? If all you plan to do is outputing it in the terminal, then this design will do the job. But that does not make much sense to me, since I don't see what convenience or safety such a design brings over the trivial ADT design.
Hmm. Looking at the tutorial again it's a bit murkier than I remembered (esp. which syntax is old and which is new). But my firm impression is that while they make some interesting syntactic choices with self/Self, it is effectively type classes. You can write the same things, it just might look funny. I.e. you can have multiple parameters of the self type, you can have the self type in the result, or as a type argument of another type, whatever. I don't think there's anything missing. Do you have counterevidence? And then they have a built-in feature to do what in Haskell you would do with existentials, which is what they call the OO half of it. The big thing they left out relative to Haskell is constructor classes, which might come after 1.0. There's also been talk of associated types...
SimonM is British. :) 
Well he can't help that, he's doing his best :)
&gt; I.e. you can have multiple parameters of the self type, you can have the self type in the result, or as a type argument of another type, whatever If I understood Rust correctly, you can't have return-type polymorphism, and as you said, you don't have higher-kinded types. So they're very far from having type-class support that can support e.g: class Monad. 
Spotted a typo: "programer" 
There is a more obvious approach. When you existentially quantify your value, you basically say "The only thing we can do with this value is `show` it". So why not just replace every `Value` with the `String` you would have gotten had you shown it? This means that all you have to do is replace all `Value` types in your code with `String`, and all `Value` constructors with `show`: literal :: Parser String literal = show &lt;$&gt; Literal &lt;$&gt; stringLiteral hex :: Parser String hex = show &lt;$&gt; Hex &lt;$&gt; hexparse varcall :: Parser String varcall = show &lt;$&gt; VarCall &lt;$&gt; varname Haskell is lazy, so it won't actually compute the `String` until you need it. `show` replaces your `Value` constructor, holding the `String` you wanted until you actually need it. Encapsulating your values within this existentially quantified `Value` constructor doesn't preserve any information other than their eventual shown `String`, so you don't lose any information by just storing the shown `String` instead. There are cases where existential quantification makes sense, and they are the cases where you cannot translate the given type class into an equivalent [record of functions](http://www.haskellforall.com/2012/05/scrap-your-type-classes.html).
Right, you can't have Monad (yet). But you definitely *can* have return type polymorphism. I'll quote: &gt; Traits can also define static methods which are called by prefixing the method name with the trait name. The compiler will use type inference to decide which implementation to call. &gt; &gt; trait Shape { static fn new(area: float) -&gt; Self; } &gt; &gt; struct Circle { radius: float } &gt; &gt; struct Square { length: float } &gt; impl Shape for Circle { &gt; &gt; static fn new(area: float) -&gt; Circle { Circle { radius: sqrt(area / pi) } } &gt; } &gt; &gt; impl Shape for Square { &gt; &gt; static fn new(area: float) -&gt; Square { Square { length: sqrt(area) } } &gt; } &gt; &gt; let area = 42.5; &gt; &gt; let c: Circle = Shape::new(area); &gt; &gt; let s: Square = Shape::new(area); (the "static" keyword will be going away)
No, that was actually quite good. I'm very receptive to ideas about mixing information theory with programming. I've always thought it is a very fertile avenue for research.
&gt; "The only thing we can do with this value is show it" That's not totally correct. You can also pass the existentially quantified value to a function taking a `Show a =&gt; a` (e.g. traceShow), which you can't do (and expect the same output) with just the `String`.
Yeah, I glossed over that, and also the fact that you could use `shows` or `showsPrec`, too, but it at least builds an intuition for when to use it and when not to use it.
Do you have a macro system in mind that could provide efficient type level nats then?
Yeah, it wasn't written with speed in mind. After someone (skeptical of why Haskell was a good choice) challenged me about how it could handle scale, I did do an `ab`-equivalent stress test with the then-recent GHC 7.something with the event-based IO scheduler with 10k active users (as in connected and sending a line of data per second) and it handled it. Freenode's Gibson server's highest connection count is 5291. So I don't think much above that would be a realistic test.
There's nothing wrong with trying new things! That's how we learn.
I considered this after someone pointed it out to me after the fact. We were already agreed that we liked IRC (we used it for Freenode and such) and IRC (and its clients) is great for group discussion, at the time we hadn't even thought of XMPP as a thing you'd use for that, felt more â€œIM-yâ€, one-on-one. In future work I'd re-evaluate XMPP, especially now that there are many libraries for it on Hackage.
[Here](http://www.reddit.com/r/haskell/comments/194ngx/hulk_a_haskell_irc_server_nice_example_of_real/c8lclf7) was my solution.
Yeah, it's odd how I hadn't heard of that at the time. I use that these days.
Correct, O'Reilly likes to use wrong spellings consistently, whereas I'm only using them inconsistently at the moment :)
Yes, I think that's true. My impression is that when you want to add some parallelism to an existing Haskell program, 80% or so of the time a parMap will do a good job. This explains why I've had some difficulty finding good motivating examples that are not parMap (but I hope I've found some). 
Actually the author did mention Quick check and having written the code specifically to be able to use it. Just no examples of tests were shown.
Hardly anything invalidates a list iterator. As far as I can tell, the only way a list iterator can be invalidated is if the item it points to is removed from the container. You're probably thinking of vectors iterators, which indeed are invalidated by many operations.
It might help to mention where.
Yes, epub I believe, also HTML online (CC licensed).
Thanks!
Why, is it written on papyrus? 
I believe I heard this suggestion from Johan Tibell on Google+ a while ago. Certainly for small values (Ints, Bools, etc), but even for slightly larger collections. Sadly, I can't find the post and comment thread now.
Default strict for data type fields would be nice. We can do it right now by always putting !s and removing them only when needed. However, keep in mind that this won't solve all space leaks. For example, you may be holding a list, and the bang will evaluate it only to WHNF.
Nope. Only in my dreams, I suppose.
Do note that the Typed Racket type system is implemented via macros. I'm not sure how "efficient" it is, though. All values are also types, so there we have "type level nats", but I don't believe TR provides any sort of type-level computation.
That's good to know. Thank you :-) I vaguely remember being bitten by this some time ago --- I don't remember if I was using ghc 7.0.3/4 or 6.x, and it's always possible I mucked it up somehow.
 newtype Showable = Showable (Int -&gt; ShowS) instance Show Showable where showsPrec d (Showable f) = f d Now you can store that rather than the string, and no instances appear. 
So what's the status of this book? When can we expect preorders? :-) Since a couple of months ago I think everyone is afraid Facebook will maybe eat it.
maybe: http://www.scs.stanford.edu/11au-cs240h/notes/performance.html#%2831%29 and, http://www.scs.stanford.edu/11au-cs240h/notes/perf-slides.html#%2841%29
L'est la vie :)
C'est la vie :)
It really depends a lot on what you're using the ADT for. As a mindless guideline, I'm against this for the same reason I'm against eagerness by default. However, there're definitely a large class of ADTs where this is a good guideline. In particular, records and the like tend to want to be eager since they're just a helpful way of grouping values together (note the key word: *values*), and (more particularly) there's no meaningfulness in having a partially defined one of 'em. However, with recursive types the situation is far more intricate. Lists, for example, get great milage out of not being spine-strict. Whereas IntMaps would be a nightmare if they weren't spine-strict. There's a definite bias towards people making everything eager simply because that's how other languages work, but that doesn't mean eagerness is "better" in any sense whatsoever. Eagerness ruins equational reasoning. Because of the eagerness bias, people are used to the failure of equational reasoning, so this is often overlooked. But, one of the greatest benefits of Haskell (and of laziness) is the fact that it *does* allow equational reasoning! This isn't something that should be discarded lightly. Also, eagerness often results in doing too much work (just as laziness often results in doing too little). Many of the great advances in algorithms over the years have been people figuring out how to make things *lazier*; that is, how to avoid or delay doing the work that an eager model of computation forces you into. Both eagerness and laziness have their place, and there's always a tradeoff between them. The biggest thing is to really sit down and consider what the differences are between eager and lazy ADTs and what that eagerness/laziness signifies in the semantics of how that ADT can be constructed and consumed. This is tied up in semantic concerns about what the differences are between "values", "expressions", and "computations" ---distinctions which are poorly taught IMO--- so there are no easy answers. But this is precisely the sort of thing one needs to think about, rather than blindly making everything lazy or everything eager. Because both of those "solutions" are wrong; the proper answer, as always, lies somewhere in the middle.
It's now in my style guide: https://github.com/tibbe/haskell-style-guide/blob/master/haskell-style.md#dealing-with-laziness
It is actually. Subreddits are also known "reddits." Check [the FAQ](http://www.reddit.com/wiki/faq#wiki_individual_reddits), and you will see that usage all over the place. 
Wow awesome
Haskell Platform's installer has a "portable install" mode that allows you to install it as a non-admin. See http://stackoverflow.com/questions/14350161/is-there-a-way-to-get-a-haskell-setup-on-windows-without-an-installation-copy/14350419
Note that if a package follows the [Package Versioning Policy](http://www.haskell.org/haskellwiki/Package_versioning_policy) for specifying its dependencies, and it wants to depend on a range of minor versions (e.g. Major1.Major2.* instead of Major1.Major2.Minor.*) it should only use explicit or qualified imports for its external dependencies. That means the import should look like this: import Data.Text (Text) import Data.Map as Map Many high quality packages (like `text`) use this style. It can be a bit of a pain to maintain an import list like this when changing code, so I often don't do this when in development mode, then switch to this style when things stabilize a bit.
You may want to have a look at [LiquidHaskell](http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/about/) which lets you encode invariants in specially-formatted comments.
except lisp syntax isn't bad, it is just lack of syntax so i think that there is some trade-off here, who knows?
Take my money already.
This makes total sense. It must be the `Haskell :: Reddit` from the definition data Reddit = Haskell | Programming | Fitness | Politics | Pics ...
If I cause a compile error, it turns the result box for that code red. If I fix it and run the code successfully afterwards, it stays red. That part seems like a bug.
Contract programming is an approach for designing software where you specify the interface of components by providing preconditions, postconditions, and invariants. Typechecking is proving a theorem that your program will evaluate to a member of a certain set. These theorems can be as simple as "x is an integer" or as complicated as "x is the text of a program which prints the complete works of Shakespeare," but the basic idea is the same. While typechecking is always done on your actual program, contracts can also be embedded in a program as assertions. If you then prove these assertions (like Spec# does: http://rise4fun.com/SpecSharp ), you've got yourself a little type system. In a language such as Ruby, which only has one type, the "Any" type, by writing "Contract Num =&gt; Num" you're essentially refining the type of your method to {x : Any | x.class == Num} -&gt; {y : Any | y.class==Num}." Checking contracts at runtime is basically trying to prove the contract by doing case analysis on every possible input, but in a lazy fashion. It fails pretty hard if your return value is a function, and you need to check infinitely many cases just to see if your program was right even once.
Did this tutorial to experiment with FP Complete's School of Haskell. The regular expression matcher is borrowed from Olivier Danvy's tech report "Defunctionalization at Work". I found the use of OverloadStrings is a neat trick to embed regexps in Haskell. Any comments are welcome!
I seem to recall comments from various clever people about how lazy data structures should really be viewed as control structures, reified. Which I guess leads to the conclusion that if what you're really doing is data, then it should be strict; and if you want it to be lazy, then you should keep in mind that it will be intertwined with control flow --- c.f. lazy Bytestrings.
Sadly Kindle can't read EPUB. :-(
Why would non-spine-strict IntMaps be a nightmare?
Yes, it'll be available for Kindle.
It would be a bit surprising that all your inserts take O(1) time and your map function O(n*log n). It's also a constant factor slower.
It's already on the front page: http://www.reddit.com/r/haskell/comments/197ax4/chasing_a_space_leak_in_shake_profiling_reduction/
lisp has syntax. You can parse it can't you? It has a very simple base lexical syntax, but you still have to choose some things when defining how you choose to use that lexical syntax like dolio mentioned. Many of those alternatives require macros. It is a common pretension to say that lisp doesn't have syntax, but then when you go to define a function you need to remember if it is (define f (a b c) body) (define (f a b c) body) (define f a b c body) ... There is a _syntactic_ distinction being made between these design choices, even if this selection does not change the lexical structure of the language.
&gt;So, does anyone have a good rule of thumb for when you should code things in continuation / function style and when you should code things in ADT style? This is a very good question for which I don't have a general answer. On specific case when there is need to serialize data (i.e. to/from files) a first-order representation seems more convenient. BTW, this regexp example was a real eye-opener regarding the usefulness of continuations for me. I find the matcher very clear and readable -- which is not generally the case with continuations.
If you wanted JavaScript, then why didnâ€™t you use that?
Cool, thanks! Looking forward to reading it.
Thanks for the package! Maybe you can put the reverse-state monad into the tardis package, too. I think it would fit in well. Kind of off-topic, and you probably know about it already: I found that you can nicely tie the knot with the Tardis monad like this: tie knot = evalTardis (knot &lt;* (getPast &gt;&gt;= sendPast)) which achieves exactly the same as [this post](http://mergeconflict.com/tying-the-knot-redux/) describes. Using the RWS monad however does not allow you to send something to the past only. Edit: another elegant use I found for your monad is memory cell allocation in my toy compiler. Using Tardis, I can get the precise allocation on the fly (by using getFuture), and send information needed to compute the allocation to the future in the same spot. Pretty cool! However, as I said, I am kind of afraid that this is really inefficient.
Also, if you want to have runtime type errors, you can do that without sprinkling Dynamic throughout your code: http://hackage.haskell.org/trac/ghc/wiki/DeferErrorsToRuntime BTW, Haskell can be used as a Javascript replacement! https://github.com/faylang/fay/wiki https://github.com/ghcjs/ghcjs
The point of `Dynamic` is a little different than â€œI don't like static typesâ€, more that â€œI have the use-case that the static type system can't or shouldn't handle.â€ It saves you using existentially quantified types when you don't need or want them, for example.
Shame that `OverloadedLists` isn't applicable here!
Oh, certainly. I should have made that clear. I was just telling EvilMachine about Dynamic as a bad solution to heterogenous lists last night on #haskell. He asked how to do heterogenous lists without existential quantification. One way this can work is to reify the typeclass as a datatype, where the typevariables are replaced with Dynamic, and store it along with the Dynamic value. This only works in a mechanistic fashion when the type variables are used in containerlike (Traversable-having) things. Of course, by using Dynamic you're sacrificing safety (as usual) Also, while I have your attention, is there a reason that my pull requests to fay-base haven't been merged?
Nice catch - adding a support ticket for this. Thanks!
&gt; Also, while I have your attention, is there a reason that my pull requests to fay-base haven't been merged? I've been ill since Friday, so I have an inbox of stuff to go through (there doesn't appear to be a way to tell the whole internet â€œI'm too sick to deal with anything at the momentâ€). I'll probably start some time tomorrow. Now, I go to bed.
&gt; Green on black Hmpf.
The classic example is the following simple program to compute the mean of a list of floating point numbers: mean xs = sum xs / fromIntegral (length xs) main = print $ mean [1..1e7] If I compile and run this program $ ghc mean.hs $ ./mean Stack space overflow: current size 8388608 bytes. Use `+RTS -Ksize -RTS' to increase it. Oh dear, stack overflow. The reason is that the even though the list is generated and consumed lazily by sum, it has to be kept around in memory because it's later used in a call to length. A better approach is to compute the mean in a single pass using a strict fold, making the fields strict: {-# LANGUAGE BangPatterns #-} import Data.List mean xs = tot / len where (tot, len) = foldl' fun (0, 0) xs fun (!tot, !len) x = (tot+x, len+1) This works: $ ghc mean.hs $ ./mean 5000000.5 I think the first version is what a newcomer to the language would write, if they weren't thinking particularly hard about laziness.
The problem here is that `Prelude.sum` isn't strict when summing `Double`s. No, I don't know why that is.
Very cool. And nicely presented! In polymorphic cases, with few type constraints, I could imagine this procedure being relatively easy to automate in an IDE. Is that something that is being worked on at all?
This is a solution to something that can be solved rather nicely with built-in support from the language. Just take a look at Agda, which has been doing this kind of thing in Agdamode for years. Holes change how you program at a fundamental level, and often, this kind of programming is described as having a conversation with the type checker.
Well, to be fair, static types can totally handle that use-case. But you have to use the right data structure, and a plain old List ain't it.
I bet you did it in Cayenne, didn't you!
It was done less long before Cayenne. In Alf, for one. But there was (at least) a problem which boiled down to putting too much trust in not-obviously-but-actually-unprovable constraints, resulting in provisionally well typed programs being prematurely executed and going wrong. The problem persisted in Agda 1. But perhaps before Agda 2 there was another system with holes which did not have this (particular) problem...
Do you mind elaborating on that point? Why would you need to evaluate programs in incoherent contexts?
Well exactly. You don't. The mistake was to construct on the one hand the term which the system hoped was well typed, and separately to accumulate the constraints upon which that status depended, but to treat the term as well typed pro tem. The fix, as implemented (for the second time) in Agda 2 is to allow a hole ?x the status of "to be solved with t as soon as ?x's type agrees with t's", but only to perform the instantiation when that status is secure. You can see this in the "show constraints buffer". Way back when, ?x was instantiated with t anyway: the constraint was recorded as a separate wish, but the damage was done. The account of holes I gave in chapter 2 of my thesis made the then radical demand that a hole be mentionable in goals (unlike in Coq) and instantiated only with something of the right type (unlike in Alf) and the right scope (unlike in Lego). What a pain in the arse!
Wow, that was 16 minutes well spent. Everything was directly on point, with no "ums" or typos. If that was done extemporaneously I am very impressed and more than a little jealous.
I'm not so sure this example of Tardis is really any more elegant. Truth is, I don't currently know of any "practical" applications of the Reverse State Monad. Nor do I know of any practical applications of the Codensity's mfix operator. Although, part of your problem here is that Booleans are a flat type. You might find a more practical example of Tardis if you build up some kind of non-flat type, perhaps future indexes of a given element.
That is a really annoying fallacy of some Haskell programmers who come from a mathematic academic background: They assume that all you need to know, is the type, and youâ€™ll know what it does. As Richard Feynman said: Mathematicians only care for the *structure*. Not the *meaning*. But in practice, you *have* to have meaning. Otherwise it becomes all meaningless nonsense. Apparently they never seem to realize, that *thereâ€™s more than one way of transforming something between certain types*. Especially with that other anti-pattern of completely meaningless one-letter type variables (and normal variables) that mathematicians love so much. Just find the functions in the Haskell libraries, that have the *exact* same types, but do *completely* different things. There are lots and lots of those. Good luck writing those in a â€œhole-drivenâ€ way. What he shows only works for the most abstract basic algorithms there are. And only by adding a shovel of willful ignorance.
I'm going to patent that: a process for automating update of a large scale heterogeneous network with status notifications. In all likelihood, it has already been patented.
Your so-called meaningless one-letter type variables are just the simplest thing to use when doing highly polymorphic things. And in highly polymorphic contexts, there often is only one (canonical) term of the desired type. As soon as you start nailing down the "meaningless type variables" as you so inanely refer to them, then a multitude of possible implementations present themselves, and it's rather hard to pick the right one using so-called hole-driven development, or some more advanced kind of term inference. That's why polymorphic functions (like `(&gt;&gt;=)` and `(.)`) are so great! Because usually there are very, very few terms that can possibly represent them. So it's possible to use holes to great profit. (But you do bring to light a salient point, when you note that the number of commonly-used functions that are of this sort is pretty low.) This is why more polymorphism is better than less: it hasn't as much to do with code reuse, but rather relates most to narrowing the field of possible implementations. It is in this context that holes (and term inference) are useful. 
&gt; thereâ€™s more than one way of transforming something between certain types With sufficiently precise types, there in fact is only one way (up to isomorphism). This is realized in a number of different contexts (homotopy type theory is one place this gets really pushed to an extreme), but one of the more accessible ways to approach this might be via streams and unique fixed points: http://aszt.inf.elte.hu/~cefp2009/materials/papers/Hinze.pdf
It sounds like an instance of the &lt;3 desirable properties&gt;; &lt;choose exactly 2 of them&gt; trope.
&gt; That is a really annoying fallacy of some Haskell programmers who come from a mathematic academic background: They assume that all you need to know, is the type, and youâ€™ll know what it does. Who's this "some Haskell programmers with mathematic background" person you're talking about?
I often use zero-letter variable names. For example writing h x = x &gt;&gt;= f &gt;&gt;= g as h = g &lt;=&lt; f 
Right, crntaylor's example doesn't do exactly what is intended. Here is a corrected version. % cat mean.hs import Data.List sum' = foldl' (+) 0 mean xs = sum' xs / fromIntegral (length xs) main = print $ mean [1..1e7] % ghc -fforce-recomp -rtsopts mean.hs &amp;&amp; ./mean +RTS -M5M [1 of 1] Compiling Main ( mean.hs, mean.o ) Linking mean ... Heap exhausted; Current maximum heap size is 5242880 bytes (5 MB); use `+RTS -M&lt;size&gt;' to increase it. The fact that idiomatic Haskell treats data structures as generators ends up causing these problems. The conceptually simplest fix is to just do what a strict programmer would expect and recreate the generator "xs" each time. (crntaylor's suggested method of doing it in a single pass is neater though and generalises nicely to an API of stream combinators ...) % cat mean.hs import Data.List sum' = foldl' (+) 0 mean xs = sum' (xs ()) / fromIntegral (length (xs ())) main = print $ mean (\() -&gt; [1..1e7]) % ghc -fforce-recomp -rtsopts mean.hs &amp;&amp; ./mean +RTS -M5M [1 of 1] Compiling Main ( mean.hs, mean.o ) Linking mean ... 5000000.5 
But his example was referring to stack space (taken by thunk build up) and not heap space (un-gc'd list items), right? *EDIT*: So it seems that I misinterpreted the poster's intention due to the bad example.
`-XTypeHoles` will be a part of GHC 7.8. :) edit: maybe I should've continued listening until the last sentence. The rest was also definitely worth it.
The purpose of holes is to allow you to partially specify a program. Holes don't force you to make any decisions about how to implement your program, they just let you leave parts of it undefined. Any suggestion to the contrary was an error on the part of the guy in the video. As an example, consider the trivial program f :: Int -&gt; Int f n = Hole The hole must have type `Int` so we know we can give back any `Int` we like. One such `Int` is `n`, so we can give that back: f n = n Another alternative is to use some sort of mathy function that produces an `Int`: f n = negate Hole again the hole has type `Int` so we're free to fill it with whatever we like. For instance, f n = negate n Obviously this is going to require that you have some sense of where you're going with the program. Having holes doesn't change that. It does, however, change how you write programs, letting you compartmentalize the subproblems. The same technique could be used in any programming language.
Well, the example is only the beginning. In my full example, I have a list like [Start 1, Start 2, Start 3, StartLoop, Start 3, EndLoop, ...] and for each distinct "Start x" I want to insert an "End x" into the list after the last occurrence of x, but defer it to after the EndLoop in some cases. In the traditional approach, I would need one pass to go over the list with `lastOccurrences`, and in a second pass go through the elements (zipped with the result of `lastOccurrences`) and emit the needed elements in a writer monad. With the Tardis monad, I can do it all in one go. See my other comment for another practical application (where I again can collapse two passes into one). Why is a list of Booleans a problem here? I did not quite understand that.
Sure, but I also think it improves that. The author spends a lot of time writing down assertions. Many of those might be trivial, but some are not as easy to determine "manually". With `-XTypeHoles`, you get a list of all relevant bindings when using a hole. The author also limits himself to one noisy hole at a time. If you were to add multiple noisy holes, then the first error will probably cause the other noisy hole to not be reported. You might even have to deal with the compiler putting the blame on something else than the `Hole`, therefore not being able to figure out what you wanted to know. With `-XTypeHoles`, your intention is clear to the compiler and it will try to help you, instead of it having to deal with you trying to trip it intentionally. :P
Er.. I think you've misunderstood my point. Having holes with no interactivity is silly. Right now, we have holes, but no interactivity. And the holes will need to be language level, not bolted on like this.
The extension that should be in 7.8 has what I think you're referring to (I've not used agda) where you put _ in place of the implementation and it gives you the type expected and possible implementations that are in scope. With good information like that coming back from the compiler, anything on top feels like an IDE job to smooth the workflow.
No no no, `_` isn't a hole in Agda, it's an inferable value. `?` is a hole. It also doesn't give you possible implementations that are in scope, it only lists the variables in scope (and only those brought into scope by binders, so not definitions). But yes, it's an IDE's job, which is my point. Without the appropriate IDE to enable the interactive component, holes are almost pointless.
It's because the syntax is magic for most (novelty/different to what people are used), so common pitfalls are not seen.
I must say, articles written on the School of Haskell are one step better than Literate Haskell, I don't even have to stop reading to take it to bits and play about. Very nice.
He's not talking about Agda, he's talking about how it works in GHC. Yes, there currently isn't much interactivity, but I don't think that should be the compiler's job. I like how it works in Agda, but it's annoying it restricts you to Emacs. There could be some support of filling holes within ghci, but ghci is also not an editor (it would probably do a bad job when it tried writing back a source file after filling a hole). This is something an IDE or text editor would need to support.
I'm confused. What is it that takes up space in the first program? Is it the list `[1..1e7]`? Wouldn't it take up equally much space if it was eagerly evaluated?
That's such an unfortunate coincidence. There has to be some faulty HTML generation/rendering involved and it managed to make its way all over to the printed version.
If you made this video, I just want to say that you have a very nice voice.
It's an untyped(?) functional language that does *everything* using asynchronous light weight processes under a distributed memory model. Programs tend to use thousands of threads and everything is done by message passing, which is built into the language (and very nice to use). Scroll down a bit to the blue envelope and read until the kool-aid part. http://learnyousomeerlang.com/introduction#what-is-erlang 
`sum` returns a pile of thunks. Forcing the first thunk means forcing the second means forcing the third, etc. each time you are adding to the stack. So it runs out of stack space.
&gt;Apparently they never seem to realize, that thereâ€™s more than one way of transforming something between certain types. Your types are not expressive enough, then.
I'm still not sure I get it. I'd love it if you could point me to anywhere where there's more reading about this.
&gt; pi is just a function with zero-arity I don't like this use of terminology. Pi is a value. Functions are values of function type. Pi is not of function type.
Have you heard of the term CAF (constant applicative form), which is basically a function without arguments? http://www.haskell.org/haskellwiki/Constant_applicative_form It's different from a function, as pi' x = 3.14 calculates "3.14" every time you call it, while pi = 3.14 only keeps a single copy of pi in memory and points to it when invoked.
Sorry, I was talking about GHC, I realise my wording was ambiguous. GHC uses _ (or I think _name for a named hole) for the syntax, and it gives you variables/functions that are in scope. This is in GHC HEAD at the moment. I don't think it makes them pointless, if the compiler is fast and you watch the filesystem for changes then it can be fairly interactive no matter what you use. 
For the historical record (which I know you already know), I implemented the holes in Alf. But I think Randy Pollak had them in Lego before that. The novelty with Alf was really the GUI.
How is it not a laziness problem of `sum` not being strict in its arguments? **EDIT**: Seems I misinterpreted the intention of the post. [This](http://www.reddit.com/r/haskell/comments/19ae5i/simple_example_where_laziness_rears_its_ugly_head/c8mhrfy) seems to be what the example should've been.
Well that's not quite right. There are many incorrect implementations for filter :: (a -&gt; Boolean) -&gt; [a] -&gt; [a] possible, even if you specified that the resulting list was smaller than the first.
`pi = 3.14` is a declaration. `pi` itself is an expression with no free variables, a constant or a value. There's no such thing as zero-arity functions in Haskell. Edit: ok, you *could* consider constants 0-ary functions, but for various reasons that would imply that everything is a 0-ary function.
So, I found that your version can be transformed to: lastOccurrencesBW' xs = flip evalState Set.empty $ foldr (\x xs' -&gt; do { a &lt;- xs'; b &lt;- not &lt;$&gt; gets (Set.member x); modify (Set.insert x); return (b : a) } ) (return []) xs so it is similar to my original implementation, with the difference that the set is threaded through the state monad instead of the first element of the tuple. I have no idea why this is faster than what I had. Interestingly, when I use foldrM instead of foldR, the performance difference vanishes: lastOccurrencesBW' xs = flip evalState Set.empty $ foldrM (\x xs' -&gt; do { b &lt;- not &lt;$&gt; gets (Set.member x); modify (Set.insert x); return (b : xs') } ) [] xs I also don't understand why using foldr' instead of foldr and Control.Monad.State.Strict instead of the lazy version makes everything slower, too.
There are two issues here. When people complains about mean leaking space it's because the list is being materialized and not GCed due to being used twice, once in sum and once in length. There might be a separate issue with sum that has nothing to do with the mean function per say.
It's only a laziness problem if you assume that the program should run in O(1) space by virtue of `[1..1e7]` being a *lazily* generated list in the first place. In a strict language, the idea of this program running in O(1) space would be clearly absurd.
Here's a trivial one for you that you probably wouldn't expect unless you're experienced: foo = do r &lt;- newIORef 0 forM_ [0..10^7] $ modifyIORef r (+1) readIORef r &gt;&gt;= print this'll probably stack overflow. The IORef will contain `(((((0+1)+1)+1)+1)+1)+1)+â€¦` built up thunks. While for addition the operation is strict, expressions involving them are not. So make sure to force it with the strict version: `modifyIORef'`
People are quick to point out that "0 arity" doesn't make sense, but the reality is the concept does have some merit mathematically. Here are two examples: http://math.stackexchange.com/questions/300473/category-theory-and-the-modelling-of-n-ary-functions-especially-the-0-ary-f http://en.wikipedia.org/wiki/Arity#Nullary Oddly enough, the abstract machine model that GHC uses to evaluate programs treats functions and boxed values in a uniform way and enters both by calling a code pointer. I think that shows that 0 arity functions are useful for more than just making certain things in CT work out nicely. 
&gt;you will be able to expect that the HTML your are trying to theme will try to follow the bootstrap conventions. That's pretty unfortunate. Is there a reason why heist wasn't used? So that you could give the theme designers full control over the theme rather than being stuck with whatever markup you give them? It seems like it would be a perfect fit for this kind of application.
Pipes does allow you to send messages back to the producer, I believe, so it's probably fair to call it a bit more generic. Conduit is largely concerned with implementing a generic producer / consumer pattern that has efficient resource finalization.
Constants are 0-ary functions iff "binary" functions (`a -&gt; b -&gt; c`) are binary functions. However, in that case, every expression (with the possible requirement that it must not have any free variables) can be considered a 0-ary function.
You say at "human" level, pi is just a value you named. But, at compiler level, what is pi? Is is a function that takes no arguments and always return 3.14 or is it something else?
"Are" in the sense of being an instance of a more general concept, not identity. Functions are values, values are not functions. Similarly: Linked lists are collections. Collections are not linked lists.
I really liked watching this. Are there other live coding examples of Haskell? I'd love to watch a screencast of somebody working through a non-trivial application. I know about these, but he seems to have stopped: http://www.youtube.com/user/HaskellLiveTV?feature=watch - Any others out there?
Having `sum = foldl' (+) 0` assumes that `(+)` is strict, which is not always the case.
You need even more expressive types then.
&gt; There's no such thing as zero-arity functions in Haskell. What about `IO a`?
I would call that "`IO` actions returning `a`".
Surprises are not necessarily nightmares. :) And O(1) insert can be very useful. It all depends on how you're going to use the IntMap.
`modifyIORef'` is unfortunately not available in ghc 7.4.2 (as bundled with the current haskell platform 2012.4.0.0). I have a list of uses of `modifyIORef` in my code to replace with `modifyIORef'` when I upgrade.
Ah, I get it. Thanks.
&gt; Functional languages are not called functional because "everything" is considered a function. This problem gets easily fixed by church encoding everything :P
Oh I see, _ is a hole in GHC with holes. I misparsed that sentence. Oh well. :) Also, you're not restricted to emacs, you can use any text editor, you just don't have the interactive component. Outside of emacs, holes are input as {! !} and that's how they remain. I think there are also facilities for querying holes, but noone uses it because its a horrible way to program with holes.
It's actually rather important that `IO` is not a function type. If you think of `IO` as functions, then you are using the word "function" in a manner that's entirely different from Haskell's use of the word, and you are setting yourself up for confusion. Traditional imperative languages combine the two notions of "depends on another value" and "is an action". Sometimes you want to depend on an input value, but not to perform an action, but because those concepts are the same, you can't express that, so your function is a trivial action that does nothing but just produce a result. And sometimes you want an action but not a dependence on parameters, so you have to create something like a 0-ary function, that is called a function despite not having parameters, which are the fundamental quality of function-hood. Haskell does not do this. Having a parameter (i.e., being a function) doesn't mean evaluated at a specific time, nor has observable results, nor depends on the state of the world at large. Because functions don't mean these things, Haskell has a new type constructor, `IO`, that's completely orthogonal to functions, and does mean those things.
Do you think it would theoretically be possible, with a strict enough type system, to provide types for functions and have GHC (or any program) just generate function implementations from their types? I've seen you talking about Homotopy Type Theory in other threads and to me this seems related (from a very high level -- I don't know anything really about Homotopy Type Theory, but it sounds to me like it's the kind of thing that makes stuff like this feasible).
Whoa. Can you elaborate on the tying the knot implementation? That's really awesome and I want to understand.
Control.Applicative.Backwards is great, but unfortunately it cannot be used to emulate the Tardis monad. For example, I can combine the Tardis with the Writer monad and get infos from the future to influence what I write. With the Backwards applicative, I would simply write in reverse order, because everything boils down to a foldr. 
In dependently typed languages, there is a notion of tactics, which creates a function from its type. Whether or not it creates the one you want depends on the type of your function, though. Parametricity and a fine-grained typing system help.
mandatory question: Which colortheme is that? :D
http://www.youtube.com/watch?v=DnjcDwvYHAo There are more videos in his channel of him live coding.
Perfect, thank you.
Interesting, would this stop me from returning an empty list, or all but one of the elements that match? 
Using heist wouldn't solve the problem. The problem a theme designer faces is that their theme needs to work for the (in theory) thousands of available plugins of clckwrks. including the plugins that haven't been written yet. Clearly they can not create custom html for every plugin under the sun. In the cases where theme designers want to customize a specific plugin with custom HTML.. that will be possible. But, for everything else.. there needs to be some common agreement between the plugin designers and the theme designers about what things will look like by default. Does that make sense? We are not trying to prevent theme designers from being able to create custom HTML for plugins. We are just trying to provide some sensible defaults for all the plugins they don't customize.. 
You can absolutely send messages back to the producer and get a client/server model this way. There is also [pipes-safe](http://hackage.haskell.org/packages/archive/pipes-safe/1.0.0/doc/html/Control-Proxy-Safe-Tutorial.html) which builds upon `pipes` to add resource finalization.
Here's a somewhat off-topic counter-point: If you read Bird's Introduction to Functional Programming with Haskell and do the exercises as you go you'll be highly likely to take laziness in your stride. I had "get my head around strictness, laziness and space usage" on my Haskell learning list for ages. Bits of that got taken care of on the way to learning other things, but the book by Bird took care of the rest. It's aimed at first and second year undergraduates, so it's not too daunting. I needed a bit of focus to get through - although it was paying for itself as I went through it. I thinnk the exercises are as important as the text for understanding and retainging it all - but that could just be me. Maybe the book just spoke to me in an atypical fashion, but otherwise my thinking is that laziness can't be much of a problem if it can understood by reading a single, accessible book.
Is there a place to track library binding statuses too?
If you are coming from maths, or even natural language, a function maps some input to some output. A door handle doesn't have the function "open door"â€”that could be thought of as the value propositionâ€”but it does have the function of transferring wrist action into an open door (simplifying things a little).
There is a chat room on irc for questions/comments etc. #haskell-mobile
Why don't you implement your own modifyIORef' and then replace it in one place when you upgrade?
pi is a value, functions are dependent values. That is, their value depends on other values. it's nonsensical to have a function with no arguments in functional style.
I very much disagree. I still find `-XTypeHoles` tremendously useful even in GHC, without Emacs support guiding the process of filling in said holes. (And it's not a bolted on hack. It's part of the compiler.) Yes, I have used Agda with Agda2-mode, and it's pretty fucking awesome. However, aside from the fact Agda's version is years old, and `TypeHoles` landed about 2 months ago in GHC, Agda also has the benefit that the programs written in it are *much* more rich with evidence and behavioral proofs. I think this makes the 'interactivity' of goals in Agda much more tangible, because terms are so rich (and I also don't think this is insurmountable, either. But Haskell is a different beast of a language, so this is worth mentioning.) Agda support for holes *is* much better than GHC, at the moment. I also agree that it fundamentally changes the way you write programs, especially with interactivity support. But I just don't think it's "silly" to use holes without the Emacs support, and I'm not going to discourage people from using them, because the Emacs support isn't there. (And I understand it's a *huge* loss to not have, if you've used Agda a lot - then it may really seem worthless! But I imagine the people who are excited about this are not Agda users. :) But I find it is tremendously helpful when writing, thinking and reasoning about polymorphic code and it's *real* meaning, much like the first time I realized how `djinn` worked. I've found it very enlightening when exercising my skills on things like `Codensity` or `Yoneda`.
There is a very good reason why newcomers tend to think of definitions in Haskell as zero ary functions--they are not necessarily evaluated. This has profound implications not only to the cost model, but also in that non-termination is a constant value.
"A wiki is where good documentation goes to die." - Jacob Kaplan-Moss @ PyCon, a couple of years ago.
Thanks! Added a brief description and a link to the specification and resources.
Thanks! Added proper packaging and starting to add tests.
Thanks! Just started to add a test suite and automated regression testing.
Write an interpreter for a simple scripting language. There's actually some tutorials out there to help you in this very task. It also shows off how haskell can elegantly do things like text parsing and IO that opponents of FP complain about.
I agree with Peaker, having been developing large Haskell projects for some years. simonmar has his reasons, and I suppose that fits into his development style. But for me, making grepping for a function slightly easier is trumped by making it much easier to know where a function came from *without* grepping. And without guesswork about the intended mnemonics of the function name.
I'm going to be the smartass. You commented out the type signature, which would have restricted `pi` to be a `Float`. Technically, your definition of `pi` is polymorphic, so it very well *could* be a function (but still not, in the technical sense, a function of zero arity).
I find this especially useful when explaining catamorphisms to people. In a catamorphism, you replace n-ary constructors with n-ary functions.
Write a raytracer that uses distributed process http://hackage.haskell.org/package/distributed-process. For bonus points use repa. Or modify a repa raytracer to also use distributive process.
These are all coroutine libraries, in the sense that all transfers of control are explicit and cooperative. Generators are a subset of coroutines, and all these libraries have some sort of generator abstraction. I'm not sure what you mean by one-shot continuations. Like, semi group mentioned, pipes let's you send data both ways. A Client is a Consumer that sends a parameter with each request.
And I guess I can finally answer your question, as the course and the exam ended :). Basically as Axman6 wrote, many structures such as list and trees are really simple to implement and understand in Haskell. It really is very intuitive. Using abstract data types (basically hiding the implementation via modules) we also worked on some other data structures such as queues [implemented as 2 lists]. But this is not the whole truth: Actually people kept complaining throughout the semester about the lack of C/C++, so the professor eventually gave in and we had 2 weeks of these languages. So basically, the second program (after the obligatory hello world) we got to see in C was the swap program with pointers, etc and then the next ones were implementations of linked lists / stacks / queues/ hash tables and some sorting algorithms like an in-place implementation of quicksort. We even got to C++ classes. Of course this was kind of terrible, as I guess everyone with the exception of the people doing computer science (who also had an obligatory C++ lab course..) could not really keep up with the lectures/homework. I was lucky enough that I had 1 week in between exams and it took a lot of hard work to be able to understand the C paradigm after just writing Haskell code for so long. But now in hindsight I am really glad things worked that way: While learning Haskell terms such as "no side-effects" keep occuring and I kind of thought that I understood what it meant. But it was only a "buzz-word" I was using, until I actually got to use pointers, then everything made a LOT more sense. And in regards to your question: I think having approached several data structures both in a functional and in a mutable way, has made things a lot clearer and understandable.
Hmm, having used it now for a few months, I can say that I actually really disliked it. Initially it was fairly ok: Doing the exercises in the first couple chapters helped a bit with getting a hang of the syntax. But afterwards it started to get really boring: really boring examples, really boring exercises and pretty bad explanations. I pretty much stopped reading it. Reading LYAH and browsing through /r/haskell and the beginners mailing list, as well as some blog posts and then solving Project Euler or hackerrank.com problems was so much more effective.
They removed that restriction years ago.
And...? I don't understand why people are so opposed to the notion of nullary functions. It's a perfectly sensible notion and an obvious extension of the idea of n-ary functions. It also has a long history of being accepted in such esteemed circles as logic programming and tree automata.
[Samefile is not a standard unix tool](http://www.schweikhardt.net/samefile/) but it should be in this age of terabyte storage. For 3 years a haskell implementation has been sitting on my to-do list but alas, y'know, the usual.
I think part of the confusion comes from imperative languages which have a certain evaluation model. For example, in Python there's a distinction between `x.foo` and `x.foo()` where the former returns a function object and the latter invokes that function to compute something. (Function pointers in C and other imperative languages are similar.) However, in Haskell we don't use parentheses to signify function invocation. Thus, there's no syntactic difference between `foo` (i.e., the name of a function) and `foo` (i.e., the function with zero arguments passed in). But then, part of the reason we don't have that syntactic distinction is because we don't have the same evaluation model as those other languages do. In imperative languages we're always focused on "doing" things. So it's important to know whether we're just referring to some procedure vs whether we're doing it. On the contrary, in Haskell we're more focused on "defining" things. Laziness means that we don't need to distinguish definitions like pi = 3.14 from definitions like pi = compute_pi_as_accurately_as_possible The former will allow constant propagation since the definition is so simple it'll be inlined, but that's about it. We don't have to worry about the cost of calling "`pi()`" repeatedly because laziness means that after we compute the answer once, we'll save that answer and just refer to it directly from then on. This is very different from eager imperative languages which would either recompute "`pi()`" every time we use it, or else require jumping through hoops in order to make it lazy/memoized.
Ideology depends on the context you're in. For example, compare my comments [one](http://www.reddit.com/r/haskell/comments/19buc7/how_can_i_explain_the_way_to_define_constants/c8n42ux) vs [two](http://www.reddit.com/r/haskell/comments/19buc7/how_can_i_explain_the_way_to_define_constants/c8n48fg). I'm a proponent of the fact that "nullary functions" are a perfectly well-defined concept; however, whether it's appropriate to use that well-defined concept is going to depend on the context and what you're talking about (e.g., Haskell, Python, Prolog, tree automata). While the concept is well-defined theoretically, I don't think it's especially helpful for thinking about Haskell since it's at odds with the syntax and semantics of the language.
Ah, now it's come full circle! I used Haskell for git-annex because I needed a command-line program to write to prove to myself that I could write something in Haskell that was not a toy.
At the compiler level `pi` is just another name for the expression `3.14`. In this case, since `3.14` is an extremely simple expression, chances are that we'll inline the definition of `pi` everywhere it's used, and therefore there will be no such thing as `pi` in the binary output. In other cases we may have that `pi` is just another name for some really complicated expression. In that case, since it's a complicated expression, the compiler will realize that it's probably a bad idea to inline it everywhere. Instead, we'll compile that expression down however we do, and `pi` is just a name for that expression. This time `pi` does exist in the binary output, but whether we call it a "function" or not is a peculiar notion. Basically, we'll start off with `pi = whatever_expression` and then when someone uses `pi` we'll evaluate `whatever_expression` to some value like `3.14`, and thereafter we'll have `pi = 3.14` and `whatever_expression` can be garbage collected. All this is certainly helpful to know about in terms of writing efficient programs, but the exact details will, of course, depend on which Haskell compiler you're using (just like optimizing C depends on which compiler you're using). If you're just starting out with Haskell, I agree with kamatsu that you shouldn't worry too much about how everything gets compiled down. First work on the semantics of the linguistic level; you can worry about the binary level later.
I would also recommend a look at [RubyMotion](http://rubymotion.com). Not that Haskell isn't awesome. But RubyMotion has a solid year's head start and a lot of community, documentation, tutorials, and proven results (many apps built with it). Folks looking to legitimize Haskell as an iOS tool could study RubyMotion's approach.
I have a theory that thinking of constants as 0-arity functions is going to hinder getting productive at continuations. For a fixed `X`, `Cont Float X` is a function, but it's *not* a function in the same sense that a vanilla `Float` is a function (of 0-arity). In fact, the better way is thinking of `Cont Float X` as a *generalized* constant, the way that `Float` is a constant. (This is all pure waffle, obviously -- I'll prove my case at length elsewhere.)
&gt; but whether we call it a "function" or not is a peculiar notion. In case you care about the gritty details, here's an idea of how GHC does things. In GHC all values (functions, constants, expressions,...) are "thunks", aka suspended evaluations of expressions. It's important to keep this notion distinct from the notion of "values". A value, in the jargon, is something concrete which cannot be evaluated any further. For example, if we have an ADT like the one for lists, then `[]` is a value because it simply *is* the empty list and can't evaluate any further. Similarly, `(x:xs)` is a value if `x` is a value and `xs` is a value; again, because the `(:)` is a constructor and so it can't be evaluated any further. Notably, however, we don't always require things to be fully evaluated values. The expression `(x:xs)` is in weak-head normal form (regardless of `x` and `xs`) because it has a constructor at the very top/head. Usually WHNF is all we actually care about in terms of the execution model. Everything is stored as thunks, but occasionally we need to get the value of the thunk in order to do things like case analysis. Case analysis only works if we know the value (i.e., the constructors) of the scrutinee. So, given a thunk like `foo` (or `pi`) let's say that we need to get it's value in order to do something. Whenever this happens the flow of control will "enter" the thunk. That is, we'll look at the internal representation of `foo` and notice either that (a) it has already been evaluated to WHNF, or else (b) we need to evaluate it. If it's already WHNF then execution returns to wherever it was before and we can execute the case analysis on `foo`. If it's not in WHNF then we'll evaluate the thunk / execute the expression until we get back something which is in WHNF, and then we'll return. This general notion of execution is what's called call-by-name. In GHC and most other Haskell compilers we do a slight variation called call-by-need (aka lazy evaluation). The only difference is that in call-by-need, after we've evaluated the thunk to WHNF we'll update the thunk so that, in the future, we can just get that value back directly, rather than needing to reevaluate it. Actually, GHC does a still further variation on the theme: sometimes it uses call-by-need, other times it uses call-by-name. In particular, if we know that reevaluating an expression is "cheap enough", then it's more efficient to not bother saving the work. This is especially important in parallel programs. Now, with all this in mind, does it make sense to call `foo` (or `pi`) a function? Er, not really? Thunks are kinda like "functions" in that the flow of control will enter them and return when it's done; but that's not really a *function*, rather it's more like a procedure or a subroutine. In imperative languages people often use the term "function" when what they really mean is a procedure/subroutine. Whereas, in Haskell we follow the mathematical tradition where "function" means a mapping from some things to some other things. Thus every function, in this sense, must be something of type `a -&gt; b` for some `a` and `b`. Does that make sense?
Strings in Haskell are very poor performance to their array counterparts in other languages. There are packages that supply performant alternatives so in theory it isn't a problem but a lot of people will still use the lazy strings and occasional suffer for it.
Interesting. Thanks for the opinion!
&gt; However, in that case, every expression (with the possible requirement that it must not have any free variables) can be considered a 0-ary function. Basically, it's misleading.
Luckily, nowadays Haskell has many commercial success stories. http://cufp.org/videos/keyword/55 http://fpcomplete.com/technology/case-studies/ Parsing in Haskell is very concise and can give wow effect. I used it several times for processing log files.
You're probably not going to get very far without binding libraries to IOS APIs unless you're only going to write terminal apps. 
I would consider a program that exploits deterministic parallelism (e.g. strategies), such as Simon Marlow's [parallel k-means examples](https://github.com/simonmar/par-tutorial). 
I appreciate the effort here, but the point rather remains that there are functions where there are multiple implementations for the most restrictive type signatures. There might not be many, or there might only be one sensible option, but you need more than just types.
People do complain about IO, but I think *nobody* complains about using functional languages for parsing--quite the opposite really. That said, I think a scripting language is a perfect project. I personally started with the [Write Yourself a Scheme in 48 Hours](http://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours) tutorial. It might not be the best option, but it certainly worked well enough. You could use it as a stepping stone for writing an interpreter for your own language. Language design is fun.
Ok, I should clarify what I meant: `-XTypeHoles` is silly/useless, but less so than not having them. Just vastly more so than not having interactivity.
I'd prefer to use Objective-C than Ruby to be honest.
*Definition*, period :) What is on the left hand side is identical to what is on the right hand side.
As Peaker said, when your type system is expressive enough, it's just a matter of precision: the filter I wrote above is the only possible (sensible or not) implementation of that type. The only problem with it (since getting the types right is just a matter of habit, after a while it takes no effort) is that it introduces a new datatype that's basically a list "ornated" with a predicate and the original vector, and it can be pretty inconvenient to work with. Fortunately, a solution [exists](https://personal.cis.strath.ac.uk/conor.mcbride/pub/OAAO/Ornament.pdf). Unfortunately, it's even more awkward to use. But that's not a typing problem, it's a tooling problem, which brings us back to the original problem: we need better tools.
The keyword you are googling for is the *Curry-Howard correspondence*. Essentially, it says that proofs in an intuitionistic logic are equivalent to expressions in lambda calculus (i.e. computer programs) and that the type of such an expression is precisely the theorem that the proof is supposed to prove.
How does your solution ensure that it returns the right elements? Why would this not be valid: toVect : {A : Type} -&gt; Filtered {A} p v -&gt; (n : Nat) ** Vect A n toVect filtnil = (0, nil) toVect (filtcons _ _ fxs) = toVect fxs toVect (filtskip x _ fxs) with toVect fxs | (n, xs) = (S n, x::xs) edit - Or just `toVect _ = (0, nil)` edit2- this is all interesting, thanks :)
`toVect` was just a convenience function, and it loses information. But that's not the point, `filter` doesn't use it. If you want to write a `toVect` that (provably) doesn't the lose information `Filtered` holds, you'd stuff it into a structure that prove it forms an isomorphism with `filter`. The ornaments machinery derives that for you for free. EDIT: here it is, now with an informal proof! I also just realized that using IsNo/IsYes was stupid and could've used just the Dec (P x). toVect : Filtered {A} p v -&gt; ((x : A) -&gt; Dec (P x)) ** (n : Nat) ** Vect A n toVect {p} filtnil = (p, 0, nil) toVect {p} (filtcons x _ fxs) with toVect {p} fxs | (p, n, xs) = (p, S n, x::xs) toVect {p} (filtskip x _ fxs) with toVect {p} fxs | (p, n, xs) = (p, S n, x::xs) -- cfilter (p, n, xs) = filter {n} p xs {- toVect (cfilter x) = x toVect (cfilter (p, 0, nil)) = toVect filtnil = (p, 0, nil) toVect (cfilter (p, S n, x::xs)) [assume p x = yes _] = toVect (filtcons x _ (cfilter (p, n, xs))) [we know that toVect (cfilter (p, n, xs)) = (p, n, xs) by the inductive hypothesis] = (p, S n, x::xs) [same for filtskip, assume p x = no _] cfilter (toVect x) = x cfilter (toVect filtnil) = cfilter (p, 0, nil) = filtnil cfilter (toVect (filtcons x _ fxs)) = cfilter (p, S n, x::xs) [where (_, n, xs) = toVect fxs] [assume p x = yes _] = filtcons x _ (cfilter (p, n, xs)) = filtcons x _ (cfilter (toVect fxs)) [inductive hypothesis cfilter (toVect fxs) = fxs] = filtcons x _ fxs [same with filtskip, assume p x = no _] -} Now you stuff all of this into: iso : Iso (Filtered p v) (((x : A) -&gt; Dec (P x)) ** (n : Nat) ** Vect A n) iso = (cfilter, toVect, proof1, proof2) And you're done.
It is a matter of definition. Defining any expression as a zero arity function is simply not useful, and is confusing when discussing types, because "function" usually refers to function types.
Zero arity functions would be more rightly evaluated every time their value is needed. A thunk is only evaluated once. I think that's a meaningful distinction. In addition, a value "id" of type "a -&gt; a" could be a thunk as well, but we would call it a unary function not a nullary function returning a unary function.
Oh, fair point. It's a function `(t : Type) -&gt; (NumDict t) -&gt; t`.
Ah, that's true too.
[Software Foundations](http://www.cis.upenn.edu/~bcpierce/sf/) builds up to this over its first few chapters. It's not Haskell, but if that's your background then the FP material early on should be smooth going and perhaps gently enlightening.
chapter 8 looks interesting. i really like erlang's model of concurrency and that suspiciously looks like some sort of supervisor tree. (slightly related i am looking forward to the otp-like distributed-platform lib release)
Functions in GHC are not guaranteed to be evaluated over and over when applied to the same input. And bound names (through polymorphism and type-classes) are not guaranteed to be evaluated just once.
Sometimes it's *too* smart, and that causes a bad trade of time and space. And there is no good way to suppress it :( http://stackoverflow.com/questions/6208006/any-way-to-create-the-unmemo-monad
The post I linked describes how you can use the RWS monad to tie the knot by feeding the writer (or state!) back to the reader environment. You can refer to the reader environment even when it has not been computed yet, because of laziness (you just need to make sure that the values are forced only after they really have been computed). Since the Tardis monad provides functions to send and read values from the past and from the future, you can use it to tie the knot by sending values to the future (which corresponds to the Writer in RWS), and read from the future (corresponds to the Reader). You just need to make the future available by sending it back to the past, hence the `getPast &gt;&gt;= sendPast`.
I think there are bindings
Thanks for the help. I still don't think it is possible to fully emulate the Tardis. For one, Control.Applicative.Backwards only works on Applicatives. But even for Applicatives it seems impossible to have backwards travelling state while still going forward with the rest (i.e. an underlying Writer monad).
Where? Can one actually write Cocoa code from Haskell?
I think the entire premise of this project is really brilliant. Right now it's probably not immediately inspiring because it takes a minute or so to switch between patterns for an average user, but imagine somebody getting REALLY good at improving on this, with their own custom library of one or two-letter function names and performing by constantly improvising patterns every few seconds while programming at lightning speed. But the real reason I think this is brilliant is because this is sort of what I always imagined programming was about: extending human ability. I feel like the super-heroes of the future will be programmers that command an impressive array of remote machinery as if it were an extension of their own body.
I'm thinking about implement a haskell-based [*todo.txt*][1]... which unix tool can be interesting to implement, but not very complex? [1]: http://todotxt.com/
Hey, a very simple LISP variant can be very cool...
[moria](http://www.vim.org/scripts/script.php?script_id=1464)
Okay, starting to get off topic here, but I have to ask. Why?
Ugh, the latter is so much worse. It means they can go after you for any biased private reason at all, and just cite one of the many complex public reasons in order to do it. They might as well have no rules, and just state that they retain 100% authority to kick you out for no reason at all; it would be effectively the same.
I wish you could hear what he's saying better. I'm having a hard time following him.
Automatic captions didn't work at all. :( All I got is "chocolate" and "administrator" from those.
The stackoverflow link doesn't give Oleg's solution [1]. There is also the `-fno-full-laziness` option that Don mentioned recently [2]. [1] http://okmij.org/ftp/Haskell/index.html#memo-off [2] http://www.haskell.org/pipermail/haskell-cafe/2013-February/106605.html
*nods
Cheers! 
The other replies have some excellent links on the "propositions as types" correspondence, where types are read as logical statements, and having an expression or value of that type counts as a proof of the statement. This is the more powerful approach, but there's another connection which is also worth knowing. "Parametricity" says that every function having a given type has to satisfy certain properties. This is kind of going in the opposite direction - the type "proving" properties of the functions having that type, rather than having a function of a given type proving that the type "is true". Wadler's "Theorems for Free" is a good start: http://homepages.inf.ed.ac.uk/wadler/topics/parametricity.html
I wrote this a few months ago to ease people into the relationship: http://wellnowwhat.net/Programming/Curry-Howard.pdf
[Yes](http://www.haskell.org/haskellwiki/Video_presentations), there are. Did you try looking? 
You can have backwards state with applicatives, I blogged about it [a while ago](http://ocharles.org.uk/blog/posts/2013-01-22-deriving-traversals.html) - maybe this is along the right lines?
http://community.haskell.org/~simonmar/slides/CUFP.pdf This is the slide he was using, there's also a more detailed paper online, that I have read recently, but was unable to find.
It gets messy because if you have the notion of zero arity, then is a function A -&gt; B a unary function, or a zero-arity function to a unary function, or a zero-arity function to a zero-arity function to a unary function, or what? If you want to make a construct for it that has a syntactic representation, it'd be a lot cleaner to talk about that, but values are just values.
Right. I mentioned somewhere else in thread that sharing isn't always a win (and that GHC doesn't always share). And, as Oleg says, it's something we should always bear in mind. Time and space are both valuable resources, but their value lies chiefly in their scarcity with respect to a specific application running on a specific architecture (i.e., it's only valuable when you need it and don't have it). And, of course, those specifics are always changing. HPC isn't the only place that the tradeoff becomes crucial. Large-scale scientific work in general also runs into this problem. E.g., in machine translation we run into this problem all the time, and it's exacerbated by the fact that we're working with models so large that it's important to distinguish space in memory vs space on disk; knowing when to lookup, when to recompute, and when to cache is crucial in MT. As for research, I haven't sat down to think about this particular problem too much; but I think it fits into a more general problem I've been working on. Namely, there are a number of things we want to distinguish in programming: eager vs lazy, pointed vs unpointed, shared vs unshared, analytic vs parametric,.... The approach to date has been to pick one side for each of the debates and to cripple the other by requiring workarounds to emulate the non-default side. I think it would be better to eschew such decisions and to admit that these are actually different modalities and that we want all of them, and we want all of them on relatively equal footing. This is elegant in the sense that modal logics have been very well studied and it's not much of a stretch to see how to use modal logics for types via Curry--Howard. But, as you say, this is certainly on the forefront of current research.
How so? Your response is begging the question by citing the very thing I was refuting. It's clear that you dislike this perspective, but it's not clear what *precisely* you think is misleading about it. Okay, let's say that all closed terms are functions of the appropriate arity given their type; so what? What is problematic in this, other than your preference that "function" is not used to refer to nullary closed terms? In fact, the collection of closed terms has a noble history under the name of "combinators", so clearly it cannot be the grouping of all closed terms together under one roof that you despise. And again, in logic programing they have long grouped them all together, except they tend to call things "functors" rather than functions or constructors. And tree automata, which generalize the terms of logic programming, have also fully accepted this idea. The only time I've encountered such vociferous objection to the idea of nullary functions is in the context of some new Haskell learner expressing the idea as a question and then the Haskell community berating them for such "foolishness". And that leads me to wonder: what is it that is so dangerous about this idea? What is it that so many in the Haskell community feel threatened by? Why is it that we must shame newbies in order to indoctrinate them into our community? Hazing rituals like this do nothing to help either the newbie nor the community at large. And I see nothing in this beyond the hazing of some group wanting to police their borders and make themselves feel superior to others. As I mentioned [elsewhere](http://www.reddit.com/r/haskell/comments/19buc7/how_can_i_explain_the_way_to_define_constants/c8n48fg) there are indeed legitimate reasons why it is not advantageous to adopt this perspective when first learning Haskell. But that is a matter of whether the idea is a helpful one for a particular purpose, not whether the idea has any merit whatsoever. Moreover, that's the sort of thing I never see people bring up when they feel the need to haze someone who's only trying to learn.
I don't agree with your assessment of dependent types. Part of this is because of the continuum you mention. There are two things that I think should be kept separate: (1) Curry--Howard as a correspondence between programming and proving; (2) wanting to prove things about some program. From the Curry--Howard perspective dependent types (i.e., LF) correspond with first-order intuitionistic predicate calculus (cf., simply typed lambda calculus `&lt;-&gt;` first-order intuitionistic propositional logic). Thus, dependent types have directly to do with Curry--Howard just as much as anything else. Now, if you don't just want to simply prove things in general, but want to prove things specifically about a program (where the program and the proof are in the same language/theory), that's when you need dependent types in the way you suggest.
Why not simply take something useful and well-know in a imperative or OO language, then rewrite it in Haskell to show that the code is shorter and more elegant than in the first language? It is a good proof to show that Haskell is a nice multipurpose language and not a academic language. (sorry for my english, i'm a french)
This was a great talk. Slides here: http://gbaz.github.com/slides/wheeler-feb-2013.pdf Next meetup will be Wednesday March 27, details of the talks TBA. As usual, the best way to stay on top of things is to register for the meetup: http://www.meetup.com/NY-Haskell/
Excellent presentation. I just went through your slides, haven't watched the video yet, but the demonstrations were very helpful. I'm doing a project for school about FRP and am just doing literature review... this is very helpful.
I was mostly trying to word dump anything that would be useful to look into to study the CH-correspondance more deeplyâ€”the Coq and Agda groups are very interested in it and I think it's practical to associate dependent types and CH-correspondance. I agree with you though that dependent types aren't really on the same continuum as theory&lt;-&gt;type correspondance. It's more that DTs are useful for making "more interesting" types and thus "more interesting" theories.
This is a great library already. I refactored my Riemann client's types from using hprotoc's generated code to protobuf's encoding and it's a big improvement. The `field` lens accessor is slightly annoying (you can't so easily get raw access to the fields) but it's easy enough to package away. The real win is how simple the encoding becomesâ€”it makes me feel like the Haskell types are more canonical than the .proto file.
Hmm, so I finally took the dive and checked out pipes/enumerators in more details. Very interesting and definitely easier to understand than I was thinking it would be! As you said, its really similar to coroutines but one think that strikes out as different is that in pipes the "pipeline monad" keeps the coroutine state, while in traditional imperative coroutines the state is internal to the coroutines. This is particularly noticeable if you want to pull or push data from multiple coroutines. With imperative coroutines this is done using the same functions as when you want to run a single coroutine: while True: x = resume(coroutine_a) y = resume(coroutine_b) print(x + y) However, in pipe's case you apparently need to do some very complicated monad stacking to do the same thing. I wonder if someone has coroutine code out there that lets you write code in the imperative style... do coroutine_a &lt;- new_coroutine code_for_coro_a coroutine_b &lt;- new_coroutine code_for_coro_b forever $ do x &lt;- resume coroutine_a y &lt;- resume coroutine_b lift $ print (x+y) The examples I have found so far either only allow for a single "hidden state" (pipes; conduit) or return an explicit continuation function from the resume function (that coroutine lib from the Monad Reader that you mentioned). The first approach is usually bad for branching and forking stuff and the second one is bad if you want to avoid people accidentally going "back in time" by calling an old continuation.
Thank you! Chapter two of [my thesis](http://www.testblogpleaseignore.com/wp-content/uploads/2012/04/thesis.pdf) is a literature review which might help too :) It is missing stuff about FRP in Scheme and some recent FRP work by some researchers at Microsoft, but it might help get you started, or at least point you to a bunch of papers :)
I never saw a Monad instance declaration for the indexed state. Does that mean that its not really a monad and you are "abusing" the RebindableSyntax extension in order to get the do notation?
I wonder if better performance can be achieved by utilizing the the non-commutative subsystem of Singular: http://www.singular.uni-kl.de/Manual/latest/sing_461.htm#SEC501 
Static types and seemingly decent object system.
one particular idea i've head for a while: what about a 'wireshark' clone? computationally, its a simple problem, and haskell could avoid two shortfalls c/c++ has in comparison: 1. buffer overflows 2. verbosity
That's not due to laziness though, just them being lists. If they were eager lists, the problem would be pretty much the same.
Not on the wiki list (last updated 9 Sept) http://shuklan.com/haskell/ --------- Also, no videos, but superb Lecture Notes: http://www.scs.stanford.edu/11au-cs240h/
It sounded like he'd be putting the code up soon, as soon as he fixed some broken bits.
Objective C's "static types" are mostly a lie. Dispatch is entirely dynamic.
I know that you say you don't like the monad stacking stuff, but do you realize that your second example reads almost exactly like the pipes monad stacking solution? If you replace `resume` with `request` and just add two `lift`s, the `pipes` solution reads amost exactly like your idealized solution: {-# LANGUAGE NoMonomorphismRestriction #-} import Control.Proxy p () = forever $ do x &lt;- request () -- source1 y &lt;- lift $ request () -- source2 lift $ lift $ print (x + y) source1 = fromListS [1..4] source2 = fromListS [10..] main = runProxy (source2 &gt;-&gt; runProxyK (source1 &gt;-&gt; p))
I know it sort of looks similar when it comes to calling request and respond but the way you have to initialize things is still kind of unintuitive and its more static, with the different subroutines "baked in" to the types. For example, I imagine that making a scheduler that keeps a list of coroutines and that can add and remove coroutines dynamically should be possible to do right now but its going to need some clever tricks while in an imperative language it would be a straightforward task. That said, I really have no idea what the "perfect" solution would look like so maybe I shouldn't be complaining. Its just that in imperative languages coroutines feel like "natural" extensions of the language while in Haskell all solutions to the problem seem to fight a bit against the language due to the state management...
`IState` is an indexed monad, which is strictly more general than a monad. I could have used an `IMonad` type class: class IMonad m where (&gt;&gt;=) :: m i j a -&gt; (a -&gt; m j k b) -&gt; m i k b return :: a -&gt; m i i a (&gt;&gt;) :: m i j a -&gt; m j k b -&gt; m i k b a &gt;&gt; b = m &gt;&gt;= \_ -&gt; b fmap :: (a -&gt; b) -&gt; m i j a -&gt; m i j b fmap f a = a &gt;&gt;= (return . f) join :: m i j (m j k a) -&gt; m i k a join a = a &gt;&gt;= id fail :: String -&gt; m i j a fail s = error s But I didn't want to go to the trouble of defining it when I was just going to use it once. Note that all monads are indexed monads whose indices aren't used.
What level of ghc does this work at? I imagine its compiling down to objective-c at some level? Do you have more information about what's going on here? Looks awesome!
The GHC iOS branch uses the LLVM backend to generate arm machine code. No objective-c is necessary.
Maybe yes, but my Haskell-fu might not be enough to get it working. However, it feels like it is impossible to fully emulate the Tardis monad with the Backwards applicative. It becomes apparent when I try to emulate (flip runTardis (bw,fw) $ execWriterT $ do ...). The writer should write in forwards order, while the Tardis monad can still send values to the past.
ah, that makes much more sense :)
What would you call `main :: IO ()`? Is it the `main` value, contrary to tradition of having a "`main` function"?
I call main an IO action, not a function.
I don't think you can do what you want with applicatives in that way because that would require determining what computation to run next (the writer) based on the output of the tardis - and that is a monad. However, you can have a forward writer and a reverse state thing in parallel with the product of two applicatives.
Apologies, that was written in haste. It's not so much annoying as the inevitable tradeoff you get when annotating your serialization via types like `Field`. The `field` lens is a great way to get around thatâ€”especially if you're writing your own lenses to access the serialized objects.
*C'mon*, is anyone really supposed to be able to read with that color scheme?
Thanks, any pointers are appreciated, the more papers the better. Between the Haskell FRP page, Yale Haskell Group FRP section, and searching on the ACM DL I've grabbed quite a few sources already.
How can Haskell quasiquotation be used for replacing tokens on the Haskell level? http://stackoverflow.com/questions/12075530/how-can-haskell-quasiquotation-be-used-for-replacing-tokens-on-the-haskell-level
Sorry, it seems that I won't have time to look more into this in the foreseeable future. I recorded this as https://github.com/feuerbach/smallcheck/issues/14 so that it doesn't get lost. Or, if you (or someone else) would like to experiment with this approach, please do!
Example ? Xcode project
There's an example on the [wiki](https://github.com/ghc-ios/ghc/wiki).
Just curious: will CloudHaskell be covered in this book? I didn't see it in my cursory search.
Is type theory something that exists outside of CS / Mathematics? I wonder If I could twist it to rival the "set theory" that so annoys me in music theory. 
Linguists (semanticists) use it quite extensively to try to model meaning in natural language. It's a major part of Montague grammar. Here's some quick links from google: http://en.wikipedia.org/wiki/Montague_grammar http://plato.stanford.edu/entries/montague-semantics/ What's set theory in regards to music theory? Send me a link or two? (I guess I could just google that, too, but I don't wanna.)
http://en.wikipedia.org/wiki/Set_theory_%28music%29 http://musiced.about.com/od/s/g/erialism.htm http://en.wikipedia.org/wiki/Milton_Babbitt http://www.jaytomlin.com/music/settheory/help.html In my opinion, it derives from very 1950s era math thinking, making some rather tenuous connections in music. 
[Homotopy Type Theory](http://homotopytypetheory.org/)
Yes, there is a chapter on it, i'll add it in the next revision to go up on ofps.
Doesn't this beg for an extension for strictness annotations for type classes?
This really helped me a lot. The salient part is from minutes 41 to 50.
The compiler can already infer whether (+) is strict for a given instance and should make the appropriate optimizations on `sum` if that's the case. So I think it begs for a smarter compiler. (And indeed, the compiler is smart enough as of GHC 7.8)
Yes - libspotify apps require Premium!
Can somebody quickly explain where the difference between the "atomic" and "propositional" view in the introduction lies? T |- (p-&gt;q) can be transformed into T,p |- q, and for p E T the union of T and p is T. These "views" are equivalent to me.
thanks for this.
&gt; I'm practicing my tech writing. No spaces before question- or exclamation marks.
Excellent! I've seen several of your videos, but the small examples with detailed explanation make this all much clearer. 
The difference is the same as the difference between bounded quantification in (typical) set theory and dependent type theory. In set theory the proposition: âˆ€x âˆˆ S. P is just an abbreviation for: âˆ€x. x âˆˆ S â†’ P The variable `x` ranges over _all sets_ (or, _all things we can talk about_), and `x âˆˆ S` is a proposition that may or may not hold for any particular value of `x`. In type theory, the corresponding `Î x : S. P` is the primitive notion in the language. There are only quantifiers that range over a particular type, and inhabitation of a type is not a proposition. You cannot write the equivalent of `x âˆˆ S â†’ P`, because `x : S` is not a type, it is a judgment, or part of the syntax of quantifiers. The latter is the way SEAR operates. There are quantifiers that range over all sets, but set membership is not a proposition; there is only quantification over the elements of a set, and sets and elements are different syntactic classes, and so on. It makes no sense to ask if some set is an element of some other set, or whether an element `x` of `S` is also an element of the set `T`.* Etc. [*] If we know that `T` is a subset of `S`, then we can ask if `x` is an element of `T`. But in SEAR, being a subset means there's an injective map `f : T -&gt; S`, and asking if `x` 'is an element of' `T` means asking whether there exists a `y : T` such that `f y = x`.
Since you're practicing your writing, I'll give some tips: &gt; cabal is a command-line program for downloading and building software written in Haskell. It can install all kinds of fascinating and useful software packages from the Hackage repository. It is excellent and indispensable, but it currently has a troublesome flaw: it sometimes mysteriously refuses to install things, leading to cries of "Aaagh! cabal hell!!". The above paragraph keeps referring back to `cabal` as `it`, which increases the reader's cognitive load. The reader must retain the first sentence in memory to understand the rest of the paragraph, perhaps referring back to it if they already flushed the first sentence from memory. A well-written article resembles an efficient program: you strive to stream all the information in as little memory as possible so that the reader can ideally use the smallest and most efficient cache while reading. &gt; A little extra know-how prevents this. This tutorial aims to show you how to install cabal packages with confidence, especially if you are new to Cabal and Haskell. Welcome and let's get started! The second paragraph repeats the same error as the first paragraph. Your first `this` refers to something in the previous paragraph, which prevents the user from mentally freeing the former paragraph. Every paragraph should make sense in isolation if you want to improve readability. &gt; Your system may have a package manager, like apt-get, yum, or macports, and it might offer packages for the Haskell software you want to install. In this case you may save time by using it instead of cabal. It probably offers more stable, better-integrated packages, and they may be pre-compiled. Every paragraph's first sentence should serve as an abstract for that paragraph. Readers use the first sentence of each paragraph to judge whether or not to read it. In fact, well-written essays will still read well if you just replace each paragraph with its first sentence. &gt; In short: this tutorial is about using cabal-install, which is cabal on the command line. Your summary sentence belongs in your first paragraph. The first paragraph behaves like an abstract for the rest of the article. &gt; It is often available as a system package, otherwise get it by installing the Haskell Platform, or just GHC. Avoid passive tense as much as possible, because it requires the reader to infer the actor in the sentence, increasing their cognitive load. For example, you could rephrase the above sentence as "System package managers often provide `cabal`, but you can also obtain it from the Haskell Platform". &gt; To check that it's installed, at a command prompt do: Get to the verb of a sentence within about 7-ish words, the earlier the better. Sentences resemble thunks, and you cannot force the thunk until you get to the verb. You actually stick to this rule pretty well, although you lapse a few times throughout the article. You also do several things very well: * You motivate everything you teach by introducing each topic as the solution to a specific, practical problem. * You emphasize showing the reader rather than telling them. Finally, spend lots of time rewriting for articles that you care a lot about. I find that my most well-received posts are the ones I rewrite repeatedly over a week. You always view your own writing with fresh eyes after every full night's rest.
You're very welcome.
this is great! One point I would like made more explicit is what you mean by nested timeouts in chapter 6, and what the 'expected behavior' is, as it's not clear to me.
You're welcome!
Super demo! The method for representing polyrhythms is elegant.
[dupe](http://www.reddit.com/r/haskell/comments/19e2ln/haskell_patterns_ad_nauseam/) Two different domains for the same site?
Really? So I get automatic memoization and buildup of such a table?
To me, that instantly makes me think of foldl, which is handily in Prelude.
Consider using [free monads](http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html) or [operational semantics](http://apfelmus.nfshost.com/articles/operational-monad.html).
I don't understand the question. If you use a function of type `() -&gt; a` multiple times, you might get memoization, but I don't think there's ever a case of a memoization table. There might be floating of subexpressions that don't depend on the inputs outwards to be computed once.
Well, [djinn](http://hackage.haskell.org/package/djinn) will do that for you as long as thereâ€™s a unique solution.
This seems a good fit for the [pipes](http://hackage.haskell.org/package/pipes) package. A producer proxy would read user input and generate a stream of commands. A pure but stateful proxy in the middle (polymorphic in the base monad) would combine each command with the previous state and pass the updated state downstream, to a consumer proxy tasked with displaying it to the user. Now, It may happen that the command producer and the display consumer have different and incompatible error conditions. If you represent them with **EitherT**s form the [errors](http://hackage.haskell.org/package/errors) packgage, you would need to sprinkle a few [hoist](http://hackage.haskell.org/packages/archive/pipes/3.1.0/doc/html/Control-MFunctor.html)s and lifts to harmonize the two monad stacks. See [here](http://stackoverflow.com/questions/14875000/combining-proxies-with-different-eithert-in-base-monad/14877871#14877871). 
You have only a short section about sandboxing. In my opinion, that should be the main focus of your article. There is already enough documentation for a beginner to get started with cabal-install and get a few packages installed easily. Beyond that use case, there is no excuse not to use sandboxing. Without sandboxing, the cabal system is just too fragile to be used in a serious development environment, as is clear from the rest of your tutorial. The need for more documentation and tutorials about the sandboxing environment is huge. There a number of gotchas, and it is a shame that so many people have to learn the same lessons the hard way, over and over again. I think not much work has been done lately on improving and documenting the sandboxing tools due to the expectation that "any day now" sandboxing will become a feature of cabal-install itself. After all, work was almost completed for including it. Unfortunately, that seems to have gone the way of Hackage 2 - feature-creep and bikeshedding are delaying yet another essential piece of Haskell infrastructure effectively forever.
Looks pretty useful! (Is there any hope to get a Binary instance for functions?)
How would that work?
http://www.haskell.org/haskellwiki/Cloud_Haskell#Serializable see cloud Haskell. http://hackage.haskell.org/packages/archive/distributed-process/latest/doc/html/Control-Distributed-Process-Closure.html In particular, binary instances for Closure values,http://hackage.haskell.org/packages/archive/distributed-static/0.2.1.1/doc/html/Control-Distributed-Static.html#t:Closure
I think he means by serializing the code for the function.
So the advantage of this package over hprotoc/protocol-buffers is that you define the types in haskell instead of a .protoc file? Are there other differences such as performance?
Yeah, sandboxing is the only sane way to deal with cabal-install at the moment. This wastes storage indeed, but that's not as bad as wasting developers' time.
Well, you can send functions using channels between threads. I suspect that's an idea you can't transfer to network channels, but as Haskell's always good for a surprise, I thought I'd ask.
&gt; I'm not fully on board with this rule, but issue noted! I suppose it depends on your intended audience. I'm unaware of any native English speaker to put spaces before punctuation, however, and to this American reader (at least) it looks rather jarring. Further, I've never seen it in published text of any kind. A small deal in the scheme of things, of course.
That is the main difference. Since this is Haskell you can also build up parameterized types and reduce duplication in the message definitions. There are also a few other improvements like using text instead of utf8-string and using a standard serialization library (currently cereal but binary can be supported now too). Performance is good enough that it doesn't matter much for us (one of our apps decodes at 300MB/s vs 550MB/s) but it could be improved. The slowdown mostly seems to be from using cereal vs. the very specialized Text.ProtocolBuffers.Get. The newly released binary (0.6+) package might help bridge the gap here.
Thanks! Interesting thoughts. Yes this section could/should be expanded greatly. But I'm not as convinced that multiple sandboxes are or should be de rigeur for all users: - Consider that many folks just want to install and use the latest apps with as little fuss as possible. I think an occasional package reset is a reasonable option in this situation. It's another kind of sandboxing (one-way, temporal sandboxing) that's very easy to understand. - At times I have found it better to concentrate my effort on managing one package set, rather than having to handle all the different interesting breakages (which still arise sooner or later, and always at an inconvenient time) in each per-project sandbox. - Sandboxing is not cost-free - disk space, build and upgrade time, cognitive load (one more tricky subsystem to manage), troubleshooting/fixing time when it breaks tool X. I've experienced this with both cabal-dev and hsenv/virthualenv, great as they are. (This will hopefully improve once/if there is a single blessed ubiquitous sandboxing system built in to cabal). Other reasons I didn't write more on this topic, still in effect: - time - either cabal-dev or hsenv/virthualenv could easily fill a tutorial of their own - I envisioned this tutorial as a simple guide for newcomers, suitable for School of Haskell (actually, it was inspired by SOH) - you're right, I'm expecting built-in sandboxing in cabal soon, and not wanting to document things perhaps soon to be obsolete Looking ahead, I see the benefit of a comprehensive document covering every aspect of cabal install, but also of the short reliable-easy-path newbies' guide I aimed for. Maybe the darcs hub version should become the former, and the SOH version should remain as the latter. Or it should be comprehensive, but as a group of tutorials, not one. Maybe other folks want to help grow it ? Also, I don't know if you're right about cabal sandbox and hackage-server, but I feel optimistic based on all the [cabal-install activity](https://github.com/haskell/cabal/commits/master) and [hackage-server activity](http://hub.darcs.net/simon/hackage-server/changes). 
The site seems to be broken for me on Chrome. The synopsis reads merely "Pure operations: summing numbers.", and the Show solution buttons do nothing.
You said that GHC ensures that a function won't be evaluated multiple times given the same input, and you don't understand being asked if that means it caches the results of being given certain inputs?
Doesn't this actually implement forkOS instead of forkIO?
I was thinking mostly of the performance implications of using Java threads (which are 1:1 with OS threads). You might also expect slightly different semantics. Still cool though.
Yes, of course. To get serious, we will need also something that makes use of the Java concurrent API, where you have thread pools and can submit tasks that are managed by the thread pool executors therein. And of course, it will be important to point out the semantic differences, as I try to do in the second part as I experience them. 
In FRP everything is sort of defined "at-once" with a single "meaning" of the render state at every given point in time. For things that are actually closer to single-step or game semantics, FRP might be possible, but other solutions are certainly more straightforward.
Is there a well-done `Partiality` [1] monad around somewhere? The pattern shows up here, in `attoparsec`, and probably a few other lazy consumer libraries. I think it'd be nice to have `Decode` be a stack consisting of something like `EitherT (Return String) (Partiality (Maybe ByteString)) (Return a)` instead of a custom type. EDIT: On closer look, bos seems to have already generalized the attoparsec `Partial` result called `IResult` quite nicely. [1] http://www.cse.chalmers.se/~nad/publications/danielsson-semantics-partiality-monad.pdf
Thanks, nice summary.
Cool emacs (?) setup! Could you give some pointers of what you have been using (theme, extensions, etcs)?
It's pretty standard really, color-theme-arjen, plus a dodgy hack of other people's elisp... https://github.com/yaxu/Tidal/blob/master/tidal.el That's a mashup of highlight-tail-mode and an interactive haskell mode originally by Rohan Drape for his haskell supercollider library.
thanks! This seems to be exactly what I want. I'll go with your first approach for now, and maybe switch to pipes once I feel more comfortable. Edit: I'm working through this as best I can. Is there a reason for the use of "loop" in sampleEngine, as opposed to leaving out the where binding and just recursing directly? It looks to me that they're equivalent.
You can send anything between threads using a channel because "anything" is just a pointer, but pointers don't really survive network trips as well as they survive in-process trips.
But what is `pageW`? I scanned and scanned the post and I didn't find it bound anywhereâ€¦
&gt; runhaskell Setup.hs configure I believe that is what is giving you trouble. You should be using the cabal command instead, like this: &gt; cabal install xml
if you have cabal (which comes with haskell platform, and I think comes with ghc) then you can just run "cabal install xml" and it'll install everything for you. The warning you got from compiling is actually really common, it just means that the package was put together using an old method, you can usually ignore it. the line "Setup.hs: /usr/local/share/doc/xml-1.3.3: permission denied" means that haskell didn't have permission to write to /usr/local/... try running "sudo runhaskell Setup.hs build" If it has what you need and you understand the library, then I'd use that one. In general using a smaller (less complete) library that will do what you need it to isn't a bad idea. I never really liked haxml because it had way too steep of a learning curve for me.
The two ways are equivalent. In practice, I think the non-`where`-binding approach sometimes causes space leaks because of quirks in the way `ghc` optimizes things, although I may be mistaken. Also, it may not be necessary in the case where the helper recursion function doesn't close over any argument. Usually you do this kind of pattern if you have something like `map`: map f xs = go xs where go [] = [] go (y:ys) = f y:go ys In that example, the recursive `go` function closes over the `f` argument so that you don't need to keep reapplying `map` to `f` at each stage of the recursion. It might be the case that if the recursion doesn't close over anything that you don't need to do this trick.
This stuff finally allowed me to stop wondering if I ought to be using cereal instead :)
Glad it installed. I've never actually used text.xml.light before, so I probably can't help you there.
Sorry, my fault. Should've used LHS. Fixed.
As pointed out by Tekmo [here](http://stackoverflow.com/questions/15192897/partiality-monad-transformer/15194482#15194482) it ought to be possible to define `Decode` in terms of a `FreeT ((-&gt;) t)` layered with an `Either`. This is my intended `Partiality` monad.
Tip: Indent code using 4 spaces if you want it to show up formatted in a reddit comment. I've never used the `xml` package before, but I would recommend just going down the list of functions in Text.XML.Light.Output until one gives the output representation you want. We could give more help if you tried it on a simple XML document and showed us both the input and output.
That is not what Silas said, which is possibly why he didn't understand your question. GHC can, at its option, avoid evaluating a function application. It's not required to, though.
Clay writes readable html. I might include blank lines between paragraphs, but otherwise I noticed nothing i'd change if it were my code. The style sheet is all one line. This has little effect on performance, but renders it unreadable. As this post is about style, interested readers might want to examine Clay's output?
I really don't know how it's actually done. But as far as I can tell, the "returns another function" is slightly misleading, since it sounds like it's doing some super smart code flow analysis and then magically "creates" new code. I think it's far more simple. If you call "max" like "max 9" and assign it to "newMax" it remembers the nine and the function. (That's like a closure I guess, remembering/storing call context) So if you call "newMax 7" it just takes the 9 it had stored and then calls the original "max" function (also stored for reference) with the two values. 
In short: it depends on how it's called. [The commentary on function calls in GHC](http://hackage.haskell.org/trac/ghc/wiki/Commentary/Rts/HaskellExecution/FunctionCalls) might be of interest. In Fay [I literally curry](https://github.com/faylang/fay/blob/master/src/Fay/Compiler/Decl.hs#L186) n-functions into n 1-arg JS functions that return more 1-arg JS functions. Then in the optimizer, for saturated calls that can be detected statically, it also [compiles fully saturated n-arg JS functions](https://github.com/faylang/fay/blob/master/src/Fay/Compiler/Optimizer.hs#L242) and calls them instead. â€œOn average, about 80% of all calls fall into this category (see the eval/apply paper for measurements).â€ In GHC it's more low-level and thus more interesting.
hsenv automates all that.
Good :) binary is also a good deal faster than cereal.
IMHO this is an odd way to approach it, when you can understand it on the source language level: max x y = if x &gt; y then x else y max = \x -&gt; (\y -&gt; if x &gt; y then x else y) -- desugared f = max 5 = (\x -&gt; (\y -&gt; if x &gt; y then x else y)) 5 = (\y -&gt; if 5 &gt; y then 5 else y) f 9 = (\y -&gt; if 5 &gt; y then 5 else y) 9 = if 5 &gt; 9 then 5 else 9 = 9 In short, if you have an application, the thing that is applied is either a lambda expression, or the name of a function, in which latter case you can just *inline* the definition of that function (which is a lambda expression). In any case, you end up with something like (\x -&gt; expr) arg and then you perform beta reduction (i.e., replace each x in expr by (arg)). In fact, one could implement Haskell this way (at least, most of it). 
First, there are several ways to implement curried functions, so there's no definitive answer to this question. I'll give the answer for a closure-based implementation. With closures each function is represented by two parts: the code to execute and the values of free (I use this loosely) variables. So the function max would be represented by a code pointer to max, and an empty array of free variables. Say that you call max with the single argument 5. The max function needs two arguments to do any work, so it will allocate and return a closure. The closure will again contain the same code pointer, but the array will now have 5 in it. This represents the partially applied function. When this function is called with a 9, the code can proceed, because it now has two arguments (the 5 from the closure and the 9 from this call) and it will return the result. This is a very simplified account of what happens in most real implementations, but it should give you the idea.
Tools such as cabal-dev or hsenv can of course help with this sort of thing to some degree, but I don't think it'll get you all the way there. I don't like the wrapping that hsenv and co require (cabal is just still cabal), or the GHC_PACKAGE_XXX flags set (which cabal dislikes), etc. For instance, if you wanted sandboxes within your sandboxes (using cabal-dev for a project, for instance) that would be no problem at all. Also, it does not rely on any existing ghc installation (how do you install hsenv in the first place?) and works fine across platforms. In the end, cabal-dev or hsenv are just too much "magic" for my taste. 
Thanks a bunch for those links. I had yet to read *the commentary on function calls in GHC*. It would seem as if that the compiler is not just funneling argument after argument to the function, but rather carefully determining what should be done. Some examples from [the paper](http://community.haskell.org/~simonmar/papers/eval-apply.pdf) linked to in the commentary. In the push/eval method of currying *if there are too few arguments, the function must construct a partial application and return. If there are too many arguments, then only the required arguments are consumed, the rest of the arguments are left on the stack to be consumed later, presumably by the function that will be the result of this call.* This indicated that the function has logic beyond what is simply written by the programmer since the functions *entry code is required to check how many arguments are on the stack* before processing anything. This alone makes things much more clear. It's logical that the compiler is doing a fair bit of work under the hood to make currying much easier for the programmer. The eval/apply approach is not dissimilar. &gt; In the eval/apply approach, the caller first evaluates the function k, and then applies it to the correct number of arguments. The latter step involves some run-time case analysis, based on information extracted from the closure for k. If k takes two arguments, we can call it straightforwardly. If it takes only one, we must call it passing x, and then call the function it returns passing y; if it takes more than two, we must build a closure for the partial application k x y and return that closure. It's clear that the compiler isn't just writing a simple if statement and feeding it variables, but rather, looking at how the function is used or being used and acts appropriately. It may *call a function* or *build a closure* depending on the situation. Clearly, some black magic ;) Unfortunately I don't have time to finish reading the paper, but it has so far helped to clear some things up. Thanks.
See [my reply](http://www.reddit.com/r/haskell/comments/19ms5z/i_understand_the_concept_of_currying_and_how_it/c8pggcp) to chrisdoner's comment. I've come to see/believe that it's not simply creating a function and feeding it arguments but rather is doing some complex code analysis to determine how best to curry a function.
This paper describes exactly how it is implemented in GHC and the other implementation strategy that is used by many interpreters. http://research.microsoft.com/apps/pubs/default.aspx?id=67488 This video (and Max's paper) describes another way it could be implemented by GHC: http://vimeo.com/6688091
I would like to answer your questions with a few questions of my own. &gt; What exactly is the first parameter being compared against? Is the 5 being compared with a Null value or something similar? Suppose I were to write a different function: f y = if 5 &lt;= y then y else 5 What exactly is the `5` being compared against? Is it compared with a Null value or something similar? What if I write it this way? f y = let x = 5 in if x &lt;= y then y else x ...or this way? f y = (\x -&gt; if x &lt;= y then y else x) 5
well, your idea is a piece of code for a human to execute ("do this", "do that"). if you wanted it to be automated you'd have to write it in a programming language and you would always get the problem "how do you install it in the first place". hsenv was in the beginning written in bash, but I've rewritten it in haskell because: bash sucks, I don't hate haskell and I wanted to use existing code (for parsing of ghc/cabal related stuff) instead of using half-assed regexen.
Thanks for the links. I don't have time to read/view them now but I will soon.
I think I misread "not guaranteed to be evaluated" as "guaranteed not to be evaluated". Still, it's kind of hard how you can miss the implication that this will (sometimes, though not always) mean some kind of memoization table.
I thought binary had been superceded by cereal, like json -&gt; aeson and parsec -&gt; attoparsec. And as part of that, I thought I had heard cereal was faster. What is the actual background behind there being two competing modules for this job, and is there a decent/accurate comparison of the two anywhere?
I don't understand how the free monad approach is helping anything here. How is that any different than simply returning the action to be taken, and then running that action in the main loop (which is obviously in IO already)? I don't see any difference in the end result, and the free monad approach means anyone reading the code now has to go learn what free monads are before they can figure out what is happening.
Yes, correct, AFAICS. The process of replacing the lambda variable by the value the lambda is applied to is called "beta reduction". And it is performed on the right part of the lambda arrow -&gt;, no matter if the expression there is another lambda or not. The point is, when you see: (\x -&gt; ....) a you know: well, we can make this easier (or not, depending on a) by beta reduction. Whether the ... part is another lambda or not is immaterial. Hence, the problem of functions with multiple arguments, currying, etc. simply does not exist in the lambda calculus. 
He specifically asked how to abstract away the IO layer of his program and I was answering his question. Also, both of the articles I linked explain the practical advantages of doing so. The biggest advantage is the ability to switch out the interpreter. For example, this is very useful for testing programs since you can very easily mock external interactions by changing the interpreter. You could also use this feature to easily switch out your game's back end, even dynamically while the program is running. It is also a powerful way to sandbox game scripts that users might write.
Should `GeneralizedNewtypeDeriving` be ~~cast back into the pit from whence it came~~ removed from GHC? It seems to me that `GeneralizedNewtypeDeriving` treats a `newtype` wrapper and the original type as really the same: i.e. for `newtype B = B A` the compiler is willing to implement F A ~ F B where F is any context - not just a type constructor. Whereas I would say that `B` has the same representation as `A` but is slightly different from it. In fact, that's my normal use case - the good old `newtype Age = Age Integer`. If I want to anything more fancy than `deriving (Eq,Ord,Show,Read)` I'll write the instances myself. Is the runtime cost alluded to in [section 7.5.4 of the GHC user guide](http://www.haskell.org/ghc/docs/latest/html/users_guide/deriving.html#newtype-deriving) really an issue?
Let's not throw the baby out with bath water. GeneralizedNewtypeDeriving is one of the more productive extensions. A creative solution seems more appropriate. 
It's fairly established in older literature and definitely in French writing.
Yeah, I should have specified: I've never seen the convention in published English texts typeset within my lifetime. Despite having quite a few years of French courses behind me, for whatever reason the convention persists in looking wrong.
Theoretically, the problem comes from the notion of type equality used by GADTs and type families, which is much too strong (in the sense that too many types are different). The "beautiful" way to handle this would be to move to full dependent types, and have GADT indices be *values*, not *types*, using a restrictive value-level equality, and keeping type-level equality free to move to more powerful notion of isomorphisms.
&gt;Sentences resemble thunks, and you cannot force the thunk until you get to the verb. The [periodic sentence](http://en.wikipedia.org/wiki/Periodic_sentence) thus constitutes a mental space leak - thanks for the neat metaphor!
It has certainly gone out of fashion but if I wanted to write a Victorian instruction manual On The Use of CABAL INSTALL or _Effective management of Haskell packages for the modern Calculator_ it's the first thing I'd do ;)
The way I've implemented GeneralizedNewtypeDeriving (the two times I have), gives a much more restricted version. It actually creates the instance declaration you'd have to write by hand. This limits what you can do, but also makes sure that what you do is totally safe. It doesn't open up any weird loop holes, and it does not destroy abstraction.
There's plenty of examples, here's a simple one: do args &lt;- getArgs print $ if null args then "No args" else args There's really no doubt what you want this program to do, and it makes perfect sense, but it's not allowed in Haskell.
Why does it fail, exactly? Is it because the branches of the condition aren't the same type?
Because `String` and `[String]` aren't the same type. The following works: if null args then print "No args" else print args But bring the `print` to outside the `if` and the type system demands the two to match.
Your version is simpler; Apple's is easier to market. The whole thing is pretty much why I have no interest in developing for iOS, and am mostly reading this story to assess how close Android support might be.
This is not really unique to GADTs. Good old type classes are based on a discrete category of types, as well. You're doing case analysis on types with them, after all. [Here's](http://hpaste.org/83463) an example. My `Foo` type is not a functor, because `C T` does not imply `C U`, even if `U` is a newtype of `T` or vice versa. So you cannot write `fmap` for it. But generalized newtype deriving gives us the power to map a `Foo Int` to a `Foo NotInt`, and thereby pass the `C` dictionary for `Int` where one for `NotInt` is expected. I don't think you can defeat type soundness this way, but the above rather defeats the canonicity assumption of type classes, so exactly which duplicate dictionaries are thrown out by the compiler could make a difference to your end result.
I know this is somewhat missing the point but if there is a suitable type class unifying the two sides of the 'else', we can rescue this as follows: {-# LANGUAGE GADTs #-} data Printable where Printable :: (Show a) =&gt; a -&gt; Printable do args &lt;- getArgs case (if null args then (Printable "No args") else (Printable args)) of (Printable s) -&gt; print s *Edit: fixed typo*
Both branches of the if statement have instances of the Show class, but they are not the same type. You would need a more flexible typing of the lowly if statement: if _ then _ else _ :: (Show a, Show b) =&gt; Bool -&gt; a -&gt; b 
Correct. The typechecker sees this: if ( Bool ) then ( String ) else ( [String] ) and throws a fit. The typechecker doesn't *have* to reject this. But it keeps things simple. A more complex typechecker could handle it.
That is a very fine title and I would like to read that manual. :)
We need dependent types! :)
Actually you'd really want dependent types. if_then_else_ : {P : Bool -&gt; Set} -&gt; (b : Bool) -&gt; P true -&gt; P false -&gt; P b This would allow a show instance to work its way in fine.
Another example I like is the type systems can make advanced "meta" features harder to write yourself (since you need to play along with the type system). For example, you can't write the untyped [Y combinator](https://en.wikipedia.org/wiki/Fixed-point_combinator#Y_combinator). You need to encode the recursion with either "letrec" or recursive ADTs. Similarly, its tricky to [Scott encode](https://en.wikipedia.org/wiki/Mogensen%E2%80%93Scott_encoding) things in Haskell. The code itself is trivial but you need to use existential types (or something like that - I don't remember 100%).
Note that even in Haskell 98, being able to turn type isomorphism into equalities is a problem because it allows you to violate module assumptions (like the single instance property). I think the stronger notion of type equality is probably the correct one (unless perhaps you have the univalence axiom...which does not have a known computational interpretation in interesting settings) and it is generalized newtype deriving that is wrong.
Thanks, I think we agree on just about all points. I sure hope you are right that my pessimism about cabal sandbox and Hackage 2 are unfounded. There is occasionally a burst of activity on these fronts, but so far, still nothing we can use. I would much rather readjust and re-learn even half a year later, due to a not perfect API being published, than wait that long for fixes to major infrastructures problems that hobble my ability to work on a daily basis.
Good point!
Thanks for your example. I had realized feature of plain typeclasses but couldn't come up with a compelling example. Another reason for choosing GADTs instead of type classes or type families for my discussion was that I think of types in Haskell not merely as sets, but as sets with some extra structure. That is why First and Last (as Monoids) are different types, even though they are represented in the same way. So I had generalized that typeclasses is what give types their structure. But the realization motivating this post is that GADTs give types some structure, too. (Though in no more fundamental a way than ordinary types -- it's just that ordinary data types only had four categories to choose from for their domain; GADTs have many more).
I think the problem is not GeneralizedNewtypeDeriving per se, but the fact that it is inconsistent with the sort of pattern matching that GADTs use (which, in the metatheory, implies the K axiom). I find things to be much clearer in a dependently typed setting. So let's try to replicate the example in Agda: {-# OPTIONS --without-K #-} module gadts where open import sets.empty open import equality open import function.isomorphism data A : Set where a : A data B : Set where b : A â†’ B data Switch : Set â†’ Set where switchâ‚ : Switch A switchâ‚‚ : âŠ¥ â†’ Switch B -- haskell's GeneralizedNewtypeDeriving essentially makes an equality out the -- isomorphism between A and B, then uses it to substitute B for A in type class -- instances. -- So let's do that manually. -- A and B are clearly isomorphic isom : A â‰… B isom = record { to = Î» { a â†’ b a } ; from = Î» { (b a) â†’ a } ; isoâ‚ = Î» { a â†’ refl } ; isoâ‚‚ = Î» { (b a) â†’ refl } } -- use univalence to convert the isomorphism A â‰… B into an equality eq : A â‰¡ B eq = â‰…â‡’â‰¡ isom -- this is the only method in the dictionary of the Switchable instance for A switchA : A â†’ Switch A switchA x = switchâ‚ -- we obtain the corresponding method for B by substituting using the equality switchB : B â†’ Switch B switchB = subst (Î» X â†’ X â†’ Switch X) eq switchA -- this is the problematic step unswitch : Switch B â†’ âŠ¥ unswitch = {!!} -- the following fails to type-check: -- unswitch (switchâ‚‚ x) = x -- so no contradiction can be derived this way bottom : âŠ¥ bottom = unswitch (switchB (b a)) So in Agda, with the option `--without-K`, there is no problem, as the commented out definition for unswitch uses an illegal pattern matching. Incidentally, that definition also fails with K enabled, although I'm not really sure why. The --without-K flag is needed anyway, here, because [univalence](http://ncatlab.org/nlab/show/univalence+axiom) (which I'm postulating through my [agda-base](https://github.com/pcapriotti/agda-base) library for HoTT) is inconsistent with the K axiom. 
&gt; This is not really unique to GADTs. Correct me if I'm wrong, but aren't all GADT statements in Haskell desugared to constructs that rely on type classes? I.e., are all problems with GADTs also problems with type classes?
Great paper! I was hinting at this kind of thing with my category language -- in fact I envisage kinds marking categories. So a positive type constructor might have kind Hask -&amp;gt; Hask; an standard invariant type constructor (like Endo) might have kind Iso -&amp;gt; Hask; and a GADT like switch might have kind Hask -&amp;gt; Discrete -&amp;gt; Hask. Consider the category Sub of types with compatibly-represented subtype relations as morphisms. Then the variances of generics in e.g. Scala are represented simply as another kind -- if F has kind Sub -&amp;gt; Hask, then we could have a function F Nat -&amp;gt; F Integer and know that it can be implemented trivially (without traversing F); but we would not have F Integer -&amp;gt; F Nat (which is a good thing).
I meant to quantify on arbitrary notions of isomorphism, not picking a specific idea of type equality as in the case of univalence. In Coq, you get to prove the type differences which are constructively provable (eg. bool != nat), and you cannot either prove or disprove, say, (unit * nat = nat), because there are valid models of either. That strikes me as a reasonable, prudent view of type equalities, robust to further axiomatic extensions such as univalence (... or various irrelevance choices). But even with this view you understand that this is probably not the right setting to use GADTs, and that term indices, that do have clear notion of decidable equalities, are a safer choice. I contend that the core mechanism of Haskell or ML GADTs is quantification over code/tag equalities, not whether those codes/tags live in types or terms. Something I'm, rather ironically, not quite sure of is how to mix that vision of GADT with term indices only with the work done on GADTs in presence of subtyping. (I'm not aware however of all the subtleties of generalized newtype deriving, and there may certainly be wrong things here. But the core idea of lifting behaviors through type aliases is reasonable. Of course, the reason why people use newtypes *today* is not as an abstract abbreviation mechanism, but rather to define *distinct* type-directed behavior through the type class mechanism, which means that maybe considering them as equal types is not a great idea to begin with.)
They use equality constraints, but I don't think equality constraints are considered type classes although type classer are considered constraints.
Now that one obstacle is out of the way -- the need to install software to learn Haskell, how can we recruit more programmers from other languages? 
Actually, Oleg has a trick for performing type equality constraints using only type classes. But yes, in general, one shouldn't confuse equality constraints for other constraints.
Surely the point is that it can be coerced to either - and the type you coerce to is whatever is required at that point in the program?
Oleg's trick is better than that--it allows you to encode *decidable* equality using type classes. Of course, it needs terrible stuff like overlapping instances to work. Still, I use it frequently when I want to perform non trivial code generation using the type class system.
I don't have any good examples on hand, but (hopefully) it should be obvious that there are numerous programs which are "sensible" but which don't type check. The reason it should be obvious is the halting problem... Let's say that a "well-defined" program is any program which always halts. (There are plenty of better notions of well-definedness, but they all come down to the same issues, via Rice's theorem.) However, we know that it's undecidable to determine if an arbitrary program halts; thus, there can be no algorithm for determining exactly which programs are well-defined. Type inference/checking is an algorithm for (approximately) detecting which programs are well-defined. But that means there's no type system which can exactly separate the well-defined programs from the ill-defined programs--- because if there were, then we'd have an algorithm for solving the halting problem. Thus, every type system must either be conservative (and reject some sensible programs) or else be too lax (and thus accept some nonsensical programs). To be worthwhile, we prefer to err on the side of conservatism. A lot of the research on type systems is all about trying to reject fewer sensible programs. However, this leads to things like type inference/checking becoming undecidable, for precisely the same reasons outlined above. If a perfect type system existed, we could never enforce it.
There must be a different reason, because learning such complex languages like C++ takes years and yet people do it. BTW, I've known several C++ programmers who learned Haskell (I am one too). 
That doesn't mix so well with global type inference, and in the general case is AFAIK undecidable - you might reasonably have the coercion chains A -&gt; B -&gt; C and A -&gt; D -&gt; C, and no way to uniquely pick one. Of course, if Coerce was for actual isomorphisms, you'd probably be able to special case into the type checker that it's OK to just pick one option, since the ambiguity can't introduce any bugs. 
The ones they bring! :)
&gt; There must be a different reason, because learning such complex languages like C++ takes years and yet people do it. Haskell's going to have an uphill battle against C-style/C-derivative languages. If an employee starts using Haskell in a shop of non-functional programmers, they're writing code that's going to have a significantly higher overhead to maintain. I'd love to use Haskell at my workplace once I've learned enough to be useful, but I'd end up with angry coworkers and -- more importantly -- angry clients when we (a Drupal shop) handed off the codebase.
JHC implements currying[1] by either ignoring it when appropriate and calling the functions directly or by constructing a tagged object which held the arguments received so far, and calling an 'apply' with the existing object and the new argument. This can either construct a new object with more references to arguments or constructing a thunk/calling the function directly. [1] My work concentrated on the GRIN/C/LLVM stage.
Here's an interesting way to do it, but I don't think Haskell directly uses it because it requires a whole-program transformation. I know I read it somewhere, but I have no idea what to google. Say you're starting with this: f :: Ta -&gt; Tb -&gt; Tc g :: Tb -&gt; Tc bval :: Tb And you're using them in functions like this: doSomething :: (Tb -&gt; Tc) -&gt; Tc doSomething q = q bval main = do print $ doSomething (f a) print $ doSomething g The compiler can automatically create some supporting datatypes and functions so that f and g are only ever invoked with full complements of arguments. Value_Ta_Tb_Tc = F0 -- represents values of type Ta -&gt; Tb -&gt; Tc Value_Tb_Tc = F1 Ta | G0 -- represents values of type Tb -&gt; Tc apply_Ta_Tb_Tc :: Type_Ta_Tb_Tc -&gt; Ta -&gt; Type_Tb_Tc apply_Ta_Tb_Tc F0 a = F1 a apply_Tb_Tc :: Type_Tb_Tc -&gt; Tb -&gt; Tc apply_Tb_Tc (F1 a) b = f a b apply_Tb_Tc G0 b = g b doSomething :: (Type_Tb_Tc) -&gt; Tc doSomething qclosure = apply_Tb_Tc qclosure bval main = do doSomething (apply_Ta_Tb_Tc F0 a) doSomething G0 It might not be immediately obvious, but you can transform all higher-order values in the program this way so that you wind up with a first-order program.
* Write a bunch of little programs a la rossettacode (but higher quality) in languages like C++, Java, and Python and then translate them into imperative Haskell code and then iteratively refactor that code into idiomatic Haskell. You could do it with tic-tac-toe, for instance. * We could also use stuff like Ruby Warrior in Haskell, because there isn't any easy and idiomatic Haskell code bases to point people towards (I don't think xmonad and pandoc are well suited for this even though they're often recommended). * Improve Haskell support in vim, emacs, and Sublime Text. You could do this in a sweeping way by creating something like ensime (which has basically made editors like vim, emacs, and sublime text complete scala IDEs -- refactoring and all). * Improve the advanced track on Haskell's wikibook. As it stands, people have to comb through blogs and research papers to understand the more advanced topics in Haskell (which are useful in everyday code bases). * Create an infographic that gives people a 10,000ft overview of Haskell and all its topics (arrows, cps, frp, and so on) and their uses so people can see what they don't know and if they should learn it or even if they need to learn it. Basically, scope out Haskell's boundaries so people aren't left with the feeling of staring into an abyss of infinite ignorance. Something like a [degree plan](http://www.cs.utep.edu/DeptCS/ugrad/PrereqChart2008a.gif) but more informative. * Lastly, I think Haskell needs a book like Odersky's Programming Scala. Read that and it's obvious what's missing from existing Haskell books. You really need to train people to stop thinking of programming in fp languages as they do in imperative languages where they're basically giving fairly one-to-one sequential orders to the compiler. 
You're assuming that the host language is Turing complete, but it isn't obvious to me that Turing-completeness is a necessary property for a language to be useful. Isn't there research on total programming to get rid of our dependence on Turing completeness?
Part of the problematic step seems to be the inability to use types to index. The following also fails: data Switch : Set â†’ Set where switchâ‚ : Switch A switchâ‚‚ : Switch B switchB : Switch B â†’ âŠ¥ switchB x = {!x!} You can't match on `x` because that would require matching on a type, I guess? So we can't do that. I'm not sure this should be bad tho. It might be a bug. You might want to ask the mailing list.
Another way of seeing the "issue" is that the expression: f $ if a then b else c is equivalent to: if a then f b else f c but the above situation is an example of where the second one typechecks but the first one doesn't.
[D.A. Turner's 2004 paper on total functional programming](http://www.jucs.org/jucs_10_7/total_functional_programming/jucs_10_07_0751_0768_turner.pdf).
The problem with hsenv is that you cannot work on more than one project at a time. 
DrBartosz was before coming to Haskell one of the "gurus" of the C++ world (I guess he still is) and was involved in D. He then starting pointing to Haskell as a way of better understanding the oddly functional parts of C++ (like template meta programming). Learning Haskell makes you a significantly better C++ programmer IMO. Probably getting good at plain C and idiomatic jave (like "Effective Java") also makes you a better C++ programmer because it helps you fully grok more parts of the language. 
Ah! That's good context. I still stand by my point, though. There are going to be far more "just ship a decent website" programmers, and they're going to be more amenable to new tech in the vein of node.js than Haskell's monumental learning curve.
I agree this is a drawback of hsenv. It's also a drawback of any system using the approach of the linked page. 
To date, I strongly suspect the majority of Haskell programmers saw some value in the language and learning it. Examples of such value would include (but not be limited to) an elegance, a more robust foundation for abstraction, productivity, or simply a fun intellectual curiosity/puzzle. Additionally, I don't know that we've been successful at capturing the attention of programmers who use tool X and language Y as a means to an end. I'm imagining a persona where these folks feel that they have work to do and they want the path of least resistance so they can get back to the problem domain they are working in. Assuming there is a truth to the picture I'm painting, then we could look at a "typical" web developer. In my simple model, these folks need to get something working in the web browser. Javascript runs in the web browser so it becomes the right tool for the job implicitly. I say "typical" but anecdotal experience tells us that we all face slightly different challenges and it's hard to generalize. For another example, take a look at /r/gamedev. The sidebar in that subreddit is full of advice aimed at aspiring game developers. One bit of advice that is pointed out over and over again is to not focus on the tools and language choices but to pick something (anything really) and push it from idea to finished game. In that community all the technology choices are based on getting the job done. In conclusion, I suspect *a way* to attract more users is to demonstrate that Haskell is *a path* of least resistance for more types of software development (than we currently have evidence for).
Having a package of the week/month would do a lot to promote the libraries available on Hackage. I often hear programmers say they choose languages based on the availability of libraries. I believe python has something similar: http://pymotw.com/2/ Perhaps that's a good use of the SoH?
Yes and no. It is true that both approaches uses some "global" symlinks to swap between sandboxes. However, with the manual approach, if you don't want to swap compiler, you could still use cabal-dev to have per-project sandboxes within the larger sandbox, if you wish.
I would really like to see more about large program organization. Much is written about small programs, but not much how you structure larger ones. Especially hairy ones like games and such that keep large amount of data around. Simple CRUD apps could make good examples I think. Keep up the good work!
The complexity is mostly optimizations. It could just be done (at the caller) by calling the function x times, and (at the function) by constructing x-1 closures.
&gt; Note that even in Haskell 98, being able to turn type isomorphism into equalities is a problem because it allows you to violate module assumptions (like the single instance property). In the long run, I believe the problem is with the type-class mechanism as it stands, rather than type conversions. Single instance is a quite nasty assumption (because it immediately makes type classes global). The "module assumptions" would be better served in a dependent type system by exposing the actual instance choice at a type level (eg. have (Ord Nat lt_nat)) so that multiple instance can soundly coexist and we can reason on compatibility. ML applicative modules are another way to use phantomish abstract types instead of real dependencies, that make a different convenience/expressivity choice (I suppose Scala path-dependent types are similar), but I for one suspect that dependency on terms (or at least values) is the long-term solution to get out of this quagmire, because they allow fine-grained reasoning on coherence properties. 
Awesome! I'm a little hesitant on licensing issues, though. I mean, this is not Wikipedia or Wordpress, you are (representing) a company with a commercial interest.
No, those two are not equivalent in general.
In a sense, this is what has been done in the 2004-2007 work of Vincent Simonet and FranÃ§ois Pottier (eg. [A Constraint-Based Approach to Guarded Algebraic Datatypes](http://gallium.inria.fr/~fpottier/publis/simonet-pottier-hmg-toplas.pdf)): they index GADTs over arbitrary "models", sets with a subtyping relation, so exactly the irrelevant counterpart of arbitrary categories as opposed to setoids. Using a natural number domain, for example, gets them something close in applications to Hongwei Xi's Dependent ML system. The practical question however is how to allow users to pick such domains. Types are a canonical model with a well-understood (hmm...) notion of subtyping, and that allows low-tech embedding in existing languages. Term values would form a rather discrete category (maybe not if you allow partially undefined values in a lazy language); maybe you could accept user-defined notions of subtyping to encode a particular problem domain, but you would then have to prove that all projections from those terms to types (eg. in the Coq pattern `match ... return (match tag with ...) with ...`) are compatible with this subtyping choice, and that is no piece of cake. Another area for generalization would be to move from equality or subtyping constraints to arbitrary notions of "propositions": if you add a Prop kind and prop-formers that your system can reason on statically, you could add rather arbitrary logical constraints to GADT constructors (dead clause analysis would then need to solve more complex satisfiability problems). I believe such generalizations have been done in the community of applied pi-calculi, to reason and cryptography for example.
Well, as I explained not reevaluating the function does *not* necessarily require a table. In the case of `() -&gt; a` it's a single result that's stored. When floating independent subexpressions outwards, it doesn't require a table either. I didn't get the question because I did not imply in any way that you "get a memo table" for free.
There's some dangerous combination: I forget exactly what, but it was Gil Hur who pointed it out. IIRC, if you combine large indexing (as you've just done), injectivity of type constructors and excluded middle, you can derive a contradiction. As a result, injectivity/disjointness of type constructors was removed from the unification algorithm that handles the indices for case splitting. Personally, I find large indexing more worrying. It seems much harder to model. But then, I think of types as things humans invent in order to classify relevant structural distinctions and see no reason why it should be desirable to confuse one type for another just because they have the same cardinality. Which makes me a weirdo in this modern age. (There may, of course, be a universe-based way to reconcile these positions.)
Yes, I know. But sending arbitrary stuff around is an *idea*, and Don's post looks promising transferring it to a network domain.
Because of laziness. f = const "hi" a = undefined
I am in the situation of trying to convince my co-workers to adopt Haskell. I think the most difficult part is to convince people that it is worth it. Sure, Haskell has a lot of theoretical benefits, but does that really translate into successful software projects? Until we have a number of largish real-world well-known Haskell projects in the specific application area, people are going to be skeptical. Of course this is a chicken-egg problem, so we just need to charge ahead ;) The one thing that worries me a little bit about adopting Haskell is the potential for space leaks due to laziness. I think this could be a real problem in production code and it worries me because it can be hard to detect and who tells you that it doesn't show up at the worst possible time. I think that this is a real problem for server side code. In my opinion the Haskell community needs a better story here.
cereal is a fork of binary, created to address specific needs. It mainly changed these things: (1) it offered better error reporting (2) eventually also offered incremental decoding (3) somewhat different API, and some new combinators cereal did this by converting into CPS, much similar as attoparsec did to parsec, which also gave a speedup. Therefore it's likely faster than any binary-0.5.* version which used a state monad. binary has since then caught up and also offer these benefits, and expanded its API. For decoding, I've put just a few comparisons in the "get" benchmark in binary. Compile with --enable-benchmarks and see what difference it makes on your machine. You are likely to find that although they both use CPS in a similar approach, binary will be faster. On my machine binary is roughly 35% faster. binary-0.7 seems to always be faster than binary-0.5.
[Obligatory Oleg link](http://okmij.org/ftp/tagless-final/course/Boehm-Berarducci.html)
That's interesting. How is the seq analysis implemented in GHC? Does strictness analysis work even in presense of rewrite rules?
If it's a function, then "function" and "value" are synonyms in Haskell, and you need a new name for the type for which (-&gt;) is the type constructor.
Univalence doesn't mean that we're confusing isomorphic types, just that whatever you say for one, can be transported to the other, which is the basic principle of abstraction. Since the equality coming out of univalence is not definitional, the type checker will prohibit you from actually mixing up isomorphic types (unless you explicitly ask it to, using J), so the "relevant structural distinctions" that you mention are safeguarded.
What seq analysis?
Related: the correctness of stream fusion (which is a variant on unfoldr/destroy fusion), in the 3rd chapter of [my thesis](http://code.haskell.org/~duncan/thesis.pdf). Also, the intro chapter gives (I think!) a fairly good explanation of both foldr/build fusion and unfoldr/destroy, why we want them and how they work.
I meant to say that the compiler shouldn't use the rewrite rule if it detects a `seq` in a list given as an argument to `build`.
&gt; This thesis is about a technique for making beautiful programs run fast. I like it already :-) Looks really promising, thanks
Thanks. That was an exceptionally well-written paper. I wish every computer science paper were so clear.
System.Directory is shipped with GHC so you shouldn't need to install separately.
Oh :P cabal is too eager to install latest versions of things. I will try to find the incantation that does not have it trying to upgrade this.
Looks like I just needed to install an older `unix-compat` and now it's happy :)
Thanks. Fixed.
You might also enjoy the paper [Build, Augment and Destroy. Universally](http://www.cs.ioc.ee/~tarmo/misc/builds-subm.pdf) by Ghani et al. They essentially explain shortcut deforestation (`fold/build`) via alternative semantics that match those given by an initial algebra. This was (sort of) my first real introduction to algebras as of a few months ago: the concept of treating `fold/build` and `unfold/destroy` as initial algebras and final coalgebras respectively, with fusion laws coming up as universal properties. I ended up writing about this [as an explanation to show how to use GHC RULES](http://neocontra.blogspot.com/2013/02/controlling-inlining-phases-in-ghc.html) (and the [code is right here if you want to skip all that](https://gist.github.com/thoughtpolice/4411740),) although the theory side leaves to be desired. But the code was enlightening and fun to write. I may go more into this later.
When I first started learning Haskell, I started with LYHFGG. I honestly felt it was a waste of time (although, it makes sense if you're trying to make Haskell another Python in that you want to teach it to everyone -- including non-programmers). I couldn't really do anything in the language except write simple scripts. I immediately started looking for solid code bases to learn from (like I do after learning any language), and I found I just didn't understand Haskell. I had to go a step backwards and read Real World Haskell which did show me the language fairly well in that I could read Haskell, but I still couldn't write anything complicated without struggling. And I think that's because programming is a form of communication. When communicating in an imperative language, we can easily mentally model that communication as if we're giving sequential orders to a person. But communicating in a functional language is far more abstract, and consequently, I think, a superset of imperative languages. In FP, we're given some basic rules of communication from which we can develop a common language with a person, and then we're tasked with describing (rather than ordering) the ways things ought to be. Odersky carries you up that ladder of abstraction by showing you imperative Scala and then refactoring it into idiomatic Scala. I don't recall LYHFGG or Real World Haskell even attempting things like that or acknowledging that there is a mental hurdle there. For example, look at [Section 9.4 in Programming Scala](http://books.google.com/books?id=MFjNhTjeQKkC&amp;lpg=PP1&amp;pg=PA169#v=onepage&amp;q&amp;f=false) which is also relevant and necessary for idiomatic Haskell and yet I don't think it was ever mentioned in LYHFGG or Real World Haskell (although, I don't know about Thompson's book -- the 3rd edition is news to me).
I suppose we're in different industries hence the confusion. I come from a strictly web/internet based background; *all* of my programmer friends (exception of a two and me, maybe) have never even touched C/C++/LISP/Assembly/etc... Most of them are in the Ruby/Python/PHP camp.
You might be interested in some case studies we've done recently: https://www.fpcomplete.com/page/case-studies
This is a very good point. I come from imperative background and when I had to teach Haskell to non-Haskellers, I based my course on an example that any imperative programmer could implement in their own language: a symbolic calculator (I used the same example in my book, C++ in Action). 
I feel like we are already moving in this direction with the rising popularity of (bidirectional) higher inductive types. The rule "pattern matches must be functorial" produces some obvious obligations and is not that hard to add to the language, imo.
I wouldn't call `main` a function, but otherwise this seems reasonable.
I'm looking forward to seeing what you come up with!
I had a similar need while working on multivariate distributions in a Bayesian network library for Haskell last year. I newer got around to get a finished version. I'll take a closer look at it later (perhaps next week). Have you written about HLearn somewhere?
There's three blog posts about training univariate statistical distributions based on their algebraic structures here: http://izbicki.me/blog/category/computer-science/haskell I'll have a new post about multivariate distributions and copluae around the end of this month. If you look at the github source, there's quite a bit more work that I haven't written about on the blog yet. It uses a pretty inefficient system for handling both continuous and discrete data, and the HVector is designed to fix that. A lot of the work (e.g. new variants of AdaBoost) is a test bed for a paper I have currently under review. My next goal is to get multivariate distributions using copulae working correctly, and then I plan to use this as the basis for a DSL for Markov and Bayesian networks. I'd be really interested in having a look at what you did for your Bayesian network library. All the things I've seen previously have not been very Haskelly. I think the design I've selected will really show off all of Haskell's strong points.
That was basically my original thoughts, but it doesn't account for the difference in underlying representation of (a,b,c) and (a,(b,c)). The first is contiguous in memory and the second is not. So there is an O(n) penalty for every time you would do this.
Benchmarks?
LYAH is the text that turned me from Pythonista to Haskeller. "It'll improve your Python skills," suuuure... ;)
cereal exists because the halvm network stack needed to parse strict bytestrings, with careful exception handling. It was never faster, AFAIK.
I disagree with relating the concept of Tao to monads. Maybe explaining or grokking monads is hard or elusive, but monads themselves are not elusive. There's nothing mystical about them, they're clearly described in terms that anyone (given proper education) can understand. Further, saying that monads are somehow mystical only hinders the understanding of those trying to learn, leading them to believe that there is some sort of enlightenment needed to use them.
The inline editor/compiler really is amazing.
I haven't run any. Andrew Cowie says that his http-streams client library (which he's going to release within the next couple of days) is 30-40% faster than http-conduit in his testing, but there could be any number of reasons for that.
This is a parody right? :3
Are you quoting yourself in some of those quotes? that's an odd thing to do. 
If I understand what you're saying correctly, that's essentially how the functions in the heterogenous-vector library are implemented.
Whatever you are willing to consider equal, you are willing to confuse. I'm afraid univalence does mean that you insist on the capacity to confuse isomorphic types, even when it's meaningless to do so. Definitional equality is an accident of implementation, and if it happens to be decidable, a disappointing underapproximation to the equality one might want. I'm afraid that I do want equality of functions to be extensional, even though computers are too stupid for that ever to be captured by a decidable definitional equality. As a result, definitional equality insists on too many irrelevant implementation distinctions (between harmlessly interchangeable things, as opposed to between data representations where change requires computational mediation) to be a good guardian of relevant structural distinctions. But I'm happy if there's some well managed way to talk about types at a finer level than isomorphism, e.g., by encoding types in a universe: equality of codes might well be finer than isomorphism of the types codes mean. I'd be really pleased if the types we wrote down were automatically internal syntactic objects, so that we could choose to interpret them in a variety of interesting ways, each with its own notion of equality. It may well be that uniqueness of equality proofs for the syntactic objects is sustainable alongside a multiplicity of isomorphisms between the underlying sets, as long as we are clear which we mean. As far as representation abstraction is concerned, yes, I would like to be able to say "type R can serve as the underlying representation for type T" and shift functionality from T to R accordingly. Whilst I accept that making R equal to T is sufficient to achieve that goal, it is not necessary. It's clear that function extensionality, quotients, and working up to isomorphism (and other weak equivalences) are important things to have a handle on. However, univalence does not have a monopoly on approaches to those concepts. As an axiom with no computational behaviour, I find univalence deeply troubling. But I shall cheerfully applaud a computational theory with decidable equality and typechecking judgments which validates the axiom and computes canonical values.
io-streams is Handle-esque, while conduits/pipes are more inversion-of-controll-esque.
&gt;All functions in Haskell are pure, including main `main` is not a function.
go on...
This is exciting!
It'd be nice if there were an io-streams-core package that contains only the core definitions, without the support for attoparsec, zlib, etc. This way, authors can hook into the io-streams framework without bringing in so many dependencies. Then again, large programs will likely end up using all these dependencies anyway.
https://www.scss.tcd.ie/Edsko.de.Vries/ct/catsters/linear.php
"they're clearly described in terms that anyone (given proper education) can understand" If by "proper education" you mean some knowledge of Category Theory then you're right. I did try this approach: http://bartoszmilewski.com/2011/01/09/monads-for-the-curious-programmer-part-1/ .
I had my tongue deeply planted in my cheek ;-)
Actually, now it's even simpler than that: https://fpcomplete.com .
This looks like a really nice, really slick IO library. I particularly like (and agree with) the attitude towards monad transformers. One question - which feels like a stupid one but I'm not sure if it is...why does write take a (Maybe a)?
Yes, but there's quite a bit of room between Tao and category theory. Personally, I know little about category theory, yet I can use monads productively. But the point is, they can be described precisely (regardless of whether I personally have the education to understand it or not), unlike Tao. I know it was probably intended more as a literary device than a proper analogy, but I think it hurts more than it helps.
I've always felt that I like to organize my code concurrently with different little agents handling different kinds of IO or computations. Even in functional languages this is nice, and you can potentially take advantage of multiple cores. This looks like a nice way to do that.
If I understand correctly, because main doesn't map an input to an output; it is simply a value.
Somebody's OutputStream is somebody else's InputStream. Reading a Nothing means the steam has ended, thus writing a Nothing means there won't be any more data. See the haddocks: http://hackage.haskell.org/packages/archive/io-streams/1.0.0.0/doc/html/System-IO-Streams-Core.html
From the docs: &gt; write :: Maybe c -&gt; OutputStream c -&gt; IO () &gt; Feeds a value to an OutputStream. Values of type c are written in an OutputStream by wrapping them in Just, and the end of the stream is indicated by supplying Nothing. It seems to me write takes (Maybe a) to provide symmetry with read. This allows simple stream composition, etc.
Why don't just use already existing library [fixed-vector](http://hackage.haskell.org/package/fixed-vector)?
The API looks awesome in its simplicity. Looking forward to playing with it.
Thanks for the comment. It's another perspective I've yet to consider.
I thought that this would have better as a layer on top of conduit or (some other package), rather than rolling everything from scratch.
&gt; "complex code analysis" makes it sound much scarier than it really is. I'll change that to "complex *to me* code analysis" in the future ;)
I can get how and why a compiler/runtime would repeatedly call a function if it were to do something such as change the case of a character. The issue was less clear for functions such as *max*. It's more clear now, thanks.
Unless I'm mistaken, fixed-vector is parameterized only by its length, whereas vector-heterogenous is parameterized by its length and a potentially unique type for each element in the vector.
iostreams seem to be "closer to the handle" than either conduits of pipes. I can see proxies from the pipes packgage using iostreams underneath, rather than the other way around.
Troy &amp; Abed learning Haskell!
Sorry, I missed this point.
Some stream transformers can buffer or delay output (builder &amp; zlib transformers come to mind). You need a way to indicate that the stream is finished so that the appropriate finalization can be run.
Imho it might be clearer to have a seperate `close` function instead. Writing gets easier and people are actually encouraged to finalize the stream. The current interface emphasizes the symmetry of `read` and `write`, though.
Indeed fixed-vector require that every element have same type. But think same approach could be extended to work with heterogeneous vectors.I'll try it out
Looking at socketToStreams http://goo.gl/BcJo6 in System.IO.Streams.Network, can one interleave the use of both the io-streams library and the network library to read and write bytestrings from a socket? i.e. (inStream,outStream) &lt;- socketToStreams sock writeLazyBytestring (C.pack "hello") outStream bs &lt;- NBS.recv sock 1024 NBS.sendAll sock (C.pack "world") bs' &lt;- readExactly 1024 inStream
`optparse-applicative` is a dream to use after the alternatives.
One of the major motivations for conduits was to work well with exceptions, and it was designed and built from the ground up with that in mind. I have never heard of anyone having problems with that in conduits. Pipes are also reported to be exception-safe now; see Tekmo's comment below. EDIT: Fixed to reflect Tekmo's report that pipes are exception-safe now.
Isn't it connect that you are looking for? This already exists http://goo.gl/ZPloa connect :: InputStream a -&gt; OutputStream a -&gt; IO ()
Why do you say that? In practice, I have not found that using conduits in practice gives a feeling of "inversion of control" at all. You write small pure pieces, you connect those to small IO pieces, and you connect the IO pieces together into simple and intuitive processing pipelines whose operational semantics is clear. That was one of the major motivations for conduits in the first place - to fix the feeling of "inversion of control" you used to get when you used iteratees/enumerators. I think conduits achieved that goal. However, it is true that streams seems conceptually closer to the traditional model of IO in Haskell.
Pipes are exception-safe now, including asynchronous exceptions. See the pipes-safe library.
Happy to take a pull request :)
If I'll have time.. :)
For conduits, besides the conceptual differences, the main practical difference right now is that conduits are battle tested in a lot of real life applications created by quite a few people. Before version 1.0, the conduits library underwent a significant period of performance tweaking, API tweaking, and robustness tweaking in a variety of scenarios, including both simple scenarios and complex ones. By this time, there are plenty of successful applications, including commercial ones, already deployed using conduits. Streams looks really simple and elegant. A nice way to work. It looks like there are people who are working hard on maturing it, but that always takes time. I'm looking forward to it.
&gt; [...] thus writing a Nothing means there won't be any more data [...] actually (the way I understood it), writing a `Nothing` may also mean to trigger a flush; it's up to the actual OutputStream implementation what writing `Nothing` means
Hmm... I can imaging that it could be useful sometimes, but generally it seems to be a bad idea. `write` should block until the data is "sent" to prevent space leak, and `read` should block until data is ready. As a result the `pipe` combinator would be hard to implement correctly and, more important, to use correctly. I can imaging that I miss the idea, just IMHO.
I really think we should refactor the whole IO library to be based on this interface. If you turn a Handle into a Stream, you're getting an unnecessary layer of buffering. Encoding/decoding should be done as a stream transformer, which would make it much more accessible (right now it's hidden in the depths of GHC.IO). There might be a little performance cost to doing all this, because right now the IO library goes to some lengths to reuse buffers rather than having streams of ByteString, but my guess is that the overhead wouldn't be very high and the compositionality you get as a result would be worth it. Basically I would replace Handle with some device-specific types (File, Socket, etc.), provide operations on those, with some type classes for the common operations. Then have ways to create an InputStream/OutputStream from any device that supports the equivalent of hGetSome/hPutBuf respectively.
That would be nice. It would be good if it were still possible to access the current faster IO when needed via a different module. But if that would make this refactoring too difficult, I would still be in favor. Also - if the API only provides hard-wired IO types such as File and Socket rather than a general Handle type, how hard would it be for someone to create a new custom IO type? E.g., specialized hardware, or some external IO library via the FFI.
Yeah, buffering/blocking is something that entered my thoughts too, and I don't know enough about how io-streams does it (I've only read the API docs and a blog post!) to be able to evaluate further...
I use similar approach for random input stream (https://github.com/Yuras/pdf-toolbox/blob/master/core/lib/Pdf/Toolbox/Core/IO/RIS.hs). It works for me, but in general it is not very convenient (at least my implementation :) ). And ~10% slower. At first glance it is even harder for output streams.
The issue is that seek will mess all streams created before. It is possible to handle, but harder and possibly slower. Ideal solution should add overhead only when you actually do seek.
A `Handle` is really just a wrapper around an existentially quantified device already. I suggest removing the wrapper, since we hardly ever want to make a list of heterogeneous Handles anyway, and if you did, you could write your own existential wrapper. Furthermore, having to map everything to a `Handle` means that we lose some type safety - e.g. `hSeek` failing on things that are not seekable. The idea would be that to add a new device, you create a new type and provide ways to create `InputStream` or `OutputStream` from it. You could have type classes for operations that many devices have in common, e.g. an overloaded `close` operation. 
&gt; newtype EitherP e p a' a b' b (m :: * -&gt; *) r &gt; = EitherP { runEitherP :: p a' a b' b m (Either e r) } &gt; &gt; type ExceptionP = EitherP SomeException &gt; &gt; data Status = Status { &gt; restore :: forall a . IO a -&gt; IO a, &gt; upstream :: IORef (IO ()) , &gt; downstream :: IORef (IO ()) } &gt; &gt; newtype SafeIO r = SafeIO { unSafeIO :: ReaderT Status IO r } &gt; &gt; bracket :: (Monad m, Proxy p) =&gt; &gt; (forall x. SafeIO x -&gt; m x) &gt; -&gt; IO h &gt; -&gt; (h -&gt; IO r') &gt; -&gt; (h -&gt; ExceptionP p a' a b' b m r) &gt; -&gt; ExceptionP p a' a b' b m r ...and with the continuation-passing style you have to jump through considerable hoops to do so. It's not so much that writing in this style makes exception-safe IO impossible; rather, that you have to reduplicate a lot of the functionality that is already implemented efficiently in C in the runtime system, and rely on kludges (or at least semi-kludges) like masking asynchronous exceptions everywhere but "tryIO" blocks.
You can reuse the buffer in io-streams using `System.IO.Streams.Builder.unsafeBuilderStream`. It's unsafe, of course.
It could be implemented as a bounded channel. Not sure I want to add an STM or BoundedChans dependency to this library though.
&gt; Why does the buildRequest function require the connection as a parameter? Because the Host: field is mandatory, and we already specified it care of where we connected to in the first place. So buildRequest digs it out of the Connection, unless you call `setHostname` to manually override it. I suppose we could skip that and set the Host header in `sendRequest`, but then the Request object wouldn't contain all the request headers that are actually going to be sent. &gt; That forces you to open the connection before you even start building the request header. That's an interesting point. Do me a favour and open an issue on GitHub so we can discuss it? I think the reason I just described is a good one, but I'm happy to throw it around a bit. &gt; ... example is meant to be contrived. But still. You had better... Good suggestion. Thanks. I'm still developing the exclamation reflex. AfC 
The first thing I'll try with the `pipe` combinator: (in, out) &lt;- pipe chunk &lt;- read in write (Just "Hello") out That is an example of "hard to use it correctly". If I understand the idea correctly, the code will dead-lock. When writing before reading, I need to be very careful to ensure it will not block on full channel. Do I miss something?
Correct, this code will deadlock. The pipe primitive would only be appropriate for situations in which the input and output streams generated would be read from and written to by separate threads. After thinking about the buffering issues (i.e. wanting to have a bounded buffer here) I'm not sure this would belong in core io-streams.
Hmm, there is no README for this GitHub project. But it looks like this is much more than an Objective-C parser and quasiquoter. This seems to be a fork by Manuel Chakravarty of Geoffrey Mainland's parser and quasiquoter for GNU C, called language-c-quote on hackage. I see that Manuel already has some commits related to Objective C in Geoffrey's main Github repo. So are we to assume that new code is just a continuation of that previous work, and that it will soon be merged back into the main repo and pushed to hackage? There is also a language-c package on hackage, but that code seems unrelated.
And let's say the underlying file happens to be something like a unix pipe - or perhaps that will be its own type. You can only make that into an `InputStream` once then, right? So would that operation be in the IO monad, and raise an error if you try to do it a second time? And would there be a function to query whether this File is streamable? This is all starting to sound rather messy.
What does "not thread-safe" mean in this context? What can go wrong exactly?
Yup, obviously the design needs some careful thought. It is problems like these that caused me to abandon this idea before, but I was hoping that Greg's formulation of streams (which is nicer than we had before) would lead to a clean design.
I explicitly avoided trying to solve every conceivable problem in this library, random access especially, because I wanted to come up with a flexible and above all *simple* abstraction for straightline streaming first, and realized that it was really easy to go off "into the weeds" here.
You can still program in the iostreams style, too. Nothing prevents you from opening a handle using bracket outside the pipeline and then streaming from the handle: withFile file1 ReadMode $ \hIn -&gt; withFile file2 WriteMode $ \hOut -&gt; runProxy $ hGetLineS hIn &gt;-&gt; hPutStrLnD hOut I only provide pipes-safe because people wanted better resource management than opening everything beforehand and closing everything afterwards. With pipes you have a choice between which approach you prefer and both approaches coexist well with each other with neither requiring buy-in from the other.
The host of the request can be a `Maybe String` so that the request value is always completely defined, and `sendRequest` can match it. I also find surprising that the connection value is needed there, and that building such a pure request must happen in IO. Also, about this request interface, is a monoid instance provided as well ? It should be enough for this type of usage, and a state monad looks a bit overkill at first glance :) Other than that, I am happy that Snap gets tooled with a clear model for streaming IO, both downstream and upstream. Thanks.
The problem comes when you need to acquire a resource in the middle of a streaming computation; concretely where Snap is concerned I'm thinking of HTTP file upload, where our iteratee-based solution is an egregious hack involving MVar finalizers. When you program in direct style this kind of thing gets a lot easier, because hey, you're still in IO.
Hmm. It sounds like by "resource" you mean the actual data in the case of a file. But this model implies a flow of data, not random access. You would presumably support reading from a unix pipe, for example. So perhaps it doesn't make sense to support seek at all. Perhaps that could be supported by a variation of io-streams, or a different IO model altogether, that is more random-access in nature. Lack of thread safety is obviously a very big limitation for real world applications. The underlying IO primitives actually work just fine with concurrent threaded access, and almost every non-trivial modern application uses them that way. You are correct that understanding their behavior in that context is not simple, and every developer has to put some effort into learning how it works. But if you are abstracting away the underlying primitives completely, I think a little more needs to be said about how to work with io-streams in the context of concurrency other than just that they're "not thread safe".
If by "work just fine" you mean "you get no guarantees about the atomicity or ordering of your write() with respect to any other concurrent write()s that may be happening", then you are correct :) As far as io-streams goes, the current advice is simple: if you want to be able to use an `InputStream` from two threads concurrently, you need to wrap it using `lockingInputStream`.
Yeah. You would want to use pipes-safe to acquire a resource within a pipeline. I think we can both agree that pipes targets a different audience. In fact, I need to write pipes bindings to iostreams (both to and from) so that users can get the best of both worlds.
Right. As we've discussed, I'm not saying any of this stuff to knock pipes (which I think is really interesting) or conduit (which is itself a nice library). I'm just trying to explain why I felt that I needed to write something different again to serve as the basis for the next major version of Snap.
I understand. I have to put on my critical hat now if I have to depend on it in the future. :)
`ResourceT` also provides a bracket-like API as a special case. But `ResourceT` is far more than `bracket`. It provides a language for specifying any ordering of allocation and release of multiple resources, with bracket-like guarantees. It also allows you to release a resource conditionally at an earlier stage of processing if you can determine that it is no longer needed. I'm not sure why you are mentioning continuation-passing here. `ResourceT` is not continuation-passing style, except in that you give it functions to acquire and release resources as in `bracket`. The conduits library, which is one of the most famous clients of the resourcet library, is also not continuation-passing style.
&gt; objectively [...] in my opinion
I don't think that's quite accurate. I had some discussions with Michael about conduits before the first public version and before either of us had heard of pipes, and already then that was one of the main topics of our discussion. And I'm pretty sure Michael discussed that with Oleg at the time, too. So, rather, I would say that getting rid of "inversion of control" was an important motivation for both conduits and pipes. In my opinion, the most important thing conduits took from pipes was a unification and simplification of its types. And it is hard to overestimate the importance of those improvements. Overall, it was gratifying to see the creative effects of cross-pollination between conduits and pipes. That kind of interaction is an important part of what makes the Haskell community so strong. It will be interesting to see further developments now that io-streams is jumping into the fray.
Right. I plan on writing conversion functions like: fromIOStream :: (Proxy p) =&gt; InputStream a -&gt; () -&gt; Producer p a IO () The correct mental model for `iostreams` is that it is a replacement for `Handle`s and `Socket`s.
&gt; The pipes library was the most faithful attempt to get it right, in my opinion, but I still find myself unsatisfied by the idea that it's just another free monad; the same is true of the IO monad itself. Free monads are, kind of by definition, uninteresting; it's just a way to structure your library as a monad if you can't find a way to cram your types into a monadic interface otherwise. First of all, I would say that `IO` should be seen for what it is: a synthetic type constructor created by "purifying" some category. There's plenty of interesting theoretical ideas in that, and I'm working on a blog post explaining that. Secondly, free monads are interesting. An example is `Writer [a]`, which is (roughly) isomorphic to `Free ((,) a)`. Thirdly, `pipes` is not just a free monad, it's five (!) categories ^(if I recall correctly). I have only skimmed the article, but I have the same impression of io-streams [edit: as you].
I skimmed though you code and it looks that you use unsafeCoerce incorrectly. Check description of unsafeCoerce# for details http://www.haskell.org/ghc/docs/latest/html/libraries/ghc-prim-0.3.0.0/GHC-Prim.html
I've said this many times before, but my favorite pedagogical approach will always be Dan Piponi's [You could have invented monads](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html). That article made everything click for me because it presented monads in the context of solving real problems.
The issue is that while the correct term is "value", common parlance in Haskell is to call everything a function, even things that accept no arguments. It's unfortunate really, because there is a really big insight to be gained when one realizes that `IO a` values are not functions and do not need to be artificially applied to an empty set of arguments to be "run". A lot of imperative programmers bring this weird intuition that function application equals execution, and then they get confused when Haskell IO actions like `getLine` don't require any arguments, even empty arguments.
So maybe the operation of creating a stream from an IO object should not be standardized. A function that creates a pipe could give you a pair of the pipe object and the stream, making it impossible to try to create a second stream for it. A file object creation could give a pair, but also provide a function to create an additional stream for it, corresponding to dup. Etc. Just some ideas.
I'll second that. There seems to be a really common false dichotomy where people think monads either have to be taught using category theory, or using bad analogies. The "show how to solve real problems, then show how those different problems all share a common pattern" approach Dan used is much better.
&gt; Do me a favour and open an issue on GitHub Done. It's #9. I included kstt's suggestions.
Some might consider it to simply be a 0-arity/nullary/constant function? main is a function that takes 0 arguments and always returns (). But.. it has some interesting side effects along the way.. 
Everything you've ever wanted to know about Category Theory as a Haskeller, that is. Not much here about abelian categories, for example.
I tell you what simon, it would be nice to have a page with brief description of these internals (like your comment here), and linked to the papers for further reading. Maybe something like this exists already and I'm unaware of it, just a thought.
The [IO a] is the single most amazing thing in haskell IO system. It drags a user kicking and screaming into a wonderful world of monads!
While it's not as direct as io-streams it's possible to do this sort of thing without ResourceT in conduit: import Data.Conduit import qualified Data.Conduit.Binary as Cb import qualified Data.Conduit.List as Cl import qualified Data.Conduit.Text as Ct import qualified Data.Text as T import System.IO main :: IO () main = do (src, mval) &lt;- Cb.sourceHandle stdin &gt;+&gt; Ct.decode Ct.utf8 &gt;+&gt; Ct.lines $$+ Cl.head case mval of Just val -&gt; withFile (T.unpack val) WriteMode $ \ h -&gt; do src $$+- Ct.encode Ct.utf8 &gt;+&gt; Cb.sinkHandle h Nothing -&gt; return ()
How does it handle redirects? Do i have to handle 303 and 301's on my own? Does it protect me from redirect loops? http-conduit have support for this iirc. And it's customizable in the lower level library of http-conduit but uses a sane default in the higher level calls.
The `get` convenience function handles redirects, including loop prevention, yes. I don't think the lower-level functions do but maybe Andrew will make some automagic to deal with that in a later version.
Or quillen categories. Can I get some dependent types up in here?
What made you decide to use a monad for building the request parameters rather than using record syntax? q &lt;- buildRequest c $ do http POST "/api/v1/messages" setContentType "application/json" setAccept "text/html" setHeader "X-WhoDoneIt" "The Butler" q &lt;- buildRequest c $ do http POST "/api/v1/messages" setContentType "application/json" setAccept "text/html" setHeader "X-WhoDoneIt" "The Butler" vs let q = defaultRequest { method = POST, url = "/api/v1/messages", contentType = "application/json", accept = "text/html", headers = [("X-WhoDoneIt", "The Butler")] }
&gt; main is a function that takes 0 arguments and always returns (). This is kind of just being pedantic in this case, but [Haskell doesn't have functions of zero arguments.](http://conal.net/blog/posts/everything-is-a-function-in-haskell) main is a constant applicative form, which is basically any definition that's not a function. edit: mgsloan beat me to it But the real issue here is that main itself doesn't perform side effects and then return () - it evaluates to an IO action (a monadic value) that does that. To see the difference, try "print 1 `seq` ()" - it evaluates to () without printing anything. "print 1" is being evaluated, but that evaluation doesn't actually perform the printing - it just creates an action that will do so when used in the right context.
Four categories. Technically I previously mentioned a fifth category, but then I discovered it was not pipe-specific at all (and it inspired my "`MonadTrans` is missing a method" blog post, which describes monads in the category of monads, where the fifth category was a Kleisli category where the objects are monads). Edit: No wait, it's 5. I forgot the ordinary Kleisli category.
I have absolutely no doubt that your goals are aligned with the community. But please make that clear in your terms as well! It's just that legal stuff is precisely relevant for the case where things do go awry, as unlikely as that may be. I have looked at the terms of Wordpress and Tumblr, they are actually more refined. Wordpress adds a clause that you can delete your content. Tumblr explains that it cannot do so because of reblogging. Tumblr specifically adds that the license is only needed for 'normal' operation, i.e. that the site allows you to have a blog and that it obviously needs to display your content. The School of Haskell is more specialized than a general blog platform; I don't think that a "standard" template can do it justice. For instance, one thing that FPComplete could do with the current license is to collect tutorials in a book and sell that for money. Compared to Wordpress and Tumblr, which a providers of a very general service, FPComplete focuses on Haskell specifically. If I were to write the terms of service, I would note the specific purpose along the lines of "Allow display on our website, with the FPComplete brand attached, but otherwise the content is yours to control/remove." 
For those who cannot make it, but would like to help with the projects, could there be google hangouts or irc chats rooms for collaboration?
But `getChar` *is* a function (proof: http://www.haskell.org/onlinereport/haskell2010/haskellch7.html#x14-1430007.1). HR will be fixed sooner or later, but right now there is nothing wrong in "main is a function" statement.
same here.. more approaches to explore the solutions on this one are a good thing. let us see where it moves from here!
I looked in the data-default package and my impression was that all it does is define a default values (`def :: Default a =&gt; a`) for a bunch of types. How can you omit record fields with it (and how do you choose your own default values for them)?
I've been out of the Haskell loop for a bit... What's going on with respect to string literals and ByteStrings here. The example doesn't work on the Haskell platform I have installed. 
Ah never mind, it's OverloadedStrings. Does everyone just know they have to do that now or would it be a good idea to mention it when it's being used?
Our IRC channel will be #odhac, I believe. (I haven't got around to creating it yet...) You could hang out there during the hackathon and arrange a google hangout with the group of people you'd like to join. *UPDATE* The channel `#odhac` is now created.
&gt; It's named caledon after the "New Caledonian Crow" - a crow which can make tools and meta tools. Oh, at first glance I sort of assumed it was made by someone who was a big fan of cgibbard and dons.
 instance Default Foo where def = Foo { a = 1, b = 2 } Now you can use it as: def { a = 3 }
&gt; What made you decide to use a monad for building the request parameters rather than using record syntax? Seemed like a nice way to express it. I got the idea from Snap.Test in the snap-core package; it's a nice facility quietly built into Snap that allows you to write test cases for a web server. Sure it's "do-notation abuse" but on the other hand one is building up a small piece of state based on various conditions and it seemed to be a valid use of the state monad. Finally the individual functions are strongly typed. Those examples there are string literals, but there are a few that take more complex arguments. Of course there's no reason you couldn't do it with an exposed record but there are a few internal behaviours driven by what gets set there [the biggie is content type; if that gets set then `sendRequest` will switch from chunked transfer-encoding to sending precisely that many bytes] so there would still be some post-processing of the object needed before you could look at it as a Request. It's not an allocation or performance bottleneck at present. {shrug} if everyone wants it to be a record we can change it, but it seemed nice enough way to express the API for starters. AfC
&gt; is a monoid instance provided as well? I suppose one could be. If I re-express the internal machinery using a monoid rather than state I'll certainly expose it. AfC
Can't you implement the yield/await stuff on top of io-streams though? For example, in languages like Python or Lua generators are accessed via a "handle" interface (with a "getNext" function). The "yield" thing is more of a convenience for building the generators, since you have a builtin mechanism that takes a subroutine written with "yield" statements and gives back a handle to the an abstract generator. (the io-streams page mentions that some difficulties arise relating to the CPS/inversion of control libraries, but I don't know if these apply in this case...)
That's the purpose of `defaultRequest` above, it's a record that defines default values for all fields, so you only have to override then ones you want to change. Presumably the monad version does the same behind the scenes (but less efficiently by repeatedly modifying a record).
&gt; Does everyone just know they have to do that now or would it be a good idea to mention it when it's being used? I certainly got burned by that when I was started Haskell. I got in the habit of putting `{-# LANGUAGE OverloadedStrings #-}` in every single source file. I kept getting burned but it until about last month every time as I'd then forget to `:set -XOverloadedStrings` in GHCi. So I finally buckled and put that in a .ghci for this project, which certainly helped testing. I should probably just put it in ~/.ghc/ghci.conf I heard some talk that this was going to change in the next Haskell report, ie that Haskell 98 limitations would be a explicit fallback rather than an implicit default. That true? Be nice if it was. But you're right, I should have included that in my blog post. Sorry! AfC
Now released: https://github.com/gwright83/Wheeler
I think the former :-)
I meant exactly that. As far as I could understand from documentation functions are different and this is like to be untested. Also I'm wary of undefined behaviours in runtime systems. They have capability to create very weird and difficult to track bugs. It's better to be on safe side
In the Unsafe.hs file is a completely different implementation than the one described in the Readme. It's not actually exported, it was just there for experimentation purposes. Is that what you're referring to? The implementation in Heterogenous.hs should be correct.
I don't know exactly what additional structure you expect pipes to have. Do you think I missed something in the [24 equations](http://hackage.haskell.org/packages/archive/pipes/3.1.0/doc/html/Control-Proxy-Class.html#g:3) I defined in the `Control.Proxy.Class` module. Or how about the [50 equations](http://hackage.haskell.org/packages/archive/pipes/3.1.0/doc/html/Control-Proxy-Prelude-Base.html) in the utility library? Contrast that with other libraries which have a grand total of 0 equations that they respect.
&gt; a synthetic type constructor created by "purifying" some category This is exactly why it's a free monad. The construction goes like this: 1. Start with raw, impure primitives. 2. Make values out of them (the "purifying" step). These values still don't have any defined semantics (which I claim still makes them uninteresting), but at least they are pure. 3. Make a monad out of them. This does not add semantics where none existed before. It only allows you to chain your operations conveniently. &gt; Secondly, free monads are interesting. An example is Writer [a], which is (roughly) isomorphic to Free ((,) a). I didn't mean "no free monads are interesting." I only meant "monads aren't interesting just because they are free monads, so a monad that is nothing *more* than a free monad is uninteresting." &gt; Thirdly, `pipes` is not just a free monad, it's five (!) categories Yes, categories are desirable. Tekmo went through great pains to shape `pipes` this way because he believes that programming with categories and the nice axioms that come along with them is better than programming without. However, this was still somewhat arbitrary in the sense that there was no inevitability, no particular denotation he was shooting for. He started with some primitives, turned them into a monad, and wrote some additional functions that satisfy the category laws. It's surely no accident that this was possible, but that doesn't necessarily make his choice of primitives any simpler or any more enlightening than any other choice of primitives.
I never use "drawn from" when I give Haskell lectures, I use "gets", as it's meant to be like the assignment syntax of GCL or similar.. I don't think there is an official name, which is why tutorials don't mention it. 
io-streams pretty much requires IO, as it's all IORefs, and the whole design is based on mutation.
Not quite all of them. SCC supports non-linear, branching pipelines. 
io-streams provides `read` for consumers and `write` for producers, which are similar in spirit to `await` and `yield`. However, it throws these operations into the IO sin bin, where you have to rely on documentation and implementation to know whether or not these operations block or move on. With conduit/pipes, you know that it "blocks" until control is explicitly returned to you.
That would not be a great idea for the reasons others have already given, but the following should work just fine: pipe :: (OutputStream a -&gt; IO r1) -&gt; (InputStream a -&gt; IO r2) -&gt; IO (r1, r2) 
I'm noticing heavy use of `$!`. For the following code snippet, does it really make a difference? return $! () I still think we should have a name for this common idiom in the standard libs. Throw in the `$!` if it really makes any difference. pass :: Monad m =&gt; m () pass = return () 
`connect` reads a value from the input stream and writes it into the output stream. The idea of `pipe` is opposite: when you write value into the output streams, it should appear in the input stream
The idea of denotational design is still very new to me. Can you give me an example of an abstraction built from a denotation that I can study?
Interesting, I guess this explains why his curriculum focuses so much on dynamics and not so much statics.
In a nutshell, you try to find a model, usually in the form of some mathematical object, which is necessary and sufficient to describe the language you want to create, then you use it to inspire everything possible about your interface. The simpler and more precise your model is, the simpler and more precise your interface will be, and the easier it will be to learn and reason about. The more faithful your interface is to your model, the more predictable it will be. It differs a bit from your approach to pipes in that you focus heavily on free structures, which I see as a kind of degenerate case where your implementation precisely *is* your model, but in general this need not be (and rarely is) the case when your goal is a *simple* model. Consider the difference in complexity between your proxy type and the model for behaviors as in classical functional reactive programming, which is just `(Time -&gt;)`. Despite the simple model, implementations for it are often much more complex than your implementation for pipes, but I would argue that the benefits of the simple model are worth the complexity compared to a simpler implementation and a more complicated model. (Actually, even pipes has a little of this tradeoff in the form of the fast proxy type; its model is the somewhat simpler "correct" proxy type, yet it's implementation is (slightly) different (and the implementation must be hidden in order to not violate the model). Off the top of my head, all I can think of to read is a lot of Conal Elliott's stuff. "Type Class Morphisms" is a good one, especially in the context of Haskell. There is a book I remember reading which was also good, but I will have to get back to you with its title.
I went to check the io-streas source and apparently the *do* implement [a generator monad](http://hackage.haskell.org/packages/archive/io-streams/1.0.0.1/doc/html/System-IO-Streams-Core.html#g:7) for inversion of control. But according to the docs you can't do resource management and exception handling inside it - I wonder if this is too big of a limitation or if you can get around by doing all the allocations before the "control-inverted" code.
&gt; However, keep in mind that pipes are designed to be free so that they can be used as a general purpose structure. I have many application domains in mind for pipes, not just streaming binary data. This is actually a benefit of simple denotations, though. Simple denotations are typically quite general compared to more complex ones.
Oh, something I think might be worth reading. I read it a long time ago and remember liking it at the time. http://www.bcl.hamilton.ie/~barak/teach/F2008/NUIM/CS424/texts/ds.pdf You may find that these seem kind of "trivial" once you get down to it, but that's kind of the point. It's an approach that makes so much sense that once you've realized it, it's kind of hard to imagine doing it any other way. At least, that has been my experience. Edit: Oh, this pdf seems to be missing bits of various figures. :(
This is mostly just me playing with new YouTube features... but it occurred to me some people might find it useful!
I was on my phone when I typed my earlier reply to this comment, and now that I'm home I realize that I didn't exactly answer your question. Here's a go at an example. Let's consider [vector clocks](http://en.wikipedia.org/wiki/Vector_clock). Their purpose is to track the partial ordering of events in a distributed system. (I've been dealing with them off and on at work since last summer, so this has been on my mind a lot.) If you aren't yet familiar with them, it may be good to read up on them a bit, especially on how they are used. One issue that vector clocks have is that with a dynamically changing number of actors, the vector clocks can grow very large, making communication and many other operations fail to scale properly. There have been many attempts to make vector clocks more efficient, with varying degrees of success. Most of them have varying behaviors and properties. Now, let's say I wanted to make my own vector clock library. The first step in a denotational design is, of course, to come up with a model. Unfortunately, I don't know of any particular methodology for this step; it's the hard part which requires background and creativity. My approach often starts by observing existing presentations of the subject and trying to distill a general framework out of them. My first observation here is that the "vectorness" of a vector clock is too tight of a constraint. We don't actually care about the ordering of the components of the vector. It's more of a map than it is a vector. Well, I happen to already know of a denotation for maps and partial maps (`k -&gt; v` and `k -&gt; v+1`, respectively). In the interest of simplicity and generality, I choose the former, since I know that I can always compose it with something like `Maybe` to get a partial mapping. Come to think of it, this makes a lot of sense in the general case, too, since we can potentially have an infinite number of actors, so we would want our denotation to support that. So let's choose our model to be something like `[[VectorClock]] = Actor -&gt; Tick`. Now we just have to define `Actor` and `Tick`. Let's do the latter, first. We know we need to be able to increment and compare them. Beyond that, it doesn't really matter what we choose, as far as I can tell, so let's just leave it abstract and assume some axioms about them. A similar point applies to `Actor`, so we can just leave it abstract as well. We have a choice now. We can choose to abstract over `Actor` and `Tick` or just make them universally or existentially quantified. The first would be more general, and the last would be potentially easier to optimize. Universal quantification sounds pretty ridiculous, though. Regardless of what my choice here is, I am forced to think about it explicitly since otherwise I would not have a well-defined model at all. In fact, I might choose to make multiple choices! Perhaps I could make a few type classes that minimally describe all the things I can possibly do with such a denotation, and require a few existing ones that fit as well, and then make several implementations which all conform to the interface. For example, one implementation could be the obvious `Map String Word64` kind of thing, but another could be something slightly less obvious, interval tree clocks (designed as a more efficient replacement for vector clocks with slightly different behavior from the original vector clocks but still in conformance with the model we came up with). This would be a case where the denotation does not necessarily resemble the implementation; the model I came up with for interval tree clocks is `Q -&gt; N`, that is, a multiset of rational numbers. Interval tree clocks also require that each actor have a kind of key to designate which of the `N`s to increment (there is no designated, single tick for any one actor, but a whole set of ticks), which I model as `Q -&gt; 2`, that is, a set of rational numbers. The actual representation for interval tree clocks doesn't actually resemble this model at all, though; Google can be your friend, here, if you're curious. There are probably also many other possible representations for this model (perhaps some that are more efficient, even?). Anyway, once you have come up with a model, the implementation is already pretty well motivated, and there isn't so much to think about unless it just happens to be a difficult implementation for other technical reasons. FRP implementations tend to have this property, for example. :( I'm afraid I may have rambled a bit, since I don't actually have the time to cover the complete design of a library along these lines, but I do hope I have given you a taste of this style of thinking about design.