This comment encouraged me to add some comments to the READMEs for the relevant projects (conduit, conduit-extra, and conduit-combinators) explaining the breakdown among those three, which is essentially: * conduit: core abstraction, minimal dependencies * conduit-extra: light-dependency helper functions * conduit-combinators: the fully-powered abstraction, which I recommend using, when you can handle having quite a few extra dependencies __EDIT__ I'm posting this to Reddit before just modifying the READMEs in the hope that someone will ask me some clarifying questions that will help me improve the text I'll end up writing. So fire away!
I'm so glad that I just read the post this refers to and am able to get the joke. Bravo, /u/twistier.
&gt; But if we take A to be something like "Fully annotated System F terms" (use a GADT and existential) and B to be untyped lambda calculus then the fact that Bool is a subobject classifier would mean that type reconstruction for System F was decidable I'm not sure if this example works exactly (since I'm not sure the erasure function can be made monic) but the idea is certainly along what I was thinking. And it is close enough to working: specifically, if Hask were a topos we could construct something just as bad using the epi mono decomposition of the erasure map. For example, if we use `A'` to be the type of lambda terms together with type derivations in the intersection typing system (which types exactly the SN programs) and set `B` to be the type of untyped lambda terms (note specifically that both of these are real Haskell types and should therefore be objects in idealized Hask) and then use the epi-mono decomposition of the erasure map to construct an `A`, which, up to isomorphism contains exactly the strongly normalizing lambda terms, `Bool` being the subobject classifier would make `chi` a function which decided if a term was strongly normalizing. Something which is clearly not computable. Having these not computable properties would make "Hask" basically useless, IMO. You would lose any of the nice properties you get from knowing that all functions are computable (like knowing all real valued functions are continuous). And, it just seems like nonsense. However, there is a more fundamental reason why `Hask` can't be a topos. If it were, then it would serve as a model for Girard's System-U (since Haskell has polymorphic types, and polymorphism is not just restricted to "some universe" but really ranges over all types) or at the very least System-U^-. But, we know that these systems are inconsistent and so therefore we would have that in Hask the initial and terminal objects coincide--which is very bad when you are dealing with a topos!
I'm curious whether there is something like this to analogously define Show/Read instances from one applicative definition...
&gt; use the epi-mono decomposition of the erasure map What exactly are you trying to demonstrate in what logic? it seems like you're trying to derive a constructive taboo. but you need to only use constructive logic to do so, and I think that this might sneak in choice already... With regards to "serving as a model for system U" I am confused for a different reason. We've specified we're only dealing with the total, terminating fragment, which is strongly normalizing. But we know system U is not strongly normalizing. So you seem to _really_ be arguing that haskell's impredicative polymorphism leads to inconsistency. But it doesn't, because Haskell's type system is very restricted and stratified. That said, I have no idea what this discussion is about, since nobody has claimed `Hask` is actually a topos, just that it shares many qualities with one. So I'm sure there is some actual reason `Hask` isn't a topos, and we'll probably find it looking at some size issue such that the power object doesn't work right. But that's sort of neither here nor there except in a "challenge accepted" sort of way.
I want to be able to parse and then navigate a Jason structure whose layout is unknown in advance. Predefining a layout is therefore not useful
&gt;What exactly are you trying to demonstrate in what logic? it seems like you're trying to derive a constructive taboo. but you need to only use constructive logic to do so, and I think that this might sneak in choice already... Any topos (indeed, any pre-topos) has an epi-mono factorization system. That means that if a type theory is a topos and can express the both the type of "programs with proof that they terminate" and "programs" then we should be able to construct a type of "programs that terminate" which is equiped with a monomorphism into "programs." If your type theory forms a topos then you have an associated classifying map. This isn't a problem, it is just an example of how the classifier map in some sense detects something which can not be computed, and so if your topos is boolean than you have (internally) non computable functions. The point being that *I* want Hask to have properties which are inconsistent with this. &gt; With regards to "serving as a model for system U" I am confused for a different reason. We've specified we're only dealing with the total, terminating fragment, which is strongly normalizing. But we know system U is not strongly normalizing. Yes. The paradox in system U means that a topos (which is, essentially, lambda-HOL plus some axioms) can't have full impredicative polymorphism unless it collapses. System U^- is, after all, just lambda-HOL where you replace the STLC layer of with System-F. &gt; So you seem to really be arguing that haskell's impredicative polymorphism leads to inconsistency. But it doesn't, because Haskell's type system is very restricted and stratified. Haskell's type system isn't so stratified--newtypes over polymorphic types mean it is essentially fully impredicative. You can easily translate any system F program into a haskell one given at most a linear increase in the size of the typing derivation. Impredicative polymorphism does not break termination on its own, and surely what ever category we use for thinking about Haskell should have it. &gt; That said, I have no idea what this discussion is about, since nobody has claimed Hask is actually a topos, just that it shares many qualities with one. &gt; So I'm sure there is some actual reason Hask isn't a topos, and we'll probably find it looking at some size issue such that the power object doesn't work right. But that's sort of neither here nor there except in a "challenge accepted" sort of way. I was genially curious if there was a way to think of Hask as a topos (is that what you mean by challenge accepted?). But, I think the fact that Hask isn't a topos (and why) is actually somewhat interesting. Beyond that, `Bool` is not a subobject classifier in type theory or haskell (unlike set theory) because we work in a setting where [propositions are not booleans](https://existentialtype.wordpress.com/2011/03/15/boolean-blindness/) and that is a good thing. The way Hask is different from the category of Sets is at least as interesting (to me) as the ways they are similar. 
Hey, I'll definitely have to take a closer look at this sometime hopefully this week, but it may be what I was looking for as a complement to my own json-builder! (Which itself is definitely in need of some work...)
You can use this library to parse a structure into say, an Aeson-like syntax tree, so that's not really an issue...
&gt; I am curious if all Pipes users get asked "Why not use Conduit?" Probably not, because `pipes` users don't seem to feel the urge to publicize they're using them... =)) But nevertheless, I think it's a valid question to ask in order to learn about the relative merits (and maybe `pipes` can steal some of the good parts of conduits it hasn't yet... =), but so far I am quite happy with the `pipes` API which feels much more consistent than `conduit` with its traces of historic cruft ) &gt; I realise pipes are billed as the more "principled" choice for this type of programming, but I've yet to see anywhere Conduit seems fundamentally unprincipled. This can be said about many concepts in Haskell can't it? The question for me is rather what the cost of being more principled with `pipes` is relative to using the "unprincipled" `conduit`... so far the additional cost I notice with `pipes` is that there's a bit less packages with `pipes` support than with `conduit` (and even on the package naming side `pipes` seems to be more principled: the `pipes-` package name prefix is used consistently), so this seems mostly a network-effect issue to me. But network effects haven't kept us from using Haskell in the first place... so... :-)
I think /u/jpnp refers to [this](https://www.fpcomplete.com/user/edwardk/revisiting-matrix-multiplication/part-1).
Sounds like I need to finish and release http://github.com/ocharles/pipes-tar...
What exactly is "plain imperative"? Even the most imperative program usually has parts that are purely functional, e.g. every expression that doesn't use a pre- or post-increment operator.
I don't think Scala will ever gain much more traction than it currently has, mostly because it is a syntactically very messy language that is hard to read even to people familiar with concepts from both ends (OO and functional ones).
The JVM is not a traditional architecture though. It has a lot of assumptions from the high level Java language built into the low level instructions which is the reason most alternative languages on the JVM are such a mess of features drawn from clean designs on the one hand and Java on the other.
&gt; maybe pipes can steal some of the good parts of conduits it hasn't yet... Does pipes have full support for leftovers now? &gt; The question for me is rather what the cost of being more principled with pipes is relative to using the "unprincipled" conduit... You missed Neil's point. This was a gentle way of calling out those who incorrectly label conduit as "unprincipled". Not only is conduit completely principled - it's built on essentially the *same* principles as pipes. There was even talk of potentially merging the two at some point. Last I heard, the only real difference was with regard to leftovers. Conduit provided a full-featured leftover function, with the law that users of this function must ensure that it is called only on a conduit that has already returned data. Pipes did not provide this. The issue lurking behind that difference is that if you use leftovers without following the law, conduit will exhibit behavior that is a violation of the monad laws. That is reasonable, because anyway the monad laws themselves are not checked by the compiler and rely on human verification. So `Conduit` is still a *bona fide* monad in the same sense that the Haskell class `Monad` actually represents monads. It's possible to change the types and APIs of conduit to enforce this law at the type level, but /u/snoyberg determined that this would make it significantly more complex to use conduits in practice, so he decided against it. /u/Tekmo also decided against this (at least in the past, not sure what pipes does now), and instead omitted support for leftovers from the library, with whatever limitations that entails in practice. If you prefer the latter approach, just never use leftovers. You can even create a module which re-exports all of `Conduit` except leftovers - but no need for that, because if I understand correctly, that already exists, and it's called "pipes".
Are you sure that syntax correlates with traction?
Considering the many languages which are wildly different in semantics but similar in their C- (or Algol if you prefer) like syntax and all successful in part because of that I would say it is more important than most people would imagine.
A bit offtopic. Are there an easier way to install it in archlinux instead of using AUR? (don't want to install the 20+ aur packages it needs..)
From the example, it looked like I had to define a data structure representing that JSON to be brought in
Cool, I didn't know your package! One of my goals was to make something that's fast and easy to use without TemplateHaskell and Generics.
I'd rather not copy systematically stuff around as the underlying structures might become quite big and these functions might be called from within loops (the above example represents a single linear solve). However, let's say I write `withA = bracket createA destroyA` (of type `(A -&gt; IO A) -&gt; IO ()`), and in the body of `withA` I do the mutations, allocate memory for the result, copy it there and return it. Does escaping the `bracket` garbage collect for the `A` right away? Were you referring to such a setting?
What laws are broken by leftovers? I am surprised if it's actually the monad laws. I think the answer to this might depend on if leftover support is implicitly baked into the composition operation or is a separate step that you need to perform before composing.
Well this doesn't solve the problem right now, but you could always vote for it on the AUR! Do you want to avoid the dependencies because it's a hassle to install them all or because you don't want to clutter your system? If the former, you could always use pacaur or some other AUR interface. 
You seem to assume that Haskell has no industry traction, and that there can only be one functional language in widespread use - but I believe this is quite untrue, seeing how today languages like C# and Java, or PHP, Python and JavaScript coexist within very similar problem domains, and without either clearly crushing the "competition". Also, while I am convinced that making effects explicit at all times is the way to go, I also believe that not doing it is often a better choice, especially when you have people on your team for whom functional thinking isn't natural yet. It's a tradeoff between theoretical superiority and practical limitations.
I've verified that conduit obeys the monad laws and published those results. I don't know what you're trying to imply about conduit here.
It's a conversation I'm trying not to reopen to be honest. I'll say it briefly here: Conduit has first class support for leftovers, detecting end of steam, and finalizers. It loses finalizer ordering during composition and has no left identity that can't throw away leftovers, but provides alternative composition functions to recapture those leftovers. Nonetheless, these two points do technically constitute a violation of the category laws, which conduit had never claimed to follow. The business here about the monad laws, though, was just completely wrong, which is why I responded to it.
Realistically: how much "action" can one get with a Java library? It's only business programming, not bare-handed bear-murder.
Based on the few samples I have observed myself: it depends. Some people find it much easier to pick up declarative languages like Haskell, Prolog or Erlang. Others find it easier to pick up imperative languages like Java or Python. The difference is striking, really.
&gt; Personally, I would advise against rolling your own date representation because dates can be pretty complicated (leap years, leap seconds, etc, etc) and you surely have better things to do than spend time solving a hard problem that has already been solved. *Have* they been solved in Haskell? The last time I tried to do anything with dates and times I found the Data.Date/Data.Time to have absolute bare minimum functionality compared to things like e.g. Joda Time. 
Functional programming is super easy for total beginners. I taught Lisp to both children and adults with no prior knowledge of programming and it was very easy for them. We used an approach based on this book: http://realmofracket.com/ Now, Haskell is of course far more complex than Lisp. For me, the very sophisticated type system make it more difficult for beginners. Moreover, the type system solves problems they are not yet aware as total beginners. This might confuse them to have to think with types while they still don't understand the value of doing it.
[This post](http://www.haskellforall.com/2014/02/pipes-parse-30-lens-based-parsing.html) summarizes the issues. Basically you have to use a different type of composition in order to guarantee that they won't get dropped. It's not an issue of whether or not the implementation is law-abiding, but more of an issue of whether or not the implementation can drop leftovers or not. The `pipes-parse` library is where leftovers-handling is addressed. This approach leads to some surprisingly cool tricks. For example, you can restrict a parser to a subset of the input while still streaming by using `zoom` (the same `zoom` from the `lens-family-core`/`lens` libraries): -- Restrict a parser to the first 10 elements zoom (splitAt 10) parser -- Restrict a parser to elements that satisfy some predicate zoom (span (&lt; 5)) parser
Where have you published them?
It's ancient, so I don't know if it's still accurate (or even was back then). This is the first thing I found: https://gist.github.com/snoyberg/2827896
Ok. I will check it out as soon as I can have a good internet connection.
The University of Chicago's "Honors Introduction to Computer Science" starts with Haskell.
I second this. And the "all in the browser" experience is also great for starters: http://debug.elm-lang.org/edit/Mario.elm I don't know how hard elm is for those not knowing HTML/CSS. I know elm allows one to do with HTML/CSS, but still it helps me a lot knowing what it compiles to.
I was also the introduction to programming in UoT. But they switched to Java... It pissed Dijstra off to the extend that he wrote an memorable letter: http://chrisdone.com/posts/dijkstra-haskell-java
Hey, what about http://ghc.io ? Safe haskell, all in browser. 
Sure, but then if you import that module into another module you have to import qualified or you could have a name clash again.
I would argue most non-programmer's first programming language is...the spreadsheet, which is actually at a high level very much like a functional programming language. We have all had the experience of starting with data in one column and mapping that data through a series of operations to transformed data. And all the operations are immutable just like a pure functional programming language. When you think of it this way, is it a coincidence that the words Excel and Haskell sound so much alike? (I keed, I keed)
Let's start from the beginning. What do you mean by "How can I improve this, in order to achieve purely-functional signatures"? I thought you wanted something without any IO, IORef or other mutable stuff, but the fact that you mention the type signature `(A -&gt; IO A) -&gt; IO ()` makes me think that you might mean something else.
Interesting!
I always import qualified, regardless. Not knowing where something is coming from without sophisticated dev tools is something I do not like. Just as bad as 'from foo import \*' in Python.
Are you outside the UK by any chance? My experience is UK universities are very functionally focused, where as other countries (particularly in the US) have more focus on C and other low level imperative stuff (with a couple of notable exceptions, like CMU). EE seems like it would go for a lower level approach, as you normally need low level access to ports/code that will work on an embedded system (I guess?). I did CS, but did a reasonable amount of simulation of hardware which was all done in haskell. 
I managed to find most of them, which one were you looking for?
Bird and Wadler don't assume any programming knowledge. 
Technically a value of type `IO a` is a function (wrapped in a newtype). This is of course an implementation detail. See for example [here](http://hackage.haskell.org/package/base-4.8.0.0/docs/src/GHC-IO.html).
You could also look at the Template Haskell functions in `lens`, particularly [`makeFields`](http://hackage.haskell.org/package/lens-4.12.1/docs/Control-Lens-TH.html#v:makeFields). 
Are there any plans to integrate with the hackage-security project?
Wouldn't it instead be a bookshelf with a single hole? 
Derby definitely doesn't and I'm pretty sure neither does Notts Trent. Or Liverpool and Sheffield. Those are the only ones I'm fairly sure of. Most places use C#/Java. The only ones I know of that *do* Haskell as a first choice are Oxford and Uni of Nottingham
&gt; Is Haskell a good choice as a first programming language? One of the problems with this question is that answering depends on what you assumptions are about what should be taught/learned in a first programming course. Some people take a more mathematical view and place the emphasis on logic, functions, algebraic reasoning and will answer "yes". This is indeed what you have in many UK CS courses and in textbooks such as Richard Bird's "Thinking Functionally". This can work well with Haskell (or SML or maybe even Scheme). Others will say that an operational view of computer should be taught first; they will argue for some for of (real or toy) assembly language/C combination. A third alternative is the "object's first" approach, which will start with Java (or maybe something like Python). This is in my opinion the least desirable choice because it combines the distance from the hardware with the lack of any mathematical precision...
Oxford, Cambridge, Imperial, Edinburgh, Glasgow, St Andrews, Warwick, Southampton and Nottingham are the ones I know of. They are a good selection of the top universities for CS, at least. 
Bielefeld University in Germany
I have a proposed new README for the conduit project as a whole at: https://github.com/snoyberg/conduit/tree/include-tutorial#readme I think it gives enough of this information, but suggestions are welcome. /u/MaxGabriel and /u/ndmitchell may be interested as well.
I can't speak for the stack authors but here are the obvious options: * have stack use the hackage-security library so that stack can read hackage format repositories natively, including the security features and automatic use of mirrors. * have the tool that mirrors from the upstream repository (in hackage format) to the stack mirror (in stack repository format) use the hackage-security library to ensure security between hackage and the stack mirror. Currently stack uses a different repository format based on a git repository. The first option would allow stack to read not just the central hackage repo directly, but also other repos that people set up (e.g. using the cli tools available to create secure hackage format repos to serve using plain http file servers).
This is not true. Most experienced programers shoot for correctness and clarity of expression. Optimization follows as it becomes and empirical exercise between your profiler/instrumentation tools and you to optimize accordingly with hard cold facts instead of silly conjectures on how the computer/CPU, memory, primary/secondary caches, network latencies, etc., are going to behave with lines of cold that your can fit into your mind. It's also not true that "premature optimization" is cheap. That's just unadulterated nonsense. It's the reason behind Fowler's article: http://www.martinfowler.com/ieeeSoftware/yetOptimization.pdf
Well, back when I last worked on json-builder (which was a few years ago) my goal was to be able to serialize aeson's json syntax tree as quickly as aeson itself, except in a data structure agnostic way. And in that I was successful, and even improved a few corner cases which made their way back into aeson. Since then, aeson's serialization has seen further work I haven't kept up with, and there's also [BufferBuilder](https://www.reddit.com/r/haskell/comments/2wwbuj/announcing_bufferbuilder_encode_json_25x_faster/) which takes a very similar approach as json-builder, except faster and with not quite as nice of an API. Also, depending on how the review goes, I might be perfectly happy to abandon json-builder in favor of a merger of highjson with a "native" json-builder/bufferbuilder type of interface.
Excellent, reminds me of [Scala Worksheets](https://confluence.jetbrains.com/display/IntelliJIDEA/Working+with+Scala+Worksheet). Recently there has been a lot of excitement over the REPL but modern editor feedback techniques almost entirely obsolete the REPL IMHO. Editors like Intellij give an outstanding amount of feedback with contextual options on error/other concerns, Test Driven Development is great for saving contextual test setup and thus is a structured way to optimize feedback loops - although I do use it sparingly and when appropriate since the technique can lead to large, unruly, hard-to-maintain test suites. I do feel that REPL based feedback is a bit throwback and that editor plugins that give in-line feedback (like the post is showing) absolutely optimize feedback cycles when used with complimentary development techniques like Test Driven Development/[Type Driven Development](https://www.youtube.com/watch?v=045422s6xik).
You can come pretty close [with this](http://www.haskellforall.com/2014/06/spreadsheet-like-programming-in-haskell.html).
[Thesis version of 'Getting a Quick Fix on Comonads'](http://bir.brandeis.edu/handle/10192/30632) [Related Boston Haskell meetup talk video](https://www.youtube.com/watch?v=F7F-BzOB670) 
Premature optimization usually isn't cheap. What I said was "**these kinds of** premature optimization are cheap": struct Date { uint8_t month : 4; ... }; The 4-bit size that makes the Date struct smaller is a "premature optimization" (if we haven't profiled it), but it is cheap. One could say it *aids* in clarity of expression, because it makes it clear month fits in 4 bits (it can be even clearer if we make the type even more precise). 
* Guilt-Free Ivory: https://github.com/GaloisInc/ivory/tree/master/ivory-paper * Comonads: Kenny's thesis version: http://bir.brandeis.edu/bitstream/handle/10192/30632/FonerThesis2015.pdf?sequence=3 So... Iavor, Trevor, Garrett, Oleg, and Kenny. I sit next to three of them and can toss a rock at a forth. EDIT: * Iavor's is on github: https://github.com/yav/type-nat-solver/tree/master/docs * /u/kwef (Kenny) put his on github https://github.com/kwf/GQFC * Trevor indicated he will post his somewhere, but the raw latex is already available (see above)
 &gt; let f = (&gt;4) :: Int -&gt; Bool &gt; let g = (&lt;10) :: Int -&gt; Bool &gt; import Control.Applicative &gt; let fAndG = liftA2 (&amp;&amp;) f g &gt; filter fAndG [0..100] [5,6,7,8,9]
and them they ask what is wrong with 10 tools doing the same thing‚Ä¶
Just finished first year at Uni of Nottingham, we did C, Haskell and Java. For anyone interested I had Graham Hutton as my course leader and he was a brilliant teacher and the course was based on his book, which I would thoroughly recommend to anyone who wants to get started with Haskell.
+1 for Graham Hutton, albeit I took his course in 2000 :-)
Haskell has less traction than F# or Scala in sheer numbers, but the quality and stability of the community are superb. I wouldn't exactly consider those languages "alternatives" or "competition" either, they're not exactly competing for the same niche - their use cases are "reasonably close to typed functional programming while being able to run on JVM/.NET with minimal friction", whereas Haskell's use case is "typed functional programming without compromise". The latter is certainly more niche still, and probably will be for the foreseeable future, but it has a certain aesthetic to it, and some really interesting benefits for some use cases. Haskell is not going to get big IMO, and it wouldn't be a good thing if it did; the unofficial motto "Avoid success at all cost" is, I believe, spot-on. If Haskell wanted to become more mainstream, concessions would be required, and that would require loosening some of the principles that make Haskell what it is. Haskell has not one string-like data type, but *at least* 5 of them; that's confusing for beginners, but it is the right thing to do, and reducing these types to one or two would make Haskell weaker in some of its strongest areas. And pretty much everything about Haskell that makes it less palatable for people coming from an imperative background and needing results sooner rather than later, is like that. And for exactly the same reasons, Scala isn't going to take Haskell's niche either: it will never be the "no concessions made" functional-programming language that Haskell can be. Bringing Haskell to the JVM; well, that could work, but there are two obstacles here that make this unlikely. The first one is that the number of people skilled enough to pull something like this off is very small, and they're all tied up in more pressing needs. And the second one is that the JVM itself, despite all its impressive merits, is somewhat at odds with the Unix philosophy, making it kind of a hot iron in the Haskell community. But then again, Haskell doesn't need to become mainstream; it is already quite successful in terms of being influential and attracting a good user base. It acts as a breeding ground for cutting-edge programming concepts, and as a safe harbor for those seeking sanity away from the trenches of day-to-day enterprise programming.
Use the Applicative instance of (a -&gt;)! liftA2 (||) f g = \x -&gt; f x || g x For longer argument chains you can use `Traversable`, fmap or (sequence [f,g,h]) = \x -&gt; f x || g x || h x
Mainly because it is a hassle to keep that many packages up to date through AUR (even when using a AUR manager). Maybe if it could be added to its own repository like the [haskell-core] it would make it easier. 
Yeah I've met some of the Nottingham staff/students at various user groups, they all seem ridiculous smart. Bastards. He wrote Programming in Haskell too
&gt; Welcome to GHC.IO! &gt; **Prelude&gt;** import Diagrams.Prelude &gt; *Could not find module `Diagrams.Prelude'* &gt; *It is not a module in the current program, or in any known package.* ...and in the other one, I get to make Mario jump.
The Glorious Glasgow Haskell Compilation System, version 7.10.1
As others have mentioned, the `Applicative` instance for `a -&gt;` can be used for this. I sometimes define a lifted version in a `where`: f = x ^||^ y where (^||^) = liftA2 (||)
&gt; [This](https://superginbaby.wordpress.com/2014/11/18/learning-haskell-as-a-nonprogrammer/) is an interesting testimonial. Note also that she went on to teach Haskell to her 10-year-old, and is now the co-author of [this book](http://haskellbook.com/) (the other author being Chris Allen of [bitemyapp](https://github.com/bitemyapp/learnhaskell) fame).
It seems to have something to do with the fact that `[0..9]` has the type `Num a =&gt; [a]`. If I use a concrete type, I don't get this behavior: &gt; let xs = [1 .. 9] :: [Int] &gt; length xs 9 &gt; :sprint xs xs = [1,2,3,4,5,6,7,8,9] I have some intuition for why this might happen, but don't know enough about the GHCI internals to be sure.
yeah, you have a point. So what's missing to making a live in-browser game dev bench? Editor, REPL to the left, game on the right. One of those reactive thingies from the FRP Zoo, would it render e.g. to javascript?
Yes, my bad. Corrected!
St Andrews doesn't start with Haskell, but there is a module for 3rd years which introduces Haskell and Python.
I usually do or . sequence [f, g, h] but I don't see anything too bad with \x -&gt; any ($ x) [f, g, h] if you want to use Alternative (or MonadPlus) just for funsies, you can do isJust . asum . mapM mfilter [f, g, h] . Just
I want to believe you are right. Instead, one that first learns imperative langs must be de-wired to think functionally. Still I think that introducing in order GHCi through :t , lambda expressions and partial evaluation can build up a solid base and in few pages to a total beginner. I am writing a 20-page tutorial that includes this, plus pattern matching and recursive functions, datatypes as functions, pure vs IO, and Monad as "datatype of chainable computations". No pictures but the beginning is written as a GHCi type-along thing, the bare minimum of formal definitions, program reductions etc., just tons of REPL and a half-page "database" program to showcase most of the stuff mentioned before. Wrote it in .lhs, but it would be cool to turn it into an iHaskell notebook or even better a "live" format for the browser.
Paging /u/ryantrinkle :)
The Remote Monad paper looks exemplary. It's very welcome to see a paper analyzing existing code for patterns in the Related Work section. 
Added.
Atom is ungodly slow and resource intensive. If you're used to the speed and responsiveness of vim, you'll likely be disappointed
Good comments, agreed on all fronts. I'll try to get to that tonight or tomorrow and then merge it in to master.
&gt; I recently switched to the Atom text editor Did you try Leksah? Is there anything we could fix or add that would make it an option for you? &gt; started using the Hydrogen package for evaluating code in-line in my editor For something similar in Leksah press Ctrl + Enter. * The result goes to Panes -&gt; Log and Panes -&gt; Output. * In the Output pane the result is rendered as HTML (using pretty-show and WebKitGTK). * If the result is already HTML it will be displayed that way (right click in output pane and select "Always HTML" if HTML detection fails). * Replays the last expression when you modify the code in your package (you can evaluate `main`, then change your code and watch the results in the output pane). * There is a scratch pad in Panes -&gt; Debug that you can use to write expressions without modifying your code. * Supports Haddock embedded code (strips leading `-- &gt;`). * Supports doctest (also strips leading `-- &gt;&gt;&gt;`). * Works with ghci commands as well (except for `:set prompt`).
Yes, sorry, let me rephrase. Now I'm returning in IO just to test the orchestration of groups of calls, but I'd like datatypes that encapsulate my representation of pointers (`newtype A = A (Ptr A) deriving Storable` with GNT on), and carry around other properties as well, such as the fact that data has changed across a function, the possibility of logging (which here would imply making copies of MPI-distributed arrays, potentially very large) along with some algebraic properties of the operands, for the pure subset of FFI calls. I guess this should be a stack of monad transformers?
&gt; On the other hand, as a Vim user I've found that they have Vim bindings, which looks promising. Can you list the vim bindings you need from most important to least important? I had hoped Leksah would inherit Yi and CodeMirror's vim support, but it might be a good idea to add something to GtkSourceView for now (since both the Yi and CodeMirror integration work is stalled).
A bit late to the show, but here's mine: [D√©j√† Fu: A Concurrency Testing Library for Haskell](http://www.barrucadu.co.uk/publications/dejafu-hs15.pdf)
I haven't had a single problem with it's speed, but then again, starting the editor isn't really a bottleneck in my development process.
&gt; Did you try Leksah? Is there anything we could fix or add that would make it an option for you? I haven't really tried Leksah much. I work in multiple languages, so I much prefer an editor that's general purpose, though I think Leksah has a noble goal :) Sounds like Leksah has some great features!
I know I've used language integration for other languages where you have a command to automatically send the selected / current expression to a REPL running in another frame. I bet you can set up something like that for Haskell.
My understanding is that the performance of Atom has changed drastically in recent releases. I haven't noticed any performance problems in the couple months I've been working on a medium-large Python codebase with it. Switching files is a breeze, no problems with syntax highlighting or editing text.
Not yet.
No I don't have to do `Ptr` operations anymore. I used `with` , `withPtr` etc throughout and the types are mapped with an inline-c Context, so now I am passing newtypes around. I'm afraid I can't do without the innermost IO layer, but surely I'm missing something here. Could you give an example please?
Thanks for the poke, /u/tom-md! Here's the camera-ready submitted version of my paper, with literate source code ready for anyone to "play along" with the paper text: https://github.com/kwf/GQFC.
As a vim user, I have so many shortcuts and habits that I can't even list how many there are. Vim editing is a language - asking for the top used shortcuts is like asking for your list of top used Python functions. The real way to get good vim support would be to embed NeoVim. I'm not sure if the project is mature enough for that, but if any IDE implemented that, I'd switch to it from vanilla vim immediately.
I was very excited about LightTable when it came out. I think Atom pretty much killed it (though there is a little bit of activity still). Chris Granger moved his company onto a more ambitious project and I think they only have one person working on LT part-time. I don't have a lot of experience using Atom with Haskell yet (in fact I don't have a lot of experience with Haskell period). I did get it set up using ghc-mod and showing me type info, using GHC 7.10 and ghc-mod master. That was a bit of a pain, but then ghc-mod in general has been a pain with 7.10. I don't find it very difficult to jump between editors. I "grew up" on Emacs, and have also used Sublime Text for a significant period of time, but Atom has pretty much obsoleted it as far as I'm concerned (especially since I'm much happier using an open source editor than a proprietary one). My point is I don't know if I'm a very good person to tell you if it's worth it to move from vim to atom. But it's super easy to install and customize, so it's worth playing around with. Other than Haskell, I do use Atom and Hydrogen "in production", and have for the last couple of months. It's been quite solid. It's crashed on me less (read: not at all) than Emacs on OS X has.
Same here, and I'm logged in to LinkedIn.
And even now, with 10+ years experience, I've been prototyping algorithms in Excel because it gives me convenient access to visualisation tools.
was going to try leksah but I wasn't able to install it through neither of cabal, yaourt (the arch package manager) or nix. - `yaourt -S leksah` fails due invalid dependencies already reported on the AUR - `cabal install -j leksah` prints out this http://pastebin.com/f2RW6MhW - `nix-env -i leksah` ends with the error: http://pastebin.com/1HfMj0rS i have no deep knowledge of any of those package managers so I can't fix the problems myself.
Does anyone actually find these applicative/traversable versions easier to work with? They look hideous to me. FWIW, in [SubHask](http://github.com/mikeizbicki/subhask), the `(||)` and `(&amp;&amp;)` operators are overoaded to work on any `Lattice`. This lets you write things like `f || (g &amp;&amp; h)` directly, with no syntactic overhead.
Congrats! Functional Pearls are a rough genre of papers to pull off, and getting one accepted is no mean feat. I'm not sure when I'll first get to pull this out and use it "in anger" but I'm looking forward to finding out.
That is no longer true. I work on a mid-size code base and everything is instant. It's as fast as Sublime now. 
I'm not sure what you are asking, really. Are you looking for an example of mutation?
The applicative style I don‚Äôt mind [edit: even when it comes to the arrow instance], the `liftA*` HOFs I like a little bit less. As for `Traversable` I don‚Äôt like the loss of static information when turning a finite number of things into a sequence of 0 or more‚Äîesp. when that sequence is immediately consumed anyway. And of course lists-as-tuples breaks down for actual tuples, i.e. when the elements are not necessarily homogeneous. You can ((!!) &lt;$&gt; const "Hello" &lt;*&gt; const 0) () but you can‚Äôt (fmap (!!) $ sequence [const "Hello", const 0]) ()
Oh, I'm sorry, I misunderstood what you meant. I haven't slept all day, I need my coffee.
I think the list will significantly differ depending on a person. For example, for me scrolling shortcuts are not that important, but for somebody with better scrolling habits they might be essential. [That being said, this post on Stackoverflow seems to do a good job at demonstrating basic Vim keyboard shortcuts](http://stackoverflow.com/a/5400978/2290598). As you can see, there is a lot of them. Add to that what /u/NiftyIon said about Vim being like a language. You can combine those shortcuts in many ways like: "cw" = change until the end of the word; "ciw" = change the whole word under the cursor; "ci(" change everything inside brackets; "c2w" = change next two words; 2w = "move cursor two words right". Combinations are endless. In other words, it might take some work to implement it. So, before adding common Vim bindings to Leksah, perhaps you should try to investigate how many Vim users are currently not switching to Leksah because there are no Vim bindings there. If I were to name one unintuitively important binding, then one of the most annoying things for me would be the lack of commands like ":wq" or a scenario where ":q" does not actually close the current tab / window or closes the whole editor instead of only the currently focused tab.
Maybe if I could go back and unlearn programming, I could finally master haskell.
Here is a way to examine if "T being of fixed size" is necessary -- just consider the case of a potentially infinite list, equipped with the ZipList applicative and the obvious traversable. Now consider if that works as a transformation for all applicative F. I sort of suspect that it will work out ok, but I haven't checked? (also note that fixed size traversable = representable, and in a sense you may be leveraging "distribute" as much as "sequence"...)
But you can't encode the sigma type "a program and a finite execution trace of that program to a normal form" as a type in Haskell as far as I know? (Certainly not in Haskell '98, which is all I think we are considering here..) If we were in Coq it would of course be a different story, and this is why something like `Prop` is a much better notion of a truth object there.
probably not in Haskell 98, but you certainly can in any version of Haskell with GADTs and you might be able to do it in any version of Haskell which has polymorphic containers (using Oleg style tricks for faking GADTs). The idea being that you have a type level representation of terms and encode the reduction relation and the notion of normal forms as GADTs indexed by that type level representation (you probably also want a singleton). Then you wrap the entire thing up in an existential. It won't be pretty, but I'm sure you could do it. Edit: it actually only took a few minutes with GADTs {-# LANGUAGE GADTs #-} data K = K data S = S data Step a b where StepK :: Step ((K,a),b) a StepS :: Step (((S,x),y),x) ((x,z),(y,z)) StepL :: Step a a' -&gt; Step (a,b) (a',b) StepR :: Step b b' -&gt; Step (a,b) (a,b') data Done a where DoneS :: Done S DoneK :: Done K DoneS' :: Done x -&gt; Done (S,x) DoneK' :: Done x -&gt; Done (K,x) DoneS'' :: Done x -&gt; Done y -&gt; Done ((S,x),y) data ReductionTrace a where DoneTrace :: Done a -&gt; ReductionTrace a StepTrace :: Step a b -&gt; ReductionTrace b -&gt; ReductionTrace a data Comb a where CombS :: Comb S CombK :: Comb K CombP :: Comb a -&gt; Comb b -&gt; Comb (a,b) data NormalizingSKTerm where NormalizingSKTerm :: Comb a -&gt; ReductionTrace a -&gt; NormalizingSKTerm I'm pretty sure you can encode the entire thing into Haskell98 + RankNTypes or even Haskell + PolymorphicContainers by simply encoding GADTs into ordinary Haskell datatypes where the existential is implemented System-F style and you use `forall f. f a -&gt; f b` as your encoding of the type of equalities between `a` and `b`.
Hm, well, my understanding from remarks made [here](http://r6.ca/blog/20121209T182914Z.html) and elsewhere was that any traversable `T` is equivalent to a so-called "finitary container". That is, `T` is equivalent to a functor of the form X ‚Ü¶ Œ£ (a : A) (X^(c(a))) where `c : A ‚Üí Nat`. The functor for possibly infinite lists is not of this form (and so is not traversable), though it strikes me I can't think of a very simple proof. Here is one though (I think it should work). Take some kind of typical set theoretic model. I'm cavalier about impredicativity so I don't know if this actually works exactly. The cardinality of `Œ£ (a : A) (1^(c(a)))` is the cardinality of `A`. The cardinality of `L(1)` is aleph0. Thus, if the two functors are equivalent, `A` has cardinality aleph0. But then `Œ£ (a : A) (2^(c(a)))` is still countable while `L(2)` has cardinality continuum. It's a shame I can't think of a more "type theoretic" proof, whatever that means. I suppose an easier way to see that it's not traversable is to observe that `sequence (repeat (\_ -&gt; do {x &amp;lt;- get; put (x + 1)}))` doesn't terminate (and I suppose `sequence` is supposed to be total). P.S. Is there some way to write in TeX here?
Yeah, basically. They are able to sort movies by rotten-tomatoes rating, IMDB rating, etc..
why?
Yeah I was pleasantly surprised when I found this out about a month ago too. https://www.reddit.com/r/haskell/comments/3am3qu/should_i_put_a_powered_by_haskell_tag_w_haskell/?ref=share&amp;ref_source=link
One thing that comes to mind given the definition of [`??`](https://hackage.haskell.org/package/lens/docs/Control-Lens-Lens.html#v:-63--63-): (??) :: Functor f =&gt; f (a -&gt; b) -&gt; a -&gt; f b fab ?? a = fmap ($ a) fab is that l√∂b ffs = fix (\fa -&gt; fmap ($ fa) ffs) can be written as l√∂b ffs = fix (\fa -&gt; ffs ?? fa) l√∂b ffs = fix (ffs ??) l√∂b = fix . (??) The paper talks about `l√∂b = fix . flip` when instantiated with the reader functor in which case `(??) = flip` so it's not too surprising. **Edit:** It can help to define a right-associative version of `(??)` to see how a spreadsheet gets evaluated: ghci&gt; import Data.Function (fix) ghci&gt; let infixr ???; fab ??? a = fmap ($ a) fab ghci&gt; let l√∂b = fix . (???) ghci&gt; let spreadsheet = [length, (!!0), \x -&gt; x!!0 + x!!1] ghci&gt; spreadsheet ??? error "‚ä•" [*** Exception: ‚ä• ghci&gt; spreadsheet ??? spreadsheet ??? error "‚ä•" [3,*** Exception: ‚ä• ghci&gt; spreadsheet ??? spreadsheet ??? spreadsheet ??? error "‚ä•" [3,3,*** Exception: ‚ä• ghci&gt; spreadsheet ??? spreadsheet ??? spreadsheet ??? spreadsheet ??? error "‚ä•" [3,3,6] and `spreadsheet ??? (spreadsheet ??? (spreadsheet ??? ...))` is exactly what `fix (spreadsheet ???)` is.
It is familiar, but not in the way that you paint it. I believe that Haskell is much better than Java, but not that it is just better. There are things in the JVM that I wish I had in the GHC runtime. And when you need these things, you can't use Haskell. When I need a streaming library with a resource finalization story, I will use conduit. I admit as a user I don't really know *why* conduit is better than pipe in that aspect, but this seems to be something that is agreed upon by both authors. So when I am being told I should just use pipe instead, without even considering what my use case could be, I file the person in the Dunning‚ÄìKruger folder.
Sounds good! Looking forward to your review.
The problem is: They are competing. On funding and on brains. The amount of money put into research is limited. And the people capable of language development on that level even more so. So it would be nice if we could transform "compete" into "complement". Regarding the JVM backend: Purescript seems to work on this. Let's see if they can optimize lazy evaluation for the JVM. If they can, this will be huge leap. Regarding the Unix principle of doing one thing well: Some of my most favorite projects oppose this principle: The Linux kernel, the Emacs (although I am more a vi user...), C++, the KDE desktop, IntelliJ, Excel. It's a nice principle but it has it's limitations. I think it should be used with caution. And regarding the "Enterprise programmers": Most of them aren't using Scala either. I think the communities for Scala, Haskell, Clojure and Purescript are quite similar in their behavior and their goals. It would be great if they could cooperate even more and inspire each other with their ideas.
Thread there also answers OP's question about the stack: &gt; The site is built with Scotty, postgresql-simple, blaze-html
thanks!
Interesting. Through my mentoring in GSOC I am being exposed to the first step in your process, and it is causing me to rethink the way I do software. I am accustomed, due to commercial time pressures, to get something going as quickly as possible and then move on. I am now thinking that taking the time to do it right at each step can save time in the long run, through having a simpler, smaller, more understandable code base.
Thank you for the example, this is indeed the case. It's working perfectly in my case, `test` above runs correctly, but I find the possibility of mutable data a bit scary per se. I was "just" asking about a type arrangement to 1. represent the mutabilty (a sort of StateT ?) and maybe 2. hide IO. 
I'd be surprised, as that would violate the NIH principle... üòà
That actually looks pretty good. I was avoiding including ``lens`` at first but I have since decided to use it, and it looks like it provides a simple records' solution, too. Thanks. :-)
I study cs in Sweden, we learn haskell as a first language. I think a big reason for this though is that it is sort of unknown, so even students who have programmed a lot previously is new to it.
I use x@(y:ys).
I'm the creator of http://instantwatcher.com. Yes, I moved it over from Rails to Haskell in the last two months. The stack is Scotty for the web routing, postgresql-simple for database lookups, and blaze-html for templating. I'll try to figure out why Wappalyzer is giving a false negative. I put the "Made with Haskell" notice on the bottom to help raise Haskell's profile as a web application programming language. Thank you for noticing. The new UI is in flux, so your feedback on that is welcome. 
Thanks a lot! I cannot manage to have it working, anyway. When i go to your blog, i do not see any button pointing to a feed. In these cases usually RSS metadata are embedded in the post list, so i try to add http://parametricity.com/ to my RSS reader (i am using https://github.com/swanson/stringer which is ridiculously easy to deploy), but i get an error. It could be just a problem of caches, i do not want to take too much of your time about this. Thanks anyway!
I'd love to try leksah, but every time I've tried to compile it (every couple of months with whatever latest ghc) it's failed. I believe I've seen errors in both dependencies and in leksah on separate occasions, though it's been awhile so I can't be sure. If a Linux binary is feasible to distribute it'd be very helpful. 
Could you please briefly elaborate on the migration experience? Was it painful? One shot or gradual? Is there any ruby left now? What about performance, code size, etc? I think I recall that there was a post about it. Has anything changed your view? 
Personally I go with the vanilla product type when my application is pretty small and 3/4 fields are enough; and switch to records once it get's larger; at which point you're also more likely to bring in the lens package.
Who invented what where, and when? And if we can get an answer to "why," that will cover all 5 Ws :-D
&gt; I also pull in a light weight lens library (like lens-family) from the get go I'll do the same, except I'll just pull lens.
&gt;how often do you use newtype in your code? For this purpose: I think... not often enough ;) I like the idea of extra safety, but dislike the syntactic noise caused by the newtype (un-)wrapping.
Yes, IORef seems to be the idea I was looking for. I will look in this direction then; thank you so much !! Regarding your question, this approach doesn't distinguish between operations like writeIORef which do perform some mutation and operations like readIORef which merely access the mutable part. Do you need to distinguish those two cases? yes, I'd rather distinguish the pure subset of functions ("getters") from the mutating ones. How would this be possible? 
I'm curious whether HOL-light is different in this regard, but the style of proofs I see in Agda and Idris makes maintenance really hard. Let's say you have a function that returns a list. Now you make a change, and for the change to be sound it is required that the returned list be non-empty. It turns out that for the arguments that you pass to that function, its result indeed will always be non-empty, but you have to prove that. In "normal" mathematics, you often prove things by analyzing an object, in this case the function. The proof is detached from the object itself. Remember how in school you defined the Euclid algorithm and then proved, by analysis of the algorithm you already defined, that it terminates and computes the right thing. But in Agda/Idris-style proofs, it seems hard (or at least uncommon) to do such proofs. Instead, the function is expected to assemble all the proofs about itself. Which is what makes these proofs hard to compose and maintain. If the author of the function, at the time of writing it, didn't envision some specific property that the function satisfies, you're out of luck.
I too, play DayZ
The records (clunky as they are). Type declarations exist for documentation purposes, for when you want to express that a certain sort of data is used for some particular purpose. This assumption does not hold for getters in records. Take this: data Person = Person {name :: String, spouse :: String} Both `name` and `spouse` refer to names of people. Using the 2nd strategy here would be misleading: data Person = Person Name Spouse type Name = String type Spouse = Spouse This arrangement suggests that `Spouse` is somehow different from `Name`; that it fulfills some different role, whereas, in fact, both fulfill exactly the same role: identifying people. Using `Person` in conjunction with functions also becomes awkward: getFirstName :: Name -&gt; Name At first glance, this looks as if you couldn't pass in a `Spouse` into this function. With ordinary record, you at least know what you're getting: getFirstName :: Sting -&gt; String &gt;&gt;&gt; let p = Person {name = "Mars Winklebottom", spouse = "Snickers Winklebottom"} &gt;&gt;&gt; getFirstName (spouse p) "Snickers"
Hm. How about that.
Eli5?
At that point you could just newtype it.
It's a reference to category theory, a branch of maths that has a lot of applications to type theory. Category theory uses a lot of graphs and diagrams in its illustrations, and such illustrative diagrams are said to "commute" if multiple paths between the same source and destination have the same effect or functionality.
In Idris there're [Provisional Definitions](http://docs.idris-lang.org/en/latest/tutorial/provisional.html) to separate algorithms and proofs. 
The appearance of `stack-ghci` raises a question -- since this already causes downstream toolchain fracturing -- how well were alternative options researched? Why am I asking this -- because I'm a bit scared how people will now have to choose between hacking on `stack-ghci` and `ghci-ng`, for example.
I see what you did there...
Yes, we're all in the same bind...
These puns are pure gold. Maybe I should just return to my desk? &lt;walks away slowly&gt;
Are you having troubling extracting value from this?
Let us join our forces and return stronger.
I don't get it. I think your joke might be out of context...
Three Monads are hanging out with their friend, Jack. They're all pretty bored. "I know!", says Jack. "Let's play a counting game. We'll go one by one, and whoever says the highest number wins. I'll judge." They start counting. "const 1", "Just 2", "print 3". "I'll pass, I can see where this is going," says the first Monad. Maybe and IO keep playing for a while, "print 91", "Just 92". Then IO gets an evil gleam in his eye. "Hey Maybe," he says, "what do I have in my hand here?" Maybe looks at the empty hand, puzzled. "Nothing?" IO smiles. "print 93". Maybe realizes what's happened. "Nothing," he grumbles. "All right," says Jack. "Game's over." IO grins. "The winner is Cont, with the high number of 1!"
You can see how this is done in the Coq standard library: the function div_eucl is defined in [BinNatDef.v](https://github.com/coq/coq/blob/trunk/theories/NArith/BinNatDef.v) by giving just the computational part, then there are various proofs about it in [BinNat.v](https://github.com/coq/coq/blob/trunk/theories/NArith/BinNat.v). The underlying logic of Coq and Agda are basically the same, so you could write the same proofs in Agda too. I think the main reason that people don't is that Coq has tactics to generate proof terms for inductive proofs, while in Agda you would have to write out the proof term (a recursive function) by hand. The proof then looks quite similar to the function definition (they are both recursive functions), so it's shorter to combine them into a single object. The main exception is that Coq and Agda don't provide a very nice way to first define a function and then prove after the fact that it terminates. (Although there are various approaches to doing so, e.g. "Bove-Capretta predicates". Probably this could be made more smooth).
Beginners usually want to extract a from IO a, which they can't do, but don't need to.
&gt; GHC 7.10.2 should be the first release which works out-of-the-box with GHCJS does this mean I don't need to (fail to) build ghcjs in a vm anymore? 
&gt; Haddock comments for GADT constructors yay
I'd look at [servant](http://haskell-servant.github.io/tutorial/api-type.html) for "type level sums". also Free monads, which should be simpler.
not yet. http://stackoverflow.com/questions/4808702/is-there-any-kind-of-statistical-natural-language-processing-library-for-haskell http://stackoverflow.com/questions/11428601/nlp-parser-in-haskell However, there is a lot of functionality in different places: https://hackage.haskell.org/packages/#cat:Natural%20Language%20Processing
I too, have myopic quarterly deadlines.
Thanks for the sources! I guess I can stick with NLTK for now, but I'll keep an eye out on this. Do you have any NLP background, or were you able to find all this via Google? 
a homomorphism has to take 1_F to 1_G - otherwise it is no homomorphism. https://en.wikipedia.org/wiki/Monoid#Monoid_homomorphisms Better explained (more general) in the german Wiki: https://de.wikipedia.org/wiki/Homomorphismus (if you can read the math before "Beispiele"). 
Well, as far as we're concerned I think, it means that GHCJS doesn't need any patches to GHC or Cabal anymore for a custom build - so yes, I imagine this will simplify their end-user build process a lot. (And you can also always join the NixOS side where you don't need a VM for such a thing. :)
I get the diagram joke, but not this one. :&lt;
two years, four bots and two extracted libraries later, it's finally ready for hackage! this is the library that powers /u/intolerable-bot and runs all of the automation on /r/dota2, it's a little clunky but it supports most of the stuff you'd want a reddit bot to do there's a [readme on the github page](https://github.com/intolerable/reddit) with a couple of examples, and i'm planning on getting a full tutorial out soon-ish enjoy
Been using this (via GitHub) for a little while now. It's a very straightforward library; as a relative Haskell newbie I found it easy to just pick up and use, particularly with the help of sample code via [`intolerable-bot`](https://github.com/intolerable/intolerable-bot). Great work, happy to see it on Hackage!
thanks for pushing me to get it onto hackage, it was a bit of a waste just stagnating on github honestly just need to update the bot now for some of the recent changes...
Are there any small examples included with the API? I couldn't find any.
other than the ones on github? not at the moment i'm planning on adding some to the hackage docs once i've written a tutorial
How does the cont monad come into play? I get the IO and Maybe part
Lol pretty much any random ruby or Javascript program will have a few of these errors lurking somewhere 
Technically `Cont` should have said "const 1" rather than "1", but that might have spoiled the punchline. `Cont r a` has type `(a -&gt; r) -&gt; r`. This can be interpreted as: it takes a single argument, which is the "future" of the computation. Then, it provides a result for the computation as a whole. `Cont` can call its argument to produce the result, in which case the monadic computation continues normally, but it can also just ignore the argument and give a final result for the computation. In this case nothing that comes afterwards matters. Edit: I've updated the original post to use "const 1", since it probably won't actually ruin the joke for almost anyone.
Same here in three weeks... 
&gt; If I was to rewrite this using Pipes/Conduit I would not be able to make a Consumer/Sink that turned the streaming data from the file into a lazy list and then pass that lazy list to render because that would sort of defeat the purpose of using Pipes/Conduit instead of lazy IO, correct? Correct &gt; Instead render would need to take a Producer/Source instead of a lazy list of data. Or I could write render as a Consumer/Sink? If you were to switch to `pipes`, the equivalent type would probably be: render :: Monad m =&gt; Pipe Data Color m r I'd have to look at the `render` function to say for sure.
For me the problem has never been the concept of FP, but how to apply it: How do I parse command line arguments? Or actually get anything done? Most examples are like one short introductory paragraph and then you get [advanced|obfuscated] monad operations without further explanation.
It's a pretty decent read. I'd also suggest the prolog-based one: Representation and Inference for Natural Language, if you're into computational semantics.
Or, apparently, someone who's had 4 glasses of wine. Bookmarked for ~~later~~ tomorrow.
Really interesting and mostly-accessible article! Thanks for sharing.
canvas is garbage. WebGL if you need acceleration, but otherwise should work with just the DOM+SVG as you say. PureScript is what you should look into, IMHO. It's very nice.
Gonna have to do some updates myself now! My bot is hosted on Heroku, for which Haskell support depends on Halcyon, and that won't pull sources from GitHub -- so I've just had the library duplicated in my source tree, and haven't updated it since some time before [this commit](https://github.com/intolerable/reddit/commit/0334d18bfb2629a97350e4499c930628b4a2d519). :P Being able to get it from Hackage now means I can clean that whole situation up, which is a great excuse to make several other improvements I've had on my mind, so yay! ~~That does remind me, tho -- one of the things I tried to do initially was have it forward any PMs it received to me, and IIRC I ran into two issues with that. I think one was the missing `ToQuery` instance for `Message`, fixed in the above link, which was preventing me even attempting to send a message from the bot (again, IIRC -- I may not on this). The other problem I'm more sure of, though, because it involved code that actually compiled and ran; I was able to check unread messages, but couldn't make it mark them as read, which would complicate things considerably. Does that sound familiar?~~ **Edit:** Everything works!
Ah good then
I just decided to learn Haskell today. Is learning Haskell as someone with a comp sci degree, but completely incapable of writing a proof going to be a pointless endeavour? I'm not worried that I can't do it. I'm worried that the community will see me as borderline useless and not want to work with me. I had put offs learning Haskell until today for just that reason. 
I am very unknowledgable in this area. What is wrong with canvas?
Shouldn't a `spouse` be a `Person`? type Name = String data Person = Person { name :: Name, spouse :: Maybe Person } Edit: then, you have: firstName :: Person -&gt; Name firstName p = case words (name p) of [] -&gt; "" (x:_) -&gt; x And: &gt;&gt;&gt; let mars = Person "Mars Winklebottom" (Just snickers); snickers = Person "Snickers Winklebottom" (Just mars) &gt;&gt;&gt; firstName mars "Mars" &gt;&gt;&gt; fmap firstName (spouse mars) Just "Snickers" &gt;&gt;&gt; 
Does anyone know how to make this work with Spacemacs? I keep getting this is 'not a function': :error-filter (lambda (errors) (flycheck-sanitize-errors (flycheck-dedent-error-messages errors)))
In my (possibly biased) opinion, the file size issues with GHCJS are often overstated. When you do get large file sizes it is normally because you are using a large amount of Haskell code (haskel-src-exts for instance). The ghcjs rts includes some code that has been unrolled making it larger than necessary, but this is to make the code you write run faster. ghcjs-dom has [lots of bindings](https://github.com/ghcjs/ghcjs-dom/tree/master/src/GHCJS/DOM/JSFFI/Generated) (including things like Canvas, SVG and WebGL). BTW ghcjs-dom has almost no size overhead as ghcjs inlines calls to it very well and what you get out winds up looking almost hand written code.
I have no idea where you got that idea from. Haskell is elegant and pure, which makes it nice for proofs, but is not it's purpose. Haskell is not Coq or Agda, by any means. We'd love to have you as a member of the community, writing proofs is not even popular among us! (I myself have written more proofs in Java than Haskell)
Alright, I tried installing from source following the instructions on the README on [GitHub](https://github.com/leksah/leksah). The compilation failed, here's the logs. http://pastebin.com/UkstMagC Everything seemed to be going fine until around line 405 when gtksourceview3-0.13.1.5 failed to build. While this log is of an attempt to install leksah inside a sandbox, I did attempt the install process outside of a sandbox and got the same errors. Usually the error messages from cabal make at least a little sense, but I'm not sure what this one is saying. 
Without (&gt;&gt;=) you can't pick "what to do next based on what came before". Here, getLine &gt;&gt;= putStrLn is a good starting point for something you can't say without more than an `Applicative`.
Sorry for the scattered response, but there's a handful of relevant bits and pieces I think might be worth mentioning. I think you only get the correspondence to S and K with the `-&gt;` applicative, rather than for applicative functors in general. Applicatives are less powerful than monads because the components connected by `&lt;*&gt;` can't depend on each other. How would you use applicative IO to prompt a user for their name, read their name in, and the print `"Hello, " ++ name`? If you can crack the above example with an IO Applicative then I've clearly severely misunderstood something :) Also, you should be able to use monads for anything you're using applicative functors for (since every monad is also an applicative functor). From Control.Monad: ap :: (Monad m) =&gt; m (a -&gt; b) -&gt; m a -&gt; m b ap m1 m2 = do { x1 &lt;- m1; x2 &lt;- m2; return (x1 x2) } which is the monadic equivalent of `&lt;*&gt;`, and `return` is the equivalent of `pure`.
Yea but why does Cont win with 1 if IO is at 93?
 You get infinite/spiral nesting. Try to print `mars`.
As an author of jsverify (javascript quickcheck clone), I agree with everything said there. I'm part of the industry, though maybe on the "academic" side, and with jsverify I miss the dialogue with the rest of the community. I'm not sure, if anyone really uses jsverify; but they seem to be mostly happy with it, as there are very little bug reports (there are!, found one myself today), feature or help requests. So, so far I have developed jsverify mostly for my own needs. Obviously it works for me, but I don't mind if someone could benefit from it too. Also I agree, that it's hard to come with good examples, showing the power but not taking ten slides to explain. Haven't really found one, toy examples are too simple or/and irrelevant. jsverify site has _only_ container examples: better than no examples, but still not quite there yet :( Better documentation is the most requested "feature". And that's quite hard, especially if you try to keep it understandable yet complete and not too verbose. I think we should write different types of it: API references; examples: bigger standalone ones, and also examples in the API reference; and tutorials: examples with narratives, but also texts explaining techniques and the way of thinking. Recorded talks work especially good for "tutorial" purpose, it would probably take much longer than 1h40min to get http://hackage.haskell.org/package/discrimination than using other means.
This should allow to bootstrap the first part of GHCJS with `cabal install ghcjs` from Hackage ... at least I hope it will :-)
If you think of Maybe as the "throw exception and fail" monad, Cont is the opposite, the "succeed no matter what the rest of the computation tells you" monad. Cont skipped the rest of the computation/game and returned 1.
Figuring out the URL. It works, but it's brittle. It would be great if we could download from https://github.com/commercialhaskell/stack/releases/latest/stack-latest-linux.gz and have it download the correct file.
It would have been slightly more appropriate to use *** Exception: Prelude.tail: empty list for this joke. It could be the reason he didn't is because we all hope that will go away someday.
This isn't an intro-to-math article aimed at haskellers. It is one in a series of intro-to-category-theory articles aimed at developers in general. You can see the sorts of things that build up to it in the toc: http://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/ That said, it seems like you have a good point about many other things one could talk about -- like extensions, etc. Sounds like you might want to write an article or two yourself! 
I got bitten by `liftA2 (||)`, where I really needed monadic one [||^](http://hackage.haskell.org/package/extra-1.3.1/docs/Control-Monad-Extra.html#v:-124--124--94-) from `extra`. For `(-&gt;) r` it doesn't matter, but for e.g. `State s` it does.
Welp, time to port my Multiplayer snake AI to haskell I guess. Awesome stuff
You can try the prepackaged sdist already (automatically updated every two hours): `cabal install http://hdiff.luite.com/ghcjs-sdist/ghcjs-improved-base.tar.gz &amp;&amp; ghcjs-boot` (You still need to install `happy` first and have node.js (the default name it looks for is `node`) in your path) Building the sdist requires a bunch of additional tools, which in particular on Windows can be a bit problematic, but the above command should work on all operating systems.
I don't see how your `putStrLn'` can work. `putStrLn'` is an action that does... something. It results in a function that takes a string to output, so it can't be outputting the string itself. `&lt;*&gt;` combines the effects of two actions, but doesn't introduce any new effects itself: the `String -&gt; ()` from `putStrLn'` and the `String` from `getLine` are combined purely. So this can't result in anything being output either. What am I missing? **Edit:** `($ "Hello, world!") &lt;$&gt; putStrLn'` and `($ "Nuts to your white mice.") &lt;$&gt; putStrLn'` must output the same thing (basic Functor law). How can we possibly use `putStrLn'` to output a custom message?
I once involved my non-programmer friend in my haskell project. I gave him some basic haskell training and overview of code and he had basically no problem with understanding existing code and writing new. I'd definitely recommend haskell for non-programmers (disclaimer: I don't know any other functional languages, but I mean haskell opposed to imperative langs).
But the most amazing thing, following the topic of this reddit entry, is that it is possible to perform distributed computing in the same way. To distribute a `worker` in the Transien monad (that may be multithreaded) among different nodes and return the results can be done using `callTo`: distribute :: TransientIO a -&gt; (a,(HostName,PortId)) -&gt; TransientIO a distribute worker xs= foldl (&lt;|&gt;) empty $ map (\(x,(h,p)) -&gt; callTo h p $ worker x) xs All the nodes should run the same snippet that invoke `distribute`, although it is not necessary to run the same binary neither to have the same architecture. https://www.fpcomplete.com/user/agocorona/moving-haskell-processes-between-nodes-transient-effects-iv If instead of getting each result in a different thread we need them in a single thread, `collect` will collect the results in a list. mapReduce :: TransientIO a -&gt; (a,(HostName,PortId)) -&gt; TransientIO [a] mapReduce= collect 0 . distribute `collect` can also be used with `async` But it is more direct to use the monoid instance of Transient for this purpose: mapReduce :: Monoid a =&gt; (a,(HostName,PortId)) -&gt; TransientIO a mapReduce worker xs= foldl (&lt;&gt;) mempty $ map (\(x,(h,p)) -&gt; callTo h p $ worker x) xs This program will wait for all the node's result and will `mappend` them in a single result
The problem is that `nowAll abundants` is getting re-evaluated multiple times when you use ghci. However, if you compile the program with -O2 it will only get evaluated once. ghci performs hardly optimizations at all, so you should run compiled code for compute intensive programs. For compiler options I generally use -O2. That said, you could use a better algorithm. Computing `abundants` only takes 20 secs in ghci (even less compiled), so your problem is in determining the numbers which can be represented as x+y where x and y are abundant numbers.
I have solved quite a few of the PE problems and I almost never have used memoization. Use math, not memoization.
&gt; What exactly is [the class of lambda terms that don't need bookkeeping operators for their reduction]? The class has been firstly formalised as a logic, in particular as the "Elementary" (or the "Light") restriction of Linear Logic whose name come from the fact that it has well-known complexity properties---it is correct and complete with respect to the class of functions computable in ELEMENTARYTIME (respectively PTIME). See [1] for the historical (and challenging) paper or [2] for a more recent simplification. &gt; Is there an algorithm to determine if a lambda term is in that class? Yes, there is. Elementary Affine Logic has been then adapted into a typing system for lambda-calculus, equipped with a type inference algorithm [3]. &gt; [D]oes that mean that applying the optimal reduction without the bookkeeping - only the abstract algorithm - will produce the right result for that term? Yes. *References* 1. J.-Y. Girard, ‚ÄúLight linear logic,‚Äù in Logic and computational complexity, Springer, 1995, pp. 145‚Äì176. 2. A. Asperti and L. Roversi, ‚ÄòIntuitionistic light affine logic‚Äô, ACM Transactions on Computational Logic (TOCL), vol. 3, no. 1, pp. 137‚Äì175, Jan. 2002. 3. P. Coppola and S. Martini, ‚ÄòOptimizing optimal reduction: A type inference algorithm for elementary affine logic‚Äô, ACM Transactions on Computational Logic (TOCL), vol. 7, no. 2, pp. 219‚Äì260, 2006.
Small summary of the blog post: JPS is a heuristic for A* that gets is speed-ups from "jumping over" uninteresting open spaces and pruning neighbors. * Jumping Let's say that we're going in a straight line from A to B which are 5 tiles apart. Unlike an A* based on Euclidean distance or similar heuristics, the nodes between A and B are not expanded, since we know that all shortest paths from A to B will involve them anyway and that there's no point in branching from them. The basically same idea applies to going diagonally. * Pruning neighbors Similarly, we *prune neighbors*: Let's say that we are at A, which has a right neighbor B: X X X A B C X X X When we expand B, the tiles marked with X are pruned away from B's list of neighbors, since they can be optimally reached from A (B's ancestor) without going through B. The assumption is that, since A presumably already took care of those tiles, we, at B, don't need to anymore.
Okay, so how about installing it? I just installed purescript through the AUR, for ghcjs there isn't a single package on the [new AUR](http://aur4.archlinux.org/), and the ones on the old are outdated. However, I'm favoring GHCJS a bit now after reading about the [Differences between PureScript and Haskell](https://github.com/purescript/purescript/wiki/Differences-from-Haskell). The record stuff seems like a good thing, but I'm really unsure about strict evaluation and explicit forall + named instances just seem to add verbosity for no reason.
Applicative computations only have "one shape", so to speak. That is, you generally take a pure function f :: a -&gt; b -&gt; c -&gt; d and feed your monadic/applicative arguments into it, like so: f &lt;$&gt; getLine &lt;*&gt; getLine &lt;*&gt; getProcessID "explorer" &lt;*&gt; getFreeMemory All four of the arguments will get evaluated, no matter what you do. The pure function does need its four arguments, after all. Crucially, you cannot express something like: "if the 2nd argument was `exit`, I don't even want to look at the other two". Because of this limitation, anything recursive, as well as most interactive programs, become impossible to encode. A monad, on the other hand, can very well abort mid-way: f :: IO () f = do a &lt;- getLine b &lt;- getLine if b == "exit" then putStrLn "Aborting. Good bye!" else do c &lt;- getProcessID "explorer" d &lt;- getFreeMemory putStrLn $ "Got these data: ..."
The problem with software (and especially if you are not the end user) is even though you might prove that a function do whatever it's supposed to do, you still have no guarantie that is was correct to call this function in the first place. For example, you might need a `minimum` function to compute the shortest path from A to B (and select the path with the minimum weight) to then realize (but too late) that your weight provided to your `minimum` for some reason works the other way around and that in fact you needed the `maximum`. The best way to insure that your program does what is supposed to do is to write specification which clear examples which can be easily understood. Of course, a proof is a bonus.
I was vaguely aware of that, but I don't really "know" it. That sounds magical. How can you implement an interface "compatible" with a library without depending with the library itself? Reminds me of the motivation behind Backpack. How is this done, and what does it mean for ``vinyl`` to be "compatible" with ``lens``?
The problem with long term is, us developpers, usually think long term as longer than the actual life span of the software. If everybody was thinking (too) long term we probably wouldn't even have a programming language to do stuff with. Everybody will be still designing Haskell 3001. 
Well, maybe if we would think a little bit more long term half our industry wouldn't rewrite the same programs over and over again and the other half wouldn't spend so much time on workarounds for preventable problems.
Yep, but half of the software developpers would be without a job ;-). Having said that, I agree with you, but at the end of the day, some bosses know what they do (remember their job is to make money and some manage to do it) and if that implies paying the same people over and over to do the same job that's kind of their problem, but they might have done the math and be doing what is best for them (a bit like buying an office vs renting it ).
I'd only like to add that commutativity is a very desirable property which indicates things are ok and behave properly. It is a good questions to ask because if the diagram does not commute, it usually indicates something is missing or some problem with the design. 
I remember some talk about having a simultaneous release of the platform when the 7.10.2 comes out. Is it still the plan?
I [did the same thing in C a few years ago](https://github.com/arirahikkala/astar-jps). It's really messy by my current standards but I thought it was nice back then. Anyway, some thoughts on your code: The rotation functions would be both more aesthetically pleasing and probably better performing if Direction was a newtype over Int. Then rotations just become additions modulo 8. diagDist should probably be a parameter on aStar rather than defined inside the library. JPS should always produce an optimal path (under the assumption that the graph is an 8-connected 2d grid with uniform movement costs, and the usual A* assumptions). I'm not sure what you mean with "99% optimal".
Also, fwiw, GHCJS is perhaps about to get easier to start using: https://www.reddit.com/r/haskell/comments/3e3x7k/ghc_weekly_news_20150721/ctbakqx
&gt; The only use of type have found are to abbreviate long type signatures ... use it as code documentation?
Holy! How have I missed this series? I need to go back to the start. Awesome stuff.
that's pretty much the essence of pathfinding yeah. They can of course be extended to general graphs instead of just square grids and the conditions for wether a node is a valid one for our path can be more complicated than wether it contains True or False, of course.
 putStrLn :: IO (String -&gt; ()) is rather difficult to implement in practice. In general, you can start to recover limited forms of context sensitivity in your Applicative by adding more powerful primitives, though.
You can indeed use monads for everything you can use applicative functors for, _when lambda abstractions are available_. However, I've been trying various permutations of `&gt;&gt;=`, `=&lt;&lt;`, `return`, `join`, `&gt;&gt;`, etc. in an attempt to find a calculus of two combinators (roughly equivalent to SK) that, when applied to the monad `(-&gt; r)`, forms a Turing-complete language. I can't find a pointfree way to express `ap` (or `liftM2`, a necessary component of `ap`) in terms of `&gt;&gt;=` and `return`, or any two other monad functions that don't degenerate to an applicative functor.
There's a [page on the lens wiki](https://github.com/ekmett/lens/wiki/How-can-I-write-lenses-without-depending-on-lens%3F). Unlike a lot of other libraries, a lot of the core `lens` types, including `Lens`, are not even newtypes, just type synonyms: type Lens s t a b = Functor f =&gt; (a -&gt; f b) -&gt; s -&gt; f t Anything of that type is a `Lens`, and you don't need anything but `base` to write it. Once you wrap your head around that type, the whole thing starts to make a lot more sense. (It took me a while, and writing a bunch of lenses by hand for various types.) The magic in `lens` itself is the collection of combinators, and more restrictive "optics" like `prism`.
I guess you mean trivial one-off programs. I think we should strive to get the better alternatives to work well even for those cases. Otherwise once you pass a triviality threshold you have to rewrite.
That is awesome. I wonder if that includes a lot of useful terms. Do you know if there is any inferencer around I can download instead of having to program my own? Thank you very much. Edit: also, I'm really sorry for asking so much, but there isn't really a lot of places I can get that information from. What you said became extremely useful to my work. I hope I'm not annoying. And seriously, thank you.
If it works with any web applications built on top of `wai`, why is it called `yesod-devel`? Wouldn't `wai-devel` be a more appropriate name? Also, I haven't used the existing `yesod-devel` tool in a while. Is there anything in particular that this project will improve on (other than support for all `wai`-compliant applications, which even on its own seems like a pretty good step forward to take)?
not yet, but this kind of technical debt tends cause problems sooner or later... and fixing that later gets more expensive with each additional release... also, you dont even have any upper bound on api-builder... 
Oh, you could take a look at [modular-arithmetic](https://hackage.haskell.org/package/modular-arithmetic) a tiny little library I wrote that helps make newtypes like that. It would let you have the type ``Int `Mod` 8`` or `Int/8` with the usual `Num` operations that automatically mods by `8`. It would make the code neat and prevent mistakes, but it might be a tiny bit less efficient. Currently, it does a `mod` at every operation and I haven't really benchmarked it. It probably inlines well, but I haven't checked. I've been meaning to learn how to use criterion anyway so I could add some benchmarks and, more ambitiously, I think it's possible to add some rewrite rules that consolidate multiple operations and only do one `mod`. (It might not be worth it, but would be really cool!)
I'm not Haskell expert at all, but from what I learned recently reading "Thinking Functionally with Haskell" by Richard Bird you can avoid exactly the problem by using "where" clause to extract/evaluate expression that you don't want to be evaluated on each iteration.
In fact, this won't even work for vim. The only reliable fs notification (on Linux) is file creation, due to the way vim writes to a temp file, removes the original, and renames the temp. Acting on 'modify' causes a race condition, since the 'modification' is deletion. Will the app that's waiting (yesod-devel in this case) read the file before or after it is recreated?
&gt; (e.g. I imagine you could usually safely trigger a recompile on *.hs)? Emacs creates ‚Äú`.#file.hs`‚Äù locks. But I guess all common editors‚Äô patterns are easy to spot.
For a project for a large enterprise, I once wrote an application of 10s of thousands of LOC that took over a year of full time dev effort. It used exclusively lazy IO, but the IO logic was simple - reading in a large stream of bytes, doing lots of complex but incremental processing in many steps, and writing out a large stream of bytes. Lazy IO worked perfectly and was trivially simple. Believe me, this application was way complex enough without adding extra complexity for no reason.
Last time I used the site was awhile back, so I don't have any specific recollection of the speed then, but this feels *very* snappy compared to most web apps!
&gt; But often the size of a vector is not known at compile time. In this case, SubHask lets you use a string to identify the dimension of a vector. In `hlearn-allknn`, the data points are represented using the type `UVector "dyn" Float`. The compiler then statically guarantees that every variable of type `UVector "dyn" Float` will have the same dimension. This trick is what lets us create the type `UArray (UVector "dyn" Float)`. I'm quite curious how this works. When *is* the size known? Is it user input read from `IO`? (Does the string `"dyn"` also appear in the user input?) How is the sameness guaranteed? Is there a `Maybe`-returning smart constructor with a runtime check, or something more sophisticated? How does `UArray` learn what the size ended up being (to do the runtime offset calculations which it presumably must do)?
That's a great question that I have a lot to say about. I'm planning on writing a bit more about this at some point, but here's a quick explanation. One way to construct the type `UVector "dyn" Float` is via the `unsafeToModule :: [Float] -&gt; UVector "dyn" Float` function. There's nothing "IO unsafe" going on here. This function is called unsafe because it can possibly break a type system invariant if you're not careful. *Mathematical aside*: I've specialized the type above, but `unsafeToModule` function is part of the `FiniteModule` class. Every [free finite module](http://math.stackexchange.com/questions/304752/difference-between-free-and-finitely-generated-modules) can be represented as a list of basis vectors. Vectors are a special type of free module, and in this case we know the dimension is finite. That's why we have access to this function. Internally, `UVector :: k -&gt; * -&gt; *` is a data family. It has two instances. One for `k::Symbol` and one for `k::Nat`. If `k::Symbol`, then the `UVector` must store the dimension of the vector. The key is that we ensure that this value will be the same for every vector that uses the same symbol. Therefore, when we add two vectors (for example), we don't have to check to see if they have the same length. The type system guarantees this property for us. The `unsafeToModule` function is unsafe because it can break this abstraction if used incorrectly. There are other safe ways of creating modules (you'll notice the code for the `hlearn-allknn` executable doesn't call this function at all), but internally these safe methods all use this unsafe function. Now we construct a `UArray` using the `fromList1 :: a -&gt; [a] -&gt; UArray a` function. Notice that we *must* pass in an element of the type that will be inside the array. This is how the array knows how big the vectors will be. *Mathematical aside*: `fromList1` is provided by the `Constructible` type class. `Constructible`s roughly corresponds to freely generated semigroups, and the type family `Elem` tells us what type generated the semigroup. The gory details are all in the SubHask module. In particular, checkout the [SubHask.Algebra.Vector](https://github.com/mikeizbicki/subhask/blob/master/src/SubHask/Algebra/Vector.hs) and [SubHask.Algebra.Array](https://github.com/mikeizbicki/subhask/blob/master/src/SubHask/Algebra/Array.hs) modules.
Probably the biggest problem of `unsafeInterleaveIO` in real-world Haskell applications is getting `IOException`s in the middle of pure code. You see a function whose type looks like this: example :: IO [String] ... and you run it and bind its result: ... strings &lt;- example ... ... and no exception has been thrown so far. You're clear, right? Wrong. You try to evaluate the list returned by that function and you get an exception in the middle of pure code because evaluating the list triggers an `IO` action that was deferred by the `unsafeInterleaveIO`. This is a symptom of a bigger problem with `unsafeInterleaveIO`, which is that it interferes with equational reasoning. Once you tie side effect order to evaluation order most of the program transformations that you would normally consider safe in Haskell are no longer safe. Tying side effects to evaluation order means that all Haskell types (even pure ones) are conceptually inhabited by `IO`, because every Haskell value can contain potentially unevaluated effects deferred by `unsafeInterleaveIO`.
Saying that `unsafeInterleaveIO` is fine because you can think of it as introducing threading and non-determinism seems like a pretty bad defense. Unneeded concurrency and non-determinism are sources of probably no less bugs as impurity! So under one world view, lazy I/O is terrible because it violates the spirit of Haskell and purity. Under this world view, lazy I/O is terrible because it supposedly simplifies but in fact insidiously turns simple programs into non-determinstic, concurrent programs which typically just want to single-threadedly do a bit of incremental stream processing. Either way, lazy I/O is terrible!
I like my job now, but using richly typed Haskell libraries fulltime (+ music), damn...
I'll have to read this at some point... never did implement jumping for my hex grid A* (in C#). I wanted weighted costs, which rules out jumping anyway, but I wound up expanding it into a library which includes a faster boolean-only version, which might benefit. How do you manage your priority queue? In C# [I found one](https://github.com/BlueRaja/High-Speed-Priority-Queue-for-C-Sharp) with a clever object-identity trick that allowed O(1) `.Contains()`, which isn't an easy option in Haskell.
google :)
Yes it is symmetric lenses. I'm not aware of anywhere that discusses this representation of isomorphisms. I have a bit of a blurb about Van Laarhoven prisms in r6research.
Cool, I'll double back to the wiki then. I guess I was hoping for a direct analogue to nltk but I am more than happy to check this stuff out. Thanks!
Wow, I'm in the middle of a contract but I'll definitely have to apply.
Unfortunately the problem remains: after some edition the CPU stay 100% and sometimes the memory usage grows very fast until blocking the PC. I have to revert to 0.13.2.8 that is oldest version that i have downloaded, that does not have such problem. Although it slow down and from time to time I have to restart it. Each version is less responsive. The versions of two years ago did not have these problems. I think that it is necessary to fix these performance bugs. It does not matter to have more features if the memory leak renders them unusable. I¬¥m a heavy user of leksah. it is my IDE. Maybe I will look for some older version and will try to use it with the latest GHC.
As for practical examples, I just used quickcheck for testing database queries. I'm using acid-state for a small toy project, which has the benefit of having pure queries. You can declare the queries as state and reader monads. I also implemented a small DSL/command type together with an evaluator and an arbitrary instance. Using this DSL I was able to easily test data and query facility, without modifying the state manually in the tests
I rolled my own priority queue that's really simple. I should benchmark some of the standard Haskell ones eventually and see if they are more performant. [Priority Queue](https://gist.github.com/ZacharyKamerling/8b11118bc12b4fa3f5d0)
You are doing pattern matching already, that's what the [] and (_:rest) are doing. They're matching the empty list and a list with at least one element respectively. 
Wow, nice to see full time haskell job
What's your experience with opaleye been like? 
&gt; but it crashed instantly when I ran it That is not good! Was this a recent version? There was a crashing bug if `ghc-pkg` was not in your `PATH` or if your `PATH` was in `.bash_profile` instead of `.profile`. It should work with both profile files since version 0.15.0.2 and should not crash if it is not set at all (should give an error in the log Pane). Please run `/Applications/Leksah.app/Contents/MacOS/Leksah --verbosity=DEBUG` and post the output here or in a [github issue](https://github.com/leksah/leksah/issues).
A presentation of stack with an eye towards doing freeform experiments conveniently. I'm still figuring it out, so suggestions are most welcome! 
&gt; On line 54, when I call "abundants" is it reevaluating the list comprehension during every iteration? No. Or at least, not when the program is compiled with `-O2`. To figure out whether it was the case, I added a call to [`trace`](http://hackage.haskell.org/package/base-4.8.1.0/docs/Debug-Trace.html#v:trace), which prints a message to the console when it is evaluated: 52 twentyThree = do 53 let abundants = [ x | x &lt;- [1..28123], trace (show x) $ abun x (factors x)] 54 sum [ x | x &lt;- [1..28123], not $ x `elem` nowAll abundants] The result was that all the numbers from 1 to 28123 were printed to the console, and then nothing. If `abundants` was being recomputed on every loop, the numbers from 1 to 28123 would have been printed again and again. Moving the `trace` call to `nowAll` is also printing a single set of numbers, so the subexpression `nowAll abundants` is also only evaluated once. 48 nowAll :: (Show a, Num a) =&gt; [a] -&gt; [a] 49 nowAll [] = [] 50 nowAll (w:wx) = trace (show w) $ helper (w:wx) ++ nowAll (wx) Moving the `trace` call to the outer loop, there is a gap between 1 and 2 as `nowAll abundants` is computed a single time, and then the other numbers trickle out. 52 twentyThree = do 53 let abundants = [ x | x &lt;- [1..28123], abun x (factors x)] 54 sum [ x | x &lt;- [1..28123], trace (show x) (not $ x `elem` nowAll abundants)] It's taking a while for the numbers to be printed out at first, but then they start printing faster and faster, and after reaching 28123 the program does terminate. Since the entire thing only took a few minutes on my machine, it seems suspicious that it's taking hours on yours. Are you running your program in interpreted mode, via `runhaskell` or `ghci`? If you are compiling your program, did you use any optimization flag? For many more examples of using `trace` to figure out which parts of a computation are being re-evaluated, see my blog post on memoization, "[Will it memoize?](http://gelisam.blogspot.ca/2015/06/will-it-memoize.html)".
&gt; A one-object category is a monoid. This is like saying that a group is a groupoid with one object. And there's a reason that in Aluffi's book Algebra Chapter 0, this is labeled as *Joke* II.1.1 rather than *Definition* II.1.1. It is obtuse to take "one object category" as the *definition* for monoid. It's certainly a *characterization*, but it's backwards. You don't need anything close the complexity of a category to define what a monoid is. The idea that typed languages "form a category" is a crude simplification. You can't account for polymorphism properly by having types as objects and functions as morphisms. And the setups which *do* account for polymorphism don't actually do things this way. (Usually your contexts are your objects, or you might have to work in a fibered category).
the proliferation of meanings for "referential transparency" hurts me greatly :(
gasche, liquidHaskell can be used to prove _termination_ of quite complex examples, but requires the user to provide termination hints. For example, below we prove termination of the Ackermann function, since the user specifies in the type signature that the expressions `[m, n]` are lexicographically decreasing. {-@ ack :: m:Nat -&gt; n:Nat -&gt; Nat / [m, n]@-} ack m n | m == 0 = n + 1 | m &gt; 0 &amp;&amp; n == 0 = ack (m-1) 1 | m &gt; 0 &amp;&amp; n &gt; 0 = ack (m-1) (ack m (n-1)) With these user provided hints liquidHaskell can scale to complex examples. As another one, we proved termination of the mutually recursive [quicksort](https://github.com/ucsd-progsys/liquidhaskell/blob/master/tests/pos/RecQSort.hs). Though, I am not sure how/if one can use all these mechanisms to check complexity bounds. 
I actually was running it in ghci, I didn't realize that it would make a difference. I'm not a very experience Haskell programmer. I'm running into issues using runhaskell. I turned twentyThree into main which gives me a bunch of errors that i'm not familiar with. Can you tell me what i'm doing wrong? runhaskell -O2 results in the following errors: twentyThree.hs:20:44: No instance for (Eq (IO t0)) arising from a use of `elem' Possible fix: add an instance declaration for (Eq (IO t0)) In the second argument of `($)', namely `x `elem` nowAll abundants' In the expression: not $ x `elem` nowAll abundants In a stmt of a list comprehension: not $ x `elem` nowAll abundants twentyThree.hs:21:38: No instance for (Enum (IO t0)) arising from the arithmetic sequence `1 .. 28123' Possible fix: add an instance declaration for (Enum (IO t0)) In the expression: [1 .. 28123] In a stmt of a list comprehension: x &lt;- [1 .. 28123] In the expression: [x | x &lt;- [1 .. 28123], abun x (factors x)] twentyThree.hs:21:39: No instance for (Num (IO t0)) arising from the literal `1' Possible fix: add an instance declaration for (Num (IO t0)) In the expression: 1 In the expression: [1 .. 28123] In a stmt of a list comprehension: x &lt;- [1 .. 28123] twentyThree.hs:21:50: No instance for (Ord (IO t0)) arising from a use of `abun' Possible fix: add an instance declaration for (Ord (IO t0)) In the expression: abun x (factors x) In a stmt of a list comprehension: abun x (factors x) In the expression: [x | x &lt;- [1 .. 28123], abun x (factors x)] twentyThree.hs:21:58: No instance for (Integral (IO t0)) arising from a use of `factors' Possible fix: add an instance declaration for (Integral (IO t0)) In the second argument of `abun', namely `(factors x)' In the expression: abun x (factors x) In a stmt of a list comprehension: abun x (factors x) 
Main should be of type IO () twentyThree is of type Integer You want the last line to be: print $ sum ... Actually it seems odd to me that the current twentyThree compiles as do blocks should be monadic. Does anybody know if this would have been a compiler error in older GHC versions?
thank you for saying it! the relation between purity (in the *standard* sense) and functionality is clarified by longley in the paper, "when is a functional program not a functional program". in any case, purity has little to do with church-rosser, as you note.
Oh, right... on a uniform-cost grid you'll never need to update a cell's cost, so checking if the queue contains a cell isn't needed. So of course your implementation doesn't even *have* a `contains`, heh.
Well, that research looks incredibly interesting. I wonder if functional programming was chosen for any reason related to how biological cells process information.
Oh sure, I'm not asking for myself though. I've already converted all my work and private projects to be able to use cabal sandboxes and stack side-by-side -- just like to have resources handy for learners ya know? Examples go a long way :)
&gt; completely incapable of writing a proof [So you only write type signatures but no definitions?](https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence)
Most certainly. I will add a postscript once I'm done, then :)
The link to Beginning Web Programming in Haskell is wrong. It should be https://www.youtube.com/watch?v=GobPiGL9jJ4. I will have the blog post updated.
It's been two days. I fully expected to find this up on hackage by now :P
The only missing piece is TLS SNI (to ditch nginx completely and switch to unproxied keter).
&gt; I'm not sure what you mean. You can obviously break the abstraction using the `unsafeToModule` function. Maybe a better way to phrase it is that the programmer has to be careful to ensure that they don't create vectors of different sizes with the same symbol. Wait -- so it's *not* actually enforced that all `UVector "dyn"`s will have the same dimension, and this is an invariant the programmer must always uphold herself? I feel like we might be talking past each other, though I'm not sure where or why. It's possible I've misunderstood something. I'm quite familiar with the pattern of providing an abstraction with certain invariants which are normally upheld in the type system, along with "unsafe escape hatch" functions which can be used to violate the invariants (`unsafeToModule` in this case), and which place the responsibility for upholding them onto the programmer's shoulders. I feel like I keep trying to ask about the first half of this (how the invariants are normally upheld in the type system), and keep getting answers about the second half (how they can be circumvented with `unsafeToModule`). But let me try again: what other ways are there to construct a `UVector (n :: Symbol)` besides `unsafeToModule`, ways which aren't themselves unsafe (even if they are internally implemented in terms of `unsafeToModule`)? How do these ensure that the same-dimension guarantee is upheld? &gt; It's okay because there is a free construction to turn any semigroup into a monoid. If you look closely at the code, you can see that `UArray` is defined as: Sorry if I wasn't clear: I was referring to [this `UVector` instance](https://github.com/mikeizbicki/subhask/blob/master/src/SubHask/Algebra/Vector.hs#L337-L342) (not `UArray`).
I think this is a good and valid point, although I wouldn't put the blame on the packages. This should be the responsibility of tools like cabal and stack. For instance, stack only copies executables to `~/.local/bin` if explicitly requested; otherwise they are installed to a local install directory which you probably wouldn't have in your PATH.
doesn't cabal keep binaries in the current sandbox? should be fine 
I agree. This actually got me wondering: tools like stack and cabal shouldn't even build the executable parts for packages that are built as dependencies, I think. Would that break anything?
If you want to use it, open an issue on the Github tracker. I didn't realise people actually wanted it!
Ah yes. The magic of ``newtype``! Thanks a lot. :-) 
Why does the program stop instead of just returning 0 an infinite amount of time once it has gone through the entire list ?
Wonderful! `def = return ()` addresses all the objections I [made](https://www.reddit.com/r/haskell/comments/3di7cs/dont_use_default/ct6hmfn) in the previous thread.
I'm not sure I understand your question. The second pattern match essentially removes one element off the front of the list, and it's associated expression adds 1 to the listLength of the rest of the list. So listLength [x] = 1 + listLength [] = 1 + 0 = 1
Also, we are really talking about the `State` applicative functor here, as the changes to the state are independent of the values.
It is something of a hack, but when you have a lot of fields to override, I find it comes out a lot cleaner. Getting automatic layout from `hindent` is nice, too :) Without that little bit of extra sugar, you end up with foo (&amp; a .~ x &amp; b .~ y &amp; c .~ z) which I find worse than the (ab)use of `State`.
This is close to what I wanted, and I tried using the `pointfree` utility on `liftM2` and got results similar to these... The problem is that these actually require 3 combinators: `&gt;&gt;=`, `fmap`, and `flip`. Even the first result requires `flip`, because it uses infix notation with `.` in a few places to flip its argument order. Still, these results will probably be useful, so thanks! Edit: And a full combinator calculus would also require `return`, so that's 4 combinators. But maybe that's good enough; it roughly corresponds to Haskell Curry's [BCKW System](https://en.wikipedia.org/wiki/B,C,K,W_system), with the _W_ (duplicating) combinator replaced with `&gt;&gt;=`. Edit^2: I can form (what seems like) a Turing-complete combinator system that can act on all monads (not just `(-&gt; r)`) using the following combinators: * _B_ = `fmap` (= `.` for `(-&gt; r)`) * _C_ = `flip` * _K_ = `return` (= `const` for `(-&gt; r)`) * _N_ = `=&lt;&lt;` I used _N_ for bi**n**d because _B_ was already taken, and I haven't seen any other combinators named _N_. It turns out that using `=&lt;&lt;` instead of `&gt;&gt;=` as a basic combinator has some nice properties. `ap` (_S_) can be expressed even more succinctly, as C(BN(CB)). And `join` can be expressed as N(NK).
Is there a `State` applicative? I've never seen one. And if there were, [ApplicativeDo](https://ghc.haskell.org/trac/ghc/wiki/ApplicativeDo) would be helpful here.
I like this (and the equivalent, monad-free Endo approach) because it fairly directly expresses my usual interaction with default configurations anyway: 90% of the args I don't care about and I just need to find the one or two I do and switch 'em up.
You can't use `Void` as there is no way to construct it. If you have foo :: State x Void -&gt; whatever Then you are asking the caller to provide a state computation that will produce `Void` - and the only way to do that is to refuse to terminate. That's probably not what you want ;)
I have a feeling that it will break something, somewhere. Since build-tools have never been supported properly by cabal, many people don't use them. I wouldn't be surprised if some packages depend on a library assuming that its companion executable be available as well. Given that stack keeps these generated executables inside the sandbox bin directory, I'm not terribly worried about it, so I'd be inclined to want to avoid the possible breakage.
Here's an [example](http://hub.darcs.net/simon/darcsden/browse/stack.yaml) of a stack.yaml for an older project (lots of deps not in stackage). In your example, why is acme-missiles not under extra-deps - is it because local packages aren't supported there ? And what exactly does extra-dep: true do ?
Ah, yes, of course. What I meant was a `State` applicative that is not a `Monad`. Like in the case here, one can provide `Applicative` instance for the settings and, in the future, use the `do` notation with `ApplicativeDo` (which, AFAIK, is ready to be merged).
Why *State Settings ()* instead of *Settings -&gt; Settings*? With &amp;~ you can still use do notation for the latter. Or you could use *Endo*, but that doesn't look that nice.
Okay. I'm trying to implement an optimal evaluator right now. I'll contain my need to ask more and let you know if I do find/make such inferencer. See you!
The reverse applies, you can just use `state` to lift a pure function into this. It depends which style you want to encourage, which means a trip to the bike shed to see which colours are in this season ;)
See my answer at https://www.reddit.com/r/haskell/comments/3eb622/another_approach_to_default_function_parameters/ctdf6pg. Essentially it depends on which style you want to prefer, and for me `State` wins.
Perhaps I'm being dense, but then what is a helpful definition for purity?
I'm glad you liked it, although I wouldn't wave it in the face of Scala-users. Its combative tone would only embitter them.
When I'm being loose, I mean "the beta-eta equations hold for functions", and when I'm being strict, I mean "beta-eta holds for all the types I care about". People generally want Haskell to be pure in the loose sense (it's not pure in the strict sense because general recursion breaks eta for sums), but unfortunately `seq` breaks eta for functions. So Haskell isn't pure, though people pretend it is. 
If you pass `--prefer-nightly` or `--prefer-lts` to `stack new` or `stack init`, stack will pick the latest nightly/LTS snapshot *that you already have*, so that you only get new snapshots when you decide to (i.e. by explicitly changing a `stack.yaml`). There is no clean way to remove old snapshots yet, though there is an active issue for implementing that in GitHub (milestone 0.3).
The bar | in Haskell is used for guard clauses, so you can do: listLength xs | null xs = 0 | otherwise = 1 + listLength (tail xs) But it's generally better to pattern match on constructors, as there's less room for error (and you get "incomplete pattern match" errors!)
&gt; why is acme-missiles not under extra-deps - is it because local packages aren't supported there ? Exactly. Local packages, including those you `stack unpack` into the project, are declared in the `packages` section. &gt; And what exactly does extra-dep: true do ? It disables test and benchmarks for the package, which would be ran if it was treated as a part of the project proper, and not just another dependency.
That's an interesting bit of trivia, thanks.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/purescript] [PureScript/Haskell developer job opening @ Symbolian (Berlin) (x-post from /r/haskell)](https://np.reddit.com/r/purescript/comments/3ec82x/purescripthaskell_developer_job_opening_symbolian/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger/wiki/) ^/ ^[Contact](/message/compose/?to=\/r\/TotesMessenger))* [](#bot)
Let me fix that.
Is it just me or is HTTP/2 a bad idea? Great work that Warp supports HTTP/2 but somehow I wish it wouldn't. 
What's nice about this approach is that it's a library-less solution to the problem. So you can publish a library with `foo :: (FooSettings -&gt; FooSettings) -&gt; FooArg -&gt; FooResp`, and /u/ocharles can publish a library with `bar :: State BarSettings x -&gt; BarArg -&gt; BarResp`, and I can use both, using the style I prefer: let f = foo (execState $ do { ... }) fooArg b = bar (do { ... }) barArg or let f = foo (...) fooArg b = bar (modify $ ...) barArg
Example of new hyperlinked source taken from the thread: https://hackage.haskell.org/package/ghc-exactprint-0.3/docs/src/Language.Haskell.GHC.ExactPrint.html Edit: Source code highlighting and hyperlinking is not in 2.16.1 yet but in the master.
Or we could try to use a typeclass, so the caller could supply a function / list of functions / state monad expression / whatever, as they chose. [My first attempt](https://gist.github.com/dave4420/ed05f798f54d0510a32e) works for the function instance, and works well enough for the list of functions instance (it is no hardship to pass `id` instead of `[]`) but I cannot see how to get it to infer the type for the state monad. (I'm trying to avoid requiring a type annotation at the call site.)
GAK. You're just doing that because you happened to have some kinda related operators lying around for State? Please NO! You really don't need to use weird lens operators, or any special operators at all for this approach, just do: name "Alice" . favoriteColor "Blue" -- feel free to call this `setName` if you want name :: String -&gt; Params -&gt; Params -- etc Using ordinary function composition, it's totally readable! Feel free to split onto multiple lines. createEmployee $ name "Alice" . favoriteColor "Blue" . id (currentId + 1) . role CEO With ordinary function composition, earlier settings are preferred to later ones if you happen to set the same thing twice (not really very common anyway, but totally reasonable behavior in any case). No need to introduce `&amp;`, `.~` or whatever, just use a regular function, seriously!!! Notice that `name "foo"`(or `setName "foo"` takes _fewer characters_ than something stupid like `name .~ "foo"`. If you are really morally opposed to writing any parentheses in your individual settings, you can also do: `settings [id $ currentId + 1, name "Alice", ... ]`, where settings is basically just `mconcat` (for `Endo`).
ok thank you very much. I get it 
This is TERRIFIC, thanks to all involved!!
Try `instance (m ~ State s) =&gt; Tedious (m ()) s` . On my phone so I can't check, but something was discussed recently that I think was the same issue. Not that I'm sure using type classes for this is a good idea. EDIT: Actually, I don't think this will work.
&gt; What category do you think is best to start with for a programmer if it's not types and functions? Frankly, I don't think it's appropriate to start with categories at all. Categories are a natural tool to use when you are working with diagrams. In algebra, you start working with diagrams as soon as you learn about homomorphisms. They naturally come about when talking about the first homomorphism theorem. In topology, you learn about lifting properties, where a continuous map into one space can be extended uniquely to the covering space. In both cases, though, the focus will be on the relevant subject matter. Most intro classes will not mention categories at all. In Aluffi's algebra book, which introduces categories to the student in the first chapter, he doesn't bother defining functors until 450 pages in. It's not that he couldn't. It's that knowing what a functor is just isn't useful until you have developed a large toolbox of example categories. But a programmer's life is very different from a mathematician's. I have never seen a convincing use of universal properties to help me write a function. Knowing about products, coproducts, and exponentials was interesting, but I don't think I've ever used it. (And knowing more general ideas about limits has been useless because Haskell can't talk about subtypes or quotients). The same goes for my understanding of F-algebras and coalgebras: interesting, but I've never been able to find relevance. Perhaps these things *could* be useful to programmers. But it would require something more substantial than these kinds of posts. Kmett claims he has used his command of category theory to aid in refactoring his code. I could get behind that kind of thinking. But not until some general techniques had been developed for doing it. 
I really would like to know what the arguments of the downvoters are. What problems do HTTP/2 solve? Support your creepy overloaded websites working on hundreds of seemingly useless single files where you usually go host them on separate domains to save some bytes of header space? Thats crazy. The problem is your website and not the transfer protocol. HTTP/2 is useless and complicated. HTTP/1 was designed for simplicity and now decades later we know by today how we could make an easier and sufficiently efficient protocol. But no, let us go down and confuse their protocol so they will only be few to implement it. 
I already used Elm before mailboxes were introduced for non-toy projects, and it still works out great. I'm from the German company, Evan [mentions in his talk](https://www.youtube.com/watch?v=oYk8CKH7OhE#t=54m49s). ;-)
Can we get this on Hackage? 
Kazu knocks it out of the park yet again. Very impressive work.
&gt; what the arguments of the downvoters are. This is a release of the Warp web server, not a "rehash finalized IETF decisions" forum. &gt; What problems do HTTP/2 solve? * Allow servers to handle more concurrent users by reducing TCP connections * Reduce latency, round trips, and total network IO. * Reduce need for DNS hacks to parallelize connections. * Reduce overall DNS requests. &gt; Support your creepy overloaded websites working on hundreds of seemingly useless single files where you usually go host them on separate domains to save some bytes of header space? Like a fun video game? A mapping application? A CAD modelling tool? Those things use lots of javascript and lots of assets -- graphics, data, etc. Why is that creepy or crazy? Why should that use case not be supported?
That's ok if you don't want to ever allow people to read back the configuration - for example `asks name` to find out what the name is currently configured to. If you do want to do that, you've now doubled the size of the API - and as I mention in my blog post, this is something I look to avoid. And - while it's not to of interest to me to tone police - just because you disagree with something does not make it stupid.
I watched /u/edwardkmett's talk about discrimination, and he mentions some open problems. This talk was from 2 months ago. Are these still problems?
&gt; So Haskell isn't pure, though people pretend it is. To turn the qualitative judgement into a quantitative one, I tend to count the number of lines in programs written in that language that make use of the impure operations. For example, C++ isn't pure because in practice you use an impure operation every 4th line or so. In contrast, you will generally get through an entire introductory book on Haskell without reaching that first impure line of code.
Solved. =) The [promises](http://hackage.haskell.org/package/promises) package was enough for me to figure out how to do productive stable unordered discrimination. [discrimination](http://hackage.haskell.org/package/discrimination) 0.1 has the productivity improvements in it. HEAD now has a number of further performance improvements, but I haven't released that yet.
Heey I've just tested and it seems that natural number division (which is a quite complex algorithm) works with just the abstract algorithm! This is the division implementation I'm using: div = (a b c d -&gt; (b (e -&gt; (e d)) (a (b (e f g -&gt; (e (h -&gt; (f h g)))) (e -&gt; e) (e f -&gt; (f (c e)))) (b (e f -&gt; e) (e -&gt; e) (e -&gt; e))))) But now I have a strange [side effect](http://stackoverflow.com/questions/31599684/is-it-usual-for-interaction-nets-to-leave-piles-of-redundant-fans). I wish there were more people who could answer that kind of questions :(
This is beautiful. Extremely useful. Some issues I found: - seems you can't click on a re-exported module eg. 1. Go to https://hackage.haskell.org/package/ghc-exactprint-0.3/docs/src/Language.Haskell.GHC.ExactPrint.html 2. You can't click "module Language.Haskell.GHC.ExactPrint.Transform" in the exports, however you can click it in the imports. - Also there seems to be some issues with `let` bindings, eg. 1. Go to https://hackage.haskell.org/package/ghc-exactprint-0.3/docs/src/Language.Haskell.GHC.ExactPrint.html 2. Click "import Language.Haskell.GHC.ExactPrint.Parsers" 3. Click "parseModuleWithCpp" in the exports 4. Click "as" in the last line of the function, it doesn't highlight the `let as` just above it.
Like HTML, JavaScript, and CSS, I sincerely doubt we'll be able to replace HTTP with something sane, no matter how hard we try. With work you can make it efficient, but if you strike out on your own you'll only be able to talk to yourself, and that's just crazy. The work done on Warp here allows us to ignore these concerns and work in a magical kingdom where everything is simply better without expending any additional effort on our parts. Could it be even better at some layer? Yes, but if we as a species possessed the capability stop and rationally pick the best overall option, the world wold not look like it does now.
Thank you for your answer. &gt; Reduce latency, round trips, and total network IO. It just compresses headers, which is not the solution for this header mess, and multiplexes it over one connection. But multiple TCP connections isn't a critical problem. Also [pipelining](http://kb.mozillazine.org/Network.http.pipelining) is already done in HTTP/1.1. HTTP/2 can't get it much faster than that. &gt; Allow servers to handle more concurrent users by reducing TCP connections But this only works well for us, because Haskell can serve multiple streams well. Its difficult for other-PL users to implement good schedulers (especially ones optimized for the web application) and still provide a good API for it. Also note that in HTTP/2 the client decides what should be send first, which totally breaks all benefits your server could bring in from good concurrency. &gt; Reduce need for DNS hacks to parallelize connections. &gt; Reduce overall DNS requests. Is this an accountable issue? Especially when DNS is cached by default? &gt; Like a fun video game? A mapping application? A CAD modelling tool? Those things use lots of javascript and lots of assets -- graphics, data, etc. Why is that creepy or crazy? Why should that use case not be supported? Now we need to talk about the purpose of the web. In how far is the web just an information source or a real application? And the question is still open: how will HTTP/2 work better than HTTP/1.1? 
Nice to see job offerings in Haskell, not for me personally but motivate students in our CS/CE degrees (I teach an Msc FP course using Haskell). I've also noticed that there seem to be more offers in the US (this one is one of the exceptions). Are US companies and institutions generally less risk adverse thus more likely to try out non-mainstream languages?
You're overtly conflating the idea of "not up to my standards" with "not better than HTTP/1.1". It's better than HTTP/1.1 in many *measurable* ways, and you're outright ignoring them.
Super useful! Thanks to all involved, this is huge!
Tell me a *measurable* way. 
This is incredibly useful!
Several were already given to you. Smaller payloads. More parallelism without subdomains. That's measurable. Please don't just cause a ruckus here because you're mad about HTTP/2. 
I don't see how our descendants are any different from someone now who doesn't understand the core but is a new hire on a project. At least it's written in Haskell which provides a huge safety net for someone trying to understand it.
&gt; I cannot apply because of the lack of excellent functional programming skills in Haskell/Purescript... Yes the challenge is bootstrapping oneself into Haskell. In some ways it is easier than bootstrapping for another language because I think Haskell employers would be more open minded to taking on someone who has been learning in their spare time but experienced and battle-proven in an imperative language. 
This seems like something that might make it worth the effort to regenerate haddock documentation for all existing Hackage packages. I wonder if there would be an easy way to do that.
&gt; Haddock-2.16.1 released with source code highlighting and hyperlinking No. &gt; This change is not in 2.16.1 as it is rather recent
Also check: http://queue.acm.org/detail.cfm?id=2716278
What if I told you, SSH is complicated too. ;)
I don't think anyone WAI has been imposed, if for no other reason than how would anyone have done that? :) I think it's telling that, since the advent of WAI, pretty much everything released since has been based on WAI. It points to the fact that the abstraction is useful, and allows for greater experimentation in the space, and (presumably) less time spent reimplementing the same functionality in each framework. I'm not saying it makes sense for Snap &amp; Happstack to start using WAI, but long term they might be forced to to stay competitive. If you look at what happened with Rack &amp; Ruby, it went from a novelty to pretty much the only API to code against. At that point, chosing a framework that doesn't allow use WAI means missing out on a large ecosystem of useful middleware etc. It'll be interesting to see how this plays out though.
Never used GHCJS or PureScript. I can confirm that Haste is easy to set up and produces small JS files. `js_of_ocaml` is another functional-language-to-browser compiler, it has excellent DOM libraries and produces good code, but most OCaml libraries don't work with it, so you're stuck with the rather poor OCaml stdlib. Canvas requires you to do everything manually - figuring out what the mouse is hovering over, for instance - but in return it's much faster than SVG. If you ever expect to deal with thousands of elements at once, SVG could become a bottleneck.
&gt; I can confirm that Haste is easy to set up and produces small JS files. Sounds good :) &gt; If you ever expect to deal with thousands of elements at once, SVG could become a bottleneck. Like I said, it's a turn-based game, thing only have to be updated when the user does something or the opponent did a move. There are no constantly changing parts, so there aren't really any performance concerns.
Thank you very much for long response, you really cleared things up!
That fixed it up, thanks a lot!
The first thing that comes to mind is [FRPNow](https://www.reddit.com/r/haskell/comments/3ai7hl) (which was recently discussed), which is based directly on Fran but fixes the time leaks.
Huh. Why *don't* we have this?
"the identity refactoring" I love Haskell. 
Without answering your question exactly, I often feel that UX is well interpreted by continuous time FRP: the renderer should be exactly a function of a `Behavior UI` and therefore able to choose it's update times. The description of the `UI` should be free of any such constraint.
&gt;But doesn't this cause problems for UArray which assumes all contained UVectors have the same size? For instance, if you were to add u and v from your example along with zero to a UArray. I forgot to mention that there's two equivalent ways to represent `zero`. The way I mentioned is what gets constructed when you call `zero`, but this is equal (i.e. `(==)` returns `True`) to a construction where the size is known and it's just filled with zeros. This second representation is what will actually get stored in the `UArray`.
I strongly agree: avoid complexity and confusion. I find the type `State HTTPSettings x` pretty confusing ("what do settings have to do with State?") . `HTTPSettings -&gt; HTTPSettings` makes a lot more sense to me ("I'm providing some modifications to the default settings"). I think excess complexity for slightly nicer syntax is a bad tradeoff. &gt; We should be able to tell new users that they can understand how APIs work by reading the types. Indeed, we say this a lot. We use it to justify sparse documentation that often has no usage examples. So let's not make libraries less usable with confusing types.
Then I would suggest you stay away from implementing any protocols since other people don't seem to have your problems with it.
GHC 7.10.2 will be out real soon now
The main reason it was not in use was the fact that it serializes requests on a single connection. This is bad for page loading latency compared to requesting the same resources in parallel.
No? The [GHC development snapshots](https://www.haskell.org/ghc/download) aren't enough?
Hah, it was only couple of minutes ago I was browsing source and had to google various identifiers.
While this is an amazing tool, I think this was a bit disingenuous: &gt; Most current source manipulations are based on haskell-src-exts which provides a separate parser and as a result lacks support for many modern language extensions. We can parse our entire code base with haskell-src-exts. It supports most modern language extensions. I understand why this tool wasn't based on it (there will always be differences and lag, and sadly haskell-types isn't there (yet?)), but it's a great package nonetheless.
There were a lot of changes required in GHC to support this. You need 7.10.2, or to build the ghc-7.10 branch in the GHC repo.
What people? The hand-few people that managed under hard struggle to implement an over-complicated protocol? Whats with all the people who tried hard but you never heard of? 
The dev snapshots are 7.10.2 snapshots right? Do you know for sure whether these changes are in there or not?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/nocontext] [Dota \*and\* Haskell? Are you single?](https://np.reddit.com/r/nocontext/comments/3efbv6/dota_and_haskell_are_you_single/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger/wiki/) ^/ ^[Contact](/message/compose/?to=\/r\/TotesMessenger))* [](#bot)
It's still an invented layout, specific for these purposes. I've been following /u/chrisdone's lead and avoiding that recently. But yes, that is a less operator heavy version.
Naaah, we are handling feature creep just fine. Maintainability goes up every commit. haskell-mode will be fine.
The last commit to ghc-7.10 that matters is https://github.com/ghc/ghc/commit/5560861e4562a2e270e6610e6ef3cf120253bb16 which landed on 30 June. Any GHC 7.10 snapshot built after that date should be usable.
Well, as long as I can still create my HTML and SVG as HTML and SVG and then set some CSS classes dynamically with elm, it should be an option as well ‚Äì I don't care about how to create HTML with it but that seems to be an important part of it as its the very first thing its website shows me. Is there a site comparing it with Haskell and / or PureScript? I kind of like lazy evaluation, though if neither PureScript or elm do that, I guess I'll have to compare some other things... Which a comparison page would of course simplify quite a bit \^\^
Thanks for looking that up.
&gt; a sign that you really are using a type that's "too big" for your needs. This is the key observation.
Well done! Looks like very idiomatic Haskell to me. Some small points: buildScene :: SceneDesc -&gt; IO(Scene) Could be: buildScene :: SceneDesc -&gt; IO Scene The files in the repo could use a little more structure. See the `stack new` command, or [holy project](https://github.com/yogsototh/holy-project). README would be nice, TODOs can go in their own file. For saving the picture you could consider JuicyPixels. Sometimes you use more parens then needed, lke `case (aspect&gt;1) of1` or `normalize $ (o - (Vec 0 0 (-2)))`. [HLint](http://community.haskell.org/~ndm/hlint/) will probably suggest all these changes. Finally I think your code would run faster by enforcing some strictness and inlineing/unboxing some stuff. I'm not a hero with this, so I hope someone else can give you feedback on that. 
I tried playing with this and I absolutely love it. It just works.
Thanks for th√© advice! Any suggestions for reading about making the fields stricts? I am not familiar with this concept...
Code looks very good for a someone learning Haskell for just 2 month IMHO! Your style still uses a bit more parenthesis than necessary (comes with the C/C++ background, I suppose). Regarding performance: you should mark record fields as strict using the bang (!) annotation; this allows GHC to do unboxing which really improves performance for numerical code. Also, your are using a parallel map in `generatePixels` but you have to be careful with the granularity of the computations, or this can actually decrease performance. Use `parListChunk` strategies and gather the parallel tasks into bigger chunks, e.g. one spark per image row: ``` generatePixels w h f = Image w h (map (\i -&gt; f (fromIntegral (i`div`w), fromIntegral (i`mod`w))) [0..(w*h)] `using` parListChunk w rdeepseq ) ``` These two changes improved performance on my relatively old desktop from 25s to around 6s. 
It shows up in the release page and you can download it. I'm building it for alpine linux right now.
Just tested compiling in 7.10.1, worked with minor modifications: - add some dependencies to cabal build-depends: base &gt;=4.7, bytestring -any, parallel -any, vector -any, deepseq-generics -any, split -any - fixed typo "refr" in raytracer.hs "where transmitedRadiance ... (refract (direction refr)" - Modified Color.hs: {-# LANGUAGE DeriveGeneric #-} import GHC.Generics data Color = RGB Double Double Double deriving (Show, Generic) 
My bad and thanks for the feedbacl, I should have taken the time to clean this before posting :), will do asap!
Thanks, I will have a look into HLint and stack new.
1/ -XOverloadedStrings - no run time errors 2/ I think that means you don't like this kind of music, better walk away
This is really fresh: https://www.youtube.com/watch?v=_pDUq0nNjhI But there are many sources.
From an amateur sound designer's perspective, I appreciate the effort and certainly see how cool this is. You should have probably explained the background somewhere, like given references or something, because it's not obvious RP someone who doesn't know your thinking to understand what the point of this is. From a programmer's point of view, you should avoid strings and use type constructors instead. ``OverloadedStrings`` has nothing to do with the fact that if you misspell a word (a sound name), then your program will crash instead of telling you at compile time that you made a mistake.
My only comment is to avoid using Unicode in your source files. Yes, it's supported, but it's unnecessarily annoying to type and often difficult to read. cos0 is just as understandable as using a theta.
He also said that he tried to eliminate the need for Events but was unable to.
Thanks, fixed. (Just in case: if you see any more typos, you can also report them by selecting them and pressing Ctrl-Enter. I should probably -make the line in the header which says that- more prominent.) (Also, /u/ArtyomKazak is the old nick, /u/peargreen is the new nick. I guess you got it from the first lens over tea post here on Reddit?)
You'll want to do client side rendering with Elm if that's what your asking. I'd say the primary difference is that those other compile to js languages have more advanced language constructs, but that elm has more features and libraries specifically for creating websites. Here's a link to elm's website. http://elm-lang.org/
Oh, it was you. I posted there just not to bother you anymore. I don't even know how to thank you anymore... Edit: are you actively working on that kind of thing? Do you know more people who are too? Is there any school, group or something I can join? Edit: would you allow me just one more question? :( As you can see all I want is an efficient way to evaluate some lambda calculus terms such as that `div`. Those terms are strongly normalizing and use no fixed points, just folds, etc. I guess that's why most of them work with just the abstract algo. Those terms are listed on a site I'm making, the [Lambda Wiki](http://lambda.wiki). But just the abstract algo isn't enough due to redundant fans, the readback is way too slow. But, curiously, I've tried some unjustified rules such as backward fans annihilation (`a-DUP=DUP-b =&gt; a--b`) (between them are the auxiliary ports) and that makes `div` reduce **extremelly** fast, it gets rid of **all** redundant fans. But others terms are now incorrect. I'm under the impression I'm very close to some kind of perfect solution, as if there is some missing piece/rule/system that does what I want, but I just can't see it. Do you have any insight to me? What would you recommend me to do now? Edit: thinking about it I guess all I need is a way to remove redundant paired fans, not just safe unpaired fans as mentioned on the book. Maybe linking them and erasing pairs together? The article [20] established for lambda-terms (or pure proof-nets) the following result: the number of regular paths in a term M is finite iff this term is strongly normalizing I guess that means any term that is strongly normalizing can be reduced using the abstract algorithm? 
My cat really hated that :)
Is "disingenuous" the right word? A quick check of the `haskell-src-exts` issue tracker on Github suggests that there are problems with [constraint kinds](https://github.com/haskell-suite/haskell-src-exts/issues/201), [pattern synonyms](https://github.com/haskell-suite/haskell-src-exts/issues/194), and [role annotations](https://github.com/haskell-suite/haskell-src-exts/issues/187), just to pick from the first page and a half. Your code base might not use those, but that doesn't quite falsify the assertion. Perhaps "some" would be better than "many", but I didn't really take that sentence as snark, particularly since they note that they're hooking into GHC both for compatibility and access to additional type information.
Maybe some included Clojure projects are thin wrappers around mature Java libraries. So even commit count is low, bug count is even lower. Also github issues aren't only bugs. Don't know how that is taken into account. Probably bugs / feature requests / questions are in about the same ratio independently of the language
&gt; using monads to generalise several fp techniques and revolutionise IO for ever. Despite the popularity of Moggi's research, I find it difficult to say it "revolutionized" IO. The `IO` monad in Haskell is glorified continuation passing. And the story for combining effects in the language is still pretty weak. On the other hand, I would agree that CT has done good things for uncovering applicatives. And free monads are an interesting programming pattern. &gt; I argue strongly that in fp, it's more important than in mathematics I would like to hear more support for this sentiment. I find the opposite to be true. Perhaps it has more to do with the particular branch of math you're interested. If you asked an algebraic geometer or homotopy theorist, you'd be told off. On the other hand, you wouldn't need categories for most problems in combinatorics. And I imagine computational fields wouldn't need them much either. &gt; but an undergraduate mathematician will struggle to find you more than 5 This is only because Haskellers focus on the vocabulary. An undergraduate math student has surely seen several without them ever being named as such. The image of a set, inverse image of a set, the powerset functor, many will see the fundamental group functor, the free group construction, formal linear combinations, the Galois correspondence, covering spaces, a host of forgetful functors (always used implicitly). If you went back 10 years and asked an MLer or a Lisper about functors, you'd get the same response. To an MLer before the rise of Haskell, a list was a list. An option was an option. You wouldn't even think of them as two flavors of the same idea because you don't need them for effect propagation and because you generally wouldn't need to write code parametrized over both. &gt; I think it's elitist to deny the subject to the general population. If you find my opinions elitist, that is unfortunate. But if you check my history on r/math, you would see it is hardly my aim to deny people understanding of anything. I spend a lot of time in the \#\#math channel on Freenode as well, and every week, we get a new soul asking confused questions about the relationship between types and categories. They read about category theory from a Haskell blog and the enthusiasm makes them excited to learn more. But there's no good "story" for learning about categories through functional programming. What language do we use as a basis for discussion? People here will use "Hask", but even ignoring issues of well-definedness, what useful things can I really say about Hask? Jumping into a discussion of functors is less pointless, because there are several useful examples. But still, once I have spelled out the definition and the basic examples, where do I go from there? One of the big uses of functors in mathematics is the yoneda embedding, and the idea that you can take a "shitty" category and embed it into a very nice category of functors. But the yoneda embedding doesn't quite work in Hask like it does in Set, and to give a solid intuition, you would need to have developed the idea of limits and colimits first. But I don't have solutions. I just want to raise awareness of a problem I see with the way category theory is taught by the Haskell community. I know in my own experience, I was just as confused as anyone who comes to \#\#math, and I found myself frustrated for almost a year trying to make sense of the subject. And ultimately, I came to feel that the importance of category theory in functional programming was simply out of proportion with the hype.
My suspicion about Clojure matches yours, that most are wrappers. Also I filtered only issues marked as bugs, not features. 
Haste might not check GHC's version, but 7.10 broke compatibility in a major way that affects Haste (the Monad-Applicative stuff). There is an issue about in [on the repo](https://github.com/valderman/haste-compiler/issues/304). I don't care about prebuilt binaries, I use the packages from the arch-haskell repo, cabal for packages that aren't on there.
If anyone is interested, the collected data is here: https://github.com/steveshogren/github-data/tree/master/cacheddata
Thanks a lot! I will definetely run HLint more often! 
Hey man, thanks for posting this -- I never really have an opportunity to use Haskell, so seeing somebody's "learning" Haskell project along with feedback on here is really handy for understanding.
I don't know that FRPNow is continuous in the sense that Conal describes. Something like `Behavior Temperature`pretty much has to poll at some sampling rate, either implicit or passed in as a `Behavior Time` argument, but then you have to decide in advance what your sensor resolution is. That's still better than systems where all signals are sampled at the same fixed rate, though.
I'm definitely not a Haskell expert, so my apologies if it is incorrect. If Haskell can guarantee evaluation using features, then it would be 0. I'll try to update it today. BTW, what are said features called? I'd like to read more about them.
Yeah, even clojurescript libraries often end up being thin wrappers.
Added a link to this blog post to the [Haskell Wiki page on Stack](https://wiki.haskell.org/Stack).
I'm glad it helps. The biggest part of the learning for me was doing the exercises of the Introduction to Haskell course. It has a good learning curve and "forces " you to use monads and so on... I definetely recommend taking the course if you feel motivated to learn.
You couldn't apply `head`, etc directly but you could bind it to get `Behavior [a] -&gt; Behavior a`, right? It's conceptually the same while remaining in the domain of behaviors.
&gt; I used the GitHub API to query for the number of bugs created in repositories with more than 15 forks created in a span from 2011 to 2015. While even [my most popular repository](https://github.com/gelisam/hawk/) only has 12 forks and thus doesn't contribute to the statistics, I should mention that I like to use GitHub issues to discuss new features, not just bugs, so this might not be a very good metric.
XPost Subreddit Link: /r/idris Original post: https://www.reddit.com/r/Idris/comments/3ebtet/david_christiansen_coding_for_types_the_universe/
IIRC, the sense of "continuous" in question is one of denotation. Of course the eventual user of the library will need to sample at some rate, but the point of continuity is that _any_ sampling policy should work.
Why so negative? Exploring musical patterns with algorithms is a pretty exciting thing. Op is not in bad company, as Paul Hudak developed Haskore, a language for composing music. Yes, parsing strings at run-time is ugly when you can have compile time type safety. But Tidal may have been developed by people coming from a musical background who have other priorities. Why discourage these people?
I think the part that people often forget is that in order for the code to be correct, it has to be easy to reason about. While Haskell prevents many kinds of bugs with its type system, that same type system often necessitates convoluted solutions. So, while the code may end up being self-consistent, it doesn't necessarily do what was intended. I think the key to writing robust code is in being able to express intent clearly. Here's a great real world example from core.async where the pure functional approach results in code that's more difficult to reason about and maintain: &gt;[When I first wrote the core.async go macro I based it on the state monad. It seemed like a good idea; keep everything purely functional. However, over time I've realized that this actually introduces a lot of incidental complexity. And let me explain that thought.](https://groups.google.com/forum/#!topic/clojure/wccacRJIXvg) &gt;[What are we concerned about when we use the state monad, we are shunning mutability. Where do the problems surface with mutability? Mostly around backtracking (getting old data or getting back to an old state), and concurrency.](https://groups.google.com/forum/#!topic/clojure/wccacRJIXvg) &gt;[In the go macro transformation, I never need old state, and the transformer isn't concurrent. So what's the point? Recently I did an experiment that ripped out the state monad and replaced it with mutable lists and lots of atoms. The end result was code that was about 1/3rd the size of the original code, and much more readable.](https://groups.google.com/forum/#!topic/clojure/wccacRJIXvg) &gt;[So more and more, I'm trying to see mutability through those eyes: I should reach for immutable data first, but if that makes the code less readable and harder to reason about, why am I using it?](https://groups.google.com/forum/#!topic/clojure/wccacRJIXvg)
Provided all the work done is open sourced, I don't see how the downsides are any worse than those with some random individual at the helm. 
Yep that's why I'm filtering and counting only issues with the bug label.
Newman's lemma states that if a rewriting system is strongly normalizing (SN) and WCR, then it is CR. The lemma is helpful because it's often easier to prove SN and WCR than it is to prove CR directly. (This is similar to how we often use the critical pair lemma to prove SN from the easier proofs that something is weakly normalizing and has no critical pairs.) However, if we have a general fixpoint combinator, then we don't have SN, so even if we do have WCR there's no guarantee that we have CR. You may find [these notes](http://homes.soic.indiana.edu/classes/spring2015/csci/b522-jsiek/2015-02-10.tar.gz) helpful for seeing how these various notions fit together. As discussed in those slides, the terminology in this area is terrible, since the area was independently rediscovered by numerous different fields and each field made up their own (conflicting) names for things. &gt; How is `{œÄ_i : e ‚Üù* f_i | i‚ààI}` supposed to be read? It's a set/family of paths `œÄ_i` where the path `œÄ_i` shows how we get from `e` to `f_i`. Aka, `œÄ_i` is a proof that `e ‚Üù* f_i`, where "`‚Üù`" is the reduction relation and "`‚Üù*`" is the reflexive transitive closure thereof. That is, whenever `e ‚Üù* f_i` is the case, we know that we can get from `e` to `f_i` in finitely many steps; and the specific proof of this fact (i.e., `œÄ_i`) tells us which steps we take and in what order. In the standard presentation people choose `I=2` and thus we have that `{œÄ_i | i‚àà2} = {œÄ_0, œÄ_1}` where `œÄ_0 : e ‚Üù* f_0` and `œÄ_1 : e ‚Üù* f_1`. Similarly, we have that `{œÜ_i | i‚àà2} = {œÜ_0, œÜ_1}` where `œÜ_0 : f_0 ‚Üù* g` and `œÜ_1 : f_1 ‚Üù* g`. Thus, the standard presentation simplifies to `‚àÄe, f_0, f_1, œÄ_0 : e ‚Üù* f_0, œÄ_1 : e ‚Üù* f_1. ‚àÉg, œÜ_0 : f_0 ‚Üù* g, œÜ_1 : f_1 ‚Üù* g`. I gave the more general formulation because that's the formulation I use in my own work as it nicely avoids the need to binarize everything &gt; What does "value" mean in this context? "Value" means a normal form (or head-normal, or weak-head-normal..., depending on the exact semantics of the language in question). Generally, it's defined to be any expression solely constructed by intro forms (aka data constructors).
&gt; I'll probably make it flashing and/or moving, then. Maybe can even manage rainbow-like changing colors. Aarrrgghhhh!! Hehe. :) 
The trick is to use lazy evalution to "pretend" you have a structure with all of your answers filled in. You do this explicitly in imperative languages and implicitly in Haskell. To see how, google "haskell memoization". (I also wrote a post about it if you're interested http://lukahorvat.github.io/programming/2014/11/18/haskell-memoization/)
I made my suggestion from experience of doing a significant amount of benchmarking on a similar raytracer, although I did not benchmark the change from add/removing every INLINE. It would be wise for oilandrust to heed your advice probably.
Also I think we would all be very interested to hear the specific benchmarks you got after doing the suggestions.
Brb, gonna go rewrite all my code
What beta-eta equations are you talking about? For a term `t` which is a sum type, it should be the case that both `t` and `case t of left x -&gt; left x | right y -&gt; right y` are denotationally equivalent, which is the eta-equation I expect sums to satisfy. Or by eta do you mean something more general like both `h t` and `case t of left x -&gt; h (left x) | right y -&gt; h (right y)` should be denotatinally equivalent, something that doesn't hold hold for domain semantics unless `h` is strict?
Except, Clojure doesn't make it difficult to write monadic code. The point the author makes is that the monadic solution doesn't add any conceptual value. There aren'y any problems addressed by using monads in that scenario.
I agree with your understanding of continuity; I don't see why a fully continuous system would have trouble with `whenJust`.
The basic observation is twofold, both parts falling out of the fact that Haskell is specifically call-by-need: * Haskell doesn't evaluate values until they are used, so defining a large structure (say, an infinite list of fibonacci numbers) doesn't do any work up front. * Haskell keeps values around until they are no longer usable, so looking up the same value a second time is fast. This leads to a programming style in which, in contrast to the imperative idiom of iteratively updating a structure, you iteratively define a structure in terms of itself; as long as your implicit dependencies bottom out, your computation will terminate.
Stats classes are really neat--you should definitely take one if you get the chance. The reason not to connect the dots sometimes is a pretty good one, I think. The lines emphasize the changes from one neighboring point to the next, which helps a lot when you have an x axis with a meaningful order--variables like time or distance. When you have categories that can go in any order, though, the lines don't show anything meaningful anymore and can even mislead by showing a trend that depends only on the order you pick for your categories on the horizontal axis. 
I don't nitpick that kind of thing usually but in the context of this sentence I had to :) You're*
On the contrary, Haskell is one of the few languages that has *actual* variables :)
Crucially, it would be assuming that those factors aren't correlated with *language choice on GitHub*. But code on GitHub is inherently public and social! People posting to GitHub in a given language are extremely self-selected. I don't think it would be easy‚Äîor even necessarily possible‚Äîto eliminate the noise and spurious correlations created by social dynamics rather than the languages themselves.
As Darwin226 already mentioned, the "trick" is to define a large data structure mapping your inputs to outputs. Since the data structure is lazy, it gets computed as you use your function exactly like normal memoization. Conal Elliott summarized it well: in Haskell, functions are not memoized but values are. So what you do to memoize a function is turn it into a value‚Äîwhich works because it's lazy. In a sense, all you're doing is creating a large map that contains exactly the same information as the function itself and leaving all the memoization logic up to the Haskell runtime. (Which is great because then you can't mess it up yourself‚Äîconsider how doing mutation would be tricky in a multithread context, for example!) Conal wrote the [MemoTrie](https://hackage.haskell.org/package/MemoTrie) package to make this systematic, and a [blog post](http://conal.net/blog/posts/elegant-memoization-with-functional-memo-tries) about how it works. For a more basic introduction, take a look at a [blog post](http://jelv.is/blog/Lazy-Dynamic-Programming) that I wrote myself. It has pictures! (Seriously, I think the illustrations can really help you understand.) It focuses specifically on how you'd implement classical dynamic programming algorithms, but the big idea is just as applicable to normal memoization.
The Pierce endorsement is huge. I'm totally getting this. 
Aye, this is not about producing software, but using code as a UI. No software is produced, code is written then deleted during a performance. There's a bit of info here http://dl.acm.org/citation.cfm?id=2633647&amp;dl=ACM&amp;coll=DL&amp;CFID=696269403&amp;CFTOKEN=10981852 preprint: https://raw.githubusercontent.com/yaxu/Tidal/master/doc/farm/farm.pdf
&gt; There is no special syntax for applicative functors because it is hardly necessary. Sure, until you want the argument order and effect order to differ, for example... I hope for ApplicativeDo.
The 7.10.2 release has been cut (i.e. it's tagged in the Git repo, and bindists are being prepared and tested), the official announcment will follow shortly as soon as we have all bindists in place. Obviously, early adopters (i.e. those who know what they're doing and know where to download the release...) are free to install GHC 7.10.2 before the official announcement... :-)
You mean that there isn't an higher level library providing box plots? Hard to believe. A quick search on Google gives me this https://github.com/timbod7/haskell-chart/wiki, which seems doing what you want
Books with weird layouts or mathematical typesetting tend to work badly as eBooks. PDFs are sometimes OK, but not well optimised for screen reading. The ‚ÄúLittle X‚Äù series hit the 'weird layout' button square on.
Great talk! Using "simply typed" universes like the ones in the video (where the "child" type, for lack of a better term, is simply `REAL`, `INTEGER`, or `STRING`) is cool, but I always get tripped up when I try to extend my universe to do something more interesting -- i.e., have polymorphism or dependent types in my child universe, and still expressing (to the point of validation) all of that in the host type system. Is there any information available about how to do that?
Oooo. I‚Äôd pre-ordered this on Amazon. Excitement mounts :) Humph. Just discovered that there‚Äôs a 30% discount if you order directly from MITPress, but that will incur transatlantci shipping charges, so it‚Äôs no good to me!
That hasn't been my experience so far, except for a few books that for whatever reason tried to reproduce the page layout of an actual book perfectly (Pierce's TaPL was horribly screwed up by this). I can see where that could be at times uncomfortable on a mobile device, though it usually seems to work well enough for my taste, but I'm mostly reading CS books on my desktop nowadays anyway.
&gt; However, I tried that, and I can tell you it‚Äôs way too complicated. You end up with very long types that make things barely unreadable. &gt; https://github.com/phaazon/igl/blob/master/src/Graphics/Rendering/IGL/Buffer.hs#L114 Meh, I've seen scarier type signatures than this. Also, I think you could try using type-level lists instead of type classes, but unfortunately GHC can't solve complicated constraints using these. Perhaps something like the type-level natural number solver could be developed, but as far as I know no one got around to implement it.
&gt; Document your bind function. I'd like to expand this: we should have `Monad`, `Functor`, `Applicative`, `Alternative`, `Monoid` instances documented for all types in base. (Or am I really the only one who constantly forgets what `&lt;|&gt;` and `&lt;&gt;` do on maybes?) What's the easiest way to make a pull request against base?
The simple way to fix this is to allow comments on Hackage. 
But how can you document these things? Does haddock allow documenting instance methods?
No, but Haddock does allow documenting instances, and most classes don't have more than a couple of methods. For an example of horrible documentation, look at `Monoid (Maybe a)`: &gt; Lift a semigroup into Maybe forming a Monoid according to http://en.wikipedia.org/wiki/Monoid: "Any semigroup S may be turned into a monoid simply by adjoining an element e not in S and defining e\*e = e and e\*s = s = s\*e for all s ‚àà S." Since there is no "Semigroup" typeclass providing just mappend, we use Monoid instead. (In all fairness, tho, I guess it was supposed to be just a note.)
Some of the arguments hinging on beginners are pretty dumb. As a beginner (it wasn't so long ago) I would never have groked monads without seeing them in do notation first. Why? Because do notation is much, much more intuitive than trying to work out why `m a -&gt; (a -&gt; m b) -&gt; m b` is so great. Once I had a handle on writing stuff in in fairly intuitive monads like `IO` and `Maybe` in do notation, I was able to get more of a handle weirder ones like list. I started understanding why `do line &lt;- getLine; return line` was redundant. Then more insights followed such as how to use `=&lt;&lt;`, `&lt;$&gt;`, `&lt;*&gt;` and how they all relate to each other. Even now, do notation is often much more readable. There is nothing wrong with writing a multi-line version of a function with named intermediaries even if a one-line version was possible.
TaPL is one of the most beautifully typeset pdfs I've seen. It's a pleasure to read.
This is really nice. I use the same combination of QuickCheck + servant to provide arithmetic exercises to my daughter. It is not done yet (verification of answers and logging missing) but you can check it out here: https://github.com/ggreif/leleka
If you use `:i` on `Maybe` when `Control.Applicative` and `Data.Monoid` are imported, two of the results you get are: instance Alternative Maybe instance Monoid a =&gt; Monoid (Maybe a) The former knows nothing about the values inside the `Maybe`, so it cannot combine them. That's how I remember it just takes the first `Just`. The latter has a recursive `Monoid` constraint, and it wouldn't have it if it were superfluous. So that means it combines the values inside two `Just`s. Also: (&lt;|&gt;) :: Alternative f =&gt; f a -&gt; f a -&gt; f a Note the `a` is `forall`d in this method, and has no constraints. Thus there's no way to constraint the `a` of any alternative value, meaning it never does any sort of combination on the values themselves (can only combine the "shapes" of the Alternative type itself).
Yep, I have to compile it from source anyway so bindists don't do much for me. Also I just used your 7.10.2 deb for cross compiling as well. But if anyone wants to try out 7.10.2 in docker: docker run --rm -i -t mitchty/alpine-ghc:latest And you can get the latest version I compiled and am working on getting upstream.
I disagree - in PureScript's Pursuit, we purposefully left out the ability to document instance methods, because we decided that documenting interesting instances is best done on the data type itself.
Depends on the e-reader. I read TaPL on an openinkpot-powered device, and it was quite ok. Now I have a Kindle, and reading almost any pdf there is very inconvenient.
&gt; meaning it never does any sort of combination on the values themselves (can only combine the "shapes" of the Alternative type itself). Yes, which means that I still don't know whether it'll be the left or the right value and have to test (and I do test, every single time). --- &gt; The latter has a recursive Monoid constraint, and it wouldn't have it if it were superfluous. So that means it combines the values inside two Justs. There's a basic rule of usability: [don't make the user think](http://www.sensible.com/dmmt.html). In a nutshell, if a user has to do some clicks to get to the goal, 3 straightforward, mindless clicks are better than 1 click that requires the user to stop and think. Thus, ‚Äúbut it's so easy to deduce from this and this rule which should be obvious to you if you have spent some time using Haskell‚Äù is particularly bad justification for not writing Just a &lt;&gt; Just b = Just (a &lt;&gt; b) Nothing &lt;&gt; Just x = Just x &lt;&gt; Nothing = Just x in the docs, especially since it's already obscurely written as ‚Äúe\*e = e and e\*s = s = s\*e‚Äù. Same goes for everything else.
No currently there's no paradox, there's no way to say `type : type` or something. Further, it should be the case that `Set`/`U` is closed under arbitrary sums and products. Evidence for this fact is that we can interpret `U` straight into Agda's own types without complaint.
Probably the usual?
Chart is not what I call a high level library (or not near as R ggplot is).
[Yes you can](https://hackage.haskell.org/package/rainbow-0.26.0.6/docs/Rainbow.html#g:1)
That's an option, but I'd hardly consider that a good solution.
Do doesn't really jump out at you as a function of a specific monad
So I spent some time today to apply your suggestions, here are the rough benchmark that I did (using the shell time command), compiling with the flags ghc -O2 -threaded -rtsopts I ran the test on my macbook pro, 3Ghz quad core i7, all rendering were 1024*1024 with recursion of depth 5. The cornell box scene has the yellow torus with 576 triangles and the outside scene as the yellow torus and a textured torus with 1152 triangles. * Before optimization: cornell box: 17s out scene: 36s * Strict types + -funbox-strict-fields : cornell box: 12s out scene: 23s * Inlining cornell box: 10s out Scene: 20s I tried few combination of inlining, inlining first only primitives such as vector operations and intersection tests and then inlining more high level functions. I couldn't notice significant changes, at best in the order of half a second. Overall the strictness had a bigger impact. I also tried to change the granularity of the parallelism (per pixel vs per row) as suggested by pbvas but couldn't notice significant differences. Note that I haven't implemented the kdtree yet, I only test the ray against the bounding box of the meshes so pixels that hit the box take a lot more time that pixel who doesnt since they are going through the whole list of triangles. Overall I using strict types gave a good speed up. Inlining was not as clear and maybe it is because ghc is doing some inlining by default? Probably there is room for further improvements. I plan to implement the KDTree next and then add texturing from images. Next I will look to throw several rays per pixel and I guess performances will become even more important for this. I think that once I have all the features, it will be a good time to do a second batch of optimization but overall I am quite satisfied of the performance for such a high level language, it is good enough to get quasi instant feedback while developing.
Currying means that &gt; grabRange l u = filter (\x -&gt; l &lt; x &amp;&amp; x &lt; u) is the same as &gt; grabRange l u xs = filter (\x -&gt; l &lt; x &amp;&amp; x &lt; u) xs because if you supply only one parameter to &gt; filter :: (a-&gt;Bool) -&gt; [a] -&gt; [a] like &gt; filter cond you supply the (a-&gt;Bool) argument in (a-&gt;Bool) -&gt; ([a] -&gt; [a]) and are left with &gt; :t filter cond &gt; [a] -&gt; [a] So when you prepend 2 of your own parameters you end up with the signature of grabRange. 
Ok, but how does it determine x?
The first uses eta reduction: https://wiki.haskell.org/Eta_conversion. As for the second question, the expression supplied to filter is a lambda function, indistinguishable from f x = l &lt; x &amp;&amp; x &lt; u The x, is not created by f as much as assumed to be provided. 
Yeah, most languages don't have variables, they have mutable locations/cells.
I'm not against documenting it. For me, finding the docs requires more thought (or at least, more distractions) than `:i` in my interactive shell. And indeed, I tend to just remember that when there's an unnamed bias, it tends to be towards the first one, and not the last one.
See my comment below
Of course not, it just indicates that the following block is using different syntax to make monadic code a bit easier to write.
Aren't you afraid it will become a terrible mess of security vulnerabilities and ill advised tips like the PHP one ?
&gt; Isn't that what the do is for? I agree. Just because it is about monads doesn't make it more magic than pattern synonyms or whatever syntactic sugar Haskell has. This kind of discussion usually turns into useless but heated bike shedding.
`x` is provided by `filter` ([it's definition](https://hackage.haskell.org/package/base-4.8.1.0/docs/src/GHC.List.html#filter))
I'm kind of confused about what you're specifically referring to -- I'm going to go through my understanding and maybe you can help me understand? main = do string &lt;- getLine let yelled = map toUpper string putStrLn yelled desugars to main = getLine &gt;&gt;= \string -&gt; let yelled = map toUpper string in putStrLn yelled and for a Maybe monad its someFn maybeString maybeInt = do string &lt;- maybeString int &lt;- maybeInt return (int == length string) which desugars to someFn maybeString maybeInt = maybeString &gt;&gt;= \string -&gt; maybeInt &gt;&gt;= \int -&gt; return (int == length string) The desugaring is all about the `Monad` type class function `&gt;&gt;=`, not specific to any individual monad. Are you wanting the desugaring step to somehow differentiate between the `&gt;&gt;=` for `State s a` and `Maybe a`? Or for there to be some reasonably easy way to see what `&gt;&gt;=` is defined for `State` vs `Maybe`?
Thanks for the kind words! Later on in the talk, there's a universe that contains dependent pairs - that's the binary formats example that I took from _Power of Pi_. It's what lets the universe specify things like "read a byte, convert it to a nat, then read that many more bytes". Are you looking for hands-on examples, or academic papers that push the limits of what can be expressed that way? In any case, _Power of Pi_ is really well-written and has good examples.
Precisely what I'm talking about.
I'm not using any epaper devices. If I get this, I'm most likely going to read it on my desktop PC.
No, I just want it to be clear when you look at a piece of code that the code is being transformed by code specific to the relevant monad. 
The use of currying in your example simply let's the programmer avoid naming the omitted parameters. They still exist (as parameters to the function returned, but it gets optimized down to the same thing as declaring them explicitly), and if you're choosing to declare the full type of `grabRange` then you still have to include them. The `x` comes from the lambda. `filter` has the definition `(a -&gt; Bool) -&gt; [a]` (correct me if I'm wrong, I'm on a phone and my Haskell is rusty). The first part of that is asking for a 1-argument function that returns a Boolean. The `\\ x -&gt; ...` simply says I'm providing this one argument function, name that one argument x.
What proof language does it use?
Like the mess that is /r/haskell, haskell-cafe or #haskell on freenode? No, I am not too worried that an invasion of alien php programmers will destroy the purity of hackage.
That's what `do` _always_ means in haskell. Some folk have referred to monads as the reprogrammable semicolon. Admittedly the layout rule means you don't actually need any semicolons and you rarely see them in the wild. I always want to see and think about the types before reading the code, as I find that's where a lot of information is. The types are the clue to what the do notation actually means each time. Haskell is definitely a think more, write less kind of language, so like mathematics, it's very difficult to both read fast and understand at the same time. 
You have to put the voucher code in to the order form.
Should we have a monthly/quarterly hiring thread perhaps stickied perhaps? This is inspired by /r/elixir - https://www.reddit.com/r/elixir/comments/3c0867/relixirs_q3_2015_hiring_thread/
I believe that having misguided advice on reddit, irc or a mailing list is significantly less harmful than having it on the official documentation of a package.
I see your point, but when I go to Hackage I expect to find canon, word-of-God from the developers, and not a blooming garden of possibly conflicting advice (as expected in /r/haskell or other discussion platforms). Also, I don't think comments would interact smoothly with having multiple sets of Haddock pages available (e.g. Hackage vs. Stackage vs. project site), or even multiple package versions. 
btw, since you liked abstract refinement types, I suggest you look at our new paper which extends it in a rather interesting way: http://arxiv.org/abs/1507.00385
Not the OP, but as another person with Opaleye in production, my experience has been great! The setup is a tiny bit verbose (there is some TH floating around for that, though I haven't used it), and you end up having to write a few more type sigs than you want, but the composability / type safety is really really nice. You get most of the power of SQL (and really most, not most like ORMs tell you they given you) but you can actually re-use parts of queries, and you get type errors if you mess stuff up.
I'll take a look, thanks!
In documentation that might work, but in code it would defeat the purpose of `Monad`.
I was following along, getting lulled into feeling like there was nothing amazing going on cause everything seemed pretty straightforward, when suddenly item : (b : Bool) -&gt; if b then Nat else List Nat item True = 42 item False = [1,2,3] Dang, shit just got real.
Is the actual talk available?
That's not Currying, it's eta conversion. Currying is taking grabRange' from `(a,a) -&gt; [a] -&gt; [a]` to `a -&gt; a -&gt; [a] -&gt; [a]`. So grabRange is already in curried form. Currying *allows for* eta conversion, but they are not the same thing.
Your complaint is about [ad-hoc polymorphism](https://en.wikipedia.org/wiki/Ad_hoc_polymorphism), not about `do` or about monads.
My complaint is purely superficial 
Definitely a typo
Sheesh, all this time I thought this was Currying. A book like LYAH, or even Real World Haskell doesn't make the distinction clearly enough it seems. I get it, though. It was just one of those typical things you didn't know you weren't knowing.
You can partially apply types in Haskell, though there are certain restrictions on type synonyms. (You should view type synonyms as a sort of lightweight, low-power macro system -- so you're not allowed to do anything with them that looks like you couldn't do it without them.) But even so, this is definitely just a typo.
I actually found this particular piece of documentation very useful.
Unknown unknowns are the basically the source of all of life's problems, when you think about it. They're certainly the source of virtually all bugs.
I'm writing a set of data structures in C, and creating bindings to haskell so I can test them with QuickCheck, which I really like as a testing framework. A lot of the APIs that operate on the C structures are mutators, so they're all in the IO monad, and I'm already using monadic QuickCheck to test them. What I want to do though, is to have a PropertyM that's parametrized over these data structures. For example, if I have a C data structure called CList, I need to make CList an instance of Arbitrary to do that. It's trivial to write an IO action that generats a random CList, but I would like to push that into the Arbitrary typeclass if possible so that I don't have to do stuff like "run genRandomCList" every time in my properties.
Hell yes!! Congratulations.
&gt; If it's too meta I apologise. Why, no, meta-discussions are awesome (but, yeah, I too can relate to wanting to apologise for them). &gt; If you dislike this part - creating new habits I don't ‚Äì ‚Äúokay, let's create a habit for \&lt;something\&gt; and the problem would be solved‚Äù is actually often the 1st solution that comes to mind when I'm presented with a problem. The trouble is that instilling habits also requires noticing (for instance, I made `ci` an alias for `cabal install`, and to train the habit of using it I have to notice when I *forget* to use it). For me, the noticing part is hard (especially considering that I invent new potentially-useful habits for myself to train pretty much every day, so there's a lot to be noticing). An obvious solution that comes to mind is to make a list of habits (sorted by priority) and review it every day, but I guess that if I did it, small habits like using aliases, `:i`, etc would never be created. And... maybe it's actually alright? Okay, I'm going to do just that and see whether it works or not.
I apologise, i skimmed through this post and got it wrong. Since it mentioned `Frames`, i thought that it was an implementation based upon very low level graphical primitives. Now i recall that `Frames` is a library for numerical calculation, and i can see that [the code of the box plot](https://github.com/rrottier/chart-boxplot/blob/master/Boxplot.hs) is actually quite high level, using primitives exposed by Charts. /u/rrottier the best recommendation i can give you is to try to contribute the code back. Maybe for some reason it will not make it in the library, but it will be a great way to have a chat with the library author, and learn something!
A heated bike shed would be great during the winter.
On the other hand, if we adopt a wiki-like culture (with meta-discussions), it will also attract more attention from people to get it right. 
Can the same approach be used to Servant.Docs module? So that we may not need to hand write those ToSample instances.
It could use a better layout, though. If the documentation text was below the instance declaration, there would be more space to document individual methods.
I do this a lot with unsafePerformIO, and have even defined an instance for Property on IO using unsafePerformIO. I don't find a lack of purity in tests to be any real problem. 
So, this is something I hesitated about when first writing *servant-docs*, glad you ask about that. The thing is, people will rarely have "nice `Arbitrary` instances", in the sense that you will usually receive a lot of garbage instead of "usernames", addresses and what not. As far as I can tell, this is not the type of "sample response" or request body people want to display in their API docs. Now, if enough folks are interested in what you're suggesting, we could add a module to *servant-docs* that would not require any `ToSample` instance from you, just `Arbitrary` or `Random` or whatever suits you, this is absolutely possible.
Two things comes to my mind: first, to introduce time in a expression without it , it is just a matter of adding the timestamp to the event and keep it in the state together with the event value. And second, the big problem of FRP is that it contemplates a single "scene". That is why it does not have a Monad instance. There are no different sections of the scene that interact among them. In a graphic game the single scene may be right, but to modelize widgets that interact among them in a GUI, or to modelize asynchronous inputs in a computation, it is necessary a monadic interaction where each subsection return something to the next section/ widget. In most of the cases only some sections of the "scene" affected by the event will change and thus only parts of the expression should be executed. That is not possible in the "single scene" model.
I like to call [this function `hush`](http://hackage.haskell.org/package/errors-2.0.0/docs/Control-Error-Util.html#v:hush)
Do use Tekmo's `errors` package; it is nice not having to define such little things all the time.
It looks good. A quick skim didn't turn up anything obviously in need of improvement. One suggestion I would make would be to change if | x == 0 -&gt; fromIntegral &lt;$&gt; return x | x == 1 -&gt; fromIntegral &lt;$&gt; getWord8 ... into case x of 0 -&gt; fromIntegral &lt;$&gt; return x ... Additionally you could pull `fromIntegral &lt;$&gt;` outside the `case` because `fromIntegral &lt;$&gt; empty == empty`.
I don't think that would work, because the types of the different `if` branches are different (`Word8`, `Word16`, etc). Thus need to do the type conversion inside the if branches.
The original definition as given by Quine is that a syntactic context `K{_}` is referentially transparent if and only if for all `M` and `N`, if `M` and `N` have the same meaning, then `K{M}` and `K{N}` have the same meaning. There are a number of *similar but actually very different* things that people also call "referential transparency": 1) Definiteness: in an expression like `x^2 + 2x`, each occurrence of `x` stands for the same value 2) Extensionality: if we want to know the value of `M` which contacts a subexpression `N`, the only thing about `N` that matters is its value 3) Determinacy: the value of an expression is the same wherever it occurs 4) Substitutivity: you can replace equals for equals 5) Unfoldability: if `f` is defined `f x = ...x...` then `f e` is equal to `...e...` 6) Purity: no side effects You can see a discussion of these [here](http://www.itu.dk/people/sestoft/papers/SondergaardSestoft1990.pdf), along with examples of programming languages that have one, but not the others. People make the mistake of thinking these are all the same thing but they're ***definitely not***. To make matters worse, if you add in denotational semantics, not just a normalization-based semantics, it gets *every messier* because the concept of "value" becomes very tricky. Additionally, notice that referential transparency is a property of *syntactic contexts*, but most people talk about it in terms of languages. Most people mean one of 1-6 when they say a PL is "referentially transparent". *EDIT* It's worth adding that in the strict Quinian sense, almost no PL, not even Haskell, is referentially transparent. Quotation marks of any sort create opacity, because what `x` means depends on whether its inside `"..."` or not. Similar for code quoting/quasiquoting, string interpolation, etc.
[**@Wollan**](https://twitter.com/Wollan): &gt;[2015-07-26 17:32:59 UTC](https://twitter.com/Wollan/status/625358162514837504) &gt;Reading 'Learn You a [#Haskell](https://twitter.com/search?q=%23Haskell) for Great Good' \(LYAH\) and I found a subtle reference to [#PSN](https://twitter.com/search?q=%23PSN) hack April 17 2011: [*pic.twitter.com*](http://pbs.twimg.com/media/CK2370WWcAA-k3V.jpg) [^[Imgur]](http://i.imgur.com/U9SL4Qf.jpg) ---- [^[Mistake?]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=http://reddit.com/3eoelt%0A%0APlease leave above link unaltered.) [^[Suggestion]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Suggestion) [^[FAQ]](http://np.reddit.com/r/TweetPoster/comments/13relk/) [^[Code]](https://github.com/buttscicles/TweetPoster) [^[Issues]](https://github.com/buttscicles/TweetPoster/issues) 
(For clarity, /u/Peaker is referring to the second suggestion, not the first) Good point. I don't see any way of removing the repeated `fromIntegral` then.
Out of curiosity, what IO do you do?
Thanks for taking the time to look over the code and reply! I gave the case expression approach a try but even though I liked removing the redundancy of the x ==, I didn't like that it wouldn't parse if I aligned the numbers vertically. For some reason, scanning the following quickly gives me the impression that it's oscillating. 0 -&gt; fromIntegral &lt;$&gt; return x 1 -&gt; fromIntegral &lt;$&gt; getWord8 -1 -&gt; fromIntegral &lt;$&gt; getNegInt16 2 -&gt; fromIntegral &lt;$&gt; getWord16le -2 -&gt; fromIntegral &lt;$&gt; getInt16le 3 -&gt; fromIntegral &lt;$&gt; getWord24le -3 -&gt; fromIntegral &lt;$&gt; getInt24le 4 -&gt; fromIntegral &lt;$&gt; getWord32le -4 -&gt; fromIntegral &lt;$&gt; getInt32le 6 -&gt; fromIntegral &lt;$&gt; return (x - 5) -6 -&gt; fromIntegral &lt;$&gt; return (x + 5) _ -&gt; empty
That's a very interesting observation and arguably a weakness of layout syntax! You *can* align the cases if you use `case x of { ... ; ... }` but it's fairly unusual for haskellers to use explicit braces like that.
There's some more explanation in the two answers to [this StackOverflow question](http://stackoverflow.com/q/2259926/1186208).
You could sort the numbers, then there would be no oscillation
D'oh! You're so right.
Hm. no packages on Hackage depend on `layers` (other than `acme-everything`). What are the downsides that prevent people from using it? Hmm, I also just noticed that apparently there has been further development of it on Github that hasn't been released to Hackage. Even a version bump to 1.0.0. Strange. 
Sorry, I should probably have been more clear about the libraries I have been using. As /u/maxgit mentioned I am trying to emulate a bit of what R ggplot does, mainly that consists of just wrapping the chart library functions with some defaults I like so that I can do a plot with a one line command rather than the 2/3 lines that the chart library requires. So most of it is fairly mundane. Box plots are a little more interesting because they actually do more than just plot the values. A box plot needs to group the data into the separate categories and then calculate the median as well as some quartiles. So it is not really a pure Chart like the rest of Tim Dockers library because it has these data manipulation and statistical functions and dependencies. As such it will probably not be appropriate for inclusion in the library. That being said I haven't actually spoken to the library author so that might still be worth doing. Any thoughts on the code itself?
Related discussions. [1](https://www.reddit.com/r/haskell/comments/1xnmiv/monad_layers_an_alternative_to_transformers/?), [2](https://www.reddit.com/r/haskell/comments/2ucazk/what_ever_happened_to_layers_package/?), [3](https://www.reddit.com/r/haskell/comments/1aazok/layers01_modular_type_class_machinery_for_monad/?).
Thanks for stepping in, I have updated my initial post to link to the libraries and make it more clear. Any thoughts about my code? 
2007 actually! https://wiki.haskell.org/index.php?title=Do_notation_considered_harmful&amp;oldid=16516
I remember the frustration of seeing code with &lt;- and thinking "oh that is like assignment in C#" and "return" and thinking "oh that is like return in c#" and then getting into a right old muddle! 
No, cognitive dissonance a feeling of discomfort or unease when you are harbouring contradictory beliefs. If you dismissed it and felt okay afterwards, then this isn't a case of cognitive dissonance.
If you're interested in learning `lens` these are the articles I read that built my understanding: * https://www.fpcomplete.com/school/to-infinity-and-beyond/pick-of-the-week/a-little-lens-starter-tutorial * http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html * https://www.fpcomplete.com/user/tel/lenses-from-scratch * http://blog.jakubarnold.cz/2014/07/14/lens-tutorial-introduction-part-1.html * http://artyom.me/#lens-over-tea
&gt; https://hackage.haskell.org/package/smallcheck-1.1.1/docs/Test-SmallCheck-Series.html#t:Serial This looks cool! I haven't heard of SmallCheck before but I'll try it out :) Thanks!
Ok maybe I am confusing it for stubbornness. Because rather than question my understanding of 'curry' when faced with this function, I assumed I was right and Haskell was just a bit weird.
Since you liked Conal's talk so much, you might like his actual denotational design paper (http://conal.net/papers/type-class-morphisms/type-class-morphisms-long.pdf).
I'm really looking forward to implementing something with Servant at some point. I'd love to generate RAML with JSON Schemas (or similar things) from my server source code :)
Haskell already provides functionality for most UNIX commands, typically in Data.List. Generally we just avoid all commands for the "builtin Unix" style stuff, and only shell out for compilers.
[J-Bob](https://github.com/the-little-prover/j-bob).
I had already made the decision to go with Conduit, so by artificial limitation, that package was not in my field of view.
Yeah this is basically what I'm doing now. I use run to setup a random instance of my data structure and then test on it.
For type theory, I really like the first chapter of *Homotopy Type Theory*. While the rest of the book assumes PhD-level mathematics, the first chapter only assumes basic mathematical maturity of the kind you have after calc 2. For model theory... *Principles of Model Checking* covers the modal logic aspect from a verification perspective, with Cave et al. 's *Fair Reactive Programming* for companion reading to the logic parts. Bart Jacobs' *Introduction to Coalgebra* and Kozen &amp; Silva's (very short) *Practical Coinduction* cover reasoning about long-running and concurrent programs. Barr &amp; Wells' *Category Theory for Computing Science* is as boring as it is thorough. It could be a good companion read to Bartos Milewszki's blog series on the topic. Runar somethingsomething has a Scala talk with "Reasonably Priced Monads" in the name. It's based on Swiersta's *Datatypes a la carte*. Gabriel Gonzalez (/u/Tekmo) also has some blog posts on the subject. I also liked the monad coproduct paper. Lindsay Kuper has a few papers about deterministic concurrency, but while they're interesting I haven't actually tried to use her results in the real world. /u/neelk's blog *Semantic Domain* and /u/pigworker's blog *Pig worker in a space* are both good for spiritual growth-type stuff. Still missing is a good treatment of linear logic, fault tolerance, distributed systems... This stuff seems less appealing to mathematicians, and as a result the stuff I've seen was less mathy, more disorganized, less insightful. E: I just wanted to reassure you that I picked those texts for their practical value to the working software engineer. Math *works*! It lets you develop better, more maintainable, more extensible code. Algebra and logic are particularly good at this, though I don't know much analysis and for all I know it could be another trove of software design insights. When I was a kid, I had a sandbox. I also had a chest full of random toys, many of them random implements for playing in the sandbox. Sometimes I'd see a shiny new toy at the store, begged my parents to buy it for me, and if I was lucky my chest would be one toy richer. That's kind of how I view math. The sandbox is software development, the math I know is the chest of toys. Getting new toys lets me build more complex and interesting sand castles, and it's also plainly more fun! 
The code seems fine. [The configuration part](https://github.com/rrottier/chart-boxplot/blob/master/Boxplot.hs#L29-L37) seems a bit redundant due to the repetition of `plot_candle`, but this is due to the library, and maybe to the problem of configuration objects in Haskell. I am not an expert in Haskell, but i tried to experiment with data vis in the past, and i am fascinated by how calculation and presentation logic tend to get coupled to each other in these cases
oh, man `Demand.Promise.!=` gives me some serious intellectual impedence coming from a C language family background. I kept reading it as "not equal to" then wondering why `promise_` was generating a value that made it resolve to `False`. But I bikeshed.
And not just the instances, we should document the typeclasses properly as well. I was just trying to figure out what `&lt;|&gt;` does, because I'm still learning and haven't learned about the `Alternative` typeclass yet (although I do know the `Functor`, `Monad`, `Monoid` and other typeclasses). So I looked up the docs in Hackage for `&lt;|&gt;` and for the `Alternative` typeclass. I was greeted with [this](http://i.imgur.com/0s1fRrA.png), which is completely useless. It's nice to know it's a monoid on Applicative Functors, but different monoids do different things. E.g. `(+)` vs `(*)`. Reading the next sentence didn't help me much, because I have no idea what "least solutions of the equations" means. From a quick google search it seems to be some mathematical thing, but I have no idea what it means and couldn't find any short explanation of it. The equations for `some` and `many` don't really help me understand them much, because they are recursive. I looked down at their definitions/comments, which only say "One or more." and "Zero or more", which just confuses me more. How does it decide if it's One/Zero or more when there is only 1 argument? And why does some `some` have "**One** or more" while `many` has "**Zero** or more"? The name "many" would imply (at least to me) that there would be more things than in "some" and yet that seems to not be the case from the definitions. When I finally look at `&lt;|&gt;`, I see nothing of use either. The documentation of `&lt;|&gt;` just says that it's an associative binary operation, which is nice, but it doesn't tell me what it does at all. So what can I expect when I run it on some random types? ----- Maybe I'm just overthinking it, but I don't get what the typeclass does at all.
&gt;only assumes basic mathematical maturity of the kind you have after calc 2. :(
inb4 lens
lens pipes conduit are all pretty general 
The key observation to me is that it's a shame that Haskell lacks any other macro language to express what I really want. I don't want chaining operators, I want layout to express this. All I have at the moment is `do` notation, that's really the reason I'm using `State`. I'm well aware it's too big, but in this case I'm willing to live with that for what it buys me.
Look at the types: (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c ($) :: (a -&gt; b) -&gt; a -&gt; b `(.)` is function composition, `($)` is function application. What might be confusing you is that you can turn `long . things . like $ this` into `long $ things $ like $ this`, because of the associativity of `($)`.
My intuition has been that (.) gets better optimization than ($) because sometimes the functions can be actually merged, but then again GHC is very smart and might be able to do that for ($) as well. Do you happen to know whether there's a real benefit in that regard?
Summoning /u/ersafanrend to tell his story of our compiler. :-)
Excellent talk by Mathieu, and its great to see it heard at Google. Perhaps a more precise title would be "Distributed programming in Haskell with Cloud Haskell", because there are multiple RTSs and libraries to do distributed Haskell programming.
Sorry man, you only managed `EQ`, not `LT`.
I'm afraid the GHC optimiser is basically just black magic to me.
You can consider both ($) and (.) to be syntax, in the sense that the optimizer sees through both.
*ahem* http://lambda-architecture.net/
Looking at the hackage [reverse dependencies](http://packdeps.haskellers.com/reverse), sorting in descending order, and using a bit of domain specific ~~black magic~~ knowledge to ignore domain specific and _especially_ common packages, I get these (_very_ loosely categorized): * Testing: QuickCheck, HUnit, hspec, test-framework, tasty, criterion * Parsing: parsec, attoparsec, binary, regex-posix * Printing: pretty * Interaction: process, cmdargs, optparse-applicative, async * Data Structures: unordered-containers, semigroups * Utilities: random, safe, exceptions, MissingH, either, bifunctors, comonad, free, profunctors On second thought, looking at the most popular packages to find obscure packages may have been a poor idea.
the libraries already mentioned (lens, pipes/conduit/machines, async) are all great examples. it's also worth mentioning that scalaz was designed to provide an approximation of many of the great features of haskell/prelude in a scala environment. other libraries that come to mind are mtl.
[There is a nice explanation on Stackoverflow about this.](https://stackoverflow.com/questions/940382/haskell-difference-between-dot-and-dollar-sign/1290727#1290727)
($) takes a function and a value and turns them into a value. (.) takes a function and a function and turns them into a function.
The ‚ÄúMonolithic framework style‚Äù is a relatively recent thing. Composable peices (using ‚Äúcontracts or whatever‚Äù) have been around for a long time. Lisp and Smalltalk systems are built that way. One of the lessons of Smalltalk is that the composable pieces cannot all be small‚Äîyou need big ones too, and it's hard to get them right. You end up growing oyur own large scale control structures. And one of the lessons of Lisp is that growing your own control structures and abstractions on an equal footing with the language's own very rapidly becomes unmanageable as systems and teams get to a certain size. And it isn't very big. And a lesson of both is that once you've spent a few years in one wonderland of abstract beauty learning the magic spells they use there it can be very, very difficult to get a job in another.
On ipad and can't test, but something along the lines of countWire init = hold . accumE ($) init . fmap (+1) &lt;|&gt; pure init
 Prelude&gt; fromEnum '.' - fromEnum '$' 10 
I'm building a system for customizable filtering, aggregation, and analysis in Ruby. I've mostly just imported ideas from Haskell and elevated them to the class level. A Ruby class can be considered as a function from `A` to `B` where `A` is the duck-type of the object used to instantiate it and `B` is the duck type of an instance of the class. With that in mind, you can now compose classes just like functions. [I wrote a gem for exactly this functionality.](https://github.com/parsonsmatt/beethoven) Now, the signature of `filter` is `(a -&gt; Bool) -&gt; [a] -&gt; [a]`. Partially applying the filter gives you `[a] -&gt; [a]`. Functions of the form `a -&gt; a` are endomorphisms and endomorphisms form a monoid via composition. I wrote a highly generic Filter class that can be composed with other Filters, and now it won't be too hard to get a UI setup where an end user can have arbitrary filters on arbitrary attributes. The signature of `foldr` is `(a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b`. If you partially apply the function and starting value, you get `[a] -&gt; b`. Which, hey, you can easily compose with those arbitrarily composing filters to reduce to a single value! And when you can reduce an arbitrary collection to a single value, you can now filter attributes on a collection where the attributes themselves represent a collection, arbitrarily deep. Probably the nicest part of all of this is that the parameters for the filtering/aggregation are easily serialized. Right now they're stored as JSON and Ruby classes are instantiated from it, but there's no reason the processing engine couldn't be done via Haskell, Elixir, Java, etc... I could honestly not have implemented this system in an OO paradigm. Just not smart enough. But designing it in FP terms made it not only feasible, but not even particularly difficult, and the end result is actually far more powerful than what I set out to build. Case in point: the system was originally built for analyzing a stream of records. A few Adapter classes later, and it's now just as useful for selecting a list of people eligible for a program. Highly general, composable, and functional code pays dividends for well after the initial feature is done.
Dot (`.`) composes two functions into a single function (`h = f . g`), exactly like the mathematical concept `f of g`. Dollar (`$`) is a syntactical trick to avoid having to use lots of nested parentheses. Instead of writing `putStrLn (show (+ 2 2))`, you can write `putStrLn $ show $ + 2 2`
Recently ported a medium size REST app to servant. It's fantastic. The biggest win IMO is the auto-generated client handlers, which can be exposed as a library for other services to consume, and used internally for testing. Think there's some work being done to integrate servant-client with ghcjs as well.
It may be of modest counterfactual interest to consider an entirely alternative scenario in which operations have known input and output adicities. That is, an i,j-adic operation has i inputs and j outputs (and we may go further and detail the types). Think of specifying a calling convention for a stack machine, where the operation expects to pop its i inputs, then push its j outputs. Now, if we have two such operations, f : i,j-adic and g : k,l-adic, then we can define their juxtaposition f g by considering two cases. If i=l+m, then f g : k+m,j-adic, feeding the first k inputs to g to generate f's first l inputs, but taking f's last m inputs directly. If l = i + n, then f g : k,j+n-adic, with g's first i outputs feeding through f and the other n outputs being passed on directly. Note that if i = l, then these definitions coincide. Juxtaposition plays the role both of application and of composition, which is why I mention the possibility in the context of the OP's query, well answered factually elsewhere. Hilariously (but crucially), juxtaposition is associative: fixing the adicity of the operations does the work, but it's still useful to write parentheses so you can see what you're doing. Where a wee bit more notation is necessary is in higher orider programming, where an operation is *mentioned* rather than *used* (thus becoming 0,1-adic), or where a variable standing for the 0,1-adic knowledge of such an operation is invoked (thus acquiring the adicity indicated in the type of its 1 output). Of course, that's not what happens in Haskell: the function in an application is being used and its argument is being mentioned. We use punctuation of various sorts to fix up what we mean when those defaults don't suit us. Most commonly, we use parentheses to fix the situation where a function is being used to compute an argument, f (g x y z). Anyhow, that's just by way of demonstrating that there are roads less taken in the design space which do lead to a synthesis of application and composition. I'm not making an recommendation, just observing that the Haskell choice is not inevitable (and thus may embody some nontrivial worth).
LYAH actually does a pretty decent job of introducing some of the "general purpose libraries" (or really, general purpose concepts) that are all over Haskell - Functors, Applicative, Monad, Writer, Reader and such. Although it doesn't use monad transformers (as current Haskell code tends to) so some of its code examples don't work exactly like described.
What about the official wikibook?
That's closer to how conduit works. The main differences in the pipes implementation are: * await does not signal end of input * await is a special case of request, which can send an argument upstream * yield is a special case of respond, which can bind a response from downstream The easiest way to learn about pipes is to read [the official tutorial](https://hackage.haskell.org/package/pipes-4.1.5/docs/Pipes-Tutorial.html)
I sacrifice a PHP programmer every week to the GHC gods.
&gt; (Virually) all functions that produce events are wires that wrap their input into the Event and produce it as output: &gt; &gt; something -&gt; Wire s e m a (Event a) Why do you say that this produces an event? Because `(Event a)` is in the last position, which is usually where the output goes, right? And where does the input usually go? On the left of a function arrow? That's only true when using monadic composition! Wire is an Arrow and a Category, not a Monad, so in the type `Wire ... a (Event b)`, the input is `a` and the output is `Event b`. So you shouldn't be looking for a primitive of type `Event a -&gt; ...`, you should be looking for primitives of type `Wire ... (Event a) b`. [Here](http://hackage.haskell.org/package/netwire-5.0.1/docs/Control-Wire-Interval.html#g:4) are a few such primitives. &gt; (Virually) all functions that consume events expect for their input another Wire wrapped in the Event: &gt; &gt; Wire s e m a (b, Event (Wire s e m a b)) -&gt; Wire s e m a b &gt; &gt; How to jump from one to the other?! We have already established that this is not the primitive you want to use, but for completeness, here's how you would convert from an `Event a` to an `Event (Wire ...)`. First, create a function of type `a -&gt; Wire ...`. Then, use `fmap` to make it a function of type `Event a -&gt; Event (Wire ...)`. Finally, use `arr` to lift that function to `Wire ... (Event a) (Event (Wire ...))`. `Event a` is the input of that wire and `Event (Wire ...)` is the output of that wire, so it converts from one to the other, as desired.
That's not an accepted convention. For example, Chris Done's [style guide](https://github.com/chrisdone/haskell-style-guide#operators) argues against both and claims that the former is worse.
`f . g x` is perfectly fine; it just means `f . (g x)`, not `(f . g) x`.
[slides](https://github.com/meiersi/HaskellerZ/tree/master/meetups/20150530-ZuriHac2015_Mathieu_Boespflug-Distributed_Programming)
Those are just his style guidelines, though. I personally disagree with most of the things he says in there, and prefer [Johann Tibell's guidelines](https://github.com/tibbe/haskell-style-guide/blob/master/haskell-style.md). Certain things Chris insist on would probably make me go nuts; like no spaces in tuples, lists and records, heh. 
I was just looking at Pierce's book. i was aware of it but never added it to my 'todo' list. However, after Conal's presentation, I'm thinking about this stuff again. I might dive into it! I just ordered a book titled "Algebraic Specifications in Software Engineering: An Introduction."[1] Pretty old book (apparently the link is to a recent republication of a book from 1980s. The table of contents looks interesting and it includes some stuff on denotational semantics. [1]https://books.google.com/books?id=mLerCAAAQBAJ&amp;lpg=PA310&amp;dq=Algebraic%20Specifications%20in%20Software%20Engineering&amp;pg=PA310#v=onepage&amp;q&amp;f=false
Can't wait to play around with this. It could use a little hello world example though.
Try this: https://github.com/saep/nvim-hs-config-example
Nothing comes to mind that isn't proprietary or NDA'ed, unfortunately.
Neat, seems similar to Uni Flux. (how does it compare, /noob)
Both, but mostly the latter.
From the title I thought this might be about HPC :(
A very uncommon library that is general purpose is Transient. It uses a different approach for the generation of effects, from early termination to distributed computing. It fits in what you mention since it changes the way you structure the code in Haskell, eliminating the problems of the applications with blocking IO, multithreading and events (and cloud computing) that makes them non-composable. A transient application can be composed with any other using standard monadic/applicative/monoid combinators, even if uses multithreading, events or cloud computing (The cloud part is under development) This is an example of composition of two transient applications the simplest one and the most complicated: main= keep $ do r &lt;- async (putStrLn "what is your name?" &gt;&gt; getLine) &lt;|&gt; intergalacticMassiveDistributedCalculationOfTheMeaningOfLifeTheUniverseAndEverithing liftIO . putStrLn $ "The response is: " ++ r will print the name entered and 42. https://github.com/agocorona/transient
I think a more appropriate comparison to the work done to GHC's garbage collector is all the work done on the JVM's garbage collector when trying to squeeze every bit of performance. It would be an anomaly for "normal" Haskell programs to run into any of the work done here as a first-order problem.
Thanks for the great write-up! Since the post touches static/constant large data structures and their impact on GC: in a recent project I've been working on, I also need a (fairly large) lookup table, which is constructed as a top-level value at runtime (rather than hard-coding a huge 'Vector.fromList' or similar, or having it in C with some lookup function). Before reading Simon's post, I was only wondering whether I could be sure that lookup table is computed only once (I have it '{-# NOINLINE #-}'), or this needs some tricks. After reading the post, I was wondering whether it's possible to tell the runtime "This should never be GC'ed" (maybe using static pointers?)
You don't need `NOINLINE` to prevent something from being computed multiple times, GHC will never duplicate work unless it can statically be proven to be small and bounded (and even then it happens rarely). Your static lookup table will sit around in the heap, and will quickly end up in the old generation. This means it should be traversed fairly infrequently, depending on the GC settings and the behaviour of your program. However, it does still have an ongoing cost - memory is "rented" in a GC, rather than "bought" and "sold" as with malloc/free. The goal is to reduce the ongoing cost as much as possible, which is what generational GC aims at, by looking at older objects less frequently. An idea that's been floating around for a while to make this better is the "stable heap": https://ghc.haskell.org/trac/ghc/ticket/9052
thanks, fixed!
I also disagree with a lot of them; I just wanted to point out that the "matter of style" was not uncontroversial.
Too bad I missed you in Berlin, was sick... I started doing [fuctional 3d!](http://www.orouiller.net/projects/ray-hs) recently as well (just for fun.) I will take time to learn more about LambdaCube!
Cool. I'll take another look at the repository. No promises, since I _am_ trying to graduate. What I actually had in mind, though, was to base it off of the [work](http://big-bang-lang.org/big-bang/) I'm doing for my PhD. It turns out that if you try to design a novel type system that provides sufficient flexibility to handle scripting-style languages without type annotations, you get something a) a lot like an abstract interpretation and b) good at typing things in many paradigms; I want to explore that.
You seem to be basing your rather muddled thoughts on a fantasy timeline. Interactive, distributed, multi-user systems with collaborative on-line editing of rich hypertext documents were available in the late 60s.
Netflix using reactive programming. 
&gt; I see that most of the books on Haskell show how to use haskell for practical projects. That's a new one on me! 
So what?
Wow, fascinating post, thanks Simon! I had a question about this remark you made: &gt; A weak pointer is created by newForeignPtr in Haskell - it‚Äôs a pointer to a data structure outside the Haskell heap that needs to be finalized when it becomes garbage. These things are quite heavyweight, due to the extra work the GC needs to do to track them. Can you explain this? Here is what I imagine happens with weak pointers, tell me if this is correct: * They are initially allocated in the nursery. But the nursery keeps track of a list of all weak pointers. * As we trace through the nursery, we mark all live objects and copy them to some other space. * We now check the list of weak pointers in the nursery. If any are unmarked, they are garbage and we run their finalizers. So the overhead is just proportional to the number of weak pointers that are created. The only difference is that we have to traverse all the garbage weak objects, whereas for objects with no finalizers we only have to visit pointers that are live. My other question is, what if you just have a weak pointer with no finalizer? In that case, it seems you have no additional overhead. You just don't follow weak references while tracing from your roots. (Though I wonder how `System.Mem.Weak a` can start to return `Nothing` on dereferencing without some sort of finalization?) I'm asking because I'm wondering if the situation is as bad as on the JVM, where finalizers (and weak references, which use finalizers to null out the reference when it becomes garbage) have huge overhead: http://www.fasterj.com/articles/finalizer1.shtml
Talking in terms of decades is not the same than talking in terms of years. And if this non-related absurdity is all you can say about this discussion then it is not worth to waste my time. Any other thing?
If we go that route, we will provide a counterexample to the claim, "There are exactly two disjoint sets of language features: the kind people complain about and the kind nobody uses." We will have a language feature that people complain about and that nobody uses. 
1) I'd personally use a record instead of a type synonym for `BoxValues`. Accessors seem less prone to foot shooting than positions inside a tuple of doubles. 2) I don't get the point of `liftEC` in `candle`. Since you're only creating a `PlotCandle` anyway, I think it would be more configurable from the user side if you used: candle ... = def &amp; some_param .~ foo &amp; (...) or if you want to stick to the state operators: candle ... = def &amp;~ do some_param .= foo (...) This way, the user gets a raw `PlotCandle` and can easily further edit some parameters, while in your version it's `EC (Layout x y) (PlotCandle x y)` (or so I think), so the user would have to go through `liftEC` again to get to the `PlotCandle`. 
Why? Dependent types are the right way to go regardless. They can replace a whole slew of haskell type system extensions.
Surely you mean getThing = (getBit2 &lt;* getSyntaxCruft) &lt;**&gt; (Thing &lt;$&gt; getPart1) or getThing = getBit2 &lt;**&gt; (Thing &lt;$&gt; (getSyntaxCruft *&gt; getPart1)) 
https://hackage.haskell.org/package/lifted-base
There are a few points here. In our case, all the weak pointers were dead by the time GC ran, so the overhead was in managing that list and then traversing it to run all the finalizers. Yes, the overhead is proportional to the number of weak pointers created, however the finalization is done in the single-threaded part of the GC, while N-1 other cores are waiting, which makes it a bigger issue. That could be improved of course, but for us it was just easier to avoid ForeignPtr. The actual mechanisms behind weak pointers are quite involved, see http://community.haskell.org/~simonmar/papers/weak.pdf If you have a `ForeignPtr` with no finalizer (`newForeignPtr_`) then there is no weak pointer. These are very cheap.
regarding your "btw": there is a proposal on `libraries` right now to move `MonadIO` into `base`. One motivation is to generalize (some of) these IO actions.
It was great fun talking to you guys in Amsterdam. There have been some side-effects of your tour too; I attended the [London Haskell meetup last week](http://www.meetup.com/London-Haskell/events/223598997/), where Derek Wright did a walkthrough of some of the LambdaCube3D code. Also, I'm in one of your pics, woohoo! :)
this? https://mail.haskell.org/pipermail/libraries/2015-July/026008.html
cool. I don't see readFile and friends, but maybe they'll take a patch. https://hackage.haskell.org/package/lifted-base-0.2.3.6/docs/doc-index.html
I'm also interested in GHC extensions, so your blog is perfect for my needs! Thank you!
I just played around with it a bit. This is pretty damn cool :)
What are advantages of `nvim` over `vim` ?
/u/simonmar do you plan on getting all of these patches into GHC proper?
Basically the same reasons that emacs is better than vim: * multithreading * separation of server and client * support for better languages than vim lang (elisp for emacs; ruby, python, Haskell, Perl for neovim (I guess some work in vim as well)).
What if Reddit is a type that involves IO already?
as in `type Reddit = IO Reddit_` ? 
Off topic, but beware of using `readFile` at all. Lazy I/O can cause memory and file descriptor leaks.
Yes; they are almost all in.
He'll be going the PHP route: defer all errors until runtime and just copy and paste the code in a place that looks good ;)
I have no idea dude 
https://phabricator.haskell.org/D1106 not merged yet
&gt; Pureli is dynamically typed and has a lisp-like syntax [MRW](https://i.imgur.com/1cUwTWm.gif)
&gt; I've often needed this when writing parsers of various kinds Oh. I just make the constructors for the term's type match the order in the grammar, so the Applicative parser reads like the grammar, with = instead of ::= and &lt;|&gt; instead of |, interspersed with the usual Applicative operators. &gt; Defining helper functions for the sole purpose of reordering or skipping arguments is, in my opinion, an ugly kludge I have to concede that one, yes. 
Something I was thinking about - any data structure that does balancing is going to be (at least partially) spine strict. Lazy lists are the exception rather than the rule. Most interesting data structures have some invariants they need to preserve, and preserving these invariants in the presence of edits like cons, snoc, append, etc, generally means forcing things to some extent.
Hehe. I probably mean getThing = flip Thing &lt;$&gt; getBit2 &lt;* getSyntaxCruft &lt;*&gt; getPart1
Emacs does not actually have multithreading. It deals fairly well with asynchronous calls which makes some things to work much better than in Vim, but it doesn't have real multithreading.
For various reasons I can't give you exact stats, but typically we allocate and reclaim multiple GB/s on each machine.
also: - true color: themes that as good in the terminal as in gvim - a built in terminal emulator, i like to have a small splits with ghcid and a repl inside nvim: https://i.imgur.com/ureKZxQ.png
"Did you really think I was saying that any rebalancing must be fully strict?" No. But I did read you say that rebalancing introduces strictness. My argument is that in an amortized setting, I don't see why it must do so in an "observable" way, or even that there is a strong tension in that direction. i.e., a fingertree can't be potentially-infinite. But you can have amortized deques that are. So my general claim is that most amortized-rebalancing structures we want to look at can externally appear to have essentially the same (non) strictness properties as fully lazy ones. This seems, to me, to be opposite to what your argument is.
nicely done, looks really cool for "just" an undergraduate project. anyways, feels like racket or some other kind of scheme.... but i'm curious, how are you "enforcing" the *purely* functional part of the language?
Looks very complete, even with nice website and tutorials. How many hours did this take? I can't imagine how much work must go into creating a language and toolchain.
Very interesting! Wow this looks almost professional, I'll read some of the source latter. I have a question though: what's up with the []? For example they are in the "do blocks" and in a letrec, why? Are they vectors? Special syntax?
Summary: The talk is about C# primarily. It covers the notion of a category (objects and morphisms, in the form of C# objects and methods), functors, and monoids. For a more in-depths introduction - one that involves good stuff like the Yoneda lemma, natural transformations, and the strongly laxative monoidal functors -, you can check out [Steve Awodei's CT lectures](https://www.youtube.com/watch?v=BF6kHD1DAeU&amp;list=PLGCr8P_YncjVjwAxrifKgcQYtbZ3zuPlb).
Chris Done's style gude isn't an accepted convention either. In fact, it's rather unusual in a number of ways. (That said, I follow it pretty closely myself.)
Haha. Lisp too stronk
For testing the API I'd just use the generated functions from `servant-client` (it uses `http-client`), then fork a new thread for the web server and test that way. For the DB tests I'd make them separate and pass the connection pool directly to them, (but they'll still get tested via the web handlers obviously). ------------------------------------------------------------------------------ -- | UserActivity API type UserActivityAPI = "activity" :&gt; QueryParam "limit" Limit :&gt; QueryParam "before" EpochTime :&gt; Header "Authorization" JWT.JSON :&gt; Get '[JSON] [Feed] ------------------------------------------------------------------------------ -- | Proxification userActivityApi :: Proxy UserActivityAPI userActivityApi = Proxy ------------------------------------------------------------------------------ -- | Auto generated client function userActivityGET :: Maybe Limit -&gt; Maybe EpochTime -&gt; Maybe JWT.JSON -&gt; EitherT ServantError IO [Feed] userActivityGET = client userActivityApi host where host = BaseUrl Http "localhost" 8000 ------------------------------------------------------------------------------ -- | User Activity Tests userActivityWebTests :: Config -&gt; SpecWith () userActivityWebTests config = do describe "User Activity Tests" $ do runIO $ runApp config $ redis flushall Right (token1, uid1) &lt;- runIO $ regUser config (UserName "test1") it "Should get a user's activity feed" $ do np &lt;- newPost Right p1 &lt;- runEitherT $ postPOST (NewPostBody np) (Just token1) Right p2 &lt;- runEitherT $ postPOST (NewPostBody np) (Just token1) k &lt;- runEitherT $ userActivityGET Nothing Nothing (Just token1) k `shouldSatsify` isRight let Right items = k length items `shouldBe` 2 
&gt; the strongly laxative monoidal functors Can't say I've experienced that particular side effect of monoidal functors yet. 
Almost? Lets be honest, anyone here would believe it is even a company if told so.
C# has LINQ; C++11 and C++14 both have many functional concepts. The definitions of many languages are inherently non-functional, but at their core, many languages are moving in that direction. This isn't definitive by any means (and those examples also contain non-functional changes), but they certainly help the problem of building more composable code.
I suggest to use `hspec-wai`. I have an example in my recent project if you want to take a look https://github.com/eckyputrady/livelog/tree/master/code/api/test For Auth Token / sessions, I think you should pass it on the HTTP header yourself. EDIT: wrong example link
Thanks, /u/creichert. I have a function (`app`) that returns an Application. I then use `run port app` inside main to start the server. In the `hspec-wai` readme, the app also returns Application and so I am guessing I can adopt the example to my server as well. One question I have is around code organization. Do you normally clone the project and run tests by changing the `main` or do you just add to the normal server launch code but have an environment variable that decides whether to launch or test? Would you normally clean up the test parts of the code before deploying it in production? Testing is new to me so trying to learn some best practices. Thanks! 
You want to share your `app` function between you're test and executable. You will have two `main` modules. One for your app and one for you're tests (for example, `app/Main.hs` and `test/Spec.hs`. The best way to do this is to move as much code as possible to a cabal `library`. Then you can import it in you're Spec and executable with minimal differences. The code that /u/eckyputr posted is a good starting point. You should not need to modify you're code just to test it, use `cabal test` to test and `cabal build` to generate you're executable. 
Only a Doctor can prescribe them.
Great presentation! I love the relaxed style. Very informative and still easy to follow.
Thanks a lot! Basically, For now Pureli is limited to only some IO operations: read and write from stdin/stdout and files. if you called the "pure" function, which is an IO action without side-effects that just returns a value in IO context, any call to an IO operation in the expression passed to pure will result in a runtime error, so we can assume everything beyond "pure" is pure. if you want to take an IO action that produces some result and bind the result to a variable from inside an IO context, you use "let!". and if you want to sequence IO commands, you use "do!".
Thank you! Formulating the idea, approach and relevant learning took a while. I started out with a more simple calculator language to try out the parallelism stuff first, the design and implementation of Pureli started a few months ago. I don't really have a good estimation on how many hours it took though :)
Basically, any call to a function in pure context creates a spark for the evaluation of each of it's parameters.
The original talk was in Scala, although there was no video unfortunately. Pseudo C# is awkward to be sure, but it was great to reach an audience that wouldn't be exposed to these kind of ideas otherwise. Slides (using Scala) are here: http://www.slideshare.net/kenbot/category-theory-for-beginners Follow up presentation because the original one was too long: http://www.slideshare.net/kenbot/your-data-structures-are-made-of-maths
Let's not forget that Facebook has hired one of the world's top Haskell experts to work on one of the world's most powerful spam filters. This isn't really a scenario of average Joe Haskeller working on average old programming project. I mean the fact that a company like Facebook has taken an interest in Haskell at all is a pretty good start towards world domination compared to where we were a few years ago.
Start with what you want to do. Write a type for that function &gt; As an example of the calculation I'd like to do with these entities, one simple function I'd like to have is one that takes a portfolio, a desired allocation %, an account to put money into and the amount to put in. It would tell you to buy X amount of Asset A to keep the allocation % what you want. achieveAllocation :: Portfolio -&gt; Percentage -&gt; Account -&gt; Cash -&gt; AssetClass -&gt; Cash achieveAllocation p target acc amount assetClass = undefined -- compute amount of money needed to achieve desired allocation % Do this for the most important things you want to achieve. At this point, you will have a bunch of undefined types. This is when you have to decide your data representation. Some choices might be simple, like: type Cash = Decimal while some might be more complicated. data Portfolio = Portfolio [Account] data Account = A401k .... | RothIRA .... | Brokerage ... --domain knowledge for this part. Something to keep in mind in this process is that you want to "make illegal state unrepresentable." In practice, this can be hard to achieve perfectly, but Haskell's type system can get you a good portion of the way. For example, if an asset is only domestic OR international, but never both, and never neither, then data Market = Domestic | International data Asset = Asset Market .... -- continued would be better than something like data Asset = Asset { isDomestic :: Bool, isInternational :: Bool ... -- continued } because in the latter situation an asset can be constructed that is marked both as domestic AND international, and thus every function dealing with assets has to worry this "impossible" case. Anyway, once you have data representations chosen, go back to the function types you wrote earlier and start implementing them. I usually start with the smallest ones first, and then build the bigger ones from the smaller ones. Chances are, you will run into some issues implementing your functions, and its possible that this is the result of a bad data representation. If this happens, do not be discouraged. Use this new knowledge to look at your data with new insights, and see if you can fix the problem. (Sorry if this part seems a bit vague. This is where creativity and problem-solving skills come into play.) Hopefully, once you finish writing your functions, you can test them to see that they work in something like ghci, or even better, you can write tests for them using something like QuickCheck to see if you messed up anywhere. If you encounter a situation where you have functions that take too long to run, you might have to look at your data representation AGAIN to see if changes for the sake of performance need to be made. However, the more practice you get, choosing a good representation will get easier. Sometimes, looking at similar programs can give you good ideas. Hopefully some of this was helpful to you.
One guess is that `Asset` and `Portfolio` would be the primary types, where maybe newtype Portfolio = Portfolio { assets :: [Asset] } deriving Monoid and data Asset = Asset { assetClass :: AssetClass , account :: Account , amount :: Amount } I'm not sure how you'd want to represent amounts, whether you want to track specific stock symbols, etc. A basic guideline is to make types that can't be combined senselessly, e.g. don't use the number `4` to represent both "4 shares of GOOG" and "4 USD". Maybe also: type AllocationStrategy = AssetClass -&gt; Rational type Allocation = AssetClass -&gt; Amount You can define for example: currentAllocation :: Portfolio -&gt; Allocation byAccount :: Account -&gt; Portfolio -&gt; Portfolio market :: AssetClass -&gt; Market byMarket :: Market -&gt; Portfolio -&gt; Portfolio invest :: Money -&gt; AllocationStrategy -&gt; Portfolio -&gt; Asset This representation views an account as just a "sub-portfolio" so you can also choose to apply your allocation strategy to the portfolio as a whole, to a particular market, etc. If this program is supposed to keep track of your investing over time, I would consider using a growing list of events as the "database", e.g.: data Event = Buy Asset | Sell Asset type History = Seq (UTCDate, Event) apply :: Event -&gt; Portfolio -&gt; Portfolio portfolio :: History -&gt; Portfolio -- simple fold portfolioAt :: UTCDate -&gt; History -&gt; Portfolio Even if this exact design doesn't make sense for you, I've tried to make `Portfolio` into a natural unit that can be filtered and combined, and to make functions that result in `Portfolio -&gt; Portfolio` transformations. This kind of thinking seems to lead to readable programs, at least sometimes. Edit: Also, I guess the `History` thing doesn't really follow double-entry book keeping conventions. Maybe take a look at the `hledger` program for inspiration? I'd be interested to see what you end up with!
&gt; `withCString :: MonadIO m =&gt; (CString -&gt; m r) -&gt; m r` &gt; isn't a realizable goal First of all, a correction (at first I thought I was losing it): withCString :: MonadIO m =&gt; String -&gt; (CString -&gt; m r) -&gt; m r Why is this not a realizable goal? The reason you can't write that today is because this calculation uses `alloca` internally. But /u/spirosboosalis is talking about doing this generalization globally. Is there any reason why `alloca` can't be generalized? And if it can be, why can't `withCString`? I am usually opposed to this kind of type generalization, because functions with more precise types result in code with more precise semantics. But in this case I have been in favor for many years. The reason is exactly because of functions such as `withCString`.
When used correctly, lazy IO is reliable, fast, and leaks nothing. In simple IO situations, it often results in the simplest and fastest code and is the best option. But not always. The thing with lazy IO is that although it looks simple, you definitely do need some technical knowledge about it to know when it is appropriate and how to use it correctly.
For what it's worth, *servant* itself uses *hspec-wai* in its tests. To be entirely honest, this doesn't feel entirely satisfactory to me. While an API type doesn't say something about the client "flows" you should test, there's a lot of boilerplate that could be automated away I think. What I would like is somehow symmetric to the recent work on mock servers: you'd derive client functions and call them *in a way that makes sense for the application* -- you don't want to just hit randomly the various endpoints with some randomly generated data. That could do for testing servant itself, but not applications. We would probably want to somehow describe the actions we want to see performed and then have the description be "run". This all sounds quite similar to what /u/bartavelle is saying. I'm just wondering if we could turn that into a reusable solution that we could ship in a future release.
&gt; ligatures are about making things harder to read so they are prettier. That is not at all true.
&gt;I couldn't find a way print the type names from regular Haskell code This might be a super dumb comment because I not actually sure if it applies at all, but does the `Typeable` class do what you want here?
Probably, yes.
Both of those have this effect though when used in the context of a programming language where every character matters. The first type benefits from that effect in natural languages where you can still read words no matter what happens to the characters in between the first and the last. This effect does not exist in the same way in programming languages, particularly those like Haskell where you don't always use natural language words as variable names.
&gt; This effect does not exist in the same way in programming languages, particularly those like Haskell where you don't always use natural language words as variable names. And yet most papers featuring Haskell do extra work to simulate a number of these ligatures using (lhs2TeX and) LaTeX macros, so at the least I'd say it's [up for debate](https://31.media.tumblr.com/55d170c898b240783dc332124bbd197d/tumblr_inline_n4foafra0C1sew80h.jpg). And then there are cases like replacing `-&gt;` with `‚Üí`that seem like obvious wins, since that is what the ASCII characters are approximating.
Papers are very different from actual code. Nobody really needs to read papers and find that one character that is broken. The authors do that in the more readable source code. I would go so far as to say virtually nobody really reads the source code in papers with the aim of full understanding as we do actual source code. Most people just skim it to get a general idea of what it does but wouldn't find small mistakes in source code in a paper without prompting to look for them specifically.
¬øPor que no los dos? I'm using ligatures differently than most others by trying to include only those that make sense (which is why I need haskell user feedback because I don't know haskell). For e.g. -- I'm actually increasing spacing and for others I'm moving them closer to create a visual grouping (e.g. ...) For some I replace them with the unicode/math symbol they're trying to look like so that e.g. -&gt; becomes ‚Üí and &lt;= becomes ‚â§ [example images](http://imgur.com/a/x9k2N)
This is really helpful. Thanks so much for taking the time to write it. How do you suggest I represent an allocation. For example, $50 in Domestic Bonds in an account. Given the following definitions: data Market = Domestic | International data AssetClass = Bonds Market | Cash Market | REIT Market | Stocks Market Is there a better way to do this than have a set of (AssetClass, Double) ? I thought of doing something like: type Amount = Double type Allocation = [(AssetClass, Amount)] Then, my accounts would have a set of Allocations. Would I define my accounts as a list / set of allocations? data Accounts = A401k Allocation | RothIRA Allocation | Brokerage Allocation 
I must say I am pretty satisfied with your approach: it makes code reading clear and do not fall on the `==` problem. Particularly in Haskell, I think that `=&gt;` could be changed to the Unicode `‚áí` without problem since it doesn't conflict with `&gt;=` (or `‚â•` in your version, I guess). For other ligatures (such as `&gt;&gt;=`), I am not sure yet. **EDIT:** It would be interesting to see something like `f x = g . h` become `f x = g ‚óã h` as function composition, but I am not sure how annoying it could be in a code.
Thanks for taking the time to put this down. The way you defined Asset was how I was thinking of implementing it, but I started thinking it looked too similar to Classes in other languages I knew, and was wondering if this was idiomatic for Haskell. The suggestion on tracking events and history seem great too. I think I need to spend some more time digging into some books as I work on my toy project. If it becomes anything interesting, I'll share it on this subreddit. 
Absence: The total width isn't changed = still monospace. In typography terms the advance width is unchanged - just moving where the glyph is on the "canvas" A coding font is not a tool you change every day so having to "learn" that e.g. &lt;= is ‚â§ (the very math symbol it replicates) is not too difficult. For editability I turn ligatures of on cursor line. 
I have not. I'll take a look.
Yes. Thank you! :)
It's customisable prior to download with "Ligatures: Off" and other options ;) 
Curious. Link/details?
I sent it to the maintainer, so I hope so.
Great, I like your ray tracer :)
Might they make 7.12 ?
The release notes link does not work (yet).
Just as a mental mas... exercise I've tried to write a `Representable` instance for [length-indexed vectors](http://pastebin.com/5jWS0rFM), but unfortunately GHC complains about overlapping instances and conflicting type family declarations :(
Yay! [Special Implicit Parameters](https://downloads.haskell.org/~ghc/7.10.2/docs/html/users_guide/other-type-extensions.html#special-implicit-params).
From where? I got to [release notes](https://downloads.haskell.org/~ghc/7.10.2/docs/html/users_guide/release-7-10-2.html) w/o any problems.
Personally, I hate the ligatures used in Haskell papers.
Very webish list. I would have expected something more computational and graphical, such as "make a cellular automata and visualization" or if you want something that feels less academic "Implement ipsec ESP with TFC and ESN".
Sadly our CDN does seem to take a while to catch up. 
Very glad they have so many binaries available and linked from the main webpage on release day. (I remember some releases where the only binaries were for Linux and Windows, and other platforms trickled in much later if at all.)
Funnily enough I started working on your reddit idea a couple of hours after I saw the announcement of the `reddit` package, using a Git backend as you mention. Redditors with intermediate Haskell skills think alike? I'll try to put it up over the weekend after a bit of cleanup, though it's far from finished. Actually it's currently mirroring all new threads in your subscribed subreddits as JSON files checked into a Git tree, and I use a hacky combination of shell and Node.js to generate HTML views -- in a format I think is way nicer to read than reddit.com! My hope is to make it suitable for running as a cron job, using the search API to only mirror what's missing locally. Maybe refreshing old threads according to some priority-based schedule, e.g. boosted by your participation, rate of change, etc. Oh yeah, and keeping track of what you've already read. So I imagine being able to check reddit once a day without missing anything interesting.
alloca has the same problem, the point of alloca is to temporarily do an allocation. When MonadIO doesn't give you enough information to be able to form a bracket operation that lets you know when to deallocate.
You can do it, you just need to merge both of your Representable definitions for `(Vector n)` together. In the usual terminology `Vec n` is represented by `Fin n`. `Fin n` is the set of integers `{0..n-1}` `Fin 0` is uninhabited, so the function passed to `tabulate` can never be invoked.
I mean the recursion of `tabulate`. In the code I've posted, for the `Vector 0` case it's: tabulate _ = N while for the `(1 &lt;= n) =&gt; Vector n` it's (changing the terminology): tabulate f = V (f $ Fin p) (tabulate f) where p :: Proxy (n-1) = Proxy The problem is GHC doesn't allow both of these two declarations: instance Representable (Vector 0) where type Rep (Vector 0) = Void and instance Representable (Vector n) where type Rep (Vector n) = Fin n But apparently toning down the smartassery and just writing: instance Representable (Vector 0) where type Rep (Vector 0) = Fin 0 compiles just fine (modulo changes to `tabulate` so it "narrows" the domain of the function passed to it) so nevermind, thanks ;)
Nice project. Do you have control of the evaluation (lazy vs. eager/strict) or is that left up to the Haskell runtime ?
This is a really good question. On the one hand, I would like to continue working it. On the other hand, I still feel inadequate working on it because I'm still pretty inexperienced in software development and a lot of other stuff. I lack formal PL design/implementation education and I also do not have much experience with Lisp languages. I do want to explore those areas more though, and I might use Pureli as an incentive to do just that. Also, There are more subjects other than PL I would like to explore (like concurrency, emulators, web and graphics) so I don't think I can devote all my free time to Pureli. I will probably still work on Pureli from time to time, but I will probably devote a lot of my time to other things as well. Perhaps a few years from now I will feel a little more adequate and will try to tackle this head on and take Pureli to the next level. Thanks for the question. I am glad too hear you like the feel of Pureli!
ask your doctor if monoid is right for you
Is `if` a special case of this or are the true and false expressions both calculated regardless of the condition ?
Looks like it's there now.
I think it's pretty reliable, and becoming even more so. IIRC, the next version of GHC will automatically derive a correct instance for all types.
I'm not sure I need any of those for a text editor (but I'm probably wrong ;-)).
what about `withCString :: (MonadMask m, MonadIO m) =&gt; String -&gt; (CString -&gt; m r) -&gt; m r`? It seems to have an overloaded bracketing operation. (not suggesting to add everything into base, just hypothesizing)
You'll note that there are many missing instances for MonadMask that you'd expect given the above exposition. It gets you some of them, not all.
I looked at the example, but I still don't really understand what Special Implicit Parameters are.
If you choose to test it then please add your findings here: https://github.com/larsenwork/monoid#ligature-support
Hopefully this means we can get ghcjs support for stack now? 
https://capnproto.org/
So our needs require maybe a bit more scalability than yours. We use thrift now as our client/server api with consul handling addressing and discovery. We do use rabbitmq as a queue but that was mostly about message durability when receiving work. It provided the correct semantics for processing work while ensuring we don't drop messages. For thrift messages we mix using messages with thrift types and messages that are in protobuf format. We already made extensive use of protobuf throughout our codebase so we just send them over the wire as a bag of bytes. Our's is only a little cross language but the basic approach should work just fine.
I have a similar design in the program I am developing, javascript clients with websocket connection to the haskell server receieving messages. As you said, something like RabbitMQ is too heavy-weight. What I settled on is Redis Pub/Sub + JSON over websocket connection + MessagePack stored in Redis. All my message data types I wrote instances for Aeson To/FromJSON. Then I wrote a small function to convert any Aeson Value into messagepack, and a function to convert messagepack back into an Aeson Value. The conversion from messagepack to Aeson Value just errors on things like non-string object keys and binary data, which is fine for me since I will only be decoding messagepack objects that were originally encoded from an Aeson Value. Then over the websocket connection to the clients, I send JSON. Inside Redis, I store the messagepack encoding of the object. I then use Redis Pub/Sub so that the server does not need to poll. Once the server receives a redis message, it in turn sends the message to the client.
You might be onto something, but I don't think that is very precise. :) "an amortized structure with the same overall costs"? "strictness properties determined only by those directly imposed"? Maybe it would help if you phrased it in terms of the algebra of the two data structures? As is, you have a homomorphism (or something) from the algebra of one data structure to the algebra of your "amortized" version of the structure. And the amortized version has the same strictness for all operations while also having some additional properties. Or something like that.
That should work in 7.12, but error doesn't use an implicit call-stack in 7.10.2. So for the time being, you'll have to include a custom error function in your library to use the new functionality. 
They don't seem to have Haskell client anymore, do they?
One immediate problem: `&lt;=` could also be `‚áê`. This doesn't really come up in Haskell, but it's something to think about.
Okay, in case anyone really finds it intersting, I've set up a [GitHub repo](https://github.com/SrVictorMaia/optlam) with the source code for the optimal evaluator I used and this particular test. Thanks!
Useful tutorial. I have a similar repository layout for my GitHub hosted blog, except that rather than using submodules I just rsync the compiled site to a clone of the repository that has the other branch checked out.
&gt; Are you using "posts/_*" (hidden posts) for drafts? Kind of - the rule I have for that skips compilation of hidden posts unconditionally, so I need to add a "debug" configuration flag of some sort to make it truly useful (and I am procrastinating that because I haven't actually needed it yet :) ). &gt; Your solution for comments (GitHub comments) is also interesting. I chose that to keep everything hosted in a single place, and also to avoid having a relatively heavyweight blob of JavaScript in the posts. One improvement I am considering is using the GitHub API to automate creation of the issues/comment threads.
Your wish is emptyflask's demand: https://github.com/Homebrew/homebrew/pull/42253.
Haskell has great solutions for parallel programming. [This lecture by SPJ](https://www.youtube.com/watch?v=hlyQjK1qjw8) surveys some of them, the semi-implicit approach being the closest one. It also highlights a problem with Pureli's idea - the cost model matters and right now we can't just leave it up to the system to parallelize everything if we care about it. It's a good thing this is more of an experiment than a production system :)
The algorithm is a graph with manually managed memory that has mutation all around (all types end in IO), quite the anti-case for Haskell. Yet, with `do notation` it could actually work quite well, but when I attempted it I spent a [few hours just trying to derive unbox](http://stackoverflow.com/questions/31687381/derivingunbox-doesnt-work-for-types-with-more-than-6-ints) for the Node type, so I gave up and just made in JavaScript. Might not take a long time to convert it when I figure this out, though! Haskell unboxed vectors are mad fast and it has threads so that might be a cool project.
You're correct! It is possible and there are actually two implementations on Hackage, [LambdaScope](https://hackage.haskell.org/package/graph-rewriting-lambdascope) and [LambdaINet](https://hackage.haskell.org/package/LambdaINet), both using the graph-rewritting package. I was specifically trying to manually control the memory in order to study the garbage collecting implications, though!
&gt; Have I just devised a stylistic abomination? In theory, no ‚Äì I think that it's probable that if everyone was using it and knew about it, Haskell would've been slightly easier to read. In practice, it won't work because it's a new pattern that people aren't used to, and it violates patterns people *are* used to ‚Äì for instance, ExceptT (extractTextFromQuux &lt;$&gt; fetchQuuxViaAPI) is much more easily understandable than your variant for me, merely because it's not the 1st time I see it.
&gt; When I was writing Haskell Which language are you using now? Just curious.
&gt; In practice, it won't work because it's a new pattern that people aren't used to, and it violates patterns people are used to Can't deny that. (`veryLongRightToLeftExpression &amp; ExceptT` is admittedly a bit excessive, in any case.)
Cool. Does anyone know if [stackage nightly](http://www.stackage.org/nightly) will have this by tomorrow? Not that I'm impatient. EDIT: update: [nightly-2015-07-30](http://www.stackage.org/nightly-2015-07-30) did not end up getting 7.10.2.
thanks for explanation! can you also mention why/where this could be useful? Why just not pass parameters directly and explicitly?
A lot of people have had this idea. F#, for instance, has (|&gt;), which is equivalent to (&amp;). Haskell, too, follows the left-to-right convention with (&gt;&gt;=), probably because of the the do-notation. But, in your example: why not invert &lt;$&gt; too to get a consistent ordering? fetchGlubs = do txt &lt;- fetchQuuxViaAPI &gt;$&gt; extractTextFromQuux &amp; ExceptT parse Parser.glubs "fetchGlubs" txt &gt;$&gt; fmap destroyInconsistentGlubs &amp; catMaybes &amp; (first show &gt;&gt;&gt; hoistEither) Now everything flows from left to right/top to bottom. More generally, the underlying tension between left-to-right and right-to-left information flows is caused by prefix function application (where the operation is to the left of the input) and our writing systems that go from left-to-right. The weird type signature of (.) is a direct consequence of prefix application, for instance: (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c) (f . g) x == f (g x) -- no need to switch f and g around when using (.) Similarly, fmap: f :: (a -&gt; b) f $ x f &lt;$&gt; [x,y,z] -- just replace $ with &lt;$&gt;, everything else stays the same `&lt;$&gt;` is actually additionally bugged because it's left-associative, meaning that one can't write g &lt;$&gt; f &lt;$&gt; [x,y,z] If you had never been introduced to the prefix-way of writing things, you'd probably feel more comfortable writing x y z f to signify that x, y, and z are going into the little f-factory instead of f x y z signifying that they go *back* to the left into f. It's an annoyance that results in one thread every month or so, but I guess we're stuck with it.
&gt; But, in your example: why not invert &lt;$&gt; too to get a consistent ordering? The inconsistency is intentional. My example does not try to resolve the tension you correctly point out, but rather to exploit it to introduce an arbitrary break in an expression between the interesting parts and the uninteresting ones. (By the way, `lens` actually defines `(&lt;&amp;&gt;) = flip fmap`. It doesn't fit the weird bidirectional style I demonstrated, though, as the "point" is always on the right-to-left part of the expression.)
In this *specific* case, it's a way for the compiler to pass in call stack info without having to change how your function is called. Otherwise, they're really not that useful. That's why people don't like the extension very much. Occasionally, they can save you some typing or even make your code more readable when you're passing around configuration or state through a bunch of nested functions‚Äîbut it's never a big gain. And even when there would be some benefit, you can often reorganize your code in a way that both fixes your problem *without* implicit parameters and is, on the whole, more readable. (Moving functions around, grouping values into a record or using a reader monad can all help.) I've never had code where I desperately wanted implicit parameters, and I think that's consistent with most people's experiences. The one time I used them was for some very repetitive test code. They made my code neater because I didn't have to manually thread a `result` parameter through all of my assertions‚Äîbut in hindsight, I could have had the same effect with a record wildcard. It would have made for simpler code! People used to use them as a proxy for typed holes. You'd drop an `?x` into your expression and the resulting error would tell you what type was expected there. But now that we have real typed holes we don't need this any more. So on the whole, implicit parameters, while clever, are not useful in practice, except for this special case.
"Shape-preserving", as you imply, suggests imagery which is a bit too physical. Alternatives with less baggage include "context-preserving" and "structure-preserving". "Structure" in particular is nicely ambiguous, in that you can think of it in a more concrete sense (skeleton of a list, etc.) or in the abstract mathematical sense (as the general case requires) and it works either way.
&gt; But... why? One plausible motivation would be, in a do block, keeping the meaningful parts of an expression up front (and in right-to-left order) while moving the boilerplate to a less obtrusive position (think of `lift . lift . lift` and similar annoyances). &gt; Is that separation really useful? That's up for debate :)
I think this is somewhat backward. The uninteresting, boilerplate stuff should be at the top, as this simply describes the context (it also makes it clear that this can be parameterized by passing it in as a continuation), and the interesting things that follow should be using something like `&amp;` to illustrate what happens to data, in order. The love of `after_q . q . after_r . r` in Haskell has always baffled me, especially in the light of the willingness to swap the meanings of `::` and `:` PS: any reason for `ExceptT` rather than `MonadThrow`? Since you're in IO, you have exceptions anyway.
To me, it looks more confusing that way.
With implicit parameters, this means that, to use `f`, you need the variable `?stk` of type `CallStack` in scope. However, since these are special, GHC inserts them automatically, so you can use `f` as you would normally and the compiler will automatically provide `?stk` with information about the call site to `f`.
Why not describe it how it is? It's diagram-preserving. Kids these days. Back in my day, we wrote homomorphisms on the chalk board until the dust gave us allergies.
I think that LTS was supposed to be switched to 7.10.2 when it comes out.
I've never found the argument that you should use a Reader monad instead of an implicit parameter very compelling. Translating existing code to a monadic style is incredibly tedious (eg switching to do-notation or inserting the Applicative operators), whereas you could just enable ImplicitParameters and change your type signatures. So I think ImplicitParameters has a big usability benefit over Reader monads. That being said, I've almost never wanted **just** a Reader monad.. :)
`plate . boiler $ x &amp; (cool &gt;&gt;&gt; stuff)` - that is interesting as well. Only the parentheses are unfortunate (`(&amp;)` and `(&gt;&gt;&gt;)` can't be mixed). &gt; any reason for `ExceptT` rather than `MonadThrow`? None, other than the snippet being adapted from test code which is very far from finished.
I certainly think `&amp;` has its place (its very handy in GHCi), but I personally wouldn't like seeing it being used to separate interesting from uninteresting code, mainly because mixing so many operators is really hard to parse. What I continue to advocate is giving intermediate terms names (and types!) with a where- or let-statement, which additionally gives a lot of useful information, and stick to associative operators with well-defined relative fixities. EDIT: To clarify, when I say intermediate terms I mean intermediate functions more than the values flowing through them.
Hmm, I always have to open the haskell mailing list archive site in Chrome since I disabled TLS &lt; 1.2 in FF... Who should I contact to get this fixed?
FWIW functions always seemed like pretty sensible boxes to me if you think of `r -&gt; a` as being a box with an `r`-shaped key, though I basically agree that the metaphors can be over relied on.
Data.Reflection is another option which is less disruptive than the Reader monad, though a little harder to understand initially.
Here's an issue on the stack github tracker about it. https://github.com/commercialhaskell/stack/issues/337 (I haven't been following the discussion and don't understand why GHC-7.10.2 makes a difference here.)
It's not so much a concern about breaking code, but more that we want it to be clear when you'll incur the cost of implicit call-stacks. Implicit parameters are really just extra function arguments that GHC passes around automatically, so they're not free. You probably don't want to pay this cost everywhere, eg in a tight loop, so functions have to explicitly opt-in, by adding the (?stk :: CallStack) constraint to the type signature. 
That's terrifying
that `f` in the type signature means "functor" which is a more general type than a list. A functor is essentially any data structure that can be mapped over, including lists and ADTs. All lists are functors, but not all functors are lists.
If you use protobufs for the actual payloads, what are you using Thrift for? Is the IDL basically, binary send(1: binary protobufPayload) 
I'm not sure what you mean by the "Standard distribution", AFIK there is no such thing, though the Haskell Platform is probably the closest in spirit. There have been recent discussion to include stack as a component in the Haskell Platform, something that I hope happens as stack is certainly more beginner friendly. Stack is not a replacement for Cabal the library. Stack builds upon Cabal, and all valid stack projects need a valid Cabal file. The cabal-install tool is what stack offers a drop in replacement for, but I don't think cabal-install is going away soon. There are still lots of people who prefer that workflow, and who have developed systems based around it. 
Mark Lentczner has proposed that the next Haskell Platform release be based on `stack` and Stackage: http://projects.haskell.org/pipermail/haskell-platform/2015-July/003129.html And by "next" he means the one after the imminent 7.10.2 release.
I did read a few chapter in the past and experimented with it, it is a very good resource, but i didn't follow it when i was getting started with Pureli.
Looking at BERT, I immediately think, "Oh, this is like Erlang!" And then I noticed father down, Binary ERlang Term. Either way, the concept of an atom, or a comparable, equatable term which can be categories matched against seems to be very useful--as long as a standard is set for which atoms mean what,
&gt; In order to execute Arduino actions in the main loop, we're supposed to put them in `reactimate` and run it in every loop You should not run `reactimate` on every loop. Look at my implementation above: `main` first compiles `networkDescription`, which contains the only call to `reactimate`. Then I start the main loop, which does not contain any call to `reactimate`. Instead, it contains calls to `fireMilliseconds` and `fireKeypress`, both ordinary `IO` actions which you can run inside your `Arduino` computation via `liftIO`. I'll give a more complete example as a StackOverflow answer.
If you've never known cabal, I think at this point it's safe to forget it ever existed. I don't think there's any point in knowing how to use both tools aside from collaborating with other developers who prefer cabal. And who _are_ these people? /Seinfeld
[This seems relevant](http://imgur.com/a/GbpYE).
This is pretty hardcore. I didn't know you could do that.
Hmm... This certainly appears to maybe be a bug, buy I think it's unrelated to the one you cited. In my case, I am installing the same major version as what ships with GHC. I guess ``apt`` pulls the latest minor version (which might explain why GHC 7.10.1 has stopped working, if the Cabal minor version was bumped since I last ran the Travis-ci build and no longer matches with what GHC 7.10.1 ships with). I tried this business of running ``configure`` again and it didn't seem to work. Is there at least a work-around? Two more strange things: * This is not popping up in older versions of GHC which I am installing in the matrix. * Working with GHC HEAD (7.11.\*) and Cabal HEAD (1.22.\*) reports that both GHC AND Cabal have changed versions (to 7.10.2 and 1.22.2, respectively). https://travis-ci.org/tebello-thejane/bitx-haskell/jobs/73248228 :-/
No. He has proposed the next release include the stack tool. There's no one course of action decided onto how the platform snapshot be selected, etc.
that's bad advice imho
This is incredible, i'm definitely going to try this out soon. 
And it seems it's already merged in Cabal HEAD: https://github.com/haskell/cabal/commit/70600e9718cfd510ac5daa805d97238b8a2f1098 Moreover, the Cabal devs have been working on http://www.well-typed.com/blog/2015/07/cabal-setup-deps/ to address a related bigger issue. So Stack is not the only tool addressing these kind of issues...
Some thoughts below. I would really like haskell (or, really, a smaller, cleaner language very much like it) to see more use in industry, but‚Ä¶ **Libraries** There are both not enough and too many. In the cases where there are libraries there might be two or three which provide some facility, in different states of maturity, and it's hard to know which to use in production. Often the duplication reflects some long dead turf war between researchers‚ÄîI don't want to inject that into my product. The supporting documents for libraries have been, in almost every case I've seen, remarkably unhelpful because they take the reader on an intellectual adventure revelling in the beauty and elegance of the implementation‚Äîbut are pretty short on indications of how to *use* the damn thing. Writing robust parsers with `parsec` *is* quick and easy, in fact, but you'd think the opposite from reading the docs. Which brings us on to **Code Re-Use** One of the lessons of CommonLisp, with its macros and MOP which also allow very high levels of abstraction, is that the people who inhabit a very highly abstracted codebase day in day out can be very productive in it, but that people who come in to one from outside take a long time to become productive, and therefore *changing jobs* becomes a very difficult thing. There's another problem with this, too: as one of my buddies said once: ‚ÄúI converted 1000 lines of Java into just 10 lines of Haskell and 990 lines of GHC language extension pragmas.‚Äù He exaggerates for comic effect, of course, but I've found that each of those clever, clever libraries that I use comes with a baggage of language extensions, and that sort of thing very properly disturbs mainstream development managers who like stable, well defined platforms. **No Sugar Daddy** IBM made FORTRAN and COBOL into indelible influences on the industry. Microsoft did a lot to make C++ a going concern, and now C#/.Net. Java had Sun and then IBM, and now Oracle are ensuring its place as the COBOL of the 21st century (and I mean that as a complement: there will still be Java jobs twenty years from now). Haskell needs a sugar daddy. It isn't going to get the things it needs to be successful in the mainstream without one. **Testing** QuickCheck is nice, but I find that teaching it what an instance of `Arbitrary`looks like for a type that anyone will actually pay me to write (rather than algorithms and data structures homework problems writ large) is hard work and brings the flow development to a stand-still. And I find that ‚Äúcorrectness‚Äù relative to problems that anyone will pay me to solve (rather than‚Ä¶) is often very hard to express in universally quantified predicates. **Code Re-Use, redux** *Re*-use must follow *use*. Re-use has proven to be largely a red herring in every other case, all the way back to the first FORTRAN libraries. Or, rather, the benefits of re-use are not the ones that its advocates promote. &gt;Reducing code duplication means developers can focus on building systems and designing algorithms rather than debugging the same code and patterns over and over again. Nope. Mainstream developers, the ones I know, anyway, spend surprisingly little time debugging anything, and certainly not ‚Äúthe same code and patterns over and over again‚Äù When haskell advocates start talking like this I being to wonder if they've ever seen the inside of a modern industrial development shop. And, further more, mainstream developers also spend little time ‚Äúdesigning algorithms‚Äù. Customers don't pay for algorithms, they pay for features. Mainstream developers *build features* and devising new algorithms is rarely part of that. I see no evidence that haskells strengths help much with *building features that customers will pay for*. 
What's the problem here?
Pigworker knows that, but ADT also means Abstract Data Type, which is a very different, nearly contradictory thing. This phenomenon is known as ADT, Acronym Denotation Tension.
We tried... the tech failed unfortunately.
Awesome!
I still don't know what your exact issue is
Ohhhhh. *smacks forehead*
Most of the differences lies in the defaults. Cabal-install and Stack are roughly capable of the same things, but while cabal-install organically grew, Stack was designed by trying to make the most common cabal-install workflows the default actions. Cabal-install will get you whatever latest version of a package it finds, even if it may conflict in a subtle way with some other package you also want. Stack by default gets you versions of packages that are meant to work together. This means you might get older packages, but they are likely to work together. Stack automatically creates and manages sandboxes for your projects, such that dependencies for different projects do not collide, which is a common problem when people do not opt in to sandboxes with cabal-install. It's also worth mentioning that Stack aims to create a more dependable build configuration by relying on "snapshots" of the package database. If a Stack build works today, it should also work without errors in five years, because it has access to the state of the database from five years ago. Cabal-install is trickier in that situation when you haven't version pinned your packages precisely right.
Well yes, it would be, if the metrics existed, which they probably don't because that's not how it works.
A definition for `fmap` isn't really arbitrary. In order to meet expectations, a Functor must behave in a certain way. For example, you will want the following law to hold: fmap id = id You could for example define `fmap` for list as: fmap f [] = [] fmap f (a:_) = [f a] But then the law wouldn't hold since then `fmap id [1, 2, 3] == [1] /= [1, 2, 3]`. You want to replace every value of type `a` in the datatype with `f a`. Such a definition can be done mechanically, and ghc can even do it for you! Using the `DeriveFunctor` language extensions, you just specify `derive Functor`, et voil√†, you have a `fmap` function for free!
&gt; The victory of C++ over Smalltalk had somewhat to do with performance figures, pre JIT and what have you, no interpreted language could come close to C/C++. Actually I don't think Smalltalk was ever a mainstream contender --- it was a weird research language with few implementations and running on specific hardware. The contender for C++ was C itself and maybe Objective-C. 
&gt; Cabal-install will get you whatever latest version of a package it finds, even if it may conflict in a subtle way with some other package you also want. ...care to give an example of what you mean by conflict? Afaik `cabal-install` can only select conflict-free package configurations unless they don't exist (or the package maintainers were inaccurate in their `.cabal` files). So if `cabal-install` can't find an install plan it's not its fault, but rather a garbage-in-garbage-out effect... Stackage on the other bypasses all of the build-dependency information contained in `.cabal` files, and imposes a radically constrained pre-computed inter-package dependency. So while `cabal-install` development continues to explore [more sophisticated dependency resolution schemes](http://www.well-typed.com/blog/2015/01/how-we-might-abolish-cabal-hell-part-2/) allowing for flexible package configurations based on declarative inter-package deps, Stackage insteads dumbs down the whole process to force everyone to use the very same package versions (Stackage is a bit like the Debian packaged subset of Haskell libraries, which are also constrained to one fixed version per package). No wonder Stack is able to progress fast, as they eschew the hard problems, and focus on the easy bits and user experience at the expense of Hackage.
Stack uses the same cabal-as-a-specification, hence the backwards compatibility, [FP Complete does write extensively about what and why they are doing stack](https://www.fpcomplete.com/blog/2015/06/why-is-stack-not-cabal) and stackage. Also they do mirror and use hackage, still, afaik.
&gt; Speed and parallelism: Measurements or bust. Seriously. There's no other way to make this argument. Also, what tools are there to instrument &amp; measure Haskell code? While I welcome this demand for empiricism in comparing programming languages, we should bear in mind that it was never the reason for uptake of any specific programming language or paradigm in the past. Regarding speed and parallelism: I think it's easier to make a case for Haskell for concurrency than parallelism. As long as performance is acceptable, I don't have to make any measurement to prefer concurrent primitives that cannot deadlock (STM) and do not need to structured using callbacks. 
When I say "conflict in a subtle way" I mean "a conflict that does not look like a conflict based on the dependency version constraints". For that reason, neither cabal-install nor Stack can *know* machinistically that there even is a conflict, but as people we know based on our testing, so the two versions do (hopefully) not end up in the same Stackage LTS snapshot.
The most important rule is - if you are reading more than one file and you care in any way about the order of events in time, you should be suspicious of lazy IO. If you are reading many files, you almost certainly care at least about the order in time of when files get closed, so you should be *very* suspicious of lazy IO. It's still possible to use lazy IO in those situations if you have solid guarantees that files will be read all the way to EOF and when that will happen. But it can be quite subtle to get such guarantees. And even if so, you had better make sure that the next person reading your code will understand completely what is going on. I still don't agree that lazy IO should absolutely never be used for this case - but I agree that it's *almost* never, and I won't argue very strongly against those who say absolutely never. (Hi there, /u/Peaker. :)) It is theoretically possible to keep using lazy IO in some even more complex situations by using `unsafeInterleaveIO`. In my experience, that is never worth it. Resist the urge, step back, and use a different IO library. Never do this.
So you're in fact referring to the garbage-in-garbage-out issue? But wouldn't it be better to fix the problem at the source, e.g. by fixing up the incorrect dependency meta-data on Hackage and helping the experience for everyone (not just those using Stackage), rather than encoding this in yet another external mechanism like LTS Stackage that only benefits Stackage users?
&gt; FP Complete does write extensively about what and why they are doing stack and stackage. Well, they give some motivation, including "The results are in and the 1,200+ respondents are unequivocal: package management with cabal-install is the single worst aspect of using Haskell." why they're doing Stack, but I still have no satisfying answer why they couldn't improve `cabal-install` instead. It sounds really more like they wanted the easy way of starting from scratch and building their own NIH-complaint thing rather than having to work together with the Cabal developers which are seemingly not open to collaboration and incapable of implementing a good UI with `cabal-install` as the survey "unequivocally" told them... Don't get me wrong, I do think that `cabal-install` needs to be improved, but I don't think we need yet another tool to add to the confusion of new users
Do you have a concrete idea of how to fix the problem at the source?
I was thinking of [curating the Hackage meta-data](https://github.com/haskell-infra/hackage-trustees) as the trustees do...
Pretty sure you're supposed to pretend python lists don't exist, and just write L[L[1,2]] for nested lists.
Ok, lemme put it differently, do you think that the `cabal-install` developers should just give up, as `cabal-install` doesn't fit into the strategic needs of fp complete, and thus won't get funded nearly as much as Stack, while Stack aims at replacing `cabal-install` and will eventually by economic consequence supersede `cabal-install`'s features sooner or later?
Okay. Couldn't the work done as part of the Stackage curation help the work of the Hackage trustees?
Probably not, because Stackage curation is just about selecting snapshots of the entire hackage database at once, whereas the Hackage trustees solve a much more general problem by fixing the constraints on individual packages.
&gt; I wanted to cram as much of Haskell into Python as possible while still being 100% compatible with the rest of Python, just to see if any useful ideas came out of the result. Did any useful ideas come out of the result?
The originator of Stackage and Stack, /u/snoyberg, actually participates actively in the cabal project. I don't think there is a problem here of duplication of effort. Stack and cabal-install offer different approaches, with different advantages and disadvantages. Both are high-quality tools representing a huge amount of work and thought. The real problem is confusion. Rather than focusing on the approach of each tool and what each has to offer, some people are busy arguing over non-existent defects of one tool or the other that they don't completely understand. I think the confusion is partly caused by an Internet negative feedback loop, where wrong or outdated information is repeated and thereby amplified and repeated even more.
Hey, author here. I vehemently agree that this is an issue. Part of the inconsistency actually does arise from the language--Python desugars L[1, 2, 3] to L.\_\_getitem\_\_((1, 2, 3)), i.e. the same thing as L[(1, 2, 3)]. Until I can think of a better way of doing it, the consistent solution is just to write L[[1, 2, 3]] instead of L[1, 2, 3]. This is very unsatisfying to me, though.
My issue with saying 'context' is that it's so generic a description as to become unhelpful for teaching. What does 'context' even mean? I feel we're going to start running into the _functor tutorial fallacy_ pretty soon, haha. Structure works better for me, though I feel structure and shape are close to synonymous.
Is there a simple explanation of optimal evaluation anywhere? In what sense is it optimal? Does it work on untyped lambda calculus, or does it come with restrictions? Can it be useful for general purpose code or is it more of a research trick?
Really enjoyed this presentation. Thanks Joe.
Oh, that's big topic and I'm not a PL researcher, but off the top of my head: It's almost as if someone set out to make the syntax of haskell as misleading as possible. `type` does not create a type. `data` does. Huh? `newtype` kinda does create a type but looks trivial while what it actually does is very subtle. Don't even get me started on `do` notation. I'm sure that half the confusion that half the monad tutorials try to fix is a result of what must have seemed like a clever pun when used with `IO` getting way out of control. A `class` is not a class, it's a declaration of a bunch of functions and maybe some relationships between them. An ‚Äúinstance‚Äù of a `class` is nothing of the kind, it's a specialisation of that bunch of functions. Pattern matching or `case`, pick one. (hint: pattern matching, maybe with a way to sugar it). A function defined using multiple patterns appears as a bunch of stand-alone equations. A function defined using guards appears as one equation and this clunky pipe notation. Why? Let me me have multiple equations, each with a guard Unnecessary parens. Since `deriving` is a keyword that introduces the last clause of type definition, why the parens if there are multiple type classes? Speaking of which, give me a way to make `deriving` work with my own type classes. I'm an adult, I can handle it. See also `module` and `where`, perfectly good delimiters there, why the tuple? There are plenty of perfectly valid and useful constraints on my program which the type system cannot express (mainly through lack of dependent types) which then leads me to write a ‚Äúsmart‚Äù constructor. At which point we've basically re-invented object orientation so stop pretending that FP is so superior that we never need such a construct. Just as some idioms used in procedural or OO languages vanish in functional languages, some idioms used in functional languages vanish in procedural or OO languages. A smart constructor and the functions which work with values of the type that it returns belong together and the language should help with that. Bet you a pound that all the way through this you were thinking ‚Äúbut that doesn't matter, because‚Ä¶‚Äù and if there were only haskell programmers in the world, you'd be right.
Yeah, could be. That boat has sailed, however.
Barring niche markets (like games) where performance is critical to the product, the only numbers that *really* matter to a business manager when it comes to hiring programmers are salary cost per head and number of already-trained programmers seeking jobs. They don't care about performance, library selection or maintainability. Java had no libraries until industry needed them. They picked up Java because hiring Java devs got really cheap, really fast, because Sun promoted the crap out of it, and it became the safe choice. Java is really about risk aversion.
What you describe as list comprehensions aren't. `[x + y | x &lt;- [1..4], y &lt;- [6,7,9]]` is an example of a list comprehension. 
I've known about reflection for two years and I still don't understand it.
My only real complaint against Haskell is the tooling ecosystem. Stackage is a step in the right direction. For industrial use, I need stable packages and dependency graphs. Also, we really need an IDE that is on par with Visual Studio, IntelliJ, and even Eclipse (beyond plugins for those IDEs). I understand the philosopher kings at the gates of Haskell are brilliant, but the average developer is, well, rather average; so much so that anything less than an equivalent to IntelliSense is a huge barrier to mainstream commercial adoption (that is just my opinion).
I think the source is on github, and I'm not sure if they have heavy proprietary licensing. I'm not sure what an NIH-complaint is, but it is important to realise that a new implementation that is spec compliant, or nigh-compliant can further refine the underlying specification. It is, and has the novelty, of being a faster moving vehicle to test these new systems out. Regardless, I think there is another movement to solve the problems of cabal-install by recombining with Nix, the packaging system.
I think Data.Reflection took me at least as long to understand. The Given typeclass is simpler, though less powerful, and I think less well-defined if you nest calls to `give` (I *think* it was going to be deprecated at one point). The API consists of two terms: give :: a -&gt; (Given a =&gt; r) -&gt; r given :: Given a =&gt; a If you have some expression that wants access to your configuration you use `given`, so you have `someExpr :: Given Config =&gt; SomeType`, and when you want to actually pass the config you use `give` to supply it. I grokked it better by noticing we're basically just converting `-&gt;` to `=&gt;`, which is essentially a function that the compiler applies arguments to implicitly (typeclass constraints are converted to functions in Core iirc). You can even write two combinators to make it a little more obvious: implicit :: (a -&gt; r) -&gt; (Given a =&gt; r) implicit = ($given) explicit :: (Given a =&gt; r) -&gt; a -&gt; r explicit = flip give Though `implicit`'s type is equivalent to `Given a =&gt; (a -&gt; r) -&gt; r`, and GHCi will rearrange the terms to make it a little less clear what's going on. The only difference Reifies adds is proxies to allow you to have more than one implicit value of each type.
Oh right, that's right in your comment sorry.
Almost all your complaints are with the names of keywords; changing them doesn't make anything simpler. What's left is a handful of syntax tweaks, and changing those similarly doesn't make for a simple streamlined language. I agree that Haskell has some daunting complexity. The biggest one to me is all of the cryptic infix operators like $, $!, =&lt;&lt;, !! etc, which can't be understood except to have memorized them. However I also think it's super easy as a layman to think "just take all this and cut out the confusing stuff", but in reality that's very hard to do. IMO (also as a layman so what do I know haha) a lot of the perceived complexity stems from the currying syntax which is fundamental to the language, and it's simultaneously a huge selling point for the language so there's no avoiding it. The syntax encourages short/terse function/variable names, which demands more memorization or documentation-hunting. Things like guards vs pattern-matching seem minor in comparison to me.
&gt; Do you have a concrete idea of how to fix the problem at the source? Don't upload garbage to hackage? ;) More seriously, packages should follow the [PVP](https://wiki.haskell.org/Package_versioning_policy). Hackage can also do some automatic checking and reject packages that don't follow it. There is a lot of [thought and work](http://www.well-typed.com/blog/2015/01/how-we-might-abolish-cabal-hell-part-2/) being put into this issue. There is also a Google Summer of Code project in progress to give Cabal/cabal-install nix-like package management. This has the potential to dramatically reduce many of the common cabal hell issues.
Of your cases against Haskell, the first 3, Library Availability, Community Support, and Hiring are chicken and egg problems. In particular, the Community Support point is particularly tenuous. The number of StackOverflow questions has little to nothing to do with how good the community's support is and everything to do with how many people are using the language. That just means that you'll have to ask a StackOverflow question rather than read one that is already there. And you have no evidence on whether that question is more or less likely to be answered and if so, how quickly. That is the real issue regarding Community Support, and based on my experience, I would guess that Haskell would fare quite well. The Library Availability point might have beet true 5+ years ago, but I think it's actually quite good now. We obviously have nowhere near the sheer number of packages that established languages have, but I'd be much more interested in seeing an itemized list of pieces that are important to industry. I imagine Haskell would actually do surprisingly well.
Yeah, Nix is beautiful. I'm using NixOS on my laptop. What's nice about Stackage is that it doesn't depend on declaring what people "should" do... Because that's apparently quite hard to ensure. And in the meantime people need to install packages. Mostly I'm replying to the somewhat rude tone of the poster who's railing against Stack as being antisocial or whatever.
&gt; I agree that Haskell has some daunting complexity. The biggest one to me is all of the cryptic infix operators like $, $!, =&lt;&lt;, !! etc, which can't be understood except to have memorized them. However I also think it's super easy as a layman to think "just take all this and cut out the confusing stuff", but in reality that's very hard to do. F#'s ¬¥&lt;|¬¥ and ¬¥|&gt;¬¥ instead of ¬¥$¬¥ and ¬¥&amp;¬¥ are easier to read, though. 
As a sometimes-pycharm user, which is considered cream of the crop for Python, I still find that many of the "Unix is my ide" arguments hold up just fine. The big loss in Python is resolution of things that can only be known by speculative code evaluation, but it really has never been an issue. The other side of the coin is that developing in an ide really locks you in once you start depending on it. Have you ever tried to change the way a c# or Java project is built from source without the ide that the build devs had? It's miserable. It's just not *hackable* and I can't tolerate it anymore. I'll take the Unix way over that mess any day of the week.
I find number of libraries to be a poor measure of library availability. There are languages with a lot of libraries (e.g. PHP) where a very high percentage of available libraries is not fit for its stated purpose, where you need to spend a very long time evaluating libraries and their weaknesses and flaws. There are also languages (e.g. C++) which tend to create huge libraries where other languages (e.g. Perl or Haskell) tends to create many small ones instead.
Enumeration sugar? IIRC, it gets desugared to enumFrom(To(By)) 
hmmm. AFAIK, stack only uses `Cabal`, the library, not `cabal-install`, the executable.
And for the kind of things that people do in Python, that may be fine. I think that you too are you're missing the point of what strong IDEs are for. It's not about clever editing and syntax highlighting, it's about blurring the line between edit, compile and run time, all the time. Strong IDEs really are about making languages that no way resemble Smalltalk or LISP usable in the same ways that Smalltalk and LISP are usable (and were twenty or thirty years ago). Python already resembles Smalltalk (a bit) and LISP (quite a lot) so of course IDEs don't help much there. Haskell‚Ä¶needs a strong IDE.
This. Programming language advocates love to argue the toss about this feature or that metric, but mainstream success means seeming like the safe choice: technically, economically and socially. Many of the opinions about what's important encoded in haskell point in exactly the opposite direction.
The release notes link yields a 404.
My vim setup gives me instantaneous inspection of types, insertion of type signatures, compiler/syntax errors/lint suggestions on save, and smart autocompletion. While it isn't as nice as a built-in REPL solution like emacs has, it's also not too tricky to get a GHCi REPL in another pane/window that I can send commands/code to for evaluation. I've never programmed in a LISP or Smalltalk environment, and I'm sure it's a case of "I've never used this before and can't imagine the utility" -- what am I missing?
Agreed -- I have been learning programming for under 2 years, and about as much experience with OOP, FP, and imperative programming. FP is *far* easier for me to understand, and the hard parts of FP tend to be a lot simpler than the hard parts of OOP.
In a Smalltalk environment all of these things are objects which (with only a very few restrictions) may be examined and modified at any time within one uniform environment: * all the classes which implement your application * all the classes of the libraries which came with the environment * all the classes which implement the environment * all the methods on all those classes * every previous version of every method and every class * the bytecode for all those methods * all instances of all those classes * all the threads running over methods on those instances * all the stack frames in all those threads * all the ‚Äúblocks‚Äù (Smalltalk for closures) created on the way the Smalltalk environment is really all of these things, live in memory, combined with a set of a smart, context-aware editors for each kind of thing, and completely seamless plumbing between all of those. It's hard to describe the difference this makes to how you write programs, but it is profound. Test-Driven development comes from this world, where it goes like this‚Ä¶ repeat until done: 1. tinker about in a workspace (basically a REPL) to figure out what I want to do 1. write a new test calling the code I want 1. run all tests, red bar 1. the debugger pops up, because the code doesn't exist yet. *Nothing else stops or breaks* 1. write a stub, in the debugger 1. run all tests, red bar 1. the debugger pops up, because the code doesn't do the right thing yet. *Nothing else stops or breaks* 1. make the code do what's needed to pass the new test, in the debugger 1. run all the tests, green bar! 1. use automated refactoring to get the code into a better shape. *Nothing else stops or breaks* 1. run all the tests, green bar! all this in the context of your already running, partially written system, and all inside one GUI window. 
That's exactly my point though: Scheme sacrifices most of the advantages that draw people to Haskell, replacing them with its own set of advantages. It's simpler than Haskell, but it's not a simpler Haskell, nor is it clearly superior to Haskell by virtue of being simpler in many ways. It's just different; that's not the same as making a better language by cutting things out of Haskell.
In my opinion, these types of analyses, while interesting, are pure speculation and not incredibly helpful. As someone who started learning Haskell about a year ago and is now using it a bit in production, I think the question that has to be asked is "what's the experience for a haskell noob as soon as they google 'haskell'?" In other words, if wider adoption is a goal, we should be looking at * What is the platform installation like? * What learning resources are available to new users and which ones are put front and center? * What are the IDE setups like (yes, this important to some people)? * Is there a list of popular/recommended libraries for various domains people might need for practical haskell? My point here is that theorizing about what might be holding Haskell back is, well, just that - theory. I recommend talking to people who seem to see the benefits of using Haskell but aren't using it yet for their work and asking why. 
Maybe it's more like he taught tofu to become a lion ?... :-)
My point is that a haskell with fewer distinctions made that don't need to made be *would* be a simpler haskell. It doesn't need to be Scheme.
&gt; My issue with saying 'context' is that it's so generic a description as to become unhelpful for teaching. That can be a problem. The only real solution, perhaps, is starting with concrete examples and only beginning to talk more abstractly once the learner is somewhat comfortable with them. An intermediate step in this process might sound sound like this: "A functorial value is made of values plus some other stuff. `fmap` changes the values, but leaves the other stuff untouched". (Saying "functorial value" rather than "functor" unfortunately makes the sentence a bit clunky, but I feel it is important to insist on the distinction. In "a functor is made of values" we find the the same conflation that happens when it is said that a `Monad m =&gt; a -&gt; m b` function "returns a monad".) &gt; I feel we're going to start running into the *functor tutorial fallacy* pretty soon, haha. Heh :) One additional aspect is that, while the functor tutorial syndrome is quite benign, it predisposes for the much more dangerous monad tutorial syndrome. In particular, the box metaphor, which admittedly is not *that* bad for functors, breaks down awesomely once you try to extend it to monads, as is well-documented. 
That seems like an odd criticism, seeing that lisp adherents also criticize mainstream languages for lack of syntactic regularity. Frankly, aside from infix operators Haskell's expression level language is pretty lisp-ish itself; seeing as we can rely on the expression level language to a larger degree than most mainstream languages. I'm not a fan of guard syntax either, but I don't think language X not being a lisp really has much effect on mainstream success, and I say this as someone interested in lisps.
THIS! So much this! I too find FP much more intuitive. I never quite wrapped my mind around all the different design patterns in OOP code because it always seemed like such a hack. OTOH, I've found Haskell abstractions and common idioms to actually make sense. Not saying that everyone feels this way, but that there is definitely a portion of people who do. It makes me wonder how many people never got into programming because 'programming' to them meant learning an unintuitive object-oriented language.
[This](http://devblog.avdi.org/2015/05/11/in-which-i-make-you-hate-ruby-in-7-minutes/) gives an example of the workflow /u/keithb just described.
I've also used Eclipse when writing Android apps and I have to say, I don't think I've ever seen what you're calling a "strong IDE." Eclipse certainly does *not* make Java feel like lisp.
Like [this](http://hackage.haskell.org/package/containers-0.5.6.3/docs/Data-Tree.html#v:flatten)?
Yes, Eclipse has lost its way. Back when Eclipse was still VisualAge for Java it was actually much better at that. 
See [The Optimal Implementation of Functional Programming Languages] book, it has the most. It works on the untyped LC. It is optimal in the sense that it needs the least amount of parallel beta-reductions possible to achieve the answer. I believe it has a lot of potential to be used in practice but I'm no researcher so who knows.
I'm confused by whether you want operations on a tree or operations on a list, because you gave two different type annotations for the two `flatTree` examples you present (the first one not typechecking). If you want something that is neither what you presented, but instead of type `NTree a -&gt; [a]`, then the simplest implementation is {-# LANGUAGE DeriveFoldable #-} data NTree a = Node a [NTree a] | Leaf a deriving (Foldable) flatTree :: NTree a -&gt; [a] flatTree = foldr (:) [] 
Not very. If this person meant stand-alone applications, then Smalltalk apps are distributed as a binary that the Smalltalk VM runs. Just as Java apps are distributed as a bunch of binaries that the Java VM runs. Just as .Net apps are distributed as a bunch of binaries that the CLR VM runs. "Enerprise" style apps can be deployed that way too, or by pulling changes into an already running system, hot swap style. Some people who haven't used Smalltalk think that the image and its history file is all you have, but that's not so. Smalltalk is developed, and teams of Smalltalk developers collaborate, using distributed version control (and have done since long before git or hg came along). And that version control is *semantic*, not syntactic, it versions classes and methods themselves, not a dead, flat, textual representation of them. 
Yep, it does the same thing in Hask. :) I may borrow that term, it's the best description I've seen so far.
&gt; In my opinion, it is better to show people that learning Haskell is actually just as difficult or less than learning any other popular language out there. But why resort to opinion? Can you present/produce tangible studies or results? Has anyone ever done such a study for *any* set of languages?
&gt; What's nice about Stackage is that it doesn't depend on declaring what people "should" do There are a lot of things that maintainers should do if they want their packages to be robust and broadly useful. One of them is that they should tell you what other packages their package depends on. Because packages change over time it's not enough to just say, "I depend on XYZ." You need to say, "I work with these particular versions of XYZ." If you don't say that, you're doing your users a disservice. If you go too far in the other direction and say "I only work with this one particular version of XYZ", then you've unnecessarily constrained what your users can do. This is just one of a number of things that you MUST do if you you want to maintain quality packages that are robust and can be depended upon by your users. I don't know about you, but if I'm going to go to the trouble of maintaining packages for broad use, I definitely want to take a little extra effort to maximize the usefulness and minimize the problems for my users.
Agreed, I don't see a ready-to-use implementation anywhere. I myself am still sad about the news of the compiler-rewrite, but that's just a personal sore-point and has no bearing on the many merits of the protocol.
Agreed! I hope that my finding Stackage a useful concept isn't read as saying that I think bounds are unnecessary. But again, I feel like I'm probably repeating things that have already been said.
Using `lens`: plate :: Plated a =&gt; Traversal' a a in `Control.Lens.Plated` provides the `children` functionality as toListOf plate :: Plated a =&gt; a -&gt; [a] while universe :: Plated a =&gt; a -&gt; [a] gives you `flatTree`. `universeOf` gives you the second form of `flatTree` you asked for, described in terms of a passed in `Fold`. universeOf . folding :: Foldable f =&gt; (a -&gt; f a) -&gt; a -&gt; [a]
Thanks! I'll take a look at the book. One thing that worries me a bit is that it seems strange to treat beta-reductions as unit cost operations. Don't they have unbounded cost when implemented on actual computers? How much computation can you pack into a single such operation? I don't know if that's a big obstacle in practice, just being curious.
Saying that stack was created due to NIH is very unfair IMO. Given that it has [grown organically from FP Complete's internal tools](https://github.com/commercialhaskell/stack/wiki/Stack%27s-origins), it would have been a waste - *of time*, for everyone else - to, instead of having released stack as it is, have waited until everything it does got merged into cabal-install.
These seem like very old instructions. Perhaps you should try here: http://www.yesodweb.com/page/quickstart EDIT: Oh I see you've tried the quickstart. Why did you also need the Yesod for Newbies instructions?
Code reuse is pretty easy to sell, even if you don't care about quality at all. When you reuse code, you get more done with less code, less time, fewer developers, and fewer bugs. Reuse is "better for business" since the costs are less. (Don't forget the morale cost of developers feeling demoralized at having to write repetitive code which they instinctively recognize as such.) What isn't better for business is developers building mediocre abstractions that don't get reused enough to pay for their development costs. Unfortunately, a lot of Haskell programmers have bad judgement in this regard, which contributes to Haskell having a dubious reputation in this area. OTOH, my impression is that most people who don't know FP have very little idea how to actually build good abstractions that allow for real reuse. So they don't really know what they are missing.
Because it's a tutorial ? I figured I'd start with the shortest thing(QuickStart) , move to the tutorial( longer) and then maybe try the book. Soooooo after doing the quick start what should I do then? Is there a more updated tutorial ? How did you learn it ? Any recommendations for a newbie ? 
This is pretty nice. can't we easily implement the "Finder" thing? sounds like taking input, inferring the types of it, searching hoogle for the right type signature and try running the input on all results comparing the output.
Oh I see, it's linked from the yesodweb site. Sorry, since it's so old I assumed you stumbled across it through a less official direction. Hopefully someone more knowledgeable can help you out.
FWIW I love qualified imports and use them almost exclusively, and I find code that uses implicit imports very hard to read.
I try to almost always use qualified imports and when I'm not, I specify what I import. One thing I noticed is that many tutorials and code examples I run into do not do this which is a shame. If you are planning on making a new tutorial or code example, please, use qualified imports or specify what you import! that way anyone can know exactly what comes from which module and this can save a lot of headache.
Great if Stackage works for you, but not everyone wants to be confined to the walled garden that the stackage subset represents...
&gt; ``cabal install`` There's your problem. You simply shouldn't be using ``cabal``. The quick start guide recommends using ``stack``. Do that. Debugging your cabal problem is not necessary nor worth the effort.
Fair enough... but then now would be a good time to start merging back functionality to `cabal-install`, if `stack` is only an interim solution till `cabal-install` catches up... or not?
In what sense? Church numerals in Haskell perform terribly compared to the optimal evaluator, but Ints/Integer obviously are in another level. My hope for optimal evaluators isn't even optimality, to be honest. Is how simple the algorithm is to parallelize. I think I'll work in a CUDA version just for the lulz and see what happens.
Try the same set of instructions but replace `yesod-platform` with `yesod`: cabal install yesod yesod-bin --max-backjumps=-1 --reorder-goals Depending on the specifics of your install you can probably get away without using the flags: cabal install yesod yesod-bin Sometimes, it's useful to use sandboxes but install `yesod-bin` in your user package directory as it offers the `yesod` command-line tool: $ cabal install yesod-bin $ export PATH=~/.cabal/bin:$PATH $ mkdir my-yesod-tst $ cd my-yesod-tst $ cabal sandbox init &amp;&amp; cabal install yesod Install directions in tutorials can get outdated. The yesod-platform is not the recommended way to use Yesod anymore but many of the instructions in the tutorial are still valid. You should cross reference what you read there with the first few chapters in the book to get comfortable with Yesod. Feel free to post questions on the yesod mailing list too.
As a relative beginner, these sorts of suggestions are very daunting and don't reflect the kinds of problems I would actually need to solve. That makes it less motivating to want to work on them too, as they seem divorced from "the real world".
You're absolutely not confined to stackage. You just have to edit stack.yaml to add things that are outside stackage. Stack makes it even easier to add "outside" things than if you're using cabal: you can directly add git repositories and even git *commits* within repositories. They will be automatically fetched and you can distribute the stack.yaml with your sourcecode to duplicate the same setup elsewhere. With cabal, you have to manually download and put stuff into a directory, and then add it to your sandbox. And, crucially, this has to be done manually *on every machine that compiles your project*.
http://i.imgur.com/omJYHde.jpg :-D
okay, so use stack instead of cabal in all the commands in the tutorial? Isthat what you're saying? 
AFAIU the codebase of nvim is better than the vim one. Is that a good reason to switch ? Or if you prefer why should I care ? I understand that it might be possible to do better plugins, but until I need this killer plugin I probably don't need to switch ( and to be honest I'm trying to use as few plugins as possible). However, there is one bug in vim which really bothers me : I use langmap and mapping to &lt;Plug&gt; mappings (used in certain plugins) use the langmap whereas they shouldn't. Basically because of setting langmap all remap to &lt;Plug&gt; doesn't work (if they contains one letter I mapped). If nvim can manage langmap properly I'll probably switch now !
&gt; you can directly add git repositories and even git commits within repositories. They will be automatically fetched and you can distribute the stack.yaml with your sourcecode to duplicate the same setup elsewhere. You can also do this with cabal and git submodules.
Where is Yesod mentioned in his comment? As a contributor, I have given time submitting patches and helping keep compatibility with `cabal-install`. It's still used in our CI and I have no interest in losing compatibility with `cabal-install`. I do not endorse the parent comment and I'm not sure why you are taking it there. 
I see it go both ways. `Data.Text`, `Data.ByteString`, `Pipes.Prelude`, `Text.Blaze.Html5`, and `Yesod.Table` (from my `yesod-table` package) all essentially require a qualified import to be used. On the other hand, record accessors and lenses tend to just prefix the names. I would encourage you to use qualified imports where possible. I think that it tends to be cleaner, and if it's a library that you expose to others, they can pick how they want the prefix to read. For example, I usually do: import qualified Data.Text as Text But a lot of people prefer import qualified Data.Text as T If the library author had just named everything `textLength`, `textIntercalate`, etc. instead, then individuals would lose the freedom to choose their own prefix. Of course, this isn't a terribly compelling argument, but it's one reason why I personally like qualified imports.
So we were always using protobufs, going back years. Thrift is a bit newer. I picked thrift just to not write the network mechanics for a newer more distributed version of one of our major components. Basically just not using thrift for encoding some of our messages. There are RPC calls that use a more conventional thrift message, but yes, one of them does look like what you wrote.
Yes. That tutorial is out of date. The quickstart guide will tell you exactly what stack commands to run. (The rest of the tutorial, besides the install part, is probably still valid.)
I'm not saying any language "won" over any other one with metrics. I'm saying they can help OP's argument.
The notion of "flattening" something into a single value (for example into a list) is captured by the `Foldable` typeclass. You can read "foldable" as "flattenable". The `Foldable` typeclass has the [toList function](http://haddock.stackage.org/lts-2.20/base-4.7.0.2/Data-Foldable.html#v:toList) as a convenient shortcut for folding into a list.
The problem is not with `cabal-install` here. `stack` will not fix it. Go run `stack install yesod-platform` if you don't believe me. The problem is with an inconsistent set of dependencies and following directions outlined in an older tutorial.
okay! Thank for your reply. I was just wondering because it seems like the instructions in the book [basics chapter] (http://www.yesodweb.com/book/basics) doesn't seem to mention starting a project and all that stuff that was done in the quickstart. 
Are you still having trouble or have you made it past the install stage? If not, what is your current problem after following some/any/none of the advice in the comments?
I refer you to the answer I gave some moments ago.
Sure, that'd be awesome. Unfortunately it seems not enough is done toward that end, because people apparently find curated snapshots an efficient usage of time. I personally prefer the interface of Stack, but I'd be happy to see cabal-install do sandboxing by default and not generate wildly different (and non-working) build plans for the same project only three years later because things changed.
Cool, just wanted to make sure you weren't stuck.
I'm mirroring his comment with a different choice of free variable. The issue is that the cabal infrastructure was developed assuming that packages follow the PVP. Some people came in and decided that it was too much work to follow the PVP and started publishing packages that don't have proper upper bounds on dependencies. Then they discover that their packages don't work well with the community infrastructure and complain that the infrastructure is broken. So now we have a fractured community of people who follow the PVP and people who don't. If you fix your choice of web framework and that choice happens to be Yesod, then we get this situation where users are being told they have to use stack. But if you instead flip the choice of free variables and fix yourself to the long used and community standard build tool cabal-install, then it makes perfect sense that you've ruled out Yesod since it does not follow the guidelines that cabal-install was designed for. Which would you rather use? A project that takes this "my way or the highway" approach, or one that tries to work well with all the established infrastructure?
&gt; I'm just recommending to follow the quickstart guide's recommendation to use stack. In that case, I'd say it's pretty dubious to rely on users using a build tool only a few months old and dismiss users who are using the community standard tools that have been around for years as not "worth the effort".
Wow! There's no longer any doubt you have an axe to grind here. Let me just point out that this guy didn't say he *wanted* to use cabal. He said he wanted to get *Yesod* installed. He probably doesn't know anything about cabal.
I've just pushed a commit to the book's website to mark the tutorial as outdated. Here are the sources of information on how to get started: * Quickstart guide: always up to date, what the site points to first, what users should follow * Paper book: by nature will fall out-of-date, but includes a comment that you should follow the quickstart guide * Online book: points to quickstart guide * Yann's tutorial: was in fact out of date. Other information on the internet may be out of date too, not much we can do about that. But we *can* make sure content pointed to from yesodweb.com is accurate. Thanks for bringing this up so we could fix this out-of-date link. __EDIT__ [Link to commit in question](https://github.com/yesodweb/yesodweb.com-content/commit/b056f584eb2c79175f059aa45532f0ce667d53b2). Will take a bit for yesodweb.com to grab the update, so don't be surprised if the old content is around for another hour or so.
I did grow up with OOP. I only knew OOP languages before hearing about Haskell. It's an unfortunate circumstance that a name has been thrust into popularity with a particular meaning when others exist. I don't blame you for immediately thinking of OOP when you hear names it has. But you don't have to be so combative.
Can this functionality be turned on (for debugging) and turned off (for production) just be changing imports, perhaps with CPP?
What choices does this make that differentiates it from other reactive libraries, like Auto and Netwire?
&gt; 1) Mixing typed and untyped functions (and functions with variable arity) is pretty terrible. You need everything typed to get much mileage out of the type system. That's actually a *very* interesting observation I'd like you to write more about somewhere... more permanent than a Reddit comment. The reason I find it interesting is because it seems "gradual typing" has become a very popular idea recently.
&gt; Let me just point out that this guy didn't say he wanted to use cabal. The user pasted a cabal-install command line, so it's pretty clear that something had previously given him the impression that this was the right thing to do. Maybe it has something to do with the fact that GHC ships with a package called Cabal? I don't understand why you're so quick to accept the user's choice of "Yesod" while at the same time so quick to reject the user's choice of "Yesod + cabal-install". Three months ago you would have been saying something completely different because at the time stack didn't exist and cabal-install was the universally accepted build tool for Haskell. My only axe to grind, if you must call it that, is that the efforts going towards stack would be better served if they were directed at contributing back to cabal. The stack-vs-cabal split is confusing users and this post and one other thread currently on the front page of /r/haskell both suggest that it is doing just that. I have no problem with separate tools built for the purpose of exploring possibilities. cabal-dev, cabal-meta, and others have done that with great success. Let's do that again with stack instead of creating this split that is obviously confusing users.
That's exactly what I had in mind. Something like #if DEVEL import Prelude hiding (head, tail) import Data.List.Located (head, tail) #endif ought to work, for example.
I would say I consider it reactive. I assumed that was the original intention, but I can only speak for myself.
Krapital ^^ It's a mess.
about refactoring: arguments like &gt; The compiler will make sure that any changes you make are reflected throughout your entire code base are true for any strongly typed languages (e.g. C++), and even though I agree Haskell is much more easy to refactor than C++, makes it hard to sell to C++/Java people : the ones who likes typed language don't see the benefit of Haskell and the ones who don't would prefer to move to dynamic language. 
Using apt-get on Ubuntu I still get the old platform. Now trying the generic tarball.
Well I recently watched a [talk](http://begriffs.com/posts/2015-07-22-essence-of-frp.html) by Conal Elliot, the creator of both the concept and term FRP, and he describes the original semantics. Most important were continuous-time, and a precise denotation. He was a little disappointed that this term has been muddied since then by ascribing it to system that do not have these properties. Since watching it, I have started trying to avoid muddying it further. He doesn't give a name for the alternative semantics, but I think "reactive" is at least adequate at describing a paradigm involving time-varying values, but it is a term that I use just because I don't have a better one.
Unlike Netwire, `varying` doesn't have implicit time or inhibition. A Netwire 'wire' written in `varying` would be something like Monad m =&gt; Var m t (Event b) where t is time and b is the value. `Event` allows inhibition explicitly. Unlike Auto, `varying` is small and more focused on tweening and allowing you to build data structures that change in a smooth manner over time (or another domain). Auto has a lot of extra functionality like serialization, etc. I purposefully made the `varying` API tiny in an effort to make learning it really easy. They're all capable of expressing one another in their own terms though, so they're all worth a visit.
Wow - there's a lot of these!
Could you explain what a plate is and where does the name come from ?
&gt; how to pass contents of `Behavior` to `reactimate` &gt; [...] &gt; I feel like `apply` is the one to solve it, but I'm not quite sure how it goes. How would you do it? I would attach its value to an event which occurs on every frame. To do this, I would indeed use `apply`, or rather its variant [`(&lt;@)`](http://hackage.haskell.org/package/reactive-banana-0.8.1.2/docs/Reactive-Banana-Combinators.html#v:-60--64-) which replaces the event's value instead of applying a function to it.
I thought the Haskell community might find this interesting.
Nitpick re the first sentence, Hughes' paper was from 1984 no 1989, though a version of it was published in 89.
So, given an event, we define a behavior by the event and then transform the behavior to event by the same event again. Is this what often happens?
Evan Czaplicki's talk (https://www.youtube.com/watch?v=Agu6jipKfYw) linked in frp-zoo is great! After watching it I would say that varying is definitely in his Arrowized FRP category. The main type in varying is an instance of `Arrow`. Signals are not connected to the world, and when a signal is not being run - it's cold. 
That `compareNormal` function bugged me an unreasonable amount, and rewriting it taught me to love monoids, so I thought I'd share the refactor :) compareNormal = foldMap comparing [ state , city , county , coordLat , coordLong , startDate , endDate , historicState , historicCounty ]
The name `plate` is a variation on the name of Neil Mitchell's `uniplate` library. In it he builds a single generic traversal combinator `uniplate` that can be used to extract the "shape" of a data type. The `plate` in uniplate is short for `boiler-plate`. In `lens` `plate` is defined to be a traversal of all the 'immediate children' and is supplied by a typeclass. It uses `Data.Data` to derive this definition automatically if need be but can be written by hand. The combinators in Control.Lens.Plated are designed to use such a self-similar traversal to let you use combinators like rewrite :: Plated a =&gt; (a -&gt; Maybe a) -&gt; a -&gt; a which takes your function and applies it, bottom-up to the tree in question, repeatedly until it returns Nothing, and builds up a new 'a' out of sub-trees that have all been normalized in this fashion. There are a couple of dozen other combinators for monadic rewrites, etc.
Neat!
Yes, that pattern does happen often. It's a typical way to implement a counter, for example: eventNetwork :: forall t. Event t () -&gt; Event t Int eventNetwork tickE = countE where countB :: Behavior t Int countB = accumB 0 ((+ 1) &lt;$ tickE) countE :: Event t Int countE = countB &lt;@ tickE The pattern allows us to create an output event whose occurrences occur at the same time as in the input event, but with different values. This can often be accomplished more easily with `fmap`, but if we need to keep some state between two occurrences, as we do when implementing a counter, we need to use an intermediate behaviour to hold this state. One thing which is unusual about your event graph however, is that it only consists of a single occurrence of that pattern, and nothing else. When I write an event network, it usually has tons of events and behaviours defined in terms of each other, and tons of occurrences of that pattern and others. Maybe you're not taking full advantage of the expressivity of FRP? Since you're implementing a tic-tac-toe, I'd would have expected a behaviour representing the current state of the board, a behaviour representing the currently-selected cell, an event representing the addition of a token to the board, an event representing a three-in-a-row, and an event representing a draw. Are you implementing all of your event logic as one giant state machine, represented by a pure transition function from one state to the next? FRP allows such a monolithic function to be divided into many smaller independently-useful pieces.
Nice! I'm trying Monster Proof, it looks like Dragon Box, but to learn proofs instead of Algebra and Geometry? I really liked Dragon Box, I'm glad to see that others are applying the formula to other domains.
&gt; I feel like Haskell is actually very fun. I don't feel this is an argument that belongs in an article about Haskell for industry. Aside from it being irrelevant to business needs, I'm sure Python was fun back in 1999 but now look at it. I feel like the idea of a 'fun language' is simply a language one doesn't have to use for work. (Unless one's work is fun already; then the fun feeling of the language you use can be maintained.) For example, every time I've been to Brisbane, it's been on holiday to visit family, so my impression of Brisbane tends to be of a really relaxing place where I'm not expected to do or care about much. If, on the other hand, I lived there and commuted to work every day, I'm sure my impression would be different.
Java programmer and Eclipse user here. Is it even possible to get a nice IDE given Haskell syntax? I mean, when I type User user = ..... user. the IDE knows I'm calling a method of User class, so it is easy to suggest. I don't know how such a thing can even be possible in a language when you need to type `function argument` instead of `object.method()`. The cleaner syntax of Haskell somehow makes it more difficult to assist. 
Assuming that the `Instrument` type the OP showed is actually the full type. Maybe it's more like: data Instrument = Bond Something | Stock SomethingElse computeBondValue :: Something -&gt; Value computeStockValue :: SomethingElse -&gt; Value
You will just get more options for auto-completion but they will be correct possibilities, unlike some IDEs which just try to guess if some symbol is available or not in that context.
Thanks
Great suggestions. Thanks. 
Thanks. I love all the suggestions on this thread. Obviously, more than one way to do things well in Haskell :)
Thanks for the OSX support.
Java programmer here, with a non corresponded love for Haskell. Lets say I want to try your approach: Replace an existing service with a well-defined protocol with one written in Haskell. Lets assume I (or someone) can write the Haskell code that replaces the Java code I have. My major fears are infrastructure related: Is there anything like a Java web server that I should use? Or should I run a stand alone application? How can I measure memory/cpu usage? how do I access MySQL servers? How do I run scheduled tasks? What about authentication/authorization? What about dates, timezones, DST? What about UTF-8? I'm not saying this problems are not solved. I'm not saying this information is not present in the wiki, or in RWH. What I'm trying to say (and maybe my English is not enough) is that most Haskell tutorials/books focus in the code part, and somehow forget the infrastructure part. You said "a few days of company time"... but most companies rather spend time on meetings than testing a new programming language. Also, there is the 'who is going to do it' problem. The people interested in trying Haskell are typically the ones the company can not afford to stop working in most urgent problems. Anyway, I need to re read RWH, become a real man, and try Haskell in production for once... 
Thank you always for your fast and detail answers, also reading my code and giving me great suggestion. The reason why my system is a state machine is because I've been so confused and paranoid that I could not step in the FRP framework, and honestly I've been confident with this usage. At this point I am not afraid of asking this question: when do you use behaviors over events? You suggested implementing these items within FRP: - a behaviour representing the current state of the board - a behaviour representing the currently-selected cell - an event representing the addition of a token to the board - an event representing a three-in-a-row - an event representing a draw What did make you use behaviors in the first two cases, and events in the rest? I've heard that behaviors are forever and events are temporary, but it doesn't quite make sense to me. Also it more confuses me that there are FRP libraries that don't distinguish behaviors and events. If FRP works without differentiating behaviors and events, what is the motivation or benefit for reactive-banana and us to do so?
[They're probably depending on the temperature decrease caused by the program exiting due to an uncaught exception.](https://xkcd.com/1172/)
[Image](http://imgs.xkcd.com/comics/workflow.png) **Title:** Workflow **Title-text:** There are probably children out there holding down spacebar to stay warm in the winter! YOUR UPDATE MURDERS CHILDREN. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php/1172#Explanation) **Stats:** This comic has been referenced 400 times, representing 0.5366% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_ctmbxcm)
&gt; It is not in github, so I don't know how to report it quickly. Unless the maintainer is a regular here, reporting it to /r/haskell will be even slower :) I'd try sending an email to the maintainer listed on [the hackage page for the project](http://hackage.haskell.org/package/cpphs-1.19.1).
I don't know if this is a CDN issue, but there's something wrong with the [source tarbal](https://haskell.org/platform/download/7.10.2/haskell-platform-7.10.2.tar.gz) (the one whose sha256 is 3afb7ce099023..); it's missing the `hptool/src/Releases2015.hs` file.
Maybe you should try to understand why people make that claim instead of your just claiming they're wrong. In the meantime, call me when JetBrains shows up with an IDE for Haskell. &gt; Many people claim that learning Haskell is like learning to program from scratch; although these people are wrong
Good idea.
&gt; Is there anything like a Java web server that I should use? There are lots of Haskell web servers to pick from. You can even just program using the `wai` interface to `warp`, which is the shared interface that all servers use and it's pretty simple to use &gt; Or should I run a stand alone application? I think whether or not to use a server or a stand alone application depends on your customer's needs &gt; How can I measure memory/cpu usage? Use the `ekg` library. It's an in-process monitor that reports all of these stats &gt; How do I run scheduled tasks? This is not Haskell-specific: you can use `cron`, just like with any other language &gt; What about authentication/authorization? `yesod` and `snap` have plugins for authentication and authorization that work out of the box &gt; What about dates, timezones, DST? The `time` library handles all of these &gt; What about UTF-8? Use the `text` library
This is neat! It is almost like we have stack traces! hspec-expectations now has source location information integrated, which is the #1 place I need source locations. Hopefully I can stop using my TH-based package: http://hackage.haskell.org/package/file-location
Since Hughes is a coauthor on this paper, I would tend to defer to however he wants to characterize when he wrote what.
So basically your problem with Haskell is you learned a different language first?
As I understand it (which is: not really), types disappear in one of the intermediate compile steps, so AFAIK those two types really are identical at runtime. I don't think they even wind up with individual dictionaries; [`hint`](https://hackage.haskell.org/package/hint)'s readme talks about *coercing* interpreted expressions to values, not simply using them, which implies that's resolved entirely during compilation as well. (I.e. the legal interactions with functions are baked into the coercer for each type, allowing the types themselves to be discarded after interpretation.) tl;dr: GHC compiles are slow because they do *a lot.* :)
&gt; regrettably Any chance you'd expand on that? I'm curious what gets lost that might be useful.
&gt; I don't like this style very much. Yeah, don't worry -- you seem to be in the middle of a "crawl before you walk"-style introduction which wants you to understand the implementation details before even *mentioning* all the nice ways we have of hiding them. This is a common pedagogical style, but strikes me as a rather odd fit for Haskell, because hiding implementation details is pretty much our thing. :) Don't get me wrong, it's still good to know what's going on under the hood; but you'll rarely see code like this in practice (pretty much never more than one or two lines of it), and I feel like new-Haskeller retention might be higher if that were made clear *before* the guided tour of the drive train.
This seems to explain the sitation: &gt; This paper dates from 1984, and circulated as a Chalmers memo for many years. Slightly revised versions appeared in 1989 and 1990 in the Computer Journal and the Year of Programming. http://www.cse.chalmers.se/~rjmh/Papers/whyfp.html
"Functional Programming" isn't a language. This is like claiming that music doesn't matter because people have different favorite instruments.
Maybe "fun" is the wrong word for this, but don't underestimate the benefit we get from working with tools that are joyful to use!
It's no surprise that people who've only seen C#, Java and (god help us) C++ aren't very impressed with Object-Orientation. But those are only pale reflections of the real thing, which is truly remarkable.
Very cool to see how easy it is to extend Esqueleto, having only just started using it myself. I was skeptical about it as well, but this helps dispel some of my worries.
What I'd be interested to see explored are tuples inductively defined on top of such a record representation. It seems to me that tuples should be inductive (essentially an HList) and that records subsume tuples (records with numeric field names). Much more work for the compiler though. 
Thanks for reporting this. In the mean time you can grab it from the github repo: https://github.com/haskell/haskell-platform
Since a `var` is also an `exp`, how about writing your parser to accept `exp = exp` and add a semantic analysis phase where you check that the `exp` on the left hand side of an assignment statement is really a `var`?
Thanks for the bug report (one of many!). http://hackage.haskell.org/package/cpphs-1.19.2
I don't see how following the PVP and specifying dependencies according to it can solve all problems related to build plans, without being overly restrictive. Only the lower bound can be reliably specified, but there's no 100% sure way to specify the upper bound, without going for the least yet unreleased. This often leads to spurious restrictions when the author forgets to bump them; while the package could probably work even after a breaking change, if it didn't use the functionality that changed. I've seen this frequently in packages depending on `lens`, like `diagrams` or `chart`, that more often than not can be forced to install by "--allow-newer" when they complain about the lens version.
I think they might be referring to [these](https://ghc.haskell.org/trac/ghc/wiki/Records) problems.
No argument there, but I think the fact that the first version appeared when it did is an interesting part of the story of growing interest through the 80s (and underlines its prescience). 
&gt; If you upload packages to Hackage you're supposed to follow the common set out rules. Most systems break down if everyone does as he pleases disrespecting common rules. I didn't mean disrespecting rules. Let's say package `A` depends on package `B`, and the author of `A` knows that it will only work with `B.0.0.*`. That can be safely put into the .cabal file. But if `B.0.1` is not yet released, there's no way of specifying the upper bound, other than going with `B.0.1`, which might often be throwing the baby out with the bathwater, as the changes in `B.0.1` might not actually break `A`, even if they are breaking in general. As package authors are not robots who constantly check all and run tests over all dependencies of their packages, there will be slips in the schedule that will force the users either to wait for the package author to bump the version, or to override the bounds themselves. Nothing short of a combined package database - semantic version control system, that could track changes to individual functions and detect breakage on this level could truly solve this (which would be cool by the way).
My understanding is that several stack features are things proposed to and rejected by the cabal-install authors: mirroring hackage via S3 and github (rejected as insecure), some other security features I forget rejected as the wrong path, and curated package sets rather than the wild west (you can see plenty of arguments about that in this thread). (Although I saw a proposal recently for cabal to allow folk to bundle several packages into one package to allow for some BackPack multiple versions functionality, and it was mentioned somewhere in the discussion that in principle you could then make a stackage version a single package, but I'm probably getting it muddled up a little. One of stackage's authors felt it was a mistake and would break several things that depend on a package containing a single version of the package, as it were.) TL;DR Cabal-install authors don't want it to do what stack does. 
Issue there is that you're allowing assignments to function calls, like this func() = 1 Which isn't allowed in Lua's grammar.
All merged now, so yes they'll be in 7.12
The associativity of all these left-to-right operators is `infixl 1`, which means you can just write them: a &amp; b &lt;&amp;&gt; c &gt;&gt;= d &amp; e &lt;&amp;&gt; f And it just reads left-to-right nicely. I disagree about giving intermediate names. When you have a linear data-flow, it's much nicer to see it as a visual linear pipe-line, than to decipher the linearity back from finding the variable names (and seeing they're not re-used elsewhere).
Though it's worth mentioning that in the presence of optimisations... anything goes. Maybe tuples trigger a different set of optimisations. Maybe you can `{-# UNPACK #-}` records. The only way to know for sure when it comes to the final build is to profile.
&gt; There's a cost associated with using implicit call-stacks, since GHC has to pass them around Has anyone measured this cost? EDit: Great work on the library, btw!
&gt; Mainstream developers build features and devising new algorithms is rarely part of that. I see no evidence that haskells strengths help much with building features that customers will pay for. Algorithms are a means to an end. You cannot implement features without using the right algorithm, occasionally having to design your own algorithm. All the benefits of functional programming, like easy to refactor, better modularity, etc... will certainly make *implementing features* easier. Of course the process of design features, testing user interfaces, will be the same in any language.
[Here](https://www.reddit.com/r/haskell/comments/3f4s3r/advantages_and_disadvantages_of_haskell_in/ctlxky9) If people want haskell to be successful in the mainstream then they are going to have to do better than "yeah, we use all the words you're used to to mean very different things‚Äîso sad that you had to waste your time learning some other language before haskell, suck it up.‚Äù
Can someone paste this somewhere that doesn't require nonfree javascript to be read?
These are unsupported assertions. Haskell has a *terrible* module system, little better than Java's, and the [work to fix it](https://ghc.haskell.org/trac/ghc/wiki/Backpack) is substantial and only just now starting to near fruit. I would like to see some demonstrations of how haskell code is intrinsically easier to refactor than either Java or C# accompanied by their automated refactoring tools. It might be *harder* to design and test features in a functional language if those features do not naturally map onto mathematical functions, as many do not. *Edit: typo
Well, in a complete Haskell program which type checks (not in ghci) I think all the types are just inferred, so the type of sequence [] would be inferred from where it is used. Defaulting always to the same type will not give the correct answer when another type is expected: Prelude Data.Maybe Data.Either&gt; isJust (sequence []) True Prelude Data.Maybe Data.Either&gt; isLeft (sequence []) False Prelude Data.Maybe Data.Either&gt; :t isJust isJust :: Maybe a -&gt; Bool Prelude Data.Maybe Data.Either&gt; :t isLeft isLeft :: Either a b -&gt; Bool 
https://mail.haskell.org/pipermail/haskell-cafe/2015-July/120605.html
Thanks so much!
I like this but I'd like to hear more about why ExceptT is not satisfactory. I find it pretty easy to use and rather powerful actually. I'd hate to see fragmentation on this in common libraries - but the point remains that there is a lot of code that does throw exceptions and is at best documented in a Haddock. 
Michael discusses this in his blog post https://www.fpcomplete.com/user/commercial/content/exceptions-best-practices, although some of his objections to `ExceptT` also apply to my proposal. The most important problem with `ExceptT` in my opinion is composability. 
There is nothing wrong with impredicativity in principle, but it's very very hard to combine with type inference. There have been tons of attempts in the literature (boxy types, HMF, MLF, ..) and none have emerged as a clear winner (although I rather like HMF myself, but I might be biased since I worked on a type checker for HMF :). The interaction with type classes and type families is also far from clear, as is discussed at length in the document that Ben links below. 
Thank you for taking the time to answer. May I ask some follow up questions? &gt; There are lots of Haskell web servers to pick from. I know that. But would be the obvious choice? I mean, in the Java world, most of the time you end up using Jetty, Tomcat or, if you want more tools integrated in the server, JBoss. What is the most popular Haskell option? Being the most popular is somehow important: that means you would probably find more information more easy when you have a problem And one note: &gt;This is not Haskell-specific: you can use cron, just like with any other language In the java world we typically use some library (that may relay on cron or not) 
While I'm sympathetic to offering users a choice... how do you explain newcomers that there's the standard cabal frontend tool, and then there's a 2nd newer tool? Do you patronise and claim that `stack` is the successor to `cabal-install`? Or do you provide a more impartial description of all the pros/cons of one tool over the other to help the user make an informed choice? Cause right now I get the impression that the former is happening more and more, maybe also because as we all know, [new is always better](https://www.youtube.com/watch?v=rxBvgCyERkw) applies to everything.
Well I mean giving the pipe a name, so the fact that it's linear is should still be clear. For instance, I prefer: descriptive &lt;=&lt; meaningful $ a where meaningul = fmap b . c descriptive = fmap f . e . d With whatever types are appropriate (this frequently requires ScopedTypeVariables). Granted, style discussions are inherently subjective; I just find it a lot easier re-reading my own code later when I've written it in this style.
Yeah. I wasn't aware of the workflow afforded by that environment. Sandi Metz has done a pretty good job communicating the sort of software design that goes on in the Smalltalk world to Rubyists, and I know my own Ruby code has gotten significantly easier to work with after learning and studying it. Why are all the good languages so underrepresented D:
Michael presents three problems with `ExceptT`: 1. It's non-composable. If someone else has a separate exception type HisException, these two functions do not easily compose. 2. It gives an implication which is almost certainly false, namely: the only exception that can be thrown from this function is MyException. Almost any IO code in there will have the ability to throw some other type of exception, and additionally, almost any async exception can be thrown even if no synchronous exception is possible. 3. You haven't limited the possibility of exceptions, you've only added one extra avenue by which an exception can be thrown. myFunction can now either throwE or liftIO . throwIO. Your proposal deals nicely with 1, in a similar way that `MonadError` does but without requiring any monad. It seems to me that 2 and 3 are actually the same issue but, as noted in the passage below, your approach does not deal with them: &gt; a type such as simpleHttp :: (MonadIO m, Throws HttpException) =&gt; String -&gt; m ByteString &gt; does not tell you that this function can only throw HttpExceptions; it can still throw all kinds of unchecked exceptions, not least of which asynchronous exceptions. But that‚Äôs okay: it can still be incredibly useful to track some exceptions through your code.
Here are a few places in which I've used Writer in my projects. [cabal-rangefinder](https://github.com/gelisam/cabal-rangefinder#readme), a tool to fill in the version ranges in a cabal file, modules [CabalFile.Parser.Types](https://github.com/gelisam/cabal-rangefinder/blob/8a04ec6244d392f829644a9e95f21af06d3eb230/src/CabalFile/Parser/Types.hs) and [CabalFile.Parser](https://github.com/gelisam/cabal-rangefinder/blob/04a32955fa442eddbfa598aff8afa4d3bc20b7bc/src/CabalFile/Parser.hs). type Fragment = Either String Dependency type Cabal = [Fragment] -- | The string which remains to be parsed, the pieces which have already been -- parsed, and a result. Or, if unsuccessful, Nothing. type ParseC a = StateT String (WriterT Cabal Maybe) a I'm parsing a `.cabal` file into a list of fragments, each of which is either a raw String which I want to preserve as-is or a range of versions for a particular package which I want to tweak. My parser is expressed via a stack of monad transformers which includes a Writer. I use the Writer part to accumulate the fragments as I successfully parse them. [Hawk](https://github.com/gelisam/hawk#readme), a Haskell text processor for the command-line, modules [Language.Haskell.Exts.Location](https://github.com/gelisam/hawk/blob/153c4bbec8991bb00eff913954d91bfac59a4aaf/src/Language/Haskell/Exts/Location.hs) and [Data.HaskellModule.Parse](https://github.com/gelisam/hawk/blob/153c4bbec8991bb00eff913954d91bfac59a4aaf/src/Data/HaskellModule/Parse.hs). -- | A value obtained from a particular location in the source code. -- -- The location only indicates the beginning of a range, because that's what -- haskell-src-exts provides. type Located a = Writer (MinPriority SrcLoc) a The package [haskell-src-exts](http://hackage.haskell.org/package/haskell-src-exts) can parse Haskell code into a bunch of datatypes, many of which have a field of type [SrcLoc](http://hackage.haskell.org/package/haskell-src-exts-1.16.0.1/docs/Language-Haskell-Exts-SrcLoc.html#t:SrcLoc) indicating where the construct represented by the datatype was found in the source code. I want to group a bunch of those datatypes together in order to delimit the part of the file which contains the imports, the part of the file which contains the module declaration, etc. To do this, I need to combine the SrcLoc values of all the pieces I am touching while doing some other grouping operation on those pieces. To do this, I wrap the pieces in the above Located monad, which is just a Writer which keeps track of the earliest SrcLoc I have encountered. Hawk, module [Control.Monad.Trans.Uncertain](https://github.com/gelisam/hawk/blob/153c4bbec8991bb00eff913954d91bfac59a4aaf/src/Control/Monad/Trans/Uncertain.hs). newtype UncertainT m a = UncertainT { unUncertainT :: ErrorT Error (WriterT [Warning] m) a } I am compiling Haskell code using GHC, which results in a bunch of warnings and errors. I also do my own processing of the command-line parameters, which can result in warnings and errors as well. To unify the way in which I report those warnings and errors to the user, I have created the above monad transformer which includes Writer. I use the Writer part to accumulate the warnings as I encounter them.
Yes, you are right. I consider these to be less important though: For 2: It is useful that we can have both checked and unchecked exceptions (and indeed, in Haskell this is an absolute necessity since we have asynchronous exceptions). For 3: We can _never_ limit the possibility of exceptions, no matter what we do, unless we get rid of `throwIO` and `throw` completely. (That might in fact be worth considering, but would of course be a rather disruptive change.)
leaving off upper bounds is even worse, as when a breakage occurs cabal isn't able to find *any* install-plan, not even the install plans that worked in the past. And fixing a missing upper-bound potentially involves editing *several* cabal files on Hackage, and if the author was too lazy to add an upper bound in the first place, she'll probably not even care about adding upper bounds after the fact. In the meantime, users suffer (who get confronted with hard to understand compile errors, rather than having cabal install just select a working albeit older install-plan), and packages depending on the broken package get transiently broken, thus causing bug reports filed against innocent packages... But of course there's salvation... just use Stackage! Throw away all the effort that went into maintaining PVP-compliant build-dep bounds, and solely rely on package sets. And while at it, why declare build-dep versions bounds at all? We don't even need such a full cabal solver if the stackage-sets already tells us exactly which version to pick for each package... Just like package managers such as `dpkg` only have a trivial dependency resolution algorithm... and actually, why not have just a big docker image with all Stackage packages preinstalled? Then you don't even have to *install* any build-deps anymore, ever! 
&gt; But of course there's salvation... just use Stackage! Throw away all the effort that went into maintaining PVP-compliant build-dep bounds, and solely rely on package sets. If this can save me from having to sandbox everything and having to waste literal hours installing the same packages over and over and over again, then sure.
By all means, do that (as a user)... but please, don't upload packages to Hackage then if you are not willing to follow the PVP (which is expected to be followed by package authors uploading their package to Hackage), and as I understand it, Stack indeed supports distributing packages via Git-urls, so there's no need to upload to Hackage anyway if you're not willing to meet the quality standards for Hackage.
One very valuable use I've found is keeping track of Surrogate-Keys in order to invalidate pages in our [CDN](https://docs.fastly.com/guides/purging/single-purges). Every object comes out of the database with it's key. Writer (Vector Key) ObjectType And these all get accumulated together so they can be sent in a header as part of the response.
I was under impression that [Stackage has made the upper bounds part of the PVP obsolete](http://www.yesodweb.com/blog/2014/11/case-for-curation) and I didn't realize people have such strong feelings against it, given that I've never experienced problems with it (other than having all Cabal hell break loose when I needed packages outside the curated set, and going back to sandboxing everything - and I think Stack solves this by allowing to mix sandboxes and a central package database, though I'm yet to try it). Perhaps the right thing to do would be to continue using upper bounds at Hackage, and make Stack just ignore them. It would be a shame to split the community because of something this stupid...
Is there any reason that same solution wouldn't work in Haskell? I can't think of a reason there would be any issues, though in writing a toy function I did need FlexibleInstances in some cases, so maybe that's hiding some bigger issues.
1) Since data types are cheap in Haskell, why not just lift the two exceptions to an appropriate ADT? Alternatively, I wonder if a trick similar to the article one can be played with the exception type of ExceptT, keeping around a type level set constraint about it's possible types. 2) I find this weak. Anyone understanding exceptions in Haskell will understand the guarantees.
Yes, I agree with you. It's not my point; it's Michael Snoyman's. (Link on Edsko's post above.) 
Thanks to you and others! This is the thing that I was missing for my use case, and now was able to compose: -- get signal and event, replace contents of the event with signal value substEvValue :: Wire s e m (b, Event a) (Event b) substEvValue = mkSF_ $ uncurry (&lt;$) 
Yeah, but I *think* you wouldn't need to actually need to compose the instances to get use out of it, provided you delay what you're coercing *to* as long as possible. Not so much a generic From but one specific to monadic error handling, since when you choose a specific type for your monadic stack there will be multiple inputs to the coercion function but only a single output. Though `catch` might be harder to write. I suppose I'll try and see if I can make something workable.
You're right, exactly what I showed would be pretty silly. Imagine, however, if Instrument was actually a very complex type where computeValue was 90% the same but had a small function for the exact instrument type that has to be run, and that small function is also useful stand alone in other contexts.
Ah, that might work. You will presumably build up all sorts of constraints like (From FooError a, From BarError a, From BazError a) =&gt; ExceptT a r but maybe you're OK with that. I suppose you're effectively getting an open sum type.
Sorry, I lost track of the conversation. I probably shouldn't have made that comment :)
Usually every mainstream success can be summarized by one clear advantage. For example, in the case of Java the clear selling point was "garbage collection". If you can't explain one single, concise, and powerful reason for Haskell adoption that appeals to a broad swath of programmers then the language won't break into the mainstream. Edit: Some other examples of how mainstream languages concisely sell themselves: * Python - "beginner-friendly" * C - "low-level" * C++ - "fast" * C# - "desktop applications" * PHP - "easy way to build a web site" * Javascript - "only language supported by browsers" * Perl - "scripting" * Shell - "installed on every system"
I like this. Would prefer that it worked in any `MonadThrow m` monad, because a lot of code "throws" things but isn't in IO directly. class Throws e where throwChecked :: MonadThrow m =&gt; e -&gt; m a
It's no problem.
Hi /u/bossboom, the resolution for this issue is to use `stack --resolver ghc-7.10 setup --reinstall`. Details here: https://github.com/commercialhaskell/stack/issues/695
This article prompted me to look again at some snippets I wrote recently... and I discovered that with GHC 7.10 I can now write (with DeriveAnyClass): data MyError = MyError deriving (Show, Exception) Unfortunately I'm a bit confused... Exception has 2 constraints: Show and Typeable. Since DeriveAnyClass is built upon Generic... I was afraid that there might be some clash between it and Data.Typeable But now I tried again, and I can simply do data MyError = MyError deriving (Show) instance Exception MyError eschewing Typeable altogether... even if by checking in ghci, Typeable is still among the constraints: class (Data.Typeable.Internal.Typeable e, Show e) =&gt; Exception e where does anyone have any idea why is that?
yesod-platform is deprecated, that's the problem here: http://www.yesodweb.com/blog/2014/08/deprecating-yesod-platform Using stack to install it is highly recommended because it defaults to pulling dependencies from Stackage Server, and they're guaranteed to be compatible. You can use cabal-install directly but you'll need to first install yesod-bin, alex and happy globally and then install the rest of the dependencies in your project's cabal sandbox. The stack way is not only easier, it also takes *way* less time. When I installed my yesod project with cabal sandbox it took about 2 hours to download and compile all dependencies and build the project; stack reduced this to like 10-15 minutes.
It's because `Typeable` is now generated automatically for all types (and giving a user-defined instance is actually an error).
I love it. At least comparing to the alternatives. Somehow I am wondering if the community will ever succeed to standardize the practices around "error reporting". I found the "x different ways" so awful readability wise. 
Not to my knowledge, so I put together a small micro-benchmark. https://github.com/gridaphobe/located-base/blob/master/bench/Bench.hs http://goto.ucsd.edu/~gridaphobe/call-stack-bench.html Take the results with a large grain of salt, as I have very little experience with micro-benchmarks. But it looks like using the call-stacks in a tight loop is (unsurprisingly) bad for performance; on the other hand, using them in functions that succeed or crash immediately has a smaller, but still noticeable, impact on performance.
&gt; I don't think they even wind up with individual dictionaries What does this mean?
keywords/syntax is not what's keeping haskell non-mainstream. it's an IDE, learnability, etc.
not vinyl's records themselves. but there are vinyl-style things built on top of it that _do_ collapse things down. which i think we're agreeing on actually :-)
I had looked at that. Unless I'm misreading something, I believe that parser has a mistake, in that you can assign to a call expression. func() = x This is what I'm trying to avoid, as it is not correct in Lua's grammar.
but unsafeSqlFunction violates encapsulation, they should never have exported it!!!11!1!1!111!1! 
an Earley parser (not Parsec) handles left recursion https://github.com/ollef/Earley/blob/master/README.md
I agree with your second sentence. Can you agree that needlessly confusing vocabulary does not help learnability?
If you want haskell to succeed in the mainstream then you need to make it appetising to people for whom it is their third or fourth language.
link? that sounds cool.
agreed. I write all my web apps in x86. why won't haskellers shut up about "sharing datatypes between client and server" and all that scripture?
In order to view the google groups web-UI in a reasonable way, your browser has to execute supposedly non-free JavaScript code. It's important to be at least aware of this issue so you can make a conscious decision whether you are ok with (ignoring) the software's license terms you are implicitly accepting when executing remote javascript code...
Where are those operators, like (@&gt;), from? Is that groundhog? Also what exactly is tell..is it kind of like put from State where you just set the state which in the case of Writer is the accumulated w?
Note that representing Writer as a tuple like `WriterT` and like the above makes every `&gt;&gt;=` leak as it has to keep around the left result to `mappend` it to the right result when the RHS is done. For most cases, it is more efficient to encode Writer semantics on top of `StateT` (except you have to do a bit of special stuff for `listen` and `censor`). 
While not logging in the usual sense, we have a bunch of service calls that include WriterT in the stack and write out events that are used for analysis that way. It's handy for two main reasons: * The values written form part of the result (as opposed to invocations of methods that in conventional code would need to be mocked), so are more easily tested. * A lot of these services produce multiple events, which if they were dispatched in IO would cause multiple HTTP calls to the service handling these events. With WriterT they can be accumulated over the entire lifecycle of the request and sent in one bulk request once it completes.
It's much more heavy-weight (in terms of extensions, and requiring a specialized monad), though.
Ah i see. It explicitly fails when it detects it parsed a non-var. Works I guess. I just prefer that it only can parse vars.
Yes. /u/tekmo [wrote this up very well](https://mail.haskell.org/pipermail/libraries/2012-October/018599.html). The reason Yesod still uses the approach above is: * The memory leak is almost non-existent in our case, since we're not using datatypes that can collapse (all of the datatypes are `Builder`-esque, so collapsing thunks won't help) * Every attempt I made at optimizing this with a different Writer-like implementation made performance work NB: I did this work years ago, and could be getting the details all wrong. Caveat emptor!
Just gonna let you know that Haskell was my 7th language (after QuickBasic, C, C++, Java, C#, PHP) and I never felt confused. Honestly, thinking that class means the same thing in even Java and C++ is a mistake, so I tend not to carry too much vocabulistic baggage from one language to another.
So you're telling me this code is safe to give to another Haskell compiler? And will continue to work in future versions of ghc?
People who've learned 3/4 languages can handle different vocabulary. Haskell was my 5th or 6th, depending on your opinion of Matlab.
This is super helpful. Thank you. 
The problem I'm describing is another separate problem from the laziness problem. It's an associativity problem. (a, _) &gt;&gt; ((b, _) &gt;&gt; (......))) Becomes: (a &lt;&gt; (b &lt;&gt; (c &lt;&gt; (....))) So it necessarily leaks memory just to satisfy this associativity of mappends. By using `StateT` instead of `WriterT`, you reassociate it to be: ((...(a &lt;&gt; b) &lt;&gt; c) ...
yeah, I just don't think that the keyword vocabulary is needlessly confusing. in good faith, I will say that the documentation for the maybe instance for monoid (https://hackage.haskell.org/package/base-4.8.1.0/docs/Data-Monoid.html) is kind of absurd. We shouldn't remove it, we should just say the same thing less formally in plain English. Now that I know what this random mathematical extraction means, the jargon is actually helpful. Makes it easier to Google. I just have to get around to submitting a documentation patch :-) 
Aren't you making some assumptions about the `mappend` action for the given type?
I think the cases where the asssociativity of `WriterT` is fine is when your result is in a pure/lazy monad *and* you have a lazy `mappend`. That means you might be able to lazily consume the eventual mappend result and that will evaluate the monadic binds and mappends as you read the eventual result. But for monads based on `IO` or `ST` (or other strict monads) this will not work, because you will have to force the entire RHS to be evaluated *and* executed before you can finally GC the left hand monoid result (and I think this assumes nothing about the particular implementation of `mappend`).
Are these comments related at all to the implementation I linked to above from monad-unlift? It seems like you're just talking about the official monad transformers from the transformers package.
Wouldn't it be possible to achieve this with ImplicitParams, ie. type Throw e = forall a. e -&gt; IO a launchMissiles :: (?throwIllegalGlobalDestructor :: Throw IllegalGlobalDestructor) =&gt; IO () ... to avoid the reflection black magic?
Oh! I didn't mean they would share one. "Individual" was a redundant word, there. What I meant was, I don't think dictionaries are still a thing at all in the final binary. (And that's just a guess anyway, I'm already fuzzy on the details at the STG level.)
I'd say that checked exceptions are an exercise in futility, especially when it comes to pure code. Imagine this use case: at :: (Throws NegativeIndexException, Throws TooLargeIndexException) =&gt; [a] -&gt; Int -&gt; a In most cases, the programmer will make sure that `at` only receives legal indices anyway, hence catching exceptions will not be necessary. The constraint, however, necessitates one of two solutions: 1. Write some probably non-sensical handler. In most cases, the function using `at` won't be able to do anything about the problem, nor will it be able to meaningfully report it to its caller. 2. Let it bubble up. Here, we have the opposite problem: the exception will be passed beyond whatever function had control over the index in the first place and go upward to God-knows-where. We thus end up with some main function that contains dozens of handlers that do nothing because they can do nothing. In both cases, the alleged benefit of handling all corner cases is negated by the habit of writing empty handlers, which checked exceptions promote (see `catch (Exception e) { }` in Java). Then, one is better off making the handling of errors optional - then, at least one won't be lulled into a false sense of security. A better solution would be to enforce that the originator of a value, or one of its sub-functions, be responsible for handling illegal values and even then, a warning would be preferable to an error. Checked exceptions are too weak to provide this, however.
Esqueleto is great. I can't even express the feeling I had when I first changed a database schema and had GHC report to me all the queries that needed to be updated.
I always use `MonadThrow`. It's easy, it's generic, and it allows arbitrary types of exceptions, including exception hierarchies. W.r.t. the "x different ways": I guess you're referring to [this](http://www.randomhacks.net/2007/03/10/haskell-8-ways-to-report-errors/) * `error` is a non-starter. It should only be used to indicate bugs. * `Maybe` and `Except` are subsumed by `MonadThrow` - both are instances. * `Monad.fail` is of the devil. Also, it only allows string. * `MonadError` can't deal with asynchronous exceptions, and it doesn't use the `Exception` class. The latter is a big problem if you want to catch many different types of exceptions, which occurs when you're using other people's libraries. Rolling your own `MyGenericException` never ends well. * `throwDyn` lifts everything into the IO-monad. Unacceptable. To summarize the benefits of `MonadThrow`: * Generic: the caller can receive the error via Maybe, Either, IO, or a monad transformer. * Can throw any exception, not just strings or pre-defined types. * Provides good interoperability between libraries: one can natively catch whatever exceptions some library throws, and add one's own exceptions to the mix. `MonadError` doesn't allow this because it requires us to specify the type of the exception. * `Exception`, being a type class, is extensible. See the previous point. * Easy to use via `throwM`/`catchM`.
This is the most understandable explanation I've ever read. Thank you for your answer a lot. I will make two versions of this program and see how FRP changes the readability. 
Herbert's PPA is awesome, but it provides only GHC and cabal-install, not the full HP.
Do you mean `a -&gt; (a -&gt; [a]) -&gt; [a]`? 
Thanks for the link. Gabriel's proposal seems great, I wonder why it didn't get in?
I'm not an expert but based on what SPJ said in a talk once I think you're wrong about that. I think the dictionaries *have* to exist at run-time to support polymorphic recursion. (Hopefully someone who really knows what they're talking about will say something now.) EDIT: found it: https://www.youtube.com/watch?v=6COvD8oynmI#t=27m35s
We strive for a major release approx. every six months, so 1.24 should be out in one or two months' time.
How is this different from http://hackage.haskell.org/package/control-monad-exception ?