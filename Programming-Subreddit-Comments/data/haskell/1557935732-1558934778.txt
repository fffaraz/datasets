Literally any beginner-level intro to Haskell will explain the meaning of type variables in the first chapter or so. Congrats if you manage to not read any of them and still find the guts to write a rant about how Haskell is not beginner-friendly.
&gt; I see the clear advantage of having IO made explicit in the type system in applications in which I can create a clear boundary between things from the outside world coming into my program, lots of computation happening inside, and then data going out. Like business logic, transforming data, and so on. [...] But the difficulty of IO isn't to recognise that I'm doing IO, it's how IO might break my program in unexpected and dynamic ways that I can't hand over to the compiler. Indeed, IO annotations are very useful when we can isolate the IO portion of our program to a small number of functions, but they aren't providing any benefits if every function is annotated with IO. For this reason, many of us try to find ways to reduce the number of functions which use IO, even when the program itself performs a lot of IO. In [another comment in this thread](https://www.reddit.com/r/haskell/comments/bopyhh/comment/enm0210), I linked to a few approaches for GUI programs. For other domains, there are a lot more options, but the overall idea is always the same: write your program in a less error prone DSL which does not allow arbitrary IO everywhere, and then write a function which transforms programs written in your DSL into programs which perform IO. This way, only your transformation function is annotated with IO. There are also more powerful techniques than just annotating which functions use IO, and those more powerful techniques can catch more dynamic kind of bugs, such as trying to write to a file after it has been closed.
Where is FFI handling explained in that project? I looked but couldn't find it.
In GRIN the primitive operations and foreign functions are defined via [externals](https://github.com/grin-tech/grin/blob/master/grin/src/Grin/PrimOpsPrelude.hs#L16-L22). This is a flexible approach. This method is not explained in the paper.
Yes, the last function `done` actually requires at compile time that all fields have been handled and have the same type.
By the way, the behaviour of `groupBy` in Data.List is often useful as well, even if it does something a little different. If you have a list of e.g. sections of a document, and are trying to convert it into a tree, ```groupBy ((&lt;) `on` sectionDepth)``` is exactly the thing you want at each step of the recursion, since it'll collect all the sections of greater depth following the first one in each group.
Is there a significant difference between this and guards? appEvent (Some thing) = someFunction appEvent general | isSpecificThing general = specificFunction | otherwise = fallthrough where isSpecificThing (VtyEvent ...) = ... isSpecificThing (SomeEvent ...) = ...
I'm excited about this project! A little nitpick: I generally prefer designing big APIs like this around qualified imports instead of adding \`Std\` to all the function names. It makes it harder to swap out a module to use this new set of modules. It's also more verbose. `Std.print` is one more character than `printStd`
On the contrary, the fact that nobody has created these extensions means that its harder than you think, and the fact that people _have_ created Haskell and enjoy using it means that it is useful! (Also the fact that even in effectful languages like Scala, many people still choose to use IO-like constructs also means that it is useful!)
Could you do something like that with one of the emacs folding modes in the repl?
You mean rename Std.XXX.YYY to XXX.YYY? That may bring some troubles when using stdio together with other libraries so I just add an extra Std name space. As for printStd here the suffix means std stream, not really related to the package name, but in general I prefer qualification style.
Oh I see. I was reading examples thinking printStd etc were just namespaced versions of Prelude functions.
It is possible to write `(--&gt;) @Type = (-&gt;)` and `(--&gt;) @(Kl m) = Kleisli m` with trickery, it may take a while until it's practical. I think of it as the more Haskell-y solution, subject to change :)
&gt; The fact that nobody has created these extensions implies that these compiler checks are in general not terribly useful to the average programmer. Or that it is more complicated than a week’s worth of effort. The problem is that most languages have existing standard libraries that perform IO, and these libraries were developed before any set of IO annotations. And let’s be clear, it’s not just strictly input and output that we are concerned with; it’s any action that could globally change the application’s state. That set of actions is much larger than just the obvious IO-performing actions in, for example, the C or C++ standard libraries. In a sense, Haskell did exactly what you propose: it included these annotations from the start. But rather than making this important information an adjunct piece of information, as annotations usually are, they were represent clearly in the type system.
Unless you are writing really very different software from what I am writing, the core logic of it is not spending all that much in IO. I write a lot of web / api / networking stuff and yes that's networking &amp; db, but not mostly, at least not at I write it. I make sure all the logic is pure and anything IO get's moved over to that as soon as I can. It is a blessing considering that almost all other environments I work(ed) in (kill me javascript/php) have me moving strings to strings basically. With some horrible shit in between to make sure that fits. And ofcoure often it does not because I never thought of that case; with Haskell I hardly ever have that. It takes me longer to write and think but in the end I don't have jump of bridges when bugs appear and I cannot remember what kind of stuff is in stringA or stringB. I am not sure, but it seems to come with experience that I really do not think about moving all side effects to the outer edges of my code; the core is pure (as any language allows it) and that's where I spend most time writing things of values.
You don’t need to explicitly add IO annotations in C++/C — side effect analysis is a well-studied problem for performance optimization purposes in mainstream compilers.
I've been here before with GTK. It's a pain in the ass. You're right, it doesn't make the code as it is executed "cleaner." It can even sometimes make code less resilient to refactor, and harder to manage. But it makes your assumptions about that code independently testable, outside of IO, and critically, outside of GTK. The longer you work with a project like that, and the more complex it gets, the more that will start to pay off.
Complete noob here coming from the C world. I'm trying to work with binary data and need a gentle shove in the right direction. I am deserializing a header, and I can use hGet to grab the contents of the header, but that leaves me with an IO ByteStream. This is great since I am processing and re-writing the binary data file where the header won't change, but in cases where I need to work upon values stored in the header, I need to convert the IO ByteStream to an integer in many cases. The data in the header is little-endian, so if someone can post a working snippet or even a link to an example, it would help me out immensely. For example, I can do something like: `headerValOne &lt;- hGet fileHandle 4` to pull 4 bytes 00 / 08 / 00 / 00 , but I need the integer value 2048.
I think that haskell programmers expend too much time in "side effect concerns" &amp;#x200B; Instead of trying to have the job done in the most simple and functional and composable way (sorry for the redundancy) you spend a lot of time praying forgiveness to some god of functional programming for your IO sins. and trying to contort the code using verbose types, insert a lot of accidental "idiomatic" complexity to wash your sins against such enigmatic entity. &amp;#x200B; Stop crying. shut up. Use the IO monad. do your work and don't mess haskell with your scruples.
I don't understand why this question is getting downvoted. Dependency injection + discipline gives you most of the same benefits in other languages too. As others said, the problem is the last part: in other languages you have to maintain the discipline yourself whereas in Haskell you can lay your types out in a way that there's no other way to write your code.
&gt; CoRec is the concept I was looking for but didn't know how to put a name to :) It's more commonly known as an "open sum" or an "open union".
I empathise with your frustration, but I think you’re letting your emotions get the better of you. Saying it’s just not what OP is used to ignores the fact that he’s had fine experiences with other things he’s not used to, like Coq and Prolog; worse, it seems like you’re either wilfully ignoring this or just didn’t read the original post fully, since he addresses this point there. I don’t mean to be harsh, but this kind of response makes the Haskell community look bad (no matter how aggressive OP was being, letting your emotions make you miss points already addressed is not good).
this is so great, thanks for the concrete example. It makes it easy to visualise
So, I can give you an `IO Int`. You should never expect to "escape" from `IO`, they type of `main` is `IO ()` anyway, so you never *need* to. bytesToNum :: (Foldable f, Num r, Bits r) =&gt; f Word8 -&gt; r bytesToNum container = foldl' shiftInto 0 container where shiftInto i b = let j = fromIntegral b in (i .&lt;&lt;. 8) .|. (j .&amp;. 0xFF) (.&lt;&lt;.) = unsafeShiftL test :: Handle -&gt; IO Int test fileHandle = do headerValOne &lt;- hGet fileHandle 4 return . bytesToNum $ unpack headerValOne --- GHCi, version 8.4.4 &gt; bytesToNum [0,1,0,0] :: Int 65536 &gt; bytesToNum [0,0,8,0] 2048 It uses stuff from [Data.Bits](https://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Bits.html), which is really useful. `bytesToNum` is quite generalized, but you can use it as `[Word8] -&gt; Int32`, or `Unboxed.Vector Word8 -&gt; Natural`. If you want to work on `ByteString` directly, you might re-implement bytesToNum in terms of `ofoldl'` from the mono-traversable package.
Thanks! This is getting me pointed in the right direction.
Static checks are a bit better than proper hygiene.
I know I do, but that's because I'm writing Haskell for myself (and, if something useful comes out of it, the community secondarily). Heck, *right now* I'm been wrestling with a problem that *I already solved* but I'm trying to use structured recursion instead of general, unstructured recursion -- it touches on `IO` only tangentially since I'd like to use the same technique to implement (actually *reimplement*, that's done too) a `Gen a` from QuickCheck later. If I were writing Haskell for work, I'd spend a lot less time trying use all the bells and whistles and more time just getting the things done. I'd have already moved on to the next feature. I would necessarily have IO *everywhere*, but I wouldn't flinch from adding it anywhere I needed, even if that was just for the equivalent of `LOG.debug("Internal decision point")` statements that we often have turned off in production. One nice thing about Haskell is that when I go to clean up code by refactoring, I'm much less likely to break stuff on a code path our tests don't cover. I'll chase the platonic ideal of this process in my own time, if I think it interesting.
&gt; Stop crying. shut up. Please consider if your communication is respectful.
I don't see how Haskell could target your language when elementary logic can't even handle simply typed lambda calculus. Can you clarify?
&gt;More specifically, I definitely agree that writing an application using these approaches requires the application to be structured in a completely different way than if you were using callbacks. I think this is the key point here. I've never written GUIs in anything other than an event-based style, and I'm not sure how to do otherwise. Do you know of any resources on this? Anyway, from what you've said, I definitely think I'll have to look at these approaches again - it sounds like they can work very well when you use them properly. (Of course, it's using them properly that's the challenge...) (By the way, I saw both those games on your website and I've played them already - they're amazing! Thank you for making them!)
I'm already aware of the benefits you suggested. The problem is that I don't really know how to get there. Could you elaborate a bit more on the actual techniques I could use to get rid of `IO` in a GTK application?
First, you take your GHC Core program and check if it is compatible with optimal reductions (using ideas from [this paper](https://dl.acm.org/citation.cfm?id=1131315), for example). If it is, just compile to Formality Core. If it isn't, compile to a HOAS-based interpreter of GHC Core written in Formality Core. In that case, there would be an interpretation overhead, but HOAS is quite good. If eventually Formality Core gets as fast as I think it could get (with FPGAs and whatnot), then this could be practical.
The point isn't to create "more pure" monads, but higher order classes, like `MonadGTK` or whatever, so that the `IO` monad can be declared an instance of those classes using the GTK methods.
Wouldn't this interpreter end up "optimally" performing the same set of reductions as the original Haskell program though (i.e. constant factor speed up at best)?
It is a different evaluation model, so, things like church-encodings (as used on that free monads library, for example) should be drastically faster, and most things could benefit from having parallel reductions.
I don't see how that would translate through an interpreter though.
The point is, you can set up an interpreter that essentially emulates the bookkeeping mechanism. It'd, thus, take care of the λ-terms that the raw optimal algorithm can't deal with. With this, you can implement a λ-calculus interpreter compatible with all terms, in the same way other optimal evaluators (BOHM, lambdascope, etc.) are. You'd get the usual benefits of running with the optimal algorithms (parallelism, sharing), just with a constant slowdown due to the overhead. This is one of the problems I'd like to have someone working in (since I'm busy with other things).
I agree, this does seem possible.
Please ELI5 optimal reductions in this context. &amp;#x200B; Checked out your projects. Very well made. Please, now help me understand the theory I cannot understand that is beyond the code that I can understand.
Do you have any performance numbers on non trivial computations? (A month ago I almost started an llvm compiler for formality because of my cache-saturated fantasies. )
you can look at how Elm does it when starting out: https://github.com/rtfeldman/elm-spa-example/tree/master/src . They decided FRP was actually a bit overkill for most projects and all you need is a view that takes a model and sends an update message to an update function that modifies the model and then displays that. This architecture is really simple but scales well if you think of the model as a relational database.
One thing that is really nice about restricting side effects that is not mentioned here is that it's easier to check that dependencies are not doing stuff it shouldn't. To check dependencies you just need to ctrl+f for unsafeperformIO and then also check the IO functions. In javascript it is impossible to know what dependencies are safe and which are not: https://hackernoon.com/im-harvesting-credit-card-numbers-and-passwords-from-your-site-here-s-how-9a8cb347c5b5
Seconded. Would love to know more about this. Is it related to the stuff in Andrea Asperti’s _The optimal implementation of functional programming languages_ and the parallelization work that’s been going on with ghc for the last 20 years, neither of which I have read yet?
Do you plan to implement this idea for a toy lambda calculus language?
Isn't 3 million rewrites / sec already pretty good? To put it in context 3GHz processor carries out around 3 million atomic operation / sec. That would mean one rewrite is basically atomic operation.
Are you saying that Formality Core + HOAS-based interpreter for lambda calculus equals to BOHM?
Cool project! It looks like a similer project to `foundation`. Can someone give a comparison?
&gt; To put it in context 3GHz processor carries out around 3 million atomic operation / sec. By "atomic operation", do you mean an actual atomic memory operation like compare-and-swap? Even for that, 3 million per second sounds a bit low. A modern 3GHz x86 processor can easily do more than three *billion* instructions *per core*, and more if you push the out-of-order and vectorisation units.
Optimal sharing means you're not wastefully duplicating terms (and hence performing subsequent redundant reductions) with your evaluation mechanism. Wiki has [a nice section on it](https://en.wikipedia.org/wiki/Lambda_calculus#Optimal_reduction).
Author of stdio here, I'm aware of lots of prelude alternative on hackage, but stdio is not one of them. The problem stdio focused on is limited: simple, high-performance input and output. Currently, by importing stdio, you get: &amp;#x200B; \+ Cross-platform high performance I/O devices: socket, regular file, tty, std streams, etc. \+ Unified exception and resource management, high-performance timers. \+ Packed data structures based on \`ByteArray\`s, \`Parser\` and \`Builder\` works on them, UTF-8 based texts and tools. \+ Common format support such as JSON. &amp;#x200B; HTTP protocol support is on the roadmap. Basically, you get a standard input and output library similar to many other languages, aiming at completing base, rather than replacing it.
Ooops, I have mixed up MHz and GHz. Lesson learned, don't respond to post still half awake! :)
Most languages have normal order evaluation, that is all arguments are fully evaluated exactly once before the function is called. Haskell is call-by-need so evaluation of arguments are evaluated at most once when they are used the first time. Optimal evaluation order is the order which minimizes duplicate work. Note that I didn't say avoid all duplicate work which is often impossible. So to find the optimum we need a ton of bookkeeping which often makes this slower than a mostly optimal strategy that uses heuristics. Anyway, optimal evaluation is fairly abstract but it basically ends up doing dynamic fusion. This blog post gives a decent intuition https://medium.com/@maiavictor/solving-the-mystery-behind-abstract-algorithms-magical-optimizations-144225164b07 . Can't say too much about the theory behind this because interaction nets give me a headache.
I agree that the "Elm architecture" style used in gi-gtk-declarative-simple can be hard to use for larger programs, and that alternatives approaches on top of gi-gtk-declarative might be better depending on your needs. I'm currently writing about an alternative style that resembles MTL style interfaces for GTK that uses declarative markup. It's basically a simplified version of what's going on in Komposition. You can then write your application in terms of (mutually) recursive actions depending on the interface, and modularization becomes much more tractable than with the Elm architecture style. The write-up was meant for something bigger, but perhaps I can publish some of the code together with the existing gi-gtk-declarative examples, if you're interested.
What does it mean to do duplicate work? Does this mean something like `print (fib 10 + fib 10)` would only evaluate `fib 10` once?
How is this approach better than using just plain `IO`? As far as I can see, you would have to wrap every single GTK function in `MonadGTK`, which seems impractical.
Yes, I would be incredibly interested in that code! Despite my previous reservations, I do personally think that `gi-gtk-declarative` is probably the best approach for non-event-based GUIs today in Haskell; my only problem is with the complexity of using it. Since you're someone who knows a lot about the area, I do have a few questions about the Virtual DOM approach (and `gi-gtk-declarative` more specifically) which I've been wondering about for a while: 1. For non-dynamic applications where widgets are very rarely created or destroyed, what advantage does the Virtual DOM approach have? It seems like a lot of effort for very little gain to create patches etc. when they aren't needed. 2. Is there any way to integrate `gi-gtk-declarative` with a GUI designer such as Glade? For complex GUIs such as mine it's often easier to create the GUI in a specialised application than to make it in code.
It allows for better encapsulation, modularity &amp; abstraction. Also, what u/IronGremlin said.
Make some unix-style command-line binaries for processing data. These already exist or are possible with awk but who cares :) * take n - Only keep the first n columns of text * drop n - Drop the first n columns of text * sortBy n - Sort the output based on one of its columns
Same here. I have an application which has a GTK GUI with about 800 widgets, is multi-threaded (so uses a lot of STM) and does a lot of network IO. Did it in the event-driven style and I am also not sure if FRP or declarative approaches would be possible or have the necessary performance. I think FRP is on my list to look at, but it still has to prove to me that you can build such an application with it. The MVC library from Gabriel Gonzalez looked promising, but it also seems to be for more simple applications without multi-threading. Eventually I ended up with the ReaderT pattern for the application and the GUI code was still completely IO. Also GUI interaction with threads is a topic on its own. &amp;#x200B; Anyway, in my current project I started now to use the ReaderT/RIO pattern with the HasX classes (which I didn't use for the first project, just a raw ReaderT with a config and a TVar for application state) for a bit more separation. This project has the same requirements, but in this project I will use fltkhs, and so far its ok. Not great, and there are still a lot of MonadIO constraints carried around, but currently I'm fine with that. Still not sure how to do the GUI though.
Why monospace though? Font designers work hard on making variable width look good, and you decide to throw it all away with monospace! It makes sense for code to use monospace because indentation has meaning. But for ordinary text it's just harder to read. Readibility is your main concern when selecting fonts.
&gt; This is one of the problems I'd like to have someone working in (since I'm busy with other things). I am far from being in a position to help (and even took a sidetrack with my decision to now tackle SICP), but I am very interested to get further into the topic. Partly because it is fricking cool, partly because I hope to get some ideas for my research (dealing with unbounded parallelism). So far, I have started to read the original paper by Lamping ("An Algorithm for Optimal Lambda Calculus Reduction"). I still have not have a non-shallow understanding of why and when duplicating happens and how to avoid it, will have to implement it to properly get it. For this, I plan to implement the lambda calculus and the graph language in redex. Stepwise reducing is great! The other paper I wanted to read is Mazza's "A Denotational Semantics for the Symetric Interaction Combinators". ---------------------- My intuition is that when I understood the both papers above, then getting up to speed with your current work is not so much a theoretical undertaking anymore, but instead one of syntactic cleverness. Is that correct, or are there other important theoretic areas one would have to understand that are not yet covered by the two papers above? Just a wild question: Is your work completely orthogonal to advanced type magic such as dependent types? (I also hope to catch up in that area, and I love toy projects and connecting cool things). Thank you for the work you put in investigating this fascinating topic!
Point taken and forwarded to people responsible for branding.
To clarify, we are still a young company and at this stage try to prioritize local applications over remote ones; however, we will consider all candidates. If you have any questions regarding the position itself, please feel free to just reach out.
&gt;vast majority of Haskell functions are named 'a', take 'b' and return 'c'. The standard library also includes specific functions with concrete types, e.g: isLower :: Char -&gt; Bool -- from Data.Char openFile :: FilePath -&gt; FileMode -&gt; IO FileHandle -- from System.IO Only type names that are truly generic are named a, b, c, ...; for example the `flip` higher-order function swaps arguments of some other function: flip :: (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c Because the function is truly polymorphic there is nothing one can say about the types of its arguments. (BTW this is the same in generic programming in C++ or Java: List&lt;T&gt; when we want to generalize for all types T.) Perhaps you find it more readable if you wrote it like: flip :: (thing1 -&gt; thing2 -&gt; result) -&gt; thing2 -&gt; thing2 -&gt; result But after a while I think you'll find the shorter notation is very readable. It also makes the standard library a very *re-usable* piece of code. I often compose simple processing pipelines using these these small library functions instead of resorting to re-writing them from scratch using recursive definitions or a loop (in an imperative language).
Doesn't work for me. Does it only work for certain schools?
This still reinvents parts of network, text/text-utf8/text-short, vector and aeson. So if not alternative to `base`, then at least above mentioned libraries. Why `stdio` and not them?
Well in your example the work has already been duplicated by the programmer - I don't think the evaluator will divine how to un-duplicate that. Instead, consider something like `let a = fib 10 in a + a`. What order do the substitutions take place in? We could evaluate it by subbing in the definition of `a` first and arrive at `fib 10 + fib 10`, then evaluate the two fib 10s. But we could have evaluated the `fib 10` first, then end up with `let a = 55 in a + a` which gives `55 + 55` which gives `110`, which resulted in no work duplication. The ultimate goal here is trying to find an evaluator that never duplicates work. This is not in general possible, as whatever reduction scheme you choose, I can construct some pathological example which will cause it to behave badly, but if you're willing to restrict your terms to certain well-behaved shapes, you can say "ok, for this nice class of lambda terms, my evaluator behaves optimally in the sense that it never induces unnecessary duplication of work." That's my cursory understanding of the topic.
No. Let me try to ELI5. Many programming languages have lambdas capable of capturing closures. As such, you could say JavaScript, Python and many others come with a "lambda calculus evaluator". In most of them, a lambda application, `f(x)`, is evaluated by first evaluating `x`, then substituting it in `f` (usually by copying its reference to a new location in memory that is owned by `f`). But this isn't optimal. First, because `x` may not be needed, so, evaluating it is wasteful. But second, and most importantly, even if it *is* needed, evaluating it this way causes work to be duplicated. Take this term, for example: `λx.(x I) λy.(λa.(a a) (y z))`. The JavaScript-like evaluation goes like this: ``` (λx.(x I) λy.(λa.(a a) (y z))) (λx.(x I) λy.((y z) (y z))) (λy.((y z) (y z)) I) (λy.((I z) (I z))) (λy.(z z)) ``` Notice how `I z` was evaluated twice? This could be avoided if we instead performed substitutions before evaluating the argument, i.e., if we performed "lazy evaluation": ``` (λx.(x I) λy.(λa.(a a) (y z))) λx.(λy.(λa.(a a) (y z)) I) λx.(λa.(a a) (I z)) λx.(λa.(a a) z) λx.(z z) ``` But lazy evaluation can be even more problematic. Take this, for example: `(λa. λt. (t a a) (f k))`, where `(f k)` is some slow computation. In that case, substituting before evaluating would give us `λt. (t (f k) (f k))`, and then `(f k)` is evaluated twice, wastefully. Haskell is quite brilliant and has the best of both worlds: it perform substitutions first, but **memoizes** substituted terms, so that reducing one reduces all of them simultaneously! That's called "sharing". But if the substituted term is a function and it is applied, it must be un-shared (because it now must have its own closure, thus, its own space in memory). In certain situations, Haskell's un-sharing causes it to duplicate work and, thus, be, again, inferior to JavaScript's strategy. So, what is going on? Haskell beats JavaScript in some cases, JavaScript beats Haskell in others. And there are many, many different, vastly creative ways to evaluate λ-expressions in the market. What is the best way to do it? That is, is there an evaluation strategy that is as fast as possible in all cases? An "optimal" one? Long story short, there is, but you need to give up of the usual, textual representation of terms, and compile them to a special kind of graph with specific rewrite rules called "an interaction net", reduce that graph, and then translate back to a λ-term. We animated this process [here](https://github.com/moonad/formality-core/raw/master/interaction-combinators.gif). This approach is optimal, in the sense it duplicates the least amount of work possible. It also gives us a bunch of computational benefits that we weren't even looking for. It is very parallel-friendly, as graph rewrites can be performed independently. Beta-reduction is a constant-time operation, allowing us to measure the complexity of evaluation (which may have use to, for example, smart-contract platforms that must measure work done). Garbage collection isn't necessary, because erasure nodes naturally free values that go out of scope. And so on. Turns out interaction nets are a great computational model with much better characteristics than the λ-calculus, or even turing machines. Some (at least I) believe, with mature compilers and perhaps even hardware support, this evaluation model could surpass even languages like C, not because of optimal sharing itself, but mostly because of the immense parallelism potential it gives us. There is a problem with this idea, though. It doesn't work in some cases! There are λ-terms that cause the graph to get into a corrupt state and then the result of the graph reduction doesn't correspond to the normal form of the λ-term it represented. To solve this problem, researchers extended the graph with a bunch of new nodes called "the bookkeeping mechanism". Those new nodes resulted in a large practical slowdown. I attribute this to the reason people gave up of the algorithm in practice. But there is another way around: avoid problematic λ-terms! Formality (and Formality-Core) does exactly that. It is a functional language with lambdas and whatnot that is compatible with the most efficient version of the optimal reduction algorithm, i.e., the one without the extra bookkeeping nodes. Targeting Formality gives you the benefits of optimal sharing (that I explained/explored on [this blog post](https://medium.com/@maiavictor/solving-the-mystery-behind-abstract-algorithms-magical-optimizations-144225164b07)) today, and might give you the benefits of lightweight, GC-free, massively parallel evaluation once we have mature enough compilers/hardware.
Why you think it needs to be 1000x faster in particular? Note that our Rust implementation performed 30m rewrites/s. This, to me, is actually "pretty good", at least in the sense it can be used to build apps, today. It should be roughly as fast as Python in practice. That's not great, but that didn't stop it from being massively popular, right? I find it very likely that, in the short term, we could achieve another order of magnitude (i.e., 300m rewrites/s) with a team dedicated to the compiler (a LLVM backend?), and possibly another one or two with GPU/hardware support.
Yes, that's what I meant by "I'd like to have someone working on this". But I'm focused in many other things to take this problem right now, sadly.
[ELI5](https://www.reddit.com/r/haskell/comments/bp55ua/new_tool_for_exploring_optimal_reductions/enr3d42/)
[ELI5*](https://www.reddit.com/r/haskell/comments/bp55ua/new_tool_for_exploring_optimal_reductions/enr3d42/) **\* 5 years of CS**
Also the colour on the body text fails WCAG contrast ratio limits for accessibility level AA.
I am not saying it is too slow to build useful apps. Just that one rewrite takes 1000x more time than atomic operation.
Those packages accumulated issues over time and become harder and harder to accept changes and simplifications, I think I'm not the only one who constantly complain about some design choices made a long time ago but can't really do anything about them. I'm just one of those Haskellers who try to provide alternatives. If a new design could lead to better code or performance, it would probably get tractions over time. &amp;#x200B; Just name a few issues I'm trying to solve in stdio in case you're still wondering: &amp;#x200B; * A full feature UTF-8 text toolkit. * A faster \`Builder\` and \`Parser\` based on unpinned \`ByteArray\` s . * A full feature vector module unified with bytes. * Cross-platform I/O made easy. &amp;#x200B; That will be a long list of course, and solving them in one go seems challenging. People often miss those great figures who created current Haskell eco-systems, for example, the author of aeson, Bryan O'Sullivan and many others. Their devotions inspired me, and worth to be revolutionized continually. &amp;#x200B; Finally. I did various Haskell stuff before, but none of them are as large as this one. From my limited experience, people do this kind of programming all the time driven by their own need and philosophy, and this is one of many ways how to help the community keep prosper.
Ah, fair enough, but consider C certainly doesn't use only a single native CPU operation to perform a function application! But Formality does need a graph rewrite for a single MUL, so, to have the C performance in non-functional, tight loops packed with numeric computations, we're looking for a roughly 1000 speedup, indeed.
Can / will anyone implement it beside you?
&gt; So far, I have started to read the original paper by Lamping ("An Algorithm for Optimal Lambda Calculus Reduction"). I still have not have a non-shallow understanding of why and when duplicating happens and how to avoid it, will have to implement it to properly get it. For this, I plan to implement the lambda calculus and the graph language in redex. Stepwise reducing is great! The other paper I wanted to read is Mazza's "A Denotational Semantics for the Symetric Interaction Combinators". Those are two works I'd suggest, so I think you're good! &gt; My intuition is that when I understood the both papers above, then getting up to speed with your current work is not so much a theoretical undertaking anymore, but instead one of syntactic cleverness. Is that correct, or are there other important theoretic areas one would have to understand that are not yet covered by the two papers above? No, once you read the optimal lambda calculus reduction book (actually, just the part that doesn't involve the bookkeeping mechanism), and symmetric interaction combinators (just the definition, reduction rules and key theorems), you'll know the main pieces of my work. The rest is just raw engineering to make it work in practice. &gt; Just a wild question: Is your work completely orthogonal to advanced type magic such as dependent types? No, quite the opposite, Formality (not Formality-Core) is a dependently typed language with inductive datatypes, equality, sigma, and everything else you'd expect in a language like Agda. It is still very scary though as we've mostly worked on the theory (making those things consistent is hard). We want to make it much more user friendly, inspired in Python/Elm, soon.
Equal is a strong word, but they'd achieve very similar things, yes.
Could you release an interface to your system as a Haskell library? It would be nice to have the interface available as Haskell types instead of JavaScript. Plus, languages like Haskell, Idris, and Agda are all written in Haskell, so having Haskell bindings available makes it much easier to use your backend for these languages.
No plans right now. We are a small team, and there are many other things we're focusing on Formality first. I'm considering to start a company dedicated to this with a friend, though, but this raises many problems, funding, etc. If things go really well, I'd probably to set up a team to take this, as I'd love to be able to compile Haskell and other languages to Formality-Core, but that's not certain.
&gt; constant factor speed up at best I feel like I read a fair number of complaints about the constant factors being pretty big in a lot of cases (I'm mainly thinking things like Reflection Without Remorse), so that might still be nice to have.
It should be available in all major languages as a library eventually. I've written it in JS because it is the safest short-term option, being available on browsers, mobile, desktop, etc. Haskell, Rust and C are probably next.
Another question regarding FFI - Does GRIN only support FFI that is natively an LLVM input? Or does your GHC project compile FFI files directly to GRIN?
FFI is not restricted, so a foreign function can have native/machine code implementation with a standard calling convention. Currenlty GRIN uses C calling convention for foreign functions. What do you mean by FFI files?
What kind of computation would you be interested in measuring?
Sounds like the way forward from there might be to start taking optimistic shortcuts? Not sure how you'd generate machine instructions for the fast bits while preserving all of the important properties of the parallelism and optimality, though...
I've found some papers about the compilation of interaction nets: - [Compilation of Interaction Nets](https://www.researchgate.net/publication/222033040_Compilation_of_Interaction_Nets) - [A lightweight abstract machine for interaction nets](https://pdfs.semanticscholar.org/18c5/13de01f803a9991be0e7775c9a17cb5971d9.pdf?_ga=2.138480371.1981605350.1535319556-184743586.1535319556)
Should I give up the effort with [ghc-grin](https://github.com/grin-tech/ghc-grin)?
I'm not sure I understand this question (English-wise), could you rephrase it? Is "give up" the same as "consider doing it" in this context? Or I don't get what you're giving up of.
What does boxing mean and why do you need a box before using a duplicated value?
&gt;I mean : a function in a package foo \`caller\`:: \[a\] -&gt; Result uses a function \`callee\` in the package bar to obtain the list: &gt; &gt;caller callee but caller traverse the list only to obtain his length, and callee traverse the list too for other purposes. So the optimization problem is: how to traverse the list as few times as possible to optimize the executable which use these two packages foo and bar ?
Ah, I see. I'm not aware of any way to do this without manual intervention.
On interaction nets, duplications are performed explicitly with "duplication nodes". That is, when a lambda has two occurrences of a variable, each occurrence is copied with a duplication node. Due to the way interaction nets work, once a duplication or an argument is done, you'll have a bunch of duplication nodes interacting with themselves. To complete the process, they must destruct themselves. But this becomes a problem when you try to copy a function which itself has duplication nodes. For example, on the expression `λx.(x x) λf.λx.(f (f x))`, the first term must duplicate the second one, which itself has duplications. Their duplication nodes would react destructively, getting your graph into a corrupt state. The solution to this is to label each duplication node, so that, when the nodes of the first lambda collide with the nodes of the second one, they duplicate each-other instead of destroying themselves, resulting in `(λf.λx.(f (f x)) λf.λx.(f (f x)))`. But now you now have two terms with duplication nodes with identical labels, so they will interact destructively getting yourself into a corrupt sate. Labels aren't sufficient. That's why you need an additional bookkeeping machinery. Formality solves this by using boxes to keep track of where/when values are duplicated. Essentially, boxes divide terms in "layers". Terms can never migrate from a layer to another, and a term in layer N can only duplicate a term in layer N+1. This allows us to prevent corrupt states by labelling duplication nodes with the layer it is from. Does that make sense?
&gt; Actually, most testimonials of people using Haskell in industry that I'm aware of seem to suggest the opposite: write clear, simple, robust code and avoid unnecessary type level magic and clever bullshit. You are not giving an accurate description of the average Haskell developer IMO. Well, let's be real. "Avoid unnecessary type level bullshit except OUR streaming library and OUR SQL library." But I think we're in agreement! Successful Haskell hackers in industry are the ones who repel this tendency and don't spend hours throwing random type equations at lambdabot.
Quite seriously: if you use C you're doing user harm. Period. No quantification allowed. It almost doesn't matter what you're doing because you're granting oxygen to the ecosystem. Whatever you do, please stop giving credibility to that space. &gt; Probably Haskell will be a lesson for me that too many possibilities lead to obscure, difficult code and when writing code in other languages, I should focus on making my code readable at constant speed and limiting the use of fancy features unless they're obviously needed. If you had done this with Haskell you'd be better off. But sadly, it sounds like you didn't have real things to do with it and therefor never actually used it in anger. You know you are when a lot of types start looking like "... -&gt; IO a" or "(MonadIO m) =&gt; ... -&gt; m a" and then you just don't care.
PureScript is a pure, strict FP language. Its FFI files are JavaScript files. The PS compiler simply passes those FFI files through as-is without processing them, so I'm trying to figure out how the PS compiler would have to process those FFI files to add GRIN as a backend. A separate topic, but I heard that GRIN presumes the source compiler handles garbage collection/memory management. Is that right? Currently PureScript presumes the backend runtime does garbage collection.
GRIN will take care of the memory management tasks, but it is not implemented yet.
&gt; Those packages accumulated issues over time and become harder and harder to accept changes and simplifications Yes, this is true. Part of a problem is that maintainers don't have time, neither there are people they trust enough to delegate. Yet, starting with proposing radical changes is by far not the best way to build up trust. Begin with solving existing non-controversial issues, build up from that. &gt; If a new design could lead to better code or performance, it would probably get tractions over time. That's subjective. I suspect the improvements aren't of order of magnitude, are they? Porting any codebase of non-trivial size is just not worth few percent improvement (except maybe if you are Facebook). Buying bigger or more machines is cheaper. Big migrations are highly risky, often enough they fail. --- &gt; - A full feature UTF-8 text toolkit. In `stdio` newtype Text = Text Bytes type Bytes = PrimVector Word8 data PrimVector a = PrimVector !(PrimArray a) !Int !Int data PrimArray a = PrimArray ByteArray# and `text-utf8` newtype Text = Text !Array !Int !Int data Array = Array ByteArray# you don't need to squint to se that the data representation is the same, and `text-utf8` looks like full feature to me. See e.g. https://github.com/text-utf8/META/issues/3 for more information for pointers how to help to get `text-utf8` "done". --- &gt; - A faster `Builder` and `Parser` based on unpinned `ByteArray` s . &gt; - A full feature vector module unified with bytes. This is trickier. Something like data Bytes = Bytes ByteArray# Int Int under some name would be useful; even it would add new member to string-zoo. That however can be done in a single-purprose library, which doesn't do anything else, than providing utilities to work with that type. In particular, I don't even think it's need to be in `bytestring`. Latter is GHC boot library, and if something can stay outside of those, IMHO better. A small package would get more easily get adoption, being small increases chances. (I recall someone proposing renaming `ByteString` to `PinnedBytes`, and adding `Bytes`; good otherwise, but the renaming doesn't pull its weight). Think providing instances. `stdio` is just too big. You would need to write `stdio-store`, `stdio-cborg` or reimplement that functionality. if there were a package with above mentioned type, than providing convertions between `text-utf8.Text` and `Bytes` would be very easy task. --- &gt; That will be a long list of course, and solving them in one go seems challenging. Call me pessimistic on this, but this exercise will fail. We might learn things from it, but as dependency, `stdio` is just too big. I'm more optimistic that pushing changes one at the time to existing libraries, will get us there sooner. (And making small libs to fill the gaps). Take `aeson`, it's not only parsing and building fast; it's also all the bug fixing, API polishing etc. You are changing everything at once. I'm not a maintainer of `aeson`, but have fixed some bugs there, which got exposed only because a lot of people use it. I trust `aeson` as a dependency. It will take you years to build that trust for your JSON implementation. --- &gt; Finally. I did various Haskell stuff before, but none of them are as large as &gt; this one. From my limited experience, people do this kind of programming all &gt; the time driven by their own need and philosophy, and this is one of many &gt; ways how to help the community keep prosper. I agree with that. But please, don't put (or imply) *standard* into the name of your package (or description of it). --- I know that building stuff from scratch and trying out things is fun. But someone have to do the non fun parts of maintenance too.
Ah, so for PS to implement GRIN as a backend, PS would have to define how to call the JavaScript FFI files as part of implementing the GRIN backend. Alternative to that, PS would have to compile the JS FFI files to a language which is easier to call from GRIN, such as C. Does this sound right?
I get the frustration that comes with learning. I've dealt with frustrated students, and the vast majority recognize the difference between "i don't understand this" and "this is a horrible mess". Frustration is not an excuse to spew out a bizarre rant about something that you did not even attempt to learn, and that you are not interested in learning. I just thought was is a poorly written post that achieves nothing. I don't know what empathy has to do with this; you can empathise with someone and still come to the conclusion that they are wrong.
Thank you very much!
&gt;I agree with that. But please, don't put (or imply) *standard* into the name of your package (or description of it). Actually, I do want to use a more cult name, something like diesel, a good name for a wild IO manager LOL. I never intend to push a standard impression over my users. I could change it in the next beta, but probably not worth the trouble. &amp;#x200B; Most of your point and examples can be summarized as: change too many things together is dangerous and we should choose to improve what we have for safety (you probably should read more about stdio to see the difference on UTF-8 handling). That's indeed a pessimistic viewpoint. I think our community needs some competition. for example, the competition between JSON serialization implementations is common in many other languages, I'm willing to take the responsibility to maintain another JSON implementation, slowly building trust, it's not a big deal. &amp;#x200B; &gt;That's subjective. I suspect the improvements aren't of order of magnitude, are they? Porting any codebase of non-trivial size is just not worth few percent improvement (except maybe if you are Facebook). Buying bigger or more machines is cheaper. Big migrations are highly risky, often enough they fail. Come on, my friend. Finding an order of magnitude improvement means the old implementations is a mistake. Which is unlikely to be true. But everybody would like a 2x\~3x boost on performance. Anyway, next time when you want to solve a task and find my library hit the right spot, then don't be restrictive, give it a try. We hope to hear anything about your experience.
&gt; do people prefer text formats or video formats? Text, it's searchable, and easier to bookmark a specific section/page/paragraph. That said, I fear this might be a generational bias on my part. If you do video, linking to a (mostly) text transcript with some stills inserted when necessary (if the text refers to something that's otherwise presented only visually), you can get the best of both worlds. You could even just initially link to a wiki page with the speech-to-text version of the video and have volunteers correct, format, and pull the stills.
JSON libraries in other languages don’t require to change everything in one go, do they? Show me how to use stdio JSON stuff in a Servant app, and I might try it. Or that using stdio for IO stuff somewhere in wai (or actually again servant, there was servant-snap, there could be servant-server-stdio) is an improvement somewhere. I won’t read anything if you don’t give me a link. In fact I cannot find any pointer in Text module. Neither there are mention of utf8 on https://haskell-stdio.github.io/stdio/ And btw, our community doesn’t really need any extra competition: there are streaming libs, serialisation libs, WASM backends, prettyprinters what else. And a lot of stuff stale after initial excitement. TL;DR `stdio` needs a lot more of discoverable evidence than just a list of bullet points. Latter doesn’t convince me. More performant: show me the numbers, more correct: show me the issues in other stuff etc. For others, the stdio paper is freely available at https://sigplan.org/OpenTOC/haskell18.html I still don’t understand why IO loop stuff needs to be in the same library with JSON stuff.
Thanks for pointing out document issue, I'll try to update the GitHub pages, while you can read haddock on hackage ; ) &amp;#x200B; Anyway don't be so mean please, stdio is a young project. and the \`our community doesn’t really need any extra competition\` attitude is really poisoning. If our library can't solve your problems currently, you're free to choose others, or open an issue to tell us what exactly needs to be done to make things easier. if you have problems or bugs when using our library, open an issue and tell us what is it. Try to stop someone making new libraries simply because it doesn't fit your expectation won't help anything.
Sorry, I didn’t meant to be mean.
It takes hygiene in Haskell, too, it's just simpler (no unsafePerformIO, no unsafeCoerce...)
do you have any good astronomy libraries behind the pseudoscience? cause thats what i'm interested in.
Code formatting pls (certainly that doesn't work on mobile)
Sorry, I was using nureddits triple backticks.
To support JavaScript FFI GRIN has to be extended with JavaScript support e.g. JavaScript calling convention. It should not be that hard. :)
Who invented the `($)` operator?
[removed]
That's patently not true. While sequences and arrays have their uses, lists are pervasive in haskell.
What applicatives benefit from this reassociation? It also reminds me of chained matrix multiplication, as an example of when there's a non-uniform cost model associated to the bracketing you choose for your operations.
You should read the paper [Notions of Computation as Monoids](https://arxiv.org/abs/1406.4823), which derives exactly your `DiffApp`.
Go pick that fight somewhere else please.
If you write against a narrower interface, you know that function doesn't use any IO outside of the implementation of that interface. That can help you reason about the function, and also let's you stub out the interface for testing.
Keep your monad abstract when you write the callback. `myCallback :: MyInterface m =&gt; Something -&gt; m ()` Then in you describe how to implement MyInterface for IO, and you can pass `myCallback` to a function expecting `Something -&gt; IO ()`
I need to do some testing, but `Writer [a]` should.
More practical tutorials would definitely be good - at least/especially if maintained and kept tested and current over time. Mark Watson's "Haskell Tutorial and Cookbook" is one of the best of these AFAIK. I think it deserves more prominence. https://leanpub.com/haskell-cookbook/read
It's not pseudoscience if it's not presented as science, fam.
Why did you choose to be a jerk?
I've been programming for more than 20 years, about 15 years professionally. I believe explicit control over effects at the type level is exactly what you want in a language where you you will be working with several programmers, the domain of the problem is complex, and the code has to last a long time in the face of changing business requirements. It turns out that the correctness of a typical program relies on the correct order of operations when performing IO actions. If you use memory after it has been freed your program is in a bad state. If you try to write to a file handle you no longer hold reference to you get an error. No other language I have worked with has given me explicit control over where these operations happen in my code and when they happen. Haskell's type system is rich enough that I can explicitly separate out IO actions involving network file descriptors from file system descriptors so that code handling descriptors cannot be used interchangeably by mistake. I also get fine-grained control over the sequencing and interleaving of these effects... and I can still use pure code which gives my programs a lot of freedom over how they are evaluated and executed. And no other language I've worked with has made it as easy to maintain software for the long haul. I can come back to a piece of code I haven't touched in months when the requirements change, make a fairly radical refactoring, and trust that the compiler will guide me through the change so that I implement the change correctly (along with updating the specifications/tests). I'm hoping linear types will make it in so that even the lifetimes of references can be checked statically. This will make Haskell a *very* pragmatic choice for large software projects. That being said I do still enjoy writing scripts in untyped languages but I don't go too far with those; mostly little prototypes or helper tools.
I use slices of unpinned memory frequently enough that I've actually put these types in [byteslice](http://hackage.haskell.org/package/byteslice-0.1.0.0/docs/Data-Bytes-Types.html).
Mercury is in Taurus
It wasn't in the original Haskell 1.0 report. It was in the Haskell 1.4 report. It was in the Haskell 1.2 report. It wasn't in the Haskell 1.1 report. [Bisect Complete] Not much fanfare in the 1.2 report about it, other than that it is useful in CPS style. I'm guessing one of the committee members put it forward for addition, but that's just a guess. I have no idea where the name came from. That puts its integration into Haskell in the 1991-2 timeframe, but doesn't really nail down the original invention.
If your API is only for a browser app, you can do something like [this](https://github.com/imalsogreg/servant-reflex/pull/80/files#diff-53f1f82f91e96f90f6061ae86d0e096eR60) to keep your api unstable. It's a monad attached to every callback on the client(1), that checks for any error client error(2) and then automatically reload if there is an error. That little hack fixes so many worries in my case. (1) : reflex in this case (2) : I check for 4xx and decode errors. It depends of course how you use status codes. I don't.
i realy wanted to know if there were any astronomy libraries to make sure that parts correct. the rest is just memes with fantasy feel goods. whatever
OK, so """" Our powerful natural-language engine uses NASA data, coupled with the methods of professional astrologers, to algorithmically generate insights about your personality and your future. """" ISN"T ? its not presenting itself as entertainment, done.
"Astrology puts our temporary bodies in context with our universe's vastness, allowing irrationality to invade our techno-rationalist ways of living." Indeed, they are positioning themselves in contrast to science.
I'm struggling a bit to read your notation, but I'll take a crack at answering the question. The first part is simply stating an "obvious" property of assignment -- setting `x` to `a` in some state `s` (giving you a new state `s'`) means that if you look up `x` in `s'`, you will get the value of `a`. I'm assuming that `s[x -&gt; A[[a]]s]` is (informally) "`s`, except the value of `x` in `s` is now `A[[a]]s`". Similarly, in the second case, it's stating that assigning to `x` should not change the values of any other variables. This is not actually true in the general case (for example, `y = (x = 5)` is valid in some programming languages, and does in fact update both `y` and `x`), but in most "sane" systems, we want this to be true. So of course, by definition of `s[x -&gt; v]`, we only change the value mapped to `x`, and not any others.
This looks very in depth, definitely the kind of thing I would recommend to a friend alongside resources like LYAH. Thanks! &amp;#x200B; I was planning on using a specific project to motivate the first few sections, and then go in-depth with less practical application later. But that's because I haven't really fleshed out my ideas yet :)
Sorry about the notation that is rather hard to read but this explained it all, thank you very much!
I've read most of Category Theory for Programmers. If you want to learn Haskell, I don't think that's the right resource. It doesn't assume knowledge of Haskell, but it only introduces the pieces of Haskell that it needs to convey the ideas of Category theory. To write significant, useful Haskell programs, you need parts of the language that Milewski doesn't need until fairly deep in the book. That said, it's a great book, and I'd definitely recommend it for learning category theory. I haven't read the other book, so I can't really make a statement about it. I just read through their website and while (at least at the high level) it looks like a good resource for Haskell, it doesn't look like it relates the Haskell back to category theory. My advice is to learn Haskell in some capacity first, with whatever resource you choose. Then take a step back to Milewski's book and the abstractions and realizations you have about how you program and how things fit together will probably be much more profound and enjoyable. I've been thinking recently of doing my own text-based introductions to Haskell and Category theory which would go together in parallel, but I'm not even convinced myself that this is a good approach to being able to *understand* both well.
These are great, thank you! Just what I needed. What I've called `DiffApp` above is the Cayley Representation for Applicatives (*Notions...*, sec 5.4), and it makes complete sense that I stumbled into it, as my [`Phases` applicative](https://hackage.haskell.org/package/tree-traversals-0.1.0.0/docs/Control-Applicative-Phases.html#t:Phases) that I was trying to speed up is *almost* the Free Applicative (`Free''` from *Notions...*, sec 5.3), modulo a design decision.
I'd say go for 'Haskell From First Principles' first. Having read it, I can say it's pretty good for what it is. I've also followed Bartosz Milewski's lectures on youtube ( haven't gone through much of his book ) and I think this order of doing things is better. You are going to work with Haskell, and run actual programs; so you might be hooked for longer - but then you will get to CT and see how so many concepts start to make so much sense! Also, there are a lot of Haskell examples in his lectures (probably in his book too, can't remember), which will help even more. I've done them in this order and it's been a very pleasant experience. :)
Thanks for your answer. My current plan is go through "Haskell Programming" and take my time so that I can write clean functional programming and also find a book about Category Theory that is not necessarily for programmers but "Category Theory for Programmers" just seemed like it will teach me both. I believe that neither Category Theory nor FP is a prerequisite of the other so I think learning them at the same time won't cause a mess. What do you think?
This seems like the best course of action. Also I'm curious if you actually enjoy writing Haskell like all Haskellers say and if you find it useful.
They aren't prerequisites for each other, so you're correct. However, when you look up things about the types of proofs you can get out of category theory, you often find comments like "This proof isn't enlightening at this level of abstraction." Learning category theory is primarily difficult, in my opinion, because it is so abstract. Knowing Haskell can knock that down a notch, giving you an actual foundation on which to put the things you're learning. And that also makes learning them more fun, because you're realizing things about how the code which you thought you already understood actually works in theory. That said you also need to be a little bit careful, because Haskell and Category Theory don't have a direct correspondence. Haskell types form a category, but there are lots of categories that don't look like **Hask** (what we call the category of Haskell types) at all. So you while Haskell will help a lot with learning Category Theory, you have to be careful not to make that connection to deep in your thinking. I'd still recommend Milewski's book even if you aren't going to learn Haskell from it. If you're comfortable with the pieces of Haskell he's using (which don't get advanced for a *long* while) it's a really, really good introduction to the key ideas of category theory, and from the name, it's aimed at programmers. A theory book on category theory would probably be considerably drier and also much harder to follow. Milewski also uses what he likes to call his "butcher's knife" to butcher some of the math and avoid talking about dry, formal proofs, focusing instead on intuition. I think that kind of approach is really important for such abstract topics. You can reconstruct (or find elsewhere) the formal proofs if you're interested.
Galois has a lot of them Haskellers
I’ve heard mixed things about Galois. I’ve heard that most engineers are not doing Haskell day to day but clearly Haskell is being used. Would love to know the real story.
I do find it useful, it's a very pleasant language to solve problems in, even if very simple (problems). I haven't written much Haskell, but I am strongly for FP, I'm even slowly transitioning my job to be more FP. Bottom ( pessimistic ) line, I wanted to learn many languages, as different as possible, to see what they are about. Learning Haskell gave me a different perspective &amp; knowledge which is spilling into code I write in any other language. I think it is one of the best things I've done for myself, as a programmer. Highly recommend it - even if the book is lengthy, it's going to do a lot of good.
My team has 9 of us at the moment using nothing but Haskell all day.
Dang, I thought we might be in the lead with 3. :)
Standard Chartered is sort-of Haskell and their strats team is ~40
Mine has 0.
&gt;My team has 9 of us ...aaand, is your team working in any specific company? And, are you hiring?
I'm trying to build https://github.com/sdiehl/kaleidoscope on gentoo. When I do `stack build --fast`, I get a bunch of errors like setup: '/usr/lib/llvm/4/bin/llvm-config' exited with an error: llvm-config: error: missing: /usr/lib64/llvm/4/lib64/libLLVMDemangle.a llvm-config: error: missing: /usr/lib64/llvm/4/lib64/libLLVMSupport.a llvm-config: error: missing: /usr/lib64/llvm/4/lib64/libLLVMCore.a ... And indeed, the files named do not exist, either in that directory or any other. Upon investigation, I can get these same errors by running `llvm-config --libs --link-static`. It appears that [gentoo doesn't support static linking to LLVM](https://wiki.gentoo.org/wiki/Project:LLVM/Shared_libraries). By any chance, has anyone run into this problem before and managed to solve it?
Writing a large program in EAC seems painful to me. I tried to write an exponentiation function for church numerals in it and gave up. I'm not very optimistic about Haskell compiling to any Turing incomplete language. is there a Turing complete subset of lambda calculus in which you can reliably perform optimal reduction?
It is very related to the former. I believe OP has cited that book in the past
We work for a Managed Service Provider + Telecom parent in Alaska. We may be posting a position soon, but it requires onsite in Anchorage, AK and valid US work permits.
This is how you perform exponentiation of `2 ^ 8`, for example: ``` def main: (~8 #~2) ``` But of all the things, you chose Church number exponentiation? It is one of the things that pushes elementary duplication to its limit. In fact, plain `Nat -&gt; Nat -&gt; Nat` exponentiation is impossible, `Nat -&gt; !Nat -&gt; !Nat` is the best you can do, i.e., it must receive / return a boxed value, which is probably why you couldn't do it. I find Formality-Core quite usable when writing just simpler for loops, branching, etc. Also, it emulate arbitrarily recursive functions with Church nats (this: `(~n [rec] function_here)`), bounded by a maximum recursion depth `n`. This limitation isn't much different from JavaScript's stack size limit, if you think about it.
The intersection of onsite Anchorage and Haskell must be interesting to hire for! ;)
I've [mentioned it here and there](https://www.reddit.com/r/haskell/comments/9d7qcm/how_do_you_become_an_employed_haskell_developer/e5gy3an/?context=0) - so far it's been training people into Haskell instead of hiring people who actually know it.
We are almost 50 engineers now using Haskell and Purescript. We’re based in London and still hiring , if you’re interested hit me up.
which company?
That makes sense, but I'm still not sure what `MyInterface` would look like. It has to be wide enough to encompass all GTK methods, but narrow enough to disallow `IO`. Currently my best guess is something like the following: class MyInterface m where getWidgets :: m Widgets gtkMethod1 :: String -&gt; TextBox -&gt; m () gtkMethod2 :: TextBox -&gt; m String -- etc., etc., etc., for the rest of GTK Which clearly is impractical.
4 kinda 5 of us at LiveView Technologies liveviewtech.com
Habito would be my guess
I read both. I started with CTFP and gave up at Chapter 26, partly because of lack of enough knowledge of Haskell (and partly because I didn't read carefully and taking notes in the previous chapters). Then I went through HPFFP. Now I am reading CTFP again.
Many of our projects use Haskell in some way or another, but not all of them, and not all of the projects that use Haskell are all Haskell all the time. Right now, for example, I have one project I'm running that's a mix of Haskell and Rust; another project I'm spinning up that will likely end up mixing Haskell, C, and a few other things; a third that's Haskell + Haskell EDSL; and another that doesn't use Haskell at all. I don't know if we have any numbers, but I'd guess you're right that most engineers are not doing Haskell 100% of the time every day, but if I had to guess I'd say the majority (maybe not "most", but &gt;50%) of engineers touch Haskell code regularly. As for OP's question, I hear we crossed 110 recently. Again, not all Haskell folks, but a whole mess of us do quite a bit of Haskell on a regular basis. All that being said: We are not a Haskell shop, we are an R&amp;D shop whose mission is to improve the trustworthiness of critical systems. We just like Haskell an awful lot. But we also use Rust, C, Python, Coq, Lean, Verilog, and all sorts of other things, as the situation demands. If that sounds fun to you, by the way, [we're still hiring](https://galois.com/careers/#careers-list).
LeapYear in Berkeley, CA (potentially SF at some point in the future) has around 10 engineers who work in the Haskell part of the codebase. We're hiring for all positions! The platform team works on the core application, written entirely in Haskell. I'm on the infrastructure team, and we deal with a lot of languages, but I find ways to sneak Haskell in ;) Recently, wrote a GitHub merge bot in Haskell. https://leapyear.ai/careers
One more thing, just HPFFP is not enough, you have to learn some ghc language extensions to understand some examples in CTFP.
These won't be explained in CTFP?
Explained in one sentence. Almost nothing.
You don't need all of GTK, only what you use. Also, interfaces trivially compose. In principle you could provide a class for each GTK function. In practice you'll probably want to group things but the ideal lines depend on what you want to know about the callbacks. Read vs write is a common distinction, sometimes valuable. You should also consider building higher-level interfaces atop the lower level constructs - they can communicate more and might be easier to mock (or at least valuable to mock separately from their translation into GTK). As an example, maybe you have some banner text that can be set from multiple places. If you provide that to your callbacks as a function `setBannerText :: WriteGTKInterface m =&gt; Text -&gt; m ()` then in order to test those callbacks you need to mock out `WriteGTKInterface`. If you provide a typeclass `CanSetBannerText` with `setBannerText :: Text -&gt; m ()` then you can mock it in a way that just records the last banner. (Note that the names here are chosen to communicate in the context of this comment - there are probably better choices in light of Haskell idioms and your particular code base.)
\&gt; I find ways to sneak Haskell in ... with the strong support of half the company. A little different from those of us who've snuck Haskell in in other contexts :D &amp;#x200B; Incidentally, as an former LeapYear employee I recommend checking them out - great team!
;) hi david
Also make sure you're considering tags as well as branches: you could have a tagged leaf commit that is not included in any branch.
I wonder how many of us there wishing we didn't have to use our primary language / technology stack at work and trying to carve out some free time to use Haskell.
At least one more.
I would imagine it would technically be Google, though thats not in the spirit of your question ;)
Sadly, probably not.
Just to emphasize a repeated point, Category Theory for Programmers does *not* teach programming, functional or otherwise. It uses C++ because it's trying to teach category theory concepts *using C++, and* Haskell. It uses C++ because it's a useful tool to demonstrate how programming relates to functional programming. It assumes you already know C++. It also assumes you already know Haskell. &amp;#x200B; So, CT4P is definitely *not* a way to teach or learn programming. It's meant to teach category theory to people who are programmers with some amount of experience in programming.
&gt; And btw, our community doesn’t really need any extra competition wtf?
I just picked it because it was the first lambda function I ever wrote, haha. That makes sense. Do you think it might be possible to detect which parts of a program can be optimally reduced at compile time and use regular beta reduction for everything else? Giving up Turing completeness is a tough pill to swallow
Will your project (Haskell + Formality-Core) subsume my project [(GHC + GRIN)](https://github.com/grin-tech/ghc-grin)?
As a current LeapYear employee, I _also_ recommend checking them out. Hi, David. Hi Brandon.
In my company everyone needs to know and code in haskell, erlang or lisp. Right now i have 6 haskell codders from 9 employees.
Okay! You have _really_ piqued my interest here. I know this sub tends to talk about Galois as the go-to Haskell shop, but I have to say, I have been getting really interested in logic-based programming, e.g. Coq and Lean. My degree is in Physics and I'm currently working on a numerical analysis type application (in Python, of course). I simply don't know where to begin getting industry experience using languages like Haskell and Coq. I absolutely love the symbolic programming involved in the application I'm currently working on (which involves working with and extending the sympy framework), but it's not the focus. I've realized that our application could benefit a lot from a logical programming paradigm for some user-facing functionality and, in particular, for configuration and specification writing, e.g. for system modelling, and I intend to try to shift some of the focus of our development in that direction -- but I'm by no means the boss. I do sometimes work with Verilog as it's prominently featured in some of our products (and I will be working with it more intimately in a future project). But It's these languages I mentioned in which I'd really like to get my fingers dirty. I've been here for about a year and a half at this point. I'm not sure I can reasonably apply, as you suggest, because it's an awkward time to leave and I don't have a huge amount of experience in my current position yet (although I'm loving what I do!) but any suggestions you may have to start down a career path working with functional, logic-based and symbolic programming, I'm all ears :)
HPFP. You will get FP, Haskell, lambda calculus, some CT jargon, some good practices. But you wont go wrong with either one in my experience (have both).
I moved into infrastructure (DevOps style) about 20 years ago because I didn't want to do Java or C++ any more, and while embedded software in C would have been fun, the pay was far less at the time. Now that I'm more senior, I can choose the tools for each job so I manage to use Haskell a lot (amongst many other langs). It's nice to hear that Haskell is getting more popular as a primary development language, but like all things without the strong promotion of a big company, it takes a long time to get mass adoption.
Team Mordor has I think 7 people hired specifically for Haskell.
Habito
&gt; Part of a problem is that maintainers don't have time I think We don't have enough time and people to have a competition.
I think IOHK has a few. Blockchain engineering company.
In France, Vente Privée, FretLink and Tweag are the biggest Haskell users (who are open about it). And who are not Fintech companies.
Mercury.co is Haskell-based
Ah, ol' Lambda-Man is a legend here at University of Edinburgh! Shame he's not wearing his costume in this video.
I hope that the technique of the paper could solve this kind of optimization.
It could have been worded differently but I think the argument OP is trying to make is that it's better for the community to have people join forces to improve the core libraries everybody already uses than end up with several one-man-shows working on their replacement for said core libraries and competing for contributors to their specific sub-ecosystem. A commonly voiced complaint of Haskell newcomers is that you have to spend time deciding which sub-ecosystem to buy into (e.g. pipes/conduit/streaming/machines/io-streams, yesod/happstack/snap/servant, stack/cabal, aeson/waargonaut, and the list goes on...) which to outsiders mostly seem to accomplish the same task but with a slightly different bikeshed colour. In this context Haskell really feels like an academic community as everyone seems to be doing their own "research" into how to express common everyday problems into yet another slightly different Haskell API representation they can publish ~~paper~~package to Hackage and where `build-depends` are basically citations. And when you're shopping for a package to accomplish task X you basically end up doing the equivalent of literature research.
Amazing feedback for the position. Do you accept reddit gold?
Which company is that?
Scrive is currently at 9. We are actively recruiting more.
Facebook has large teams using Haskell. I'd estimate that about ~50 people there use it daily.
I heard they use a strict-evaluation variant of haskell
I can highly recommend Okasaki's book on data structures: [https://www.amazon.com/Purely-Functional-Data-Structures-Okasaki/dp/0521663504](https://www.amazon.com/Purely-Functional-Data-Structures-Okasaki/dp/0521663504), if you are looking for inspiration or techniques.
They do, although writing it feels almost identical to writing normal Haskell and my understanding is they're using normal Haskell more and more too.
Are you contracted by US' DoD/CIA and/or NSA? To me that would be hard to reconcile in some cases.
Nice suggestion. Will buy that book
I was thinking how we could make use of the newly added [`Ap`](https://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Monoid.html#t:Ap) the lifting wrapper which captures this behaviour instance (Applicative f, Num a) =&gt; Num (Ap f a) where (+) = liftA2 (+) (*) = liftA2 (*) negate = fmap negate fromInteger = pure . fromInteger abs = fmap abs signum = fmap signum This is the best we can do with [`-XApplyingVia`](https://github.com/ghc-proposals/ghc-proposals/pull/218) since it requires a type signature like [`-XTypeApplications`](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#visible-type-application) and I wonder if there is another way that uses `Ap` instead of an overlapping instance -- &gt;&gt; as -- [[1],[1],[1]] as = [1] : rest @(via Ap) where rest :: Num n =&gt; [n] rest = [1, 1]
We at Serokell have already hired a little more than 45 Haskellers, it almost completely covers our current needs
Once we can write [`1 @Int`](https://gitlab.haskell.org/ghc/ghc/issues/11409) we can avoid a type signature as = [1] : [1 @(via Ap), 1 @(via Ap)] but I don't want to apply via `Ap` at each element
The addition of integers is great! It makes it easier to explain what optimal evaluation gives us with a concrete example. Consider the following term: let f = \x -&gt; 1 + 2 in f 5 + f 8 If you run this with GHC it would perform the computation `1 + 2` twice, in `Formality-Core` it doesn't. In this example, you could apply let-floating to avoid the duplication of work: let u = 1 + 2 in let f = \x -&gt; u in f 5 + f 8 And I would assume that GHC does. You cannot do this for all terms in function bodies because some of them might depend on the argument. I am not sure if you can always do this for those computations that do not depend on the argument, but I guess it would be very annoying anyway. But I like to view the magic behind optimal evaluation a bit differently. Another optimization that you could apply to the original term is constant folding to get: let f = \x -&gt; 3 in f 5 + f 8 GHC might do this as well as or instead of let-floating. The cool thing is that `Formality-Core` does this lazily, on-demand, at run time. It is like "Just in time optimization". Once it has optimized a term it will share it across all calls, so you never optimize a function twice. While integers are great to illustrate the idea, what's even cooler is that it does the same with another kind of constant folding: beta reduction. It will optimize your function by applying all known functions and then share the result across all calls of the function. Because we are at run time, all functions will be eventually known. The example term in `Formality-Core` looks like [this](https://imgur.com/a/0SsNrD8). If you use the rules from [here](https://imgur.com/kmsHdUz) you can reduce it and see that the computation `1 + 2` is only performed once.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/itMcQH5.jpg** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme)^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20enw78kh)
I work for an insurance startup in London looking for at least one, maybe two senior Haskell developers. PM me if you want details.
Observing that the single biggest issue with laziness is probably lazy IO, and that Mu was designed before modern streaming libraries were as capable as conduit/pipes are now, would you say that Mu would be necessary if SC were starting a codebase today? Or did it have more to do with eg. easier-to-understand performance characteristics particularly for people who aren't used to Haskell? I once tried to watch Lennart Augustsson's talk on why Mu but the audio quality was too poor.
&gt; You don't need all of GTK, only what you use. This may work for very small applications, but what about large applications which use a large subset of the GTK library? I will reiterate what I said above: it is simply impractical to rewrite the whole of GTK to get a nicer interface. As for the rest of your reply, I agree completely; once there is such an interface, writing functions like those becomes easy. The problem is getting the interface in the first place!
The GADTs tutorial in this repo is good imo! https://github.com/i-am-tom/haskell-exercises
The sole reason for Mu being strict, as told by the people who originally developed it, is that it targeted a pre-existing strict runtime.
If `n` is a numeral, then its interpretation does not depend on the state. `A[[n]]s = A[[n]]s'` for all `s` and `s'`. Think about it; does the expression `5` have a different meaning when `x = 2` than when `x = 1`? So, in the final line, `A[[n[y -&gt; a']]]s = A[[n]]s` because numerals don't have any variables to substitute. Then, `A[[n]]s = A[[n]](s[y -&gt; A[[a']]s])` because, again, numerals don't have any variables, so they don't depend on the state, and you can change the state to whatever you like without changing the numeral's meaning. The `s'` state variable that appears, and the `A[[n]]s'` step as a whole, is just for emphasis; the author is trying to show that you can set the state to *anything* and still get the name integer, `i`, from evaluating `n`.
Use [`streaming`](http://hackage.haskell.org/package/streaming). Pseudocode: caller :: Stream (Of a) m a -&gt; m Result caller s = do l &lt;- length_ s doStuffWithLength l callee :: Stream (Of Int) m a -&gt; Stream (Of Int) m Bool callee s = do r &lt;- all_ (&gt; 5) (copy s) doStuffWithResult r You cannot do this with pure lists, you need coroutining.
Your type `Q` looks a bit like [`CoT`](http://hackage.haskell.org/package/kan-extensions-5.2/docs/Control-Monad-Co.html#t:CoT).
&gt;Is a flipped design feasible? Yes. See every other programming language ever :D &amp;#x200B; More seriously, consider the humble *if* statement. You pass in a boolean and two legs of computations. It would royally suck if your language evaluated both legs before deciding which one to take. In Haskell, I find myself wanting to program *if*\-like things (See *when* and *unless*). In Java, I have run into the bug where I do *someOptionalExpression().map(x -&gt; doSomething(x).orElse(getThing());* and when I debug it I realise that *getThing()* was called both when I had a value and when I didn't. &amp;#x200B; Have a look at the Java 8 (and onwards) implementation of Streams. In particular look at ReferencePipeline and SliceOps. Compare its implementation of \`limit(int)\` against Haskell's \`take n\`. Browse these tickets relating to Streaming misbehaviour in the core libraries: [https://bugs.openjdk.java.net/browse/JDK-8075939](https://bugs.openjdk.java.net/browse/JDK-8075939) [https://bugs.java.com/bugdatabase/view\_bug.do?bug\_id=8075939](https://bugs.java.com/bugdatabase/view_bug.do?bug_id=8075939) &amp;#x200B; It seems much simpler to start with laziness and ask for strictness than to start with strict operations and ask for laziness. &amp;#x200B; You can turn on Haskell strictness at the module level using {-# LANGUAGE Strict #-} although I haven't tried it.
Maybe you could abuse http://hackage.haskell.org/package/base-4.12.0.0/docs/Control-Monad-Fail.html#t:MonadFail ? ~~~ data Cases = A | B | C | D main = putStrLn (fallthrough D) fallthrough :: Cases -&gt; String fallthrough c = head $ concat [ do A &lt;- return c return "a" , do B &lt;- return c return "b" , return "c or d" ] ~~~
Well, I don't think you can use a newtype to make "[1]:[1,1]" literally work, but something like `getAp &lt;$&gt; [1]:[1,1]` might. I'm still on 8.4.4 (and probably will be for quite a while), so I don't have Data.Monoid.Ap.
&gt; reasoning about performance in a lazy content Okasaki is a good read for this. The paperback is 20 years old at this point.
They've certainly worked with DARPA in the past. HACMS (Hack 'em -s) and SMACCM (Smack 'em) used Ivory and Tower, both developed by Galois.
&gt; Giving up Turing completeness is a tough pill to swallow It's really not. You can get as close to turing completeness as you want in a total language, you just can't get all the way there. Just have your unbounded loops count the number of iterations and abort when they get to 2^128, you'll outlast any of today's hardware. Not long enough?; just abort when you get to 2^2^128 instead, you'll outlast baryonic mater.
I think it depends on the type of callee, it isn't polymorphic enough you maybe able to smuggle through the `Maybe Int` monoid through it. But, if callee is also `[a] -&gt; Result`, I think you are lost.
I lead a team of 6 Haskell engineers at [ITProTV](https://www.itpro.tv). We have some non-Haskell legacy services, but pretty much all new backend development is in Haskell. Day to day I'd say it's about 80% Haskell. For what it's worth, our frontend team also does a lot of Elm.
Seconded. I have one from 1999 on my bookshelf; it looks dense (it's a thin book) but it's not. Recommended.
You might love [coconut](http://coconut-lang.org/), a superset of Python that tries to be Haskell. Unfortunately the tool-chain sucks (ie. transpiler is very slow) :( But it's still pretty cool.
By the way, now Formality-Core includes an `inf` syntax for non-ending computations. It essentially applies a Formality term over and over until a condition is met. It doesn't change the language or the theory in any way (i.e., each step of the loop is guaranteed to terminate), it is merely a handy external utility. It is also useful to represent constant-space non-fusible loops.
This is the third non-Haskell post you've made in the subreddit --- might I kindly suggest moving discussions like these to somewhere they're a little more on-topic?
I would love to do Haskell 24/7 but unfortunately most companies do not hire remote and that means only a tiny amount Haskell companies hire remote. Considering the passion you clearly need to persist doing something like Haskell, it kind of amazes me that everyone still wants bums-on-seats.
I am interested in learning about any language I can. :) I love cool ideas, so I am happy! :D
It is my startup. Java, javascript and similar crap are not welcome.
I just checked it out and omg it is awesome!! :D
Great explanation, thank you very much! Also, happy cake day!
Yeah I was a bit worried about this, it's part of a module for uni that follows from Haskell but isn't exactly, Haskell. I'll use r/findasubreddit :) Thanks for the heads up!
&gt; GHC might do this as well as or instead of let-floating. The cool thing is that Formality-Core does this lazily, on-demand, at run time. It is like "Just in time optimization". Once it has optimized a term it will share it across all calls, so you never optimize a function twice. Yes, I really like that way to describe it, as it is very close to what the algorithm is doing. It is basically a static-time optimizer running at runtime, if that makes sense. It has its drawbacks, but can do some quite cool things as a result. Thanks for the writeup!
Large applications probably started smaller. You can grow the interface to your needs organically - adding another method to an existing interface or adding a new interface, as needed. &gt; it is simply impractical to rewrite the whole of GTK to get a nicer interface. You're not rewriting the whole of GTK. At the limit, you're rewriting the whole of the GTK API. In a large project, this doesn't significantly move the needle.
Seems like the dependency bounds for webdriver are too loose, or the imports are.[1] It's probably worth a bug report. If webdriver followed the PvP, this particular issue wouldn't have happened, though there are tradeoffs. aeson-1.4.3.0 exports (&lt;?&gt;) from Data.Aeson; aeson-1.4.2.0 does not. You might try specifying a bound for aeson on the cabal command-line in order to use a version that will work with webdriver-0.8.5. [1] If you do unqualified, wildcard imports from a package, you are supposed to use tighter upper bounds, since the mere addition of a exported symbol can break source compatibility.
We ([Obsidian Systems](https://obsidian.systms)) currently have 21 Haskell developers.
I really want to apply for Haskell jobs, but it always feels like if it's not a web shop then it's a defense contractor or something in finance. :(
We have about 10 haskellers at [Symbiont.io](https://Symbiont.io). We are in Noho, NYC (and opening office in Amsterdam), building (our own) enterprise blockchain tech (contract lang, sdk, etc) and hiring more haskellers!
Sadly, even though some of us get to choose our tech, we don't get to choose how our company operates. I'd hire remote in a heartbeat if given the chance.
I'm also using Haskell in infrastructure! Adoption is low, but I also have enough autonomy (and write enough documentation) that I can write tools in whatever I like. It's a great time.
I would always suggest getting involved with an open source project that has at least a few other, active committers. Seeing people who have concrete experience in developing software *with other people* using technologies of interest to us is always valuable. So if there are projects you think are cool, you might troll their issues lists for small things you might be able to help out with, and start helping out. Plus, as you take on bigger things and get involved in the wider community, you'll make connections into companies using that technology, which can turn into job opportunities.
Yup. We list them [on our web page](https://galois.com/about/). :) So if that's an issue for you, you might look elsewhere.
At https://hackage.haskell.org/package/clock-0.8/docs/src/System.Clock.html#toNanoSecs I see this function definition for `toNanoSecs`: ``` -- | TimeSpec to nano seconds. toNanoSecs :: TimeSpec -&gt; Integer toNanoSecs (TimeSpec (toInteger -&gt; s) (toInteger -&gt; n)) = s * s2ns + n ``` What's with the use of `(toInteger -&gt; s)` and `(toInteger -&gt; n)` in the fields for pattern matching against the `TimeSpec` value? I haven't seen that syntax before and didn't know you could do that.
Creating a list of problems you would like to include might help entice other people to contribute. Additionally initializing a stack project in the haskell_way subdirectory would probably make it easier on you as well as potential contributors.
I mean, it is by the very definition of the word pseudoscience.
It looks like they are using [ViewPatterns](https://hackage.haskell.org/package/clock-0.8/docs/src/System.Clock.html#toNanoSecs).
i feel like a more fun question is: what cool things are folks working on and being paid to do! haskell is at best a symptom / faciliitator
The thesis that forms the basis of the book is available for free online and covers almost the same topics.
Just wanted to say thank you for all you replies to the questions here!
Indeed, thank you for the link!
You're welcome and thank you :)
If I understand correctly, the second index of `Term` always determines the first index. If you remove the first index, I suspect your instances would typecheck. That's a bit of a hint. You warned that this is a distillation of your actual code, so maybe it's not redundant there. If it is redundant there too, you might consider writing the type family that computes it from the `IsT`/`AndT`. If you do wrote that tyfam, you could use it in the instance context instead of trying to pattern match via `Union`, which is the ultimate problem with these instances as written. Happy to help more; let me know.
Specifically, one path forward is to define `F` such that you can write `t ~ F a` and `u ~ F b` in your instance contexts instead of `v ~ Union t u`. As written, you're trying to invert `Union`, but `Union` --- both conceptually and as you've defined it here --- does not have a (deterministic) inverse (c.f. "injectivity"). (GHC doesn't do any backtracking, etc.) HTH.
What are the core packages for networking, cryptography and hashing? I am working on a school project which will require a bit of each of these, I also need to generate UUIDs and might want to dip into CRDTs. I am interested in stability and simplicity over features.
I've setup a small project with Stack, but it's ended up being a single 150-line Main.hs file. Stack feels like overkill for this. What are my alternatives?
Sounds wonderful, thanks :)
Stack's fine for that. Cabal isn't really much simpler than slack. If you have no hackage/stackage dependencies, you can stick `#!runghc` at the top, and not even have an explicit build process. If you also don't use GHC-specific extensions, you can use `#!runhaskell` instead.
&gt;Large applications probably started smaller. You can grow the interface to your needs organically - adding another method to an existing interface or adding a new interface, as needed. Unfortunately, this doesn't help with an application which is already large, such as my own. &gt;At the limit, you're rewriting the whole of the GTK API. Yes, this is what I meant; sorry for being ambiguous. &gt;In a large project, this doesn't significantly move the needle. I would disagree here. The larger the project, the more API you need to rewrite. And the GTK API is *huge*; on my machine, it takes longer to compile than any other dependency.
http://hackage.haskell.org/package/cryptonite is cypto primitives. Though you might look for something higher level. http://hackage.haskell.org/package/network is what I use last time I needed non-HTTP networking. For HTTP / Rest there's some higher level stuff. For running a HTTP server, I used http://hackage.haskell.org/package/wai HTH
Or something involving blockchain
Tsuru uses employs some haskellers, myself included.
If I have some program that takes a line of user input with getLine and prints some output which of the following is more performant and why? main = do ... do stuff main vs main = sequence_ . repeat $ run where run = do ... do stuff
Based on the ASM I'm seeing on godbolt.org, the former seems more efficient by a tiny amount, but the difference is massively overshadowed by what ever "... do stuff" stands for.
You are eliminating a LOT of jobs there. You might be a little picky, especially combining that with something primarily Haskell.
Oh duh, I should have thought to look at the ASM. Thanks! I'm just trying to understand what is efficient and what isn't, since it's kind of less obvious in Haskell. For things like iteration less is obviously more efficient, but for more syntactical things like this I don't really know how to think about the efficiency conceptually to try to gauge it.
Oh duh, I should have thought to look at the ASM. Thanks! I'm just trying to understand what is efficient and what isn't, since it's kind of less obvious in Haskell. For things like iteration less is obviously more efficient, but for more syntactical things like this I don't really know how to think about the efficiency conceptually to try to gauge it.
Oh duh, I should have thought to look at the ASM. Thanks! I'm just trying to understand what is efficient and what isn't, since it's kind of less obvious in Haskell. For things like iteration less is obviously more efficient, but for more syntactical things like this I don't really know how to think about the efficiency conceptually to try to gauge it.
Oh duh, I should have thought to look at the ASM. Thanks! I'm just trying to understand what is efficient and what isn't, since it's kind of less obvious in Haskell. For things like iteration less is obviously more efficient, but for more syntactical things like this I don't really know how to think about the efficiency conceptually to try to gauge it.
Oh duh, I should have thought to look at the ASM. Thanks! I'm just trying to understand what is efficient and what isn't, since it's kind of less obvious in Haskell. For things like iteration less is obviously more efficient, but for more syntactical things like this I don't really know how to think about the efficiency conceptually to try to gauge it.
Eliminating jobs I don't want to do on ethical grounds is AOK with me. I already did the web thing and I don't care for it. There will always be new backend work to do.
Eliminating jobs I don't want to do on ethical grounds is AOK with me. I already did the web thing and I don't care for it. There will always be new backend work to do.
Eliminating jobs I don't want to do on ethical grounds is AOK with me. I already did the web thing and I don't care for it. There will always be new backend work to do.
Eliminating jobs I don't want to do on ethical grounds is AOK with me. I already did the web thing and I don't care for it. There will always be new backend work to do.
Oh duh, I should have thought to look at the ASM. Thanks! I'm just trying to understand what is efficient and what isn't, since it's kind of less obvious in Haskell. For things like iteration less is obviously more efficient, but for more syntactical things like this I don't really know how to think about the efficiency conceptually to try to gauge it.
Oh duh, I should have thought to look at the ASM. Thanks! I'm just trying to understand what is efficient and what isn't, since it's kind of less obvious in Haskell. For things like iteration less is obviously more efficient, but for more syntactical things like this I don't really know how to think about the efficiency conceptually to try to gauge it.
Oh duh, I should have thought to look at the ASM. Thanks! I'm just trying to understand what is efficient and what isn't, since it's kind of less obvious in Haskell. For things like iteration less is obviously more efficient, but for more syntactical things like this I don't really know how to think about the efficiency conceptually to try to gauge it.
Oh duh, I should have thought to look at the ASM. Thanks! I'm just trying to understand what is efficient and what isn't, since it's kind of less obvious in Haskell. For things like iteration less is obviously more efficient, but for more syntactical things like this I don't really know how to think about the efficiency conceptually to try to gauge it.
Oh duh, I should have thought to look at the ASM. Thanks! I'm just trying to understand what is efficient and what isn't, since it's kind of less obvious in Haskell. For things like iteration less is obviously more efficient, but for more syntactical things like this I don't really know how to think about the efficiency conceptually to try to gauge it.
Oh duh, I should have thought to look at the ASM. Thanks!
Oh duh, I should have thought to look at the ASM. Thanks!
Eliminating jobs I don't want to do on ethical grounds is AOK with me. I already did the web thing and I don't care for it. There will always be new backend work to do.
Eliminating jobs I don't want to do on ethical grounds is AOK with me. I already did the web thing and I don't care for it. There will always be new backend work to do.
Eliminating jobs I don't want to do on ethical grounds is AOK with me. I already did the web thing and I don't care for it. There will always be new backend work to do.
Eliminating jobs I don't want to do on ethical grounds is AOK with me. I already did the web thing and I don't care for it. There will always be new backend work to do.
Oh duh, I should have thought to look at the ASM. Thanks! I'm just trying to understand what is efficient and what isn't, since it's kind of less obvious in Haskell. For things like iteration less is obviously more efficient, but for more syntactical things like this I don't really know how to think about the efficiency conceptually to try to gauge it.
Bit late to the party but a good source might be the Movement (techno festival in Detroit) 2019 playlist. Can find it here: [https://open.spotify.com/user/chadlobocinco/playlist/5pAuUFvRPzarkmfbLZUbQb?si=xG0PkakvRi28t\_S7XB3pmg](https://open.spotify.com/user/chadlobocinco/playlist/5pAuUFvRPzarkmfbLZUbQb?si=xG0PkakvRi28t_S7XB3pmg).
Bit late to the party but a good source might be the Movement (techno festival in Detroit) 2019 playlist. Can find it here: [https://open.spotify.com/user/chadlobocinco/playlist/5pAuUFvRPzarkmfbLZUbQb?si=xG0PkakvRi28t\_S7XB3pmg](https://open.spotify.com/user/chadlobocinco/playlist/5pAuUFvRPzarkmfbLZUbQb?si=xG0PkakvRi28t_S7XB3pmg).
Bit late to the party but a good source might be the Movement (techno festival in Detroit) 2019 playlist. Can find it here: https://open.spotify.com/user/chadlobocinco/playlist/5pAuUFvRPzarkmfbLZUbQb?si=xG0PkakvRi28t_S7XB3pmg.
Bit late to the party but a good source might be the Movement (techno festival in Detroit) 2019 playlist. Can find it here: [https://open.spotify.com/user/chadlobocinco/playlist/5pAuUFvRPzarkmfbLZUbQb?si=xG0PkakvRi28t\_S7XB3pmg](https://open.spotify.com/user/chadlobocinco/playlist/5pAuUFvRPzarkmfbLZUbQb?si=xG0PkakvRi28t_S7XB3pmg).
Bit late to the party but a good source might be the Movement (techno festival in Detroit) 2019 playlist. Can find it here: [https://open.spotify.com/user/chadlobocinco/playlist/5pAuUFvRPzarkmfbLZUbQb?si=xG0PkakvRi28t\_S7XB3pmg](https://open.spotify.com/user/chadlobocinco/playlist/5pAuUFvRPzarkmfbLZUbQb?si=xG0PkakvRi28t_S7XB3pmg).
&gt; godbolt.org As an aside, I'd never heard of godbolt.org before -- it's quite awesome.
&gt; godbolt.org As an aside, I'd never heard of godbolt.org before -- it's quite awesome.
&gt; godbolt.org As an aside, I'd never heard of godbolt.org before -- it's quite awesome.
&gt; godbolt.org As an aside, I'd never heard of godbolt.org before -- it's quite awesome.
&gt; godbolt.org As an aside, I'd never heard of godbolt.org before -- it's quite awesome.
Oh duh, I should have thought to look at the ASM. Thanks! I'm just trying to understand what is efficient and what isn't, since it's kind of less obvious in Haskell. For things like iteration less is obviously more efficient, but for more syntactical things like this I don't really know how to think about the efficiency conceptually to try to gauge it.
I'm using Reactive Banana to do exactly that. The GUI wrangling still has to be in IO (in RB its MomentIO). However I've worked to separate the pure components from the GUI. The diagram editing uses a Free Monad (google it) wrapped around an automaton functor: data AutoF i o a = AutoF o (i -&gt; a) -- | The automaton can either generate an output @o@ and get the input for the -- next step, or it can perform an action in monad @m@ and use the result as the input. newtype AutoT i o m a = AutoT (FreeT (AutoF i o) m a) deriving (Functor, Applicative, Monad, MonadIO, MonadTrans) The clever bit is the following incantation: yield :: o -&gt; AutoT i o m i yield v = AutoT $ liftF $ AutoF v id Now I can write code that looks like this: interactiveThing :: AutoT Int String MyState Int interactiveThing = do x1 &lt;- lift $ somethingInMyState x2 &lt; yield x1 lift $ somethingElse x2 The runAutoT function takes an AutoT value and returns, in the underlying monad, a pair consisting of an output value and a function from an input to a new AutoT. The Void says that the top-level action can never terminate: it has to be an endless loop. runAutoT :: (Monad m) =&gt; AutoT i o m Void -&gt; m (o, i -&gt; AutoT i o m Void) runAutoT (AutoT step) = do f &lt;- runFreeT step case f of Pure v -&gt; absurd v Free (AutoF o next) -&gt; return (o, AutoT . next) From the point of view of runAutoT you have a plain ordinary state machine: each input triggers a transition to the next state. But from inside the AutoT monad you have a sequential language where you can interact with the outside world by exchanging an output for an input using "yield". So now I can write sequential stuff like "user clicks a box button, user starts a drag at (x1,y1), user ends drag at (x2,y2), return a box with those corners" as a sequential piece of code instead of a fragmented state machine. The input to the machine is mouse events, the output is drawing instructions in the Cairo "Render" monad, and the underlying monad is the diagram state. Or, roughly speaking, type Editor a = AutoT MouseEvent (Render ()) (State Diagram) a The outer loop is the only bit in the IO monad. It gets mouse events, passes them to the current AutoT state to get a new state and an output, and then renders the output.
This is an interesting approach, and one I've not seen before! However, I'm finding it a bit hard to understand exactly what's going on here; if your code is online, could you give me a link? Also, you mention briefly at the beginning that you're also using the `reactive-banana` library; did you use this together with `AutoT`, or are they used in separate parts of the program? (Incidentally, `yield` here reminds me of [reinversion of control using `ContT`](http://blog.sigfpe.com/2011/10/quick-and-dirty-reinversion-of-control.html), which uses a very similar trick.)
Sorry, the full code is proprietary. You will need to read up on Free Monads before it makes any sense. Most free monads are based on functors with a bunch of different constructors, one for each primitive. This is the same concept but with only one primitive. I'm mostly using AutoT and Reactive Banana in different bits of the program but they do have to interact. Turning an AutoT into a Reactive Banana component that transforms events from one type to another is pretty trivial. Yes, its very similar to continuations, but there is no equivalent of callCC inside the AutoT monad. I spent quite a bit of time playing around with variations on the theme before settling on this one.
This reminds me of the old structured programming wars (showing my age here). Why bother with structured programming when you can get most of the same benefits in other languages as long as you observe the right discipline. History has repeatedly shown that automation is better than discipline with manual checks.
&gt; You will need to read up on Free Monads before it makes any sense. I'm already aware of how free monads work. I guess I'll just have to stare at the code some more until it starts to make sense... :) &gt; I spent quite a bit of time playing around with variations on the theme before settling on this one. I'm wondering: what exactly was it about this particular implementation which worked better than anything else?
What's wrong with finance though?
Where the caller here invokes the callee?
Note on the haskell mergesort implementation: You run splitAt recursively which is `O(n)` runtime for linked lists. We do this at each level so we add an extra `O(log(N)*N)` overhead. The standard library avoids this problem quite cleverly http://hackage.haskell.org/package/base-4.12.0.0/docs/src/Data.OldList.html#sortBy First we split the input into runs sequences [1, 4, 3, 2, 5, 8, 7] = [[1,4], [2, 3], [5, 8], [7]] and then we merge these sequences pairwise [ merge [1,4] [2, 3], merge [4,8] [7] ] merge [1,2,3,4] [5,7,8] [1,2,3,4,5,7,8]
Instead of `sequence_ . repeat`, I would recommend `forever`, which should be equivalent to your first variant after inlining.
Yes, sorry, a more complete example is: module Main where import Streaming import qualified Streaming.Prelude as S callee :: Monad m =&gt; Stream (Of Int) m a -&gt; Stream (Of Int) m Bool callee s = do (b :&gt; _) &lt;- S.all (&gt; 5) (S.copy s) return b caller :: Stream (Of a) IO Bool -&gt; IO () caller s = do (l :&gt; b) &lt;- S.length s putStrLn ("The length of the stream is " ++ show l) putStrLn ("callee returned " ++ show b) main :: IO () main = do caller (callee (S.each [1,2,3])) &gt; main The length of the stream is 3 callee returned False The secret sauce is the function [`copy`](http://hackage.haskell.org/package/streaming-0.2.2.0/docs/Streaming-Prelude.html#v:copy) which does "Duplicate the content of stream, so that it can be acted on twice in different ways, but without breaking streaming.". The last part gives you the guarantee that the stream will only be traversed once (effects won't be duplicated).
&gt; `getAp &lt;$&gt; [1]:[1,1]` It works as `[1]:fmap getAp [1,1]`
Nice
Bit late to the party but a good source might be the Movement (techno festival in Detroit) 2019 playlist. Can find it here: [https://open.spotify.com/user/chadlobocinco/playlist/5pAuUFvRPzarkmfbLZUbQb?si=xG0PkakvRi28t\_S7XB3pmg](https://open.spotify.com/user/chadlobocinco/playlist/5pAuUFvRPzarkmfbLZUbQb?si=xG0PkakvRi28t_S7XB3pmg). 
Pretty printers, Data.Discimination
You maybe don't want to export `elements`, `o`, and `identity` fields directly, since they can be use to create a new `Group` that isn't a mathematical group. E.g.: * `groupZ { identity = 1 }` * `groupZ { o = (*) }` * `groupZ { elements = fromList [0,1] }`.
Better link: https://github.com/fghibellini/nix-haskell-monorepo
George Wilson's fantastic [Contravariant Functors](https://www.youtube.com/watch?v=IJ_bVVsQhvc) talk gives some examples.
my hope was to have feedback visible to everyone in the discourse thread, but I guess that's not how things work 🤷
The type of `(^)` is `(Integral b, Num a) =&gt; a -&gt; b -&gt; a`, meaning that the base can be any number, but the exponent must be an integer type. There is a floating-point version of `(^)` called `(**)` which might be more suitable for your purpose: &gt; f x = x ** x - 100 &gt; f 4 156.0 &gt; f 3.5 -19.78821977103364
The way the error is reported is interesting You can directly request the type of "f 3.5" Prelude&gt; :t f 3.5 f 3.5 :: (Fractional a, Integral a) =&gt; a [Integral](https://hackage.haskell.org/package/base-4.12.0.0/docs/Prelude.html#t:Integral) basically means integer-like type, while [Fractional](https://hackage.haskell.org/package/base-4.12.0.0/docs/Prelude.html#t:Fractional) means floating-like type. So they contradict, and there probably no type which satisfies both typeclass. But Haskell typechecker cannot detect such error. So it instead prints another error, which deserves a separated explanation and a bit misleading in this case. If you try to do as it suggests ("Probable fix: use a type annotation to specify what ‘a0’ should be") you would get more meaningful error: Prelude&gt; f 3.5 :: Double &lt;interactive&gt;:20:1: error: • No instance for (Integral Double) arising from a use of ‘f’ • In the expression: f 3.5 :: Double In an equation for ‘it’: it = f 3.5 :: Double
Aha, it seems that `llvm-hs` supports both static and dynamic linking. This works: stack build --fast --flag llvm-hs:shared-llvm I'm a bit surprised that it's necessary. http://hackage.haskell.org/package/llvm-hs-4.1.0.0 says that flag is enabled by default; I can't find the string "shared-llvm" in the kaleidoscope codebase, so I don't see how it would be getting unset; and I don't imagine I've previously done anything that would globally install the package.
&gt; and there probably no type which satisfies both typeclass If you want to be cruel you can technically define an instance of both `Integral` and `Fractional` for `()`. This is because all the laws are equalities, there are no inequalities or injectivity/lossless requirements. The lack of such laws is fairly reasonable due to how a huge quantity of numeric types wrap at some point and thus lose information. However with that said the existence of `fromRational` and `toInteger` basically guarantees that any non-trivial instance of such a class would be wonky. As you'd need to decide what value to give `toInteger 0.5`.
Short: Yes. &amp;#x200B; Longer: Yes, but. We do accept only contracts that we believe are positive for the world. See \[our boundary policy\]([https://lifeatgalois.com/articles/the-boundary-policy/](https://lifeatgalois.com/articles/the-boundary-policy/)).
wow, TIL: godbolt.org
One can define a meaningful `Fractional` instance of `Word`, implementing modular division modulo 2^64.
To implement Fractional correctly, you need the modulus to be prime.
Can be used for sorting. You can check this presentation of Edward Kmett [https://www.youtube.com/watch?v=cB8DapKQz-I](https://www.youtube.com/watch?v=cB8DapKQz-I)
Applicatives.
It also means that `Arbitrary` instances can be written for functions, using (for example) the [`CoArbitrary` typeclass](http://hackage.haskell.org/package/QuickCheck-2.13.1/docs/Test-QuickCheck-Arbitrary.html#t:CoArbitrary).
It's similar to Edward Kmett's [`Data.Functor.Day.Curried`](http://hackage.haskell.org/package/kan-extensions-5.2/docs/Data-Functor-Day-Curried.html#t:Curried) type from the `kan-extensions` package. If you specialize it on just one `f` type, it becomes: newtype Curried f a = Curried { runCurried :: forall r. f (a -&gt; r) -&gt; f r } which, again, is a difference list of `Applicative`s, but associated differently. In particular, this one doesn't wind up building and reassociating pairs each time it's used.
Eliminating jobs that conflict with my ethics is AOK with me. As for the web, I did the full-stack thing for several years, and I'm over it. There's always backend work to be done, and there the choice of tool has been mostly up to me, which is nice.
Ooh, I like
And because it parenthesizes to the right, it doesn't work well on infinite traversals: &gt;&gt;&gt; quitOn p m = if p m then Left m else Right m &gt;&gt;&gt; traverse (quitOn (==5)) [0..] Left 5 &gt;&gt;&gt; import System.Timeout (timeout) &gt;&gt;&gt; timeout 1000 . print . fromCurried $ traverse (toCurried . quitOn (==5)) [0..] Nothing Fortunately, `Backwards` helps &gt;&gt;&gt; forwards . fromCurried . forwards $ traverse (Backwards . toCurried . Backwards . quitOn (==5)) [0..] Left 5
Any way to either stream it to youtube or record it a post after the class? I'm interested (40 years procedural programming fortan, cobol, ASM, C, C++, C# is hard to overcome ...)
Yeah, it's *super neat*. I learned about it from: https://www.youtube.com/watch?v=zBkNBP00wJE and was excited it support Haskell, too.
The Haskell Meetup in Austin was pretty bomb when I left a few years ago. You should check it out.
Reddit thread is for discussion. ;) We don't need to be directed to another discussion thread, just the content. :P I mainly posted my link because the first one was just empty until I tweaked my uMatrix settings to allow scripts to load from the CDN you are using. (its hostname doesn't match yours, so uMatric treats it as a third-party script and blocks it by default; it seemed to be a valid CDN, so I've allowed changed my local configuration, but others might have the same problem.) I find it disappointing when any page shows no content simply because I blocked the scripts.
Thanks for coming back with your workaround, even if it shouldn't be necessary and is still a bit mysterious.
That fixed it. Thanks.
Wait, why do you say Core isn't lazy? The only thing I can think of is the fact that `case` = WHNF in Core but can mean anything in source Haskell. This is rather moot, as Haskell `case`s of whatever strictness can be converted to a sequence (or absence thereof!) of Core `case`s.
You are right. I thought hiding the contructor \`Group\` would prevent this, turns out it's not.
Pretty printing, discrimination, sorting, serialization, working with equivalence classes, NFAs and DFAs make nice divisible/decidable instances, termination checking with well-quasi-orders, almost any kind of decomposable form of information sink, really.
Yeah, that's poorly worded on my part. I meant to say that evaluation is explicit in Core via `case`s. I'll reword it tomorrow! Thanks for pointing it out.
"Queerious" &amp;#x200B; No.
Don't come then! ¯\\\_(ツ)\_/¯
Alas! We don't have appropriate equipment for that right now :( We're a pretty young space, just two months old, so we don't have good video stuff yet.
[Rosetta Code](https://rosettacode.org/wiki/Rosetta_Code)?
&gt; I'm wondering: what exactly was it about this particular implementation which worked better than anything else? It was being able to write "yield". I could see that a diagram editor was going to be a state machine, but the great vice of state machines, if you code them as a literal state machine, is that the functionality is scattered around the code: you can't represent a linear sequence of actions by the user as a linear sequence of statements in your code. "yield" gets around that.
&gt;I find it disappointing when any page shows no content simply because I blocked the scripts. 💯 &amp;#x200B; Thanks for the clarification!
Oh, it took me some time to wonder why we need to *specialize* in this case, but we need because `countdown` is **recursive**. Recursive bindings aren't inlined, so optimisations don't fire that way. Something to mention in the post? If `countdown` weren't recursive, the *inlining* would do good job without specialization: countdown :: MonadState Int m =&gt; m () countdown = do v &lt;- get case v of 0 -&gt; pure () _ -&gt; do put $ v - 1 -- countdown -- commented out, countdown is not recursive anymore main :: IO () main = S.evalStateT countdown 10 would optimise to (with `ghc Countdown.hs -O -fno-specialize`): -- RHS size: {terms: 4, types: 9, coercions: 0, joins: 0/0} main1 :: State# RealWorld -&gt; (# State# RealWorld, () #) main1 = \ (s_a3sF :: State# RealWorld) -&gt; (# s_a3sF, () #) -- RHS size: {terms: 1, types: 0, coercions: 3, joins: 0/0} main :: IO () main = main1 `cast` &lt;Co:3&gt; -- RHS size: {terms: 2, types: 1, coercions: 3, joins: 0/0} main2 :: State# RealWorld -&gt; (# State# RealWorld, () #) main2 = runMainIO1 @ () (main1 `cast` &lt;Co:3&gt;) -- RHS size: {terms: 1, types: 0, coercions: 3, joins: 0/0} main :: IO () main = main2 `cast` &lt;Co:3&gt; which essentially *does nothing*, `main1` is `return ()`. `countdown` binding would stay unoptimised, and there aren't specialised version, but `countdown` were inlined in `main`, dictionaries cased etc. and eventually all code (which does nothing intersting) evaporated.
You can see example in the `co-log` library where Divisible and Decidable are used for more modular logging: * https://github.com/kowainik/co-log
Yes, specialisation is only ever needed if the inliner can't or won't handle it. That happens for 1. Recursive functions 2. Functions with huge function bodies In the former case, inlining would never reach a fixed-point (aside from causing endless code bloat). In the latter case, the inliner refrains from inlining due to code size concerns. The specialiser worries about the same concern, but there's the possibility of being able to *share* one specialisation at multiple call sites.
Love your posts Sandy, keep up the excellent work!
Thanks for the nice post! I have two suggestions. First, you say “When -XTypeApplications is enabled, rewrite rules are allowed to match on types too!”: it feels like all that follows requires you to enable -XTypeApplications, but the specializer can fire regardless, I believe? Maybe it needs clarification. Second, you say that GHC 8.10 improved on the current state — it'd be nice to have links on the corresponding tickets / commits pertaining to this.
Informative post. I have two small quibbles with &gt; but nobody actually writes code against transformers in the real world. Choosing a concrete monad transformer stack is limiting, and at the same time, prevents you from restricting access to pieces of the stack. - I sometimes write code against transformers. The fact that MTL doesn't fix the priority of effects in some cases is seen as and advantage (more freedom for the interpreter!) but can completely break logic that expects a particular order. In fact, I can't think of reasonable examples for which being "polymorphic on effect priority" (so to speak) is actually useful. - One can restrict access to pieces of the stack using typeclasses like [zoom](http://hackage.haskell.org/package/lens-4.17.1/docs/Control-Lens-Zoom.html#t:Zoom) and [magnify](http://hackage.haskell.org/package/lens-4.17.1/docs/Control-Lens-Zoom.html#t:Magnify) from [lens](http://hackage.haskell.org/package/lens), and functions like [withExcept](http://hackage.haskell.org/package/transformers-0.5.6.2/docs/Control-Monad-Trans-Except.html#v:withExcept). Also by making the logic be polymorphic on some part of the stack. The [mmorph](http://hackage.haskell.org/package/mmorph) package can be useful for massaging stacks.
This post is about specialization, not (yet again) about transformers vs mtl vs extensible effects. Be merciful.
&gt;I could see that a diagram editor was going to be a state machine, but the great vice of state machines, if you code them as a literal state machine, is that the functionality is scattered around the code: you can't represent a linear sequence of actions by the user as a linear sequence of statements in your code. "yield" gets around that. I realise already this is incredibly useful. My question was more along the lines of 'why did you use a free monadic representation instead of (say) `ContT`?'. Also, that SO question really helped; thanks for linking! It's been a while since I last read about free monads; I can't believe I didn't immediately parse `data AutoF i o a = AutoF o (i -&gt; a)` as 'it will output `o`, then get the next bit of input `i`'. Another quick question: why did you want a monad specifically and not an arrow? Anyway, now that I understand what's going on a bit better, I think I can now guess at the general architecture for a program using `AutoT`. If I'm understanding correctly: * The program itself is contained in an `AutoT event output IO Void`. It runs as a state machine: when it receives a new event as input, it computes the next GUI state given the current GUI state, then outputs the new state and waits for the next event. * The main loop runs in `IO` as a wrapper around the above state machine; it runs the state machine until it yields, then renders the output. When an event is received, it passes it to the 'paused' state machine so it can resume computation. This is a very cool architecture, and one which looks like it could be incredibly useful for [my own program](https://github.com/bradrn/cabasa)! Would it be OK with you if I try it? (I'm just worried about the fact that this is from a proprietary application originally...)
common packages like `contravariant`, `bifunctor`, `comonads` have a lot of typeclasses in `Data.Functor.*`, they rarely seen in the wild. And not so clean, when worth to use them (especially for beginners).
We are not ready to present the DSL yet but the comparison to CQL is relevant. We base our DSL on entities, attributes and relations just like CQL and we are also mapping multiple tables to multiple tables. We do, however, use other operators, different visualizations and other executions engines. Our key proposition is to ensure that data transformations are transparent to data prep developers as well as to domain experts (who might only be familiar with spreadsheets). This focus gives us different constraints and priorities.
Don’t comment if you have nothing productive to say.
If you use stack you don't really need that, the snapshot determined the version of most of your dependencies so doing an update is as simple as choosing a newer snapshot in your stack.yml and fixing error if you have any
&gt; This transformation is known as the case of known case optimization. It's actually known as case-of-known-constructor.
Nice find! Indeed, adding a `NOINLINE countdown` on the non-recursive version does show the bad case again. I find optimization to be a hard thing to wrap my head around!
That's kind of why I use \`cabal\`. \`stack\` isn't really built to use updates.
Note that `stack` works with snapshots which are package sets. What you can do is update to newer resolver, and then `stack` will complain if bounds are restrictive. However, if swapping snapshots is not convenient for you, you could try `cabal v2-build`. Cabal uses `build-depends` specifications to *find* (*solve*) an install plan. One analogy is that `stack` &amp; Stackage approach is mere type-checking of given constraints (package versions in Stackage snapshot match package definition requirements), when `cabal` solver is *type-inference* (find an install plan which would satisfy the requirements). There is `cabal outdated` which fits solver approach well: https://cabal.readthedocs.io/en/latest/developing-packages.html#listing-outdated-dependency-version-bounds Note that JS package dependency approach is neither. There aren't package-sets, resolvers, or snapshot. But there aren't solver either, as `npm` doesn't need one. It's ok to have multiple versions of the same package in a tree of `node_modules`. Haskell dependencies on the other hand are "flat", i.e. a package can be only once in the install plan. (Not exactly, but to the first approximation). To my understanding, some NodeJS tools try to flatten dependency trees, but it's not a hard requirement. Still, I think that `cabal` solver approach is in a spirit closer to what `npm` does. There's also [`packdeps`](https://hackage.haskell.org/package/packdeps) though, which doesn't use solver at all. It's a heuristic approach, it will tell you that some upper bounds are restrictive, **but** it won't tell you if there exists an install plan which would be able to reach that install-plan. Current example is that you could relax `hashable` to include `hashable-1.3` released recently, but in practice there aren't install plans with `hashable-1.3` as there is still a restrictive `hashable &lt;1.3` bound in `unordered-containers`. So the lesson here, it's not enough to only relax the bound, you'd actually need to compile (and read the changelogs) with newer versions to see if relaxing is ok. I'd recommend to install package from GitHub, as `packdeps` isn't on Stackage. So to wrap this up, one way to go (with `stack`): 1. use `packdeps` 2. if there's something: read changelogs 3. relax bounds 4. add new package versions to `extra-deps`, loop until `stack` is happy 5. profit! You may or may not dislike the manual 4th step which `cabal` solver automates.
https://packdeps.haskellers.com/feed?needle=your-package-name one you get your package on hackage. Prior to that, I don't know.
There’s a page on the stackage website that tells you what packages are updated between Stackage versions: https://www.stackage.org/diff/lts-13.20/lts-13.21 Stack uses a set of packages that might not be the latest (but are known to work together), so you may not get the latest packages even on the latest LTS (or nightly), but I wouldn’t worry about that unless you eg run into a bug and want to check it’s reproducing against the latest version of a library.
Thank you. It works with aeson-1.4.2.0.
So stack snapshot defines exact versions of all dependencies? I don't want even minor/patch version changes - that's how it is working by default in JavaScript with npm and that's why I use ncu + exact package versions, avoiding this behaviour. (I clarified it a bit in OP.)
Yep, stack uses exact versions of all dependencies. If you have eg LTS-13.21 in your stack.yaml file, you’ll get exactly these versions: https://www.stackage.org/lts-13.21
Glad to hear that, thank you :).
Just use splitmix or tf-random
&gt; For example in JS world there is ncu utlity (npm-check-updates). Is this comparable to `cabal outdated`, see https://cabal.readthedocs.io/en/latest/developing-packages.html#listing-outdated-dependency-version-bounds ?
What Vim plugin can I use for auto-completion and finding definitions? I want a replacement for YCM.
SimSpace is remote.
It should be possible to use haskell-ide-engine as a LSP server, and there one or two vim plugins that will at as LSP clients.
&gt; Unfortunately, this doesn't help with an application which is already large, such as my own. True. You could still introduce and grow your interfaces gradually, though. Work a callback or several at a time, starting where you think you'll get the most bang for your buck. Documentation that some callbacks are written against nothing outside of an interface is both most valuable and most easy to obtain when that interface is smaller.
Thanks. The example is only an example of this kind of inter-pakage optimizations. Sure in every example that one could present, there is a way to optimize it manually but that is not the solution I'm looking for, as I mention above. The paper may be a true solution.
At least 30!
Good approach, and one of the most practical I've seen so far in this thread. I'll definitely consider doing this.
What's the problem?
Hopefully one day I'll pay for all my countless stupid questions that random people on the internet took the time to answer through the years!
That worked. Any idea how to make it auto format on save? I can't seem to get it work with ALE's formatting.
https://wiki.haskell.org/Non-strict_semantics
I don't have one in Haskell, however in Scala, it can be used for auto-derivation of typeclasses. &amp;#x200B; [https://github.com/scalaz/scalaz-deriving/blob/898c37d03dcc0fe239d007ccd0705eef091e362e/scalaz-deriving/src/test/scala/examples/Same.scala#L27](https://github.com/scalaz/scalaz-deriving/blob/898c37d03dcc0fe239d007ccd0705eef091e362e/scalaz-deriving/src/test/scala/examples/Same.scala#L27) &amp;#x200B; I know that link won't tell you much on its own (you would need to read up the whole library, as well as understand Scala), the important part is that, because Scala's typeclasses are like "core" typeclasses, you can make typeclass instances for the typeclasses themselves; as you can see, in that like, an instance is being made for the typeclass \`Divisible\` for the type (class) \`Same\`. This allows you to derive typeclasses using Divisible.
No, sorry.
[parseRefTime](https://github.com/facebook/duckling/blob/master/exe/ExampleMain.hs#L139) uses Duckling without any HTTP.
haskell is not lazy at the type level
&gt; Also, ive never actually submitted anything to Hackage, (This is actually my first open github repo too) so id appreciate any feedback on how it looks from a packaging pov if im going to upload. You already have the `.cabal` file, so you can just go create a Hackage account, run `cabal sdist` and upload your tarball. It's also a good idea to run `cabal check` before that and maybe implement the recommendations you get. After a cursory look, I'd also recommend loosening the `cabal-version: 2.0` field value to `&gt;= 2.0`, and adding `README.md` to `extra-source-files`.
Perhaps even more surprising is that data ShouldFail' = ShouldFail' { nope :: AssertEq 'True 'False } huh = ShouldFail' { nope = undefined } still compiles. Maybe the laziness is present to avoid extra work/exit early if possible in the presence of type-level recursion? Seems fishy...
Some of my favorite examples are in [co-log](https://kowainik.github.io/posts/2018-09-25-co-log) and in [dhall](https://hackage.haskell.org/package/dhall-1.23.0/docs/Dhall.html) :)
This is good! Would you like to go ahead and contribute this?
IO has its uses (and it's the non-IO functions that really bring the utility as pointed out in other comments), but in the type of application youre describing you're right that it would become mostly just an annotation. I would still minimize the code that does IO, but in such an application I would create sub types of IO -- ReadData, WriteData, etc types that define the kind of IO being done. In my opinion, this makes the code easier to understand and modify and makes it harder to sequence different kinds of IO in the wrong order accidentally. This would be very useful for a program that does large amounts of IO.
It just boils down to: well-typed programs don’t go wrong. That’s what the types are there for. So type-checking, and hence the “evaluation” of type-families, is only done when we need to determine that a term (value/expression/program) is correctly typed; i.e. that’s why you only see the error when you define the ‘shouldFail’ term. Basically, all your type declarations are just ways of telling the compiler what rules a correct program should abide by; and your ShouldFail type is basically saying: a program with this type is always wrong. So only when you actually created a value of the ‘ShouldFail’ does the compiler raise the error. Finally, wrt to ‘undefined’, it has type ‘forall a. a’; and even though you can TypeAppy/unify it with type ‘ShouldFail’, you don’t create a value of ‘ShouldFail’, but a value of type ‘forall a. a’. That’s why you won’t get an error when you do ‘undefined :: ShouldFail’
[A somewhat related question at Stack Overflow](https://stackoverflow.com/questions/54298813/when-does-type-level-computation-happen-with-type-families). I still don't have a clear image of how (and how many times) evaluation happens at the type level. And having a good mental model becomes more more necessary the more one plays with type family-heavy code (like extensible record libraries).
Updated with this! The ticket is [here](https://gitlab.haskell.org/ghc/ghc/issues/16473) and the patch is [here](https://gitlab.haskell.org/ghc/ghc/merge_requests/668)
Thanks!
Fixed, thanks!
I think that makes perfect sense. In most languages with powerful type systems, the way you encode an invalid state is via an uninhabited type. It's not erroneous for that type to exist in your program, it should just be impossible to inhabit it. `undefined` is, as you see here, the loophole. `undefined` inhabits every type in Haskell, including `Void` and `AssertEq 'True 'False`. You can essentially think of `a` in Haskell as being equivalent to `Maybe a` in a language like Agda, where `undefined` is the `Nothing`. Now we look at how it works in Agda: ``` record Unit : Set where data Void : Set where data Bool : Set where false : Bool true : Bool data Maybe (a : Set) : Set where nothing : Maybe a just : a -&gt; Maybe a proof : Bool -&gt; Set proof false = Void proof true = Unit foo : Maybe (proof false) foo = nothing ``` `foo` has `proof false` inside the type, which you can think of as equivalent to `TypeError ...`, but because it's wrapped in a `Maybe` (equivalent to Haskell's `undefined`), it type checks just fine.
Use `stack script`: https://docs.haskellstack.org/en/stable/GUIDE/#script-interpreter
Yes, at least if you assume haskell without extensions. Reduction and evaluation strategies are unique etc. However, dependently typed languages like Idris or Agda can verify programs using their type systems
Just `2.0` is perfectly fine as a cabal version. Ranges for `cabal-versions` are considered errors in new cabal: .cabal:1:28: error: unexpected cabal-version higher than 2.2 cannot be specified as a range. See https://github.com/haskell/cabal/issues/4899 expecting ".", "-", white space, "&amp;&amp;" or "||" 1 | cabal-version: &gt;= 2.4 | ^
There’s Liquid Haskell which is quite experimental. For Java, Ada, and C I believe there is more industrial use of tools like KeY, ESC/Java, Frama, etc.
&gt; I'd also recommend loosening the `cabal-version: 2.0` field value to `&gt;= 2.0` Why?
Up to a point, because the core of the language is relatively simple. Java isn't a great language to verify, but has had more development in verification tools. Haskell is nice for semi-formal reasoning, but it's advantages are minor at the industrial level of verification.
Had this doubt myself. Ended up going with http://hackage.haskell.org/package/duckling-0.1.6.1/docs/Duckling-Api.html#v:parse to use it as a lib, rather than a server
There's still one left!
\&gt; I still don't have a clear image of how (and how many times) evaluation happens at the type level. And having a good mental model becomes the more necessary the more one plays with type family-heavy code, like extensible record libraries. Exactly! I'm putting together a workshop on Type-level programming and ultimately finishing with an extensible records library (\`row-types\`. I need to understand type family semantics well enough so that I can explain to attendees on the spot in the case that I get some questions about when type family evaluation happens.
\&gt; Basically, all your type declarations are just ways of telling the compiler what rules a correct program should abide by; and your ShouldFail type is basically saying: a program with this type is always wrong. So only when you actually created a value of the ‘ShouldFail’ does the compiler raise the error. &amp;#x200B; Intuitively, I understand this behavior immediately. However, I have read repeatedly across several bits of documentation that "type family evaluation is strict", and I think that's where my confusion lay. It seems I need to modify this understanding with "... as long as you are constructing a value within your program with the type you are interested in evaluating." This is quite an interesting point-- that the "specification" of types is a different phase from the act of asserting all values in your program are well typed. Basically, the \`ShouldFail\` type synonym says that "if you constructed a value with type \`ShouldFail\`, that value would be ill typed"; Types of values are checked, but types themselves are only checked for well-kindedness. Without a manifestation of a particular type (a value) in your program, a type error will never be thrown. &amp;#x200B; Thanks for providing the extra bits I needed to understand this!
I don't like the qualifier *formally* in front of verify. It says *how* you want to verify, but not *what*. &amp;#x200B; For verifying everyday-business-logic, I much prefer Haskell to Java: * I wrap primitives in nominal types to make sure I don't mix up data. Nicer in Haskell. * It's nicer to just *not have null* in Haskell, rather than to check for it at runtime in Java. * Equality is much more sensibly defined in Haskell. * I like that Haskell keeps data and behaviour separate, as opposed to OOP which mixes them together. E.g. when I do a getSomething from some data in Haskell, I know that it's a simple getter - it's not going to be a fetching action. * Haskell pattern matching has pretty good warnings for when you haven't covered all cases. * Exceptions can and do happen in Haskell, but more often Haskell uses return types to signal error, making it easier for me to notice when I need to handle an error. For verifying more advanced/exotic properties, Haskell also excels. One example is the sized list, in which you *promote* values to the type level, so that you can have the type *SizedList 3 String* (as opposed to *SizedList n String*), which holds exactly three strings. It will not compile if you try to get the fourth String out of it. People prove much more exciting properties using the type system, for instance ensuring that a Red-Black tree is balanced, or that a list is ordered. Anything relating to multi-threading will be easier to reason about in Haskell than in Java. This is because more Haskell code tends to be written without mutation or other side-effects. More importantly than that, however, is that it's very *visible* in Haskell when mutation or side-effects take place. I won't comment on JavaScript (since I'm not proficient in it) except to say that most of the verification I do uses static type-checking at compile time, which (vanilla) JavaScript does not do.
I wrote a probabilistic programming library using the free monad approach a while back. It's on github [here](https://github.com/statusfailed/pp). I recall it was a fairly nice representation, but my code is incredibly slow (I think that was more on me than the representation though!) You may also want to check out the paper [Practical Probabilistic Programming with Monads](http://mlg.eng.cam.ac.uk/pub/pdf/SciGhaGor15.pdf) by Adam ́Scibior which has another approach (I haven't read this in detail yet... shame on me!) I hope that helps!
If you write in a subset without exceptions, bottom and unsafe, it's possible to translate a lot of it into something like Coq and reason about it directly there. Most Haskell however generally isn't written like this (with async exceptions, technically almost all code is extremely difficult to reason about as it could be interrupted at any point by another thread). That's why newer general-purpose dependently typed languages like Idris and Lean don't allow untyped exceptions or nontermination. The fact that any value could be bottom means it suffers the same difficulties Java does, where every value of type `T` is really of type `T|null`.
US/CA is remote, but not what I mean; Remote is worldwide, or the moon for that matter. So I consider 'really close by' still as 'bums on seats'; I do not consider a 10 hour flight to be an issue for meetings etc. It looks really interesting; maybe I should look for a job to get a green card...
Thanks for the pointer to that paper! The GADT they use seems like an interesting approach and there's a lot of good explanation as to why they chose the representation they did. &amp;#x200B; I'm primarily planning to implement variational inference so I'll need a lot of transparency in the representation.
Do you have an example of where these classes are used in pretty printing? Just curious.
My experience at Galois has been that if you want to do Haskell work, there's plenty available; if you want to branch out and try other things, there's plenty of that too. As a programmer who primarily uses Haskell, I've never had a problem finding opportunities.
Pretty much right, except the program is in `AutoT event output (State Diagram) Void`. AutoT has an instance for MonadState (which I didn't include in my extract), so you can then use `get` and `put`. The internal state is the diagram (plus miscellaneous other stuff). The output is the Render update to reflect the diagram changes on the screen, plus anything else you want as output. (My actual application is rather more sophisticated than this simple explanation. The state machine output is actually a set of things that have changed, and the main loop outside the AutoT then tracks this and figures out exactly what needs to be redrawn by doing set operations on its record of what has been drawn in the past. But you probably don't need all that. I'm not sure I do; it may have been a case of premature optimisation). By all means use this technique. I'm a director of the company that owns the original code, and I say as a director that I release the fragments of code posted here and on Stack Exchange into the public domain.
Wow, verifying red black trees are balanced with the type system does sound exciting - do you have any link to that?
Up to a certain point. &amp;#x200B; Haskell is nice because it has a strong, rich type system that lets you encode certain guarantees at the type-level to ensure correct-by-construction invariants, or do formal or semi-formal equational reasoning (as in Liquid Haskell). &amp;#x200B; Where you start falling short of proper dependently-typed languages is dealing with non-termination (you could be proving vacuous things in Haskell if you just pretend all programs terminate), and some properties are probably too hard to track using just type-level tricks (or really annoying to encode using singleton type to simulate dependent types). &amp;#x200B; On the other hand, tools like Liquid Haskell can make it much easier to prove certain properties than in a dependently-typed language, especially for non-expert users, because a lot of the proof work is delegated to SMT solvers, as opposed to the programmer. &amp;#x200B; I think where Haskell is "easier" to reason about than say Java is that, by default, people will have a programming style that is more prone to equational reasoning. You could write everything in IO, but that is considered bad style in Haskell world. Most of your logic should be pure, and the type system lets you enforce this purity which enables reasoning tools that are unsafe in the presence of side effects. You can get all of this in an imperative program, but you'd have to perform the same side-effect analysis first, which could likely involve code that is not pure, even though operationally equivalent to some pure code.
I was actually just working on this for a course I'm taking on software verification. I don't think you could keep a red-black tree balanced using the vanilla Haskell type system but, it's not too hard using refinement types. I used LiquidHaskell to verify the invariants of a red-black tree along side some other interesting data structures (leftist heap, binary tree etc.). If you're interested, you can check out the [repository for this project](https://github.com/jackastner/liquid-structures/). The red-black tree implementation is in [Set.hs](https://github.com/jackastner/liquid-structures/blob/501567138929b29d6a856cafa92cafb646a25de9/src/Set/Set.hs#L77) and it is also briefly explained in my [slide set](https://github.com/jackastner/liquid-structures/blob/master/slides/slides.pdf).
Perversely, probably the best language to use if you want to verify a body of code by using source-level reasoning is C. The fact that C code is unsafe, and everybody knows it’s unsafe, means that there’s an inordinate amount of research done on reasoning tools for C code bases. Haskell has no equivalent of CBMC, Frama-C, CPAChecker, and similar tools, that can be used to prove deep properties of C code. The best that can be done is either trying to use the type-system to enforce various invariants, or translating Haskell code into Isabelle/HOL, or Coq or similar. However even there the state-of-the-art in the Haskell world lags behind the state-of-the-art for C, with projects like seL4 and CompCert/VST providing frameworks for translating C code into theorem proper definitions and using them in anger in significant projects.
Thanks that helps me understand.
&gt; verifying red black trees are balanced with the type system does sound exciting https://doisinkidney.com/posts/2018-07-30-verified-avl.html (links to a talk about red-black trees) Most of the stuff you can do in Agda/Idris is *possible* with enough GHC extensions, even if it require more code. The "proofs" require more manual checks, and the code is actually slower, until you REWRITE all the proof terms to something non-computational.
I would also like to mention CakeML in this space. Lots of tooling around verification are part of the project, but can be reused for (other) CakeML programs.
If you're in the New York area, I hope you can make the first of our monthly Haskell co-hacking events. This is just people who are passionate or curious about Haskell getting together to meet and encourage and help each other out on any project.
You can't enforce red-black trees in the vanilla type-system, but GADTs can do it: [https://www.reddit.com/r/haskell/comments/ti5il/redblack\_trees\_in\_haskell\_using\_gadts\_existential/](https://www.reddit.com/r/haskell/comments/ti5il/redblack_trees_in_haskell_using_gadts_existential/)
&gt; every value of type T is really of type T|null In some ways it's worse due to laziness. To me, `null` feels like a single troublesome value, and bottom feels like a whole class of troublesome values. (Then, async exceptions on top of that.)
On [this line](https://github.com/fghibellini/nix-haskell-monorepo/blob/master/monorepo-nix-expressions/monorepo/nix/release.nix#L3), you should also have `overlays = []`, to increase reproducibility (otherwise the user's overlays may leak into evaluation).
I find writing unit tests far easier for haskell than c++ (that's what industrial verification is?). I also usually solve easier problems in haskell than c++ so there's that.
I make websites and web shops for a living and this is primarily done with PHP (what can I say, it pays my bills) but, I also use Haskell. Quite often I need to do a migration from some custom CMS to something like Joomla or Wordpress. Previously I was doing this just with PHP and it was slow and buggy process, but with Haskell I can make complex migrations relatively painless (MySQL support is kind of sucky in Haskell though). Also, quite often my clients ask me to import stock from their distributors into their web shop. Those distributors usually don't have an API so I have to write a scraper to scrape articles from their web catalogue and import them. Anyway, people quite often forget that if they are working with PHP (or some other language) they can also use other programming languages for non permanent thigs like import / migration scripts.
Partners asked me to do some data entry / conversion for a flat fee. I wrote a Haskell script using Parsec to parse the ad-hoc structure and managed to do the job in a fraction of the time that they estimated would take anyone. When working with Haskell, you don't *have* to expose the fact that you are using it to your customers. I find that most of my customers do not care about the technology (they just want the job done proper) or they care about the wrong aspects of technology. To the latter, if you don't jump through the proper hoops and tell them you are not using mainstream tools, they see it as a liability and might turn you down. You're better off giving vague answers when they ask about the technology you are using (say for instance "we use a variety of industry-proven tools to manipulate data in standard formats" rather than "we use c# .NET 4.0 and Python").
I use Haskell at my job ... does that count? Also use Rust, Elm, and Php/JS/html
interesting, do you have a link to some issue where this was discussed? I assume you learned this the hard way :D
Where can I get a code sample of mine reviewed by others? I have to write scripts for my research and I wrote one in Haskell for the learning experience. I'm sure I'm doing some things stupidly so I'd like to get advice from others. Would it be acceptable to post it here?
I run a web site as a side project that keeps track of web comics. I wrote the back end originally using Perl some 15 years ago but I replaced it with a Haskell implementation in February 2018. The site development was pretty much at a stand still for the three years I took to rewrite it all (on my free time, mind you) but I've caught up with the development pace with what it would have been if I had persisted with the old mess that I had. I'm earning some money, enough to be a net positive, via ads (optional to spare my users' nerves), Liberapay and Patreon. My most recent development with it was to make a mobile client app using Qt.
I think question is much more "How can I make a dollar programming on the side?"
I don't know enough about how you'd do variational inference in probabilistic programming, but I would guess the free monad approach will be tricky because the code under the bind (i.e., the `a -&gt; ProbProg b` part) is an opaque function - this works in MCMC approaches because you only need to sample from the result of that function when it's available at runtime. I'm not sure how the variational inference would deal with it!
We use Haskell pretty extensively, here’s an [interesting overview](https://code.fb.com/security/fighting-spam-with-haskell/) of one area where we use it and why it was chosen over other languages. I realize this doesn’t directly answer your “can I make a dollar” question, but maybe it addresses the motivation behind it.
About four years ago, a company I'd been doing code-related web content contract work for asked if I wanted some actual code projects. The first one was a reddit bot. I knew that one of the engineers had a strong interest in Haskell, and since I'd been inching my way through learning it for a good year prior, I decided why not! They've got someone who can maintain it, so it shouldn't be an issue. It was my first non-trivial Haskell project. A few months later I was hired full-time remote, and I've since been brought local to San Francisco. When I joined we were 100% node.js, but all our new projects are in Haskell and most of the old backend has been migrated/replaced. tl;dr: in my experience, to make a dollar with Haskell, publicly write some useful Haskell.
https://codereview.stackexchange.com/ and tag with Haskell is, I think the best solution.
I agree, otherwise 'Go and apply for an IOHK job' seems like an obvious answer.
&gt; PHP (what can I say, it pays my bills) Done that before. Never again. I'll go back to working at K&amp;G before I write another line of PHP. (I will write Hack to replace PHP, though.)
&gt; MySQL support is kind of sucky in Haskell though I'm considering that beam backend.
Industrial verification shouldn't be restricted to unit tests, but also make use of proofs, otherwise it's just testing.
I took over management of a companies ERP system which was written in PHP, they wanted an external form, I wrote that in haskell. I'll write all new features I can in haskell. I'll just slowely move on isolating and eliminating as much PHP as I can, but it was nice I could start out with a part haskell. Customers really don't care about languages though, what they do care about is the fact I can write stuff faster in haskell, because it'll be cheaper. If they ever ask I'll just say the tools I use are for getting out changes faster. Haven't had them asking though. I don't think they will.
There is a trick to represent balanced structures using polymorphic recursion (which is part of vanilla Haskell). For example, perfect binary trees (n nodes at height 2^(n)) can be represented as: data Tree a = Dup (Tree (a, a)) | Stop a The general technique is described in *Manufacturing Datatypes* by Ralf Hinze (1999, [PDF](https://pdfs.semanticscholar.org/8d6e/f8df15d8667620460fe09f24b9244a64f30c.pdf)), and its application to RBT is the starting point of *Red-black trees with types* by Stefan Kahrs (2001). Insertions/lookup/removals are logarithmic time, but there is an annoying constant factor which seems to require extra type-system features. data Red data Black data Node color a x = Node a x x deriving Functor -- Generalized balanced Tree data Sapling f x = Grow (Sapling f (f x)) | Freeze x deriving Functor data RedBlackNode a x = R (Node Red a (Node Black a x)) | B (Node Black a x) deriving Functor type RBS a x = Sapling (RedBlackNode a) x type RBT a = RBS a ()
We started a pure Haskell company, Bitnomial, and it’s been great for us. The performance and assurity are a perfect fit and the refactor-ability has been amazing at scale.
Here's a question for an answer: How do you make money developing open source full time? I don't have a great answer to that question myself, but I think we should all be trying harder to solve it.
Sure, just *in*formal proofs.
Thanks.
Do you hire? :D
I wrote a gadget for parsing and translating html files at my old work...
This is great advice. I would just warn that you should be 100% transparent if there are any plans to transfer the code base to another developer later on. I would hate to stick someone else with a code base they can't maintain.
Patreon? like [Joey Hess](https://www.patreon.com/joeyh)? https://liberapay.com/ ? https://snowdrift.coop/ ?
&gt;AutoT has an instance for MonadState (which I didn't include in my extract) I did notice that in the SO extract. I was wondering what it's for, but that approach makes sense. &gt;The state machine output is actually a set of things that have changed, and the main loop outside the AutoT then tracks this and figures out exactly what needs to be redrawn by doing set operations on its record of what has been drawn in the past. This sounds very similar to the Virtual DOM approach; you may be interested in the [`gi-gtk-declarative`](https://owickstrom.github.io/gi-gtk-declarative/) library, which implements Virtual DOM for GTK. (On the other hand, Virtual DOM involves diffing widgets, and you're diffing diagrams, so you probably won't need this.) &gt;By all means use this technique. Thank you! I do have a few more questions/comments. In no particular order: * I tried to implement your approach myself yesterday, and it seems to require some sort of multithreading: you have one thread which receives events and feeds them to the continuously-running state machine, and another which runs the GTK application and feeds events to the auxiliary thread. Is this correct, or is there some clever way to implement this without multithreading? * There seems to be [at least one](https://github.com/mstksg/auto) implementation of a state machine transformer on Hackage, and it looks pretty good; however, it's an Arrow rather than a Monad. I notice it was suggested on your SO question; is there any particular reason why you decided to make your own implementation rather than use this one?
no, it's just something I noticed from reading nixpkgs. `overlays` is read from `~/.config/nixpkgs/overlays` just like `config` is read from `~/.config/nixpkgs/config.nix`.
look at https://github.com/NixOS/nixpkgs/blob/master/pkgs/top-level/impure.nix
Helped a college student with a Haskell assignment once. He paid me $5. Customer was happy. Asked for a repeat later. Got paid another $5. I've earned 10 individual dollars writing Haskell. Was quite fun. Would repeat.
Maybe unpopular answer: get better at sales. If you maintain a large, popular open-source project, there are plausibly companies that stand to lose $X0000 (on the low end) as a result of major bugs or lost developer time. You could probably convince said companies to pay Y% recurring to prevent that from happening. You might even know which companies those are, based on who's contributing to your repo. Find them and reach out to them, make a case for a support contract. Send a template purchase order. Make sending you money as easy as possible *for a large, bureaucratic organization.* If you want them to send you $25 through Patreon, you will be laughed at. [Relevant thread.](https://twitter.com/SwiftOnSecurity/status/1067682759592869889)
While I'm super happy to see a Haskell role I can't help but be discouraged by the "significant Haskell experience" requirement. I feel in love with Haskell five years ago, however, I have never had the opportunity to use it professionally aside from the odd script or cron job.
Makes sense. I actually get a little cringy when i see a library with “h” “py” or “j” in the name, and those are for and by developers. So, focus on the “what”.
So im hearing migration is a thing. Perhaps i could work on developing some open source tooling for migration between some specific targets? Get useful tools out there, people use them, ill know how to use them, provide service?
Yeah. Thats what i was getting at. Even not just “how can i” but “how do people”. Im a bit mystified when it comes to commerce as a whole, let alone independent programming work. Didnt mean to make it so narrowly haskell focused, but that is the tool ill be using.
Im familiar with your companies product. Might be hard making in roads in that sector, but Id like to make a social networking site where the users were shareholders, not the commodity. So, watch out?
IOHK won't employ everyone. The real answer is "come to Scala" :^)
Publicly write some useful haskell. Id love to write some useful haskell. Publicly? Like, at a starbucks? JK. Is there something you have in mind?
It's because you're calling it with two lists that have different sizes :)
You don't. You take your FP skills and use them in a good, non-antiquated language that someone actually needs, i.e. Scala.
yep, this is the only answer i could think but the lists come from a txt from my professor and seems like it is right
Duckling is supposed to be used as a library. Not a service. The executable is an example application of the library, and I hate that it's in the library itself. The entire `snap` dependency unnecessarily inflates the already huge library. I wish the library was split up into `duckling-core` and a library for every locale, since almost no one needs more than one or a few locales (&lt;10% of the library) in their use case. Like /u/alexfmpe said, `Duckling.Api.parse` is the main function to use.
That’s great. Howd that start? Friends in college with a mix of finance / programming background? Im imagining HBOs “Silicon Valley “ Nice website.
visible or just verifiable? mimblewimble based blockchains hide amount, so how could you claim "all info is visible"? even in bitcoin you can commit to a script using p2sh but still not reveal it.
Thats really cool. “Qwantz” is my favorite.
I can concur the refactorability! We've pretty much refactored almost the entire code base like 2-3 times in 2-3 years, and it's been relatively painless (it'll never be perfect), but doing that in any other language... I would definitely not be comfortable doing. It's something that only starts paying off after a few years, but if you've made the decision to go with Haskell, and 2 years in you need to refactor 30-50% of a 150k+ loc codebase, resulting in 30k lines editted/removed/added, and it takes 2 people less then a month, that's a huge win.
Hit all the points. Great answer.
You know that the lists must be the same size, and you could even look at the text file to confirm that. However, the Haskell compiler has no idea where the lists are coming from and needs to prove to itself that this function will work on *any* pair of two lists, not just the ones you're planning on giving to it. &amp;#x200B; As a more specific answer, you're missing these cases: * The first list is empty and the second is non-empty (aka second list is longer) * The first list is non-empty and the second list is empty (aka first list is longer)
Thats eye opening. Also the way you wrote that reads vaguely threatening. I like it.
Compile time or run time? Assuming compile time, although you know the method will always be called with lists of the same size, there's nothing that's enforcing that. So the compiler thinks it's possible for one list to be empty and the other not.
Do you know any other company that hires professional developers without requiring significant experience in whatever language they are hiring for? Isn't that a normal and expected requirement?
For a migration just output SQL statements
getting it in run time
I cant tell if you’re joking or not, but how would you answer if i asked the same question in the scala context?
Rude. Non commerical Haskell is incredibly valuable and shouldn't be belittled by any means. Haskell is commonly a hobby and passion.
Time for debugging!! distancia :: [Double] -&gt; [Double] -&gt; Double distancia xs ys | length xs == length ys = distanciaPonto xs ys | otherwise = error $ "The arguments to distancia must have equal lengths, but the argument lengths are: " ++ show (length xs, length ys) distanciaPonto [] [] = 0 distanciaPonto (x:xs) (y:ys) = (x-y)*(x-y)+distancia xs ys HTH! Good luck.
most companies encourage candidates who have experience in other related technologies and willing to learn their language to apply. For Haskell, where the population of developers and jobs are less than for blub, i would expect this flexibility to be even greater
My company doesn't put language requirements. There's an expectation that any developer that can pass basic interview questions about data structures/algorithms/design can pick up a new language in ~6 weeks Haskell is a special case and I can see why you'd want to be up front (either to sell it as an opportunity or did weed out people that don't like fp) For what it's worth, my company has about 600k employees and 20k devs
Some of met at the Chicago Haskell group and others in our prop trading jobs. It’s fun :)
Totally agreed. The syntax, type system, laziness, etc all come together to make something really special for engineering systems that scale elegantly.
Simply put, the function only knows what to do if either both arguments are empty `[]` or they're both not. So if (for some reason) it would get `(x:xs) []` or `[] (y:ys)` as arguments, it'll break, error, and stop your program. There are a few ways to fix this: * add `distanciaPonto _ _ = error "this should never happen, but hey, somehow it always does at some point"`, for example, as the third line. (This does mean that if this function ever gets arguments that are not of equal length, your program will throw an exception) * make the type signature of the function `[Double] -&gt; [Double] -&gt; Maybe Double` and evaluate to `Just`s in the first two cases, and as a third case `distanciaPonto _ _ = Nothing` (You'd have to change the second case to something like `fmap (((x-y) * (x-y)) +) $ distanciaPonto xs ys`, though) * make the `distanciaPonto [] [] = 0` case into the default and put it below the second one, resulting in: ``` distanciaPonto :: [Double] -&gt; [Double] -&gt; Double distanciaPonto (x:xs) (y:ys) = (x-y)*(x-y)+distancia xs ys distanciaPonto _ _ = 0 ``` Any of these are valid solutions to have Haskell not give the `"Non-exhaustive patterns..."` warning, and which one you choose is up to you.
I bet. Good luck.
Here's a recent example from this subreddit: https://www.reddit.com/r/haskell/comments/bph91n/which_companies_employ_the_most_haskellers/entr095 Many people/companies value "intelligence/problem solving ability/ability to learn" over experience in any particular language. The view is that programming languages are the tools of the trade, but the core skill of "being able to write code" is more or less language agnostic.
thanks, there is a mistake in another function, still did not find it though
&gt;Do you know any other company that hires professional developers without requiring significant experience in whatever language they are hiring for? Isn't that a normal and expected requirement? Yes. I have applied to and been hired for roles utilizing languages where i had zero experience. The company/interviews were not language specific. A bigger name example of this is Jane Street. I'm not hating on the company or the ad. As I mentioned I'm happy to see Haskell jobs.
&gt;Many people/companies value "intelligence/problem solving ability/ability to learn" over experience in any particular language. This.
You may be interested in Semi-Formal Development using Haskell : https://iohk.io/blog/semi-formal-development-the-cardano-wallet/
Haha, I didn't intend for it to come off that way; I certainly don't condone intentionally writing bugs into your software to make money! The heart of the matter is that there are a lot of things that, for a company, create value other than just the software itself. There seems to be a common misconception among developers that the only thing that has value is the source code itself; I guess we would rather swear at compiler messages for a few hours rather than pay someone to answer our questions and be done in 30 minutes. But businesses are inherently risk-averse; e.g. Google *really* doesn't want a $1B product with millions of users to go down because of an unmaintained OSS C++ library. You can make a product out of *removing that risk,* and make actual value that someone would pay for without a single bit of technical work. For instance, 24/7 phone support is crazy valuable, and something that companies willingly shell out enterprise plans for. How many popular open source projects do you know of that offer phone support?! Put another way: open source has done a ton of societal good, but from the perspective of a market economy, it's actually an *inefficiency*; it prevents resources from being easily allocated to the most valuable capital. Just look at OpenSSL: it forms the backbone of the entire commercial internet, yet before Heartbleed came around, its funding rounded to zero and its last active maintainer was considering dropping it! Removing that inefficiency by making it easier for companies to pay for open source = money.
What is this company? No way you'd share in a PM?
My approach is similar, and I think I can use some of the ideas listed in here. Thanks for the slides!
I would like to propose [`data-reify`](http://hackage.haskell.org/package/data-reify) ([paper](http://www.ittc.ku.edu/~andygill/papers/reifyGraph.pdf)). It is also what [`ad`](http://hackage.haskell.org/package/ad) uses. I know people don't like that Haskell packages are documented in papers. This one very nicely illustrates the problem and explains the solution. With examples.
I do Haskell stuff on fridays at my job. IT consultant, monday through thursday is client work (in Scala), friday is internal stuff (currently a server monitoring thingy that will also be used on client's servers) in Haskell. The company I work for is a pretty small one, founded by people from the same uni I went to (UCPH). They're not Haskell coders, but did learn the basics of it in uni. They want to try getting some Haskell clients in the future - if that happens, I'll be able to code Haskell every day :D
Totally, we think we should focus to maintain [https://github.com/csabahruska/jhc-components](https://github.com/csabahruska/jhc-components).
&gt; Isn't that a normal and expected requirement? Everyone's jumping on your case like you've committed a *faux pas*, but no, there is absolutely nothing wrong with stating that basic competency in the language is a requirement, especially for a remote position.
That's interesting, I realize now I've seen something similar before. The RB tree is a bit too complicated for me, but I've seen finger-tree implementations that use that technique.
I personally wouldn't even consider them proofs. To me, and this isn't the dictionary definition or anything, but to me, an informal proof is one given in natural language or using some other kind of shared understanding that isn't logic (e.g. pictures, evidence). Testing generally doesn't display (even informal) reasoning about things. Rather it lists requirements in the form of programs that check those requirements, without any statement of how those requirements might be met.
That's a good one if you're thinking about ML like languages rather than Haskell specifically.
Haskell and antiquated? And Scala and good in one sentence? Oh boy.
Is there a video of or notes from the talk posted anywhere?
what kind of job is this? dream?
Loved the Yuru Camp assets at the end. Nice slides!
I had this talk in my mind when I wrote the above: [https://www.youtube.com/watch?v=n-b1PYbRUOY](https://www.youtube.com/watch?v=n-b1PYbRUOY) Scanning over it now it looks like both Agda and Haskell are demonstrated.
Would you consider remote, non-us based candidates?
I have my own business as a side project where I have a SaaS-thing that I have customers pay for. &amp;#x200B; It's not a lot of money, but customers are paying to use software that I wrote in Haskell. They don't know that it's Haskell, it's nothing I write about in my marketing. Customers are happy, sometimes they complain or want changes, as with any software. &amp;#x200B; Would I do it again? Well it's an ongoing thing, I'm trying to grow the business, slowly but steadily.
Nice set of slides!
Tried `git annex importfeed https://haskellweekly.news/podcast/feed.rss` but the web server seems over capacity: https://user.fm/files/v2-9466bdde6ba1f30d51e417712da15053/episode-1.mp3 451 A file of this size would cause "taylorfausak@fastmail.fm" to exceed the per-10 minute file bandwidth limit of 500.00 MB, try again later
It might be nice to release the actual audio file via torrent or some other p2p network. That way distribution can be handled independently without having to pay for expensive web hosting.
That's expected for a company of that size. The bigger the company, the more lax the focus on specific skills is (in favor of general competence). Large companies have the budget to take a generally skilled person and educate them in their stack. Small companies do not have this privilege most of the time.
Great info. I dislike this presentation format though. I always end up going left all the way, getting confused with the shear amount of information that's missing, and then going back through and navigating properly while "spoiled" on the "punchline". Honestly, a vertically scrolling wall of text (+ videos) is better for sharing on the web. I can certainly appreciate this format when it is going to be paired with you as a speaker.
There are 10 episodes so far! Didn’t realize this from the announcement
Sorry about that! I wasn't expecting my file host to hit its quota so quickly. I moved to a new file host, so you should be able to download the audio files now.
Excited for this! I only managed to download one episode before the bandwidth limit was hit, but look forward to listening to "Improving Haskell"! I prefer being able to download via CastBox on Android, but if they were uploaded somewhere else free like SoundCloud or YouTube i'd stream/download from there as a backup.
Yes! We've been doing this for a while. We've been going in stealth mode while we figured out if this is something we wanted to keep doing or not. Plus since the episodes are so short (about 15 minutes) it didn't feel right to announce with only one episode available :)
Please don't troll this sub. It's posts and comments are quite nice without this.
I completely misjudged my file host, but quickly moved to another one. The audio files should be available now. Try again!
It's like Scott-encoding the base functor and then currying the result. Personally, I'd rather pattern-match. I'm also not a fun of TH, so I'd probably avoid it for that reason, too. Thanks for sharing!
&gt; hires professional developers without requiring significant experience in whatever language they are hiring for? Even if it was normal (and I don't actually think it is), it can still be wrong: https://www.reddit.com/r/ProgrammerHumor/comments/4k994j/if_carpenters_were_hired_like_programmers/
Page doesn't show anything (not even an error) if third-party JS is blocked.
Took a while for me realize that there are additional slides "below" the main ones (just press the down arrow key to access them).
Congrats! Super excited that you guys were able to get this going! \^\_\^
We don't educate/train. (Maybe other larger companies do?) When I say we expect a dev to be able to pick up a language, I mean through self study and application. This goes for both college hires and industry hires
&gt; informal proof is one given in natural language or using some other kind of shared understanding That's what they are. They aren't using formal language alone. Instead they are using a shared understanding of how Haskell incompletely maps on to formal language paired with some verification of the Haskell code doesn't introduce significant divergence from the formal language. That's often combined with the evidence of absence implied by the absence of evidence when none of the QuickCheck / HSpec tests fail. There are places that use testing, and do not pair it with any separate reasoning, but I don't think that often gets called "(industrial) verification"
One of Facebook/Google/Apple/Microsoft/Amazon
Never herd of Scott-encoding before and it seems that it really is like what I am doing! Thanks for your reply!
I only found gi-gtk-declarative after I was committed to the current technique. No multithreading is required; in the IO world mouse events and GTK signals are received via the usual callback process. In my case I use Reactive Banana events and behaviors, but you can do the same thing in IORefs. Store the current continuation in an IORef. In the mouse event callback apply the continuation from the IORef to the mouse event data, giving you a tuple containing the output value and a new continuation. Stuff the continuation back into the IORef to await the next mouse event and do whatever is necessary with the output value. It so long since I did this I can't remember why I didn't go for the Auto Arrow package. Probably because I couldn't see how to do "yield".
Too many listeners is a good start. ;-)
Is there a normal RSS-feed to copy? I tried to search for it in Overcast but it hasn’t found it yet.
Yep, it's what we like to call a luxury problem.
This is still incurring the cost of training even if the company isn't actually doing it, in the sense that new hires will have lower/negative productivity for a longer period.
Ahh. Alright. Fair enough. Makes sense. Didn't know one of the FAAMG's was using Haskell. Kinda cool.
It's not Haskell or red-black trees, but you may find these tree implementations which verify the tree properties at compile time interesting: Braun Tree: https://github.com/cedille/ial/blob/master/braun-tree.agda BST: https://github.com/cedille/ial/blob/master/bst.agda And Verified Functional Programming in Agda (Aaron Stump) goes over the implementations.
I'm sure everyone in FAAMG is using Haskell to some extent. In general, individual services and products within the company choose the language right for them. One of the huge benefits of SOA and/or microservices etc
I don't disagree
[There is](https://youtube.com/watch?v=HSUvP9dzZPc), however, I'm not too fond of it due to my insecurities which is why I didn't link straight to it. I'm also not sure how much it adds on top of the slides.
🤝
That's fair, I probably won't use this format again.
Right. Or use 'space' to navigate to the next slide. Sorry about the confusion.
Thanks!
I'd be interested in reading more about whatever you work on :)
It's called [Minibrain](https://github.com/gizmo-mk0/minibrain), which is a game where you build neural networks - inspired by the game mechanic from [Bug Brain](https://www.youtube.com/watch?v=CUmpPwywYbk&amp;list=PLbaxG4ouFKxW1ks_qsDbubBCLUMa3vzqX) and the graphics of [MiniMetro](https://www.youtube.com/watch?v=3SeRgpQMRXE). (I haven't uploaded anything other than code, so no screencaps or anything as of yet.) I also have scenes: one for the menu, one for the description of the scenario (briefing), one for the editor (where you put together the network), and one for the simulation. Currently only the editor part is done. My approach for scenes is suboptimal, and I'm thinking about redesigning it to more resemble yours. For reference, this is how it is looking right now: data SceneData = SceneData { currentScene :: Scene , titleData :: TitleData , briefingData :: BriefingData , editorData :: EditorData , simulationData :: SimulationData } Admittedly ugly. I use SDL2, NanoVG for rendering, and reactive-banana for the UI stuff. The `Input/Editor.hs` contains the UI code for the editor. If you have suggestions, questions, or more than enough money, let me know.
I am not joking. My income increased by a factor of literally 300x since I switched from Haskell to Scala - Scala opens all kinds of doors for you - and jobs are available everywhere, while pursuing Haskell is a great way to win a Darwin award. If you learn Haskell, it's only as a small stepping stone in a software career, as a good way to learn FP, but nothing more. The same question simply doesn't make any sense in Scala context, you just go to your local job board and make money, you don't even have to know Scala - our company is currently coaching juniors with no background - demand is just that crazy high.
I think both approaches can be useful. I like the existential types approach because I found it to be pretty flexible. Good luck with your game!
Building off what /u/bss3 said, are you familiar with http://hackage.haskell.org/package/generics-sop ? Seems like even GHC.Generics could underly your approach here, except it uses balanced trees of */+ instead of the linear chains you're generating. HTH.
Probably I'll also go with that approach. Thanks!
What would `1/2` be? That instance would not be total.
No, I am not familiar with generics-sop and it really hit the spot of what I was trying to do some days ago until I came up with this solution! Thanks a lot for mentioning it! &amp;#x200B; Are you familiar enough with it to tell me if it really could underlie my approach? It really is what I'm looking for but I think I dealt with the problem in a much simpler and less bloaty way (bloaty as in free of type constructor pattern match/indirection layer hell).
Yes. We have experience working with non us based personnel.
The link to your ported implementation of the rng seems broken.
F famously used Haskell for spam filtering or something of that sort
The link to Chris Wellon's article is broken.
provide some java examples, and I'll translate them into haskell
Maybe try to write the equivalent code in Java: https://gist.github.com/soupi/c7c94a45d006bc70f3b896f327ea47a3
Any update on the benchmarks on your branch?
Doh! Thanks for the heads up; fixed.
Also fixed, thanks!
Any chance of official transcripts? I tend to zone out speech if I'm doing anything else. Makes podcasts and audiobooks very frustrating.
I'm happy you asked! I really want transcripts. I tried doing them by hand, but it was far too time consuming. I gave up part way through a single episode. I'd be interested in an automated solution, even if it required some manual work to tidy things up.
Java developers absolutely do think about many of the use cases for algebraic data types. The tool of choice for solving this problem in Java is a class hierarchy. This Haskell: data Foo = Foo1 Int | Foo2 Double | Foo3 String op1 :: Foo -&gt; IO () op1 (Foo1 i) = ... op1 (Foo2 d) = ... op1 (Foo3 s) = ... op2 :: Foo -&gt; IO () op2 (Foo1 i) = ... op2 (Foo2 d) = ... op2 (Foo3 s) = ... op3 :: Foo -&gt; IO () op3 (Foo1 i) = ... op3 (Foo2 d) = ... op3 (Foo3 s) = ... is replaced by this code in Java: public interface Foo { void op1(); void op2(); void op3(); } public class Foo1 implements Foo { private int i; public Foo1(int i) { this.i = i; } public void op1() { ... } public void op2() { ... } public void op3() { ... } } public class Foo2 implements Foo { private double d; public Foo2(double d) { this.d = d; } public void op1() { ... } public void op2() { ... } public void op3() { ... } } public class Foo3 implements Foo { private String s; public Foo3(String s) { this.s = s; } public void op1() { ... } public void op2() { ... } public void op3() { ... } } There are some differences in modularity. In Haskell, the data type is closed (you can't add more constructors without modifying the original definition), and the operations are open (you can add as many operations as you like in new code). In Java, the data type is open (you can add new subclasses in new code), but the operations are closed (you cannot add new operations on Foo without modifying the interface). These are merely modularity concerns, though; the same code can be written either way, and Java programmers learn to live with the way things are even if they wish for the opposite... or they apply design patterns on top of the base language. There are patterns in Haskell for doing things the other way, as well. There are even patterns for leaving both open (look up "the expression problem" to start reading...)
That always happens me too. A little reminder on the first slide would have kept me out of trouble. (Aside from that I like the format a lot.)
That always happens me too. A little reminder on the first slide would have kept me out of trouble. (Aside from that I like the format a lot.)
I don't have any useful leads on auto-transcription other than knowing it exists, but glad it's on your radar!
You can close the class hierarchy by making the interface an enum with abstract method, and have the subclasses be nested classes. You can have one of your ops() be the Scott-encoding of the data, too. The operations are semi-open even in Java, they just have to be implemented in terms of the existing operations. Sort of like how `join` works for any `Monad` even though it's not a member of that typeclasses since it can be defined as `(&gt;&gt;= id)`.
Thanks, that clears up a lot of problems for me!
Thanks for posting this; I look forward to seeing the replies from folks with more Haskell experience than I have.
You can definitely get the same result with generics. It's even possible to derive the base functor of recursive types with generics (i.e., replace `makeBaseFunctor` in recursion-schemes) ([generic-recursion-schemes](https://github.com/Lysxia/generic-recursion-schemes)), but I wasn't satisfied with the ergonomics so I didn't bother releasing it. This is also similar to https://hackage.haskell.org/package/catamorphism which gives a Boehm-Berarducci encoding instead of Scott. (some related comments here: https://www.reddit.com/r/haskell/comments/54h33m/scott_encoding/) Now I'm so tempted to implement these encodings with generics...
Nice work illustrating the point with a small example! &gt; It used a single Word64 (instead of a larger structure like a list) to represent the board, so it should have had needed to allocate a lot. A "not" is missing somewhere?
By "useful" I mean an application that interacts with the real world. Demonstrate that you not only understand language constructs in isolation, but can combine them to do something notable. By "public" I mean something that *other people* can interact with with trivial effort (or even stumble across accidentally). This is entirely about self-marketing: something you can show off in less than a minute and have it be at least mildly interesting _before_ anyone knows it's written in Haskell. When the reddit bot project was offered to me I hadn't thought through all of that, but in hindsight it was perfect. The website is slant.co, which does contextualized product recommendations using a free-form question system. So, the bot listened for queries in user comments and fetched data from our web API based on the query: easy for anyone to play with, plenty of content to explore, and does something useful beyond evoking a chuckle. On the technical side, I needed to tackle: - A timed main loop (nearly trivial, but still harder than a one-shot console command or a web framework which handles listening for you) - Consumption of the reddit API, via the existing [`reddit`](http://hackage.haskell.org/package/reddit) package - Consumption of the slant.co API, via raw HTTP requests, interleaved with the `reddit` monad - Plugging the slant.co JSON response into a template that produced Markdown - Accessing a persistent data store (I used postgres) to track which subs the bot was allowed in, and keep a "last seen" timestamp (so it didn't re-reply to all of its old invocations on restart) - Deployment to a hosting service (Heroku in this case) which absolutely deserves a bullet point None of these are large or difficult problems, but they're all _different_ and needed to be wired together into a working whole. (They were also nearly all 100% new to me, which made this a pretty huge personal milestone.) Being able to do this was a solid demonstration that I have general programming skill, not just rote tutorial knowledge. (And, as I have learned to my horror, that is an _extremely_ important thing to demonstrate.) As a stand-alone project with observable behavior, it was enough to get me additional projects without anyone else even looking at the code. (That happened later, when hires were being made; good functional style and separation of concerns is what earned me full-time.)
This is super cool! Feel free to reach out to us if you need anything (contact@repl.it)
Thanks for this! I've looked into your generic-recursion-schemes and it's a nice work on Generic and if I had known the existence of SOP I would probably have tried a similar solution! The catamorphisms package it's exactly what I wanted to achieve but since recursion-schemes is a lot more complete I tried to do just the one simple thing that bothered the most to me while using it. It's actually pretty great that I made this and shared with you guys because this way I got to learn a lot of new things that I couldn't find when searching to resolve my personal problem!
All that after a year of inching toward haskell? That’s quite a feat. Thanks for the post. General advice and specifics. Its really helpful.
Or just use a strictly evaluated language
This is why I do audiobooks / podcasts only when commuting and exercising. I switch over to repetitive music once I start working (or other programming).
Is there not an open-source text-to-speech that would give you something to throw up on a wiki and let volunteers refine?
I don't see how this is relevant given that the random number generator was the main culprit.
Yep. The change in Attempt #5 was to add strictness annotations; all attempts before that were lazy. - Adding strictness annotations in Attempt #5 saved 30ms (from 101ms to 70ms). - Replacing the PRNG in Attempt #3 saved 612ms (from 738ms to 126ms).
&gt; so it should **not** have had needed to allocate a lot. Yep, thanks!
Haven't gotten around to it, but then again I'm happy to wait until 8.10.1 lands!
Thank you very much for sharing your results! I think your implementation of a random generator deserves a separate package! I'm working on the [treap](https://github.com/chshersh/treap) Haskell package where I really need a fast pure random generator. Currently, I'm using the [mersenne-random-pure64](http://hackage.haskell.org/package/mersenne-random-pure64) package (because it's the only Haskell package with pure random generator that I found) but I wonder how fast your PRNG could be in comparison.
I am going through the upenn course spring 2013 seems nice so far https://www.cis.upenn.edu/~cis194/spring13/lectures.html
Read the first chapter \`The Algebra Behind Types\` of \`Thinking With Types\` ([https://leanpub.com/thinking-with-types](https://leanpub.com/thinking-with-types)) (This chapter is available in sample).
Yeah I thought about it! I wasn't really willing to take on the burden of maintaining a package. The Haskell code is MIT Licensed, and the original C code is in the public domain, so anyone reading this is free to copy / paste the code into their project or use it in a new package. One thing that jumped out to me was this quote from Chris Wellons' article: &gt; My preference is to BYOPRNG: Bring Your Own Pseudo-random Number Generator. You get reliable, identical output everywhere. Also, in the case of C and C++ — and if you do it right — by embedding the PRNG in your project, it will get inlined and unrolled, making it far more efficient than a [slow call into a dynamic library](https://nullprogram.com/blog/2016/10/27/). Not sure how much that applies in Haskell; dynamic linking isn't very common in Haskell (is it possible? not sure). I thought that I once read GHC is able to inline across module boundaries, maybe even across package boundaries. If someone wanted to chime in and confirm that I'd love to learn more about when and how GHC is able to optimize this sort of thing.
Two points speak to me, mostly about syntactic weight: 1. Defining types in a Java program often means creating at least two new files (the class and its tests, and if you're defining multiple alternatives you have to repeat that dance many times), and then a lot of thinking about which operations you might want to move across from other classes (where a previous programmer has ?reasonably decided that "adding a new class would take too long, I'll just add this method here..."). 2. Algebraic types give you a good way to exhaustively check that you've got every option. This follows from /u/cdsmith's observation that an ADT is closed. If you know that "an X is either Foo, Bar, or Baz, each constructed differently", then after you set up your four new classes` (`X`, `Foo`, `Bar`, `Baz`), how do you dispatch on whether an `X` is `Foo`/`Bar`/`Baz`? Either: * Each decision of this type becomes an instance method on `X` and overridden in each subclass, or * You have chains of `instanceof` tests which are brittle once you start adding/removing subclasses of `X`. Neither feel like acceptable options to me.
I've seen a guy called N-O-D-E distribute his stuff using dat, but I don't know if the network itself is good: https://n-o-d-e.net/dat.html
To answer your question about GADTs, the first thing you'd do is change syntax. With the `GADTSyntax` GHC language extension, you can rewrite this: data Foo = Foo1 Int | Foo2 Double | Foo3 String as this: data Foo where Foo1 :: Int -&gt; Foo Foo2 :: Double -&gt; Foo Foo3 :: String -&gt; Foo This is still just a normal algebraic data type, but it's defined using GADT syntax, where instead of listing the types of fields, you declare the types of the constructors. I prefer this to Haskell's standard algebraic data type syntax, even when I'm not using any new features of GADTs at all! The value-level names and the types are nicely related by `::`, and the constructors are declared with type signatures that make sense. As long as your type is monomorphic (that is, it has no type variables) GADT syntax doesn't really buy you anything new. But let's add a type variable to `Foo` data Foo a where Foo1 :: a -&gt; Int -&gt; Foo a Foo2 :: a -&gt; Double -&gt; Foo a Foo3 :: a -&gt; String -&gt; Foo a Still just a regular algebraic data type, but this time with a type variable. But now we're ready to cross over into proper GADTs. What GADTs allow you to do is add constraints to the type of a constructor. When you construct a value of the GADT, in addition to the positional fields, the value carries evidence of the constraint. When you pattern match on that constructor, the solution to that constraint is available to satisfy any other constraints. Example: Suppose, in `Foo` above, you only want `Foo2` to be usable when the type variable `a` is a real number: data Foo a where Foo1 :: a -&gt; Int -&gt; Foo a Foo2 :: Real a =&gt; a -&gt; Double -&gt; Foo a Foo3 :: a -&gt; String -&gt; Foo a Now you can construct a `Foo a` using `Foo1` or `Foo3` far *any* type `a`... but if you use `Foo2`, then `a` must be an instance of `Real`. It can still be any of plenty of different types -- `Int`, `Double`, etc. -- but it must have that instance. Then you can write this: multiplyFoo :: Foo a -&gt; Rational multiplyFoo (Foo2 x y) = toRational x * toRational y multiplyFoo _ = -1 Notice you don't need a `Real` instance in the type of the function. That's because if you pattern match on `Foo2`, you *know* there's a `Real` instance, because constructing with `Foo2` couldn't have type-checked without it. Type classes are just one kind of constraint you can add to a constructor. The other is a type equality constraint: Foo2 :: (a ~ Double) =&gt; a -&gt; Double -&gt; Foo a which can be written in this shorthand: Foo2 :: a -&gt; Double -&gt; Foo Double In this case, the constructor `Foo2` can only be used to construct a `Foo Double`, and not any other type.
Depends on what you are verifying. You'd have to have a pretty strange dialect of Haskell to make worst-case execution time at all possible to calculate. Meanwhile languages with mutation make it possible to preallocate everything and avoid garbage collection so worst-case execution time is at all possible. It should be possible to formalize an always terminating subset of Haskell (the simply typed lambda calculus for example) but figuring out the exact feature set would be difficult and it'd probably be easier to just use it as a backend from something like Coq. I doubt Haskell would make it much easier to formalize concurrent algorithms other than making it easier to require to pass state between threads using pure message passing, something like `send :: MySerializableClass a =&gt; Process -&gt; a -&gt; MyConcurrencyMonad ()`. But you couldn't do something like proving your queue to be lock-free (or that your lock-free algorithm doesn't smash the stack.) I feel there are lots of important side effectfull mutable state properties that Haskell is obviously unequipped to verify.
As I see it, it is due to the 1. Extremely compact and terse syntax. 2. The availability of pattern matching. 3. The type checker that can call you out on missing case branches. Compact and terse syntax lets you to quickly build data types and model possible states and review them quickly. Seriously, just look at the sample code from /u/cdsmith. I am having a hard time figuring out the types by looking at the Java code. Here Haskell greatly helps here to get the data types right. And you might already be aware that if you get your types right, which is considered to be the most important part of writing a program, implementing the logic becomes mostly a mechanical task. The availability of pattern matching with exhaustiveness check means that you will get quick feed back on whether or not your data types are defined sensibility. If they are not, you will often find difficulty in matching on the type exhaustively. And then the type checker and terse syntax of Haskell lets you to safely and quickly remodel the type. Your program might be a thousand lines long, but your types can be much shorter, say a 100 lines. And in Haskell it becomes possible to somewhat guarantee the soundness (I am not talking about correctness, as in being well typed, but just the soundness of your design) of a thousand lines of program by using a mere 100 lines of types. And I think this is a very powerful capability in the context of real world software development. (By the way, I don't think Haskell was explicitly designed with these things in mind. But instead it was just made so that people can do "research stuff" easily. It might be a happy coincidence that we are finding quite a lot more useful for real world software dev).. On a tangential note, you can see that just like evaluating any other programming language feature, ADT's are best evaluated by looking how it works with other features of Haskell, and how it might be unwise to cherry pick language features from one language and implement it in a completely different one. &gt;Generic Algebraic Data Type examples, but I still struggling to understand what is GADT. ​GADT's allow you to explicitly specify the concrete type your constructors return. For eg, if there is a data type defined as data MyType a = Con1 a| Con2 a| Con3 a Try and see if you can change this so that the `Con1 Int` can be made to return anything other then `MyType Int`. GADT's allow you to do things like that. For eg data MyType a where Con1 :: Int -&gt; MyType String Con2 :: Int -&gt; MyType Float Con3 :: Int -&gt; MyType Char I am not sure if this is how GADT's should be explained, but this was how I figured it.
I am sure there are services which provide transcripts. I heard on some podcast they outsource whole production pipeline and that’s including transcript. Can’t really provide any links though.
Thanks. I read through the beginning of that paper and if I understand correctly, it offers a solution to the problem of representing loops within a computational graph? For the purposes of a PPL the computational graph will generally be a DAG so the problem of loops may not arise.
Man, congrats on doing such a great job. The overall quality is such top notch its pleasure to listen. Many niche / starting podcasts struggle so much with bad audio it’s hard to actually listen to them. I so happy you have no such issues. 🙂
This is exactly the issue I've run headlong into. And it's not possible to put constraints on the arguments to &gt;&gt;= because the signature would not match that expected by the Monad typeclass. I may end up trying to do something with \`-XRebindableSyntax\`.
I think it's typical to use the Type itself to represent all possible \`elements\`. &amp;#x200B; With an explicit \`Set\` of elements, it's always possible to just call \`o\` on values not in that \`elements\` set and the compiler won't complain. This could of course violate the group laws as it wouldn't have been tested on these inputs in \`formGroup\`. &amp;#x200B; You could classes like \`Bounded\` + \`Enum\` or similar instead for \`formGroup\` specifically.
I'm not really familiar with the needs of a probablistic programming language, but you mentioned backprop. I had some fun a while back doing reverse mode AD using a lens-style point free dsl. The backpropagation computation graph was basically held in returned closures. I think it is not crazy to represent dags using a category/arrow-ish/pointfree dsl, it's something I think about a lot these days, at the expense perhaps of an unpleasant point-free programming style. Just some thoughts. http://www.philipzucker.com/reverse-mode-differentiation-is-kind-of-like-a-lens-ii/
Facebook does so officially. MS have at least active research on programming languages related to Haskell. Google is murmured to have some smallish projects in Haskell. Apple? Nope. Amazon? Who knows?
I recently wrote a library with a very simple seeded Xorshift-based RNG, which has the same implementation (and therefore reproducible results) for many different languages. It's not published on Hackage or anything, because it is so simple. But exactly because of its simplicity it might be performant and suit your needs. [SimpleRNG](https://github.com/Qqwy/SimpleRNG)
It adds a lot. Not new content so much, but more interest and impact for those who have the time. Incidentally I like very much the video format, where both you and the current slide are always clearly visible. IMHO you should also link that twitch video of you playing it. In fact, here it is: [18 minute Nyx playthrough](https://www.twitch.tv/videos/423291178). If this is going to expire, consider saving it on youtube. Between the game, slides, and videos I think you just up-levelled Haskell game development. Great job.
Congrats! BTW the Google Podcast link doesn't seem to work for me, even tho I have the app installed. I think you used an old one for play music, [this one works instead](https://www.google.com/podcasts?feed=aHR0cHM6Ly9oYXNrZWxsd2Vla2x5Lm5ld3MvcG9kY2FzdC9mZWVkLnJzcw)
Oh yeah, of course. If the customer is getting source code, then the choice of language is part of the contract. &gt;Of course I'll give you the source code, here it is in Befunge-98.
Thank you for this write-up. I had heard of GADTs but had absolutely no idea how they worked. After reading your explanation I feel I have at the very least a basic understanding.
A long time ago I built the web-site for my company with Haskell and Yesod, but later switched to Python and Django. [Our products](https://www.soundradix.com) are implemented in C++ but have a little bit of code generation implemented in Haskell as well as "magic constants" calculations done using Haskell with [ad](https://github.com/ekmett/ad) while some other magic constants calculations are done with Python and scikit-learn.
You guys are on Spotify as well, awesome! Subscribed
&gt; Admittedly ugly. I don't see there anything particularly wrong. Abstractions should be only used if they give you some concrete value and not because of some perceived aesthetic reasons. I have been guilty doing this myself quite often, and in most cases it didn't pay off, but was just a hassle. It can be quite tempting to create the perfect abstractions, especially in a language like Haskell. :)
Thanks a lot! Also thank you for reminding me about the gameplay video, I will add a link to it to the game site. Afaik since I made this video a 'highlight' it will not expire.
Thanks to /u/simonmic for reminding me, you can watch me die repeatedly in this [full game playthrough](https://www.twitch.tv/videos/423291178)!
I agree... partly. For this game, this could work without a problem. However, the scene logic and game logic is tightly coupled. What I like about OP's solution is that he decoupled the two. So if I decide one day to re-use parts of the code for another game, I'd have a much easier time with OP's solution than mine. ^(()[^(This blog post)](https://www.gamedev.net/articles/programming/general-and-gameplay-programming/haskell-game-object-design-or-how-functions-can-get-you-apples-r3204/) ^(revolves around a similar idea.)) But you're right. There is no gain for this game from rewriting this. But there is a long-term gain for me: I'm doing it to familiarize myself with this kind of code structuring through practice.
I made several dollars with Haskell, so here we go. Specifically, I co-run a functional programming consultancy: [https://monadfix.io/](https://monadfix.io/). It started out as a single `index.html` page saying "we'll answer your Haskell questions for money". Several people were like "okay, sure". Note: this doesn't require much of entrepreneurial shrewdness. You don't need to write sales emails, you don't need to draft (or read) contracts, you don't need to register a business. Heck, you can even accept money via Ethereum or something if Paypal doesn't like your $country. People can be surprisingly willing to go through the hoops to pay you (after you've done the work and it turns out there are no *easy* ways to transfer the money). You also won't make much money this way, but you asked about a dollar, and you can definitely make a dollar this way. Now, how do you make more money? (Apart from doing sales from morning till evening.) * You can find people who want to help you with consulting. Promise them you'll handle all the business stuff. Nobody wants to handle business stuff. * You can learn how to do less pleasant work. Can you set up CI on Windows for a Haskell project? Can you give a workshop about parsing XML with Haskell? Can you interface Haskell with Java? None of those are impossible even for an average developer – let alone a smart one with some free time – but they require touching Windows, XML and Java respectively, so nobody wants to do them. * You can make it easier for people to purchase your services. Registering a business can be done for like $100 [in Estonia](https://www.leapin.eu/) with all taxes and paperwork handled for you. Not every company can easily do business with a random person on the internet (even if some people inside the company already know you and trust your expertise), so make it easier for them. Put your business details in the footer so that they can sleep safe knowing that if worst comes to worst, they can sue you. ([They won't.](https://archive.vn/QHkz8)) By the way, some of the practices in this latter category can look like "fake professionalism" or "shady marketing shenanigans" or whatever, but it turns out that many of them are literally just caring about a better experience for your customers. Like, if everyone else has colorful "contact us" buttons plastered all over the page, yeah, it looks tacky (FPComplete is especially guilty of this), but the fun thing is that people *are* genuinely easily-distractable and bad at finding information on web pages, so you should help them. I have personally observed an otherwise smart person failing to find our email on the previous version of our landing page, even though it was right in the middle of the screen in big font inside a pretty pop-out rectangle. Or if you're saying "I can write property tests and set up a file-watching script" and the competitors say "We have your back with millions of test cases generated on-the-fly and automatic analysis at every save", well, you get Brutal Honesty points, but now you've made it harder for *developers* to convince other people in the company (like people with Actual Purchasing Power) to order your services. ~~Why do you hate your fellow developers? Tsk, tsk.~~ This comment is getting long, but feel free to PM me if you want to chat about any of this. I can give better advice if I know what you like doing and what kind of skills you have already.
&gt; So if I decide one day to re-use parts of the code for another game, I'd have a much easier time with OP's solution than mine. There has been a lot of code written with this premise, and unfortunately in a lot of cases it didn't work out. Writing good reusable abstractions is very hard. IMHO even harder in the game domain, because if you don't write a game of the same genre again, your needs might be quite different and the abstractions can make it harder to fit it into. Instead of writing too much high level abstractions, which have doubtful reusability, it might make work better to write low level modular and decoupled systems (rendering, collision detection, ...) which might be easier to reuse in a new game.
My last employer did that. They said they were more interested in fast learners than an exact technology match, which was great because I wasn't an exact technology match. I was employed there for multiple years and left on good terms.
Making a dollar would be a side effect, so it's forbidden in haskell
It helps that we work at [ITPro.TV](https://ITPro.TV), where we record training content for IT professionals. So we've got the audio thing down pat lol
Thanks for the heads up! I updated both the announcement post and the podcast page to use the Google Podcast link rather than the Google Play Music link. For some reason Google's podcast management interface still suggests the GPM link.
Awesome! A question though - I'm just starting learning haskell. Is this noob friendly? Or is it for people who already know some haskell?
Thanks a lot for the advice and the offer for more advice. I think right now a single index.html page would be about the right speed for me, and its good to know that I can do the work and people will want to pay me afterwards. Many would say I'm undervaluing myself, but part of the issue for me is my thinking I'm probably going to screw it up and shouldn't expect money. Its good to know that there are low stakes paths in which I can build trust in others, confidence in myself, and a better approximation of the value of my work. (all the while getting good experience of course) As for the bullet points, I could see those as bases for a strong full time business. I'm going to have to file that away in the long term strategy part of my brain.
Nobody who can help me with some hints or some pseudo code
Great question! The content is very friendly to beginners as well as intermediates. The hosts on the show vary in experience so the content is given from a few different perspectives.
You just need to capture the effect in a monad. startBusiness :: (Interaction m, Profitable m) =&gt; m () startBusiness = do ps &lt;- mapMaybe developIdeaIntoPlan &lt;$&gt; interactForBusinessIdeas forM ps $ \p -&gt; if planShowsPotential p then implementBusinessPlan p else startBusiness
Yeah, I know you wouldn't condone that. No one would condone that. I'm just saying sometimes bad bugs happen to good people. Its always a shame. Sometimes no one notices. Sometimes they shut your site down during the first critical hours of your ipo. Ya never know what could happen....
Yeah, I actually know the service quite well since it is heavily advertised in Leo Laporte Podcasts. Never had the idea that you use Haskell though. Some people doing podcasts assume they don't need better gear or quality until they get traction, but they never will, if the quality is so poor. &amp;#x200B; Anyway I went though most of the episodes and they are really nice. I am already testing the flow with ghcid alrady. I never used Haskell in production so I am learning so much. I would also love to hear more about your infrastructure - what other components/services do you use and how to you interact with them using Haskell. Great job!
GADTs were also brought up in the hask anything thread this month. : I responded with a write up https://www.reddit.com/r/haskell/comments/bj5s5u/comment/en6fj0u
Can I specify the amount of threads `stack build` should use?
This should be its own blog tutorial somewhere.
Google also put out that MLP training document a few months ago on GitHub
Re the classic example at the very bottom, the one with empty and nonempty lists: couldn't this also be solved with dependent types? Is there a relationship between GADTs and dependent types, or do they just intersect in some cases?
The poor r/JavaScript poster
Nice. I use BeyondPod, which I guess is hooked up to the google podcasts listings. I just searched "Haskell" and it came right up.
Haskell's certainly easier to informally verify.
If I remember my company history correctly, our co-founders used to nerd out on Leo Laporte and they owe our existence to him to some degree. So now there's a good relationship there :)
DataKinds paired with GADTs are a *very* limited form of dependent types. In dependent types, all values can be used in defining types. DataKinds creates types for values (and kinds for types) that can be used in defining GADTs.
I use DoggCatcher, and I was able to find it the same way.
Nice slides. I am assuming as the game gets more complex (and 3D) Haskell's problems become more apparent. I still think if someone made a very performant (whatever that means) multi-platform (Desktop and Mobile) game or graphics, library in Haskell (or even something similar to Haskell) it would be a dream come true. I don't know if there is something like this already so I may be wrong.
Can you tell me what is different in the 2nd edition of the book?
Time ago I found a tutorial that started with this very advanced monadic program which uses to be presented in the chapter 100 of the standard Haskell books/tutorials: &amp;#x200B; main= do name &lt;- getLine print ("hello " ++ name) &amp;#x200B; Amazíng !!!! &amp;#x200B; I think that this is the right way for a practical haskell tutorial.
I can't find the RSS feed
Same applies to `1.0/0.0`. None of `Fractional` instances are total. Yes, in this ring division by even numbers throws `DivideByZero`. This is still way more lawful than an `Integral` instance of `Word`, for which `rem (x * y) y /= 0` in 99.99% of cases.
FWIW we managed to figure out a solution. [The gist](https://gist.github.com/dyokomizo/808cf8f93a62b481ef2ab707b9abc267#file-fixed-hs) was updated with an example that works. We were using type level computation (TLC) to keep track of interesting properties of our data and we were using indices in the data as phantom types, but this ended being a problem, because we did the TLCs directly on the data definition (in our case both GADTs and data families had this problem) which made type class inference fail (e.g. due to injectivity like /u/nifr said). *** Our solution was to move the TLCs away from the data definitions and use an "evidence aware" wrapper. Now all the TLCs happen at smart constructors juggle the wrapper and underlying data. The original code was like this: data Term :: [*] -&gt; * -&gt; * where Is :: a -&gt; Term '[a] (IsT a) (:&amp;:) :: Term t a -&gt; Term u b -&gt; Term (Union t u) (AndT a b) The first index of `Term` was used to keep track of all types used in the `Term`. Trivially a `Term` that is some specific data value only uses it's type (i.e. `'[a]`). An "and" had to compute the set union of all the types tracked by each of the component terms. These computations ended up being a pain in type class definitions. The fixed code looks like this: newtype E (t :: \[\*\]) a = E a `E` carries the value but also has a phantom type for the TLC result. data Term :: * -&gt; * where Is :: a -&gt; Term (IsT a) (:&amp;:) :: Term a -&gt; Term b -&gt; Term (AndT a b) The data definition is simplified and has reference to the TLC. is :: a -&gt; E '[a] (Term (IsT a)) is a = E (Is a) (.&amp;.) :: E t (Term a) -&gt; E u (Term b) -&gt; E (Union t u) (Term (AndT a b)) infixr 7 .&amp;. (E a) .&amp;. (E b) = E (a :&amp;: b) The smart constructors do the TLCs now, and use the wrapper to keep track of the types used by the terms. As neither the underlying data nor the wrapper use the TLCs anywhere in their definitions, we don't have problems with type class definitions. *** This is an interesting pattern that I wasn't aware of. A quick search on Google found nothing on it, but it seems to be basic enough that somebody must thought about it before.
Bit tangential to the key point but ... is there a reason you had to implement `containsName` and `replace` by yourself? It seems odd that there wouldn't be similar/more general versions of these functions already somewhere. I guess the hard part is finding them out.
When you are dealing with fields and division rings 0 is the one acceptable exception to the invertibility requirement. Is `rem (x * y) y == 0` a law of Integral?
I think these posts are really, really valuable to make GHC and GHC Plugins more approachable!
Might be --- finding things in GHC is hard though. Seems easier to just write a few lines of `syb`!
oh yeah this is perfect. Thank you!
[removed]
Your efforts are much appreciated!
It might be worth first writing out what your internal computation graph needs to look like (i.e., what data you need to have in order to do variational inference), and only then figuring out how to create a nice DSL syntax to create that graph. Unfortunately I can't really advise on that, because I don't know how variational inference works :-)
Is there a way to show all hlint recommendations found in my code in ghcid?
[removed]
Recent [Prelude](http://hackage.haskell.org/package/base-4.12.0.0/docs/Prelude.html#t:Fractional) says that `Fractional` is "customarily expected to define a division ring". You are right that proposed `Fractional` instance for `Word` does not define a division ring, because `(/)` is defined only for odd divisors. Further, [Prelude](http://hackage.haskell.org/package/base-4.12.0.0/docs/Prelude.html#t:Integral) says that "`Integral` instances are customarily expected to define a Euclidean domain". Euclidean domain implies integral domain, which must satisfy the cancellation **law** `a * b == a * c ==&gt; b == c`. Which - surprise, surprise! - is true only for odd `a`. **Conclusion:** the existing `Integral` instance of `Word` is at least as unlawful (and for the same reason) as the proposed `Fractional` one. I mean, we are all very accustomed to overflows and usually underplay their significance. In general I would strongly prefer an instance, throwing errors in my face, than an instance, silently violating simplest algebraical identities.
Does this mean that dependent types are strictly more powerful than GADTs, or do they just have different use cases?
Can't you just add a phase control to the INLINE pragma? Or use [GHC.Magic.inline](https://hackage.haskell.org/package/ghc-prim-0.5.3/docs/GHC-Magic.html#v:inline)?
Use the `-j` flag: `stack build -j8`
I think do blocks and using them with monads should be very early, but it's dangerous to put it too early as you don't want to encourage writing every function monadically. It's not just that it's bad style; it leads to worse, harder to read, harder to maintain, harder to test, and harder to optimize code. I think it's important to be able to default to the declarative style and switch to monads and do notation only when effectful behavior is actually needed (and yes, sometimes effectful behavior is needed just because it's faster/more efficient/easier to read in that case, and that's fine).
You should consider spending some time doing QA or support as a primary focus. You might come away from that experience with a whole new take on programmer discipline.
&gt; For example, polysemy benefits greatly from a late specialization pass, and would benefit further from aggressive inlining after the late specialization pass. Unfortunately, GHC doesn't do any inlining passes after -flate-specialise, so it feels like we're stuck on this front.
One of my favorite examples of ADTs and where the lack of them is particularly painful is an `Expr` type: ``` data Expr = Var String | Lit Int | Let String Expr Expr | Add Expr Expr | Mult Expr Expr deriving Show instance IsString Expr where fromString = Var eval :: Map String Expr -&gt; Expr -&gt; Expr eval m (Var v) = case lookup v m of Nothing -&gt; Var v Just e -&gt; eval m e eval _ (Lit x) = Lit x eval m (Let v a b) = eval (insert v (eval m a) m) b eval m (Add a b) = case (eval m a, eval m b) of (Lit x, Lit y) -&gt; Lit (x + y) (a', b') -&gt; Add a' b' eval m (Mult a b) = case (eval m a, eval m b) of (Lit x, Lit y) -&gt; Lit (x + y) (a', b') -&gt; Mult a' b' ``` Here are some example uses: ``` foo :: Expr foo = eval [] . Let "x" (Lit 5) . Let "y" (Lit 6) $ Mult "x" "y" -- Lit 11 bar :: Expr bar = eval [] $ Add "x" (Add (Lit 5) (Lit 3)) -- Add "x" (Lit 8) ``` As far as I am aware, doing the above in Java or similar is massively more verbose and painful.
The list is very subjective. &gt;The difference in performance from a compiler built in Haskell vs Rust is probably not big enough to care about I wouldn't necessarily say that comparing the compile speed of GHC, written in Haskell, versus rustc, written in Rust. &gt;It takes less time generally to write code in Haskell than it does in Rust This is definitely not true for me. &gt;There are more beginner friendly resources for learning Haskell than there is For Rust This could be, but you only need to read the one official Rust book to become proficient in Rust. I read that one book, and got into Rust. I read dozens of Haskell tutorials before I got anything other than quick sort working. &gt;Haskell has an incredible ecosystem for language development. Rust has most tools you’d reach for but not as many options. Language development, as in writing new languages? What does that have to do with tooling? Or if you are talking about tools, then I'd say the same thing as the tutorials. There's only one set of Rust tooling, but they work way better than everything I've tried in Haskell. &amp;#x200B; On the whole, Rust is much, much more polished as a product than Haskell. When I get started with Rust, I go to the web site, click Get Started which tells me to install Rust and read the book. By the time I'm done with these, I'm proficient. What's the on boarding process for Haskell?
Yes, dependent types are strictly more powerful than GADTs.
Can you use SPECIALIZE pragmas do move the specializations you need up in the phase order, before the inlining you want?
I was referring to the self-recursion splitting
FWIW, I think the format is brilliant. Allows the user to skip things like the "What is Haskell" section.
Op I'm not sure how on paper, a programming language looks perfect and then turns out that you found things different...what happened?
Hi all, I am a haskell newbie and a reddit newbie. I wanted to try to tie my love of music with haskell. The goal I had was to try to make a rudimentary audio plug-in. I see plenty of things that have to do with making music (i.e. Tidal, Euterpea) . Though can some one point me in a direction to go with making an audio plug-in. What are the pain points in doing this? Any examples of this that I can see? Has anyone tried this and can give me some insight.
Wrapping (`Word`, `Int`) and approximations (`Float`, `Double`) are very frustrating. I don't really know what the best way to handle them in is general. Often the most pragmatic solution is to treat them as unbounded and exact (current instances do this), but then you can't truly rely on various laws and can run into buggy edge case behaviors. You can do the above and explicitly throw whenever this illusion breaks (overflow / NaN / infinity etc.) but totality is also a very useful thing to rely on. You can also treat them for what they truly are, but then you lose of instances that users would probably expect to have (no `Num` for `Double`). Even if you think you can handle losing the above instances you then start to realize how much you effect other types that use them internally, and then you kind of have to take away various instances for `Seq` in case it's size gets larger than `maxBound :: Int`. If we were using a total proof-y language like Agda I'd say the approach of absolute correctness and totality makes sense, but in Haskell I'm not so sure.
&gt; The library depends on several library- and user-written functions that are complicated and necessarily self-recursive. &gt; [..] &gt; Optimization concerns should lie solely in the responsibility of library-writers Even if it were possible with your approach, it would require manual annotation for users.
If you were to manually insert `inline` calls using the plugin to make sure recursive definitions get inlined once, that would require you to inspect the call sites instead of the definition sites. This means not only looking at modules which import Polysemy (which is what's being done right now) but also modules that import [modules that import Polysemy] (because it is desired that the optimization kick in for user-defined functions as well). From my limited perspective, implementing the code to find all those call sites sounds like a lot of work compared to merely splitting up the functions and asking the inliner to handle the rest.
&gt; You can close the class hierarchy by making the interface an enum with abstract method, and have the subclasses be nested classes. Could you elaborate on this? I’m very curious what you mean. I can see how you could do it easily enough with ADTs that are really just enums, like `data T = A | B`, but I’m less sure about how it would work with ADTs that include constructors, like `data T = A Integer | B String`.
It would be very interesting to also use this machinery to transform every recursive function of the form: fold f b [] = b fold f b (a:as) = a `f` fold f b as into fold f b = go where go [] = b go (a:as) = a `f` go as
I worked for vacation labs last summer. They build webapps with Haskell. They also let you work remotely. I also TAed haskell courses on NPTEL. They also paid the TAs quiet generously
Oh, many programmers are undisciplined. But Haskell can't fix that! Nothing can, except education. What I'm suggesting is that if you _do_ want to be disciplined, Haskell can help.
Wouldn't the new compiler pass to allow inlining/optimising of self-recursive functions be useful to a much larger audience than only users of Polysemy? :D
Are there any examples for creating a Hacker News style website like [this](https://github.com/reasonml-community/reason-react-hacker-news) but using Haskell? I'm asking for examples because I have no experience with making web apps 😅.
Gauging interest in this. I have friends from other communities who have had success with accelerating dev and improving quality on an open source project by just doing n months of full time work on it. Links: * [Repo here with my work-in-progress](https://github.com/chrisdone/prana) * [Some slides I started here providing the "why"](https://chrisdone.com/pdfs/prana.pdf) People that might be interested in this: * Web developers: think, updating a function or module in-place and not restarting the server. * Game developers: again, think updating something in-place and not restarting the game. * General developers: * being able to inspect any value, lazily, like Chrome developer tools does (or SLIME). * handling exceptions like Common Lisp does, i.e., not unwinding the stack so that you can put that file there that was supposed to be there, and then retry the computation, or update something in scope. * tracing primops * tracking what threads are running, and what they're waiting on, if anything * fuzzing primops * Haskell hackers: being able to easily hack on the interpreter you're using, because it's written 100% in Haskell. * Editor developers (admittedly a small audience): you could write Emacs with this. * Distributed computation hackers: easily try out distributed evaluation designs. I will be working on all this anyway, but it might land in a year or longer. It's my passion project, so if I could work on it full time to accelerate that, it would be awesome.
It's hard to say. I haven't done any analysis into these sorts of functions in any other context. My guess is that GHC's optimizer is breaking down in my case, since nobody else seems to be complaining about it!
Shows a spinner for me.
I think this is interesting what you are up to. However I always wonder about the live reloading promise of interactive coding environments. How is this supposed to work if I have some module A loaded and then I change types in a module B, where A depends on B. One can obviously reload all the modules in the dependency, maybe one could even allow multiple versions loaded at the same time? All closures have to track type+module version then in order to not mix up things? Or do just reevaluate all the computations that have been done up to that point (but there could be some which would not compile anymore)? For me these environments sound pretty confusing and think I would lose track quickly. This is at least my experience with e.g. Jupyter and Mathematica. At some point I restart everything and debug what does not work anymore. Then you mentioned Emacs as a successful example of a live coding environment, well successful it is, but even the "static" part, loading modules and keeping the configuration working, is a huge pain for me. I haven't used SLIME however, I've often heard good things about that.
Sure.
To clarify: is this a completely new interpreter (including parser, lexer etc.) which happens to use the same STG language as GHC, or is it simply something which attaches to the GHC backend by providing an STG interpreter?
It's an interpreter of the resulting STG from GHC, see [the slides](https://chrisdone.com/pdfs/prana.pdf). So it comes as a drop-in replacement for GHC/GHCi. I build e.g. `base` with `--ghc-with prana-ghc`. Rather than outputting `.o` it outputs `.prana` files which contain the AST/bytecode to be interpreted later. The GHC integration is provided in [prana-ghc](https://github.com/chrisdone/prana/tree/master/prana-ghc), which exposes both a binary that's a drop-in for `ghc` and a library for converting GHC's STG to our own AST in the `.prana` files. `prana-interpreter` only depends on `prana-types` (where the AST comes from) and `prana-primops` (which are for the most part derived automatically from GHC's `primops.txt`), it doesn't depend on `prana-ghc`. The goal is for the interpreter to re-use GHC's runtime as the host, but beyond that avoid dependency on any of GHC's API. That stuff is pushed into `prana-ghc`.
You can use a code 'reddit' for a 190 euro discount in the next 48 hours :)
I agree with mpickerings sentiment of improving GHCi instead. If there is something wrong about GHCi then it should be fixed. If GHCi needs to be replaced for these things to be fixed that seems ok to me. The Bytecode interpreter being written in C is a weak argument for rewriting it completely in my eyes. Yeah it's ugly, yeah people don't like to touch it. But I don't think that will change. Another bytecode implementation written for both performance and features will eventually be just as ugly. I do think interpreting STG instead of core is a good idea. But that's neither an argument for nor against a complete rewrite. However my biggest problem with this is that I don't think two interpreter implementations are sustainable in the long run. Either way I'm still interested to see what comes from this. Maybe it will turn out to be a lot better than GHCi and will eventually replace it. If not hopefully someone can apply the lessons learned to GHCi instead.
I'll write more when I get back from lunch, but in essence, you should at least have the choice.
I'd throw at least $5 at this and GRIN, among other things.
There is a really great step-by-step guide for building a forum, which is quite close to HN: [https://www.reddit.com/r/haskell/comments/8ll6zz/building\_forum\_software/](https://www.reddit.com/r/haskell/comments/8ll6zz/building_forum_software/)
Indeed it would! I’ll add one today.
I'll see your $5 and raise you to $50. Best wishes, Henry Laxen &amp;#x200B; Question: Where do I send the $$$?
I don't really know anything about this - just ran across it the other day but you can look into it and see if it has any value for you: [http://euterpea.com/haskell-school-of-music/](http://euterpea.com/haskell-school-of-music/)
&gt; The Bytecode interpreter being written in C is a weak argument for rewriting it completely in my eyes. &gt; Yeah it's ugly, yeah people don't like to touch it. But I don't think that will change. I think you just argued very convincingly why! It's ugly, no one wants to touch it, and that won't change. That's a strong argument for rewriting something. I mean, assuming we share the same values, that code should be at least clean, accessible and modifiable. &gt; Another bytecode implementation Technically this is an AST interpreter, but in the optimization phase of the project, the ADT will be swapped out for pattern synonyms that read the bytes directly from a bytebuffer instead of loading a real tree into memory. So the whole "AST vs bytecode" distinction is blurred. You get reasonable-ish performance, and clean to read code. &gt; written for both performance and features will eventually be just as ugly. Optimising the codebase for performance resulted in GHCi's interpreter. This effort isn't that. It's about features, and writing something in Haskell is a good way to make that clean. If the interpreter is slow, that's fine. What it offers in features greatly outweighs raw performance numbers. &gt; Either way I'm still interested to see what comes from this. Maybe it will turn out to be a lot better than GHCi and will eventually replace it. If not hopefully someone can apply the lessons learned to GHCi instead. Right, either way I will be working on it in my own time slowly. Technically I can still expose the same `ghci` frontend as people are used to, but it would generate my bytecode instead of ghci's own bytecode.
Great! If I see an enthusiastic response, I'll start an indiegogo crowdfunder and you basically commit $x and then if we reach the goal, it's taken, otherwise you get your money back. I'm not sure that the community is particularly excited about this, it's a hard sell to a community that never had a good interpreter. I'll give it a few days to collect responses.
That already exists: it's called "the static argument transformation" (SAT), and was described in [Santos' original thesis on optimizing functional languages](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/03/santos-thesis.pdf), in fact. The reason GHC hasn't ever done by default (though you can ask it to) is due to "reasons" that meant SAT wasn't always a clear win in the end. In particular you introduce a closure in the inner body of `go` (but in return you can inline and specialize `go` to its call site, which could in theory be a huge win). There are various rumblings (and there have been for years) about getting SAT back on the table in the default optimization set. I don't know what the current status is now.
\&gt; Technically this is an AST interpreter, but in the optimization phase of the project, the ADT will be swapped out for pattern synonyms that read the bytes directly from a bytebuffer instead of loading a real tree into memory. So the whole "AST vs bytecode" distinction is blurred. You get reasonable-ish performance, and clean to read code. I've written a bytecode interpreter in C a while ago for a different language project. And I would be astonished if you can achieve both high level Haskell code and decent performance with the pattern synonym approach. For really good performance I suspect you need a bytecode which is specifically made in order to be interpreted quickly, to reduce dispatching overhead use registers and maybe add superinstructions etc. An alternative route which marries performance+high level features could be interpreter generation from a bytecode specification. The specification is translated to lowlevel c code in various flavors instrumented by hooks for the fancy features. Ideally this specification could be used for codegen too and additionally yield a slower highlevel interpreter in Haskell itself. But I should say, I find your goal of a feature-rich high level interpreter to be perfectly valid and I am looking forward to what you are going to say about hot code reloading ;)
Most people who are going to touch performance heavily already know the tricks for making GHC optimize well -- things like "arity alignment" (always apply a function in the most "saturated" way as defined by its LHS), but more importantly in this case, pushing top-level loop bodies downward, so loop bodies can be inlined at call sites. Self-recursive bindings not being inlined at the top level has been the case forever, so "everybody" knows to factor them out and push them downward, making it easier for GHC to move them into a better spot later. Why/how would they notice if GHC did this? So there is also probably an element of this being a self-fulfilling prophecy, in a sense. But GHC already does this optimization, in a sense (and I'm sure you know this), so my guess is this: GHC looks for groups of recursive bindings and when it finds them, it picks a "loop breaker" -- one of the definitions that will never be inlined. By doing so, GHC is then allowed to inline *all the other* bindings in the group into their call sites. This "syntactic trick" you're doing is effectively a clever way of forcing the same optimization by making every single-bodied self-recursive function into a "double barreled" mutually recursive function, and picking the copy as the loop breaker. GHC could in theory do this for any self-recursive single function, I suppose. But it's hard to say whether it will be a net win or not. In this case it's effectively a way to both inline the body and it also (effectively) peels off one iteration of the loop. It's hard for me to guess what the final result looks like without Core. It is also not clear to me what impact this would have on unboxing and whether or not GHC would more easily be able to unbox primitive types past that `NOINLINE` (if you had instead made `factorial` a non-recursive top level function and moved the body further inward manually like most people do, the entire loop body ends up inlined at every call site instead, which can expose further specialization and unboxing opportunities.) Either way, manually inserting loop breakers is a nice and obvious trick, in retrospect.
Count me in. There is the challenge to have Prana move along with GHC when the latter develops, of course. But as a lecturer of Haskell courses I'd be all for it.
I am just starting to learn and I love this show, so yeah, it's beginner friendly. :-)
I'll second support for improving GHCi. There are secondary factors besides the actual code, such as the Haskell ecosystem and community. I don't want to see any more fragmentation of the community/tooling. Why don't you reach out to the Haskell committees/GHC developers? Maybe they're in favor of a GHCi rewrite or have suggestions to incorporate your ideas into GHCi. They may also be able to help solicit funds.
&gt; &gt; Yeah it's ugly, yeah people don't like to touch it. But I don't think that will change. &gt; That's a strong argument for rewriting something. For what it's worth I think the reason why people don't want to touch it has more to do with who is working on GHC than the code being ugly/hard to work with. It is a very small cross section of people who are interested in writing that kind of low level code, interest in GHC, have the skill to make things actually better, and the motivation to do so instead of working on LLVM or similar more popular projects. The code itself sure is a factor, but seems like a small one in comparison to me. Anyway designing a Haskell interpreter from the ground up also sounds like a really fun project so I can't blame anyone for going with that route :)
Thanks, that's very close to what I was looking for! Do you know if I can use Yesod + Opaleye instead of Yesod + Persistent + Esqueleto? The blog post uses the latter and this [page](https://guide.aelve.com/haskell/web-frameworks-mi360n4k) also says that I need to use Persistent with Yesod.
I'd certianly put up $5 for a month if you think you can get it sufficiently done in a month, but my inclination is that I'd prefer to put up $60 for a year, because this seems like a big project that is unlikely to get to a usable point in a month. In my experience, asking for crowdfunding for a year of work on a free software project can work out well; OTOH I understand you might be taking a pay cut in doing this and prefer to only take that pay cut for a month.
Oh, is there somewhere I can contribute to GRIN? I'd throw tons of money at that
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/haskell_jobs] [Haskell job offer, Houston TX](https://www.reddit.com/r/haskell_jobs/comments/bs46m4/haskell_job_offer_houston_tx/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Yeah, there definitely are pros as well :) thanks
Sounds interesting, I'm willing to pay up to accelerate work on this idea.
&gt; necessarily self-recursive I'm almost certain this is untrue. You can always use a recursion scheme, even if you have to dip into the fully general version using M-types.
Hmm, you might be right. I know Java enum classes can have (`final`) instance (non-`static`) fields -- we'll often include the C name (as a java.lang.String) and value (as int) for the enums we use for interop. I'll have to think about it and get back to you.
If I scroll down I do see a spinner; but I even if I wait I don't get any further content.
An enum type has no instances other than those defined by its enum constants. It is a compile-time error to attempt to explicitly instantiate an enum type --[JLS](https://docs.oracle.com/javase/specs/jls/se8/html/jls-8.html#jls-8.9) --- So, yeah, you couldn't use an `enum` to model `data T = A Integer | B String`. You can still close the class hierarchy without using `enum`: Make constructors private and have all sub-classes be `final`, nested classes. public abstract class T { private T() {} public static class A extends T { public A(BigInteger _1) { this._1 = _1; } public final BigInteger _1; public &lt;R&gt; R scott(Function&lt;BigInteger, R&gt; onA, Function&lt;String, R&gt; unused) { return onA.apply(_1); } } public static class B extends T { public B(String _1) { this._1 = _1; } public final String _1; public &lt;R&gt; R scott(Function&lt;BigInteger, R&gt; unused, Function&lt;String, R&gt; onB) { return onB.apply(_1); } } public abstract &lt;R&gt; R scott(Function&lt;BigInteger, R&gt; onA, Function&lt;String, R&gt; onB); }
&gt; it's a hard sell to a community that never had a good interpreter This seems right. It might take a more detailed writeup (maybe with mockups comparing how things work now to how they might work) to get folks really excited. I'd be happy to contribute something.
.. and it's practically sold out!! ;p
What does "tracing/fuzzing primops" mean? A new ghci command like `:fuzz`? Are the new features you outline above just a collection of things that you would like the interpreter to do, or are they things that _become possible due to some specific design decisions you have in mind for the project_? i.e. could some of these features be added to ghci in a straightforward way?
Why is GHC generating GHCi bytecode from Core instead of STG? I wonder about this design decision since I think within the GHC runtime system, STG is a more natural fit. Is this historical because of Hugs maybe? Or are there technical reasons?
Will there be live-streams from the event?
Won't be live streamed, but we will publish the recordings at Monadic Warsaw's YouTube channel.
Ah. Just as nice!
Thanks for the analysis - this helps crystallize a lot of intuitions I have. Do you have any resources on arity alignment or other "well-known" optimization tricks?
It has been this way since the initial conception as far as I can tell, but that was close to [20 years ago](https://github.com/ghc/ghc/commit/5b54f875651ec0d098651507345ae8d2204ec39e). STG seems more natural to me as well but maybe I'm missing some details wich made core easier to implement.
Tracing in the sense e.g. if you're doing concurrent programs, you could see trace whenever an MVar is blocked/read, whenever a thread is forked, or killed, an exception thrown, etc. Or add arbitrary delays to some primops, to expose race conditions in your code (fuzzing). Yes, a command like `:fuzz` or so. Those ones come for free, just because I already have all the primops handled explicitly and derived. So they could be tracked and traced easily. You can add anything to GHCi's interpreter in theory, but in practice I don't know how easy this would be.
As I understand it the issue is that the cabal build system does not know about the combination dynamic+profiled (https://github.com/haskell/cabal/issues/4816). As a follow on to this ghc doesn’t ship base (&amp; other libs) in that flavour (though I think it technically could with https://github.com/haskell/cabal/pull/5606).
****
I don't have numbers but I have somewhere a core output of [an experiment](https://github.com/mgsloan/prana/blob/master/src/Prana/View.hs) I did that I can share. It ends up compiling the pattern match down to a switch. Found it, me and Michael Sloan experimented with a sketch [here](https://gist.github.com/mgsloan/cdcfe0df1827a6cd2829dd7f050e1efa). This code readMaybeInt :: Stored (Maybe Int) -&gt; Maybe Int readMaybeInt s = case readTag s of StoredNothing -&gt; Nothing StoredJust (readStorable -&gt; x) -&gt; Just x compiles down to: case ipv1_a3Sr of { GHC.Word.W8# dt1_a2Jg -&gt; case dt1_a2Jg of { __DEFAULT -&gt; Main.main4; 0## -&gt; GHC.Base.Nothing @ Int; 1## -&gt; GHC.Base.Just The trick is that the pattern synonyms let you pretend you're working on an AST that's on the heap, but actually you're working on a packed vector of bytes that can be pre-fetched into the cache. The advantage of this optimization is that it requires no change to your interpreter code. You just unimport the real ADT type, and import the pattern synonym type, and now your code doesn't do pointer chasing anymore, or even have a decode/deserialize step to load the code in from file.
So? Why are there flags for something that can’t be done? Is there any other way to execute Haskell code calling from other language apart from FFI and shared libraries?
This is not worth creating a Twitter account, but count me in for $5.
Yes, I believe you that it works and it is actually a pretty cool idea. I am just saying that even if you use this byte packed AST as bytecode you will still be far away from the performance you can achieve with a more specialised bytecode optimised for fast interpretation within an interpreter written in C. Even for a low level interpreter, things are not at all straightforward according to my experience (This can also be seen by the greatly varying performance of interpreters used by mainstream languages, however dynamic typechecks also play a big role). But I think you are on the right track to approach this differently than GHCi. A high level interpreter in Haskell with all the runtime type infos available is nice for all these high level introspection features you are planning to implement. As far as I understood you will also catch type errors this way. Furthermore an implementation in Haskell feels much more like a reference implementation for the STG actually implemented in GHC. Many people here in this thread seem to believe that improving GHCi would be a better solution - but I think what you are proposing here is an incompatible, alternative approach with its own benefits and it seems near impossible to find a one fits all solution here.
Would it be possible to use this to generate combinators for the composition of multiple functors? I've noticed that pattern matching really starts to get verbose in this case, eg \`Fix (Compose ((,Int)) (Maybe (Compose MyFunctor)))\` requires an algebra that pattern matches on \`Compose (n, Compose Nothing)\` or \`Compose (n, Compose (Just MyFunctor foo))\` (I have a good reason for using this kind of formulation, I promise).
i would chip in
&gt;Fix ((Int,) \`Compose\` Maybe \`Compose\` MyFunctor) I think the way the generation is now it does not pattern matches like you would want to! But give me a more concrete example please, so I can make sure I understood what you want!
I'm (the main developer) preparing a patreon page for the GRIN Compiler project. It will be on in one week.
Would the [GRIN Compiler](https://github.com/grin-tech/grin) be appropriate? The ultimate goal of the project is to make Haskell/Idris/Agda fast and to improve tooling.
Sure, where is your patreon? )
Sure, we are encouraging a wide variety of applications.
I think you're gonna end up with a real HUGS style problem here, where all that effort went into some cool features and then no one's gonna use them because they'd rather use GHC. I'd also support improving GHCi instead. That's a cause I could donate some to.
Frankly, if it's not in GHCi, I'm doubtful it'll catch on. It'd be a shame to see all this effort go into something that no one ends up using. I also generally dislike fragmenting tooling even further. So I'd also support improving GHCi instead. That's something I'd give some $ to.
My prior code experience was mostly hobby, with a little semipro right at the end; this had its downsides, but did allow me some eclectic language exploration. I played with Smalltalk for a while, so I was already comfortable with higher-order functions and non-strict evaluation; and I naturally fell into a very functional style with C#. (It was the blog of Eric Lippert, one of C#'s designers, which led me to Haskell.) So as far as the core language was concerned, after basic syntax the only really new things I had to grok before I could get stuff done were IO isolation, immutability and sum types. The type system was a completely different story! I managed to stack some transformers for the reddit bot, but it was a while longer before I tackled MTL, and I had a (mostly unjustified) phobia of lenses for a couple of years. But as it turns out, cobbling together a transformer stack is generally enough for small projects.
By “GHCi” you mean “that haskelline `:foo` interface that `ghci` and `intero` use”?
Mostly I just meant the command `ghci` that ships with GHC. But to do that, I doubt a total rewrite like this would be the way to go. That'd be a conversation for more informed GHC devs though.
Absolutely! I've been using this to represent partially-substantiated Merkle trees: ``` data Tree a = Leaf String | Node [a] deriving (Functor) type Hash = Int type PartiallySubstantiatedMerkleTree = Fix ((Hash,) `Compose` Maybe `Compose` Tree) ``` and this type to represent lazy Merkle trees (where `m` is some Monad). ``` type LazyMerkleTree = Fix ((Hash,) `Compose` Maybe `Compose` Tree) ```
Okay, thanks for your answer! &amp;#x200B; It really is an interesting use case! And although the pattern match as it is now will only go like this \`fix f (Fix a) = f a\` (because it can only generate from data types and not type alias) and from there maybe you could compose with the resulting combinator from the \`Compose\` type to get something like \`fix . compose (bimap f g)\` and \`g\` can be the combinator for \`Maybe\` for example; the option to get some combinator to pattern match like you want seems possible to me! &amp;#x200B; Since you are working with a particular data type maybe you can try to use a isomorphic type that will play nicely with the current functionality of \`data-combinator-gen\`! &amp;#x200B; Either way I'll take note of your use case and try to work on it! You can feel free to open an issue or try to help me with this project :) &amp;#x200B; Thank you once again :D
I’ll pledge $5
&gt;It's an interpreter of the resulting STG from GHC, see the slides. I did look at the slides, but I'm still a bit confused, which is why I'm asking. &gt;So it comes as a drop-in replacement for GHC/GHCi. I feel this wording is very confusing. To me, this implies that prana will be a completely new compiler, along the lines of UHC/JHC/hugs/etc., as I think of GHC as being the whole toolchain packaged within the `ghc` executable (lexer+parser+typechecker+optimiser+codegen etc). On the other hand, your other comments strongly imply this isn't the case...
[https://haskell-miso.org/](https://haskell-miso.org/) has built-in websocket API and is easy to get started with. There are examples on the Github page of websocket integration among many other things (like deployment scripts). [https://github.com/dmjio/miso](https://github.com/dmjio/miso)
I would recommend Yesod on the backend and PureScript on the front-end if you expect much complex javascript interaction.
Does miso have a future?
Does PureScript have a color picker?
There is one for the Halogen framework: https://discourse.purescript.org/t/a-color-picker-component-in-halogen/521
How does one unify two paths of a GADT? I currently have a pretty basic GADT like data Expr :: [*] -&gt; * -&gt; * where Empty :: Expr '[] v Add :: (s, v) -&gt; Expr ss v -&gt; Expr (s:ss) v and two functions that both compile on the different cases unwraptest' :: Expr '[] v -&gt; [s] unwraptest' Empty = [] unwraptest :: Expr (s:ss) v -&gt; [s] unwraptest (Add (s,v) exprs) = [s] But how do I write a function that patternmatches and applies the relevant function? Naive implementation produced a bunch of yelling about rigid skolems that even type application/specification didn't fix
Hey, I'm very sorry about the late answer; I've seen it on my phone and forgot afterwards. To answer: absolutely not, Formality-Core is still extremely experimental, and being a backend for Haskell will be specially challenging. I think both fronts should be attacked in parallel, and I'd say a GRIN is more likely to be better in the long term, while Formality-Core is riskier but may bring higher rewards.
Are you aware of https://github.com/blitzcode/hue-dashboard? It's based on [threpenny-gui](http://hackage.haskell.org/package/threepenny-gui).
&gt;Does miso have a future? Yep. Very bright future 👍 &gt;I don't know if miso on GHCJS makes sense because I want to use the web UI on smartphone. Smartphone doesn't like sluggish GHCJS. Works well for me on smart phone when pre-rendering is used. Alternatively, you can fund me to continue work on miso-native.
I personally much prefer Haskell on the front end. I don’t want to use pseudo Haskell and have to give up (or make much harder) codesharing and the entire Haskell ecosystem I am used to. Particularly since long term things like Haskell-&gt;WASM make me doubt how strong PureScript’s future will be.
PureScript's FFI and row types make it absolutely fantastic for front-end work. It's not a perfect language, but in many ways it improves on Haskell. The editor tooling situation is much better with PureScript. I've generally found the gains from code sharing to be less than the costs of using a language that's poorly suited for the front-end. GHCJS's runtime is a monster to include.
It has implemented some case by case changes to Haskell, but IMO nothing that really takes it up a notch. PureScript's FFI doesn't really seem that different to GHCJS's. The row/structural type implementation seems pretty far from ideal. There are no variants, sums or products. It also seems to have it's own magical syntax and kind instead of the more DataKinds/Agda-y approach. Having symbols be first class values is also pretty essential for various things like auto-serialization to various formats. I also don't like strictness. I don't want a strict pure language because it makes various convenient and performant things noticeably more awkward, and i'm perfectly happy to throw in a `seq` or `!` every once in a while for perf / space. Personally I have found GHCJS's runtime to be an acceptable cost for the glory of Haskell on the frontend. Plus I'm excited to see how huge Haskell-&gt;WASM will be for high performance front-end Haskell dev.
Think about where the type variable 's' is bound; i.e. where you would need to stick explicit `forall`s. In particular, it should be clear `forall s. Expr l v -&gt; [s]` is binding `s` before it opens the `Add` constructor, so the s there may or may not the already bound s; which it why is complains that `s1 ~ s` can't be deduced. If you want an existential, you'll have to CPS it. &gt; :{ | unwrap :: Expr l v -&gt; (forall s. [s] -&gt; r) -&gt; r | unwrap Empty k = k [] | unwrap (Add (s,_) _) k = k [s] | :} &gt; unwrap Empty length 0 &gt; unwrap (Add (1,2) Empty) length 1 A universal quantification wouldn't work as-is. You have to determine if `s` in the output matches the `s` that `Add` was created with, and if so return the singleton, and if not return `[]`. I'm not exactly sure how you'd accomplish it, but if non-linear patterns are supported in type families, you could probably do that.
Row types give [polymorphic variants](https://github.com/natefaubion/purescript-variant) great, and the implementation of records is IMO best-in-class of any functional language. Symbols are first class through the `SProxy (s :: Symbol)` type (though you may mean something else?). I agree re strictness for the most part. Semantically, I wish it were lazy, but the ridiculous ease of calling PureScript code from JavaScript and the lack of a massive runtime make up for it (for the front-end). I'm definitely excited to see how Haskell-&gt;WASM will go, but that project is in the future and PureScript is great now.
I hope both of you have seen this post from earlier today: https://www.reddit.com/r/haskell/comments/bs5aiu/mit_spinout_offering_up_to_15m_to_ventures_in/.
Oh, that works perfectly, since now I can just stick `id` on this to get to what I wanted (I was just going to recurse over the structure and then concat). Thank you!
Now to figure out how to monetize profunctor optics.
do it!
Oh, it's a fixed `r`? I don't think that I understand then. I tried `unwrap Empty id` or even `unwrap Empty (id :: forall [s] -&gt; [s])` assuming that it would take `r ~ [s]` , but apparently that isn't the case.
`id` won't work as a continuation there because `s` would escape the it's scope.
I would go with Elm, very simple and really has everything you need for simple UIs. https://elm-lang.org/
It's not a fixed `r`, but `r` can't depend on `s` -- otherwise `s` would escape it's binding scope. `length` works, `fmap show` works, but `listToMaybe` or `id` don't.
How does fmap show work? It would depend on `s` to choose the dictionary, no?
I looked through PureScript docs for variants and couldn't find anything. I guess they have been implemented in an external library via a bunch of FFI and unsafe*. The syntax for them looks kind of gnarly though, and now there are two different ways to implement named sums, `data FooBar = Foo Int | Bar String` vs `type FooBar = Variant (Foo :: Int, bar :: String)`, so anything designed around one approach won't work with the other. Ah I see they do seem first class, the syntax sugar just hides that fact (`(foo :: Int)` is talking about `"foo"` and not the value of the `foo` type variable). In that case can a single `fromJSON`/`toJSON` function be defined for all Records and Variants? I still would like it much more if `Row` didn't have a magical kind, and an `Agda`/`DataKinds` approach was used instead. While Haskell-&gt;WASM is a future thing, GHCJS works just fine for our team right now, so it being a future thing isn't a huge tragedy.
The `r` would always be `[String]` which doesn't depend on `s`. You would have to carry around the `Show` constraint, which might involve changing the type of the `Add` constructor in addition to the Rank2 type.
GHCJS runtime might be ok, but GHCJS FFI prevented me from calling haskell from javascript in electron. Some of electron API wanted to call haskell functions with more than 11 arguments. GHCJS FFI doesn't allow something like that. I'm waiting for Haskell-&gt;WASM compiler with a better FFI.
The audio is impossibly quiet.
Why in the world were you interacting with Electron? No one wants desktop apps that are slow as shit and take up excessive memory. The 11 argument thing is interestin and unfortunate. Although I do wonder why electron would want 12+ arguments.
&gt; pre-rendering Do you mean pre-rendering on the server? Do I need to learn nix? Last time I used GHCJS, I didn't like extra dependencies.
It's an implementation of [F-algebra](https://en.m.wikipedia.org/wiki/F-algebra). I'm no expert. From my understanding, it's a generalization of the "normal" algebra in category theory. Bartosz Milewski's [lecture](https://youtu.be/zkDVCQiveEo) and [blog](https://bartoszmilewski.com/2017/02/28/f-algebras/) may be helpful.
Are there slides?
It's going to be simple. Since I haven't learned yesod, yesod can be an overkill. PureScript can also be an overkill when jQuery does the job.
Wow, amazing lineup and array of topics!
I wonder if it might be interesting to apply category theory to the kind of modelling done in “Algebraic Models for Accounting Systems.” Basically a business is a directed graph of accounts with an edge from A to B being a transaction that debits A and credits B. If the business does multi-currency/multi-asset accounting then it becomes more interesting and this isn’t dealt with in the book.
It's technically possible, but you'll lose almost all integration benefits that Yesod has with Persistent. Personally, I prefer Servant + Persitent + Esqueleto or Servant + Beam ([https://tathougies.github.io/beam/about/faq/](https://tathougies.github.io/beam/about/faq/)) &amp;#x200B; I also find this discussion on relational mappers quite informative: [https://www.reddit.com/r/haskell/comments/8qxvir/a\_comparison\_among\_various\_database\_edsls\_selda/](https://www.reddit.com/r/haskell/comments/8qxvir/a_comparison_among_various_database_edsls_selda/)
The elements of your algebra are not of type \`m\`, but of type \`m -&gt; r\`. In your view, the inhabitants of m form a basis of the free R-module that underlies the algebra (assuming that \`m\` has finitely many inhabitants). The product (in the mathematical sense) of two algebra elements \`f : m -&gt; r\` and \`g : m -&gt; r\` is then given by \`mult (\\\\ x y -&gt; f x \\\* g y)\`. Essentially, if you write elements \`a\` and \`b\` of the algebra as linear combinations of basis elements, the \`mult\` map tells you how to write the product \`ab\` as a linear combination of basis elements. The \`unital\` function applied to the unit element in the ring should yield the unit element of the algebra, again essentially as a linear combination of basis elements. &amp;#x200B; I'm pretty sure that (with the right compatibility laws) this is exactly equivalent to the data of an algebra structure defined on a finitely generated free R-module (i.e. a finitely-dimensional algebra if R happens to be a field) whenever \`m\` has finitely many inhabitants. In the infinite case I would have to sit down and work it out to make a definite judgement.
Here is a list of the most relevant ones: * The book explains *both* Cabal and Stack as build tools, and drops the explanations about EclipseFP * The Conduit library has changed quite a bit since the 1st edition, all the code blocks have been updated * The chapter about web applications has been rewritten using Spock for the back-end (instead of Scotty) and Elm for the front-end (instead of Fay) * New section about data type-generic programming, which was before just a small comment in the "attribute grammars" chapter * New section about using message queues (in particular, AMQP) to communicate different Haskell processes, substituting the previous usage of Cloud Haskell * When the 1st edition was published, Applicative was not yet a superclass of Monad, and MonadFail did not exist; the book is now up-to-date
I wonder for what is Haskell being used at Bloomberg. If they took all that effort to optimize their process, it must be something non-trivial.
I'm in favour of people doing useful things with Haskell as soon as possible and then they could choose what to do. &amp;#x200B; Style is a matter of taste and patience. What for some haskellers look idiomatic and cool is for me horrible and complicated, and even non composable and therefore, non functional.
Where could I find more information about the Agda-y approach? Do you mean current Ada records, or perhaps some design enabled by dependent types?
Initial objects are natural resources. Final objects are deadbeats. Economics as category theory. :)
Awesome!!! Please consider grab the slides from the laptop instead of aiming a camera at the projector screen. A fair few excellent talks from last years conf were unwatchable.
The design enabled by dependent types. In Agda a row/record implementation would not involve a magical Row kind, Row would be a regular type like any other, and Record would be a type constructor that takes in a row.
Yeah, we will have the speakers record their screens and use that as the main video track, the projector recordings went surprisingly bad last year.
Sorry about that! We're pretty new to the whole A/V stuff; we'll see if we can improve it next time!
A lot of the build issues would be solved by using [nix](https://nixos.org/nix/), but since you have 5k engineer the move towards nix would be non trivial. (just teaching nix alone would be a significant effort). Your workbench.yaml would be nix files, your caching problems would be solved by the nix store and your OS versions wouldn't matter because the haskell code would use the nix store.
Hmm, an initial object in this variant of accounting theory could be a special account that represents something like the source of all values in the beginning of the accounting period. Correspondingly, a terminal object would be introduced at the end of the accounting period to "close the books." I'm not sure this would be useful, but it seems somewhat reasonable.
Any experiences in using nix for enterprise java projects?
I'm somewhat confused by the "mutable snapshot" idea. My understanding is the the primary benefit that comes because of immutable snapshots is ease of reproduction. If the snapshots become mutable, that benefit goes away. If the problem is changing all the resolvers manually, you could have a global stack.yaml which is used by most of the projects and sets the resolver, along with local stack.yaml files for project specific configuration (docs: https://docs.haskellstack.org/en/stable/yaml_configuration/). Would that resolve the issue?
The folks at [https://www.oicos.org](https://www.oicos.org) have given this some thought.
\&gt; The syntax for them looks kind of gnarly though, and now there are two different ways to implement named sums, ..., so anything designed around one approach won't work with the other. &amp;#x200B; This seems like an odd complaint. `purescript-variant` is a library, nominal types like `FooBar` are supported directly by the language. They aren't the same thing, but even if they were, would you rather the language did not support adding new types that could possibly replicate existing built-in types? Or are you arguing that open variant types ought to be built-in? Because that wouldn't be compatible with the goal of not adding unnecessary features to the language.
&gt; I looked through PureScript docs for variants and couldn't find anything. I didn't realize they had been implemented in an external library via a bunch of FFI and unsafe*. Just like Haskell's variant type emulations, but much more pleasant to use IME. &gt; Also is there support for anonymous/positional Sum/Product that are backed by an Array (specifically Array Type) rather than a Row? To replace any every-tuple-size-is-a-unique-type constructor noise. The convention for tuple is to use a constructor like `data a /\ b = a /\ b` which has a fixity that makes `1 /\ 2 /\ 3` work out as a constructor and pattern, or to use a record like `{ fst: 3, snd: "bar", "3": [1,2,3] }`. It's more common to just use an anonymous record though - the type of `uncons` is: uncons :: forall a. List a -&gt; Maybe { head :: a, tail :: List a } and usage takes advantage of named field puns to do: case uncons (1:2:3:Nil) of Nothing -&gt; ... Just { head, tail } -&gt; ... There's not an array backed type, though you could make one with `unsafeCoerce` and PureScript's type level programming facilities (just like Haskell's). &gt; In that case can a single fromJSON/toJSON function be defined for all Records and Variants? Yes, [records have a straightforward Generic-based JSON encoder/decoder](https://github.com/paf31/purescript-foreign-generic). &gt; I still would like it much more if Row didn't have a magical kind, and an Agda/DataKinds approach was used instead. I'm unfamiliar with this approach - what benefits and advantages does it have over PureScript?
JQuery's a fine choice if you don't need much javascript. Yesod is batteries included, but at it's core, [it can be quite minimal and simple.](https://github.com/parsonsmatt/yesod-minimal/blob/master/src/Minimal.hs)
Haven't read it yet, but I'm really excited about the title of it.
IME, knowing Clojure or Scala or F# sets you up to get over the Haskell bump faster than a Java or JavaScript or Ruby dev, but not that much quicker. Indeed, if you think that your non-Haskell FP experience means that you don't need to learn much, you'll faceplant harder than most.
Unfortunately, there's a lot of things that you don't learn about Haskell until you're using it in a professional environment. Hobbyist Haskell programming has very different constraints than professional work, and it can be hard to adjust. Sometimes employers don't have the bandwidth to ramp-up a hobbyist (or junior) to their stack, and they need to filter for that in their job search process. There are many folks working on better material for using Haskell professionally, myself included. With any luck, we'll be able to break that floodgate open :)
For me, Haskell was my first functional language, so while I have little standing to contest your claim, it seems very surprising to me. Why do you think this is so? Did you “face plant harder” than your peers despite having Scala experience or something? I do not think Haskell is a particularly difficult language to learn, but many struggle because of a lack of high quality pedagogical resources (which has changed a lot recently)
&gt;low stakes paths in which I can build trust in others, confidence in myself, and a better approximation of the value of my work Sure, that's a nice way to do it. (Though I would still suggest that you read something like [How I went from $100-an-hour programming to $X0,000-a-week consulting](https://training.kalzumeus.com/newsletters/archive/consulting_1) to make the process of raising prices slightly less painful.)
I would *love* to see a talk on this! (Apologies, but I honestly don't have time to read PhDs on all of the topics that interest me.)
There are 30 slides and if you press 's' some speaker notes will show up. The slides try to aim to be accessible ---a committee member said that maybe they were too accessible. I'm also happy to answer any queries =)
Thank you for that, but it's *really* hard for me to absorb without an actual person speaking :). ... but no worries. Hopefully, you'll get to give some (recorded) talks which we can enjoy! :)
Although not Haskell related I think the [Pijul](https://pijul.org/) project could be a good match. Pijul is a patch based version control system somewhat similar to \`darcs\`. Pijul's underlying theory is rooted in category theory, see the paper [A Categorical Theory of Patches](https://arxiv.org/abs/1311.3903). Not sure they have heard about this RFP, so I left a note on their forum.
Sorry, I still haven't internalized the for all parts of the initial / final definitions. I meant node with input degree 0 are natural resources and node with output degree 0 are deadbeats.
Well, that makes sense. Thanks for the answer!
This is so cool. I haven’t had the chance to try `polysemy` out yet, but it looks like I’m going to be writing Haskell professionally again very soon, so I’m sure I will eventually. Either way, these blog posts are wonderful.
Since the announcement at the end of the talk was cut out, let me take this opportunity to remind people that next month (Jun 24-25) NY Haskell is hosting Compose 2019, and now that all the talks are announced, it is a good time to get tickets and lock down travel plans: [http://www.composeconference.org/2019/](http://www.composeconference.org/2019/)
There's been some work on double-entry bookkeeping category theoretically: http://rfcwalters.blogspot.com/2011/02/mathematical-economics-double-entry.html (paper: https://arxiv.org/abs/0803.2429)
I actually had the same idea as I'm currently taking part in a course on lie algebras in physics. My goal is not so much the mathematical rigurosity but more to get to know how to actually use haskell and also deepen my understanding on lie algebras. My plan is basically to create a library/module that lets you explore lie algebras interactively. I only implemented basic data structures and dynkin diagrams at the moment. The code is on [github](https://github.com/jumper149/haskell-lie). I am still struggling with cabal on archlinux tho..
/u/hvr
Haskell was my first functional language as well. I'm speaking from the perspective of watching coworkers with non-Haskell FP experience having a hard time, *until* they accept that the patterns and idioms that they've learned don't carry over and accept a more "blank slate" mindset.
I had to look up the definitions myself, my only exposure to category theory was a university class I abandoned in, like, 2009! BTW you might enjoy this blog post: https://martin.kleppmann.com/2011/03/07/accounting-for-computer-scientists.html
Slightly surprised to see no mention of 1ML. Is there a reason you don't talk about it in the proposal?
yesod is still a good job for the server-side stuff, it's a very complete ecosystem
Surely `-fuzz-primops` would cause the testsuite to uzz primops!
I’m still surprised that patterns do not carry over though. Many structural patterns that are common in Haskell, for example monoid or applicative, are obviously useful regardless of language, no? As a second example, functional strategies for operating on sets, such as map and recursion, are definitely useful across functional (and even in imperative) languages. Many learn Haskell not for Haskell but because it is the archetypical FP language - I do not think this would be the case if there were no transferable skills between Haskell and other FP languages.
Me not personally but people are doing maven builds: https://github.com/NixOS/nixpkgs/issues/19741 And there is the mavennix thing: https://github.com/icetan/mavenix
Slides seem to overflow for me in Firefox, but the content is really interesting! Thanks for the org sources!
Thanks for the kind words! Means a lot coming from you!
There is a third way to dispatch on the type of X: the visitor pattern. Have a visitor than has three methods for visiting a Foo, a Bar and a Baz. Then the only method in X is one that accepts such a visitor. The different implementations of that method in Foo, Bar and Baz just call the appropriate method on the visitor, passing itself into it. I believe that this can accurately model ADTs (and possibly even GADTs), but it's a lot of boilerplate code.
Thank you! Very insightful :-) &amp;#x200B; As for the "everyone already knows this" part: Isn't this argument less strong when we're talking about code that might be generated by e.g. TemplateHaskell or custom rewrite rules, which because of its autogenerated nature misses the finesse of a crafty human's oversight?
Oh man, i really enjoyed reading this. I genuinely thought/hoped Homotopy Type Theory would somehow tie into this sort of programming by isomorphism etc structure reuse, which I think in some respects you're talking about with these slides?
Because of superior performance and ease of tooling, I'm considering Elm on the front end. Since miso is similar to Elm, I'm also considering miso.
Declarative style is not the same as complicated and noncomposable. The latter comes from what I agree is badly written code. But the declarative style usually results in \*more\* readable code; imagine if we defined take as take :: Int -&gt; [a] -&gt; [a] take n xs = evalState step (n, xs, []) where step = do s &lt;- get case s of (0, _, ys) -&gt; return ys (_, [], ys) -&gt; return ys (n, x:xs', ys) -&gt; do put (n - 1, xs', ys ++ [x]) step In my opinion at least it's pretty clear what's happening here because it's so imperative and shows the steps of how you would take elements from a list. It was easy to write (and I checked that it's correct). It's composable, but it's also really complicated. Compared to the actual definition (the comment is my own): take n xs | 0 &lt; n = unsafeTake n xs | otherwise = [] -- this is 'unsafe' because if m &lt; 1 it takes the whole list. All the work happens here. unsafeTake :: Int -&gt; [a] -&gt; [a] unsafeTake !_ [] = [] unsafeTake 1 (x: _) = [x] unsafeTake m (x:xs) = x : unsafeTake (m - 1) xs This is just as composable (same type) but much easier to read and therefore I think also easier to understand. The stateful monadic version also isn't going to be any more efficient. &amp;#x200B; This is really what I was trying to get at with that comment. Defaulting to the latter "style," which I guess would be more accurately termed a 'paradigm' results in more readable and maintainable code. But on the other hand, there are certainly times when using a monad to implement a non-monadic function (like [this](https://www.reddit.com/r/haskell/comments/b5g70h/practical_uses_of_the_tardis_monad/) problem) is a good idea. That doesn't mean you should try and write everything that way.
Ignorance may be the issue. Thankyou for mentioning it!! I look forward to reading about it =)
&gt; monoid or applicative Hard to always use correctly without HKTs in your type system. Most of the Gang of Four patterns are... not exactly a focus of Haskell code. The interpreter pattern is useful in every language.
The tradeoffs that other languages make guide what is idiomatic and practical in those languages. The tradeoffs are different in Haskell. If you assume that idiomatic Scala or F# or Clojure will make idiomatic Haskell then you'll be in a lot of pain. If you believe that this pain is a result of Haskell being a shitty language, instead of your own internal biases, then you'll hate Haskell and talk about how impractical it is. As a Haskell dev, I don't assume that I can just walk into an F# shop and immediately be productive and an expert. I must approach it like a beginner, so that I can have an open mind and learn how things are done in this new land. Likewise, folks new to Haskell need to adopt a sense of humility. If they assume that their experience with purely functional Scala/etc sets them up for immediate success in Haskell, they're going to be disappointed.
Referencing GoF is a bit cheeky - we're comparing functional languages to Haskell, and GoF is explicitly an OO text.
I think your point about attitudes is important and spot-on, but you are moving the goalposts a bit here. Originally you said that "...knowing Clojure or Scala or F# sets you up to get over the Haskell bump faster than a Java or JavaScript or Ruby dev, but not that much quicker." This seems like a fairly distinct claim from what you assert here, which is that open-mindedness and humility are important (which they are, regardless of one's experience, language, or context). I still have a hard time believing that knowledge of Scala or F# will not improve your Haskell productivity when compared to purely having experience as a Java developer.
I think everyone recognizes that Haskell has a steep learning curve. Knowing purely functional Scala sets you up higher on the curve than Java. But there's still a lot to learn to be a proficient Haskeller - it's much bigger than the difference between eg a Java or C# dev, it's more like the difference from Ruby and Java (dynamically typed vs static requires a significant difference in how things should be done). If I assume that I don't have much to learn as a Java dev entering a Ruby shop, and then run into a lot of problems, I can either a) reconsider my assumption that I don't have much to learn, or b) decide that Ruby is really bad. I see the latter more often than I'd like.
Thank you so much for explaining. Like I mentioned, I don't do a lot of FP outside of Haskell so I was pretty confused by your point at first. Now, I think I have a better understanding of your view, namely, that the difference between, say Scala and Haskell, is much larger than that between conventional OOP languages, which makes a lot of sense. &amp;#x200B; As an aside, I find it interesting that a lot of people in your experience end up concluding Haskell is a "bad language" (whatever that means). Usually in my (purely anecdotal, and in no way robust enough to draw any sort of strong conclusion) experience, when someone struggles with Haskell, they end up chalking it up to the language being for geniuses with category theory doctorates.
Happy to be of help! You might also be interested in checking out /r/programminglanguages to discuss ideas :).
That's kinda what i mean by "a bad language" :) I'm a bit of an idiot and Haskell saves my bacon every day.
As a fellow idiot, I agree! Long live GHC for saving me many hours of debugging my own mistakes :)
Good read. Inspiring, yes. “Make the process of raising prices slightly less painful for yourself “ I like your optimism.
You might have better luck going to /r/The_Donald and promoting Bernie
This looks promising, though I am having trouble trying to install it, due to ghc version =[
As are a lot of other languages currently used in industry: Java, C++, C#, PHP, Python, JavaScript, Ruby, etc.
&gt; Just like Haskell's variant type emulations, but much more pleasant to use IME. I'm not a fan of Haskell's variant situation either, so them being more pleasant isn't surprising. &gt; The convention for tuple is to use a constructor like data a /\ b = a /\ b which has a fixity that makes 1 /\ 2 /\ 3 work out as a constructor and pattern, or to use a record like { fst: 3, snd: "bar", "3": [1,2,3] }. It's more common to just use an anonymous record though - the type of uncons is: Sometimes I just want a heterogenous contiguous array, and this doesn't really feel like a proper substitute. I want positional semantics, (e.g `[1, "foo"] + [2, "bar"] = [1, "foo", 2, "bar"]`, `delete 1 [1, "foo", 2] = [1, 2]`). Replacing Servants `&lt;|&gt;` with a combination of positional Sums and Products is one example of a good use-case for this where you genuinely do want positional semantics and don't want to deal with names or name collisions. Dealing with CSV's or other external formats that are positional is another good example. Another would be if you want to do first class manipulation of custom control structures, as opposed to manipulating "data", where you may want to store it as a heterogenous sequence of instructions of varying types. &gt; There's not an array backed type, though you could make one with unsafeCoerce and PureScript's type level programming facilities (just like Haskell's). I would like for the language to be able to provide the right set of core components to avoid library code needing to mess around with unsafeCoerce for things as essential and universally useful as variants. Having it done at the library unsafe* level also implies that the compiler is going to have a harder time optimizing it. &gt; Yes, records have a straightforward Generic-based JSON encoder/decoder. Hmm I was very much not expecting to have to use `Generic` for something like this. Part of why I want everything to be defined in terms of structural types like records and variants, and newtypes, is because it should obviate the need for `Generic` or even the built in deriving mechanisms entirely. Everything of that nature should be doable with something equivalent in power to `GenericNewtypeDeriving`. &gt; I'm unfamiliar with this approach - what benefits and advantages does it have over PureScript? It's more unified and less magical/hardcoded. All values are types, all types are values. `Row` has type `Type -&gt; Type`, `(foo :: Int, bar :: String)` has type `Row Type`. `Row` is just a homogenous data structure like `Array` and all the value level functions you can call on it also work on the type level. The benefit is less obvious if you narrow your focus to just `Row`, as when people hear `Row` they assume it's at the type level, but if you look at building positional Sums and Products out of `Array` it's more obvious why you want everything to be treated the same at both the type and the value level.
&gt; This seems like an odd complaint. purescript-variant is a library, nominal types like FooBar are supported directly by the language. But nominally typed sum constructors have little to no value to me. Nominal types as a whole / nominally typed newtype constructors and destructors absolutely have value and I do want those. If you have nominally typed Sums or Products then you have to have things like Generic / Data / built in deriving mechanisms as you can no longer just rely on GeneralizedNewtypeDeriving and direct structural programming to handle everything. You also need a hardcoded-into-syntax notion of pattern matching instead of the possibility of it being a standard library function (`(=&gt;) : Variant r -&gt; Record (map (-&gt; a) r) -&gt; a`). &gt; would you rather the language did not support adding new types that could possibly replicate existing built-in types? Or are you arguing that open variant types ought to be built-in? Because that wouldn't be compatible with the goal of not adding unnecessary features to the language. By fully committing to having Rows/Records/Variants as builtin Types and telling users to build everything on top of that, you actually simplify the language substantially. You can ditch so many different extensions and features from `NamedFieldPuns` to almost every `Derive*` to `Generic` to `Data` to `DuplicateRecordFields` to the built in deriving mechanisms. It's also worth noting that "not adding unnecessary features to the language" is not really a fair assessment, because basically every functional language including PureScript DOES bake in variants, just typically only the nominal kind unless you are [Expresso](https://github.com/willtim/Expresso). If you want to support both nominal and structural typing without having an overly complex language, you need to minimize the complexity and overlap of them. With good structural typing the only nominal typing feature you need is `newtype` and the ability to smoothly interact with it. Structural typing realistically needs a healthy set of primitives (Array, Row, Int, Symbol, Record, Variant, Product, Sum) to avoid lots of library level unsafe*, and to keep things contiguous (HList is not a real solution), as I can't really think of a way around that.
"Has-kell" ಠ\_ಠ
You're probably looking for \`&lt;$&gt;\` and \`&lt;&amp;&gt;\`
The infix version of `fmap` is `&lt;$&gt;`: &lt;$&gt; :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b &lt;$&gt; = fmap I.e. evens = (*2) &lt;$&gt; [1..] allCaps = toUpper &lt;$&gt; getLine To chain many applications together we need `Applicative` and its equivalent `&lt;*&gt;` &lt;*&gt; :: Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b For example type Result = Maybe String query :: String -&gt; Result query = ... getName :: Result -&gt; Result getName = ... uppercaseName = toUpper &lt;$&gt; getName &lt;*&gt; query "foo"
There are people working on it :) and it’s an interesting and frustrationing problem. It shouldn’t be though.
To catch this at compile time add ‘-Wall’ and ‘-Werror’ to your ghc options.
Thx! `&lt;&amp;&gt;` is exactly what I want.
I am building a scheme interpreter for my second haskell project. The project has come to a decent stage and I'm currently writing tests using \`tasty\` and \`tasty-hunit\`. I have never written tests before so I would like to get others opinions on whether I'm testing the right things. Please take a look at my tests: \[haskeme/test/Spec.hs\]([https://github.com/abiduzz420/haskeme/blob/master/test/Spec.hs](https://github.com/abiduzz420/haskeme/blob/master/test/Spec.hs)) Any kind of suggestions/advice are welcome. Thanks in advance :)
`&gt;&gt;=` is not a build-in operator but a member of the `Monad`-typeclass from the package `Control.Monad`. The functor typeclass provides `fmap`, which can be used infix and `&lt;$&gt; = fmap`.
The question at hand was how relevant FP language experience outside of Haskell is for working in Haskell. This is different from the question you seem to be interested in, as you keep arguing that OOP patterns don’t show up in Haskell. Great job mate, but in case you didn’t figure it out, Haskell was invented to avoid OOP patterns. I really don’t understand why you have such a hard time understanding this. Another set of patterns irrelevant to Haskell are CBM based design strategies for Prolog, but who cares - we aren’t talking about that.
Beginner here, I'd suggest you go through [CIS194 UPenn Intro to Haskell course](https://www.seas.upenn.edu/~cis194/fall16/)
[removed]
&gt; PureScript's FFI doesn't really seem that different to GHCJS's. PureScript’s FFI is a vastly superior user experience than GHCJS’s. We’ve used both at work. It’s not just the syntax, but the fact that there is no runtime mismatch, which requires boilerplate in GHCJS (think of callbacks) and is transparent in PureScript, and types such as Strings and objects/records are shared without serialization. It’s not even close.
Note that `&lt;&amp;&gt;` was only added in a fairly recent version of the `base` library (version 4.11), so if you're using an older version of GHC you may have to define it yourself. Personally, I find `&lt;$&gt;` to be much more useful than `&lt;&amp;&gt;`. The former is probably my favourite Haskell operator (if such a thing exists) — there's something very satisfying about being able to do `f &lt;$&gt; datatype` just as easily as `f $ value`.
You might be aware of [GHCJS](https://github.com/ghcjs/ghcjs/blob/ghc-8.6/README.markdown) already. (it's not wasm but you used "miserable front-end developer") The main problems detailed [in this reddit post](https://www.reddit.com/r/haskell/comments/5ydqrc/compiling_to_webassembly/) are still valid and unresolved. Namely, no garbage collection per se (which is why rust shines at wasm), no tail call optimisation, no (simple) way to have haskell RTS. So yes, it looks like Asterius is the only to have chosen an _opiniated_ way of solving these ([forking ghc](https://tweag.github.io/asterius/custom-ghc/)).
I was miserable when I used GHCJS.
2 cents : I wasn't able to run any serious example with Asterius. Even the docker image does not work well on my laptop. If you find any alternative I'd **love** to know them !
The /proc problem with ghc is also easy to encounter when working with chroots. I've run into it several times in different situations. https://bugs.debian.org/787227 https://bugs.debian.org/773768
I agree that in most cases `(&lt;$&gt;)` is more readable, but `(&lt;&amp;&gt;)` is rather useful when using in conjunction with `LambdaCase`: imho `(\case {..}) &lt;$&gt; mx` is less readable than `mx &lt;&amp;&gt; \case {..}`
It is in `base`, so I'd say it is built-in (considering it even has special syntax support via `do`-notation, even more so): :i (&gt;&gt;=) class Applicative m =&gt; Monad (m :: * -&gt; *) where (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b ... -- Defined in ‘GHC.Base’ infixl 1 &gt;&gt;=
Right, `Control.Monad` does just a re-Export. I didn’t know that.
wasm-cross is still maintained, it just doesn't have the same funding or resources as Asterius.
FYI, your third expression: ``` map (++"!") . map show . map (+1) $ \[1,2,3,4,5\] ``` …is the same as: ``` map ((++ "!") . show . (+1)) [1, 2, 3, 4, 5] ```
(supervisor here) It is definitely related to structure reuse, but not necessarily within HoTT. One can get surprisingly far just syntactically; of course, since the setting is MLTT (and Agda as prototype), I say 'syntactically' a little bit tongue-in-cheek!
Yea we're definitely still making progress. I've got a couple things to make a blog post about soon. I do need to just do some housekeeping on wasm-cross though.
No, it isn't. The second one says `Free f` is a `Functor` regardless of what `f` is, while the first one says `Free f` is a `Functor` as long as `f` is a `Functor` (which turns out to be the case, as you'll find when you try to write `fmap` for `Free f`).
Awesome, thank you!
I think you're missing brackets around `Free f`. Once you put those in, the first one is saying "I can provide an instance of `Functor` for `(Free f)` if I'm given an instance of `Functor` for `f`" and the second one is saying "I can always provide an instance of `Functor` for `(Free f)`". So they're two different things - the first one being more limited than the second. If you look at the implementation [here](https://www.stackage.org/haddock/lts-13.23/free-5.1.1/src/Control.Monad.Free.html#line-208): instance Functor f =&gt; Functor (Free f) where fmap f = go where go (Pure a) = Pure (f a) go (Free fa) = Free (go &lt;$&gt; fa) it utilizes the functor instance on `f` in the `&lt;$&gt;` call, so you cannot get rid of the requirement that `f` have a `Functor` instance.
Asterius main dev here. Just fyi we do have garbage collection now, and we perform optional switch to use the wasm's native tail calls to replace trampolines :)
the day youtube adds audio filter/correction options will be a worldwide relief
&gt; for the miserable haskell front-end developers? The title says "haskell to webassembly" though, and imho the webassembly landscape is much broader than just frontend development. Having webghc &amp; asterius to compete and explore different approaches is beneficial in the long run since they might fit for different scenarios. Asterius takes an approach similar to ghcjs (take a ghc IR and start compiling from there, custom runtime, etc), and has first-class JS interop experience, generates smaller code, which makes it more suitable for the frontend scenario. WebGHC uses an external toolchain to do cross compilation, and the generated wasm code is more self-contained, which make it more suitable when you don't need to care about code size, and the target wasm runtime only provides a fixed ABI set (emscripten, wasi, whatever) instead of full js capabilities. CDN providers are investing into such wasm runtimes for edge computing heavily. The status of asterius is available as weekly reports, and since last announcement we got gc support, Cabal support, plus a lot of bugfixes; we're using the ghc testsuite to discover and sweep up the unimplemented dark corners right now! I'll be at Zurihac this year and would love to have discussions in case you're there too :)
Scala, Java, Javascript, Python, and C++ have all claimed both the FP and OOP titles from time to time.
Would it be fair to say that instead of using a checking judgement ("does the body type-check under the given signature"), there is some kind of inference + subtyping check going on ("what is the inferred type, and is it a supertype of the given signature"), which is messing things up? Also, does the special casing for "if there's one constraint" mean that this breaks when more things are added? E.g. would the following type-check or not - {-# LANGUAGE OverloadedStrings #-} f :: (Member (State Int) r, Member (State Text) r) =&gt; Sem r () f = modify (+ 1) . modify (&lt;&gt; "!!")
I got a job at a “Haskell shop” several years ago having maybe 1-2 yrs experience with Haskell, mostly on personal projects. I did also teach a short survey course on programming languages that included Haskell, but I don’t think that was significant in the hiring decision. Since then I’ve seen many developers with little Haskell experience get jobs at similar places (all of these folks were enthusiastic about Haskell, willing to learn, and were generally very good, well-rounded software engineers). So that’s all to say, I don’t think it’s hopeless. Being open to relocation helps, and so does keeping an eye out for opportunities (see the Haskell Weekly News, https://functional.works-hub.com, etc...) Also, I guess it goes without saying, but don’t assume it’s going to be an awesome job just because they use Haskell. I’ve experienced the opposite more than once.
Code a project at night in Haskell that people use. Charge people for it. Say you own a company using Haskell. Name yourself CTO. Profit. &amp;#x200B; Now you don't need to break into the market because you made yourself a job.
In "hasql" we use them to compose the encoder of parameters of a statement: [http://hackage.haskell.org/package/hasql-1.3.0.6/docs/Hasql-Encoders.html#t:Params](http://hackage.haskell.org/package/hasql-1.3.0.6/docs/Hasql-Encoders.html#t:Params)
Great point "don’t assume it’s going to be an awesome job just because they use Haskell"
\&gt; I also understand that most companies using this technology are smaller meaning everyone needs to be senior level devs that need to execute and whom are not sill on the learning path. &amp;#x200B; I don't think this is the case anymore. At all but the smallest shops, slots definitely open up for jr-mid haskell devs. Additionally, there are many polyglot jobs that have some scala, some haskell, etc., or perhaps some ops and some haskell, or soforth. If you're looking for "haskell and nothing but" you're going to overly constrain yourself.
your profit product link?
"slots definitely open up for jr-mid haskell devs" any links that prove this statement?
Consider that employers may be looking for "Senior-level dev who uses Haskell" and not necessarily "Senior-level Haskell Developer" (even when they may say they're looking for the latter.)
From some brief research, it looks like llvm8 can emit wasm. Hypothetically, wouldn't this allow the ghc llvm backend to compile to wasm? (With some hand-waving and magic sprinkles)
I'm working on one now! Message me in a month when it's done and a year when I IPO and live like Warren Buffet.
Your example doesn't typecheck; the `State String` bit would infer correctly, but the numeric one wouldn't. This is not a fundamental limitation, instead you check "is there exactly one given that this wanted can unify against" --- I just haven't gotten around to programming it yet!
Apply to SimSpace. We have hired several people in your situation and it worked out well.
I don't think this is a real problem. Haskell shops know that there aren't a huge amount of wicked-dope professional Haskell programmers in the wild. I made my way into my first job knowing not much more than how `StateT` works --- but I had an awesome non-Haskell resume proving I knew what I was doing. I suspect if you're applying to hs jobs with 2+ years of hs experience, and not getting any bites, the problem is more likely with you.
Discussion of a previous attempt (sadly, never finished): https://www.reddit.com/r/haskell/comments/3rbpb6/examples_of_warnings_in_haskell/
Oh crap.
Thank goodness!
This very sad. I use hse all the time.
Speaking the truth.
I don't think the MonadError type class gives you enough information to implement this in general. I think you'll have to work specifically at the ExceptT level, or whatever specific transformer you are using. Specifically, you'd need to know what "m1 without e1 errors" is as a type, because *that's* what you need to project to m2 in the non-error case.
Love me scoped tool directives. :)
&gt; Work a regular dev job and then spend every evening practicing haskell for 5 years. Not many have the stamina for that. I don't have a Haskell job (yet?). I do Java, Python, C/C++, JS. But, this is the approach I sent with; not regretting it. I do Haskell when not all work for 10 or so years now.
Currently, GHCJS has a few problems. * Compilation consumes a lot of CPU and RAM resources. * It does especially when template haskell is involved. * It is slow on smartphones. Server-side rendering helps a bit, but it is not enough. * Only a small subset of haskell functions can be exposed to javascript. * For example, A haskell function with 11 arguments cannot be exposed to javascript. * The compilation output is large and slow to load. Is wasm-cross or asterius going to solve those problems satisfactorily?
Fntastic. Now please transform &amp;#x200B; &amp;#x200B; main= do name &lt;- getLine print ("hello " ++ name) &amp;#x200B; &amp;#x200B; To something declarative
I have two ways of doing this. 1. A monad transformer to adapt `MonadError` instances. Given some application monad `m` (with application-wide errors), transform it into a business monad `BusinessT m` (with business-specific errors) in which to run the `businessFunction`. The two key instances are: instance MonadError ApplicationError m =&gt; MonadError BusinessError (BusinessT m) instance BusinessRepo m =&gt; BusinessRepo (BusinessT m) 2. A type class to convert between monads with different error types. This is an adaptation of [`Zoom` and `Magnify` from lens](https://hackage.haskell.org/package/lens-4.17.1/docs/Control-Lens-Zoom.html) to `MonadError`. https://gist.github.com/Lysxia/f24b6ef11e8f198399cbcd8106bfb1a6
Is this a "Matthew is burned out" thing, or an "everyone should move to the GHC API based stuff anyway" thing?
The latter.
Why?
So I changed Add to be `Add :: (Show s) =&gt; (s, v) -&gt; Expr ss v -&gt; Expr (s:ss) v` to try to use `fmap show` for `k` but it's still yelling at me. How should I go about doing this?
Yes, that's true. I think it's just that I don't run into that particular situation that often. (On the other hand, I do use `mx &gt;&gt;= \case {..}` all the time, so maybe I'm just not noticing when I need a functor instead of a monad.)
These are really interesting. It's going to take me a while to digest these. Thank you. Two quick questions though: 1. Do you have recommended literature for me I can use to learn more about the lens library in the way you're using it here? 2. Would it be impossible to just run the whole application in a monad stack similar to the following? newtype MonadApp a = MonadApp { unwrapApp :: forall e AsApplicationError e =&gt; ReaderT AppContext (ExceptT e IO) a}
I feel like you're not actually paying attention to what I say. Of course that should be monadic. &amp;#x200B; I've taught Haskell to several people before. I used to introduce monads with the State monad (although more recently I've been using the Except monad instead, because...) and I've literally seen newbies write code as "contrived" as that. So yes. In particular I think they were trying to implement reverse. &amp;#x200B; Usually we spend about 2 hours getting introduced to the syntax of the language and implementing some of the "declarative style" functions from the Prelude. Then I try and present a problem that creates very redundant/messy code and show how the pattern can be abstracted with a monad and the bind operator (state/except like I said above). Then we talk about IO. In about 3 hours we get to monads. I'm not saying it should be very delayed; as I said above I do think they belong very early.
Pretty cool how nobody seems to care about backwards compatibility with GHC &lt; 8.4 any more.
Very much this. Finding Senior devs who are interested in Haskell is much easier than finding Seniors who have had a career in Haskell. Freckle has hired a number of Seniors who have had no professional Haskell experience.
simsspace just literally posted a job only asking for 1yr experience https://twitter.com/shajra/status/1132418942889287680
You could volunteer to take the package over. Also: while I've had use for old Haskell compilers, I have basically no sympathy for companies using old GHCs
You really have no idea what you are talking about, huh.
Try `Control.Monad.OnError.mapError` from my [`on-error`](https://github.com/simspace/on-error) library. It is designed to turn a polymorphic action written in the mtl style with one error type, e.g. myAction :: (MonadError E1 m, MonadFoo m, ...) =&gt; m Bar and to turn it into another action with the same constraints, except with a different error type: mapError (f :: E1 -&gt; E2) myAction :: (MonadError E2 m', MonadFoo m', ...) =&gt; m' Bar It works by instantiating `m` to `ExceptT E1 m'`.
I can only speak for myself. But I joined a startup that had no engineering team when I was at this stage. I eventually just wrote a service in it without permission. No one is gonna tell you to take it down if it’s working. That kept compounding til a solid 40% of our backend was Haskell. Started simple but we’re putting out a service with DTs this month. Just takes a bit of confidence to get started and grit to stick it out. Best of luck.
Hopefully this will help improve the ergonomics of the GHC API.
Is your code on older versions open source? You could ask if somebody can help you with migrating to a newer GHC version...
The convention for most things I've seen is to support three major versions. With ghc 8.8 coming out soon, that puts 8.2 out of reasonable support range.
Warren Buffett is a notorious penny pincher who clips coupons, lives in a modest home and buys used cars, or is that the joke?
I think an additional assumption is that the base types are used to define a set of composite types through algebraic operations like product and sum (coproduct) with specific properties (e.g. associative, commutative). As far as I understand, the 'algebra' in (G)ADT refers to the composition properties.
It's more fun. I learn more. Also, I haven't really looked that hard for a Haskell-specific job. I prefer not to move to a more expensive part of the country, and I enjoy the people I work with, if not all the technologies we deploy.
Please consider if your communication is respectful.
 GHCi&gt; :{ GHCi| unwrap :: Expr l v -&gt; (forall s. Show s =&gt; [s] -&gt; r) -&gt; r GHCi| unwrap Empty k = k [] GHCi| unwrap (Add (s,_) _) k = k [s] GHCi| :} unwrap :: Expr l v -&gt; (forall s. Show s =&gt; [s] -&gt; r) -&gt; r (0.00 secs, 0 bytes) GHCi&gt; unwrap Empty (fmap show) [] it :: [String] (0.00 secs, 59,816 bytes) GHCi&gt; unwrap (Add (1,2) Empty) (fmap show) ["1"] it :: [String] (0.00 secs, 64,056 bytes) You should change your unwrap to allow the passed in function to see the `Show s` "Constraint".
No, that's not the only difference. Consider that data family D (x :: Type) :: Type data instance D Int = DI Int and data D :: Type -&gt; Type where DI :: Int -&gt; D Int both create the function DI :: Int -&gt; D Int They also share the "different data constructors for different type parameters" property. A note on jargon; we sometimes like to separate *parameters* from *indices*. In `data D a = D a`, `a` is a parameter, because the definition of `D a` is "the same" for all `a`. In `data family D (a :: Type) :: Type`, `a` is an index, because each `a` can produce a wholly different definition for `D`. It is therefore more accurate to say that GADTs have the property that there can be different sets of data constructors for different type arguments; we name the arguments that cause the changes "indices". Back to my counterexample, for the data family, this is illegal: d :: D a -&gt; Int d (DI i) = i While it *is* legal for the GADT. So "different data constructors for different values of the indices" doesn't cut it. Instead, think of it this way. Normal data types only allow you to store values. data D a = D a The only difference is that a GADT lets you store two more things: types and contexts. View `data D :: Type -&gt; Type where DI :: Int -&gt; D Int` as syntactic sugar, for data D a = a ~ Int =&gt; DI a -- DI :: a ~ Int -&gt; a -&gt; D a When you construct a `D a` with `DI`, you have to provide a value of `a`, and you also have to "store" the fact that `a` is actually equal to `Int` (`a ~ Int` is a constraint, just like `Eq a` or `Ord a`). If you *have* a `D a`, then you know that `DI` is a possible constructor, by the definition. You can then match on the `D a` without knowing what `a` actually is; if it matches `DI`, then you extract both the `a` value and the fact that `a` is equal to `Int`. In the above definition of `d`, the fact that `a` is equal to `Int` is combined with the `a` value to cast it into an `Int` value suitable for return. The data family version of `d` fails, because if you have a `D a`, you don't have an enumeration of its constructors, so it's not safe to match on it. Because a GADT is so similar to a normal data type, just able to store more interesting things, all GADTs can be written in the normal data type syntax, just with the correct extensions. data Elem (x :: k) (xs :: [k]) :: Type where Here :: Elem x (x : xs) There :: Elem x xs -&gt; Elem x (y : xs) is the same as data Elem (x :: k) (xs :: [k]) = forall xs'. xs ~ (x : xs') =&gt; Here | forall y xs'. xs ~ (y : xs') =&gt; There (Elem x xs') IMO this is a lot more "honest" than the GADT syntax (which I am not really a fan of), but sometimes the truth is ugly, so you don't really want to write this. --- I don't know what &gt; * The output of each type constructor is the data type itself. &gt; * The images of the type constructors are exactly the equivalence classes of the data type. mean—could you clarify?
[We are hiring](https://old.reddit.com/r/haskell/comments/bg3p72/job_interos_is_hiring_haskell_developers/) for a range of skillsets in Haskell.
Your right. More like Melnichenko.
i stopped respecting you when you showed through multiple posts that you have no idea what you are talking about and just want to spread misinformation
&gt; What are you doing in the Haskell sub I've been subscribed longer than you've been a redditor. I'm not a founder of the subreddit, but I've been doing Haskell since 2009, nearly as long as there's been a Haskell subreddit.
this is super cool, thank you for posting
Is this strictly for US citizens because of security clearance requirements or are permanent residents considered too?
Ok, this is quite bizarre. When defining things as you say in GHCi, everything is fine. When I go to put them into a file and compile the file, then I get a compiler error of • Could not deduce (Show s0) arising from a use of ‘k’ from the context: l ~ '[] bound by a pattern with constructor: Empty :: forall v. Expr '[] v,
Sweet!
The delta between it and GHC kept increasing all the time. We wrote ghc-lib-parser to eliminate that problem, while keeping all the advantages of a separate version independent library. It still makes me sad though! I've spent a lot of time in HSE.
GHC HQ only supports back to 8.4, and ghc-lib-parser inherits that restriction - we'd love to go further back if we could. It's probably not that hard to patch GHC to build with 8.2, and then ghc-lib-parser would pick it up for free.
Where are you located?
While not specifically related to haskell-src-exts I do have one or two OSS projects related to haskell.org infrastructure which might benefit from being compilable with GHC &gt;= 8.4 and it'd be great if somebody would volunteer to take it upon themselves help with the thankless job of migrating them... :-)
Then I guess I'm nobody as I for one still [try to support GHCs back till GHC 7.0 as it's usually quite easy](https://old.reddit.com/r/haskell/comments/bk3sjl/library_support_for_older_compiler_versions/emjrtb3/) to not be ageist about GHC versions
Can you post it on the mailing list/cross post it here?
&gt;Then I guess I'm nobody as I for one still try to support GHCs back till GHC 7.0 as it's usually quite easy to not be ageist about GHC versions. Apart from GHC HQ itself, which is very ageist, which is why ghc-lib-parser is ageist.
How large is the runtime part in the wasm binary approximately? I've generally been an opponent of a leaner approach maybe even using the wasm GC infrastructure if wasm provides one at some point. Also using direct compilation to wasm like asterius does. But asterius also provides an additional runtime written in javascript or typescript (e.g. the GC) and I am not particularily fond of that. I've recently started to appreciate the WebGHC approach where a relatively fat runtime is included within the binary, since this allows for simpler wasm vms with much smaller attack surface. There are even projects embedding a wasm vm inside the Linux kernel etc.
This is WebGHC.
Here's a few dollar idea for someone with some free time: Either pick up support for HSE or make a reasonable effort at getting ghc-lib-parser documentation to sensible level. Then, set up patreon/github sponsor link. &amp;#x200B; If this would happen I'd drop my netflix subscription and spent the money here instead. :)
&gt; grin-tech now renamed to: https://github.com/grin-compiler/grin
So far my favourite part is: &gt; Category Theory is a branch of Mathematics mainly used by haskellers to annoy everyone else
I found this today while randomly browsing GitHub packages and looks amazing. What are you opinions about it?
The ghc-lib-parser docs are those of GHC, so document the GHC API and it comes for free and benefits lots of people.
Very nice package! I always find it more awkward to work with Higher-Kinded data types, because it's less convenient to write instances due to the `f` type parameter. It's very lovely to see the approach where you can define simple plain Haskell data type and convert it to HKD version externally. I also see that the example heavily uses the `Last` monoid. type Partial a = HKD a Last -- Fields may be missing. I learned recently that this `newtype` is going to be removed in future `base` version ([and I have some concerns about this decision](https://gitlab.haskell.org/ghc/ghc/issues/15028#note_197730)). I'm afraid that HKD approach will become less useful and powerful without the `Last` and `First` monoids. Could we do something about that as a community?
I think GHCi has some extended defaulting rules that normal compilation does use. Try `k ([] :: [()])` as the RHS for the `Empty` case. That's still a weirdly uninformative error.
It's certainly older than SO, due to u/dons? He refers to it as an already well-known thing on [his blog](https://donsbot.wordpress.com/2007/03/10/practical-haskell-shell-scripting-with-error-handling-and-privilege-separation/) in 2007.
I have the feeling that executables produced by GHC are quite large from time to time. Is there some tool to find out which libraries contribute the most to the size?
These guys are awesome. David gave us an invited talk at Uber Palo Alto a few weeks ago, and Ryan has been helping me formalize a data model for Uber and Apache TinkerPop, motivated in terms of Category Theory [here](https://www.slideshare.net/joshsh/a-graph-is-a-graph-is-a-graph-equivalence-transformation-and-composition-of-graph-data-models-129403012) and also described [here](https://groups.google.com/d/msg/gremlin-users/MPNdNOAnYbs/sbowvioiDgAJ). I think their push for CT in industry is very timely.
Real World Haskell (co-authored by dons around that time) also has a ["Monads as a programmable semicolon"](http://book.realworldhaskell.org/read/monads.html#id642960) subsection.
&gt;The output of each type constructor is the data type itself. Each data constructor the (G)ADT has to return an value of the type that it's a constructor of, so like, you can't have data Cat where Something :: String -&gt; Dog &gt;The images of the type constructors are exactly the equivalence classes of the data type. Every instance of an ADT corresponds to exactly one of its constructors. That's my guess of what it means for ADTs to be "closed". I guess "partition" is a better word than "equivalence class". &gt;The only difference is that a GADT lets you store two more things: types and contexts. Yes, but that information can be inferred directly from the closed assumption, can't it? The types are all still totally erased, so "storing types" doesn't seem like a good way to put it (total Haskell noob though, so I'm probably wrong). &gt;IMO this is a lot more "honest" than the GADT syntax (which I am not really a fan of), but I'm told the truth is ugly, so you don't really want to write this. Interesting idea. That `~` thing is something like data Equals :: * -&gt; * -&gt; * where Reflect :: Equals a a right? So whether you consider GADTs or equality to be more "fundamental" is just a choice.
This is almost oldest literature could found at Google
I have no idea -- its not my listing. Why don't you contact them and ask?
https://github.com/gibiansky/IHaskell
I would search the #haskell irc archives too, I feel like that was a common way of attempting to explain monads in those days.
Is the problem that you want to continue maintaining backwards compatibility for older GHCs with newer versions of IHaskell?
Storing types is how it's implemented. GHC Haskell is converted to a more basic language, called Core. In Core, types and contexts are are stored inside GADT constructors and extracted by `case`: data Nat = Z | S Nat data IsS (n :: Nat) :: Type where IsS :: IsS ('S n) prf :: forall (p :: Nat -&gt; Type) (n :: Nat). (forall (m :: Nat). p ('S m)) -&gt; IsS n -&gt; p n prf isS IsS = isS In Core, `prf` looks something like this (I'm doing this from memory, sorry): prf @(p :: Nat -&gt; Type) @(n :: Nat) (isS :: forall (m :: Nat). p ('S m)) (arg :: IsS n) = case arg of IsS @(m :: Nat) (eq1 :: n ~ 'S m) -&gt; isS @m `cast` eq1 Type erasure is an implementation detail. Do not use it as a reasoning tool. After all, *all* types get erased; there's no difference between `[]` and `False` at runtime, so why should we treat them as different? "Type erasure" is, however, sometimes used to refer to the rule "types cannot be matched on", which *is* a valid reasoning tool.
Type literals for tensor dimensions are also fundamental to the [hasktorch](https://github.com/hasktorch) project (u/austinvhuang et al.)
I have support back down to GHC 8.0 at the moment, and it's not great that my options are either to maintain some nightmarish CPP monstrosity or drop support for GHC 8.0 and 8.2.
Yes as well as the blog post there a couple of messages on haskell-cafe mentioning it in 2007. RWH was online in draft form at the same time...
Great explanation! Just goes to show that types aren't enough documentation by themselves. (I think adding this to the Hackage docs would be good pull request, I certainly didn't understand the class until reading this)
\&gt; Typeclasses in Haskell feel like "OO bolted in" &amp;#x200B; They don't.
Thanks! I now remember that the example with reverse that I wrote was probably his answer on SO (but still didn't find since there are so many answers of his :p).
`strip` it.
No, this is not what I asked. I know about strip and the binary is already stripped.
If you have a value of type IO ByteString, you can fmap over it with a pure function of type ByteString -&gt; MyType. While the result is of type IO MyType, you keep IO out of the parsing.
Recently I started diving a bit deeper in the dependently-typed functionality empowered by working with types of a different kind than 'Type'. This made me wonder: Have you ever encountered the need to go higher than 'kinds'? I know it's _possible_ and that languages like Agda, Coq and Idris IIRC allow you to go arbitrarily 'high', but when is this useful in practice?
&gt; Does this violate the purity concept? Your intuition is correct: it does not. With most applications, especially the ones of the kind you are intending to create, the idea is as follows: 1. the application reads some input. (This requires interaction with the outside world). 2. the application performs some meaningful calculation/transformation of the input. 3. the application shares its results with the user (This requires interaction with the outside world). ```haskell myprogram = do input &lt;- readSomeInput let result = findTheAnswerToLifeTheUniverseAndEverything input writeSomeOutput result ``` While the first and third step require you to run in some `IO` context, the middle step does not. The more you manage to keep these three subsections separated, the easier it will be to comprehend, test, refactor and modify the middle step. Also, the smaller/simpler you manage to keep subsections (1) and (3), the larger the easily-testable/refactorable part of your code will become.
I didn't look at your code, but I normally approach testing as a way to totally convince myself (and my users , eventually) that the code I wrote is correct. So that means I'm trying to break it, and if there's part of my code that's particularly hairy I will focus on that more. I'll do what I need to build confidence including factoring out parts of functions for the purpose of testing, or adding assertions to functions to check invariants which I'll try to excercise in other tests. i.e. a paranoid and creative endeavor that is often necessarily messy in some ways
Not being able to play with this right now i have a question. In the readme all the “user” hkd examples are commented, does this mean that they are just logically the hkd form? Seems they must be. I dont like that a little because you dont get to work directly with your data type. Sure its nice not to have to make standalone derivings, but that only has to be done once and i like to work with my data throughout my code. On the other hand the approach is really cool in that it allows any data to be treated as a hkd. Interacting through a generic lens or monoid/applicative interface isnt a big ask. I think ill give it a try. Thanks.
I deeply believe that this idea is due to Phil Wadler. But I failed to find the proof.
\`MultiParamTypeClasses\`, \`FunctionalDependencies\`, \`FlexibleInstances\`, \`AmbiguousTypes\`/\`TypeApplication\` (when you chain two typeclass functions with forall types), \`TypeFamilies\`, \`import Foo.Bar.Baz.Instances ()\` and other parts of the language make me understand that typeclasses are not as frictionless as the rest of the language. Typeclasses are well thought, well engineered, well understood with properly reviewed papers, and very useful; but that doesn't change the fact that for any minimally-advanced program using typeclasses we have to enable lots of extensions and hope we don't get ourselves in an overlapping-instance mess. Maybe "OO" could be overreaching it, as there are great ways to use typeclasses without getting yourself into the usual OO AbstractFactoryFactoryPrototype hell, but at the same time, you can get many of the benefits of typeclasses without trying to get yourself into subtyping by going, for example, with [http://www.haskellforall.com/2012/05/scrap-your-type-classes.html](http://www.haskellforall.com/2012/05/scrap-your-type-classes.html) .
Thanks, all of what you wrote makes perfect sense and i totally agree, it does make things a lot more maintainable. It's something i like about Haskell in general, code is short and *if* you can read it, it's actually pretty readable.
I saw it once credited to Simon Peyton Jones, again without proof.
Nice! With fetching of dependencies submodules stack and nix are now consistent with each other and no longer require custom setup scripts for this!
Not sure how representative this is, but you could try (e.g. with stack and ghc-8.6.3): cd .stack-work/install/x86_64-linux/lts-13.0/8.6.3/lib/x86_64-linux-ghc-8.6.3 ls -lh
I've been using the git version of `stack` since about a week ago and it's great. Much faster compilation for rebuilds and I like the new logging output.
I think the post tries to make a sort of profound conclusion connecting the two paradigms and in doing so it distorts what type classes are in Haskell. A type class is a means to establish a correspondence between certain group of types (instances of type class) and one or more values (i.e. methods) in a unique way, meaning that no matter how you obtain a dictionary you'll end up always with the same dictionary. Which is nice indeed and enables a form of polymorphism in the language. But what is the connection to OO? Type classes in Haskell are rather like interfaces, not like classes in OOP. Further, the fact that we can have super-classes is not equivalent to sub-typing in OOP, so I fail to see the parallels. &gt; typeclasses are not as frictionless as the rest of the language Various features of Haskell present varying degrees of "friction", but I'd say that type classes are not really problematic and if you follow a few simple rules you should not end up in "an overlapping-instance mess". ---- Finally, I'm also not sure the notion of co-data is used correctly here, but I lack training in category theory to really say for sure. IIRC, co-data is co-inductively defined data (as opposed to data defined through structural induction), that is something like infinite data structures and streams. Data is eliminated by recursion and co-data is constructed by co-recursion. Something like that, it appears to be a totally different type of duality. What you have seems to play remotely with sum vs product duality, in the beginning you have a sum type with two injections `Foo1` and `Foo2` and later you have them as projections and claim that these are sort of "methods". ¯\_(ツ)_/¯
For me, the "tabulation" approach is more or less subsumed by "memoization". We can memoize pairs consisting of optimal scores and the associated solutions, and by keeping the second component lazy, we can ensure that we only pay the cost of what's needed to reconstruct it. Operationally, this behaves quite like the tabulation approach. However, a naive use of memoization in this way is still going to store the pair, so even if most second components are never evaluated, they will still be allocated, costing some constant factors. Then "tabulate and reconstruct" can be seen as an optimization to work around that. # Memoization The essence of a dynamic program is a function `dyn :: (a -&gt; b) -&gt; (a -&gt; b)`: given a (partial) solution function `f :: (a -&gt; b)` and a "problem" `x :: a`, `dyn f x` calls `f` on "subproblems" of `x`, and uses their solutions to reconstruct the solution for `x` itself. Then the actual solution function is given by `fix dyn`, and what memoization does is to provide a more efficient implementation of `fix` by exploiting the structure of the problem space `a` (and possibly the particular `dyn` as well). The usual example is to store results in a matrix when `a` is `(Int, Int)` or really any product of integers. Also worth mentioning is the [MemoTrie](https://hackage.haskell.org/package/MemoTrie-0.6.9/docs/Data-MemoTrie.html) library, which implements memoization for any ADT `a`. # Tabulate and reconstruct The first phase of the "tabulation" approach can be seen exactly in the same way, as "memoization" of a simplified dynamic program, computing scores without their solution. But as I said earlier, we can actually also fit the other phase, the "backward pass", into this framework, with a dynamic program to construct lazy pairs `(n, s)` of scores and solutions: `dyn :: (a -&gt; (n, s)) -&gt; (a -&gt; (n, s))` (for example, for various subsequence problems, `n ~ Int`, `s ~ String`). Formulating `dyn` as an argument of a `fix :: (t -&gt; t) -&gt; t` function is a really neat thing, because it is decoupled from the particular memoization strategy. `dyn` doesn't care whether the subproblems are stored in a matrix, or in a trie, or reevaluated by calling `dyn` itself. In particular, we can reuse it to implement "tabulate and reconstruct" strategy: first compute and store the scores `n` only, and then derive the solution from it. ## Tabulate The first phase is really a memoization, and indeed we can convert `dyn` into `dyn_n :: (a -&gt; n) -&gt; (a -&gt; n)`. Given a "partial scoring function" `f :: (a -&gt; n)`, we can turn it into a "partial solution function" `a -&gt; (n, s)` by producing an undefined solution for `s`, which should be ignored by `dyn` as long as we only care about the `n` component. scoresOnly :: ((a -&gt; (n, s)) -&gt; (a -&gt; (n, s))) -&gt; (a -&gt; n) -&gt; (a -&gt; n) scoresOnly dyn f x = fst (dyn (\y -&gt; (f y, undefined)) x) ## Reconstruct Once we have memoized the scores `scores :: a -&gt; n`, we can actually reconstruct the solution with naive recursion (`fix`), by interposing the `score` function. This is efficient only if `dyn` builds the actual solution on top of at most one subproblem. It might take some effort to convince yourself that this does the right thing, but at least the code is only a few lines: solveWith :: ((a -&gt; (n, s)) -&gt; (a -&gt; (n, s))) -&gt; (a -&gt; n) -&gt; (a -&gt; (n, s)) solveWith dyn scores = fix (\f a -&gt; (scores a, snd (f a))) solve :: Memoizable a =&gt; ((a -&gt; (n, s)) -&gt; (a -&gt; (n, s))) -&gt; a -&gt; (n, s) solve dyn = solveWith dyn (memoFix (scoresOnly dyn)) -- where memoFix :: ((a -&gt; n) -&gt; (a -&gt; n)) -&gt; (a -&gt; n) -- is your favorite memoization function If the solution `s` needs to reuse more than one subsolution, `fix` can be replaced with another `memoFix` in the definition of `solveWith`. There is an opportunity here to use a different memoization functions from `scoresOnly`, which may still use less resources than the whole matrix of scores for example. ## Ew, `undefined` The above assumes that `dyn` is lazy: the solutions `s` of subproblems are not used to compute the scores `n`, crucially so that we can put `undefined` in `scoresOnly`. There is another trick to make that assumption explicit in the type of `dyn`, which is to parameterize it by an applicative functor to hold the solution: dyn :: forall f. Applicative f =&gt; (a -&gt; (n, f s)) -&gt; (a -&gt; (n, f s)) This way you can project `dyn` safely to ignore `s` or not: dyn :: (a -&gt; (n, Proxy s)) -&gt; (a -&gt; (n, Proxy s)) -- (a -&gt; n) -&gt; (a -&gt; n) dyn :: (a -&gt; (n, Identity s)) -&gt; (a -&gt; (n, Identity s)) -- (a -&gt; (n, s)) -&gt; (a -&gt; (n, s)) `solve`, which uses `dyn` twice, must keep it polymorphic or duplicate the argument: solve :: Memoizable a =&gt; (forall f. ...) -&gt; a -&gt; (n, s) solve :: Memoizable a =&gt; (... Proxy ...) -&gt; (... Identity ...) -&gt; a -&gt; (n, s)
I really like the model in [frpnow](http://hackage.haskell.org/package/frpnow)
I've been calling these **Models** for a while, because a data type like ``` data User f = User { name :: f String , age :: f Int , ... } ``` Is a model for a `User`, abstracting over how it's represented/put together. Not only can you use `User Identity` for complete data, but `User m` can represent depending on some environment `m`, like loading it from a database or querying a user to build it. Just about any monad is useful in place of `f`. Past one level I haven't been able to get them to cooperate fully with applicative functors, because you end up with something like a ``` data Post f = Post { body :: f String , author :: f (User f) , ... } ``` with nested occurances of `f`, and no way to `join` them together.
I would want a different type constructor for nested models personally. That would allow you to specify if it has a user, or a user ID, or nothing, or an io action to retrieve a user, etc
Yea, because it uses dimensions library. We thought about it but decided to try our own approach using Peano numbers. Also, I made a brief look at the examples and saw that they mostly use statically-defined types for dimensions. And results of dimensional transformations don't depend on the data. Maybe I am wrong due to sleepiness :-) Anyway, it would be interesting to talk about it with someone from the Hasktorch team. &amp;#x200B; Thank you.
It's not quite as pretty, but `HKD a (Compose Maybe Last)` (with `Last` from `Data.Semigroup` should be equivalent, no?
I've been on it for a few weeks now and it's been good :)
Really cool! Been playing around with the Racket implementation so will definitely check this out.
Does frpnow have the ability to race events to see which one occurs first or if they both occur at the same time?
I've waffled back and forth on that for years. `f (User f)` keeps things simple, but ``` data Post r f = Post { body :: f String , author :: r (Const User) , ... } newtype Const d r = Const d ``` Lets you say more things. But then you go down a rabbit hole were you really want a type family that lets you say something like "everything is populated down to 2 layers deep and past that there are references to ids". And `f` can represent that for you anyway. ``` data OffInIoSomewhere x = Loaded x | UnloadedDbId Int | KnownToBeNothingEvenThoughItShouldBeSomething | Compiled (IO x) ```
Yeah I actually struggled with that for a while as type literals were new to me, but managed to get runtime-dependence using proxy types. Definitely a hurdle, but surely something that can be abstracted away eventually. The hasktorch guys might have something in mind already, they were very helpful and know the space much better than I!
FRPNow style futures don't remember when the occurrences happened, so if they both happened in the past, it doesn't work. Otherwise, it can do that. It would need a different type, though: race :: Future a -&gt; Future a -&gt; Behavior (Future (These a b)) The `Behavior` is necessary because the result depends on what time you observe the `Future`. The neat observation of FRPNow is to use `Behavior` for operations that are time-dependent even if they are only time-dependent for efficiency reasons.
Just to see which of two is first: http://hackage.haskell.org/package/frpnow-0.18/docs/src/Control-FRPNow-Lib.html#first
When transferring ownership (imaginary ownership, but still) of an `IORef` to another thread, what are the options to make sure that the last written value before the ownership transfer is guaranteed to be observed in the receiving thread? If I understand the documentation correctly, I can do `atomicModifyIORef' ref (\x -&gt; (x, ()))` after the last write in the original thread, or I can do the same thing before the first read in the receiving thread. Are both options correct?
If you look at all the senior devs you've come across, how many of them are really that good? If you don't think you're at good enough level yet, then you aren't using it enough. Try to convince your team to start using it on some small projects. If you can't, I don't think you can do much except for doing your own project.
&gt;Do you mean pre-rendering on the server? Yes. Although this is optional. &gt;Do I need to learn nix? Last time I used GHCJS, I didn't like nix much. You don't have to master nix, but the command \`nix-shell -p 'haskell.packages.ghcjs86.ghcWithPackages (p: with p; \[ miso \])'\` should put you into a repl with ghcjs and miso in your ghc-pkg list. &gt;How slow or fast is miso on the front end? Extremely fast, and you don't have to sacrifice the sanity of your code or perform manual DOM manipulation to achieve performance. Here's a comparison of load times: [https://dc25.github.io/myBlog/2017/11/26/minesweepers-written-using-elm-reflex-miso.html](https://dc25.github.io/myBlog/2017/11/26/minesweepers-written-using-elm-reflex-miso.html)
In Haskell, usually, there are many semantically-equivalent ways to represent the same concepts. However, in practice, it is often desirable to have more convenient tools and ways to write programs. I see at least three problems with `Compose Maybe Last`: 1. `Compose` doesn't have `Semigroup` and `Monoid` instances. 2. It's not that easy to extract `Maybe a` from `Compose Maybe Last a`. 3. It's much harder for beginners to work with that data type. `Compose` is not a beginner-friendly topic. I can see how you can easily use `Compose Last Maybe` instead of `Compose Maybe Last` and get weird errors.
Does Typed Racket have a strong enough type system compared to Haskell? Like does it support type classes and such?
There's an old book, [Algorithms: A Functional Programming Approach](https://www.amazon.com/Algorithms-Functional-Programming-Approach-International/dp/0201596040/ref=sr_1_1?qid=1558928802&amp;refinements=p_27%3AFethi+A.+Rabhi&amp;rnid=1000&amp;s=books&amp;sr=1-1&amp;text=Fethi+A.+Rabhi), by Rabhi and Lapalme that builds a small library for doing dynamic programming using the tabular approach. I can't find my copy at the moment, so I can't tell you whether or not they derive it from memoization.
Neither the Racket nor the Haskell version of Pie is defined by simple translation to the host language: both versions do all typechecking themselves before either compiling to Racket code (in the `#lang pie` case) or being interpreted (in the `pie-hs` case). So the type system of the host language doesn’t matter, and in fact, I think `#lang pie` is implemented with dynamically-typed Racket, not Typed Racket.
&gt; then said Bytestring would be applied to a function That's not quite right; we can say that a function is applied to a Bytestring, but not that a Bytestring is applied to a function. You can, however, say that a Bytestring is _passed_ to a function, _given_ to a function, or _fed_ to a function. &gt; myfun "command" p1 = myPrintTextFun &gt;&gt; whateverFun p1 &gt; &gt; Does this violate the purity concept? Technically, `myfun` is pure in the sense that when given the same string and the same parameter `p2`, it always produces the same IO action, namely `myPrintTextFun &gt;&gt; whateverFun p1`. However, executing this IO action does have side effects, namely printing which command was entered and probably fetching some JSON data from the web. This is definitely not pure, as printing is a side-effect, and the function could return different result at different time, e.g. if the website's database is updated or if the internet is down. So even though the function which _returns_ an IO action is pure, that overall algorithm is not pure at all, because it involves IO. You cannot implement a program which takes commands from the user, prints feedback on the console, and fetches json from a website in a pure way, since all of those things are side-effects. You can, however, structure your code in a way which separates the pure parts of your program from the parts which perform side-effects. For example, parsing the json into a custom type can be done purely, so I would definitely not implement that as a function which returns `IO MyType`, I would probably make it return `Maybe MyType`, or perhaps `Either String MyType`.
Our goal with the Racket version was to make an implementation that used techniques that were easily portable to other implementation languages, so the only Racket-specific things we used were inheriting the reader and some of the DrRacket integration for stuff like "go to definition" and type tooltips. While writing _The Little Typer_, we wrote a number of implementations, and settled on Typed Racket after having it catch a number of subtle bugs from a previous implementation in super-minimalist Scheme.
I hope you have fun with it!
Thanks for the explanation! I do think that makes sense.
&gt;Yeah I actually struggled with that for a while as type literals were new to me, but managed to get runtime-dependence using proxy types. What do you mean? I am interested and want to say something about our approach, but also I don't want to spoil the contents of the next parts of this blog post ;-)
Ohkay that's very helpful, convincing myself that code I have written is correct to a great extent. Thanks for sharing :)
Also, IO code is technically pure: it's just data that contains instructions, a "todo-list" you build up and eventually pass to the runtime for execution. That is, unless you go for unsafe IO (outside of the monad), all your code counts as pure. However, it is still a very good thing to factor out as much non-IO code as possible.