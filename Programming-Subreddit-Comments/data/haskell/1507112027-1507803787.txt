Integration between https://github.com/junkblocker/codesearch and ghc-exactprint would be amazing.
I confess that the above has somewhat mixed units, as I pretty freely and sloppily made word machine assumptions when talking about LCP(S) above. The goal in separating out the bounds is that the LCP(S) term should be the term that inflates to the usual hard 2NW or randomized N log_σ N bound when you start comparing fixed sized words. Similarly for random strings it typically looks like N log N, dominating the other term. There is no free lunch, but by separating out the terms we can at least talk about when radix sort _can_ be faster.
Is there a simple way to formulate the bound with non-mixed units?
The cost is based on how much of the structure has to be inspected, the lengths of the longest common prefix LCP(S), not the total size. 
It is a fundamentally out of place sort. AFS is unstable, and this has to be stable or it can't handle products. 
Thanks for all your hard work! Sorry for releasing something so half-baked :)
It doesn't (and can't) introspect declarations generated by TH at all. It can handle CPP macros (this is all part of the brilliant free lunch provided by ghc-exactprint) - but not cabal macros. It would probably be feasible to detect `cabal_macros.h`, though!
Me too! I wanted to be able to jump to definitions in a terminal without getting bogged down in text editor tooling, so I hacked up the v0 version released here. As I mentioned in the writeup, this project really exists so I can (eventually) write complicated one-line search queries. We'll see how that goes...
Not sure. The bounds I gave above pretty much follow https://www.cs.helsinki.fi/u/tpkarkka/opetus/13s/spa/lecture03-2x4.pdf
This seems like an edge case. I'd be using this tool to discover code. If a file is syntactically invalid then I'm probably editing it. If I'm editing it then I probably don't need this tool. 
`par` can be used with `unsafeInterleaveST` to do the trick. Lindsey Kuper's `Par` monad can also be seen as a form of always safe (if somewhat restricted) parallel ST. Well, its deterministic fragment, anyways.
It seems like the right complexity in terms of word operations is O(n log σ + LCP(S) / log σ), where LCP(S) is measured in bits. That prevents my initial objection and also shows why the choice of σ is a tradeoff, so setting it to 2 isn't always best.
For the record, starting with this release we are being a bit more conservative in merging patches; this is a consequence of our effort to shrink the release cycle. For this reason this is a fairly small release consisting almost entirely of bug-fixes. See the [closed ticket list](https://ghc.haskell.org/trac/ghc/query?status=closed&amp;milestone=8.2.2&amp;col=id&amp;col=summary&amp;col=status&amp;col=type&amp;col=priority&amp;col=milestone&amp;col=component&amp;order=priority) for a full list of fixed issues.
As others have mentioned, this is an 8.4 change and not present in 8.2.2.
You might take a look at this [findDistDir](https://github.com/dan-t/cabal-lenses/blob/42157cc6b666b667173657f19aafc3cfc0cdd788/lib/CabalLenses/Utils.hs#L95) function, which gives you the directory where the `cabal_macros.h` file is located.
Did I say there was anything wrong? The defn of `Bool` given in the book does not derive from `Ord`. The defn, on page 66, is `data Bool = False | True`. Given that defn, `foldr (&gt;)` is a type error. We must not assume that the reader knows what a lattice is, what bottom is, etc. Yes, `instance Ord Bool` is well accepted and uncontroversial to some programmers. However, when a programmer is confronted by the expression `False &gt; True`, isn't it more likely that they'll reach not for their knowledge of Heyting Algebra, but for their knowledge of the C programming language, and will arrive at the right answer for wrong reasons? 
I'm afraid to say that's a really bad idea to make assumptions about internal cabal implementation details (and you shouldn't even assume that cabal even generates a `cabal_macros.h` file to begin with). From a quick glance, that `findDistDir` will in general most lilely not locate the right dist-folder when using `cabal new-build`.
As long as there's no way to get this kind of information from cabal directly - I'm not aware of one - what else are your options?
If you use ghc-mod (and via it cabal-helper) it should be able to sort things out. This is what we do in haskell-ide-engine. And we are working toward having a more tooling-focused backend for it, see https://github.com/alanz/ghc-mod/tree/hie-integration-rebased-split-up-2/core
I'm still waiting for this one! [*Multiplying by 1 - an important form of computation and how it reveals distinctions between kleislis - Edmund Cape*](http://www.composeconference.org/2017/program/) It teases about something akin to [*Information Effects*](https://www.cs.indiana.edu/~sabry/papers/information-effects.pdf), or an accounting of the creation and destruction of information.
Nah, I think it was about something else. Still no idea what though. It was a very confusing presentation.
That talk was entirely incoherent. I wouldn't get your hopes up.
Does that mean you can hit `O(n^2)` with respect to input size? By having a couple really long strings of length `O(n)`, and the rest really tiny of length `O(1)`.
I mean there are plenty of instances, particularly for `Ord` that you just have to know the ordering of and where there isn't a concrete reason why they are the way they are. You just get used to it and eventually you'll never forget that `True &gt; False`, plus as you said `C` (and literally every other language) helps you out with that too. 
General concurrent programming in `ST` is inherently unsafe; there's nothing to prevent multiple threads from scribbling the same parts of the array. A primitive approach might be to use `unsafeIOToST` to embed a carefully vetted concurrent program in `ST`. But a compiler is generally free to treat `ST` and `IO` somewhat differently. It doesn't currently, but one idea I looked at (and have not rejected) is that we can take advantage of the fact that in `ST` (but not `IO`) `x &gt;&gt; _|_ = _|_`. So these `unsafeIOToST` and `unsafeSTToIO` functions are really quite unsafe. One could also imagine a Haskell compiler using utterly different representations of `ST` and `IO`, in which case the conversions might be entirely unavailable.
Yes, indeed we're looking at performance over many different dimensions here. I'm collecting a little table, "+" indicates what approach is good in that dimension: * Executable sizes (+TH) * Compiler performance as function of e.g. number of constructors (+TH) * Number of modules to recompile (+Generics) * Per-module compilation overhead to start ghci or nodejs interpreter (+Generics)
It is a consumer facing app. I don't understand what makes TH a bigger problem under GHCJS, besides the fact that it takes a long time to compile. The javascript size is something that can probably be ironed out.
By the way, I've had to modify the deriveJSON function to take a type rather than a name, and to omit phantom types from the constraint list. I'd be happy to share if you have a use. The stuff I'm using is here: https://github.com/seereason/th-typegraph. I've run into several instance deriving functions that I had to modify similarly: safecopy, web-routes, cereal.
Verified, cabal passes -O1 to ghcjs by default.
You want to look into `MonadBaseControl` which I’m pretty sure `Yesod` is already an instance of. You just need to switch to the `MonadBaseControl` versions of `forkIO` (not recommended) or `async`. This should be a simple change of your import lines. Try this: https://hackage.haskell.org/package/lifted-async-0.9.3/docs/Control-Concurrent-Async-Lifted.html
&gt; I want to call it exactly when data is received Then perhaps polling isn’t a good idea after all. Maybe use a blocking socket?
I guess server must just run. But when there is a data coming from a socket, it should react. I don't know when I may receive data, so I can't call `receive` in particular place and wait. That's why I'm guessing about using a thread. What do you exactly mean? :)
If I'll stick to poll-in-thread solution, then this looks like what I need, thanks :)
The short answer is "yes" GHC can do optimizations like that. The modern story for moving the `not` inside the `case` is with [join points](https://pldi17.sigplan.org/event/pldi-2017-papers-compiling-without-continuations) (note, although I wasn't involved in that paper, it came out of the research group I'm part of)
FWIW, I’m not a fan of `MonadBaseControl`, due to its semantics often being difficult to reason about with some transformers. `MoandUnliftIO` accomplishes the same thing for a smaller class of transformers for whom it is obvious to reason about. Not sure if Yesod uses this yet though, as it’s fairly new. The fundamental problem is called “non-algebraic effects,” which causes major problems for the semantics of pretty much any effects system. Monad transformers that are in any abstract way isomorphic to `Reader` make non-algebraic effects really easy to reason about, which is what `MonadUnliftIO` takes advantage of.
Cool! I'll take a look to see whether there's something worth adding upstream.
Found this post by googling nanocoin, which seems to be a Ponzi scheme somebody I know fell for. I always told him to stay away from fake crypto and get with the real deal but he was sure of it. Now he lost all his money. Just wanted to share. Hope your Haskell project is working out!
Interesting. I just spent a good chunk of time trying to parallelize array initialization with `par` and `unsafeInterleaveST` in [this gist](https://gist.github.com/andrewthad/48024623cf39e672f4f7b4242b25d5b3). In my benchmarks, I cannot get it to run faster than the plain old serial implementation, which is somewhat surprising. It's also really difficult to prevent the fizzling of sparks. (I have most of the work of the main thread to ensure the sparks even have a chance of being picked up). I found your [Deamortized ST](https://www.schoolofhaskell.com/school/to-infinity-and-beyond/older-but-still-interesting/deamortized-st), leading me to [Sparking Imperatives](https://unlines.wordpress.com/2010/04/21/sparking-imperatives/) as well. It's worth noting that my implementation in the gist is a little different than either of these. I use `spark#`, `seq#`, and `unsafeDupableInterleaveST` instead, but I believe the end result is pretty much the same. To address my problem with fizzling and GCed sparks, I think I may just need to do something more computationally expensive (like a sorting algorithm). Also, it's kind of a bummer that to avoid duplicating work, you have to slightly imbalance the load. Otherwise, the main thread gets its part done and then kills a spark that may have been almost finished.
I completely agree that in general, concurrency inside of `ST` is unsafe. It's undermines the very determinism that `ST` is supposed to guarantee. However, for certain problems, it's possible to write algorithms where this is ok.
It appears that it does work, but yes you are right about the lack of polymorphism in the return type. Whenever you `:t` a function of that kind it always fully expands out the type synonym showing you that `MonadXX m =&gt;` constraint (unlike say with `String`). You also need `RankNTypes` enabled to actually define the original type synonym.
Am I right in thinking that the compiler would’ve fused `not . not` before GHC8.2 though? Join points are just a cleaner and more predictable way of getting the same results in this instance?
My issue with overlapping instances is that they aren't coherent, even without orphans. So depending on where you call a function (and also how you define the type signature, e.g `Show [a]` vs `Show a`) can change its behavior. This is fixable by requiring `{-# Overlappable #-}` always, so `{-# Overlaps #-}` will fail unless its over an `{-# Overlappable #-}`, and perhaps `{-# UnsafeOverlaps #-}`for when you are ok with incoherent behavior. Honestly `{-# Overlaps #-}` isn't that important IMO, the instances that must be treated with care are the ones that can be overlapped, the ones that overlap other instances can be treated as a coherent truth as long as they themselves aren't overlappable. Perhaps we could just deprecate `Overlaps` and say to use `Overlappable` instead, similar to what we did with `{-# LANGUAGE OverlappingInstances #-}`. Then for example if you had `instance {-# Overlappable #-} Show a =&gt; Show [a]`, and `showInList x = show [x]`, the signature would be forced to be `Show [a] =&gt; a -&gt; String` instead of the dangerous `Show a =&gt; a -&gt; String`. I would be fine with overlapping instances if they were safe in the absence of orphans, and then banning or being extremely careful about orphans. Although I would like to see a long term solution for orphans, IMO a good solution is blessed packages: where if package `X` creates data type `D` and package `Y` creates class `C`, then `X` and `Y` can state in the cabal file one single package that is allowed to produce instances tying the two packages together (e.g. `instance C D where ...`. If they both declare a unifying package but disagree on it compilation should fail, only one declaring it is fine.
I'm confused as to what you are getting at, what /u/enobayram suggested works just fine, and does the exact effect composition you want. Try it out. queryDB1 :: MonadDB1 m =&gt; m A queryDB2 :: MonadDB2 m =&gt; m B combine :: A -&gt; B -&gt; C combine &lt;$&gt; queryDB1 &lt;*&gt; queryDB2 :: (MonadDB1 m, MonadDb2 m) =&gt; m C query :: C
A question: why is loading via `LoadIm` restricted to `Unsigned 56` when the registers hold `Unsigned 64`? Edit: nevermind, this is addressed later
Let’s see if “case of case” would have been enough not . not = \x -&gt; not (not x) = \x -&gt; case not x of True -&gt; False False -&gt; True = \x -&gt; case (case x of True -&gt; False False -&gt; True ) of True -&gt; False False -&gt; True = \x -&gt; case x of -- case of case True -&gt; True False -&gt; False And this is where I lose it. I have no idea if GHC is able to see such an obvious identity case statement and drop it entirely. I would hope so though.
You can import the Safe variant of that module: https://hackage.haskell.org/package/lifted-async-0.9.3/docs/Control-Concurrent-Async-Lifted-Safe.html
I don’t know about your particular requirement but if timely response is a stricter requirement, you might want to spawn a thread and have that thread be blocked on the socket. Right after you `accept`, you can convert that socket to a handle and use `hGet` or similar functions that block on input. This is assuming you are doing a TCP server so I’m not sure what role Yesod plays here. 
It does see it. --- module A where f x = case x of True -&gt; True False -&gt; False --- $ ghc -O -dsuppress-all -ddump-simpl A ... f = \ x_ap4 -&gt; x_ap4 ... 
Yep, just need space for the tag and register bits. 
I linked my StackOverflow question because I figured that be easier to read with the better formatting of code. 
You won't have to poll with this solution. You would write something like: spawnProcess :: Socket -&gt; Yesod (Async a) spawnProcess socket = async $ forever $ do thing &lt;- liftIO $ listen socket process thing 
The measure you described in a couple of places around here was |S|, the total size of all of the input strings. The measure I'm using LCP(S) is if you were to take the strings and sort them then look at just the common prefixes between the neighbors. Everything you inspect past that point is wasted work. This is &lt;= |S| in all cases. Note, its just the LCP between neighboring strings. With discrimination I pay a cost involving the LCP(S) (which all string sorts have to pay) and another bound involving n log sigma, while a class multikey mergesort, etc. must pay LCP(S) + n log n. Elsewhere under this thread are more details. "A couple of really long strings" impacts both |S| and LCP(S), but having a couple of long strings basically only adds an additive, not a multiplicative amount to LCP(S). On the other hand nothing stops your strings from all being length n and discriminating on the last character or something.
yes. it was just "case of case" followed by "case of known case" and then eta reduction, all of which have been in GHC for years.
On a related note I would like to mention that the parallel computation might not be as fast as the serial one, due to the cache and other memory management stuff. I once had to do the opposite of what you are trying to do. My application had both a concurrent part (independent process using forkIO) and computation intensive work on Vector (which runs faster on single core). And my application would consistently work better if I limit the process to a single core. So I tried to use the ST to force the computation in serial, but unfortunately it did not improve performance. If I remember correctly more that 50% of time was spend in GC, and the percentage increased with the number of cores... In the end I had to limit the runtime to one core to avoid the over-head. Here is a link to the code https://github.com/dfordivam/sandbox I discussed this with Jost Berthold and he did a detailed analysis of this here https://gist.github.com/jberthold/949ca8bdba1ae2732c56f94f22e72a77 
You've already got an answer on stack overflow. Alternatively look up records. Here's my answer on another of your posts: https://www.reddit.com/r/haskellquestions/comments/74c8jv/is_it_possible_to_access_a_specific_piece_of_a/dnx5tb3
I am trying to understand dependent types with Haskell, and was going through `DataKinds` extension. If I have code that looks something like this, data B = B1 | B2 data TypeB = ConB B data TypeA = ConA Int In repl, *Main&gt; :k 'ConB 'ConB :: B -&gt; TypeB That is saying type `'ConB` has kind `B -&gt; TypeB`and I could do, *Main&gt; :k 'ConB 'B1 'ConB 'B1 :: TypeB to fully apply the types to get kind `TypeB`. How can I do the same with `ConA` *Main&gt; :k 'ConA 'ConA :: Int -&gt; TypeA As far as I understand it, when `Int` got promoted to kind, the kind `Int` is uninhabited. Then how can I create a type of kind `TypeA` if I can find a type with kind `Int`. All of the examples I found when talking about `DataKinds` used `Nat` when talking about numbers.
I think I get it now, thanks. That sounds really cool. How does that compare to inserting it all into a particia tree and then extracting, shouldn't that take `O(LCP(S) + n)` time? Since inserting takes `O(LCP(x, tree) + 1)` time and so does extract min.
Thanks
He mentioned zeromq so the `receive` he's talking about is probably [this one](https://hackage.haskell.org/package/zeromq-haskell-0.8.4/docs/System-ZMQ.html#v:receive) which means to do the equivalent I think he needs to spawn a thread and then call `receive` without the `ZMQ_NOBLOCK` flag.
&gt; BSD grants an implied patent license. Are you sure about that. I was under the impression that it didn't contain one and thus FB was giving you strictly more rights than BSD. Can a lawyer answer that concretely? Because if it does normally imply patent licensing then this alternative no-suing stuff is pretty shitty, but if it normally doesn't imply patent licensing then I have no complaints about the explicit patent licensing.
Logically that is what a MSD radix sort is doing. It is after all a form of trie-based sort! =) The main difference is that it builds up in a breadth first fashion rather than depth first. (This makes accounting for the fact you only pay for LCP(S) a bit easier, but you can get the same either way.) That covers the LCP(S) cost, but the other cost your bound fails to consider is the cost of whatever branching factor you have for each node of the trie. That is the "sigma" factor. (As noted elsewhere in this thread the fanout of your trie affects the units we encode LCP(S) in as well so this choice isn't completely free.) The best of both worlds sort would be to use something called a burstsort to back my discrimination framework. Burstsort is a PATRICIA trie but modified such that it accumulates a largish number of elements in a bucket before it "bursts" the node and pushes the values down further into the tree. This helps with the cache locality problems that MSD radix sort has. Burstsort is also currently the fastest string sort in practice, so this has some nice potential for showing off that we can get all of this to be not just linear, but _fast_. (You can also view all of the crazy instance stuff as basically serializing out the data structures into some form of orderable strings lazily to compute the LCP(S) and then doing a string compare on it, so the string sorting stuff is all relevant.) There are some bit and pieces in the productive stable unordered discriminators ("Grouping") that could be faster. Notably, I could use a linear probing hash table and mangle inputs with a simple tabulation hash to get O(1) w.h.p. expected path lengths and constant factors within about 10% of an actual memory access. Grouping is done in MonadPrim anyways, so a mutable hash table would be acceptable in that fragment. These are the two major items I have waiting for someone to want to throw a grad-student at the problem -- or for it to become enough of a performance bottleneck for me to warrant the attention. Also, in theory the grouping (rather than sorting) stuff has potentially weaker bounds on it as LCP(S) is no longer rigidly determinable. It comes from the serialization order, but in the unordered discrimination setting this is much more arbitrary. Any walk of the data structured involved will do, but we kind of have to pick the order in advance, so the shortest potential walk is no longer a realizable asymptotic goal.
Indeed! For example the primitive package does or will expose an st monad friendly version of Mvar codes. 
I would recommend against records unless you are also suggesting creating a new type for each of the possible data constructors. Partial records should just not be allowed: data FooBar = Foo { x :: Int, y :: Int } | Bar { z :: Int, tag :: String } -- Evil, do not do this. data FooBar = FBFoo Foo | FBBar Bar data Foo = Foo { x :: Int, y :: Int } data Bar = Bar { z :: Int, tag :: String } -- Fine
With `&gt;=&gt;` the type DOES match just fine, do you not agree with that statement? Like seriously try it man.
Because you can already achieve that goal just fine with typeclasses. The following works just fine: queryDB1 :: MonadDB1 m =&gt; m A queryDB2 :: MonadDB2 m =&gt; m B combine :: A -&gt; B -&gt; C combine &lt;$&gt; queryDB1 &lt;*&gt; queryDB2 :: (MonadDB1 m, MonadDb2 m) =&gt; m C query :: C
I've heard only about the TH compile time problems in GHCJS, yet. Do you have any emperical data of TH vs Generics compile-time and JS-size, to share? 
Firstly, there was a crazy TH bug in ghcjs that Luite solved a while back. That made things a lot more manageable: https://github.com/ghcjs/ghcjs/commit/73618904369fe9a2adb7caaf751e24568a5d0bf4 However, I have a file that has a very expensive piece of template haskell code - it basically does a deep traversal of the type contains type relation - and it simply cannot finish, it dies with some kind of nodejs stack error. Other things are slower, but in a linear feeling way. The empirical data I have about javascript size shows a javascript .js_o file that was 27MB with Generic instances dropping to about 14MB with custom driveSerialize, deriveJSON, and deriveSafeCopy functions. With -O0 that size drops to about 9MB, with -O2 it rises to 14.4MB. All of this ultimately produces code via template haskell, so its not quite correct to call this Generics vs. template haskell. This particular file produces lots of instances for lots of types, a few of which have lots (hundreds) of constructors.
Doesn't this lead to nondeterministic behavior in pure code in the presence of `_|_`? Wouldn't `flip const` be safer than `seq` as a backup for when `par` isn't deemed appropriate?
Looks like [eta-conversion](https://wiki.haskell.org/Eta_conversion)... Where ```not . not == id``` are eta equivalent. Also &gt; Extensive use of η-reduction can lead to Pointfree programming. It is also typically used in certain compile-time optimisations.
Unfortunately, at the moment you can't use `Int` there. I say "at the moment" because when `DependentTypes` are finished properly, then `Int`s and `String`s (`[Char]`) will be usable at the type-level as-is. Think of `Nat` and `Symbol` as (overly-magical) stop gaps. When you look at [`GHC.TypeLits`](https://hackage.haskell.org/package/base-4.10.0.0/docs/GHC-TypeLits.html) you will see that `Nat` is defined as data Nat :: * That is, **we see that `Nat` exists as a regular data type**, which is also promoted to a kind when `DataKinds` is enabled. Ergo, you can use `Nat` in your data-type-cum-kind definition: data TypeA = ConA Nat Now, this might be problematic if you wish to use this as a data type *and* a kind, since, at least according to my limited experience, you can't write `Nat`s at the term-level, so you can't do `let p n = ConA n` for some arbitrary `n :: Nat`. You certainly can't promote arbitrary numbers at run-time to the type-level, since that is a feature of a dependently-typed language, which GHC Haskell is not (yet). 
You are restricting queryDB1 and queryDB2 to run the SAME monad with the same effects . That is again what I was intended to avoid. That don´t count as composable components. Composing using glue code can be done with objects in an object oriented language. No matter if the glue code creates a new monad, it is glue code. Imagine that I add a third database, you can not compose these two components with the third seamlessly. You have to add more glue code in order to add it to the applicative. Of course , by some code you can embed these component within new ones and combine them within a new monad, newqueryDB1= gluecode queryDB1 newqueryDB2= gluecode2 queryDB2 newQueryDB3= gluecode3 queryDB3 newqueryDB1 &lt;*&gt; newqueryDB2 &lt;*&gt; newqueryDB3 but that is not what was intended. And I don't count other necessary tweaks. The difference between seamless composability with algebraic guarantees in one side and composability trough glue code in the other, is immense, even if the glue code restore the applicative properties, like in the above case, that has little advantage. since it needs to be tweaked again with each new component to be added. I can combine anything in applicatives, including C routines if I use glue code. That does not mean that the C code is composable. 
Is using the Nix package manager the main way to install GHCJS without any issues? I'm just starting to read about Nix now, and I'm wondering how it can compare to `pip`, something I'm used to. **edit**: thanks guibou
You are restricting queryDB1 and queryDB2 to run the SAME monad with the same effects. That is again what I was intended to avoid.
"BSD grants an implied patent license." That is my understanding too. There is comment by lawyer Daniel Berlin at https://news.ycombinator.com/item?id=9113515 which also says this.
Nix is like pip but for all your packages, not only python. (So python, Haskell, C, c++, anytool,,… can be handled by nix.) Nix guarantee reproducible and sandboxed build, not like pip. Nix is different than pip in the way it stores all packages in a shared cache. Nix support on Windows is kinda partial and broken , OpenGL support sucks too. WIith nix, add / edit a packages is easy. Appart from some major drawbacks with OpenGL / Windows and distribution, Nix is really great.
I mean if you want to run `queryDB1` you can just use a `DB1Monad` and if you want to run `queryDB2` you can just use `DB2Monad`. Of course if you run `compose &lt;$&gt; queryDB1 &lt;*&gt; queryDB2` you need to be able to support both dbs, because you are using both dbs. That's as simple as `DB1MonadT DB2Monad` or similar. Can you give me some sort of concrete benefit for running `queryDB1` on one monad, running `queryDB2` on another totally separate monad, then stitching their results together into some brand new monad created on the fly? Like I want to see at least one actual use case or piece of example code where such a distinction, which as far as I am currently concerned is basically an implementation detail, matters.
Ok it seems like you care a great deal about what the eventual monomorphized monad stack is used at the end to actually execute everything. I don't know why you care since the result and testability and algebraic properties and such are all identical or better than what you get with creating new monads on the fly. Can you give an actual use case for where you can do something easily in OOP that is hard in FP relating to this? This all seems like a lot of faffing about over an implementation detail. To me adding a third database seems trivial to deal with. foo &lt;$&gt; newqueryDB1 &lt;*&gt; newqueryDB2 &lt;*&gt; newqueryDB3 :: (MonadDB1 m, MonadDB2 m, MonadDB3 m) =&gt; m () Now when you eventually run this you use a stack that supports all these effects, so you just add `MonadDB3T` and perhaps `deriving MonadDB3` to like one place in your code, and everything works great.
Thanks! That explains why we are limited to `Nat`.
&gt; foldr `f a [b, c]` is (b `f` (c `f` a)) really. Similarly, `foldr (&gt;) True [False, True] == (False &gt; (True &gt; True))`.
Oh right, my bad. I'm too used to foldl apparently
I just turned on `LANGUAGE IncoherentInstances` and it all made sense.
Anything written by Gabriel Gonzalez is likely to be well written with helpful documentation describing how it works.
I’m pretty new to haskell, what’s wrong with partial records and pattern matching against each data constructor?
.. in vim, you left that one out.
[QuickCheck](https://en.m.wikipedia.org/wiki/QuickCheck), [Parsec](https://wiki.haskell.org/Parsec), [Json](https://hackage.haskell.org/package/json) - my 2 cents. Because they are both easily graspable and usable early in the learning curve. It allows you to code basic jobs like compute an average of world population by country from input json. I quickly added a little `cabal --enable-coverage` and [Warp](https://hackage.haskell.org/package/warp) to have quick micro-services personnaly. Others may cite Pipes, Conduits, Arrows.
**QuickCheck** QuickCheck is a combinator library originally written in Haskell, designed to assist in software testing by generating test cases for test suites. It is compatible with the GHC compiler and the Hugs interpreter. In QuickCheck the programmer writes assertions about logical properties that a function should fulfill. Then QuickCheck attempts to generate a test case that falsifies these assertions. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
[pandoc](https://github.com/jgm/pandoc) does not use a ton of advanced features, yet is still well crafted and used even by many non-Haskellers :)
viz. http://hackage.haskell.org/package/pipes-4.3.5/docs/Pipes-Tutorial.html , http://hackage.haskell.org/package/foldl , ...
Let's keep this type of language off our subreddit, please.
I second QuickCheck, I actually find it easier to read the code than the documentation. It also provides a pretty good examples of: * how to organize a library into modules * how to construct layers of functionality using monads * differences between pure and IO-based monads * how to define useful type classes, like the `Property`class On top of this, learning how QuickCheck works will provide you with valuable insight in how to do testing of Haskell programs. I'd also suggest [Attoparsec]( https://hackage.haskell.org/package/attoparsec ) over Parsec. I found the code of this project pretty well organized, and provides some excellent examples of how continuation passing style (CPS) can be used to improve the efficiency of a Haskell program.
Pragmas are easy to add if they're just exposing existing functionality, but in this case it also needs to implement logic for translating from boxed to unboxed representation. That said, the unpack pragma is still pretty simple to add, and it's just unfortunate timing that it missed the cut iirc
[dependent-map](https://hackage.haskell.org/package/dependent-map) is pretty useful. With [Type.Reflection.TypeRep](https://hackage.haskell.org/package/base-4.10.0.0/docs/Type-Reflection.html) keys and a couple orphan instances you can make a heterogenous collection that can store up to one value of each type.
Removed for garbage title. Repost with the actual title ("Code Quality Research: Functional Languages Beat Procedural/Object-Oriented") if you want to.
Plugging my own work, expressing a [FROM clause](http://hackage.haskell.org/package/squeal-postgresql-0.1.1.4/docs/Squeal-PostgreSQL-Query.html#g:4) in squeal.
Don't you mean "replace the shell"?
So basically the issue is that `x (Baz 0 "")` and `z (Foo 5 6)` both typecheck and both will crash at runtime. If you ever call the wrong function (or make the wrong record update) on the wrong constructor, you will not receive any compile time indication that you have done such a thing.
You'll need to introduce a primitive operation that splits `MVector s a` into sub-arrays `MVector t1 a` and `MVector t2 a`, processes them separately before merging back. Maybe something like this: `parSplit :: MVector s a -&gt; (forall t . MVector t a -&gt; ST t ()) -&gt; ST s ()` I remember seeing a paper mentioning something along this line, but forgot where. However, blindly parallelizing quick sort this way won't give you much speed up due to granularity being too fine grained. You'll have to watch the size of the array to decide whether to sort it in parallel or sequential. 
What are the disadvantages of compiling with CPS that Simon mentions but does not explain? 
The vectors I had in mind were going to be at least 100,000 elements, so hopefully, but I certainly agree that for small vectors, the standard serial implementation would win out.
Is this in a PR somewhere? I haven't heard of it before.
i think its in master or planned to be? if its not in, easy to fix. I know Dan Doel has poked at it a teeny bit but point being: its an example of IO ish data structures that could reasonable given an ST friendly wrapper that would then allow some nontrivial uses of deterministic concurrency to be written on top in userland... and if we commit to a tooo strong a divergence between IO and ST, you may loose that opportunity
There are libraries to perform arbitrary shell commands, so yes you could use ghci as a shell, but you'd have to relaunch (not just reload) after installing any new packages if you used stack. I think a non sandboxed normal ghci session might be able to see new packages without a relaunch, but I'm not actually sure
Probably mostly just lack of time, though there may have been larger problems that I can no longer remember... 
Just to increase the intrigue, I've updated this problem with some additional findings. What it comes down to is that I'm able to use a combination of `forkIO`, `MVar`, and `unsafePerformIO` to get the performance I expect. However, with sparks, I cannot get anywhere near this level of performance.
You can execute any shell command using `:!`, such as `:!ls`, `:!git bidule -foo -bar --no-bar-with-foo --root master --forward reverse checkout`.
I thought join point inference only kicked in for let bound recursive functions where every usage is fully applied and in a tail call position? Iirc the transformations necessary to simplify `not . not` are already in [Compilation by transformation in the Glasgow Haskell Compiler](https://www.microsoft.com/en-us/research/publication/compilation-by-transformation-in-the-glasgow-haskell-compiler/) and explicit join points enable this for tail recursive functions and make join points more robust in general.
Nice! Are you familiar with [Torsors](http://math.ucr.edu/home/baez/torsors.html)? It looks like this algorithm could be applied, in theory, to anything that was a member of a hypothetical `Torsor` class, and also `Ord`, where an `Interval` could be represented the N-Torsor of the members of the some Group N, and `TimeUnits` could be reinterpreted as just `Units`of some (mostly) arbitrary type. Perhaps you could achieve a library which provided utilities for reserving resources of ordered elements, wherein a 'reservation' would simply represent a 'consumed' span of resources in the origin Set. Like, for example, a block of IP Addresses, or simply a reservation system that allowed for arbitrary intervals of time. 
Can you help me with my programming homework I am a novice programmer and i really cant debug it by myself
I like them for their ability to express complex (in fact arbitrarily so) constraints on data so that it can only be properly formed, and so nonsense cases don't have to be considered during implementation.
module name is the one in `module Foo.Bar where`, and cabal finds it by looking for Foo/Bar.hs
Any chance you have an example of this? I'm building in a hurry(got ~2 weeks to a Chrome update that'll break our site...), so I've [been littering my handlers w/ runDB calls](https://github.com/Southern-Exposure-Seed-Exchange/southernexposure.com/search?utf8=%E2%9C%93&amp;q=runDB). Once I'm less in a hurry, I'd love to refactor in a similar way that you describe...
Spec.hs only has one component, so that isn't applicable. I managed to get the `exitcode-stdio-1.0` test suite working fine with almost the same test suite section. Also, the problem is that the .cabal file won't even parse, not that its data are faulty.
GADT can be used internally to refine mere phantom types for operations that need them, while exposing only the simpler non-GADT data structure. The trick is described [here](http://mpickering.github.io/posts/2016-06-18-why-no-refinement.html) with a deeper analysis in the [ghostbuster paper](http://www.cs.ox.ac.uk/people/timothy.zakian/ghostbuster.pdf) A practical usage is [non-recursive extensible records](https://hackage.haskell.org/package/microgroove).
Ugh. Yeah. The title is super confusing.
It's a long post, so maybe confusing, but I'm looking to *avoid* just calling out to coreutils. I want to do things the "right" way, *i.e.* using functions that have more type constraints on the things I want to do. For particular questions, see the final list in the original post. Well, if I `stack install &lt;package&gt;` then `&lt;package&gt;` won't be visible to me until I exit and restart `stack ghci`.
Well, the whole point of this exercise is to learn haskell. Simply calling out to `/usr/bin` binaries is exactly the kind of thing I want to avoid. Maybe this wasn't clear in my long post?
I'm curious why `TimeUnit` is one of the Constructors for `TimePeriod` (in addition to `TimeInterval`). It seems like a "TimePeriod" should only denote a length of time? The `Calendar` type says TimeUnits are only encountered in the leaves, but there is nothing that prevents this because it only keys on `TimePeriod`s
[`singletons`](http://hackage.haskell.org/package/singletons-2.3.1). Who says that Haskell doesn't have dependent types? :) {-# LANGUAGE DataKinds #-} {-# LANGUAGE GADTs #-} {-# LANGUAGE PolyKinds #-} {-# LANGUAGE TypeFamilies #-} import Data.Kind data family Sing (z :: k) type Π = Sing -- Because I can data instance Sing (z :: Bool) where SFalse :: Sing False STrue :: Sing True type family IntOrChar (z :: Bool) :: Type where IntOrChar False = Int IntOrChar True = Char dependentlyTypedCode :: Π (z :: Bool) -&gt; IntOrChar z dependentlyTypedCode SFalse = 42 dependentlyTypedCode STrue = 'a'
Why are you using the `detailed-0.9` test type? You probably shouldn't: https://github.com/haskell/cabal/pull/3726
&gt; [23:11] The other cool thing about Monoid for me was that there are almost always two, because there's almost always one that's disjunctive (or additive, or concatenative), and there's almost always one that's conjunctive (or a product, or multiplicative; often a cross product, List's is ZipList). I wonder if it would make sense to have two separate type classes capturing this dichotomy? We'd still need newtypes in order to distinguish e.g. Min/Max from */+, of course, but it seems like it might be worthwhile to be able to write generic functions which use both kinds of monoids in the same expression, like this: dotProduct :: (Conjunctive a, Disjunctive a) =&gt; [a] -&gt; [a] -&gt; a dotProduct = foldr dappend dempty . zipWith cappend This might not be the best example because I don't immediately see the use of the Min/Max instantiation of that function, but the idea is that if we give ourselves the tools we need to express such operations, I bet we can find much more useful examples.
I often find DSLs using GADTs more direct and appropriate, but out of curiosity: Have you considered the 'dual' tagless final encoding? If not, why?
Compensation?
Although, somewhat tragically, GHC can't optimize `dependentlyTypedCode @False` to `\_ -&gt; 42`. That's because Haskell functions aren't total, so `undefined :: Π False` is always valid and GHC tries to preserve bottoms.
Yes, at least I think I have but I'm not that familiar with the terminology. So in squeal, most of the SQL DSL is embedded using final encoding, since expressions, queries, manipulations and definitions for instance are all `newtype`s over `ByteString` which is after all what a SQL statement or expression is in the end. I kind of went with my gut on whether to use GADTs or `ByteString`s for each sub-DSL. For `FROM` clauses I used a GADT by analogy with heterogeneous lists. I guess in the end it seems like I used initial encodings for the atomic pieces like aliases, groupings and relations and final encodings for the composites like expressions and statements.
I think it is missing some things that I'd want. Induction-recursion for instance is missing on Coq's "Gallina" and is very useful for reasoning about the semantics of dependently typed programs in a dependently typed programming language. As for top level definitions, interacting in some kind of server or repl like capacity in `main` can be thought of as really being corecursive where the concern is productivity not termination.
I don't know if it's the type of thing you're looking for but https://tidalcycles.org/ is the coolest Haskell+music combo I've seen.
"Sufficient", sure. STLC is sufficient too. Sufficiency isn't really what most would care about, though (then again, maybe I have a looser definition of sufficiency than you). CoC (with or without fix) by default lacks dependent eliminations, sigma-types, implicit product types, a cumulative hierarchy of universes, quotient types, subtyping, intersection types, etc. Depending on what you're looking for, some of those things are more desirable than others, but there are many bells and whistles that I'd consider expected for any dependently typed language that's intended for broader use, and the CoC lacks them. If you're looking at bootstrapping a language by compiling everything down to a simple core, [ιλP2](http://homepage.cs.uiowa.edu/~astump/papers/from-realizability-to-induction-aaron-stump.pdf) is a more sensible, modern target. [Here's a screen-cast](http://homepage.cs.uiowa.edu/~astump/cedille/state-of-cedille-aug-2017.mp4) on the kinds of things you can do in it (including induction, induction-recursion, and a bunch of other things), despite being just barely more complicated than the second-order dependently typed lambda calculus (making it more complicated than CoC in some ways, and less complicated in others).
Do you know if Tidal can create orchestral music? Most of the examples I heard had a very EDM/Techno vibe. 
I don't think it can -- at least not without building it up out of samples which I imagine is not what you're after. That's why I said I wasn't sure if it was what you're looking for. I feel like I've seen some cool things posted to this subreddit that were more on the compositional side, but I can't find any of them now. Hopefully someone else will be able to chime in with some of them.
This sounds interesting. One of the future goals I want to achieve with the library is to get rid of the fixed units of time, so that I can create arbitrarily granular calendars, and also to have a generic TimeUnit not to depend on a concrete "UTCTime". I will take into account this idea for the next version. Thank you!
That's difficult to wrap my head around :D I still don't get it completely `dependentlyTypedCode @False`takes no argument, or not? What's the connection to `undefined :: Π False` then? I also think that's rather disappointing. I would assume that most of the dependently typed code would be statically known, with only parts being dynamic.
That's a good question, and actually I thought about that, too. First, the reason why I wanted to have both things withing the same type: - Having `TimeUnit` and `TimeInterval` enclosed by a single type (`TimePeriod`) would give me a more uniform way to access the time value within a node or a leaf. - Semantically, I justified considering `TimeUnit` as a `TimePeriod` since I regarded a `TimeUnit` as the case where the `start-date` and the `end-date` in a `TimeInterval` have the same value. Second, even thought the type of `Calendar` does not prevent a `TimePeriod` from being a `TimeUnit` or a `TimeInterval` in the nodes or leaves, that is something I decided to enforce with the Constructor of the Calendar. In general, my strategy was to have smart constructors to avoid the necessity to manually create either `TimePeriod`s, `Calendar`s , `Reservation`s, etc. 
I haven't really played with it, but the [mezzo](https://hackage.haskell.org/package/mezzo) library seems pretty cool as a tool for writing music and using the type system to enforce various things (rules of counterpoint, etc). You might also want to check out their [experience report](https://dl.acm.org/citation.cfm?id=3122955.3122964).
Really good IMO. [Euterpea](http://www.euterpea.com/) and [The Haskell School Of Music](http://www.euterpea.com/) are among the best resources available for learning Haskell.
Thanks for posting this. I've got to look into that library more; I didn't know this was possible.
Nice level of detail in the post.
Do you have a particular "core" in mind as your ideal then, if not CoC?
Wouldn't this be semiring/rig? edit: hmm actually this comes with extra distributive laws, so there is more structure.
How about using newtypes to disambiguate them? dotProduct :: (Monoid (Conjunctive a), Monoid (Disjunctive a)) =&gt; [a] -&gt; [a] -&gt; a or [some form of named instance](https://mail.haskell.org/pipermail/haskell-cafe/2017-May/127147.html)
I agree with this. I'm trying to build a non-Turing complete language myself. I started with something roughly similar to STLC, but I've mutated it a bit. My aim is to find the minimal grammar and type system necessary to assure non-termination, while providing decent type inference. This idea definitely seems to be the zeitgeist, and I'd love it if I could recruit others to help me, so please contact me if you're interested.
I thought I wanted this too until I found out about Blum's size theorem. https://existentialtype.wordpress.com/2014/03/20/old-neglected-theorems-are-still-theorems/ &gt; We can, but at a cost. One limitation of total programming languages is that they are not universal: you cannot write an interpreter for T within T (see Chapter 9 of PFPL for a proof). More importantly, this limitation extends to any total language whatever. If this limitation does not seem important, then consider the Blum Size Theorem (BST) (from 1967), which places a very different limitation on total languages. Fix any total language, L, that permits writing functions on the natural numbers. Pick any blowup factor, say 2^{2^n}, or however expansive you wish to be. The BST states that there is a total function on the natural numbers that is programmable in L, but whose shortest program in L is larger by the given blowup factor than its shortest program in PCF! &gt; The underlying idea of the proof is that in a total language the proof of termination of a program must be baked into the code itself, whereas in a partial language the termination proof is an external verification condition left to the programmer For a practical example see the greatest common denominator algorithm. I'll still keep writing Idris though. 
There's also [vivid](https://hackage.haskell.org/package/vivid), an EDSL for SuperCollider.
As a Haskeller, I think this article is a little misleading, or at least a little too aggressive. I admit I know essentially nothing about Ecto, and I haven’t really written Elixir, so if this article is primarily intended to be a comparison with Ecto, I can’t really speak to that. If it’s seriously supposed to be a comparison with ActiveRecord, however, I think it’s too shallow to be a meaningful comparison. I’ve worked with ActiveRecord. It is *very* different from Persistent. It is very object-oriented, very stateful, and very dynamic. ActiveRecord models are not plain data, and in fact, the whole philosophy behind ActiveRecord is that they are not plain data. One of the most notable ways this manifests is [ActiveRecord’s notions of validations](http://guides.rubyonrails.org/active_record_validations.html), which are extraordinarily stateful and remarkably ad-hoc. They also work really, really well (almost disappointingly so… it feels like they shouldn’t work as well as they do!). I would not trade Persistent for ActiveRecord. Persistent gives me other things, like type safety and the ability to map my data to ordinary Haskell data structures. I’m still forced to do more myself, even with Esqueleto—I need to write my own validation functions, worry much more about how I load associations (ActiveRecord will just lazy load them on-demand, which is *super* convenient and lets you think of your data as a graph, not a bunch of flat rows), and worry more about mapping user input to my model—but that’s okay. That’s the Haskell philosophy. I just think that reducing ActiveRecord to its querying operations is, frankly, missing the point: ActiveRecord is all about dynamism and state and libraries that provide mixins and idioms that cooperate with that state. You can’t easily get a feel for what that’s like without actually using ActiveRecord for a bit.
The problem is, half of what I want to do in a functional language is write interpreters, analyses, and type checkers for Functional languages. And those are the exact things you can't do in CoC. The other problem is that while you can write things without recursion, it's often terribly awkward. So using fix, and just using heuristics to detect termination, seems like a better approach.
Donya Quick gave an excellent presentation about Kulitta and Euterpea at ICFP'17
So I've always loved pandoc and use it all the time, but I'm kind of surprised by some of its source code. I do not have a ton of experience writing Haskell so it may just be my own inexperience, but there are parts of Pandoc's source that seem.. not good. Like really imperative with huge functions that are pretty dense. Is there like a performance reason for such a style?
+100 for saying this. I've been learning this the hard-way, writing Haskell, coming from a Rails background.
`@False` is a `-XTypeApplications`, which instantiates the implicit `z :: Bool` to `False`. So, `dependentlyTypedCode @False :: Sing False -&gt; IntOrChar False` and since `Sing a` only ever has a single inhabitant (well, if it weren't for bottom) for any concrete `a`, I had hopes that GHC could optimize away the case match on the argument; the `STrue` case can't happen after all. GHC however refrains from doing so, since `dependentlyTypedCode @False (error "I want to see this for debugging reasons")` would not throw an error otherwise. Or maybe it's just to preserve strictness properties of the function (different side of the same coin, I guess), which would no longer be strict in its argument if we ignored it. Note that when using [`SingI`-style singletons](https://hackage.haskell.org/package/singletons-2.3.1/docs/Data-Singletons.html#t:SingI), this should work, because GHC can specialize for the dictionary to get out a concrete *value* out of `sing`. See [a problem I had recently](https://stackoverflow.com/questions/45734362/specialization-of-singleton-parameters/45737382#45737382). I think the most important insight was that in Haskell we can't turn type applications into value applications without incurring a `seq`, which *I think* has a non-zero cost.
Modules and files are different things (though we can only have one module in one file). Compare: -- detailed asks for the Module (to import `tests :: IO [Test]` from), not the file name. type: detailed-0.9 test-module: Foo and -- exitcode-stdio asks for the file (to make an executable), not the module type: exitcode-stdio-1.0 main-is: Foo.hs 
This is expected from the Algebraic Data Types Magazine.
What are your biggest difficulties with the solutions present in the Haskell ecosystem? Knowing this is of great value to anyone willing to contribute to the ecosystem!
This is fascinating. I cite another paragraph: &gt; For another example, there is no inherent reason why termination need be assured by means similar to that used in T. We got around this issue in NuPRL by separating the code from the proof, using a type theory based on a partial programming language, not a total one. The proof of termination is still required for typing in the core theory (but not in the theory with “bar types” for embracing partiality). But it’s not baked into the code itself, affecting its run-time; it is “off to the side”, large though it may be
I still don't see why dependent types are necessary to get type level `Int` and `Char` and so on. I'm personally not sold on dependent types for day to day programming (correctness is important but theorem proving for day to day developing doesn't sound fun) and probably won't turn the extension on much, but I absolutely want `DataKinds` to also promote `Int` and `Char`. I basically just want unified value level and type level syntax, but without necessarily the option to go from values up to types, I'm happy with type -&gt; type, value -&gt; value and type -&gt; value. I just hate how verbose type level programming is and the lack of `Char` and `Int`.
I mean it's a general purpose language, so whatever you could normally do with Java / Python etc. you can just do with Haskell. I have yet to find a project that is less enjoyable in Haskell, and I have found many that are far far more enjoyable (front-end dev Haskell &gt; JS IMO, and compiler dev Haskell &gt;&gt;&gt;&gt;&gt; Java pretty much objectively).
Just purchased it, Thanks 
[Discussion about this research](https://www.reddit.com/r/programming/comments/732fes/a_largescale_study_of_programming_languages_and/?submit_url=https%3A%2F%2Fcacm.acm.org%2Fmagazines%2F2017%2F10%2F221326-a-large-scale-study-of-programming-languages-and-code-quality-in-github) on /r/programming.
Troll title!
"in a total language the proof of termination of a program must be baked into the code itself, whereas in a partial language the termination proof is an external verification condition left to the programmer." This is very interesting... Is there some simple example of total languages that are useable? 
*Solved* : how do I use ghc plugins with stack? As I was writing this post I accidentally solved my problem. As far as I googled relevant information was very scarce (that's why I decided to ask here) so allow me to post anyway: Add your plugin to `build-depends` section. After finding out. [`Numeric.AD` is a good example](https://github.com/ekmett/ad/blob/f37cee0a40ab64eb3eb848bc40d97d865ebd3c5f/ad.cabal#L127). I'm in a situation where consulting [Herbie](https://github.com/mikeizbicki/HerbiePlugin) might make sense, so I was trying around `stack build --ghc-options "-fplugin=Herbie -v"` only to fail. The error says that it `Could not find module ‘Herbie’`, which is understandable, but I didn't know how to solve it. Now I'm failing to compile `herbie-exe`, which the Readme says is optional but apparently isn't. I'll need to look into that but haskell-related things are solved :)
Simon Jakobi and I started working on a [port of David Cope's 'Emily'][1] to Haskell a while ago, but did not get around to finishing it. Cope is a classical composer who has written a series of computer programs that can analyze and emulate the style of other classical composers like Bach, Mozart, and so on; and also be creative themselves. [1]: https://github.com/sjakobi/i-love-emily
More like title gore, but yeah. Sigh :(
Thank you for the resubmission. I must say I'm highly sceptical of "empirical" studies of software development methods even when they confirm my pre-existing biases.
Haha, I guess we Haskellers are a bit biased, writing interpreters and compilers all day. But I think most of programming does not need to be Turing-Complete. If you do want to write an interpreter then as I've said you do have the top level `fix`. The only questions is about convineince, and I think(correct me if i'm wrong) we can compile a haskell-like language with recursion to this (Turing-Incomplete language + top level `fix`)
sure, what are you working on ?
But this is irrelevant if we add a top-level `fix`, right?
didn't read the article, but OOP is more intuitive and easier to read.
Why not give a Control.Monad.Par a try? parallelProduct :: PV.Vector Double -&gt; Double parallelProduct v = runPar $ do let chunk = div (PV.length v) 2 p1 &lt;- spawnP (serialProduct (PV.slice 0 chunk v)) p2 &lt;- spawnP (serialProduct (PV.slice chunk chunk v)) (*) &lt;$&gt; get p1 &lt;*&gt; get p2 Gives me 2x speed up on two cores with very similar code. Also try Control.Monad.Par.Scheds.Sparks if you really want sparks.
Using wikipedia as a reference - https://en.wikipedia.org/wiki/Semiring - https://en.wikipedia.org/wiki/Lattice_(order) Semiring and Lattice are close: `+ ~ \/`, `* ~ /\` Semiring - is _commutative_ monoid (+, 0) - is monoid (*, 1) - multiplication distributes over addition - Note, without this or similar law havinga a name for two monoids over same the carrier set won't make sense! We _need_ a law to tie them together. - has annihilation rule: 0 * a = 0 --- Law | Semiring | Lattice -------------------+----------+-------- + / \/ commutative | Ye | Yes * / /\ commutative | No | Yes (commutative semiring is) + / \/ associative | Yes | Yes * / /\ associative | Yes | Yes + / \/ idempotent | No | Yes (idempotent semiring is) * / /\ idempotent | No | Yes a \/ 0 = a | Yes | Yes a /\ 1 = a | Yes | Yes Distributivity (distributive lattices) Distributivity of \/ over /\ | a ∨ (b ∧ c) = (a ∨ b) ∧ (a ∨ c) | Yes a + (b * c) = (a + b) * (a + c) | No !!! Distributivity of /\ over \/ a ∧ (b ∨ c) = (a ∧ b) ∨ (a ∧ c) | Yes a * (b + c) = (a * b) + (a * c) | Yes, semiring multiplication doesn't commute but both way work Note: we use e.g. /\ in logic. And though a /\ b and b /\ a, or a /\ a and a aren't syntatically the same, the are logically equivalent. Plug: http://hackage.haskell.org/package/lattices
**Semiring** In abstract algebra, a semiring is an algebraic structure similar to a ring, but without the requirement that each element must have an additive inverse. The term rig is also used occasionally—this originated as a joke, suggesting that rigs are rings without negative elements, similar to using rng to mean a ring without a multiplicative identity. *** **Lattice (order)** A lattice is an abstract structure studied in the mathematical subdisciplines of order theory and abstract algebra. It consists of a partially ordered set in which every two elements have a unique supremum (also called a least upper bound or join) and a unique infimum (also called a greatest lower bound or meet). An example is given by the natural numbers, partially ordered by divisibility, for which the unique supremum is the least common multiple and the unique infimum is the greatest common divisor. Lattices can also be characterized as algebraic structures satisfying certain axiomatic identities. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
`MinMax` is a `Lattice` (and thus Semiring), but aren't Field - which is used for `dotProduct`. On the other hand, `(R, +, 0, *, 1)` aren't a lattice (idempotency and distribution fails)
In your [**Facts of Haskell**](https://blog.ramdoot.in/facts-of-haskell-ff158ab7e48d) you say &gt; Although I used the word “equation” above, I am wrong: they are just part of the definitions of the function “func”. They are called equations &gt; Functions in Haskell are normally defined by a series of *equations*. For example, the function `inc` can be defined by the single equation: &gt; &gt; inc n = n+1 &gt; &gt; — [Haskell Report](https://www.haskell.org/tutorial/goodies.html)
There is a e-reader version of it, you can try that one. I used it in a regular e-reader and I have no complains.
All 'mainstream' total languages (Coq's Gallina, Agda, Idris, Lean, ...) are usable enough to write interesting programs in them (for example [CompCert](http://compcert.inria.fr/), a C compiler with a formal proof of correctness). The majority of code people want to write is structurally recursive, and much of the rest, including Bob's GCD example, is covered by techniques such as well-founded induction/recursion. Throw in sized types and coinduction, as implemented in Agda for example, and the programming experience starts to become almost tolerable. ;) Still, the complexity cost of this termination stuff should not be understated. One of my favourite things on this subreddit is people asking for a termination checker in Haskell -- be careful what you wish for.
I am using the ereader version on kindle without any issues. There is a beta epub version now but I have not tried it.
You will need singletons if you want to do (simulate) any kind of pattern matching on types (or do the whole thing at the type (class) level).
To recurse on the type level list, you'll need a type class where the recursive cases are the instances. It's not enough to just get an `Int` because the number of `return`s changes the type of that value. class Returns (lst :: [*]) where returns :: FigureOutHowToComputeThisTypeFrom lst instance Returns '[t] where returns = return instance Returns ts =&gt; Returns (t ': ts) where returns = return :&lt;|&gt; returns @ts 
Wow you can do front end? I need to get into this 
Where can I get the epub version? I looked at the site and haven't found any info on it.
I actually went to college with one of the authors. Talking with Daryl back when this was published a large part of the problem was just that there isn't enough data to get good statistical significance to these complicated regressions.
"Competitive" ;) Depends on location and experience, you'd have to negotiate that with our CTO. - Jonathan
This works, and it is probably the most reliable solution. I don't like that it is implemented with `unsafePerformIO` under the hood, but as I learn more about the GHC runtime, it appears that this is inevitable.
How would that work? Are `Conjunctive` and `Disjunctive` type families?
Both of those look to be amazing. I'm holding off on HSoM because there are rumors significant updates are coming down the pipeline. 
&gt; I could just do this by having the types turned into symbols, but I have not found a way to do that. If your types have Generic instances (neither `Int` nor `Text` do...), you can ask for the type's `Rep`, which is always an `M1 D` ("Metadata about a Datatype"), from which you can extract the type's name as a type-level Symbol. {-# LANGUAGE DataKinds, DeriveGeneric, TypeFamilies, UndecidableInstances #-} import Data.Text import GHC.Generics import GHC.TypeLits data User = MkUser deriving Generic -- | -- &gt;&gt;&gt; :kind! TypeName Int -- TypeName Int :: Symbol -- = "Int" -- &gt;&gt;&gt; :kind! TypeName Text -- TypeName Text :: Symbol -- = "Text" -- &gt;&gt;&gt; :kind! TypeName User -- TypeName User :: Symbol -- = "User" type family TypeName a :: Symbol where -- special case a few common types which don't have a Generic instance TypeName Int = "Int" TypeName Text = "Text" -- Generic case: TypeName (M1 D ('MetaData name _ _ _) f ()) = name TypeName a = TypeName (Rep a ()) 
They would be newtypes for which you can implement different `Monoid` instances with `FlexibleInstances`. newtype Conjunctive a = Conjunctive a newtype Disjunctive a = Disjunctive a instance Monoid (Conjunctive Bool) where mempty = coerce True mappend = coerce (&amp;&amp;) instance Monoid (Disjunctive Bool) where mempty = coerce False mappend = coerce (||)
http://sfultong.blogspot.com/2017/09/sil-explorations-in-non-turing.html This may already be somewhat out of date, because I've been experimenting with changes to the core grammar. The key features I want to hit are: * refinement typing * an LLVM backend * integrating an instruction giving the predicted time/space runtime behavior into the core grammar
You can find the epub version on the gumroad download page after purchasing the book. Below is the changelog. frankly though, the ereader version is good enough. &gt; - A *beta* version of the epub has been incorporated into the release alongside the usual screen and ereader optimized PDF files. This is mostly unstyled and plain as it is intended primarily for the readers that will be using screen readers. In order for the first chapter to be screen-read properly you will need to install math font support for your screen reader. Please see these websites for more information: &gt; &gt; * http://accessibility.psu.edu/math/mathml/ &gt; * https://github.com/mathjax/MathJax-docs/wiki/List-of-math-enabled-screen-readers &gt; &gt; More work will be done to hammer out more precise instructions for screen reader compatibility with the epub. 
Always good to remove dead code. It's a shame it will take three major releases though.
I am using emacs with intero to write haskell code (I am new at both emacs and haskell). When I do C-x C-l to get the intero repl, it is not finding the modules I loaded in my script. These modules are in the same directory and I can load the program fine in ghci. Any ideas how to make it work in intero?
There are some nice libraries that use Supercollider's sound synthesis engine although they don't have any of the GUIs and libraries that are available with "normal" SuperCollider, which means it is quite harder to manage node order, plot, scope, meter, etc.
I tracked alongside the original article. I chose to ignore the Ecto feature that didn't translate and that didn't make sense to me (changesets). Had there been code demonstrating validations with ActiveRecord and Ecto, I would've translated them to idiomatic Haskell.
I spent a weekend making music, and poking around the code, and learned 2 things. 1. Tidal is the coolest idea ever 2. It has _so_ many cool directions it could grow in. So while I'd say that people aren't using it for orchestral music (indeed, it's hard to just "compose" a piece as opposed to live play one) it's certainly possible. It's all based on samples, for starters, so... use the right ones and that'll take you a step in the right direction.
Check out [csound-expression](https://hackage.haskell.org/package/csound-expression), it's quick to get up and running.
I'll have to spend some time digesting Blum's theorem. However, the example of GCD doesn't seem particularly bad. To define GCD in a total language, first I'd make a definition of GCD in terms of a fixed point combinator, then I'd just replace the fixed point combinator with a church numeral and also apply an "abort" function to the end for the case where the iteration limit is reached. Sure, the runtime expansion of that expression would blow up given a naive runtime, but that seems to me to be more an issue of a naive runtime.
As they say in IRC - don't ask to ask, just ask. Identifying it as homework up front and being rather specific about what you tried and what issues you are having is a good way to get more help. The questions that get the least help usually are homework dumps where someone just posts the homework problem and no more.
I believe I had this error and fixed it by manually installing Cairo first.
It did not work: $ stack install cairo cairo-0.13.3.1: configure . . Configuring cairo-0.13.3.1... setup: The pkg-config package 'cairo' version &gt;=1.2.0 is required but it could not be found.
Then your language isn't total
I've been reading the e reader pdf version on my kindle paper white and it works really well. The ereader version that they have seems to be not designed for reading on the Kindle yet, but unless you need to resize the text the pdf version is really well formatted as each pdf page is the size of the kindle paper white screen. The kindle paper white ui doesn't know how to handle 10+ bookmarks though. 
I don't know if this related, but have you already installed gtk2hs-buildtools? I believe I also had to do that before installing helm.
Have a look a Language-server-protocol implementation for vim. It does Linting, Auto formatting, documet symbols etc.
[removed]
[removed]
Boy is that an agonizing read. I'm not sure how big the FP effect had to be, while also correlating with smaller tems and more complex domains, before anyone will accept any of these ideas. People say the effects here are small, but to me they're startlingly large. Almost suspiciously large.
[removed]
[removed]
`stack install cairo` (which is implied by `stack install helm`) installs the Haskell package named [cairo](http://hackage.haskell.org/package/cairo), which is a set of bindings to a C library which is also named [cairo](https://www.cairographics.org/). You must install that library separately via your OS's package manager, stack won't do that for you.
I have just installed gtk2hs-buildtools, but it did not help. $ stack install gtk2hs-buildtools Copying from /home/notooth/.stack/snapshots/x86_64-linux-nopie/lts-8.20/8.0.2/bin/gtk2hsC2hs to /home/notooth/.local/bin/gtk2hsC2hs Copying from /home/notooth/.stack/snapshots/x86_64-linux-nopie/lts-8.20/8.0.2/bin/gtk2hsHookGenerator to /home/notooth/.local/bin/gtk2hsHookGenerator Copying from /home/notooth/.stack/snapshots/x86_64-linux-nopie/lts-8.20/8.0.2/bin/gtk2hsTypeGen to /home/notooth/.local/bin/gtk2hsTypeGen Copied executables to /home/notooth/.local/bin: - gtk2hsC2hs - gtk2hsHookGenerator - gtk2hsTypeGen $ stack install cairo cairo-0.13.3.1: configure . . Configuring cairo-0.13.3.1... setup: The pkg-config package 'cairo' version &gt;=1.2.0 is required but it could not be found.
Your effect in this thread has been suspiciously large :)
I think you want totality in most places, but productivity in a few places. I also want W-types and M-types to be easily usable. I too, think that Turing completeness is overrated. I'm actually fine with disallowing all programs that take more than 2^128 CPU cycles before producing anything. If we get to the point of optional mortality and pure computronium, I'm willing to push that to 2^(2^128) cycles, but I think that's enough for anything in this universe.
So should I just use `exitcode` after all?
I just bought a Kindle:) PDF version is displying really well. I have no issues with it so I will just stick to it. Thank you for help
*sigh* would it have been so hard for them to make a decent parse error? It doesn't even mention the apparently faulty argument. Thanks for the help, though!
Thanks for the tips. I have just installed helm successfully.
Yes. The detailed test suite type is pretty much never used. Quoting myself from that issue: &gt; `exitcode-stdio-1.0` is roughly 44 times more popular [30,098 vs 677] than `detailed-0.9`
In the `Cabal`'s `HEAD` errors are: For missing `test-module`: /home/ogre/Documents/haskell-mega-repo/futurice-prelude/futurice-prelude.cabal:170: The 'test-module' field is required for the detailed-0.9 test suite type. For incorrect `test-module`: /home/ogre/Documents/haskell-mega-repo/futurice-prelude/futurice-prelude.cabal:172: Parse of field 'test-module' failed. Latter may be made better, but someone need to make a PR for that.
Thanks! That solves the type to `Symbol` issue I had.
I updated the code to automate the symbol part, but I am still have trouble with this. class Returns (lst :: [*]) where returns :: Server (InAndOut2API lst) instance Returns '[t] where returns = return instance Returns ts =&gt; Returns (t ': ts) where returns = return :&lt;|&gt; (returns @ts) Looks like it is having trouble with the types on the last part. • Couldn't match type ‘ServerT (InAndOut2API (t : ts)) Handler’ with ‘(a0 -&gt; m0 a0) :&lt;|&gt; ServerT (InAndOut2API ts) Handler’ Expected type: Server (InAndOut2API (t : ts)) Actual type: (a0 -&gt; m0 a0) :&lt;|&gt; ServerT (InAndOut2API ts) Handler The type variables ‘m0’, ‘a0’ are ambiguous • In the expression: return :&lt;|&gt; (returns @ts) In an equation for ‘returns’: returns = return :&lt;|&gt; (returns @ts) In the instance declaration for ‘Returns (t : ts)’ • Relevant bindings include returns :: Server (InAndOut2API (t : ts)) (bound at src/Servant/InAndOut.hs:64:3) 
&gt; Model.all &gt; Repo.all(App.Model) &gt; getAllUsers :: DB [Entity User] &gt; getAllUsers = selectList [] [] Shouldn't the Haskell one be: &gt; selectList [] [] :: DB [Entity User] Or the Ruby one be: &gt; def get_all_users; Model.all; end To make it more comparable? 
I recall reading about this on this reddit. Take a look at the source code. The reason is obvious once you do: https://github.com/ghc/ghc/blob/master/libraries/ghc-prim/GHC/Tuple.hs
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ghc/ghc/.../**Tuple.hs** (master → f6bca0c)](https://github.com/ghc/ghc/blob/f6bca0c5e3c53fa6f06949d4f997d0f1761ae06b/libraries/ghc-prim/GHC/Tuple.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply do011jo.)^.
Except it's not! There's no segmentation fault, at least not anymore - it appears to be a fairly arbitrary cap.
Just as an aside: The Kindle does not natively support ePub (at least my Paperwhite doesn't). Mobi is the format you'd want. However, with tools like Calibre, it is fairly straightforward to convert ePub to Mobi. Anyway, enjoy your Kindle!
In contrast to methods which are explained in detail, in studies like this I try to identify what the assumptions are. In particuar, what *is* software quality, and how is it modeled? Here, it appears to rest on _bug fixes_, and identifying _defective commits_
/u/gelisam gave the correct answer but maybe an example will help. So, "stack install cairo" requires a binary package. On Mac OS, for example, you need to also install the binary package cairo, e.g.: brew install cairo Depending on your OS, use your package tool to install the native cairo package before install package packages.
That's a pretty small difference. All of the things here seem to use polymorphism to determine what row to get. You have `Model.all` in Rails, where `all` dispatches on the model to figure out what to return. Ecto's `Repo.all(App.Model)` takes the model as an explicit parameter. Haskell uses types to determine what database, so selectList [] [] fails with an ambiguous type error at compile time. You either need to let type inference do it's thing, eg: records &lt;- selectList [] [] let names = map (userName . entityVal) records or you need to provide an explicit type signature somewhere: records &lt;- selectList ([] :: [Filter User]) [] -- or, with ScopedTypeVariables: records :: [Entity User] &lt;- selectList [] [] 
&gt; Ok, so that’s nice, now we have function combination for functors. But we have only defined our clever new function application for lists. The concept of automatically joining containers during function application is generic, so lets implement it for another functor. The canonical second example is Maybe: I think this paragraph is the first breaking point in this article. It could easily be interpreted as: &gt; &gt; But we have only defined our clever new function application for lists. &gt; Well, yea. This problem was pretty fundamental to lists. Converting a list to a list of lists and concatenating them is very specific to lists. &gt; &gt; The concept of automatically joining containers during function application is generic, so lets implement it for another functor. &gt; Wat. &gt; &gt; *proceeds to define it for maybe* &gt; Other than a similar type signature, that is completely unrelated. What on earth does it mean that `IO` is also one of these things? Granted, it's better to leave them finding things unrelated than it is to leave them finding false relationships. But I dunno, it seems to me like this article motivates `[]` and `Maybe` more than it motivates `Monad`.
62-tuples are more than big enough for most people, so probably nobody has seen the need to revisit the code, even if bigger tuples are possible now.
The point is that as a programmer you write in a total language, your `main` is describing only the inside of a `while` loop, or `fix`. Youre not allowed to use fix in your code, only once at the top level.
Also, if you want a 63-tuple, it's probably because you're interfacing with a database, or reading some other tabular data (`csv` files, free-form JSON objects, &amp;c), in which case you're actually looking for extensible, anonymous records -- not kludgy tuples... 
As explained in the SO answer, it's entirely arbitrary =P It's limited only by the number of declarations in `Tuple.hs`. But that obviously doesn't exactly scale. It'd be great if we had some way to talk about n-ary tuples. For instance, it'd be sweet if we had: data TUPLE (as :: '[Type]) = {- compiler magic or something -} type () = TUPLE '[] type (a, b) = TUPLE '[a, b] type (a, b, c) = TUPLE '[a, b, c] except with the type synonyms being n-ary syntactic sugar by the compiler. We can of course already define `TUPLE` sort of, in the form of `HList`. But the fact that it must be treated as an inductive, linked list style data structure is pretty awful for both the optimizer and performance in general. If values of type `TUPLE as` had the same runtime representation as today's tuples, and the optimizer were aware of this special type, I think it would work quite well.
ad - automatic differentiation library. When I was a beginner I was amazed by the way this worked. https://hackage.haskell.org/package/ad
A few crazy ideas after thinking about it for five minutes: * files are immutable, older versions are garbage collected. Advantage: if you made a mistake and need to rollback to a recent version, it's probably not garbage-collected yet so you can look at a recent older version. * a program may appear to instantly write its potentially-infinite output to a file, but in fact it's only storing a thunk pointing to the paused program. If you don't force your file, I guess it won't survive a power failure. a unix pipe-style composition is just sugar over writing the output of a program to a file and running the second program with that file as input. * Instead of installing applications via a package manager, all registered applications would be "lazily" installed, so they would only actually get install when you force that thunk it upon first use * GUI applications compose in some way, like instead of placing windows side by side or on top of each other, the OS would provide combinators like "two applications divided by a vertical draggable divider" and "a bunch of tabs to switch between multiple applications" edit: err, why did you edit your question to add "As arrogant functional programmers who think we can do everything better"?? I liked the idea of dreaming about an alternate world which was designed differently, but I'm not sure I like the premise anymore, at least not its wording.
&gt; the mutable file system creates tons of 'It works on my machine' configuration problems. Not to be rude or anything, but what would you do with an immutable file system?
First of all, forget files entirely. Secondly, you could have a datomic style database, where it's just an append-only value, accumulating facts. Saying immutable files are useless is the same as saying immutable values are useless in a functional language.
The language runtime can collect garbage when there are no references into a region of memory. What do you do with garbage in an append-only filesystem with finite storage capacity?
Same thing, I'd like to treat the disk as just another level of cache, so it's effectively a part of the program's memory and would be garbage collected. The datomic example is another solution which does not consider space-outage.
&gt; Just as an aside: The Kindle does not natively support ePub (at least my Paperwhite doesn't). Mobi is the format you'd want. However, with tools like Calibre, it is fairly straightforward to convert ePub to Mobi. I could be wrong but if you use your amazon email address it automatically converts epub to mobi. Either way people should try using the ereader PDF before the epub (which is far less mature and tested)
I don't understand most of your points. It'd be awesome if you could explain further. &gt; the mutable file system creates tons of 'It works on my machine' configuration problems. The problem is primarily library versioning and non-reproducible builds, not the filesystem. I don't know why you are blaming the filesystem for this. If you have an append-only filesystem, you can very well have issues because you installed libraries in the wrong order and yay, you have to nuke your OS? &gt; programs are just arbitrary binaries that do not allow any static analysis or restrictions. Well you have file permissions for restrictions. &gt; too many concepts and moving parts (process hierarchy, environment variables, etc...). These all deal with different things? While I agree that having declarative configuration files for everything would be nice instead of environment variables, I don't really understand what's the problem with process hierarchy? &gt; multi user is mostly obsolete. Um what? Nobody uses multiple users on the same computer? Admin accounts and user accounts? &gt; treating persistence as IO rather than just another level of caching in the memory pyramid. You are talking to another program when you are doing IO (the OS). When you are working on your own doing random computations in memory, you are not involving OS (well mostly), only your own. There is a fundamental difference? The OS tries to help your code by caching files (c.f. disk buffer cache) but there's only so much it can do when there are a bunch of processes running and requesting memory.
Well, don't you think "not considering space-outage" is a big deal for an OS?
Haha yep. GHCJS allows you to frontend. I personally use the react-hs front end library, but there is also reflex-dom. 
Submit a proposal?
with pure, total functional programming, you completely redesign OS scheduling. Rather than using timer-based interrupts and having a lot of overhead in swapping process states in and out of memory, you just chop each processes' AST up into bitesize chunks that you can execute to finish. It should make for the perfect RTOS.
There would be *lots* if details to work out. For example, how do you handle the ability to write `instance Functor ((,) a) where ...`?
Thanks for your comment. &gt; &gt; &gt; But we have only defined our clever new function application for lists. &gt; &gt; &gt; &gt; Well, yea. This problem was pretty fundamental to lists. Converting a list to a list of lists and concatenating them is very specific to lists. Yes, the concrete example is specific to lists. But joining functors isn't (which should be actually one of the points of my article). I took `[]` as a first example, as it is one of the most intellectual accessible Functors for many programmers. &gt; &gt; &gt; The concept of automatically joining containers during function application is generic, so lets implement it for another functor. &gt; &gt; &gt; &gt; Wat. &gt; &gt; &gt; &gt; &gt; *proceeds to define it for maybe* &gt; &gt; &gt; &gt; Other than a similar type signature, that is completely unrelated. I don't think so -- obviously, otherwise I wouldn't have written it this way. Let me try to clarify my approach: 1. *The practical approach:* the reader is expected to already have a rough concept of functors. It should be known that there is `fmap` to operate on the “inside” of them, so I hope to get across the idea, that if you are operating on more than one instance of a functor, it can be necessary to handle the “outside” in a meaningful way, too. 2. *The theoretical approach:* I don't talk in my article about theory, but this is actually very close to the definition of a monad in category theory: An endofunctor with the two natural transformations `η: 1 → T` and `µ: T² → T`, where `µ` is our *bind*. Note that it is about `joining` the product of two `T` into one. The whole explanation might even come out nicer, if I'd stayed with the problem that `fmap`-ing a function `a → F a` to a functor `F` yields `F (F a)` and then defined `join`, eventually ending up with the definition of bind being: `(&gt;&gt;=) :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b` `(&gt;&gt;=) ma f = join $ fmap f ma` The reason I didn't do so (even though I like the approach very much from a didactically point of view), is that it doesn't match Haskell's definition of the `Monad` type class, where everything (including `join`) is build from the mandatory definition of `&gt;&gt;=`. &gt; What on earth does it mean that `IO`p is also one of these things? Yeah, I confess, that I'm deliberately leaving explaining that to others... ;-) &gt; Granted, it's better to leave them finding things unrelated than it is to leave them finding false relationships. But I dunno, it seems to me like this article motivates `[]` and `Maybe` more than it motivates Monad. I'm sorry, that my approach doesn't work for you. I have to accept that. But I was really hoping, that the `Logger` example would illustrate, that `[]` and `Maybe` were merely first, simple examples and not the primary point of the article. :-/ [_EDIT:_ please forgive the strange formatting of the example bind definition, I failed to get a "preformatted" block] 
It's meant as a joke, we Haskellers I think seem a little distant from the outside,talking about category theory, catamorphisms and moands, we have a whole different vocabulary and we do things very differently and claim it's way better (which I think it is). 
That's interesting!
Very much so. I didn't mean to imply it'd be easy, but it would be interesting :)
1) The reproducabillity problem is a real problem that's caused by the dependence upon config files and other persistent state on the file system, that's why projects like Docker are successful which containerize everything. To me those things represent bandages upon a bad system. 2) That's nothing. What can you say about the interface that the program provides to the world? What are it's dependencies? What kind of resources it requires? I want to treat programs much like modules in Haskell, they expose an interface and require dependencies. This also entails built-in package management. 3) Most computers are PCs or mobile phones with only one person behind them, the distinction between Admin and User is from an old age of mainframes and time-sharing. 4) I'm not sure what is your last point.
Depends on the purpose, for a company managing a server farm that's practically non-existent, disk is cheap and they just pile on more and more disks to keep up with the generated data. That's why Datomic is okay with this(according to Rich Hickey). For personal use, It's a problem.
&gt; * the mutable file system creates tons of 'It works on my machine' configuration problems. Look at [NixOS](https://nixos.org/). It seems to solve this problem. &gt; * programs are just arbitrary binaries that do not allow any static analysis or restrictions. Computers are von Neumann machines. This issue is inherent to them, so long as the machine runs compiled machine code. There are lots of efforts to solve this, and almost all of them boil down to moving the abstraction onto a virtual machine that runs an AST and little else. A friend of mine [tried to solve this](https://github.com/daeken/RenrakuOS) with the .NET virtual machine. SmallTalk and Lisp systems also come to mind. &gt; * too many concepts and moving parts (process hierarchy, environment variables, etc...). Some of this is unavoidable, I think, unless you restrict all software to a single unified virtual machine, like Squeak and other SmallTalk VMs do. &gt; * multi user is mostly obsolete. There's value, I think, in segmenting permissions by responsibility. &gt; * treating persistence as IO rather than just another level of caching in the memory pyramid. Trouble is, no one has ever found a coherent way to do that that isn't an absolute mess. HFS/HFS+, for example. &gt; * not taking the internet into account from the very start. See [Plan9.](https://9p.io/plan9/) &gt; forgetting anything you know about operating systems, especially the Unix tradition (this can be hard), Nope, I can't do that. I've been thinking about this problem for too long and I've had too much exposure to too many platforms. I've also been exposed too much to CPU architecture design to step away from it and think about the userland. &gt; how would you design a modern operating system based on pure functional programming? I'd probably revive Lisp machines and/or build a platform out of something like Erlang, Lisp, [Pony](https://www.ponylang.org/), or SmallTalk, on a .NET CIL, JVM, ErlangVM, SmallTalk VM, or other system designed with the appropriate abstractions.
The various instances of `(,)` might be more a bug than a feature :-) We have the `Writer` type for when you want the instances.
&gt; If you don't force your file, I guess it won't survive a power failure. Or store the state of the paused program on disk!
In general being able to define contiguous structures in Haskell in a first class way the same you can define linked/inductive structures in Haskell in a first class way would be fantastic. Maybe type level contiguous arrays are one solution. And then you can define a type to have internal structure equal to a type level contiguous array of your choosing, and then indexing into this contiguous structure has type equivalent to the type you get when you index into the type level contiguous structure. This would allow you to write heterogeneous arrays and hashtables (without any `unsafeCoerce` or similar), as well as arbitrarily large tuples.
One of the major design goals of an Operating System is to allow the execution of arbitrary software. Something that necessitated that kind of design consideration no longer meets that design goal, and would therefor no longer be a success as an operating system. Whether or not it's still useful as an environment to execute software is another question - It certainly seems like an interesting idea.
The type level arrays don’t need to be contiguous. There’s no reason that stuff can’t be done with type level linked lists. The problem is just defining contiguous data structures for runtime
I was thinking about this the other day when I cat'ed a large file that some sort of lazy evaluation would be great in a shell environment
I'm worried that with an inductively defined type level list the implementation might lead to `O(n)` indexing. As anything that even remotely resembles: instance Indexable (n - 1) as =&gt; Indexable n (a : as) where type Index n (a : as) = Index (n - 1) as index ... Will take `O(n)` time unless everything is fully inlined (which I'm guessing will not happen on a very large list, as GHC would have to repeatedly inline thousands of times), regardless of whether the value level list is contiguous or not. With some compiler magic the above should be avoidable, but just starting out with a contiguous type level array avoids this all together.
That's working under the assumption that the only way to write code for type level linked lists is with inductive type class instances. We don't yet have a spec for how you would model contiguous data structures with type level lists, so I can't say this definitively, but I would be surprised if that model depended on the type level list being contiguous. Any model should not really care about the implementation of the type level data structure, as long as it can be indexed and traversed. Now, compile time performance may suffer with linked lists, but that's a whole other consideration.
I like qualified imports, but I feel like Atom's Haskell syntax highlighter makes them too prominent. I wrote this tiny package to reduce their opacity to 50% unless you're hovering over them. [This GIF](https://gist.githubusercontent.com/tfausak/58694a3bf5d9fb8671a5d8ce9b1d1f03/raw/e5f2fc703f4fb0a728c87980871c7a72c78ee520/atom-haskell-scry.gif) shows what it looks like. 
Well, okay then. So what's the prevailing way to write these tests? Should I use assertions or something?
An editor that could -never- delete a document. An environment where everything had to be JiT'd and pass analysis in order to execute. A computer that only allowed a single user. A computer that failed to function properly when your ISP encounters a service outage. This sounds kind of like chromeOS, but worse.
A friend of mine wrote his MSc thesis on using Typed Assembly Language with Rust's borrowing semantics. It holds promise as a way to write statically checked executables that allow the full range of desirable functionality. And no management necessary, since the code loader can prove the program is well behaved.
Yes, I understand that. I thought your main post was talking about Linux alternatives (i.e. usable by everyone) not just big companies...
Might there be a use case for some sort of code generation thing? That is, code written and run by programs, not people. Probably pretty niche if it ever even comes up, but still.
Your `(x:xs)` case only modifies the list's `head` without removing it, so the list passed into the recursive call is the same length, leading to infinite recursion, repeatedly subtracting the same `head xs` from the same accumulator value. FFR, it's pretty hard to advise someone who hasn't given an explicit problem statement, but I'm assuming you want to compute `x1 - x2 - ... - xn`. The idiomatic solution would be `foldl subtract (head xs) (tail xs)`, or `foldl1 subtract xs`, because we like using higher-order functions to avoid writing code that more or less already exists. If you want to define this function recursively, you need to actually apply some kind of function to the result of your recursive call, for example: `length (x:xs) = 1 + length xs` and of course only recursively call the function on the tail of the list. There's a subreddit called /r/haskellquestions for exactly this kind of problem.
Although I think you could get by with `Control.Exception.assert`, [HUnit](https://github.com/hspec/HUnit) is the minimum viable test suite. I usually end up using [hspec](https://github.com/hspec/hspec).
Don't get so hung up on the Datomic thing, it's just an example. Also I'm not claiming to have all the answers, just proposing some ideas.
Could you give an example of some js programming with haskell? Maybe just link me to a git repo?
&gt;It is an unwritten law, that everybody learning Haskell sooner or later has to write a tutorial or explanation of the topic of monads. Uh, I think as far as laws go, the consensus I've seen is that people are writing too many monad tutorials?
Not nesserally Just-in-time, it could be checked or compiled beforehand. And power outages could be dealt with, I think, using some kind of checkpoints system that allows the computer to recompute all the lost values if needed(this is good only with pure computations). Don't know about the ChromeOS thing... 
&gt; a unix pipe-style composition is just sugar over writing the output of a program to a file and running the second program with that file as input. Isn't that how it is right now? A pipe just connects stdout of the program on the left to stdin of the program on the right.
This function says: when function `sub` is called with some list where the first item is `x`, and the rest of that list is `xs` call `sub` on the result of `subtract` where `subtract` is called on the result of calling `head` on `xs`, and the value `x` and the result of the previous operation is appended to the head of `xs` So you're infinitely popping the first item off, subtracting it with the second item, and popping it back onto the original list. I think what you want is: sub :: [Float] -&gt; Float sub [] = 0 sub (x : xs) = (subtract x) (sub xs) You either want that, or: sub :: [Float] -&gt; Float sub [] = 0 sub (x : xs) = (minus x) (sub xs) where minus = flip subtract Depending on what you're trying to subtract from what. 'where' is a keyword that defines a locally scoped term. 'flip' is a function that reverses the first two arguments of a function. So, `subtract 3 5 == (flip subtract) 5 3` You could've also written: `where minus x y = subtract y x`
Look at the [reference type system](https://www.ponylang.org/media/papers/fast-cheap.pdf) for Pony. It provides a way to write statically checked systems that *almost* eliminates the garbage collector's work. [Its GC](https://www.ponylang.org/media/papers/OGC.pdf) does almost no work, as a direct consequence of its type system. It's *weird.* It virtually guarantees that, barring compiler bugs and FFI, a Pony program is automatically provably well-behaved. It still kinda blows my mind that a good type system can be so powerful.
Thanks, that covers a lot of my questions. Also thanks again for warning about that subreddit, I didn't know about that.
&gt; Most computers are PCs or mobile phones with only one person behind them, the distinction between Admin and User is from an old age of mainframes and time-sharing. AFAIK (although tbf I only know this information second-hand), this is used very commonly in enterprises (think accounting etc.). The IT department has admin access to your PC, and you can't just install random stuff, in order to prevent malware etc. Also useful for public computers, such as those in a library :D. Another use case I can think of is parental controls in PCs for kids. &gt; 4) I'm not sure what is your last point. Um my phrasing seems poor in hindsight, perhaps because I didn't fully get your point. Could you explain what you meant by "treating persistence as IO rather than just another level of caching in the memory pyramid."? --- IMHO, the reason that code works and is pervasive today is because we build abstractions in layers. The OS is at the bottom-most layer as an application programmer. The language is a level above that. What you're trying to do (I think) is mash both of them together, so that your new OS offers you all the nice features that Haskell provides you today (say package management via Stack and interface management via GHC). My suspicion (correct me if I'm wrong here) is that you're feeling that all packages (not just Haskell ones) should be doing this stuff for a better user experience overall. Which is why we need these features inside the OS. Well, if that's the case, it is unfortunate but you are going to have a hard time bringing other languages (compilers/interpreters/package managers etc.) to target your OS with your demands of all this metadata regarding the code...
I think that was a joke.
If it was sufficiently broken before, it should be sufficient to turn it into a do-nothing operation and change the documentation to say that it does nothing. Of course, that's only okay if it was so broken that no one could be relying on it.
NixOS is kind of cool, it is a bit like an immutable file system with the hash based names. Plan 9 is sort of what I was thinking when I started to think about this problem, but as I dag deeper I discovered that this approach doesn't really work on distributed systems, you can't really hide the cost model from the programmer and provide a unified interface. Rather than that, I thought we would treat each computer as an actor (per the actor model) and have a protocol to connect all the computers running the same OS. So each computer exposes an interface. Storage could be made more functional by using hash-based addressing like in IPFS. Yes I was thinking of baking a managed VM into the OS. Also with a built-in compiler and package manager. A program in my world is just a module that exposes an interface and has dependencies to other modules. All user-interaction is done through a REPL like ghci, you input nomadic do-lines and the previous name bindings (`x &lt;- return 5`) will be persisted such on the next restart you continue from where you left. in the REPL you can invoke functions from modules and you'd have some primitives that do parallelism and other impure stuff(in a monadic way of course). So that's essentially what I have in mind.
Looks like I didn't explain well what I want. I want to input into ghci a list like [5,4,3], and it will subtract 4 from 5, and 3 from the result of that, (and return it -&gt; edit). And probably a deeper explanation on what exactly the function is doing would be very nice. (I really want to understand FP and recursivity but its not being easy) Thanks. PS: Why would I use "flip" if it inverts arguments in a function? Why not type them already flipped? EDIT: Clarification on what I want to do.
You're right, but I think u/cies010 meant that they should all either be expressions or declarations. In my opinion, showing the Haskell version as a declaration makes it look worse (or at least more verbose). I haven't actually tried it, but I think `TypeApplications` should let you do something snazzy like `selectList @User [] []`. 
1) Fair enough. But I do think that of all things, access control should not be baked into the OS. What Unix provides is a very limited access control mechanism. You want to leave it to the software above that has much more domain knowledge and can actually provide much more intelligent decisions. 2) You can look at my other comments about this persistence thing. Basically what I mean is that we treat the disk as an extension to the RAM, much like the RAM is an extension to the CPU cache, and it is an extension of the CPU registers. I want your program to be a monad where previous bindings are persisted through restarts. 3) That's precisely the thing. I want a core language that's more high level than machine code and it's semantics to be the bottom level of abstractions. To treat the computer as a machine that evaluates expressions in this language. Other languages are welcome but will have to be compiled to this core language not machine code. The OS should provide an execution environment for other programs, I disagree with the kind of environment currently exists and the kind of programs you need to write in order to execute. Basically, in Haskell everything is pure, and all impurity is shoved into the Haskell run-time which interpreters the monadic value that your return from `main`. So we've moved impurity outside our program, what if we move it further such that the process is pure, and further and further until everything is absolutely pure and the only impurity is in the operating system itself which interprets an expression and actually performs the IO.
To provide a bit more direction, there is a video that I like that talks about this. I don't agree with him about everything and I think i'm more of a fundamentalist. But here it is: https://www.youtube.com/watch?v=L9v4Mg8wi4U Let me know what you think.
Isn't that what log-structured file systems are about? You can choose to limit the no. of versions of a file, but that reclamation might be expensive. 
&gt; multi user is mostly obsolete. Umm, no. Any academic or large company institution will tell you you're wrong on that one. 
I've tried it. The monad comes first in the `forall` for `selectList`, so it's `selectList @_ @User [] []`. 
You got it wrong, which amply demonstrates why I didn't include it in the blog post: non-trivial uses of `TypeApplications` are confusing. https://github.com/bitemyapp/persistent-activerecord-ecto/blob/master/app/Main.hs#L92
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [bitemyapp/persistent-activerecord-ecto/.../**Main.hs#L92** (master → 986d218)](https://github.com/bitemyapp/persistent-activerecord-ecto/blob/986d21819a902ba12a21713280c365ec2b9b7bed/app/Main.hs#L92) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply do0g4ef.)^.
Yes, look at my second comment to @theindigamer
I’m not trying to say you were *wrong* about anything. Just that I think that one paragraph was a massive mental leap for most learners. There’s no shortage of tutorials out there that start from some particular monad and show how some other type can implement the same signature. In general, I don’t think this works. It just entangles the examples in the reader’s mind. That said, starting from an understanding of Functor is a nice approach
The yesod scaffolding site is exactly for that purpose. Also the yesod book covers the fundamental well.
Ah, bummer. Type applications are pretty fiddly. It might be worthwhile to put an explicit `forall` on `selectList` and friends so that it could work without `@_`. On the other hand, that makes the order of type arguments part of the public API. ¯\\\_(ツ)\_/¯
That's the approach that Idris takes in Type Driven Development, you end with a concept of `fuel`. Which decreases with each iteration so you can convince the termination checker. Then you have just one bit of your code that's partial that produces infinite fuel. But you're essentially stepping outside the soundness of the logic for that part of the program and you're back to the problems introduced by bottom. I think this is the same thing as your top level fix definition.
This has nothing to do with Haskell. This grammar is *almost* in [EBNF](https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form), so that might help. It's also sloppy, because it doesn't indicate where / if / when whitespace is singificant. --- The first line says a `num` is any single decimal digit. The second like says a an `int` is either a `num` (by itself) or a `num` followed by an `int`. Example `int`s: 3, 03, 403 The third line gives 4 forms for an `expr`. Example `expr`s: 3, 403, ---403, +3403 -+-4*-03430
**Extended Backus–Naur form** In computer science, extended Backus-Naur form (EBNF) is a family of metasyntax notations, any of which can be used to express a context-free grammar. EBNF is used to make a formal description of a formal language which can be a computer programming language. They are extensions of the basic Backus–Naur form (BNF) metasyntax notation. The earliest EBNF was originally developed by Niklaus Wirth incorporating some of the concepts (with a different syntax and notation) from Wirth syntax notation. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
Wouldn't that cause GCD to fail for large enough numbers? So to make it total you'd need the result to be a Maybe type or something right? Or give it infinite fuel, which amounts to making an external argument to the termination of GCD. Another example is the Collatz conjecture which is not super useful I guess but still an open problem in mathematics. I think Idris hits the right balance where you can choose which parts of your program to make total and step out of the totality checker when it's needed or much simpler. I need to checkout NuPRL some more. 
Types correspond to propositions, and programs correspond to proofs. Therefore Nat is a proposition for which we are willing to accept any natural number as a proof, whereas Bool is a proposition for which we are willing to accept any boolean as a proof. The proposition corresponding to the type `Nat -&gt; Bool` is `Nat → Bool`, that is, that the proposition Nat entails the proposition Bool. Since there are plenty of proofs for both propositions, those propositions are clearly true, so if you take the classical logic interpretation that every proposition is either true (⊤) or false (⊥), the proposition `Nat → Bool` is equivalent to `⊤ → ⊤`. Not nearly as exciting as `a -&gt; b`! The type corresponding to the proposition `∀ a b. a → b` is `forall a b. a -&gt; b`. This time you can't use values of type `b` to write a trivial proof because you don't know anything about the type `b`, so in particular you don't know any value of type `b`. A more interesting example is the proposition `∀ a. a → a`, which corresponds to the type `forall a. a -&gt; a`. This time you still don't know much about `a`, but you do receive a value of type `a` as input, so you can return it. Therefore, the identity function is a proof that A entails A!
&gt; Why would I use "flip" if it inverts arguments in a function? Why not type them already flipped? In case you haven't given them names, yet. (Internet search for "point-free" to see some examples.)
Sure - So, in my first definition, it is more or less the same as yours above, but it's saying 'call subtract `x` and the result of `sub xs`' `subtract x y` says 'subtract x from y' - So it's (y - x), not (x - y), which can be confusing. So, in full, the first function says: For function sub given a list, subtract the first item from that list from the result of sub as called against the rest of that list Note, key thing to understand about lists: `[x]` is the same as `(x:[])`. And you defined sub as returning `0` when passed an empty list. So, my first definition of sub starts evaluating list [3,2,1] - Round 1: sub is called on `3:restoflist` `subtract 3 (sub restoflist)` Ok, now we call sub again on the rest of the list `subtract 3 (subtract 2 (sub restoflist))` and again: `subtract 3 (subtract 2 (subtract 1 (sub restoflist)))` And one last time: `subtract 3 (subtract 2 (subtract 1 (0)))` So that's : `0 - 1 - 2 - 3` For a final result of `-6`. The other definition works the same way, it just flips the order of the args passed to subtract. So it'd look like: `subtract (subtract (subtract 1 0) 2) 3` Or, `3 - 2 - 1 - 0` We could also have defined it like: sub :: [Float] -&gt; Float sub [] = 0 sub (x : xs) = (subtract (sub xs) x) But generally, I like to write things out to flow from right to left. For bonus points, the reason we do that, is because idiomatic haskell tends to use `.` and `$` instead of parentheses. `sub (x:xs) = subtract x . sub $ xs` instead of `sub (x:xs) = subtract x (sub xs)` That means it's really easy to go along and toss in another function in between, in case we ever want to do anything else to the result of `(sub xs)`, and we don't have to go back and edit to make sure that all our parentheses line up, we just add a `.` in the right place. For a real short example like this, it's not much of a difference, but when you're chaining 5-6 functions together, it really saves on the cognitive overhead when you need to add an operation in there somewhere, or re-order things. So, I used flip because of force of habit, but, it's a good habit. 
https://hackage.haskell.org/package/is-0.4/docs/Data-Generics-Is-TH.html
not OP but could you link what you are referring to? Which books?
You can do that using `Prism`s and `is`.
The book link is here: http://www.yesodweb.com/book It also has a scaffolding section in it: http://www.yesodweb.com/book/scaffolding-and-the-site-template
Lens's `is` and `isn't`, combined with `Prism`s, provide this: {-# LANGUAGE TemplateHaskell #-} module Demo where import Control.Lens import Control.Lens.Extras data Whatever = What | Ever makePrisms ''Whatever isWhat :: Whatever -&gt; Bool isWhat = is _What main :: IO () main = do putStrLn $ if isWhat What then "I have a What" else "I have an Ever" putStrLn $ if is _What Ever then "I have a What" else "I have an Ever" putStrLn $ if isn't _Ever What then "What is not Ever" else "What is Ever"
Sorry, I did a bad job of being specific here. To clarify - I'm specifically making the point that these seem like bad design tradeoffs for the specific purpose of a general purpose desktop operating system. If you're compiling everything from source, that means the average installation gets significantly more complex, which means slower, and more points of failure. You could do something similar to nix, and have some kind of package trust where someone else does some of this, but you still have the problem of taking multiple languages and giving them all a homogenous set of static code analysis tools, which sounds like an awfully ambitious project that could severely raise the barrier of entry for new languages into the OS. And, again, more complexity, so more points of failure. For the outages, I was talking about being unable to connect to the internet, which for many using personal computers, is a fact of life with a non-trivial rate of occurrence. Not everyone is networked all the time, or wants to be. For me, if my OS necessitated a connection to the internet to function in any significant way, that'd definitely be a signal to swap to a new OS. Also, not everyone gets to have their own computer in every household. So, multi-user is fairly critical to the desktop OS, by pure virtue of cost. My ultimate point is that what you're driving at seems to be closer to like, an ideal development environment OS, or an ideal microservice OS, not a general purpose operating system - The stuff you're talking about axing in your OP are some pretty mission critical features for a lot of general purpose OS applications.
I was under the impression that one has to finish before the other began, but I don't know.
&gt; a unix pipe-style composition is just sugar over writing the output of a program to a file and running the second program with that file as input. Isn't that how it is right now? A pipe just connects stdout of the program on the left to stdin of the program on the right.
I actually meant to reply to the parent comment, but now that you mention it: Piped commands run concurrently, but they may have to block while waiting for input.
[Here](https://github.com/liqula/react-hs/tree/master/react-hs-examples) is a few examples for how to do it with `react-hs`.
I like this idea! I write more Elm than Haskell, but I may put something equivalent in my style sheet and try it out.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [liqula/react-hs/.../**react-hs-examples** (master → 36aaeeb)](https://github.com/liqula/react-hs/tree/36aaeeb0ce6ea4030200f60eb0df7ff0959bfac0/react-hs-examples) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply do0j9tq.)^.
[Here](https://github.com/liqula/react-hs/tree/master/react-hs-examples) are a few examples for how to do it with `react-hs`.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [liqula/react-hs/.../**react-hs-examples** (master → 36aaeeb)](https://github.com/liqula/react-hs/tree/36aaeeb0ce6ea4030200f60eb0df7ff0959bfac0/react-hs-examples) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply do0jad8.)^.
Yeah I definitely agree that it shouldn't be impossible to do it with a type level linked list. I am just somewhat nervous, but if it turns out that's easier to use type level linked lists carefully than it is to use a contiguous type level list then that is fine.
thanks :)
 data Cons h t = Cons h !t data Tip = Tip foo :: Cons Int (Cons Int Tip) foo = Cons 1 (Cons 4 Tip) That might be slow though. Not sure how GHC optimizes this stuff. Something like `foo :: Proxy 2 -&gt; Int` might work but that doesn't allow for different types for different indices. So you would need something like: type family Index (t :: Nat) where Index 0 = Integer Index 1 = String Index n = TypeError (Text "Index not found.") tuple :: Proxy n -&gt; Index n but that is awkward. Also, type families don't allow for quantification.
This is the HList style solution. It’s a very slow inductive type with linear time access. Access needs to be constant time.
&gt; Wouldn't that cause GCD to fail for large enough numbers? That's correct. For the language I'm working on, I'm adding refinement typing so that the domain of functions can be limited to what they will correctly compute. I haven't fully thought it out, but my hunch is that the general purpose computing can be made pragmatic using this system, probably with a lot of help from the compiler.
Read up on the EROS research operating system from the 90s and its predecessor KeyKOS form the 80s. More than half of the problems you list were solved better. [House](https://en.wikipedia.org/wiki/House_(operating_system\)) is also worth looking into, being an operating system written in Haskell.
I think what you're trying to get at is the notion of a partiality monad (or totality comonad). You can use `fix`, but now you're tainted into the world of partial functions. Better yet, you can have a computational complexity monad, where return types are tainted with their complexity and there are injections (e.g. subtyping witnesses) along the complexity hierarchy; an infinite set of functions that have types like `inject : (Π_(a : t) O(f(a)^2)(b)) -&gt; (Π_(a : t) O(f(a)^3)(b))`. You can check these subtyping relations with an SMT solver, though I'm not sure how feasible it is for nontrivial examples.
There may be a delay between a write() on one side of the pipe and the data being available to a read() on the other side of the pipe, but it's often very small. In most OSes there is a maximum amount of data that can be write()n to a pipe and not read() before the next write() is partial / blocks. Usually that's not a lot of data -- 4k or so.
You basically shouldn't ever use functions like that. Using them is likely to cause [Boolean blindness](https://existentialtype.wordpress.com/2011/03/15/boolean-blindness/). You should case split instead. For enumerations, these types of functions are mostly fine. But, something like `if nil xs then head xs else default` are decidedly anti-patterns; due to the unnecessary use of partial functions. (Also, the bug that typechecks here, but wouldn't typecheck if written as a case block.)
There HAS to be a better way of writing that.
It doesn't solve all possible variants of this case for sure, but... λ&gt;listWhatever = [What,Ever,What,What] λ&gt;[ n| n@(What) &lt;- listWhatever ] [What,What,What] It's a -really- terse list comprehension. Don't forget, too, that you can do stuff like: λ&gt;data MoreStuff f g n = Stuff f g | Little n deriving (Show) λ&gt;somelist = [Stuff 5 6, Little 7, Stuff 8 9] λ&gt;[ x | x@(Stuff{})&lt;- somelist] [Stuff 5 6,Stuff 8 9] To capture multiparameter constructors without typing a bunch. Seems like if you're mostly just filtering lists, this is probably a better solution than pulling out the big guns.
While NixOS doesn't implement all the stuff you've been talking about, I think NixOS is _such_ a huge improvement over what you see elsewhere in OS design that anyone who wants to theorize about how to improve OSes better have used NixOS in anger. This is analogous to the fact that, although Haskell is an incomplete implementation of FP ideas compared to, say, Idris, it has such a mature and developed ecosystem that by working with it you'll get a better perspective of the challenges and tradeoffs in language design (in this analogy, MS Windows would be COBOL, macOS would be Pascal, Debian would be C, Gentoo would be C++, Arch would be Java, NixOS would be Haskell, and Genode/seL4 would be Idris). It may not look like it, but NixOS is surprisingly production-ready (Awake Security ships a product running it, IOHK runs it on the Cardano core nodes, etc.), and the community around it is fantastic (especially `#nixos` on FreeNode). BTW, if you want more exposure to NixOS's ideas (beyond the advertising copy on the website), you might be interested in the [PhD thesis](https://nixos.org/~eelco/pubs/phd-thesis.pdf) in which Eelco Dolstra originally described it.
Agreed, most programmers and computer scientists don't know as much about capability theory as they probably should.
Ah ok, that makes a lot of sense, thank you. So does that mean that the STLC is rather useless for proving things then? I was under the impression that it was equivalent to natural deduction on propositional logic, however if without polymorphism it can't prove general results, and all base types can be returned trivially by using their values, it doesn't seem particularly useful.
Changesets are one of the better aspects of Ecto, providing a way to describe the updates to a record that will eventually be submitted to the DB in a data structure. Ecto.Multi can then be used to compose multiple Changesets that should all be executed transactionally. Validations on Changesets gives you a chance to check the consistency of the data, such as ensuring required fields aren't set to nil before updating the DB and potentially getting a constraint error.
A lot of dependent type stuff *is* unnecessary for day-to-day programming. But some of it would be really convenient, regardless. Just because dependent types were initially picked up by the theorem proving crowd doesn't mean they're the only ones who can get value out of them. 
It's not an existential, though. It's a universal. Edit: To distinguish, look at the type of the constructor. If the constructor will accept an argument of any type (and it doesn't reflect that type in the result type), it's an existential. If the constructor requires an argument that works for all types, it's a universal. 
Hmm! That's true, the STLC doesn't have explicit `forall`s. Does natural deduction on propositional logic have ∀s? By any chance, are they restricted to always be at the outermost level of the proposition, e.g. `∀ a b. a → b` would be allowed, but not `(∀ a b. a → b) → (∀ a b. a → b)`?
&gt;1) Fair enough. But I do think that of all things, access control should not be baked into the OS. What Unix provides is a very limited access control mechanism. You want to leave it to the software above that has much more domain knowledge and can actually provide much more intelligent decisions. The entire point of an Operating System boils down to controlling access to and allocation of resources (CPU time, filesystem access, memory, etc), to ensure that all user programs running above it play nice with each other. If you don't want it to do that, what's left?
Copy on write filesystems such as zfs or tfs treat files as immutable too. 
Also BtrFS.
Yep. That's what zfs does.
Thank you very much for pointing out! Although I've mentioned that the functions are "series of equations" in many parts of the same article, this stanza contradicts that, somewhat apologetically. I just removed that confusion now.! It pays to get these reviewed by some hakellers here, and thanks again
[todobackend](https://www.todobackend.com/) has some Haskell examples. Seems like they all use [Persistent](https://www.stackage.org/package/persistent), which I guess is described in the [yesodweb book](http://www.yesodweb.com/book/persistent). Not sure if the examples there will answer your specific questions, but I find the site fascinating, even if it is a pretty basic API.
You can make a value required in the Haskell type if you don't want it to be nullable. We have some helpers for validating/filtering some upsert code too that works on a similar principle.
It seems to me as though every part of it that I personally see lots of value in does not really need dependent types. Unified type and term level synax: does not require dependent types. `Int` and `Char` on the type level: does not require dependent types. Dependent types means `value -&gt; type` which I personally don't care about. I just care about all the things that seem to have been tacked on to dependent types despite the fact that there is nothing "dependent" about them.
Now I'm curious, does `lens` have a lens for `_63`??
In fact it only goes to `_19`. But oh my how many instances are needed even for those! http://hackage.haskell.org/package/lens-4.15.4/docs/Control-Lens-Tuple.html
Also Coyotos, which is supposed to be the EROS successor.
The state of exception handling in Haskell seems frankly, frightening. Library writers can export "pure" functions that throw unchecked exceptions, and to my knowledge, I have no way of knowing this. I could easily use such a function in my own actually pure functions, and be getting side effects that I only find out about by reading a library's documentation after my program inexplicably crashes. I love Haskell, but this is my biggest concern with it right now. Is there a way to solve this defect via code, or at least is there an upcoming GHC version that will deal with this stuff in some way?
Why is supporting tuples of size greater than 2 important? Why not just use data types? What are some use cases for tuples with size &gt;2?
Err, yes, mistranscribed on the way back to reddit, apparently. Embarrassing :-P
Arguably, `TypeApplications` makes the order of type arguments part of the public API...
You're out of luck afaik. Helm binds to some C libraries and relies on those, so GHCJS isn't going to cut it, nor is there a WebAssembly backend for compiling Haskell.
Isn't stm a [library](https://hackage.haskell.org/package/stm)? Why is it tied to the GHC release cycle?
[*Does Haskell have variables?*](https://stackoverflow.com/questions/993124/does-haskell-have-variables) might interest you. I hope to see more entries from you. As for tips for learning and exploring, here are some things I find useful * Use `InstanceSigs` {-# Language InstanceSigs, ApplicativeDo #-} data Pair a = P a a instance Functor Pair where fmap :: (a -&gt; a') -&gt; (Pair a -&gt; Pair a') fmap f (P a b) = P (f a) (f b) instance Foldable Pair where foldMap :: Monoid m =&gt; (a -&gt; m) -&gt; (Pair a -&gt; m) foldMap f (P a b) = f a `mappend` f b instance Traversable Pair where sequenceA :: Applicative f =&gt; Pair (f a) -&gt; f (Pair a) sequenceA (P fa fb) = do a &lt;- fa b &lt;- fb pure (P a b) * Use `TypeApplications` to simplify types ghci&gt; :set -XTypeApplications ghci&gt; :t sequenceA sequenceA :: (Applicative f, Traversable t) =&gt; t (f a) -&gt; f (t a) ghci&gt; :t sequenceA @[] sequenceA @[] :: Applicative f =&gt; [f a] -&gt; f [a] ghci&gt; :t sequenceA @[] @IO sequenceA @[] @IO :: [IO a] -&gt; IO [a] * Use `TypeApplications` to solve constraints, here the compiler makes use of its knowledge that `Const m` is `Applicative` when `Monoid m` ghci&gt; :t pure pure :: Applicative f =&gt; a -&gt; f a ghci&gt; :t pure @(Const _) pure @(Const _) :: Monoid m =&gt; a -&gt; Const m a * If a function is higher-order, pass it `id` ([relevant thread](https://www.reddit.com/r/haskell/comments/4rxxpv/interesting_useful_neat_applications_of_id/)) and see if something neat falls out, sometimes you discover existing functions (&gt;&gt;=) :: M a -&gt; (a -&gt; M b) -&gt; M b join :: M (M a) -&gt; M a join = (&gt;&gt;= id)
Indeed, but not much published literature around Coyotos. Lots to read on EROS, and it covers the historical angle with KeyKOS and Gnosis as well.
I'm still not putting it in the post ;)
Thank you for pointing out the stackoverflow question! Interesting question and answer. Since I am almost in the beginning stages, it is slightly difficult for me to understand the tips above now. However, thanks for triggering me to lookup these, and understand their significance and usage. Will surely read them up, and probably write about them if I understand them well enough to use and experience. 
Thank you very much the detailed explanation! This will surely help learn a lot. Thanks again. (at least I don't have any other question right now)
&gt; λ&gt;[ x | x@(Stuff{})&lt;- somelist] what are the ```{}``` in this line for? 
If you want to print some local values for debugging purposes, it's easier to show a tuple than to create a new datatype used in the one place. If you want to shrink a data type or generate random data for your record, you may want to reuse the existing tuple type when implementing Arbitrary. More generally, many typeclasses can have records implemented in terms of an existing tuple instance.
This tutorial is awesome, thanks for the time you put in to this! I am very new to haskell, so this might be obvious, but in ``` data Registers = Registers { r1 :: Unsigned 64, r2 :: Unsigned 64, r3 :: Unsigned 64, r4 :: Unsigned 64, pc :: Ptr } ``` Why isn't it `r1 :: Word`, since we already have that type?
For avoiding having to write out all the fields of the constructor. 
interesting, thanks.
Yeah don't put words in the mouths of the entire haskell community
&gt;one non-undefined member defined ?
&gt; Can you give me some sort of concrete benefit for running queryDB1 on one monad, running queryDB2 on another totally separate monad, then stitching their results together into some brand new monad created on the fly? I mean to develop and combine both components using a new monadic/effect system: the graded monad, that can combine components that execute different effects without glue code. 
&gt; Can you give an actual use case for where you can do something easily in OOP that is hard in FP relating to this? This all seems like a lot of faffing about over an implementation detail. To me adding a third database seems trivial to deal with. That's the problem: Haskell does not offer any better solution for component reusability than Object oriented. That is not what is expected form functional language. What you present of your component is the same than an OOP language: an interface or a class. Additionally you have to create a new monad for every new component. That is not necessary in other languages. In contrast, you can present an unified interface with well know mathematical properties and precise rules for combination so that any non expert can use immediately at the place where it is needed, without further ado, guided by the type system. To permit that combination of heterogeneous components it is necessary a new definition of monad that takes the effects as first class: the graded monad. That is for me a better solution from every point of view, and it is the combination of components with equational guarantees that are expected from a functional paradigm.
But we already don't need glue code. Did you check out the code example you were given earlier. It type checks just fine with `&gt;=&gt;` and works exactly as expected. Seriously man we can already do this easily.
But bro the typeclass approach already totally solves this. You already get everything you are talking about. Have you ever tried using the polymorphic on `m` with typeclass constraints approach? It gives you all of that and is perfectly composable. 
Triples are pretty handy for binary trees or double-linked lists. I think RB binary trees would need quadruples, not sure, but in general anyway Tuples are good for quickly and flexibly building data structures.
stm is a library, but the primitive is implemented by GHC.
The purity of Haskell is often touted as one of its great features... if you put the same values into a function, you will always get the same output. OK, so in this ridiculously simple python example, would this not be true? `def add(x, y): return x+y` Putting 5 and 3 into such a function would always return 8, wouldn’t it? Could I get a real world example of why state seems to be so controversial among functional programmers. I was also wondering if I could see a list of awesome monad examples that you might not see in LYAHFGG or Real World Haskell (the language’s two most well known free books). Especially ones that don’t seem to have an English synonym. Thanks.
Note though that many useful lattices *aren't* distributive (which IIRC means that compiler analyses using them are suddenly much more precise), which rules out a semiring instance.
That is not the same thing. On Linux at least (not sure about other unixes), the pipe operator does not create a file; the first program's output is simply passed to the second program's stdin. Windows IIRC emulates pipes by saving the first program's output to a temp file and feeding this file into the second program.
Great article! 
I'll have a shot at the first part. As your example shows, it's possible to write _a_ mathematically pure function in Python, but that doesn't meant that all of your functions are mathematically pure. If you're working with Haskell, _every_ function with type `Int -&gt; Int -&gt; Int` is going to be a mathematically pure function. If someone refactored your Python to be def add(x, y): printARandomNumber() return x+y then it's no longer mathematically pure. In Haskell, that would have a different type: `Int -&gt; Int -&gt; IO Int`. It'd still be pure, but we'd have to go further down the rabbit hole to get into that.
Yeah, it's a joke, yet here we are looking at a new monad tutorial from someone learning Haskell, at a rate of one per week. Who are these tutorials for? The only people who ever reply are those who are already experienced enough in Haskell to understand what the person is getting at. At some point it becomes less of a "monad tutorial" and more of a "here's a blog post about monads, can you check how well I understand them?"
Wow. I started with pandoc (something I wanted to do for a long time), and it's already so readable it's crazy!
Yes, propositional logic only allows basic propositions, but they all have implicit foralls on the outermost level. So any system supporting natural deduction should be able to prove, for example `a &amp; b`, which would hold for any value of `a` and `b`.
I don't understand the "fusion" law in the article, it is written that: &gt; f . foldr step default = foldr step' default' &gt; where &gt; step' a b = step a (f b) &gt; default' = f default But I got that: Prelude&gt; :{ Prelude| compose f step def = foldr step' default' Prelude| where Prelude| default' = f def Prelude| step' a b = step a (f b) Prelude| :} Prelude&gt; (compose (+1) (+) 0) [1..10] 66 Prelude&gt; ((+1) . foldr (+) 0) [1..10] 56 What did I miss? This aside, I liked the article. However I'm still not totally convinced by all recursion-schemes. I'm intensively using "catamorphisms" but I'm usually writing my "anamorphism" with an explicit recursion. "hylomorphism" are, from my point of view, difficult to see and I usually prefer writing them explicitly. I think I never identified any "paramorphism", but I'm convinced I used some without realizing. Finally, I'm still wondering about the curious beast which is the "Zygohistomorphic prepromorphisms" ;) 
No it does not work. It needs the same, even more glue code than OOP since the effects may be different. I'm more ambitious. 
&gt; I think I never identified any "paramorphism", but I'm convinced I used some without realizing. I believe a function that collects only "leaf" nodes in a Data.Tree is easier to write as a paramorphism.
Ńice! Now how do I get this in Emacs?
In natural deduction, we have the rule *schema* P Q ----- &amp;-intro(P, Q) P &amp; Q "Schema" being the crucial word: There is one distinct rule `&amp;-intro(P, Q)` for any two formulae `P, Q` -- hence, the quantification happens within the meta-theory, not within the formal language itself. In other words, the formulae of propositional logic do not contain quantifiers, but the language in which we state its rules does. The meta-level quantification is usually left implicit, hence probably your confusion. The situation for STLC (with products) is equivalent: We can define the term schema &amp;-intro(P, Q) : P -&gt; Q -&gt; P * Q &amp;-intro(P, Q) := \(p : P). \(q : Q). (p, q) for any two types P, Q.
In natural deduction, we have the rule *schema* P Q ----- &amp;-intro(P, Q) P &amp; Q "Schema" being the crucial word: There is one distinct rule `&amp;-intro(P, Q)` for any two formulae `P, Q` -- hence, the quantification happens within the meta-theory, not within the formal language itself. In other words, the formulae of propositional logic do not contain quantifiers, but the language in which we state its rules does. The meta-level quantification is usually left implicit, hence probably your confusion. The situation for STLC (with products) is analogous: We can define the term schema &amp;-intro(P, Q) : P -&gt; Q -&gt; P * Q &amp;-intro(P, Q) := \(p : P). \(q : Q). (p, q) for any two types P, Q.
It works without parentheses [ x | x@Stuff{} &lt;- somelist ]
I really like the recursion-schemes approach and get a lot of mileage out of it (e.g. to make ASTs that can later have annotations attached to the nodes, or holes punched into the tree, or...) But the cata / ana / zygo terminology, while cute, kind of drives me nuts. These aren't standard terms in category theory, so why not just call them fold, unfold, etc? It feels like the names close a really excellent idea off from the wider community of programmers-who-don't-morphism.
You've probably used `dropWhile`. It's \p -&gt; para (\(x,xs) r -&gt; if p x then r else (x:xs)) [] 
I'm personally fond of just naming fold/unfold/refold and then just dropping the rest of them on the floor or reconstructing them from distributive laws or, like Hinze and Wu, adjunctions as needed. I can't say that beyond the basic three that I've gotten any real mileage out of the work that I put into understanding the rest.
The actual rule is that if you know `f . phi = phi . fmap f` then `f . cata phi = cata phi'` Adding 1 to all of the elements then adding them up does not yield the same answer as adding 1 to the result, so you fail the precondition to apply this rule.
&gt; But what makes a functor a monad? This doesn't start well...
Surely you mean `∀ a b. a → b → a &amp; b`, not `∀ a b. a &amp; b`? Since the foralls for that logic are at the outermost level, the programming language corresponding to it must also only have foralls at the outermost level. The STLC doesn't have System F's support for polymorphism, and that's just as well since that would include more foralls than the logic you're trying to model. So, what would STLC + implicit outermost foralls look like? I think it means that instead of t ::= Nat | Bool | t -&gt; t | (t, t) the schema for types should instead look like t ::= Nat | Bool | t -&gt; t | (t, t) | a That is, unbound type variables are allowed anywhere in the type. The term level remains the same, so we can write `\x -&gt; x : a -&gt; a` and `\x y -&gt; (x, y) : a -&gt; b -&gt; a &amp; b`, and we cannot write a term of type `a -&gt; b` because there are no rules for constructing a value whose type is a type variable, we have to use the values we're given as input.
Not with Helm, no, but I can confirm that it's possible to write browser games in Haskell. For [my ludum-dare 34 game](https://github.com/gelisam/ludum-dare-34), I used Haste + custom bindings to some javascript library I stumbled upon, [sprite.js](http://spritejs.readthedocs.io/en/latest/).
Even worse, somebody could pass in an `x` or `y` to your seemingly-pure function that overload `__add__` to an effectful operation. So you can't even analyze your function in isolation. You need to potentially re-understand your function in every calling context (or even worse, re-understand it once per execution trace, since the particulars about `x`, `y`, and `__add__` may well depend on the whole history of the program's run)
Suppose, you want to express cos^2 (x) using sin(x), but you are bad remembering the trigonometric identities. No problem, use the code from our demo to automatically generate the expression! After running $ stack build &amp;&amp; stack exec hmep-demo The produced output was something like Interpreted expression: v1 = sin x0 v2 = v1 * v1 result = 1 - v2 From here we can infer that cos^2 (x) = 1 - v2 = 1 - v1 * v1 = 1 - sin^2 (x) Sweet! 
Ah, nice, thanks!
Stdout, stderr, and stdin are just files though. Whether they exist as a sequence of bytes on a hard drive depends on a number of things, but they are just entries in the file system, and programs write to and read from them just like any file.
Would you care to expatiate on your concern?
Nice work! Thanks for the reply.
The thing that really gets me about nixOS is just how cumbersome everything is and how unusable things are unless you know how to do things. So you end up with a few experts, some people with copy and pasted-ish cookie cutter configs, and a slow organic learning process. Arch Linux, on the other hand, does nothing for you so it's a very difficult learning curve as well, but it's very easy to pick up relative to it's "difficulty" just from the sheer amount of effort that's gone into polishing it up in many ways. I do think nixOS is improving, though? I've poked my head in the door a few times now (so I suppose I should preface what I said with a large grain of salt as an "outsiders perspective") but I do hope to eventually get into it since I love the idea of it so much. The two biggest issues for me right now are the ugly looking nix language (because I'm biased and prefer Haskell-like syntax and types) and the lack of cohesive user-land configuration (or not? I can never really tell)
&gt; Is there a way to solve this defect via code Wrap your program in an IO loop (if it's long-running, you probably do already) that catches all exceptions and recovers/logs. Sorry.
&gt; shouldn't ever use functions like that One useful pattern that I sometimes use is `when (isNothing ...) (logError ...)`, but I agree that these kind of functions should be avoided when possible.
Honestly that just isn't true. Give an example of said boilerplate. 
&gt; Because there is nothing interesting about hylomorphisms when viewed via our Fixable machinery, we skip directly to paramorphisms. What a pity! As it turns out, hylomorphism is [the mother of all recursion schemes](http://www.cs.ox.ac.uk/people/nicolas.wu/papers/Hylomorphisms.pdf).
One issue is that infinite loops, unchecked exceptions and undefined are all semantically the same. So without sacrificing turing completeness and adding in a theorem prover we can't really avoid the issue of library code returning `_|_`. I personally think exceptions in pure code are dirty and should be replace by ExceptT or similar. And if everyone agrees to such a policy you won't have to worry too much about the issue, but it won't be enforced by the compiler. 
`if isNothing x then fromJust x else default` --- I'd say they are barely acceptable for nullary constructors (including all the constructors of an enum, and `Nothing`), but I'd generally encourage case analysis anyway. It's harder to write incorrectly.
No idea if these are in any way feasible, but here are some ideas for the design: * Build the OS shell around inductive types. Something like 9P, but instead of everything being a stream of bytes, type everything and leave the selection of the bytes to the operating system * Make programs communicate in a fashion similar to modular synthesizers. For example, games currently tend to specify their own key mappings (localization still often doesn't work well). Go around the problem by exposing input slots like `moveDir :: Vec 2 [-1,1]` and `pause :: Bool` which allows users to specify their key mappings once for all applications * Provide abstract functions such as `sort` and inside OS select specific algorithms based on profiling. Prevent direct, machine level programming altogether * Build dependently typed specifications of the various instruction sets and some sort of guided workflow to progressively transform high level programs to the target instruction set * Graphics side, provide Clifford algebra based tools for rendering arbitrary, curved shaped shapes with collision detection. Use physical constraint simulation for a sort of n-dimensional typesetting * Implement first class support for syntax graphs as a data type. Support direct editing of those graphs, for example with a spray can model: just pick up a bottled axiom or proof with a VR controller, spray it where it sticks and see the graph do tricks. Force layout should be included * Images, videos and such are functions * The system should be completely functional without the internet * Even further, the system should treat the internet as an hostile entity which tries to manipulate the user through psychologically invasive advertising. Make it easy to reduce cognitive load by showing only the interesting parts of various content sources * Model the user as a flesh blob with a nervous system. Basic properties such as the mass of limbs allows anticipation of the future input. Same thing for behavioral patterns. This is useful for speculative execution and reduction of observed latency * Avoid everything programming related which has the word 'modern' in it, unless it's peer-reviewed * Joining the logical hive mind should take only one command
Benchmarks given in libraries should never be assumed to hold true for `O0` nor `runghc`. Since no one runs performance sensitive code that way. 
They're file handles, not files. There's a difference.
This is a plea to explain why I'm wrong. A lot of smart people seem to think recursion schemes are interesting. The justification is usually to avoid explicit recursion. And yet, understanding enough about recursion schemes to avoid needing explicit recursion seems considerably more complex and error-prone than just using explicit recursion. So I understand what the schemes are, but I don't understand why anyone would use them. (Also, doesn't the article get the sense of the naming wrong? It suggests mnemonic devices for remembering that catamorphisms destroy things and anamorphisms build them up... but actually, it's catamorphisms that build things up, and anamorphisms that break them apart.)
Fair enough! I'm trying to work my way through the code, so I'll let you know if I have any questions.
[Miso](https://github.com/haskell-miso/miso) and [Concur](https://github.com/ajnsit/concur) are also great options.
Makes it sound like a functor is a monad. 
That's why I mentioned `tuple :: Proxy n -&gt; Index n`.
But what if you are not allowed to pattern match on that datatype? 
Why not just write all our Ada or C programs as communicating independent tasks that only loop unboundedly at the top level?
Yes. Just have a `Fix` monad. data Fix a = Pure a | Fix (a -&gt; Fix a) The major problem preventing this sort of thing is the wider issue of not having a nice way of combining effects. We need to be able to write something like: main :: () effects Input, Output, Recursion, Nondeterminism
I would like to have an usage point of view concerning the Either type vs its strict version. For which (day to day) usage is the lazy common type more appropriate than its strict less common counterpart (https://hackage.haskell.org/package/strict-0.3.2/docs/Data-Strict-Either.html). Thanks
Huh. I was sure that one of my projects from a while back used `always`. I went and checked, and it turns out I was mistaken: it gets the behavior it needs using `check`/`guard`, and it doesn't actually use `always` at all. So I guess that means I'm okay with removing it. It seems like there should be a situation in which `always` could be useful, but that doesn't seem to happen in practice. (And `always`s buggyness is another strike against it.)
The fusion law stated in the original paper is `f . foldr step default = foldr step (f default) . map f`. I think this article got it wrong.
When defining functions in Haskell, we can use *substitution* and evaluation to figure out what's going on. So we can take our definition, and for each recursive case, we can inline the case that matches. That lets us expand the problem, so we can see what ends up happening. Let's take `sub [1..3]`, and attempt the expansion with the first definition: sub [1, 2, 3] -- the list is (x:xs) case, so we choose that definition: sub (1 : [2, 3]) = subtract (head [2, 3]) 1 = subtract (head (2 : [3])) 1 = subtract 2 1 = -1 Now, let's try the second one. This one has an observed problem that it never returns. Let's see if we can figure out why: sub [1,2,3] -- this matches the (x:xs) case, so we choose that definition = sub (subtract (head [2, 3]) 1 : [2, 3]) = sub (subtract 2 1 : [2, 3]) = sub ((-1) : [2, 3]) -- now, we recurse. This matches the (x:xs) case, so we choose that definition: = sub (subtract (head [2, 3]) (-1) : [2, 3]) = sub (subtract 2 (-1) : [2, 3]) = sub ((-3) : [2, 3]) -- hmm, now we recurse again... = sub (subtract (head [2, 3]) (-3) : [2, 3]) = sub (subtract 2 (-3) : [2, 3]) We never advance in the list! We're just always recursing on that first element. Generally speaking, when we write recursive functions, we need to ensure that we're making the inputs smaller in some way. WIth lists, it's somewhat easy. We typically have a `someEmptyValue` to use for the empty list, and a function `combine` that works to combine an element from the list with a partial result. someRecursiveFunction [] = someEmptyValue someRecursiveFunction (x:xs) = combine x (someRecursiveFunction xs) This lets us handle the recursion safely. When we recurse, we know that we're always processing a smaller list, and that we'll eventually hit the base case of the empty list. In fact, we can write a single function to handle these safe step-wise recursions: safeRecurse combine empty [] = empty safeRecurse combine empty (x:xs) = combine x (safeRecurse combine empty xs) This function has the type `safeRecurse :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b`. So, we can express your `sub` function using it: sub = safeRecurse (\x rest -&gt; subtract rest x) 0 `safeRecurse` is defined in the libraries as `foldr`.
&gt; So I understand what the schemes are, but I don't understand why anyone would use them. If you prefer `fmap`, `foldr`, and `unfoldr` over using explicit recursion for lists, then you are using recursion schemes. Sure; they're the simple/monomorphic recursion schemes, but there's still value there. I generally think that type classes are a nicer way to approach this sort of polymorphism than refactoring all your data types to use `Fix`, but I haven't worked on a production grade compiler where tree transformations over subtly varying ASTs was something I needed to do. &gt; (Also, doesn't the article get the sense of the naming wrong? It suggests mnemonic devices for remembering that catamorphisms destroy things and anamorphisms build them up... but actually, it's catamorphisms that build things up, and anamorphisms that break them apart.) I think it's a matter of perspective. We say that a `cata`morphism over lists breaks a list down, and in doing so, builds up some other thing. So it is a `cata`morphism over lists. sum = foldr (+) 0 This breaks a list down into a sum of it's numbers. Likewise, `unfoldr` is the `ana`morphism over lists because it builds up a new list from a seed value. In a sense, we are consuming that seed value by breaking it down into a new value. But we're concerned with the datastructure that we're building up, so we call it an anamorphism.
First off, I'd like to point out that /u/dalaing has the correct answer. My answer here is a bit of unfair fun with pointers, I'd rather people not think I'm trying to start a language war. Depending on what you want to define as the Python language your code could be rather unpredictable. If you import my BigBrother module before using add then 2 + 2 will equal 5: def big_brother(k, val): ptr = ctypes.cast(id(k), ctypes.POINTER(ctypes.c_int)) ptr[6] = val big_brother(4,5) Many people will say this isn't fair - if you write arbitrary places in memory you get what you deserve. I'd agree, but it's a demonstration of what my friend calls the Python ecosystem as a "language of consenting adults" - *if* we're all well behaved things won't go wrong. Haskell the language doesn't allow for this nonsense at all (I'm not sure about any official definition of the Python language). Haskell as used makes it difficult and obviously bad.
I'm not entirely sure, but I ran into a simlar issue that required me to configure my stack.yaml and then using stack solver.
Ya I ended up creating a stack yaml for that project and did stack build on the project. Turns out stack ghci was using the global yaml and that didn't have the right dependencies.
Then you can't write `isConstructorName` functions. And the persons that can pattern match should be exposing functions like `maybe` and `fromMaybe`; not `isJust`/`isNothing` and `fromJust`.
It's totally ok to use `stack-hpc-coveralls`. You can check Travis CI config of one of the projects I'm working on to check settings for coveralls.io This repository has code coverage: https://github.com/serokell/importify/blob/0b0fbe162297481a9480d23b2f4ea9531eed2d13/.travis.yml And here is the example of repository which uses `cabal` inside Travis CI to send report to coveralls.io https://github.com/mrkkrp/path-io As a side note I want to say that our team decided to drop test coverage report. Because test coverage slows down CI a lot (from 3 minutes to 50 minutes in our case). Unfortunately, nothing you can do currently to improve situation :( 
I didn't even know that somebody bothered to make a strict `Either`! The default lazy one has been fine for me in every situation. If I need strictness, some combination of `seq`, `deepseq`, and/or bang patterns does the trick. 
If you ever want to check if a given `Either` value is a `Left` or a `Right` without caring about its contents, the lazy version can be more efficient, because it doesn't need to force the `Either`s contents. For a trivial but concrete example, imagine you have a `x :: Either Int String`, and you call `either (&gt;0) (const False) x`. With a strict `Either`, if you had a `Right stringvalue`, you'd end up forcing `stringvalue`, even though you don't need it at all! This dosen't apply just when you're using `const`, of course. Any function that doesn't always consume its entire input generally works well with lazyness. `head`, for example (or `listToMaybe`.) Another function in which lazy `Either`s can be useful is the very common `&gt;&gt;` (including `do` notation, which can desugar to `&gt;&gt;`.) `x &gt;&gt; y` (or equivalently `do { x; y; }`) needs to know if `x` is a `Left` or a `Right`, but nothing more. With a strict `Either`, it needs to force the contents of `x`, even though you might never end up using that value. All this is the very common, very standard, lazy versus strict tradeoff that often comes up in Haskell. Strict data structures have their advantages too, there's no denying that. But it's important to remember the advantages of lazyness and make a well-informed decision on what to use. Lazy data structures can save work, but in certain situations can end up being too lazy, and waste memory. Strict data structures can waste you're CPU time by computing information and then throwing it away, but help against space leaks. It's a good idea to have a grasp on the performance characteristics of both, and use whichever one is most appropriate in each situation. But since you asked about day to day use: The thing is, using strict `Either` you're almost always going to end up doing extraneous computations, but using lazy `Either` you're usually not going to actually have a space leak. So unless you know you're going to need the strict version, I'd stick to the lazy version of `Either` as a default for day to day use. Plus, pretty much all of the Haskell ecosystem uses the lazy version, so that's the best way to go if you want any amount of compatibility.
That's all true, but why not represent a triple as nested tuples? `(a, b, c)` is isomorphic to `(a, (b, c))` after all. PureScript doesn't have tuples built in to the language; the tuple library implements n-tuples as nested pairs: https://github.com/purescript/purescript-tuples/blob/v4.1.0/src/Data/Tuple/Nested.purs
Ah, I see. So you fear the reader might get the impression, that *every* functor is a monad. That would indeed be fatal. But I hope that reading on it becomes clear that only some functors are monads: &gt; Functors for which these operations are defined are called monads [...] I hope that it should be clear to the reader from the context, that this is an mandatory requirement, in the sens that functors for which this is not true are _not_ monads.
Performance. Nested tuples introduce more indirection (assuming the compiler doesn't optimize it away).
Code clarity is always a balancing act. I can't categorically say whether I prefer recursion schemes or rolling my own recursion. To me, the messy thing about using, say, `foldr`is that I'll often have to manage some state in a tuple. Pretty soon I'll violate separation of concerns (a programming principle I hold rather dearly) as managing the accumulator can quickly become unwieldy. Often times that can be alleviated by writing an explicit recursion or switching to a state monad. 
Yeah, this was my thinking too, I just wish there was a better way. This isn't really any different than imperative languages, which is disappointing, especially since Haskell provides the means of doing it much better.
I know that bottoms are inevitable, but this issue is largely present because of the `error` function, no? If a library writer is making a function that can fail on a system level and wants to express as much if `error` were not present in the language, then wouldn't the writer have to handle the failure in their function and encode such a possibility in the type signature? Maybe I just don't understand it well enough, but the way I look at it, if a function can recover well enough to execute the `error` function, then shouldn't it also be able to evaluate to something more transparent?
You can get something almost as good if you allow for open terms. In logic, just as important as the terms is the context in which we type them. If we use a simple context like a : A, b : B, c : C, we can prove any tautologies based on our arbitrary types A, B, and C. The universal quantification isn't first-class, but it is clearly universally quantified in the metatheory.
The commenters here have provided good answers to your question, but I would like to add that, at least from my understanding, the real reason that purity matters is composability. The purity present in Haskell allows us to compose functions (composition like in math, so like f(g(x))) to an arbitrary depth (in terms of nesting) with strong guarantees that the behavior of such a composed function depends on the input to the inner-most function alone. Another way of expressing this in more pythonic terms is this: imagine you had a python function that called another python function, which called another and so on in a long chain of calls. In order to fully understand the behavior of the first function, you would have to inspect each subsequent function call to build a picture of this behavior. I'm not sure about Python, but you may have heard a "best practice" that is something along the lines of "avoid long chains of method calls", and this is the very reason. The difficulty of understanding the total behavior of your function grows very quickly, especially if you are calling functions from third-party libraries or even functions written by coworkers. In Haskell, this "long chain of function calls" is not only acceptable, but common and even encouraged. Purity (and Haskell's type system) allows us to accomplish this.
&gt; The state of exception handling in Haskell seems frankly, frightening. Haskell's exceptions are one of the ways that Haskell isn't better than other languages. Java has checked exceptions, which are pretty controversial. But basically every language with exceptions allows them to be thrown from anywhere in a way that isn't tracked in the types. &gt; Is there a way to solve this defect via code? Yes. Catch exceptions in IO using the [`safe-exceptions`](https://hackage.haskell.org/package/safe-exceptions) package. Report them using a service like [Rollbar](https://hackage.haskell.org/package/rollbar) or Bugsnag (working on it!). Just like you would in any other language that has exceptions. &gt; is there an upcoming GHC version that will deal with this stuff in some way? Very likely not. There are *really good reasons* to have runtime exceptions. Haskell's denotational semantics do allow for bottom / `_|_` / `undefined` values. Consider: undefined :: forall a. a undefined = undefined which loops infinitely. It's type signature reads as a promise: "I'll eventually return a value of type `a`, it'll just... take infinite time." If we want to lie in our types, this is our tool to do so. So we can write `head`: head :: [a] -&gt; a head (x:_) = x head [] = undefined If you write this, you'll get an error message when you do `head []` -- it's `&lt;&lt;loop&gt;&gt;`. This is no good! It doesn't tell us how or why our program failed. So, since we already *can* have these expressions that fail at runtime, we might as well allow ourselves to attach useful notes so that we can recover from them more effectively, or use that information while debugging. We could have `error :: forall a. String -&gt; a`, which just transmits a message. But as Haskellers, we like to have types and stuff, so instead we have these runtime exceptions that you can throw and catch and make instances of, with all sorts of useful information in them. Now, we *should not* write partial functions. They're bad. And maybe, if they were more painful, we'd write fewer of them. Maybe if we couldn't do `error "Called head on an empty list"`, we'd prefer functions like `head :: [a] -&gt; Maybe a` or `head :: NonEmpty a -&gt; a`. Unfortunately, Haskell wasn't designed with totality in mind, and that's given us the current situation.
`fromMaybe default x` or `maybe default id x` would work as well, though.
Thanks for your detailed answer ! 
Haha, I don't understand half of the things, but it sure sounds awesome!
Its about abstracting away the bare metal In my opinion.
Am I dreaming
That AST stuff seems really useful, any examples or posts explaining how to do that?
Was not aware of this lib, looks really neat!
There's another scheme to make things cleaner in that case that works sometimes. That is, a `foldrAcc :: Foldable f =&gt; s -&gt; r -&gt; (a - &gt; s -&gt; s) -&gt; (a -&gt; s -&gt; r -&gt; r) -&gt; f a -&gt; r` though I forget what that one is called. Sometimes I find it cleanest to roll my own recursion, but to still keep the recursion in a separate function from the operations performed during the recursion.
There's a function to change the implicit size parameter that gets passed to the generators. You could write a generator using the `Gen` monad, and draw from it using the `forall` function. You could declare a `newtype LargeList = LargeList [a]`, write an `Arbitrary` instance for it, and write your property with type `prop_foo :: LargeList -&gt; Property`.
What’s the difference?
I'm not sure that eliminating `throw` in pure code is that useful. Consider that Haskell has [asynchronous exceptions](https://simonmar.github.io/posts/2017-01-24-asynchronous-exceptions.html); this means that any source code location might raise an exception (if for example it's running in a thread that receives a "stop" directive). This choice is not obviously right, but it's defensible, as Simon Marlow explains in the linked article. [This](http://joeduffyblog.com/2016/02/07/the-error-model/) blog article (not about Haskell) has convinced me that in practice, failure should be untracked, since the type system isn't typically strong enough to prove it impossible and panicking rather than returning nonsense when your preconditions are violated is better for debugging. What you probably want (if it existed) is something like [Safe Haskell](https://ghc.haskell.org/trac/ghc/wiki/SafeHaskell), except instead of the Safe Haskell guarantee of "The types don't lie, and you can't break abstraction" you want "This code never `error`s, has no `undefined`s, and never `throw`s from pure code".
That's sensible, thank you. It's probably a much more difficult issue than I first thought.
 [welp](http://www.reactiongifs.com/wp-content/uploads/2013/10/tim-and-eric-mind-blown.gif)
I need to write a post on the support for complex batch inserts I've done for persistent :)
This is really cool. Quick question: what if input sources depend on runtime values (say in a program written by your average Joe)? It won't always be possible to replace those with compile time imports, will it?
This is truly beautiful, but I'm not sure how it could be extended to GUIs, and if it can't, we still need input and output (technically not reading and writing there, but still I/O)
NixOS is like democracy: it's the worst system except for all the other ones that we've tried
Dhall is actually interpreted in this case, which is why it doesn't have this problem, although I suppose a compiled language could do something like staged compilation to support this For Dhall specifically, you can import functions like any other values so you would distribute your program as a function hosted at some URL, such as: http://example.com/myFunction : Natural → Text ... and then an end user would invoke your program as: http://example.com/myFunction +100 ... and the Dhall interpreter would resolve the import, type-check the program, and then evaluate it
For simple GUIs you can push state into the controls and then your display is a pure function of all impure inputs (i.e. current time, mouse position, context of text entry box, etc.) For example, the program might look like this: λ ( inputs : { mouse : { x : Double, y : Double } , text0 : Text , text1 : Text , time : Double } ) → λ(GUI : Type) → λ(horizontalBox : List GUI → GUI) → λ(verticalBox : List GUI → GUI) → λ(textOutput : Text → GUI) → horizontalBox [ textOutput "My awesome GUI" , verticalBox [ textOutput "Mouse: ${Double/show inputs.mouse.x} x ${Double/show inputs.mouse.y}" , textOutput (inputs.text0 ++ inputs.text1) ] ] For stateful it's trickier and there are several approaches. One approach is that you treat everything as a discrete stream of events and you assume that there is a built-in "scan" function you can use to turn any stream into a stateful value. So, for example, if I wanted to count how many times my `text0` input updated, I would do: λ(scan : ∀(a : Type) → ∀(b : Type) → (a → b → b) → b → a → b) → λ ( inputs : { mouse : { x : Double, y : Double } , text0 : Text , text1 : Text , time : Double } ) → λ(GUI : Type) → λ(horizontalBox : List GUI → GUI) → λ(verticalBox : List GUI → GUI) → λ(textOutput : Text → GUI) → let countUpdates : ∀(a : Type) → a → Natural = λ(a : Type) → scan a Natural (λ(_ : a) → λ(n : Natural) → n + +1) +0 in horizontalBox [ textOutput "My awesome GUI" , textOutput "Number of updates: ${Natural/show (countUpdates Text inputs.text0)}" ] 
I'm not sure I understand how that function works. Do you have an example?
&gt; If a library writer is making a function[...], then wouldn't [it be better to] encode such a possibility in the type signature? Whenever I think something like this, I remind myself: "Do I really want all numeric code that does division to have `Maybe` in the type signature?"
Hah that's a very fair point.
More detail on /u/MichaelBurge's third suggestion: newtype LargeList a = LargeList {getLargeList :: [a]} instance Arbitrary a =&gt; Arbitrary (LargeList a) where -- for example arbitrary = LargeList $ replicateM 10 arbitrary ++ arbitrary -- Possibly a custom shrink implementation 
I wonder if it makes sense to expose some general-purpose hooks like "transaction entered", "transaction rolled back", "transaction about to exit", etc. that a library author could use to make something similar. They might have clearer semantics, and could be useful for profiling, debugging, or monitoring tools.
I'm not sure what you don't understand so I'll try to give a run down for each type variable... So, your 'container' type is f, the 'element' type is a. You have a throwaway state type s that gets modified by each element - this is where you keep track of details not a part of your final return result. There is a function to update said state for each element. Then you have your return type - this type is always what you want to return/evaluate to if your execution stops right now. Then you have a function that takes the current element, the current throwaway state, and the current return value to compute the next return value. Basically, bookkeeping information for processing goes into type `s` and information for the final produced value goes into type `r` If it's still not clear, I can give worked example when I'm not on mobile.
So it's not `error` that is the problem. It's `throw`, that is the function that some library writers are calling in pure code that leads to exceptions that you have to catch. `error` is basically the same as `trace "error ..." undefined`, and I am definitely glad it exists so that when you call a function with invalid parameters (e.g. Divide by zero) you can get the information you need to fix your code. I would probably be ok with `throw` being deprecated and removed and telling people to use `Maybe`, `Either`, `ExceptT` or `throwIO` etc. if they want catching to be possible.
Staged compilation will have to forego full static type checking though, because the URL may be unavailable at compile time, or it may change in between compile time and runtime. You can't avoid runtime type checking, right? Every function call you make, assuming lazy semantics, will need a fetch over the URL followed by runtime type checking... Am I misunderstanding something?
I think that URLs are presumed not to change values, so the compiler can cache the result of the fetch and the type-checking.
So Dhall is currently only interpreted so I can only guess at what it would look like compiled. However, I can clarify a few things Dhall enforces a phase separation between resolving imports, type checking, and evaluation: * First you resolve imports * Then you type-check the expression * Then you normalize the expression So evaluating a function (which is part of the normalization phase) would not trigger a type-check or URL resolution because those all occur in a preceding phase. Dhall's semantics are completely unaffected by strictness vs laziness since the language is total and due to this phase separation. For simplicity, I'll assume that the only difference between a Dhall interpreter and a Dhall compiler is the last phase. Instead of normalizing the expression you generate code: * First you resolve imports * Then you type-check the expression * Then you generate code Also, because Dhall is total you can optionally do partial evaluation before generating code Based on that model I can say a few things about compilation: * If a URL is unavailable at compile time then compilation should fail since code generation has to occur after type-checking which in turn has to occur after import resolution * If a URL is available then the URL reference should not persist to the generated code * In other words, the URL is resolved immediately and substituted with its contents and then the remaining phases proceed as normal It's not as clear to me what would happen if some running Dhall in interpreted mode referenced code that was compiled ahead of time, but I can say for sure that: * The interpreter should be able to infer the type of any compiled code, remote or otherwise * Even if that means just storing the type of the compiled code alongside the object code * There would be an additional type-checking step when the interpreter was run * This is true today even for a purely interpreted system * There's always a type-checking step every time the interpreter is run, regardless of whether code it references was type-checked and normalized ahead of time However, the phase separation still implies that for each run of the Dhall interpreter or hypothetical compiler all type-checking precedes all evaluation, so type checking is "static" in that sense Does that answer your question? 
This is true for each run of the interpreter, meaning that duplicate references to the same path or URL only translate into a single fetch. However, the URL is not necessarily guaranteed to return the same result for multiple runs of the interpreter This is one of the reasons that the Prelude is distributed over IPFS, because that you can safely cache indefinitely as you can guarantee that it will never change. In fact, you can use `ipfs mount` to mount the distributed hashtable as a lazy local filesystem underneath `/ipfs` which will lazily fetch and cache any path you try to access underneath that directory, which is nice if you want to avoid duplicate fetches permanently. Edit: You can also bake in any URL reference by just interpreting the file in place, which will resolve all URL references and normalize the file. That's the equivalent of freezing the file since it removes all imports
This sounds promising... how would I set a minimum size for my LargeList? I couldn't figure out how to write the generator for a class (e.g. Num) rather than a specific type (Int, float, etc). Thanks for the help.
&gt; There's a function to change the implicit size parameter that gets passed to the generators. I don't suppose you know what that function is called?
Is this the RealWorld#?
Isn't this pretty much how Elm started?
We're using Nix in production, so I've been writing a lot of Nix for a while, and except for when I was an absolute beginner, I haven't been missing types in Nix. I think the main point you should realize with Nix is that it has no runtime! By the the nix is done evaluating, you're left with things it's built. So, even if Nix had a type system, the type-checker would be run exactly when Nix is run anyway, so you wouldn't really avoid any "runtime bug"s. So, the lack of types in Nix isn't an inconvenience like Javascript where your program is a minefield of runtime exceptions, it's more like C++ template metaprogramming, where you know it's fine if it compiles, but the error messages can be terrible if it doesn't.
&gt; The justification is usually to avoid explicit recursion. Not quite. Their aim is also to abstract and generalize certain common patterns. So something like a catamorphism will look a lot like plain recursion, but a paramorphism or an Elgot algebra will do a better job of boiling the problem down to something compact. &gt; And yet, understanding enough about recursion schemes to avoid needing explicit recursion seems considerably more complex and error-prone than just using explicit recursion. How do you mean? I would definitely say that recursion schemes can clarify thinking in many cases. They're not more dangerous, and in fact you can avoid the `foldr`/`foldl` situation since there's only one `cata`. &gt; So I understand what the schemes are, but I don't understand why anyone would use them. Well, it takes a decent amount of time and effort. I have had more than one occasion where I got something to compile, tested it and thought "this shouldn't work but it does". I have a [blog post](http://blog.vmchale.com/article/integer-partitions) that's on a different topic, but it does feature a couple of paramorphisms. The paramorphisms allow us to avoid defining a recursive function that has an extra return value, and I think it's fair to say that what you lose in accessibility you gain in lucidity.
This isn't really ready for this kind of announcement, but who am I to complain about a little extra publicity. It does, however, at least get the job done currently!
&gt; switching to a state monad. State monad seems to have nontrivial costs in terms of testability.
Did you come up with a concrete solution to your specific problem? I didn't want to be a dick about helping, but just giving you an answer would have made the exercise pointless.
Chronomorphisms/futumorphisms/histomorphisms feel like they have a huge amount of potential, even if they aren't particularly user-friendly.
&gt; These aren't standard terms in category theory, so why not just call them fold, unfold I think they may have wanted to emphasize they work on general functors and not just list? But I'm inclined to agree with you. They really are just folds. &gt;It feels like the names close a really excellent idea off from the wider community of programmers-who-don't-morphism. Eh, their choice.
&gt; "hylomorphism" are, from my point of view, difficult to see and I usually prefer writing them explicitly. Hylomorphisms are actually one of the places that I think they're most useful. GHC is a smart compiler and it will almost certainly do some form of fusion for you, but it's still nice to have the assurance - if I can write it as a hylomorphism there is some sort of fusion that can occur. &gt; I think I never identified any "paramorphism", but I'm convinced I used some without realizing. Integer partitions work nicely with paramorphisms :)
Please no, let me have my tuple instances, I like them. I hope that at some point we have `Functor` as a superclass of `Bifunctor`, hopefully that would force people to accept `instance Functor ((,) a)` due to the pretty universally liked `instance Bifunctor (,)`.
Regarding remote code injection, you say this isn't a problem, since &gt; Dhall is totally pure and doesn't support any effects at all (besides heating up CPUs ☺). And sure, your program can't `rm -rf /`, but it could simply give the wrong answer, and if you make any decisions based on the it (which presumably is why we run it in the first place), you're still vulnerable. 
Thanks for the config, I'll definitely use some lines from it=) Elas, `stack-hpc-coveralls` does not have a couple of features that I need (it can't upload reports in manual mode or use tokens specified in separate files, for example), so I guess I'll fork it and patch for my needs. &gt; And here is the example of repository which uses cabal inside Travis CI to send report to coveralls.io Yes, I've already seen these examples, but that's what exactly what I wanted to avoid - using `cabal` at all.
https://hackage.haskell.org/packages/search?terms=blockchain https://github.com/search?utf8=%E2%9C%93&amp;q=language%3AHaskell+blockchain&amp;type= 
`resize`
The dyna/histo stuff gives you a way to do dynamic programming at least for some well-behaved cases. Unfortunately the asymptotic complexity still suffers for most dynamic programming use-cases. futu gives you the dual of dynamic programming where you can fill in more than one result as you go. I've yet to see chrono successfully employed as you'd need to want both of these things. Due to the fact that dyna almost always comes with an asymptotic hit, I tend to just work in something like ST and build a mutable kernel I can process everything with, which makes me sad.
 {-# language TemplateHaskell #-} import Control.Lens import Control.Lens.Extra data Whatever = What | Ever makePrisms ''Whatever ghc&gt; is _What What True
Could you give a bit more details on why one would want to use this over Hydra? Things that come to mind are easier setup and reduced resource usage but I’ve never had to setup Hydra myself so I don’t know if these actually apply.
Actually at work I'm working on one client in which we created a language similar to Dhall, but for expressing database queries (select/filter/fold) and a composition operator for them, which very similarly supports let and variables, and `Foo.bar` imports from other online files. And all this is desugared to a normal form that has no `let` or lambdas, and is just a tree of function composition. In my hobby project [Duet](http://chrisdone.com/toys/duet-delta/) I'm currently working on the IDE for it which will be a structured editor (which I'm currently using Reflex to develop), but once I'm done with that I have it in mind that users can write fairly non-trivial programs using different DSLs that they import. So if they put in their `main` expression something of type `UI` then it'll be an ADT like a free monad that's capable of rendering text and input boxes and buttons with a small FRP API to connect them together for which I'd use Reflex to bring to life. Likewise, another DSL could be [the classic terminal ADT](https://gist.github.com/chrisdone/ab861230b3f853bc4b2a4319b0575f79) which supports simply print, get, and return. Like your post, my idea would be to both use it to teach newbies about basic programming, but also to show generally how you can express interactive code without actually "doing" input/output in your language, but rather generating descriptions of work to do. 
Mainly that Hydra does not currently have support for building pull requests. There has been work done in this area, but it's not currently in master. Hydra is certainly the more powerful option, as it tracks build steps, can create channels, has many different types of options, has durability over restarts, notifications, and much more. Ultimately it would be good to get Hydra knowing about GitHub web hooks and the status API, but this was the quickest way to scratch my itch (blocking pull requests that fail various checks).
Here's an example of exactly that in a trivial FRP library I wrote to produce SVG: https://github.com/chrisdone/snappy#click-events-and-state Hit "see code" and you can see that the button's label is determined by a scan over its clicks. The "selection area" below is an example of a more complicated action which turns out to be another scan.
Elm is based on the ideas of "functional reactive programming" aka FRP. There are lots of flavors and areas of experimentation, although the style of writing Elm apps has changed over time to look more like React (decoupling state with rendering) than e.g. Reflex (coupling state and rendering).
Yeah I meant the original FRP version of Elm. Where the rendering was pure and then you added state with a built in "fold" function over a stream of events. I don't quite grok Dhall syntax yet but this is what the code seems to be doing here.
In what way this difference (coupling or decoupling) expresses itself in code? I remember [this comment](https://www.reddit.com/r/haskell/comments/6ilu3b/reflex_or_purescript/dj8gyxq/) saying that Elm could be expressed as a subset of Reflex, are these two notions compatible?
What do you gain from the `Functor` instance of tuples? Why not use `over _2`? What do you lose? The `Foldable` and `Traversable` instances for tuples are very confusing and error prone (note the yelling around the FTP, virtually all of it is about the tuple instances).
&gt; so I guess I'll fork it and patch for my needs. If you do that, could you open PR to owner's repo? I also would like to use manual uploading. This actually allows to have code coverage badge without slowing down CI.
Great cryptocurrency `cardano-sl` implemented completely from scratch in Haskell: https://github.com/input-output-hk/cardano-sl/ It uses Haskell bindings to `RocksDB` — efficient storage for blockchain: https://github.com/serokell/rocksdb-haskell There's also Bitcoin client implementation in Haskell called `haskoin`: https://github.com/haskoin/haskoin And small projects which implemented very simple and naive blockchains: https://github.com/aviaviavi/legion https://github.com/tdietert/nanocoin
Sorry for the nitpicking. These paragraphs are missing a full stop: &gt; "Ridiculous!" you exclaim as you spit out your coffee. "Nobody would ever use such a language." You probably wouldn't even know where to begin since so many things seem wrong with that proposal &gt; This is exactly the Rube-Goldberg machine I'm referring to. We have come to expect a heavyweight process for source code to depend on other source code &gt; Dhall doesn't need to support an explicit operation to read input because Dhall can read values by just importing them &gt; Most of the time people need to automate reads and writes because they are using non-programmable configuration file formats or data storage formats &gt; In fact, most objections to programmable configuration files are actually objections to Turing-completeness Concerning the article itself: very interesting stuff. The inspiration paper has been on my to-read list for some time.
&gt; So, even if Nix had a type system, the type-checker would be run exactly when Nix is run anyway, so you wouldn't really avoid any "runtime bug"s. There are cases where a typesystem for nix would prevent runtime bugs, e.g. when writing a package with multiple configuration options or when writing library functions.
Right. It does answer my question: for the hypothetical Dhall compiler, the URL resolution is being doing at compile time. Essentially, if I am understanding this correctly, the behaviour for URLs is as if you had a preprocessor macro that fetched the code over the URL and compilation proceeds as usual. However, my original question still stands: what if input sources (say some CSV file for some data analysis, so no static type checking needed for the contents) depend on runtime values? Contrived example: take a value from stdin, if it is a natural number, compute the corresponding busy beaver number (else exit quietly) and get the CSV file with that number as its name from the BB folder and does something with it.
AST size does not correlate to execution size though? For example, the Y combinator: (\x -&gt; x x) (\x -&gt; x x) Is quite a small AST but can be "executed" forever?
Their choice maybe, but our loss for adoption.
Dhall still has safeguards here even if I didn't go into them because the post is mainly about removing effects from the language There are two things you can do to prevent against this sort of injection: * First, obviously you can secure the host so it's not compromised * You already depend on code fetched remotely from secured hosts anyway (like private GitHub deployments) * You can host your code on IPFS so that the contents are pinned by the hash * You can normalize expressions ahead of time to remove all imports (i.e. "bake them in") The latter solution is the strongest defense because you can always eliminate remote dependencies if necessary
Input sources have to be statically known. In other words, you can't fetch a path or URL with a name computed from a runtime value. You'd have to explicitly create a function that maps inputs to imported expressions or a data structure you can index into (like a record or list) that stores imported expressions
There’s lot of organizations and stuff doing various projects that might be deemed Haskell and block chain At my job we’re building replciated db systems that have ownership and authorization modelling. There’s also DAH (Edward Kmett joined em recently), adjoint (Stephen Diehl is their cto), plus a few others like iohk. Point being: Lots of mostly open source Haskell in that space. Plus some closed source too. 
So the basic idea is there are a bunch of nodes which host pure functions that the interpreter can compose and run locally?
Hydra can already build pull requests and update the status API, actually :) See the comments on https://github.com/NixOS/hydra/commit/c6f07534359dd567b6dfe9397dc22e1f90925397 for the former and the GithubStatus plugin for the latter
Last I recall 1ML wasn't complete and had unfinished corner cases, and most importantly only a prototype interpreter. Andreas "vanished" into the Google V8 team, never to be heard of again.
&gt; you can't fetch a path or URL with a name computed from a runtime value Isn't that an issue? I mean it isn't for Dhall but wouldn't it be an issue if one wanted to write a general purpose (compiled) programming language with this model of IO in mind?
Interesting read. Why aren't we considering `import` as an effect? Utilising immutable storage is one thing, but most file systems are not.
Isn't this similar to having an application which requests some json data from an API somewhere? Not sure how (/if) security is handled there.
The architectural assumption is that all inputs are code and all code is human-generated, so the overhead of adding each newly authored file to an existing record or function is small
Yes, that's exactly correct
That's why Dhall uses IPFS to host the Prelude because it is immutable However, reproducibility vs non-reproducibility is orthogonal to the central idea of the post. The thing I'm trying to convey is that there are better ways to compose code than reading and write values to some handle/mailbox/buffer
u/watsreddit has a great answer. I'll add to it by saying that purity enables local reasoning. Our functions don't get entangled with their context or environment. They only care what is coming in and going out. This means we need very little context to understand them. Compose a large pipeline of functions and you need only understand their relationships at the edges. Pure functions complect input and output. Impure functions complect input, output, time, environment and mutation of that env.
Unsurprisingly when this doesn't get documented people can't find it and end up reinventing the wheel, which is a tad frustrating.
`maybe` is equivalent to case matching; it's the Scott encoding of `Maybe`. I have no problems with `maybe`. If you have more than a few constructors, Scott encodings can get unwieldy `fromMaybe` is also completely fine, and is definable in terms of `maybe`: `fromMaybe = ($ id) . maybe`. (`maybe` can be defined in terms of `fromMaybe` and `fmap`: `maybe d f = fromMaybe d . fmap f`.)
Actually I did not. I kinda gave up on Haskell and moving on to Python, learning Haskell as a first language is being very hard to me (even though I know some C#, its so little I think I can consider that it wasn't my first language). I will probably learn it when I have more experience, because I like it. If you have a opinion on what I did, feel free to say it, I'm accepting suggestions.
Christ man I'm not going to import lens for every one of my projects. Also I like the applicants and monadic interface and I like the conciseness of not having to use Writer. 
True, I only mentioned maybe because I occasionally am too lazy to add the extra import statement for fromMaybe. Also agreed that this isn't really reasonable for larger types except maybe ones that contain higher rank functions.
Well, I did say total functional programming, so the Y combinator wouldn't be allowed. The question is, can you compute the runtime characteristics of an expression faster than you can compute the results of that expression? Maybe in the worst case, no, but I suspect the average case will be much better. So the scheduler could know how long it takes to execute a chunk of an AST before it executes it.
Oh, now I get it. I didn't realise that earlier.
OK, consider the Ackermann function. How do you recognize that it is "big" to evaluate?
To me that sounds like "`fmap` is right there, so let's abuse that". A custom Prelude per project solves that problem -- and you don't really need to import lens for `_1` and `_2` - there are smaller lens packages providing it, and writing a custom more monomorphic one is: _1 f (x, y) = flip (,) y &lt;$&gt; f x _2 f (x, y) = (,) x &lt;$&gt; f y 
How so? It's still pure.
The main benefit of a type system for Nix would be improving error messages and discoverability
n-ary tuples do not need to be inductive, they can be backed by packed arrays. Check out [microgroove](https://github.com/daig/microgroove)
I really doubt that, if by "bugs" you mean an actual bug in the behavior of whatever it is that you're building (as opposed to a "bug" in the sense that you expect a NixOS configuration to work, but it errors out). Because whatever constraints you intend to express with types, you can already implement them with "runtime" (Nix runtime, i.e. build time) assertions, so the lack of a type system doesn't limit your ability to catch errors at build time.
&gt; However, we can easily statically analyze Dhall programs on human timescales because the typical Halting Problem objections don't apply. I don't know about Dhall, but it is easy to write Coq programs for which type-checking takes unreasonable amount of time (even some programs that people care about have this property), so I think this is jumping a bit too quickly to conclusion. If your language has both unbounded-length computation and type-level computation, it's probably not going to be easy to analyze all programs.
Correct me if I'm wrong, but this sounds like the input/output is being hoisted to the runtime environment, or interpreter (remember, a compiler is just a specialised interpreter), rather than eliminated completely? How does this approach deal with needing to do input/output at that level? Is there a point at which you *must* write code that physically reads and writes bytes from silicon, or is it turtles all the way down?
I can't speak about other total languages, but the one I'm working on is based on STLC, which thus doesn't allow general recursion. So you couldn't represent the Ackermann function, but you could represent a truncated version up to n recurses (which I think is good enough). Even within this system, my current naive runtime can blow up quite large for relatively small ASTs, but you can tell which expressions will blow up by the amount of repetition of variable terms within a lambda and the level of nesting of such lambda terms.
Yeah, I'm trying to figure out how this actually solve anything too. If you have to import some input from FS, it doesn't matter how you define that import because the program is still loading from disk. Even if the compiler is the only thing that can run IO operations, the program is requesting that IO so it's the same thing. I fail to see how this is any different than, say, composing an application of pure functions and then lifting them for IO, other than part of the language being magically obscured. This basically wouldn't work for any kind of server either, as it would be unable to receive requests or respond. 
I definitely see how it solves a lot of problems with IO, but as you say, it's feels like an obfuscated version of lifting into IO. 
I'm not really seeing a distinction between calling a function that reads from a file and writing an import statement that causes the interpreter to read a file. You could make a similar claim that Haskell's `readFile` doesn't *actually* read a file, it just describes what file the runtime should read and its continuation can treat the file content as a normal object.
There are a few things that limit Dhall's ability to do type-level computation: * Dhall is not dependently typed (its type system is System Fw), so you can't do much computation at the type level * Dhall uses explicit type abstraction and type application (i.e. no type inference) * This means that you don't run into the exponential time worst case of inferring the types of expressions like `id id id id id id id` I haven't formally proven it, but I believe the combination of these two things means that type-level computation is pretty tightly bounded by the size of the syntax tree
The key distinction is that there are two languages in play: * The language you implement the interpreter in (such as Haskell) * The language which is interpreted (i.e. Dhall) The interpreter is unsafe, but static, so there is an upper bound on the amount of unsafe effectful code that you need to inspect and audit The interpreted language is the safe user interface. This code is effect free and safe. There might be a lot of it but who cares because we don't have to inspect or audit it 
Can you read a Haskell function or type from a file?
Please format your code using four space indent, otherwise it is impossible to read :) You may also get more luck in /r/haskellquestions. Please post some expected inputs and outputs for your function.
 triangle :: Triple -&gt; Triangle triangle (a, b, c) | a &lt;= 0 || b &lt;= 0 || c &lt;= 0 = Illegal | a * b * c == 0 = Flat | a == b &amp;&amp; b == c = Equilateral | a == b || a == c || b == c = Isosceles | (a * a) + (b * b) == (c * c) || (a * a) + (c * c) == (b * b) ||(b * b) + (c * c) == (a * a) = RightAngled | a /= b &amp;&amp; a /= c &amp;&amp; b /= c = Scalene | otherwise = Impossible 
You may be interested in this http://www.haskellforall.com/2012/08/the-category-design-pattern.html?m=1 
Ah ok didn't realise that was a forum, will do so now.
F-omega has the full power of the simply-typed lambda-calculus at the type level (but you may have restricted it further?). This still allows to express computations with towers-of-exponential reduction lengths (or result sizes). (Working with F-omega probably means that people will not write large type-level computations as part of important/interesting/human-written programs, so you could set an arbitrary limit on type-checking computation steps and reject programs that require too much time.)
Oh yeah, that is correct. Either way I'll fix the post
How is that different from Haskells main :: IO () being a pure function, which is executed at runtime?
Hahah wait I just forgot to re-compile when I tried it. Stupid me! Thanks anyway.
Thinking about it more, it's not true that "people will not write large type-level computations in Fomega". For example you can express the type `List n a` of lists of size `n`; it sounds reasonable in a configuration language to export data sequences at a precise type characterizing their dimension, and to do some computations on those dimensions (that's especially reasonable as you expect many of those dimensions to be constant in the program source, and thus easier to compute with). There is no bound on the type-level computations as there is no bound on the size of data you would like to type precisely.
I just uploaded a small library to github and Hackage with some examples of how I've been doing things like this lately. I suspect there are other approaches to the same problem out there, but this one has the benefit that I understand it :) https://github.com/matt-noonan/functor-friends
&gt; Due to the fact that dyna almost always comes with an asymptotic hit, I tend to just work in something like ST and build a mutable kernel I can process everything with, which makes me sad. That's what I've been doing in production code, but it hurts in a lot of ways :( Testing is pretty unergonomic with state monads which is a pain.
Oh I just meant ease of testing. You have to know a "right" state at a given time and whatnot. It's good in terms of purity.
Yes indeed. There is a `bool` function in base which accomplishes exactly what you need! From hoogle: bool :: a -&gt; a -&gt; Bool -&gt; a base Data.Bool Case analysis for the Bool type. bool x y p evaluates to x when p is False, and evaluates to y when p is True. This is equivalent to if p then y else x; that is, one can think of it as an if-then-else construct with its arguments reordered. Examples Basic usage: &gt;&gt;&gt; bool "foo" "bar" True "bar" &gt;&gt;&gt; bool "foo" "bar" False "foo" Confirm that bool x y p and if p then y else x are equivalent: &gt;&gt;&gt; let p = True; x = "bar"; y = "foo" &gt;&gt;&gt; bool x y p == if p then y else x True &gt;&gt;&gt; let p = False &gt;&gt;&gt; bool x y p == if p then y else x True
It seems to me you could make it part of the protocol to always include a hash of the code in the URL and then your compiler could verify that the file contains what the url promises, so you don't even have to trust the host at all.
But doesn't this use pattern matching or am I just confused? bool :: a -&gt; a -&gt; Bool -&gt; a bool f _ False = f bool _ t True = t
Pattern matching is the only way that evaluation happens in Haskell, so you might need to weaken your constraints a bit!
I think you can do this with type classes. class Conditional predicate where if' :: predicate -&gt; value -&gt; value -&gt; value data True data False instance Conditional True where if' _ t _ = t instance Conditional False where if' _ _ f = f main = do putStrLn $ if' (undefined :: True) "yes" "no" -- yes putStrLn $ if' (undefined :: False) "yes" "no" -- no 
I always tought that pattern matching translated to `case` clauses. Someone knows what comes before?
The only thing I can contribute about crypto and Haskell is Cardano, which is developed by IOHK, that is a highly international team.
I think the only reason I haven't done this so far is because if you want to freeze your dependencies you can normalize your code However, I could see people wanting something in between where they pin their dependencies using hashes when they don't want to normalize their code, such as preserving the original code for readability
You mean a church encoding? The lambda calculus is turing complete and doesn't have builtin conditionals -- it's just function application, variable declaration, and lambda abstraction, all of which we have in Haskell. For example: true :: a -&gt; a -&gt; a true = \a _ -&gt; a false :: a -&gt; a -&gt; a false = \_ a -&gt; a branch :: (a -&gt; a -&gt; a) -&gt; a -&gt; a -&gt; a branch = \b x y -&gt; b x y example :: String example = branch true "a" "b" 
One of the reasons that Dhall uses similar syntax for `List`s and `Optional` values is so that people who choose to extend Dhall with more advanced features (like refinement type or dependent types) can make `Optional` a type synonym for lists of at most one element. Using syntax like Liquid Haskell, you could imagine `Optional` being a synonym for: Optional a = { xs : List a | len xs &lt;= 1 }
That seems to be equivalent to part of what I'm doing. But I don't seem to be able to be able to map a Bool from some conditional expression to these representations of true and false.
You can use let if' c x y = (bools' !! fromEnum c) x y . `fromEnum` does not use pattern matching, since it is automatically derived (I don't know if this counts). However, `!!` still uses pattern matching.
That's too bad. I don't think Haskell is too difficult per se, it might even be easier for someone who has less knowledge of the imperative paradigm and its concepts that functional programming defies in such strange ways. But Python is surely the best language to learn programming in general for a beginner.
 newtype LL a = LL [a] instance Arbitrary a =&gt; Arbitrary (LL a) where arbitrary = do n &lt;- arbitrary LL &lt;$&gt; replicateM (abs n + 10000) arbitrary
It all boils down to `case` expressions.
Sounds like interesting research :-)
You can convert back and forth between the two representations: fromBoolToChurchEncoded b = \true false -&gt; if b then true else false fromChurchEncodedToBool c = c True False
If you discover that you need to reduce your set of constraints and you're willing to drop the Zurich part, there are several blockchain companies in NYC that use Haskell: [BlockApps](http://blockapps.net/about-blockapps/), [symbiont](https://symbiont.io/), and [kadena](http://kadena.io/). If you're thinking about moving somewhere for Haskell, I highly recommend NYC. It's got a fantastic Haskell community in general and is especially good if you're interested in Snap/Reflex. :)
It also allows more sharing, as your compiler can cache the one piece of code, even if it's provided by different sources in different projects.
It also allows more sharing, as your compiler can cache the one piece of code, even if it's provided by different sources in different projects.
You can actually get that already too by using IPFS to host your imports and using `ipfs mount` to mount it underneath the `/ipfs` directory. Then the name of the file includes the hash (and therefore the integrity check) and the first time you access something it is cached permanently
Well you can't do anything with a bool value without some form of pattern matching. If you can't do any case matching it's the same as making every non-lambda type into unit.
That's pretty sweet, but I meant automatically caching the same piece of code from completely different hosts, since it's all universally identified by the hash.
Other quite good examples of full-featured Yesod applications (not necessarily just APIs) that might help you: - https://github.com/fpco/stackage-server - code behind https://www.stackage.org/ - https://github.com/fpco/schoolofhaskell.com - code behind https://www.schoolofhaskell.com/ 
That's basically how IPFS works. Content is addressed by hash so you can get it from any peer hosting IPFS. For example, that's how Dhall's Prelude is hosted: the root host is a small EC2 instance I own that has no special address but the network automatically routes requests correctly based on the hash in the name and also mirrors and caches files as a transparent optimization
Thanks for the suggestion. Your definition for arbitrary wouldn't compile, but this did: `arbitrary = LargeList &lt;$&gt; resize 100000 arbitrary` Unfortunately `generate arbitrary :: IO (LargeList Int)` doesn't generate anything.
I think I understand how this works but `generate arbitrary :: IO (LL Int)` doesn't seem to do anything.
Played a bit around with a date picker widget before and it almost felt worthwhile to expose the different parts of the widget separately. For instance some way to render the current month in a clickable grid with something like mkDateGrid :: (...) =&gt; (Dynamic t Day -&gt; Demux t Day -&gt; Demux t Month -&gt; m r) -&gt; Dynamic t (Map Text Text) -&gt; Dynamic t Day -&gt; m [[Event t r]] mkDateGrid renderDate dynAttrs curSelected = _ so that users can recombine the pieces if they want something slightly different than the default.
Ah, that's really cool!
I think [this](https://github.com/haskell/hackage-security) is a good resource to read more about hackage security.
Though it's also worth noting that this is still not *quite* what `micro-ci` is doing, as `micro-ci` is a web hook rather than polling. But that really is essentially an implementation detail. This plugin does look good, thanks.
Add deriving Show to the newtype definition? That's my best guess
No it’s not. Function application evaluates the function. 
 {-# LANGUAGE ScopedTypeVariables #-} module Main where import Test.QuickCheck import Control.Monad (replicateM) newtype LargeList a = LargeList {getLargeList :: [a]} instance Arbitrary a =&gt; Arbitrary (LargeList a) where -- for example arbitrary = LargeList &lt;$&gt; ((++) &lt;$&gt; replicateM 10 arbitrary &lt;*&gt; arbitrary) -- Possibly a custom shrink implementation main = do x :: [Int] &lt;- generate (getLargeList &lt;$&gt; arbitrary) print x x :: [Int] &lt;- generate $ resize 1 (getLargeList &lt;$&gt; arbitrary) print x x :: [Int] &lt;- generate $ resize 11 (getLargeList &lt;$&gt; arbitrary) print x This should work (I actually tested it this time). I wouldn't use `resize` on the list generator, since that also makes the element generators try to make large elements. 100000 is also pretty big for a quickcheck size parameter.
I understand that, but I was trying to find a way to do that without resorting to conditionals as well. I do appreciate your help! I did find a way that satisfies my constraints... eventually: toNativeBool :: (Bool -&gt; Bool -&gt; a) -&gt; a toNativeBool = (\a -&gt; \b -&gt; \c -&gt; c a b) True False
Well function application may as well just be casing on the set of mapping’s from inputs to outputs =P
Why is Hydra doing polling instead of webhooks? Webhooks are great...
For starters, nodes can import functions and types from other nodes
&gt; although I suppose a compiled language could do something like staged compilation to support this At this point I'm getting flashbacks to [the argument that C is a pure language](http://conal.net/blog/posts/the-c-language-is-purely-functional), just from the other side.
Right, I guess I'm trying to see if there's a way to do that without appealing to conditionals. Thank you for cluing me in to church encoding, I've been reading and learning a lot for a few hours now :P
*What* is read would seem to be beside the point?
I know that the actual contents don't matter, but I'm trying to put people in a state of mind where they treat effects as an implementation detail of the compiler. People normally associate functions and types as a compiler-level feature and not a language-level feature so talking about using effects to read in functions/types emphasizes that effects become the responsibility of the compiler instead of the programmer
Heads up, I think argblhf might also have been pointing out a further issue with your code. Try drawing a triangle with side lengths 1,1, and 2000. It's impossible! You'll also need to check the triangle inequality. 
 data Bool = Bool (forall a. a -&gt; a -&gt; a)
Nice! Decoupling the two would be neat for several reasons. I plan to have a bog standard one that you can place on the page and call it a day. But I definitely want to work towards a more modular design so you can pick and choose.
As the users guide describes _only_ the bootstrapping step is unsafe unless you verify the keys. That said, it's a great question to ask: "why should I believe that the keys in the root.json are safe to bootstrap with?" The best answer I can give at the moment is the following repo: https://github.com/haskell-infra/hackage-root-keys We should probably publicize it much better :-) There you can see emails from the roots of trust verifying that the keys are theirs. The hackage operational keys are then verified by those roots of trust. I see that some of the roots of trust do not have emails that are gpg-signed verifying their root keys, but just the root keys themselves. Those keys were collected by hand at ICFP, and I suppose you'll have to take people's word that they were indeed collected by hand, and trust in the github accounts of the contributors to the repository who checked them in. This is probably something we could improve in the future.
Brilliant stuff. Has anyone contributed benchmarks to Aeson's build pipeline which get automatically updated and published with every release? For such an important library in the Haskell ecosystem we should have them in place. And how many perf bugs are basically caused by GHC not being able to INLINE the right stuff? I had [discovered the same in Lucid](https://github.com/chrisdone/lucid/issues/63) and am not satisfied by the status quo of the compiler being dumb about such things. As I mentioned on [haskell-cafe](https://mail.haskell.org/pipermail/haskell-cafe/2017-January/126152.html) - &gt; If inlining certain functions can give a boost to the performance, then is it unreasonable to expect the compiler to have better heuristics about when commonly occurring code patterns should be inlined? In this case monads and mtl being the commonly occurring code patterns. &gt; At a broader level, the promise of writing pure functions was to be able to talk about 'intent', not 'implementation' -- the classic `map` vs `for loop` example. Additionally, pure functions give the compiler enough opportunity to optimise code. Both these higher level promises are being broken by this experience. Hence, I'm feeling "cheated" The other place where the compiler continues to play dumb is specializing typeclasses, as I learnt second-hand from [this post](https://jship.github.io/posts/2017-08-11-writing-performant-haskell-part-3.html) This are two core language/ecosystem features (monads and typeclasses) that the compiler fails to optimise for. 
(viewing on mobile) Is the actual date picker not injected on the page? Wanted to play around with it. 
[removed]
Explicit type tests are often not the right thing to do, but that does not mean they are never the right thing to do. This is the sort of absolutist attitude that makes it hard for people to learn how to use goto correctly.
I really like this solution.
First sentence: &gt; Monads answer the question of how to get imperative behavior in a pure functional language. This is wrong from the start. You don't need monads for this, you can have concrete functions like `oneAfterTheOther :: IO () -&gt; IO a -&gt; IO a` and `chain :: IO a -&gt; (a -&gt; IO b) -&gt; IO b` and build your imperative behavior in a functional language around them. No need to reference monads. Or, looking at it from another way, take monad instance for Either, Maybe, Identity, Reader, Writer, etc. They don't have anything to do with imperative behavior. Sorry if this seems harsh! Monad tutorials are notoriously tricky to write (especially because the "click" moment when you understand monads seems so deceptively easy to convey to others), so even people who are normally good at technical explanations end up confusing their readers on this particular subject.
You can also pattern match on the boolean value: fromBoolToChurchEncoded b = \true false -&gt; case b of True -&gt; true; False -&gt; false However, there has to be a pattern match on a `Bool` somewhere otherwise there is no way to tell whether or not the value was `True` or `False`
I suggested a few ways on #haskell when somebody else asked this a few days ago. Besides `if` and pattern matches, list comprehensions, guards, and `unsafeCoerce` can consume `Bool`s. if' b true false | b = true | otherwise = false if'' b true false = head ([true | b] ++ [false]) if''' b true false = fromMaybe false (fmap (const true) (unsafeCoerce b))
I think it’s unfair to say GHC is failing in those areas. It’s doing remarkably well compared to just about every other language out there; no one else is inlining as well, and the only ones specializing as well are ones that eliminate all virtual dispatch by default (which would destroy code size in Haskell). GHC suffers in these areas because we are being extremely ambitious in what we are asking of it. Properly inlining and specializing everything is essentially a supercompilation problem, which means it is as of yet unsolved, even theoretically.
I'd rather the compiler have heuristics for optimising commonly occurring code patterns that wait around for the perfect solution to this problem. Is there any reason an mtl-monad/bind should not be inlined, by default? 
I didn't think it was worth embedding at this stage. When I'm not as embarrassed by my CSS abilities, it will be included with the posts. :)
&gt; I'd rather the compiler have heuristics for optimising commonly occurring code patterns that wait around for the perfect solution to this problem I agree. My point is that even those heuristics are incredibly hard. &gt; Is there any reason an mtl-monad/bind should not be inlined, by default? I think this is one special case where, although unfortunate, it would be a net benefit to add to the compiler (unfortunate because e.g. anyone using `RebindableSyntax` with stuff like linear or indexed monads is going to feel some pain). But the monad hierarchy is a pretty special example. `traverse` is a little harder; it's got a class constraint in a class method! How far do you go with this? If we wan't to fully specialize the applicative instance, we have to specialize the `traverse` instance. But it's less obvious that we should *always* specialize `traverse`. Do we always specialize `Data.Map` functions with every `Ord` instance? My point is that although I agree we should probably specialize `Monad` specifically extremely aggressively, there aren't really many other obvious examples. GHC's current heuristics are the result of many years of tuning by some very talented people; they're not just *failing*.
&gt; Or, looking at it from another way, take monad instance for Either, Maybe, Identity, Reader, Writer, etc. They don't have anything to do with imperative behavior. Really? I definitely think they do. `Monad` *as a class* just encodes calling one function, *and then* the next, *and then* the next. Honestly, I think the general concept of *monoids* is extremely representative of imperative programming, and monads are just a higher order version of monoids. Any operator that only cares about the order of a sequence of computations is inherently encoding something imperative. (Of course I'm really only talking about non-commutative monoids.)
This might not be quite what you're asking for (I'm not sure), but if you're in the IO monad (say, in a do block), "when" is a function that let's you effectively emulate the behavior of an if without an else. While it isn't quite a statement and is still an expression, in practice it's worked for me if I ever needed to go that route. https://www.haskell.org/hoogle/?hoogle=when
Are you referring to SPECIALIZE or INLINE? I was referring to the latter, where it _seems_ to cause no harm in inlining for monads/binds.
Inlining (almost?) always implies specializing. The two kind of go hand in hand in this problem. As far as this conversation goes, they’re basically the same problem, as you have to decide whether it is worth copying the code into a new location and running the simplifier over it in yet another place, regardless of whether you’re talking about specialization or inlining.
The entire program can be expressed as a single fix point. And then you have Turing completeness. All you need is one fix (together with tupling of course). Exercise: Write two mutually recursive function using a single fix. 
Blum's theorem essentially says that you can get arbitrary blow up in size by restricting to a less powerful model of computation. That was the main point of Harpers post. If nothing else, to keep the proof terms smaller, it might make sense to have a turing complete model of computation. 
&gt; Any operator that only cares about the order of a sequence of computations is inherently encoding something imperative. I like your point here. It's close to having me convinced I need to reword the second part of my argument, but I'm not sure yet. Just because monads have to do with sequencing, does that really give them a connection to imperative behavior? They're still fully referentially transparent. Actually I think I've found what I need to reword. Add a "getting" in there so it now reads "They don't have anything to do with getting imperative behavior" and I think the argument is vastly improved. Another addendum to my original post: WIWIKWLH's [Monadic Myths](http://dev.stephendiehl.com/hask/#monadic-myths) would have been a good response as well.
&gt; x depends on y package which failed to install. some example packages which have failed so far, with that message included, include `wxhaskell` and `snap`. Is there a general purpose cabal command which helps the installation to just run successfully? While I'm here I should ask, is wxhaskell a good way to go. I just want a GUI that isn't too obscure, and has lots of documentation. `threepenny-gui` doesn't seem to have much of a tutorial going for it. 
Thanks, that's fixed it. I should have thought of that!
Thanks, that's fixed it. I should have thought of that!
That's really not the same as you won't be able to decide at run-time.
&gt; I'm trying to see if there's a way to do that without appealing to conditionals. From "inside" Haskell, you can't switch between `data Bool` and a Church "bool" without doing a pattern match. But you *can* write arbitrary programs from the ground up using Church encodings, and those programs will not utilize pattern matching at all.
Because the plugins system doesn't let you add new endoointsz iirc.
Without looking, I suspect that even `when` uses one of the banned methods of finding out if the argument was a `True` or a `False`.
NB: "Performance debugging" is not read like "performance art".
Ghc translates haskell into an intermediate language called core. It is pretty close to haskell but much simpler and mirrors the runtime behavior better so it can be useful to look at when optimizing. Anyway, in core all laziness comes from let bindings and all evaluation stems from case matching. 
&gt; I was also wondering if I could see a list of awesome monad examples that you might not see in LYAHFGG or Real World Haskell (the language’s two most well known free books). Especially ones that don’t seem to have an English synonym. Thanks. There is the [Tardis Monad](https://hackage.haskell.org/package/tardis-0.4.1.0/docs/Control-Monad-Tardis.html), but this is an english name (apparently a tardis is a kind of police box ;). Except to impress people in interview, that one is not that useful, but it is funny. There is the [Dynamic Monad of reflex](https://github.com/reflex-frp/reflex) which represents a varying value in time and which is useful to build interactive interfaces. There is all the parser monad, such as the one used in [megaparsec](https://github.com/mrkkrp/megaparsec) which helps to build parser, but most cases only uses the `Functor` and `Applicative` instances. There is the [probability monad](https://hackage.haskell.org/package/probability), which will, well, handle probability. A huge amount of containers can be monad, for example, the [Sequence](https://hackage.haskell.org/package/containers-0.5.10.2/docs/Data-Sequence.html) is. A function is a Monad, as well as a tuple. Theses ones are common, but not usually depicted in book as such, they are usually called `Reader` and `Writer`. Amazingly, the combination of both, a function involving tuple, is a Monad, and is usually called `State`.
I'm not sure what you're saying it true. My understanding is that a strict constructor forces the argument _on construction_, not when you pattern match on it. The strict variant's constructors should be equivalent to ˙Left' a = a \`seq\` Left a` and the same for `Right`. I think this makes the use cases more obvious.
I’m quite aware of how ghc works.
If we had something like Idris's !-notation it might be workable.
Will do when I have a working solution=)
&gt; The other place where the compiler continues to play dumb, is, specializing typeclasses, as I learnt second-hand from this post What exactly do you mean? The compiler very aggressively specialises overloaded functions and if there is a case where it doesn't then opening a ticket would be useful. 
This is a good post. 
&gt; `Monad` _as a class_ just encodes calling one function, &gt; and then the next, and then the next. I would dispute the _just_ in that claim, not only that there is more to the class than just bind, bind itself does more (hint: `join`). I think the function that actually fits the description "calling one function, and then the next, and then the next." better, or at least as well as (&gt;&gt;=) is ($). 
Except `$` doesn’t really have any nice monoidal properties. I’d say `.` fits the bill better, bringing us to the conclusion that it’s `Category` that matters =P
Thanks. I think `vectorOf` is a better fit than `resize`: newtype LongList a = LongList {getLongList :: [a]} deriving ( Eq, Ord, Show, Read ) instance Arbitrary a =&gt; Arbitrary (LongList a) where arbitrary = LongList &lt;$&gt; (vectorOf 1000 arbitrary) -- trivial test case example... prop_ll xs = length (getLongList xs) == 1000 
As I mentioned in my comment above, the SPECIALIZE thing is **second hand** info based on that blog post. Would you be able to take a look at the blog post and comment why the blog's author needed to manually add SPECIALIZE pragmas? Some relevant text from the blog post: &gt; These functions use typeclass constraints too, meaning that the typeclasses will be desugared to corresponding record values just like we saw with our xshow function. GHC will probably not know the concrete type for a in these functions at compile time, so we will take a performance hit when we use any of the functions from Integral, Show, or Storable. &gt; As we are not diving into the core output in these posts, we will go ahead and add specialized versions for xshow and its variants in the public API. The updates to the code are available on GitHub. We are doing this to rule out the chance of paying the runtime costs here too, even though we have a hunch GHC would likely know which dictionaries to use at compile-time for these functions. When we get to benchmarking below, I encourage you to experiment with removing the specializations on xshow and its variants in the public API. In my testing, any performance differences were negligible.
I don't see any reason why they needed to. GHC will automatically specialise functions in the same module or if they are marked `INLINABLE`, across modules. The `SPECIALISE` pragma is intended to prevent code duplication in very wide module hierarchies. 
&gt; My understanding is that a strict constructor forces the argument on construction, not when you pattern match on it. Your understanding is correct, but the implications of that are somewhat unexpected. When is a value constructed? Normally, only when you pattern match on it! That seems a bit confusing but let's look at some examples. Let's say we have: x :: Either Int String x = Left (1 + 1) Has `x` been constructed yet? No! Whether it's the strict or lazy version of `Either`, `x` itself is a lazy variable. Now, let's do this: case x of Left _ -&gt; "left" Right _ -&gt; "right Now, by pattern matching on `x`, we force it to calculate whether `x` is a `Left` or a `Right`. If it's a lazy `Either`, it stops there. But if it's a strict `Either`, then forcing `x` also forces its contents, and so it performs the computation `1 + 1`, despite never using it. `1 + 1` is a simple computation, so this is not much of a waste of resources. But imagine you're implementing a parser. You have a function `parse :: String -&gt; Either ErrorMessage AST`, where `ErrorMessage` and `AST` are types you've defined. Now, what happens when you do `parse "some string to be parsed`? Absolutely nothing, of course, regardless of whether the `Either` is lazy or strict. In either case, it just returns a thunk, which when evaluated will be a `Either ErrorMessage AST`. It hasn't done any parsing. Now, what happens if you pass it to `isRight`? (`isRight (parse "some string to be parsed")`) `isRight` is defined as `isRight (Left _) = False; isRight (Right _) = True`. This pattern matching means the thunk gets evaluated, constructing the `Either` value. If it's a lazy `Either`, the function will do just enough work in order to determine that the string is syntactically correct (and thus `parse
Fair enough
[@theindigamer's latest tweet](https://i.imgur.com/t10DLeS.jpg) [@theindigamer on Twitter](https://twitter.com/theindigamer) - ^I ^am ^a ^bot ^| ^[feedback](https://www.reddit.com/message/compose/?to=twinkiac)
Cool. Thank you for indulging my intellectual curiosity!
Good point
Not going to lie, the learning curve of a slightly unintuitive functor instance is about 1/10th that of lens. Also that doesn't really address the bifunctor stuff, since `instance Bifunctor (,)` is great and in the long run we are going to want `Functor` to be a superclass. And `fmap` on tuples isn't even close to abuse, at worst it is very slightly unintuitive. 
I don't know why `Bifunctor (,)` is so great either. I think instances give meaning to things. `(a, b)` does not mean that `b` is annotated with an `a`. For that meaning we have `Writer`. I am not talking about the learning curve. I am talking about error-proneness. `length` on the wrong thing is more likely to work (incorrectly). The code conveys a very straight-forward meaning that is not the right one. It is OK to have to learn to read something. It is not OK to have something be very readable to all - such that all readers receive the wrong meaning.
I suspect I'm focusing too much on particulars and missing the point the article tries to make, but... What I fail to see is how you would implement much of the world's needed functionality in programs composed this way.For example, how would a trivial CRUD website app be structured in this paradigm? At some point somewhere, you're going to have to write a Dhall module that deals with the database. That, or you'll have to implement database-as-dhall-modules synchronization system that also has to recompile your application at every database change (i.e. world state change). But how would you even do that, since Dhall can't do it in the first place (in this thought experiment where a Dhall-like language becomes general purpose programming language, so it's Dhall all the way down). On the other hand, the ability to import directly from URLs and such seem like mere convenience features that could be baked into any language. I.e. tinker with GHC to make runtime code loading / hot swapping more reliable and jumping to URL imports is trivial, sans the "module as a value" structural preference.
I'll adjust my model of computation *after* one of my programs is affected by the blow up.
For a good time, try `(?) i t e = case i of {True -&gt; t; False -&gt; e}` Then you can use it like a ternary operator `x &gt; 10 ? "foo" $ "bar"` Guaranteed to amaze and annoy your coworkers
A quick follow-up to understand. What if that +100 needed to come from a database? How would you wire that program?
I'm not sure what the content has to do with the title. My Dhall music player is not going to be precompiled against every audio file every recorded (or that ever will be recorded). It still needs to read input...
Yeah. `replicateM len` should be the same as `vectorOf len`; I just didn't remember the specific combinator name. You might not want to fix the size, which is why `++`ed an `arbitrary` list, but if a fixed size works, then your solution is simpler.
The simplest answer is that you can put a REST API in front of the database (such as using `postgrest`), then you can import a value from the database by URL The next simplest answer is that the language could have built-in support for importing values from databases (i.e. via a `jdbc` URL) However, I think a better answer is that the language runtime is internally implemented in terms of a database that gives the illusion of a file system so you could import values by paths but each path in this virtual filesystem is just a database query that is lazily evaluated on demand
&gt; For example, how would a trivial CRUD website app be structured in this paradigm The only app-specific part you would implement out-of-band (i.e. not within Dhall) is how to render Dhall expressions to a web page. The rest of the web page logic would be implemented within Dhall Note: this assumes that you replace Dhall with a richer programming language with more built-in pure functions. Dhall is deliberately an impoverished programming language since it's niche is configuration files which have to stay simple &gt; That, or you'll have to implement a database-as-dhall-modules synchronization system that also has to recompile your application at every database change I shouldn't have used the word compiled in the article because you could just as well interpret the language (which is what Dhall actually does). So any time you load the web page you would in the worst case reinterpret the supplied Dhall expression. However, because Dhall is a total language you can normalize the expression ahead of time to reduce the interpretation cost Similarly, the database would just be another Dhall expression that you import into your code, which is interpreted
I shouldn't have used the word "compiled" because this paradigm works even better if interpreted (which is how Dhall works). So you could interpret each new audio file
[Digital Asset](http://www.digitalasset.com/) has a pretty heavy Zürich presence. Much of the work on DAML, the Haskell-like smart contract language we use that runs on a distributed ledger, takes place there. And for that matter, speaking to /u/mightybyte's point about NYC, we also have an office there. =) That said, were not really doing anything with Snap/Reflex at this point. As to your questions: I can't speak to how hard the community dealt with the financial crisis as I only really started getting to know the Zürich Haskell crowd after it had come and gone. That said, the community seems pretty healthy and ZuriHac gets larger every year. Notably, people there keep coming up to me with new companies using Haskell that I'd never heard of before, which is a good sign that the corporate investment in Haskell is growing.
Install tools out of necessity only cover some particular domain of interest. In the case of Cabal and Stack we're talking about Haskell and some tools. Failures are often because a package is just Haskell bindings to a C library that should be installed via OS packaging (ex. `apt install libwxgtk3.0-dev wx-common` etc).
ну вот, треба мені було тільки з країни виїхати
GHC does not give any code special treatment (even code from the standard library). That includes `Monad` binds. As far as GHC is concerned the `Monad` class and instances are not distinguished in anyway from other user-defined type-classes If you think these instances should be inlined, make a pull request to add an `INLINE` or `INLINABLE` annotation to the instance methods. If you're wondering why GHC doesn't do this by default it's because it's not always a win and it's not a free optimization (it bloats interface file sizes and code sizes which can also mean that less code fits in your L1/L2 cache which deteriorates the performance of tight loops). This is why you should always benchmark before adding these sorts of annotations
Hydra has a force-eval endpoint already.
Isn't that what programs do at the moment?
проходьте матеріал віддалено, як повернетесь — приєднуйтесь! розклад потижнево є на github https://github.com/KyivHaskell/haskell-study-group#%D0%A0%D0%BE%D0%B7%D0%BA%D0%BB%D0%B0%D0%B4
I'd guess, you'd have a `musicPlayer : Music -&gt; Audio` and then for playing each new song, you'd write a new program, like this `import Mungo; import MusicPlayer; musicPlayer Mungo.inTheSummerTime` Which isn't what programs usually do. (Am I on a right track here?) 
The difference is that you are interpreting data instead of a program. Imagine if instead of opening up a file you "opened" a Dhall expression that computed the sound. Opening a file would be a special case of Dhall's support for code referencing other expressions by their paths, but you could also "open" a URL (I imagine some audio programs support this, too). Audio files could reference other audio files, so if you had some sort of "concatenate audio" operator, you could open the following Dhall expression: concatenateAudioSamples [ ./file1.wav , http://example.com/someFile.wave , someInlineAudioExpression ] ... and it would retrieve/compute the necessary audio samples, concatenate them, and play them
Yeah you're right I was thinking of IO as an example when I wrote that
I ended up needing to go down this route, so I wrote a library for this: [concurrent-st](http://hackage.haskell.org/package/concurrent-st-0.1/docs/Control-Concurrent-ST.html). For sure, `forkST` is a can of nondeterminism, but it works for now. It would be interesting to see what kind of safe interface could be built on top of something like this.
[slides](https://drive.google.com/file/d/0ByK3AAy5ubqaZmZHc0ZTRDBWZjA/view?usp=sharing) | [coda](https://github.com/ekmett/coda)
I mean it's literally the most used bifunctor instance by far. And it is very accurately modeled by bifunctor. To me the meaning `Functor` just means "contravariant in the last type variable` but to each their own. 
Subtle.
Yay! Approaching valiant parsing.
&gt; you can have concrete functions like oneAfterTheOther :: IO () -&gt; IO a -&gt; IO a and chain :: IO a -&gt; (a -&gt; IO b) -&gt; IO b and build your imperative behavior in a functional language around them. Well, then you *have* a `Monad`, you just aren't *calling* it a `Monad`. (And, if these concrete functions follow the monad laws, you even have a monad.)
We don't really have associativity (at least not operationally), so I'd say it's [unital magmoid](https://www.reddit.com/r/haskell/comments/2la0cx/category_the_essence_of_composition_first_section/clsy9l9/) that matters. :P
There is a comment about Valiant parsing in the unused slides. =) Unfortunately, Valiant parsing in general has some weaknesses relative to the approach used here. Difficulties with *, and it doesn't give you the strong error production properties that I'm getting out of hacking up layout, so once you find an error you're basically stuck. It is great in the case where the parse goes through, not so good in the failure mode.
I'm looking at the [Discover Pony](https://www.ponylang.org/discover/) page now and it sounds really cool! I'm curious, though: how much extra work is it for the programmer, compared to writing Haskell, to gain these safety guarantees?
Specification of such functions as READ, WRITE, OPEN, MOVE, and much of the procedure control definition will be left to the compiler, thereby reducing the work of the systems analyst. The analyst will specify the various relevant sets of data and the relationships and rules of association by which these data are manipulated and classed into new and different sets of data, including the desired output. and: Current programming languages tend to have three primary limitations: (1) they are machine dependent to varying degrees; (2) they require ordered procedure statements; (3) segmenting a program into discrete runs is required. Reference: http://doi.acm.org/10.1145/366920.366935 Authors: obert Bosak, System Development Corporation Richard F. Clippinger, Honeywell EDP Division Carey Dobbs, Remington Rand Univac Division Roy Goldfinger (Chairman), IBM Corporation Renee B. Jasper*, Navy Management Office William Keating, National Cash Register George Kendrick, General Electric Company Jean E. Sammet**, IBM Corporation My question: Should we add "function of a Glump" to Haskell
Pretty sure the intro could have *easily* taken being more epic!
It’s worth noting that the main problem with getting monad instances inlined isn’t whether they’re marked with `INLINE` or `INLINABLE`; GHC’s size heuristics are usually more than enough for that. The problem is making sure that every function you call with these instances gets specialized to them, or else the ability to inline doesn’t matter.
Very little. The type system just adds the cognitive load of reference capabilities: Learning the difference between `iso`, `val`, `ref`, and `tag`, etc. and how they need to be consumed and recovered to gain or surrender capabilities is a definite mind-bending experience. (Exceptions are also type checked, but they don't have types - it's just an `error` that causes a `try` block to hand off to its `else` clause.) Once you get the hang of it, you'll wonder why other languages don't do it. [But those safety guarantees are automatic.](https://tutorial.ponylang.org/capabilities/) If your program compiles, *it can not crash.* (Modulo compiler bugs and C FFI.) I think the word for it is "wild."
The Monad typeclass is a generalization of how we chain IO actions, chain error handling, chain nondeterministic functions, etc. It's not how we do any of those things in the first place. Talking about it in the context of "how can you do imperative things in a pure functional language" is a non-sequitur. The answer to that question is that we return a value that represents a recipe or machine for doing the imperative things. This is already confusing enough to people new to Haskell without also talking about how that process can be generalized to also deal with a ton of other stuff.
So I still do not understand why `let` requires braces. It seems to me that `let .. in .. ` is from parser standpoint the same thing as `{ .. } .. ` if we threat `let` as opening keyword and `in` as closing keyword. 
Sorry for the vague post, but I hope you enjoy considering the possible Programming Language parallels. For example, I think the hypothetical inclusion of "local type class instances" in Haskell would generally increase cognitive load. Against that foil, the status quo of only top-level instances feels to me like Absolute Direction.
Isn't that what [plugins](https://hackage.haskell.org/package/plugins) does?
Do Haskell programs read files?
At 30 minutes semi-direct products are mentioned wrt serialisation. If people are interested in this more, check out [How to Twist Pointers without Breaking Them](http://ozark.hendrix.edu/%7Eyorgey/pub/twisted.pdf)
You can view `let` and `in` as a set of parentheses, but the system I'm aiming to use uses layout to recover from unbalanced parentheses. Letting 'in' trump layout breaks that rule. Now to understand where a layout block ends you have to know how layout interoperates with the dyck language rather than having the dyck language be something you only have to parse / accumulate within a layout block. Mutatis mutandis for '(do foo; bar)'. If you only want to handle unbalanced opening contexts rather than both unbalanced opening and closing contexts then you can come up with some middle ground that actually allows both parsers, but then an unmatched ) or 'in' breaks the rest of the parse, while the story used here lets us resume at the next outdent in both the unmatched ( and ')' cases without ambiguity.
You want FP parallels to natural language phenomena? Here's a fantastic paper on using continuations to compute natural language semantics, by Chris Barker: https://www.nyu.edu/projects/barker/barker-cw.pdf . I'm new here so I don't know if this has been discussed before, but it's basically why I'm learning Haskell. See also this introduction to a book on the subject by Barker and Chung-chieh Shan: https://www.nyu.edu/projects/barker/barker-shan-continuations-book-intro.pdf
What wasn't clear to me is when `root.json` gets downloaded. Is it just the first time I use `cabal update`? If so, does that file ever change, and how would I get updates if it did? If not, is it pulled every time I use `cabal update`? Does it matter if I'm in a cabal sandbox when I call `cabal update`?
That also fits the area of "seems to assume familiarity with ongoing discussions about hackage security". I think I'm missing a table of contents, or introduction section, or something similar. It seems to start in the middle of the conversation and I'd really like the "here's what you do, here's what you're trusting when you do that, here's what's guarding what" overview first.
I imagine you could bind duktape and dump the environment
You could u
[20:54](https://www.youtube.com/watch?v=Txf7swrcLYs&amp;feature=youtu.be&amp;t=20m54s): &gt; In Visual Studio Code, positions here are lines and columns, but we don't do tab stops, it's just the total number of characters on the line. So I could build a little Monoid for gluing together these positions but it's only a Monoid. Presumably, the first "Monoid" in that sentence was meant to be "bicyclic semigroup"? Let's see, how would tab-stops change anything... `&lt;tab&gt;`, `&lt;space&gt;&lt;tab&gt;`, ..., `&lt;7*space&gt;&lt;tab&gt;` all indent as much as `&lt;8*space&gt;`, more spaces than that indents as much as `&lt;tab&gt;&lt;tab&gt;`, etc. To parse that monoidally, we need to take a sequence of spaces and tabs from the middle of a line, and do whatever corresponds to cancelling out the inwards-facing parens. I think that means to eliminate all the redundant information which we don't need for `mappend` nor for computing the final amount of indent. If our sequence contains spaces in between two tabs, we can replace each group of 8 spaces by tabs and drop the remainder. This leaves us with some number of spaces on the left, some number of tabs in the middle, and some number of spaces on the right. And when `mappend`ing two such triples, we have `&lt;i*space&gt;&lt;j*tab&gt;&lt;k*space&gt;&lt;l*space&gt;&lt;m*tab&gt;&lt;n*space&gt;` so we can replace the middle `k+l` spaces with some number of tabs. Hmm, except if `j` or `m` is zero. I should probably use a different constructor for "no tabs, only spaces". All right, here's my guess for what the bicyclic semigroup for positions with tab stops would look like: data Pos = Spaces Int | Tabs Int -- number of spaces on the left of the tabs, always &gt;= 0 Int -- number of tabs, always &gt; 0 Int -- number of spaces on the right of the tabs, always &gt;= 0 instance Monoid Pos where mempty = Spaces 0 Spaces k `mappend` Spaces l = Spaces (k + l) Spaces k `mappend` Tabs l m n = Tabs (k+l) m n Tabs i j k `mappend` Spaces l = Tabs i j (k+l) Tabs i j k `mappend` Tabs l m n = Tabs i (j + ((k+l) `div` 8) + m) n indent :: Pos -&gt; Int indent (Spaces k) = k indent (Tabs i j k) = (i `div` 8) + j*8 + k 
Great talk! I really like the idea of having robust, efficient parsers. It makes a lot of sense to me for this to be a primary concern when designing a language's syntax. It also seems like a really solid argument for there being an advantage to syntax where indentation is significant. At [33m47s](https://www.youtube.com/watch?v=Txf7swrcLYs&amp;t=33m47s), the claim is made that for most binder syntaxes we care about, the scope intervals are always nesting. This seems to be true if all you care about is "What names are in scope"? However, often languages have name shadowing, so this property doesn't work out if we want to answer "Where was this name bound"? I didn't quite follow why the nesting property is helpful, though. I suppose it is because it fits better with an interval map? Otherwise you need to explicitly store intervals in the lowest node that they fit into.
We (http://dfinity.org) are looking to hire Haskell engineers pretty aggressively both in Palo Alto (SF Bay Area) and Zug, Switzerland. Let me know if you're interested in speaking with us :)
Hold on to your hats, we're about to go primitive with this. {-# LANGUAGE MagicHash, UnboxedTuples #-} import GHC.Prim import GHC.Exts (Int(..)) import Data.Primitive.Array import System.IO.Unsafe import GHC.IO import Control.Exception if' :: Bool -&gt; a -&gt; a -&gt; a if' b t f = unsafePerformIO $ do arr &lt;- newArray 2 f -- Create an array with two elements, t and f, in it writeArray arr 1 t b' &lt;- evaluate b -- Make sure b' is evaluated before... readArray arr (I# (dataToTag# b')) -- we convert the Bool to an Int
It does get downloaded during first update. It may change, and if it does, the new one will also be downloaded by `cabal update`. The new one must be signed by a quorum from the prior one for it to be accepted. See section 5 of the TUF spec for details: https://github.com/theupdateframework/tuf/blob/develop/docs/tuf-spec.md#5-detailed-workflows It should not matter if you are in a sandbox or not.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [theupdateframework/tuf/.../**tuf-spec.md#5-detailed-workflows** (develop → e45084f)](https://github.com/theupdateframework/tuf/blob/e45084fd9d678bd886f76701cb28e3d02ac68f31/docs/tuf-spec.md#5-detailed-workflows) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
Could you elaborate on this? If I'm using a ReaderT over IO, what do I need inlined and what do I need specialized?
http://hackage.haskell.org/package/trifecta-1.7.1.1/docs/Text-Trifecta-Delta.html is the version I used to use for dealing with positions and also allows tabstops. It works fine if you are going to report positions with display columns, but just like the Position I show in the talk it is only a Monoid. The thing you are referring to is a better Position monoid, not a better "Bicyclic semigroup". The bicyclic semigroup is used for handling parens and paren-like behaviors. They look somewhat similar but are very different: -- bicyclic semigroup B n m = )*n (*m mappend (B a b) (B c d) | b &lt;= c = B (a + c - b) d | otherwise = B a (d + b - c) -- positions P a b: a lines, b columns mappend (P a b) (P 0 d) = P a (b + d) mappend (P a _) (P c d) = P (a + c) d The Position type I showed was because it was how I have to report the positions to VS Code, not because it is particularly better for reporting to users or because tabs are hard. The issue I was mentioning about it being only a monoid has to do with the fact that rather than use that Position type, I use a Delta in terms of number of utf-16 code units since the beginning of input, which forms a full group, which enables the machinery at the end of the talk where I speed up relative maps and relative hashmaps.
I'm confused. It's difficult to wrap my head around the examples, because the `Conc` code looks postfix, but the pseudo code examples are prefix?
&gt; If you're wondering why GHC doesn't do this by default it's because it's not always a win and it's not a free optimization (it bloats interface file sizes, increases compile times, and possibly means that less code fits in your L1/L2 cache which deteriorates the performance of tight loops). This is why you should always benchmark before adding these sorts of annotations Do you know if anyone in the GHC team has done this and therefore, has some data to show why this shouldn't be automatically done? In general, I'm not a big believer of "I can't predict the general impact on perf, till I benchmark" That means that I don't understand the underlying mechanics properly. I hate not being directionally correct wrt perf of the code that I've written. In this context, I'm finding it hard to believe that there aren't thumb rules that can predict whether something should be specialised or inlined (based on whatever you have mentioned - function size, number of call sites, cache sizes, etc). 
Name shadowing makes more specific intervals. When I'm looking at a use-site the most specific interval wins, which is what I was referring to by the nesting of intervals. The general idea for how to deal with understanding if alpha renaming is safe is to use two maps, 1.) for use sites, that gets entries removed when you close over that use-site with an interval for the same name, that one plays the role of the usual 'free variable list' as all the names left over are free. 2.) a second map where you don't actually remove the occurrences on bracketing them with an interval. Now if you go to rename a symbol and there is an existing occurrence of a symbol with the target name in the result interval, it probably isn't safe semantically. The first map is enough to figure out what occurrences bind to what use site. The second map is enough to figure out that in foo bar = \baz -&gt; bar renaming bar to baz would change capture semantics.
Note: The semi-direct product of a delta and a map I mention in the talk is a proper semi-direct product, but it isn't a twisted functor. It is sort of to "Semialternative" what a twisted functor is to Applicative. Map wise the difference corresponds to union vs. Intersection.
&gt; but the pseudo code examples are prefix? Are they?
That's what I'm confused about. It would be a lot easier to reason about if it were all postfix, but either way, the article needs to be explicit.
If you are only handling simple JavaScript, you can write a simple interpreter on your own! Handling just assignment, object field accesses shouldn’t be much work. The parser has already been written for you: https://hackage.haskell.org/package/language-javascript
 foo :: (MonadReader Int m, MonadIO m) =&gt; m Int foo = do x &lt;- liftIO randomIO y &lt;- ask return (x + y) main :: IO () main = do i &lt;- runReaderT foo 5 print i In this example, `foo` needs to be specialized to `ReaderT Int IO Int`, and we want `foo` to inline its calls to `&gt;&gt;=` and `ask`. But for said inlining to occur, said specialization must occur first. A function of type `Monad m =&gt; m ()` can't inline `&gt;&gt;=`, because it doesn't know which `Monad` instance to inline it from. - Step 1: Specialize main = do -- We want GHC to specialize foo when it is called like this. -- This isn't guaranteed to happen for all functions. -- But it should for foo in this case, because it is so small i &lt;- runReaderT foo1 5 print i foo1 :: ReaderT Int IO Int foo1 = do x &lt;- liftIO @(ReaderT Int IO) randomIO -- liftIO is statically dispatched, not inlined y &lt;- ask @(ReaderT Int IO) -- Same for ask return (x + y) When GHC decides to specialize `foo`, it will create a copy that is specialized (`foo1`). This version of `foo` will not take any runtime dictionaries as arguments. Until the simplifier runs, it will still call functions from dictionaries. But the functions being called will be statically known, instead of being pulled out of runtime dictionary arguments. This is still a performance improvement, as static dispatch is faster than virtual dispatch. - Step 2: Inline / Simplify I won't try to sketch out the exact behavior of the simplifier, because it's very complex. But `ask`, `liftIO` and `&gt;&gt;=` are all probably small enough that GHC will always inline them. So `foo1`, now that it is calling those functions statically, will very likely avoid all the dispatch entirely by inline them. And in this case, `foo` is probably small enough to inline into `main` anyway, but that's not really the point here. The point is that we could only inline that which was known statically. So for `foo` to inline its usage of `&gt;&gt;=`, it had to first become specialized to a particular monad in order to know what `&gt;&gt;=` to inline. Therefore, you need to specialize any polymorphic function when you want that function to inline all its calls to things like `&gt;&gt;=`. - Pragmas: Of course you don't have to manually sprinkle `INLINE`, `INLINABLE`, and `SPECIALIZE` pragmas all over the place in order to get good behavior with this stuff. `SPECIALIZE` would be especially annoying, as you don't necessarily know how you want the function to be specialized in the place that you're writing it. But GHC has some heuristics that do a lot of work here. Extremely small functions are always implicitly marked with `INLINE`. Relatively small functions are always marked with `INLINABLE`. And all functions that are marked with `INLINABLE` will always be specialized at all call sites. So you get surprisingly far without writing *any* pragmas yourself, since a great deal of functions you write will be automatically marked `INLINABLE`, and will therefore always be specialized, and will also be inlined whenever GHC considers it worthwhile. It's not all roses though. Although I'd wager that the vast majority of Monad instances will be marked `INLINE` by GHC's heuristics, there are definitely times when this isn't the case. This can cause some pretty obvious performance problems. So anyone writing a Monad instance should likely always annotate with manual `INLINE` pragmas (this is a reasonable alternative to having GHC special case and do this for literally all Monads, since it affords more control). But since we're already extremely good at inlining `&gt;&gt;=`, the more likely problem is that a big function fails to be specialized. If you write a massive function, GHC is not going to automatically mark that as `INLINABLE`, so the function therefore won't be specialized by default. In fact, since it won't expose an unfolding, it's *impossible* to specialize that function anywhere other than the same module in which it was defined. This is for more often the culprit. Failing to specialize a large function that calls many other functions means that a large chunk of your program is going to operate entirely on virtual dispatch, with no inlining of dictionary methods. Needless to say, this is incredibly slow for `Monad`. The conclusion being: Always mark your monadic functions with `INLINABLE`. Doing this will dramatically increase how often `&gt;&gt;=` becomes inlined. It kind of sucks that you can't just tell GHC to mark all functions in a module as `INLINABLE`. There is the `-fexpose-all-unfoldings` flag, which one should hopefully never use. This instructs GHC to always expose an unfolding for every function, even if the function was not `INLINABLE` or `INLINE`. But the behavior of this unfolding will be very different. It will not be specialized by default, so it's very hard for these unfoldings to become specialized. You can fix this with `-fspecialize-aggressively`, which tells GHC to specialize *all* calls to functions that have unfoldings; but you should also hopefully never need this flag either. Also, the unfolding will be a post-optimization unfolding, which is a problem because this includes inlining, which makes these unfoldings much larger, much less likely to be inlined, and much less likely to fire valuable rewrite rules. All in all, these two flags do *kind of* provide a work around for GHC not putting `INLINABLE` everywhere, but they're very bad and were only ever intended for experimentation (particularly when testing GHC itself). The actual answer is to just manually put `INLINABLE` on all the things. It'd be nice if there were a compiler flag to do this automatically, and actually this is exactly what the `ReflexOptimizer` does in Reflex.
The question is where would you put a limit. It should be at some "natural" boundary. Adjusting the model is not as easy. You need to have a proof theory for that and be confident in the theory, i.e. you are not inadvertently introducing bottoms. 
&gt; http://hackage.haskell.org/package/trifecta-1.7.1.1/docs/Text-Trifecta-Delta.html is the version I used to use for dealing with positions and also allows tabstops. Ah! Trifecta's Delta seemed so mysterious before, but it's so clear now! Delta seemed to be a generalization of a file position, but I didn't understand why it was named Delta instead of Pos, nor how a position could have a sensible Monoid instance. Now that you have explained monoidal parsing, it makes a lot more sense! A Delta isn't a position, but rather (as the name implies) a delta between two positions, represented using just enough information to be able to implement `mappend` and to extract the line and column (and apparently the byte offset since the beginning of the file and the beginning of the line as well). In the `mappend` implementation, the later constructors absorb the earlier constructors (e.g. if either of the two arguments is a `Directed`, the result will be a `Directed` as well), and that seemed weird since a newline followed by a tab still contains a tab, not just a newline. But it all makes sense now: if we encountered a newline, then we already know the number of characters since the beginning of the line, so we can just compute where the next tab stop is and store that, we don't need to remember that we've encountered a tab. Nice!
Ahh, I see! So the intervals can nest, but the inner intervals shadow the outer. Makes sense, thanks for the explanation.
Does your JavaScript by any chance happen to also be valid Nix record syntax? :)
Can you create a node JS subprocess to evaluate and return the result?
Take a look at [nodejs-interop])(https://github.com/TerrorJack/nodejs-interop) :)
Is that approach related to Montague semantics?
I agree, though I think the hard parts are really already done by the totality / productivity checkers and we just have to be a *tiny* bit more explicit to go from the current Boolean answer provided by such tools to a upper, natural bound on primops. At that point we just reject expression that would take more than 2^128 primops, with a flag to push that to 2^2^128 primops. :)
As well as the haskellquestions subreddit you will also get help on Stack Overflow with this kind of thing.
I *believe* the term for this is "non-algebraic effects," though I don't have the theoretical background to know for sure. But the kind of effect we're talking about here is functions that have effects in the "negative" positions (think function arguments, but generalized to the point of `n -&gt; ((n -&gt; p) -&gt; p -&gt; n) -&gt; p`, where `n` represents negative positions and `p` represents positive ones). The key about negative positions is that the person calling the function is always the one supplying them; e.g. function arguments are supplied by the caller. `liftIO` inherently only has the `m` effects in positive positions, meaning there is just no way to use `liftIO` alone to have effects in negative positions. So you have to look for alternative classes that will allow you to encode non-algebraic effects. `MonadBaseControl` is probably the most common one. It's a bit complicated, so I'll leave it to the docs to explain it. But it should do the job fine. `liftBaseOp_` looks like it'll do nicely. liftBaseOp_ :: MonadBaseControl b m =&gt; (b (StM m a) -&gt; b (StM m c)) -&gt; m a -&gt; m c unsafeInterleaveLiftedIO :: MonadBaseControl IO m =&gt; m a -&gt; m a unsafeInterleaveLiftedIO = liftBaseOp_ unsafeInterleaveIO I personally don't like `MonadBaseControl`, but this is opinionated. I think it allows some fairly bonkers semantics and unpredictable behavior since it's a bit too liberal in what it allows. For instance, I highly doubt the above would actually behave how you would want. If you threw it in a `StateT`, the `&gt;&gt;=` would cause a strict match on the `StM m c`, which would force the interleaved `IO`. [`MonadUnliftIO` has come up as a good alternative](https://hackage.haskell.org/package/unliftio), although I'd prefer something even more reasoned than that. I like it because it only supports `ReaderT`-like monads, which are the only ones that provide consistent semantics for this stuff.
There's actually a package which takes care of that, it's called something like `inline-javascript`
OT: EK is perhaps the only one who can get away with talking C++ at a Haskell gathering, and Haskell at Scala World ;)
By squinting and tilting my head sideways, I managed to cook up a way to do it via `MonadLiftIO`, too: unsafeInterleaveLiftedIO :: (MonadUnliftIO io, MonadIO io) =&gt; io a -&gt; io a unsafeInterleaveLiftedIO action = do d &lt;- askUnliftIO liftIO $ unsafeInterleaveIO $ unliftIO d action The `d &lt;- askUnliftIO` line is pretty weird; something about impredicative types, [according to the documentation](https://hackage.haskell.org/package/unliftio-core-0.1.0.0/docs/Control-Monad-IO-Unlift.html#v:askUnliftIO)... Thanks for the explanation. It's given me a couple of topics to research. :-) 
I feel documentation is the biggest hurdle for greater adoption of the nix ecosystem right now.
I think it is a great idea to give this a name! I like this style of programming too, and it is the biggest reason why for me the type systems of languages like C#/Swift/Kotlin are still not good enough.
I would be very interested in seeing any examples people have to hand of code that they feel exemplifies the style discussed in this blog.
Be nice to see some gists demonstrating the author's interests in this paradigm 
I agree. Process could help - pull requests aren't ready until the come with accompanying documentation and changelogs, for example.
&gt; In APL and J you end with character soup Well, it's just a matter of naming things, isn't it? In array programming languages you kinda need to put a different thinking hat to write your functions wihtout variables, not so intuitive as concatenative style but still rather powerful.
Hi /u/roconnor, I'm really glad you wrote this! I explored this style of programming last year after reading /u/AndrasKovacs's [excellent comment on mutually recursive families of types](https://www.reddit.com/r/haskell/comments/3sm1j1/how_to_mix_the_base_functorrecursion_scheme_stuff/cwyr61h/). I think it exemplifies the "functor oriented" style of programming taken to an extreme. In normal "first-order" programming we work with things of kind `*`. In "higher-order" (or "functor oriented") programming we work with things of kind `* -&gt; *`. In "multi-kinded higher-order" programming (for want of a better word) we work with things of kind `k -&gt; k` for different choices of kind `k`. It would be good to collect some examples of this sort of thing.
I gather that this method involves using Data.Functor.Compose and similar newtypes a whole lot?
Additionally, I agree that Haskell doesn't support this style of programming well, although it probably supports it better than any other language! Personally I'd rather see better support for this style than for dependent types. My hunch is that the applications are far broader. Unfortunately I suspect that ship has now sailed, with regard to GHC at least.
I wonder if languages like Idris would be more up to the task...
 are the basics to get started understanding what this is about. Class | First-order | Higher order | |------|-------------|---------------| | Kind | `*` | `* -&gt; *` | | Types | `Int`, `Bool`, `String`, ... | `List`, `Maybe`, `Pair`, ... | | Unit | `()` | `Identity` | | Zero | `Void` | ??? `Const Void` ??? | | Sum | `Either` | `Sum` | | Product | `(,)` | `Product` | | Compose | Does not exist in first order | `Compose` | | "List" | `List a = Nil | Cons a (List a)` | `Free f a = Pure a | Effect (f (Free f a))` | 
I think it's unlikely. This "higher-order", or "functor oriented", style of programming seems to be orthogonal to dependent typing.
&gt; What are the main "building blocks"? https://www.reddit.com/r/haskell/comments/75fo8k/functor_oriented_programming/do5vqa5/
Indeed, it seems more likely that you want a language with good support for quotient types.
Can't see why Higher-order Void can't just be a new vacuous functor: data Void a
I think [The Essence of the Iterator Pattern](https://www.cs.ox.ac.uk/jeremy.gibbons/publications/iterator.pdf) might be considered an example of functor orientated programming. I riffed off this idea back in 2013 (!) [here](https://ocharles.org.uk/blog/posts/2013-01-22-deriving-traversals.html) to build a traversal out of functors.
`Const Void` is isomorphic to that, isn't it?
Cool, I guess one could throw Data.Functor.Day and some newtypes from bifunctors there as well.
Yes possibly. Maybe `* -&gt; *` is even more rich than I realised!
[A Uni ed View of Monadic and Applicative Non-determinism](https://people.cs.kuleuven.be/~tom.schrijvers/portfolio/scp2017.html) (for when PDF is publically available) has relevant info
Of course, but maybe it warrants being its own type.
Sort of. It's got many different variants of the same structure as "first order." It's not that Compose doesn't exist in "first order", it's just that `Compose` is actually a different higher order product! Basically all the things listed here so far are different components or possibilities within the "cartesian closed category" hierarchy. So really we're just talking about category theory. Your "first order" stuff is in Hask, and your "higher order" stuff is in the category of endofunctors.
+1 for this, as I am not sure to grasp the thing. Show some code !
I wonder whether those `newtype` wrappers could be made less bothersome by converting with `coerce`.
&gt; It's not that Compose doesn't exist in "first order", it's just that Compose is actually a different higher order version of Product! I don't think this is correct. `Sum` and `Product` are (I believe, but haven't checked) actually a categorical coproduct and product. `Sum` distributes over `Product`, for example. `Compose` is a "monoidal product" in the sense of "monoidal category" but not a "product" in the sense of satisfying [the defining properties of a categorical product](https://en.wikipedia.org/wiki/Product_(category_theory)#Definition).
`Compose` is really 'the' tensor product, if you see functors as vectors.
I believe it is this type of code: https://gist.github.com/danoneata/f46bfb5dc3ad2f15667c2024ff5178be It seems to be a rewrite of a talk, but I can't find the original talk. Look at like 107-109 for the amusing part :)
Can you flesh that out with details? It sounds like wishful thinking!
Can you flesh that out with details? It sounds like wishful thinking!
A simple example is head where the functorial approach leads directly to thinking `head :: [a] -&gt; Maybe a` is a natural transformation, rather than the buggy `[a] -&gt; a` approach thinking that head is extracting a functor-less a out of a list. More complex might be the [streaming library](https://hackage.haskell.org/package/streaming-0.1.4.5/docs/Streaming.html) which achieves excellence in functoryness API design. 
Expanding on this {-# LANGUAGE KindSignatures #-} {-# LANGUAGE PolyKinds #-} {- We want to show that both `[a]` and `Free f` are solutions to the equation t ~ 1 + (a * t) For `[a]` the solution is at kind `*` and for `Free f` the solution is at kind `* -&gt; *`. Ideally we'd like to be able to express this in Haskell with newtype KList a = KList (1 :+ (a :* KList a)) but we don't have access to "kind polymorphic" unit, sum and product operations `(1, :+, :*)`. Instead we can try passing them in as arguments. newtype KList (unit :: k) (sum :: k -&gt; k -&gt; k) (product :: k -&gt; k -&gt; k) (a :: k) = KList (sum unit (product a (KList unit sum product a))) This is still not sufficient because `newtype` (and `data`) can only construct things of kind `*`. We can get to a sort of halfway-house by choosing to work with `k -&gt; *` instead of general `k`. `k -&gt; *` generalises both `*` and `* -&gt; *` and gives us what we need, modulo wrapping and unwrapping. -} newtype KList (unit :: k -&gt; *) (sum :: (k -&gt; *) -&gt; (k -&gt; *) -&gt; (k -&gt; *)) (product :: (k -&gt; *) -&gt; (k -&gt; *) -&gt; (k -&gt; *)) (a :: k -&gt; *) (b :: k) = KList (sum unit (product a (KList unit sum product a)) b) data Identity a = Identity a data Sum f g a = Left' (f a) | Right' (g a) data Compose f g a = Compose (f (g a)) data Const k a = Const k data Product f g a = Product (f a) (g a) type List a = KList Identity Sum Product (Const a) () nil :: List a nil = KList (Left' (Identity ())) cons :: a -&gt; List a -&gt; List a cons a l = KList (Right' (Product (Const a) l)) fromList :: List a -&gt; Maybe (a, List a) fromList (KList (Right' (Product (Const a) l))) = Just (a, l) fromList (KList (Left' (Identity()))) = Nothing type Free f a = KList Identity Sum Compose f a return' :: a -&gt; Free f a return' a = KList (Left' (Identity a)) wrap :: f (Free f a) -&gt; Free f a wrap f = KList (Right' (Compose f)) unwrap :: Free f a -&gt; Either a (f (Free f a)) unwrap (KList (Left' (Identity a))) = Left a unwrap (KList (Right' (Compose f))) = Right f 
A major reason I don't use this style in libraries is that its performance is not composable (See the issues with Free vs MTL). One way to improve this is for GHC to support better data flattening via UnboxedSums and UNPACKing polymorphic types (possibly by requiring a manual SPECIALIZE pragma in the calling module). This way we'd be able to get the same performance for layered functors as for a baked-in solution so it would not be risky to use in upstream code.
Just as another datapoint, [generics-sop](http://hackage.haskell.org/package/generics-sop) programs also very heavily use this "functor oriented programming".
Yeah! One of the benefits of this approach is that it elegantly resolves some stuff that is clunky in Montague analyses.
True, but as yet not even the *syntax* is composable. For example, working with `forall a. f a -&gt; g a` is much more fiddly than working with `a -&gt; b`.
Oh yes, you’re right! So Compose is something different, but not anything we haven’t already explained in this higher order context ;)
Yes, and Day is something else different which also seems to be a monoidal product!
Are there any Finch (from Scala) like alternatives from Haskell REST APIs? (GitHub.com/finagle/finch) I tried Scotty, and Spock but they both have pretty sparse documentation. It's a bit hard to find documentation things like extracting request headers, query parameters, managing sessions and cookies etc. I know that this is how things seem to be in general, but it'd be nice to have recommendations.
https://ghc.haskell.org/trac/ghc/ticket/13177
That is the talk: https://www.youtube.com/watch?v=3U3lV5VPmOU
&gt;[**YOW! Lambda Jam 2016 Conor McBride - What are Types for, or are they only Against? [63:38]**](http://youtu.be/3U3lV5VPmOU) &gt;&gt;Doctor Who: I think my idea’s better. Lester: What is your idea? Doctor Who: I don’t know yet. That’s the trouble with ideas: they only come a bit at a time. &gt; [*^YOW! ^Conferences*](https://www.youtube.com/channel/UCAvGvvEemkeX8hurdPXr7hA) ^in ^Science ^&amp; ^Technology &gt;*^4,850 ^views ^since ^Jun ^2016* [^bot ^info](/r/youtubefactsbot/wiki/index)
A few other parallels: 1. * accessing the nth property of a data structure (relative to the number of fields) * accessing the property named "foo" (absolute, independent of the other fields) 2. * referencing a child node via a key (relative to the Map into which to perform the lookup) * referencing a child node via a pointer (absolute, independent of the Map) 3. * sending data in streaming library (relative to the order in which the stream transformers are composed) * sending data on the internet (absolute urls uniquely identifying the destination) I think the verb conjugation parts (time independence and evidentiality) are even more insightful: which properties of terms and variables does the language force programmers to know about? Statically-typed languages force us to know about their types, that is, the set of values they could possibly contain. C forces us to know whether variables are allocated on the stack or on the heap. Java forces us to know whether a variable holds a primitive or an object. Rust forces us to know the lifetime of each variable. [cryptol](https://youtu.be/sC2_5WaavFc?t=6m44s) forces us to know how many bits of storage each variable uses. A language with linear types would force us to know how many times a variable has been used. We think of most of those as part of the type of a variable, but different programming languages choose to encode different kinds of information in their type system, in the same way different spoken languages choose to encode different kinds of information in their verb conjugations.
Reminds me of McBride's talk "Is a type a lifebuoy or a lamp?" https://skillsmatter.com/skillscasts/8893-is-a-type-a-lifebuoy-or-a-lamp
`inline-c` is so great, I wish there was a variant for every language! But is there one for javascript? I couldn't find any.
&gt; Unfortunately I suspect that ship has now sailed, with regard to GHC at least. Why? Writing a typechecker plugin which sees that `Compose f (Compose g h)` is equivalent to `Compose (Compose f g) h` does not seem harder than the arithmetic plugins we have which see that `Vec a ((i + j) + k)` is equivalent to `Vec a (i + (j + k))`.
Wow I didn't expect it to be so hefty
Because we've gone the way of dependent types and `TypeInType` because that's what the masses, and those who were putting the work into GHC, wanted. I don't see any reason why the current state of GHC should be compatible with a fully generalised "functor oriented" style of programming. I don't see any reason why it *wouldn't* be compatible either, but I think the onus is on those who think it is to provide evidence.
I struggled with fiber products for a long time, until I found database joins to be a decent example. Today, I'm struggling to understand fiber products. Also, this was completely off-topic, as I just realised the cover shows a pushout rather than a pullback.
&gt; I just realised the cover shows a pushout rather than a pullback. A pushout is a pullback in the opposite category. What's the problem?
Where can i buy it?
Right, there aren't any actual arrows on the cover. But the `g a` is quite suggestive that it's actually showing a pushout rather than a pullback (fiber product), so my example is probably off.
This seems similar: https://pothi.com/pothi/book/bartosz-milewski-category-theory-programmers
Your table also extends to other categories such as `Pro(C)`, profunctors on your base category `C`. Then we gain a new kind of composition in addition to the other sum/product/compose, namely profunctor composition : `Procompose` in https://hackage.haskell.org/package/profunctors-5.2/docs/src/Data.Profunctor.Composition.html#Procompose . Which is a kind of composition that doesn't exist in the lower-kinded categories. 
In practice, it quickly becomes messy. Also, J hooks and forks are rather kludgy and limited, in Conc you can easily combine functions of any arity. Also, Conc preserves linearity, and I believe it's a nice feature to have. 
I don't know what finch is like, but for REST APIs I use servant.
Yep, until the fancy symbol was invented, one wrote Rs for "rupees" and Re for "rupee". Also, hey, that's pretty cheap...
The [code for this](https://gist.github.com/paf31/5c1279796d66fe04a177e34b0d674ac6) was linked here last week, but I thought people might appreciate a slightly more detailed write-up.
&gt; I have been thinking about writing this post for a few years now, and wanted to write something convincing; however, I do not think I am up to the task. Instead of trying to persuade the reader, I have elected to try to simply name and describe this style of programming so that the reader might notice it themselves when reading and writing code. Hopefully someone more capable than me can evangelize this approach, and perhaps even create a practical language suitable for this style of programming. I think this was a wise decision. Provoking discussion can be sufficient to illicit impressive arguments from the community (not that author is not capable of making one ... but one only write so much)
&gt; I struggled with fiber products for a long time You could use the e-book version then.
I'm the author of that gist and I was really surprised to find this link given that I consider myself a novice at Haskell. The code is an attempt at solving [these exercises](https://stackoverflow.com/a/10242673/474311) by /u/pigworker (Conor McBride) on applicative functors. BTW does anyone know what's the "lovely way" of implementing the `duplicates` function (exercise 6) using different calculus? 
One thing I've found invaluable for this style of programming is typed holes with type-directed search. It tends to help find implementations when you have a lot of rigid type variables lying around. PureScript has type-directed search, thanks to /u/kritzcreek, and GHC has a simpler version merged into master. I'm looking forward to using it in GHC, and hoping that it gets expanded to do more interesting forms of type search.
I imagined it to be a (near) synonym for Church encoding, which to me is a lot about deconstructing things into their fundamental operations, rather than their fundamental components. So, for example, tuple a b op = op a b where instead of fixing the data type that binds together the a and b, you think, "hey, the user probably doesn't care how I store this, as long as they will be able to supply their own operation later that gets access to all the data". like how at this point there are three different ideas of the word. We clearly need some examples!
Every (open) file has a file handle, but not every handle is backed by a file. For example default/un-redirected input and output are not files (console input/output), another example would be open network sockets.
The link was posted [here](https://www.reddit.com/r/haskell/comments/73e7l3/i_made_bartosz_milewskis_book_category_theory_for/dnqzm2b/) by /u/codermikael who put the converted PDF version (made by /u/hmemcpy) on [Lulu](http://www.lulu.com/shop/bartosz-milewski/category-theory-for-programmers/paperback/product-23352094.html), a book self-publishing website. It's actually a fantastic deal, you're basically paying for printing and shipping only which is very generous from the creators! I've actually read the blog posts already but wanted this on my bookshelf :)
Well that made me chortle
Oh wow, this is amazing! Though I must admit, I would have waited a bit! The PDF is just a very first conversion -- no proofreading/editing was done yet, and it's not typeset using math fonts yet! However, it's nice to see this in hard copy, I'm sure Bartosz would be proud :)
&gt; I imagined it to be a (near) synonym for Church encoding That's definitely not what it is.
It's amazing to realise that the "series of blog posts" Bartosz has written amounts to a complete, high-quality, 500 page textbook on category theory! 
BUT I WANTED IT THEN AND THERE To be honest, I will inevitably go to the PDF version if I need to read something and am too lazy to stand up and get the book from my shelf :) Thank you for your work!
Wait, does BM get the proceeds from this?
No, this is just a service that prints PDFs, nobody is getting paid, as far as I can tell. Maybe BM will want to sell it officially one day. 
It was posted on this sub a few weeks ago, but unfortunately I don't remember the name.
He should, then I will buy it.
Can you post a few more photos of the inside? Want to see what the pages look like :)
Yeah, thanks. I also ended up using servant. It's fairly similar to Finch's server side.
[nodejs-eval](https://www.reddit.com/r/haskell/comments/71za1t/ann_nodejseval_execute_nodejs_scripts_in_haskell/)? This would word for what the OP wants, but it seems to be missing one of `inline-c`'s killer features, the ability to splice some specially-crafted Haskell values into the C snippet. Now that I'm thinking about this more, however, that missing feature is the easy part!
Not until the result of the function is scrutinized by some sort of `case` or primitive.
A rank-N type can have `forall`-quantified types as arguments to the function arrow `(-&gt;)` type constructor. Without that `newtype`, you could still write something like this: away :: (forall a. HiHo a -&gt; IO a) -&gt; HiHo Silver -&gt; IO Silver An *impredicative* type can have `forall`-quantified types as arguments to *any* type constructor. For example, a list of actions: each :: [forall a. HiHo a -&gt; IO a] -&gt; HiHo Silver -&gt; IO [Silver] GHC doesn’t really support this—the `ImpredicativeTypes` extension is broken—so the standard way to work around it is to hide the impredicativity by introducing a `newtype`, manually wrapping &amp; unwrapping so the typechecker knows when to abstract &amp; instantiate those quantifiers. each :: [UnliftIO HiHo] -&gt; HiHo Silver -&gt; IO [Silver] 
Awesome, I ordered it! Here's a code that you can use to get 30% off: https://www.retailmenot.com/view/lulu.com?c=9213533
Ordered
That's what I was thinking. Maybe a heterogeneous list would be a better alternative if someone is looking for tuples that long.
You're also assuming there's no bugs that would let you break out of dhall and run arbitrary machine code or whatever. Of course, this is true no matter what data you're fetching and whether you call it 'code' or not.
The problems OP mentioned in Haskell are quite exactly those solved in current dependent languages, i. e. the ability to define basic functors as functions as opposed to irreducible first-order terms.
I don't see how quotients would help, care to elaborate?
You may be interested in [Grammatical Framework](http://grammaticalframework.org), a type-theoretic approach to natural language semantics. It's pretty crazy to see [source code](https://github.com/GrammaticalFramework/GF/tree/master/lib/src) for all these different languages.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [GrammaticalFramework/GF/.../**src** (master → b251d3f)](https://github.com/GrammaticalFramework/GF/tree/b251d3f9a44de467514584adaa29857e586dc544/lib/src) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
I feel bad, BM doesn't get any income for this. It'd be great if we could back this project somehow.
Congratulations, you invented the C preprocessor. Or, less snarkily, it's true that for certain classes of problem, like configuration files, a more restrained computation model is useful. Heck, even most business software would probably be better served by something closer to that end of the spectrum. But if you cut back too much, all that happens is more people start trying to pretend they're systems programmers and work in C. 
I find linearity (along with associativity) one of the selling points of concatenative calculus. I am still wondering about the implications of replacing `Apply` with `Compose` in data Term = Apply Term Term | Lambda Term | Variable DeBruijnIndex because that is the true nature of concetanive programming, isn't it? But obviously I am missing something, because if we still have access to `Lambda` and `Variable` then we should be able to write `let` expressions as a syntactic sugar for lambda composition. For example let a = 1 b = 2 in a + b is equal to 1 2 -&gt; a -&gt; b -&gt; a + b
Oh, I thought quotient types would let us express the law's we'd expect above. Maybe I'm misunderstanding what quotient types do
That's fabulous to hear!
Could set up some sort of funding page? I'm not exactly sure how that works and I suppose it would at least require his agreement, but it's definitely something I (and probably many other people) would happily contribute to!
As I've recently learned and can now fully confirm from the experience with my compiler project, error should not be used there. The alternative to deepSeqing your resulting data structure is using an error state monad -- It would force the whole thing while determining whether there was any error condition. It's a bit fiddly to put in, but it's really worth the effort. :)
Thanks for these insights!
Where can I buy some Cardano? :)
Thanks for these insights! &gt; Notably, people there keep coming up to me with new companies using Haskell that I'd never heard of before What are some ways of finding these companies? Maybe it's just a matter of attending HaskellerZ and ZuriHac and mingle.
Interesting, thanks! Do you know of a good way to find other Haskell and/or crypto companies in Zürich/Zug?
I agree wholeheartedly! And I intend for `coxswain` to someday help enable this style, I suspect via the extensible sums (variants) more so than the products (records). EG (this is somewhat sugared) newtype At x f = At (f x) interpF_G :: V (At a) {F, G | rho} -&gt; V (At a) {G | rho} interpF_X :: V (At X) {F | rho} -&gt; V (At X) rho https://ghc.haskell.org/trac/ghc/wiki/Plugins/TypeChecker/RowTypes/Coxswain Does that connection seem plausible to you?
Any pictures of what it looks like inside?
It seems like the main contribution here is adding variables to a stack-based language, but that's hardly new: Factor already has variables, for example. Once you do this though, you throw out the main advantage of concatenative languages which that you can reason freely without concern for variable capture. You might as well just write Haskell. My main problem with stack-based languages like this is that the simplicity is only superficial. For example, suppose you try to define `map` as such: [] _ map = [] (xs x Cons) f map = xs f map x f Cons The issue with this definition is that `f` will be applied to `x`, yes, but also to _the entire rest of the state of the program_, including the result of `xs f map`. As such, any nice properties you might want to talk about like `(1, 2, 3) [f] map == (1 f, 2 f, 3 f)` (assuming parentheses are lists) are not going to hold. You can use a type system to limit `f` to one argument, but then you need to write signatures and there's still no syntactic clue regarding what is happening. There are issues with the types in this post too. For example, `add : Int Int -&gt; Int` is not really the whole story. This is because `add` is applied to _the entire stack_ returning a new stack, thus the type is really more like `add : {a} Int Int -&gt; {a} Int` (where `{a}` denotes the stack variable `a`). This is important because functions like `dip` potentially operate on _the entire stack_ and need types like `{a} b [{a} -&gt; {c}] -&gt; {c} b` (i.e. it applies the function on top of the stack to the _entire stack_ below the second element). You also end up needing polymorphic recursion quite a bit as recursive functions often build up a stack and then fold it back up to produce a result. I think a better way to go if you want to a language optimized for pointfree programming is to look at John Backus's [FP](https://en.wikipedia.org/wiki/FP_%28programming_language%29) and [FL](https://en.wikipedia.org/wiki/FL_%28programming_language%29) languages. In FP and FL, functions operate only on the arguments relevant to them, not the entire state of the program, and this makes reasoning about what is happening much easier. There's no reason you couldn't throw a postfix syntax on it too if that's what you were after, and variable-free pattern matching is also an option. At the end of the day though, I maintain none of this is worth it. Pointfree languages fail on basic things like parameterizing an existing function: You often need to rewrite the entire thing to get a function to take another value, and it's especially painful when dealing with higher-order functions. FL had some ideas for how to handle this, but it still ended up being quite awkward. I'd just write Haskell, use the pointfree style where appropriate, and that's it. If you wanted to go all out, add syntax for FP's construction form (i.e. `[|f, g|] == \x -&gt; (f x, g x)`) and something equivalent to Factor's spread (i.e. `{|f, g|} == \(x, y) -&gt; (f x, g y)`.
**FP (programming language)** FP (short for Function Programming) is a programming language created by John Backus to support the function-level programming paradigm. This allows eliminating named variables. The language was introduced in Backus's 1977 Turing Award lecture, "Can Programming Be Liberated from the von Neumann Style?", subtitled "a functional style and its algebra of programs." The paper sparked interest in functional programming research, eventually leading to modern functional languages, and not the function-level paradigm Backus had hoped. FP itself never found much use outside of academia. *** **FL (programming language)** FL (short for Function Level) is a programming language created at the IBM Almaden Research Center by John Backus, John Williams, and Edward Wimmers in 1989. FL was designed as a successor of Backus' earlier FP language, providing specific support for what Backus termed function-level programming. FL is a dynamically typed strict functional programming language with throw and catch exception semantics much like in ML. Each function has an implicit history argument which is used for doing things like strictly functional input/output (I/O), but is also used for linking to C code. For doing optimization, there exists a type-system which is an extension of Hindley–Milner type inference. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
Now I can finally learn to sound smart!
If you want a concatenative calculus, you can get rid of Apply, Lambda, _and_ Variable: All you need is `Compose`. The catch is that you also need primitive combinators if you want to be able to do anything interesting (e.g. there's no way to write the y combinator with Compose alone). If you have not read about Joy already, I highly recommend this article on [combinator bases for concatenative languages](http://tunes.org/~iepos/joy.html). 
&gt; This is because `add` is applied to the entire stack returning a new stack, Nope. The point is, there's actually no stack at all, only function composition.
There is a stack. What do functions operate on if not a stack of some kind? Functional concatenative languages like Joy are fundamentally about composing functions from stacks to stacks. If there were no stacks (or lists or whatever you like to call them), how could you explain the semantics for something like `dip`? Functions like `dip` have no fixed arity: They apply a function to the entire rest of the stack.
&gt; how could you explain the semantics for something like `dip` Let me explain `apply`: `apply : forall a.., b.., -&gt; a.. (a.. -&gt; b..) -&gt; b..`. This is a polymorphic function. But unlike functions in most languages, it is polymorphic not over *a type*, but over *a type set*. So, if `f : Int, Int -&gt; Int`, then `apply` in `{f} apply` monomorphisate into `Int, Int, (Int, Int -&gt; Int) -&gt; Int`. No stack semantics required at all. If `dip` is a function such as `{f} {g} dip` is `g {f}`, then `dip = swap apply`. 
&gt; Let me explain apply… I understand that explanation. I think the point here though is that `apply`, like all functions, operates on a value. What is that value? It's a stack! If it's not a stack, what do you call that value? &gt; {f} apply monomorphisate into Int, Int, (Int, Int -&gt; Int) -&gt; Int The type of `{f} apply` should be the same as the type for `f`. The semantics are {f} apply == f`, after all! I'm afraid you've lost me completely. &gt; dip = swap apply No, that's not correct. `{f} {g} dip == g {f}`, yes, but `{f} {g} swap apply == {g} f` and `g {f} /= {g} f`. It is not possible implement `dip` using first-order functions without some sort of place to temporarily stash the second argument.
I looked closer at the idea of "parallel concatenation" after seeing your edit and I think that's the entire problem with Conc. What could the semantics of `,` possibly be? Functions like `dip` are not of a fixed arity, so if I write `dip,dip`, what does it mean? What does `apply,apply` mean? Does `swap,swap` do something different if I change the type signature of `swap` so it takes three arguments instead of two and ignores the third one? If so, isn't that bonkers? It seems like `,` only makes sense _if_ you locally know the arities of the functions in question and _if_ you're willing to have the semantics of your program depend on said arities. I maintain this is really bad idea. `,` is basically `dip` but with "magic" so that it knows how many places under the top of the stack to apply the second function.
&gt; If it's not a stack, what do you call that value? A product? &gt; The type of `{f} apply` should be the same as the type for `f`. And it is. Generalized composition of `-&gt; (Int, Int -&gt; Int)` and `Int, Int, (Int, Int -&gt; Int) -&gt; Int` is, obviously, `Int, Int -&gt; Int` &gt; No, that's not correct. Yes, I forgot about `,id`. 
&gt; A product? This is just a matter of nomenclature then. A stack in the Joy sense is just a product where one side is always another stack or null. In Joy, the semantics of functions are stack to stack. In Conc, I'm not clear that there is sensible semantics at all given `,`.
&gt; It seems like `,` only makes sense if you locally know the arities of the functions in question and if you're willing to have the semantics of your program depend on said arities. Yes. But with static typing, you already have to know arities before execution. Why not take an advantage from this? Surely it could be used to write an unreadable code. But you can write an unreadable code in any language, so it's not really the point. 
&gt; Yes. But with static typing, you already have to know arities before execution. Why not take an advantage from this? Two reasons: 1. It makes the operational semantics of your program dependent upon the types. What does `f,g` mean? You have _no idea_ unless you know the type of (at least) `g`. If you do think you know, I'd encourage you to write out the semantics in terms of the type of `g`. 2. You do not always know the arity! Again, what are the semantics of `apply,apply`? 
&gt; Again, what is the semantics of `apply,apply`? It doesn't typecheck. &gt; It makes the runtime semantics of your program dependent upon the types in a very severe way. In languages where functions can take and return arbitrary amount of values, the semantics *already* depends on actual arities. In fact, even Haskell is not that clear compared to languages where functions are called with explicit number of arguments. 
Amazing. Just got this. I'm not sure this includes the second series on CT, but amazing nevertheless. Thanks to Bartosz I'm now able to understan Mac Lane books on CT!
&gt; It doesn't typecheck. Ah, I see. So `,` is just a convenient form of `dip` for when the arity of the second function is known. &gt; In languages where functions can take and return arbitrary amount of values, the semantics already depends on actual arities. No they don't, unless I misunderstand you. For example, Scheme has multi-argument functions and multiple-value returns, yet Scheme has no types at all and its formal semantics do not involve the inspection of arities. &gt; In fact, even Haskell is not that clear compared to languages where functions are called with explicit number of arguments. Haskell's semantics are perfectly clear in this sense. I'm talking about semantics formally: Whether or not you think curried functions are clear to programmers is another question altogether.
&gt; So , is just a convenient form of dip for when the arity of the second function is known. You can do `id,apply` as well. You just can't have several variadic functions in one `,` expression. &gt; For example, Scheme has multi-argument functions and multiple-value returns, yet Scheme has no types at all and its formal semantics do not involve the inspection of arities to work out what to do. So you can't have something like `dip` in Scheme? 
&gt; So you can't have something like dip in Scheme? You could, but it wouldn't be completely equivalent and I'm not sure I understand the relevance of the question. It was never `dip` I took issue with, after all: It was `,`, which you indeed _cannot_ write in Scheme. I think perhaps we are using terminology differently or something along those lines and thus are talking at cross-purposes. I will just leave this be for now. I hope that seems reasonable!
Thanks for the links! This has been on my to-read list for a while, I think having the physical copy might help.
Nice! I really enjoy reading things like this even though they're technically still way beyond my ability to *really* comprehend. The utility of this is, admittedly, still somewhat lost on me but if I were to advance a guess, I would suppose that this is mostly interesting for being able to implement the concept of "an alternative to EDSLs" (from the compiling to categories paper) in a more general way; perhaps even in a way that might eventually allow the compiler to shift one representation into another representation by (apologies for fuzzy terminology here) 'casting objects/representations into new categories' and going from there? Or am I totally off the mark?
I'm not sure I understand what problem this saves us from, exactly. The fundamental fallacy of the developer as an architect is to imagine that a program would run so much more smoothly if only those pesky users couldn't keep doing arbitrary things with the software, or that our program would be so much simpler if only we didn't have to worry about the unreliability of external resources like the file system, the network, etc. How does such a system successfully encapsulate the inherit unreliability of user input or non-extant / accessible resources? In what manner is an interpreter compilation error more 'safe' than a runtime error? Is this not just a runtime exception of a different color? Are we not, as programmers, still 'stuck' with dealing with unforeseen states of systems which have not been fully described by our programs?
Here is what I said on the previous thread about this: The abstract for Compiling to Categories says this: &gt; It is well-known that the simply typed lambda-calculus is modeled by any cartesian closed category (CCC). This correspondence suggests giving typed functional programs a variety of interpretations, each corresponding to a different category. A convenient way to realize this idea is as a collection of meaning-preserving transformations added to an existing compiler, such as GHC for Haskell. This paper describes such an implementation and demonstrates its use for a variety of interpretations including hardware circuits, automatic differentiation, incremental computation, and interval analysis. Each such interpretation is a category easily defined in Haskell (outside of the compiler). The general technique appears to provide a compelling alternative to deeply embedded domain-specific languages. (from http://conal.net/papers/compiling-to-categories/) The approach here, on the other hand, is to try to provide a single embedded DSL which can be used with any CCC. I don't think you would necessarily use these for the same things though. Using the GHC core means you have access to the entire GHC front end, whereas with the EDSL approach, you need to add all of that functionality to your DSL manually. If you want something as simple as pattern matching, you'd be out of luck. On the other hand, using a DSL is nice because everything is contained in one module, all in Haskell. So, this probably has its applications, but I would probably only use this for writing relatively simple DSLs and terms. I was mostly just curious to see if I could write it. Compiling to Categories also deals with CCCs where there is some constraint on the types of the objects themselves, which I haven't done here. I suspect it's possible though, at which point you should be able to use this with all of the examples listed there.
Amen!
I agree it would be awesome to reward him, but thanks should be given that he specifically licensed his content to allow these remixes and republications.
&gt; As the Yesod app is statically compiled resulting binary doesn't need any dependencies. Hmm... `integer-gmp`? 
Thank for you writing such a detailed explanation. If it isn't already, something like this should be part of the core GHC docs with other people also contributing the **correct** know-how. It seem that even people writing multi-part blog posts about Haskell perf seem to get it wrong, as it is being suggested by https://www.reddit.com/r/haskell/comments/755k4l/comment/do46rxo (again, second hand info) While what you are saying *seems* logically correct, it still doesn't explain why this patch was required -- https://github.com/chrisdone/lucid/pull/67/files The functions are small enough that GHC should have automatically considered for inlining AND specialization in the first place. 
Thank for you writing such a detailed explanation. If it isn't already, something like this should be part of the core GHC docs with other people also contributing the **correct** know-how. It seem that even people writing multi-part blog posts about Haskell perf seem to get it wrong, as it is being suggested by https://www.reddit.com/r/haskell/comments/755k4l/comment/do46rxo (again, second hand info) While what you are saying *seems* logically correct, it still doesn't explain why this patch was required -- https://github.com/chrisdone/lucid/pull/67/files The functions are small enough that GHC should have automatically considered for inlining AND specialization in the first place. 
That comment is correct. I should have also mentioned that all functions are treated as `INLINABLE` with respect to other functions in the same module; i.e. all functions will be specialized at call sites in the same module, no matter the size. GHC has all of this documented throughout its manual. If you ask the questions "How does `INLINABLE` behave? `SPECIALISE`? How does the simplifier interact with these?" you will find all these answers. I suppose the problem is that it's not clear what questions one needs to ask to understand this stuff. As for that PR, I suppose it's possible I've overestimated GHC's size heuristics? I'm not sure why that PR is necessary. I'd have to do some Core inspection.
Mine is coming tomorrow. I'm really excited for it 
Thanks for the explanation! I need to bite the bullet and just read a book on type theory... 
I currently use an Acer Chromebook 14 on [GalliumOS](https://galliumos.org/) as my laptop and developing in Haskell is not as easy as using my desktop. If it's to start using Haskell and do small exercices, it works fine, but as soon that you start compiling bigger projects, the compilation time gets high enough that you lose all momentum. To fix this, you can use GHCi, but you still have to restart the process every hour or so to free some RAM. Even on my much more powerful desktop (16GB) I once had to restart GHCi because it uses so much RAM. Also, installing libraries using stack takes a lot of time (more than 15 minutes to install ghc-mod) so you have to prepare yourself to do something else while waiting for the libraries to install. For the development tools that I use, I mainly use Atom with IDE-Haskell (https://atom.io/packages/ide-haskell) and other plugins to integrate the REPL. Again, it's not bad. but not super fast. The things that are not instantaneous are mostly error highlighting and type info on hover. Compared to IntelliJ's IDE, the IDE-Haskell plugin is very simple. It doesn't support auto import, jump to definition or renaming, but I found it easy to get used to it. It has more than enough features to make you productive and to make you feel like you're in an IDE (not in Notepad++). I've heard that Emacs works well with Haskell, but I haven't had the time to try it, so maybe you should look into it! I have tried to ssh to a more powerful machine, but you I couldn't use all the tools that I'm used to, so I now use X2go (Like X11 forwarding, but faster) to forward the Atom window to my laptop from my Desktop and it works really well. In that case, you use your Chromebook more like a thin client so I don't know if that fits your needs. Also, if you want to develop using your Chromebook, you need to install another distribution or using Crouton so you can install and use all cli tools you need to develop. Acer Chromebook 14 specs: CPU: Intel Celeron N3160 RAM: 4GB Storage: 32GB emmc
For the record, this was a terrible idea. I was attempting to recreate Java Hibernate's lazy-loading feature for foreign keys, after having implemented an "ORM" interface. The codebase is already reasonably complex, but all the code rests in the `SQLPersistT` monad, which is a synonym for `ReaderT`, and is thus supported by both `unliftio` and `MonadBaseControl`. Things went awry. Exceptions from `SQLite` about library functions being used incorrectly, and `STM.atomically was nested` errors. The latter pops up nowadays when one nests `unsafePerformIO` calls (even simple `trace` functions), and is to be expected when you have no idea when a piece of `IO`-like code is gong to be evaluated. To live is to learn. 
To OP, if you got a Chromebook with 8gb of RAM instead, you'd notice a massive improvement in speed (in general), so that's one thing to keep in mind as well.
I can never get the symbol to work, still using Rs.
I used one (an Acer Chromebook C720) for over a year with Arch Linux on it. I used Stack and Spacemacs most of the time. It was rough, but it wasn't much worse than the hardware I'd had access to up until then. :/ It only had 2GB or RAM, so I learned to tolerate lock-ups and pauses pretty well. As another commenter said, you'll have to learn to wait for dependencies to be built. If you can, try using the same Stack resolver for as many projects as possible. This will ensure that you can keep dependency building to a minimum. Intero uses a ton of RAM, so you might want to look into ghcid or other alternatives. My preferred, excellent-in-theory advice would have been to use Nix on Arch, but that's kind of broken now. **NixOS, or Nix on any other Linux, is probably the best idea**, because on Nix, almost all Haskell packages are available from binary caches, which is a *huge* benefit and would probably make your life a couple orders of magnitude easier. I have a much better laptop now, though, and I'm happy for it. TLDR: not undoable, try Nix(OS), avoid Intero
At the other extreme, I've been using a chromebook pixel, which is much like a MacBook in terms of performance, via crouton. I ran into unrelated X server issues so I'm just using vim through a terminal. However, I found I have to semi-regularly wipe my .stack snapshots as it's easy to run out of disk space, and ensure that all dependencies compile in that directory as far as possible. This means using custom snapshots (and the new extensible snapshots). It also means I have to install custom stack.yaml files to any open source libraries to which I wish to contribute; I then rebase before PRs. Docker and nix are out of the question, too. I do have access to another machine to which I can ssh; I have on occasions generated static binaries on that machine and transferred them to the chromebook using ssh. I typically run a simple http server from the base directory so I can access files through the browser.
In lambda calculus you can also get rid of `Lambda` and `Variable` with SKI combinators, all we need is `Apply`- no? Yet not even GHC Core is not crazy enough to do that. Yet it seems almost all concatenative languages do exactly that and I cannot figure out why.
Or the result is being used as a function in an application. 
Atom ide-haskell includes goto definition. “Haskell Ghc Mod: Go To Declaration” in command list.
I've cloned it and am trying to make it export to epub, and sub-sequentially to mobi. I'll make a PR back when I manage to make it work properly. Currently it exports the epub, but I couldn't make it do it using the `ctfp.tex` file, I get an error when I try to include the `preamble.tex` file in the epub, and also still having problem with the image paths. You can see it here: https://github.com/mlopes/milewski-ctfp-pdf/blob/master/src/Makefile#L13
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [mlopes/milewski-ctfp-pdf/.../**Makefile#L13** (master → 8677933)](https://github.com/mlopes/milewski-ctfp-pdf/blob/8677933404e4af7d913e5d6f9ee547eed89ae440/src/Makefile#L13) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
You might want to try `performMajorGC` from from `System.Mem` instead of restarting GHCi.
Local variables compromise the concatenative property, sure, but using them judiciously helps avoid the common Forth problem you hint at of “overfitting” a point-free solution to the structure of a particular task. They’re “goto for data”, providing unstructured access to values, which can make your code more resilient to changing requirements—parameterising an existing function, &amp;c. As a concatenative programming convert, I just think that a compositional, point-free “pipeline” style is a good *default*, not that you should fanatically avoid variables like Chuck Moore (though he has his reasons). Of course all functions are *technically* applied to the whole program state (typically a stack), but a type system enforces that they can only read or modify particular parts of that state—everything is fully polymorphic over the part of the state that it doesn’t touch. It doesn’t really cost you anything, and it buys you some convenience. And while (again, technically) you often need polymorphic recursion, it’s largely restricted to the specific case of stack-kinded type variables in higher-order stack-polymorphic functions, which can be a simple special case in a typechecker. These mostly show up not in user code, but in standard-library code (e.g., a fold where the stack acts as the accumulator) which is only written once. In [Kitten](https://github.com/evincarofautumn/kitten), the inferred type of `map` automatically restricts its functional argument to one input and one output. Factor’s “spread” is called `both`, of type `&lt;A, B, C, D&gt; (A, B, (A -&gt; C), (B -&gt; D) -&gt; C, D)`, and FP’s construction form is called `both_to`, of type `&lt;A, B, C&gt; (A, (A -&gt; B), (A -&gt; C) -&gt; B, C)`; there is also a `to_both` of type `&lt;A, B&gt; (A, A, (A -&gt; B) -&gt; B, B)`. Note that these *could* have more general stack-polymorphic types—also exposing evaluation order—but they’re restricted by signatures to better convey their intent. These types are just syntactic sugar for types that are fully polymorphic over the “rest of the stack”. Everything just comes together very nicely in the types, especially when you want to enforce substructural properties and avoid the need for GC. To me, that’s the main benefit of concatenative programming—it readily admits a “low floor, high ceiling” language. 
Why do local variables compromise concatenative property? Isn't it possible to convert all local variables into series of `dup`, `swap`, `drop`? 
I guess that is this stuff: https://stackoverflow.com/questions/25554062/zipper-comonads-generically If you have a zipper, you can traverse your data structure, but you also get the other values at each step, which is very handy when checking for duplicates.
&gt; integer-gmp To **run** binary on production operating system, you don't need any dependencies (no GHC, no Haskell libraries, no GCC, etc). You can just run the binary on a plain OS (eg. Ubuntu, Alpine, etc) without a need to install anything.
It is possible. `dup`, `swap`, and `drop` correspond to the W, C, and K combinators in combinator calculus, or the logical rules of contraction, exchange, and weakening, and an “abstraction algorithm” can convert from lambdas to pure combinator form such as BCKW or SKI (often with a loss of efficiency). The problem is that when you have local variables in your source, you can no longer factor out a function just by cutting and pasting code, because you need to make sure the same locals are in scope. IDEs typically handle this for you in a language like C# or Java with lots of locals, but it’s nice that it’s not necessary in purely concatenative code. 
&gt; Rather than using timer-based interrupts and having a lot of overhead in swapping process states in and out of memory, you just chop each processes' AST up into bitesize chunks that you can execute to finish. Nice. Basically coroutines/cooperative multitasking, which *could* work if you had a provably-bounded execution time for each 'chunk'. I think for general-purpose workloads the preemption/timer interrupt model still works best, though. The more sensible long-term goal is to forgo timer ticks altogether in a different way, by using many cores and giving each task its own hardware thread.
Well, you can just substitute the locals beforehand - e.g. foo = 5 local + where local = 1 2 + to foo = 5 1 2 + + And after this step you can factor whatever you want. The downside is, your application now has two steps, but I don't think performance will be much different. Btw, what is the asymptotic complexity of "abstraction algorithm"? 
Thank you!
It’s not always that simple to do the substitution, depending on the dataflow—consider the case when locals are calculated dynamically and used multiple times in different degrees of nesting in closures. The result of a naïve abstraction algorithm, with no optimisation of combinators, can easily have exponential time complexity. I’m not certain of this, but I think you’ll always pay a logarithmic time penalty in pure combinator form, for the same reason that purely functional data structures do over mutable data structures. But these combinators are also easy to optimise, so it doesn’t really matter for an ahead-of-time compiler.
&gt; TLDR: eminently doable, try Nix(OS), avoid Intero Having moved to nixos for a bunch of reasons, I use dante instead of intero and am very happy with it. I don't seem to get the ram issues that others see, but maybe I'm not doing enough...
Use Code SAVENOW30 to cut 30% off the price! (Expires tomorrow apparently)
 http://www.cs.kent.ac.uk/~cr3/toolbox/haskell/dot-squashed.ghci didn't work for me
Haven't you mentioned that you are going to implement `,` in a pure untyped language?
I will occasionally work on a Dell Chromebook 11 w/ i3 via crouton. Compiling is a bit slow but otherwise it works fine. The main issue I have is that a single installation of GHC+packages takes up nearly the whole 16gb ssd, so I can only ever really have one project at once, and do a total recompile when switching projects. Can't even use nix, the immutable nature often leaves a lot more data lying around than equivalent apt-get and stack installs
And the result of said application is scrutinized by a case statement or primitive. Eventually a case statement or primitive has to be involved. 
[archived version](https://web.archive.org/web/20130707154702/http://www.cs.kent.ac.uk/people/staff/cr3/toolbox/haskell/dot-squashed.ghci)
And since I've had issues accessing that link, here's another copy: -- if your editor doesn't understand :e +line file -- (jump to line in file), you'll need to change -- functions find and loadEditErr below :set editor gvim -- 6.6.1 doesn't have this, omit this def for later ghci's :def cmd \l-&gt;return $ unlines [":def cmdTmp \\_-&gt;"++l,":cmdTmp",":undef cmdTmp"] -- make commands helpful let { cmdHelp cmd msg "--help" = return $ "putStrLn "++show msg; cmdHelp cmd msg other = cmd other } :def . cmdHelp readFile ":. &lt;file&gt;\t\t-- source commands from &lt;file&gt;" let pwd _ = return "System.Directory.getCurrentDirectory &gt;&gt;= putStrLn" :def pwd cmdHelp pwd ":pwd\t\t\t-- print working directory" let ls p = return $ "mapM_ putStrLn =&lt;&lt; System.Directory.getDirectoryContents "++show path where {path = if (null p) then "." else p} :def ls cmdHelp ls ":ls [&lt;path&gt;]\t\t-- list directory (\".\" by default)" -- todo: merge redir/redirErr, but keep vars/afterCmd distinct, for nesting let redir varcmd = case break Data.Char.isSpace varcmd of { (var,_:cmd) -&gt; return $ unlines [":set -fno-print-bind-result" ,"tmp &lt;- System.Directory.getTemporaryDirectory" ,"(f,h) &lt;- System.IO.openTempFile tmp \"ghci\"" ,"sto &lt;- GHC.Handle.hDuplicate System.IO.stdout" ,"GHC.Handle.hDuplicateTo h System.IO.stdout" ,"System.IO.hClose h" ,cmd ,"GHC.Handle.hDuplicateTo sto System.IO.stdout" ,"let readFileNow f = readFile f &gt;&gt;= \\t-&gt;length t `seq` return t" ,var++" &lt;- readFileNow f" ,"System.Directory.removeFile f"] ; _ -&gt; return "putStrLn \"usage: :redir &lt;var&gt; &lt;cmd&gt;\"" } :def redir cmdHelp redir ":redir &lt;var&gt; &lt;cmd&gt;\t-- execute &lt;cmd&gt;, redirecting stdout to &lt;var&gt;" let redirErr varcmd = case break Data.Char.isSpace varcmd of { (var,_:cmd) -&gt; return $ unlines [":set -fno-print-bind-result" ,"tmp &lt;- System.Directory.getTemporaryDirectory" ,"(f,h) &lt;- System.IO.openTempFile tmp \"ghci\"" ,"ste &lt;- GHC.Handle.hDuplicate System.IO.stderr" ,"GHC.Handle.hDuplicateTo h System.IO.stderr" ,"System.IO.hClose h" ,"let readFileNow f = readFile f &gt;&gt;= \\t-&gt;length t `seq` return t" ,"let afterCmd _ = do { GHC.Handle.hDuplicateTo ste System.IO.stderr ; r &lt;- readFileNow f ; System.Directory.removeFile f ; return $ \""++var++" &lt;- return \"++show r }" ,":def afterCmd afterCmd", cmd, ":afterCmd", ":undef afterCmd" ] ; _ -&gt; return "putStrLn \"usage: :redirErr &lt;var&gt; &lt;cmd&gt;\"" } :def redirErr cmdHelp redirErr ":redirErr &lt;var&gt; &lt;cmd&gt;\t-- execute &lt;cmd&gt;, redirecting stderr to &lt;var&gt;" let { merge [] = [] ; merge (l:c:ls) | i c &gt; i l = merge ((l++c):ls) where {i l = length (takeWhile Data.Char.isSpace l)} ; merge (l:ls) = l:merge ls ; grep patcmd = case break Data.Char.isSpace patcmd of { (pat,_:cmd) -&gt; return $ unlines [":redir out "++cmd ,"let ls = "++if ":browse" `Data.List.isPrefixOf` cmd then "merge (lines out)" else "lines out" ,"let match pat = Data.Maybe.isJust . Text.Regex.matchRegex (Text.Regex.mkRegex pat)" ,"putStrLn $ unlines $ (\"\":) $ filter (match "++show pat++") $ ls"] ; _ -&gt; return "putStrLn \"usage: :grep &lt;pat&gt; &lt;cmd&gt;\"" } } :def grep cmdHelp grep ":grep &lt;pat&gt; &lt;cmd&gt;\t-- filter lines matching &lt;pat&gt; from the output of &lt;cmd&gt;" -- (also merges pretty-printed lines if &lt;cmd&gt; is :browse) let find id = return $ unlines [":redir out :info "++id ,"let ls = filter (Data.List.isInfixOf \"-- Defined\") $ lines out" ,"let match pat = Text.Regex.matchRegex (Text.Regex.mkRegex pat)" ,"let m = match \"-- Defined at ([^&lt;:]*):([^:]*):\" $ head ls" ,":cmd return $ case (ls,m) of { (_:_,Just [mod,line]) -&gt; (\":e +\"++line++\" \"++mod) ; _ -&gt; \"\" }"] :def find cmdHelp find ":find &lt;id&gt;\t\t-- call editor (:set editor) on definition of &lt;id&gt;" let { b browse "" = return $ unlines [":redir out :show modules" ,":cmd case lines out of { (l:_) -&gt; return ("++show browse++"++head (words l)); _ -&gt; return \"\" }"] ; b browse m = return (browse++m) } :def b cmdHelp (b ":browse ") ":b [&lt;mod&gt;]\t\t-- :browse &lt;mod&gt; (default: first loaded module)" let loadEditErr m = return $ unlines [if null m then ":redirErr err :r" else ":redirErr err :l "++m ,"let match pat = Text.Regex.matchRegex (Text.Regex.mkRegex pat)" ,"let ms = Data.Maybe.catMaybes $ map (match \"^([^:]*):([^:]*):([^:]*):\") $ lines err" ,":cmd return $ case ms of { ([mod,line,col]:_) -&gt; (\":e +\"++line++\" \"++mod) ; _ -&gt; \"\" }"] :def le cmdHelp loadEditErr ":le [&lt;mod&gt;]\t\t-- try to :load &lt;mod&gt; (default to :reload); edit first error, if any" let { cmds = [".","pwd","ls","redir","redirErr","grep","find","b","le","defs"] ; defs "list" = return $ unlines $ "putStrLn \"\"": [":"++cmd++" --help"| cmd &lt;- cmds]++ ["putStrLn \"\""] ; defs "undef" = return $ unlines [":undef "++cmd| cmd &lt;- cmds] ; defs _ = return "putStrLn \"usage: :defs {list,undef}\"" } :def defs cmdHelp defs ":defs {list,undef}\t-- list or undefine user-defined commands" 
It's interesting that it was 10 years ago. Are there also new(er) features in ghci we can take advantage of?
&gt; Also it's unwritten reddiquette to never talk about upvotes or downvotes. Really, why not? Explaining downvotes allows the poster to improve their future posts, and allows the downvoter to see fewer disagreeable posts in the future. It's win-win! Also, I don't know aboutThe unwritten reddiquette, but [the written one](https://www.reddit.com/wiki/reddiquette) clearly encourages explanations: &gt; Consider posting constructive criticism / an explanation when you downvote something, and do so carefully and tactfully.
Very cool. Granted, Reflex-DOM was *way* overkill for that, so it's hard to truly compare.
I don't particularly know why you're having trouble with this, but the simplest approach would simply be: biappend :: ([(int, int)]) -&gt; ([(int, int)]) -&gt; ([(int, int)]) biappend (a1, b1) (a2, b2) = (a1 ++ a2, b1 ++ b2) However, that's not very satisfactory, in my opinion. A better solution would be to use [bifunctors](https://hackage.haskell.org/package/bifunctors-3.2.0.1/docs/Data-Bifunctor.html), or arrows if you don't have an up to date ghc. The most general way to do this would simply be: bimap (&lt;&gt;)
I was about to make a comment about how there's no reason minified GHCJS programs should be that much bigger than GHC native programs. But then I took a look at the size of some native code generated by GHC and... yea I guess it's not GHCJS's fault =P [jl](https://github.com/chrisdone/jl) takes 2.6M on my NixOS machine.
Use &lt;&gt; from Data.Monoid. 
&gt; biappend :: ([(int, int)]) -&gt; ([(int, int)]) -&gt; ([(int, int)]) &gt; biappend (a1, b1) (a2, b2) = (a1 ++ a2, b1 ++ b2) I am getting these errors when I try to compile that: Assignment1Dominoes.hs:84:10: error: • Couldn't match expected type ‘[(int, int)]’ with actual type ‘([a0], [a1])’ • In the pattern: (a1, b1) In an equation for ‘biappend’: biappend (a1, b1) (a2, b2) = (a1 ++ a2, b1 ++ b2) • Relevant bindings include biappend :: [(int, int)] -&gt; [(int, int)] -&gt; [(int, int)] (bound at Assignment1Dominoes.hs:84:1) | 84 | biappend (a1, b1) (a2, b2) = (a1 ++ a2, b1 ++ b2) | ^^^^^^^^ Assignment1Dominoes.hs:84:19: error: • Couldn't match expected type ‘[(int, int)]’ with actual type ‘([a0], [a1])’ • In the pattern: (a2, b2) In an equation for ‘biappend’: biappend (a1, b1) (a2, b2) = (a1 ++ a2, b1 ++ b2) • Relevant bindings include b1 :: [a1] (bound at Assignment1Dominoes.hs:84:15) a1 :: [a0] (bound at Assignment1Dominoes.hs:84:11) biappend :: [(int, int)] -&gt; [(int, int)] -&gt; [(int, int)] (bound at Assignment1Dominoes.hs:84:1) | 84 | biappend (a1, b1) (a2, b2) = (a1 ++ a2, b1 ++ b2) | ^^^^^^^^ Assignment1Dominoes.hs:84:30: error: • Couldn't match expected type ‘[(int, int)]’ with actual type ‘([a0], [a1])’ • In the expression: (a1 ++ a2, b1 ++ b2) In an equation for ‘biappend’: biappend (a1, b1) (a2, b2) = (a1 ++ a2, b1 ++ b2) • Relevant bindings include b2 :: [a1] (bound at Assignment1Dominoes.hs:84:24) a2 :: [a0] (bound at Assignment1Dominoes.hs:84:20) b1 :: [a1] (bound at Assignment1Dominoes.hs:84:15) a1 :: [a0] (bound at Assignment1Dominoes.hs:84:11) biappend :: [(int, int)] -&gt; [(int, int)] -&gt; [(int, int)] (bound at Assignment1Dominoes.hs:84:1) | 84 | biappend (a1, b1) (a2, b2) = (a1 ++ a2, b1 ++ b2) | ^^^^^^^^^^^^^^^^^^^^
because it should be Int, not int. I'm assuming you don't have much experience with Haskell, or programming in general?
Yea I am very much a beginer
I mean you can just use `&lt;&gt;` directly, you don't need `bimap`.
Yeah you can thank aggressive inlining for that.
&gt; If it's zero, then we've successfully made change. Hooray! If it's not zero, then we have to consider how many ways we could make change out of that number—but, since we know that that we've already calculated that result, because it's by definition less than `given`! Shouldn't we be able to just use the fact that `0` is less than `given` to treat it like all the other numbers rather than special casing on it? &gt; This is why our implementation of `lookup` is isomorphic to an implementation of `!!` over `[]`—because they're the same thing. But what if we use a different `Functor` inside an `Attr`? Is there a way to make the cache lookup performant? The way I always read it, histomorphisms work well when you only need to look up the last (or last 2) answers, but if you need to look far back you'd prefer something else (dynamorphisms?) grow (Seed height rand) = (choose choice, Seed (height + 1) nextrand) where (choice, rand') = randomR (1 :: Int, 5) rand (_, nextrand) = split rand' Why do you need to `split rand'`? Shouldn't it be enough to pass `rand'` where you're passing `nextrand`? (Branch, _) -&gt; Fork (Stop (Stalk (Continue next))) (Stop Bloom) (Stop (Stalk (Continue (snd (grow next))))) Aww. No random determination of which field of the `Fork` is the mandated `Bloom`? Is the `snd (grow next)` meant to ensure independence of the two seeds? Doesn't it limit the height of the second branch? Wouldn't it be better to define a `split` on `Seed`s?
&gt; J hooks and forks are rather kludgy and limited Yet I find myself missing them in Haskell.
That's fine. I'm happy that you chose to start with Haskell. Here's a quick mind dump that may help you out. Keep in mind Haskell is a large language and much of this subs discussion is targeted at more advanced/interesting uses. You can go quite far without ever touching dependent or RankN types.
Oh, yea! You're right. 
&gt; Shouldn't we be able to just use the fact that 0 is less than given to treat it like all the other numbers rather than special casing on it? That's a good point. I'm putting these blog posts together in an .epub sometime in the future; I'll rework that example. &gt; The way I always read it, histomorphisms work well when you only need to look up the last (or last 2) answers, but if you need to look far back you'd prefer something else (dynamorphisms?) Dynamorphisms are indeed a more efficient way of solving dynamic programming problems such as these! I'm going to cover them in a later installment. That `snd (grow next)` was an error I thought I had excised. Good catch. Thanks very much for your comments. You've got a good eye.
Interesting! Histomorphism uses a type equivalent to the cofree comonad. Funny how often that pops up 
GHCJS has to ship the whole Haskell RTS with it, what is the smallest size a GHCJS output can have?
The cofree comonad has a form that resembles `Fix` no? I imagine that's part of why it pops up in recursion.
To my knowledge this is the only TOML syntax checker out there.
Well JavaScript is a strict language so I'm not totally stunned. It's hard to make small binaries for such a target as I understand it.
Good point. I didn't consider that since Idris is strict it may be easier to compile to a strict language.
Yeah, GHCJS has to preserve laziness. It benefits from the existing strictness analyzer, but there's still a lot of apply-unit and make-closure operations left in the code, I'm sure.
My toy programming language uses this style and the separation of concerns is fantastic, but oh god the boilerplate! My ast looks something like this: type Ast = Fix ( K Path * K Syntax.Tokens * Syntax.ErrorF / ( K Desugar.Error + K Resolve.Error + AstF ) ) An example function for syntax highlighting to html looks like this: pipeline :: Text -&gt; Lazy.ByteString pipeline src = let tokens = Lexer.lexer src syn = Parser.parseModule Env.empty tokens labeled = Labeling.label out syn desugared = Desugar.desugar dig dig dig1 labeled resolved = Resolve.resolve dig1 desugared connected = Labeling.connect path (get out) path dig resolved labeled in Abstract.asHtml ast resolved where dec = deC . sndk . sndk . out dig = Syntax.cstf . dec dig1 = rightk . dig cstf = Partial.get (sndk . dig) toks = get (unK . fstf . sndk . out) path = get (fstk . out) asts = fromMaybe [] . Partial.get (fstk . dig) astfs = mapMaybe (Partial.get (rightk . dig1)) . asts syne = fmap (get fstf) . get dec derr = mapMaybe (Partial.get (leftk . dig)) . asts rerr = mapMaybe (Partial.get (leftk . dig1)) . asts ast = get (sndk . sndk . out) The entire `where` clause is packing/unpacking/digging/converting boilerplate! Still not sure what to think of it.
you should not ask for solutions to your homework assignments then. you learn a lot about the language and how to think in a functional way by trying to solve those issues by yourself
There's a typo in the type signature of `cata`. The last two pieces got swapped.
I'm fairly confident that almost all of that 2M is due to the large RTS and base library, the large number of other libraries, and the optimizer duplicating (and then twisting) *massive* amounts of code. I'd guess laziness accounts for little more than 2x the size of Idris generated code.
Numbers default to `Integer`, which have infinite precision, while in `powerDigitSum` you requested an `Int`, which is too small to hold `2^100`: &gt; (2 :: Integer) ^ (100 :: Integer) 1267650600228229401496703205376 &gt; (2 :: Int) ^ (100 :: Int) 0 &gt; maxBound :: Int 9223372036854775807
How is backpack making a better guarantee about specialization? Using an `INLINABLE` pragma guarantees that the function will always be specialized at every call site.
Thanks; fixed.
any news on this? I'm also interested, if someone has put it on GitHub I think I can take a look and perhaps contribute, thanks!
And then there's BuckleScript. Input: let port = 3000 let hostname = "127.0.0.1" let create_server http = let server = http##createServer begin fun [@bs] req resp -&gt; resp##statusCode #= 200; resp##setHeader "Content-Type" "text/plain"; resp##_end "Hello world\n" end in server##listen port hostname begin fun [@bs] () -&gt; Js.log ("Server running at http://"^ hostname ^ ":" ^ Pervasives.string_of_int port ^ "/") end let () = create_server Http_types.http Output: ``` 'use strict'; var Pervasives = require("bs-platform/lib/js/pervasives"); var Http = require("http"); var hostname = "127.0.0.1"; function create_server(http) { var server = http.createServer(function (_, resp) { resp.statusCode = 200; resp.setHeader("Content-Type", "text/plain"); return resp.end("Hello world\n"); }); return server.listen(3000, hostname, function () { console.log("Server running at http://" + (hostname + (":" + (Pervasives.string_of_int(3000) + "/")))); return /* () */0; }); } create_server(Http); ```
As a novice haskeller, my goal is to someday understand articles like these. Sadly, today is not the day. Me: Hmm wierd terminology I don't know, let's see what the article looks like. Article: "Though we've only just begun our dive into Bananas..." Me: Is Banana a monad? 
Time for [HaskellClean](http://www.algo-prog.info/ocaml_for_pic/web/index.php?id=OCAPIC:OCamlClean)? :)
Time for [HaskellClean](http://www.algo-prog.info/ocaml_for_pic/web/index.php?id=OCAPIC:OCamlClean)? :)
Well, globals are allowed, so what makes them so much different? They cannot be dynamic, but they can be definitely used multiple times in different degrees of nesting in closures.
I wonder how much aggressive inlining harm performance by increasing instruction cache / decoding pressure.
 main :: IO () main = pure () Gives a 941 K bytes js file. Minified with google closure compiler, I got 198 K bytes. Using `zopfli` gzip compression, I can go down to 50 K. Useless information, but well ;) 
I don’t think it’s *dead* code, I think it’s mostly *duplicate* code, with many differences in how the copies end up optimized.
Awesome. Any pictures from the inside? I haven't received my copy yet :)
 $ ghci Prelude&gt; let digitSum n = if n==0 then 0 else n `mod` 10 + digitSum (n `div` 10) Prelude&gt; let powerDigitSum n = digitSum $ 2^n Prelude&gt; powerDigitSum 100 :: Integer 115 Prelude&gt; powerDigitSum 100 :: Int 0 
One advantage is that you probably get a compiler error when not everything specializes properly. At least I think you would, haven't used backpack yet. I find it kind of annoying to debug why some specialization failed.
Is 4x speedup (on 8 cores) for a sort considered good ?
How is kitten able to infer that the functional argument of `map` has exactly one input and one output?
I have worked on typed concatenative languages rather extensively and I agree with everything you said except for one major point: I do not think the stack is the right solution to pointfree programming. I come down much more on the FP/FL side of things. For example, let's assume we have the following syntactic forms (and their equivalent Haskell semantics) and that literals (e.g. `"foo")` denote constant functions that return their values: -- Apply (for when you want it) f:x == f x -- Compose (as inverse composition like in Kitten) f0 f1 ... fN == \x -&gt; fN (... (f1 (f0 x)) ...) -- Construct (lifted from FP) [f0, f1, ... fN] == \x -&gt; (f0 x, f1 x, ... fN x) -- Spread (similar to Factor) {f0, f1, ... fN} == \(x0, x1, ... xN) -&gt; (f0 x0, f1 x1, ... fN xN) -- Match (assumes constructors are unary) \[P0 -&gt; f0, P1 -&gt; f1, ... PN -&gt; fN] == \x -&gt; case x of P0 x' -&gt; f x' P1 x' -&gt; f x' ... PN x' -&gt; f x' -- Branch cond =&gt; f0, f1 == \x -&gt; if cond x then f0 x else f1 x You could then write lovely functions like this: length = \[Null -&gt; 0; @tail length [1, id] +] Or like this: map:f = \[Null -&gt; Null, Cons -&gt; [@head f, @tail map:f] Cons] Or maybe with a nicer syntax for matching: Null map:_ = Null Cons map:f = [@head f, @tail map:f] Cons What I really love about this compared to a stack-based language is that functions operate just on the values they need, not the entire state of the program. I know types _help_ with stacks, but this is just so much more clear and beautiful to me. You also need no types for "the rest of the stack" this way: You're just operating on tuples when you take more than one argument. I am completely ranting here, so I will stop! If you'd like to talk about this more though, feel free to PM me and I'd be happy to take this to another channel. \^\^
It seems to me that the thing described is in the direction of extensible records. What's the difference? What are the advantages and disadvantages?
And consequently, Futumorphisms use Free.
You're right. I've had [bad experiences](https://stackoverflow.com/questions/44737623/specializing-imported-function-in-ghc-haskell) with the specializer before, and I was never able to get a working solution. However, I just [ported my code](https://github.com/andrewthad/mergesort/pull/1) to rely on the GHC specializing it, and in the benchmarks it has identical performance.
I don't know. I was a little disappointed, but maybe that's the best I can expect. Here's a fun experiment I came up with while I was benchmarking: {-# LANGUAGE BangPatterns #-} {-# OPTIONS_GHC -O2 -Wall -threaded -fforce-recomp #-} import Criterion.Main import Control.Parallel.Strategies (withStrategy,parList,rseq,r0,parMap) import Data.List (foldl') main :: IO () main = do defaultMain [ bgroup "spinning" [ bench "serial" $ whnf spinSerial workUnits , bench "parallel" $ whnf spinParallel workUnits ] ] {-# NOINLINE workUnits #-} workUnits :: [Int] workUnits = replicate 64 30000000 spinSerial :: [Int] -&gt; Int spinSerial = evaluateThunks . map spin spinParallel :: [Int] -&gt; Int spinParallel = evaluateThunks . parMap rseq spin evaluateThunks :: [()] -&gt; Int evaluateThunks = foldl' (\i () -&gt; i + 1) 0 spin :: Int -&gt; () spin = go where go !n = if n &gt; 0 then go (n - 1) else () Then, run it with: &gt; ghc -threaded -rtsopts parallel_spin.hs &amp;&amp; ./parallel_spin -G -L 10 +RTS -N -C0.002 benchmarking spinning/serial time 2.074 s (1.953 s .. 2.105 s) 1.000 R² (0.998 R² .. 1.000 R²) mean 2.007 s (1.983 s .. 2.025 s) std dev 28.52 ms (0.0 s .. 31.84 ms) variance introduced by outliers: 19% (moderately inflated) benchmarking spinning/parallel time 302.4 ms (300.6 ms .. 304.3 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 298.4 ms (295.8 ms .. 300.1 ms) std dev 2.859 ms (1.736 ms .. 3.708 ms) variance introduced by outliers: 12% (moderately inflated) This shows best speedup I can on an eight-core machine when I am literally having the cores do nothing except increment a register repeatedly. There is about a 6.7x difference. So, if 6.7x is the best GHC can do when there are no memory accesses, no allocations, no anything, I feel a little better about the 4x I can get with my mergesort.
After posting my other comment, I remembered one of my gripes with GHC's specializer. The pragmas needed for cross-module specialization have weird effects on local specialization. In my code, I have: sort :: forall a. (Ord a, Prim a) =&gt; Proxy# a -&gt; ByteArray -&gt; ByteArray sort _ arr = ... And then, with no pragmas at all, I can just write this: -- defined in the same module as sort sortWord16 :: ByteArray -&gt; ByteArray sortWord16 arr = sort (proxy# :: Proxy# Word16) arr I can export both of these definitions, but only `sortWord16` performs well. This makes sense because `sort` is a big function (which calls other big functions), so GHC is not going to expose the unfoldings of these unless we tell it to. We can add an `INLINABLE` pragma to `sort` and to all of the other auxiliary functions it calls. Now, `sort` performs well when used in other modules, but oddly, `sortWord16` has awful performance.
&gt; So, if 6.7x is the best GHC can do when there are no memory accesses, no allocations, no anything, I feel a little better about the 4x I can get with my mergesort. You said you have 8 cores, but do you have 8 real cores, or 4 real with hyper threading ? 6.7x on 4*2 hyperthreading is great, but 6.7 on 8 real cores sucks (for this algorithm). On my computer, 6 "real cores", 12 hyper threaded, I have a 7x factor. Which is "good" on a 6 "real" core machine. 1.310s versus 188ms However, the CPU usage climb to 1200% during both the serial AND the parallel code execution, so I'm a bit convinced that the parallel garbage collector is doing too much things during the serial execution. Actually, the timing are 1.320s for serial with -N but 1.010 for serial with -N1. So the correct comparison is 1.010 versus 188 wich is a speedup of 5.3 which is "correct" for a 6 real core machine (I'm sure the hyper threading cannot do anything nice here).
I recommend starting from the [first entry](http://blog.sumtypeofway.com/an-introduction-to-recursion-schemes/)! It goes very slow (probably too slow for some people's taste), since I wanted to make this material accessible to novice Haskellers.
It'd be really interesting to see if equality saturation (a newer way to optimize programs that I haven't seen implemented anywhere yet) would have a large effect. From what I understand, its biggest benefit is that programs implementing optimizations don't really have to worry so much about when those optimizations take place anymore and so it should greatly reduce the variety of duplicate-but-slightly-differently-optimized code. Plus it also has some interesting applications for machine learning heuristics in compiling/optimizing code and that's always neat.
Interesting. It's not instantly clear how to express something like `(*) / (+)` though. &gt; You're just operating on tuples when you take more than one argument Which are basically encapsulated stacks. 
I'm not sure the change problem is a good example case for histomorphisms. If I recall, that's a one-liner with clever use of lists.
Cool. Any ways that `htoml` was doing a good or bad job for you?
That's a good point about hyperthreading. When I get to my computer, I need to check to see if I have eight real cores or not. Something I am confused about is why parallel gc negatively affects the non-concurrent spinning benchmark. I understand why parallel gc can her performance generally, but, this code shouldn't be doing any allocations. And if there are no allocations, the garbage collector should not run. My understanding is that the default behavior of criterion is to run garbage collection before each iteration of the code that is being benchmarked. So, I guess there are two things that don't make sense: 1. How can the garbage collector decide to run when there haven't been any allocations since the last time it ran? 2. How is garbage collection taking so long to do basically no work (since there isn't any garbage to collect)?
I replied to this, but I accidentally did it at the top level.
Looking for some feedback on my NES emulator. I'm new to Haskell. Specifically I am unhappy with the Nes.hs and PPU.hs and CPU.hs. I don't like how everything is in the IO effect. I am looking for strategies to break up Nes.hs and move things into there own modules (PPU and CPU) and constrain effects better (perhaps constrain functions that only write to the CPU, PPU or to both etc) https://github.com/dbousamra/hnes 
Here's my comment from the top level, plus the info I discovered about my laptop. That's a good point about hyperthreading. When I get to my computer, I need to check to see if I have eight real cores or not. Something I am confused about is why parallel gc negatively affects the non-concurrent spinning benchmark. I understand why parallel gc can her performance generally, but, this code shouldn't be doing any allocations. And if there are no allocations, the garbage collector should not run. My understanding is that the default behavior of criterion is to run garbage collection before each iteration of the code that is being benchmarked. So, I guess there are two things that don't make sense: 1. How can the garbage collector decide to run when there haven't been any allocations since the last time it ran? 2. How is garbage collection taking so long to do basically no work (since there isn't any garbage to collect)? Also, I found out that my processor is an Intel Xeon E3-1505M v5, which in fact only has four cores but uses hyperthreading to expose eight virtual processors.
It was mostly just not as fast as `htoml-megaparsec`. I also removed an `aeson` dependency, which is good for `tomlcheck` specifically since it's the sort of tool that may be used by non-Haskellers and they might not appreciate the bump in compile time. I kept everything else since it had a phenomenal test + benchmark suite.
I agree that the “tacit” applicative style of FP is nice, I guess it’s just not the language I want to build and use. I like the synaesthetic association of knowing where my values are, “sitting” on the stack rather than “floating” between functions. A curious alternative that I’ve experimented with is type-directed lookup of values—often there’s only one value of a particular type in scope, so you can get fairly tacit code by plugging holes in expressions with the “nearest” value of the expected type, e.g.: map : (a -&gt; b) -&gt; List a -&gt; List b map = if null ? then Nil else Cons (? (head ?)) (map ? (tail ?)) zip : List a -&gt; List b -&gt; List (a, b) zip = if null (the List a) || null (the List b) then Nil else Cons (head ?, head ?) (zip (tail ?) (tail ?)) Each of the `?` placeholders above can be unambiguously filled in by the compiler because there’s only one value in scope that’s type-correct to put there; the arguments to the `null` calls in `zip` need to be specified explicitly because there are multiple possible arguments in the same scope, but they can be referenced by their type with `the`. These could also be implemented with pattern-matching: map : (a -&gt; b) -&gt; List a -&gt; List b map = case ? of Nil -&gt; Nil Cons -&gt; Cons (? ?) (map ?) zip : List a -&gt; List b -&gt; List (a, b) zip = case (?, ?) of (Nil, _) -&gt; Nil (_, Nil) -&gt; Nil (Cons, Cons) -&gt; Cons (?, ?) (zip ? ?) 
also Gabriel Gonzalez's The functor design pattern: http://www.haskellforall.com/2012/09/the-functor-design-pattern.html is also a well-known post.
You may also be interested in Bartosz's category theory videos and accompanying website, since he starts at the very beginning of all of this. https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/ https://www.youtube.com/watch?v=I8LbkfSSR58&amp;list=PLbgaMIhjbmEnaH_LTkxLI7FMa2HsnawM_ This is how I started learning CT stuff, he does a very good job of teaching.
You could use the `bool` function from prelude. Hence you could rewrite that as f n = (3 n + 1) (div n 2) (mod n 2 == 0) According to [pointfree](https://hackage.haskell.org/package/pointfree) this is equivalent to: f = ap (ap (bool . (1 +) . 3) (flip div 2)) ((0 ==) . flip mod 2)
All recursive data structures have a form that resembles `Fix`. The free monad, for instance: `type Free f a = Fix (Compose (Either a) f)`
&gt; It's not instantly clear how to express something like dup,dup (*) / (+) though. [fst, {id, dup *} /] + Here's the reduction: (X, Y) [fst, {id, dup *} /] + [(X, Y) fst, (X, Y) {id, dup *} /] + [X, (X, Y) {id, dup *} /] + [X, (X id, Y dup *) /] + [X, (X, Y dup *) /] + [X, (X, (Y, Y) *) /] + 
Because `(.) == (&lt;$&gt;)` in this context, and due to association, that last one is the same as f = bool &lt;$&gt; (1 +) . (3 *) &lt;*&gt; flip div 2 &lt;*&gt; (0 ==) . flip mod 2 Using `&lt;$&gt;` there triggers the part of my brain responsible for reading applicative syntax and makes the whole line make more sense to me. YMMV, of course.
You could use [`ViewPatterns`](https://ghc.haskell.org/trac/ghc/wiki/ViewPatterns).
If there is one it almost certainly is less than 0.1% as popular as Haskell. Haskell is one of the least crufty languages out there, and by far the least of any language that can even come close to being considered mainstream.
PureScript is the best candidate available.
It's worth adding that while you *could* write that, you really shouldn't. :)
What are you using if anything to replace intero? I've come to like all the goodies, so at the moment I'm maintaining stack and nix in my projects and emacs still gets to use stack. It's a bit of a hack though.
[PureScript](http://www.purescript.org/)?
&gt; htoml-megaparsec I cannot find it on Github. I'd gladly take a move to megaparsec as a PR and add you as a contributer. For sake of "one htoml to rule 'm all" :)
I have no answer to your questions, sadly ;( This benchmark is more also frustrating, because I had hope that GHC fold the `spin` loop to a constant, like GCC / clang are doing for a similar C construct. (GHC: https://godbolt.org/g/f3ZGmy GCC: https://godbolt.org/g/Wbw2qK ) &gt; Also, I found out that my processor is an Intel Xeon E3-1505M v5, which in fact only has four cores but uses hyperthreading to expose eight virtual processors. If you have four real cores, a 4x speedup for an algorithm which is not supposed to scale linearly is nice, I think (with no real argument) that hyperthreading is not supposed to help a lot because the data set fits in the cache (If I understood your benchmark, you use 1 million `Word` (which fits exactly in your 8 MB cache)).
I am midway through Bartosz' videos right now and they're all amazing. They go slow enough to make me feel smart rather than confused.
It's frontend-only though. Haskell targets backend. (Also, I think people tend to underestimate the difference laziness makes).
If a language were to get to this stage in terms of ecosystem/tooling I'm pretty sure we would've heard of it by now.
There are some typos (e.g., `CVCoalgebra` vs `CVCoalg`), and the program for `sow` is simply wrong: `Continue` cannot take `seed`, but `Continue (sow seed)`, and `split` (from `System.Random`) doesn't take `seed` as argument either. A URL to the full program would be helpful. 
Not sure what you mean by front and back but both languages are general purpose programming languages. If you're referring to one generating js and the other generating native code, well there are options to switch up the defaults. There is a fork of purescript that generates C++ and a version of GHC that generates js.
&gt;The many String types, and their lazy/strict variations. Eh whether this is a problem depends on your domain. The lazy/string variants exist for a reason and in fact judicious use of them can speed up your code. &gt;The many preludes and how Haskell isn't ready to go right out of the box. I've written several [libraries](https://hackage.haskell.org/user/vmchale) without a custom prelude. &gt;The many syntax options making others' code difficult to understand. Haven't run into this myself. There are certainly obscure extensions that could do that but it's not really that bad. &gt; Hackage What's your qualm with Hackage? Hackage + cabal is better than anything Idris, Elm, or PureScript have. &gt;I know about Idris, Elm, Swift, and Elixir. Is there some other language that's very similar to Haskell, but is a better tended garden? Nope. Swift isn't a functional language. Elixir has a completely different model. PureScript is frontend only though it does fix a two of the problems you allude to. &gt;Compile errors due to formatting but point somewhere else Parse errors you mean? Elm seems to be pretty much the same. Idris has worse error reporting but it's doing more advanced things so no complaints there. Anyway, Haskell 2020 is three years off, but it does have the chance to solve some problems.
&gt; Compile errors due to formatting but point somewhere else you could use bracketed haskell version
&gt; Hackage + cabal is better than anything Idris, Elm, or PureScript have. I don't know about PureScript, but I think Elm has something awesome : it enforces a versioning policy by monitoring changes in a codebase, and forces you to "tag" your release in your VCS.
The C++ backend seems to be in progress. The ergonomics do not yet equal GHC + cabal and I doubt the speed is yet comparable.
Have you try Rust?
&gt; The many String types, and their lazy/strict variations There are only three string types. String, and the lazy/strict versions of Text. ByteString is for storing bytes. If you use it for strings then you need to care about encodings so it's really a different beast. &gt; The many preludes and how Haskell isn't ready to go right out of the box. Just use the normal prelude. Every programming language I use with any regularity requires libraries that are not part of the "standard library". In this regard, Haskell is no different. &gt; The many syntax options making others' code difficult to understand. Do you mean language extensions? Yes there are many and it takes a long time to learn them all and how they interact. Worse yet, is that there isn't a great place to learn them all. I tend to recommend looking through ICFP and Haskell symposium proceedings, which is not great general advice. &gt; Hackage Hackage and cabal work quite well for me. I'm not sure what you're specifically struggling with. &gt; Compile errors due to formatting but point somewhere else Make sure you're not using tabs and learn the [indentation rules. (see for example 2.7)](https://www.haskell.org/onlinereport/haskell2010/haskellch2.html) Maybe you'd be more at home with F#?
I agree that something has to demand the value somewhere. But this could also be the runtime system. You can call that a primitive if you like. It’s perfectly possible to implement Haskell without any case expressions. Or primitives. But you do need a runtime system. 
It does, but I am still skeptical of the package management model. I don't actually know what it does to handle the diamond dependency problem. 
I like Rust and use it regularly, but I don't think it's very similar to Haskell.
It's not a functional language and it lacks several Haskell features, particularly monadic IO but also code generation from generics. It's also a strict language, which is different.
&gt; bool &lt;$&gt; (1 +) . (3 *) &lt;*&gt; flip div 2 &lt;*&gt; (0 ==) . flip mod 2 bool &lt;$&gt; (+1) . (*3) &lt;*&gt; (`div` 2) &lt;*&gt; (== 0) . (`mod` 2) In my humble opinion, this is slightly more readable :)
Of course is not the same, however I find it closer to Haskell than say Elixir. The type system is amazing and in complete honesty OP didn't say what he likes about Haskell. It was just a suggestion :)
Can you elaborate how a Haskell without case or primitives would work? By primitives I mean the functions that do for example 64 bit integer addition, they obviously won't case on 1, 2, 3, 4...
&gt; It kind of sucks that you can't just tell GHC to mark all functions in a module as INLINABLE. `-funfolding-creation-threshold=really_big_number` ?
Fair enough. It has sum types, which is quite nice :)
[Eta](http://eta-lang.org/) tries to create a different culture, less centred in experimentation and more into getting things done.
Just implement the lambda calculus. All non-primitive data types can be encoded, eg, by Scott encoding. For numbers I’d use an inductive representation of binary numbers. For Char, and 256 element enumeration (or 128 if you only want ASCII). The biggest design choice is what to do for the IO monad. It will require runtime system support. 
I have no clue. While this happened to me a lot in Haskell, it never was an issue with Elm. Granted, I use Elm a lot less than Haskell, and include a lot less libraries, but I suppose it is also related to the fact that the enforced PVP reduces problems.
Its package manager pulp is a commandline tool written in purescript via node. So it's not front-end only. 
Just ghc-mod. The autocomplete is really nice to have.
SML?
But how would you do anything (or even reduce at all) with those values without a case or primitive. Like say print them to the screen or take them in as input. 
Stack and LTS stackage might address point 4
:)
&gt; It's frontend-only though. No, you can run it against node.js just fine; you will just find that some libraries that use browser-specific APIs won't work, but the same goes the other way. And while no production-ready implementations exist yet, the language itself doesn't rely a lot on compiling to JavaScript - there is no fundamental reason why a PureScript compiler couldn't target, for example, LLVM, the GHC RTS, or the JVM. You would, however, lose all the libraries that rely on JavaScript FFI.
Have you used it? How far is it yet? 
And typeset all your presentations in Comic Sans. 
Clean is very similar to Haskell and significantly less popular, so it does not have as many things in pretty much any dimension. (It does have some form of linear types though)
A bit off topic but how did you build an intuition for reading applicative syntax? For this example, how do you read it?
Could pattern synonyms help with the boilerplate?
Really enjoying this series, thank you. 
You can see the `ap`/`&lt;*&gt;` operator as something which "adds one argument" to a function. It really is a form of function composition as /u/agrif2 pointed out. You can see this by recovering `fmap` from `&lt;*&gt;`: &gt; :t \f x -&gt; pure f &lt;*&gt; x \f x -&gt; pure f &lt;*&gt; x :: Applicative f =&gt; (a -&gt; b) -&gt; f a -&gt; f b and by appending applications of `ap` you get n-argument fmap: &gt; :t \f x y -&gt; pure f &lt;*&gt; x &lt;*&gt; y \f x y -&gt; pure f &lt;*&gt; x &lt;*&gt; y :: Applicative f =&gt; (a1 -&gt; a -&gt; b) -&gt; f a1 -&gt; f a -&gt; f b 
Include a support and donations page in the pdf. And let BM nominate a paypal, btc, eth address.
&gt; There are only three string types. Two types too many. Though the C programmer in me says, three types too many.
If you don't like these things you're going to hate other languages.