I like the irony of the implementation overflowing its `code` element in a decidedly non-pretty way. :)
Another thing that bothers the hell out of me is the convention of using synonyms for names of different functions. I mean "run", "eval" and "exec". To this day I have to always consult the docs before choosing between "evalStateT" and "execStateT", because the names are anything but descriptive.
I sometimes apply the following trick: ($ initialState) $ runStateT $ do ....
&gt; You could have the first name be an indent-point, much like the first name in "let", etc. The layout rules are specifically ways to elide '{', ';', and '}', and those characters are "put back in" between lexing and parsing. You can't use them to elide ',', and you certainly wouldn't do it without eliding the '{' and '}' as well, which is going to make the formatting of your deriving clauses a little awkward. 
Your code is unnecessarily terse. The `Maybe` monad is nicely understood intuitively when chaining computations that either return a value or fail, and for this purpose, chaining them with bind(or using the `do` notation) makes everything really clear. It is not at all obvious from your article or your code what exactly your code is doing, and your definition from `inp'` looks like something lambdabot returns when you ask it to `pl` a turd. 
Well, imagine commas were replaced by semicolons? I think that should work? Deriving clauses are easy since "deriving" is a keyword?
Where did that ∀ x ∈ D come from?
I think the problem in that SO question was in fact a bit different. I don't know how to solve your problem; I could describe, if you want, how I install haskell related stuff on my system (also, ghc-mod). However, the best thing you can do is going to #nixos irc channel, and ask for help there. Then, as /u/bflyblue says, the current version has a bug, and you should wait a few days for the correction to land in nixos-unstable.
`inp'` is ugly, as others have noted. Have an (attempted) improvement: &gt; :t foldr ((liftA2 . liftA2) (:)) (const $ return []) (Monad m, Applicative m) =&gt; [b -&gt; m a] -&gt; b -&gt; m [a] That looks familiar! &gt; :t \lst b -&gt; mapM ($b) lst Monad m =&gt; [a -&gt; m b] -&gt; a -&gt; m [b] And because mapM f . fmap g === mapM (f . g), we get: inp' str y = mapM (\c -&gt; check c y) str 
I meant changing the rules like in a usual Nomic game :-P
Ah, I see. Looking at annotations, it seems these are meant for use in the GHC API, so they are not accessible from code (not even TH) it seems. I guess you could try to use [type level strings (Symbol)](https://www.haskell.org/ghc/docs/7.8.2/html/users_guide/type-level-literals.html) to annotate things this way. Another option is to write your own quasi-quoter. Packages like the data base library [persistent](http://www.yesodweb.com/book/persistent) do this to add more information than Haskell data type declarations allow.
I'm sure it's *possible*, but it would require some significant changes to either the record declaration syntax, the layout rules, or both. For example, currently layout requires elision of '{' and '}', but you can't elide these in the record declaration syntax since they are the lexemes that *indicate* record syntax (it is not mandatory; `data Foo = Foo Int String` is a fine data declaration). In all the other places layout is used the lexeme before the '{' is the indicator that either '{', ';', and '}' are used or layout is used. While `deriving` is, [indeed](https://www.youtube.com/watch?v=xYnzZ2rczSo), a keyword, it still has to be part of the same declaration of the `data` keyword and not any inner layout. Right now, data declarations never have any nested layouts, so the positioning of `deriving` is pretty flexible; using layouts for record declarations would make things a bit more restrictive and, IMO, awkward. `deriving` would have to start on a column more indented than `data` but less than the first field identifier.
I think you are mistaking the left hand side of the function definition for application when it is actually just binding the arguments to variables. ``` flip f x y = . . . ``` is not actually applying f to x and y. That happens on the right side: ``` . . . = f y x ``` where `f ` is applied to `y ` and `x`. ``` flip f x y = . . . ``` is equivalent to this in python: ``` def flip(f,x,y): . . . ``` The first argument provided to `flip` is bound to `f` inside the body of `flip` and must have type `a -&gt; b -&gt; c`. The second argument provided to `flip` is bound to `x` inside of `flip` and must have type `b`. The the third argument provided to `flip` is bound to `y` inside of `flip` and must have type `a`.
Unfortunately not, we ran into some issues with getting it recorded (namely, Adam had to show up a few minutes late).
In this case, it's sarcasm. Ask yourself what that line of code does and how quickly you can determine that. The overwrought solution seems more like an exercise in obfuscation. 
I think at that point of sophistication, one might as well go ahead and use Knuth-Plass linebreaking with excessively tuned costs for glue: There's no place for half measures in overkill.
There is the half-dead http://vmkit.llvm.org/ . Edit, this is the inverse of what you are looking for, sorry.
They stop trying to engage and then go on rants about how Haskellers are in some conspiracy to ignore the latest type theory research. My point was there's a few vocal PLT people who are very willing to criticize compiler projects that made compromises to get to point of being production ready ( often triaging certain features ) but are themselves unwilling or not skilled enough to actually hack on compilers themselves. 
Evaluation gives you a value; execution is just for the effects. I find it easy to remember.
I'm using `ghc-mod` version `5.2.1.0` and things are working. However, now I'm seeing some weirdness with `syntastic`. It's not registering `ghc_mod` as an available syntax checker. And when I force it to include `ghc_mod`, I get weird errors (`syntastic: error: checker haskell/ghc_mod returned abnormal status 127`).
We can glue a result onto a list of results using (:) (liftA2 . liftA2) (:) That teaches cons to gather results two contexts deep. The outer context is that of a reader, the inner context is the context of failure in my case. foldr (liftA2 . liftA2 $ (:)) The foldr teaches the function to accumulate over a list of results. Over here accumulation from the point of view of a reader simply threads a read only environment. Accumulation in terms of the failure context allows total success or failure. The above is a common pattern - developing code that works on a much simpler domain and reflecting it into a more complex one. 
Ah, I see, point taken. To me, OP is a very earnest haskell novice--one novice to another. It's very probable that OP is not being sarcastic.
&gt; There's no place for half measures in overkill. Mantra of the week. Thank you!
You're using low-level combinators to rebuild higher-level concepts... The result isn't very readable in my humble opinion, inp' in tchakkazulu version is much improved, in complexity of the code as well as readability. Even in check, your solution is too complicated and without knowing the precedence of &lt;*&gt; and &gt;&gt;= it is ambiguous. Applicative is unnecessary here : check :: Char -&gt; (Int, Int) -&gt; Maybe Char check c offset = lookup c keyToLoc &gt;&gt;= flip lookup locToKey . (+ offset) Or you could just bite the bullet and use the do-notation though that is a matter of taste : check c offset = do start &lt;- lookup c keyToLoc lookup (start+offset) locToKey I find this version much clearer myself but I may be biased.
Looks neat.
Please always do the latter. It's a mental energy drain to convert these 1-liners from pointfree form, and honestly, I just can't do it properly after 6pm anymore. Also, the flip wasn't all that obvious because of the noise from &gt;&gt;=, so I wasted 20 seconds wondering why it looked like a type error. If anybody pushed code like the former into our SVN at work, I would open a ticket for them to stop being a smartass and realise that code is there for humans to understand at a _glance_ and manipulate quickly.
I'm going to be honest, Haskell portability beyond OSX (for devs), Windows (for marketing) and Linux/BSD (for production) is probably not useful, because anything smaller and more specialised you're probably going to want to write finely tuned C program for. At the moment we already have those four systems. If you want to run Haskell on a teensy, be better off checking out ajhc than putting the effort into giving GHC yet another backend to maintain, which buys you not much. On the foreign library argument, I'd be surprised if it took less time to just rewrite said library with proper types in Haskell than to leverage and hack X library to not feel like an IO-bound abomination.
How/Why does Clojure benefit ? When picking a library on the JVM, it certainly pays to look around. There are indeed awful libraries out there, but the sheer volume of them means there are also some good and even great ones. Take care to separate imperative from badly designed, they are not one and the same. 
Parens BasicEnv (Map.Map String Val)
Hm, actually I just tried with ghc-mod 5.2.1.1 and with that I get : .ghc-mod-wrapped: &lt;command line&gt;: cannot satisfy -package-id yaml-0.8.9.3-4bbf33d471e4ee345a518d72bf25c49e (use -v for more information) EDIT: never mind, just needed a `cabal configure`.
Sure, let me know. The trip is only a few hours so I'd be willing to make a visit for a talk or somesuch (H-town is one of my favorite places, and I have plenty of friends there still, so it's rather easy to convince me). Also, yes, the Austin meetup has had some fantastic turnout; we normally get about 20 people or more in there. Unfortunately I've been quite busy lately and couldn't make it this past month - but Sukant (one of the co-organizers) has done a good job in my absence. And there is basically guaranteed beer afterwords. If you can swing by sometime, I'd recommend it!
It's insane, because I understand everything you said and it makes perfect sense, but my misunderstanding remains. I must be making a vast and crucial error somewhere, so from now on i'll number everything I say and you can pinpoint what's wrong. 1. The first argument that f takes is x. 2. If so, then x :: a, since the type of f is (a-&gt; b -&gt; c), and from that it's clear that the first argument is of type a. But that's apparently wrong. So I must either be wrong about 1 or 2. 
I understand that bit (but thanks for your great explanation!), but now, taking what you've just said as fixed, look back at the type of f: (a -&gt; b -&gt; c). So if y::a then y must be the first argument of f. But y is the second argument of f, since we say "flip f x y = ...".
It's (1). x isn't the first argument of f, it's the first argument of flip f.
As a user of Enaml in the python world and having dabbled in Kivy (also python) this really seems like the most interesting of the options currently available. I wasn't at all impressed with any of things that have actually been built with it though. It's more like having a canvas to draw rectangles and things on and less like having native widgets. Even the official page looks like ass. http://qt-project.org/doc/qt-4.8/gettingstartedqml.html
The Chat.hs example on that wiki page is a dead link now.
We write `flip f x y = ...` as the left hand side of the equation because we are *defining* what `(flip f) x y` is. Only on the right hand side of the equals sign are we actually applying anything. You could read the equation like this: "for a function `f :: a -&gt; b -&gt; c` the function `(flip f)` applied first to `x :: b` and then to `y :: a` is equal to `f` applied first to `y` and then to `x`". Here's a concrete example: suppose I define the function secondArgGreater :: (Ord a, Ord b) =&gt; a -&gt; b -&gt; Bool secondArgGreater u v = v &gt; u and now I want a function `firstArgGreater` which, applied to two arguments, returns `True` if and only if its *first* argument is greater than its second argument. I could write firstArgGreater :: (Ord a, Ord b) =&gt; a -&gt; b -&gt; Bool firstArgGreater u v = u &gt; v Or, equivalently, I could write firstArgGreater = flip secondArgGreater
A number of the responses so far have provided good explanations of the solution in general form, but, when I have this kind of question, I find it easier to understand in terms of a specific examples (that I can generalize afterwards). So let's assume we have the mystifying definition of `flip`: flip :: (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c flip f x y = f y x and that this definition works (we'll verify this), though we still need to investigate why it works. Assume you have this simple (silly) function: intThenChar :: Int -&gt; Char -&gt; String intThenChar i c = show i ++ (c : []) As examples, this function has these sample inputs and outputs: &gt; intThenChar 6 'r' "6r" &gt; intThenChar 3 'd' "3d" If we ask GHCI what the type of this function is, we see that it's what we expect (we declared it, after all): &gt; :t intThenChar intThenChar :: Int -&gt; Char -&gt; String (You might see `[Char]` instead of `String`, but they're the same) So, what happens if we ask GHCI about the type of `flip intThenChar`? &gt; :t flip intThenChar flip intThenChar :: Char -&gt; Int -&gt; String Huh, it looks `flip` changed the order of our parameters. We can try the flipped version on a few examples to verify this: &gt; flip intThenChar 'f' 4 4f &gt; flip intThenChar 'g' 1 1g Which just shows that `flip` changed the order that we gave the parameters to `intThenChar`, but it didn't change the definition of `intThenChar` (the `Int` still appears before the `Char` in the resulting `String`). So it seems that `flip` does what it sets out to do (we've verified that it works correctly), and hopefully we're starting to see why. At this point, we might try implementing our own version of `flip` that's specialized to `intThenChar`-like things: myFlip :: (Int -&gt; Char -&gt; String) -&gt; Char -&gt; Int -&gt; String myFlip anIntThenChar aChar anInt = anIntThenChar anInt aChar And we note, of course, that the names above have the following types: anIntThenChar :: Int -&gt; Char -&gt; String aChar :: Char anInt :: Int Now from here, we can try to generalize our definition of `myFlip`. I think the `String` is the silliest part of the type of `myFlip`. After all, there's no reason to restrict the output of the function, as long as it takes an `Int` and a `Char`. So if we replace it with a lowercase letter (how about 'c'?), we get: myFlip :: (Int -&gt; Char -&gt; c) -&gt; Char -&gt; Int -&gt; c myFlip anIntThenChar aChar anInt = anIntThenChar anInt aChar Not much difference! Since that worked (feel free to verify by calling `myFlip intThenChar 's' 2` or something), let's try replacing more of the definition. In particular, let's try replacing `Int` with a lowercase 'a', and let's rename `anInt` to `y`. There's nothing special about these names, as long as we're consistent (i.e. all the `anInt`s become `y`s). myFlip :: (a -&gt; Char -&gt; c) -&gt; Char -&gt; a -&gt; c myFlip anIntThenChar aChar y = anIntThenChar y aChar Now let's replace `Char` with a lowercase 'b' and `aChar` with `x`: myFlip :: (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c myFlip anIntThenChar x y = anIntThenChar y x Finally, since `anIntThenChar` is just a function, let's name it `f`: myFlip :: (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c myFlip f x y = f y x And here we note that `myFlip` is now completely equal to the original definition of `flip`. Edit: formatting
Can you expand on your problems with those materials? RWH is known to be quite out-of-date these days, so I'm not surprised you ran across that issue. The Typeclassopedia is more an intermediate resource rather than a tutorial. I always had good luck with LYAH for the basics. Unfortunately there is a lack of extensive, up-to-date intermediate-level resources now that RWH is getting older.
What topics would you like the tutorial to discuss? Also, what kind of length did you have in mind?
&gt; I am experiencing with learning haskell suggests there is not anyone who actually knows the language, as a result. Part of learning Haskell is recognizing that it discusses programming in a way that is fundamentally different than you are used to, people often mistaken it for a form of obscurantism when the reality is that's it's just a different way of thinking based on different foundations of ideas. Frankly, Haskell will be uncomfortable to learn at first and part of learning is pushing through that uncomfortableness and training your brain like you would a muscle. It may not mean much, but trust me when I say it does get easier if you stick with it. Real World Haskell and CIS 194 are in fact written by very competent people, and you will learn a lot if see them to the end. Wish you luck and always feel free to ask questions on StackOverflow if asynchronous nature of the medium that suits you more!
Yeah, no. I'm not talking "intermediate-level resources". I'm talking "basic".
&gt; The Typeclassopedia is more an intermediate resource rather than a tutorial. I agree. For me, the typeclassopedia was quite a useful thing to read, but only after quite a bit of actually hacking about with solving problems in Haskell. It solidified lots of concepts I'd half picked up but its not something you can sit down and read from a cold start. 
I'm looking for something which goes from point A to point B to point C ... The length is not so much an issue so long as it actually helps me make progress and does not use "hand waving" with a subtext of "oh, you don't need to know this" or "I know I'm explaining this poorly".
Thinking differently is not the issue. Explanations which are either incomplete or seem incorrect is. I have no doubt the people behind RWH and CIS are competent; they just seem to stink at explaining the material.
I have read it several times. I'd rather set my hair on fire, thanks.
Sorry for the late reply - I don't recall seeing your edit before. Objectivity is certainly hard to achieve. If it was easy, everyone'd be doing it. Social psychologists are pretty clear that us humans are all subjective, prone to fallacies and hypocritical. I don't claim you're wrong. I just want people (myself included) to make the effort to be more objective - in large part because I believe that's more effective advocacy in the long run. Though of course that's a subjective belief, no doubt based on fallacies etc etc. 
Any particular place where you want to start? You mentioned that you've gone through several tutorials, was there a particular point where they became completely unintelligible?
No particular point where the tutorials "became completely unintelligible". For example, (I know English is not his first language but ...) in LYAH, the sentence structure and atypical grammar oftentimes leaves me wondering just how accurate a picture of the language I have to the point I don't even want to read it anymore.
If I wrote sum :: Int -&gt; Int -&gt; Int sum a b = a + b Would you say that b is the first parameter to a? Of course you wouldn't, you would say that b is the second parameter to `sum`. Now when I write flip f x y = ... x is not the first parameter to f here, it's the second parameter to flip (or the first parameter to (flip f)), same for y of course, it's the third parameter to flip (or the first parameter to ((flip f) x))
RWH: The material is presented, in my opinion, in so bland a way I don't even want to bother with the language. I couldn't even get through chapter 3 before I wanted to run from the computer, screaming. CIS: In attempting to complete the assignments to make sure I am picking up the material, I couldn't get passed the second homework. (I think I remember reading someone else having the same problem a couple weeks ago, too, by the way. Don't recall if I read that here or not.) I eventually found out, "Oh, you need to use '&lt;blank&gt;' and '&lt;blank&gt;' from the prelude." But how the hell am I supposed to know that? It's not mentioned in the course. Instead, the course, in addition to giving incomplete information says, "You should probably read RWH (no, not going to happen any more) and LYAH (doubly no)." If I knew what was in the prelude or in "Data" or whatever other modules there were, I would probably already know enough about haskell to not need the tutorials, no?
C, the projects with which I am most familiar I am prohibited from mentioning publicly by contract. Sorry.
1 is wrong. Note that the arguments of `flip`, as you can see in the left hand side of the definition, are: 1st argument of flip: f 2nd argument of flip: x 3rd argument of flip: y Now, when `f` is called, which happens after the equals sign (in the right hand side), the first argument being passed to `f` is `y` and the *second* argument passed to `f` is `x` (not the first). We can see this in the call `f y x`. So, in this call: 1st argument to f: y 2nd argument to f: x Looking at that function call, if `x` is `a` it wouldn't type check because the second argument of `f` should be of type `b`. Here is how the type of `f` matches up with our call to `f`: f :: a -&gt; b -&gt; c -- The type of f f y x -- The call to f
&gt; Typeclassopeadia (I really don't care how it's spelled), and then &gt; is written by someone who either does not know how to communicate in English or does not care to communicate in any intelligible way. P. S. I wish you good luck in learning Haskell. Well, and learning to learn. P. P. S. Sorry if I offended you.
You might want to have a look at: - Programming in Haskell (http://www.cs.nott.ac.uk/~gmh/book.html) or - Thinking Functionally with Haskell
OK, I'm happy with that. My problem is just with labeling f as type (a -&gt; b -&gt; c) when the f that we input into flip *doesn't* take the arguments in that order. What does a -&gt; b -&gt; c mean then? I thought the function definition said this in words: flip :: (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c flip has the following type: it takes a function (which takes an argument a, and then an argument b, and outputs c), an argument b, and an argument a, and then outputs c. If that's the case, then the parentheses make no sense to me. You're telling me that f doesn't take any arguments, but it's telling me the opposite: that f has the type (a -&gt; b -&gt; c). (This must be the most tedious attempt to explain something in the history of reddit - I can't thank you enough for persevering)
No, think of it like this flip(f, x, y) = f(y, x) On the left side of the "equation" is a description of the arguments of flip. Nothing special is done in this case, they just get a name. On the right side f is actually used, and gets parameters passed in.
Fixed, thanks!
Is your problem with the syntax and the lang. or with functional programming and the concepts behind it? (This is why it would be interesting to know where you opted out). Till now I only saw you complaining about the style or the grammar of certain books - but is this really *the* problem? Maybe functional programming or Haskell is just not for you.
I internalized (liftA2 . liftA2) as an idiom long ago. By the way, `const = pure`, and `return = pure`. So: foldr ((liftA2 . liftA2) (:)) ((pure . pure) []) is basically a foldr with (:) and [] thrown 2 levels into an `Applicative`. I'm not sarcastic when I say I think that's very nice and not obfuscated. It simply relies on the semantic-editor-combinator idea. However, in this case, the `Applicative` is simply diving into the function results so it may be less nice than simple point-ful code.
Everybody here has been very polite, which is good. I'm sorry that I'll be the first slightly unpolite one, but I'll have to comment: It seems that you keep coming back to the fact that you find RWH boring and LYAH written in a style of English that you don't like. These are perfectly OK opinions to hold, but they are not sufficient to label those sources as "incompetent".
What would you like A, B and C to be? Or, let's take it in even smaller steps: What's A for you? If you can just pose a *single concrete* question here, I'm sure you'll get an excellent reply that will nudge you in the right direction.
People on `#haskell` will actually be very happy to help you. You'll find people there at most times of day.
&gt; I eventually found out, "Oh, you need to use '&lt;blank&gt;' and '&lt;blank&gt;' from the prelude." But how the hell am I supposed to know that? You do not *have to*. It is easier to pull resources from libraries to have less work to do but you can always reimplement the functionalities yourself. In fact, it is a good exercise to write your own versions of basic libraries to master the concepts.
I had the same problem. Try http://hackerrank.com It accepts haskell as a submission language and gives almost immediate feedback. The compiles are remote and they don't share their command line arguments so what works in their submission box might not work at your command line without messing with the compiler parameters. The point being that if you try to write the smallest version of this code challenges on hackerrank.com you get a starring point example that works in their compiler and you can reset to the working example if you mess up completely. I started with hello world and went from there. If you truly get stuck I find that solving the problem in a language you know helps you get back on track. I wish they had J as a language choice but I have a version of J on all of my devices and am solving http://projecteuler.net problems in number order to improve my understanding of J. I have solved 1 through 15 in python and J but only like two or three in haskell before I switched to J.
Right. Answer these questions for me then. flip :: (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c flip f x y = f y x 1. What are the arguments of `flip` called? 2. What are their types? 3. Where is `f` called? 4. What arguments do we call `f` with? 5. What are their types? Now answer those questions again, for this version of the function: flip' :: b -&gt; a -&gt; (a -&gt; b -&gt; c) -&gt; c flip' x y f = f y x
&gt; "We take a function f which takes two arguments a and b and spits out c. And then we combine it with b and a, that is, a and b reversed, and spit out c." &gt; (...) because if we're equating a and b across the function then we'd also have to equate c. This was the right intuition. Notice how `flip` takes `f`, `x`, `y` as arguments, but the right-hand side is `f y x`. We flipped `x` and `y`, because `flip` receives them in the reverse order that `f` expects them &gt; But possibly, the c that we get from f x y is different from the c we get from f y x. No! You can't call `f x y`. That makes no sense, because `f` takes an `a` and a `b`, in that order. 
`f :: a -&gt; b -&gt; c` means the first argument we apply to `f` must have type `a`. Since `y` has type `a` we can apply `y` to `f`, and the resulting value has type `b -&gt; c`. So: f :: a -&gt; b -&gt; c y :: a f y :: b -&gt; c x :: b f y x :: c 
Do you mean this one? https://registry.hub.docker.com/u/biscarch/haskell/dockerfile/
It's clicked! Thanks so much. I *really* do genuinely appreciate your patient help.
I really want to see your comments.
Yes, I think most Haskell coders at all levels understand fold and it's importance in FP. I find your code the very opposite of how you consider it. it lacks clarity and intent. And to me, that's rather ugly code.
Well, I think: foldr (:) [] is clear (`id`). Now if you lift `(:)` and `[]` two Applicative layers, they become: (liftA2 . liftA2) (:) (pure . pure) [] So the above code is pretty straight-forward, if you know the lift-into-applicatives idiom.
We "flip" the order, because f expects first something of type `a` and then something of type `b`, but flip receives its parameter of type `b` first (well second) and its parameter of type `b` last, so we match `a` with `a` and `b` with `b`
Could some of this functionality be incorporated into `ghci`?
The numbers denote the expression the operation is applied to: - Starting row - Starting column - Ending row - Ending column Or at least that's my guess.
All of it could be. It's a direct copy of the necessary GHC source tree file structure with some modifications. I've been using this for a couple of weeks to get a feel for how reliable it is, now I'm happy for others to try it out. I think /u/hvr_ (author of ghc-ng for 7.6.3 and below) is interested in merging such functionality in by patching it back onto the GHC main repo. It'd be a waste of the GHC team's time if I submitted these features early and then kept submitting updates, because I'm still feeling my way around the design space. But ideally before GHC 7.10 we'd then decide to merge some or all features into GHC mainline, objections permitting. I'm certainly writing code with the intention of it being mergable. I'm not re-organizing any modules or anything.
I think that what is confusing you is that, in this case, *you cannot extract type information out of the parameters*. f, x and y in the left-hand side do not carry any information useful to types. The type of flip is thus *entirely* derived from its right-hand side.
Great stuff, thanks for putting this together!
Gotcha, thanks.
You might be interested in the Hindley-Milner type inference system. It's the basis of Haskell's type inference system and is quite simple and elegant. It's easiest to understand by doing some very small examples by hand (like flip!).
An [official image][0] actually grew partially out of that effort. So now you can `docker pull haskell:7.8`. I've also done some work on [emacs in a docker container](https://github.com/ChristopherBiscardi/docker-emacs), which I've been using about 50-60% of my development time. I'm not entirely sure what the best way to include a customized emacs/shm/ghc-mod and a haskell/project base is though. [0]: https://github.com/darinmorrison/docker-haskell
and it's even better with a little point freeness added. inp' str y = mapM (flip check y) str 
I asked it on the twitter and people responded with http://www.cs.nott.ac.uk/~gmh/book.html which has already been recommended... And another http://www.nlda-tw.nl/janmartin/vakken/TFIT/Extra%20materiaal/Bird_Wadler.%20Introduction%20to%20Functional%20Programming.1ed.pdf Hope that helps.. And do give your suggestions after you find better resources 
I don't know if you will find it better than all the other resources you have already mentioned, but you should at least give the [haskell wikibook] (http://en.wikibooks.org/wiki/Haskell) a try. It's freely available online, well-written, outlines the information in a clear, constructive way, and offers short exercises throughout that help cement the concepts. In addition, it is comprehensive without being too much information at once (or so I think) and it's different enough from the other haskell tutorials available that it might appeal to you. Also, the fact that there's not one specific author helps keep the wikibook from being limited to one person's perspective on teaching haskell. While your concerns about using up people's time on irc is valid, a lot of people hang out there with the express purpose of being able to help other people, and asking a lot of questions is not necessarily a bad thing. In fact, the more you frequent the irc chat, the more the other users will get to know you. Ideally you start to become part of the community, which will be helpful in the long-term for learning and using haskell (particularly if you feel the tutorials do not really hold your attention). If you don't feel comfortable participating in the regular #haskell channel, you can always try #nothaskell, which is especially committed to being an open, non-judgmental irc community. Their user base is international enough that there are people lurking there at all hours of the day and night.
Please excuse this throwaway account. I want to post my take on your situation and not get involved beyond that point. So, I don't want replies flooding my inbox. I get the feeling you are frustrated and, perhaps, angry as a result. From the look of your comment karma, I would guess other redditors think you are being "unhelpful", to put it diplomatically. You say you know C best but I can't help but wonder if you don't know "hostility" even better. Yes, it would be nice if someone had a Haskell guide "for the rest of us", so to speak, complete with a yellow and black cover. Yes, it would be nice for someone to anticipate all those questions people have and compile them into one document. As far as I can tell, that document does not exist or, at the least, not one which satisfies you. I can't help you. Beyond suggesting steps you have said you would not do, I don't know what else anyone can offer. Part of me, and I say this with no disrespect intended, is curious as to just how much you actually know about programming. It is not impossible programming in Haskell simply is "not for you". If that's the case, that's the case. No offense but all the education in the world cannot cure "natural dullness" in a subject. Ask yourself what your motivation for wanting to learn Haskell is. Ask yourself how strong that motivation is. If you really are having so much trouble you don't even want to consult the people on #haskell, ask yourself if learning Haskell is even worth it to you. If not, just put your books down and walk away. If learning Haskell is worth it to you, however, consider a tutor. You say you don't want to consult #haskell because you "do not pay them anything to answer questions nor would [you] be able to afford to pay them anything, anyway". Maybe you should wait until you can afford to pay someone. Maybe you could take a local community college course or, because you "keep odd hours and people often sleep when [you are] awake", maybe you could audit a course and ask the professor to record the lectures and answer questions over e-mail. Whatever you do, as I said, ask yourself if this language really is "for you". Sometimes, it's not. Peace.
I like it, but I'm going to take a second to bang the minimalist drum. Why must there be an identity for composition? Or rather, what advantage is there to be able to introduce identity elements when (re)composing program structure? I imagine we'd like to be able take more complex expressions, equate them to an identity, and then eliminate them entirely during compilation. However, what advantage is there when actually do the programming--building up the expression, to being able to stick "id" in there? If it has no value during the programming, we should discard it, weakening our assumptions and strengthening our theory. If the existence of identity is useful in certain contexts, it can be easily re-introduced in just those contexts, which will be subsumed by our wider theory. For this reason it would be better to say "Semigroupoid: The Essence of Composition". Having abandoned the guaranty of identity, we turn to another axiom: associativity. In the context of programing, we liken our morphisms to functions or operations, and liken our objects to states (of the program, system, or whole world). But, while reaffirming this analogy how can we defend associtivity? The order we perform operations matters. When we pretend it doesn't, it's because we are sloppily calling a family of operations (each with a different input / output state) as a single operation, ignoring (for now) the differences in those states in the name of simplicity[1]. These simplifications are very valuable, and we wouldn't want to do without the abstractions they enable. However, just like any other axiom, be can remove it from our larger theory, and still transport everything we establish in the larger theory into a certain case where associativity is valid. Are the undeniable advantages to this assumption, even when there are so many cases that it is proven untrue? When composing, do we routinely leverage this axiom to great success? I do not think this axiom that valuable for programming. For analysis in the certain contexts it is valuable, but I must deny it's universality in composition. I say better still is "Magmoid: The Essence of Composition"[2]. [1] Many of these are around ignoring the temporal aspects of our state or operations. Once these are considered, operations that look identical (e.g. "LEA EAX 0x00004001") are different because they have different behavior in different states [from (t, 0x00004001 in L1 cache) to (t + 10ns, 0x00004001 in L1 cache) or from (t, 0x00004001 swapped out) to (t + 10ms, 0x00004001 in L1 cache)] [2] Magmoid isn't the most common word, but "category without (necessarily) identity and without (necessarily) associativity" has turned into "magmoid" in several places on the web other than this post here.
&gt; However, just like any other axiom, be can remove it from our larger theory, and still transport everything we establish in the larger theory into a certain case where associativity is valid If you remove associativity, there isn't very much left to work with. More sensibly, you can replace associativity with "weak associativity" up to some notion of equality of morphisms. This is the approach taken by [higher category theory](http://ncatlab.org/nlab/show/higher+category+theory).
I like to think of the identity laws as unit tests for composition. You can read these laws: f . id = f id . f = f ... as saying that the composition operator must not have any "side effects" intrinsic to the operator itself (for a very vague definition of side effect). If composition did have side effects then no identity would be possible. So you can have a composition without identity, just like you can write a library without tests or proofs, but I don't recommend it.
&gt; But ideally before GHC 7.10 we'd then decide to merge some or all features into GHC mainline Just be aware the GHC 7.10 freeze is planned for in about 2 weeks. So please try to get it into shape by then! :-)
&gt; But, while reaffirming this analogy how can we defend associativity? The order we perform operations matters. Are you thinking of commutativity? Funny that you'd mention magmas. I wrote a whole blog about magmatic categories: http://bartoszmilewski.com/2014/09/29/how-to-get-enriched-over-magmas-and-monoids/
&gt; Why must there be an identity for composition? I am only a beginner in Haskell, but to me the identity seems incredibly useful for composition. E.g. You have a list of numbers, and you want to pair them up into pairs of the original number and its square. We already know how to square: `(^2)`, we already know how to pair outputs of two functions: `&amp;&amp;&amp;` (from `Control.Arrow`), and we already know how to apply a function over every element of a list: `map`. But, if we don't know how to apply the identity operation, then we cannot compose these items to achieve what we set out to do. Yet, if we have the identity `id` then we can write this: &gt; let f = map (id &amp;&amp;&amp; (^2)) &gt; f [1..5] [(1,1),(2,4),(3,9),(4,16),(5,25)]
I would like this to happen. I expect the upheaval of managing issues and management and things (maybe a bit of “if it ain't broke…”) has prevented it from happening so far. I think whether or not I can get my changes merged into HEAD for 7.10 will be indicative of whether a split would be worth doing.
Thank you for your comments. You're right, there's a lack of mathematical examples in the project. I think it was because of the objectives we had with the project proposal, but it would be very interesting to include more. And it would also be really nice to say more about Remark 6.1. I think that's what you're referring to with your second comment, right?
Yes, that is the remark I was referring to.
If you think of functions as instructions, composition is putting together instructions to make a plan of action. If: put in teabag . (poor in boiling water . stir) Makes a cup of tea, I would rather (put in teabag . poor in boiling water) . stir do too, rather than resulting in something different like an unexpected cup of coffee.
Shouldn't this rather be the following? stir . pour in boiling water . put in teabag Where (at least to me) `put in teabag` and `pour in boiling water` do even commute.
&gt; Yet, if we have the identity `id` then we can write this: &gt; &gt; let f = map (id &amp;&amp;&amp; (^2)) &gt; &gt; f [1..5] &gt; [(1,1),(2,4),(3,9),(4,16),(5,25)] dup :: a -&gt; (a, a) dup x = (x, x) let f = map $ second (^2) . dup f [1..5] [(1,1),(2,4),(3,9),(4,16),(5,25)] No need for `id`.
&gt; laws as unit tests If our test frameworks are good enough, this is always true. Any axiom (or arbitrary true proposition) can be turned into a unit test. It's not unique to the existence of identities; and therefore, is not a strong argument in favor of identities in particular. Associativity makes similar, but weaker, "side effect" free guarantees since it prevents you from counting the "depth" of composition. (e . f) . (g . h) = e . (f . (g . h)). I'm not advocating for a theory (or library) without proofs. I'm asking for proofs to be established in the most widely applicable theory possible. (Fewer axioms = more widely applicable.) I think there are interesting proofs and engineering that can be done with fewer axioms, as the *essence* of composition seems to be satisfied by a Magmoid. Maybe I'm wrong. What are some interesting results that require identities or associativity but *nothing else*?
Well, okay. But, in a way, that's cheating, since `dup` does contain `id`. At least, if you want to compose it out of existing functions, instead of writing a new one, you could define it as such: dup = id &amp;&amp;&amp; id I will admit right-away that I understood only half of your original comment. But, let me ask you a genuine question (keep in mind: I'm quite new to all this): Do you think it really hurts to have `id`, from a purely practical point of view?
They're going to be super useful for people like me writing plugins for text editors.
&gt; Do you think it really hurts to have id, from a purely practical point of view? Most of the time, no. However, there are contexts where `id` has to be directly injected (think of adding another option in your sum type just to represent it), in which case is can make analysis more verbose and less obviously true. Also, /u/Guvante gets it.
&gt; `dup` does contain `id` Really? With just the Control.Arrow.Arrow methods and `dup`, can you write `id`? If not, `dup` is the weaker assumption (which makes for a more-widely-applicable theory), since you've shown that having `id` implies that you have `dup`.
&gt; Funny that you'd mention magmas. Not unusual for me, since I wrote https://www.fpcomplete.com/user/bss/magma-tree. I did enjoy your article, too, as it made me think about unital magmas and where they might fit in to my thoughts on algebraic containers.
I was in a similar boat at one point. I have found most Haskell tutorials insufferable, and have never finished one that I recall. Nevertheless, I used Haskell professionally for two years and still use it for hobby projects. The most effective way for me to learn was the fact that I had to use it for my job—it was sink or swim. My coworker and I asked each other questions when we were stuck, and we both got more proficient with the language and ecosystem over time. Looking at it now, I don’t consider Haskell terribly different from the imperative languages I came from, although I do find Haskell much nicer to use! A Haskell program is just a collection of data types and functions separated into modular units, same as your typical C software. Thinking about how the GHC runtime represents values in my Haskell programs was very helpful for me when I was learning, so that I could mentally desugar the Haskell code into C. Maybe you would benefit from something like that? 
(blah, they mispelled Hindley-Milner)
But not with too much point freeness! inp' = flip (mapM . flip check)
To be quite honest, I do not really understand the point of this kind of paper. 
Thanks. Now it's dawning on me what he is talking about.
&gt; Also, /u/Guvante[1] gets it. Yes, sorry, I completely misread you. Sure, if you get useful stuff out of less Axioms, by all means I'll be happy with it.
Haskell is very garbage collection(GC) intensive; even though JVM is pretty good the performance hit is likely to be high. If you need to use JVM GUI or database libraries, you could interface to a JVM program using MPI or in-memory http. What is the use case for a JVM that would justify the consumption of several brilliant-person years? I think the task is beyond mere mortals - it would require brilliance. Wouldn't that talent be better used in creating a next generation type system? or improved compiler optimization? Understandable syntax errors?
One example where the associativity of function composition is valuable is with the showS trick for faster string concatentation. For identities, how can you define isomorphisms without identities? There might be a way, but it much easier with identity morphisms. Isomorphisms are a major part of category theory, but they also turn up in programming, like in the lens library. One reason why I think identity arrows are important is that composition should not be though of as a binary operation, taking two morphisms to one. Instead it should take a string of morphisms with matching domain/codomains and turn it into a single morphism. A string of morphisms is of finite length, and can be of zero length (note that the zero length string must have a starting object), hence identites should exist. This is similar to the way that a monoid operation should take a list of elements and reduce it to a single element. This is all related to monads, but I would have to work it out for the string of morphism case above. Singling out the binary operation I feel is the wrong approach, instead we should focus on all arities, including the 0-arity operation.
Alright, sorry, wrong use of *contains*.
&gt; One example where the associativity of function composition is valuable is with the showS trick for faster string concatentation. Actually, I'd argue that the poor performance of string concatenation is due to the forced right association in the free monoids called (cons) lists. If your data structure looks more tree-ish, you can have excellent performance for both foldl (++) and foldr (++). If you don't enforce a particular nesting / assume associativity, you naturally have to maintain a tree structure.
For future language designers to experiment in directions that seem to be the most productive? To provide empirical evidence to the world at large that languages have a measurable impact on software/software system reliability. That's just a couple possible points, I'm sure others can come up with many more.
Very cool paper. There's one thing that bothers me though — they don't account for languages being specifically picked to solve certain problems. Namely, if I had no context, I'd say from reading the paper that Erlang sucks at concurrency. From what context I do have, I find it likelier that concurrency issues are common in Erlang because concurrency itself is a hell of a lot more common in Erlang code.
"scripting" is a paradigm?
Is that why we no longer have any monad and lens tutorials? Because of all the machine checkable formality?
Perl and PHP being listed as scripting languages I can understand. But they list Python and Ruby as scripting language. Those are (or try hard to be) OO languages.
Community college.
https://www.edx.org/course/delftx/delftx-fp101x-introduction-functional-2126 As far as I know it's still possible to enrol (free). It follows the aforementioned "Programming in Haskell" (http://www.cs.nott.ac.uk/~gmh/book.html). I wouldn't worry about errors in examples but be proud having found those.
I don't know enough about Ruby but Python's per-object dictionaries tend to make me view it more as a scripting language. An object can have more or less any structure you like on the fly^1 . Which, coupled with the ability to poke around inside anything, makes it simply awesome for some applications and a maintenance booby trap for others. ^1 Which makes it really *object*-oriented
Is this with Fedora 22 rawhide? I see the problem there too on armv7. Not sure if there are any patches that can be backported to help alleviate this
No GHC? :(
id is useful to remove conditionals. getAction = fromMaybe id . flip lookup actions Is there away to avoid having to handle the Nothing case upstream without id? Hmm, maybe I shouldn't do that as it defeats the short circuit behavior of Nothing. 
fpcomplete.com is my guess. When the Student is READY, the teacher appears. why does haskkel have such a MULTI-DIMENSIONAL learning curves??? 1.)haskell is a RESEARCH language. 2.)it's superpowerful beyond language and meta- language and so your question is poorly founded. THINK IN HASKELL rather than literate haskell or 'English.' or in Buddhism, the pointing finger to the sky is NOT the moon. 3.) Please write a book on anti-Real World Haskell the 1000 errors at all levels. 4.) Haskell is powerful. As a result there are much fewer competent teachers at ALL LEVELS. Perhaps teachers are obsolete? 5.) since it is parallel, lazy, abstract, meta, evolving, space and resource leaks, THERE ARE TRAPS and pitfalls all over. 6.) tradeoff. Python duck typing. The code works bu the quality is low. Haskell the quality is high, but the code is fragile. 7.) one way I use is stubbing it out and genetic HAND algorithm brute force fuzzing. Yes, I am NOT a true expert in math or logic or calculus algorithms. Nor software architect, systems integrator, engineering hardware tech assembly lang genius. that's why haskell is the intersection of many areas and the description of learning HERE is like the blind men describing the elephant teaching fable. Actually IMHO, the friendship of Haskell reddit is above average and insert bad joke about GHC - Glasgow Haskell Compiler. SHE is like the insecure SCOTTISH mistress, who votes for British rule, so she is 'independent' but really part of the British empire. end joke. quick reality test about LLVM C vs GHC LLVM Clang error messages. 1.)pattern search on search engine 2.)plug in fuzzy pattern using 10 top C books 3.)reduce the problem to where I can solve it. - low level of C expertise. compare Haskell 1.)pattern search on Hoogle and search engine not much shows up 2.)books - the code is broken and there are FEW BOOKS, just blogs 3.) error messages are highly cryptic and still to me MAKE NO SENSE, so trial and error is hard. 4.) THE BEST BOOK IS THE ONE I AM WRITING FOR FREE using the haskell source code. Detective work as to why templates, versions of code changed with REAL WORLD goals. the Real World Goals are taken from scala and other well written books. alas, too bad i as co-author am slow in becoming an expert, let alone teacher. so the writing of the book is a learning experience and a JOKE (yes, when you see my code, you start laughing) and I laugh often for it relaxes me. PPS. at the very least PLEASE someone put on github /code repo the top ten FREE pdf papers associated with keywords clusters of HACKAGE packages. Yes, that includes term re-writing. Yes, that includes university papers WHERE I CANNOT understand the proof and I frankly do NOT care. PPS. to be fair, it seems that the university courses are decreasing as colleges shift to JAVASCRIPT and PYTHON. dire prediction: Haskell will go the way of LISP and Prolog request: will someone write a stripped down version of EMACS using not LISP but Haskell My DIRECT interest is integer sequence and there haskell seems to do well, but IT IS NOT INTEGRATED with Haskell Hackage and/or fpcomplete.com The On-Line Encyclopedia of Integer Sequences® (OEIS®) https://oeis.org/ On‑Line Encyclopedia of Integer Sequences Given an integer sequence, find its name, and formula. 
&gt;You would have to enter the city through the central building, `main`, and travel within the structure. Haha this one is great 
PHP "city" would be a sprawling chaotic hub with badly-constructed *everything*. Leaking pipes, exceptions (which would be large beasts that can tear everything down around them when left unchecked) running rampant through the streets and knocking down the shoddily-built constructs. No one uses a tool for what it's made for: screwdrivers are used to make holes in the walls, etc. Hammers ("arrays") are the exception: everything is hammered in ("hardcoded") in everything else. In spite of all of these problems, the people are optimistic and happy, putting support struts on buildings on the verge of collapse with a smile. All over the city, huge shiny domes can be seen. Coming nearer, you are informed by Bloggers how glorious this particular Framework is. Inside the domes, it's exactly the same as on the outside, but it is far cleaner than outside, with less errant code littered around. The shoddy constructions, misinformed populace and exceptions remain, though. At the center of the city, there is the PHP library: a huge DIY store that contains tools for everything, but they're awkward to use, often blowing up at the most unexpected of moments. Deep under the city, the PHP Core Council works on the future of the city. Many of them have clearly gone insane a long time ago, rejecting even the slightest notion of typing. Others try to push forward valiantly, but progress is painstakingly slow.
The two terms are not disjunct. Ruby is a scripting language because it is often used to make scripts. It is also object-oriented. It is also a web-programming language, when used so. It also supports functional style (more or less). It is, obviously, also a general-purpose programming language. It is also often an imperative language (except when it isn't, or is so only below the surface, like in RSpec scripts). I'd argue Python is most of those things as well, with a different set of restrictions for being considered functional, and having an object orientation pasted on as an afterthought, but still. Doesn't mean it's not a scripting language. Existence of large projects doesn't change this fact. It is not the presence of the large projects that would disqualify them from being scripting languages; rather, it is the absence of quick write-in-five-minutes applications that disqualifies Java and its ilk from the category, since everything has to go through the compiler, and succint code is (or was) almost impossible. EDIT: I just saw the article itself. Yes, splitting languages by paradigm makes no sense, since most of the language are multi-paradigm. CoffeeScript compiler can hardly be called a script, even though submit-when-button-clicked two-liner may be.
while still readable (i thought about posting it myself), the other version is readable for novices as well, while yours might not.
&gt; I wrote https://www.fpcomplete.com/user/bss/magma-tree Oh, that was you. I liked the article very much and it inspired me to write mine.
that's a grey area. even perl and php support (some form of) oo.
You're right. I was got by my overwhelming urge to write English left to right. Hopefully the point about why associativity is useful was still clear. 
Not on GitHub?
It is now: https://github.com/ghc/ghc
Just wanted to mention that I enjoyed these, too ;)
There's a school of "thought" where people decide what works best by joining up with a particular tribe and believing and loudly advocating whatever that tribe believes and advocates. This is pretty much how most of what we "know" about programming is decided - seeing which tribes shout loudest. Academics can't gloat about this - in many ways they're just another group of tribes. Once in a very long while, someone actually does real science. The results tend not to be definitive because real science is hard work, but it's still more objective and reliable than all the tribes shouting at each other. The results tend to upset *all* the tribes. I've been learning Haskell for a couple of years. In that time I've managed to find myself in more than a few arguments because I'm the kind of arsehole who can't just let people make overblown claims about their favorite language or attack other languages, especially if I'm not entirely sure whether they're right or wrong - my favorite advocate is the devils advocate. Anyway, the idea that (from the abstract) static typing is only "modestly better than weak typing" and "somewhat better than dynamic typing" is going to seriously ruffle some feathers in the Haskell community. Not all feathers by a long way - there are a lot of very smart, very balanced and very helpful people there - but e.g. in the past when I've argued (based on other empirical results) that I don't really take much seriously other than total SLOC for a project because total SLOC predicts development time and error rates, but other metrics (once you control for total SLOC) don't, claims were made that Haskell allows source code to be 10 times smaller for the same functionality. Actually, I have no doubt that's true for a few special cases, but while Haskells type system can be more about how you express what you want rather than compile-time error checks (due to pervasive and often surprising use of type inference) and laziness often means you write fewer functions (one function can do several jobs without being inefficient - within limits, if you don't use extra results you don't pay the price for computing them) I don't really think Haskell is *that* much more concise than most other languages. OTOH 2 years non-intensive isn't really that much experience with Haskell - there's a lot to learn. Also, there's a point when I break that minimum SLOC rule. The empirical evidence is about things that real programmers actually do (more precisely, things that actually happened a statistically significant number of times in the code studied) - anything sufficiently outside the norm isn't covered. This is, of course, me making an excuse for ignoring an empirical claim, but not an unreasonable excuse. Anyway... I doubt Haskell or other functional languages were seriously considered by the relevant studies and even some fairly common styles in FP are far too cryptically concise for me - especially "point free style". It's not just me, either - in a talk, Simon Peyton-Jones (one of Haskells inventors, still playing a leading role in developing the language and the GHC compiler) said he doesn't like point free style either. I think it was a talk about lenses (don't worry - they're not that difficult but I won't try to explain here) but I'm not sure. 
&gt; The data indicates functional languages are better than procedural languages; it suggests that strong typing is better than weak typing; that static typing is better than dynamic; and that managed memory usage is better than unmanaged.
Maybe because it's not entirely written in Haskell. "Just" 81.8% Haskell of ghc are written in Haskell an 12.1% in C according to [github language statistics](https://github.com/ghc/ghc). To see the language stats: click on the coloured bar quite at the top and below the commit, branch, releases and contributor stats. 
You can at least do this (not a really useful example, but you should get the idea): {-# LANGUAGE TypeOperators #-} import Data.Map type a $ b = a b returnMap :: Monad m =&gt; Map a b -&gt; m $ Map a b returnMap = return Doesn't work for data declarations though
Do you allow remote work? Because if you do, then it will be very easy to get more developers. 
[Haskellers.com](http://www.haskellers.com/) might be a good place to start. A few people listed within commuting range of Atlanta there.
Your employer has a point. Haskell is probably not the right choice for them, especially for a small company like the one you work at. Finding Haskell devs is hard, finding ones in your area is harder, and finding ones that are looking for a new job, are a good fit for the company, etc. is next to impossible. 
The compiler is written in Haskell. It's the RTS that is written in C and C--. ...you know you aren't a systems language if you implement the RTS in another language.
There is some truth here, outside of a few tech hubs in the US it can indeed be quite hard or impossible to find Haskell developers willing to work local and are on the market at the same time you're looking. It's just a matter of statistics. 
The programming language should not be the only criteria if the developer is a good fit for the project or not. My company hired me, although Java is a main language and I explicitly stated that I didn't have project experience in Java in my application. 
That is true, but let us not kid ourselves, Haskell is not your average language you can quickly learn to a sufficient degree.
Much nicer to just change `check` (which is only used in this one location, `inp'`) so that `flip` isn't necessary. It's idiomatic to create multi-argument functions where the inner-most argument varies least, so: check :: (Int, Int) -&gt; Char -&gt; Maybe Char check off c = ... inp' str y = mapM (check y) str Also, what kind of name is `inp'`? At call site `fmap (inp' inp)`?
Not everything needs a citation, sometimes one can apply rational thought. I am a Haskell coder, and I most likely wouldn't join OP's company, simply because it it not likely that they are doing something that I am passionate about, and because I am not relocating for a company with a one-man tech team. As much as we wish it was different, there are only few capable Haskell dev's out there looking for a job and willing to relocate. It is hard enough to find good coders in more popular languages as it is. 
&gt;One certain condition is that I need to be able to give them the resumes of at least 5 other Haskell programmers It is just *one* condition. If he was unable to find the resume of 5 haskell developers in the area, that is a pretty good sign finding one is hard.
Learning Haskell is surely a time-investment. To be honest I would also hesitate to use it in a start-up but test it in small projects to gain confidence. But it is frustrating, because nobody wants to use it in a professional environment because nobody uses it.
In a few years, less than the mean lifetime of a distracted developer in a highly congested traffic area, probably all work will be remote.
I didn't claim he did, and I didn't say that either. My point was basically that I prefer clarity over conciseness (where the two come into conflict) and that I don't always agree with how other people balance those scales in FP - which is no surprise and no big deal in general. A better criticism of my point would be since I avoid point-free style I never get used to it - being unfamiliar isn't the same as being cryptic or otherwise bad. Which is true, but we can't familiarize ourselves with everything, we have to make choices. 
Also, if you have one Haskell programmer already, it's easier to have that person train new Haskell programmers.
Interesting paper, but I hardly could continue reading after I saw this: &gt; TypeScript :: bitcoin, litecoin, qBitorrent O_o &gt; Since languages can be identified by the extension of a project’s source files, GitHub Linguist counts the number of source files with different extensions. Ok. Great method. https://github.com/bitcoin/bitcoin/search?l=typescript Such a typescript. A little XML'ish but c'mon extension is ".ts", of course it's a typescript. Well, that doesn't make the research completely wrong, but many analyzed projects that are assigned to typescript seems to be not typescript but C++, or XML, or something that uses QT. Great. Really, I can't understand, how they didn't notice that bitcoin, litecoin and qBitorrent are not written in typescript. That could be one of the most interesting points of the research, comparasions of javascript and typescript projects. As language are mostly identical except the typing part it would be interesting to see how it impacts the projects.
Whether they actually want to line up bus-accident replacements, or this is just a test of whether you could find 5 Haskellers with CV's in the area, I'm fully in favor of entertaining your employer's request here, if it means more industry uptake for our favorite language. My [CV](http://web.mit.edu/greghale/Public/GregHaleCV_2014.pdf). I'm not in Atlanta but would move and take a salary cut for a job in an interesting domain coding mostly in Haskell. Good luck. Hope they let you do your thing :)
Tell them. The conference is only next week. Perhaps the proceedings haven't gone to press yet.
Finding Haskell devs is not hard, in my experience. We've had lots of applications, people willing to move from other countries to work in Haskell. They're also generally very competent (more so than programmers of other languages). Perhaps building a team of 100 Haskell devs is hard, but 5-10 is not.
It's likely too late. Camera-ready deadlines are usually a few months before the conference itself.
Brandon Moore is listed as one of the reviewers for the conference. Is that our Brandon Moore? If so - congratulations.
Someone tried to start an [Atlanta FP users group](http://fse22.gatech.edu) about four years ago. It never took off, but if you can track down any of the people on the list, maybe they can either send you a CV or help you find others.
It's about control. It's always about control.
Yeah, which is why you should aim for having two people work on the project upfront.
My God man. If you're the only programmer who will know? My boss calls my creations Perl scripts and see no need to correct him.
With associativity and identity laws, you can prove that inverses are unique: if `f . f_ = id = f_ . f`, and f_1 and f_2 both satisfy this law, then `f_1 = f_1 . id = f_1 . (f . f_2) = (f_1 . f) . f_2 = id . f_2 = f_2`. 
This is correct. The goal is to see if there are actually Haskell programmers who could either (1) be added to the team or (2) replace me. It is not to hire more devs immediately but to make sure that they could feel confident about finding other devs in the future.
http://silk.co Our backend is fully written in Haskell. Also, we're located in Amsterdam, a lovely city which seems to attract people as well. :)
&gt; There's a school of "thought" where people decide what works best by joining up with a particular tribe and believing and loudly advocating whatever that tribe believes and advocates. This is pretty much how most of what we "know" about programming is decided - seeing which tribes shout loudest. Academics can't gloat about this - in many ways they're just another group of tribes. &gt; &gt; Once in a very long while, someone actually does real science. The results tend not to be definitive because real science is hard work, but it's still more objective and reliable than all the tribes shouting at each other. The results tend to upset all the tribes. The problem is, this work is not what I would call real science. I'd call it cargo-cult science. We see some of the trappings of science (scientists gather data and use statistics, and write up their results as two-column papers, don't they??), but the fundamental assumptions of the work are simply not going to lead us to truth. If we were to do real empirical science wrt software engineering, it would be via rigorously controlled experiments, not large-scale observational studies like this, which fall somewhere between "worse than useless" and at best "hypothesis generating". This is especially the case in software development, where there are so many factors in play, all of which are very difficult to isolate. I would welcome some ingeniously designed experiments which do properly control for these factors and that could tell us something interesting about software development. This work is not that, and it's not even close to that. I'm reminded of [the old joke](http://en.wikipedia.org/wiki/Streetlight_effect): &gt; A policeman sees a drunk man searching for something under a streetlight and asks what the drunk has lost. He says he lost his keys and they both look under the streetlight together. After a few minutes the policeman asks if he is sure he lost them here, and the drunk replies, no, and that he lost them in the park. The policeman asks why he is searching here, and the drunk replies, "this is where the light is." Yes, one can trawl through a bunch of data on GitHub, do some statistics on the data, and draw some tentative conclusions, but this is like the drunkard looking under the streetlight for his keys. It's where the light is, but looking there isn't going to help us find out what we're actually interested in.
I know more Haskell teams that are not distributed than are, so I fail to see the reasoning.
I'm at ~intern-level Haskell in Athens. Not that that helps much.
&gt; would move and take a salary cut ATL has a really low cost of living compared to salaries. Half million dollars buys you a 8 bedroom suburban home or a 4 bedroom urban one or duplex. Compared to Mass, most salary cuts wouldn't be after COL considerations.
Because nobody developed the necessary libraries &amp; made them easily accessible. Also, writing well performing numerical code in Haskell is arguably far more of a black art than doing so in Julia. When I start Julia I can immediately compute the eigenvalues of a 1000x1000 random matrix: eig(rand(1000,1000)) Or plot a function: f(x) = sin(3x^2 - 4x) fplot(f, [0,3]) Or compute covariance: cov(rand(10), rand(10)) Or do a fft: fft(rand(100)) Figuring out how to do that in Haskell will take *a lot* more time.
Because working mathematicians don't really care if a programming language implements functors, monads, typeclasses, purity, and all that. Unless, of course, they have a foot in computer science. They only care about getting the result of their numerical computation in the most simple and efficient way possible.
Julia is not just another dynamic language. Some thought and execution went into creating a JIT that will produce efficient numerical code. The benchmarks I have seen for Julia are impressive. GHC has taken a different approach to producing efficient code. I bet GHC code gain a JIT, that would be a win in some situations.
https://singpolyma.net/cv
I'd seriously consider Atlanta for a Haskell gig. I'm in Texas. http://bitemyapp.com/
Journal of Pointless Appeals To Authority, June 2004: "Does This Worthless Reference End The Discussion?", Mr. Dewey, Cheatum, &amp; Howe, Dept of Supposition, University of Anecdotes
I can assure you, scientists like me are not overqualified to write software at startups - the opposite is more likely :) Thanks for the compliment - made my day!
&gt; Who likens objects to states? Arguably, the article, though not explicitly. One of the (many) ways we compose programs is by sticking two state machines together to get a bigger state machine. The morphism of this composition is the state transition, and you can only compose morphisms when the states match. You'll also see this when using indexed monads to do resource management. I believe /u/tekmo experimented some with this in earlier iterations of pipes.
I live in Atlanta: I went looking for a Haskell user group, and only found a Clojure group. Starting a meetup would cost you about $150 a year if you paid it out of pocket, and you're likely to find everyone doing it or everyone interested over the course of that year in town, (just be careful to pick a night that's not the iOS, Clojure or F# meetup night). Also consider F# meetups as a fishing location. (iOS and Clojure are the same night sadly) I'd also consider heading to the iOS developers meetup and asking. A lot (proportionally) of iOS dev's are looking at Haskell after Swift now, and wouldn't be surprised to find one or two who'd fit your criteria. "Haskell Atlanta", I like the sound of that :D Sorry, I'd not be qualified as a resource currently.
[My resume](https://docs.google.com/document/d/1ORf48b557nK0gYI4h6J0xP7nzOVRoqtOogYsukUMREU/edit?usp=sharing). I am a Haskell programmer, but I don't get to use it as work much. 
Cool, thanks :) Yes - I'm curious to find out what happens! May be worth a new /r/haskell thread in a week's time?
I know it's an unpopular point of view around these parts, but there's a reasoned argument to made that a dynamically typed language with a sufficiently developed FFI ( Julia, Python ) represents a local maxima in the design space for this kind of interactive data and numeric workflow. Not to say that Haskell can't be made to do these things, of course you can per se but from an interactive point of view this kind of workflow inside of ghci isn't ideal for 2 big reasons. First, GHCi bytecode interpreter performance has never been a priority, and even using fast libraries like vector under ghci means that half of the fusion and optimization's that fire under -O2 simply aren't present and performance isn't great. Having a dumb runtime ( CPython is really really slow ) that simply orchestrates calls out to C and Fortran is a dumb solution but it's been unreasonably effective. If you try to roll a library like pandas, which is effectively a giant heterogeneously typed array of structures, you either have to use this two-pass workflow where you predefine a schema and then load data. Or you rely on fancy types ( composite HLists ) and inference basically becomes unwieldy under ghci requiring a whole slew of annotations to even do something simple like project into a column of a heterogeneous table. Compared to pandas under Python where you can just fire-and-forget at a amorphous CSV and not worry about types at all. Every time I've tried to recreate his kind of workflow in Haskell it ends up being a bunch of value-types-of-vectors and rolling a simple interactive shell and it's start's looking a heck of lot like a reimplementation of Python. Lest I not be all doom and gloom, I think that a Haskell-like language ( perhaps interpreted instead of AOT-compiled ) with row polymorphism and type-providers could make a lot of these problems mute. Unfortunately I think such things are outside the scope of GHC Haskell for the foreseeable future. 
&gt; Yes, but still better than the shouting tribes. But our options aren't a) dubious empirical studies or b) "shouting tribes"! And I'm not at all convinced that dubious empirical studies like this one are valuable at all. Like a weather forecast worse than random guessing, badly conducted experiments are often worse than useless. They don't get us closer to the truth and can be actively *misleading* and lead people to hasty generalizations. How does that help us? Already I've seen people using this "study" as ammunition for whatever viewpoint they'd like to espouse. ("AHA, static typing and FP FTW!!" or "AHA! static typing only marginally better, Ruby FTW!!!1!") If anything, studies like this make the tribalism worse, because it raises certain issues but just doesn't give us any further avenue to get at truth, just some misleading, noisy and confusing data. &gt; And apparently pretty common and widely accepted scientific communities, though with the usual caveats and provisos. The fact that terrible "science" is widely practiced does not make it any better at getting at truth. And I would not say that this sort of "science" is "widely accepted" so much as it is "widely practiced". I also hesitate to use the term "science" for what is being done in this work. See [Feynman's lecture](http://neurotheory.columbia.edu/~ken/cargo_cult.html). I am personally skeptical that empirical efforts to study these things will ever produce insights of value, but I am at least open to the possibility that good research could be done. In the meantime, I consider it useful to make actual technical arguments. Things like - not tracking effects in the types means the typechecker cannot alert the programmer about whole classes of bugs in managing these effects. Or - the ability to define abstractions like `Applicative` means I can eliminate what would otherwise be code duplication. Or - proper tail calls are necessary for modularity in some cases. Or - strictness results in loss of modularity in some circumstances. I can make actual technical arguments for these things, and I don't need empirical study to see whether they are true, any more than I need empirical study to know the number of comparisons done by mergesort. Empirical study might help us to better understand how much these sorts of things *matter* (given various assumptions), but even here there are serious caveats.
Speaking as someone who has looked into Julia's internals, I don't think Julia's compiler is even at the foot of the sophistication of GHC. There are only two reasons that I'd choose Julia over Haskell: 1. Julia's libraries and ecosystem are geared towards scientific computing. Things are just plainly available and easy for one to get started. 2. Julia is a scripting language, so it attracts the same folks who are comfortable with scripting languages. On the other hand, there are a lot more reasons that I'd choose Haskell over Julia. I'll just give a few here: 1. Haskell has a well defined semantics, Julia doesn't. Take a look at Julia's method overloading for example, the result of a function call will depend on when and how the method table was updated. 2. Julia's type system is too ad-hoc. It tries to use abstract interpretation to infer types, but there are corner cases, insufficient type information for primitives/intrinsics, opaque function types, etc. that just get in the way. Without full type information, a Julia function can't be compiled into efficient low level code. And, no, type annotations don't work in ways that you might expect them to. 3. There really isn't much secret sauce to Julia's compiler. It implements a macro expansion layer, an ad-hoc type inference, and an ad-hoc unboxing mechanism that gets tangled with the codegen that lowers Julia AST to LLVM. So it is fair to say that Julia mostly relies on LLVM to optimize code. GHC on the other hand, does a LOT more optimizations before lowering, and cares a lot more about the soundness of their implementations. 4. Although Julia supports first-class functions, you really can't assume the same level of support of functional programming found in a Haskell or ML compiler. To get efficient code out of Julia, you either have to call high-performance foreign functions, or write in an imperative style that uses explicit loops, array indexing, and throws language safety out of the window. Doing anything related to function composition and closures will slow you down significantly. So it is fair to say that I don't buy the "high performance" story that Julia folks are trying to sell, their benchmarks are superficial, and despite its attempt at typing inference, there is still a lot to be desired at eliminating the overhead introduced by dynamic types. From a language design point of view, Julia is moderately interesting because it combines lisp-like macro with type-based method dispatching (remotely similar to Haskell's type class), and a just-in-time compiler. But there are also a number of of design flaws I won't go into the details here, just as in most other scripting languages like Perl, Python, Javascripts, and so on. 
Yes. It´s a feeling of control. What is necessary are success histories. And the big success history of remote working is open software.
&gt; Or - proper tail calls are necessary for modularity in some cases. Considering that tail calls fall into two classes AFAICT - either for optimization or for looping (different because stack overflow in a long loop is a correctness issue) I'd like to see the rationale for calling that a modularity issue anywhere you have loop constructs. A lot of the claims for tail calls come from the "lambda papers" for Scheme and I haven't read all of those - is this from one I missed? 
When people say Haskell takes a lot of time to learn, they usually mean that functional programming takes a lot of time to learn. Which it does.
&gt; I know it's an unpopular point of view around these parts, but there's a reasoned argument to made that a dynamically typed language with a sufficiently developed FFI ( Julia, Python ) represents a local maxima in the design space for this kind of interactive data and numeric workflow. I'd like to see such an argument made. &gt; Having a dumb runtime ( CPython is really really slow ) that simply orchestrates calls out to C and Fortran is a dumb solution but it's been unreasonably effective. Indeed, we're thinking of binding to [NAG](http://en.wikipedia.org/wiki/NAG_Numerical_Library) for numerical analysis at FP Complete. &gt; Or you rely on fancy types ( composite HLists ) and inference basically becomes unwieldy under ghci requiring a whole slew of annotations to even do something simple like project into a column of a heterogeneous table. Compared to pandas under Python where you can just fire-and-forget at a amorphous CSV and not worry about types at all. Every time I've tried to recreate his kind of workflow in Haskell it ends up being a bunch of value-types-of-vectors and rolling a simple interactive shell and it's start's looking a heck of lot like a reimplementation of Python. So are you saying that you want dynamic untyped stuff or static typed stuff? It's difficult to tell from the description of your efforts. &gt; I think that a Haskell-like language ( perhaps interpreted instead of AOT-compiled ) Isn't it more straight-forward to just make GHCi's REPL compile to `-O2` object code and then load that in? I mean, unless you think inventing a new language is easier. &gt; type-providers could make a lot of these problems mute Can't type-providers easily be made with a bit of template-haskell? I could write something to demonstrate if you'd like. E.g. you write `source "stock.csv"` in your file and it will read a few rows of the file to guess the data types based on what they look like and produce a `Stock` type with a field for each column, with an instance of `FromRecord` from `Data.Csv` and then you can just read a file into a `Vector Stock`.
I was just giving those as examples of the sort of thing one can make precise technical arguments for, but if you are curious about that particular claim, there's a paper by Guy Steele: "Why object-oriented languages need tail calls" that talks about the issue. I found a mirror here: http://www.eighty-twenty.org/index.cgi/tech/oo-tail-calls-20111001.html Also see this post by Dan Doel: https://plus.google.com/+DanDoel/posts/RmoC2dMikxQ Regardless of the outcome, I very much like actual arguments of this sort, rather than more fuzzy handwavy arguments that such and such technology is "more fun" or "simpler". When I see actual technical arguments, I feel like I am getting closer to understanding the nature of software development and what things are important.
&gt; So it is fair to say that I don't buy the "high performance" story that Julia folks are trying to sell, their benchmarks are superficial, and despite its attempt at typing inference, there is still a lot to be desired at eliminating the overhead introduced by dynamic types. High performance compared to python and Matlab are the most important ones. Julia basically aims to be a replacement to Matlab (even though they don't directly state that as a goal.) Yes you can write faster Haskell, and yes you can write way faster C and Fortran, but the main advantage is the ease of writing numerical code. How do you take e to a matrix power? e^A just works, and is designed to work. Matrices are first order citizens in Julia, unlike Haskell where in order to get the same effect you have to use dedicated packages which will NOT play nice with other packages that you might need because of the strict type system, which is what I want to get to: &gt;Julia's type system is too ad-hoc. This is a feature that is very useful in numerical computing. Want to make the same function work for both Ints, BigInts, Floats, Matrices of any of these, hell even Any[] collections? It just works. Don't get me wrong, I love static typing, but Julia is not made for a use case where static typing is particularly useful, and in any case, it's type system is still stronger than in python as you can explicitly define certain things to be of certain types, with types behaving sort of like objects.
Fwiw, I'm developing a library for calling python that works like this: {-# LANGUAGE QuasiQuotes #-} import Python data Matrix square :: Int -&gt; IO Int square = defVV "export = lambda x: x * x" getMat :: [[Int]] -&gt; IO (PyObject Matrix) getMat = defVO [str| import numpy def export(x): return numpy.matrix(x) |] det :: PyObject Matrix -&gt; IO Float det = defOV "from numpy.linalg import det as export" main = do print =&lt;&lt; square 5 -- 25 print =&lt;&lt; det =&lt;&lt; getMat [[1,2],[3,4]] -- -2.0 The interface is end-to-end garbage collected with Foreign Pointers, so you don't have to worry about manually releasing the Matrix python object as you would with existing libraries. Automatic conversion from haskell values to python values is provided by JSON serialization. All the functions are defined in their own lexical scope, so you don't have to worry about complex interactions between multiple functions. And python Exceptions bubble up into haskell Exceptions of the same type, rather than creating looming segfaults. I'd be interested in getting some feedback from other python programmers that are also interested in wrapping their existing python projects in haskell. I'm not ready for an official release yet, since I'm still exploring ways of guaranteeing thread safety and other nicities, but I'd be happy to get anyone set up if they're interested.
I'll talk about your last sentence: 'The Python community could just as "easily" embrace Haskell as embrace Julia.' This is just not true (I wish it was though!). For Python guys Julia is just the same thing without so many objects and with some functions that start with '@'. Functions even have keyword and default arguments, just like in Python! Julia is a lot like Python, I don't need to argue over this. On the other Hand: take 1 $ [(*2), (+7)] &lt;*&gt; [1..5] Haskell has typeclasses, lazy evaluation, immutability, bad unix-hacker-unfriendly words (monad, functor, lens), pattern matching, ... you get the point. Haskell is far too weird for most Python programmers. And they just love mutability, god damnit!
Okay, but that form of composition (basically we're talking about &gt;=&gt; or its indexed equivalent) *is* associative. And the fact that it is associative gives rise to useful laws about how refactoring monodic code works as one would expect.
&gt;Julia is wildly unsuitable for general purpose programming What is about Julia that makes it unsuitable for general purpose programming?
This is the read I wish had back when I was first exposed to Category Theory.
Continuations - I should have thought about them. A construct used to build you're own control flows is much less useful if those control flows blow the stack. I actually think C++ in particular needs a special syntax for tail calls. In a language like scheme it's much more reasonable to expect the programmer to know if a call is really a tail call. In C++ there's destructors. Even in C there's references to local variables that may be passed to the "tail" call so the stack frame can't be recycled. When it's a correctness issue, it makes a lot of sense to tell the compiler the tail-call intent so it can error if it's not a valid tail call. I once commented to that effect [to this Programmers.SE answer](http://programmers.stackexchange.com/a/22095/8709). 
&gt;except that there is no domain specific optimizations on vectors or matrices. Gaining performance relies on calling external BLAS library, or writing explicit loops and explicit indexing in C style with @inbounds annotation that turns of array bounds checking. Julia is not even two years old. Give it time. 0.4 is already addressing many problems. But yes, this is still a huge problem. &gt;(1) expect high performance Just higher performance than Matlab or python with comparably written code, which is all that matters for this use case. Doesn't matter if it is an order of magnitude slower than C as long as Matlab is two. &gt; Julia is a step towards the right direction when compared to Matlab or Python, but still far off. Oh, I agree, but given that a huge amount of numerical code that I have to deal with written in matlab it is still a godsend. 
There is nothing stopping people from doing a little type inference on Matlab or Python programs so that the code can be compiled more efficiently. In some sense, Tracing JIT can already achieve similar result. I'm not saying Julia did something wrong here, but rather it didn't go the extra mile to make sure things are properly done as they should have been done, as years of research have already shown: 1. Mutability is best treated as a different type (such as Ref in ML). Unless you do that you are never going to be able to implement array fusion properly. 2. Stop treating function type as "Function" type, and instead spell out its argument type and return type explicitly. 
Even still, I would appreciate your resume if you would be willing to send it to me. Also, pretty cool that your in Georgia. Glad to know that there are more people with interest in Haskell who live here.
this is just worse is better all over again. you're not wrong, but you will lose. again. some clarification because that sounds a bit dickish: programming in matlab is a pain. not everything can be vectorized, and if it can't, it's slow. practically, for end users, julia is a huge leap forwards. just yesterday i was speaking to someone who writes radiative transfer code. world class scientific code. they don't even use source control. they aren't going to care about holes in the type system. they just want something better than they have now. we've waited for "smart enough" compilers so long that it's a joke, not a research aim...
&gt; Okay, but that form of composition (basically we're talking about &gt;=&gt; or its indexed equivalent) is associative. Is it always, though? Oft times, we make the assertion / assumption that we are working with a monoid / monad / category. We hide some of the "fiddly bits" by just not tracking them in the state, because otherwise some (or at least one) of our operations stops being *a* morphism and turns into *a family* of morphisms. A concrete example doesn't spring to mind, but I can imagine that the properties of a complex loop with the right operations fused into the condition or update might have different properties of the same loop with those operations fused into the repeated section. If our notion of eqaulity "sees" those differences, we don't really have associativity. I think it's a matter of precision vs. abstraction. Matrix chain mutliplication was my earlier example. If all you care about is the value of the final product: multiplication is associative. If, instead, you want to know how many scalar operations would be performed: that "observed" multiplication is not associative.
Just say what you've said. I'd suggest prioritizing: &gt; Implementing my own monad transformers and having them integrate nicely with mtl and others. &gt; Code profiling and management of strict memory constraints with lazy evaluation. &gt; I still sometimes get confused when I run across really bizarre types. If you really want to slam through that facility, try: www.codewars.com/users/tel/authored &gt; Learn Pipes and/or Conduit --- separately One way to think about it is that you're proficient in Haskell itself, but not the broader, more advanced Haskell ecosystem that includes fundeps, mtl, streaming libraries, etc. Is that a big deal? Depends on whether they expect you to hit the ground running. I don't think that's reasonable for most full-time hires onto Haskell projects. They should expect some training time.
I usually let my projects page or github profile stand in for that, I haven't made a CV in a couple years. I can make one if a strictly typical looking CV will help you.
Well this is kind of a circular argument, but not having (&gt;=&gt;) be associative is considered a contract violation of the Monad typeclass. &gt; Instances of Monad should satisfy the following laws: http://hackage.haskell.org/package/base-4.7.0.1/docs/Control-Monad.html#t:Monad
And [my CV](http://savannidgerinel.com/page/name/cv) as well. I'm working on creating a Haskell related business, and I know at least five Haskell developers in my work who would jump if I could guarantee steady Haskell work. Possibly as many as twenty such people regularly attend our meetups. 
&gt; There is nothing stopping people from doing a little type inference on Matlab or Python programs so that the code can be compiled more efficiently. That's not true. The type system of Julia is designed to be easy to optimize. Specifically, collections specify their element type. A lot more effort went into optimizing matlab and python than went into julia, yet they run much slower than julia. That's simply because of a different design.
Is Haskell the best choice for this project or are you just trying to scratch a personal itch? Does the company have any existing code assets that are in use? In my experience, the best approach is to try to stick to as few languages as possible in a small company. I made the mistake of writing a support tool in Scala and now it's unlikely that I will be able to easily hand it off to another developer.
* Create one subreddit *per season*. Hopefully there won't be enough time to accumulate too much detritus. * Make a rule that a top-level comment must be a concrete proposal. * Moderate heavy-handedly. might just work...
Can't we just steal Python's notorious [hashtable algorithm/implementation](http://stackoverflow.com/questions/327311/how-are-pythons-built-in-dictionaries-implemented) and reimplement it in Haskell?
Saw a talk by Aaron Contorer, founder of FPComplete, about getting wider Haskell adoption. He said that companies that they had spoken to found they were attracting higher quality interview candidates as a result of using Haskell.
Yes. Quite circular. The `Monad` typeclass laws also indicate that `return` should be the identity of `(&gt;=&gt;)`. However, there are useful abstractions that do not require an identity of `(&gt;=&gt;)` like the [`Bind`](http://hackage.haskell.org/package/semigroupoids-1.2.2/docs/Data-Functor-Bind.html#t:Bind) typeclass. (A `Bind` is just a semigroup in the category of endofunctors, what's the problem?) Losing associativity does seem like quite a blow. However, I think there's useful math to be done there. A category is an indexed monoid. Monoids some from group theory. Quasigroups, Loops, and Magamas (both unital and non-) also come from group theory and have interesting properties. I think that it's also possible you can apply the same "indexing" transform to Quasigroups, Loops, and Magmas to get interesting near-categories. I feel mildly justified because Semigroupoids (categories without identities; indexed semigroups) have turned out to have interesting properties. I also feel that depending theory (and practical libraries / data structures) around the "simplest" mathematical structures available tends to produce more-widely-applicable theories (which encourages "cross-polination" between disciplines) and more-widely-usable data structures. I got no problem with assuming identities or associativity, when they are necessary for the result you want to show. Heck, assume inverses, compactness, or the continuum hypothesis, if you require them for your interesting result. But, when distilling the essence of something be careful to avoid assumptions you don't need.
This is what they said in 1999. Still hasn't happened. 
&gt; The type system of Julia is designed to be easy to optimize. Specifically, collections specify their element type. Sure, Julia pretty much mandates that argument types to be known in order to do a reasonable job at inference, because its algorithm is based on abstract interpretation. But knowning arguments type, or element type of an array, is not a priori for a more general type inference approach. Haskell 98 requires no such specification and yet is able to figure out array element types just fine. I can't speak much on behalf of Matlab, but type inference for Python [has been explored before](https://github.com/yinwang0/pysonar2), and is pretty accurate I'd say and likely did a better job than Julia's. Whether some Python compiler makes use of this technique is beyond my knowledge though. 
&gt; Julia basically aims to be a replacement to Matlab (even though they don't directly state that as a goal.) They don't? That certainly was how I first heard about Julia. 
Right. At the end of the day, I can write tiny, clear, elegant, simple, clean, code in Julia very quickly and it runs fast. I can do it in a very interactive environment. If I want a package I can install it effortlessly with a command in my editor. Julia is amazing not because of one thing, but many things. In some ways it is the antithesis of Haskell.
I would recommend going through the steps to install Light Table and the Julia plugin Juno. It may be a huge step in the right direction and could totally change your workflow.
Do what SBCL did, and make a decent page for the proposals: http://www.sbcl.org/gsoc2013/ideas/
From my experience, proficiency means usefulness - I suggest you come up with a way to use haskell productively in some of your work. I'm trying to make my portfolio website entirely in haskell, for instance (hxt, blaze, all the goodies :]). But, as for the learnings, check this out: http://dev.stephendiehl.com/hask/
&gt; we're thinking of binding to NAG for numerical analysis at FP Complete. What kind of numerical analysis? Harwell HSL and the libraries on COIN-OR are also very good for some functionality that can overlap with NAG. To the point of this post, they could use better Haskell bindings. I don't see an organized effort to improve package quality and interoperability (edit: in this specific application domain - stackage and similar efforts are interesting but not very useful for me) to anywhere near the same degree as, say, www.juliaopt.org, which could otherwise make Haskell much more attractive to practitioners (and authors of new libraries) in these areas.
Not really. Python's implementation is written in C and introspects into Python objects to handle refcounting/etc, Haskell hashtables need to be walkable by the garbage collector.
I haven't used pandas, but I was curious why you can't use tuples instead of HLists?
Try out ["The Haskell School of Expressions: Learning Functional Programming through Multimedia"](http://www.amazon.com/The-Haskell-School-Expression-Programming/dp/0521644089).
Where Julia shines is if you need to apply a nonstandard numerical operation in order to draw conclusions from that data. If the calculations you need to do are textbook, then there's some well-proven C or Fortran library out there and it's already been wrapped in a convenient Python package. You could do the same thing with Julia or Haskell FFI, but chances are the Python version is already widely used, thoroughly debugged, has more features, etc. On the other hand if you need to start experimenting with your own custom algorithms and you need performance, it's either write in C/C++ (or Fortran, it's not all that bad a choice if all your operations are numerical in nature) yourself, which is unfriendly and unproductive, or use something higher-level. In Python the performance will suck as soon as you need to, say, write a loop when you can't rearrange the algorithm in a vectorized way. Haskell could be made to work well if you're familiar with it, but for most data analysis tasks it's a big initial time investment to get the hang of lazy evaluation, immutability, strict type checking, boxed data structures, etc (and how to work around some of these to optimize performance). In Julia you can smoothly transition from writing high-level Python-like code that might not always perform well, to gradually replace bottlenecks with lower level equivalent refactorings - you can stay within the same language (no need to shift gears from interactive use at the REPL into a compile-test-debug-tweak cycle), but adjusting idioms to reduce memory allocation, switch to in-place operations, devectorize, etc. In the end you can get to good-enough performance and something that might even be worth calling a library and putting up for other people to use, without really having to adjust your workflow at all. It's nothing fundamentally new that you couldn't do in Haskell or OCaml or Common Lisp before, but it is a very attractive package, easy to transition to and be productive quickly without having to relearn how to program. Especially if you were using Python, there is very convenient Python interoperability through PyCall to use any legacy code that you haven't ported over yet.
This thread is a dupe of several older threads on reddit and HN. you can find my remarks there. cheers with the dupe thread :) I appreciate Julia as something people can cargo cult port Matlab code over too. But I dont find that a compelling value prop personally :) (though for certain people or organizations that can be very very compelling I guess)
How are the learning resources with Julia? Since it's geared towards data science, would studying it help me understand the subject better?
New and not too numerous yet, but getting better all the time. There's a bit of a list here http://julialang.org/learning/ And if you ask technical questions on the julia-users mailing list, chances are pretty high you might get an MIT professor to answer back.
Links?
Link to the library?
What reasons do you have to believe that they won't just interview the 5 people whose resumes you give them? It just sounds like they're trying to get you to undermine your own bargaining position.
&gt; But taking a matlab-focused data-science/machine learning book (I assume they exist) and porting all of the exercises to Julia would almost certainly be educational and fun. Or a Python one, or R. There are plenty of these out there.
For sure, but any decent python or R book is going to make heavy use of packages/libraries, which are going to either be much harder to port or trivial (if they've already been ported). Since the Matlab functionality is either proprietary or underdeveloped, the code is more likely to be bare-bones focused on the algorithm, and the syntax translation will be pretty straightforward. (Reading it over, I wish my comment were sarcastic...)
I think this is a timely reminder. There was a related post / thread on this [last week](http://www.reddit.com/r/haskell/comments/2klj3b/functional_programming_and_condescension/). For me, [this comment thread](https://www.reddit.com/r/haskell/comments/2klj3b/functional_programming_and_condescension/clo1pms) was particularly informative. I worry a bit that when these discussions happen it seems like there are more comments trying to analyze the preconditions of condescension or discussing the subjectivity of insults than there are comments that accept that there is a real problem here. We seem to be accumulating anecdotal evidence that there's a problem, and it seems like an area where that kind of evidence is going to be under-reported. If we collectively accept that there's a problem, maybe we shouldn't be so shy about pointing at things that we don't think should be acceptable. And it's not like /r/haskell is immune from this - in the last week, for instance, I've spotted a few comments aimed at beginners that have been pretty toxic / insulting. It's not the norm, but it's still something that happens here.
&gt; Since the Matlab functionality is either proprietary or underdeveloped, the code is more likely to be bare-bones focused on the algorithm, and the syntax translation will be pretty straightforward. I don't think that's as true as you think it is. There are quite a few independently-developed comprehensive Matlab toolboxes out there, many of which are widely used in tutorials and courses. CVX, Yalmip, TFOCS, MPT, Chebfun, just to name a few. But yes, if you're interested in learning the subject more than the existing tools for practicing the subject, you would want to find a tutorial that focuses on the math and aims for from-scratch implementations as opposed to "here's how to do data science using Pandas."
C++, Java, Scala, Rust all have preachers too. I hate C++ a lot more than I should because of the people who program it insisting on X (i.e. OOP, RAII) being the One True Way. Perhaps let's not make people hate us for the same reason.
I'll defer to you on the state of Matlab toolboxes; it's been a while since I've looked.
i trust you to know how to use google just as well as I can :) these threads often totally miss that the various tool chains fundamentally different and equally valid value propositions. There is no Versus, different tools have different stories, and are each amazing for different needs. Some of these needs are Anthropologicaly driven rather than technically driven, but because the users are human, in some sense thats ... valid! matlab: you like paying for your tools, and you have huge piles of matlab code, because you use matlab, and have for a long time! julia: youre working with people who like matlab but you need something faster that your collaborators can read and understand because matlab style algorithms have been seared into their brains. Plus your doing slightly different algorithms that the off the shelf ones. and you're ok with the tooling still being slightly immature etc etc r / python: its not matlab, and for most batteries included use cases, the tools are pretty darn mature. Though just like the others above, when things aren't of the batteries included sort, its still a lot of work. I've been spending a lot of time over the past two years (both too much and not enough), exploring some foundational numerical computing tooling problems. I think Haskell has a very compelling substrate for a nearly magical story for certain key pieces. I will not elaborate more in this current thread and forum at this time, but will be doing quite a bit of writing + releasing libraries in this domain once my job interviewing stuff this month finishes up and I've a more stable income situation. Some of my stuff has been delayed wrt release for nearly a year because of some work instability putting a lot of this stuff on the back burner, but that soon will change. Though at this point I need to view my numerical computing work as a labor of love rather than something that has any chance of supporting my work on it. (engineering time is shockingly expensive ya know!)
It's like learning to code all over again! 
Because Julia said it was MATLAB but fast.
I'm sorry if I was unclear. I was trying to refer to the purely theoretical discussions that don't get into the details of a particular complaint at all - it just seems to me that it effects the signal to noise ratio in a pretty horrible way. It's possible that "listen and believe" isn't the right default stance, but I'd argue that "listen" is a good first step, and that some parts of the Haskell community aren't doing as well as they could on that front. Most of the complaints that I've seen have been subjective and emotional - but they're still valid and real to the person who's voicing them. If the default community stance is that your feelings aren't valid until you've run some gauntlet of logic to defend them, that seem pretty crappy to me - surely we can do better collectively. From what I've seen, I've formed the opinion that there is a real problem here. Maybe I've just happened across a lot of anecdotal evidence. It's also possible that I kind of quietly assume other people have access to a similar volume of anecdotes. That's probably what gets me a little fired up - because it implies that the basis for some of these meta-discussions is "lets assume that all of the strong, negative emotions of these folks are invalid or questionable" and then goes from there. Of course, I've now started and engaged in a pretty theoretical meta-discussion :) If that's hypocritical or is adding noise to the conversation then I'm sorry. I guess I've been watching this particular space and brewing on it for a while.
&gt; Julia basically aims to be a replacement to Matlab (even though they don't directly state that as a goal.) Don't forget parallelism is also a large part of this ideal, not just raw numerical performance.
Oh c'mon! Many sorts of studies require so many controls to do in the same fashion as one would in a controlled lab setting that it would be prohibitively expensive and impractical. So we make do with the well-known use of "natural experiments" instead. This is the case throughout _nearly every_ field in the social sciences, except for very minimal silly results in psychology which turn out to be unreliable too because of their small sample size and lack of control. So given the choice between small sample sizes over a tight span of time and wide-scale longitudinal observation, I'd pick the latter. But luckily, I don't have to pick! We have researchers working both ends -- all the better. And as a (sometimes, sorta) good bayesian, I tend to think that we can take all these partial results and their partial information and smush them together to get a slightly better handle overall on all the complex things going on. Studies like this are only part of the picture, sure. But as the other reply to you observes, I don't believe they are useless or worse yet negative. In particular, you're making a sweeping statement, but I believe, despite its flaws, that the very study you're commenting on is actually quite honest and useful! So I think we already have a counterexample :-P
I think your post is rude. I'd say the OP has done more than 'read and understood LYAH'. They may be a beginner (and everyone's definitions of such a word will be different in regards to this topic), but it looks like the OP has worked hard to learn Haskell. Let's not insult them.
This is a completely unhelpful answer, but if you're looking to get to know the things you listed under not comfortable, there is * [Parallel and Concurrent Programming in Haskell](http://chimera.labs.oreilly.com/books/1230000000929/index.html) * [Purely Functional Data Structures](http://www.amazon.com/Purely-Functional-Structures-Chris-Okasaki/dp/0521663504/) * [Tracing the Compilation of Hello Factorial](http://blog.ezyang.com/2011/04/tracing-the-compilation-of-hello-factorial/)
Thanks, I actually own the first book you list! I just haven't had time to get through it yet. All my free time recently has been spent doing practice interview questions on a whiteboard in C. I'll probably order 'Purely Functional Data Structures' right now, because I've heard good things from several people.
You may want to try Judy as it already implements much of what you've mentioned. http://hackage.haskell.org/package/judy-0.2.3/docs/Data-Judy.html
http://stewart.guru/s/json-python-0.3.0.1.tar.gz
What evidence do you have for that inference being better than Julia's?
As someone who hires Haskell programmers, I'm interested in the following things: - evidence of working code in Haskell (github/darcs) - evidence of ability to deliver completed tasks (e.g. made it to Hackage, announced, has a web page, packaged in distros). - evidence of ability to design and implement medium-sized projects (&gt;1..5k lines of Haskell) - evidence of ability to integrate many different components into a system (network, db, protocol design, FFI, concurrency, web, GUI, analytics). - working knowledge of operational behavior of GHC runtime/ability to reason about performance/ability to think analytically about performance issues - advanced skills: completed a FFI binding, completed and maintained several different libraries that have active use, built a network service, built a DB binding, built a DSL, submitted patches to GHC, wrote a system in Agda/Idris/Coq and Haskell The biggest filter for most Haskellers is lack of delivery of significant projects, and lack of experience designing systems from the ground up in a functional way. A few modules here and there aren't the same as designing an app in the 1k to 5k LOC range, implementing it, and completing the work. Maybe &lt;10% of the candidates pass these initial filters. The more of these you can point to, the stronger you will appear.
If it is useful, I advertised on twitter and reddit a few months ago for a London role using Haskell and had 25 resumes in about 3 days. Of those, 5 had PhDs in CS, 15 had masters degrees in CS. When I advertised in NY I had about 10 resumes over a week. That was a few years ago. We had similar experiences in Portland, OR &gt;5 years ago. I think there are more Haskellers now, and getting 10-30 applicants for a good role in a good location is reasonable.
Well, because Haskell is so intertwined with CS research, you have to fix the problem that CS has with women - namely, the community fosters misogyny.
Where are you from? The Netherlands had a whole bunch of women in CS.
http://www.lambdaladies.com/
&gt; the community fosters misogyny. [citation needed]
From where I'm standing, the community tries very hard to increase fairness. You may argue that it's not yet trying hard enough, consistently enough and I'd be inclined to agree. But "foster", in the sense of actively supporting, is a word that doesn't fit the situation that I see.
http://scienceblogs.com/interactions/2007/09/29/if-you-thought-physics-was-mis/ http://www.nytimes.com/2014/04/06/technology/technologys-man-problem.html?_r=0 http://femalecomputerscientist.blogspot.com.au/2013/03/terrifying-escalating-sexism.html http://geekfeminism.wikia.com/wiki/Computer_Science http://geekfeminism.wikia.com/wiki/Timeline_of_incidents
They may say they want to include more women, but usually this means "we want more women to fit into our culture" not "we want to change our culture so women will feel welcome"
Rails Girls' target population is huge - people with little or no prior programming experience who are interested in web development - and already motivated, and so it's not unexpected (though still laudable) that they can have events in many cities and attract enough women to them. How would Haskell do that? Maybe trying is the best idea, but a priori it seems Haskell Girls would have a tougher time...
&gt; Alternatively, what do people think such a project should look like? Conference organizers may hire consultants who can help them draft policies to make the events more inclusive not just for women but, more generally, minorities. [Strangeloop](https://thestrangeloop.com/news/doubling-down-on-diversity) has been [quite proactive](http://bridgetconsulting.com/?p=131) in that area with a measurable positive outcome. Another example: [!!Con](http://bangbangcon.com/) decided to have [live captioning](http://composition.al/blog/2014/05/31/your-next-conference-should-have-real-time-captioning/) of the talks which turned out to be amazing for people with an hearing impairment but also non-native speakers.
It's less about the compiler and more about being careful with the design of the language and standard library. You basically make a lot of small compromises – using machine arithmetic, immutable type definitions sans inheritance, type stability in library functions, allowing mutation, etc. – that allow Julia to be fairly easily mapped to efficient machine instructions. That's why you can't just rub some LLVM on Python, Haskell, whatever and get the same results. Dynamic typing is also nice for the interactive experience and for reducing boilerplate – you get something a lot like type classes with more flexibility and less cognitive overhead, albeit without static typing to look after you. There's no way I'm going to convince a bunch of Haskellers that those tradeoffs are OK, but there's clearly a good set of people who don't mind them.
Unless you are seeing this behaviour from a significant number of Haskell-related commentators I suggest you take this up privately with the individual or individuals concerned. Admonishing all of us is likely to be counterproductive.
I've seen most of the links you provided. Most of the incidents I spotted seem to come out of the open source community, dev conferences. This is *not* CS research (which I have been talking about). To be clear: that plenty of people in *technology* deny that there's a problem is fact. I never disagreed to that statement. I just have the opinion that CS research is somewhat better, as stated above. I'll keep a geekfeminism tab open. I feel like learning some feminism as a man is obligatory if you want to avoid being a dorky bull in a haskell shop.
[This](http://pchiusano.github.io/2014-10-11/defensive-writing) is a pretty good post by Paul Chiusano on issues of common sense, charitable audiences, and defensive writing but the lessons in general apply across the board about communication on the internet.
Agreed. I think Paul hit on something very important in that post.
Show them this: https://www.fpcomplete.com/business/blog/utilizing-haskell-fynder/ I'm currently putting together some answers for a more detailed question set. If you'd like me to share my experience directly with your employers at any point fee free to get in touch.
&gt; concerted efforts to try to ameliorate this situation Your assumption that the group of people using a certain programming language should mirror the composition of the general population is wrong.
The real premise is that there is nothing gender-specific about Haskell the language. Thus, there must be some other reason for the significant gender bias in the Haskell community. I'll let you come up with your own reasons for what those are, but I can think of a few that aren't too great.
I've always wondered how Goguen's work relates to more mainstream FP, but don't have the maths chops to answer that question myself. His [tossing algebraic flowers down the great divide](http://lmgtfy.com/?q=tossing+algebraic+flowers+down+the+great+divide) answers some of those questions and provides an overview, but generates even more questions for me.
All these incidents make me wonder. Isn't the Haskell community supposed to be known for being super-helpful and newbie friendly? It makes me very sad to see that this is changing. What happened? Did the people who were helpful and friendly switch to being arrogant for some yet-unknown reason? Did the community grow to include arrogant people? Or perhaps it's simplistic to divide people according to whether they are intrinsically arrogant or not. Did the circumstances change? I like tomejaguar's hypothesis about the twitter format, but that doesn't explain the /r/haskell comments and real-life interactions which others said they noticed.
It's not “wrong”, any more than it's “right” to assume a programming language *shouldn't* mirror the composition of the general population. I'd rephrase your comment as “I disagree that we should want more women in Haskell”, because that's the essence.
Outright lies? See, this is an example: It denies it has a problem.
&gt; Isn't the Haskell community supposed to be known for being super-helpful and newbie friendly? That's always been my experience. &gt; It makes me very sad to see that this is changing. Me too. 
You can only expect a subgroup to mirror the properties of the group when the selection was random. Any [sampling bias](http://en.wikipedia.org/wiki/Sampling_bias), like the propensity to use a rather academic functional programming language, and you give up any expectation of representativity for the general population. Let's take an example less likely to distract us from logic: would you expect the hair color distribution among My Little Pony fans to be representative for the entire human population? If you find differences, would you campaign to erase them by actively encouraging blond people to watch the show?
&gt; I want more women in haskell. Why do you want more women in Haskell? 
A wild guess: because people want to get stuff done without the type-system getting in the way too much. Like, "what is the result of this calculation if I increase elements of array X at indices 1, 7, 11, 16 and 28 by 10%?" Take a, say, biologist who uses a computer as a *tool* [i.e., is not interested in type theory at all], and demonstrate the workflow and amount of black-magic incantations you have to memorize to get this done in Julia and Haskell. Ask him which he prefers. I have tried to use mutable vectors and matrices in Haskell, and it wasn't a pleasant experience. The attempt itself convinced me that monadic jails aren't a user-friendly way for isolating effects.
It's not only possible but the the most obvious reason. Nobody is actively stopping them. There're no gender restrictions on Haskell books or `ghc` or `cabal install`. The question raised in discussions like this tends to be “why wouldn't they want to?”
yeah, like for instance because they'd be the only woman at the hundred-person community events?
Yeah, people don't naturally communicate in soundbites. You can't fill in the necessary elaboration, so you cut your message short and then shift burden onto your audience to fill in the gaps. If they can't fill in the gaps, _they're clearly not smart people_. If they disagree and won't fill in the gaps, _they're clearly not smart people_. I saw that kind of exchange on Twitter at all the time. It's an awful medium for conversation. Before I stopped using it, anyway. Now I just use my account for announcing project releases. :-)
&gt; Is there any reason to expect the hair colour distribution to be different? sampling bias
&gt; It makes me very sad to see that this is changing. What happened? The few contentious make a louder sound than the many happily getting along.
I doubt there is much that can be done other than wait. A lack of diversity is going to be one of the problems with being small. So long as the community is inclusive, there isn't much more that we could do other than to try and grow the community itself.
Awesome. Good luck getting 5 resumes in Glasgow. 
Very good point. Other than arrays of structures, multi dimensional arrays are also difficult to do in a statically typed setting. Do you track the number of dimensions at the type level? Then you quickly run into cases where you need dependent types. If you don't track them at the type level you have to simulate some kind of dynamic typing. You also want to generate specialized code for 1-d, 2-d, 3-d, etc arrays. The Julia compiler does this, but GHC doesn't, and it would need a totally different compilation strategy to do that. I think eventually a statically typed solution is better, but we currently don't really know how to do that. Perhaps when dependent types are more production ready, that will win eventually. A few improvements are necessary: 1. A type specialized compilation strategy like MLton, rather than a type erasure compilation strategy like GHC. 2. Fallback to run-time checking if the compiler can't check the dependent types at compile time. E.g. if you have a `Vec (n+m)` and you need a `Vec (m+n)` the compiler shouldn't reject your program until you provide a proof that `n+m = m+n`, but it should insert a run-time check that `n+m = m+n`. 3. Better type inference for dependent types. If you're allowed to do I/O at compile time, then dependent types also subsume type providers.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Law of total expectation**](https://en.wikipedia.org/wiki/Law%20of%20total%20expectation): [](#sfw) --- &gt; &gt;The proposition in [probability theory](https://en.wikipedia.org/wiki/Probability_theory) known as the __law of total expectation__, the __law of iterated expectations__, the __tower rule__, the __smoothing theorem__, __Adam's Law__ among other names, states that if *X* is an integrable [random variable](https://en.wikipedia.org/wiki/Random_variable) (i.e., a random variable satisfying E( | *X* | ) &lt; ∞) and *Y* is any random variable, not necessarily integrable, on the same [probability space](https://en.wikipedia.org/wiki/Probability_space), then &gt;&gt; &gt;i.e., the [expected value](https://en.wikipedia.org/wiki/Expected_value) of the conditional expected value of *X* given *Y* is the same as the expected value of *X*. &gt; --- ^Interesting: [^Law ^of ^total ^variance](https://en.wikipedia.org/wiki/Law_of_total_variance) ^| [^Expected ^value](https://en.wikipedia.org/wiki/Expected_value) ^| [^Law ^of ^total ^probability](https://en.wikipedia.org/wiki/Law_of_total_probability) ^| [^Law ^of ^total ^cumulance](https://en.wikipedia.org/wiki/Law_of_total_cumulance) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cluoapl) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cluoapl)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
&gt; On average What exactly are you averaging? There is only one sampling done here.
 You could find many haskellers in hackerrank, in the last lambda-calculi challenge I was the only Lisp hacker in the top 20, mostly others were using Haskell and Scala. Lambda-calculi is restricted to only programs in Haskell, Scala, Clojure, Lisp and Rackets (out out my head). By the way, they are looking for people to submit new problems, is not so easy to design an interesting challenge. There is a paid version of Hackerrank for enterprises, you could see the participants ranked by language, nationality, score and so on. 
Hey Jens, Can it handle multiple versions of the same package, or is it an inherent limitation due to rpm? Consider the scenario where the build of A depends on B and C but each of B and C internally depend on conflicting versions of D, AFAIK cabal can deal with it, but rpmbuild cannot because it would need two versions of ghc-D-devel RPM installed via rpm. I wonder if there's a way around it, because I would like to be able to deploy distribution builds of packages that have a rather complex dependency tree.
That's a great post! 
Pandas allows you to do things like this. Given a CSV file: height, weight, date of birth, income 189, 76, 1978-08-16, 42000 ... Create a table with horizontally age brackets, vertically income brackets, and in each cell the average BMI = weight/height^2 of the people in that age/income bracket. It has features for automatically determining the data type from the data in the CSV file, features for grouping/aggregation by a column, plotting that data, efficient representation of huge data sets, etc. The primary use case is a statistician who has a bunch of data files on his/her disk, and he/she wants to explore that data interactively. I don't see an easy way to fit this into the Haskell type system, but if you have any ideas I'm interested.
&gt; Studies like this are only part of the picture, sure. A prediction which does no better than random guessing is not "part of the picture". It adds precisely zero bits of information! :) I think the onus is on you (or anyone else who wishes to defend this research) to establish that the study has enough signal that we have actually learned anything from its conclusions. &gt; And as a (sometimes, sorta) good bayesian, I tend to think that we can take all these partial results and their partial information and smush them together to get a slightly better handle overall on all the complex things going on. I think you might be able to turn this into an actual formal argument. If you are interested, I encourage you to try. Formalizing the argument will also make it clear what assumptions go into it and we can determine whether those assumptions hold in this particular case. Right now, it's too handwavy for me. :)
Its called haskell. 
but maybe we should try to have more female speakers in our conferences. that is something that can be influenced.
"the "different availability of aptitude at the high end" hypothesis: that women are underrepresented because they have, on the whole, less intrinsic aptitude in science and engineering, and further that a small difference in aptitude would make a large difference in the overall numbers of women performing at a high enough level to assume a tenured position;" you don't think that's sexist?
As a newbie myself, I've made this observation: From the outside looking in, the Haskell community is intimidating. Once you dive in and start participating, it's very helpful. I think part of this is due to the feeling like you've attained super powers that comes with understanding of concepts like monads or applicatives. &lt;omitting spiderman reference&gt;
This is all doable in user land quite trivially. See my numerical library on github for what a multi dimensional indexing api might look like. Github.com/wellposed/numerical and bit.ly/prealphadocs for a relatively recent snapshot of the docs One problem in numpy and julia etc is sparse matrices are second class. My stuff doesn't have that problem [edit:fixed bitly link]
I don't like communicating via walls of text either, though. Twitter helps with the misguided monologue problem.
More pointedly: "General population" here is programmers in general, not the entire human race. We start from a population that is not 50/50 then take an extremely small portion of that population. And yes, the "propensity to use a rather academic functional programming language" skews the numbers further as there's a known discrepancy between men and women in advanced math/science/engineering academics. Still, even given this skewed general population, it would be a good indicator of a welcoming community if the Haskell community included more women. Furthermore, diversity in any community is *generally* favorable. 
From the paper: "we recode the model as above using treatment coding and observe that the Functional-Static-Strong-Managed language class is significantly less defect prone than the Functional-Dynamic--Strong-Managed language class with p = 0.034" Recall that any `p &lt; 0.05` in most fields of research is considered at the level of statistical significance. There's lots of critiques of p-values, but as the current scientific standard of the existence of "enough signal" to measure a non-null effect, it is what we have, and this study provides it. The ANOVA analysis unfortunately shows that measuring the size of the effect is hard because it is drowned out in by the number of commits -- ideally another study would attempt to control for that before running further analysis, in order to let the other components of the signal in more clearly. With regards to the smushing different results together approach, here's a reference: http://en.wikipedia.org/wiki/Meta-analysis That article also goes through the associated pitfalls. But again, this is how real fields that cannot do lab-only experiments do real studies, warts and all. It might not be as rigorous as you like, but its sure science!
Your sampling is not random and you only have one bias. There's nothing to be averaged out.
I think this is an issue, and I think we will have to start naming names. 
Not to pick on people I don't know, but this is a good example here to make a point with. Now lots of men have kids too. In fact, very often (though of course there are many times this is not true), if a woman has a kid, there is also a man who has that kid. And roughly, as a whole, there are in fact as many women who have kids as men who have kids in society. However, for example, married men who have kids often can go out to meetups, because it is expected their _wife_ will take care of the kids. I'm not saying this is something that _we_ can do anything about soon (although larger academic conferences etc. often offer childcare, which _does_ make a world of difference), but it is certainly a pressure we can recognize -- that despite women and men both having kids, the impact of kids on the time of women, in general, is often different than the impact of those kids on the time of men, due to many structures that are generally, but not always, in place in broader society.
&gt; "General population" here is programmers in general, not the entire human race It's "human species" and no, we're not talking about sex/gender/hair color in Haskell programmers vs. all programmers. Even if we would, the sampling bias argument still stands - Haskell programmers are not representative for all programmers.
I think we are in agreement that empirical study can in principle help us understand under what circumstances and to what extent theoretical advantages actually pay out in practice. With one caveat. I don't need studies to confirm my direct observation, just like I don't need an empirical study to tell me what color an apple is--the apple is whatever color I perceive it to be. For instance, I can directly observe how much time I spend working around the fact that a language doesn't let me define/use Applicative, Monad, etc. (Something I've done recently in using Elm.) And I feel quite comfortable generalizing my experience to other developers who are familiar with the same concepts that I am. Same goes for other language features (like proper tail calls). Of course, I can't speak for all developers, but that question is much less interesting to me. I usually don't care about what's better "on average", I care about what's better _for me_ and people with similar experience. :) Maybe if I were hiring an army of 1000 developers I would look to empirical studies for guidance on how to maximize productivity. But if I am hiring &lt; 10 developers, and I get to pick them, these empirical studies aren't likely to be valuable (even if they are done well). &gt; Or, and this is insane, it may be the case that code duplication turns out not to be harmful at all in certain cases! Someone once made the point to me that for teams, eliminating code duplication can result in less concurrency in development, which can mean lower overall velocity. That is, if we have two subteams, A, and B, which have a need for common functionality, L, it may be better to just allow A and B to solve problem L independently, rather than having them coordinate to develop a shared library L. The reason being that the communication has overhead and it sequentializes the work more. This is analogous to parallel/distributed algorithms, where a parallel algorithm may do more total work (some work is duplicated, or not solved as efficiently as it could be), but with greater parallelism, so the runtime is ultimately less. Related to this, I've heard the claim that technical debt in shared *libraries* is much worse than technical debt in individual services / apps. It can be better to avoid developing and relying on a bad shared library and instead wait until the problem can be solved well, with minimal debt, at which point the library becomes an asset that makes future work easier. In any case, these insights are useful, and more importantly, they give me a framework by which to make decisions. In fact, I consider getting to these sorts of insights to be pretty much the goal of any empirical work. Me knowing that "on average, code duplication is okay" (whatever that means exactly) is not all that useful. I don't want a black box that says "trust me", I need to know, in my particular scenario, what makes the most sense. And that requires having a more analytic model, not just empirical data.
&gt;Since you seem to be "in the loop", what do you see as a viable way forwards? Assuming that someone had time to put into this, what steps would you recommend taking to get the Haskell code to perform as well as the Python code? IntMap should get closer for this problem, or you could write a hash table implementation with Storable keys and values. The biggest issues are indirections from boxing values and walking heap in the GC. &gt;Could we use an implementation written in C to make all of this faster? Not really, if you want to store Haskell objects in the hash table then their liveness needs to be visible to the garbage collector. To improve much here you'd need to rework the GHC runtime. &gt;Would that be slower because the FFI would have to pin objects (probably via `StablePtr`)? Is that worthwhile, and are there any existing C libraries that have performance better than `hashtables`? StablePtr won't help you here, it's implemented using a hash table....... In general we will never beat C for this particular usage pattern, as long as we are storing boxed objects. You pay a GC cost of `O(|live_set|)` at every major GC but the C or python versions don't pay any ongoing cost for values already inserted into the table. When the types in question are primitive like `int` then you don't pay to allocate at all. 
Here is my problem with this. First off, there are reported incidences of sexism, both large and small all over stem http://stemfeminist.com/. These are bad. But that doesn't mean that the representation of women in STEM as a whole is as bad as it is in computer science. Nor does the representation in computer science look as bad as the representation in PL research. Nor does the representation in PL research look as bad as the representation at Haskell events, etc. And I simply refuse to believe that the social problems get worse at each step here. So yes sexist behaviour is bad, and we should try to prevent it, end it, oppose it, etc. However even if we were much better at this, it would still not be sufficient to really change the composition of our community. And that's actually frustrating! Because if we could just find the bad actors and stop them, and that would fix things, then that would be great! At least we'd have a clear plan of action. As it stands, I'm all for codes of conduct, etc. But that clearly is only the beginning. The key, I think, is making Haskell a more open and accessible community and language, in general, for everyone. This is because if we do broad outreach to lots of newer people, then odds are, those newer people will be more diverse than our current composition (in fact its hard to see how they couldn't be!). As we do such outreach, then targeted outreach as a component of that becomes significantly less weird and strange. When we do have conferences and larger events, as we grow, we will be able to get more sponsors. And one thing you can do when you have more sponsors and money at your disposal is follow suit with what StrangeLoop did, and offer diversity scholarships, which are a great way of helping people that you would like to encourage to participate to be able to actually do so. I think things like this, over a long enough span of time, can eventually turn things around. I agree that there must be some desire to do these things, or it won't happen. But to me the "cultural problem" is really more along those lines than anything else. Again, this is not to speak out against policies, codes of conduct, trying to set a standard where people feel more comfortable even when they are one of the only people from X or Y group in a room (so ultimately they stick around and then the next people feel less alone, etc.). But it is to suggest that those don't mean anything unless coupled with outreach way beyond that.
Clearly you have enough time to be insulting, if not constructive. If you can't be constructive, then don't post anything. If you can't handle that, then perhaps you should find another community: http://www.reddit.com/r/haskell/comments/2lfofw/quick_reminder_to_be_courteous/
And ironically the shorter version is more poignant due to the removal of some superfluous words... :)
&gt; I don't want a black box that says "trust me", I need to know, in my particular scenario, what makes the most sense. And that requires having a more analytic model, not just empirical data. Right, there's an old joke about an insurance claims adjuster who lives in Kansas because on average people there live longer, etc. Clearly numbers without insight only take us so far, and numbers can be used for many different purposes. But again I just think that because a study isn't telling you _all_ the things you want doesn't mean its not an interesting study. It just means science is hard :-P
I don't care about this kind of "representation" problems (sex, ethnicity, sexual orientation, etc.). There is nothing restricting people to access the Haskell community, and this is generally true for a lot of field in the computer science domain. If people want to be part of the community, well, welcome. If they are not interested, nobody force them to be part of it. People usually don't know and don't care about if they discuss with a male or female, with an homosexual or heterosexual person. I work in computer science and I have female friends who are really good, far better than me for some, and male friends who are worse than them. To be clear, I don't say there is no problem regarding to equality in the society, but that the bottleneck is usually not in computer science communities, but far before in the society/eduction/etc. So trying to correct those inequalities at computer science level is useless. And seeing always people pointing those differences and asking how we could correct them starts to annoy me. If you want to solve it, it's a political/educational/societal question, not a computer science communities one.
providing a creché / childcare is a great way to attract more parents to events, especially mothers. we've had great success with this continuously. of course it means that either at least two people will have to look after the kids rather than participate in the entire of the event (you can swap each meet up or presentation) or professional childcare will have to be paid for. another option might me to provide money to parents to babysitters.
That doesn't sound like a good suggestion, nor a suggestion anyone in this thread (or in the world?) would seriously make. I don't know why you would raise it in the context of this discussion, except to indicate that you don't want to take the discussion seriously. And if you don't want to take it seriously, perhaps just don't participate?
[This](http://www.youtube.com/watch?v=jVoFws7rp88&amp;list=UU_QIfHvN9auy2CoOdSfMWDw) is one the most condescending toned talks out of strange loop this year, and it came from a Rust guy. The thing that irritate me is that it was amusing and informative as well
aren't communities social constructs? can't we challenge this issue inside the haskell community? wouldn't it be very beneficial to haskell both as a language and community to have a higher number of users and participants who are women than any other language or community?
I do not think that is it sexist to state an observed correlation between your generic makeup and aspects of any of your mental or physical facets. There's a lot of heritable traits. I do think it is sexist to assume an individual woman matches some average profile, even if that profile is data-driven; the ways in which we are difference from our demographic profile strike me as more important than how we follow our demographic profile. We should reserve judgement *at least* to the point we are pre-judging others. However, once we start considering populations "in the large", like percentage of women attendees of ACM conferences, we would expect our data-driven profile to apply. If it does not, it is clear the profile needs to be updated. (No data is completely sacred, and sociological profiles tend to drift, at least some, over time.) On the other hand, I think the modern data (since ~1920) on the aptitude of women toward STEM subjects has been biased due a pervasive cultural effect that starts remarkably young and discourages girls from "un-ladylike" interests, so that much of the "genetic" effects are actually cultural. I think that it will be difficult for ACM members to counter-act this effect on the female population of conferences in the current generation. Still, making STEM welcoming to women (of all ages) is a laudable goal, not just in conferences and official outreach programs, but we also have to start "at home" with our daughters, nieces, and grand daughters.
Both tuples and HLists are really slow inside of inner loops. What you really want is an unboxed tuple of unboxed arguments to get the equivalent machine representation of the data structure... but you can't (easily) do this in ghci.
I have not read what you are referring to, but that sampling argument makes little sense. A different mean isn't a requirement for that sort of hypothesis, as differences in _variance_ can have a much larger effect. Higher variance can be caused by any number of factors such as willingness to take risk, be an outsider, not have a family etc. No matter what, the most important thing to remember is that statistical distributions don't apply to individuals.
As a graduate student in biochemistry I did use Haskell as a tool (both for myself and others). Haskell is wonderful because you catch data quality issues that you would have otherwise missed if you had used a more lenient tool. For a researcher, data correctness is extremely important: garbage in, garbage out.
Interesting. The docs link doesn't work for me... Do you have a list of examples somewhere? For example a function for summing over the i-th dimension of an array, which turns an n-dimensional array into a (n-1)-dimensional array? What is second class about sparse matrices?
You have the same problem in the medical field, actually, if not worse. And that better be considered science in some regard, or we're _really_ in trouble. I mean you're right about causality and confounders. But you know what your objections are called? "subjects of future research!" The problem is not having studies like this. The problem is not that they raise as many questions as they answer (good science should). The problem is just trying to treat the bulk of them as being sufficient in themselves for important insights, when they're really scientific "work product" on the way towards those better ultimate results. But again, that's how empirical science works, in general. I mean if you look closely the supposition and statistics in a whole bunch of intermediate research in experimental physics, you get the same problems. Ditto bio, etc. And even when things look right and look like the right methods have been applied and aren't plagerized and aren't falsified, they can still be wrong and retracted! (Follow http://www.retractionwatch.com/ for a while and you'll see that no field is at all immune to this stuff). I mean its nice you want to hold things to a certain notion of a high standard you think exists in "real science". But that "real science" seems to me to be what mathematicians call "a story we tell to children". Real science is partial, continually in progress and churn, and messy as heck.
You think women do not wander into haskell community because there's something we can fix from inside? :)) Here's my uneducated and ignorant guess on the cause of the gender bias. I think programming in general is anti-social behavior. It's like leaving society, going somewhere into a cave in the mountains, turning your back to the light and staring into a wall...for years! Something i have no problems doing. But i think women are much more social creatures than men. I do not think they would want to spend their lives like this. That's why women flock to professions that are heavily based on human interactions. Medical field, legal field, education, managerial positions, show business, fashion, music, dance, pop culture, food, sales etc. Programmers are modern day hermits. And no woman wants to be a hermit. 
I recently went to a girls raising event for women entrepreneurs with my SO. I was pretty much the only guy there out of about 50. As someone who doesn't like attention, being the only guy in the room felt uneasy for some reason. So I can definitely see it from their perspective. Although I imagine that unease still wouldn't be as bad as a woman in a male dominated, highly technical field where everyone is unconsciously judging your competence[1]. [1] https://www.youtube.com/watch?v=nLjFTHTgEVU
How do you know this? Isn't it at least conceivable that there are structural reasons that keep (more) women from joining the programming/haskell community? The fact that you and I don't know about them doesn't necessarily mean they are not there. I think this needs to at least be properly investigated, and if it turns out that, no, there are indeed no structural hurdles, then I agree that we can stop talking about it. 
You're saying that you don't necessarily want a woman Haskell meet-up with posters of things like pink lambda symbols? Huh, figure that.
Okay, I think I understand all your arguments at this point. I think there are some essential disagreements still, but I don't want to keep trying to unpack them here. &gt; You have the same problem in the medical field, actually, if not worse. And that better be considered science in some regard, or we're really in trouble. On that note, I have a book recommendation for you: [Good Calories, Bad Calories](http://www.amazon.com/Good-Calories-Bad-Gary-Taubes-ebook/dp/B000UZNSC2/ref=sr_1_1?ie=UTF8&amp;qid=1415291439&amp;sr=8-1&amp;keywords=good+calories+bad+calories). You can safely ignore most of Taubes' claims about carbs being evil. It's worth reading just for his history of nutrition "science" in the US and how our society came to decide what is or is not healthy. I found it both fascinating and appalling.
Note - put backticks around code: `code * 10` or else it treats it as the markdown italic command. 
&gt; Fallback to run-time checking if the compiler can't check the dependent types at compile time. E.g. if you have a Vec (n+m) and you need a Vec (m+n) the compiler shouldn't reject your program until you provide a proof that n+m = m+n, but it should insert a run-time check that n+m = m+n. In general, this could probably be done for non-dependent types, too. It's a form of gradual typing and can share a lot of practice with some of the sophisticated contract systems with blame. For code where you don't have test or requirements other than: it does "what I want" on this test data, there seems to be more value in getting a testable binary then applying static type checking rules. I tend to engineer program in terms of libraries, so types are *soooper* valuable to me from the beginning, but that's not true of all developers or all projects. There could always be compiler options or pragmas to warn or error on turning statically-checked types into run-time-enforced contracts.
&gt; One problem in numpy and julia etc is sparse matrices are second class. How so? Sparse matrices in Julia are just the same as any other type, no more or less a first class citizen.
You can view the problem through another angle, like viewing the difference between the full product of the list and what you want to obtain. This would yield to a much simpler solution using only a `map` operation to transform the input in the end.
Somewhat related, I recently wrote some stuff about how to be civil in technical discussions: http://pchiusano.github.io/2014-11-03/labels.html Obviously, direct insults are bad, but more generally, using labels that carry baggage tends to derail discussions, even when these labels aren't applied directly to the other person.
I'm not on twitter, and I don't really see these ugly exchanges you're talking about. Is it helpful to be posting this here? I.e. are the twitter Haskell people also reading /r/Haskell? Maybe it would be best to call people out on twitter publicly and directly?
Apples to oranges comparison if I ever saw one. 
No, he's right, even if he was rude. Haskellnoob is a weird sort. Taking her claim that she's a woman at face value, you'll notice that she only brings it up to establish her authority on this matter, which she then conveniently uses to validate every geek meme on this issue. And because she reinforces problematic but entrenched positions -- that it's not a problem that there are few women in tech, that it's a non-issue, that it's actually more sexist to try to include women, or whatever -- she acts in a manner that sets everyone, but particularly women, back. In return, she gets personal validation from telling men what they largely want to hear. This sort of internalized misogynistic thinking in women is so pervasive that it has a name: special snowflake syndrome. Women are told all their lives that being a woman is bad. Is it any wonder then that they learn to disassociate themselves from other women? That they take pride in being the "girl that's not like other girls?" How many times have you heard an adolescent boy say "I'm not like other boys" or "I'm one of the girls" with pride? It's business as usual with adolescent girls though. Particularly in tech, we see this a lot. There are a hundred men at a conference and only one woman. Someone comments on this disparity. The one woman says, "it's not your fault there aren't more women -- it's women's fault for not trying hard enough." And the men get to feel relieved that they aren't really sexist, and the woman gets to feel special for being the only one who made the cut. It's bullshit. There is an obvious and disturbing disparity in CS. I came from math, and while it's not 50/50 there either, it is way, way closer to parity. CS isn't rocket science, despite what many nerds seem to want to think. Anyone who can pass undergraduate topology should be able to rock Haskell. The problem isn't lack of intelligence or any other psychometric bullshit. The problem is culture. Our culture. It sucks. It needs desperately to be opened up.
&gt; I think programming in general is anti-social behavior. Not anti-social, asocial. Anti-social behaviors are those that harm society, asocial behaviors are those that do not engage society. &gt; Programmers are modern day hermits. That is not true at all. Many programmers definitely engage in social behaviors, either in other aspects in their lives or engaging in social programming.
A data point. @HaskellTips followers are 97% men and 3% women according to twitter. I don't know how representative that is though.
&gt; not "we want to change our culture so women will feel welcome" So how do you do that? If you change the culture beforehand, without the "outsider's" input, you are being presumptuous about what the "outsiders" want. In effect, you have a majority of men who are trying to make their culture "women-friendly". Then some of the women will feel more welcome, because they got it right, and some of the women will feel alienated by being treated differently, or being assumed to be different, or being presumed to have a particular personality or preference, etc. And you don't know what percentage these two groups will have. You might as well end up causing more harm than good. 
You can filter instead of using an explicit if-then-else, then extract the filtered values and call the prelude's `product`: product . map snd . filter (not . (`elem` xs) . fst) . zip [0..] After this it would probably be productive to use a set instead of a list for your element test.
FWIW in my uni's math majors were 50/50 male female.
Ha, well it certainly is common, and this may be one of the things that leads to miscommunication. 
We should be wary of flamewars. If someone is being unproductive, I think a polite response about that is better than something that provokes even more heat. Otherwise it just gets even more of a mess.
Here's my solution. It may not be that efficient, but I hope it'll give you another perspective on the problem. prods :: [Int] -&gt; [Int] prods xs = zipWith prod (replicate (length xs) xs) [0..] prod :: [Int] -&gt; Int -&gt; Int prod xs x = product ys * product (drop 1 zs) where (ys,zs) = splitAt x xs 
I can't upvote this enough. 
&gt; The problem is culture. Our culture. It sucks. It needs desperately to be opened up. Ok. What does that mean? How? I'm absolutely serious here, not trolling, to be clear. What is "our culture"? Do you mean CS as an academic discipline or "computer programming" as the widespread profession? What are the elements of a "culture" here that you think are abundant? Are they really elements of a culture, or external stereotypes about a culture? Can we even identify a uniform culture? [I'll buy that there's a toxic SV culture, for example, but that's from afar, by reading articles]. And what does it mean to open up that culture? How would we do that? Again, it starts by defining what a culture is, right? I'm being sharp with my questions here not to be hostile, to be clear, but because I feel like I share your general concern that "There is an obvious and disturbing disparity in CS" and I am serious about wanting to see what can be done. But when a solution is proposed like "open up our culture" I have a hard time envisioning the big steps one can take to do that, or what it even means. The small steps, I think we can move on. Recognizing the problem shouldn't be hard (although it seems hard for some on this thread). But fixing it _is_ hard, and since you seem like you've put some thought into this, I'm just trying to press for more explanation of your ideas :-)
Did i miss some alternative meaning of the word hermit? English is not my native tongue. 
Thanks for the replies!
I've heard fantastic reviews from conferences that had childcare. I never thought about doing that at meet ups to draw more women (and parents in general).
The culture is fine, and it's not "too hard". I'm sorry that we don't care about your gender, except actually I'm not. Biology has more women, and I don't care about that either. The disparity isn't disturbing at all, it simply exists, and attempts to close it have led to donglegate and https://wiki.gnome.org/FoundationBoard/CurrentBudgetFAQ &gt; The GNOME Foundation had a temporary lack of reserves due to processing the funds for the Outreach Program for Women (OPW). How helpful! This really contributed value to the programming community! I'm wasting my time though. *internalized misogyny*, huh? I recognize this buzzphrase. I know your kind, and you'll never remove us.
How about `productsExceptCurrent xs = map (div $ product xs) xs`?
I suspect you have the word "hermit" correct. However I think "no woman wants to be a hermit" is too definitive. I would guess there exists at least one. 
Yes of course, it is just my personal experience not meeting any women programmers while seeing quite a lot of women in there. 
What method would you suggest to investigate this that would not itself be subject to the assertion that, "the fact this study didn't find them doesn't necessarily mean they are not there"?
The problem states that division should not be used. Also, what if 0 is an element of the list?
I'm curious how "fast exact real arithmetic" would work. Short of a fully symbolic system like Mathematica, you rapidly run into problems like how to represent sqrt(2), pi, e, pi^e, etc. – i.e. irrational and transcendental numbers. Even rational arithmetic is rife with overflow problems and tends to need high precision very quickly.
Your Haskell is mostly fine, subject to the improvements suggested by other commenters. :) I'd recommend you look for a more descriptive name for `zips`, but that's a minor detail. As for O(n), that's not a Haskell issue until you've found a suitable algorithm. As a hint, consider that in your output array, each field is described by the formula b_i := product a_j for j in { 1, ..., n } \ { i } where `i` is an array index, `a_i` is the input array's element at that index and `n` is the input array's length. This is the same as b_i = (product a_j for 0 &lt;= j &lt; i) * (product a_j for i &lt; j &lt;= n) When you have found an algorithm, take a look at the [scanl](http://hackage.haskell.org/package/base-4.7.0.1/docs/Data-List.html#v:scanl) function. You may find it convenient to define a variant that fits this problem better. By the way, feel free to post questions like this to /r/haskellquestions. For anyone looking for it, the original problem can be found at https://www.interviewcake.com/question/product-of-other-numbers.
Ok. How is the very top tier of research math relevant to a programming language that could easily be a good step forward for industry programming? Sure, Haskell is more mathematical than other languages, and we're going to need PHD level research to keep making it better. Even if I grant that only men could do that (what!?), I went to a meetup last night with 20ish people, all men. None of us were top tier math researchers. We just were programmers who wanted to learn a new language. So why is that gender unbalanced? (100% is unbalance...). This problem is in no way unique to Haskell, its a general programming issue. Women are smart, they are capable, but they don't enter the field of programming. So why? How can we tweak our behavior to be less intimidating? What small things (microaggressions) are we doing that add up over time to drive out interested people? 
It actually turns out that there are a lot of women out there who tried CS and either gave up or slogged through that have written -- at length -- about their experiences, and about what bothers them. As usual, though, men don't seem to want to listen to any of these women, because women's voices don't count for squat. Jeff Atwood writes a ridiculously misguided piece on how we should get more women into CS, and it immediately goes viral. Countless women write actual, thoughtful, constructive analysis from their point of view, and no one listens -- and people actually go onto forums like this one and suggest that we don't actually *know* what women might want changed because there are no women and so it's the blind leading the blind or something. What I've seen in this game is that only one kind of woman gets signal-boosted to any degree: the token who affirms existing norms. If you are a woman in tech and you believe that everything is hunky-dory and nothing needs to be done, you will have your words copy-pasted to kingdom come. But if you are Shanley Kane, shit -- despite telling men exactly what they can do, you can bet no one will bother reading your words. Unless the words they're reading are the title to a piece Jeff Atwood appropriated for his own much less insightful drivel. If you want to know what women want, here's an idea: *listen to them*. They're people, not monkeys. They have words that they use and they write at length about these things. Broadly, this entire movement is called "feminism", and there is very much to be learned from it.
Division should not be used? That's just stupid. Good point about 0, would complicate the code. The problem would make more sense if it was talking about a commutative monoid instead of multiplication.
Oh yeah, men too.
Qt Quick Controls provides widgets with native look and feel on top of Qt Quick. You can use these from HsQML perfectly fine, as the HsTorChat application demonstrates. https://hackage.haskell.org/package/hstorchat http://qt-project.org/doc/qt-5/qtquickcontrols-index.html
Can you link to some of these posts? I'm really curious about what could be done to make women feel more welcome, as everything I've read so far has been either baseless complaints, real complaints about things I haven't seen happen (and as such, cannot change), or suggestions I'm unwilling to follow as that would be sexist (i.e. treat women different than men, help them, don't be harsh, etc).
 unreadable_pointfree_answer :: Problem -&gt; Jerk explain :: Jerk -&gt; Hero please :: Problem -&gt; Hero please = explain . unreadable_pointfree_answer
Type inference based on abstract interpretation is rather limited since it doesn't reason backwards. For example, I was trying to write this Julia function today: function cartesianarray(body, ndims...) t = body(ndims...) a = Array(typeof(t), ndims...) cartesianmap((idx...) -&gt; a[idx...] = body(idx...), ndims) return a end Notice the nasty trick of figuring out the return type of body function here? It is not just unsightly, but plainly wrong if body performs any side effect. Just an example of the limitation of Julia type inference that gets in the way. Please feel free to suggest any better way of writing this function in Julia.
&gt;You're right. My comment presumed "the blind leading the blind", since the community is already composed of a majority of men as it is. Input from women on their experience is more important to take into consideration than what some guy thinks women's experiences are like. I'm glad we agree. &gt;you don't know anything about me and you can kindly fuck off. I knew exactly one thing about you, jeandem: that you non-ironically suggested that attempts by members of a male-dominated group to improve their own diversity were doomed to failure because we all had no idea, being men, what we could do better. Now, given the first part of your reply -- the part where you recognized that you'd pretty much overlooked 50% of the world's population having opinions on this matter and that that was bad -- I'm inclined to give you the benefit of the doubt. But before you start tone policing me, I want you to think about how your callous disregard for such a large population -- a population that is in fact under discussion here! -- how that feels to those people, and the people who support them. You accuse me of being inflammatory when you just posted and were upvoted for something that is incredibly, incredibly insulting. Think about that. When you do it, I'm sure you feel like it's ok because you didn't mean to be mean. But when someone else says something that hurts your feelings, well then, it's game on, isn't it?
Baseless complaints? Who are you to judge what might be a baseless complaint?
A reasonable, albeit not particularly satisfying, response. Thanks for taking the time to respond. In my opinion, a study identifying potential structural roadblocks for e.g., women in the programming community wouldn't be unwelcome. The problem is convincing someone in the community that this is a worthy research goal.
As a followup, here is a very nice paper I just came across on the relationship of OBJ to ML and Coq, with quite a bit of history: http://www.lix.polytechnique.fr/~jouannaud/articles/objmlcoq.pdf
I would be very interested in you naming just one academically regarded male feminist today. The field is dominated by women, and with good reason. I'd also caution you against fallaciously assuming that womanhood gives you some particular license to completely write-off or ignore what multitudes of other women have said and written about. People have different experiences, and yours -- that everything is awesome and there is no problem -- is in the *distinct* minority.
There is no field? What is that supposed to mean? Hang on a minute -- does your idea of feminism mainly spring from Tumblr? Have you ever taken a woman's studies course? Are you aware of this field of study?
I'm sorry but i'm not buying it. Sounds too much politically correct to me. Sweeping under the rug the actual challenges and social differences for the sake of feel-good, group-hug kumbaya. Nothing i said paints women as outcasts or unwelcome in programming community. Rather i acknowledge the not so attractive facets of our profession and accept that what i love to do not necessarily what other people would love to do. 
Is CS *research* really that bad in general? Just because there aren't many women participating doesn't mean that the work environment is hostile in a gender-specific fashion. There could be other factors.
Something not mentioned in this discussion is that one of the motivations for a data frame is to support missing, NA, values. In Haskell you get it for free when you type your column `Maybe Double`.
&gt; I knew exactly one thing about you, jeandem: that you non-ironically suggested that attempts by members of a male-dominated group to improve their own diversity were doomed to failure because we all had no idea, being men, what we could do better. Get out. What a laughable interpretation of my original post. Let's go back to it, then: &gt; So how do you do that? If you change the culture beforehand, **without the "outsider's" input**, you are being presumptuous about what the "outsiders" want. "The outsiders" are obviously women not in CS or Haskell or whatever. That is not to say that there aren't women in the Haskell community. But the majority are men, and so any discussion of change - especially an insular discussion - will tend to be biased towards the majority. No, that's not a good thing, *but it is the way things tend to happen, in groups*. Both because women are a minority, but also because they might feel marginalized and feel like outsiders - in which case they might be less likely to speak up about what *the culture* should be like. Since people who already feel like outsiders in a group, tend not to want to make waves and suggest that the group should change. This theory is also based on past experiences - you were kind enough to submit an example: &gt; Atwood writes a ridiculously misguided piece on how we should get more women into CS, and it immediately goes viral. You see? A man, in a man-dominated community, gets more attention on a subject than someone with first-hand experience so to speak. Secondly: &gt; So how do you do that? \^ This is not a fucking rhetorical question! The question is raised because you *can* do more harm than good if you go about it in the wrong way, good intentions or not. Specifically, the next part describes *one possible way of going about it, which is suboptimal*: &gt; If you change the culture beforehand, **without the "outsider's" input**, Note the highlighted part. This is based on my experience on what tends to happen: a bunch of male programmers discuss, among other male programmers, how to be more welcoming to women. But that discussion is inherently to insular to make any good plans. Sure, they might get something right, but it may also backfire. Is this the only way to go about "changing our culture for the better". No! And there might be good strategies for going about that. The POINT is not that it is IMPOSSIBLE. The POINT is that *good intentions are not enough*. Maybe I should have emphasized that *yes*, there might be great ways of going about it that probably won't alienate anyone. But I thought I should raise the point that going about things in certain ways can cause more harm than good. &gt; Now, given the first part of your reply -- the part where you recognized that you'd pretty much overlooked 50% of the world's population having opinions on this matter and that that was bad -- I'm inclined to give you the benefit of the doubt. But before you start tone policing me, I want you to think about how your callous disregard for such a large population -- a population that is in fact under discussion here! -- how that feels to those people, and the people who support them. And maybe I can say something about *online culture*: the incredible amount of uncharitable, hostile interpretation that some people go to in order to pick a fight with people. Or to project onto other people whatever evil straw man that they had concocted in their head of the group that that person belongs to. "Benefit of the doubt", my ass. 
Well, no. This fails on several grounds. First, what you might call the theoretical. There is no reason that programming is more anti-social than many other activities. Anything can be done with hyperfocus and to the exclusion of all else; it is not in the fundamental nature of programming. And if you look at the actual job descriptions of actual programmers, you won't find anything equivalent to "go into a cave for years". You will find things like "clarify and refine client requirements" that are explicitly social in that they involve things like communication and adapting to the needs of others. (Often you don't have the coders talking directly to the clients, but there are still lines of communication there.) Second is the empirical. Programming has not always been an occupation for men. [For example](http://gender.stanford.edu/news/2011/researcher-reveals-how-%E2%80%9Ccomputer-geeks%E2%80%9D-replaced-%E2%80%9Ccomputergirls%E2%80%9D): &gt; As historian Nathan Ensmenger explained to a Stanford audience, as late as the 1960s many people perceived computer programming as a natural career choice for savvy young women. Even the trend-spotters at Cosmopolitan Magazine urged their fashionable female readership to consider careers in programming. In an article titled “The Computer Girls,” the magazine described the field as offering better job opportunities for women than many other professional careers. As computer scientist Dr. Grace Hopper told a reporter, programming was “just like planning a dinner. You have to plan ahead and schedule everything so that it’s ready when you need it…. Women are ‘naturals’ at computer programming.” James Adams, the director of education for the Association for Computing Machinery, agreed: “I don’t know of any other field, outside of teaching, where there’s as much opportunity for a woman.” We didn't get to the current state of things by men becoming less social and women becoming more in the past five decades. It was at least in part a push from within the computing world, by men, to make the profession more prestigious ("women's professions" tend not to get the same respect as men's, cf nursing, teaching). Interestingly, the same article mentions: &gt; According to test developers, successful programmers had most of the same personality traits as other white-collar professionals. The important distinction, however, was that programmers displayed “disinterest in people” and that they disliked “activities involving close personal interaction.” This is active pressure to make programming anti-social. It doesn't have to be that way. If the programming community can push itself into being male and anti-social, it can also push itself in other directions. (Another source of imbalance is the generation of programmers that came up through video gaming and hobby computers, which were marketed strongly to boys and not to girls. It is worth thinking about how we teach kids and how we get them interested in programming when they are young. I suspect the gender imbalance in access to computers is much smaller now, but it would still be interesting to see if boys and girls are using computers in different ways.) Now none of this is specific to Haskell, but that doesn't mean the Haskell community can't try to change them. There are also likely issues that are specific to Haskell that make it harder for women to participate. (For example, Haskell retains close ties to academia, and academic computer science and mathematics also have gender imbalance and leaky pipelines and such.) I don't have any wonderful insights into how we can solve these problems or how Haskell can be a shining light to the programming world (especially since it seems that we are already behind). But there are some obvious things we can do. First, stop saying that programming isn't for women (or that Haskell isn't for women, but I haven't heard that one specifically). When women are present in the community, at conferences or meetups, don't tell them that they don't belong here, or that they should be doing something else. When women are talking (about programming, about Haskell, about anything), listen to them. If you are in a mixed-gender group discussion, make sure the women there have a chance to speak. Women are more likely to be interrupted by men and less likely to interrupt. If women are telling you how something is limiting their participation, believe them and think about how it can be remedied. Try to bring in women speakers. Just because the community has a gender imbalance doesn't mean your conference program can't be 50/50. These are just some things people in the Haskell community could do to be more welcoming to women. (And I'm sure some people are doing some of them already.) (I suppose I should also add that all of these are generalities. They may not apply to all situations and all individuals.)
&gt; “I disagree that we should want more women in Haskell” It's not about what *we* want. It's about what they want.
I expect MLP fans are blonder than average, because I expect they are whiter than average. If they are not, I guarantee you it is because someone thought about how to design and market the show to non-white audiences.
It has important primitives for scientific computation tightly integrated into the language, backed by fast and well tested libraries. Compare that with Haskell, where vectors of any kind, let alone multidimensional tensors are treated as second class citizens at best (certainly by the Prelude). The existing implementations of vectors, matrices are incomplete and don't allow most of the common operations to be done easily. Compared to Julia they are also __very__ poorly documented. I guess that is the difference, when someone pays you to work on those libraries. Julias multiple dynamic dispatch together with the JIT compiler allows for much nicer and flexible ad-hoc overloading, without much performance penalty. In particular it allows for easy specialization of algorithms to specific types, for example matrix multiplication of Floats is probably done by some c or fortran routine, and there is a generic implementation in Julia as a fall-back. There are tons and tons of other things that make Julia a much nicer Environment than Haskell is currently for exploratory scientific programming, for example a much better Repl, which is not crippled with arbitrary restrictions like ghci is (for example you can just call a C function from it). 
But even if she were that archetype: so what? People should be free to be who they are, no matter if it is "conservative", "radical", "modern", etc. It's ridiculous when feminism gets used as a way to promote just *another* kind of gender role, or female ideal. That's not freedom; just a new kind of straitjacket. 
That is off topic. You said that the Python type inference you linked to would to a better job of optimizing than Julia does. So I ask again: what evidence do you have for that? Because to me that sounds like a totally incredulous claim. What Julia does is type *specialization* based on the actual types seen at run-time. That is a step further than type inference at compile time. Optimizing Python is a hard problem, and the PyPy people (and many other groups before them) have worked a long time on it. As it turns out, PyPy is doing something similar to Julia, namely specialization based on run-time types. If optimizing dynamic languages based on compile time type inference it were as easy as you suggest, then why aren't they doing that? They're stupid?? The answer is of course that it's not that easy at all: type inference by itself doesn't really help you. What you need is type specialization, i.e. compiling several versions of a function, each working on a specific type. Furthermore, Python's dynamic type system is fundamentally at odds with efficient execution: an array can have mixed elements of any type. In Julia that is not the case: you can construct an array of integers. *That* is what makes Julia efficient.
Why not use a continuous integration service? CircleCI is what I'm using for work and it's a dream to setup with Haskell...
&gt; This is my main beef with the subject. It simply does not belong to such specific subreddit as haskell. But someone has to do it. As long as it is somebody else's problem, nothing will change. I think Haskell also has specific issues and is worse than other programming communities at including women. &gt; But capitalist nature of business may very well be a evolutionary selection pressure. Statements like this can be used to justify anything. The onus is on you to provide evidence in favor, but historically the capitalist nature of business has not done a good job promoting business interests. (Consider the historical resistance to black players in major league baseball and compare the current racial makeup of teams. Also compare with football.) In general, I don't see how keeping people out can improve anything. Even if women on average are less talented than men (and I don't believe it for an instant), some women will be more talented than some men. You don't improve the talent pool by limiting selection.
That doesn't change my point. People should feel welcome into a community for their own sake, not because it pleases the incumbents. 
 f = ap ((&lt;$&gt;) . (product &lt;$&gt;) . (\\)) ((:[]) &lt;$&gt;) { `(&lt;$&gt;)` = `fmap` } f = ap (fmap . (fmap product) . (\\)) (fmap (:[])) { eta conversion } f xs = ap (fmap . (fmap product) . (\\)) (fmap (:[])) xs { expand `ap` } f xs = (fmap . (fmap product) . (\\)) xs (fmap (:[]) xs) { specialise `fmap` } f xs = (fmap . (fmap product) . (\\)) xs (map (:[]) xs) { `map` -&gt; list comprehension } f xs = (fmap . (fmap product) . (\\)) xs [[x] | x &lt;- xs] { break out the function } f xs = g xs [[x] | x &lt;- xs] where g = fmap . (fmap product) . (\\) { expand composition } g x = (\y -&gt; fmap (fmap product y)) (x \\) { beta reduction } g x = fmap (fmap product (x \\)) { specialise `fmap` } g x = fmap (product . (x \\)) { expand composition } g x = fmap (\y -&gt; product (x \\ y)) { apply `g` once } f xs = g [[x] | x &lt;- xs] where g = fmap (\y -&gt; product (xs \\ y)) { specialise `fmap` } f xs = g [[x] | x &lt;- xs] where g = map (\y -&gt; product (xs \\ y)) { substitute `g` } f xs = map (\y -&gt; product (xs \\ y)) [[x] | x &lt;- xs] { put the mapping inside the comprehension } f xs = [(\y -&gt; product (xs \\ y)) [x] | x &lt;- xs] { beta reduce } f xs = [product (xs \\ [x]) | x &lt;- xs] and I think this is the best we can do. It takes the product of the list except for the current element, and constructs a list out of doing that for all elements in the list. When doing this yourself, if at any point you feel lost, just infer the type signature. It helps *a lot* in figuring out what's going on. Also beta reduce whenever you can. That shit is amazing. I'm really happy Haskell allows us to do this sort of equational reasoning. It's one of the things I miss in other languages.
As a woman who works as a programmer in a department that is made up mostly of women, I'd recommend not comparing us to statistical outliers in thermodynamics experiments.
My point was that "there exists at least one" is not a valid argument :)) We know there are a lot of female programmers (in absolute numbers) That fact does not invalidate huge gender disparity. But my belief is the programming community itself is not the cause of it. Rather we are the product of external economical and social pressures. 
I said Pysonar does a better job at type inference than Julia because (1) it uses a more advanced technique than abstract interpretation (2) the guy behind it has a background of doing type theory research. I also said a little bit type inferencing would help optimizing Python programs. Actually any type inference at all would help Python optimizing its performance, and type inference can be done for Python as PySonar2 has already shown. What I didn't say was doing a type inference like the PySonar2 would lead to optimizing Python to the point that performance is better than Julia. What I also didn't say was that it'd be an easy job. It takes years of research before researchers have a good understanding of what type is. You and I may have different opinions on what is the best way to integrate type system into a language, but why so upset when I pointed out the drawbacks of Julia's approach? 
Is anyone able to provide a default.nix for this ? cabal2nix fails because it can't even parse the conditional expressions in the dependencies and I can't get it fixed by hand either. Any experienced nix users up for this? :)
But a community can obviously act contrary to those pressures. Otherwise there would hardly be women working in any profession, across the board, because *no* community would have individually moved out of the larger social biases against women working.
First, specialize `ap` to the `(-&gt;) [a]` monad: ap = \ fs xs as -&gt; fs as (xs as) Putting that into the expression we have let f as = ((&lt;$&gt;) . (product &lt;$&gt;) . (\\)) as (((:[]) &lt;$&gt;) as) Now simplify the compositions and operator sections let f as = (&lt;$&gt;) (product &lt;$&gt; (as \\)) ((:[]) &lt;$&gt; as) Since we have `as :: [a]`, the last `&lt;$&gt;` is just `map`: let f as = (&lt;$&gt;) (product &lt;$&gt; (as \\)) (map (:[]) as) Now `(as \\) :: [a] -&gt; [a]`, so the next `&lt;$&gt;` is `(.)`: let f as = (&lt;$&gt;) (product . (as \\)) (map (:[]) as) And since `(map (:[]) as) :: [a]`, the remaining `&lt;$&gt;` is also `map`: let f as = map (product . (as \\)) (map (:[]) as) The laws for `map` say it distributes over composition, so this function is really the composition of three `map`s: let f as = map product . map (as \\) . map (:[]) $ as So sequentially: make each element of `as` into a singleton list; remove that singleton from the whole list of `as` (this will be a problem if `as` has repeated elements; edit: after looking at the documentation, I guess not, assuming multiplication is commutative); finally, take the products of the resulting lists. Edit: Since I am looking at the docs for `(\\)`, I guess you could also replace `(as \\) . (:[])` with `flip delete as` and use only two `map`s.
A method by turning the given list into an infinite list. prods xs = (\i -&gt; product . take ln . drop i) &lt;$&gt; [1..ln+1] &lt;*&gt; [cycle xs] where ln = length xs - 1 
vagif, I think you've been downvoted because it sounds like you are speaking about women in particular ("no woman wants to be a hermit", "women are much more social creatures than men"). But I think from the other things you've written that you would agree that it is *social pressure* that creates the gender disparity, not the choices of any singled-out group in and of itself. "society pressures women to not be hermits", and "society pressures women to be more social creatures than men" might be more palatable. The only thing "PC" about those phrases is that they are much less likely to be misunderstood.
Then how can you explain that in a more general field (IT) there's no such disparity? There are plenty of women in IT. Mostly managers, business analysts, QA, web designers, graphic designers, and various satellite occupations. Just not specifically writing code. It looks to me it is not programmers not welcoming them in, but rather women choosing not to go into programming. If we want to fix this problem we should address the reasons why women prefer not to chose programming profession. 
At 20$ per month? Yeahhhh no. Not for personal projects at least. EDIT: apparently there's a free option for OSS, my bad!
It's important to distinguish between "welcoming" and "encouraging" I think. It's difficult to address welcoming because it's so broad (and people who make women feel unwelcome in coding generally don't care at all whether they do or not). Again using personal experience: the reason our department is dominated by women, and the other programming departments here are almost entirely devoid of women, can be directly linked to the fact that one of my coworkers runs a women's coding meetup, and through that has made contacts for internships and other hires that wouldn't be available otherwise. All this to say, there's certainly more that can be done than being passively welcoming - we can actively encourage women (especially novices, since, as mentioned, CS is such a male-dominated field) to participate.
There's definitely social pressure. I commented a little bit [here](http://www.reddit.com/r/haskell/comments/2lgjo9/women_in_haskell/cluz547) about possible evolutionary pressures. But i disagree with your characterization of social differences between males and females as being completely fabricated by the society. I think feminists would be offended by your characterization of women as passive cattle being pushed into their roles. While there's definitely socially encoded gender discrimination towards women in our society (traditional family structure for example), differences in social **nature** of males and females cannot be dismissed as such. We are **physically** not the same. Why should we be **emotionally** the same? 
I don't want to accuse anyone of anything. I brought up the history of how women were pushed out of programming to show that the causes were social and so the solutions can be social. The things is, though, we should not be asking who is responsible for this or who can solve the problem. That just leads to passing the buck. Programmers say that management should just hire more women. Management says women don't apply for jobs. Nobody does anything and the problem persists. Instead, we should ask what we can do to help. We are the people we are, in the positions we're in, with the abilities we have. We should use those to make things better, in whatever way we can. And it is not like you can draw a sharp line between programmers and managers. Programmers often have input in hiring decisions, and they get promoted to managers who have direct hiring powers. &gt; Everyone understands that the solution to this lies in political plane. Unions, anti-discrimination laws etc. Ironically, many programmers I know are virulently anti-union.
Wonderful idea. What percentage of participants end up bringing their kids, in your experience?
What /u/cartazio is getting at is that generic array operations, reductions and operators in Julia's standard library are written in a way that only works properly for dense arrays. There are many holes in test coverage, usability and performance in Julia's current sparse matrix support. It's a lot better than say Torch (which doesn't have sparse matrices at all last I checked), but not as good as scipy.sparse, or PETSc, Trilinos, etc. It's usable but not great. A major rethink, possibly via the traits trick, could improve matters significantly. Julia's standard library is far from set in stone, so a fair number of these deficiencies can be fixed. Now granted I'm yet to be convinced that the 1-man Haskell version is actually any better. Sounds like there are some good ideas behind it, but I'd need to see benchmarks that show for example sparse multithreaded Cholesky with comparable performance to Cholmod in much less code and/or with a much nicer API. Otherwise I'll go on writing FFI interfaces to C/C++ and Fortran code that I know works well.
To clarify, I think I was directing my comment to anyone reading yours, not necessarily to you yourself :).
:-(. I understand, but still I wish that a few poisonous voices and a frustrating discussion didn't have such a serious impact.
I think this solution gets more complicated because you have to deal with zeros.
https://github.com/bitemyapp/doc-workshop https://github.com/mietek/haskell-on-heroku
You are right, I can not know if there exists some problems or not. My post, after reading it again, was maybe a little bit too hard. Of course, if there are some reasons that prevent women to join the community — as for other stereotypes btw — , they must be fight against. For this purpose, we could as you said investigate properly to gain confidence that this is not the case. After thinking about it, what annoys me the most in those kind of discussions is the fact that usually people just throw a topic like "Look! There is a imbalance in favor of men. We must change that!": It is not based on any evidence — or even intuition — that there is a bias in defavor of women. There is also no starting idea or draft of a solution. Statistics showing that the rapport between male/female is different from the general population is not an evidence. For instance, in Switzerland, the first male midwife was promoted in 2004 [1], and nowadays the imbalance in favor of women is probably still very high. Does it mean that men are disadvantaged compared to women for accessing this field? I don't think so, it is just the interest that are different. For me an evidence or intuition that there is a problem would be women saying that they are having problems entering the community. Then, and only then, it would be possible to understand the reasons and act to correct them. [1] (sorry French) http://www.amge.ch/2004/11/10/le-premier-homme-sage-femme-de-suisse-est-un-sage-burundais/
&gt;Ironically, many programmers I know are virulently anti-union. What benefits do you believe a union would bring for programmers?
Well, those are all computable reals, so you could use something like [CReal](https://hackage.haskell.org/package/numbers-3000.2.0.1/docs/Data-Number-CReal.html). It's performance is horrible compared to double/float, but it's on par with (or better than) something like [BigDecimal](http://docs.oracle.com/javase/8/docs/api/java/math/BigDecimal.html).
Simple solution, not linear: productsExceptCurrent :: [Int] -&gt; [Int] productsExceptCurrent = go [] where go xs [] = [] go xs (y:ys) = (product xs * product ys) : go (y:xs) ys Basically we rebuild the list (backwards) as we traverse it. With pattern matching we end up with 2 lists at any given point: the list of elements before our current one (reversed) and the list of elements after it. Then we use `product` to multiply them up and multiply them together to get the value for that position. Edit: Less simple solution, linear space/time: productsExceptCurrent :: [Int] -&gt; [Int] productsExceptCurrent xs = zipWith (*) prefixes suffixes where prefixes = scanl (*) 1 xs suffixes = drop 1 (scanr (*) 1 xs) I use the `scan` functions to produce partial product lists going left to right (`scanl`) and right to left (`scanr`). I `drop` an element to offset the suffix list so the current element isn't included, then `zipWith` to multiply across. Since the two lists aren't the same size the extra element in `prefixes` is ignored.
&gt; Yet when discussing lack of women in programming, it's like we male programmers are harassing them out Although it is not directly what I was talking about in my comment, I just want to add that women in tech _are_ harassed, assaulted, and raped at work, at conferences, on the Internet. That needs to fucking stop, too.
I've been thinking about this for a while a cannot figure out how `scanl` would help.
I'm not sure that's true. There are certain principles that are shared by the vast majority of Haskellers that are underlie in our unofficial motto of "avoid success at all costs". (Typed is better than untyped, Type-inference is very valuable, Effects can and should be tracked in the types.) Persons that fundamentally disagree with those principles, or the results entailed by those principles are not going to (and likely should not) feel welcome in our community. Other communities will have similar conditions: defining characteristic(s) that might be incidentally exclusionary. That said, your particular body morphism, mental self, or sexual practices have no relation to your passion for Haskell, Functional Programming, Type Theory, or CS in general (as far as I can tell), so we should be careful not to exclude anyone based on those characteristics, even if the existing community seem to have a abdunance of a particular combination of those traits (cis-gender, male, hetero).
That was a really good article about unintended sexism.
I'm not convinced that sexism is why there are few women in tech either. I still think it is a problem, whatever the cause, just because we are missing out on potentially useful members of the community. 
&gt; Most women refuse to identify as feminists because they perceive feminism negatively. They agree with the theoretical definition of feminism: "that men and women should be treated equally", but do not think feminism actually is that. This has nothing to do with women. This applies to people of all genders.
&gt; That said, your particular body morphism, mental self, or sexual practices have no relation to your passion for Haskell, Functional Programming, Type Theory, or CS in general (as far as I can tell), so we should be careful not to exclude anyone based on those characteristics, even if the existing community seem to have a abdunance of a particular combination of those traits (cis-gender, male, hetero). No one said anything about excluding anyone. At least not me.
How about something like this: prods :: (Num a, Eq a) =&gt; [a] -&gt; [a] prods = snd . prods' 1 prods' :: (Num a, Eq a) =&gt; a -&gt; [a] -&gt; (a,[a]) prods' sum [] = (1,[]) prods' sum (x:xs) = ((x * backsum), (sum * backsum): rem) where (backsum,rem) = prods' (x * sum) xs It walks down the list once, calculating for each element the product of all the elements that came before, and when it reaches the end of the list it walks back and calculates for each element the product of the elements that came after, and this way a list is built that matches your description. Should be O(n) time and O(n) space? Edit: I seem to write 'sum' where I should have written product... :D
The evidence for discrimination against women in tech is abundant. The evidence for similar mental capacity, and similar interests, across sexes is abundant. The elastic, learning capabilities of the brain point to an incredible ability for society to determine behavior. All these things being the case, the numbers being quoted around here (&lt;1% attendance by women at some conferences?) -- if genetic differences were the main factor -- would be so incredible that they would have staggering consequences for our understanding of not just homo sapiens, but of primates as a whole. I hate to downplay the genetic differences between the sexes, because I think they're fascinating and incredibly complex. But in the specific realm of "women in Haskell", a phrase that already has so many social implications I feel uncomfortable just saying it (are we also counting trans women? trans men?), it's just *way* too early to say "Whelp, y-chromosomes and all!"
&gt; However, for example, married men who have kids often can go out to meetups, because it is **expected** their wife will take care of the kids. I think gbaz1's point is about expectations on childcare based on gender, which *in general* skew towards men not having these expectations.
And the implication of that is that same thing happens in practice. Expectation ~= Reality, when the assumption is that most people try to live up to those expectations. Another point was that men might have other responsibilities that aren't *directly* related to children, but are related to having a family. But that's apparently too much of an unreasonable point to make.
There's a lot of "friendly chatter" that's a big part of Haskell twitter scene that is well suited for this medium. It's just when people use it for debate that things get nasty pretty quick.
See /u/stevely's [second solution](https://www.reddit.com/r/haskell/comments/2lhhw6/help_in_improvement_new_to_haskell/clv4k18). Intuitively, `scanl` and `scanr` give you, for every index, the product up to that index, starting at the left or right end of the list respectively.
Not a he, but otherwise spot on.
I'm curious to ask, but if we're generalizing it why make it commutative? Can that be exploited for a better algorithm?
If nothing else, you have learned that there are some significant differences between bin A and bin B. Imagining ten random factors doesn't change a result from "these things appear to correlate" to "we know nothing at all". Even if you're completely right about other factors being all that matters, you're still left to explain why they happened to align to produce this data. Certainly any suggested explanations are at best weakly supported, but there's still a heck of a lot of falsification to get out of this kind of study.
I don't mind paying $20 a month, but some people can't afford to waste $240 a year, especially since these small costs add up (DropBox, Github, Linode, etc.). Not having to spend a lot of money is a freedom that many people enjoy and don't want to give up.
Here is a simple way to deal with zeroes: import Data.List (delete) productsExceptCurrent :: [Int] -&gt; [Int] productsExceptCurrent xs = map (\x -&gt; product (delete x xs)) xs
Hi, everyone. I made these bindings to CEF3 to help bring all the utility of Haskell web UI technologies to desktop applications. I'm pretty green in the bindings arena, so I'd welcome the suggestions and fixes of the more experienced. CEF3 is pretty powerful, so having bindings (and a solid Haskell wrapper API eventually) is useful I think, at least to me. :)
personally i program _because_ i desire earnest and genuine human communication. participating in the development of an application or some other software package is one of the most social things you can do. i love the feeling of being in a team communicating through issues, pull requests and commits etc. if software is not meant to enhance human experiences and create value for people then what is it for?
I concur; excellent article. I agree with every single point except #5. Men can be "sexy" too. Merely calling an algorithm or piece of software "sexy" does not strike me as being sexist. Or maybe I'm allowed to say it because I'm a gay man, but straight men are being sexist when they say it? I agree that it may be more professional to be more descriptive. I'm curious what other haskellers think. Should SPJ apologize for "sexy types"? I don't want to be in the business of discounting a marginalized person's concerns, but in this particular case I guess I'll need a little more convincing to believe that this point belongs in the list with the other 12.
Good question: currently that is not yet supported - in fact cabal-rpm is not yet really package version aware! It basically now always just tries to build the latest version of package dependences. But yes I have thought about this and think it makes sense in the general packaging space. We have been lucky so far in Fedora in that we have pretty much managed to avoid the need for parallel versions of libraries, but I suspect we may well hit this one of these days. Feel free to open an issue on github for this RFE. I think it should not be *that* hard to do and certainly it is possible rpm-wise. Do you have any examples from hackage say btw?
&gt; I spend more than that per month on github for personal stuff. Why not just use BitBucket? Free private repos.
What about using Jenkins or something like that?
I have a stronger/more interesting notion of first class than "it has a type". I think i'll have to find the time soon to write a blog post about what I mean! (hopefully i sort out work hunting soon i can make the time to write that)
I was just gearing up to do this myself so I'm very interested. Can't wait to take a closer look. 
We need to look at where software engineers come from, and they mostly come from universities. I only know about Hungary, but there's a striking gender imbalance at our universities as well. The imbalance stops when you go further back to high school, so there's one or more factors with a strong aggregated effect that drives women away from CS faculties (relative to men). I can only hypothesize, but among these are: different societal/family expectations, different role models (stemming from culture), some young women becoming mothers. But going even further back, I also suspect that cultural influences already shape young children enough so that their minds work differently enough at the point they go to high school, making them noticeably more or less susceptible to certain sciences, and in most of current cultures these influences drive males toward, and females away from CS. To me it really seems that there's not much to do in the industry. The difference in its driving forces on males vs females seems to be an order of magnitude smaller than in other stages of life - firstly because people are more mature and less prone to influence by the time they are professionals and secondly, the environment is much more conscientously non-differentiating. Having said that, what does seem to be a differentiating attitude is exactly that of this thread. That we need to stand up and do something about not enough women in the industry. Well, if we actually want the situation to normalize, we should be as unassuming as possible, hoping that this kind of gender blindness is effecting our general culture outside of the industry as well. This is a slow way, but I don't see any faster non-forced way. We can organize Rails Girls and Haskell Hags or whatever we come up with, but under the hood these are short term hacks that I don't think have a lasting effect.
That is because there's no such thing as a culture that women are less or more attracted to. Culture emerges from the behavior of a group's members. So out of your two quotes, the second makes no sense and the first is not actually what people want. What they want is "we want more women in our group to shape our culture together with".
Same here. 98% of my followers are men. If other people are wondering, you can discover this information by visiting [analytics.twitter.com](http://analytics.twitter.com/), which is freely available to all Twitter users. The Followers tab has this information on the right-hand side.
Sure, maybe, but I think that's still the wrong question. It's like when people blindly say they'll only use Haskell for a project without looking at the constraints and requirements of the project. First let's get women interested in computers and programming, then let them naturally pick Haskell (or not) depending on their preferences.
As am I, while being male.
I'm curious to know what the makeup of @reddit_haskell with their 4k followers. Perhaps whoever runs the account can post the info here.
Of course it's inappropriate to say, "You should have felt offended even though you weren't!" If you didn't see condescension or misogyny or what-have-you in a given interaction, that's okay! If you disagree, that doesn't inherently make you a bad person. But that disagreement is _very_ different from saying, "Your feelings are incorrect, and you are wrong to have had them." Some people claim that functional programmers are condescending. Instead of declaring that those people are wrong, it'd be better for all of us to work at understanding why those feelings came about in the first place, and work to make future interactions better.
I think this discussion should go back to Twitter. Devoting a couple of posts to this was appropriate to settle the matter but now we've had at least 5 different threads in the past few days about social issues. This is becoming a social issue in and of itself for people who just want to read about what's new in Haskell code, not dramatic social conflict.
i personally think functional programming is a better starting point for many women as although they may not have taken an interest in 'computing' most european women have done mathematics at school. it seems to me that the idea of a function as a thing which maps from one set to another without side effects comes more easily to people who have studied maths but haven't played with imperative languages before. i'm actually primarily a javascript developer and i still feel this is the case. i think haskell is a great introduction to programming and i think we in haskell community have a responsibility to work on making our spaces provide more value to women. sure other communities have this responsibility too but we can't simply shirk this off in the ways of "its somebody else's job" or "its just the way things are".
This is totally true. I try to check out once it feels like the discussion is progressing as it's too easy for someone to keep a Twitter thread derailed. I still think the forced editing is really valuable for sharing links, opinions, and observations. It's just too easy to write too much these days.
I think Haskell is tricky as an introductory language. By necessity you'd need to teach some special subset of libraries from the slant of using Haskell to program simple games, or simple console programs. You couldn't show off Haskell in real-world use until you'd gotten people to be comfortable with monad transformers. On the other hand, I agree that math is a great gateway drug into computing, and would go one step further and say that _statistics_ is an even better tool to teach because modern statistics so heavily relies on software like R. I'm actually doing two MOOCs on edX right now, the Haskell one and the data analysis one--and let me tell you, in the Haskell one there's no evidence of any women. Everyone is using pseudonyms. I may be one of the few people using my real name. In the data analysis one--completely different story. Women (and maybe even girls) are posting under their real names, asking for help with R, giving advice to people, and just participating completely naturally and equally. Maybe the difference is that the data analysis MOOC is focused on the outcome from the very beginning--you will learn data analysis and modelling. And so maybe you'll level up your career. On the other hand, the Haskell MOOC is amorphous--you will learn Haskell. What's that good for? No one explains. It's a mystery to the uninitiated.
But this topic is almost *never* framed as a purely investigative or academic one ("why are there fewer women than men in STEM fields?"). It is almost *always* framed as a discrimination narrative and a call to action ("there aren't enough women in STEM and we need to change that!"). This thread is sadly no exception.
&gt; If nothing else, you have learned that there are some significant differences between bin A and bin B. ... according to the metrics you defined, which might be completely meaningless! Also, I would take care to distinguish between "significant" (matters in practice) and "statistically significant" (the distributions are not the same with some confidence). At least in fields like medicine, we generally know what metrics are relevant (did the patients get better or not). In software, we might not even know what we want to measure or how to measure it. Is counting bugs in GitHub a meaningful measure of anything (other than the number of bugs in GitHub)? This is a rhetorical question, but I'd like to point out that there are lots of possible explanations for why bug counts could be different, and many of these explanations will have nothing to do with what we are actually interested in. So while I accept that in principle a study like this may give us more than zero bits of information, I still don't think this general approach of these observational studies is productive or useful. Also, unlike, say, medicine, where the human body doesn't change, software is changing rapidly. Accruing small bits of information at a slow rate about a subject (software development) which is changing rapidly is IMO pointless. We could do such studies for 20 years, and by the time they could say anything definitive at all, they would be irrelevant! (Consider the status of static typing now vs 20 years ago, or consider that many type classes and principles for structuring functional programs have developed in the last 10-15 years!)
I had thought the comments in there were actually pretty good; and then I scrolled down. Yikes! There's a tremendous lack of compassion in that thread, on both sides. (The irony runs thick...) I like this subreddit; I've learned a lot here, and it's quite friendly to beginners. For some reason, though, it's can't seem to handle meta-discussion without devolving into conflict.
well said, thanks for doing a far better job explaining what I'm alluding to than I'd be able articulate this week. (somehow having many business meetings saps my abiility to communicate) Yes, i'm looking forward to seeing neat benchmarks too. :) They will happen though, I'm told. Depends a lot on how work settles down this fall.
Thanks. I have no actual example out in the open yet. I'll open the RFE, anyhow. For RPM itself, if say, you have two versions: $ rpm -qpl ghc-tagged-0.7.2-1.fc20.x86_64.rpm /usr/lib64/ghc-7.6.3/tagged-0.7.2 /usr/lib64/ghc-7.6.3/tagged-0.7.2/libHStagged-0.7.2-ghc7.6.3.so /usr/share/doc/ghc-tagged /usr/share/doc/ghc-tagged/LICENSE $ rpm -qpl ghc-tagged-0.7.3-1.fc20.x86_64.rpm /usr/lib64/ghc-7.6.3/tagged-0.7.3 /usr/lib64/ghc-7.6.3/tagged-0.7.3/libHStagged-0.7.3-ghc7.6.3.so /usr/share/doc/ghc-tagged /usr/share/doc/ghc-tagged/LICENSE This: rpm -i ghc-tagged-0.7.2-1.fc20.x86_64.rpm ghc-tagged-0.7.3-1.fc20.x86_64.rpm Works without any error (nice, as designed!), even if 'rpm -i' is invoked one time for each pacakge. Same goes with the -devel versions. However, it only works because the two LICENSE files so happen to be identical between these two versions, and there are no conflicting files otherwise. In the devel package you have /usr/share/doc/ghc-tagged-devel/README.markdown which will definitely change between versions. So RPM is already good. Sadly with YUM it breaks when providing both packages to 'yum install', outputting a long error message complaining about multilib, and when you provide the --setopt=protected_multilib=false YUM continues past that error but bails on a strange error with rpmdb ('Rpmdb checksum is invalid: dCDPT(pkg checksums): ghc-tagged.x86_64 0:0.7.2-1.fc20 - u'), but leaves those two RPMs installed. $ yum check Loaded plugins: changelog, langpacks, priorities, refresh-packagekit ghc-tagged-0.7.3-1.fc20.x86_64 is a duplicate with ghc-tagged-0.7.2-1.fc20.x86_64 Error: check all But on the bright side, YUM makes a special treatment of the kernel package in this regard, and it has multilib, so I wonder how hard it would be to teach it about these packages as well. BTW, a solution came to my mind - perhaps if we detect such a problematic dependency tree, we can go back to rebuild package D adding the older version to it, generating just one binary RPM that provides both versions (with the topmost version in the actual package version). Then for it to work rpm-wise, another question comes up - in the RPM meta-data, is it possible for a single binary RPM to specify conflicting statements in its 'Provides', i.e. ghc-D == 0.1, along with ghc-D == 0.2? I'll play around with rpmbuild to check.
Thanks for the URL. It looks interesting. I'm looking forward to that blog post :) BTW, you can sum over any dimension even in many sparse formats. Suppose for simplicity that you have a 2d array `A[i][j]` which is stored as a dense vector of sparse vectors. Then it's of course trivial to sum `v[i] = sum(A[i][j] for j in 0..n)`. But you can even sum `u[j] = sum(A[i][j] for i in 0..n)`. If you know that the resulting vector is dense then it's easy: to add a dense vector to a sparse one you just iterate over all the (j,x) pairs in the sparse vector and add u[j] += x. Even if you don't know that the resulting vector is dense, you can still do it by repeatedly adding a sparse vector to a sparse vector. 
**I like it as it is now,** because this is how I usually use "runStateT". data ProgramState { database :: [(Int, Int, String, String)], history :: [String] } actualMain :: StateT ProgramState IO () actualMain = do parse arguments some more setup enter run loop main :: IO () main = do initDatabase &lt;- readFile "userDB.ini" &gt;&gt;= readIO evalStateT actualMain $ ProgramState { database = initDatabase , history = [] } So my whole program is all written within the StateT monad, I only ever use "runStateT" when bringing it back down into the "main" function, or some similar function. In this case, the order of parameters is convenient because you can use the "$" operator and just declare the initial state right below the call to "runStateT". 
Julia may still give a big performance boost even if the numerical operations you want to do are completely standard. For example for some stuff I was doing a while ago I just needed to build a big sparse matrix and then do a couple of linear solves on it, i.e. the most standard thing you can possibly do. As it turned out, building that sparse matrix in Python took longer than actually doing the solves...it's often the case that in Python the preprocessing takes longer than the actual numerical algorithm that's implemented in C/Fortran.
An old one with a similar theme: [The Functor's Prayer](https://plus.google.com/+SeanLeather/posts/Xx1jzparSCr)
Right I think the general approach is to add a version suffix to the name of the base rpm package. So for your example, the package name-version's might become ghc-tagged-0.7.2-0.7.2 and ghc-tagged-0.7.3-0.7.3. And yeah we would have to use versioned docdirs again. (Of course the ghc-tagged-0.7.2-0.7.2 package would conflict with the Fedora ghc-tagged-0.7.2 package, so it might not be ideal, but maybe cblrpm could work that out.) It is a little ugly but I think it is currently the simplest general way to handle it. I am not sure if dnf will handle this better than yum. Of course your multilib approach might be workable too with appropriate yum configuration. Might need to experiment a bit to see which is more usable. To your latter idea, I think it is better to separate the packages in general than bundling versions together.
Or because you want to keep your work internal.
Can't you compile to C and compile that to JVM bytecode? Would it go really slowly or something?
That may be so, but the post I replied to made a pretty bold statement ("There is nothing restricting people to access the Haskell community") for which I really do want to know how we can know this. Even if most of the other participants where motivated purely by politics that wouldn't mean we all have to be.
This is a reasonable explanation, and I would afford it a good chance of being at least partly true. But the problem with all reasonable-sounding explanations is that they satisfy our curiousity without us actually learning something. (If we where satisified with reasonable explanations we would never have discovered surprising things like evolution, let alone quantum mechanics). And like in programming, where we learn that in order to improve the performance of our programs we have to profile them first, if we really wanted to understand what is going on and how to rectify the situation we would have to abandon armchair reasoning and actually look at the world, preferably in a systematic way. Now, I have to admit that I'm not actually doing anything of the sort. Partly because I don't have the means (not a trained sociologist), partly because I'm lazy :(, partly because I don't care enough. I do, however, try to not settle on a conclusion one way or another without seeing better evidence first.
This could be useful in combination with Hamish Mackenzie's `ghcjs-dom` and `jsaddle` packages, which aim to provide a backend agnostic browser and JavaScript engine programming API. Currently they support a native WebkitGTK backend and a JavaScript backend where the whole application is compiled to JavaScript with GHCJS. Every target has its own specific features, like direct manipulation of JavaScript values with GHCJS, or v8 engine specific calls in CEF. I wonder if this can be nicely split into a core API and optional extensions on top of this. (Additional planned targets are browser components on Android / iOS with ARM cross compilation, so an app can be written with the same DOM manipulation API without incurring the cost of translating the whole application to JavaScript)
This is pretty cool. Thanks.
Yes. I if you read my remarks a teeny bit more closely, I said you can do it, its just not work efficient by a potentially arbitrarily large bound. In 2d outside of the super computing size workloads its probably OK, But in a higher dim array (eg 7dim) or that super computing scale, that performance can be problematic. This sort of consideration and implicit performance cost model dictated a lot of the work. Again, I probably need to write some blog posts or a book to explain this all properly. 
I actually think most languages will have a very very hard fixing their array abstractions to make sparse first class, at least not without winding up having to add a new incompatible Api. The flavor of generic api I have for example, could probably be ported to maybe idris, rust or Scala. But even then, the design would still have to be pretty different. In the case of julia I'd actually worry about hitting llvm bugs or the like just because of how I need specializing and inlining to behave. That said, would love to sit down over coffee and help someone figure out a julia Oort some time. But it'd have to be an IRL sit down for me to pay attention long enough :)
What do you mean by "it's just not work efficient by a potentially arbitrarily large bound"?
How does Twitter know if users are male or female? Does it ask you this when you sign up?
what I mean is: suppose I had a large 1 billion x 1 billion sparse matrix with ~ 3 billion entries (so depending on the element type, probably 3-15gb uncompressed in memory). Now, suppose I'm using something like a sparse morton order layout. And I want the sum of the entries in the millionth column. hrmmm... actually i can do that efficiently. It'll look a bit funky, but I can actually make that somewhat efficient. Yes, I have nice things (and can do the things you want, though not necessarily in the way you'd expect) basically the point I'm trying to make is mostly that a lot of tricks/algorithms people write assuming they have dense arrays need some very very careful rethinking to also work correctly for sparse array formats. Doubly so if you want to be able to write code that performs well in both settings. [edit] to be more precise, simulating that sort of scan/aggregation on nontrivial crazy layouts requires that I define a sort of recursive filter/merge kinda traversal, which SHOULD have the right work complexity and relatively low allocation complexity, but wont have the best possible usage of the CPU's memory bandwidth... I think. That said, i'm not sure if i'll be including that particular primitive in the first release, BUT its something I'll put some time into. And yes, I'll try to write up a blog post laying this out a bit more
 type Dst c :: * -&gt; c Do you mean type Dst c :: * -&gt; * ?
typo, thanks!
It's not about being in a shared space. (Although, public spaces seem to be men's spaces.) It's about being the outlier / outsider in a space. You feel uncomfortable, and possibly fearful, because the part of your brain that evolved to model other minds is having to work harder because these people are more different from you and that's distracting leaving a sense of unease. At least, that's my translation as a white male that has had the feeling I described when placed in situations where I was clearly a statistical anomaly or just felt like an outsider.
Reminds me of the Scala shapeless library's Poly1.
From what I understand, Twitter infers this from publicly available data, such as your username, profile description, and profile picture.
I think your decision is a bit hastily made. Look at the numbers. This thread has currently 76 points and 33 comments. The other thread you mention 40 points and 181 comments. ghci-ng has 47 points (i.e. it's similar) but only 14 comments. Now, which one represent the Haskell community the most ? That being said, the word community doesn't mean much. When people say "I thought the Haskell community had a friendly reputation", it doesn't mean it will never grow and attract vocal people. If someone has never posted or commented on a technical level in this subreddit, there's no reason to say that person represents the Haskell community when being a dick on a non-technical thread.
I don't think it will ever be possible for OP to find common ground with the people she disagrees with. This is demonstrated by these sort of sentences: &gt; It annoys me, every time. (...) So I check my annoyance (and roll my eyes in private). or &gt; Empathize, because at some point in your life, you, too, have almost certainly said something that inadvertently hurt someone else. OP advocates an environment where people just keep to themselves when they encounter a problem thus never fixing the issue at hand and accepting to have to suffer from it repeatedly. Forever. I would hardly call a solution based around not sharing your feelings a "functional approach to talking about feelings"... People defending codes of conducts and [lightweight social rules](https://www.hackerschool.com/blog/38-subtle-isms-at-hacker-school) on the other hand are trying to build a space where people who do care about not annoying / harming / insulting other people are given a framework to use in order to report / get feedback whenever something inappropriate is said. How are you going to fix a bug unless a report is filed?..
By safe space I don't mean to imply spaces with men are violent or dangerous. Part of what I mean is that it feels like a space where I am expected to be, as opposed to the common "is your boyfriend a developer?" women get at conferences and meetups.
Nice! I started working up how to do this on a Mac a few weeks ago. It is a lot harder there due to the way you have to use the multiple helper executables and package as an app bundle. A major use case is for things like 3d games, where libraries like Awesomium have taken off letting you render the menus, etc. in HTML and project things into a texture. CEF3 could be a much nicer story for that, the tricky part is figuring out how to package it. OSX is definitely the most constrained platform in terms of how you bundle things up though.
That sounds pretty error prone
Yes. However, you can independently verify the number just by scanning your followers list yourself. I find that it's reasonably accurate.
It is definitely in the ballpark
Looking forward to trying this new release out. There have always been some integration points stopping me from using the IDE (the new git shell hopefully addresses one of the big ones for me).
Yes, when I was looking for existing cef bindings, I saw your conversation (with chrisdone, I think?) discussing it in IRC logs. :) It was good to see validation that there was interest besides just mine.
Could you give some examples of what you consider to be not on topic?
I'm subscribing, not because I think this is a good idea, but because I wouldn't want to miss posts which would only be sent to TrueHaskell. Also, what's with the annoying wiggling hat?
`w a -&gt; m b` sums up a LOT of `Arrow`s, but it does not give you an explicit conversion. Since you are fine with GHC extensions, maybe a existential GADT would satisfy you instead of a typeclass? type BiKleisli w m a b = w a -&gt; m b data Functionish a b where Functionish :: BiKleisli w m a b -&gt; Functionish a b contRun :: Functionish a b -&gt; (forall w m. BiKleisli w m a b -&gt; r) -&gt; r contRun (Functionish f) c = c f I made up that BiKleisli name; I don't think it's taken, but I'm not sure it's descriptive enough either.
Is this post on topic?
I suppose this could be neater with some arrow jiggery-pokery, but this is O(n), unless I'm mistaken: productsExceptCurrent xs = zipWith (*) (scanl (*) 1 xs) (tail $ reverse $ scanl (*) 1 $ reverse xs) 
I like the way in which this solution avoids a seemingly-extraneous typeclass, but without further limitations on `w` and `m`, I don't see which non-constant continuation you could pass to `contRun`. The typeclass solution doesn't have this problem because `run`'s caller could refer to `Src c` and `Dst c`. I think it's a step in the right direction though. The OP mentions that many of their core operations require a `Category`; which extra constraints would be needed to write an instance of `Category (BiKleisli w m)`?
GMP is written in C, so the test there is speed of calling out to C (Though Haskell's performance *is* impressive).
I disagree. The Ruby community has taken a huge amount of active work to become as inclusive as it is now (and there's so much more to do). Ruby is a large community, but it's so easy to accidentally form communities where a woman or a minority would feel very exposed, picked on, or just alone. All of which pushes them out. Working to make people feel welcome is always a good thing. Actively being conscious about the biases we all have and adapting to them can help. (example: blind CFPs for conferences is so easy, there's no reason not to). A code of conduct at events. Simple wording choices for individuals, being conscious about what makes others uncomfortable, calling out bad behavior in similarly privileged peers ("dude, not cool" - in response to an off color joke). &gt; try and grow the community itself. Best way to do that is to make everybody feel welcome. Men, Women, People of Color, noobs, experts, English speakers, non-english speakers, disabled people, etc, etc. I want everybody in Haskell. 
&gt; (if what you've built does it better then you should charge for it!)... It might be better for your own purposes without being better as a service. 
I don't like the idea. /r/TrueFoo is generally created when /r/Foo is overcrowded to a point the debate quality drops a lot. I don't think this is the case of /r/haskell. Indeed, I think /r/haskell is one of the best subreddits content-wise, while still unpopular. Quite the anti-case. Also, off-topic posts aren't allowed anyway, AFAIK.
Possibly part of the problem is a misinterpretation of replies. A site with comments is basically an open discussion. People go off on tangents, following up on topics aren't really relevant to what we said, but which nonetheless interest them. When someone picks up on a missing hedge it can certainly feel like a criticism, but that isn't always the intent. After all, maybe someone going off on a tangent shouldn't need to hedge their comment with "I know you didn't mean it that way and no criticism intended, I'm just going off on a tangent, but...". 
Social constructs can certainly be changed by the people within them, in much the same way that a forest can be removed with a handsaw.
&gt; This is my main beef with the subject. It simply does not belong to such specific subreddit as haskell. Incorrect.
Your Google+ signup is 404. Edit: Specifically the one here: https://www.fpcomplete.com/business/fp-haskell-center/ The one behind the Login link in the upper right works. Being logged in doesn't change the presentation of the Haskell Center page, though, which initially gave me the impression that it was a separate login; but I see that I do have Haskell Center access from my user page.
The subreddit solution didn't age very well, no way to update the things after the summer, etc. and leaving a bunch of artifacts all over reddit is rather unappealing.
May you elaborate? I don't know what you are talking about (=
Suppose I write main = print (foldl' (+) 0 [1..10]) Supercompilation can optimize this to main = print 55 [EDIT: corrected the optimization! Thanks /u/Mob_Of_One.] at compile time. Can "tree transducer composition" do that?
If you care about the health of your field/industry, and there's evidence that a significant number of people are being pushed out of it for unclear reasons, how is that *not* a problem to fix? It becomes a "discrimination narrative" because that's the only commonly-offered explanation that doesn't involve either handwaving and a lot of "well because it is, QED" or cringeworthy confabulations where engineers pretend to be experts on biology/psychology/economics/whatever.
Don't you mean? print 55 Or have I missed something?
Not only that, even on those *programming environments backed by large corporations* you only get to play with the set of approved languages. For example, on all my JVM based projects I can only hope for Java and on the CLR C# is the only game in town, with the clients we work for.
Actually yes? You can just beta-reduce `(foldl' (+) 0 [1..10])` until you reach normal form. My compiler doesn't include a supercompiler and it does that no problems.
Hi Tekmo, mind answering my post above? You don't really need supercompilation for this, right?
Um, yes! I deliberately didn't trust my mental calculation and checked in GHCi that the answer was 55 before writing down 10. Oops :)
Your compiler does that at compile time?
I've heard "Hokey-Cokey" arrows for those things. It's kind of a garbled mishearing of "Kleisli-Cokleisli", but with an added reference to the old song whose line "You do the Hokey-Cokey and you turn around: that's what it's all about!" is clearly a reference to the distributive law which is necessary to give the things the structure of a category.
Yes...
Firstly, interesting: what compiler is this? Secondly, why is this not a form of supercompilation?
Agree. Just as an hypothesis, after reading this article one might suspect that CS aptitude correlates with the Asperger spectrum. Both Asperger and autism have higher prevalence in males. Luckily, at least stating that isn't anti-feminist. http://archive.wired.com/wired/archive/9.12/aspergers_pr.html Since we don't know enough yet, we can't reject the idea outright, nor assume it as an excuse. Also, if something like that were the case, that'd be an argument for more diversity among programmers. But I'd point to this discussion for more concrete problems: http://www.reddit.com/r/haskell/comments/2klj3b/functional_programming_and_condescension/clo1pms More specifically, I've heard the argument that some advocacies of functional programming/Haskell have traits of arrogance/aggressiveness which are more typically male, and more likely to be tolerated by males. I find this interesting. And regardless of its influence on gender, the problem is there.
You may want to take a look at this paper of ndm's which gives an example of "word count" as a good hello world for supercompilation http://community.haskell.org/~ndm/temp/supero.pdf (He might have some other papers or blog posts that go into more detail here) Also the related work section discusses how it ties into deforestation, partial evaluation, etc. Supercompilation _is_ simple in general -- it is generally the possibility of accidentally introducing infinite unfoldings (via recursion) that makes applying it to real world programs hard.
&gt;is anyone actually using Julia? Yes but it's still in heavy development and probably will be for another couple of years. However, it is more than stable enough to be used in production code.
Haskell: Appeared in 1990; 24 years ago Julia: Appeared in 2012
&gt; In that case, your function would be a restricted form of `BiKleisli w m a b -&gt; r`, and it is probably best to avoid introducing Functionish at all Are you talking about the OP's `Functionish` typeclass, your `Functionish` existential, or both? &gt; unless you want to provide a restricted version to another module. I don't understand what you mean by that, whether you're talking about a typeclass or an existential. Could you give an example? &gt; I think a distributive function of type `forall b. w (m b) -&gt; m (w b)` would also work. I don't see how. Are you assuming that `w` is a comonad and `m` is a monad, or something like that?
I like it. I'll put that name in the "sorely tempting" column, along with the alternate constructor names data Sum (ts :: [*]) where Goose :: a -&gt; Sum (a ': as) Duck :: Sum as -&gt; Sum (a ': as) (so you can specify a value as `Duck . Duck . Duck . Duck . Goose $ 0`)
Providing the inversion function `w (m a) -&gt; m (w a)` explicitly (rather than using `sequenceA`) seems to make sense too.
Beta-reduction is not enough; there is more to equational reasoning than just beta. Consider for example `\b -&gt; 1 + (if b then 2 else 2)`. This is a beta-normal form (you can't reduce it further), and yet you want to optimize that into `\_ -&gt; 3`.
 f :: Bool -&gt; Bool f b = b || (not b) can optimize to f b = b `seq` True or maybe even f _ = True It doesn't sound like your solution does that.
Running the Julia code from that post on my computer, it spent 45% of its time in GC - basic bigint code in Julia does more allocation than it absolutely has to, if in-place API's were available. GHC's more sophisticated (and harder to understand!) in terms of how it deals with memory in this example.
I don't know what tree transducers are. I was just showing a real example of Tom's point.
I don't know much about tree transducers, but a classic supercompiler example that sounds pretty unrelated to trees is optimizing a naive O(n^2) string search to get something O(n).
&gt; Are you talking about the OP's Functionish typeclass, your Functionish existential, or both? Both. &gt; I don't understand what you mean by that, whether you're talking about a typeclass or an existential. Could you give an example? Just information hiding in general. Your library might use different `w` and `m` is different states, but require can consumers of the API make no (additional) requirements. &gt; I don't see how. Are you assuming that w is a comonad and m is a monad, or something like that? Yes. It hasn't been in my type signatures, but I was assuming `(Comonad w, Monad m)`, it was supposed to be implied by the BiKleisli name.
&gt; I was assuming (Comonad w, Monad m), it was supposed to be implied by the BiKleisli name. Forget everything I said, then. I was concerned about `(forall w m. BiKleisli w m a b -&gt; r)`, but I'm fine with `(forall w m. (Comonad w, Monad m) =&gt; BiKleisli w m a b -&gt; r)`. 
Why comparing Julia (2012) to MATLAB (1984) seems perfectly acceptable? But not to Haskell (1990)? I was merely pointing out the kinks so that people don't blindly buy the "high performance" sales pitch. 
Because you were comparing Julia to Haskell. If you'd have made similar comparisons between Julia and MATLAB I'd have made the same comment.
&gt; No project is being created with that ID This is the error I get when cloning a repo after it says "processing" 3 times. &gt; git@github.com:User/Project I also tried &gt; git@github.com:User/Project.git Both result in the same error.
&gt;If you care about the health of your field/industry, and there's evidence that a significant number of people are being pushed out of it for unclear reasons, how is that not a problem to fix? If there were evidence that women were being "pushed out of [the industry] for unclear reasons," then yes, it would be a big problem. Happily, no such evidence exists. &gt;It becomes a "discrimination narrative" because that's the only commonly-offered explanation that doesn't involve either handwaving and a lot of "well because it is, QED" How is "women in tech are discriminated against, and I don't really know why" any *less* handwavey or question-begging than "women just don't seem to be as interested in tech as men, and I don't really know why"? &gt;cringeworthy confabulations where engineers pretend to be experts on biology/psychology/economics/whatever. Yet these same non-experts may freely opine on complex issues of gender politics and social justice whenever it suits their comforting victimhood narrative. Curious.
I had to google it, but yes that works very well. So might "duck or grouse". But it's for the birds.
And by the way, Morte is very similar to what I'm doing after all, except for the part you know a lot more about typing system so I got pretty much an untyped version of what you did. And I'd like to let you know that Morte is Death in portuguese so that creeped me out a little heh, but I see the reference.
Just wondering, how many Haskell job candidates do you actually get? I thought a Haskell open would struggle to get someone to candidate...
[This paper](http://research.microsoft.com/en-us/um/people/simonpj/papers/henk.ps.gz) is a good place to start to learn more about this sort of type system.
Super compilation (or just partial evaluation) does more than that, by using eta expansion. A simple example, assuming you have a function of type `f :: Bool -&gt; T` and it is in normal form. This can still involve a lot of work when you apply it. Now transform this to `f' x = if x then f True else f False`. Now f True and f False can be computed at compile time and the function f' becomes trivial. This is sometimes known as The Trick. 
Sigh. Sure, tell yourself whatever you like if it helps you sleep at night. By the way, a glance at your comment history indicates that you suddenly felt the need to comment in /r/haskell to push your politics and don't normally participate here. So, word to the wise, you probably shouldn't push your luck being a douchebag here because you won't get much slack. Frankly, if defending misogyny is all you have to contribute regarding Haskell, do us all a favor and fuck off, ok? Go whine about made-up "gaming ethics" issues or whatever it is you people do for fun these days.
Awesome! (and thanks) I have your email right? (Ie I think I sent you a super belated email 1-3 weeks ago?)
No, I personally am not particularly interested in being welcoming to people who only come here to engage in political debates, because that's strong evidence that they're not likely to engage in discussions in good faith. This goes double when they're active in toxic subreddits and are pushing unpleasant views with disingenuous arguments. If you want to convince me your presence is a net positive to the community, try contributing something of value.
Yeah, that's me. Don't particularly want to dox myself on reddit but people who've communicated with me not via a screen name have a pretty good guess :)
No, just a happy user.
I used to use BitBucket a lot + Mercurial (hg still has a special place in my heart) but Github is more mature with better 3rd party tool integration...
I don't disagree with that choice (my primary OS is Ubuntu). I do, however, see many engineers turn their noses up to legitimately good paid solutions though. My comment was primarily targeting that and not the badness of choosing to use or build something free. Sometimes my internet opinions are a little bit caustic...
Haha, no. I've never once met someone using the "I'm just being logical!" defense who wasn't trying to deflect attention from the fact that they're an asshole (and usually nowhere near as logical as they want to believe). We really like our overly flattering self-images, but programmers are not, in general, any more logical than any other similarly-educated group. Which is why technical decisions are so often made based on fads and short-term convenience, not logical cost-benefit analysis. The programming industry and its culture really need to get the hell over themselves. (And no, Haskell programmers are not exempt from this, I'm afraid.)
To me, your "I'm very curious:" bit reads like an attempt to indicate genuine curiosity - to hedge against the possibility that "where did you get the idea that XYZ" will be interpreted as sarcastic or dismissive.
&gt; &gt; You feel uncomfortable &gt; No, I don't. Sorry, my "you" there was in general, not to you in particular. It also wasn't meant to be absolutely universal. Some people haven't (yet?) experienced this sensation of unease or fear, but it is something I have experienced (albeit in other contexts) and it is often reported. I'm not trying to say your experience is invalid or wrong or "just you" or anything like that. But, your experience doesn't seem to have been a problem. Instead, there are others with markedly different experiences, and I'd like improve their experience, without negatively impacting your experience.
&gt; &gt; Although, public spaces seem to be men's spaces. &gt; What? My mother used to tell me that some behaviors or speech were inappropriate for "mixed company". However, there's a widespread phenomenon were such behaviors or speech are presented, in public space, by men. In effect, men treat public spaces not as "mixed company" but instead as a men's space. I've not witnessed this type of behavior in person, but construction-worker catcalls are the stereotypical example of this phenomenon. 
&gt; Oh cool, I guess I finally get it, so Well no, because `f False == False`. A correct example is gasche's \b -&gt; 1 + (if b then 2 else 2)
I can recommend "Haskell - The Craft of Functional Programming" by Simon Thompson. Starts off assuming nothing and there are *lots* of exercises (no solutions, but you can find lots on github). It goes at a slow deliberate pace - great for an absolute beginner. 
I wonder if I name my son Simon, how good he'll be at Haskell.
I am using this to learn as a beginner and I can recommend it, the author makes concepts easy to understand and provides the reader with many useful tips to improve your programming. Not to mention the online version is free. http://learnyouahaskell.com
Otoh, he could also end up like Simon Ritter...
Woops, it was late AM, my bad. 
How would a supercompiler deal with the case of a lazy structure that might use a large amount of memory and thus cause performance to regress?
I've been using Learn You a Haskell for Great Good. It's available online for free or you can buy a physical book. The examples are quite helpful and the illustrations are entertaining. It starts right at the basics and explains concepts well.
Thanks, I have not found a example of the thinking functionally on the net so I cannot have a oponion about that. You are right that beginning haskell has a wierd project. 
I have looked at the first chapters but I find the first chapters good except that it is much about images.
I know that book but as far as I know no exercises I can do on my own.
&gt; I know that book but as far as I know no exercises I can do on my own 
The problem I've been having is that M-x describe variable haskell-mode-hook gives me &gt;haskell-mode-hook is a variable defined in `haskell-mode.el'. Its value is (turn-on-haskell-simple-indent) Original value was nil &gt; This variable may be risky if used as a file-local variable. &gt;Documentation: Hook run after entering Haskell mode. No problems result if this variable is not bound. `add-hook' automatically binds it. (This is true for all hook variables.) &gt;You can customize this variable. It was working until about a week ago, and I just want to get the GHCI to work when I C-c C-l again. Trying to start it from the toolbar prompts me to "create a project" - which I don't want. Executing both &gt;(add-hook 'haskell-mode-hook 'interactive-haskell-mode) &gt;(add-hook 'haskell-mode-hook 'inf-haskell-mode) Don't actually work, and I still get the same error as I do before. This was when Haskell mode was fully loaded, too, and no errors appeared when I executed the elisp. I'd rather it was just how it was before - I don't want to have to mess around with Cabal to get an interactive GHCI and it's far more convenient to use emacs than a terminal next to emacs. Is there any archive of old versions, and if not, how do I restore the old C-c C-l functionality?
I hope so, they're nice to listen to on my commute. They're usually just long enough that I can get half in the morning and finish it on my way home. 
Your original solution is far superior. If you want to do it, though, what you should do is scan for zero's first. One zero - calculate that product, everything else zero. Two zero - everything is zero. No zeroes - Calculate product of whole list and then divide. O(n).
&gt; I don't know if it can be done in linear time if we can't go from the back. Sure we can, `reverse` is `O(n)` so we can `reverse` the list and traverse forward.
It's a volunteer effort and takes a lot of work to produce, and find a time that works for both hosts and the guest. I'm sure everyone wants to keep it going, but it's unfair of us to expect it on a predictable basis.
I agree, it is subjective, and a charitable interpretation would be that I just want to understand your argument better. The uncharitable interpretation is "I'm very curious, where *(on earth)* did you get the idea that XYZ? Tell me the origin of this strange idea of yours so I can rip it to shreds!!" Whether you get the former or latter interpretation very much depends on where the other person is at, how it's delivered (if in person), and what the conversation has been like up to this point. My feeling is that the phrasings I gave tend to work better, and make it less likely that the other person will get defensive.
What do you think about the school of Haskell ? And one of these two courses : - https://www.fpcomplete.com/school/starting-with-haskell/introduction-to-haskell - https://www.fpcomplete.com/school/starting-with-haskell/haskell-fast-hard Roelof 
At my work we are 10 women, 1 guy and 3 students (a girl and two guys). Most the women have issues with each other, except the youngest one (age 19). If our boss (also a woman) talks hard to us, sometimes they get very hurd. Actually I have seen my boss and 2 nearest collegues cry on more than one occasion. If our boss talks hard to me, I just accept it or ignore it. I've actually discussed this with my boss (during a break). I told her I think it's about women being more subtle - interpreting little signs and so on. If some women are so sensitive all the time, I feel like there is no way they will ever be happy. There will always be something to complain about for a sensitive woman. For instance there will always be a way to interpret something a guy says as sexist. (I'm not saying some men don't have problems - it's just not about being too sensitive. It could be about being selfish or not sharing credit for succeses.)
As far as I am aware, it is still being produced! Just be patient… :)
[THe CIS194 lectures and excersises from UPENN are excellent.](http://www.seas.upenn.edu/~cis194/spring13/lectures.html)
That's fair enough
&gt; reality not mythology. "Mythology" implies that this behavior hasn't been documented recently or reliability. That's not the case. 
Downvoted for not linking (or otherwise presenting) evidence that post explicitly claimed to have.
&gt; I don't see which non-constant continuation you could pass to contRun Nor I. exists r. forall w m. (w a -&gt; m b) -&gt; r Since `r` cannot depend on `w` or `m`, it simply can't use that `w a -&gt; m b` in any meaningful way. There's no way to construct `forall w. w a` to pass in, nor is there any way to inspect `forall m. m b`. I mean the only thing you can do is some sort of strictness check: strictnessCheck :: (a -&gt; b) -&gt; () strictnessCheck f = f boom `seq` () where boom = error "failed strictness check: too strict" contRun someFunctionish strictnessCheck :: () -- either evaluates to () -- or throws the strictness check error -- or throws some other error internal to someFunctionish
Did you compile with ScopedTypeVariables?
Functors are things that are mappable: map : (a -&gt; b) -&gt; (f a -&gt; f b) Applicatives are things that are multi-mappable: map2 : (a -&gt; b -&gt; c) -&gt; (f a -&gt; f b -&gt; f c) By using map2, you can also get map3, map4, etc: map3 f xs yz zs = map2 (\fxy z -&gt; fxy z) zs $ map2 (\x y -&gt; f x y) xs ys This works by first creating a collection of curried `fxy = f x y`, and then applying `fxy z = f x y z`. We can go one step further: map4 f xs ys zs qs = map2 (\fxyz q -&gt; fxyz q) qs $ map2 (\fxy z -&gt; fxy z) zs $ map2 (\x y -&gt; f x y) Spot the pattern here: `map2 (\g x -&gt; g x) xs`. We can even rewrite the last call to also look like that pattern: map4 f xs ys zs qs = map2 (\fxyz q -&gt; fxyz q) qs $ map2 (\fxy z -&gt; fxy z) zs $ map2 (\fx y -&gt; fx y) ys $ map f xs Lets extract out the pattern: gs &lt;*&gt; xs = map2 (\g x -&gt; g x) gs xs Now we can write map4 like this: map4 f xs ys zs qs = (map f xs) &lt;*&gt; ys &lt;*&gt; zs &lt;*&gt; qs That the first argument is treated specially is ugly. We solve that by requiring an additional function `pure` that puts the function in a collection: map4 f xs ys zs qs = pure f &lt;*&gt; xs &lt;*&gt; ys &lt;*&gt; zs &lt;*&gt; qs
ReinH recently switched to working over at AlephCloud, so he's been pretty busy lately. It'll probably pick back up when he can get his head above water.
The same course is given on my first link 
So basically, "run" lets you unwrap wrapped functions? Except with the annoyance of Identity sprinkled in as necessary to appease the type system. How about tweaking the Src and Dst types so that we don't have to deal with that Identity stuff? class Functionish c where type Src c x :: * type Dst c x :: * run :: c a b -&gt; Src c a -&gt; Dst c b instance Functionish (-&gt;) where type Src (-&gt;) a = a type Dst (-&gt;) b = b -- run :: (a -&gt; b) -&gt; a -&gt; b run = id newtype Kleisli m a b = Kleisli { runKleisli :: a -&gt; m b } instance Functionish (Kleisli m) where type Src (Kleisli m) a = a type Dst (Kleisli m) b = m b -- run :: Kleisli m a b -&gt; a -&gt; m b run = runKleisli newtype Cokleisli w a b = Cokleisli { runCokleisli :: w a -&gt; b } instance Functionish (Cokleisli w) where type Src (Cokleisli w) a = w a type Dst (Cokleisli w) b = b -- run :: Cokleisli w a b -&gt; w a -&gt; b run = runCokleisli data Isomorphism a b = Isomorphism { runForwards :: a -&gt; b, runBackwards :: b -&gt; a } instance Functionish Isomorphism where type Src Isomorphism a = a type Dst Isomorphism b = b -- run :: Isomorphism a b -&gt; a -&gt; b run = runForwards newtype Dual k a b = Dual { runDual :: k b a } instance Functionish (Dual Isomorphism) where type Src (Dual Isomorphism) a = a type Dst (Dual Isomorphism) b = b -- run :: Dual Isomorphism a b -&gt; a -&gt; b run = runBackwards . runDual 
The problem is that the type signature for scanl is interpreted as completely independent of the type signature for the entire function. That is, you can imagine it as `c -&gt; (d -&gt; d) -&gt; d -&gt; d`. However, this is a big no-no, because you use `f` inside the function, which takes a `b` and an `a`, not a `d` and a `c`. A type signature like the one you gave is supposed to work for *all* `c` and `d`, but the implementation you gave only works for the case where `c` and `d` are `a` and `b`. Remember that by default, Haskell treats the `a` and `b` in scan as distinct from those in the top level type signature. like `a2` and `b2`. it's the same as if you did `(\x -&gt; foo (\x -&gt; bar x)) `.... the `x` inside the second lambda is distinct from (shadowing) the x of the first lamda. 
The problem you're encountering is because type variables aren't scoped in Haskell. That is, when you have the code: myFoldl :: (a -&gt; b -&gt; a) -&gt; a -&gt; [b] -&gt; a myFoldl f base xs = foldr scan id xs base where scan :: b -&gt; (a -&gt; a) -&gt; a -&gt; a scan b g x = g (f x b) `a` and `b` in the type signature for `myFoldl` are different than the `a` and `b` in `scan`. We can verify this by looking at the relevant bindings GHCI shows in the error message when I try it: myFoldl :: (a -&gt; b -&gt; a) -&gt; a -&gt; [b] -&gt; a scan :: b1 -&gt; (a1 -&gt; a1) -&gt; a1 -&gt; a1 You can see that the type variables for `scan` have been renamed as part of type checking, making them distinct from the type variables in `myFoldl`. When you leave out the type signature, then the type inferencer can see that the type variables have to be the same for `scan` and `myFoldl`, and thus type checking will succeed. Because they have to be the same, if you do put the type signature then they'll be renamed in `scan` and thus won't be the same, resulting in the error you're seeing. There's a language extension, `ScopedTypeVariables`, that makes type variables scoped if you want to include the type signature. The code can then look like this: myFoldl :: forall a b. (a -&gt; b -&gt; a) -&gt; a -&gt; [b] -&gt; a myFoldl f base xs = foldr scan id xs base where scan :: b -&gt; (a -&gt; a) -&gt; a -&gt; a scan b g x = g (f x b) And correctly type check. The explicit `forall` is required to actually bind the names `a` and `b`. Without it, they won't be scoped and you'll end up with the exact same error as before.
Thanks Chris! IMO this a paper all Haskell programmers should read/implement at least once. So I like that you're keeping the document alive and making it accessible in new ways.
In practice we usually just think of an `Applicative` as the natural way to take a `Monoid` and attach an argument, but we _can_ dive into it with more rigor. There _is_ a way to get these laws from category theory, through the notion of "Day convolution". There are others, such as the strong lax monoidal endofunctor way used in the original Idiom paper, or just viewing them as closed functors is also useful, but I'll focus on this approach here, because it ties them to monoids. If you have a category, we might be able to equip it with some kind of tensor to get a "monoidal category". A tensor is just a bifunctor over that category, which satisfies some conditions. It is associative, has a unit, and meets a couple of scarier higher order conditions. It isn't a monoid itself, but its sort of the bones you hang the mooid definition off of. # Monoid The cartesian product (,) with unit () is a tensor we can equip the category of Haskell data types with to get a monoidal category. (,) is a bifunctor, it has () as a unit, we can associate ((a,b),c) &lt;-&gt; (a,(b,c)) and (a,()) ~ a ~ ((),a) have the same information content if you ignore the spurious bottom we get from lifted products. A "monoid object" for (,) is m where we can take `(m,m) -&gt; m` and `() -&gt; m` with the obvious associativity and unit laws. We normally curry the first operation and call it `mappend` or `(&lt;&gt;)`, and drop the unnecessary `(-&gt;) ()` from the latter and call it `mempty` # Monad You've heard "a monad is a monoid in the category of endofunctors", well its just a monoid object for functor composition, which is also a suitable tensor. Here we're working with the 'category of endofunctors' over haskell, so our objects will all be functors, and our mappings between them natural transformations. Let's use ~&gt; for natural transformations: type f ~&gt; g = forall a. f a -&gt; g a and consider newtype Compose f g a = Compose { getCompose :: f (g a) } This is a "bifunctor" in the category of endofunctors in that it takes two functors and gives you a functor, and given a pair of natural transformations you can map over the two functors involved. bimapish :: (f ~&gt; f') -&gt; (g ~&gt; g') -&gt; Compose f g ~&gt; Compose f' g' This data type can be associated: Compose f (Compose g h) &lt;~&gt; Compose (Compose f g) h It has a unit, `Identity`, so since f is a Functor, we can write: Compose Identity f &lt;~&gt; f &lt;~&gt; Compose f Identity It also satisfies the penagonal coherence conditions, etc. Now the monoid objects of functor composition would have class Monad m where mappendish :: Compose m m ~&gt; m memptyish :: Identity ~&gt; m mappendish is basically `join` and memptyish is basically return. Hence, a `Monad` is a monoid object for the category of endofunctors over Haskell data types, using functor composition as the tensor. # Applicative Now, similarly, an `Applicative` is a monoid object for Day convolution mapping the Cartesian product to the Cartesian product. Here we're going to be working with the 'category of endofunctors' over haskell, so our objects will all be functors, and our mappings between them natural transformations. Let's take this rather unmotivated type: data Day f g a where Day :: ((a, b) -&gt; c) -&gt; f a -&gt; g b -&gt; Day f g c We'll call that "Day convolution" -- it is a special case of a bigger thing. This is a "bifunctor" in the category of endofunctors in that it takes two functors and gives you a functor, and given a pair of natural transformations you can map over the two functors involved. bimapish :: (f ~&gt; f') -&gt; (g ~&gt; g') -&gt; Day f g ~&gt; Day f' g' This data type can be associated: Day f (Day g h) &lt;~&gt; Day (Day f g) h It has a unit, `Identity`, so since f is a Functor, we can write: Day Identity f &lt;~&gt; f &lt;~&gt; Day f Identity I invite you to check these properties. It also satisfies the more esoteric conditions like Mac Lane's pentagonal coherence condition, etc. necessary to be the tensor for a monoidal category. Now if you look at the type signature of `liftA2` you can almost read it off that data type mappendish :: Applicative f =&gt; Day f f ~&gt; f mappendish (Day f g h) = liftA2 (curry f) g h Finally, a monoid object has a morphism (here a natural transformation) from the unit of the tensor (here Identity). Normally we're dealing with (,), where the unit is (), and so `() -&gt; a` is just a, but here we have to keep the Identity around. memptyish :: Applicative f =&gt; Identity ~&gt; f memptyish = pure . runIdentity Now the monoid laws you'd want, suitably restated, hold for `memptyish` and `mappendish`. So the take away is: An `Applicative` is a `Monoid` in the category of endofunctors, what's the problem?
And just for good measure. newtype AFunctionish k a b = AFunctionish { runAFunctionish :: Src k a -&gt; Dst k b } instance (Functionish k) =&gt; Functionish (AFunctionish k) where type Src (AFunctionish k) a = Src k a type Dst (AFunctionish k) b = Dst k b -- run :: AFunctionish k a b -&gt; Src k a -&gt; Dst k b run = runAFunctionish asAFunctionish :: (Functionish k) =&gt; k a b -&gt; AFunctionish k a b asAFunctionish = AFunctionish . run Not really sure what this is good for, but there it is.
Yes, there are 3 main features that make Julia efficient. The first is way Julia represents data types. In a normal dynamically typed language your collection types store any values. There is no concept of a collection of integers, there are just collections that happen to store only integers, but it would be perfectly fine to store a string in the middle of it. That makes the representation inefficient: you basically have to box everything, although you can special case some types by pointer tagging. In Julia this is different: there is a concept of an array of integers, array of floats, array of complex numbers, etc. Those will have the same representation as in C: a flat piece of memory storing a bunch of integers/floats/complex's. That's the same way it works in Numpy, but the difference in Julia is that the entire language is designed from the ground up around that concept. The second trick up Julia's sleeve is type specialized compilation. Functions in Julia can have type arguments, and when such a function is called, the JIT compiler will compile a new version specialized to that type argument (and cache it). This allows generic code to be very efficient. You can write 1 function that does matrix multiplication, and when you call it on a matrix of integers it will use integer arithmetic without any dynamic dispatch, and when you call it on a matrix of floats it will use floating point arithmetic without any dynamic dispatch. The third trick up Julia's sleeve is multimethods. You can dispatch on that type argument, which allows you to write a function specific to that type. For example the matrix multiply function on floating point matrices can call to a heavily optimized BLAS routine written in C/Fortran, but when you call it on a matrix of symbolic numbers it will continue to work with symbolic arithmetic.
Or alternatively look in Section 7 of "Applicative programming with effects" by Paterson and McBride. Of course, the whole paper is an excellent read.
Very interesting response, although I think you have an error in your definition of `&lt;*&gt;`, where you don't refer to `gs` or `xs` at all in the body. Did you mean something like gs &lt;*&gt; xs = map2 (\g x -&gt; g x) gs xs for the definition? Also, why doesn't the `Applicative` class enable implementation based on `map2` as an alternative to implementation based on `&lt;*&gt;` (like the `Eq` class enabling definition of either `==` or `/=`)? It seems like it would be a useful pattern.
Very cool! This worries me a bit, though: &gt; To distinguish the code for the typechecker from program fragments that are used to discuss its behavior, we typeset the former in an \`italic\` font, and the latter in a \`typewriter\` font. Is that distinction lost in this conversion?
Applicative isolates potentially effectful computation inside a type similar to the use of the IO monad. The map2 as listed here is nearly equivalent to liftA2 but evaluates in an applicative. It takes a 2 arg function, 2 arguments and evaluates them , returning the result in an applicative. To do the same in a pure function you could use bimap. 
&gt; I have not found a example of the thinking functionally on the net You can get a sample chapter or two on the Kindle (the device, or the program for your desktop). Look for it on Amazon.
You could try [H-99: Ninety-Nine Haskell Problems](https://www.haskell.org/haskellwiki/H-99:_Ninety-Nine_Haskell_Problems). These aren't tied to any particular book, but are a very good set of beginner problems. 
In fact I also miss new [videos](https://www.youtube.com/user/jekor/) from jekor since he also got a job. Seems like good haskellers find avoiding success harder and harder.
A windows user/developer: You can drop XP but not Vista. There are still a great many laptops out there which have Vista factory images, especially in Schools. I can't really think of any API you'd want to use which is minimum Win7. Care to enlighten?
~~That doesn't really help if the monoid is not commutative :) Whether we traverse the original list backwards, or traverse the reverse list forward, we're still composing it the wrong way.~~ Imagine that instead of multiplication, we're supposed to concatenate the numbers in base 10. Hmm, actually... this would still work, thanks to associativity. So I guess we don't need commutativity after all.
&gt; Did you mean something like &gt; gs &lt;*&gt; xs = map2 (\g x -&gt; g x) gs xs &gt; for the definition? Yes, thanks! &gt; Also, why doesn't the Applicative class enable implementation based on map2 as an alternative to implementation based on &lt;*&gt; (like the Eq class enabling definition of either == or /=)? It seems like it would be a useful pattern. I don't know the reason, but if you can provide map2 then it's trivial to provide `&lt;*&gt;` since it's just `(&lt;*&gt;) = map2 (\f x -&gt; f x) = map2 ($)`. As LogicOfFailure notes, map2 is called liftA2 in Haskell, so you do get it the other way around: if you provide `&lt;*&gt;` then you get `liftA2` for free.
I've started making my own [Haskell videos](https://www.youtube.com/watch?v=7mUxp-xrcUw) a few weeks ago. They're in a similar style from jekor's, in the sense that I'll be explaining existing Haskell code and writing new one.
Hi folks. Chris and I have both been pretty busy lately but we're recording the next cast later this month!
Excellent answers here. I'll try to throw in a slightly (just slightly) intuitive one. One of the most important facts about `Applicative` is that every expression built up out of uses of `pure` and `&lt;*&gt;`, no matter how many uses or how they are nested, can be rewritten in one of these two "linear" forms: pure a :: f a where a :: a pure f &lt;*&gt; x1 &lt;*&gt; ... &lt;*&gt; xn :: f a where f :: x1 -&gt; ... -&gt; xn -&gt; a x1 :: f x1 . . . xn :: f xn Another way of putting it is that any applicative action can be seen as a pair of: 1. A **list of actions** in the applicative functor, heterogeneous on their type parameters; 2. A **pure function** that takes as arguments the parameter types of those actions. Note that this ties to many intuitive explanations of the differences between `Monad` and `Applicative`. One common observation is that: * A `Monad` action is able to observe the values produced by its component actions and use them to choose the next action. This is embodied in the type of `(&gt;&gt;=) :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b`. * An `Applicative` action cannot do that. It can only execute some predetermined actions and combine their results with pure functions. So the `Applicative` laws basically say that no matter how many uses of `pure` and `&lt;*&gt;` you get in an expression and how they are nested, those uses can be resolved into a definite sequence of actions and one combining function. Seen from this angle, the `Applicative` laws for `&lt;*&gt;` are a set of rules for rewriting any `Applicative` expression into this form. One good exercise to appreciate this would be to write the `Functor` and `Applicative` instances for this type: {-# LANGUAGE GADTs, RankNTypes #-} import Control.Applicative (Applicative(..), (&lt;**&gt;)) -- | The type of free 'Applicative' functors. This type represents a -- heterogeneous list of @f@ actions, with a @Pure@ function at the -- end that combines their results. The ':&lt;**&gt;' constructor should be -- seen analogously to the '&lt;**&gt;' operator from 'Control.Applicative': -- it's like '&lt;*&gt;' but in opposite order. data Ap f a where Pure :: a -&gt; Ap f a (:&lt;**&gt;) :: f x -&gt; Ap f (x -&gt; a) -&gt; Ap f a infixl 4 :&lt;**&gt; -- | If you know how to translate any @f x@ into a corresponding -- 'Applicative' type, you can translate an 'Ap' into an action of that -- type. runAp :: Applicative g =&gt; (forall x. f x -&gt; g x) -&gt; Ap f a -&gt; g a runAp f (Pure a) = pure a runAp f (fx :&lt;**&gt; k) = f fx &lt;**&gt; runAp f k -- -- EXERCISE: write these! -- instance Functor f =&gt; Functor (Ap f) where ... instance Applicative f =&gt; Applicative (Ap f) where ... The implementation of these `instance`s basically boils down to the `Applicative` laws.
So, we could also consider the comonoid objects to get natural comonoids and comonads. -- uninteresting without linear lambdas instance Comonoid a where comappend :: a -&gt; (a, a) comempty :: a -&gt; () instance Comonad m where cojoinish = m ~&gt; Compose m m coreturnish = m ~&gt; Identity Is there anything interesting about a coapplicative formed this way? 
Great news, thanks for your effort!
I think they all wind up boring like the normal comonoids in Set. Hask^N borrows almost all of its interesting structure from Hask and Hask has no real interesting comonoids. There is a notion of a contravariant applicative you get when you look at contravariant Day convolution though. That leads to http://hackage.haskell.org/package/contravariant-1.2/docs/Data-Functor-Contravariant-Divisible.html
Good to hear! Would like to thank you in advance for all your hard work.
Oh. Excellent. I don't know how I missed this looking around. Thanks!
Will check them out. Really liked Jekor's videos. I have a hard time understanding some of the Haskell talks with slides from SPJ (even though he is very charming), or worse - writing out Haskell on a poorly lit blackboard, but good HD screencasts with walk-throughs, explanations, "how I work" etc are always fun and welcome.
Was just going for a walk tonight and listening to the discussion with Chris Doner - really high quality show, would love to get new episodes, even though I still have some older ones to look forward to.
Given our function: permWithRep :: Int -&gt; [a] -&gt; [[a]] permWithRep = replicateM that stands for "permutations with repetitions". We can create this infinite list fairly easily: allSets :: String -&gt; [[String]] allSets str = map (flip permWithRep $ str) [0..] which can be used as: take 3 . allSets $ "ab" -- [[""],["a","b"],["aa","ab","ba","bb"]]
&gt;fmap (\`replicateM\` "ab") [0..]
Is there a way to map the function "toFruit" into a list of Apple, Orange and Banana (so you don't have to explicitly apply it to every element)?
Without using `replicateM`: allSets s = iterate ((:) &lt;$&gt; s &lt;*&gt;) [[]]
Thanks. I've emailed our busness team about it to update that link. We had to change things around since Google has deprecated OpenID logins in favor of OAuth logins.
I got my first report of this issue a few days ago, and we're looking into it now. This only occurs for some people, not universally, so we're still trying to figure out what the trigger is. (It's also why it slipped through testing, for some reason the dev team isn't affected by it.) I'll try and post an update here when there's more information.
It seems like it would be sound, though termination might be trickier to check.
Speaking of sound, this would be a great band name.
Cartazio, I initially wrote much of Julia's sparse support. Looking forward to your blog post - since I am curious to see what all comes up, and how we can make our sparse support better. All of Julia's sparse support is written in Julia itself, which makes the codebase quite flexible. However, for sparse solvers, we use Tim Davis' tried and tested high performance versions. With enough free time, we can write those in Julia too, but for now, that's not the best use of one's time. We constantly grapple with having a good API that just generalizes to sparse, but we haven't found something we like, and often the abstractions lead to code that is not fast enough.
Well, at least this seems to make GHC 7.8.3 happy: class Magic a b where magic :: a -&gt; b instance (Coercible (TF a) (TF b)) =&gt; Magic (D2 a) (D2 b) where magic (MkD2 x) = MkD2 (coerce x) 
http://codewars.com has exercises in Haskell.
It's from 2007 though. Isn't it a bit outdated?
For a fuller intuition I like to think about not just "effects", but also "plain old data structures". For example the fundamental operation of a monad is `join` (which I would prefer to call `flatten`, but I'll go with the established name): join :: m (m a) -&gt; m a Instantiating this to lists: join :: [[a]] -&gt; [a] it's obvious what this does: it concatenates the lists. Instantiating it to trees: join :: Tree (Tree a) -&gt; Tree a after only a little bit of thinking, it's *also* obvious what this does: the input is a tree with trees at the leaves; the output is just a tree. We can look at the input as one big tree, and `join` is just erasing the artificial boundaries inside it (they refer to this as "tree grafting"). (Likewise `[[a]]` can be thought of as one big list, with `join` erasing all the internal borders.) This gives me the intuition that a monad is in some way about throwing away information: we can't go backwards from `[a]` and rediscover where the boundaries in the `[[a]]` were. This holds up for other popular `Monad` instances: if `join` gives us `Nothing`, we can't tell if the input was `Just Nothing` or `Nothing` (and, probably, don't care); likewise we can glue an IO action returning an IO action `IO (IO a)` into just a single action `IO a`, but can't take it apart again. How this relates to `Applicative` is that, going with the `Monoidal`/`merge` formulation from another comment which is again in some way more fundamental: merge :: f a -&gt; f b -&gt; f (a, b) Instantiating this to trees: merge :: Tree a -&gt; Tree b -&gt; Tree (a, b) I have *no idea* (at least intuitively) what that might be doing, and thinking about "effects" doesn't make it any easier. I happen to know (even though this is also not immediately intuitive) that `merge :: [a] -&gt; [b] -&gt; [(a, b)]` is like a cartesian product, returning all possible combinations of an element from one list and the other, but this *still* doesn't make it obvious to me what the tree `merge` does. Do you perhaps have any thoughts along these lines? (Another project is to figure out `Comonad` in a similar way (something where you can only *add* information, but not take it away...?), but I haven't gotten very far with that one either. Yet.)
This part of Haskell did not change (and will probably never)
This is probably slightly an updated version https://courses.edx.org/courses/DelftX/FP101x/3T2014/info but the Functional Programming Paradigm doesn't change verey much since 2009 :)
The fundamentals of anything don't change much in five years.
Well, there's AngularJS...
I think they introduced enterprise functors in 2011 and figured out how to do functional dependency injection in 2012. But the fundamentals didn't change much AFAIK.
If AngularJS is fundamental then I'm in the wrong universe.
Awesome. Might be a week or several before i get to writing things up mind you. I think at the very least the approach I've done has pretty nice specialization characteristics, and would probably push Julia's trait/interface approach to resemble haskell's if you wanted a good performance story with a good UX. edit: to clarify, to make my engineering sane, I've definitely had to lean on some pretty fancy bits of Haskell use them in pretty fundamental ways. Eg one neat thing i've done for Dense Arrays is i've managed to write their address translation codes such that the seemingly recursive computations get unrolled into their non recursive form at type checking time. And the resulting non recursive code is quite easy for the compiler to optimize!
Microsoft are supporting Vista [until 11th April 2017][1]. [1]: http://support2.microsoft.com/lifecycle/search/?sort=PN&amp;alpha=Windows+Vista&amp;Filter=FilterNO
and what about Applicative =&gt; Monad ?
Your objections seem based largely on style. This raises the question of what style of tutorial you're looking for - for example, what tutorials for other languages have you found helpful in the past? One thing you should consider is that you may be blaming the tutorials for some issues that actually lie elsewhere. For people who already have significant experience in traditional "imperative" programming languages, Haskell can be quite difficult to get to grips with, because so many of its basic assumptions are different. Someone who knows C can fairly easily learn Python or Java or Basic, because they all use the same basic computational model. Haskell doesn't. If you're encountering this issue (and it sounds like you may be), then the reality is that you will need to do some re-learning in order to understand Haskell, and no tutorial is going to be able to turn that into an effortless task. 
It really hasn't changed much since 1958
Regarding pattern synonym types being "one of the last pieces of the pattern synonyms implementation we've been missing", is there any hope of getting template haskell support for pattern synonyms?
It is crap, but it is crap that is still widely installed.
This is valid: (++) list1 list2 wrapping infix functions in brackets makes it behave like a normal function. For not being able to append to a list, it's because of the implementation. If you've come across ~~abstract~~ algebraic data types, a list is defined as: data [a] = [] | a : [a] So : is actually a constructor for the type. This structure makes it difficult to append. Alternatively, there is Data.Sequence.Seq which does allow appending, but has a more complicated structure. For comparisons, lexicographical ordering is dictionary ordering. So for example, "abc" would come after "ab" in a dictionary, so "abc" &gt; "ab". "abc" is a list - ['a', 'b', 'c'], as is ['a', 'b']. Replace the elements with numbers, and you get the same reasoning. Specifically, it is defined similarly to this: [] &lt; [] = False -- They are equal, so not &lt; [] &lt; (x:xs) = True -- The empty list comes at the beginning (x:xs) &lt; [] = False -- Same as above (x:xs) &lt; (y:ys) | x &lt; y = True | x &gt; y = False | x == y = xs &lt; ys
Functional dependency injection?
Ok thanks, that makes everything clearer! Especially for the list comparison. I thought that lexicographical ordering should mean dictionary ordering, in which case the list comparisons made sense. But Learn You a Haskell says: &gt; Lists can be compared if the stuff they contain can be compared. When using &lt;, &lt;=, &gt; and &gt;= to compare lists, they are compared in lexicographical order. First the heads are compared. If they are equal then the second elements are compared, etc. which (to me) implied that comparisons were element-wise. 
&gt; If you've come across abstract data types Algebraic data types. The data constructors here are exposed, so it's not an abstract type. I'll add that we need to parenthesize the infix operators to use them as prefix to disambiguate parsing. x - y vs. x (-) y Are parsed very differently. 
They are, with the caveat that 'not present at all' comes before things you can compare, just like in a dictionary. 
Also called "partial application".
AFAIK the changes in Haskell haven't made the old concepts obsolete or even, except in maybe a few edge cases, changed how you'd express them. The type system has got more flexible and powerful, but that just means you have some more options. You still use the Haskell 98 techniques by default. 
The key to a supercompilation is it's implicit and global. anybody from the newest noob to the most grizzeled veteran gets the let a = 1+1 optimization of a=2. Transducers seem to be narrowly applied to specific data structures. So, if i'm not aware enough to use a fold, and i write my own increment each element of a list function, I don't get the advantage of transducer (1), a supercompiler *could* still eliminate that intermediate list. (i don't think they do, but they could) Transducers are easier, because the libraries and compiler have more info about what you're trying to accomplish. General partial evaluation is hard. 1 Disclaimer: Not sure i understand exactly what you mean by transducers, I'm assuming it's something like clojure transducers, and i'm kind of hazy about their exact details.
I think the best reference for understanding the theory behind applicatives is Ross Paterson's excellent paper: Constructing Applicative Functors http://openaccess.city.ac.uk/1141/1/Constructors.pdf
&gt; Algebraic data types Yes, that's what I meant, my mistake.
If you want a list of booleans instead of a single boolean result when comparing lists, you can use `zipWith (&lt;) list1 list2`
Nixos has a declarative package manager.
You might find Kiselyov's ZFS[0] interesting. [0]http://lambda-the-ultimate.org/node/1036
&gt; Because it is a normal singly linked list. That has nothing to do with haskell, it is true in any language. Got it--that makes perfect sense. &gt; Why on earth wouldn't it? It does in ruby too: A Ruby string is not a list, though. Ruby does not support infix operator array comparison, so that's apples to oranges. /u/TastyPi's explanation explains it well. The list comparison operator isn't comparing the last element of the first list to 'nothing,' or more specifically, Nothing. If it were, the comparison would throw an error. It was that line of reasoning that was confusing me in the first place. 
Ok awesome. The fact that a list comparison doesn't return a list of booleans makes sense, now that I know it's comparing the lexicographical ordering. If it were element-wise comparison, the two lists would need to have the same cardinality.
Have you seen [urbit](http://vimeo.com/75312418)?
&gt; why can't we use prefix notation for built-in functions? For functions often used infix style (e.g. *a + b*), Haskell offers a special notation to allow use of infix functions in prefix style: Surround the infix function in parentheses: (+) a b &gt; Why is it that prepending an item to a list with the cons operator is fast, but appending requires traversal of the entire list? This is not a flaw in Haskell, or in functional languages, but an inherent attribute of linked list data structures. Operations on the last elements take longer because it takes longer and longer to reach the end of the list, by having to traverse the elements. &gt; I'm just curious as to why there is no built-in append function. Use `++`: [1, 2, 3, 4] ++ [5] Why does list comparison return a scalar instead of a list? But more importantly, why on Earth does this expression evaluate to True? Think about what `&gt;` means in the context of data structures. It often means structure A contains a larger element where B contains a smaller element, or that A has more elements. [4, 5, 6] is &gt; than [1, 2].
Feel free to contribute by spending a few minutes searching for a preprint that isn't available yet, and sending a pull request.
This is not so efficient with, for example, data D3 a = MkD3 [TF a] I want a function coerceD3 :: (Coercible (TF a) (TF b)) =&gt; D3 a -&gt; D3 b that boils down to "id" in the implementation.
In my dreams I can just write class Applicative f =&gt; Functor f where fmap = (&lt;*&gt;) . return and have the system check (or ask me to prove), when I provide an `Applicative` instance for a type that already has `Functor`, that coherence is not broken by the above construction. It looks like an area where a little bit of automatic algebraic reasoning could remove a lot of design complications (both proposals would qualify as *complex* in my book). Plus it would give us an incentive to *write laws down*.
have a look at log structured file systems as well.
There's several archived email threads re Coercible -- one might certainly contain a discussion of this example/issue. I was, however, unable to find it. I also found no mention of this example/issue in the documentation I've located so far: * https://ghc.haskell.org/trac/ghc/wiki/NewtypeWrappers * https://ghc.haskell.org/trac/ghc/browser/ghc/compiler/typecheck/TcInteract.lhs#L2046 * https://ghc.haskell.org/trac/ghc/wiki/Roles Recently "types roles" were added to GHC's coercions; the primary motivator was to soundly re-implement GeneralizedNewtypeDeriving (GND). The "lifting instances" of Coercible are very similar to what was needed for GND, so I suspect it was tempting to adopt the role-based approach to "lifting" for Coercible too (esp. since coerce was itself targeting GND). But I'm not sure (at least from the docs) how thoroughly the consequences of that choice were considered. It seems like GND (and hence roles) needed less expressive lifting than does your intention here. In particular, this sentence on the Roles wiki page is pretty misleading: "The idea is that a Coercible instance exists allowing coercions between any two types that are representationally equal." I think Coercible is pretty far from achieving that idea, currently (at least for the lay interpretation of "representationally equal"). As to would your lifting instance be sound? Intuitively, yes; it feels right to me. But can a well-typed GHC Coercion express that? I'm not sure. The evidence for a Coercible instance is a GHC Core coercion (cf. the first goal on NewtypeWrappers). The lifting instances of Coercible currently use the "Type constructor application" (cf the L2046 link above). I took a quick look at Richard Eisenberg's GHC Core formalism here https://github.com/ghc/ghc/blob/master/docs/core-spec/core-spec.pdf --- click the Raw button to download it). The type rule for that coercion constructor (Co_TyConAppCo) indicates that it is strongly correlated with roles. I suspect the Coercible lifting instances you want would essentially have to introduce matching equality Core axioms (like newtypes and type (family) synonyms do, but these would be parameterized), and that probably doesn't appeal to the formalists. Or else the "Type constructor application" coercion would have to change, which is probably also widely unappealing. Roles in GHC are a simpler adaptation of the ideas in http://www.cis.upenn.edu/~sweirich/abstracts.html#weirich:newtypes. Maybe that paper's machinery treats this -- I don't know. I would suggest emailing the question to ghc-devs cc'ing Joachim Breitner, Richard Eisenberg, and SPJ, if you don't get any solid responses here. HTH. Great timely question.
[Some discussion on Hacker News about an immutable operating system](https://news.ycombinator.com/item?id=7166173)
It seems to me Vista is very close to 7, minus some capabilities, cosmetics and more aggressive (annoying) UAC. I don't think it will cost a lot to MS to support it.
&gt; why isn't there an equivalent to: &gt; &gt; "THIS IS A LIS":'T' Due to the nature of linked lists (the way the `String` datatype is implemented in Haskell), that would be a *very* expensive operation. Since it's so expensive, someone decided to not include it, because then people can't use it by accident. If you want efficient handling of text, make a note to eventually learn the `text` library. The `text` library provides the function `cons` that works like `(:)` does for `String`: λ&gt; cons 'H' "ello, world" "Hello, world" and *also* the `snoc` function that does exactly what you talk about: λ&gt; snoc "Hello, world" '!' "Hello, world!" The reason it can do this is that `Text` values are not built on linked lists, but some other much more efficient datatype, which means `snoc` is a comparatively cheap operation.
Urbit has a referentially transparent file system, `%clay`, that lives on in `arvo`. It kind of has to sit on top of a `unix` sync layer, though. Theres no built in editor, but once changes get synced into `%clay`, they are referentially transparent. Or you can just store data directly in your app with `%gall`.
This release focussed mostly on bug fixes, since we spent quite a lot of the time working on the self-hosting compiler implementation (tabled for now due to performance issues). Thanks to all of the contributors who made this release possible - this release has a record number of compiler and library contributors. Also, the new website design is now online, thanks to /u/gb__ and [@goodworkson](http://twitter.com/goodworkson), and any suggestions regarding the website content would be greatly appreciated! http://purescript.org 
Thomas Schilling (https://github.com/nominolo) wrote a tracing JIT for Haskell which yielded good results, but at times was let down by the lack of information available at the Cmm (I think?) level.
&gt; the standard atomic unit of reference is the function Why? Shouldn't the atomic unit of reference for a "file system" be a "file"? The hash thing you talk about refers to files, not functions. &gt; directories could be replaced by categories I'd say directories could be replaced by a category's objects. Files reside within the object. The operation of changing directories is a morphism from one directory object to another. I dunno, this sounds like how file systems already work.
There's an open ticket for it at https://ghc.haskell.org/trac/ghc/ticket/8761; I don't know of anyone looking into it yet, so feel free to jump in!
I wrote an essay surveying various attempts at operating systems written in functional languages during my undergraduate. It's not great but the big list of references might point you at something interesting: http://files.kitb.pw/essay-fp.pdf
On the file system level, you already have "executables" such as "mv", "rm", "cat" which are "verbs", and data files which are "nouns". What is considered "noun" and what is considered "verb" is actually fuzzy, since you can compile code (data) into executable. Maybe you are thinking about a system where "data" doesn't persist? This is already the case for embedded system (e.g. DVD) What you described in the 2nd paragraph is actually very similar to a database. Database use types (row/document definitions), and they already store these entries by RowIDs. 
I don't know if you fully understand what "functional language" means
The link to http://purescript.readthedocs.org/ in the README does not work.
Functional doesn't just mean "verb noun". You need immutability too, for example. Basically, like a database transaction log where there are no updates or deletes, only inserts. Deletes happen when nothing refers to the old forms anymore. I think ZFS may or may not satisfy these.
Thanks I'll fix that now.
Some years ago an ICFP contest made us expore an "operating system" that was written in SML. And then there is the Plan9 filesystem, which appears to have certain immutable characteristics, though I do not think that it has an FP underpinning.
I can only recommend the purescript book [Purescript By Example](https://leanpub.com/purescript). It explains purescript (of course) but also has a very good explanation of the most important concepts of functional programming. The only exception is the explanation of monads: I guess if you don't already know the concept, you won't get it from that book. But the rest is really nice, I liked especially the examples. They worked without any problems, something I really appreciate on a book. I read the book on my Kindle and the source code formatting was OK and the figures were readable (something that is hard to achieve for technical books, because of the hardware limitations on the Kindle).
I wish I had a need to use it!
sgraf812, this is your boss, i'm gonna need you to copy a silly flash game of your choice by next week. 
&gt; It was much more stable than Linux was at the time http://i.imgur.com/Ufbr5ej.webm
I tried that at first, but ruled it out when I realized I needed to use the partially-applied types `Src c` and `Dst c` elsewhere in my code. 
What is a file? Is it legacy metaphor plucked from a 70s office environment as was the desktop, the file (system) cabinet, print, terminal, move, tabs, spaces, keyboards, or a real phenomena? With in an operating space there needs to be a strategy of reference, a name space, something, but it doesn't have to exist as a two dimensional pseudo-geometry. A purely logical space could be presented but a user is preoccupied with the concept of **where**. Asking where is the data is not asking what, and that's how our brains work I feel. It would be possible to declare what I want: I want bitstring sha1:ou8odt… Not only does such a non-file system act locally but universally. Networked computers wouldn't be requesting place, "I want data at this path", or "You need data at this path", instead a computer is able to again declare, check locally then use any available store to retrieve the actual desired bit sequence. Likewise I feel I want to speak to `publickey` would be relevant. Every "file" or file though is always the input argument to another function. A functional OS becomes constructed from a fractal of the same metaphor: a process is a function, a function is a function, a networked service is a function, a stream is a function, a connected device is a function—all boxes operating on some on data patterns. A printer is a function, it turns digital representations of data to inked versions. For me it makes no sense to have this be a file. As for human readability, I'd shift what was the name (or creation date if desired) of the file (or bit sequence) inside the bit sequence as integrated meta data. Change the name, change the hash output. If you want persistent editing then one can embed the previous hash of the previous edit. There is no reason why names cannot be the same, the whole: jimmys-birthday-0001.jpg, jimmys-birthday-0002.jpg, jimmys-birthday-0003.jpg …is insane. Likewise you can have two identical bit sequences with different file names currently. On top of that one may then logically project a local tagging or local unique naming scheme for human readability via aliasing. It doesn't however become the hacky windows short cut when you want to express logical membership to two different directories (groups). There will be much more to say and refine. The obvious problem is maintaining up to date references—if hash is the reference a new source code update wouldn't have this change known by all the functions that "call" it. I think this can be solved via software support.
And declaration. A file system demanding a user prescribe **where** the data exists inside a pseudo geometry, is not saying what data the user wants.
Okay so this months side-project will be about PureScript! .. How about purescript yesod bindings? would those be useful to anyone?
Yep, that course is a simplified version of the channel 9 lectures (c9 is ~50min per topic, while edx is ~20 min). Both have the same structure and use the same book as guide. I've been using the channel 9 videos to get a bit ahead of schedule on the edx course and they work nicely together.
Here are two theses on the two topics you're interested in. (Bonus points: since they're for a PhD, they are fairly complete reads, so if you stick to it, it should hopefully help a lot!) - Thomas Schilling, University of Kent - "[Trace based Just-In-Time Compilation for Lazy Functional Programming Languages](http://files.catwell.info/misc/mirror/tracing-jit-haskell-schilling.pdf)", April 2013. - Max Bolingbroke, University of Cambridge - "[Call-by-need Supercompilation](http://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-835.html)", May 2013. This paper actually *does* address a GHC-specific optimization, which [Max implemented as a GHC plugin](https://github.com/ghc/ghc/tree/supercompiler), but it is unfortunately really old and broken at this point. I'm sure someone dedicated enough could maintain it and release it, however. :) Max and SPJ also had a shorter paper published a few years ago about the same topic; "[Supercompilation by Evaluation](http://research.microsoft.com/en-us/um/people/simonpj/papers/supercompilation/supercomp-by-eval.pdf)". Neil's paper is also really good and does address other good insights, but it's been too long for me to tell you the difference between Max's (CHSC) work and Neil's (Supero) work.
This looks like something which you would be interested in: https://github.com/mpietrzak/yesod-purescript
Awesome resources, many thanks!
Cool! Thanks!
The way urbit does it is to encode that as `/ship/desk/version/jimmys-birthday` which guarantees global uniqueness even across an entire network. (`version` could also be a date or label)
 allSets str = let x = [""] : map (\ss -&gt; concatMap (\c -&gt; map (c:) ss) str) x in x
I've been using PureScript to Quickcheck some React Elements. It's been working really well (although my current implementation is super hacky).
A toy compiler or interpreter would be my choice. A Brainfuck interpreter should take you no longer than a couple of evenings to crank out. If you properly separate the VM and the parser at the start, you're all set to iterate like mad on that thing, replacing brainfuck with a nicer (minimalistic) language, making the memory and IO models richer, etc etc etc.
Great idea, but the first three exercises of the Undergraduate Course I am shadowing dealt with compiling a simple language, so I don't think I could away with doing it twice.
Maybe [Dawkin's Weasel](http://en.wikipedia.org/wiki/Weasel_program)? I've done it in python and think it wouldn't be too hard in Haskell either.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Weasel program**](https://en.wikipedia.org/wiki/Weasel%20program): [](#sfw) --- &gt; &gt;The __weasel program__, __Dawkins' weasel__, or __the Dawkins weasel__ is a [thought experiment](https://en.wikipedia.org/wiki/Thought_experiment) and a variety of [computer simulations](https://en.wikipedia.org/wiki/Computer_simulation) illustrating it. Their aim is to demonstrate that the process that drives [evolutionary](https://en.wikipedia.org/wiki/Evolution) systems — random [variation](https://en.wikipedia.org/wiki/Mutation) combined with non-random cumulative [selection](https://en.wikipedia.org/wiki/Selection) — is different from pure [chance](https://en.wikipedia.org/wiki/Chance_(philosophy\)). &gt;The thought experiment was formulated by [Richard Dawkins](https://en.wikipedia.org/wiki/Richard_Dawkins), and the first simulation written by him; various other implementations of the program have been written by others. &gt;==== &gt;[**Image**](https://i.imgur.com/K6UStwa.jpg) [^(i)](https://commons.wikimedia.org/wiki/File:Mustela_frenata.jpg) - *The software's name takes itself from dialogue in Hamlet: Hamlet: Do you see yonder cloud that's almost in shape of a camel? Polonius: By the mass, and 'tis like a camel, indeed. Hamlet: Methinks it is like a weasel.* --- ^Interesting: [^Infinite ^monkey ^theorem](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) ^| [^The ^Blind ^Watchmaker](https://en.wikipedia.org/wiki/The_Blind_Watchmaker) ^| [^Wild ^Weasel](https://en.wikipedia.org/wiki/Wild_Weasel) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+clyle44) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+clyle44)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I really like that you link to relevant chapters of your book in the example. I do wish that they could be ran in place though, so I could play with the result. 
The Idris folks [have implemented](https://github.com/idris-lang/Idris-dev/wiki/Static-Arguments-and-Partial-Evaluation) a form of partial evaluation. Obviously "general partial evaluation is hard" is not the same thing as "it's impossible". But do you perhaps have any thoughts about why it was feasible to implement for Idris, but not many other languages (for example, Haskell)? Or perhaps it's not actually "general", for instance due to the use of annotations? (My motivation with these questions is to better understand what's lurking behind the statement "general partial evaluation is hard", which I have also heard elsewhere.)
We have plans for that. I hope to integrate things with the API from the Try PureScript website soon. The only issue is that Try PureScript doesn't handle library dependencies right now - everything has to be self-contained.
Database bindings wouldn't be bad. They cover a lot of the bases. You'll get to do network level programming with TCP and parse the byte structure of a protocol specification. You'll also get to write your own CRUD DSL. You can take it a step further and generate the CRUD code automatically with template haskell. You'll be exposed to type level programming in order to enforce invariants on database operations (to keep things type safe, if you want). Lastly, you'll get api design experience. 
The neural nets isn't too bad. I did a simple thing in Octave, it shouldn't be too hard in Haskell either. Depends on what you want to do.
If you're interested in models, check out hidden markov models instead of neural networks. They are definitely a better exercise than NN. NN might be hard in haskell because of their arrayish nature, and there's also an iterative training algorithm, so you're mostly just doing the linear algebra in Haskell which isn't pretty if you mean to code every single bit. Tagging with HMMs is a fun project, especially in Haskell and can be coded in an entirely pure fashion (no side effects / mutation). Playing with lists a lot, you'll be able to do second-order linear chain, coding the inference and training algorithms (viterbi &amp; counting) will be simple enough. You'll experience Data.Set, Data.Map, Data.List functions. Then you'll try to make an n-order linear chain markov model, this will force you to look at miraculous monadic functions used on lists :D -- what I didn't do -- After that you'll see that as much as the algorithms work, they are fairly slow. You could then check the Data.Vector for doing your dynamic programming, or trying the Data.Array. Or writing some code in the State monad. You could try making a command line tool, parsing the input and providing the tagged output. This would give you enough exercise with the IO. I did this project when I started learning Haskell, it took me about 3 days and resulted in 200 lines of code. When I look at the code, it's quite messy, but worked almost instantly and I learned a lot about Haskell as I've experienced no bugs and fought mostly with the type checker. You could of course do a lot more with the tagger. You could, for example, code a tagging server to get familiar with network functions. Sky is the limit! If you lack the knowledge of HMMs check out the lectures by Michael Collins on Coursera. He's got a nice intro and there's an exercise with correct answers (so you can check if your model works). First and second week are entirely about HMMs and you could probably watch the lectures at a quicker pace. https://www.coursera.org/course/nlangp
Write a monad tutorial, obviously. Just kidding. ;)
Thanks. I'm doing a course on Machine Learning and HMMs don't appear to be on the syllabus (though I'll check with the lecturer), but your suggestion has made me realise that they are a lot of other machine learning models I could implement that are on the syllabus and wherever possible I like to kill two birds with one by doing a project that helps me learn two modules at once. 
Writing my first Monad-based program now. Unfortunately I think it is a pre-requisite of tutorials that the author has to know what he is talking about.
The problem is I am doing the Neural Nets module as well and struggling with it to be honest, so trying to write a neural network in a language I am new to is a bit too brave for me, when I will have a difficult enough time writing it in Java.
You had me at "dmenu". Real feedback: * [optparse-applicative](https://hackage.haskell.org/package/optparse-applicative) could help clean up some of your command line parsing. * (stylistic choice of course) adding the pipes after the constructor strikes me as odd. Good job. 
In `main` you have apps &lt;- fmap concat . sequence . map ((=&lt;&lt;) getApps . canonicalizePath) $ filepaths I think apps &lt;- fmap concat . mapM (getApps &lt;=&lt; canonicalizePath) $ filepaths is clearer (assuming I've correctly understood your code). You will need to import Control.Monad.
Weird choice for the name.
Source code for the JIT compiler is at: https://github.com/nominolo/lambdachine
With the greatest respect, learning Haskell and then showing basic proficiency in it doesn't sound like masters level work (Haskell is often taught as a first language on Bachelors degrees). As your deadlines are quite soon, maybe sticking with a programming language you know and doing something deeper with it would be more fitting. 
By the way, in addition to being able to do: (++) list1 list2 You can also do: (++ list2) list1 So: (++ [4,5,6]) [1,2,3] returns the list [1,2,3,4,5,6]. The expression: (++ [4,5,6]) evaluates to a function that appends the list [4,5,6] to whatever list you give it. So you can do stunts like: map (+3) list1 to add 3 to all elements of a list.
Some things are straightforward, like the arithmetic example i gave. But, how about a function that has an even predicate, if(even(x)){ return x * 2; }else{ return x; } Now, say i have a list like foo = [2,4,6,8] A general partial evaluator would be able to eliminate that even test if we were to loop over the list for(n in foo){ check(n) } it's hard because it requires examining every possible code path with every possible value. - imagine instead of having a list of even numbers, the parser somehow only accepted even numbers. It's possible to do some stuff, but doing everything is, well, hard.
If only...
I take it that it's recommended to hold off on using the PS-in-PS compiler for the time being?
If he has gotten an individual study module about learning haskell accepted, then writing a project showing basic proficiency in haskell sounds like a good idea to me? Doing a project in a language different from haskell would certainly seem wired to me in this situation.
Make up a small imperative language, and write a parser and interpreter for it. I just took a course where we had three weeks of Haskell (and two weeks of Erlang and one week of Prolog), and this was two thirds of the exam assignment, which we had a week to solve. Or write a HTTP server - that's always fun. Implementing enough of the HTTP protocol to talk to normal, modern browsers actually isn't very hard, and after that it's up to you how ambitious you get.
Definitely. It's not even up to date with the features of the current Haskell implementation. We made some good progress on it this time, but we need to spend more time on optimization.
Ins't AngularJs dead at this point? And would prefer a virtual DOM implementation instead, slow Angular templates.
* Write a JUnit-inspired testing library in Haskell * Write a BitTorrent client * Write an IRC bot that evaluates arithmetic expressions * Implement a subset of POSIX sh in a Haskell command line program * Write a universal unit converter that loads unit names and conversion constants directly from official online sources like NIST * Port jQuery (or a subset like [Sizzle](http://sizzlejs.com/)) to Haskell using [Fay](https://github.com/faylang/fay)
Not out of date: The lecturer, Erik Meijer is currently teaching a very similar course at [edX](https://www.edx.org/course/delftx/delftx-fp101x-introduction-functional-2126), and using the same text, Graham Hutton's "Programming in Haskell. Same shirts also. 
I just worked through the book this past weekend. Well, more like I skimmed it. I started wanting to absorb every word, but already knowing Haskell and the type system and such, *so* much of it was stuff I already knew. I ended up skimming or skipping huge portions of it. I'd love for there to be a condensed version for people who know Haskell already (and there ARE some cool differences to Haskell). Maybe I'll write a post about it in the future after getting a little more adept at it.
Nice project! Would you mind posting an example usage of the command line program in the README? Showing a little bit of how to use a program goes a long way towards making it easier for other users to try it out.
I've added a script I used it in but sure I will
Don't take the internet shit storm as truth. Angular still gets plenty of use in its 1.x form simply because it does work for people pretty well. 
Before you go that route, is everything else working as expected? I get crazy boosts in performance for things like adding specialize (or inlinable so the Main module can specialize). Any issues with laziness and state? Saw this a while back: http://stackoverflow.com/a/7998892. I'm not sure if this is me talking down to you to presume you don't know these, but better safe than sorry :) 
No laziness performance problems. Honestly, the performance is fine to make a playable game (about 500 units). This is an attempt at blazing fast speeds. I'm trying to push the envelope (5000+ units). EDIT: GHC actually does a really good job of inlining my code. Every time I add inline pragmas it just makes things worse. I haven't tried too much specialization though.
Since you're still at Uni you may not be using many "developer services" like Github/Slack/Trello etc. But if you are, it might be enjoyable to create some sort of integration to these services. A Slack-chat bot that listens for commands and posts things to Trello for instance. Maybe a bot that checks github commits for some command / message and posts useful information automatically. Pick a service you like/use and extend it with an integration you would find useful.
Awesome! Sorry I didn't get a chance to look at it over the weekend, I'll try to leave feedback as soon as I can try it though!
I wish there'd be a good MS Sql Server library to use on linux. Current hdbc-odbc driver i'm using is very flaky. Unfortunately odbc api is atrocious and very hard to learn and figure out. I even created a rest java service through jdbc. Though i do not use it (it probably is quite slow with all the http-json marshalling) but it was very easy to create. 
Great news! We really need some healthy competition in the SQL-connectivity arena...
&gt; However each unit has around 60+ fields. Does that mean a record with 60+ fields or a record with 6 fields of nested records with 10 fields? The second case could probably improve performance if not every aspect of unit is updated at once.
&gt; bfs is using tail recursion I don't see this 
If you have that many fields, you should probably consider an entity component system. They often have very good performance, partly due to cafe effects. I think there was a recent /r/haskell post about creating one in Haskell. In any case, they make for interesting reading: http://stefan.boxbox.org/2012/11/14/game-development-design-1-the-component-system/
Every picture is followed by a link to an HTML version with complete details.
I think that [Brigitte Pientka](http://www.cs.mcgill.ca/~bpientka/)'s group at McGill University meets your criterions.
Yess
Small update: it seems like some projects (especially those with large binary files) trigger our resource limit protections. We're continuing to look into this.
Report back on Monday.
Thanks! Expected that to be a link to csv-data or similar. I was about to suggest that you add noise to the RTT for the simulation, happy to see you used cirterion!
Yes, there's a lot of activity at McGill. On the other side of Canada, [Ron Garcia](http://www.cs.ubc.ca/~rxg/) is at UBC, and he does a lot of work on type theory and fp. 
Nice post. I think you have your stack and queue backwards. A stack is LIFO and a queue is FIFO.
&gt; cafe effects Huh? Was that an auto-correct from "cache effects"?
what is a pseudo geometry
She's not very fond of Haskell, though. She prefers the ML family for implementation and the LF family for dependently-typed proofs, or at least she did while I was there. She was and still is working on the [Beluga language](http://complogic.cs.mcgill.ca/beluga/), in which contexts are reified so that you can manipulate open terms in a safe and concrete way. I also saw that she has since published stuff about co-patterns, which seem really neat. If you do go to Montréal, there is also [Stefan Monnier](http://www.iro.umontreal.ca/~monnier/), who is more enthusiastic about Haskell, and who applies type theory to really concrete problems, like using region types to track memory allocations, and something about dependently-typed assembly-language instructions.
Link, please!
Oops, there now!
When I heard about purescript I thought that it was a hack on top of javascript to simulate some functional features. The name suggested that and I didn't care to learn more. Two weeks ago I discovered that it is a serious language with even some enhancements over Haskell. The design tradeoffs chosen for the JavaScript platform are really nice. I think now that the name purescript is well marketed. 
&gt; Unlike both “HDBC” and “postgresql-simple”, the PostgreSQL back end of Hasql uses a binary format for communication, which eradicates a bunch of overhead related to parsing, rendering and transfering of values on both ends: the library and the database itself. Isn't this a huge speedup compared to the other differences? 
Looks nice. I wonder how much of the performance gain is due to the prepared statements.
Sure, I figured that out :)
This is a pretty cool paper on the topic: [Inductive Algorithms and Functional Graphs] (http://web.engr.oregonstate.edu/~erwig/papers/InductiveGraphs_JFP01.pdf)
Impressively, and perhaps this is common knowledge, it turns out that nanofortnights are very similar to milliseconds [1 nft = 1.21 ms](https://www.google.com/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=1%20nanofortnight%20in%20milliseconds) Edit: [Apparently there is some history here](http://en.wikipedia.org/wiki/FFF_system)
He's at UdM, I'd have to see if they do English programs. But good to know, thanks! 
Looks like [they do](http://admission.umontreal.ca/english/). While I was registered at McGill I attended one of his classes at UdM, and he was switching between French and English depending on which students were attending that day :)
Yes. I mentioned this [here](http://www.reddit.com/r/haskell/comments/2lwx9y/hasql_is_up_to_2x_and_7x_faster_than/clzhqi9).
Sure was. Sent from my phone, sorry.
I've been playing with Scotty and Yesod recently - your article taught me a lot, thanks!
I'm not sure if he's still doing work on FP directly, but [Richard Frost](http://richard.myweb.cs.uwindsor.ca/from_janus/welcome.html) at The University of Windsor has done some work with purely functional natural language parsing in Haskell.
I think this may one of the best "Actually," comments I've ever seen! :)
fyi, for python calling Haskell, there is [HaPy](https://github.com/sakana/HaPy) 
In the first experiment you say: &gt; Decoding of results is actually what we’re trying to compare here Wouldn't it be better to dump the protocol to a file, load that up, and test decoders on that then? Similar comments apply to the other experiments, which seem to be testing too much. That said, it's great that despite that you're overall coming out on top!
Math is helpful, but by no means a requirement. Find a Haskell project like [Fay](https://github.com/faylang/fay) and get hackin'! Your efforts will be much appreciated.
There are tons of open source Haskell projects which welcome contributors, [including GHC](https://ghc.haskell.org/trac/ghc/wiki/Newcomers) itself. I personally have a few open source Haskell projects for which I'd be delighted to receive contributions, either [Hawk bugfixes](https://github.com/gelisam/hawk/blob/master/CONTRIBUTING.md) or a translation of the [frp-zoo toy app](https://github.com/gelisam/frp-zoo#readme) into one of the many frp libraries we're still missing.
[Hackage](https://github.com/haskell/hackage-server) and [Cabal](https://github.com/haskell/cabal/) desperately need people willing to do work. They're both pivotal to how the vast majority of the Haskell community works but neither get as much love as they need or deserve. Also I'm always looking for help with [Bloodhound](https://github.com/bitemyapp/bloodhound/). I'm available to consult on contributions to any of these or other things if you'd like it. No math needed, if it were, I wouldn't be able to do anything.
Looks very interesting, thanks!
&gt; arrayish nature, and there's also an iterative training Whenever I read something like that, I always think that there's probably some Haskell wizard like dons out there who would just redefine the problem until it's completely functional and not iterative anymore.
Well, I have access to my codecs, which are actually published as [a dedicated project](http://hackage.haskell.org/package/postgresql-binary) and are themselves covered with benchmarks ([Encoding](https://github.com/nikita-volkov/postgresql-binary/blob/master/executables/Encoding.hs), [Decoding](https://github.com/nikita-volkov/postgresql-binary/blob/master/executables/Decoding.hs)). With results countable in nanoseconds I can imagine them being orders of magnitude faster than the textual parsers of competing libraries. I cannot however access their internal APIs to write such a benchmark and what's the need anyway? Concerning the "hasql" benchmarks I guess you should consider them to be general ones with certain accents. After all most of us want to know how well the libraries perform, not their components.
The easiest way to contribute is to improve documentation for existing libraries. The less you know about Haskell, the better, because beginners are the target audience of most documentation.
Not Haskell, but lazy, pure, categorical: Robin Cockett's [Charity] (http://pll.cpsc.ucalgary.ca/charity1/www/home.html) language at U Calgary.
Could postgresql-simple be ported to use the same binary format implementation or are the two projects too different ?
Lots of people talk about contributing to existing projects, which is of course great. But another way to contribute is to just build new stuff. Find something you like or think is interesting, and build it in Haskell. Put it on hackage and github, blog about it, and you've added to the ecosystem. Hopefully you'll have fun in the process :)
Hi I´m trying to do it. I plan to create a small Haste-hplayground hello world application using phonegap/cordiva that run in smartphones. For this purpose the first thing needed is a FFI interface for addEventListener. I´m talking with the haste people to do so
Of course it could, but it would require rewriting half of that library.
&gt; After all most of us want to know how well the libraries perform, not their components. Absolutely. The finer grained benchmarks would be of aid to those of us already committed (oh dear) to `postgesql-simple` - both so we know where to benchmark, and where slow parts might be. Great work nonetheless :)
I both love and hate you right now. :) (/me needs to read up on Category Theory.)
You're right, I had been calling it tail recursion when it was not. It has been fixed.
Thanks for the correction, I've updated the post.
Thanks for the link, looks quite different from my approach.
Or a simple FAQ on the side bar. This question probably deserves being promoted to the Haskell wiki proper though and then linked to from here.
I was just pining for a package that parses the [IANA languages list](http://www.iana.org/assignments/language-subtag-registry/language-subtag-registry). It would be helpful but sufficiently low priority that I'm not getting to it yet - I'd be happy to advise if you wanted to take it on. It should be comparatively small and self-contained (hours to weeks of work, in the first pass). I expect it'd touch attoparsec and template haskell.
As I said, I broke up the units fields into a tree. I put the majority of fields which rarely change in 1 large record and the ones that frequently change into 3 smaller records.
That post described very closely what I'm trying to do here, but that person was using IntMaps which throw cache utilization out the window.
Thank you for the update. I'll try reducing the repo size and see how that works.
[This post](http://idontgetoutmuch.wordpress.com/2013/10/13/backpropogation-is-just-steepest-descent-with-automatic-differentiation-2/) shows how you could use [the ad package](http://hackage.haskell.org/package/ad) to (almost) literally derive backpropagation (one of the main learning algorithms for classical NNs).
Did you try unboxing the values inside the leaf records?
I don't think SPJ should apologize for "sexy types" ... but I wouldn't recommend pushing the terminology any further than that.
GHC 7.8+ automatically unboxes anything word size or less.
Oh I want to do that! How does new documentation get on Hackage? Just submit a pull request on github and wait for the new version?
I would **love** to do this but I find a deficit of things that I can think that if like to make, beaides weird graphics experiments that are not nearly worth the effort they require to conplete
I'm much more interested in programming something then documenting
Pretty much, yeah. Most maintainers happily accept doc patches - and the ones that don't are the ones who seem to be MIA anyway.
I assume that he's hoping you would do these projects in your spare time. I think if a candidate has done this it shows a real passion for the language. However, I agree with your concerns too. Sometimes people simply don't have time to create open source side projects (have higher priorities). For example, while I was studying at uni last year, I did very little open source work because I tend to get tunnel vision on whatever I'm focusing on (uni work), and spend almost all my free time on that. I feel guilty if I do anything different (open source work).
idk. Something to do with snakes and starts with 'H'? https://en.wikipedia.org/wiki/List_of_legendary_creatures_by_type#Serpents_and_worms https://en.wikipedia.org/wiki/List_of_snakes_by_common_name Not Hydra. Everything is already called Hydra EDIT: [Habu](https://en.wikipedia.org/wiki/Habu) is a cool name.
So're most of the rest of us, of course, which is why documentation is extra useful :-P
Congratulations on the release! Random comments below. I wonder if it might be nicer if some of your functions might be renamed to match the `Data.Map` API (I think you're already kind of doing that), e.g. - emptyKdMap -&gt; empty - values -&gt; elems - foldrKdMap -&gt; foldrWithKey (?) - etc. You might document that `batchInsert` may be replaced by a more efficient implementation (otherwise I might just do a fold myself, and miss out on performance improvements you make later). Can you implement `Ord`, `Read`,`Show`,`Monoid`?
Of course running with multiple threads will report a slower benchmark time. * Multiple threads increase the chance that another OS thread will pick up on a task, meaning it needs to load all the affected memory into the CPU cache all over again * Multiple threads means more contention/activity in the scheduler * The garbage collector is slower when there are multiple threads * Your processor may be using hyperthreading, so multiple "threads" may share resources like the FPU or ALU * Each thread receives timer events, so there's also any OS overhead incurred by the 20us (I believe, is the default) timer interrupts/signals Additionally, make sure you build with `-threaded` even when testing -N1 because the runtime has locks on a lot of resources which automatically disappear in the absence of `-threaded` and make pure single threaded apps even faster. It would be extremely unfair to benchmark the two against each other -- in some cases the cost of the lock is upwards of (double digit)% the total cost of the operation!
Yes, I'm using -threaded in both cases. Good points on everything, well noted. So then, what about forcing criterion to benchmark parallel code sequentially? Or maybe I'm just imagining it? When benchmarking sequential code using criterion with -N4, all four cores go to 100%. So it's either criterion doing everything in parallel or.. hmm.. something?
I'm not convinced it actually does. In the past when using criterion each benchmark always ran sequentially for me. Criterion is very aware of the overhead that the GC can have on runtime and is tuned to try and reduce this noise as much as possible. It wouldn't make sense for it to run multiple benchmarks at once and introduce more noise ...
Hm, okay. Let me try to get to the bottom of this. Thanks for the help so far
Pretty good! Love the documentation -- lack of documentation is a big problem with a lot of Hackage right now, but your library is excellently documented. Some feedback: * Not enough typeclasses implemented with the types. Where's Read and Show? Monoid, Functor, etc. * People import container libraries qualified (i.e. `import qualified Data.KdTree.Dynamic as KDT`) so try to use short non redundant names like `empty`, `singleton`, `insert`, `append`, `foldr` etc. otherwise we end up writing code that looks like `KDT.emptyKdTree`. See `Data.Map` for an example. And some major gripe(s): * QuickCheck should not be a dependency of the package. Make this a dependency of your test suite instead so we don't need to install it! * Data.Point2d is exposed, but has no API or documentation. Either don't expose it, or let us use it.
I very much agree with this! Please try to name things the same as `containers` when possible, assuming your API is meant to be imported qualified. This makes transitioning between data structures much easier. (I've personally been putting a bit of effort lately into some patches for `unordered-containers` that add functions with the same names as `containers` for things like `HashMap`, so this is somewhat important to me.)
Are you sure? When I tested this a few months ago I found that criterion was only doing any parallelism/threading outside of the benchmark runs (I assume doing statistical stuff). I tested by putting custom event logging in my benchmark code and then looking at a run in threadscope. It would be awesome if some other folks would verify this. I've been doing lots of benchmarks that make heavy use of concurrency in a very controlled way, with (I think/hope) great success.
I bet /r/haskellgamedev would love this
We are developing a compiler and tools for the Clafer modeling language in Haskell at the University of Waterloo. Clafer is a declarative structural modeling language (we're adding behavior now). So, it's not a programming language but there's interesting work on semantics, type system, backend reasoners (instance generation, multi-objective optimization), etc. See http://clafer.org and a [wiki based on Gitit](http://t3-necsis.cs.uwaterloo.ca:8091/) with many examples. We are looking for strong PhD candidates.
Hm, it seems you might be correct. With my laptop (it's old) when I do -N1 I get a single core at 100% (of course). When I do -N4 I get one core at 100% and the other three hovering around 60%. This probably isn't too ideal but I guess it's my laptop to blame and not criterion. However criterion might take this into account already which would be cool. edit - I should note that this is with sequential code.
That webpage is pretty dated, though... Do they actually still develop Charity?
It would be super helpful if you could try this: stick [`traceEvent`s](http://hackage.haskell.org/package/base-4.7.0.1/docs/Debug-Trace.html) in the (sequential) code you're test benchmarking and try [compiling and running with `-N` for an eventlog, and examine with threadscope](https://www.haskell.org/haskellwiki/ThreadScope). I think what you'll find is that the actual running of *your* code is happening without interference (i.e. criterion isn't doing any forking or parallelism, so you'll see one core utilized on your non-forking/par benchmark), but perhaps in between you might see criterion using more of your cores for processing the data it collected. If this isn't the case, I'd love to know, but at least you'll convince yourself that you can trust criterion for your application involving parallelism/forking.
Great, then do them. At least on Github. Aside from the obvious benefit that you become better at using Haskell, others see that experimenting in Haskell is very much real and accessible and might learn something from you. And ideas usually don't come from nothing. It may appear so but they don't. Start hacking on something and ideas will develop from there. 
&gt; have reasonable install plans rejected and users needlessly downgraded to old versions of packages With modern cabal, that happens much less often. And it is much easier to recover from it. Just like Haskell types, the PVP should not be viewed as rules that limit you. It provides richer semantic information about dependency bounds. If including better information in your cabal file results in worse builds, that doesn't mean that it's wrong to provide the information. It means that our build tools need more improvement. A careful reading of PVP as a semantic specification, not as rules, makes it clear that PVP does not *always* require upper bounds. If you omit an upper bound, the semantics are that you estimate that your package is very unlikely to break with future versions of this dependency in the foreseeable future. If that is what you are trying to say, then it is correct to omit the upper bound. But if that is *not* a reasonable thing to say - then you should *not* omit the upper bound. The PVP states that "you should ensure that your dependencies in the build-depends field are accurate". Meaning as accurate as possible, since we can only give our best estimate for what will happen in the future based on the information at hand. The infamous following sentence, that "this means specifying not only lower bounds, but also upper bounds on every dependency," is less important; it is what was believed when Cabal first started out, but we now know that it is not *always* true that bounds are semantically more accurate when they include an upper bound. On the other hand, it often *is* true.
Seconded, make some crazy weird graphics (post them here if you want) and see where that takes you (maybe you'll write some libraries you find you need, or contribute changes to existing graphics libs etc.).
Criterion does nothing about/with threads when running the code you want to benchmark. Whatever RTS settings you set, they're what gets used. The likely reason you're seeing multiple cores in use for a single-threaded benchmark, when you tell the RTS to use more than one core, is the parallel GC, which is enabled by default.
Awesome stuff. I love KD-Trees! Here's one I'm working on and an older one I made. (Sorry for lack of comments/documentation) [Blog Post + Old One](http://scroungeworldmodel.blogspot.com/2014/03/parallel-kd-tree-construction-benchmarks.html) [New One](https://gist.github.com/RTS2013/3dac3104e907ff1d0eb9) The first one supports a radius for each entity so you can get all circles/spheres in range. The second one supports a radius for each dimension so you can get all rectangles/boxes in range.
+1 Emphatic agreement. I'd rather make a release that relaxes bounds rather than have EVERY previous version suddenly become unusable for folks
Automatically determining a range of packages that can be used as dependencies is a **very** hard problem. The only way I know to do it reliably would be to try out every single combination of all possible dependencies, and then come up with a list. To give the simplest reason possible for this: imagine if someone uploaded foo-1.1.1, which works with your package, then foo-1.1.1.1 which has a bug, and then fixes that bug with foo-1.1.1.2. Anything but exhaustive search will not exclude foo-1.1.1.1 from the accepted package list. That's why Stackage does what it does: finds a consistent set of package versions. It doesn't try to guarantee you that it will describe every single possible combination of versions that you may want, just a sequence of snapshots going forward through time. For the vast majority of users, this is all you need. For the rare cases where you have to have some combination of versions that's not provided by a Stackage snapshot, you can always try tweaking the snapshot versions and try to build again the new combination.
Thanks! I fixed my comment to mention that
It's also not always "just pass a flag" to build with a relaxed upper bound, because the following pattern is somewhat common (syntax approximate): flag new-base default: false if flag(new-base) build-depends: base &gt;= 4.0 else build-depends: base == 3.* cpp-options: -DOLD_BASE If you build this package with --allow-newer=base, it will set the new-base flag to false(!), which then will almost certainly fail. Arguably this is an abuse of flags, but like I said it's fairly common to use flags in this way.
I feel I should add a clarification about Snap's policy. From the OP: &gt; For some projects, there are stated compatibility ranges, e.g. Snap declares that any API will be supported for two 0.1 releases. We do indeed try to make sure we have a deprecation cycle whenever we make breaking changes. But the problem is that isn't always possible. If you change a function's type signature, it is impossible to have a deprecation cycle. Of course there is a workaround for this. Instead of changing the function, you could keep the original function as-is, but deprecate it and add a new function with the new signature. This approach works most of the time. But if the function being changed is a constructor, then this workaround is unusable. This is because you can't change the type without implicitly changing GHC's auto-generated constructor. You can't create a new type either because then you'd have to change all the rest of your API that uses that type, which would be a breaking change. So you find yourself in a catch-22. The basic rule of thumb here is that exporting symbols that are auto-generated by GHC is generally bad for preserving backwards compatibility and providing deprecation cycles. Also, one problem with hackage adding version bounds is that then the hackage cabal file is different from the cabal file in your version control. I'm certainly not opposed to better tools for adding version bounds, but I think they should be part of cabal-install or something else rather than hackage and hackage should merely require the presence of bounds. I also want to reinforce the idea mentioned by yitz, that upper bounds should not be viewed as handcuffs, but rather as useful information about the range of dependencies that is known to work. This information makes the solver's job easier. If you don't provide them, your packages are guaranteed to break as t -&gt; ∞.
Agreed. But if the latest version of all dependencies works on a range of compilers that is "good enough". Certainly when I manually loosen version bounds I don't do anything more. 
If we used the implicit blacklisting idea that I [mentioned awhile back](http://softwaresimply.blogspot.com/2014/05/implicit-blacklisting-for-cabal.html), finding dependencies becomes much easier because you only need to look at the most recent version for each unique a.b.c.
I recently wasted hours installing hwide with a recent cabal because of upper bounds - it was thoroughly unpleasant. Note that PVP gives semantics for how to increment version numbers, which I do follow. It somewhat encourages upper bounds, but that is orthogonal. 
Arguably any use of flags is an abuse of flags, and the above is what they were originally intended for. But this untracked in ghc-pkg changes to .cabal files is really painful. 
Yep, Snap was just a handy example of someone who tries to provide more. Fortunately t approaches infinity quite slowly, and I have time to react. In contrast t can reach t+1 while I am asleep, so I want that to continue working properly. 
It only reduces the search space a bit, it doesn't actually avoid the problem I mentioned, or reduce the actual complexity of the algorithm involved.
I don't understand your response. Isn't testing on the latest version of all dependencies the Stackage approach? It doesn't seem to be what you're advocating in your blog post.
Interesting! I have a custom batch file that does slightly similar things, but hardcoded to lots of specific details about my machine. I should give this a try. Not that I also work with non hp releases, not sure if that is supported. 
A far more likely scenario is that as the GHC version approaches 8 your code will stop building. Yet for some reason we don't put GHC bounds in our Cabal files.
Just to repeat that: the *frp-zoo* is a great project that took alot of burden from my shoulders lately. Thanks for that. On the other hand the zoo could benefit from some benchmark results. So maybe someone could just take the existing examples and run them through [criterion](https://hackage.haskell.org/package/criterion)?
Morte is the feminine for dead in French, so that feels weird for me too.
Then good luck with it. It will be very interesting to see what you come up with.
When you supply an `HPPATH` via `find-hp.bat`, what we do currently is: 1. Remove all paths that contain the string "Haskell". 1. Add `%HPPATH%\mingw\bin` to the end of the path. 1. Add the following to the beginning of the path: 1. `C:\Program Files\Haskell\bin` 1. `%HPPATH%\lib\extralibs\bin` 1. `%HPPATH%\bin` Would that work if `HPPATH` is actually just a GHC installation and not an HP installation? If not - how can we fix it?
Also, I started working on some registry-related code that would set the default Haskell Platform path, but I haven't gotten it to work yet. There are permissions issues. A different approach would be to call out to external programs that can deal with the registry, such as `reg`, `setx`, and [Jonathan Wilkes' setenv](http://www.codeproject.com/Articles/12153/SetEnv). If you can help with either of these approaches, please let me know. 
Thanks, I've cross posted there...
Following on from her recent [Strange Loop](https://www.youtube.com/watch?v=1MNTerD8IuI) talk, Elise Huard is writing a book on game programming in Haskell.
There is no such thing as a "restrictive upper bound". Only restrictive build systems that use upper bounds wrongly. Providing better semantic information about your dependencies should never make it harder to build, only easier. There are also "broken upper bounds". If an upper bound is a lie - conflicts with what the package author knows about the relationship between the package and this dependency - then yes, it's not surprising that it will restrict the set of successful builds that can be found.
Please don't degrade this into a silly semantics game. A restrictive upper bound is simply an upper bound which restricts. "Oh, the build system restricted you because it followed the upper bound when maybe it shouldn't have" seems to be what you're saying. That's ridiculous semantics, and doesn't make this discussion move forward at all.
No, I made the assumption that if it compiles and the tests pass then you can assume it works. If that's not the case, you should improve your test suite. Reading the diff is infeasible, and humans are terrible at producing changelogs which focus on what other people care about.
Have to say that the name sounds fine to me - at least I can tell fairly quickly what it does!
OK, with a good test suite that might work. Then the (retorical) question is: how many libraries on hackage have a good test suite? I don't think this system is suited for hackage in general. It could work well as a separate opt-in service. Perhaps integrating with github and travis it wouldn't even be such a huge project...
I specify this with 'build-tools: ghc&gt;=7.4 &amp;&amp; &lt;7.10' 
&gt; "if you don't put in restrictive upper bounds, your package just magically stops working." That's false [...] But it does magically stop working. This happens all the time with packages on hackage that don't have upper bounds, e.g. with new network releases. 
The answer is "all of my packages" :) Shake had more test code than implementation code. HLint has 447 tests, one of which is "QuickCheck every hint". The FilePath package is 2 modules and 406 tests. Developing code without testing is just too time consuming. I'd love this to be a separate opt in service, and I'd definitely opt in and buy beers for whoever wrote it.
My script is literally: set PATH=C:\ghc\ghc-%1\bin;C:\ghc\ghc-%1\mingw\bin;C:\Program Files\Haskell\bin;C:\bin;C:\Windows\System32 ghc --version I think it's probably easier for me to install GHC where the platform goes than you to change your script. Removing all Haskel is also much nicer than my approach of killing the PATH entirely.
[...] was just ", and I wish you'd stop saying it". I didn't actually understand the rest of your comment, but I'm just saying that things absolutely do stop working without restrictive upper bounds, and not just under some constrained/uncommon set of development practices.
There are a million things that can mean that, at some point in the future, code stops building. The claim that the lack of upper bounds *guarantees* that a package will stop building is patently false. It's a matter of our tooling being broken, which is what my blog post is about. And yes, it *is* "just under some constrained set of development practices". The fact that we're recommending everyone follow those practices instead of following more sane practices is the problem here, not the lack of upper bounds.
Then you're responding out of context, because the comment I was responding to (and quoted in my original comment) was: &gt; If you don't provide them, your packages are guaranteed to break as t -&gt; ∞.
To kick off a discussion on this, I *feel* that this can be simpler, but I haven't managed to work out anything else. The first thing one thinks is that if you have `T (a -&gt; b) a`, then you might as well just store `T b` with lazy evaluation. However, when I tried that, I didn't get the results show in the blog post. Next, one might think that you could just write `render = memo render'` and get the same behavior. Again - it doesn't seem to give the same results, and spends more time recomputing unchanged trees.
You were also criticising cartazio's comment, which didn't say that.
It's just not true that an upper bound restricts cabal from installing any version, as it did in the distant past. By default you do need to tweak cabal to get it past an upper bound, but if you don't like that, change your settings. Or, there are many other things you can suggest: * Just use Stackage (last I heard that's your first choice :)) * Change cabal so that --allow-newer is the default for everyone * Suggest ways of improving the solver * Suggest a new well-specified semantics for dependency constraint expressions * Extend the syntax of dependency constraint expressions to allow distinguishing between known incompatibilities and estimated likely incompatibilities Etc. But please stay on topic.
To be honest (and I fully expect to get down voted for that) I expected more from her StrangeLoop talk. All the information given was very basic and not really specific to Haskell. Hopefully this book will expand on that. Interesting (and still basic) topics include: getting OpenGL running, an introduction to FRP (there was another nice StrangeLoop talk about that), maybe something about entity component systems. There is another book called [Killer Game Programming in Java](http://fivedots.coe.psu.ac.th/~ad/jg/) that could serve as a reference here. As I just found out this book is still available online and is still expanded upon.
I test most of my packages back to 7.2. Generally using that far back is a bad idea since there are segfaults and race conditions that have been fixed. 
Agreed, but I think the PVP contract says that you shouldn't change behaviour in a minor bump. Hopefully breaches (presumably accidental) of that are rare.
I am really looking for ideas here.
HTTP currently supports versions back to 7.0, and some people argued in favour of keeping that when I suggested switching to 7.4 on the libraries list a few months ago. The problem is that HTTP's test suite has a lot of dependencies, and not all packages go back that far (quite justifiably).
Couldn't you have `HTML = Store HTML' String` then use smart constructors to ensure that the function is always of the form `memo render'`?
Yay ... competition! ... we should do a shootout some time: https://github.com/fhaust/kdtree
'Secure' is relative to context - what's considered (sufficiently) 'secure' in one context might be considered nowhere near 'secure' in another. So your question needs to describe a specific situation with specific requirements/preferences before it can really begin to be addressed.
The table of contents, according to the preview: 1. OpenGL and GLFW 2. Functional reactive programming 3. More graphics (textures, etc.) 4. Sound 5. Physics simulation with Chipmunk 6. Game design 7. Pros and cons of using Haskell for game programming Unfortunately the complete TOC seems not to be online. Libraries and frameworks: OpenGL, GLFW-b, FTGL, ALUT, Elerea. [Edit: Added libraries and frameworks]
I like where this is going - I'll have a play! I knew I recognised the shape of `HTML` from somewhere.
Thanks! I don't think it would make sense to use the existing toy app as a benchmark, because 1. Most of the time is spent sleeping, waiting for gloss to tell us that we're ready for the next frame. *edit*: although I guess we could benchmark the code for generating a single frame. 1. In order to demonstrate the higher-order features, which are not provided by all the libraries, some of the implementations perform the same task twice per frame in two different ways. Therefore, some implementations do more work than others, so it wouldn't make sense to compare them. *edit*: although I guess we could benchmark the static part of the implementations, which are required by the spec to use the same user-level implementation technique. 1. The task is really small. Do such micro-benchmarks really mean anything? 1. The primary goal is to help people choose an frp implementation. If the code is used to demonstrate performance, contributors might be tempted to write more efficient but less readable code, but at the same time we want the code to be extra readable because for most readers, this will be the first time they see code using this library. That being said, performance is indeed an important criterion (no pun intended) while choosing a library, so I agree that it would make sense for frp-zoo to give some indication of that. Perhaps a second toy program or suite of program, written specifically to demonstrate performance? Any suggestion as to what this second toy program could be? It's difficult to find a balance between too micro and too much effort to implement.
First thing that jumps out at me is Idris - which, I know, isn't Haskell. Some of the people working on, or with, Idris, are specifically looking at it with security in mind. There's particular work on Protocols, where the idea is that you write a description of the protocol, and then the type checker ensures that your code meets the description. https://github.com/edwinb/Protocols I guess what I'm getting at is that for security, you want machine enforced properties on the code, and a dependantly typed language is more suited to such theorem proving that Haskell is. In just Haskell's type system, you can do things like have different types for input data and cleaned data, and the output functions only use cleaned data - hence you can't pass uncleaned data on. (Actually doing that can, of course, be more complex than it sounds) That's looking down a road that's signposted as, "How can we use a type systems to enforce security". The other obvious route would be "How a stronger type system enhances security" - i.e. what little things happen without effort that also assist. Just musing really, but I would imagine that covering some aspects from both sides would be a good strategy?
You should also mention the obverse: the attacks that are possible in Haskell that are easier to mitigate in C/C++. When Heartbleed hit, there was a lot of talk about Haskell/OCaml as better alternatives for writing crypto, and a fair few people involved in crypto projects pitched in mentioning that garbage-collected languages open you up to GC timing attacks. Afraid I couldn't find anything thick with details, but you can read a little bit about it [here](http://openmirage.org/blog/introducing-nocrypto).
Could you elaborate on why this would need template Haskell? I am in the same position as the TS. I am interested in taking it on. 
i respectfully disagree and when i'm done with all the crazy meetings + emails I have to deal with over the next day, I will enumerate some concrete examples.
No, you've completely missed my point. The point is that you're assuming a very specific set of development practices and tools. That's an unreasonable assumption; it precludes the possibility of improving our toolset, which I believe has been the biggest downside of this PVP debate. Concretely: we can have a Hackage server that adds upper bounds based on upload date, we can use curated package sets, we could implement something like Neil describes in his blog post (which I don't think is reasonable). Under all of these circumstances, your assertion is false. This has nothing to do with ongoing maintenance. This is about having better tools, and not letting this constantly rehashed PVP debate to blind us from improving our tools.
I want to provide a little more motivation for this blog post. When I speak to people who are interested in Haskell, more often than not they've run into some kind of build problem that scared them off of using Haskell (either at all, or at least in production). And these kinds of problems are *already solved* with Stackage. I believe we, as a community are chasing our own tails by trying to insist that build problems will be solved by following PVP upper bound rules. That's been pushed for years now, and the problems remain. Let's use better tools instead of letting our current state of affairs continue to scare off potential adopters.
Fair enough, but it's going to be hard to prove what you've claimed here, given that I've maintained codebases that have used libraries that do not have upper bounds on libraries, and continued using them for years after they were released without any ill effects. You may have a claim you want to make, but it's going to have to be a drastically reduced one in scope.
Never mind ... just did that. Basically we are in the same ballpark. My tree building is faster than yours, your radius around point is faster than mine :) Results: http://lpaste.net/114173 Code: http://lpaste.net/7539092743348289536
&gt; This is false. There is no circumstance under which omitting upper bounds will result in having "EVERY previous version suddenly become unusable." I think I'm missing your point here. If you don't have upper bounds and someone releases an incompatible new version of your dependency, won't all existing releases start picking up that incompatible new version and stop working? &gt; At this point, I feel like you're just trying to nit-pick each statement I've made to try and find a loophole. I'm sorry you feel that way, but from my perspective it's the exact opposite: your arguments seem to me to be based on sweeping generalisations ("this claim is false" etc) and refusal to see the other side's point of view.
&gt; I’d like to say that the Information Security community is solving the wrong problems by e.g. performing security audits of code, developing tools for finding buffer overflows, etc. and what they should really be doing is encouraging development in languages that prevent this sort of behaviour. You need both. Right now, the choice is between battle-proven industry-strength tools that lack security-relevant features, and newer, more experimental tools that have those features but haven't seen the same kind of scrutinizing. In other words, "enough eyeballs" is good, but not enough on its own; "formal validation" is also good, but suffers from the "who watches the watchers" dilemma. This is even more painful when it comes to libraries. The question is, which part is more important, cq. buys you a better security tradeoff. Haskell sits kind of in an in-between position - it is off-mainstream enough to not have had the kind of auditing and reviewing that you can expect from, say, `gcc`, meanwhile the language itself has many features that are beneficial to secure programming, but it's not 100% watertight either. And, one more thought: the more experimental / unusual a language, the likelier it is that it exposes a whole new class of potential security pitfalls. Take, for example, garbage collection - it pretty much eradicates a whole class of security problems such as buffer overruns, but it introduces a new one, namely timing attacks, and the fact that controlled deletion/overwriting of sensitive information in RAM is highly problematic (e.g., never put passwords in strings in Java, because strings are copy-on-write, and they might never get garbage-collected).
&gt; I think I'm missing your point here. If you don't have upper bounds and someone releases an incompatible new version of your dependency, won't all existing releases start picking up that incompatible new version and stop working? No, that's simply not true. *If* all users continue using this broken system of choosing dependencies that is being pushed as the default, then sure, that problem will eventually occur. The reason I'm discussing Stackage here is because it is a *direct* counterexample to this claim. See my blog post for more details. I clearly didn't elaborate enough in my Reddit comment to make the point, which is (one of the reasons) why I decided to write a blog post on the subject. &gt; I'm sorry you feel that way, but from my perspective it's the exact opposite: your arguments seem to me to be based on sweeping generalisations ("this claim is false" etc) and refusal to see the other side's point of view. That's not a sweeping generalization. To prove that a claim as general as what mightybyte and cartazio said was false, all I need to do is bring a single counterexample, which is why I felt so confident in making an unambiguous statement that their claims were false. A sweeping generalization would be "there's no situation under which PVP upper bounds prevents breakage." But I would never make such a claim, since it's just as patently false.
It's a well-defined behavior and they *are* safe. It's just that they're defined to be in a mod-taking manner. If you want to have arithmetic which checks bounds you could easily wrap that in Maybe and define a Num instance.
&gt; No, that's simply not true. If all users continue using this broken system of choosing dependencies that is being pushed as the default, then sure, that problem will eventually occur. But people *are* using that system now and the problem *does* occur. &gt; The reason I'm discussing Stackage here is because it is a direct counterexample to this claim. I think I understand the confusion now. I think you are saying something like "everyone should use Stackage only rather than Hackage, and not engage the cabal-install solver in any meaningful way". Under that condition much of what you're saying is true. But most other people are not assuming that pre-condition and so reach dramatically different conclusions. From my own point of view, I personally prefer the Hackage/cabal-install world, and also want to support users who are doing that, so I buy into the strongly specified versions model.
Is it just me, or is the presenter video aspect ratio wrong?
TOC from current PDF: Introduction Doing things the hard way Haskell Game Aim of this book Prerequisite Contents Thanks Introduction to graphics with OpenGL and GLFW Why OpenGL and GLFW The canvas Initializing OpenGL The Loop Let's draw something Vertices and shapes Text What's next? State with FRP No life without state FRP FRP with Elerea Transform our example Monsters! Other modules of Elerea Dynamic graphs, and levels Animations Ready to go Appendix A: Installation on Mac Homebrew GLFW FTGL Appendix B: Useful tips for Haskell development Sandboxes Warnings Linter An amazing resource (spoiler: http://dev.stephendiehl.com/hask/) 
I think it's a little disingenuous to write a book about game programming in Haskell when there's so few games written in Haskell. Are there even any commercial ones besides Nikki and the Robots?
My comment assumes you're using the tools in most common use today: Cabal, cabal-install, and hackage. It's a perfectly reasonable assumption since that's what the vast majority of the community uses. I have no problem with improving our tools. On February 26 of this year in a haskell-libraries mailing list thread, I said this: &gt; The downsides that you're claiming aren't the fault of the PVP. They are a deficiency in our tooling In this very thread I said: &gt; I'm certainly not opposed to better tools for adding version bounds, but I think they should be part of cabal-install or something else rather than hackage You're completely ignoring things I have said in the past. I completely agree with this comment from yitz above: &gt; There is no such thing as a "restrictive upper bound". Only restrictive build systems that use upper bounds wrongly. Providing better semantic information about your dependencies should never make it harder to build, only easier. Stop slamming my points, while at the same time not addressing them. I'm talking about version bounds. I don't care whether they're specified by the user in the cabal file, generated by cabal-install with a specific command or on upload, added after the fact by hackage, or anything else you can think of. I'm talking about having bounds somewhere versus having them nowhere. If you're ok with hackage adding upper bounds then you're accepting my argument. &gt; This has nothing to do with ongoing maintenance. This is about having better tools If you release foo-0.1.2.3 today on hackage and there are no upper bounds on it anywhere (which is currently the case if you don't specify them yourself), then foo-0.1.2.3 WILL stop building successfully with cabal-install/hackage at some point in the future when one of your dependencies makes a breaking change. If you had bounds, then it would not break. Period, end of story. Now, maybe the package as a whole won't break if the maintainer releases a new version. But in that case, preventing foo from breaking is absolutely about ongoing maintenance.
It's more secure than any Assembler? Yes it is It's more secure than Fortran? Yes it is So.... Yes, Haskell is more secure than other languages.
&gt; But people are using that system now and the problem does occur. And that doesn't counter at all what I said. You've chosen a tool that breaks easily, and now you're trying to say (or at least defend people who say) that things universally break under circumstances that in fact only those tools break under. These tools are holding us back. If you want to use them, go ahead. But to have an argument premised around "this is guaranteed to break" without acknowledging that it's a specific set of tools is disingenuous at best. &gt; But most other people are not assuming that pre-condition and so reach dramatically different conclusions. The problem is that some people in the community are so deeply entrenched in our current way of doing things that they can't imagine a world without those tools. Stackage is one example of a solution to this problem. There are plenty of others. The fact that the current default tools provide no solution to this problem is the issue. &gt; From my own point of view, I personally prefer the Hackage/cabal-install world, and also want to support users who are doing that, so I buy into the strongly specified versions model. Then I think you should be arguing for *why* you prefer those tools instead of just asserting that they're the "pre-condition" to any discussion. I'm challenging that status quo right now, and claiming that it's holding us back.
That's a great way to remove any doubt about whether your package will build with future versions of GHC, if that's your goal... If the goal is to communicate to users what versions of GHC the package is known to build with, how about using the [tested-with](https://www.haskell.org/cabal/users-guide/developing-packages.html#package-properties) field?
Don't blame me if you didn't read my points. You turned this into a discussion about ongoing maintenance, not me. I'm being consistent: there are many models under which missing upper bounds do not lead to guaranteed breakage. You're ignoring that. I've addressed your points. You are in fact ignoring mine. Said another way: if you're going to make such a ridiculous claim as guaranteed breakage, you better be able to back it up, because one single counterexample is all it takes to prove you wrong. And I've given that counterexample. Perhaps you should revise your original statement to be less hyperbolic and more honest, because as it stands it is blatantly false, and you seem to be unable to admit that.
I think it's a very good idea, and might work. However, I'd suggest to start developing such instrument *not* as a part of hackage (or other package-hosting service), but as a locally-runnable tool for a developer. Such tool could take as input a "proto-cabal" file: cabal but without (some or all) of upper-bounds, download the last published versions of the dependency packages, build the package and run tests (in a sandbox), and if successful, create a publishabe "real cabal" file filling in the bounds. The developer will keep the "proto-cabal" file in their repo, but include both "proto-cabal" and the automatically created "real-cabal" files in the published package. When the concept is proved and tested, package-hosting sites may deploy this tool to run automatically on packages that have "proto-cabal" file.
Quickly skimming that list it seems most of the problems can be fixed using statically typed capabilities, integer arithmetic and DSL's (for SQL, OS commands, format strings etc.). Things like this can quite easily be implemented in a dependently typed language like Idris, and with some more effort probably in Haskell as well.
Oh and only when relaxing the restraint would work. Perhaps it could run cabal install --dry-run to make sure the new packages will be used before sending the pull request.
I would second Habu!
And for the record, even the *standard* tooling provides a way that new versions won't cause automatic breakage. It's `cabal freeze`, and IMO covers a very basic best practice that *everyone* should be following: reproducible builds. Frankly, if you're building your application without some version of that, I'm surprised that you have any expectation that it will continue to build.
I don't want to use Stackage because it's based on a premise of being inflexible about dependencies: either use all the dependencies of a given snapshot together, or you're on your own. I find the the cabal-install model of allowing any combination of packages with compatible bounds much more flexible, and in practice I find it works well for me, at least within the set of packages that do have upper bounds specified.
If no one writes about the topic how would this ever change? I applaud people who write about breaking new ground in Haskell, more of this is needed.
Thanks for posting the full TOC! It looks like the chapters about texture, sound, physics simulation and game design are missing. It seems to be a book about basic OpenGL and an introduction in FRP. I thought about saving my lunch money and buy the book, based on what I read in the preview. But now I am not sure if it is still worth it. I didn't do FRP and OpenGL in Haskell yet. But I already worked with both in other languages. OpenGL in C/C++ and FRP in Scala.
It was very basic, indeed. There are some very interesting and difficult problems related to programming a complex loop-based application with lots of data in a purely functional language, but the talk was essentially about the basics of game development. Seems like there's a chapter entitled "State with FRP", so that should touch on some of the more interesting issues.
What would the safe behavior here be? Int8 is supposed to overflow, it would be unsafe if it *didn't*.
I've been playing with this type in a [virtual-dom](https://github.com/Matt-Esch/virtual-dom) clone. There's something nice in that there's a notion of `oughtToBeEqual :: Eq s =&gt; Store s a -&gt; Store s a -&gt; bool` which you'll have as well.
I would be wary saying that *Haskell* is more secure unless you can find some hard evidence, but I think it would be perfectly sensible to say that type checking is akin to an automatic security audit and we should be looking for type systems and other static verification tools than can prove the absence of certain sorts of security holes. 
Anything I can see? :) I'm planning to use the approach in this post directly with `virtual-dom` - I'm currently playing with a little framework called [Francium](http://github.com/ocharles/Francium) which has a interface (`Francium.HTML`) directly to `virtual-dom` using `lens`. In my code, I was planning to have `JSRef`'s be the render result (which is a FFI pointer to `VNode` objectS).
I think so, let me say it back. Anyone using Stackage does not need to worry about upper bounds, it's going to be completely irrelevant to their workflow. Since upper bounds become irrelevant with a tool like Stackage, those people who are using Stackage and maintaining packages on Hackage don't feel any obligation to spend a lot of extra time dealing with preemptive upper bounds, since it gives them no benefit and takes a lot of effort. Is that what you're saying? If so, I agree. If not, please tell me where I misunderstood.
Does this leak the old cached versions indefinitely? It seems likely to me, but I might have missed something about it.
Yes. I think we are saying the same thing. So, for those using Stackage it doesn't matter. But those packages without upper bounds end up being poisonous in Hackage for those not using *better tools*. Right?
You said &gt; Is there a way to make these operations safe I think it would help if you said what exactly you mean by "safe". In what way is overflow unsafe?
Not just type safe. It's much better than C because you have a well-defined guaranteed behaviour. Not to mention that integer overflow is undefined behavior in C. Easy way for checking overflow is to check if signs match in case of addition and sum of number of bits (i.e. logarithms) in case of multiplications. 
You can make games without books on writing games. 
Integer overflow does indeed lead to undefined behaviour according to hte C standard http://stackoverflow.com/questions/3679047/integer-overflow-in-c-standards-and-compilers
This is great. I use k-d trees quite a bit and have been relying on a hacked version of an old unmaintained package. This looks MUCH better!
I make that claim assuming that we're talking about hackage as it exists today. And things stand today, the statement is true. Or, if not completely true, the probability that your package will break in the future is so close to 1 that it's not unreasonable to generalize it to 1.
&gt; The consequences of the error are more likely to lead to a security problem in C because C is more dangerous than haskell. But the problem itself is the same. I don't think this is true at all. `Int8` just happens to be a datatype with modular-arithmetic-like behaviour. If it didn't exist I could create one and put it on Hackage. The datatype *itself* cannot be a source of unsafety.
&gt; to willfully misinterpret questions as attacks and respond with hostility Nobody is attacking you, they're just disagreeing with you. There's a big difference.
One thing I noticed is that a substantial amount of the people who try Haskell also want to try cutting-edge stuff like repa, GHCJS and other hard-to-install libraries like wxHaskell. Stackage probably can't help much with those.
For what it's worth I'm among the authors (e.g. Neil Mitchell, Bryan O'Sullivan) who don't preemptively put upper bounds on all their dependencies. I think authors who do so do it for some principled reason or by being nagged by users or said principled people. So an author deciding to use Stackage for their own work probably won't change the way they write up their package constraints when publishing to Hackage. Uploading to Hackage is how packages get pulled into Stackage in the first place. 
By the way, there's an easy way to deal with this for `Int8` since all operations on it are going to be in 32 bit registers (at least) anyway. Just let the type take an additional value signalling overflow. No `Maybe` needed. This doesn't extend to integer types that fill the whole register however. You'd have to lose at least one bit of precision.
I don't follow either. I still don't understand what your definition of unsafe is. You can indeed create your `Int8` in C too, and it would be safe. How does the type combine with some of the functions that operate on it to become unsafe?
I think this is the core problem. If you stick to stable stuff, you're usually fine. I only ever run into issues when installing bleeding-edge versions from source.
Well, something like SafeInt8 = Overflow | Result Int8 but packed more neatly from the machine point of view for better performance. Since this is also isomorphic to `Maybe Int8` perhaps you also would consider this to be a huge pain but I don't see how you can do better if you want to be able to handle overflows directly.
Side channel attacks of many kinds are more difficult to mitigate in a language as high-level as Haskell.
I looked at the specific issue of password/key strings here: http://stackoverflow.com/questions/11932246/against-cold-boot-attacks-how-to-restrain-sensitive-information-in-haskell/11933068#11933068 
My immediate first reaction was the same thought, but then I remembered (at least, I think I remember this) that RWH was written under similar circumstances for at least one of the authors. "I don't know how to do real world Haskell, so I'll write a book on it to learn." That seemed to turn out pretty well for everybody. Or maybe it was LYAH? Anyway, I'm not gonna knock it just yet.
I'm afraid the fact that we can write libraries which guarantee that this particular list of problems can't happen is of little importance. Take the first problem in the list, SQL injection. Sure, we can write a DSL which guarantees that all parameters are properly escaped. But wait, SQL injection? We don't need an advanced language to avoid those. Even in PHP I was using a library which was automatically escaping all the relevant arguments. So why is the problem #1 on the list, then? Clearly, that must be because many people still use raw strings instead of using whatever auto-escaping SQL library is popular in their language. So if Haskell or some other language is to be considered more secure from the point of view of this top 25 list, it should not be because the language makes it possible to implement a safe library, but because the culture of the language makes it likely that people will use that library. In Haskell, for example, using DSLs is very common, so I think it might be possible to make a case that SQL injection is less likely in Haskell programs because Haskell programmers are less likely to be looking for an SQL library which accepts raw Strings as queries. **edit**: Don't misread the above as saying that culture is the only relevant factor in achieving security. I was careful to say "secure from the point of view of this top 25 list", but apparently I didn't emphasize the distinction clearly enough, sorry about that. The people who care about security are of course using auto-escaping SQL libraries, so their top list of issues must look very different, and for them proper tools and languages do make a lot of sense, because they've already got the culture part right.
Something along this lines? * ordrea - [http://lpaste.net/6255595527044333568](http://lpaste.net/6255595527044333568) * reactive-banana - [http://lpaste.net/4840609884133130240](http://lpaste.net/4840609884133130240)
Interesting read. Although I was just quoting this as one example of how preventing one security pitfall can open up other attack vectors.
Upper bounds usually say "here is a number 0.0.9 higher than I've ever tried, and that wasn't released when I wrote the number, but consider it to work" - not what version I tested on. 
Yes, but most of the time that tells you what versions will likely work. I actually wouldn't be opposed to requiring some kind of notation that specified the specific version you're testing on. But if we do it using our current syntax (i.e. &lt;= 1.5.0.2), it is even more restrictive in the face of change than the current PVP.
&gt; I want my packages to use versions of dependencies such that: &gt; All the features I require are present. &gt; There are no future changes that stop my code from compiling or passing its test suite. Unfortunately, nothing about a series of 4 integers separated by decimals gives any information about these two questions. Sure, we try not to break the API without a 0.1 release, but which parts of the API broke? Did the parts I care about break? The version number tells me almost nothing about whether the package is compatible. I dream of a system that checks my package to see the types and functions I'm using from a dependency. If these types and functions are still exported with compatible type signatures, go ahead and update. Otherwise, lock 'er down.
Which is part of the reason why Rust has me really excited.