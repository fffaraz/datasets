go back to basic. Type classify terms. To have a more restrictive type means you have less terms. Aka you can represent less computations. For every program made up of individual steps whose result of computation get ignored to determine the next one I can assert it belongs to a corresponding monadic type or an applicative type. Every monadic type represent the set of all programs whose steps results decide what program will run next, among which are the one where we ignore the result, whic are represented by the applicative type. A type is an assertion on the term if you want. If you use as a negative position a VERY general one you can't do much. The canonical case is forall a-&gt; a. You can only do one thing. Conversely for ()-&gt;a you can do many things... So what you can do is more with terms of more restrictive type. Cf Kmett message. We want applicative to do more with them
`Parser` in `optparse-applicative` is not a Monad.
If I'm understanding you correctly, that is not true. `ZipList` is an example and so is the `Validation` applicative that /u/ephrion mentioned. Unless you're talking about it in the reverse from how I am interpreting what you said. Saying "an applicative *computation*" and "a monadic *computation*" without elaborating further makes the meaning a bit unclear (and, as a result, potentially misleading) in my opinion. Types are `Applicative`s or `Monad`s (or neither) while a computation is a calculation of a value of a certain type, so it doesn't fully make sense to talk about computations being applicative or monadic. `ZipList` makes a good example of this: You can write a version of `concatMap` that works for ZipLists, but you cannot use this to define a monadic bind for `ZipList` because it wouldn't satisfy the laws `pure = return` or `ap = (&lt;*&gt;)`. You can only see this when you look at the greater context of the type (in this case, the implementation of `Applicative` chosen for it), rather than an individual computation.
I'd more than welcome a law-abiding extension of `Validation` that happens to be a `Monad`. It just doesn't exist. You can't "ignore the argument" you have to supply it. Errors e &lt;*&gt; Errors e' = Errors (e &lt;&gt; e') If you replace `(&lt;*&gt;)` with `ap` there you get "stuck" and can't get out the second `Errors`, you can't call the continuaton `(a -&gt; Validation m b)` with an 'a' since you don't have one. The semantics of `(&lt;*&gt;)` then are necessarily wrong.
Atom has potential, but is still a bit immature. For example keyboard navigation in menus doesn't work. I'm not quite sure why, because it's a feature usually implemented in the GUI toolkit and not the application.
I see what you mean, it's a superfluous operation in a way. On the other hand, once single element cases are converted to lists, the remaining code only ever has to deal with lists, which makes it simpler.
There are the usual arguments for using types over strings: that certain classes of errors are caught automatically at compile time. This results in a much more pleasant programming experience and higher quality software. In the case of routes, eliminating errors at compile time is especially important: with type-safe routes, the compiler guarantees that certain kinds of XSS vulnerabilities do not exist. So yes, type-safe routes are definitely worth a significant investment of effort.
An example using [Data.Validation](https://hackage.haskell.org/package/validation/docs/Data-Validation.html) ghci&gt; liftA2 (+) (AccFailure ["Network failure"]) (AccFailure ["Connection timed out"]) AccFailure ["Network failure","Connection timed out"] Can be used to track the maximum severity level data Severity = Debug | Warning | Error | Critical | Emergency deriving (Eq, Ord, Show) instance Semigroup Severity where (&lt;&gt;) = max ghci&gt; liftA2 (+) (AccFailure (Critical, ["System failure", "Network failure"])) (AccFailure (Warning, ["Connection timed out"])) AccFailure (Critical,["System failure","Network failure","Connection timed out"])
&gt; I'll need about a billion examples before I finally figure out what `Applicative` is about. One thing that makes it difficult to appreciate `Applicative` is that it may require a "perspective flip," so to speak. If you're looking at it from the perspective of a *client* of an `Applicative` instance—particularly for a type that has a `Monad` instance as well—it may understandably leave you scratching your head. But if you look at it from the perspective of a *library author* instead, then `Applicative` is a class that doesn't allow your library's clients to construct "opaque" computations. Therefore, if you offer only an `Applicative` interface, that allows your library to analyze the computations more deeply than it could if it offered a `Monad` instance as well. Other comments have already offered examples (e.g., `Applicative` parser libraries can inspect and rewrite grammars in ways that `Monad` parsers cannot). This analyzability doesn't come for free—the library's types must be carefully designed for the analysis that the library wishes to perform—but `Monad` generally precludes such analysis, whereas `Applicative` doesn't. And `Applicative`'s value is further enhanced by these facts: 1. There's a few useful types like `Validation` that are `Applicative` but not `Monad`s. 2. Many classic operations that were first formulated for `Monad` turn out to only need `Applicative`. Most famously, `sequence` and `mapM`. So once `Applicative` was invented it slotted very nicely into the existing ecosystem for `Monad`. 3. `Applicative` instances are closed under composition, product and sum, so specialized `Applicative` transformers aren't necessary. This is another fact that might be of more use to library authors than clients, though—for example, `Validation e a` is the sum of the `Const [e]` and the `Identity` functors, and its `Applicative` instance can be written mechanically from knowing this fact. Basically, any time you're writing a new `Applicative` type, you can fruitfully ask whether you can build it mechanically by combining other simpler ones you already know.
Won't laziness mean you don't when the piece of code you're interested in is executed? Even if you manage to turn off GC, will it have been turned off for the right time period? 
&gt; But not all monadic term give rise to an applicative term, only the one which do not inspect the outcome of previous computation can be turned into an applicative one. Ah! Finally something which makes sense. `getLine &gt;&gt;= putStrLn` and `getLine &gt;&gt;= (\_ -&gt; putStrLn "hello")` are both monadic computations, but only the second can be rewritten in an applicative style, as `getLine *&gt; putStrLn "hello"`. Only some monadic terms "give rise to" an applicative term, got it. In another comment you write: &gt; whenever you can write an applicative instance you can write a monadic instance where you just ignore the value I had originally interpreted that comment as saying that if it is possible write an `instance Applicative Foo where ...` implementation for a type `Foo`, then it is also possible to write an `instance Monad Foo where ...` implementation. This was quite confusing, since the opposite is true: if it is possible to write an `instance Monad Foo where ...` implementation for a type `Foo`, then it is also possible to write an `instance Applicative Foo where ...` implementation, as follows. instance Monad Foo where ... instance Applicative Foo where pure = return (&lt;*&gt;) = ap I now think you meant that whenever you have an applicative computation which is executed solely for its side effects, it is possible to implement a monadic version of this computation by using `(&gt;&gt;)`, the version of `(&gt;&gt;=)` which discards the result of the first computation. For example, `replicateA_` below can be implemented monadically as `replicateM_`. replicateA_ :: Applicative f =&gt; Int -&gt; f () -&gt; f () replicateA_ 0 _ = pure () replicateA_ n act = act *&gt; replicateA_ (n-1) act replicateM_ :: Monad f =&gt; Int -&gt; f () -&gt; f () replicateM_ 0 _ = return () replicateM_ n act = act &gt;&gt; replicateM_ (n-1) act Is that indeed what you meant?
Well, here's how I like to think of it. In Haskell, the semantics of `newtype` is that the type thereby defined is **isomorphic** to the type of the values "wrapped" by its data constructor. So therefore, the following definition: newtype Mu a = Roll { unroll :: Mu a -&gt; a } ...says that the types `Mu a` and `Mu a -&gt; a` are isomorphic. The isomorphism is this pair of functions: Roll :: (Mu a -&gt; a) -&gt; Mu a unroll :: Mu a -&gt; (Mu a -&gt; a) {- Laws: &gt; Roll . unroll = id :: Mu a -&gt; Mu a &gt; unroll . Roll = id :: (Mu a -&gt; a) -&gt; (Mu a -&gt; a) -} Therfore, if you have `x :: Mu a`, although the type system forbids self-application, you can obtain its counterpart under the isomorphism: unroll x :: Mu a -&gt; a ...and apply *that* to `x`: unroll x x :: a So `Mu a` is a type of things that are isomorphic to things that can be applied to `Mu a`. Which is isomorphic to a type of things that can be applied to themselves. Now you can write this and Bob's your uncle: -- | The Y combinator, modulo a few walks back and forth along an isomorphism fix :: (a -&gt; a) -&gt; a fix f = (\x -&gt; f (unroll x x)) (Roll (\x -&gt; f (unroll x x)))
The "how" has been addressed in a sibling comment. The why is simple: the type system has been chosen to disallow [equirecursive types](https://en.wikipedia.org/wiki/Recursive_data_type#Equirecursive_types), because they complicate the type system and arise much more often from type errors than from actual intended code.
There's a fantastic booklet in the style of The Little Schemer called [Why Y Works](http://jonas.lophus.org/papers/Y.pdf) that might make more sense to some folks.
Also, when create a problem from no problem and smush it into the problem you already have, it's the same as if you did nothing to the problem. Smushing a problem into the made up problem is exactly the same.
Related to this perspective are the benefits of `Arrow`s; I couldn't find the exact post I was looking for, but [this](http://jaspervdj.be/posts/2012-01-14-monads-arrows-build-systems.html) one also explains it well.
&gt; Assuming you have GHC 7.10 and stack installed, building the program should be as easy as running stack build. And yet..... dhjmacpro:~/src/boids $ stack build Downloaded lts-3.10 build plan. Caching build plan Populated index cache. abstract-deque-0.3: download Boolean-0.2.3: download StateVar-1.1.0.1: download abstract-par-0.3.3: download abstract-deque-0.3: configure abstract-deque-0.3: build StateVar-1.1.0.1: configure StateVar-1.1.0.1: build Boolean-0.2.3: configure Boolean-0.2.3: build abstract-par-0.3.3: configure abstract-par-0.3.3: build abstract-par-0.3.3: install Boolean-0.2.3: install Progress: 4/44 -- While building package abstract-deque-0.3 using: /Library/Frameworks/GHC.framework/Versions/Current/usr/bin/runhaskell -package=Cabal-1.22.4.0 -clear-package-db -global-package-db -package-db=/Users/dhj/.stack/snapshots/x86_64-osx/lts-3.10/7.10.2/pkgdb/ /var/folders/n2/9h010cts31sc8_n4yd1z__kh0000gn/T/stack29996/Setup.hs --builddir=.stack-work/dist/x86_64-osx/Cabal-1.22.4.0/ build --ghc-options -hpcdir .stack-work/dist/x86_64-osx/Cabal-1.22.4.0/hpc/.hpc/ -ddump-hi -ddump-to-file Process exited with code: ExitFailure 1 Logs have been written to: /Volumes/HD2/src/GitHubRepositories/Flocking-Simulation/.stack-work/logs/abstract-deque-0.3.log Configuring abstract-deque-0.3... Building abstract-deque-0.3... Preprocessing library abstract-deque-0.3... [1 of 4] Compiling Data.Concurrent.Deque.Class ( Data/Concurrent/Deque/Class.hs, .stack-work/dist/x86_64-osx/Cabal-1.22.4.0/build/Data/Concurrent/Deque/Class.o ) [2 of 4] Compiling Data.Concurrent.Deque.Reference ( Data/Concurrent/Deque/Reference.hs, .stack-work/dist/x86_64-osx/Cabal-1.22.4.0/build/Data/Concurrent/Deque/Reference.o ) Data/Concurrent/Deque/Reference.hs:50:1: Warning: Tab character [3 of 4] Compiling Data.Concurrent.Deque.Reference.DequeInstance ( Data/Concurrent/Deque/Reference/DequeInstance.hs, .stack-work/dist/x86_64-osx/Cabal-1.22.4.0/build/Data/Concurrent/Deque/Reference/DequeInstance.o ) [4 of 4] Compiling Data.Concurrent.Deque.Debugger ( Data/Concurrent/Deque/Debugger.hs, .stack-work/dist/x86_64-osx/Cabal-1.22.4.0/build/Data/Concurrent/Deque/Debugger.o ) ld: warning: directory not found for option '-L/Library/Haskell/ghc-7.10.2-x86_64/lib/random-1.1' ld: library not found for -lHSrandom-1.1-1z8Ujelqc6aKgvPnbRUKkP-ghc7.10.2 clang: error: linker command failed with exit code 1 (use -v to see invocation) -- While building package StateVar-1.1.0.1 using: /Library/Frameworks/GHC.framework/Versions/Current/usr/bin/runhaskell -package=Cabal-1.22.4.0 -clear-package-db -global-package-db -package-db=/Users/dhj/.stack/snapshots/x86_64-osx/lts-3.10/7.10.2/pkgdb/ /var/folders/n2/9h010cts31sc8_n4yd1z__kh0000gn/T/stack29996/Setup.hs --builddir=.stack-work/dist/x86_64-osx/Cabal-1.22.4.0/ build --ghc-options -hpcdir .stack-work/dist/x86_64-osx/Cabal-1.22.4.0/hpc/.hpc/ -ddump-hi -ddump-to-file Process exited with code: ExitFailure 1 Logs have been written to: /Volumes/HD2/src/GitHubRepositories/Flocking-Simulation/.stack-work/logs/StateVar-1.1.0.1.log Configuring StateVar-1.1.0.1... Building StateVar-1.1.0.1... Preprocessing library StateVar-1.1.0.1... src/Data/StateVar.hs:80:8: Could not find module ‘Control.Concurrent.STM’ There are files missing in the ‘stm-2.4.4@stm_C1kFMnPqFjvDhFjgMZGUpr’ package, try running 'ghc-pkg check'. Use -v to see a list of the files searched for. dhjmacpro:~/src/boids
So `packCString` is still slower than `createAndTrim`. Why isn't `createAndTrim` public API in the first place?
I must confess a shallow knowledge here about effect sequence only the intuition that it should be so but I was wondering about it after hearing contradicting things from otherwise knowledgable folks. Phase is a nice way to see it. Could this be reflected in the type of the free applicative : data FreeA f a = Pure a | forall b. f (b -&gt; a) :$: FreeA f b There are potentially n intermediate type variables which do not escape. So to get an a we have to apply all of them "in one go"
the whole confusion comes from not distinguishing class, instance, and terms. if you gain more expressive power from a function requiring an applicative term, it has to be that not every applicative term is a monadic term. You can't say my type is more general, and I can write more function requiring that type. So obviously when you say "Every Monad is Applicative" you refer not to terms but to instances : For every Monad *instance* you can write a new *instance* which abide by the applicative rules 
Ah, I'm on Windows, where you can download the free excellent IDE and dedicated compilers as binaries, no Node dependency (or its bundled so as not to notice). TypeScript properly models jQuery, which is very useful for front end stuff - if I was more logic heavy I wouldn't necessarily pick TypeScript. 
Note: `(*&gt;)` and `(&gt;&gt;)` already must agree in effect by the accumulated set of laws governing their behavior, except they may have different operational characteristics, due to their differing default implementations.
It's just that in do x &lt;- getVar "x" y &lt;- getVar "y" return (x,y) there isn't a single place where you use a monadic function `a -&gt; m b` on something you've computed in a monad, necessitating the use of `&gt;&gt;=`. This code has to be equivalent to: (,) &lt;$&gt; getVar "x" &lt;*&gt; getVar "y" If you can't have a Monad instance where `(&gt;&gt;) = (*&gt;)` and `ap = (&lt;*&gt;)` then I guess you have to do like Validation does and have two (perhaps convertible) types, one of which is only an Applicative.
you know what you talk about but I am not sure it's that trivial for others. by "every Monad is Applicative" you mean : for every monad instance I can write an applicative instance (which ignores the result) let's continue to be explicit at every level : * At class level, the Applicative class set of rules is a subset of the Monad class set of rules (which gains the ability to choose which effect based on a value) * At instance level, every functor respecting Monad give rise to a functor respecting Applicative (where the effect has to be fixed) * At term level, every term representing an applicative computation can be seen as a monadic computation. * At function using terms level, every function taking a general monadic argument to some value can be seen as taking a general applicative term to some value. (and of course we gain the ability to write some new function which take applicative terms to some value, for instance without running the effect) happy to be corrected
You can also view the current definition in terms of preservation of a 'closed' structure. Using `b^a = a -&gt; b` to highlight what are exponentials in the target category: (&lt;*&gt;) :: f (a -&gt; b) -&gt; (f a -&gt; f b) (&lt;*&gt;) :: f (b^a) -&gt; (f b)^(f a) shows how you can map exponentials through the functor, and pairs with pure :: a -&gt; f a for the obvious laws. This has the nice property that you never need to artificially introduce a monoidal structure into the mix.
you are right, I used the word 'instance' wrongly here. I was thinking of terms, not instance in the haskell sense of witness that a type constructor k -&gt; k respect the Monad class.
In general when someone talks about a "Foldable" or a "Traversable" or a "Functor" or a "Monad" or an "Applicative" or a monad transformer or "MonadPlus" or "Alternative", etc. they are _always_ talking about the instance. None of these terms is ever colloquially applied to refer to terms written using these instances. You'll hear people talk about "Foldable" computations, or "monadic actions" or whatever when they want to talk about such terms. You may decide you want to try to be explicit at every level and tack on the word "instance", but people in general just don't. My point was simply that by using the naked word "Applicative" to talk about applicative computations you were getting everyone jumping on you for saying precisely the opposite of your intent. I somehow doubt that one thread buried 5 levels deep on reddit is going to change the pattern of discourse on the topic. You can shout until you're hoarse that everyone should act differently, but I somehow expect that this will be a frustrating and ultimately futile attempt on your part.
true, by polymorphism, if we want to have a monad in Hask, which, in the end, is what the Monad class represent. I used 'instance' in a wrong way here, and was thinking of terms
Note: Hask itself doesn't even enter into the equation You can [generalize the definition of the Monad class](https://github.com/ekmett/hask/blob/master/src/Hask/Tensor/Compose.hs#L140) to allow for arbitrary endofunctors over arbitrary categories and you still can't fix this.
This article makes me realise that strong static typing results in accidental complexity. Is there any good counter argument to the argument below? &gt; Baking a proof into the solution leads to incidental complexity. Once you run into limits of what the type system can easily express then you end up having to write increasingly more convoluted code to satisfy it. &gt; &gt; This results in code that’s harder to understand because it compounds the complexity of the problem being solved with the complexity of expressing it using the type system. ...
Indeed, those laws are what is necessary in the proof that the "Kleisli category of a endofunctor T over a category A", made of the following, is a category : * objects are the regular object of A * arrows are the arrow from object of A to TA * composition g ._K f = \mu . Tg . f * identity on A = \nu_A for this you need nu and mu to be natural transformations from id to T and from TT to T then you can prove associativity (h ._K g) ._K f = h ._K (g ._K f) and left and right identity f ._K id_A = f and id_B ._K f = f, which makes the whole thing a category. But in haskell, Functor (instances) are functors in Hask category of type and functions, and Monad afaik, are monad in that category. The naturality conditions in Hask are the 'monad laws' So it's not like we have a choice of category, unlike what I was suggesting : To be a Monad instance **is** to be a monad in the Hask category, for which the laws have to be respected **forall** types and functions. that's the essence of naturality, where you have to commute *for all* things..
Every unproven invariant adds a burden on other readers of the code to understand *why* it works. If they can't figure that out, the code is likely to be treated as a black box not amendable to refactoring and not reused in other contexts. The complexity is still there, it's just implicit.
This is an interesting article and I'm a big fan of the programmer-oriented POV on languages. To some very important degree, language is a UI for a programmer and is therefore subject to many of the same concerns as any other UI. The most crucial points made in this article seem to stem from this line: &gt; In a language that forces us to use a particular formalism to represent this problem there would be no alternative solution. In particular, this leads to the remark flexibeast quoted &gt; All of this is needed to satisfy the formalisms of the Haskell type system and is completely tangential to the problem of reading HTTP requests from a client. To my eyes this feels like a powerful argument. If formalism is (a) forced and (b) a poor fit then all together it is going to cause impedance. The UI is failing. I don't think one can argue that static types in general force formalisms—some systems are very forgiving. You *can* certainly say this about Haskell, though. So do (a) and (b) hold? I think it's clear that they can. Yogthos' ScottyT example is worth analyzing. He notes this chain of dependencies &gt; [...] we need to understand the ScottyT. It in turn requires understanding the ReaderT. Understanding ReaderT relies on understanding of monads, monad transformers and the Reader monad. Meanwhile, to understand the Reader we have to know about functors and applicatives. To understand these we have to understand higher kinded types and constructor classes. This leads us to type classes, type constructors, algebraic datatypes, and so forth. The argument would be that in order to use `ScottyT` at all we must step through lots of forced, unnecessary formalism. On the other hand, these steps are necessary only because we're seeking to fully analyze the structure of `ScottyT`. To merely use it we can do something much simpler by abusing `do` notation and a few conventions and then the public API of {`get`, `post`, `put`, `delete`, `patch`, `options`, `addroute`, ...}. This does not produce nor require a full understanding of Scotty, but certainly works. In my mind, the reasons one is pushed away from this mode of use are important. Immediately: 1. The types are not abstract, so one *can* and is *encouraged* to poke inside and question. On the other hand, to use a Clojure comparison point, I have no idea how core.async's `go` macro works, really. To try to piece it together would be a massive undertaking due to its opaqueness, so instead I treat it abstractly and simply pay the lack-of-understanding tax—especially when it fails to work. 2. Error messages can and will drive you batty. 3. Potentially, Scotty is poorly designed and the sharp edges here drive you to hack around it. (2) is straight up a problem and nobody could disagree. What's worse is that there's some evidence (Agda, Elm) to state that it could be turned into a boon with some significant elbow grease, but GHC currently is simply lacking here. (1) and (3) are both related to the idea that Haskell puts abstraction *right in your face* much of the time. You cannot easily hide it. I think the mark of a successful Haskell programmer is, thus, knowing how to ignore the complexity and let the formalism bear the weight. My favorite example of this is the natural duck-typed, subtyping effect system created by `mtl` which many professional Haskellers use naturally. It's wildly flexible yet difficult to misuse and relies on, again, letting the formalism bear the weight of many complex bits by, as the programmer, using only exactly what you need. This "aesthetic" mode of working is powerful. I think it leads to better API design and I know it leads to better use of typed formalisms. So back in the initial argument there was an assertion that when bad formalisms are forced on programmers then issues arise. I cannot disagree, but I also rarely feel this pain. Instead of feeling forced, I nearly always feel *liberated* by formalisms since I strive to use them "lightly". On the other hand, Clojure feels opaque to me. Without guidance as to how things are assembled—when we side with the power to create things, we lose the power to analyze them!—I am at the whim of the assumptions and design of the systems I'm using. When they break I'm often completely SOL. The lack of insight into the operation of my code means that when an API is a poor fit I feel lots of pain. So I feel there's a kind of duality at play here. Formalism being "forced" on you means that you have to know how to ignore it and use an API "lightly" but then gives you many inroads to analysis. Proceeding without any such formalism means that you code "heavily" through guessing and checking your structures into the operation you want (repl-driven design) but are left with little guidance when things break down. To my mind these are just two kinds of UXes for PLs. I don't want to make some sort of industrial software engineering claim about one or the other being superior. I want to talk about how we can excel in either mode. In particular, I keep talking about programming "lightly" with Haskell yet I can't claim that I don't know what every single thing in the list of items Yogthos listed is and means. Can programmers learn to work nicely with these formalisms without learning them all? I don't know, but I certainly don't know how to teach someone any other way. On the other side, there's opportunity in things like gradual typing to introduce more visibility into at least some parts of the language. It's not clear that this can be successful when it's only a partial effort—many benefits of formalism arise from it being "complete" analysis, after all—but it's certainly an interesting experiment. --- Finally, I want to mention one last thing about Yogthos' argument. He talks about the burden of proof &gt; The main drawback of static typing is that you're required to prove what you're stating to the compiler. Anybody who has done proofs knows that stating something is always simpler than proving it. In fact, many things are very simple to state, but are notoriously difficult to prove. Fermat's last theorem is a famous example of this. &gt; Baking a proof into the solution leads to incidental complexity. &gt; Once you run into limits of what the type system can easily express then you end up having to write increasingly more convoluted code to satisfy it. These all amount to two assumptions, I believe, which are not necessarily true. 1. Formalisms force upon you proofs of uncontrolled complexity 2. Formalisms require proofs to live next to your code Both of these are fluid and properties of the particulars of a type system, not of static typing at large. External proofs exist in many systems and, of course, there is a wide range of choices around how much a system demands you prove just to work in it. Thus, both of these become a question of design and not one aiming at the good old holy war we have here. So a real question is: how do you design a system which lets one choose the most economical things to prove and to manage these proofs very well? - One point in design space is to eliminate all proofs and all management. This point would win if proofs and formalism carried no weight at all. - Haskell lives at a point where certain "completeness" proofs are desirable (e.g., no nulls) and therefore all admissible code must create and furnish these proofs. Working below that level anywhere would annihilate completeness. This point wins if certain "complete" properties are desirable. - Agda lives at a level where practically any desirable property could have a proof and this proof could live either internally or externally to the code. Proof management becomes tough here, but the richness is amazing. These, I think, are interesting questions and drive at the *UX question* of "how valuable are proven statements and their proofs to programmers?" I'd much rather talk to that question than be limited to any sort of "static v. dynamic" jockying.
Well, that's a statement we could almost agree with. I would say instead: "Baking a proof into the solution *may* lead to incidental complexity". It does not, when the proof is automated enough, and close enough to the structure of the program itself. There are several excellent solutions to this problem: - Do more design work to find out elegant ways to express proofs along program, to get more static guarantees for the same convenient code; people usually think of "language design", but "library design" is also key here (the right API will make correct-by-construction programs much easier to write). - *Don't* try to prove everything through the intermediary of the type system. If an approach you try "compounds complexity", then stop! In a statically typed language you always have the opportunity of using a less-precisely-typed datatype. It is perfectly correct that strong static typing *may* result in accidental complexity. Most programming languages aspects/features have this property when not wisely used.
I prefer to think that the absence of these monads correspond with the difficulty of coming upon a wider context in which the monad instance makes sense. For example, in the [transient](https://github.com/agocorona/transient) library, the `Applicative` instance also can express parallel computations with a final concurrent result. This is because both terms of an applicative instance can be manipulated independently since the result of the second term does not depend on the first one. That is what happens with bind (`&gt;&gt;=`). So definitively, bind can not express concurrent computations. But that does not mean that there is no monad for concurrent execution. The monad could chain different concurrent computations one after another. That may make it worth to exist. do r &lt;- (,) &lt;$&gt; async this &lt;*&gt; async that .... more applicatives... But there is more: if the `Alternative` instance is coherent with an applicative instance that express concurrency, the former should express parallel executions of threads that the monad can execute in a kind of non-deterministic computation. That is something that I strumbled upon when I made the alternative instance for transient. For example: do r &lt;- async this &lt;|&gt; async that ... `r` has two possible values in each of the two threads initiated by the alternative expression, that the monad will carry on and execute simultaneously. so `&lt;|&gt;` would be the operator for parallelism, `&lt;*&gt;` for concurrency and `&gt;&gt;=` would chain non-deterministic multi-threaded computations in general. That would be the wider interpretation, which means a beautiful unification of some loosely related concepts into something that is coherent and fully functional, since it permits to express arbitrary compositions of parallel, non-deterministic and concurrent computations. I have to say that I initially rejected the behavior of the alternative and the monad instances. I used to kill the extra threads that the alternative instance produced, until the EDSL and some experiments taught me this wider interpretation. The same may happen in other examples of Applicatives. I wrote in this thread about the interpretation of the monad instance for formlets (that includes validation). 
By monadic computation, I mean a value of type `m a` where `m` has Monad constraint. By applicative computation, I mean a value (or term) of type `f a` where `f` has Applicative constraint. For me, that's the whole point of all this : representing computation as values. a value of type `m a` where `m` is a monad is a computation whose eventual run will produce a value to type `a` When you say "every Monad is an Applicative" I understand "for every Monad instance I have an Applicative instance where the result is forced to be ignored for computing the next effect" (automatically now with AMP, but that's another issue) So for every applicative term `f a` I could have build a term `m a` which represents the same computation (where I happen to not have looked at the results to determine the effect used) In that sense, ever applicative computation (term : f a) is a monadic computation (term : m a). That makes sense since we require more of an applicative term : naturally the representation of a computation which is more constrained should be setwise included in the representations of computations which have more freedom. --- There are less functions of type : (monadic computation to something) than there are of type : (applicative computation to something) The trivial case is to count the number of times effects are applied : it can not have the type : (monadic computation to int). Indeed if your monadic computation relies on the outside world, different runs would result in different results : that's not a function. 
&gt; not all **types** have a + function defined on them 
&gt; Formalism being "forced" on you means that you have to know how to ignore it and use an API "lightly" I cannot agree more on this. It tend to think of ignoring complexity just as a natural way of frustration avoiding mechanism. I got frustrated so many times while trying to learn Haskell (and other things too) and got frustrated because learning concept X brought up to many other new concepts. Then I started embracing the idea that *it is OK* that you don't know/learn everything about the domain. Learn just enough to make you productive (and in context of Scotty this would be the topmost abstraction layer which lets you to make requests). If you stick with the problem long enough, eventually you will learn more. I find myself really dumb when it comes to learning new abstractions. As a elementary school kid I was really good at solving algorithmic problems so I was attending programming competitions. We were programming in basic/c back then and I was really good at it. But when it came to learning OOP (ie Java) at a collage, most of my friends who never programmed before have grasped it in a week while I was struggling for few months to see the point behind the OOP idea. And after the collage we bootstrapped a company in Yesod/Haskell, without understanding the big picture about Monads , without any custom defined typeclasses and without knowing how Handler type is implemented. And that was enough for us to start and be productive. After some time you will learn more and then you won't even need to read how stuff are implemented. Only looking at types will give you enough info to keep you going. So don't force yourself to learn everything at once. I still don't know how my Intel processor works but I know enough to keep me going. And this "let it go, you don't need to know/learn everything at once" is not even Haskell specific. I take this approach generally in life.
The discussion is important though; not because either side is objectively right, but because it highlights that there are two radically different ways of thinking, and most programmers aren't as good at one as they are at the other. This means that programmers aren't interchangeable, programming languages *do* matter, and in order to build a successful programming team, you need to figure out how to get your programmers to be compatible on these matters, and how to find a programming language, paradigm, and workflow, that works for all of them.
I can understand initial mistrust - type system can be very overwhelming at first. But like in math, you can use it without fully understand it. I remember using some networking libraries to analyze network protocol and do web scrapping (http+html+xml) and I am pretty much noob when it comes to networking. Yet libraries allowed me to do it without scratching my head. So I find this complaint about HTTP request to be false. Yes, types can be quite sophisticated, but not necessarily from the point of the user (library API). Having type system allows programmers to share common vocabulary to talk about their code which is in touch with compiler and documentation (haddock, doxygen...).
Shouldn't `f'a &lt;*&gt; x'a` be equal to `do { f &lt;- f'a; x &lt;- x'a; return (f x) }`, though? How do you do this, if your `&gt;&gt;=` can't handle concurrency while `&lt;*&gt;` can?
You should ask the maintainer, but I would guess that it's one of those functions most people don't need, but the maintainers need to be able to change it's semantics to aid in optimisation and such.
Yeah, I forgot about `Proxy`. I meant that in general case you can't turn an arbitrary Applicative into a Monad by disregarding the value somehow. But reading /u/existsforall 's other posts I guess he didn't really mean what I've thought he meant, and it was all just a misunderstanding, anyway.
&gt; The discussion is important though; not because either side is objectively right, I think that whether this is true remains to be seen. If there were scientific reproducible evidence available that obviously demonstrates that you (1) lose nothing, and (2) get something worth it in return then the argument could have an objectively determined "right" side. 
Builds up a straw man ("Proponents of static typing accept its benefits as axiomatic") and awkwardly tears it down (type systems have trade-offs). Wasn't worth reading, sorry.
There are two different conversations here: 1) Typed vs untyped 2) Opaque stateful things vs exposed immutable data I think Haskell made the right call on the first question, but not on the second one. The purpose of every program is to interact with opaque stateful things (screen, file, network...) so giving them second class treatment is wrong. The String = [Char] fiasco shows that using exposed immutable data in APIs is a bad engineering decision even if the APIs are very stable (and God help you if they aren't).
I don't understand why you think this rhetoric is bad, or why you're characterising this as a silly fight. Although I disagree with many of the points this article makes, I thought it was well written, I enjoyed it, and it has made me want to learn Erlang, too. I also think it's good to see posts from dynamic language proponents from time to time; I think I'm personally at risk of putting myself in a statically typed FP echo chamber, and I don't think that would be a good thing. Anyway, to summarise, I'm glad it was posted here.
Further, I think it's worth pointing out that the State monad being hard to express in dynamically typed languages doesn't imply that it's hard to express in statically typed languages. I've tried to express a few Haskell idioms, such as the State monad, in dynamically typed languages. I've also seen a few people blog about attempting the same. In my experience, they are too awkward to be worth doing. But I think they're absolutely right for Haskell.
&gt; I meant that in general case you can't turn an arbitrary Applicative into a Monad by disregarding the value somehow. I completely agree with that, yes, and that's very much worth saying. My ability to miss the point is apparently undiminished! :) 
I've only casually taken a look at F* in the past but IIRC it's pretty similar to what you can do with LiquidHaskell.
&gt; The more complexity is layered on top of the original problem the more difficult it becomes to tell what purpose the code serves That's exactly what I thought when I read about new role machinery coming to fix GNTD + type family problem. Why it introduces another concept of nominal vs representational equality instead of just saying that newtypes is no longer compile-time only and may introduce generation of new wrapper instances? Whole concept of newtypes seems like unnecessary complexity to me.
I'm not disagreeing with the fact that the article may be enjoyable. A post that cites all three of Erlang, Clojure and Haskell cannot be a bad read, right? Yet I suspect that the article (and the equally disappointing [followup](http://pointersgonewild.com/2015/11/25/have-static-languages-won/) that it expands on) is motivated by an affective reaction to a silly claim more than anything else. Have a look at the last sentence of the introduction: &gt; Proponents of static typing accept its benefits as axiomatic. However, I think that's a case of putting the cart before the horse. Let's take a look at the claims from the perspective of a dynamic language user. This is the textbook example of bad journalism at play here (and yes, the general post is well-written, and I think the blog it belongs to is well-informed and interesting in general). You start with a strawman argument, and then "Let's look at ...." brings you to two paragraph that say 1. There is little experimental data on the benefits of static typing! (Yes, we know.) 2. Static typing allows to prove things, but proving things sometimes has a cost! (Duh.) Good writing and linked quotes and good cultural references do not quite compensate for the fact that this article is actually rather devoid of substance. I think the general problem with these blog posts is that they are written as in a vacuum. Reading them, it looks like nobody has ever debated dynamic vs. static type systems before. The arguments made have been made dozens of time already, and some of the most interesting points of past debates have been forgotten here. This is history reinventing itself, and not in a good way.
Let me rephrase that then; neither side is unequivocally right for all sets of priorities. There is a nonzero cost to using a static type checker, and there's a nonzero cost to not using one. So (1) you always use *something*, and (2) you always get something in return; but it's not obvious which way the balance tips, and I suspect developer brain architecture is a huge factor in the equation.
Doesn't Scala have HKTs despite the JVM being much less featureful than the .NET CLR?
&gt; I think LiquidHaskell loses a bit by living in a separate layer over the language. You don't have to see it that way. The "layer" you speak of is just a type refinement, which is a valid type system in itself where judgements have a "core type" and a "refinement".
The problem with the statement about statements and proofs I think goes further. Firstly, given some obviously true statement that you have not proved there is a good chance it is in fact false. Secondly, given some property of your code that you have documented but not stated formally there is a good chance the statement itself is incomplete or inconsistent. I've found the ruby community frustrating because bugs in packages widely used in industry seem often to come down not to incorrect implementation but to the fact that the authors without realising it simply don't know what the code should be doing. 
Still, having this built into the language itself is convenient. Also, I'm not sure if LiquidHaskell can handle declarations like (copied from the tutorial I've mentioned): (* Defining a new predicate symbol *) opaque type total_order (a:Type) (f: (a -&gt; a -&gt; Tot bool)) = (forall a. f a a) (* reflexivity *) /\ (forall a1 a2. (f a1 a2 /\ a1&lt;&gt;a2) &lt;==&gt; not (f a2 a1)) (* anti-symmetry *) /\ (forall a1 a2 a3. f a1 a2 /\ f a2 a3 ==&gt; f a1 a3) (* transitivity *) val sort: f:('a -&gt; 'a -&gt; Tot bool){total_order 'a f} -&gt; l:list 'a -&gt; Tot (m:list 'a{sorted f m /\ (forall i. mem i l = mem i m)}) (decreases (length l)) let rec sort f l = match l with | [] -&gt; [] | pivot::tl -&gt; let hi, lo = partition (f pivot) tl in append (sort f lo) (pivot::sort f hi) I mean, that would have to compile in Haskell first, and I have no idea how to write this `total_order` type.
I will answer obliquely by encouraging you to have a look at the slides of [From Soft Scheme to Typed Scheme: Experiences from 20 Years of Script Evolution, and Some Ideas on What Works](http://www.ccs.neu.edu/home/matthias/Presentations/STOP/stop.pdf), Matthias Felleisen, 2009. You will learn a lot about excellent research *and* practical experience to combine types with one of the most interesting dynamically typed languages around, and maybe thinking about why this is hard and interesting may teach us more than just pondering about the virtues of static typing or dynamic languages in isolation.
exactly. I think it makes more sense to think of terms and types. Then you can get out of haskell specifics back to general system-f and reason about polymorphism. I did witness that to be quite confusing for many people, and rightly so : there are 4 levels to choose from (classes, instances, terms, fonctions using terms), all going in a different directions from the previous one. Someone learning about those things (I did recently) will not necessarily realize that the common parlance is meant for instances, which go in the opposite way as terms. (that said my comment was also implicitly talking about terms - as computation - so I fell in the same implicit trap too)
For me (and many others :D) it is obvious that some static typing (up to Haskell 98 level) is beneficial. There is a reason that every larger python program ends up with types in comments.
If you are saying what I am thinking then I can't agree with this more. Yeah, you have your proofs but you cannot prove that the way that those proofs are represented are the most understandable nor digestible for others people understanding. To me code that can describe itself through its design and usage is more value than the proof that constraint it.
Well there are some proponents of static typing at least in Reddit that are actually like that.
Because he was discussing it in the context of `Just (+)`
&gt; What I am not clear about is what happens if you want a behavior for (*&gt;) which can not be implemented for (&gt;&gt;). If it can't be implemented for (&gt;&gt;) then you have a problem with your laws. You can have a Monad that "typechecks" for `ZipList`. It isn't compatible with the Applicative, so you can't write it, and we don't supply it. The AMP (and convention before that) requires the Monad and Applicative to agree, so (1) is what happens.
I would say the minimum complexity is probably somewhere far from the dynamic typing end of things (adding some types is easy) but certainly not at the end where as many invariants as possible are expressed in the type system (e.g. a lot of the dependently typed languages make things more complex than they need to be if you express everything in types).
Because , while bind is not parallelizable, `&gt;&gt;` is. That's why Simon want the `applicativeDo` extension: to do the reverse transformation that you mention and to permit the parallelization of `&gt;&gt;` . At the same time, it permits the more intuitive `do` notation for parallelization instead of the weird combination of applicative combinators that can become complex and hard to understand. applicativeDo will permit the parallelization of `&gt;&gt;` in transient too.
I think the first real program I wrote in Haskell was [my blog software](http://gilmi.xyz), I used scotty + blaze-html to create it. I followed [this guide](http://adit.io/posts/2013-04-15-making-a-website-with-haskell.html) as a basis of how to build a website with scotty and used the documentation on hackage to find the rest of the parts I needed. I did somewhat understood how to work with simple monads, but not with monad transformers and not with other libraries like bytestring (and it probably shows), still, I managed to build a working blog software in a weekend. I sure do miss tutorials, especially for domains I don't have experience with, But there is nothing inherited in Haskell that makes it hard to use libraries without fully understanding them. I know that from experience.
This is a well-written, but fundamentally mistaken, article. The "original problem" of HTTP request parsing is very complex. There are timeouts, malformed requests, headers and body, character encodings (before we even touch the data that we're parsing out of the requests). The [flow chart](https://camo.githubusercontent.com/4e15cccf2a9277dcca2c8824092547dee7058744/68747470733a2f2f7261776769746875622e636f6d2f666f722d4745542f687474702d6465636973696f6e2d6469616772616d2f6d61737465722f6874747064642e706e67) is massive. We could debate the success and design of `Scotty`, but it certainly isn't adding complexity to HTTP: it requires understanding (as a *user*, not an *implementor*) several types, but when you have achieved understanding of those types, you can trust the library. Compare this to a popular library like Ruby's [Mechanize](https://github.com/sparklemotion/mechanize), which in various states will raise exceptions, return parsed HTML, or return raw text. An HTTP *client* is simpler than a *server*, but even here, the dynamic language solution will require handling at least three potential scenarios. If you see it being used without taking these into account, it's a bug, which may or may not happen *in practice*, but which exists as something that *can* happen. Its simplicity is an illusion. Similarly, when you decide to parse JSON (in case you've gotten an error from the remote service, for example), you must remember to catch `SyntaxError` separately; it's not a subtype of `StandardError`, but you still *must* deal with it. If you don't, you're trading "ease" of development for a difficult-to-understand error later.
Your `Useless` monad is actually pretty useful :D
In LiquidHaskell (LH) we deliberately avoid specifications like the above because they yield SMT queries that fall outside the well understood decidable *logics* and makes verification rely heavily on brittle *solver* specific heuristics. The main advantage of external proofs (like the above) is you avoid "baking invariants" into the types. In LH we achieve this via abstract refinements, illustrated here: http://goto.ucsd.edu:8090/index.html#?demo=permalink%2F1448818952_5024.hs As an added benefit, the approach yields rather simpler proofs than in the F* version you describe, albeit at the price of expressiveness. For example, the complete code+spec+proof for "quicksort" is: {-@ type IncrList a = [a]&lt;{\xi xj -&gt; xi &lt;= xj}&gt; @-} {-@ quickSort :: (Ord a) =&gt; [a] -&gt; IncrList a @-} quickSort [] = [] quickSort (x:xs) = splice x lts gts where lts = quickSort [y | y &lt;- xs, y &lt; x ] gts = quickSort [z | z &lt;- xs, z &gt;= x] splice piv [] ys = piv : ys splice piv (x:xs) ys = x : splice piv xs ys 
Yes. However, Haxl does not make use of alternative computations, which would force to handle non-deterministic computations in the monad. The Haxl execution model is a controlling thread in the monad that spawn parallel fetches and make an implicit wait for the result. The fetching threads die. In transient, there is no controlling thread. It is based on continuations; In the Applicative instance, if both operands are parallelized, the thread which finalizes first dies. The second is the one that get the result of both, calculate the final result and continue the monadic computation. In the Alternative instance, both continue in parallel. It is designed for managing any exception of any lifted IO computation as errors (`SError`). otherwise the tread an all their children would be killed. The errors can trigger [events](https://www.fpcomplete.com/user/agocorona/publish-subscribe-variables-transient-effects-v) which can have multiple catching (suscribe) locations downstream or upstream of the current execution point, so they are more flexible than exceptions.
&gt; Remember: A list is used as a container for (usually constant) functions. Are you falling prey to [the "everything is a function" misconception of functional programming](http://conal.net/blog/posts/everything-is-a-function-in-haskell)?
Hmm, I want to make sure I am 100% clear on this. In the following code I have a type `Env`: http://lpaste.net/146165 For that type `Env` I can see two possible implementations of the `Applicative` instance, but only one implementation of the `Monad` instance. With the `Applicative` instance we can decide to abort and return the first time we get an error (Left) -or- we can accumulate a list of all the errors. For the `Monad` instance we have no choice but to abort after the first failure. I have shown in my code that for the *find them all* solution, the `Applicative` instance seems to pass all the laws. Additionally, it is possible to provide a `Monad` instance that satisfies the classic `Monad` laws that pre-date the existance of `Applicative`. But we now also have some new laws that state `return == pure` and `ap == (&lt;*&gt;)`. So are the following true? 1. it is valid to implement the accumulating `Applicative Env` instance as long as we don't provide a `Monad Env` instance. (Since it would break `ap == (&lt;*&gt;)` law). 2. it is valid to implement the `Applicative Env` and `Monad Env` instances as long as both use the non-accumulating error instances 3. It is *invalid* to implement `Applicative Env` using accumulating and `Monad Env` using non-accumulating. I am pretty clear that 2 &amp; 3 are true. I am less certain than 1 is true (though I'd like to believe it is).
Continuing on your argument, very often its simpler to protect invariants through data abstraction instead of trying to encode everything in the type system.
You're right, I was!
&gt; In fact, many things are very simple to state, but are notoriously difficult to prove. Fermat's last theorem is a famous example of this. Well, yes, but I also wouldn't try *making use* of Fermat's last theorem without knowing it were true first.
Does the LH version have to trust somehow that `Ord` defines a total order? I've checked that the F* version wouldn't typecheck if the `total_order` refinement was removed.
Yes, I tested TypeScript in VisualStudio on Windows, and I was quite underwhelmed. I think an additional issue is also the overuse of graphical UIs for project configuration. The horrible dependency manager in the GUI ... argh. Then add the JavaScript Bower/Grunt/Gulp/Closure/whatever it is this week's favorite tool. &gt; TypeScript properly models jQuery, which is very useful for front end stuff - if I was more logic heavy I wouldn't necessarily pick TypeScript. That's nice, but how does that separate it from alternatives? All in all TypeScript felt just very raw and unfinished, and it doesn't seem to have changed recently with all these huge changes going into minor versions.
&gt; Don't try to prove everything through the intermediary of the type system. Yeah I run into this a lot :P I had an incident that went like this: First I wanted a bit of phantom types to avoid mixing up two kinds of indices, and then 2 hrs later I find myself using DataKinds and TypeFamilies. It's not all that complicated, but having a list of 12 GHC extensions at the beginning of a module does appear rather intimidating and seemed very much overkill for what should've been a relatively simple program. Sometimes I have to remind myself that just using plain ADTs is already a huge improvement from the traditional programming languages.
&gt;For reasons that we'll come to when looking at monads, `Validation` cannot form a monad, while `Either` can. Up until this part, `Validation` is presented as a special case of `Either`. You should probably introduce some differentiation *before* claiming there's no `Validation` monad. Otherwise, I feel like `Applicative` as presented in Haskell is kind of obfuscated. It could have been named `Zippable` and [presented it as follows](https://www.reddit.com/r/haskell/comments/3ulr7e/missing_the_point_of_applicatives/cxg3otd): unit :: f () pair :: f a -&gt; f b -&gt; f (a, b) This has the advantage of making the "monoidal" part of "strong lax monoidal endofunctor" obvious.
can you explain the &lt;\\xi xj -&gt; ... &gt; syntax? is that builtin? are xi and xj adjacent items?
&gt; and it appears that programmers tend to gravitate towards projects for which their preferred paradigm is the better fit. I would assume that this only holds for programmers who have the experience to know what is a good fit for them. Which of course leaves us with the problem of how to match up the rest with adequate problems - or how to help programmers with figuring out what their strengths are.
If you need to understand the inner workings of an abstraction in order to use it, it's a failed abstraction. I started using Yesod before I understood what ReaderT was or even really what monad transformers were. I could just write code in the Handler monad using do notation and the documented API for inspecting request params and rendering responses. It worked just fine for me without needing a deep understanding of the internals.
My reading on `Applicative` is giving me the impression that it's close to the right abstraction for an assembly language DSL. But `fmap` is kind of obnoxious, because it means your assembly `Applicative` can contain non-assembly code. How would you suggest reconciling this to obtain a pure assembly DSL? Maybe using an `Applicative` variant built on a version of `Functor` that's generic over the function space it covers (i.e. what `Arrow` it uses)?
Do you agree, though? It sounds like you're arguing that some Haskell libraries are too complicated --- surely true! --- but you also make the point that it's perfectly possible to write simpler libraries in Haskell. Whereas yogthos is arguing that Haskell's static typing forces you into over-complicated solutions.
Well it needs the functor superclass too. and sometimes the "zipping" is actually a cross-product that doesn't feel like a zip at all.
Can the other alternatives interact with jQuery nicely and in the same style as the existing jQuery? I took a look, and many seem to be quite different, whereas TypeScript accurately models jQuery very nicely as is. TypeScript has advanced significantly in the last few minor releases - there were lots of things I wanted to do that I considered essential that I found were later additions, e.g. intersection types is only as of 1.6 (which might only be a month or two old). I only tried it first a few weeks ago, which might account for some of the difference in opinion. GUI for project configuration is nasty - I just shut it, edit the XML, reopen it. Unpleasant, but not fatal.
I chose it because I thought about its inconsistencies while reading the article's discussion of `Scotty`. It is frequently recommended for working with HTTP requests in Ruby (and has versions in Perl and Python), but presents several non-obvious complexities. It's possible to "use" the library without understanding these complexities, but it will introduce many bugs that can be hard to understand. That's the trade-off that the article is glossing over: `Scotty` may require understanding more concepts first, but the complexities are dealt with *by those concepts*. The incidental complexity introduced by libraries like Mechanize is not governed by any concept which can be learned, but is frequently undocumented, inconsistent between (or within) libraries, and is frequently discovered only via failures in production.
&gt; Understanding ReaderT relies on understanding of monads... Understanding `ReaderT` just requires understanding passing an argument to a function, actually. The fact that this _happens_ to yield a monad, a monad transformer, a functor, etc. is just an additional observation.
If `fmap` is obnoxious there, then so is `pure`, because any type you can put into your Applicative via `fmap` can be put there via `pure`. It sounds like you only want the `(&lt;*&gt;)` operation, keeping the functionality of `pure` in a trusted kernel.
I know this wasn't the point of your comment, but it is what I got out of it anyway. What don't you like about the design of the Scotty library, and are there any webserver libraries that do it correctly? (Does Spock, for example?)
Where can I read about that?
What did you use to self-study Category Theory?
I usually avoid articles like this, but I'm grumpy this morning and this particular article struck a nerve, so here we go: &gt; In fact, some of the largest and most robust systems out there are written in dynamic languages such as Erlang... It's always Erlang when someone wants to point out a high-reliability system written in a dynamically-typed language. It's never Python, Ruby, Javascript, or any other dynamically-typed language. It's certainly never PHP. Do you think the author writes Erlang professionally? He talks about Clojure a lot on his blog, where's the nine-nines system written in Clojure? Maybe Erlang doesn't owe any of its reputation for high-reliability to dynamic types. &gt; The main drawback of static typing is that you're required to prove what you're stating to the compiler. I'm not required to do anything. I *leverage* the type system. I write code with carefully crafted types so the compiler can do work *for me*. If I want to tell the type checker I know better, then I just tell it and it doesn't question it. &gt; Once you run into limits of what the type system can easily express then you end up having to write increasingly more convoluted code to satisfy it. Or we acknowledge that there are limits to what we can prove in the type system and write code the compiler can't check. Y'know, like if we were writing in a dynamically-typed language. &gt; What are we concerned about when we use the state monad, we are shunning mutability. Where do the problems surface with mutability? Mostly around backtracking (getting old data or getting back to an old state), and concurrency. This is a quote from someone the author is quoting, but I need to comment on it. So apparently the state monad is for shunning mutability, which we want to do to avoid issues with backtracking and concurrency. So really it's for when we want backtracking and concurrency, right? Except the state monad doesn't support backtracking, and it's inherently single-threaded. "So what's the point?" indeed. Next is the bit about Scotty. As luck would have it, I've actually used Scotty before. &gt; To use it effectively we need to understand the `ScottyT.` It in turn requires understanding the `ReaderT`. `ScottyT` doesn't reference `ReaderT` anywhere. If it uses `ReaderT` in its implementation it is completely opaque to the user. How did the author even come to the conclusion that `ReaderT` is involved somehow? Furthermore, why do I even care what the implementation of `ScottyM` is? Everything in `Web.Scotty` works in terms of `ScottyM`. I don't have to know anything about it beyond that it's a black box of web magic. &gt; All of this is needed to satisfy the formalisms of the Haskell type system and is completely tangential to the problem of reading HTTP requests from a client. This has nothing to do with "satisfying the formalisms of the Haskell type system." This is about library writers-- *not* the Haskell type system-- setting up constraints to guide the user down the correct path. If you look at Scotty's Hackage page you'll see an example web app that is four lines of code with no types mentioned. Where are the formalisms there? Where are all the proofs claimed to be required? What is being done beyond reading HTTP requests? &gt; Of course, one might argue that Haskell is at the far end of the formal spectrum. In a language with a more relaxed type system you have escape hatches such as casting and unchecked side effects. I was going to write something snarky here about `unsafeCoerce` and `unsafePerformIO`, but instead I'm going to take the opportunity the talk about why I tend to avoid these kinds of articles. The underlying issue here is one of ignorance: the author simply doesn't know Haskell. And I don't say that so that we can point our fingers at him and mock him. I say that to point out that in spite of him not knowing much of anything about Haskell he still holds strong opinions about it. That is ultimately the issue here. I have lots of opinions about lots of programming languages. The reason for this is because I have *used* lots of programming languages. One language that I haven't used that the author has is Clojure. For me, it's obvious that it would be completely out of my place to write up a blog post criticizing Clojure based on what I've gleaned from Wikipedia, what my friends have told me, and whatever I dig up scanning through the docs of popular Clojure libraries. All my opinions would be completely misplaced due to my own ignorance. Yet for whatever reason time and time again we see posts pop up here on /r/haskell from people who have never written any Haskell code being critical of the language. What's to gain from arguing against these articles? Yogthos isn't going to change his mind. There's no bit of insight to glean here, either, as he has no idea what he's talking about. Just more wrong statements that need to be corrected.
This is the best I've found: http://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/
yogthos is the master of "static typing means proving everything about your program to the compiler" FUD. Every post by them ignores the fact that the programmer usually has quite a bit of control over just how much has to be proven. 
How ? `new` is a parameter of `sumST'` .
Now that they supply an ARM binary distro, it feels like all the work (weeks of work getting it to compile on the Parallella Epiphany board) I previously did was really pointless.
You're telling me that I don't have to teach my kids what a Monoid is before I teach them how to count apples?
A free monad or operational seem like the right abstraction for an assembly language to me. What seems right about Applicative?
Testing for features instead of platforms is an idea that goes at least as far back as autoconf.
 sumST' :: Num a =&gt; (forall s. a -&gt; ST s (STRef s a)) -&gt; [a] -&gt; a (Note: requires `RankNTypes`)
&gt; I sure do miss tutorials, especially for domains I don't have experience with, But there is nothing inherited in Haskell that makes it hard to use libraries without fully understanding them. I know that from experience. I agree with this. I was reflecting more on the culture, which, it seems to me, tends to advocate understanding before use. I guess I was wondering if the language would appear more user-friendly otherwise.
Thanks
Don't say that! They may never have released an ARM binary unless people wanted it. Maybe putting in the work to get it to compile shows that people want it enough to bother with a binary distro :-)
[Here](https://ghc.haskell.org/trac/ghc/wiki/Roles)
I actually was thinking of your turtle library as an example of an API and a tutorial that promotes just using it without digging too deeply. I think that's one reason it's a good, complementary contribution to the Haskell ecosystem: it's a different kind of invitation for beginners.
Oddly enough, I found it vastly easier to understand `Monad` in terms of `=&lt;&lt;` than `&gt;&gt;=` (or `join`): ($) :: (a -&gt; b) -&gt; a -&gt; b (=&lt;&lt;) :: (a -&gt; m b) -&gt; m a -&gt; m b “Oh, it’s just application!” Pedagogy can be weird. What “clicks” for one student can be totally meaningless to another. 
I don't know what to think anymore &gt;_&lt;
Ah, gotcha. So `($ 3)` is a function that takes a function and applies `3` to it.
You have a lot of structure in the category of problems. It is Cartesian. If you have two problems you can tuple them up. You can try to forget one of your problems. If you have a problem you can solve it twice, so you can contract. It is closed. You can talk about entailment, e.g. given this problem I have that problem. So you can build the reader monad in the category of problems. You can also look for monoid objects in the category and build the writer monad. Even you want to deny the Cartesian structure so that you have to handle your problems and not just forget about them it is still a monoidal category, so the writer monad exists for any monoid in the category.
Let C be the category of problems involving regular expressions. For the unit counit thing, we have that the one is that from any problem there is a morphism from that problem to that problem involving regular expressions called "I tried using regular expressions" and the other being "I used regular expressions to match my regular expressions." Really, I think this is just a state monad or something.
Cartesian does not mean tuple in every category. We must go to universal properties! For problems P1 and P2, a product of P1 and P2 would (P1 * P2) would be such that you have projections (P1*P2) -&gt; P1 and (P1*P2)-&gt;P2, and for any z-&gt;P1 and z-&gt;P2, we have unique z-&gt;P1*P2 that is compatible with projections. P1*P2 is therefore the problem of deciding whether to have P1 or P2. The projections are deciding to make it one problem or the other, and z-&gt;P1*P2 means that when problem z can be turned into P1 or P2, we can turn it into a decision between problem P1 or P2. For example, if you can either solve a problem with regular expressions, or make it worse in which your house is on fire, you can also transform the problem into a choice of using regular expressions or setting your house on fire. A sum type of P1 and P2 would be (P1 + P2). This means that P1 -&gt; (P1 + P2), P2 -&gt; (P1 + P2), and for each P1 -&gt; z and P2 -&gt; z, we have (P1 + P2) -&gt; z. Therefore, P1 + P2 means we either have problem P1 or P2, but were not sure which (and we can't just choose). If baking turns I'm hungry to I'm out of ingredient, and selling ingredients turns I'm poor to I'm out of ingredients, then I know that if I'm hungry or I'm poor, there is a way to transform this to being out of ingredients. Another example of a problem is "I'm over analyzing the concept of 'problem'" Exercise for the reader: In terms of universal properties, what is the exponential object?
great analogy :-)
&gt; In particular, this leads to the remark flexibeast quoted &gt; &gt; &gt; All of this is needed to satisfy the formalisms of the Haskell type system and is completely tangential to the problem of reading HTTP requests from a client. &gt; &gt; To my eyes this feels like a powerful argument. If formalism is (a) forced and (b) a poor fit then all together it is going to cause impedance. The UI is failing. I think there is a flaw here right from the outset, which lies in the assumption that formalism that is tangential to the the problem of reading HTTP requests should be eliminated. Let's go back to the list of things that Yogthos cites: &gt; &gt; [...] we need to understand the ScottyT. It in turn requires understanding the ReaderT. Understanding ReaderT relies on understanding of monads, monad transformers and the Reader monad. Meanwhile, to understand the Reader we have to know about functors and applicatives. To understand these we have to understand higher kinded types and constructor classes. This leads us to type classes, type constructors, algebraic datatypes, and so forth. It's true that most of these things are not about HTTP requests specifically. But what are they about? I would say that they're a mix of: 1. General purpose programming tools. Type classes, type constructors and ADTs fall into this category. They're not specific to HTTP request processing... because they're applicable to *everything*. 2. Interoperability with the rest of the ecosystem. The functor, applicative, monad and monad transformer classes fall into this category. For example, the fact that `ScottyT` is a monad transformer allows you to stack it with an open-ended set of additional transformers. I think a large part of the difficulty and learning curve of Haskell isn't so much in the type system (which falls under #1), but rather in the fact that the Haskell community has very remarkably managed to unify a lot of the ecosystem around some fairly complex and abstract tools like the `Monad` class hierarchy and monad transformer stacks. If you're doing Java or something like that, most people just learn the language basics and some basic libraries (mostly by copying examples from other people), and then write variants of the same three `for` loops over and over countless times. It's really just a minority of the Java folks who move on to design patterns and "composition over inheritance" and DI and AOP and `AbstractDelegationFactoryAdapter` and such—and these people are routinely derided (rightly or wrongly) as "architecture astronauts." Whereas in Haskell, we hit people with analogues of that "astronaut" stuff *much* sooner, because so much of the ecosystem is built around highly-distilled code reuse abstractions. Want to write a web application? Well, you probably will need to learn monad transformers. This is analogous to telling Java newbies that they must learn [AspectJ](https://www.eclipse.org/aspectj/doc/released/progguide/) or [Spring AOP](http://docs.spring.io/spring/docs/current/spring-framework-reference/html/aop.html) before they can write a simple web service. But this complexity isn't fundamentally about the choice of type system. You could have Haskell's type system without the `Monad` class and friends. You could have `do` notation be a hard-coded shortcut for the `IO` type. You could force the programmer to manually manage the interoperation between different libraries every time, forever after, instead of abstracting it around monad transformers or some similar concept. Then Haskell's web frameworks would look a lot more like other languages'.
Writing something where you encode more or less invariants isn't bypassing the type system, its choosing how to leverage it. Similarly turning a car left isn't "bypassing moving forward" and doesn't indicate "value in cars that don't move."
&gt; Which largely defeats the purpose of using a strongly typed language. At this point how is it different from using a dynamic language with gradual typing as seen with Clojure and core.typed. Type systems have a cost: they forbid you from writing certain programs. They also have a benefit: they provide some amount of safety or confidence in the code that they accept. *Bad* type systems can't express complex ideas, and reject a lot of valid programs. This means that, in order to be practical, they have to have lots of escape hatches, and these escape hatches are used frequently. As a result, there's little confidence in the type system. You can't leverage the type system to make your code more safe, because it's usually easier and more practical to just bypass it anyway. Most type systems are bad. I'd rather program in Ruby or Clojure than Java or C++. Elm is probably the weakest/simplest type system I'd want to use before switching to a dynamic language. The better the type system, the more ideas you can express, and the less often you need to rely on the escape hatches. I'd guess that the vast majority of Haskell programs use no unsafe features, since the type system is so good at capturing totally safe uses of these things. Additionally, since it's more expressive, there's more stuff the compiler can catch for you. `Maybe` is a great example -- compiler verified handling of null cases! You can also get warnings on missing pattern match errors, so if you missed a case in your code somewhere, the compiler can tell you and you can easily fix it. With Ruby, a ton of my time was spent writing tests and debugging. With Haskell, a ton of my time is spent implementing. Maybe Clojure has a sweeter spot in terms of dynamic types and safety.
No. You're just encoding fewer possible invariants than otherwise. The type system still ensures the core thing -- that evaluation doesn't get "stuck" on a runtime error.
&gt;Type systems have a cost: they forbid you from writing certain programs. They also have a benefit: they provide some amount of safety or confidence in the code that they accept. Not only that, but they can make it hard to express many problems as well. You have the complexity of encoding the problem in a way that the type checker can verify it in addition to the inherent complexity of the problem itself. &gt;The better the type system, the more ideas you can express, and the less often you need to rely on the escape hatches. I provided an example of a specific case where Haskell type system forces the user to write code that's more convoluted than it would otherwise need to be. &gt; You can also get warnings on missing pattern match errors, so if you missed a case in your code somewhere, the compiler can tell you and you can easily fix it. Sure, and if you find this workflow helpful then you should use it. My point wasn't that Haskell is a bad language or that people shouldn't use static typing. The opening statement makes it very clear that I don't have any problem with languages like Haskell, nor am I telling people that my approach is the one true way to do things. My issue is with the unfounded claims regarding general code quality. I fully accept that somebody might be more productive in a statically typed language, however when that person starts telling me that my workflow is wrong because I don't use one I take issue with that. If such claims are to be made then I'd like to see some empirical evidence to that effect as opposed to anecdotal evidence and personal preference. &gt;With Ruby, a ton of my time was spent writing tests and debugging. With Haskell, a ton of my time is spent implementing. Maybe Clojure has a sweeter spot in terms of dynamic types and safety. I haven't used Ruby, but I imagine it would be a lot more problematic than Clojure just based on the fact that it's object oriented. The language encourages creating a lot of types by design, and naturally you'll have trouble keeping track of them. The quality of dynamic languages varies just as much as that of static ones, and different languages lend themselves to this style better than others. All I can say is that I've been developing Clojure professionally for the past 5 years and I'm simply not having the problems that I keep being told I should be having using it. So, it gets tiresome being constantly patronized by people who made different choices than myself. 
I'm saying that if you do choose to leverage the type system then you have to work within it. Saying that you can use escape hatches doesn't really counter that is really agreeing with the position that dynamic typing provides value.
Yeah, I noticed that none of the GPipe tutorials or sample code I could find used type annotations. I had to do a good amount of trial and error before all the types matched up correctly. Thanks for the tip about replicateM, I've definitely found myself using forM to perform an action a certain number of times and just discarding the input value. I'm not sure exactly what you mean by not wanting to explicitly recurse in the main loop, but I imagine it would be the same in the GPipe monad as in any other.
It doesn't mean tuple always, but here it works out fine to tuple. I was going to use the functor '99' to make a joke when I was looking for a Monad, but then the Cartesian one showed up and it seemed gratuitous.
The other methods include writing tests, that you still have to write in any serious application, using things like the REPL, and better tooling like [Tern](http://marijnhaverbeke.nl/blog/tern.html) and [Cursive](https://cursiveclojure.com/) that can do static analysis on your code. If the only other method that you know of is "think real hard" you really need to get out more.
I wasn't talking about escape hatches, and I don't think /u/ItsNotMineISwear was talking about escape hatches. It looks to me like you claimed that they missed your point, and then you put those words in their mouth - that's the only thing I'm disagreeing with here.
In the same way that you can pattern match in the definition of a function, such as: foo (x, y) = x + y instead of: foo xy = fst xy + snd xy you can also pattern match in a let expression. In pack, (first,rest) is matching the result of span, which returns a pair. In this case you could think of it as a more concise way of writing: pack (x:xs) = let firstRest = span (==x) xs in (x : fst firstRest) : pack (snd firstRest) or: pack (x:xs) = let firstRest = span (==x) xs first = fst firstRest rest = snd firstRest in (x : first) : pack rest
Since you didn't understand the stackoverflow answer, let me try a more [Socratic](http://chrisdone.com/posts/teaching) approach. What is the purpose of the `[]` argument you give to `translate`?
thanks, I said I understood span when in reality I didn't know it returned a tuple
Static types are type-checking that you can opt out of. Dynamic types are no type-checking *whatsoever*, and you *cannot* opt in. Gradual types are a lack of type-checking-by-default where you can opt in. All good static type systems can be viewed as "gradual type systems" in the sense that you can opt out/in type-checking any particular part of the program. The right default is opt-out, not opt-in, because with type inference, the cost of type checking is nil for the vast majority of code. It sounds like you're advocating gradual typing, not dynamic typing (if you agree there is value to type-checking certain things) and don't agree on the default (opt-in or opt-out). In any case, your claims are *against* dynamic typing as you cannot even opt-in. 
I was describing getting stuck on a runtime _type_ error, not in general. But you can of course ensure the system doesn't get stuck on a runtime error of any sort. The following is unrelated to types in Haskell but worth expanding on: The halting problem simply says that an arbitrary program in a turing complete language can't be determined to halt, in general. You can both determine that many particular programs halt, and also determine that all programs halt in a language that is less that turing complete (i.e. permitting induction but not general recursion, for example). Finally, in Haskell '98, the design is such that one need never give type signatures, as all types can be inferred for you. So that sounds a lot to me like the computer doing the work as opposed to the programmer.
You're unfamiliar with the terminology used here, and with some PLT: * "Stuck" here means that there is no reduction rule (Lambda-calculus-wise) that can continue the evaluation. Like trying to apply a boolean (not encoded as a function) to another boolean. * There are useful, interesting non-Turing-Complete (sub-)languages for which the halting problem can be solved. e.g: Idris with totality checking. Those who used these languages tend to say that Turing Completeness is overrated.
More complex than they need to be for what purpose? If your purpose is very high assurance of correctness, then probably not.
The error you posted talk about a failing pattern match of `Just v` on line 50 in the file imp.hs. Try to see what happens when you run that particular expression with the environment `[]`
Great stuff. Typo in part 3 (probably): &gt; The performance gain is nice, but it comes at a cost in code readability: a quarter of the code is now made of stuff really belongs in a library. I think you forgot a "that" in "made of stuff really belongs in a library." ?
The 2 programs aren't identical. Are you sure the cloruje program is correct BTW? It seems to me that the 'go' function should look like this: largestFactor :: Integer -&gt; Integer largestFactor num0 = go 3 0 num0 where go !i !lf !num | lf &gt;= num = lf | i &gt;= num0 = num0 | num `rem` i == 0 = go i (num `quot` i) (num `quot` i) | otherwise = go (i+1) lf num 
It probably can't be done as it is more a question of how the `Maybe` monad behaves ...
I will read it ASAP, thanks for this pointer.
This would have been a great entry in the "Blow my mind with Haskell code" thread.
I don't think that is it at all. I think experienced Haskellers are usually "good enough" at following the types so they don't have the urge to write a tutorial. Is this unique to Haskell? Possibly. There are few other languages where you can follow the types and get reasonably far.
&gt; I'm still a little unclear on your media type suggestions. Well, I'm coming more from the position of writing services for consumption within my own company. From that perspective, I believe creating media types specific to your business applications is the correct approach. I don't mean *ad hoc* media types though, rather well defined media types where effort is spent in trying to define and document them well, keep them backwards compatible, etc. Having said all that, I don't see why we can't define media types for specific internet/SaaS services. I mean, if your service just requires an "appliction-xml" but has specific fields then you *have* defined a media type, you just haven't given it a name. By keeping it "just XML" with the intention of not violating the spirit of RFC 6838 one actually violates it anyway as well as making it harder to lookup the specifications of the ad hoc media type (i.e. I have to look up documentation for a URI then instead of a media type). I'm against code-on-demand and this is why I say the media types are the part that needs good documentation. When you write a client for a web service, you can read the documentation for all the media types you can encounter from that web service and write code to handle these. It's not clear to me what's meant by code-on-demand here. The browser isn't doing code-on-demand: when you get a new browser exe it already has compiled within it how to deal with all the types it's aware of. The browser software is such that it generally accepts plug-ins that give it understanding of new media types but this isn't "on-demand" either, the plug-ins are also something that was compiled prior. 
Exactly right about rel and that sounds like the problem the GP was having. However I disagree with both your conclusions: non HATEOS APIs may be easier to *call* but they tightly couple the client and server. This makes maintanance a *lot* more difficult and calls into question why you're using REST at all. If you're ok with tightly coupling client and server then there are other technologies that might be more appropriate. 
In a dynamic language I would certainly prefer no library over half a dozen crappy ones. Even in weak static languages like C or C++ I would prefer no library over bad quality ones. Not sure about strong static languages like Haskell where you could have at the very least some guarantees of the library's behaviour but most likely the same but be true even there. A bad library can easily cost you as much or more effort in debugging and workarounds than implementing a solution yourself.
Oh, I don't disagree with you. Afaict apis are more often written to than written. I also really like a HAEOAS approach. It is non-trivial to write and consume though. But if you do, you get many benifits. No doubts about that.
The lack of features actually makes it easier. The JVM has erased generics so Scala HKTs are just erased as well (with the information stored in an annotation). The CLR has reified generics so you'd have to reify your HKTs somehow and that's harder
If you look at the types for this (&gt;&gt;=), perhaps the key is the (a -&gt; Maybe b) function. Let's pick an example: f :: Int -&gt; Maybe Int f x | even x = Just $ x `div` 2 | otherwise = Nothing f either divides by two, or if the input is odd, returns nothing. So, for example: *Main&gt; f 2 Just 1 *Main&gt; f 3 Nothing Suppose now you wanted to do this twice. Extending the example above, we would like to do something like this: 4 -&gt; 2 -&gt; 1 2 -&gt; 1 -&gt; Nothing 1 -&gt; Nothing If we try and just do f twice it fails because the types don't match: *Main&gt; f $ f 2 &lt;interactive&gt;:4:5: Couldn't match expected type ‘Int’ with actual type ‘Maybe Int’ In the second argument of ‘($)’, namely ‘f 2’ In the expression: f $ f 2 The types don't match because f wants a simple number not one wrapped inside Just. However, (&gt;&gt;=) will do what we want: *Main&gt; (f 4) &gt;&gt;= f Just 1 *Main&gt; (f 2) &gt;&gt;= f Nothing *Main&gt; (f 1) &gt;&gt;= f Nothing In words, you might say that (&gt;&gt;=) lets you operate repeatedly on a value in a special way determined by the monad. In the case of the Maybe monad, that special way means that you stop as soon as you get Nothing. I should add that there is no reason why we have to repeat f. We could use &gt;&gt;= to apply f just once to Maybe Int values we construct ourselves: *Main&gt; (Just 2) &gt;&gt;= f Just 1 *Main&gt; Nothing &gt;&gt;= f Nothing 
IIRC the show-stopper issue with rank-2 inference is that it doesn't infer principal types.
Clojure programmer here. I don't recognise this, but instead often see great interest in what the Haskellers are doing, and most people wanting to copy the good ideas. Things like typed clojure is used by many who envy the types in haskell. 
Well he did ask for "eclipse quality" and Leksah might come pretty close.
I'm pretty sure that you can just not use generics and have the same expressiveness as the JVM.
Very nice. Looking forward to part 4 as well! Kind of weird that trivial changes to the code yield a 10-20x performance increase. I wonder if more could be done so that the good performance is the default, not the bad one.
&gt;It is possible to demonstrate that certain programs, functions or classes of program are guaranteed to terminate. Sure, the point is that you can't guarantee that the program will terminate in the general case with a Turing complete language. Last I checked this was certainly the case for Haskell. &gt;It is also possible to construct languages in which all programs halt, but this can be unnecessarily restrictive and is rarely used outside of theorem provers. Sure, you have things like Coq, and they have their uses and as you point out they're too restrictive for general use. &gt;Type systems are also just a form of static analysis. It would be no different if your static analysis triggered a build error in Clojure, that is all the type system is doing. The difference is that I don't write the code for the benefit of the type checker. The static analyzer works around me. It's a much more human centric approach. 
It's considered unaesthetic and therefore a smell. Type hints are purely for optimization, when passing native Java objects/types, to spare the runtime a reflection call. They aren't meant to be used as enforcing a contract or even implying one, except when needed for performance. 
I think it might be necessary to review some basics. Is Haskell your first programming language?
we would never say that forall a. Ord a =&gt; (a,a) -&gt; Bool is a subtype of forall a. Eq a =&gt; (a,a) -&gt; Bool class constraints are a contra-variant position, and `Ord a` is a "subtype" so to speak of `Eq a` so `Eq a =&gt; T` is a "subtype" of `Ord a =&gt; T`. tho subtyping is really the wrong way to think about type classes
Absolutely! It feels almost like a joke to say that "lens has a simple API" given the ocean of uninterpretable gobbledygook that pours out of every type error or :type invocation the moment you begin using it. Design aside, it *is* a simple idea. It's fairly tragic how impossible that fact is to observe though.
Don't worry, I'm not trying to blame you for your lack of knowledge, I'm trying to figure out what you already know in order to write an explanation you will understand. Most people coming here for help have experience with non-functional programming languages, and for those people it is often helpful to contrast how functional programming differs from non-functional programming. So if you already know how to use a programming language other than Haskell, please tell us which one it is, and we will give you explanations in terms of that language. If you are not just new to functional programming, but also new to programming in general, please tell us as well, because in this case we'll have to use a different approach. An error message of the style "variable 'x' not in scope" is common to all programming languages, not just functional programming. Have you seen similar errors while using other programming languages? Or, if you are new to programming in general, which learning material have you used so far?
I like to call out most languages for living in an "ambient monad" both to say that you can't escape from the concept and to note that it's pretty nice. A lot of this stuff has been subsumed into "learning programming" in most languages. That means that most people take it for granted in their "complexity taxing" and also that it's *super foreign* despite being so foundational.
This is not the kind of proof the author is talking about. I think, anyway. I think the author means going through motions to convince the compiler that you know what you are doing. Imagine, for example, this Python code: d = {'foo': 'bar'} value = d.get('foo') return value + '!!' This requires a fromMaybe (or similar) to "prove" to the compiler that there is nothing really wrong with this function. Another case I come across from time to time is something like map show ["alex", 23, True] which is a perfectly fine constant, but irrepresentable with a static type system unless you prove to the compiler that you know what you are doing (either by swinging HLists around or by manually inserting the `show` call into each list element.) I understand there are good reasons for all of these things, and I'm thankful they are there. I'm just saying there's no free lunch.
Courtesy of the [other thread](https://www.reddit.com/r/haskell/comments/3ulr7e/missing_the_point_of_applicatives/cxg0cle), there's another concept from category theory that matches Applicative more closely: http://ncatlab.org/nlab/show/closed+functor
...So any endofunctor on a CC category like Hask is going to be an applicative? What witchery is this?
I don't think so, but no idea, really. As if the mathematics wasn't hard enough to follow, I got lost when the article started talking about "noses" and "evil".
Of course not! That would be ridiculous. Semigroup should suffice until they get slightly older.
`["Kim","53","False"]` -- shorter too !
That is what a person who likes static typing would say, and my instinctive reaction too. "If the compiler doesn't like my API, it's because my API is difficult to understand." But thinking harder, and recalling code bases I've worked with in dynamic languages, I can see that some of those APIs were super clean and neat, and still wouldn't pass through a type checker even if it was blind and deaf. They were simply not designed with automated checking in mind – their design rested on assuming that people can use their intuition to check things that are hard for computers to know.
haha whoops. Not even close. Thank you!
Any example of this you could share, please?
 &gt; {-# LANGUAGE Rank2Types #-} Okay, so &gt; newtype Nat = Nat (forall r. r -&gt; (r-&gt;r) -&gt; r) Where the first `r` is zero and `r-&gt;r` is successor. This is how you define add: &gt; add (Nat cont1) (Nat cont2) = Nat $ \zero succ -&gt; cont1 (cont2 zero succ) succ Basically, you make a new natural based on `n1` that starts at `n2` instead of zero. This operation is `O(1)` it would appear. We can also multiply like this. &gt; mult (Nat cont1) (Nat cont2) = Nat $ \zero succ -&gt; cont1 zero (flip cont2 succ) Basically here, we `n1` starting at zero, but then its successor operation corresponds to adding `n2` basically. I am not sure what the computational complexity of this is, but it might be better than machine-integers. Now here is `pred`. &gt; pred (Nat cont) = Nat $ \zero succ -&gt; case (cont Nothing $ \n -&gt; case n of; Nothing -&gt; Just zero; Just n -&gt; Just $ succ n) of; Nothing -&gt; error "No predecessor"; Just n -&gt; n This is `O(n)`. So although Church Naturals are occasionally better than machine-integers, machine-integers operations are typically `O(log (n))`, which is better in the long run (it's better to have all logs than some constants and some linears). Combine this with the fact that conversion to and from machine-integers is `O(n)`, and you can see why machine integers are used more often (here are the conversions). &gt; fromNat (Nat cont) = cont 0 (+1) &gt; toNat x = Nat $ \zero succ -&gt; let igo 0 = zero &gt; igo n = succ $ igo $ n-1 &gt; in igo x
Doesn't matter, in practical terms they still serve as type documentation is if actually needed, what are the advantage of using docstrings over those?
I encoded some of the subtyping relations in scala a few years back: http://scalaz.github.io/scalaz/scalaz-2.9.1-6.0.2/doc.sxr/scalaz/Recursion.scala.html Not everything can typecheck though without higher order unchecked variance annotations that they don't support.
It can be added to a dynamic language after the fact though. So, you can perfectly happily use dynamic typing and gradually type things as you need to.
It means: Nothing bound to whatever result in Nothing some x bound to a function `f` result in `f x` 
I'd really appreciate some advice on accessing documentation of Haskell packages. Say, I'm trying to learn haskell-servant and get curious about the ServerT type, what's the quickest way to find its definition? Hoogle is great when you're looking for something that's in it, so is anyone hosting a Hoogle instance that indexes all the packages on Hackage?
How do I work with, or learn to work with Haskell and command line arguments for my program? So that I can invoke my compiled program like: `myprogram -hello` and it prints `"Hello World"`for example. Could someone provide a small example?
I build local documentation for dependencies, then use [ghc-mod](http://www.mew.org/~kazu/proj/ghc-mod/en/emacs.html) to look up documentation for an identifier.
Thank you!
When debugging code that's not compiling with ghci, is there any way to *just* load the language extensions and libraries of the offending module? Right now I'm having to type in stuff like this a lot: :set -XOverloadedStrings import qualified Data.ByteString as B import qualified Data.ByteString.Lazy as BL import qualified Data.Text as T import MyLibrary.MyDependencyModule .... I really, really, really hope I'm doing something wrong.
Are you asking about XML *parsers* or XML *libraries* in general? An XML parser really has nothing to do with XPath, except insofar that it may produce the same XML data structure an XPath processor expects. That being said, I've used two XML libraries so far, both of which include an XML parser: HXT and xml-conduit. * HXT * is not streaming, * is arrow-based, * has its own ecosystem of `hxt-*` packages * which supports XML DTDs, RELAX NG, and (partially) W3C Schema * xml-conduit * is streaming, * understands only well-formed XML, * but it's based on `xml-types` so other libraries can provide some downstream functionality. I can't offer any performance measurements, because it wasn't critical for my use. 
Let's confuse you some more: http://begriffs.com/posts/2015-06-28-haskell-to-hardware.html :) I know that circuits aren't really assembly language, but I was reminded of this, and I thought it was really cool, so I thought I'd share.
It is not problematic because the individual invariant is problematic, it is problematic because every single function who does not care about the invariant needs to expend energy on checking what to do with it, does it maintain it, does it change it in a predictable way, an unpredictable way,... The cost is simply too high if you try to go to that extreme end of the spectrum, just as the cost of not having any types and suffering trivial bugs at runtime is too high. You need to strike a balance.
Symbols have lots of "type level prim-ops" defined directly on them. Since Haskell isn't a dependently typed language (for the moment) this makes certain things more convenient and efficient than the other choice. For example, you can test for type-equality of symbols, and you can't do that out of the box for a type level list of chars, certainly not efficiently... There may also be historical reasons as we got `Symbol` before we got DataKinds, etc. So it may be worth revisiting in the future...
This is a conclusion I'd also come to independently. VHDL is essentially a first-order lazy functional language. Assembly language is just an awkward imperative abstraction over functional circuits :) I explored in Idris a bit yesterday. Now I'm thinking that the best abstraction for an assembly DSL would involve linear types. I'm still on the hunt for a decent lazy functional hardware/software interface, by the way.
The other day I wanted to use two different libraries that use two different monad transformer stacks, and with two different functions for running computations in those stacks (for example, ones that call `runReaderT` or `evalStateT` with some other stuff mixed in), but I couldn't figure out how to mix those two functions. For dealing with disparate stacks, I found the `mmorph` package which seems to do the job, but that was only half of the problem. I was surprised it was so hard to find anything on this; there's plenty written about monad transformer stacks, but almost nothing about combining different stacks, which seems to be a reasonable question to ask once you're familiar with monad transformers. Is it because monad transformers are mostly used in the application level and not the library level?
"strike a balance" often means "things are perfect as they are". while I'm not experienced with dependent types, I wouldn't be surprised if the "balance stricken" in a decade was being way more richly typed than modern Haskell. 
One thing you might want to do is not read the whole struct from memory, but use `c2hs` to build ad-hoc accessors to certain fields inside of it. This becomes more important as the complexity of the struct you're binding to grows.
You'll need to expand on the use case -- and in particular what the "other half" of the problem is?
Hit a bug the other day using `head`, realized I should have encoded my desire for "a list with at least one item at all times" into the type system. Is this idiomatic? Or do people have a library they use for it? data Frames cont = Frames (Frame cont) [Frame cont] data Frame cont = Frame -- (details irrelevant) currentFrame :: Frames cont -&gt; Frame cont currentFrame (Frames f []) = f currentFrame (Frames _ (f:_)) = f 
Many questions ;) ## What are the options to get efficiently packed data structure ? For example, suppose I want to represent the following type: data Something = Something ... data IntA -- An integer on 20 bits data IntB -- An integer on 11 bits data SomeKindOfTree = Node Something | Leaf IntA IntB Knowing that `Something` fits on 31 bits, and `IntA` + `IntB` fits on 31 bits, plus a bit to get the right constructor for `SomeKindOfTree`, I'll be able to store everything inside 32 bits. However, As far as I know in ghc, I'll get many pointers and indirections to get to my data, so lot of memory "wasted" and cache trashed. I can pack everything inside a `Vector` of `Unboxed Int` and work with View pattern to create the feeling that my data structure looks like `SomeKindOfTree`, but what are the others options ? ## In python, I was used to solve all my parsing issues using regular expressions. However it appears that the Haskell community prefers a parsec approach. I found parsec really powerful for complex problems where I want good errors reporting, but but for simple problems such as replacing `"(.*?)"` by `'\1'`, I find parsec unfriendly. Also, I always have issue with greedy behaviors, resumed in my question in r/haskellquestions https://www.reddit.com/r/haskellquestions/comments/3rzu6x/parsec_avoid_post_processing_and_let_sub_parsers/ ## Last question in more a discussion than a question. I'm a fan of meta programming and introspection to reduce boilerplate. For example, in python, we can get the name of a function (using the `__name__` attribute), so I can easily write code to get a list of all functions which name starts with "function". For example, how can I do something like that in haskell: -- Haskell pseudo code ;) let myFonctionList = allFunctionInScope :: [(Int -&gt; Float)] -- Returns all function in scope of type (Int -&gt; Float) myMapping = (fromList (map (\f -&gt; (getFunctionName f, f)))) :: Map String (Int -&gt; Float) Which returns a mapping from function name to actual function for all the `(Int -&gt; Float)` functions. I initially thought that template haskell could solve this kind of problem, but as far as I know, template haskell knows nothing about the symbols in scope. I also initially thought that template haskell can help me implement a memoize function which works with recursive functions by replacing the recursive call by a recursive call to the memoized function, but that's not possible because, AFAIK, template haskell cannot deconstruct an already existing function. How do you solve those kind of issues ? (Edit: thank you for the instructive solutions)
I don't use Haskell because of "empirical" evidence. As you use Clojure exclusively because of "personal and anecdotal" evidence. 
Usually I am browsing the code on Hackage. I go to the "Index" link in the upper right corner (unfortunately there isn't one if you are on the [package home page](https://hackage.haskell.org/package/servant); instead you have to go to [any other page in the package docs](https://hackage.haskell.org/package/servant-0.4.4.5/docs/Servant-API.html) and then there's the "Index" link. Also, recent Stackage builds are using a new Haddock which hyperlinks the source code itself; [example](http://haddock.stackage.org/nightly-2015-11-30/Cartesian-0.2.0.0/src/Cartesian.Plane.html). I thought the Haddock on Hackage was doing this too but now I can't find a quick example. I think this Haddock has not been finally released yet?
in Haskell, I can bypass the type system with partial functions like fromJust or dynamic functions like unsafeCoerce and unsafePerformIO. and I do, but like only 1% of the time. and there is often safer-but-still-easy dynamic typing support. like Data.Typeable , using String (over Enums), Template Haskell (for build time validation), etc.
While `optparse-applicative` is much more Haskell like, I've been partial to [docopt](https://github.com/docopt/docopt.hs) ever since I met the similar library for Python. The idea is that instead of writing code that creates the options and then generates documentation from it, you write the documentation first and the library parses the arguments accordingly. Read more [here](http://docopt.org/).
If you want to make sure you have at least one element, make the function accept an (initial) element and a list. Compare this to `foldr` vs `foldr1`. We can assume for our case that the type `b` is the same as `a` for the comparison. foldr :: Foldable t =&gt; (a -&gt; b -&gt; b) -&gt; b -&gt; t a -&gt; b foldr1 :: Foldable t =&gt; (a -&gt; a -&gt; a) -&gt; t a -&gt; a As you can see, `foldr1` receives just a `Foldable` (and a function), while `foldr` receives both a `Foldable` and an "initial element" (and a function).
&gt; a list with at least one item You want [NonEmpty](https://hackage.haskell.org/package/semigroups-0.18.0.1/docs/Data-List-NonEmpty.html) from [semigroups](https://hackage.haskell.org/package/semigroups).
Of course, that's the whole point of my article. I'm saying that different approaches appeal to different people. I'm not the one arguing the "death of statically typed languages".
That was exactly my point. The balance is probably just slightly beyond what Haskell can do today for most applications unless we can find ways to make working with dependent types a lot more convenient (reduce the cost side of the balance).
The `Tree` type you have is not an iso of arbitrarily nested lists. And while it does not seem like OP has progressed that far type classes would be the way to go for arbitrary depth list concatenation.
Who said you have to use the same function for each level? *wink, typeclasses, wink*. 
Who said anything about needing arbitrary nested lists. For any 'n' we want a function that operates on a lists of lists of depth 'n'. 
Where's your sense of humor?
I've used it to pass vectors and arrays using `Quantity` from [uom-plugin](https://hackage.haskell.org/package/uom-plugin) to numeric libraries, though I've sometimes had to resort to `unsafeCoerce`. It does break the abstraction in that particular place, but it's not like I want to spend O(n) time for stripping the newtype that shouldn't even exist at runtime.
&gt; Can you give an example? Yes, examples please /u/yogthos. Without them, this is all very abstract.
Wow, I was not aware that library existed! It looks like Hspec-styled command line parsing.
Do you have a moment to talk about our lord and savior Alonzo Church?
Ok sure, /u/Peaker here's a concrete example for type declarations in a [markdown parser](https://github.com/snoyberg/markdown/blob/master/Text/Markdown/Types.hs). Any non-primitive types have to have to have static definitions. I find it shocking that this comes up in every discussion by the way. Now, that is an absolutely trivial example, but when it comes to real world apps this can quickly escalate. Consider something an equivalent of Ring and its middleware in Clojure. The request and response are represented by maps, and each middleware function can modify this map to add/remove any keys it likes. The key can point to a complex data structure in turn, and so on. In Haskell, there's no way to do that without describing the totality of the fields globally.
Sure, but the difference is that you can start untyped and only type things you want to type. As I said it's the difference of defaults. Typing becomes a tool that you can choose to use or not use.
&gt; Any non-primitive types have to have to have static definitions. Can you explain in more detail? It's completely unclear to me what you mean by "non-primitive type" and "static definition". Are you talking about having to define the type `MarkdownSettings`?
&gt; but irrepresentable with a static type system Unless the static type system has union types. I know this is /r/haskell, but H-M style systems aren't the only ones out there.
For regex, I like to use [pcre-heavy](http://hackage.haskell.org/package/pcre-heavy). It's uses template haskell and add a little bit of extra compile-time safety to using regex.
I wonder if we have, or if it is even possible to have, something like Python's [scriptine](https://pypi.python.org/pypi/scriptine) in Haskell.
How is the case partial?
Did I just not give a concrete example? Let me write some code out for you if it'll help. Let's say I have a request that looks like this, how would you describe it in Haskell without having to create a type. {:query-string nil, :server-name "localhost", :ssl-client-cert nil, :content-type nil, :path-info "/", :request-method :get, :character-encoding nil, :server-port 3000, :content-length -1, :body #object[io.undertow.io.UndertowInputStream 0x67a7b7cd "io.undertow.io.UndertowInputStream@67a7b7cd"], :uri "/", :handler-type :undertow, :remote-addr "0:0:0:0:0:0:0:1", :scheme :http, :headers {"host" "localhost:3000", "user-agent" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36", "cookie" "treeForm_tree-hi=treeForm%3Atree%3Aapplications; ring-session=cfab55f3-5cb9-4a99-94bd-c6d169d5799e; JSESSIONID=9C-lPAJ7FX8IYAxfQuSHB6rh", "connection" "keep-alive", "upgrade-insecure-requests" "1", "accept" "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8", "accept-language" "en-US,en;q=0.8,ru;q=0.6", "accept-encoding" "gzip, deflate, sdch", "dnt" "1", "cache-control" "max-age=0"}, :context ""} Now, let's imagine I add a middleware function that adds XSS protection, that will add a key with the additional info to it with that info. Let's say I add middleware for handling user sessions. The session itself is a map with arbitrary data in it.
Why is `&gt;&gt;=` defined using `forall a b.`? [Link](https://hackage.haskell.org/package/base-4.8.1.0/docs/Prelude.html#v:-62--62--61-). I can't see any use of e.g. ScopedTypeVariables, so this just seems like it'll be one more thing to explain to curious newcomers.
&gt; Did I just not give a concrete example? You linked to a source file. I'm not sure what you are using it to demonstrate. Of course you would have to create a type in Haskell to store the data you are referring to. I don't see why this is such a burden.
I think that the parent might have meant core.typed.
You can also use a `.ghci` file.
That module uses the `ExplicitForAll` extension. Doesn’t look like there’s any particular reason for it.
I'm not sure if there is any support for X-Path, but `xml-lens` (uses documents parsed by `xml-conduit`) seems to me much better.
Sure, I think we disagree on the defaults. The difference is in the workflow that each approach encourages. I'm not sure what's puzzling here to be honest. Are you puzzled by the difference defaulting to mutability and immutability makes to the workflow? I imagine you'd agree that languages that default to mutation and provide immutable data structures on the side encourage a very different workflow from those that default to immutable data and allow controlled mutation. The typing situation is exactly the same.
&gt;Of course you would have to create a type in Haskell to store the data you are referring to. I don't see why this is such a burden. That's why you use Haskell and I don't. It's really that simple.
Link: https://github.com/mattgreen/hython/blob/rewrite/src/Hython/ControlFlow.hs I'm working on overhauling my Python 3 interpreter and separating it out into modules. One of the modules I extracted contains all of the control flow information for the interpreter. Some functions in the interpreter modify the control flow, so I tried modeling that as an effect. I am looking for some feedback on this module. Not as much on individual functions (though that's OK), but rather, is the code I'm writing surprising or strange? Is it worth using an effect here? The API is rather imperative by nature, these are functions that are called to update state relating to where to go when certain keywords are invoked. If it seems too generic, that's because I was trying to avoid cyclic dependencies between modules and avoiding `Types.hs`-as much as I can. You can see usage of it here: https://github.com/mattgreen/hython/blob/e1f45c08f83a8452bda8f743102fbfd1070cc70b/src/Hython/Statement.hs#L147
Note that Haskell records are known to suck. That's the reason you do not enjoy type inference with Haskell records. With many statically typed languages that have records that do not suck (and with Haskell with [quasi-quoting trickery](http://nikita-volkov.github.io/record/)), you can do: request = { queryString=Nothing, , serverName="localhost" , sslClientCert=Nothing , content-type=Nothing ... } and request will get an auto-inferred record type. Given this, what is it that you think requires manual "proofs" here?
I am puzzled that you said "Typing becomes a tool that you can choose to use or not use." -- that is true everywhere (except dynamic types). You can always opt out. 
I'd recommend reaching for [text-icu](http://hackage.haskell.org/package/text-icu) for doing what `text` itself doesn't cover. Unfortunately I don't see reverse implemented there. It is however included in the underlying [icu4c](http://icu-project.org/apiref/icu4c/classicu_1_1UnicodeString.html#a6798a0c1f334f6681bfd7d2e72337603) so perhaps it can be added.
A new Cabal was just released so that sounds promising then :)
Ooh, thanks for that. Searching led me to [this SO post](http://stackoverflow.com/questions/12519878/what-are-these-explicit-foralls-doing): &gt; I'm not sure _exactly_ what's going on in this case, but based on the comments I would guess that the conversion to using explicit type arguments and the desugaring of do notation are, in some sense, not aware of each other, and therefore the order of type arguments is specified explicitly to ensure consistency. After all, if something is blindly applying two type arguments to an expression, it matters a great deal whether that expression's type is `forall a b. m a -&gt; m b -&gt; m b` or `forall b a. m a -&gt; m b -&gt; m b`!
Thanks for the feedback. :) I don't know much about the properties of tries, but thanks for the pointer, I will read up on the topic.
If I have a fully type-checked function, is there any way I can examine the types of internal sub-expressions in the function? Basically I was wondering if the type checking algorithm is like a black box, or if we can view the internals. Thanks!
Are there more tutorials on frames coming? I've seen a video and a blog post or two, but the documentation still seems limited. Is there a planned direction for future development or is it pretty stable as is? I'm particularly interested as to whether it can replace some of the modern data frame workflows with dplyr in the R world.
Couldn't you `coerce` there instead of `unsafeCoerce`?
But this is clearly code duplication, made all the more frustrating because there's _no_ way around it (at least maybe until `ImpredicativeTypes` lands) but it feels like there really should be.
 ghci&gt; import qualified Data.Text as T ghci&gt; import qualified Data.Text.IO as TIO ghci&gt; import Data.Text.ICU.Normalize [Normalize](http://hackage.haskell.org/package/text-icu/docs/Data-Text-ICU-Normalize.html#v:normalize) your text. ghci&gt; normalize NFC (T.pack "ma\241ana") "ma\241ana" ghci&gt; normalize NFC (T.pack "man\x303\&amp;ana") "ma\241ana" Now [`reverse`](https://hackage.haskell.org/package/text/docs/Data-Text.html#v:reverse) works as expected: ghci&gt; TIO.putStrLn (T.reverse (normalize NFC (T.pack "ma\241ana"))) anañam ghci&gt; TIO.putStrLn (T.reverse (normalize NFC (T.pack "man\x303\&amp;ana"))) anañam
Any links to `recursion-schemes` being used in the wild? (note: [http://packdeps.haskellers.com/reverse/recursion-schemes](http://packdeps.haskellers.com/reverse/recursion-schemes) was not much use).
42. What's the ultimate question of life, the universe, and everything?
Perhaps, but type hints and core.typed are two totally different things, so if someone says "type hints" I will assume they mean type hints and not core.typed.
When you started with Haskell, were you more interested in the category theory aspect or did you begin trying to do something more practical?
In other words, for arbitrary n... These kinds of things require fancier types than base Haskell supports.
The request is not a flat map, as I pointed out a session key could point to some arbitrary data and any middleware function can add an arbitrary key that nothing else knows or cares about. This adds a huge amount of flexibility as it's possible for a function to modify the request without having to track that globally. I can include a middleware dependency and inject the function and all of a sudden there's a new piece of functionality available. Only the particular piece of middleware needs to care about what it's doing. Conversely, this allows for middleware to easily modify keys. For example, you have middleware that converts string parameter keys into Clojure keywords. 
You must not assume that the englishmen you are running the compiled English on supports functional programming. It would probably be a pdf with a bunch of boxes (which they can write in), and step by step instructions.
Oops!
The proper name for that operator is "bind", I think. However, due to the way it works and the way it looks, I've also heard people call it the "shove" operator. It takes a value and "shoves" it into a function. (Overly simplified) There is a very similar operator (&gt;&gt;) which does the same, but instead of shoving the value in, it just drops it. That's how you model "one after the other" in Haskell. In the case of Maybe this means that you check the first value about whether it is Nothing. If it is, the whole expression is. Otherwise, it is ignored. (Disregarding bottom) A better intuition might come from IO in this case, where it actually means "first do this, then do that". (In general, IO is a poor example, though. It is highly magic.)
Maybe this is too far outside your interests/scope, but the current approach to data munging is pipelining by chaining of primitive operations, where %&gt;% is something like &gt;&gt;= in haskell, see - https://rpubs.com/justmarkham/dplyr-tutorial A relatively complete list of the functions is here https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf
[here's a couple versions you left out](http://www.willamette.edu/~fruehr/haskell/evolution.html)
I came to Haskell from Common Lisp. I tried learning Haskell twice. I didn't like Haskell the first time, but it stuck the second time. I really enjoy pure math and I wish I had more time to study it. It's only been in the last year or two that I tried to learn some category theory. I'm glad I did. A lot of the advanced Haskell techniques make more sense now. They make sense with out the CT, but it helps and I enjoy studying it. So for me it's a win-win.
&gt; If I have a fully type-checked function, is there any way I can examine the types of internal sub-expressions in the function? I guess it depends on what you mean by examine. You could always put an invalid type assertion on a subterm of the function (it's common to use Int or ()) and then the compiler will complain, showing you the correct type of the subterm. The type checking algorithm is not a black box and there is a really great paper that is written so you can follow along implementing your own version to learn how Haskell's type checking works. The paper is [Typing Haskell in Haskell](https://web.cecs.pdx.edu/~mpj/thih/thih.pdf). It's beginner friendly and I found it really instructive. Just keep in mind, the actual implementation in GHC is much more sophisticated than the one in that paper because GHC has a lot of extensions that require a more advanced algorithm and because GHC has other concerns to address (like being fast and maintainable). 
Wow
&gt;Machine checked properties are what you gain. Yes, we all know that. The question is whether this leads to a significant reduction in defects in real world projects or not as type errors tend to be caught very early on in development cycle. Static typing has been around for a long time, and yet we still don't have any conclusive evidence that it has a significant positive impact on development. &gt;What you mention has nothing to do with the Haskell type system. These are things you would need in order to understand the implementation of this specific library, which willingly uses these facilities in order to create a better API for handling HTTP requests. Nothing about the Haskell type system is forcing us down this path. I disagree, the library is fairly idiomatic as far as I can tell and it doesn't just arbitrarily use a bunch of features without rhyme or reason. &gt;Haskell has both in unsafeCoerce and unsafePerformIO. Use of which is strongly discouraged last I checked. The whole point is that you either leverage the type system or you end up telling the compiler to just trust you as you would with a dynamic language. 
[removed]
I was thinking of something like using a fold to apply a function to a state to produce a new state, as opposed to having explicit recursion.
Even if you are working in a dependently typed language and want to prove correctness, it's usually cleaner not to bake every invariant into your data types, but instead to take advantage of dependency by writing proofs that functions preserve invariants. E.g, instead of writing sort : (xs : [A]) -&gt; {ys : [A] | sorted ys /\ permutation xs ys} and changing the type and a bunch of clients when you realize you want stability too sort : (xs : [A]) -&gt; {ys : [A] | sorted ys /\ permutation xs ys /\ stable_permutation xs ys } you can write a more loosely typed implementation, and lemmas claiming that it behaves properly sort : [A] -&gt; [A] sort_sorts : forall xs, sorted (sort xs) sort_perm : forall xs, permutation xs (sort xs) and add more properties as you need them by providing more lemmas - sort_stable : forall xs, stable_permutation xs (sort xs) It's still "in the types", especially in the sense that any well-typed implementation of the entire interface must be correct, but it's not putting everything in the types of individual functions.
This isn't necessarily efficient, but you may want to as an exercise write: invert :: (Enum a, Bounded a) =&gt; (a -&gt; b) -&gt; (b -&gt; a) Since you can derive Enum and Bounded, then this is a sorta cute trick.
You can unpack the fields of a data structure: data Point = Point { x :: {-# UNPACK #-} !Double , y :: {-# UNPACK #-} !Double } That will remove all pointers and wasted space. If you want something like regular expressions that backtracks perfectly without greediness, use [Turtle.Pattern](http://hackage.haskell.org/package/turtle-1.2.3/docs/Turtle-Pattern.html). Using the example you just gave of replacing `"(.*?)"` with `"\1"` you would write: example = "(" *&gt; selfless chars &lt;* ")" The `selfless` isn't even strictly necessary. If there is only one possible match you can just write: example = "(" *&gt; chars &lt;* ")" The `selfless` is only there to disambiguate when there are multiple possible solutions by specifying to pick the one that matches the fewest characters.
They probably can describe it, but I would guess that they get in the way unless you're already fluent in category theory. Category theory tends to be a good tool when you have a thing and you want to abstract a way to a more general thing so that you can go instantiate your ideas into some new concrete setting. We see an example of this in Haskell with the Maybe and list both being monads. You might write something with Maybe in mind and then later go through and think about which operations are valid for other monads. Abstract away (as appropriate) and then later it's easier to switch to the list monad instead. I guess that's still sort of vague, so I encourage you to try a few examples. It's also somewhat misleading because the semantics of Maybe and List as monads are fairly different, even though each one is internally consistent with respect to the monad laws. Transpiling between any two languages requires you to understand the semantics of both languages well enough to figure out how they map onto each other. Once you know that, CT might be a convenient way to talk about those mappings.
Practical. I was sick of my scripts always breaking on unexpected inputs so I wrote them in Haskell
If your session is a map with arbitrary data, who uses it? What do they expect to find in it?
I believe what we do is generate the Foundation once per test run, store it in an mvar, and then reuse it across all of our tests. Assuming you're not mutating any state within the Foundation post webapp initialization, it should be pretty much static across all tests, so recreating it for each test case doesn't make much sense.
&gt; Are you asking about XML parsers or XML libraries in general? An XML parser really has nothing to do with XPath, except insofar that it may produce the same XML data structure an XPath processor expects. I'm not looking for all-in-one kind of libraries. But at very least we'd the XPath processor to be compatible with the model that the parser would produce. Also I could easily imagine a streaming parser which would be considering the XPath (or any other) instructions on the go and terminating the consumption, when it would decide that it got all it needed. Thanks for your candidates.
Are any instances on (-&gt;) ever actually useful?
With turtle, how can I turn `ls somedir` into a Vector? I can use `fold (ls somedir) Foldl.list`, but I've unable to figure out how to use the Foldl.vector fold.
&gt; There are files missing in the ‘stm-2.4.4@stm_C1kFMnPqFjvDhFjgMZGUpr’ package Sounds like your installation (specifically, `stm` package) got corrupted or stopped in the middle? Can you run `stack exec ghc-pkg list stm`? Did you already have a `.stack-work` directory? Try to remove that and rebuild.
Practical, still not very interested in category theory.
Sounds fun! just one caveat - [Elm is not FRP](http://begriffs.com/posts/2015-07-22-essence-of-frp.html). 
It’s not all let-bound variables. Constant applicative forms (CAFs) are defined to be any top-level definition of a “supercombinator” that is not a lambda abstraction, where a supercombinator is a constant or any expression that contains only supercombinators as subexpressions. So `fibs` is a CAF because the arguments to `iterate` are both constant (as is `iterate` itself). In particular, `fibs` doesn’t reference closure variables, which is why you don’t need to reconstruct it at every call to `fib`, and thus you can simply retain it as a global GC root. In GHC, it’s freed when the module is unloaded—either at program termination, or if you unload a dynamically linked module.
No real reason. I've seen it in computer science blogs before as an example of how difficult it is to handle Unicode correctly. I noticed it didn't work in `Data.Text`, which was surprising to me because I had thought that taking the intricacies of unicode into account was part of the purpose of `Data.Text`. I was curious if another Haskell library handled that case correctly.
Good comment, after digging around it seems the character you want simply *does not exist*. Here is a [search query](http://www.fileformat.info/info/unicode/char/search.htm?q=diaeresis&amp;preview=entity) for Unicode characters with diaeresis, it shows *ä*, *ë*, *ḧ*, *ï*, *ö*, *ẗ*, *ü*, *ẅ*, *ẍ*, *ÿ*, *Ä*, *Ë*, … but *no* `LATIN SMALL LETTER N WITH DIAERESIS`. Note the different results: ghci&gt; normalize NFC (T.pack "e\x0308") "\235" ghci&gt; T.reverse (normalize NFC (T.pack "e\x0308")) "\235" ghci&gt; normalize NFC (T.pack "n\x0308") "n\776" ghci&gt; T.reverse (normalize NFC (T.pack "n\x0308")) "\776n" This agrees with this list comprehension; it lists characters composed with [`COMBINING DIAERESIS`](http://www.fileformat.info/info/unicode/char/0308/index.htm) where reversing the NFC-normalised form does nothing. ghci&gt; [ ch | ch ← [minBound..maxBound], let str = normalize NFC (T.pack [ch,'\x0308']), str == T.reverse str ] "AEHIOUWXYaehiotuwxy\213\245\362\363\776\836\921\933\953\965\978\1030\1040\1045\1046\1047\1048\1054\1059\1063\1067\1069\1072\1077\1078\1079\1080\1086\1091\1095\1099\1101\1110\1240\1241\1256\1257\8126"
By one definition. It's functional, is is focused on reactivity, and it still manages to get stuff done. 
Yeah, it’s a totally reasonable curiosity to have. And it’s always so frustrating when you ask “How do I do X?” and the answer is “Don’t do X at all!” The problem of course is that human languages are phenomenally complex. And hey, speaking as a linguist, I think the Unicode Consortium does a great job under the circumstances. But if you’re a programmer who cares about internationalisation and localisation, good fucking luck getting what you need out of even the best libraries such as `Data.Text` and ICU. `Data.Text` seems meant to serve as something more like the C# or Java `String` class, because Haskell’s historical choice of `type String = [Char]`, albeit elegant, is woefully inadequate in the face of the vagaries of text, and when it comes to real-world performance.
Everything you said is correct.
Still not following. Are you saying that `find` has 3 possible return values?
What you are looking for is likely to break on "Extended Grapheme Clusters". I haven't tested it myself, but there are utilities in [Data.Text.ICU](https://hackage.haskell.org/package/text-icu-0.7.0.1/docs/Data-Text-ICU.html) that handle breaks on various boundaries, including this one (breakCharacter). Going on hunch, since I don't have the library, something like this should work: concat $ brkBreak &lt;$&gt; breaksRight (breakCharacter locale)
Short: No. Long: there are many variants of the lambda-calculus around, but when we don't give any additional precision we mean the "minimal" lambda-calculus, the one with functions only. You can also add base types and other type constructors (products, sums, etc.). Depending on the type system imposed on the lambda-calculus (if any), those constructions can sometimes be encoded in terms of higher-order functions, but those encodings do not always have the exact same properties as the primitive constructions. In the "simply-typed lambda-calculus" (the simplest type system), the minimal lambda-calculus is not quite enough. Finite products and sums are not encodable as functions in absence of polymorphism. Even if you add polymorphism,, negative/lazy products and the unit type are relatively easy to encode, but sums and the empty type have equational reasoning principles that are hard to get into the type system if you use their encodings instead. In my experience, even in the simply-typed system (no polymorphism), adding products to the type system is very easy, while adding sums (and the empty type) can get you into a lot of difficulties that you had not thought about before. For example, the usual easy proof of weak normalization (the fact that you can always reduce any well-typed to an irreducible value) for the simply-typed lambda calculus is not immediately extended to sums (technically: you need to reason on commuting conversions).
Practical. Had a friend who learned Haskell and I always tried to trip him with things that are easy in other languages but might be very hard in a pure functional language. He kept proving me wrong, with the final nail being "If you really feel writing this imperatively is better, here's the `State` monad".
The best libraries are the ones you don't actually notice :)
After a long time trying to install spacemacs I updated it yesterday and thought haskell-mode is really really cool. linting hints + type checking + popup warnings on hover all automatic + good integration with the REPL is really great. I'm going to move from vim to this for haskell development now. Thanks for the hard work and awesome job!
I didn't when I posted, but I've just pushed it to [github](https://github.com/hesselink/xml-bench).
Wait for hie and everything will be even more awesome https://i.imgur.com/n7M9ye2.png
Yes, I know all of this. Obviously I mean the `find` used by /u/ephrion, who wrote: case find (=="-hello") args of Just "-hello" -&gt; putStrLn "Hello world!" Nothing -&gt; putStrLn "asdf" ...which you called partial. I was asking if there's another case than `Just` what was searched for, or `Nothing`¹. As far as I know there are two possible outcomes of find with a given predicate, and both seem to me to be matched here. ¹ I'm presuming by `None` you mean `Nothing`
I often need to zip/merge two or more lists non-sequentially, according to some arbitrary (sometimes stateful) criteria. For example, imagine we have 2 lists of Int, xs and ys, and a function: mergeBalanced :: [Int] -&gt; [Int] -&gt; [Int] mergeBalanced xs ys = ... 'mergeBalanced' takes one element at a time, keeping track of the sum of the elements already taken from xs and ys. It always takes the next element from the list with the lowest sum of taken elements. Is there any nice abstraction for this and similar cases, or am I stuck with manual recursion and State?
I've used the `Applicative` instance on `(r -&gt;)` to lift operators like `(||)`. This gives you something (say, `(&lt;||&gt;)` that is an operation working on functions to `Bool`, so you can write something like `isFoo &lt;||&gt; isBar`, which looks pretty natural. Similarly `fmap` on functions can also be useful in some cases. The `Monad` instance is just the `Reader` monad but for some reason I've never used that.
From an implementation perspective, all you need is closures.
&gt; Our base datastructures have good roles annotations to prevent this. I checked yesterday, and it seems to me that the `ST` monad does not have proper annotations (that I could find). It should be `ST nominal representational`, I think.
If there is no bug in the code then yes, only these two cases can happen. However, the compiler will still warn that it is partial because it can't tell that. Regardless, there's no need to repeat the "-hello" part anyway.
Some operations don't make sense for symbols, like concatenating their names etc., `Symbol` expresses symbols better than just a string.
So i did some tests and the anwer is: no, it doesn't work. And the `MonadBaseControl` doesn't help much either - it is possible to simulate `bracket`, but simulating `bracketOnException` would probably need `MonadError` instance. Seems to me the monad transformers really do not compose easily.
&gt; This is because in Haskell, the Functor typeclass only requires implementation of fmap (see AMP for more information on these issues), which is how functors lift arrows into their target categories; it unfortunately is missing a function for how to lift objects. That sounds wrong, the objects in Hask are types so the mapping is given by the type constructor. Since the `Functor` typeclass always requires a polymorphic type, we get this mapping for free.
That's really great! I will take advantage of this thread to bring attention to people who may be interested or capable: [Add an interactive function to send the top-level definition the point is currently in to the REPL](https://github.com/haskell/haskell-mode/issues/608) This would make working with haskell-mode close to what is working with rstudio for R, for example.
I agree. Since Hask is the only category you are working with, every functor is an endofunctor and as such, the object lifting has kind `* -&gt; *`, which is the case of the data constructor.
It's kinda disappointing that to say "computation is logic" via Curry-Howard, you need to twist both computation (remove Turing completeness) and logic (switch from classical to intuitionistic). But there's another connection between computation and logic that doesn't use types and doesn't compromise on either of those points: the analogy between Gödel's first incompleteness theorem and the halting problem (see [Scott Aaronson's explanation](http://www.scottaaronson.com/blog/?p=710)). I'm not sure if these two connections are related to each other. Anyone?
Write out the intermediate structure. If I just give you `f :: A -&gt; B` it's hard to analyze that and get `inverse f :: B -&gt; A`. If I tell you that the inverse does exist and perhaps `A` is finite that `B` has equality it's possible to write `inverse` but it becomes quite inefficient. However, the *graph* of any function `f` is a data structure like this type Graph a b = [(a, b)] with the property that it must be *functional*, e.g. that each value in `a` shows up once and only once in `Graph a b`. If the `Graph` is furthermore bijective then we have `invertGraph = map swap` which converts a graph into its inverse. So now we just need a way to convert `Graph`s to their corresponding function implement :: _ a =&gt; Graph a b -&gt; (a -&gt; b) I leave out the constraint on `a` because we (a) need one and (b) can get more efficient resulting functions depending on what we know about `a`. For instance, if `a` is `Hashable` then we can transform `Graph` into a hashmap lookup for `implement`.
To make Curry-Howard work, you do need both of those compromises. Turing completeness leads to a self-contradictory logic, as you say. And using classical logic is not just a matter of adding call/cc, because a language with call/cc can't have unique normal forms (see [this answer on stackexchange](http://stackoverflow.com/questions/24711643/how-come-that-we-can-implement-call-cc-but-the-classical-logic-intuitionistic/24711644#24711644)).
First time I ever even used that stack mechanism. Brand new install. Below is output of running stack exec, etc &gt; stack exec ghc-pkg list stm dhjmacpro:/Volumes/HD2/src/GitHubRepositories/Flocking-Simulation $ stack exec ghc-pkg list stm /Library/Frameworks/GHC.framework/Versions/7.10.2-x86_64/usr/lib/ghc-7.10.2/package.conf.d stm-2.4.4 /Users/dhj/.stack/snapshots/x86_64-osx/lts-3.10/7.10.2/pkgdb /Volumes/HD2/src/GitHubRepositories/Flocking-Simulation/.stack-work/install/x86_64-osx/lts-3.10/7.10.2/pkgdb
I've been working with Clojure professionally for the past 5 years, and I have long running projects that are over 3 years old thank you very much. I've never had these problems.
Monad tutorials start with the cure for the headache (or stupid analogies) not presenting the headache first, which alienates a lot of people (a recent blog post put this in similar terms better than I can). Whilst I think I have a feel for monads, I'm sure some discussion with proper haskellers would help. In as few words as possible, what's the headache monads solve? 
Yeah, I ended up using an IORef. I love that Haskell's escape hatches are there when you need them
Take a look on GitHub tons of different apps written in dynamic languages. Some pretty sophisticated software is written in Common Lisp and Erlang and works extremely well. You're just using a false dichotomy by claiming that all dynamic languages suffer from the same problems. In practice, there's just as much variation in quality as there is in the static world.
Oh please, I've answered you and even gave you concrete examples at which point you conceded that Haskell records not being so great after all. The whole point of using a language like Haskell is that your solution is driven by types. Claiming that you'll just omit types willy nilly during this process is somewhat disingenuous. Open up any popular Haskell project how often do you see people bypass the type system as opposed to diligently spelling things out for it.
That's the beauty of it. I don't need to know these things globally. Different parts of application may use the session for different reasons. They don't have to all be aware of each other. I've also already pointed out that middleware can often associate complex data types with the keys in the request. Once again, these may be complete internal to the operation of the middleware and don't need to be exposed globally. Since keys can be namespaced in Clojure there's no risk of collisions either. Here's a concrete example of [ring-anti-forgery](https://github.com/weavejester/ring-anti-forgery/blob/master/src/ring/middleware/anti_forgery.clj) middleware. It's behavior is completely internalized and I would see having to describe it globally as polluting the code with incidental concerns that should be local.
"why me?"
 What is a maybe type and how do we use it?
This is what I do when `&lt;X&gt;` does not consist entirely of symbols.
I'm not a big fan of category theory stuff. I have started using some words like "isomorphic" (two things that share a structure, basically – you can convert between the two without losing information) and "partial" (functions that are known to throw exceptions for some input values) that are foreign to "regular" programmers, but it's just stuff I've picked up by being around Haskellers. I have tried getting into the category theory things (reading books and so on) because it seems useful, both as mental gymnastics and for inspiration, but I just can't. I have to put a huge amount of effort into it to understand it, and I'm not ready to do that.
computation != Turing Completeness. TC-ness is just a measure of problem difficulty (and therefore of the kind of computer that can compute such problems) additionally, while you're right that a logic that only makes reference to *data* cannot be Turing Complete, logics with codata are perfectly capable of being Turing Complete.
I have used repa for small things, like reproducing an lpr system I made in CV. I'm underwhelmed by the performance I get using repa along with the limitations such as kernel size. The author of Friday evidently came to the same conclusion, which is why Friday (and Mellow by extension) uses Vector. WRT high performance, the conversion to gloss format costs one copy and gloss internally performs a second. I have an issue in Mellow to track a couple up stream patches fixing these. A second performance issue is simply the immaturity of Friday/GHC in comparison with openCV/gcc (which itself falls short or commercial and custom solutions). If we can start getting simd from Friday/ghc then that would bring us a long ways.
&gt;Well, that is the point. Laws are important, but often we don’t bother to test them. That’s possibly fine if the instances are simple, and you can check the laws by just juggling definitions in your head, but when IO is involved, the situation becomes a bit more murky. Lack of formal checking of laws has always seemed odd to me. Haskell's static typing rejects "obvious" type errors -- but does nothing for TypeClass laws. My impression is that this also tends to lead beginner Haskellers, such as myself, to think of TypeClasses as synonymous with interfaces in Java of C#, whereas they really should have laws, and the laws should be treated seriously. I like the approach here, but it seems to me that it would be even better as part of the language. As we move closer to dependently-typed Haskell is this something that anyone is considering as a first class citizen? 
Have you considered that you might be among the top companies in terms of discipline, testing, actually fixing warnings instead of hiding them... in the dynamic world?
I find the quality of libraries to be exceptionally high in Clojure ecosystem myself. Most libraries are very focused, have clean APIs, the source is often very readable.
Yeah, I am still constantly surprised how little state you actually need to write interesting code. Thanks for sharing!
&gt; Constant applicative forms (CAFs) are defined to be any top-level definition Which means that `fibs` isn't one. It's in a `where` clause, not at the top-level, and effectively (i.e. after desugaring) bound inside the \n lambda, and GHC generally doesn't "float" bindings past a lambda as it can cause unintended sharing.
Sure, but the argument is regarding whether these problems are inherent to dynamic typing or not. Saying that there are a lot of poor practices around languages that are mainstream isn't very interesting of itself in my opinion. The more popular the language the more inexperienced developers use it. So, you see a lot of development practices that are subpar. Languages like Clojure and Haskell tend to attract more skilled developers since they're niche and it requires years of experience to appreciate their appeal. However, we shouldn't confuse that with the idea that the language itself prevents people from writing poor code. It's perfectly possible to write Haskell that's utterly impenetrable and inexperienced developers will simply end up with a different kind of mess than they would in Clojure.
A straw man is an argument against an imaginary argument that's not being made. However, my post responds to an article that makes precisely the argument I'm arguing against. If you haven't noticed the title of the original article is "the death of dynamic languages". The whole thesis is that static typing is superior and therefore the author concludes that only static languages will be created from now on. Thus the author is accepting the benefits as axiomatic. There's not a single shred of empirical evidence to support the claims regarding these benefits in practice though.
This a rather odd statement. To me, a good part of mathematics is building formal conceptual bridges that relates one domain of discourse to another. How is "computation is logic" via Curry-Howard disappointing?
It's the difference between: `solve :: (forall n. n -&gt; n) -&gt; n` and `solve :: (n -&gt; n) -&gt; n` (if you squint a bit) 
Recursive: fib 5 = fib' 5 (0, 1) = fib' (5-1) (1,0+1) = fib' 4 (1, 0+1) = fib' (4-1) (let b = 0+1 in (b, 1+b)) = fib' 3 (...) = fib' (3-1) (let b = 0+1; b' = 1+b in (b', b+b')) = fib' 2 (...) = fib' (2-1) (let ...; b'' = b+b' in (b'', b'+b'')) = fib' 1 (...) = fib' (1-1) (let ...; b''' = b'+b'' in (b''', b''+b''')) = fib' 0 (...) = let b = 0+1; b' = 1+b; b'' = b+b'; b''' = b'+b'' in b''' = let b = 1; b' = 1+b; b'' = b+b'; b''' = b'+b'' in b''' = let b = 1; b' = 2; b'' = b+b'; b''' = b'+b'' in b''' = let b' = 2; b'' = 3; b''' = b'+b'' in b''' = let b''' = 5 in b''' = 5 (Not exactly what is written; changed recursive call to be to `fib'` and changed `0 -&gt; (a, b)` to `0 -&gt; a`.) HOF: fib 5 = fst $ fibs !! 5 = fst $ (iterate (\(x,y) -&gt; (y,x+y)) (0,1)) !! 5 = fst $ ((0,1) : iterate (...) (1,0+1)) !! 5 = fst $ (iterate (...) (1,0+1)) !! 4 = fst $ (let y = 0+1 in (1,y) : iterate (...) (y,1+y)) !! 4 = fst $ (let y = 0+1 in iterate (...) (y,1+y)) !! 3 = fst $ (let ...; y'=1+y in (y,y') : iterate (...) (y',y+y')) !! 3 = fst $ (let ... in iterate (...) (y',y+y')) !! 2 = fst $ (let ...; y''=y+y' in (y',y'') : iterate (...) (y'',y'+y'')) !! 2 = fst $ (let ... in iterate (...) (y'',y'+y'')) !! 1 = fst $ (let ...; y'''=y'+y'' in (y'',y''') : iterate (...) (y''',y''+y''')) !! 1 = fst $ (let ... in iterate (...) (y''',y''+y''')) !! 0 = fst $ (let ...; y''''=y''+y''' in (y''',y'''') : iterate (...) (y'''',y'''+y'''')) !! 0 = fst $ let ...; y''''=y''+y''' in (y''',y'''') = let y=0+1; y'=1+y; y''=y+y'; y'''=y'+y'' in y''' = let y=1; y'=1+y; y''=y+y'; y'''=y'+y'' in y''' = let y=1; y'=2; y''=y+y'; y'''=y'+y'' in y''' = let y'=2; y''=3; y'''=y'+y'' in y''' = let y'''=5 in y''' = 5 (Exactly as written, which just goes to show that the HOF form is easier less prone to fiddly mistakes.) Depending on how well the strictness analyzer works, both allocate a linear number of *active* thunks. Tip 1: Accumulators often need to be strict. Tip 2: Haskell's pairs are ridiculously lazy; they rarely make for good accumulators. (I think the fix is "simple" for either of them, just make `x+y`/`a+b` evaluated a little more eagerly, and the GC will eat the head while you evaluate the tail.)
Assuming you have something like this: getUserById :: Id -&gt; Either Error User ... getDogById :: Id -&gt; Either Error Dog ... assignDog :: User -&gt; Dog -&gt; User ... Imagine doing this, first without monads: someFunction :: Either Error User someFunction = let user' = getUserById 7 dog' = getDogById 99 in case user' of Left err -&gt; Left err Right user -&gt; case dog' of Left err' -&gt; Left err' Right dog -&gt; Right (assignDog user dog) Now using Monad's `do` syntax: someFunction :: Either Error User someFunction = do user &lt;- getUserById 7 dog &lt;- getDogById 99 return (assignDog user dog) Or just using Applicative: someFunction :: Either Error User someFunction = assignDog &lt;$&gt; getUserById 7 &lt;*&gt; getDogById 99 Edit: formatting
Yes, the article is incorrect there. The action of 'f' on objects is that it maps `a` to `f a` at the type level. There is nothing that says there is an arrow from `a -&gt; f a`. The action on objects describes where the arrows go from and to, not some magical extra arrow. Keep in mind the definition of a functor F : C -&gt; D doesn't even require C and D to even be the same _category_. Consider a category Z of integers, with objects `..., -2, -1, 0, 1, 2, ...` and arrows given by `&lt;=`. e.g. `0 &lt;= 0`, `0 &lt;= 1`, `1 &lt;= 5`. We have identity arrows and composition. Now we can make an endofunctor `(*2)` on Z, and if a &lt;= b, then `a*2 &lt;= b*2`, but you don't get an arrow `a &lt;= 2*a`. Why? Consider the negative numbers. `-1` isn't `&lt;=` `2*-1`! (Not writing this at you mind you, but at the original author.)
This is what I had in my instinct with respect to a "feel" for monads. Good example. Thanks. Essentially, the headache is repetitive branching (or code repetition in general?). 
Nope. The `!(x, y)` just makes the match against the tuple constructor strict, but it already was. The `!(y, x+y)` is probably a syntax error, since patterns aren't allowed on the right side of `-&gt;`. `fibs = iterate (\(x,y) -&gt; let z = x+y in seq z (y, z)) (0,1)` might be strict enough, but I'm not sure. You may just get thunks around seq expressions instead of thunks around tuple constructors.
The difference is that in Haskell I can rely on type annotations to be accurate, a pure function won't do IO (given I do a quick grep on unsafe on the codebase before using it), a function that claims in its documentation it returns an Int won't suddenly return an empty array in an error case,... even if it is written by a bad developer. How do you get that level of safety from bad practices in a dynamic language? For that matter, how do you refactor a large scale codebase in a dynamic language without missing a call (say, add a third parameter to a two parameter function)? Tests can only do so much and are quite frankly a lot more effort than even the most complex of type systems if you use them to check for the same things a type system would. And of course they won't be done by that bad developer while even a bad developer is forced to satisfy the type system before he can claim that his code works.
False. Turing completeness is orthogonal to consistency. Martin-Löf's 1979 type theory is in fact based on a Turing Complete computation system, as is Constable et al's Nuprl type theory. There's even a way to look at intensional type theories like the one underlying Agda as being Turing complete, by encoding such programs using the Bove-Capretta method or related techniques.
The data-codata distinction is one way to look at this, but it's not even necessary to consider things in those terms to dismiss the "type theory can't be Turing Complete" fallacy. One way to address it is to use Computational Type Theory as an example; CTT does have codata, but that's not the reason why it's Turing Complete! Another possibility is to code up general recursion using induction-recursion or the Bove-Capretta method, which doesn't need codata either.
"computation is logic" is very disappointing through the lens of the *formal* Curry-Howard correspondence (that is, when you *define* the notion of "program" in such a way as that formal proofs and programs shall coincide, you inevitably get a lame and disappointing notion of program). However, the *semantic* Curry-Howard correspondence, which unlike the other is actually intuitionistically acceptable, requires no such restriction—since it is not concerned with formal proof, but with evidence. In this setting, the computation system is *at least* Turing Complete, and programs will serve as the evidence of propositions; but the Church-Turing thesis is not accepted, so in general programs may be of infinite width/breadth (i.e. not necessarily recursively defined). In this setting, the notion of "program" is defined first (in an open-ended manner, usually something like "Brouwer-effective operations"), and then the evidence for propositions is defined in terms of this notion; nowhere does "formal proof" come into play. This is the "non-disappointing" version of Curry-Howard. EDIT: here are a few things that might be worth reading, if what I'm saying sounds bizarre or surprising. - One thing to read would be Prawitz's "Truth and Proof in Intuitionism" (in the book, "Epistemology vs Ontology"). - The philosophical remarks in the final chapter of Dummett's "Elements of Intuitionism" may also help. - Though they are not phrased in this way, Martin-Löf's "Constructive Mathematics and Computer Programming" as well as the Bibliopolis book ("Intuitionistic Type Theory") are pronouncements of the *semantic* propositions-as-types/CH principle, as opposed to the formal/syntactic one. - Bob Constable's entire oeuvre will prove helpful. - Check out Bob Harper's blog and papers. A few relevant posts include https://existentialtype.wordpress.com/2012/08/09/churchs-law/, https://existentialtype.wordpress.com/2014/03/20/old-neglected-theorems-are-still-theorems/. In [this paper](https://www.cs.cmu.edu/~rwh/papers/trow/summary.pdf), Bob talks about behavioral type theory, which is a realization of the "semantic Curry-Howard Correspondence". 
`stack` is built upon the `cabal` library, just like `cabal-install` the tool. 
Oh sure, you use your anecdotal evidence to decide what works for you. That's perfectly fine. However, your conviction that your approach is the most effective one in general is unfounded. My experience is just as quantifiable as yours. I've worked with both types of disciplines and have not seen the same results you've seen. A proper approach would be to look at open source projects in the wild. There is plenty of data available for that. You can see whether there are patterns regarding defects, development time, and so on. In fact, [one such study exists](http://www.computerworld.com.pt/media/2014/11/lang_study.pdf) as I'm sure you're aware. The numbers in the study don't support your assertion in the slightest: Language | #Projects | #Authors | SLOC | Period | total #Commits | #Insertion | total #Bug Fixes | #Insertion Clojure 60 843 444 9/2007 to 1/2014 28,353 1,461 6,022 163 Erlang 51 847 2484 05/2001 to 1/2014 31,398 5,001 8,129 1,970 Haskell 55 925 837 01/1996 to 2/2014 46,087 2,922 10,362 508 Scala 55 1,260 1,370 04/2008 to 1/2014 55,696 5,262 12,950 836 results section: Are some languages more defect prone than others? Defective Commits Model Coef. Std. Err Clojure −0.29 (0.05)∗∗∗ Erlang −0.00 (0.05) Haskell −0.23 (0.06)∗∗∗ Scala −0.28 (0.05)∗∗∗ The results show minor differences within the standard deviation or very close to it. Statically typed languages have been around for many decades, and Haskell alone is 25 years old now. Lots of people have had exposure to them, and yet lots of experienced developers don't see the same benefits you see. It's simply amazing to me that this doesn't even give a pause to people extolling the static typing approach.
&gt; However, your conviction that your approach is the most effective one in general is unfounded. Again, I don't have such conviction. &gt; My experience is just as quantifiable as yours. I've worked with both types of disciplines and have not seen the same results you've seen. I haven't said anything to the contrary. &gt; It's simply amazing to me that this doesn't even give a pause to people extolling the static typing approach. It's equally amazing to me that you see me as some kind of Statically Typed Strawman and that I apparently hold all these opinions as do all of my bigoted brethren.
Hooray for my functional programming advent calendar! Thanks for writing, and thanks for posting. Upvotes all month for this stuff! 
Practical. I wanted to do something involving managing rollbacks of transactional state and eventually gave into the realization that working in a language where effects are explicit already was easier than trying to poorly embed that into my own half-baked DSL.
I don't mind the cabal files much. But it's safe to say of the various tools we've made at FP Complete, Stack is probably our best achievement as something we ourselves use every day and appreciate, and that the open source Haskell community ran with and improved and tested enthusiastically. Definitely changed my Haskell developing experience. It's wonderful. 
So pleased to see another 24 Days. Thanks!
&gt; For instance, the scoping rules are what you would expect them to be. Which means that, interestingly enough, the earliest LISP1s with dynamic scoping weren't proper implementations of LC.
A quick reminder to please not downvote people because they are wrong or you disagree with them. I learned something from the responses to this comment. This comment is of course not for *you*, Dear Reader, who would never do this; it is for the others who do.
And to add to *that*, Turing Completeness is boring as well!
`Text` generically refers to text, not to [Data.Text](https://www.stackage.org/package/text). You will also find other general text-mangling things in there like [Parsec](https://www.stackage.org/package/parsec). Here is a [list of hierarchical module names](https://wiki.haskell.org/Hierarchical_module_names). Stuff shipped with GHC follows these conventions. Many third party packages try to as well. I don't think it works well in practice because you get tons of modules with a meaningless `Data` or `Text` or `System` or `Control` prefix. We also get huge module names (it used to be [Text.ParserCombinators.Parsec](https://hackage.haskell.org/package/parsec-2.1.0.1), yeech) which discourages use of qualified module names. To an extent we're drifting away from these ridiculous hierarchies; at least we got the `ParserCombinators` out of Parsec.
so your 2's and your 1's and 0's ?
Yes that will work and is a standard technique, but I don't understand your question: &gt; I mean, when I call surface on a rectangle, will the result stay inside the rectangle. 
Weird. Stack has always worked flawlessly in all the machines I ever saw it used on. I use a lot more Linux than OS X, though. OS X seems to be tougher for Haskell to work smoothly on.
[2, 1..]
By the way, it's an offtopic, but can you explain why this gives me an error? Why does it deem it illegal? foo :: Show a =&gt; [a] foo = [1, "2"]
Yep, that's right.
Yeah of course it smells fishy, cause then you're writing dynamic code in a static language. I think the part you're missing has to do with locality. With the dynamic approach a function can add a key to the map that's only used in a particular context and a specific part of the application. Nothing else in the application has to be aware of this or design for it up front.
Aww yeah! :D It's really hard to overstate how great `stack` is. My AI class is letting students use whatever language they want, and I'm using Haskell. I can only really do this because "install Stack, do `stack build &amp;&amp; stack runghc assignment.lhs`" works so reliably well.
I'm so happy to see Franklin working on this series. It's a real shame I couldn't do another series this year, but with lots going on I just can't commit to it. My goal has always been to motivate others to try projects like this though, and it's great to see that happening. It's going to be really fun experiencing this project from the audience rather than the stage. Best of luck! Let me know if you want a guest article, I'm sure I can manage a post or two ;)
Turn off the "dreaded" monomorphism restriction. Or, define bar so that it's definition "looks like" a function (has at least one pattern).
Are the slides available?
&gt; Well clearly you can't infer types for anything that's not trivial That's simply wrong. :t \x -&gt; x ^.. traverseOf (_Just . _1 . _2 . _Left) \x -&gt; x ^.. traverseOf (_Just . _1 . _2 . _Left) :: (Field2 a1 b (Either b1 c) (Either b1 c), Field1 a a a1 b) =&gt; Maybe a -&gt; [b1] Does this code or type seem trivial to you? I just whipped it up in 5 seconds. Far more complex types can be inferred. &gt; I'm not talking about having types inferred in functions. I'm talking about the cost of describing all possible shapes of data up front. But you don't. For example, in PureScript (or any language with row polymorphism and/or polymorphic variants) -- records and sum types are written exactly as you would in a dynamic language, and fully inferred. Haskell has a QuasiQuoter support library for row polymorphism, so the syntax is slightly weird, but it also has this. &gt; HM pushes all relationships up to the global level. Anything you might be doing doing in any place in your application has to be described globally. The Ring middleware is a concrete example of where this becomes a mess Firstly, this is false. The HM system (e.g: in SML) has modules, which are anything but "global". The ring middleware sounds like a contrived example to justify dynamic types (and of course, you can use it in a static language, though it would be better to just use more straight-forward argument passing). Also, static types are not HM. Haskell does not use HM (it is very loosely based on HM, and only drifting further all the time). 
When I last checked (~1 year ago), optparse-applicative was roughly equivalent in power to [cmdargs](http://hackage.haskell.org/package/cmdargs) but much harder to learn and get productive with.
Lists in Haskell are homogeneous. While they can contain any type, all their elements must be the same type. So, there's no `(:)` (list cons constructor) with the right type. `Show a` isn't a type; it's a constraint.
&gt; reify your HKTs somehow and that's harder IIRC, it was actually going to require a change to the CLR bytecode to deal with non-erased HKTs, and the CLR team is not interested in doing that work, so the F* team just has to deal with it. I'm pretty sure mixing erased HKTs with reified "simple types" is a recipe for pain, and I can understand the F* team choosing not to eat that dogfood.
pass what directly where?
Very clear explanation, however this bit &gt; but doesn't know that Int is nominally equal to Char. sounds odd. I assume you put it like that because there is no type equality `Int ~ Char`? 
(2:(1:[ ] ) )
You've had a baby :)
You asked me to go back to this example. It's too long and tedious. Why not narrow it down to illustrate a point, instead of this dozens-of-lines repetitive dump? This code is equivalent to a simple nested record, which is fully inferred, and uses very similar notation to the one you used above, despite being fully inferred. In Haskell, I'd have to use QQ record package to write the record row-polymorphic without a type decl. Passing information between different closures handling the request - I'd use a framework that lets me just embed a reference to any arbitrary value in the session as a type. For example, an `IO a` value to handle the continuation of the session (Using serialization of 'static' values). Then I hide any information I want behind the `IO a` closure.
The whole point is that the session provides a global way to track this information. You need to store it somewhere in the end. Obviously, you can structure your program in a completely different way, but the whole point is that I want the language to support my workflow and not the other way around. This line of argument seems to be very common from static type proponents. As soon as you point to something that's awkward to do you're told that you just shouldn't do it.
This looks awesome. 
I'm not sure what you mean, to be honest. There's no way to get this to work with only functions, nor with functions-sums-unit-empty. I also don't know what you mean by "negative pairs"
It sounds odd because I was being careful about how I worded it. I deliberately avoided saying "`Int` is not nominally equal to `Char`" because technically, GHC has no internal notion of type inequality, so you can't write something like `foo :: (a !~ Char) =&gt; [a] -&gt; [a]`. Either two types are equal, or you *can't prove* that they are equal.
/u/skew shows you how to find `ST`'s roles via GHCi, but you're right in that `ST` doesn't have an explicit role signature anywhere in `base`. Rather, it's inferred via the definition of `ST`. newtype ST s a = ST (STRep s a) type STRep s a = State# s -&gt; (# State# s, a #) `State#` is a primitive type and therefore magical, and as it turns out, its role signature [is hard-coded](http://git.haskell.org/ghc.git/blob/1e041b7382b6aa329e4ad9625439f811e0f27232:/compiler/prelude/TysPrim.hs#l497) into GHC as `type role State# nominal`. Therefore, the `nominal` role percolates all the way to `ST` via `STRep`.
Yeah, indeed, stupid question.
I don't know if I'd call this a "convention", but I have seen a pattern where `coerce` can be used to make implementing typeclass instances for newtypes a bit snappier. Take [this example](http://git.haskell.org/ghc.git/blob/1e041b7382b6aa329e4ad9625439f811e0f27232:/libraries/base/Data/Semigroup.hs#l211) from `Data.Semigroup`: instance Semigroup All where (&lt;&gt;) = coerce (&amp;&amp;) This is a nice, pointfree alternative to what would be required without `coerce`: instance Semigroup All where All x &lt;&gt; All y = x &amp;&amp; y that (arguably) doesn't sacrifice any readability.
I wish cabal files had been Haskell in the first place. Haskell is great for creating edsl's, and doesn't have bizarre parsing rules.
What are the practical applications of `StrongSum`? It’s a tidy class, but I don’t understand the value. Is it referring to a dependent sum type or something? What would I use it for?
*I* am Spartacus!
Don't be so negative!
* you have to specify each exported module, each other module, and each module in a test or executable. Drudgery. Computers should do drudgery. * each component needs its dependencies listed. Drudgery, which is also repetitive. Potential for error by updating some dependencies but not others, and if you are a strict PVP adherent you will be updating these. A lot. You can reduce this by making the executables depend on the library, but last time I tried this a bunch of Haddock bugs crawled out. Maybe they've been fixed.
&gt; Static typing has been around for a long time, and yet we still don't have any conclusive evidence that it has a significant positive impact on development. Nothing has such evidence. To say that static typing is deficient because it lacks something that nothing else has is ridiculous. &gt; I disagree, the library is fairly idiomatic as far as I can tell... This is both true and irrelevant because, like I said, it has nothing to do with the type system. The use of those features is not a product of the type system. &gt; Use of which is strongly discouraged last I checked. Again, both true and irrelevant. Quoting your post: "In a language with a more relaxed type system you have escape hatches such as casting and unchecked side effects." Haskell has such escape hatches, thus what you wrote is wrong.
yeah, I've felt both those pains. 
What now? I've read LYAH, and although I still need to brush up on some topics like bytestrings and understand the big picture of monads (not just understand what an expression with &gt;&gt;= does, but also understand why its useful, why its so common, etc), I pretty much don't see where to go other than to read other books and hope it has something new. For example, how do I define an IO action does?
What I glean from the email is that `StrongSum` has no use, but justifies the existence of the `Pointed` class, which does have some practical use.
What do you mean?
I encountered basically the same issues trying to write an ST monad for Agda.
It sounds to me like you need a project, or some other excuse, to write more Haskell code. Once you have the basics reading N more books will be of marginal utility. Writing 1000 lines of Haskell and then reading one more book would be of much, much more utility. At least, that's been my experience. If you need help with project ideas you could ask here or make a new post. You could also look through the [suggestions here](https://www.reddit.com/r/haskell_proposals). IO actions are defined like anything else really. Take for instance `readFile`: -- | The 'readFile' function reads a file and -- returns the contents of the file as a string. -- The file is read lazily, on demand, as with 'getContents'. readFile :: FilePath -&gt; IO String readFile name = openFile name ReadMode &gt;&gt;= hGetContents If you drill down deep enough you'll eventually find that `openFile` and `hGetContents` rely on primitives that are implementation specific and implemented outside of the language.
I reject `Pointed` not because it has no laws, but because there are no useful functions polymorphic over a pointed functor. My opinion is that typeclasses are for polymorphism, not name overloading. I'm not changing the goalposts because I never thought the popular argument against them was very compelling. `Magma` would be a class with no laws too: just a binary operation. However, `Magma` has lots and lots of useful polymorphic functions. For every positive integer `k`, Magma has many `k-ary` functions: All of the binary trees on `k` elements. If you show me a function polymorphic on `Pointed` that I'd actually want to use, I'd accept it.
But GHC is great at parsing Haskell. The cabal system already requires a Setup.hs script written in Haskell. My thinking was that the package metadata could have been included in that setup script, rather than having it parsed from a wonderful new file format.
thanks, reminds me of my ultimate programming goal, but it'll be a long road, guess I'll start with simple stuff
Um, how did you write the `Category` instance that is a superclass of `Arrow` with just `Pointed`? ;)
A language with just data can still compute/be TC. Especially if you have generally recursive types (to get unbounded recursion). What you need for unrestricted computation isn't codata, it is a negation/shift. In the languages we are used to the only shift/source of co-variance is from functions/implication (which are codata) but you could instead have only data and have coroutines/subtraction (which is data). I've thought about working out/writing an interpreter for a little call-by-value language based on this idea (which would be the exact dual to the call-by-name lambda calculus) but haven’t had the time. Maybe I will do it over winter break. EDIT: I realized I think I misunderstood what you were saying. Rereading I think you were pointing out that you can encode a type of non-terminating computations using codata. And that is true. But, the ordinary terminating lambda calculus already has enough codata to do that (by way of functions, the "fuel" monad is encodable, modulo the need to always inline return, bind, and fix because of the lack of polymorphism, in System-T as the idea is just `Fuel t = nat -&gt; Maybe t`). 
The thing that kills `Pointed` for me is ultimately that if we started, say, using `Pointed` and `Semiapplicative` both as superclasses of `Applicative` then when you write functions like: quux = foo &lt;$&gt; bar &lt;*&gt; baz it will infer the weaker `(Pointed f, Semiapplicative f)` constraint all over the place, with no laws relating the use of `pure` and `&lt;*&gt;`. It is _very_ easy to lie to yourself in that situation and start to believe that you have the laws for how things relate when you didn't ask for them and don't actually have them. So if I have to pick one or the other to avoid that issue, I tend to err on the side of the one that gives me the associative structure.
I have my own compiler.
&gt; Clojure uses many of the same mechanisms that make it reliable. You clearly don't understand what makes Erlang unique and *fundamentally* different from Clojure/Haskell/... Erlang is a *process* based language with a functional core. It is the *processes* that makes Erlang uniquely suited to write highly concurrent and reliable system. I would recommend reading Joe Armstrong's book "Concurrency Oriented Programming" or his thesis before you start comparing languages. One way to put it is that Erlang is the 
The primary use I'd have for a `Pointed` class as a superclass of `Applicative` would be to be able to talk usefully about "affine" 0- or 1- target traversals with a proper subtyping relationship to the normal `Traversal`.
`Setup.hs` can be omitted, can't it? Either way, fair enough! I do wonder whether needing a language as powerful as Haskell for your build config is a bit whiffy - though for complex projects with lots of tasks where you end up with massive Makefiles, using a proper language like Haskell would be much nicer. But when working on Haskell projects, I do miss the simplicity of `package.json`.
If things like `exposed-modules` are absolutely totally necessary, then help us with more tools to automate this work away! I'm not totally sure what would help exposing modules, but maybe at least a warning if there are modules present in the filesystem that aren't in the cabal file (I've been burned by that in the past - incomprehensible linker errors that went away when I exposed the modules) (this may already exist. It was a year ago that I ran into that).
I don't really like stack, it's too opinionated about project structure and workflow, and if you're not using LTS haskell it actually takes more work than using Cabal. Cabal requiring CamelCase file names is about the max level of invasiveness I can stomach in a tool.
DNS broken?
Uh…why exactly can’t we define laws in terms of multiple typeclasses? In mathematics, laws are defined in terms of multiple algebraic structures all the time. There’s no material difference between “if `Applicative f` then…” and “if `Pointed f` and `Semiapplicative f` then…”
Thanks, I hadn't noticed the "Index" link, can be useful in various situations. In general, though, you don't know which package exactly contains the thing you're looking for.
What is hie?
I knew there would be a hoogle somewhere on stackage, but you made me notice that it's on the landing page :) stackage's hoogle returns many more results, thanks.
Didn't know about hayoo, thanks. Seems very useful, but for some reason haskell-servant appears to be missing entirely on hayoo.
True, but those conditions are implicit in Haskell anyway. What is the harm in decomposing structures into an encoding with multiple typeclasses if that gives us more flexibility? Typeclasses *already* fail to represent these structures directly, because the functions are often in the wrong position. I like to use `Monoid` as an example: it should be parameterised by a type and an `mappend`. Because it isn’t, we need to add the nominal `Sum` newtype, which is only tied to the `(+)` operation by convention.
I switched from cmdargs to optparse-applicative because I found the exact opposite to be true. Perhaps it's just a matter of personal style.
Because sometimes you may just want that actual weaker condition. * Traversable is stronger than Functor + Foldable for instance. * Applicative is stronger than Pointed + Semiapplicative. * Monad is stronger than Pointed + Semimonad. You lose the ability to talk about the weaker things at all. Currently we have a culture of only incurring laws for things that are superclasses. This gives a clear responsibility for checking the laws to the person who writes the instance. In a world where you get magical synergies, well, orphan instances, which are already bad, become _much_ crazier. Two people defining different orphans can now get synergistic laws due to instances they didn't even know someone else wrote!
I hesitantly added “top-level” after the fact, but I’m not actually sure it’s correct. When I was working with Simon and asked him about performance details of code, he indicated that more things were CAFs than I had previously thought. I can’t seem to find a good reference for this, though, and at the moment I’m too tired to go and inspect some generated code to verify it. If it’s *not* a CAF in this case, shouldn’t there be no performance difference between `fib 1000` followed by `fib 1000`?
Sorry I should have put in a link, it’s [haskell-ide-engine](https://github.com/haskell/haskell-ide-engine). We’re still in prealpha state or something like that but it’s moving along pretty well.
Couldn’t empty derived classes or constraint aliases work? The compiler infers `(Pointed m, Semimonad m)`, but I want to tell Sam Programmer that I’m assuming `m` forms an honest-to-goodness `Monad`, so I just use `Monad m` in the signature to document the stronger assumption, and as a bonus it’s more succinct. To my mind, as long as we’re making assumptions, it doesn’t much matter whether we’re doing it in the typeclass or at call sites, because all the typeclass does is provide polymorphism. At least if the typeclasses are decomposed, we can express weaker constraints if we want to, which currently we can’t do. And finally, pay no attention to the orphan instances behind the curtain. ;) 
We can make that empty class. It just means that type inference is going to give you those more hazardous types all over the place. It is like saying, "you can foldMap with a unital magma". It is _true_ that you can foldMap with a unital magma. However the result isn't in any way shape or form canonical/universal under the weaker type signature. We sit at a very nice ratio balancing expressiveness and the ability to reason about code. My experience using very finely diced up classes with laws _not_ included is that you often mistake yourself and think they are present in the code you write. This is what has driven me to move "backwards" and bundle more things together at least when working in Haskell. We're just not good at dealing with fully fine-grained class hierarchies. When you unpack operations and laws it isn't free. Without the laws you typically can't reasonably supply the default definitions for many things.
It just has Andrea Vezzosi's closed-universe-of-types solution which is super unsatisfactory for me.
The slides of the presentation: https://goo.gl/98NzmI
Oho, the documentation for the `Monad` class has significantly improved. I hadn't seen that. Thanks to whoever did it! 
I initially thought that to work with MaybeT my intermediate functions (`connect` and friends) should return a `MaybeT`. Actually, they return a `MaybeT Identity`, however I thought I had to do complex conversions (using `mmorph`) to get from `MaybeT Identity` to `MaybeT IO`. Thank you, I just dropped my `(&gt;&gt;&gt;=)` and `(&lt;&lt;$&gt;&gt;)` and `(&lt;=&lt;&lt;&gt;)` custom library ;)
Now that you mention it, see https://github.com/haskell/haskell-ide-engine/blob/master/docs/Report-2015-11.md
Would it be any more reasonable in a language that bans orphans?
I'm late to the party here, but I couldn't find an explanation of why hdevtools is not included in the referenced technologies. I prefer it to ghc-mod for its speed - is there any architectural reason it could not be updated to use ide-engine as its backend? I'd be happy to try and contribute some coding work here if it is technically feasible.
Its definitely technically feasible, and nothing is left out deliberately, it has just not been added yet. Please feel free to add it, and contributing a plugin / other support is most welcome.
Oh, by surface do you mean "area"?
Of course there are things that are awkward with static typing. And indeed, you shouldn't do them. The same thing that makes it awkward for static typing also makes it awkward for keeping track of correctness in your head too. 
I see. To me it just sounded as if "Int is nominally equal to Char" was true but GHC didn't know it even though of course I knew that that was probably not what you intended to say.
thanks! Constructive criticism is the best :)
Wow that's really neat. Didn't realize there were Coercable instances for functions.
No, it's types depending on types.
It is types depending on types, but as soon as you have a singleton that also means that the type depends on a value which is what ocharles was trying to get at. 
These microservice guys are trying to reinvent Object Oriented Programming again by trying to create a rudimentary distributed smalltalk interpreter, but without the necessary imperative sequencing. 
An internally created `STRef` is not observable outside the calculation. The isomorphism is universally quantified over `a`, so `STRef s b` is not a valid value for the return type `a` because it depends on the bound type variable `s`. So an internal `newSTRef` just introduces a nested stateful calculation, which can happily be represented as a nested `State` calculation by recursive induction.
Lambdas are the raw building material of the future.
But that's not true, you don't even know the type until you look at the values - that's the power we get from a GADT.
I think that all of this is a regression. At some time they will create a form of dependently-typed assembly language. And they will be proud that each register will have the correct type for each machine code instruction, and this would be sold as the best thing in programming ever.
The type-value isomorphism of singletons allows Haskell (a type-&gt;type dependency language) to emulate value-&gt;type dependency.
Thanks for doing the series - especially since Ollie has given his approval. :) I agree that `stack` is a great choice for the first post. For newcomers, it is a no-brainer that stack is a game changer. `cabal` has a significant learning curve, and its interface still includes a generous helping of cruft for historical reasons. But you imply that stack is a game changer even for experienced Haskell developers. I'd be interested to hear why you think that. Now that so-called "cabal hell" has disappeared over the horizon into the distant past for those familiar with modern cabal's powerful dependency management features, and with the cabal team hard at work even as we speak on even more powerful features such as Nix-based hermetic builds, what does stack bring to the table for proficient cabal users?
That's necessary, but not sufficient for a dependent type system.
Right, but this emulation is not complete, and even if it was, it is not the same as actual dependent types. Just because Naturals and Integers are isomorphic doesn't mean they're the same.
I thought that types were necessary in order to eschew some logic paradoxes
To do dependently typed programming, you need a dependent type system.
Is there a concise and complete guide like that somewhere in written form, not a video?
The main issue here is that LTS sets are manually checked to work together. Cabal just tries the solver using the latest possible version of Hackage. This frequently exposes breakage requiring annoying reinstalls if you first installed a package against a later version of a dependency, and then you install a package that requires an earlier version.
No, you're right. That whole "types are the only documentation you'll ever need" thing got stretched out of proportion, and this is the result. Doesn't mean we can't change things though.
Our biggest wins since switching to stack from a cabal based workflow with a freeze file: * It manages multi library development. With sandboxes you have to do this yourself, the configuration isn't saved anywhere. If you want this across teams, you'd have to script it yourself. * Sandboxes become unusable with larger projects. We have about 100 libraries and these are several bugs I've mentioned before that cause slowness, require nuking the sandbox etc. * Cabal repl with a multi-library setup is something I've never gotten working. With stack it worked from the beginning in a rudimentary form and has gotten better with each release. * Development on stack is very active, whereas cabal seems to lack an active development team. Bugs I report are quickly fixed, questions answered, features built even. This is not a technical point but is very important.
have you browsed the haddocks, I think hackage has some problems with documentation recently, but cabal can generate it for you. usually I find enough documentation for my needs - could you name the library you are thinking about 
Welcome to Haskell. Luckily, people like Gabriel Gonzalez and Chris Allen have been attempting to correct this deficiency. There have also been "call to arm" discussions about improving documentation, though I'm not sure than in the couple of years since those discussions much really has changed. I will say that changes on Hackage such as linked source code and inline READMEs have improved the situation from a presentation point of view, at least.
You need a way to tell cabal which modules to compile. There could be something like `ghc --make` where you specify a few of them, and then automatically include whatever else you find by chasing imports. That would be bad for release-quality builds, because you want to make sure not to include junk that accumulated while developing the code. But for getting started and for quick informal projects, it would be helpful. It would then be great if you could save the output of that process and edit it, to ease the transition to an explicit list.
How would that be a bad thing? It sounds awesome!
&gt; It manages multi library development. With sandboxes you have to do this yourself, the configuration isn't saved anywhere. If you want this across teams, you'd have to script it yourself. I'd like to know more about how that works in stack. We do it with cabal sometimes. It's possible. But this is an area where stack might be adding a lot of value. &gt;Sandboxes become unusable with larger projects. We have about 100 libraries and these are several bugs I've mentioned before that cause slowness, require nuking the sandbox etc. Some of our projects build much more than 100 dependencies. The old cabal bugs that required frequent sandbox nuking are long gone. We do nuke a sandbox occasionally, but it's because of people getting confused, not because of cabal getting confused. &gt; Cabal repl with a multi-library setup is something I've never gotten working. With stack it worked from the beginning in a rudimentary form and has gotten better with each release. It has been getting better in cabal too, in parallel. There are two repl modes: `cabal repl`, and `ghci -package-db .cabal-sandbox/...', optionally in conjunction with a `.ghci` file. Both work, and both are useful, depending on what build assumptions you want pre-installed in your repl. &gt; Development on stack is very active, whereas cabal seems to lack an active development team. Bugs I report are quickly fixed, questions answered, features built even. This is not a technical point but is very important. Sorry, that is just totally wrong, and totally unfair to the hard-working cabal team. Cabal is very actively developed by a responsive team of some of the Haskell world's top developers, including close cooperation with core GHC developers. Cabal is probably one of the most quickly growing Haskell applications. In the past, there were some feature requests and "bug reports" which were actually requests for major changes to the whole philosophy of how cabal works. Obviously, those kinds of requests cannot always be accepted by project leaders, especially for a fundamental ecosystem component like cabal. It is understandably tempting for a submitter of those kinds of issues to view the lack of acceptance of the issues, and the lack of willingness to continue arguing about it indefinitely after a decision has been made, as unresponsiveness of the project leaders. But it is not.
So, I was planning to write one, as well as an interface to the REST api, but I've been sidetracked and probably won't get to it for another couple weeks. However [you can actually deploy with a container instead of a buildpack](https://www.ng.bluemix.net/docs/containers/container_planning.html). I know stack supports containers out of the box, I don't know if it's much work to use cabal for this.
Great, that's good news. I thought I'd ask before disappearing down a technically unfeasible rabbithole.
I don't think it's stretched out of proportion so much as experienced Haskellers being able to hobble by with just the types, so the need to make the documentation even better is not as strong as in other languages, which translates to fewer documentation contributors. I know that when I personally manage to figure out how the types fit together in less than a few hours – the time it would take to read a thorough tutorial – I don't bother writing anything about it. ---- The example mentioned by OP – HTML parsing – also happens to be the kind of domain where types get you really, really far once you're comfortable with following them. What any HTML parsing library will do is essentially provide a single parseHTML :: String -&gt; HTMLTree function. There may be some variants of this (`parseHTMLWithOptions`) but the basic idea is that you give it a string and it gives back some sort of data type representing the HTML tree. By looking at [how this tree is defined](https://hackage.haskell.org/package/tagsoup-0.13.5/docs/Text-HTML-TagSoup-Tree.html#t:TagTree) you get to see everything you need to walk the tree and extract whatever information you want. This is exactly what a tutorial will tell you, except in many more words with many more examples. That is a useful service! But when I can gather that information just by looking at the types, I'm less inclined to feel the need to write the same thing but with more words. In other languages (and some other non-obvious libraries), you really do need someone to walk you through which methods are available and how they fit together. But the better the type system, the less will experienced users need this. And experienced users are the ones writing tutorials.
Positively surreal.
&gt; For one thing, Stack makes it substantially easier to manage multiple GHCs on the same machine. My workflow is that I have one default GHC, and I switch to any other by adding a single component to the beginning of my path. Everything else is handled totally automatically by cabal. When I need to switch the default - which isn't very often - it's a simple `stow` command. How could stack be easier than that? On Windows it's a few path components because of mingw/msys, so we have a batch file for that on all the Windows machines. And switching the default is still a somewhat annoying manual edit of the system path; we haven't gotten around to automating it. So that might be an advantage of stack on Windows. &gt; Another thing I like is that the LTS sets are curated, so breakage due to weird dependency diamonds is a thing of the past with LTS sets. I know you can use cabal with those sets, but it's just substantially easier with Stack. The whole Stackage/LTS infrastructure is a hugely valuable contribution of FP Complete to the Haskell community. Thanks to FP Complete! The pluses and minuses of a curated package set are well known; it depends on the use case. Both stack and cabal allow both. The stack workflow encourages more use of curated sets by default, which is a better choice. Truth is, we haven't gotten around to setting things up to use curated sets very much. On the one hand that shows that it isn't critically important, but on the other hand, stack making that the default is a definite advantage.
&gt; &gt; It manages multi library development. With sandboxes you have to do this yourself, the configuration isn't saved anywhere. If you want this across teams, you'd have to script it yourself. &gt; &gt; I'd like to know more about how that works in stack. We do it with cabal sometimes. It's possible. But this is an area where stack might be adding a lot of value. Basically there's a `stack.yaml` configuration file layout out your project. It specifies which source directories are included (and those are built from source, taken into account when checking dirtiness etc, like `cabal sandbox add-source`). These some tweaks to this, e.g. you can have source dependencies that are treated more like normal dependencies (`extra-deps: true`). See [the documentation](https://github.com/commercialhaskell/stack/blob/master/doc/yaml_configuration.md) for more details. &gt; &gt; Sandboxes become unusable with larger projects. We have about 100 libraries and these are several bugs I've mentioned before that cause slowness, require nuking the sandbox etc. &gt; &gt; Some of our projects build much more than 100 dependencies. The old cabal bugs that required frequent sandbox nuking are long gone. We do nuke a sandbox occasionally, but it's because of people getting confused, not because of cabal getting confused. That's good to hear. Is the slowness still there? Adding 100 `add-source`d dependencies made any cabal invocation unbearably slow. &gt; &gt; Cabal repl with a multi-library setup is something I've never gotten working. With stack it worked from the beginning in a rudimentary form and has gotten better with each release. &gt; &gt; It has been getting better in cabal too, in parallel. There are two repl modes: cabal repl, and ghci -package-db .cabal-sandbox/...', optionally in conjunction with a.ghci` file. Both work, and both are useful, depending on what build assumptions you want pre-installed in your repl. Why are there two 'modes'? What's the difference between them and when would you use which? &gt; &gt; Development on stack is very active, whereas cabal seems to lack an active development team. Bugs I report are quickly fixed, questions answered, features built even. This is not a technical point but is very important. &gt; &gt; Sorry, that is just totally wrong, and totally unfair to the hard-working cabal team. Cabal is very actively developed by a responsive team of some of the Haskell world's top developers, including close cooperation with core GHC developers. Cabal is probably one of the most quickly growing Haskell applications. Yes, I'm sorry, I didn't mean to downplay the efforts of the many people working on cabal. But this was directly echoing my experience with [issues filed against cabal](https://github.com/haskell/cabal/issues/created_by/hesselink) versus [issues filed against stack](https://github.com/commercialhaskell/stack/issues/created_by/hesselink). Maybe the stack issues were easier to fix, I don't know. But just to give one example, there's [this issue](https://github.com/haskell/cabal/issues/1375), open since 2013, marked both 'urgent' and 'important' with an unanswered question from someone from this January. &gt; In the past, there were some feature requests and "bug reports" which were actually requests for major changes to the whole philosophy of how cabal works. Obviously, those kinds of requests cannot always be accepted by project leaders, especially for a fundamental ecosystem component like cabal. It is understandably tempting for a submitter of those kinds of issues to view the lack of acceptance of the issues, and the lack of willingness to continue arguing about it indefinitely after a decision has been made, as unresponsiveness of the project leaders. But it is not. I've never requested anything like this to my knowledge. I like the direction cabal is going in (judging by [Duncan's blog posts](http://www.well-typed.com/blog/2014/09/how-we-might-abolish-cabal-hell-part-1/) for example). But for me currently, in my job, and based on my experience using several tools (cabal from 2009, later cabal-dev, then experimenting with cabal sandboxes, somewhere in between adding freeze files, and switching to stack starting in June this year) I have come to the conclusion that currently stack fits better. We might switch back if cabal offers some compelling feature to do so, but I don't see one at this moment. What's the advantage of cabal over stack for you? 
So this boils down to writing a stack recipe. Excellent
&gt; The main issue here is that LTS sets are manually checked to work together. The selection of versions is manual, but the checking is automated. That's cool! &gt; Cabal just tries the solver using the latest possible version of Hackage. That's a sane default, but it's only a default. You have total control of that, both on a global level and on a per-package level. And it can be controlled via a simple text file, so you can use your favorite tools to implement any policy in between. If you want to stick to an exact set of dependency versions that you know works - like a curated package set, or a previous build know to work - you can just drop that version list into the project folder. &gt; This frequently exposes breakage requiring annoying reinstalls if you first installed a package against a later version of a dependency, and then you install a package that requires an earlier version. Yes, agreed. But that is a natural consequence of allowing a selected set of dependencies to float independently to more recent versions. And with modern cabal and a little experience, it's quite easy to fix those, even when working with a huge and complex dependency set.
Yeah, we still should provide a buildpack, but it's absence doesn't prevent you from deploying.
Even in this case, better documented types would be beneficial. Many times the name of the type is not clear enough to show what it means and how to use it.
The API presented in the post is obviously contrived, but I'm not convinced it's that far away from a real world use case... though I don't have any direct examples.
Essentially a singleton is a data type where each possible value has a direct correspondence with a single unique type. For example, take the following standard data type: data Fruit = Apple | Orange Knowing that you have a value of *type* `Fruit` does not let you know *what* value you have - you could have an `Apple` or an `Orange`. However, we can create a singleton type (using the `-XDataKinds` extension): {-# LANGUAGE GADTs, DataKinds, KindSignatures #-} data SFruit :: Fruit -&gt; * where SApple :: SFruit 'Apple SOrange :: SFruit 'Orange Now if I tell you I have a value of the type is `SFruit 'Apple` you know *exactly* what that value is - it must be `SApple`. At this point the types and values are in direct correspondence. This is useful because it means that when you know a value, you know the type, and you can influence the type checker. For example, if we're type checking `foo :: SFruit fruit -&gt; SomeTypeFamily fruit`, and we pattern match the `SFruit` as `SFruit 'Apple`, then we know that the `fruit` type variable must be `'Apple` and we can now find the normal form of `SomeTypeFamily 'Apple`. In short, knowing the value has influenced the type, and allowed further types to *depend* on earlier values.
&gt; experienced Haskellers being able to hobble by with just the types In fact, I've heard some experienced Haskellers say that they get by better with type-based documentation without examples than when they have to use e.g. python or javascript libraries and come across example-based documentation without good documentation of exactly what types functions can take. Of course, documentation that has everything (examples + types + usage/semantics notes) is better than any one 
My CRUD example above I've actually implemented for a client (though sadly pre having worked out how to do the dependent server, so I needed ugly workarounds). But I'm going to take your comparison to Euclid as a compliment :-p
No, /u/kamatsu is right here. You know the type by looking at the type. You just can't look at the type until you're invoked by being passed something which can inform you of that type. But you could get passed often e.g. `undefined :: Int` -- obviously you're looking at the type rather than the value, otherwise an exception would arise. Singletons let us fake it really wall, but it isn't the same. That said, calling the servers themselves "dependently typed servers" doesn't seem awful to me, because this whole thing is really taking place at the realm of type-level programming. (if one were to say for example that "we use dependent types to create servers" then I think the precision would matter a little more.)
This makes very much sense, thank you for the explanation. Does this idiom, in the limit, lead to a distinct type per program? 
I'm just curious, and I have only a superficial understanding of these things. The argument used in the article is: &gt; This is an example of a dependently typed server: the value that we are given as input determines the type of the rest of the server. If we get a string as input, we expect a string operation as the second argument and the response is also a string; if we get an integer as input, we expect an integer operation as the second argument and the response is also an integer. Isn't it similar to say that C is dependly typed because `sin some_char` doesn't type checks? In the sense that the input determines what is going to be done, working or not?
&gt; shouldn’t there be no performance difference between fib 1000 followed by fib 1000? Prelude&gt; :set +t +s Prelude&gt; let fib n = fst $ fibs !! n where fibs = iterate (\(x,y) -&gt; (y,x+y)) (0,1) fib :: Num b =&gt; Int -&gt; b (0.03 secs, 0 bytes) Prelude&gt; fib 1000 43466557686937456435688527675040625802564660517371780402481729089536555417949051890403879840079255169295922593080322634775209689623239873322471161642996440906533187938298969649928516003704476137795166849228875 it :: Num b =&gt; b (0.02 secs, 0 bytes) Prelude&gt; fib 1000 43466557686937456435688527675040625802564660517371780402481729089536555417949051890403879840079255169295922593080322634775209689623239873322471161642996440906533187938298969649928516003704476137795166849228875 it :: Num b =&gt; b (0.02 secs, 0 bytes) I see no difference. I also see no allocation, which probably means the optimizer is foiling my attempt to be simple and ignore it. I did also check 10000 and 100000, and there was no performance change there either. (I do see allocation in both those cases.) If you do promote `fibs` to the top-level manually however, you will see quite the difference between the first `fib 100000` run and the second one. It second will be nearly instantaneous and allocate nothing, since the memory allocated by the first will be retained until the `fibs` binding goes away (which , in GHCi might be before the process ends).
&gt; he indicated that more things were CAFs than I had previously thought. That's probably true, but I don't think floating a binding to the outside of a lambda is normally done (though, there is likely a flag to allow the optimizer to try that). [I am wrong.](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/options-optimise.html) (see -ffloat-in and -ffull-laziness), but based on the numbers I've seen, GHCi isn't actually floating the fibs value outside the lambda and treating it as a CAF. That could just be a interpreted vs. compiled difference though.
That is because there is no package called haskell-servant on hackage. Do you just mean servant?
That's just one step of the induction, then, because those blocks are also free to introduce refs. Not only that, but this translation isn't exactly compositional. What about this ST program: foo :: ST s (STRef s Int, STRef s Bool) foo = (,) &lt;$&gt; newSTRef 3 &lt;*&gt; newSTRef True I don't see how that has a clear analogue in this translation. My point isn't that they're not isomorphic, but that the proof is nontrivial. I think I've satisfactorily demonstrated that.
&gt; My workflow is that I have one default GHC, and I switch to any other by adding a single component to the beginning of my path. Everything else is handled totally automatically by cabal. When I need to switch the default - which isn't very often - it's a simple stow command. How could stack be easier than that? I don't know what "stow" is. I don't need to change my path. I need to do absolutely _no_ configuration of my environment. I can just tell `stack` to compile my project. If I don't have the right GHC, it fetches it. What could be easier than that? 
Nice explanation. This is precisely why I feel completely justified in calling this 'dependent'. 
you can! but what you described is not eta, it's a more general, complex property :) eta for sums us: given G !- M : A + B form G !- case(M; x.left(x); y.right(y)) : A + B or in Haskell: etaEither : Either a b -&gt; Either a b etaEither d = case d of Left x -&gt; Left x Right y -&gt; Right y but what you're describing is a permutation property that some people (esp. Bob Harper) erroneously call "eta" but which isn't actually eta at all. or perhaps it might be seen as a global version of eta, which proven to hold meta-theoretically by (local) eta + induction on proofs. you might be able to add it as a primitive judgmental equation, of course, but you can't prove it from the true eta. anyway, im not so sure you need these things in the Church/Scott encoded versions. but let's see: A + B := forall r. (A -&gt; r) -&gt; (B -&gt; r) -&gt; r left = \x l r -&gt; l x : forall a b. a -&gt; a + b right = \y l r -&gt; r y : forall a b. b -&gt; a + b case = \d -&gt; d : forall a b r. a + b -&gt; (a -&gt; r) -&gt; (b -&gt; r) -&gt; r true eta: given G !- M : A + B = forall r. (A -&gt; r) -&gt; (B -&gt; r) -&gt; r form G !- case M left right : A + B = M left right = M (\x l r -&gt; l x) (\y l r -&gt; r y) semi-true eta (eta for -&gt; instead of +): given G !- M : A + B = forall r. (A -&gt; r) -&gt; (B -&gt; r) -&gt; r form G !- \l r -&gt; M l r : A + B super eta: given G !- K : A + B -&gt; R G !- M : A + B = forall r. (A -&gt; r) -&gt; (B -&gt; r) -&gt; r equate G !- K M = case M (\x -&gt; K (left x)) (\y -&gt; K (right y)) : R =&gt; K M = M (\x -&gt; K (\l r -&gt; l x)) (\y -&gt; K (\l r -&gt; r y)) so, i dont think even true eta for `+` can be gotten from eta for `-&gt;` in System F. so the question is, i guess: does that matter? what is the judgmental equality going to do for us in System F?
Having laws means you can reason about code written using the class using just the laws and not worrying about the particular concrete instance. This lets you seperate concerns. You can reason with a much smaller subset of concerns (just the laws of the classes you use) on one hand, and then separately prove that something is an instance of the classes in question. When you don't have laws all bets are off. You have to reason operationally about the behavior of each instance independently -- even the ones that don't exist yet that you don't know are going to be made. It is that latter part that pretty much hoses you. It is much the same thing that makes it so when you hand me a function `(a -&gt; a)` I can be pretty darn sure it does one of a couple of things (either throws an exception or is `id`), but if you hand me a function from (Int -&gt; Int) it could do an explosive number of things. The laws constrain you in a productive way, more constructions become canonical or at least unique up to isomorphism. We name `fmap` because it is unique if it exists. You don't have to keep looking over your shoulder for other ways to write it.
The difference is that Idris is a dependently typed language, so the laws there are type checked for any instance you write. This thread started out with a question of how dependent types would affect the enforcement of type class laws. The short answer is right there. The longer answer will have to come from someone who has put together a sizable Idris application, and had the opportunity to think more concretely about the utility of this particular precision.
Good question. I was just musing on the implications of these ideas. Could you point me to a simple library that uses singleton types?
If I'm reading this correctly, then I cannot reverse the string "1234", because that string is readable as an `Int`?
If you just use the dynamic solution in Haskell, I don't think it would noticeably more painful than in dynamic languages. It'd just be more painful than the usual pleasantly-typed code, relatively. It would bring the ordinary dynamic pain/uncertainty to a subset of your Haskell program, and that would stick out like a sore thumb, but syntactically it won't be much heavier and it would do the same thing.
&gt; But when I can gather that information just by looking at the types, I'm less inclined to feel the need to write the same thing but with more words. This is exactly it. So many times I've written documentation above a function, then erased it because the type signature beneath it said precisely the same thing, in a more succinct way.
I think another contributing factor is that a lot of us come to Haskell because we want it caught at compile time when things are wrong. Documentation does not typically have this property. I know when I'm writing documentation (not just in Haskell) there is a nagging "...but this will be wrong eventually" that probably leads to me writing less documentation than I otherwise would.
The easiness is purely subjective. You find your workflow easier and I find mine easier. If anything, I would argue that it's easier to be able to express yourself freely than within the constraints of a type system. As far as assurances go, it seems like a purely theoretical argument for the most part. In practice, any non-trivial application will have acceptance tests. These are equally needed with both dynamic and static typing disciplines. These serve as the contract for the application ensuring that it does what the customer asked. The additional assurances for covering the hypothetical cases that have no actual code path associated with them aren't all that interesting in practice.
Author here. Deja Fu basically works like this: it executes the computation using a deterministic scheduler, recording everything that happens; it then looks for pairs of actions in different threads which *could* make a difference if they happen in the other order. It then tries to find a schedule which will cause that to happen. This loops until there are no more executions to try. Internally, it builds up all the executions in a big graph, where each node has: what action was performed; the runnable threads; a to-do list of decisions to try; and a list of decisions to ignore until something happens which could influence their result. Here's the progression of graphs, with executions which didn't go anywhere useful and got aborted omitted, for `results $ go (&lt;*&gt;) (\_ -&gt; "") (\_ -&gt; "a") 0`: http://imgur.com/a/1glbO Each path from the root to a leaf corresponds to one complete execution. The ideal is that only this many executions will be tried, but there are many which got aborted, as my handling of exceptions currently is very heavy-handed and leads to a lot of wasted effort... You can get a feel for how much wasted effort there was by looking at how many graphs there are with no new complete executions, the only thing that has changed is that something in a to-do list was tried and then discarded at some point. To include all of those aborted executions would make the later graphs way wider than they are right now.
&gt;Is there a concise and complete guide like that somewhere in written form, not a video? No, nothing comes close to the video yet. Closest thing would be the Stack documentation itself. Could make such, but I'd need to find time between the [haskell book](http://haskellbook.com/), work, and visiting family this month. Will talk to my coauthor about it.
Actually, never mind. I just realized that you can already do this with `vector-algorithms`.
We need to make this a FAQ somewhere.
&gt; Both cabal and stack fully support all of the various workflows for how and when you do version upgrades of dependencies. I agree that stack probably has a better default - although I have no experience with it - but it's just a default. It is not just a default to have direct support for package sets and their reuse as compared to the ability to use config files that specify a version constraint for every individual package. As for the sandbox location, the path is not the only problem with cabal sandboxes, there is also the facts that dependencies and the local packages live in the same sandbox, making it impossible to reuse such a sandbox between several jobs where the versions of those slightly differ. Stack separates the LTS packages cleanly from local packages which just happens to be the separation you need for maximum reuse. Clean builds from scratch are not really a viable option if building the dependencies takes an hour or two (e.g. because yesod or something with a similar number of dependencies is involved) and builds exactly the same packages every single time. I have had that situation before stack came along and tried to make it work anywhere near as cleanly as it does now out of the box. Don't get me wrong, cabal-install was much better than building things by hand and many people have contributed and experimented on it how to best handle the (hard) packaging problem. However I would suggest that the last major innovation in cabal-install were sandboxes, in 1.18. No-reinstall cabal, which has been cited for months as the next big thing to change how we work with packages in cabal-install, sounds more like a workaround to long-standing problems than an actual solution. People have been talking about getting secure downloads and updates into cabal-install for at least a year now, probably closer to two...and very little has happened. Incremental updates are still not available either. To be honest it just seems like an old project with a codebase that has accumulated a lot of technical debt over the years and is hard to change because of it. Starting over in that situation seems like a pretty good idea and I am not sure why people so vehemently defend it as if it was more than a tool.
IDontLikeApple: Yes, you miss that haskell is a language invaded by self-centric autistic &amp; social inepts who, for some unknown reason, are proud of themselves understanding and programming any form of Haskell gibberish and enjoy making things difficult to anybody else. Unfortunately, as you can see in the comments, not only there is no documentation, but they are proud not having such documentation. 
[hpack](http://hackage.haskell.org/package/hpack) generates cabal files from non-redundant, boilerplate-free [yaml files](https://github.com/simonmichael/hledger/blob/master/hledger/package.yaml). I think it can also discover modules for you. It has some limitations currently (no support for conditional blocks). It could be integrated into Cabal, so that yaml (or toml) files could be used instead of cabal file format.
OK. Thanks for working on those!
So in only two to four years, a beginner will be capable to understand and use some haskell libraries. Great! Thanks
Which constraints are violated? As for how you might fix it -- for this toy example you could for example wrap strings in quotes, so we would call `read` on a string as well as calling `read` on the integer. For non-toy examples it's highly domain specific; maybe they are images and we can look at the image header; maybe it's a path into a DB and we need to follow the path to see what type of object we end up with; there are lots of possibilities.
Difficult to explain without the analogy message/request, method/requesthandler object/service and session-state/object-state. 
I think OP is talking about hackage not building docs, and only seeing the description page.
What nonsense. 
Don't forget to click on the readmes! https://hackage.haskell.org/package/tagsoup#readme for instance has example usages. Additionally, every function has documentation with it. Looking at it, this seems like a pretty good example of documentation to me. Maybe what we really need is some walkthroughs on how to read it. For instance: * https://hackage.haskell.org/package/tagsoup-0.13.5/docs/Text-HTML-TagSoup.html at the top says to see parseTags if I want to parse HTML 5. Great! I click on parseTags to see what it does. * https://hackage.haskell.org/package/tagsoup-0.13.5/docs/Text-HTML-TagSoup.html#v:parseTags takes a StringLike and gives me a list of tags. What the heck is a stringlike? I click on the type to see what the instances are. * https://hackage.haskell.org/package/tagsoup-0.13.5/docs/Text-StringLike.html#t:StringLike tells me they are String, ByteString, and Text. Okay that seems reasonable. Any of those are valid arguments to parseTags. * So back to parseTags. It gives me a list of tags, but what is a tag exactly? Again, I click on the type. * https://hackage.haskell.org/package/tagsoup-0.13.5/docs/Text-HTML-TagSoup.html#t:Tag tells me that a Tag is a single HTML element, but that's only partly true. Looking at the definition, I see that actually it is an open tag with attributes, a closing tag, a text node, a comment, a parser warning, or a tag position. * I also note the Extraction and Combinators sections from the table of contents. Depending on what I want to do, this handful of functions pretty much tells me how to use the tag list to do it. If I want to grab all the h1 titles in a page I can do `fmap (innerText . takeWhile (not . isTagCloseName "h1")) . sections (~== "&lt;h1&gt;") . parseTags`. `~==` is kind of wonky (as it's a "fuzzy match"), but it's wonkiness is documented there. One thing to note is that the utility of the sections function may not be immediately obvious, but in the examples the intention is made clear. Another thing is that my header example is imperfect as it doesn't handle nested h1 tags. It looks like the library support for that is currently underway (the TagTree module), so it's unsurprising that the documentation isn't mature.
I have to second this. I've had several setups crumble under my feet due to various dependency versions changing, and as a result I often end up using different editors entirely just to get some semblance of a clean workflow working again. Stack does seem like it should be able to help with this, and I'm glad it's been mentioned! Also, the project seems amazing. I hope it really works out :D
&gt; It has everything to do with the type system. The language guides you towards solving problems a particular way. Just like Java will guide you towards using classes everywhere, and Clojure will guide you towards using data structures, Haskell guides you towards its abstractions. This is very true.
I've also seen dynamic language projects grow. My experience is quite the opposite. Practically all projects I work on are broken up into smaller pieces such as modules and libraries. I never make large components more than a few thousand lines that aren't self contained. The smallest unit of composition is a a function. So, anything can be broken up to a function level. You wouldn't write a function that's hundreds of lines long, nor should you shouldn't have namespaces that contain hundreds or thousands of functions. At a higher level you should be creating components that solve a particular problem and have a small API surface then put those together. In fact, this is practically how all the code I've seen written in Clojure is structured in the wild. Most projects consist of many libraries, each having a very small and focused API. Each library can do complex things internally, but I don't need to know how it's doing them to use it. This is how encapsulation works in practice. If you're structuring your system in a way that you can't reason about small pieces in isolation then you're writing poor code plain and simple. &gt;The uncertainty in a large dynamic-language project is a huge cost. This is a completely unfounded assertion. It can be just as easily argued that dynamic languages encourage you to break things up much sooner than statically typed ones. In a sense, the type system gives you more rope to hang yourself with as your code grows. I've seen plenty of static projects grow as I've worked with Java for over a decade. It's very common for them to becomes practically impenetrable once the code base grows to a non-trivial size. The IDE lets you jump all around the project, and the type system ensures that the project compiles and runs. However, actually understanding what's happening and why is a huge mental cost. This truly does become staggering for anybody trying to work with them. &gt;Also, despite all those tests, a serious large project where failures are expensive is just wary of making serious changes, even with the testing. The uncertainty in a large dynamic-language project is a huge cost. Perhaps you should consider the fact that if this is a common issue for you, then your projects are simply structured poorly. Having high degree of coupling between components is a very bad practice regardless of typing. The fact that seeing such large monolithic projects is common in statically typed languages is actually damning for static typing in my opinion. The type system allows the developer to keep writing code without having to stop and break it up. Once the system keeps growing it becomes harder and harder to reason about everything fits together. The fact that it still compiles and runs doesn't really help there. 
Yeah. The author wrote a library and gave it to you for free. Then the lazy bum didn't do even more work by writing documentation that she doesn't need but that you would find helpful. How dare she. Next time, instead of posting a library that you might find helpful but having the temerity to not document it as you would like, she should just keep it to herself. Really. Stop calling volunteers lazy. 
I haven't followed CF or Bluemix buildpacks, but aren't the CF buildpacks just Heroku buildpacks (most of them seem to have been forked off from Heroku)? Would https://github.com/mfine/heroku-buildpack-stack work? Or maybe serve as a starting point...
Thanks! Your library is a great example of dealing with sensor values like oscilloscope readings, I'll read through it!
I can guarantee they'll need the documentation for their own library when they want to use it in another project 6 months down the road. If there is no documentation then the library should be marked alpha.
I don't see it as zero-sum? after a long day, I like to skim through any commits or issues and write my thoughts down. not saying everyone's brain works like mine, but (especially for volunteer projects) it's not always what beds to be done as what is enjoyable to do. and I love talking about my code more than writing it, much easier :)
beta and eta are the two components of a proof theory/type theory which determine whether a connective/type operator is acceptable or not. they're also called local soundness and local completeness. these are the minimal properties required for connectives to make sense. the version described above is a more general form that's derived from the true form as a meta-theorem, but isn't the one that's given/required when defining a type system. you could replace the local form with the more general form, and recover the local form as a special case. here's the general form, for sum types: given G !- M : A + B and G, d : A + B !- N : C assert G !- [M/d]N = case(M; x.[left(x)/d]N; y.[right(y)/d]N) : C and now as a special case, let `C = A + B` and `N = d` to get G !- M = case(M; x.left(x); y.right(y)) : A + B I'm not saying you couldn't chose the general form, it's just not what's presented in the core literature, and is rather more powerful as a reasoning tool, because it allows you to equate way more things. It's also nasty to use because you have to chose N. but, failure to have the general form as provable in your judgmental equality isn't an issue. it's almost never provable internally, and doesn't need to be.
You can do this kind of stuff with PhantomJS and CasperJS, but I don't know how to do it in Haskell.
I think you're looking for the [webdriver](https://hackage.haskell.org/package/webdriver) package, which allows you to use Selenium from Haskell. The readme includes an example. Not in the docs, but according to [this thread](https://github.com/kallisti-dev/hs-webdriver/issues/62) on github, you can use it with PhantomJS as well.
As someone with none of the experience you ask for, I'd say try it, benchmark and adapt if it turns out to be too slow. ;) My guess would be that the documentation author simply forgot to mention another common use case besides small arrays (or didn't consider it important enough).
I didn't know about that! It looks great.
Updoot in 12.333333 (repeating of course) seconds or fukbois will steal you're calsium and give u bad bonez!!!!!1!!!!
Thanks a lot! &gt; A corepresentable profunctor is one where p a b ~ (f a -&gt; b) In terms of folds, does that mean any "streaming fold" corresponds to some function that consumes a whole "container of stuff" in one go? Would it be fair to say that the isomorphism converts between "streaming" and "non-streaming" representations of folds?
The sense of self entitlement that people have when they are using things provided to them for free always astonishes me.
Yep. Viewed this way `cosieve` / `index` let you run a fold by feeding it inputs in the "best" form it can accept all at once.
They can put it on Github, sure, but I can't imagine deploying a library to hackage or Clojars or Hex without documentation, but that's just me.
While you technically could you really don't want to do that. HIE depends on ghc-mod internally so you won't get any speed advantage. AFAIK hdevtools is mostly copypasta from a very old version of ghc-mod that added a server-client mode of operation for speed which has since been added to ghc-mod anyways but that's besides the point.
I think the type of `(listToMaybe . scan mediaRegex)` should be `String -&gt; Maybe (String, [String])`
It'd also be useful if we could find uses of the code in other libraries. Preferably automatically.
Oops, thanks, fixing this now! What I get for typing rather than automatically generating everything involving code.
I'm an advanced haskeller. Today I lost half an hour trying to figure out how to run a parser and match a trivial thing. This was with megaparsec, which even has some docs. Still, there is no small comprehensive example that just runs a parser against a static input and matches something. I had to do some type research, which is just so much slower than seeing an example. 
&gt; find the first newSTRef, translate as above, the repeat recursively for block b. My point is that that recursive repetition isn't valid unless you use the more general isomorphism I posted earlier. So you don't get a good enough induction hypothesis unless you use that general one. &gt; That type is not under consideration for this isomorphism. Why not?
This is a great idea! Especially if hackage could have a way of browsing them easily. I think all of my packages have examples in the repositories, and I actually prefer them to inlining examples into haddocks because they can actually be automatically built (I do inline haddock examples too sometimes, and have even done .cabal description examples, because it's such a better user experience, but damn is it a pain). 
Typically an end-to-end example
Yeah, I meant to do that, and just did. Thanks!
You need to either add {-# LANGUAGE OverloadedStrings #-} on top of your module. Or add type to your line: searchInput &lt;- findElem (ByCSS ("input[type='text']" :: Text)) and also add import Data.Text (Text) in your import list. 
with codata, you dont care about termination tho, only productivity, and in fact often you *dont* want termination, because that would represent a failure to behave properly (eg w/ a web server)
This is one of the handy things about unit tests, theyre kind of like built in examples
`&lt;$&gt;` is fmap operator. 
`&lt;$&gt;` is `fmap`. `&lt;*&gt;` is either `ap` as per the other reply there, or the TIE operator.
Thanks.
i'd say that productivity and termination can only be conflated if you interpret termination as reaching WHNF, but that you should avoid this conflation in formal contexts. better to simply say, termination isn't relevant to codata in any important sense
separately tho, regarding the paper, it looks quite good! i'll give it a read :)
You're cruising for a ban here.
"library authors feel entitled to have people.." what the what? library authors don't typically go around demanding users...
Assuming the example perfectly matches your use-case, yes. If it doesn't you're going to spend the same time tutorial-reading. ;)
Your standalone stack program needs more info on the second line, for instance: #!/usr/bin/env stack -- stack --resolver lts-3.2 --install-ghc runghc --package pcre-heavy Worked for me. Unless of course you are using something newer than stack 1.8 and it doesn't require this.
We call the `&lt;$&gt;` applicative fmap at work. `&lt;*&gt;` is frequently called 'why would you put this operator here goddamnit'.
Just a list of libraries that depend on the current one (the reverse of the relation easily findable on Hackage) would be really cool. Maybe it exists and I just haven't found it.
Great post! It's cool u don't budge to the critics and instead double down on sticking to the best tool for the job: Stack! Maybe u should also rename the series to "24 days of (Stack)age" as Stackage is to Hackage what Stack is to Cabal!
Only if they want me to use their library. I think that library authors who don't help me should be unsurprised, indeed, should be perfectly contented with no cause for complaint, when I choose not to use their library. And I think that library authors who want to see their libraries used widely should make it as easy as possible for me and every one else to use their libraries. And I think that a language community that wants to see its language used widely should have a culture in which library authors are expected and encouraged to foster use of their (and its) libraries. Which of those do you disagree with?
I still save the half hour because it is the most basic thing - run a parser and match something. I'll have to do that no matter my use case.
Is [folds](https://hackage.haskell.org/package/folds) meant to supercede [foldl](https://hackage.haskell.org/package/foldl)?
Not sure about `&lt;*&gt;` but I've always liked left sparrow and right sparrow for `&lt;*` and `*&gt;` :-)
Perhaps your impression of static languages is miscolored by the terribleness of Java. Even if you build a large dynamically typed project from modular pieces, you still end up having wide-reaching changes if you decide to slightly change the architecture, or the API of a commonly used component. These changes are scary, when you don't have full coverage of every single component that directly or indirectly uses these components. I've seen people struggle with trying to figure out dynamic code, and even giving up. Things like def __init__(self, port, ...): ... What's port? If they search the codebase, they find ~22 classes like: FCPort, EthernetPort, etc. and not all of them conform to the same "duck type"! If they try to find all callers, they find that the callers just propagate this port from further callers, and by the time they reach downwards towards code they can understand or upwards towards code they know -- they've completely been lost. Haskell projects tend to be anti-monolithic and very broken up into tiny libraries and components. To break things up you often need to level up abstraction. High levels of abstraction are extremely painful when you don't have type-documentation, and easy in Haskell.
You should encode which lts to use with stack for your script, otherwise it may break in the future with a newer ghc or version of pcre-heavy. 
Regardless of the fact if it works, it's probably a good idea to specify the resolver. Otherwise your script might break when new snapshots come out with never versions of e.g. `pcre-heavy`.
&gt; I’ve started to migrate my former Travis setups based on multi-ghc-travis to use Stack instead. Care to elaborate on this? I have an as of today unresolved (SO question)[http://stackoverflow.com/q/29516932/383508] that I would gladly update with an stack based answer. 
&gt; The only time I notice slowness these days is when the solver is having trouble finding a valid build plan for a project with a very large set of dependencies. Based on this I decided to do a quick test. Tl;dr; it works pretty well, but stack fits better. What I did: convert our custom snapshot to a cabal freeze file. Convert our `stack.yaml` into a set of `cabal sandbox add-source` commands. Install one of our servers using this. Try to load it in a repl. Try to load it in a repl together with one of the libraries it has as a dependency. I did hit a few snags. The first was when there was a parse error in the cabal freeze file. The error message just said that it failed to parse on line 1. I had to resort to some manual binary search to find the place where my search-and-replace had failed. Then my install failed 3/4s of the way through (unrelated to cabal). When I fixed the issue and continued, it had forgotten a lot of the things it had already installed. I ran into the issue I linked to earlier! Your workaround (adding snapshots) defeats the purpose of why I want to add-source things (I want to reinstall when they change) so I just slogged on. For the repl, I had to 'cd' into the directory of the executable. This meant I had create another sandbox there, pointing to the original top-level sandbox. I'm not sure if this will pick up the freeze file at top level as well, but since everything was already installed, it worked. I also checked the no-op build times. For cabal it was 9 seconds, for stack 5. Both are not great, but reasonable. The freeze file helps a lot, I remember before we used that it was 30+ seconds. I couldn't manage to load two packages into the repl at the same time. I tried various incantations, but didn't succeed. I could do a manual call to ghci pointing it to package dbs, source directories and cabal macros, but that kind of defeats the purpose: I want to use a tool, not build it. So my overall conclusion: cabal works, but stack supports this kind of multi package workflow better. It works in a sandbox by default, so you have less steps to perform and less setup. It has a configuration for what things are part of the project, so you don't have to script the `add-source`s. It has a multi package repl. You can run commands from anywhere in the project, no need to cd. It shares snapshots for quicker rebuilds. It tracks file hashes instead of modification dates for less rebuilds. I'm not saying "cabal sux0rs". It has served us well over the years. But I'm not married to it, and for my use case, stack is currently just a better tool. I'm all for competition, it will make both tools better, which is why I'm sharing experiences like the above and filing tickets. But I think we should recommend stack to experienced Haskellers and to industry as well. It's currently the better tool for them.
Thank you for putting the limitations of your approach, together with relevant 'bigger picture' pointers. I feel that’s too often missing in articles.
Fixed it. there was a `stack.yaml` file in my homedir which I had to remove.
Some people call those "Swierstra star" and "Swierstra florin". http://dreixel.net/research/pdf/eyawkad.pdf
I think this is a tradition for any 24 Days series author ;)
These people deserve what's coming to them.
**foldl** is focused in a single useful variety of fold, which makes it more approachable up-front. **folds** feels more like an exhaustive exploration of the design space. **foldl** also comes with many "utility folds", and supports "folds with effects".
They are not (always) needed, but they are very important for the developer. It shows your intent. By saying `count :: Char -&gt; String -&gt; Int`, you say, "this is a function that takes a Char and a String, and returns an Int". You could have made it more general, by saying (I'm a but rusty, so it might not be correct) `count :: (Eq a) =&gt; a -&gt; [a] -&gt; Int`, or "This function takes a _thing_ that can be compared to another _thing_ and then a list of those _things_ and returns an Int". And besides that, shorter code is not always better code. Even in functional programming.
OK, in your context then that's fine. Code written for research purposes is different from code written in industry, which is my context. That said, I have been paid to take code written by researchers and turn it into a commercial product and at that point, by god I wished the researchers had provided more supporting material. But I understand your point. However, putting some code up on github or wherever on the off chance that it will be useful to someone is very different from publishing a library intended for use. I wouldn't call what you're doing publishing a library, and I wouldn't expect your not-a-library to come with supporting material. A library which has be published for people to use, I do.
Here's an interesting thing about people: they are, in general, very, *very* good at doing induction from examples. Much, *much* better than they are at doing deduction from rules. That's why giving examples, even when they aren't an exact fit, is enormously helpful.
Agree. I tried building it and there's still a rabbithole of half-solved problems with GHCJS compatibility, at least for OSX.
[removed]
You need to start up selenium on your local machine - I believe selenium listens on port 4444 by default 
I must insist; I still do not agree with this "once you're comfortable with following them". I think that, to many people, a programming language is not a life philosophy but merely a tool to get something done. Haskell is growing quickly, and we have to admit the possibility of a user base who simply wants to throw a quick hack together.
This issue comes up all the time, and it is an argument that no one can argue against. We consistently have beginners wanting more/better documentation = we need it. That's all there is to it.
Can you give a concrete example of code?
Are you trying to tell us that Haskell is not an organized religion?
Those relations do not hold automatically. Rather, it is your responsibility as someone implementing both Monad and Applicative instances on the same type to make sure they hold. Usually, if you are able to implement a Monad instance for a type, it is quite trivial to get an Applicative as well, by just setting: instance Applicative MyType where pure = return (&lt;*&gt;) = ap That way, you can be certain that the relations hold. But other (semantically equivalent) implementations of those functions could be picked for performance reasons.
yeah, but *why* should I make sure they hold? Suppose I tell you: I don't want them to hold? Why is that bad (formally)? Does it break an axiom? Does it break any laws? Is `&lt;*&gt; = ap` just convenience? The `either` issue I linked above points at some of the underlying concern, but I'm not sure if I see this rule stemming from the laws or axioms of monads (that I can tell - I tried to produce a counter-example but was unsuccessful).
Yes it does, which makes me think there is some strong foundation or support for this law, but where is it? Is there an `ApplicativeDo` paper somewhere?
Probably. And? Why is 30 minutes a magic number? The question for the library author is: how much do I want this library used, and therefore how much do I want to invest in helping that happen. “Not much and not much” is a reasonable answer. “Lots and not much” isn't.
Right. I get that, but I re-iterate the question: why? You say *convention* and I will ask: how did this convention come to be? Who sat down and said: "ah, `&lt;*&gt;` and `ap` have the same signature, so they should be the same" And when they did that, how did they know such a statement is correct?
30 minutes was the time mentioned in the comment I responded to. But a lot of open source contributors don't really care about having the largest user base -- many of them just want a good library for themselves, and perhaps some users who also can contribute to development. Guess which kind of user is more likely to contribute: the ones who are "just looking to get the job done" by copy-pasting an example or the ones willing to explore and follow the types?
~~No, it isn't.~~ Oh,wait, I see what you mean. Ok.
&gt; Today I lost half an hour trying to figure out It is.
If you define `&lt;*&gt;` as `ap`, then from the `Monad` laws, the `Applicative` laws automatically hold.
To address the edit on your original post: You do not prove laws. You prove adherence to laws. Laws themselves are not derived by proof, but because someone decides they are useful. The class relationship in `base` goes `Functor` =&gt; `Applicative` =&gt; `Monad`. Which just says that an applicative functor is always a functor, or that the functionality offered by an applicative is a super-set of that offered by a functor. Which means that if you express the functionality of a functor in terms of an applicative (which you can -- again, because it is a superset), it should hold `fmap id x = pure id &lt;*&gt; x`. The same goes for monads and applicatives: The functionality of a monad is a super-set of the functionality of a applicative, and you can express an applicative using monad functions: `pure a = return a` and `x &lt;*&gt; y = x &gt;&gt;= \f -&gt; fmap f y`. Just because we can't express certain things about category theory purely in types and have to rely on "convention" doesn't mean you can just go and disregard that. The laws between type classes are just as important as the laws of the individual classes.
Perhaps. Which is why examples and other kinds of documentation are really important. But it doesn't mean the author is obliged to write less of the code they need and more of the examples other people want.
Very true, but: for so long as Haskell library authors, and I'm talking here about libraries intentionally published for use, not punting some code on github because “why not?”, for so long as authors of published libraries believe that helpful supporting material is a very, very low priority so uptake of those libraries and by extension the language, will tend to be very slow. And these library authors, and the Haskell community at large, can decide that they're happy with that trade off. But if so, then they need to stop whining about slow uptake in industry. Can't have it both ways.
I think it is because the applicative typeclass is historically younger in Haskell than monad so ap and return are, in fact, &lt;*&gt; and pure in the sense that before Applicative was introduced as a superset of monad, you had to go from functor to monad so everything in between was included in the monad instance. in other words, the monad typeclass implied the applicative one and ap and return are this implied functionality. Now that applicative is here, these functions are still there but refer to the same concept from a different historical point of view. You could say they are legacy in this sense. It's the same as, for example, + and &lt;&gt; on SumInt. The two functions must be equal because they refer to the same concept. If you break the equality, you break the math for this type. 
Awesome! I am looking forward to trying this out. Thanks for sharing.
Damn. So the setup does not work as described on OSX?
Ugh, I need a better theme for this blog. I don't know when I'll be able to get around to that.
I like "brackety splat". Better than blue-tie, white-collar `ap`, or "spaceship", as someone else cals it.
Yes, I went and did this.
I think the simplest answer is that we want the following two snippets of code to mean the same thing: foo &lt;$&gt; bar &lt;*&gt; baz do x &lt;- bar y &lt;- baz return (foo x y) Nothing will magically break if you don't make this identity hold, but it will make your code quite confusing. Taking it a step further: now that `Applicative` is a superclass of `Monad`, many combinators are being generalized to `Applicative`, and if your definitions of `&lt;*&gt;` and `ap` give different results, your code won't be future proof. If your next question is "why generalize those cmbinators?" the answer is that it avoids us having to define parallel definitions of a lot of helper functions all over the place.
I didn't know about ghc-mod's server-client mode. Is it really as fast as hdevtools?
Thanks! I've also been using luigy's stack.yaml file and reflex development with stack has been a joy. I apologize for taking a lazy shortcut here (when stack makes it so easy to try it directly) but does the version of reflex-dom you're linking to (on improved-base) include the recently added [Websocket support](https://github.com/ryantrinkle/reflex-dom/blob/develop/src/Reflex/Dom/WebSocket.hs)? I haven't kept up with the github issues between ghcjs/reflex/stack and unfortunately have become confused about what currently lives where.
It should be `return (foo x y)`, right?
Adding "I'd almost say" before something you say doesn't magically make it not said, or less wrong.
&gt; I think that library authors who don't help me should be unsurprised, indeed, should be perfectly contented with no cause for complaint, when I choose not to use their library. So a lot of library authors have complained to you, individually, regarding your not using their libraries?
It's not, I've used Haskell briefly about 6 years ago and Scala as well. The type system just gets in your way differently. I'm not talking about verbosity of having to type annotations here by the way. It forces you to think and solve problems in specific ways. &gt;Even if you build a large dynamically typed project from modular pieces, you still end up having wide-reaching changes if you decide to slightly change the architecture, or the API of a commonly used component. My experience is that this tends not to be an issue in practice. By the time your system has grown to such size it's not likely to have drastic changes. In a sense you really should be making a new system if you're in that scenario. &gt;These changes are scary, when you don't have full coverage of every single component that directly or indirectly uses these components. Again, changes aren't scary, at least in Clojure, because most concerns tend to be local. It's very rare in my experience to have a global change of the kind you're describing. On top of that you still have integration and acceptance tests that tell you whether the system does what's intended or not. &gt;What's port? If they search the codebase, they find ~22 classes like: FCPort, EthernetPort, etc. The problem here seems to be with OO style more than anything. Incidentally, there's a great talk from Rich Hickey discussing the problem [here](http://www.youtube.com/watch?v=VSdnJDO-xdg&amp;t=48m0s). However, I think that fundamentally the uncertainty is a matter of taste. If you don't feel confident that you understand your code and it does what you want without types, then you shouldn't use dynamic typing plain and simple. However, the part you seem to struggle with is that other people think differently from you. I don't have this anxiety about it that you appear to have, and clearly many others don't either. When I see a function I'm not familiar with, I'm not just going to see it out of the blue by randomly opening a file in the project. I'm going to usually have a context as to why I'm looking at it and what it likely should do. I also have the REPL where I can quickly run it to see what's happening. I've never found this to a problem for me. &gt;Haskell projects tend to be anti-monolithic and very broken up into tiny libraries and components. To break things up you often need to level up abstraction. High levels of abstraction are extremely painful when you don't have type-documentation, and easy in Haskell. If you break your projects up, and you agree that it's a good idea to do so, then I'm not sure why you use the argument of "it's hard to maintain a large monolithic project" in the first place. In fact, it's a common argument among static typing proponents, are you basically advocating papering over bad code by using the type system? 
Great! It's still missing from the complete example though. 
"is isomorphic to"
Huh. I have no idea. :/
This could do with some evidence - can you provide an example of some equational reasoning that requires the identity `&lt;*&gt; = ap`?
Indeed.
This follows from the fact that every functor in Hask is strong. You can use `strength` twice (once swapped) and then `join`: (T a, T b) -&gt; T (a, T b) -&gt; T (T (a, b)) -&gt; T (a, b) This is why the canonical example at the end of something that isn't a monoidal monad winds up being a monoidal monad in Hask. ;)
I suggest making your non-cache fields strict, but leaving your cache fields lazy. (In C++ you cache fields might be `mutable` to allow them to be modified even when part of a `const` object, and in Java your cache fields would likely be `transient`, if they are easier to re-calculate than serialize.)
Users expect to be able to refactor code from do char '(' x &lt;- p char ')' return x to char '(' *&gt; p &lt;* char ')' If your `(&lt;*&gt;)` isn't indistinguishable (up to performance concerns) to `ap` then this isn't a legal transformation! And then we have to maintain multiple versions of everything. We lose the fact that these things are equivalent as they'd have observably different results. The key observation is that without these conditions, then despite the fact that we made `Applicative` a superclass of `Monad` WE'D NEVER BE ABLE TO USE THIS FACT to eliminate any code without reasoning on an ad hoc basis instance by instance. Literally no code that used `(&lt;*&gt;)` in lieu of `(&gt;&gt;=)` could be trusted if it was written polymorphically. You won't be able to find equational reasoning stating `(&lt;*&gt;) = ap` must hold, it is a law we added by asserting that Applicative is a superclass of Monad. After all, we had to state precisely how the two classes related. The 'coupling' between the classes is the laws requiring `pure = return` and `(&lt;*&gt;) = ap`. From that `(*&gt;) = (&gt;&gt;)` follows, etc. and the `Applicative` for any `Monad` can be shown to be equivalent to the canonical one you get by taking the default definitions: instance Applicative Foo where pure = return (&lt;*&gt;) = ap since all of the other operations are definable in terms of these. This with fits pre-existing practice and user expectations. It is enough to ensure that the e.g. Applicative for [] isn't the `ZipList` one as it is incompatible with the `Monad`, and that `traverse = mapM`. Theory to motivate this: Monads are monoids in the category of endofunctors if you equip the category with functor composition as its tensor, lets call this category M. Applicatives are monoids in the category of endofunctors if you equip the category with Day convolution as its tensor lets call this category A. There is a canonical monoidal functor from M to A that says that every Monad is Applicative. These laws state that the instances follow suit. See section 8.3 of http://www.fceia.unr.edu.ar/~mauro/pubs/Notions_of_Computation_as_Monoids.pdf
Speaking to `$`, I'd love it for Haskell to adopt PureScript's syntax around lambdas and do blocks to not require it in those cases. The parentheses are pretty ugly and difficult to keep track of, and `$` isn't much better.
Ugh. Just fixed this. Also noticed the syntax highlighting was all messed up too. Been away too long from writing, and the upgrade to blog generator changed things.
Haha, out of curiosity, did you happen to pick that up from me [last time](https://www.reddit.com/r/haskell/comments/241jcm/how_do_you_say/)? 
Is that just syntax highlighting?
&gt; I think human cognition needs both. I don't disagree. My intent was not justification, but explanation. If we can identify causes, we can hopefully find ways to address them.
&gt; which makes me think there is some strong foundation or support for this law http://www.fceia.unr.edu.ar/~mauro/pubs/Notions_of_Computation_as_Monoids.pdf section 8.3 provides the canonical construction of `Applicative` in terms of `Monad` in fancier language.
The short version of it is this: Without this law you must maintain two versions of every single combinator you'd ever write with an `Applicative` constraint and make sure you call the right one at all times, and with `Applicative` as a superclass of `Monad` you'd never get any sort of warning if you screwed up and called the `Applicative` one when you meant to call the `Monad` one. You can't prove that `(&lt;*&gt;) = ap` must hold given the other `Monad` and `Applicative` laws in isolation. This law is the very thing that couples them and prevents this 'twinning' of every definition and makes it so that the fact that `Applicative` being a superclass of `Monad` is actually something you can use.
&gt; You won't be able to find equational reasoning stating (&lt;*&gt;) = ap must hold, it is a law we added by asserting that Applicative is a superclass of Monad. I actually meant "can you show some equational reasoning that only holds *if* you postulate this law". But thanks, this is a great answer!
You can also type up this example without using the ST or IO Monad: {-# LANGUAGE ScopedTypeVariables,GeneralizedNewtypeDeriving #-} import Control.Applicative import Control.Monad import Control.Monad.State newtype SS a = SS (State [SS ()] a) deriving (Monad,Functor,Applicative) type Ref = Int newRef :: SS () -&gt; SS Ref newRef v = SS $ do x &lt;- get put (x ++ [v]) return (length x) writeRef :: Ref -&gt; SS () -&gt; SS () writeRef r v = SS $ do x &lt;- get put (take r x ++ [v] ++ drop (r + 1) x) readRef :: Ref -&gt; SS (SS ()) readRef r = SS $ (!! r) &lt;$&gt; get foo' :: SS () foo' = do x &lt;- newRef $ return () writeRef x $ join (readRef x) join (readRef x) runSS :: SS a -&gt; a runSS (SS x) = evalState x [] main :: IO () main = print $ runSS foo' The difference is that we now only can make variables of type SS(), but it still loops. I'm confused now. How can this loop without using a fixpoint anywhere?
&gt; Package sets are much closer to a solution here. By what metric are they closer? I work on a daily basis with cabal, and it just works for me. &gt; Newcomers are ill-equipped to decide for themselves which tool will work best for them by the very definition of the word 'newcomer'. This is patronizing ;-( Even in occasions when I'm a newcomer I don't want to be treated like stupid. If there's two ways to accomplish something in different ways, please tell me the two options and what the respective tradeoffs are. And let me choose. But it seems to me that newcomers will get caught in the middle of a political dispute where each camp wants to force their holy tool upon them, rather than objectively present all options in a friendly manner. &gt; All that only seems to support my conclusion that it must be more difficult to add features to the cabal-install code base than to a new one like stack. I'm not sure about that assessment... to me it seems rather that the Cabal project is solving algorithmically harder problems than Stack is, while at the same time has less developers contributing than Stack has. It's a a bit unfair imho. So it's not harder to add features, but rather the features to be added themselves are more complicated.
Yes, good catch, thanks.
Answering my own question: contravariant recursive types can lead to non-termination. newtype SS a = SS ([SS ()] -&gt; (a, [SS ()]) is contravariant
As far as which sorting algorithm is the best fit, I don't know. Based on the other things you mention, here is some other advice (which you may already be aware of): - Since you are dealing with key-value pairs and sorting by the keys, you will almost certainly want to use `hybrid-vectors`. - For the timestamps, storing them (in the vector) as `Int`s will let you use a primitive vector for that part, which is a big win. Ed explains how `hybrid-vectors` works [here](https://www.fpcomplete.com/user/edwardk/revisiting-matrix-multiplication/part-3).
Wish (sort of) granted: https://github.com/gibiansky/ghc-reskin (if you don't mind running all source files through a preprocessor, which I'm sure you do). But on the more serious side, there's a [proposal to allow this behavior](https://ghc.haskell.org/trac/ghc/ticket/10843) with an extension. Don't know if it will happen though.
I haven't seen that, sorry. My guess would be that your gcc version doesn't have objective-c support. Perhaps you installed one via homebrew? Make sure you have latest XCode and that you aren't using gcc46 from homebrew or w/e. Here's my gcc for reference: » gcc --version Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.11.sdk/usr/include/c++/4.2.1 Apple LLVM version 7.0.0 (clang-700.1.76) Target: x86_64-apple-darwin15.0.0 Thread model: posix If you *are* using `gcc-4.6`, perhaps reinstalling with `--enable-ojbc` would work.
I'll have to do some testing to figure out exactly which stack versions will work. The version installed by brew (1.4 last I checked) is not sufficient, but 1.8 might be.
I thought there was a proposal somewhere! :D
Here it looks like the culprit is the SS type. It has the form of a rose tree, and I know those are hard to prove termination for, since they are not structurally recursive. My guess is there might be something funny with that, and with Haskell's implementation of State.
that's what I thought, just wasn't sure
I suggest ["akimbo"]( https://en.wiktionary.org/wiki/akimbo#/media/File:Akimbo-photo_(PSF\).png )
No. I've actually run into (somewhat fancy FRP-related) examples in the wild where one could define both an Applicative and Monad instance where `(&lt;*&gt;)` had different semantics from `ap`. Vaguely, this was for widgets that continued running in parallel with each other and producing results over time. Using the Monad instance would result in switching behaviour, where every time the left hand widget changed which function it was producing, the right hand widget would be restarted (having its internal state reset), while there was a way to implement `(&lt;*&gt;)` which would start both widgets and simply combine their results with no switching. It's possible to argue that maybe the Monad instance was a bit superfluous, as the Applicative was far more important in that setting, but the Monad operations did see some use.
&gt; By what metric are they closer? By the metric that simply allowing you to install incompatible versions of packages will produce results where something tries to link both into the same binary while package sets avoid that problem completely. Package sets are the accepted solution to that problem that works, as shown in pretty much any non-rolling-release Linux distribution of the last twenty years. &gt; I work on a daily basis with cabal, and it just works for me. I used to work on a daily basis with cabal-install too. It works okay in single developer scenarios if that developer doesn't mind bumping the occasional bound or fixing the occasional breakage due to missing bounds. It does not work well at all once you introduce multiple systems that need to compile the same code as you then end up with wildly different dependency sets. Newcomers are not stupid but newcomers are not invested in Haskell. If you tell them that they need to study all their options for a few days before they can write their first Hello World program most of them will just tell you that Haskell is too complex and not worth their time. &gt; it seems rather that the Cabal project is solving algorithmically harder problems than Stack is What is algorithmically harder about the examples I mentioned earlier in this thread? Secure or incremental downloads are hardly a more complex feature. About the only thing cabal-install (as opposed to Cabal which is used by both tools) does that is algorithmically hard and that is missing in stack is the dependency resolution. Here the last innovation was the modular solver as far as I can tell from using cabal-install. This was a big step forward but it was also quite a while ago.
Right. For example, here are two Applicative's for list: instance Applicative [] where pure x = [x] fs &lt;*&gt; xs = zipWith ($) fs xs instance Applicative [] where pure x = [x] fs &lt;*&gt; xs = [ f x | f &lt;- fs, x &lt;- xs ] but afaik the first doesn't correspond to any Monad for lists.
&gt;When you make nice little components in dynamic languages, the interface still isn't typed and often not documented (or doc goes out of date). Again, this is completely counter to my experience. Most Clojure libraries I've used have very small API surface. It's trivial to understand how to use them. Again, functional style and the REPL play a big part here. I've never used Python professionally, so perhaps the problem you're describing is common there. &gt;So when you want to change a nice isolated component to have a new feature, and change the users you have to try and understand this interface. But if you have an objec/value passed on and on from somewhere else through various other well isolated components, you'll have a hard time deciphering the interface you have there. Again, this seems extremely contrived and does not match my experience. If this was the case then I would've moved to static typing years ago. &gt; Since interfaces can easily reexport things from other components interfaces, the claim that you only need to understand and look at a single interface is wrong. It is true, however, in a static language. Because the interface is typed, it is a clear boundary that you can stay within. Functional languages are data focused. Each function, whether it's one you wrote, from a library, or a component, simply takes an input and produces an output. The application is just a chain of these functions. Any time you change one it's very easy to see what's affected. It has simply never been an issue for me working with Clojure. I have to ask why it's so difficult for you to accept that somebody might have a divergent experience from you working with a dynamic language? Plenty of people out there feel exactly the same way I do. We work with dynamic code every day and we don't have the problems you have. It's not simply ignorance as many people who prefer dynamic typing have worked with statically typed languages. Is it really so hard for you to comprehend that people might work in different ways and have different pain points?
&gt; A ghci-ng plugin is on hold pending the Remote GHCi work Simon Marlow is doing. Simon Marlow working on an nrepl? sweet.
Since I used dynamic typing for a long time and was oblivious to the pain, and since I know most people judge static typing by their experience with Java, it is much easier for me to believe it is obliviousness than a conscious trade-off.
Interesting. Can this be implemented as do -&gt; $ olddo?
You can believe whatever you need to believe if it helps you feel smug about your life choices. Since it appears that your judgement of dynamic typing is based solely on your experiences with Python your choices seem equally oblivious. Have a good day.
Dang, that's very very strange looking to me. I guess GHCi is a bit of a language of its own in that sense, since you gotta "let" everything
Publicly releasing a library out in the open without documentation wastes a lot of people's time because thousands of people will spend half an hour trying to figure out if the library meets their needs or how to use the library. These undocumented libraries are a drain on the community because each one wastes a large number of potential Haskell contributor hours that could have gone towards improving the Haskell ecosystem, tooling, or libraries. So even if you don't expect other people to use your library you should still document it, if only to steer people away so that they don't waste too much time to realize they don't need it.
&gt; Since I There, this is all about and you alone, you are free to limit yourself to that and respect others preferences.
AFAIK, 1.6 is not sufficient. Support was added in 1.7, which became 1.8.
https://github.com/timsneath/haskell.tmbundle/tree/master/Commands Looks like there might be a little more to it than that.
Indeed, and (as you pointed out) since there exists a second, law-abiding Applicative instance distinct from the usual, law-abiding Monad instance for lists, we know that you can abide by all Applicative and Monad laws but yet not have &lt;*&gt; = ap.
well then is this defined: `&lt;**&gt;`
I'm sort of maintaining a fork of lost children [here](https://github.com/acowley/ghc) just waiting for the social tide to change a bit more. I'd like to include the current state of `ArgumentDo`. If you make a PR, I'll merge it. If not, I'll come knocking to ask for permission to extract the patch from Phab at some point.
I'm really enjoying your balanced discussion of the pros and cons of syntax and abstraction. &gt; Unfortunately, this refactoring, although good in some ways, came with a cost. It doesn’t look so great to me. Does it to you? I found myself thinking exactly that seeing the refactored code, and I was glad you identified it - I feel that Haskell gives you so much power that it's easy and attractive to abstract everything, but at the cost of forgetting what you were doing in the first place. Great power, great responsibility etc.
It's for fuzz testing, which basically means to feed "fuzzed" or random data to a server. The way these tools work is either reading in test models or generating their own then screwing with the data. At least thats my understanding... If I'm offbase somebody please educate me :) 
That said, you could view this as much the same situation as the ill-fated `Concurrently` monad in `async` (which is being removed in favor of folks putting on and taking off the newtype as needed.)
Ew. I've never been a fan of in-your-face IDE experience and my experience with haskell so far is that you spend a lot more time just thinking about code before even writing it, not to mention how terse it is, which makes the actual writing of the code like solving an equation as oppose to writing a bunch of syntax wrappers for your classes etc.
ap. But you could also sound it lang-star-rang in rangbang. (left angle, star, right angle).
That's the TIE Bomber.
Yes, I've read these. I am unconvinced by the cons and I see pros as substantial. 
I'm surprised there was a negative reaction at all! Having that in purescript was so intuitive to me that it clicked immediately and I couldn't stand the $ anymore. 
Omg ghc 8 is literally Santa Claus
One thing besides edward's answer to note is that the concrete way the lens library works can be explained without reference to these sorts of categorical abstractions in any key way (some names may still pop up) -- what you're seeing here is the way the results are _arrived_ at -- you don't need organic chem to understand a cake recipe so to speak. For example, the _core_ ideas of basic lenses were explained by SPJ, who is terrific at exposition in a clear way, in this very accessible talk, and absent nearly all the jargon: https://skillsmatter.com/skillscasts/4251-lenses-compositional-data-access-and-manipulation
I respect them in the sense that they can do whatever the want, and develop any way they want or find fun. I still believe choosing dynamic typing for anything but small projects is a poor choice for objective reasons.
&gt; If you tell them that they need to study all their options for a few days before they can write their first Hello World program most of them will just tell you that Haskell is too complex and not worth their time. Come on, Haskell is about choice. Just look at all the various library stacks to accomplish the same thing, like e.g. webstacks or database access layers. If spending 5 minutes to inform yourself about Stack vs Cabal is already too onerous and drives people away, then they should rather leave rightaway, as it will only get worse the more they go down the rabbit hole Haskell is. On the other hand, if you insist on hiding choice from newcomers, then, based on my own experience with Stack and Cabal, I would be obliged to recommend them to use Cabal and advise against Stack. So it's in your interest I keep an open mind and tell newcomers about both options and allow them to make up their own mind.
&gt; Nobody is forced to use it Note that I am not arguing for or against the specific proposal but this statement is simply wrong. Everyone is forced to be able to read alternative syntax if some popular packages start using it.
The trouble with testing is that it is very hard to come up with trivial example code that has bugs that are not even obvious in hind-sight. When writing a blog post you want some kind of short code. To make testing seem worthwhile you want non-obvious bugs in the code. Since you want to show the whole process you are explaining the bug too and so the readers don't experience the non-obviousness of the bug in quite the same way as you might before you wrote the tests.
It's a nitpick, but why use the `+` symbol when you're constructing product types?
I thought I saw it on Stack Overflow but could easily have been reddit, I definitely didn't come up with it independently. If you are the origin, I salute you sir!
I guess it's in analogy to binary addition and multiplication.
&gt; We simply cannot assume anymore a user base composed exclusively of high priests and compiler gurus who can typecheck in their heads. The fact is that for most people of the haskell community their job security in academia and their self esteem (which is more important) is good as long as Haskell is maintained artificially as a language very difficult to master. For these people, whoever that makes things easy or simpler is considered an outsider and is silenced. Anything that demonstrates some aspect in which haskell is better and easier for practical problems than other languages is dangerous. Warming the air is the only legitimate activity. 
Testing is extremely useful when you write parsers or printers or any other form of serialization or deserialization to check your edge-cases, when you have non-trivial permission checks or program flow (e.g. you can only call web handler foo on objects where you are in the admin group and the object is currently published), for complex algorithms to check invariants you do not want to check for performance reasons during every live execution of the code,...
http://stackoverflow.com/questions/3242361/haskell-how-is-pronounced
&gt;/r/haskell enters TOP 1K subreddits /r/haskell is playing in the big leagues now! Yessir! It will surely take decades, but I firmly believe that, with the proliferation of concurrent programming and increasing software complexity, Haskell's superiority to current paradigms will become apparent. Traditional imperative techniques already impose a huge cost on the programmer: Even in a single-threaded setting, hidden state, side-effects, and mutable variables all make reasoning about components and testing extremely hard. If you add multi-threading or put the whole thing into a distributed context, you get piles of nigh-untraceable race conditions. Scala, Clojure, and ML ultimately don't solve these problems because they don't stop programmers from shooting themselves in the foot. Sometimes, you really shouldn't give people what they want. A bit of impure code and a few GOTOs here and there all sound like nice ideas until you're woken up at 2 in the morning because overfull log-files have rendered the credit card payment processor unresponsive. Haskell has been the only mainstream-ish language that was principled enough to put its foot down and say "you're not going to mask that readFile as a String, buddy!" In 20 years, we'll all be grateful for that. Vote Haskell.
I always realize this as a sort of baseline anxiety. One you feel all the time while programming, so often you've forgotten it's there. It's the same sort of promise that, say, antibiotics must have given. Prior to that everyone was probably more or less healthy and if you fell sick and were taken care of you could probably survive. But there was always a risk. Then, poof, technology comes in and eliminates a huge amount of that risk magically. I cannot imagine what that must have felt like. People would suddenly be, comparatively, invincible! The weight of death that hovered above has been pushed down to a practically invisible part of daily life. That weight vanishing is the subjective feeling that losing nulls produces. It's probably bizarre to compare null pointers to, literally, death, but I also feel it's only by these overblown comparisons can you begin to identify that baseline anxiety.
Yes, in general I would agree with you. However, this specific case makes the language more regular and it's tiny compared to the other extensions, such as GADTs. With this, you won't even notice the little `$` missing. Also, if authors of popular packages begin using it, I'd say it's a vote of confidence and a good indicator that everybody else should be using them. But without being able to do it, we'll never know.
That's wrong actually,`ap` and `&lt;$&gt;` have different types. `&lt;$&gt;` = `fmap` `ap` = `&lt;*&gt;` 
Can you put this into the [stack-templates](https://github.com/commercialhaskell/stack-templates) repository (should it work reliably enough)?
I'm sorry this article didn't work for you. I should clarify that it was not an article about how awesome testing is. I mentioned that this topic is completely, totally outside the scope of the article. The purpose of the article was to briefly introduce the tool HSpec and show how to use it if you are already interested in testing. My conference presentation on type-directed, test-driven development was unfortunately not recorded, but I should turn that into an entire series of articles some day to justify the use of testing.
It is just a small thing...but if say 20% of all packages use this small thing and 20% (possibly overlapping but not identical) use that other small syntax extension and another 30% that third one,... it leads to fragmentation of the language and readers need to know all the variants. This is especially problematic if you have a non-negligible user base for the extension because then it can't be removed again either. &gt; if authors of popular packages begin using it, I'd say it's a vote of confidence and a good indicator that everybody else should be using them. I disagree. If authors of all popular packages did, maybe that would be true. To make things harder on readers you only need 10-20% of popular packages though. The point is that the situations where 100% use one syntax or 100% use the other is vastly preferable to the situation where some use one and some use another. Just have a look at some languages employing optional syntax more heavily (e.g. Scala) and look at how hard it is to learn to read them if you don't use it every day.
I apologize about the confusion. I wasn't intended a sales pitch. I was just sliding into talking about HSpec by adding some personal flavor into how I got there and why HSpec has features that make me want to use it. Do you have a suggestion about how I should have briefly talked about HSpec at all for a Day of Hackage without adding any context that might be considered polemical or distracting?
Super cool series of articles, especially for those that haven't been in the deep end of needing to optimize their Haskell programs, yet! I'm very much looking forward to part 4 :)
I mean that for the zippy `Applicative`, if you don't use `pure = repeat` it fails the laws.
Well, to be completely rude about it, most programmers aren't smart enough to use Haskell. But if the top 1% of the world's population can amass half the wealth, why can't the top 1% of the world's programmers do half the work? This is where I see Haskell not there yet. I want workshare, not mindshare.
&gt; But if the top 1% of the world's population can amass half the wealth, why can't the top 1% of the world's programmers do half the work? I'm having trouble understanding how the one thing follows from the other.
&gt; I believe it is objectively better but I agree there's no good research yet to prove it Then you believe subjectively that is objectively better even though there are not proofs that prove its objectively...have a great day.
&gt; Visual Studio is a great IDE (by far the best I have used) Other than being easier to use/learn for a few use cases (and harder for many others), what are the actual benefits of Visual Studio compared to, say, a good emacs setup, which already has great Haskell support? I only see disadvantages because VS is: - Closed source and controlled by Microsoft. - Not as easily extensible, nor as flexible. - Does not have nearly as much community support. - Much heavier in terms of system resources. - Many of the existing features and elements of the UI are targeted at specific languages such as C++ and C#. In my opinion it would be a better and more fruitful use of time to migrate any features that people like in VS to emacs, vim, spacemacs, if there are any.
I wonder if some sort of rating/response system for documentation on hackage/stackage would encourage people to spend more time on documentation?
The "..." saved him from listing out every case where it was useful.
What's the performance of this implementation compared to something like `numpy.fft.fft`?
As someone trying to get over the curve, being able to get packages as easily as I apt-get packages, is quite helpful. Maybe it was just luck, but I'd say more than half of the handful of things I tried installing by cabal failed for inscrutable-to-newcomers ways.
Becoming a bearded &amp; birkenstocked hippie wizard is one of my fears in life. :( Consider, however, that Haskell has a much spiffier, enterprisier logo than LISP, which looks like some [mutated hellspawn straight from the heart of Chernobyl](http://lisperati.com/lisplogo_alien_256.png). Managers will look at Haskell's logo and think: "now *that's* the logo of reliable software!"
without record syntax, you'd refer to the points only by pattern matching like so data Triangle' = Triangle' Point Point Point corner1 (Triangle p _ _) = p `Triangle'` is the same as `Triangle`, except the record syntax automatically defines the functions `corner1` etc. but they're implemented by pattern matching exactly like the example I gave. My favorite use for record syntax is with the pragma `-XRecordWildCards`, which will automatically expand `Triangle{..}` into `(Triangle corner1 corner2 corner3)`, and brings them into scope, which lets you do things like fun Triangle{..} = corner1 + corner2 + corner3 readTriangle = do corner1 &lt;- readPoint corner2 &lt;- readPoint corner3 &lt;- readPoint return Triangle{..} But it's all sugar for pattern matching under the hood
Probably because he is adding fields to the record.
&gt; One of the insights in the Curry-Howard isomorphism is that you can associate product types with logical conjunction in just this way. I don't understand the C-H isomorphism very much, but I don't see how this could possibly be true, since logical *and* is an associative operator in logic but the elements of product types are not associative. In particular, `(A, (B, C))` is not the same type as `((A, B), C)`, but in logic `a &amp; (b &amp; c) = (a &amp; b) &amp; c`. The "intuition" I have for why they are called algebraic types does involve logic, coupled with statistics. In particular, when one has two events α and β with probabilities A and B, and certain conditions are met (I think the only condition necessary is that the events be independent), then the probability of α **and** β happening is equal to A × B. Similarly for α **or** β. This is probably very indirect, but I always remember this fact from statistics as a cool way of navigating undergraduate basic probability theory. So I read `data A = B C D | E F G` as "A has constructor B with C **and** D **or** constructor E with F **and** G" and without thinking I associate "and" with products and "or" with sums. 
&gt; The difference between objectively true and subjectively true isn't what you think it is Elaborate please, I'm curious.
Thanks, `hybrid-vectors` is definitely news to me! That series of articles from Ed has been in my reading queue for a long time, and I clearly need to digest it as much as possible this weekend. Your advice on my timestamps is also pertinent: ultimately if I can represent them as `Int`s instead of using `Data.Vector.Unboxed`'s instance for `Word64` (I have microseconds since the epoch), my guess is I'll see better performance, since I'm using an Intel Edison which is a slow embedded 32-bit x86 chip.
We sold out very quickly last year so get your tickets soon. For more information or if you are interested in submitting a talk proposal, see the conference website http://www.composeconference.org/ 
Ah, thanks, you're right! (I should have remembered this!)
Thank you! That pattern matching looks simple enough, I hadn't thought of it.
Thank you!
Subjective truth is something that is true only for one person or another. Objective truth is truth unrelated to a particular person. Something can be objective truth -- even though it is not yet proven. And only some of the subjects might discover this objective truth, and they may deduce this truth from extrapolation of their subjective experiences (all objective truth was deduced from subjective experiences, as all experiences are necessarily subjective!). 
That example piqued my curiosity, since one thing that fans of dynamically typed programming language complain about in Haskell is the lack of simple heterogeneous lists. Will unpacked sums give us the ability to do this more easily? Something like: instance (Show a, Show b, Show c) =&gt; Show (a|b|c) where show (a||) = show a show (|b|) = show b show (||c) = show c test :: [String] test = map show [ (1||), (|"text"|), (||True) ]
Sure, I have no problem with people using VS, and I think this project is a net positive. My disagreement is with the sentiment some people express that vim and emacs are defunct and outdated, while VS is some sort of MS utopia.
A few more that I know of: [Flow](http://flowtype.org/docs/union-intersection-types.html) and [Crystal](http://crystal-lang.org/docs/syntax_and_semantics/type_grammar.html#union). And [Scala.js](http://www.scala-js.org/doc/interoperability/facade-types.html#remarks) supports "pseudo union types", used when giving types to an existing javascript library to which you want to interface.
Not exactly the same but [Frege](https://github.com/Frege/frege) has a syntactic sugar around Either type, we can do something like this: xs :: [(Int | String | Bool)] xs = [Left . Left $ 1, Left . Right $ "text", Right true] 
How is not allowing hidden mutation going to stop the log files filling up?
Nuke the thread. Delete. Delete. Delete.
Hola /u/HwanZike ! Fuzzers are used to generate semi-random inputs to detect unexpected behaviors in programs (a.k.a. bugs). QuickFuzz uses QuickCheck to generate values. In some sense, QuickCheck uses the grammar information in the form of Haskell types, that why it is 'grammar fuzzer'.
I guess it depends on one's perspective. When I started programming I entered my programs on punched cards. After that, PDP-11 TECO was a huge improvement. &lt;b&gt;Anything&lt;/b&gt; we use now is utopia!
[I'm not sure if it will work.](https://broadcasthe.net/series.php?id=19603)
With Visual Studio I have a single install and I'm good to go. I may install a few extensions that take a single click to install but that's it. With Emacs I have to deal with elisp and tying together dozens of unrelated packages until I'm left with a house of cards. 
Objective truth is objective truth irregardless of whether people have shown or proved it to be so. The Earth was spherical and not flat -- this was objective truth well before humans discovered it, noticed it, proved it or even came to be.
Is there a solution to the static case if the triangle coords are floats instead? Google doesn't seem to see "type-level floats" for Haskell that I can see. Given the probability in any realistic code base that the triangles would become floats in the future, I'd avoid the type-level trickery if it can't be made to work on floats in Haskell right now.
&gt; With Emacs I have to deal with elisp and tying together dozens of unrelated packages until I'm left with a house of cards. No you don't. You only have to deal with elisp (which isn't a necessarily a downside compared to XML), if you want to manually customize packages your configs. You can get a fully featured emacs for haskell setup going with a few clicks, a few key presses and 1/10th of the time of the VS install.
Out of curiosity, how much of that do you see happening now, and with what languages?
No one has expressed that sentiment here, though.
Regarding Lisp and Haskell. GC being used everywhere now, D allowing you to mark functions as pure, C++11 having lambdas, C# with linq and lambdas, list comprehensions in rust and python, "traits" in rust (which are like typeclasses). On the surface C++17 concepts look a bit like typeclasses to me though some have said there is a fair few differences between them. Swift treats operators as functions and allows you to define your own operators. Rust with option types. I know that Java has them but they're nullable too and thus open to abuse. C# is also getting pattern matching in the future. Lazy evaluation with python generators. Its itertools library is also very functionally focused. Type inference in statically typed languages: C++11, C#, Rust and swift to name a few. I think that's a reasonable number of examples. I see it more and more all the time. Haskell with functional purity is like RMS with copyright. It's the most pure and absolute stance, a great figurehead but it's quite dividing due to the dogmatism.
Technically yes, but by that reasoning, dynamic languages don't have heterogeneous lists either, since everything boils down to one mother "type". 
It's not so much Haskell as it is functional programming, and Haskell is the purest in that sense. There's a video I saw a couple weeks ago that illustrates it pretty clearly: http://movablefeastmachine.org/ Whether you think he's on to something or not, his idea for how the architecture is laid out is essentially how a memristor computer would be laid out. The most important part of that architecture is this: * scalability * locality What does that sound like? Haskell. Functional programming. With a memristor pc, getting a faster computer is purely a matter of economics. You add on more parts and you get a faster computer, and because everything is local it won't cause any problems, the pc just has more room now.
I suppose that is a fair opinion to have. I don't know if the top 1% can manage to do half the work though.. I can't imagine that I could do as much work as 2 other people combined, even if I thought I was a much better developer. Maybe I am just not in that 1% though!
As mentioned, you have implemented half of `MaybeT`, so you may as well use the rest.
`[(a|b)]` is no more or less heterogenous than `[Either a b]`. They are the same thing with different names. If your argument applies to unpacked sum types then it applies to *all* sum types, and vice versa.
I agree, they're not easy at all. But I think that's more that Haskell is very strict, and it's also that you're writing very conceptually complex things in very little code, so most of the work is going on in your brain. I compare it to rust, I can conceptually understand the program from a high level, but then I'm fighting the compiler to get it to compile. Not because the "rust sucks", but because my program is flawed in some way. It just *seems* harder because in less strict code you would just ship the (possible) bug it's trying to prevent. It was always this hard, we were just doing it wrong!
Ooops! You're right ;)
Class constraints in `data` always exist as runtime dictionaries. They can be optimized away by inlining and specialization along with the `data` carrying them. AFAIK there's no optimization in GHC that ever modifies the layout of `data` (or introduces alternative layouts) relative to how the programmer defined it. 
&gt; you're not going to mask that readFile as a String, buddy! `unsafePerformIO`. Which is universally derided as not idiomatic. OK. But lazy IO is in the standard library and is just as bad. It's easy not to know that you have to traverse an entire string before you do `hClose`, otherwise bad things will happen. That and other warts (`fail`) mean Haskell is not as principled as I'd like, though fortunately some warts like `fail` and `Applicative`/`Moand` are being cleaned up. Maybe lazy IO will be the next casualty. So we're making headway here. More problematic is that space leaks can make reasoning and testing just as hard as hidden state and side effects. This is the sort of thing that can also render your credit card payment processor unresponsive, and you have to haul out a profiler to figure it out. I'm not arguing against non-strict semantics. I'm just saying any tool has its drawbacks. I find it odd that programmers argue about tools in this way. I doubt carpenters sit around and argue about whether they prefer hammers or screwdrivers. They certainly don't say the superiority of screwdrivers over hammers will become apparent.
There is a subtle technical difference between sum types and coproducts that you can mostly ignore. Coproducts are supposed to have the property that `Either a b -&gt; c` is isomorphic to the pair of functions `a -&gt; c` and `b -&gt; c`. The code for the isomorphism should be to :: (Either a b -&gt; c) -&gt; (a -&gt; c, b -&gt; c) to f = (f . Left) (f . Right) fro :: (a -&gt; c, b -&gt; c) -&gt; (Either a b -&gt; c) fro fg (Left a) = fst fg a fro fg (Right b) = snd fg b The subtle problem is that this isomorphism does not hold for some partial values containing bottom. For example sample :: Either Bool String -&gt; Integer sample _ = 7 We have that `sample undefined` evaluates to `7`, but `fro (to sample) undefined` is undefined. Also technically in Haskell, pairs fail to be products, but are instead "lifted products" for similar reasons. For Haskell, lifted products are a language design choice. In Miranda, I understand that pairs are proper (unlifted) products. Whereas the problem with lack of proper coproducts is more fundamental. Some people claim that strict languages have proper coproducts at the expense of being forced to have improper product and function types, but I'm not sure I believe them. Their lack of proper function types (which played an important role in our specification of coproducts) makes it hard for me to evaluate this claim.
Are heterogeneous lists via Existential Types (https://wiki.haskell.org/Existential_type) not good enough? 
Nope. Product arises from category theory and "co-X" tends to mean, in a CT setting, that it's just an "X" where all the arrows have been reversed. It turns out then that "sums" are "co-products". The relevant arrows in this case, so you can see the symmetry, are how `fst` and `snd` are in a certain way just the inverse arrows to `Left` and `Right`. With a little looseness in syntax, you can write fst :: (a * b) -&gt; a snd :: (a * b) -&gt; b Left :: a -&gt; (a + b) Right :: b -&gt; (a + b) The other interesting arrow, the "universal" arrow, arises in saying that for any type `X` if you have `f :: X -&gt; A` and `g :: X -&gt; B` then `q x = (f x, g x)` maps from `X` to a product `(A * B)` such that `fst . q = f` and `snd . q = g`. To be clear, the universal arrow here maps *into* the product. On the other hand, for any type `Y`, if you have functions `f :: A -&gt; Y` and `g :: B -&gt; Y` then `q (Left a) = f a` and `q (Right b) = g b` has `q :: (A + B) -&gt; Y`. Here the universal arrow maps *out of* the coproduct. So again, the arrows just all get reversed.
Scala will with Dotty
Will we be able to make pattern synonyms so the above could be re-written as: test :: (Int|String) -&gt; Bool test (A2'1 1) = True text (A2'2 "text") = False ?
We all have different ways of using tools, and for me it's a deal breaker, but I'm checking back once in a while to see if things have improved.
 type rec_t = {v1: int; v2: int} (* val f : [&lt; `PlainInt of int | `Rec of 'a | `Tuple of int * int ] -&gt; int *) let f x = match x with | `Tuple (x,y) -&gt; x + y | `Rec r -&gt; r.v1 + r.v2 | `PlainInt x -&gt; x Note that the parameter `x` is restricted to be one of those three constructors (`&lt;`). Could also make the function more open by handing any case: | _ -&gt; 0 and the resulting type would be: [&gt; `PlainInt of int | `Rec of rec_t | `Tuple of int * int ] Note the `&gt;` rather than `&lt;`.
I don't know anything about pattern synonyms, but I'd say yes, because this is really not that magical in any way - we have pattern syntax just like the pattern syntax of tuples and unboxed tuples, for example. If you can use tuples and unboxed tuples in pattern synonyms then you should be able to use anonymous sums and unboxed sums too.
I was trying to do similar things with [liquid haskell](https://wiki.haskell.org/Liquid_Haskell) (refinement types with an SAT solver), i got some help here: https://www.reddit.com/r/haskell/comments/3g9nx6/anybody_using_liquidhaskell/cu0a1lv but i haven't come back to try it further.
Which one do I choose if I'm currently trying to convince my company to pay for me to go?
I haven't yet read this post but if you're interested in performance of fft in pure Haskell you should take a look at this excellent series of blog posts: http://www.skybluetrades.net/haskell-fft-index.html . Library here: https://hackage.haskell.org/package/arb-fft
Are talks from this conference going to be recorded and posted online for free?
If your company is paying, get the corporate ticket.
&gt; I don't understand the C-H isomorphism very much, but I don't see how this could possibly be true, since logical and is an associative operator in logic but the elements of product types are not associative. What's going on here is that people often adopt a loose way of speaking where two things that are **isomorphic** are "really" the same thing. So `(A, (B, C))` and `((A, B), C)` are different types, but they're isomorphic, so they're "really the same." And therefore, product types are "really associative." And another way to look at it is this: your statement about logic really is about some **specific formulation** of logic, and is not true of all such formulations. `a &amp; (b &amp; c) = (a &amp; b) &amp; c` is an equational law in Boolean algebra. But if we formulate logic in terms of natural deduction instead, then things are different—a natural deduction proof of `a &amp; (b &amp; c)` is different from one of `(a &amp; b) &amp; c`. And note that by C-H, natural deduction = typed lambda calculus, so that means that the difference between the proofs corresponds precisely to the difference between the Haskell values of the types in question: * A proof of `a &amp; (b &amp; c)` is a pair such that: * Its first element is a proof of `a`; * Its second element is a pair such that: * Its first element is a proof of `b`; * Its second element is a proof of `c`. * A proof of `(a &amp; b) &amp; c` is a pair such that: * Its first element is a pair such that: * Its first element is a proof of `a`; * Its second element is a proof of `b`. * Its second element is a proof of `c`. 
Here's another good thing with laws: It gives you a way to verify your implementation. Maybe your intuition of fmap is slightly off and you decide to modify the structure of your data. Without the functor laws, you might not even notice, that your fmap implementation might not do what other people will think it does. Sure, there is no reason to have laws for pretty printing. But that is just meant for display - it's a different matter when you get things out that you want to process further.
The thing is I'm still going if they don't pay.
On this topic, why can't we have a type of forall a. Show a =&gt; a in expressions like: &gt; xs :: forall a. Show a =&gt; [a] &gt; xs = [6, "asdf", Just ()] Why is a data constructor required: &gt; data Foo = forall a. Show a =&gt; Foo a &gt; xs :: [Foo] &gt; xs = [Foo 6, Foo "asdf", Foo (Just ())] ? 
I also spent more time playing with stack/ghcjs/reflex than I would like to admit :-) On Windows, in order to build `happy` you need to have `ghc` on your PATH. If you only use stack and stack-managed `ghc`, it won't be on your PATH and happy won't build. Using MinGW 64bit shell (from MSYS), execute ``` export PATH=`stack exec which -- ghc`:$PATH ``` to temporarily alter your PATH. Now, happy will build. Unfortunately, you'll still hit this error: ``` Configuring ghcjs-dom-0.2.3.0... Building ghcjs-dom-0.2.3.0... Preprocessing library ghcjs-dom-0.2.3.0... JavaScript exception: Error: spawn ENAMETOOLONG ``` Relevant [issue ](https://github.com/ghcjs/ghcjs-dom/issues/19). to temporarily add the stack-managed ghc to your PATH. So, I guess linux/mac only for now.
I think eta-reduction here (lopping off the final element) is fine and idiomatic here. Since you're doing a conditional, you need an `if-then-else` somewhere, or a `case`. Or you could create a lookup table, but that would be overkill for this situation. Finally, you don't need to use `foldr` here. What you have is just parse_to_array = map (\x -&gt; if x == '(' then 1 else -1) I tend not to like lambdas and pull them out instead: parse_to_array = map selector where selector '(' = 1 selector _ = -1
I really don't like lambdas either. It just seems to really increase the density of everything without much payoff. Thanks!
I'm surprised I haven't seen the argument I find most straightforward for types: *refactoring*. Refactoring complex production application code in Haskell is better than any other language, period. As a result, I am encouraged and inspired to improve the quality of my code as I never suffer needlessly for it. TDD people will whine that "yeah, that's what tests are for" but nobody EVER writes enough tests, and/or they write way too many that end up testing the tests. Fuck that, I'd rather have types for production apps that don't keep me up at night after a minor refactoring.
In the spirit of functional programming I've been making a new account every time I want to post a comment, so it's probably my fault.
I didn't claim it was objectively proven! I claimed I *believe* it was objective truth, despite the lack of scientific research that proves it. I believe it is objective truth because of my anecdotal data as well as my deductions from the theory involved.
**A**lways **B**e **C**ategory theorizing
by the way, for what it's worth, "currying" isn't what's going on here :) Currying is the act of taking a function `(a, b) -&gt; c` and turning it into a function `a -&gt; (b -&gt; c)`. `foldr` is already curried in both cases. This is sort of an issue more of partial application. in one case, it's partially applied, and the other, it's not :) The "benefit" of defining things this way is that it helps you state what you're making in a nicer way, in some cases. For example, let's say I wanted to define a function that filtered out the odd numbers from a list. I could write: justEvens xs = filter even xs Which means, "`justEvens` is a function that takes a list called `xs` and applies `filter even` to that list." Or, that takes a list `xs` and returns `filter even xs`. Or, you could write it: justEvens = filter even Which says, "`justEvens` *is* `filter even`. It's the function that selects the even elements from a list." We can also look at: doubleAll xs = map (*2) xs Which is, "`doubleAll` is a function that takes a list called `xs` and applies the function `map (*2)` to that list." But that's sort of a bit silly to state something that way, once you think about it. You are talking about defining a function in terms of what it does to an item and what is applied to an item. Why not define a function in terms of what it *is* ? doubleAll = map (*2) This says "`doubleAll` literally *is* `map (*2)`. It's the function that maps (*2) to all of the items on a list." Compare: "`doubleAll` *is* a function that maps (*2) to all items on a list" and "`justEvens` *is* a function that selects all even elements from a list", to "`doubleAll` is a function that takes a list and applies the doubling function on its items." and "`justEvens` is a function that takes a list and applies the function that filters for even numbers on it." The first has a natural interpretation. `doubleAll` *is* `map (*2)`. `justEvens` *is* `filter even`. In this way, you start talking about functions as being important in and of themselves...as things you actually want to make, pass around, and *return*. You're making a function, and it's equal to another function. Like an alias, almost. Reasoning about functions and building them up from other functions, and how certain functions are equivalent, how they're just aliases, etc., is very helpful in haskell programming in general :) And you get used to it after a while. Instead of seeing functions as being defined in how they operate on certain inputs...you start seeing them as being defined in terms of other functions.
`x = (5|)` is same as `x = Left 5`. You want to forbid something like `Either Int Int`.
How is heterogeneous list useful?
You could take a look at http://hackage.haskell.org/package/network , which uses autotools+Cabal for configuration.
Just seeing this surprised me - I expect this: &gt; "{\"key\":\"value\"}" ^? key "key" Just (String "value") &gt; "{\"key\":\"value\"}" ^. key "key" &lt;interactive&gt;:9:26: No instance for (Monoid aeson-0.10.0.0:Data.Aeson.Types.Internal.Value) arising from a use of ‘key’ In the second argument of ‘(^.)’, namely ‘key "key"’ In the expression: "{\"key\":\"value\"}" ^. key "key" In an equation for ‘it’: it = "{\"key\":\"value\"}" ^. key "key" However, when combined with _String, it works: Prelude Control.Lens Data.Aeson.Lens&gt; "{\"key\":\"value\"}" ^. key "key" . _String "value" Prelude Control.Lens Data.Aeson.Lens&gt; "{\"key\":\"value\"}" ^. key "nonexistant" . _String "" Why does it work? Shouldn't it fail the same?
* in the West
&gt; For new programmers we should demonstrate the parts of Haskell **we like**. Who is "we"? I'm clearly not part of that group, as I like Cabal more than Stack. So should I demonstrate only Cabal to new programmers? Or should I rather be the grown-up here, and objectively tell newcomers the two variants (even the one I consider less recommendable personally) and leave the choice to the new programmer? Do you really hate Cabal so much that you can't even objectively tell new programmers about its existence? 
Or we can do something like this? A balance between readability for the foreign scary lens library and the familiarity of operators most people already know. eventsOptions :: GroupId -&gt; Options eventsOptions groupId = set (param "page") ["10"] . set (param "order") ["time"] . set (param "status") ["upcoming"] . set (param "group_id") [groupId] . set (param "format") ["json"] $ defaults 
Might just be me, but the very *name* "existential type" sounds difficult enough for my eyes to skim over it as "type wizardry best reserved for other people", heh. "Anonymous sum type" I can get past with a recognizing nod.
It really depends; they are just a feature, available when you need to specify a one-off function, especially if this appears as the last argument (as in the "dangling do-block" idiom : `... $ \x -&gt; do ...`)
I think this is a badly written and misleading article. The are called sums and products because they *are* categorical sums and products (aside from lazyness issues). But even if one doesn't want do deal with category theory, it is easy to see that they satisfy the usual properties of addition and multiplication (up to isomorphism). The empty type `Void` and unit type `()` will play the role of 0 and 1, and it is very easy to write down all the isomorphisms: A+0 = 0+A = A (A+B)+C = A+(B+C) A+B = B+A A*0 = 0*A = 0 A*1 = 1*A = A (A*B)*C = A*(B*C) A*B = B*A (A+B)*C = A*C + B*C To see a concrete example, here is the last one, distributivity, in normal Haskell: dist :: (Either a b, c) -&gt; Either (a,c) (b,c) dist (Left x, z) = Left (x,z) dist (Right y, z) = Right (y,z) distInv :: Either (a,c) (b,c) -&gt; (Either a b, c) distInv (Left (x,z)) = (Left x, z) distInv (Right (y,z)) = (Right y, z) Bonus: Parametrized ADTs are like polynomials, and fixed points are solutions of polynomial equations. For example the binary tree data Tree = Leaf | Node Tree Tree corresponds to the quadratic equation `T = 1 + T*T`
&gt; I believe it is objective truth because of my anecdotal data as well as my deductions from the theory involved. So all this is subjective derived of your own perception. Nothing here is objective.
Glad to hear that it's interesting to you. Point 1 is definitely correct. I'm not sure what you mean by the second point though. I was not aware that there was a limit to the size of vinyl's records.
thanks!
thanks for clarifying!
Probably no one will read this thank-you, but thanks everyone for your responses! It was genuinely helpful! And the type theory group at my work will be grateful for this discussion as well!
Sigh. I don't use Windows so I don't encounter these things, but wish Windows were better supported by many open source projects, not only GHC.
finally an inline-r code example, thanks!
I believe the Earth is spherical because I subjectively saw pictures of it, too. And I experienced other things, all subjective 
Ahh.. my bad, carry on.
Yes, but what advantage it has over eg. `[Either a b]` or `Show a =&gt; [a]`? Am I missing something when I don't see heterogeneous list as desirable language feature?
Again &gt; If it no have proofs and not give a definitive understanding of the point of why is truth to any viewer which it is shown then it can't be a objective truth. The point you make above fulfills this requirements, you can't say the same about the superiority of static typing because there is not proof, no evidence and cannot give an objective and indisputable view of its claims to anyone who has ever used. Also, people after seeing the earth in a glimpse never going to leave with the idea that is still plane, but a lot of people who has used statically typed languages over years have happily made the switch to dynamic ones and never look back, because as I said before is just a subjective preference. 
&gt; Try this thought experiment. Your boss comes by your desk and says, "Hey Globules (pretend your name is Globules), your colleagues really appreciate the fine quality of your code, but they're having trouble figuring out how to actually use it. Do you think you could write a bit of documentation, or maybe provide a few examples for them?" There's no need for speculation. I've been in this situation as I'm sure have most other professional programmers reading this. &gt; Puzzled by this request, you reply, "I... can't. I just... can't. Maybe one of them could reverse engineer my code and do it for me?" No, my response is "ok, do you want me to spend the next week doing that, or implementing that new feature you asked for yesterday?" Care to guess the boss's answer? Any moderately large organization should have professional documentation writers. While there are good programmers that are also able documentation writers, both of these activities take time. In this ideal case, the manager's choice is between: 1. having the same programmer/writer implement one thing and document it thoroughly, or 2. having the programmer implement two things, and a writer document them later. The latter choice is almost universally a better one. Programming is a higher-paid skill, so using your programmers as documentation writers is a waste of resources. The above argument, however, does not apply in open-source setting where nobody is paid. I think that ratio of programmers vs. writers is much higher here, and especially so in Haskell community. 
Awesome! I'm going to submit a talk proposal. Can I assume that if the talk is accepted I am auto-registered? If so, if I buy a ticket now and am accepted will I be reimbursed somehow?
That's good? I guess? I'm not really familiar with the whole release process of GHC.
What behavior would you prefer instead?
I'm not sure I understand. Is your qualm with `lens` or `aeson`?
My qualm is with the type of `^.` in `lens`.
Are you OK with using `^?` instead, returning a `Maybe`?
This sounds like a potential use case for shake
It sounds to me like what's making these isomorphisms fail is not laziness or strictness, but undefined values.
If we didn't have general recursion (which implies the existence of terms with undefined values) then we would have proper everything.
Ah, thank you! I now understand why &gt; xs :: forall a. Show a =&gt; [a] doesn't make sense. It still seems like the notion of "a list of things that implement Show, about which you don't know anything else" is well defined, though. I guess there would have to be *runtime* knowledge of what type the elements are, or at least what show function to use, and that would mean that the elements of the list would in fact be a data structure containing an actual value as well as the show function to use. Why can't the compiler figure out when to give values a "hidden" class implementation record? Whenever you add a value to the list, the type of that value is either known at compile time, or would already be a value with an attached hidden Show implementation. Either way, you could say at compile time how to add the value to the list and give it its Show implementation annotation.
Oh, as I feared, my memory failed me. `records` is the one with a size limit. (This answers your question as well, /u/jonsterling.) Wait, what were we talking about?
&gt; Still, using type-level strings feels wrong. It really does, because "smart constructor" really does seem to fit the bill here. I can see the theory behind compile-time checking of triangle validity but it seems to me that in practice very few triangles are specified at compile time. I was just curious to see what answers would pop up if I asked, really.
I usually don't think of readability of individual functions being a big deal compared to readability of the program overall. That whole function is probably `parse_to_array :: [a] -&gt; Array a` or something, which is very readable. And whichever function you need to edit you can take a minute to puzzle through. EDIT: This works a lot better if most of your functions are small (not more than a couple lines).
Erlang does :) -spec x() -&gt; list(integer | boolean | string). x() -&gt; [1, "text", true]. 
What a great post! Thanks 
Has anyone gotten relfex-dom working with ghc-mod or hdevtools in stack (or nix)?
And I was just thinking about how to test my dependently typed programs to make sure I don't get any regressions in the "dependent logic"! This couldn't have come at a better time - thank you so much :) (Also thanks to the author of the package, of course)
Sure, but I don't think typical OCaml programs use _only_ polymorphic variant types.
[Atom](https://atom.io/)
[Spacemacs](https://github.com/syl20bnr/spacemacs) has an excellent Haskell layer with [hlint](http://community.haskell.org/~ndm/hlint/), [ghc-mod](http://www.mew.org/~kazu/proj/ghc-mod/), [ghci-ng](https://github.com/chrisdone/ghci-ng), code completion and excellent Hoogle integration (you can also use [zeal](https://zealdocs.org/)/[dash](https://www.dash.org/)). Take a look at [Haskell layer documetation](https://github.com/syl20bnr/spacemacs/tree/master/layers/%2Blang/haskell) for more information.
I've been using Atom recently and found it much easier to set up than others. I have used leksah and eclipsefp too. Emacs I found a pain unless I wanted to commit to emacs as a way of life. I can recommend atom + stack to other revs with a straight face.
The main problem of `Either` is it only has 2 elements, sometime you want 3 or more and there is no `Either3` or equivalent.
I really like vim setup according to https://github.com/begriffs/haskell-vim-now
The main thing you want to have (and which any self-respecting text editor should have) is the ability to insert spaces when you press the tab key. Putting actual tab characters into Haskell source files is generally a bad idea. Most editors have this feature, so as long as you're not trying to use Plan 9's acme or something you should be okay just flipping the appropriate switch. Beyond that, the ability to maintain the indentation of the previous line (pretty much a requirement for any programming language), and maybe some syntax highlighting, but almost anything will do those.
Haskforce with the recent updates is extremely usable.
Yes, it is clear. But what if you want `(Int | Int | Int)`? Can you imagine all those explanations to beginners why they can have `(Int | String | Bool)` but they can't have `(Int | Int | Int)`?
There are very few extensions where the only upside is that they save you a few keystrokes occasionally.
This is something interesting I noticed and decided to post: chuch numbers with low Kolmogorov complexity have short descriptions on the canonical form on interaction combinators. Also, I think this is a good way to get a partial understanding of how optimal evaluators work. I don't show the code for reducing nets, nor for translating λ-terms to nets and back - but those are simple and probably inferable from the examples.
Exactly! An editor without tetris and butterfly is not a real editor.
I currently use [haskell-vim-now](https://github.com/begriffs/haskell-vim-now) with the silly conceal options turned off.
It takes you a WEEK to write a "bit of" documentation? If your managers are fine with that then whatever, but personally it would seem bizzare if I worked in a team where a coder wrote up some code that no one else was able to use, and when asked to write up some docs he replied with "my time is too important, hire someone else to do it".
two things i wish were better are the overall lagginess of the user input and that the hlint hints can be intrusive... i'm not finished typing and a tooltip error message is already blocking my cursor. 
&gt; but I'm glad more and more features inspired from Haskell seem to! I think that also counts as a win. If they can't beat us, they'll join us ;)
Definitely. But it never hurts to look back and see how we have progressed, as you do regularly ;)
intellij IDEA + haskell plugin \n GL HF!
Unfortunately the maintainer of flycheck seems quite hostile to user feedback, so things like intrusive errors are unlikely to be fixed.
I would, except I don't really know elisp, and from what I've seen of it, I'm not inclined to learn it.
Oh, my bad, I misread and thought kyllo's post was the "parent".
Which plugins do you use? I've been struggling with ide-haskell with no luck.
It seemed very immature when I looked at it years ago, but that was years ago so I guess I should give it another try!
&gt; if-then statement that if x is in environment then return `[Push (Bind (x, v))]` else append the variable x to the list....so how could I do this in Haskell? Since your environment is a list of `(key, value)` pairs, I'd use the standard library's [`lookup`](http://hackage.haskell.org/package/base-4.8.1.0/docs/Prelude.html#v:lookup) function, which expects a key and such a list, and either returns `Just value` if it found the key, and otherwise returns `Nothing`. env :: [(String, Int)] env = [("key1", 101), ("key2, 102)] -- | -- &gt;&gt;&gt; lookupKey1 -- key1's value is 101 lookupKey1 :: IO () lookupKey1 = case lookup "key1" env of Nothing -&gt; putStrLn "key1 not found" Just x -&gt; putStrLn ("key1's value is " ++ show x) I notice you have a line of code in `translate` which says `where Just v = lookup x m`. So you're already using `lookup`, but instead of having a branch for both cases, you're asserting that the result will always be of the form `Just v`. This this is not the case in your example, as the lookup fails when it encounters the variable `"x"` which is not in the empty list `[]`, you get a runtime error telling you that the "Irrefutable pattern" `Just v` has failed.
It has a bit steep learning curve, but the potential - once you realize what emacs is capable of - is unmatched.
Would appreciate. Thanks.
Thank you for the feedback, I'm glad you like it. 1. I'm not sure I understand what you mean by two circles that depend on the same sliders. It would be straightforward to draw two circles in the same `Drawing` that would both depend on the very same input fields (in the same or in a different way) by just updating the function `coloredCircle(s)`. 2. The type signature of `foldp` is the following (slightly simplified): foldp :: forall a s. (a -&gt; s -&gt; s) -&gt; s -&gt; UI a -&gt; UI s where `UI` is the type for Flare user interfaces. You can think of the second argument as being the initial state (of type `s`). The function argument (first argument) is the state update function that takes a new user input of type `a` from the `UI a` component (third argument) and the *current state* (of type `s`) and produces a new state. The result of `foldp` is a new component `UI s` which "contains" the current state. In the example foldp (+) 0 (map toInt (button "Increment")) we have `a = Int` and `s = Int`. The initial state is `0`. The expression `map toInt (button "Increment")` returns a component of type `UI Int` which contains the value `1` if the button is pressed and `0` if it is not pressed. The update function is `(+) :: Int -&gt; Int -&gt; Int` which simply adds the output value of the button to the current state.
How did you turn them off?
1. Ah okay! 2. Uhm sorry, I actually understand how the folding part works. It is exactly like `foldr`, right? Just conceptually in a list of actions. What I do not understand is how the actions get from the user click to actually calling `foldp` again... Okay a last question, is it possible to make stateful components modular? That is, can I create a button "component" with a state (on/off), and include it inside another "component", without the parent having knowledge of the representation of the state of the child component? How'd you do that? If you could provide this example to me, if it isn't hard, I'd be so thankful. It is something that has been troubling me for long when it comes to understanding pure functional web components.
some formatting was lost in submitting :/ 
Notepad. I start it from the command prompt on Windows, which means that I can have GHCi running at the same time that I edit my code.
For step-by-step debugging you can use `ghci` and you can find the instructions on how to do this here: * https://downloads.haskell.org/%7Eghc/latest/docs/html/users_guide/ghci-debugger.html `ghc` also supports stack traces if you build with profiling enabled. See instructions here: * https://wiki.haskell.org/Debugging For more information and resources on debugging, check out this overview here: * https://github.com/Gabriel439/post-rfc/blob/master/sotu.md#debugging
Hi :), Thanks for the pointers. I've gone through the wiki page on debugging, and most of those require some idea about the error and/or changes to the source and run in debug mode. I'm mostly looking for pointers on what to do when the app shows an error in production where this is not really possible - what can i step through or observe as in hoed, when i don't really know much about the error itself, or what input caused it as it usually happens in production. Also the ghc stacktrace on profiling seems good, but iirc it does not support threaded mode and detriorates performance (which may or may not be a problem, i'd really appreciate it if you could tell me some of your experience with it). Going through Gabriel's guide now, thanks for that!
Fixed my article to reference that version of Hoogle.
I looked into a couple of those, but most of them only use `autoconf` and use `C-Sources` in Cabal, whilst my needs go beyond that. The GTK one is... scary ;-)
So here is a short demonstration of toggle-buttons as child components. I hope this goes in the right direction: http://david-peter.de/articles/flare/toggle-example/
I think that would be bad desing. Your function should take `(Maybe Int, Maybe Int, Maybe Int)` or you should have 3 function, one to update each values.
 &gt;Going through Gabriel's guide now, thanks for that! Just FYI, Tekmo = Gabriel ;)
Suppose situations in which you want to preserve property of value inside container ...without need to declare new data type. Eg. generating list of triangular numbers, pentagonal numbers and other numbers.
The ultimate solution is using Yi, because it can be integrated without the overhead of an RPC mechanism and you can configure your editor like XMonad. It's not there yet for me, but it's been getting more and more usable and the project alive.
I think `foldp` is actually more of a `scanr` than `foldr`, because you obviously care about all the intermediate results when folding over a potentially infinite signal.
quantification might become more explicit (I think): https://ghc.haskell.org/trac/ghc/ticket/4426 so: f :: (Eq b =&gt; b-&gt;b) -&gt; a -&gt; a f :: forall a. (forall b. Eq b =&gt; b-&gt;b) -&gt; a -&gt; a are both valid (all explicit or all implicit) but f :: forall a. (Eq b =&gt; b-&gt;b) -&gt; a -&gt; a is invalid (both explicit and implicit). I like this better than forcing explicitness always. 
A `Lens` can get a single target, but also *set* a single target. A `Getter` can only get a single target, so that's roughly what I would like (a version of) `^.` specialised to. Currently what we have is type Getting r s a = (a -&gt; Const r a) -&gt; s -&gt; Const r s (^.) :: s -&gt; Getting a s a -&gt; a In order to make a version of `^.` that only works for single target optics you'd have to do type MyGetting r s a = (a -&gt; MyConst r a) -&gt; s -&gt; MyConst r s (^.) :: s -&gt; MyGetting a s a -&gt; a and make sure that `MyGetting r` doesn't have an `Applicative` instance for `Monoid r`.
cool thanks! 
I agree with most of what you've said. However, the compiler cannot always detect exhaustivity as pattern guards can be arbitrarily complex, and even if it did detect all uses of partial functions in my code, libraries may still fail with an error ("head" comes to mind, but I can't be sure that it really is that common). In this case, it is even more important to have a good location about the error because otherwise a lot of time may go into checking project code before figuring out that it was library issue. I'm not too sure about exceptions (I haven't tried them too much, their interaction with laziness seems scary and theres a bunch of libraries for it), but if you do get an uncaught exception how do you go about figuring out where it came from? In short, while I agree that there are much less errors than elsewhere, it seems unlikely both from the point of view of the compiler and the programmer that no unexpected error will happen. When using haskell (or anything else) for something critical, the ability to atleast find the source of an unexpected problem quickly is indispensable. Tools that require already knowing something about the error or reproducing the input may not be feasible in such a condition. Hence the need for justification.
Great, we're looking forward to getting your submission. I'd say that probably the best thing to do is buy a ticket now and if your talk is accepted, then you should be able to cancel your ticket on Eventbrite and it will refund your money.
It's because of the non-empty Leaf constructor.
Try and construct a `Branch [Leaf a, Branch [Leaf a, Leaf a]]` of the type `[..[a]..]` 
I discovered to my surprise that it requires more flag busywork to get a stack trace than one would expect with GHC Haskell. For the first time ever I reached for stack traces with Haskell to resolve a Data.Vector.(!) out of bounds exception. Here's what I ended up invoking: stack build --library-profiling --executable-profiling --ghc-options="-auto-all -caf-all" &amp;&amp; stack exec -- my-app +RTS -xc I propose it'd be nice in stack to make this shorter. E.g. stack trace my-app Which would run the above. Given that stack usually makes other things like this convenient, it might be appropriate to put in the tool. But I'm not 100% sure. If others are like me, I'm not used to using debuggers but a stack trace is almost always a helpful breadcrumb if not a direct path to the source of the problem.
Right, because it's a sum type.
I put an example here https://stat.ethz.ch/pipermail/r-sig-mac/2015-October/011649.html but I am not sure anyone in the R community was very interested :-(
Bugs in Haskell. When was that a thing?
"The best editor is neither Emacs nor Vim, it's Emacs *and* Vim!" How is this not like mixing vegemite with nutella to spread on toast?
Here's a totally non haskell-related comment about gravity: To me it looks as if your planets go in ellipses around the sun where the sun is not in a focal point, but in the center of the ellipsis. That's not physically accurate, but it probably is an easy fix. The force of gravity should be proportional to 1/r^2, whereas in your case it's probably 1/r. That will result in simulations that (hopefully) are a bit more interesting, possibly longer-lived. Their behavior at extremely long/short distances shouldn't be so uncanny.
Have you seen [Accelerate](https://github.com/AccelerateHS/accelerate#readme)? I don't believe it supports AMD GPGPU, only NVIDIA, though.
Woah I just now looked at my code and you're right. I use this equation for my gravitational force: (gC * mass1 * mass2) / ((sqrt((x2-x1)^2 + (y2-y1)^2))) And you made me realize that I forgot to square the radius on the bottom! I have the (x1-x2)^2 and the same for y, which must have made me think I had radius squared in there. Nice catch. Thanks. Recompiling it made the accelerations much, much less (unsurprisingly) but it does look more realistic. The only problem is that my collisions are not working properly so when two objects should be colliding they don't and their centers get to be like 1 pixel apart and the acceleration blasts them off into infinity because of that.
Here I was sitting, thinking "n log n? Holy hell, how? How's that even possible?" - I open the link, and there I have my answer: approximation. Yeah, yeah that'll do. :D
I had the same thing happen to me too once. I was working with c++, and the ^3 formula I came up with made me wonder. I mean, I did it right on the first try (don't think I would've caught it though if I hadn't.), but it's a nice little gotcha. I only realized that the wrong exponent kinda-works too when seeing Scott Manley (youtube) play a game where the gravity was implemented that way (probably mistakenly) and he was talking about how the exponent leads to the centered ellipsis. Oh, and now that I think about it, we did a similar simulation like yours in a CS class last year. Was rather fun too, after we completed the tasks I've had a lot of fun with tweaking variables and such. Aaand I'm in story telling mode. Due to a oversight (using a x1 that should be a y1 in a expression for distance compution ), I encountered a funny bug: So we had that one initial state we should work with, the 4 inner planets and the sun. And whenever mars was horizontally aligned with the sun, the bugged distance formula would make the gravity computation think they're really close, and it would cast a **enormous** amount of force on the sun, and it would fly off towards the top left at huge speeds. Meanwhile, all the other planets wonder where the hell gravity went and go in straight lines. Interestingly enough, due to the nature of the initial state, that bug had no other visible effects. Everything was working fine, until the sun departs.
Huh. That looks interesting. No, I don't think that would be too hard to do. I'd just have to make a function to divide everything into like 20 different blocks or something and represent all those points as 1. I'm not sure how well it would work if we just had like... 5 points though. It seems more fitting for n-body problems with hundreds of points.
Maybe. Or maybe your collision detection radius is just not adapted to the new magnitudes of gravity. See, now you have a *lot* stronger gravity at short distances, which means that the gravity becomes difficult to handle adequately at a radius much bigger than before. If you increase the radius of your collision bodies, you might already be fine. See, what might be happening is that if r gets small, your 1/r^3 blows up much more than 1/r^2 would, so you get freakish acceleration at a bigger radius. That way, your body picks up enough speed to step through the other object without colliding ("tunneling"). If you step up the collision radius, you mitigate it twofold. You decrease the velocity margin for tunneling to occur and you catch collisions before breakneck speeds can even appear.
Can I do rendering in Haskell? I looked up graphics in Java and I don't believe I got any relevant results... Doing rendering in Haskell would be awesome though because each frame takes ~0.01 seconds to render and a lot of that is from the command line call. I am using a command line executor and reader in Java. String[] command = {"/bin/processGravity", inputString, ""+points.length, ""+points.nextKey}; proc = Runtime.getRuntime().exec(command); stdInput = new BufferedReader(new InputStreamReader(proc.getInputStream()));
So is Gloss a Haskell graphics thing? I think I'm missing what you're saying Gloss is.
[Diagrams](http://projects.haskell.org/diagrams/)
It's a library for doing graphics, yes. It's designed specifically for things like this. Look at [this page](https://hackage.haskell.org/package/gloss-1.9.4.1/docs/Graphics-Gloss.html). You probably want `simulate`: import Graphics.Gloss type TimeDelta = Float type World = [Point] data Point = ... main = simulate (InWindow "Points" (1024,768) (0,0)) initialWorld drawWorld (\t _ -&gt; advanceWorld t) where initialWorld :: World ... drawWorld :: World -&gt; Picture ... updateWorld :: TimeDelta -&gt; World -&gt; World ...
Gloss.
`read` is slow. Try `attoparsec` 
Hmm that sounds interesting. I'll have to investigate.
Mind explaining a bit further? This doesn't make much sense to me
Thank you
They probably got downvoted for the "revolting retro-chic" part not for using Sublime. On topic: I also use Sublime Text 3, but pretty much just as a dumb editor. Never really been an IDE person.
Well, the naive sum algorithm is *also* an approximation to n-body so... as long as it's stable.
It's great. Makes it so much simpler to display a simulation or make an interactive graphic (interactive is done with the `play` function, which takes an additional function to update the world state whenever there is any kind of input) Being able to simply describe what looks like what declaratively without worrying about anything but the input is a very clean way to manage it. Personally I recommend setting up a `Render` type class if your `world` data is complicated, to let you split up the drawing code without having a gazillion `renderX` and `renderY` functions.
That looks quite unsafe to me. Like I said, relying on position rather than name looks very error prone to me.
Even if the first version had problems where stack over-rebuilt everything, having stack traces would be a very, very good thing. Ghci is fine, but when your executable crashes with `prelude: undefined` or a `file not found`, it leaves you scratching your head about where the error came from. For the longest time I didn't think Haskell had stack traces, and I've struggled to get them to work. So far I've muddled though by not writing anything big enough to require them for debugging (I know all the places that access files, because I wrote this). Don't know if that strategy would hold up as well for something that had been worked on my many people.
Can you provide any examples of the maintainer being hostile?
For style reasons or performance reasons? 
Style. And sanity. And anything that's not top level you can't experiment with very well in GHCi, which is cutting off a huge resource.
&gt; Since LTS does not know about this obscure package, Stack helpfully tells us exactly what to add to our stack.yaml to bring it in: &gt; &gt; extra-deps: &gt; - should-not-typecheck-2.0.1 This makes me realise that needing to maintain a `.cabal` file and a `stack.yaml` file in totally different file formats with overlapping information is tedious and a serious code smell. Is there a way to write only a `stack.yaml` and not bother to write a `.cabal` file at all? Also, how do I express the equivalent of `should-not-typecheck == 2.0.*` for `extra-deps`?
haskell-src-exts should be OK for this purpose. Have you used Hint? it should fail early if there are typechecking errors, and it will return an executable expression otherwise. it does everything in the same step. 
I don't see a reason for 1) either. But ghc's typechecker monad apparently allows IO. So GHC's implementation is not an option, unless there's a part of it that actually is pure. 2): That would indeed suffice, I would say. I mean, I'm not sure what the type inference does on top of that *exactly*, but for a Expression, I would usually want to know whether it makes sense at all, and what type it evaluates to. Beyond that, it would be neat to have the code generator be able to "follow the types", that is: It knows what type a function has, decides semi-randomly on a function to use first, then recurisively fills in the parameters. Example: function f :: [Int] -&gt; Int -&gt; [Int] needs implementing. If the algorithm now tries f m n = ? transformed to f m n = map ? in such a way that it knows that this typechecks because we have map and map always returns a list. 3): Possibly. In this case though, I particularly want to try whether the genetic algorithm can learn to use and benefit from those advanced language features. Of course the machine code doesn't care, but whether you learn a function in c or haskell makes a difference to the learner. You can see an example of the source code I'm trying to typecheck here: https://github.com/vektordev/GP/blob/master/src/GRPSeedhr.hs#L48 Everything after line 46 is the genome. Note the declarations for reprogram and act in the boilderplate non-editable part. Also note that I want to use SafeHaskell, but a safe import of haskell-src-exts isn't possible. Am I doing it wrong?
I've looked at hint a long time ago. I'm not sure, but I think I need more control over things. I've got the "compile and execute this genome" part down, even if it's slow, using calls to ghc. What I'm right now working on is means for the genome to figure out what it's writing and whether it is any good. This needs not use any IO, which I think hint does. The problem with haskell-src-exts is that I can't import it safely, which I would really like to. Beyond that, your observation is right. The Exp type is probably the best type I have access to to implement a typechecker with. If you're familliar with haskell-src-exts: Does it do any IO under the hood? I don't want the genome to be able to perform IO actions, so I'd rather compile the genome as Safe and give it non-IO function declarations to implement. However, ghc currently prevents me from safely importing haskell-src-exts.
We have same situation with tuples and pattern matching eg. zip of lists. I don't mind it in throw away code, repl or self contained piece of code. 
Neat! I looked at your work, and you wrote several nice programs running on the client side. I am curious about your opinions and experiences with Haskell on the client side. Did you ever try GHCJS or Reflex? How would you compare developing in Purescript with using Haskell? As far as i can remember now, one of the main differences is that Purescript is not lazily evaluated. There are also some differences about maturity and tooling, but i am more interested in deep differences between the languages
&gt; If others are like me, I'm not used to using debuggers but a stack trace is almost always a helpful breadcrumb if not a direct path to the source of the problem. This is exactly what I'm talking about.
Are you really asking if it's an approximation apart from the ways in which it is actually an approximation? No. Though you left some out depending on how pedantic you want to be.
You could put that in a where clause if you really want. But hiding them in there is just something you'll repeatedly have to undo every time you test them.
I long dreamed about encoding the rewriting graph as a sparse matrix with a matrix composition reduction loop on a GPU. #crazee
Well, those kinds of approximations you can make arbitrarily precise. Use proper numerical data types and tiny timesteps and you can easily (programmer) make a accurate simulation (though hard to compute). From my perspective, Barnes-Hut has a error to it that can't be reduced that easily.
Ehhh. Yeah, I guess. I mean, if you use gravitons as your analogy, of course. It's like radiation. The further out, the less there are of them. And since in 2D space they can't disperse in z-direction, it's 1/r. They scale with the circle they're covering, not the sphere in that case. However, I'm suspecting OP is doing a flattened projection of 3D space, in which case the formula is actually wrong. Like, if you say that all objects in your 3D space have z-coord zero, everything happens in 2D space mathematically, but physically it can be a 3D space nonetheless.
You may want to look up gravitational softening: https://en.wikipedia.org/wiki/Softening 
Ahh that looks cool! Thanks for that.
I would love to know how to improve my algorithm if you would like to explain. I'm not very experienced with this. As for the not needing a GPU part, I am mainly interested in just using a GPU to calculate stuff because it has so many cores. I'm not saying I _need_ it. It would just be better than a CPU.
Then I'd write something like this in the `where` clause: env' = ("x", 0) : env And I would pass this modified environment to the recursive call so that the variable is available during its evaluation.
haskel-src-exts should not do IO but I don´t know. I understand that typechecking fragments of code as early as possible would speed up the assembling of valid programs.. Perhaps the opposite: to generate only valid fragments of code that match a type signature could help also. Then you may be interested in this: http://article.gmane.org/gmane.comp.lang.haskell.general/12747 https://github.com/augustss/djinn Djinn return a function given a type signature. I think that it does not work with class constraints however. Something that understand classes and dependent types like Djiin perhaps in combination with genetic programming would allow serious automatic programming. (And programmer's task would be to create the type signatures of the requirements....) An example of how Djinn and genetic algoritms would work together: Djinn+&gt; f ? (Num a) =&gt; a-&gt;a a Djinn plus would return: f :: (Num a) =&gt; a-&gt;a k :: a f x1 = k1 x1 + k2 x1^2 +... And the genetic algoritm should adjust the value of the k's according with the fitness function.
Another example: I wanted to process a text file line by line in a streaming fashion. I used the excellent pipes library. It is said to be one of the most well documented libs on Hackage, and has tutorials etc. Still, it took me forever (more than an hour) to figure out how to do line based processing. https://hackage.haskell.org/package/pipes-bytestring-2.1.1/docs/Pipes-ByteString.html The page has an example how to take the first three lines of the input, but not how I simply transform every line. Since I am not proficient with lenses, and even less with FreeT, I had to experiment a lot, and I still don't quite know whether my solution is nice or not: P.runEffect $ over PBS.lines id PBS.stdin P.&gt;-&gt; handleLine P.&gt;-&gt; PBS.stdout The pipes lenses tutorial, https://hackage.haskell.org/package/pipes-group-1.0.3/docs/Pipes-Group-Tutorial.html, only has examples on what to put in place of `id`, and even that, I don't quite understand, as all things that are put there are functions from the library itself, which are all implemented in terms of FreeT. While the tutorial is nice, I don't have time to study FreeT and lenses to achieve something as simple as what I needed. It was a lucky guess that lead me to put `id` there. /u/Tekmo, this could also interests you. Is my solution above right, or is there something simpler to stream a ByteString line by line? How could we improve the documentation?
Alright, that sounds good. And yeah, I get that. It's just the fact that it can do so many different little things at the same time with all the cores.
I use vim, but don't use many of its features. How can I set it up such that the linter is run after every save?
It really scares me that eta-expansion/reduction has an effect on the semantics of the language. Edit: I suppose the language is semantically the same, just has different performance?
I think rather than implement `sort3` myself, I just would have gone with bestPartialArea :: (Int, Int, Int) -&gt; Int bestPartialArea (l, w, h) = 2 * (l*w + w*h + h*l) + slack where [side0, side1, _] = sort [l, w, h] slack = side0 * side1 but I understand the didactic desire to use only code that's proven safe by type. (`sort`'s output will always be the same length as its input, but its type makes no such guarantee). 
I like it! It feels like the pragmatic version of dependent typing.
We tend to quotient these things out when we think about them, but that eta expansion wrapper has a cost and an observable effect: It changes the semantics of `_|_` as `seq _|_` differs from `seq (\x -&gt; _|_ x)`. Each wrapper costs both an allocation and a reduction step. We found this issue when working on `lens` when we were wondering why certain cases were slow and Shachaf went off and investigated the core.
The original motivation was that there are more opportunities for inlining in the function body. The downside is that this was happening every iteration. We fixed this ~2 years ago in time for 7.8. (Twan wrote a patch.) https://ghc.haskell.org/trac/ghc/ticket/7436
Language extensions are the product of research. Each one is meant to address a particular need. They are not official parts of the language, but there is a committee that meets every few years to integrate the most widely used and least controversial extensions. I doubt they will change much at all if/when they are made official. I don't believe that has happened with past extensions. Although, there is sometimes a bit of clarification required to standardize an extension. Most Haskell code can be written without extensions. It's usually better to use no extensions until you run into something that would be less error prone or significantly clearer/shorter with an extension. Sometimes the only way to write something is using an extension, and in that case it's always good reconsider. Maybe you shouldn't be writing this bit of code if it requires extensions? What other extensions will be required? Will people using the library need extensions to use it? Are these extensions safe? Etc. Now with that said, some extensions are really lightweight and cause no harm so people use them more liberally. Where I work we prefer haskell2010 in most code bases and bringing extensions is allowed but we like people to think twice before doing it. I'd recommend learning the extensions but I don't see it as a super high priority. Usually you can pick them up as you go.
sorry man, that's a first on that server :)) i'll talk to the host guys
My primary confusion is with your usage of conversion and not with inductive types and coinductive types in general.
I still think that with tuple its not the same. For example if I know that my tupple represent a 2d Vector then its clear what `(Int,Int)` is. But with `(Int|Int)` and the like argument position can hardly have any meaning and I can invert theme way to easily.
I think a "compiler" should be an actual first-class library. It's time to lift the barriers of hoops that tool writers have to go through to reverse engineer, duplicate, or use undocumented features. I understand that this is tricky because of constant change in internals and because of the desire to preserve important invariants, but I think there is no longer a choice.
From a UX point of view, it is definitely the case that really useful utilities are not so discoverable for users if they're not "official". Would you be happy if they were easily discoverable but not part of `base`?
Thanks for the answer. As a follow up, do you have a (simple) example of something that cannot be done easily without extensions?
It's not about discoverability. Split is so incredibly useful it really should be in base and not require extra dependency and build time wasted. 
I don't remember this being true for me. Are you sure you weren't in the middle of an expression? I'll try making a tutorial and sanity checking this memory later today.
You might be interested in "Type Targeted Testing" http://arxiv.org/abs/1410.5370 which shows how to generate QC style tests from refinement type specifications.
There are bits and pieces of `split`'s functionality all through base, it's totally inconsistent what you need to import `split` to do. Being in the platform in no way increases the discoverability of a package.
Splits are so common that they're very widely used parts of many standard libraries. It's just kind of silly that it isn't in Haskell's base considering the similar functionality in Data.List. it would be a great helper. 
what bits and pieces are in base? And being in the platform _should_ increase the discoverability of a package. that's half the point. So we should figure out if it doesn't why, and how to fix _that_.
&gt; So we should figure out if it doesn't why, and how to fix that. Basically every haskell tutorial has you trawling hackage/hoogle, so people just don't look at the platform package list.
`stack trace` and a `stack profile` would enhance discoverability of those tools. Maybe a `stack config --trace` makes more sense?
The problem is that eta has an effect on (denotational) semantics only sometimes. In this context the distinction makes no difference, thus it can be optimized away in the operational semantics. Whereas, in general, we can use `seq` to distinguish `_|_` from `(\x -&gt; _|_ x)` so we can't always replace the latter by the former. However, to throw an additional spin on things, there's no pure[1] context which can distinguish `(\x -&gt; _|_ x)` from `(\y -&gt; (\x -&gt; _|_ x) y)`; so in principle we could identify them without altering the semantics of `seq`— the problem here is that we can't convert between these under call-by-need (nor call-by-name) evaluation, we'd need true normal-order evaluation. [1] By which, here, I mean there's no context which can be used to reflect differential observations of the operational semantics back into the denotational semantics (i.e., nothing that observes time or memory costs).
I basically want a function that equals `convert` and `convert'` but is `O(n)` instead of `O(n^2)`. Any other questions?
Not sure if this is redundant with any of the parent's recommendations, but I use [syntastic](https://github.com/scrooloose/syntastic) with hdevtools for very fast syntax checking. 
Yes, there are several. They tend to not be that simple (at least in terms of understanding how they work) as evidenced by them needing an extension. Simple to write Haskell things won't typically need extensions. A relatively simple example is any type class that has more than one parameter. Here is an example, that feels contrived to me, where the collection type and the element type are both parameters to a type class: https://en.wikibooks.org/wiki/Haskell/Advanced_type_classes#Multi-parameter_type_classes A more realistic use of multiparameter type classes are monad transformers: http://book.realworldhaskell.org/read/monad-transformers.html A more complicated example that is very practical and commonly used is the `ST` monad, which requires `Rank2Types` to implement: https://wiki.haskell.org/Monad/ST
That's exactly what atom did for me. Whenever I save the error, warning and lint messages refresh. Not sure which plugin was responsible but it works a treat.
I actually wondered about this couple days ago; are there any structural reasons by which there is a `-XDeriveFunctor` but no `-XDeriveMonad`? Some non-uniqueness of any given Monad instance?
True, that is a valid choice, but that's a meta-issue. The question addressed in this article was, supposing you had certain requirements (whether you agree with them or not), what's the way to model them. There are many occasions in which I do not want a silent default. For example, I don't like when I get messages saying "You have 0 items in your shopping cart." I want the difference between having nothing in my shopping cart and having 1 or more be reflected in the type and in the type-driven message. I don't like getting bills in the mail about owing 0 dollars. I think it's a waste of paper. In other words, there are times when I prefer that something be thought of as "Nothing or Just a NonEmpty" rather than "Nil or a Cons of head and the rest".
Cool, thanks! I've put it on my to-read list.
Well the thing is that I wanted to write my own program from scratch to simulate an n-body problem. I didn't even know that n-body problems where a popular thing to program until I had gotten a ways through writing it. And what should I use other than read and show? Those are the only two things I've learned about since I'm rather new to this whole Haskell thing.
okay, what I was specifically wondering was, does GHC run the computation within the monad every time the data type binded to the aforementioned monad is encounter? That is, is monad code always computed sequentially -- I couldn't imagine it working any other way.
Why is this type terrible?
I don't like `Leaf a` and `Tree a EmptyLeaf EmptyLeaf` but I liked 3 examples to better flesh out the `Functor` instance. Let me rewrite that note, more of a "I don't know how good this type is because I just made it complex enough for here".
`&amp;` needn't be as obscure as it is. Most programmers already think this way.
I think there's a very strong argument for performance (especially in terms of big-O) being part of the semantics of a particular solution.
One of the worst ones for Agda imho might be the mysterious variable `w` that shows up in error messages even though the user never ever named a variable `w` ([see here for instance](https://lists.chalmers.se/pipermail/agda/2008/000140.html): `w != plus Z w of type Nat`, althought this one is not too bad because the `plus Z w` bit puts you on the right track). It is an artifact of the way `with` expressions are elaborated.
First, I recommend reading this post if you haven't already: http://www.haskellforall.com/2013/09/perfect-streaming-using-pipes-bytestring.html Then read this tutorial: http://hackage.haskell.org/package/pipes-group-1.0.3/docs/Pipes-Group-Tutorial.html Those are the documentation you are probably looking for, but I forgot to link to them from the `pipes-bytestring` documentation. `pipes-bytestring` deliberately does not expose an API that lets you access an entire line as a single `ByteString` chunk because that could consume an unbounded amount of memory if the line is arbitrarily long. The above post and tutorial explain the general idiom for dealing with files line-wise while still streaming. The correct solution for what you are doing is: Pipes.runEffect (over (lines . individually) f stdin &gt;-&gt; stdout) ... where `f` has type: f :: Producer ByteString IO r -&gt; Producer ByteString IO r So, for example, if you wanted to append an exclamation mark to the end of each line, you would write: Pipes.runEffect (over (lines . individually) (&lt;* yield "!") stdin &gt;-&gt; stdout) I can provide even more guidance if you can explain in more detail what you wanted to do with each line.
Basically, the reason I was asking what GHC is doing is because I wanted to know how the code is evaluated in assembly, and if it runs all the state and monad logic in some sort of global loop -- going through all the monadic variables, one by one. Or does it do anything to save time, or anything magical -- I could only imagine it evaluates each one sequentially, one after the other. Also, isn't all IO in haskell handled by a monad? 
There was a whole [reddit discussion](https://www.reddit.com/r/haskell/comments/3cfhdc/why_isnt_there_more_emphasis_on_using_instead_of/) about this, which also considered functors and monads.
Sorry that came out horribly wrong when I sent it in a hurry from my phone. Should be: &gt;cd helloworld &gt;stack exec atom.cmd . 
The best way to see how the code is evaluated is to study the core. I wrote up an introductory post on this here: http://www.haskellforall.com/2012/10/hello-core.html ... but I'll also give a short example here. Let's say that I write the following code: main :: IO () main = go (0 :: Int) go n = do print n go (n + 1) ... then I compile that with all optimizations on and output the simplified core representation: $ ghc -O2 core.hs -ddump-simpl -dsuppress-all [1 of 1] Compiling Main ( core.hs, core.o ) ==================== Tidy Core ==================== Result size of Tidy Core = {terms: 39, types: 59, coercions: 12} Rec { main_$sa main_$sa = \ sc_s3C5 sc1_s3C6 -&gt; case hPutStr2 stdout (case $wshowSignedInt 0 sc_s3C5 ([]) of _ { (# ww5_a3Co, ww6_a3Cp #) -&gt; : ww5_a3Co ww6_a3Cp }) True sc1_s3C6 of _ { (# ipv_a1aT, ipv1_a1aU #) -&gt; main_$sa (+# sc_s3C5 1) ipv_a1aT } end Rec } main1 main1 = \ @ b_X18C eta_Xe -&gt; main_$sa 0 eta_Xe main main = main1 `cast` ... main2 main2 = \ eta_B1 -&gt; runMainIO1 ((main1) `cast` ...) eta_B1 main main = main2 `cast` ... The key part is this loop, which I've cleaned up a little bit and renamed some variables: -- Think of `main_$sa` as having this type: -- -- main_$sa :: Int -&gt; RealWorld -&gt; (RealWorld, ()) -- -- ... which is then wrapped in the `IO` newtype and equivalent to: -- -- main_$sa :: Int -&gt; IO () Rec { main_$sa = -- `n` is the number to `print` -- `realWorld` is a fake placeholder for the current state \ n realWorld -&gt; -- This is the low-level routine to actually print the number case hPutStr2 stdout (case $wshowSignedInt 0 n ([]) of _ { (ww5, ww6) -&gt; : ww5 ww6 }) True sc1 -- Every `IO` action gives you back the "next real world" -- to feed to the next `IO` action of _ { (realWorld', unit) -&gt; main_$sa (n + 1) realWorld' } end Rec } That will then get compiled to an efficient loop in native code. However, I want to emphasize that the above is actually sort of a bad intuition for how `IO` works. I prefer to teach people using the example from the last `IO` section of this tutorial: http://www.haskellforall.com/2014/10/how-to-desugar-haskell-code.html To answer your second question, you can in theory provide a non-`Monad` interface to `IO` by providing monomorphic versions of `return` and `(&gt;&gt;=)` specialized to `IO`: returnIO :: a -&gt; IO a bindIO :: IO a -&gt; (a -&gt; IO b) -&gt; IO b That doesn't use the `Monad` interface at all. However, nobody actually does that in practice.
Oh, that's easy. Just nest tuples as much as you can with many differening types and try to unify them withba vastly different nesting instance. The typechecker will shit itself soon, but you can still get some pretty gigantic typechecking errors before that happens.
I dislike posting links to my own blog but I have ended up using Vector, HMatrix, Repa all in the same function. I haven't tried using python / numpy but I am reasonably confident the code would be easier to write and probably faster. Potentially my (Haskell) code could be simplified but it does feel like we need something equivalent to numpy.
IDEs, unlike a compiler, have to deal gracefully with broken code. That is, code being edited and modified right now: indentation, highlighting, auto complete, jump to definition, ... cannot fail everywhere or after some point just because one is editing one line. OTOH a compiler can just fail hard and fast. I agree with the sentiment, but this is basically why these kinds of tools have been developed independently. As a concrete example, clang's code formatter, clang-format, need its own fuzzy parser to be able to format broken code correctly within e.g. an editor. That parser is now offered as a library even though clang doesn't need it, so we are getting there, but the amount of overlap between this tool and the compiler is not as big as some might think.
HMatrix has Storable Vector under the hood; HNetCDF uses a typeclass to subsume `from/toForeignPtr` and `fmap` for both Vector and Repa arrays, which is an approach I'd like to replicate in [petsc-hs](https://github.com/ocramz/petsc-hs). On the other hand, Vector doesn't have the clever multidimensional indexing and delayed evaluation facilities of Repa arrays. What's your opinion on the matter? Would it be simpler to work exclusively with Repa arrays (i.e. if also random generators produced them, and HMatrix were based on it)? petsc-hs is not quite ready yet (so far I have only verified the linear solvers, but I'm having some bugs with the vectorized assembly of sparse matrices and I'm still understanding a couple of issues with mutable data and representing C callbacks), and perhaps it's a bit overkill for small numerical problems, but I'm basing the user-facing interface on Vector.Generic. BTW, I'm a huge fan of your blog :)
/u/tekmo made an overview of the [haskell eco system](https://github.com/Gabriel439/post-rfc/blob/master/sotu.md). [Subhask](https://github.com/mikeizbicki/subhask) is an attempt to improve numerical calculations not mentioned in the overview.
AFAICT SubHask was mentioned implicitly, since HLearn is based on it.
I've been doing it too, and it is one of the best series of simple puzzles I've done. It makes you want to learn lenses, work with bits, write parsers, etc... I would recommend it for any Haskell beginner. 
While not totally Dependently-Type, some servant-server errors are hilariously unwieldy. Unfortunately I don't have one on hand.
Author here. This post is a result of multiple long discussions in #haskell-blah -- I would like to thank everyone who contributed there! Would love to hear some feedback on the argument I make, my goal wasn't necessarily to promote Haskell or Functional Programming, but to explain to a broader audience the advantages separating code from data can have.
There is a functional programming language that does this really well. R. Meaning that R would be a better language to measure to then Python's Numpy.
As others have said: `Monad` is just an ordinary typeclass which you could have created yourself. The only compiler support which exists for them is the `do` notation which is there you spare you the writing of braces. The desugaring works according to a simple scheme (pseudocode): desugar(do x) = desugar' x desugar' f = f --base case: last instruction desugar' (f ; rest) = f &gt;&gt; desugar rest desugar' ((x&lt;-f) ; rest) = f &gt;&gt;= (\x -&gt; desugar rest)
Monad itself doesn't force this, but particular instances might. The prime example is IO, which looks like this: newtype IO a = RealWorld -&gt; (a, RealWorld) Conceptually, the meaning is "an IO operation reads in an entire world and returns a changed world". In GHC, RealWorld is basically just a token that gets optimized away, but because every IO operation requires the RealWorld of the previous operations, the operations run sequentially.
&gt; I dislike posting links to my own blog Why? I was under the impression that it was standard practice.
Is your example in pt 2 supposed to be Java, or just pseudocode? FYI - Where you write this: Counter tmp = c; That just reassigns the reference to the underlying object. You actually mutated the state of the object that was passed in. You also couldn't directly mutate the count property of tmp, since it's declared as private. So this wouldn't actually compile. Nice article though.
I mean, proper highlighting, not the one provided with geany. The one that enabled by geany kind of ducks.
Very nice write up; thanks for putting this together!
I don't know if there is much you can do. [The definition](http://geany.org/manual/dev/#filetype-definition-files) seems a bit restrictive. 
Good point and nice catch about the wiki page. I think the wiki page has some issues, and I sent this email to ghc-devs to clarify some things: https://mail.haskell.org/pipermail/ghc-devs/2015-December/010699.html In the `zip` example, with current `-XStrict` implementation, you're right that `a`, `as`, `b` and `bs` won't be strict and I think this is a problem with how `-XStrict` is specified and implemented(it looks like an inconsistency to me). However, in the case where we have `-XStrict` in all the dependencies, it doesn't matter. Because in that case `(:)` will have strict fields, so `a`, `as` etc. in this example will be strict anyway, no need to add bangs in `(:)` field binders.
I think the problem with numerical calculus in Haskell is that people are not yet sure on how to do it in a way that at the same time takes advantage of Haskell's features and that is easy to use for exploration. Much of the functionality of NumPy comes from the fact that it deals with type conversions, etc... and that it binds to libraries in C or Fortran that have too an imperative feel, so the translation is more straightforward. But I think we will get something really nice in Haskell when this is solved, but this is not yet the case.
http://tweag.github.io/HaskellR/
The importance of "leniency" of syntax is something that I believe has been studied and implemented seriously enough (if I'm wrong and there is work in this area, I'd love to look at references). There could be a case made for formally defining leniency and a clear relationship between a lenient grammar, say, and the "correct" grammar, rather than their being separate.
I'm hoping for further development of HaskellR, though currently the ergonomics are a bit rough and requires some significant conversion/glue code to talk between the two worlds.
So you're thinking of something that not only indexes by integer ranges but also by text label? What's the typical dimensionality and sparsity of the data you look at?
more commonly known as `|&gt;` in some other languages
It can feel like spamming, especially when you're doing a daily series.
I also agree that `multiset` should be standard. Also, Haskell may not have multiline strings and interpolation, but hackage does: [`here`](https://hackage.haskell.org/package/here).
We don't have a "typical" set of data. Random example: For machine learning our data is often a mix of dense and sparse. Features are most commonly two data-dimensions (NxM) but sometimes we have a data cube with a time dimension (e.g. panel data) - but that can generally be unstacked into a 2d representation. We use labels in e.g. pandas to refer to a column: `df['total'] = df['per_hour'] * df['hours']` is more readable than `df[2] = df[0] * df[1]`. We also use labels to refer to rows but (multi)indexing in Pandas is too much for this comment :) Labels in pandas can also be dates, e.g. `df["2015-01-01":"2015-02-01"]`. All this stringly typed stuff is fantastic for exploration and readability but not so much for production code. But I see no reason why the same can't be done in Haskell :) For dense numpy arrays libraries like xray (https://github.com/xray/xray) also support labels for dimensions, generally making code more readable but we haven't used xray yet.
Yes, I use that, but didn't want to bring in Template Haskell today :-).
Are quasiquotes considered template haskell? At a first approximation, I'd say "no", since you don't need to turn on the `TemplateHaskell` language pragma (`QuasiQuotes` suffices).
This is somewhat off topic. Preface/disclaimer: everyone is free to do as they wish in their code. Thank you to the author for the time and effort they put in to make this article. I like it. The article writer uses `&gt;&gt;&gt;` for composition because "it looks much more natural to [the author] for pipelines". Well now it looks funny to me and takes extra mental capacity to parse the code, because the majority of Haskell code out there that I've seen uses `.` and `$` for composition, both of which are right to left. Since this article is a tutorial of sorts, I think deviating from the norm is extra bad.
It's also worth noting that monad-generic code of the form `f :: Monad m =&gt; ...` isn't really seen in F#: http://tomasp.net/blog/2014/update-monads/ &gt; People coming to F# from the Haskell background often dislike the fact that F# does not let you write code polymorphic over monads and that computation expressions always explicitly state the type of computations
The way people code R is way closer to Python but R is a true Functional Programming Language. If you try coding in a functional style in Python it is ugly and gets ugly quick. R looks very nice. What you point out is just coding style differences not actual what can be accomplished.
I know it's controversial, but I used it in this example precisely because the problem is so simple that I felt it would not create too much of a burden despite its unfamiliarity to some readers and would be a good opportunity to expose this operator to those who would not otherwise see it. I'm sorry if my aim backfired on you.
Would there be any way to use something like template haskell to make it possible to write something like `features[n, m]` instead of `features ! Z .: n .: m`?
One of the key aspects of Numpy is that it uses really very well tested and optimized FORTRAN libraries. Many, many smart people have been working on this code for a long, long time. Do any of the Haskell solutions use BLAS, LAPACK, ATLAS, and/or Intel MKL? It would be difficult to re-implement these things as well as they are already implemented (ie, parallelized and using SIMD, vector registers and hand optimized for specific CPUs and architectures). 
I use shakespeare text package with a quasiquoter: [st| foo baz baz = #{baz} |] Bonus: variable splicing!
&gt; Still, I was used to having a multiset available without any work (however trivial), because of long using a multiset in C++, which I’d used back in the 1990s from the original implementation of the Standard Template Library, … However, `std::multiset` has completely other properties. It doesn't act as `Map a Occur`, but `Map a [a]`, which is quite a difference. For example, if I used a rank wrapper (for high scores in a game, costs in a graph, or similar), and its `Eq`/`Ord` instances don't acknowledge the payload (e.g. I have only relational, but not structural equality/ordering), I lose my original information: import qualified Data.MultiSet as MultiSet data Ranked a = Ranked Int a deriving (Show) -- Custom Eq and Ord instances, since I might use -- Ranked (Int -&gt; String) or something similar instance Eq (Ranked a) where (Ranked x _) == (Ranked y _) = x == y instance Ord (Ranked a) where compare (Ranked x _) (Ranked y _) = compare x y example :: [Ranked String] example = zipWith Ranked (cycle [1,2]) $ words "This is an example of some ranked strings, which are all different in their content." msExample :: MultiSet.MultiSet (Ranked String) msExample = MultiSet.fromList example main = do print example print msExample This results in a rather "interesting" output regarding the `msExample`: fromOccurList [(Ranked 1 "all",6),(Ranked 2 "different.",6)] Therefore, `Data.MultiSet` isn't actually a set, but a counter with biased information storage (funny enough, the information is right-biased for the `fromList*` operations in this case, although both `Data.MultiSet` and `Data.Map` declare left-biased `insert` and `union`). Compare this to the `std::multiset` variant: int main(){ const std::vector&lt;Ranked&lt;std::string&gt;&gt; example = {{1, "This"}, {2, "is" },{1,"an"},{2,"example"}, {1,"of"},{2,"some"}, {1,"ranked"},{2,"strings"}, {1,"which"},{2,"are"}, {1,"all"},{2,"different"}}; const std::multiset&lt;Ranked&lt;std::string&gt;&gt; msExample(example.begin(), example.end()); for(const auto &amp; v : example){ std::cout &lt;&lt; v &lt;&lt; " "; } std::cout &lt;&lt; std::endl; for(const auto &amp; v : msExample){ std::cout &lt;&lt; v &lt;&lt; " "; } std::cout &lt;&lt; std::endl; } Here, all elements with `v.rank = 1` are considered equal (see [cppref](http://en.cppreference.com/w/cpp/container/multiset)), but the "duplicate" elements aren't discarded. This is a real multiset: we still store the elements in their respective bags, but we don't lose the actual elements, whereas `Data.MultiSet` bags let you only read the label and the number of elements. ---- Of course, all of this isn't relevant when you actually have structural equality/ordering.
I agree :) I actually can't make heads or tails of R's syntax haha
No, I mean the entire site is running off a rinky-dink computer in someone's basement, and the owner is worried about it going down due to an unexpected traffic spike. [source](https://www.reddit.com/r/adventofcode/comments/3v64sb/aoc_is_fragile_please_be_gentle/)
So, it's possible to get something from the real world without it going through a monad? Wouldn't this affect the purity of haskell though?
HMatrix for example uses this strategy. I don't know about Repa.
But then you need templates... 
We need to go through this very carefully. IRC?
I'd honestly prefer the Eigen route of using modern optimization techniques rather than boxing up traditional cycle efficient FORTRAN libraries. There are certainly downsides to this approach, but I much prefer writing C++/Eigen compared to Matlab or Numpy (the only LAPACK based environments I've worked with).
&gt; It doesn't act as `Map a Occur`, but `Map a [a]`, which is quite a difference. That confused me throughout the whole article. Also the weird naming - surely `multiset` in C++ is actually a `multimap`, whereas what the author discusses does actually seem to be a `multiset` in that you don't have a key-values association, just a value-occurrence association.
&gt; Well now it looks funny to me and takes extra mental capacity to parse the code Now you know how everyone who doesn't use Haskell feels about `.` ;).
Thanks. Another thing about numpy is that you can operate on arrays in place and reuse arrays to save having to allocate new memory. It makes a decent performance difference but partially because making a new python object is expensive too. Do haskell libs do this, for instance by knowing that the memory won't be reused and so map or equivalent can write over the memory? 
join #bfpg
Functions in Haskell must start with lowercase letters (types like `String` start with uppercase letters).
wow i feel dumb
R can be coded in a functional-ish style (I've tried), but it doesn't even come close in terms of having guarantees about side effects or a type system that offers some confidence. At best, you can take advantage of function composition pretty naturally with dplyr, but frankly it falls far short of what haskell _could_ be if only the numerics ecosystem was active.
I'd better link to [purescript-control](http://pursuit.purescript.org/packages/purescript-control/0.3.0). There is even a cool graph there!
I think this can safely be blamed on GHC's error here, which is admittedly not very good.
Me too. Waiting for you to reply.
I wonder - if we get `Semigroup` and `&lt;&gt;` in `base`, could `mappend` be renamed something like `append`?
I think someone worked on a library for Swift for this, but not sure.
This is great, I've done all the challenges so far - but I have pretty bad code in a few of them, so this is pointing me in the right direction. edit: On day 2 you could have used splitOn instead of wordsBy, right? But that extra library might be useful, checking it out. Oh man I could have used stripInfix or breakOn for day 8!
Okay, I'm there now. Whose you?
Eh, it’s alright. This is not an obvious aspect of Haskell’s syntax, and GHC doesn’t produce a great error message for it. Basically, things that vary (variables, type variables) are lowercase, and things that are constant (constructors, type constructors, modules) are uppercase. One reason it was designed this way is to make generic function signatures more concise. Whereas in C# you might say: List&lt;B&gt; map&lt;A, B&gt;(Func&lt;B, A&gt; f, List&lt;A&gt; list) ~~~~~~ In Haskell the equivalent would be: map :: forall a b. (a -&gt; b) -&gt; [a] -&gt; [b] ~~~~~~~~~~~ But because lowercase things are variables, the syntax makes it clear that `a` and `b` *must* be type variables and *must not* be types, so we can get away with writing: map :: (a -&gt; b) -&gt; [a] -&gt; [b] 
OCaml has functors and applicatives (I think...). `Core.Std` defines a lot of these.
And wrong, since the halting problem is undecidable. The author might have been thinking of NP complete problems like graph colouring etc.
Apply `when ("a" == "a")` to the do with $. What's the purpose behind the `when ("a" == "a")?`
[I wrote a blog post about it a while ago](http://gilmi.xyz/post/2015/02/25/after-lyah) + I heard good reviews on [this book](http://haskellbook.com). If you haven't already, start building your own projects now :)
Decode the JSON into a tuple of `User` (with the well-known properties) and a `Map String String`, eg: `parseUser :: Value -&gt; (User, Map String String)`. Why do you think it's *not quite good*?
Oh thank you so much! :) 
I think there was a similar question a while back and the answer is that the standard implementation avoids overlapping patterns, while yours has a catch-all in the end. Non-overlapping patterns are good in general because they are sure to break if you change your datatype (which is what you want), though that obviously won't happen here. Another advantage is that you are free to reorder them without changing the meaning.
Thank you very much for confirming. I just opened a [bug ticket](https://ghc.haskell.org/trac/ghc/ticket/11183#ticket).
~~Maybe [this](http://ideone.com/QHOlGr) is more obviously broken?~~ ~~I'm no expert on floats, they have a myriad of idiosyncrasies, so it could be that ending on '.0' is expected, but ending with '.' seems weird even for floats.~~ Documentation says to expect a decimal point, so this behaviour is right 
Oh, OK. So no bug in `Numeric.showFFloat` and I just misread the docs.
I'm not sure if "full precision" expects a '.0' at the end of an integer. If yes, then there is no bug. And you need to use `(Just 0)` to retrieve the integer portion of the number. The Alt function preserves the '.' no matter what. It is better to wait someone else to chime in.
Could you expand on this? I'm currently "boxing up" a rather large C numerical library and I'd like to know all points of view. Granted, "boxing up" is a dumber approach than emitting machine-specific code, but I originally set out along this route coming from the computational science angle, not from the compiler angle. What does set apart Eigen? 
Yes, this obviously was a goof, don't know exactly what made me write it. Will correct it, thanks for pointing this out.
Eehhh... I wouldn't go that far. Sometimes it's much better to cut down on verbosity. If you have a data structure like an AST that's likely to change, then sure, be explicit. But if your structure is a binary tree, it's probably staying the same. I'm not really convinced that being resilient to ordering is good enough of a reason.
Oh, believe me, we are active; it's just that we're few and far apart. Development is driven by applications so it goes in all possible directions. And apparently, using a meta-language motivates the exact opposite of interface standardization.. though not all hopes are lost. Vector and Repa are solid enough that most numerical efforts rely on them alone.
OK thanks!
You're entirely correct: `fmap . fmap` is indeed a generic `fmap` which maps the elements of the composition of two functors: &gt; :t fmap . fmap fmap . fmap :: (Functor f, Functor g) =&gt; (c -&gt; d) -&gt; f (g c) -&gt; f (g d) And when we specialize `f` to `(a -&gt;)` and `g` to `(b -&gt;)`, its type is indeed that of the composition of a one-argument function with a two-argument function. It adds a post-processing step to the result of the two-argument function. The more well-known `(.) . (.)` does the same, since `fmap` is `(.)` in this case. fmap . fmap :: (c -&gt; d) -&gt; (a -&gt; b -&gt; c) -&gt; (a -&gt; b -&gt; d) (.) . (.) :: (c -&gt; d) -&gt; (a -&gt; b -&gt; c) -&gt; (a -&gt; b -&gt; d) Now, why can't we write `g &lt;$&gt; f` instead of `(fmap . fmap) g f`? Because when Haskell sees the type `Foo (Bar Int)`, it doesn't see `Foo (Bar _)` applied to `Int`, it sees `Foo` applied to `Bar Int`. It's possible to fix that by wrapping `Foo (Bar Int)` in `Compose`, but then the result is even less readable that `(fmap . fmap) g f`: &gt; :t \g f -&gt; getCompose (g &lt;$&gt; Compose f) (c -&gt; d) -&gt; (a -&gt; b -&gt; c) -&gt; (a -&gt; b -&gt; d) If you're willing to use this wrap-then-unwrap strategy, I've written a wrapper-based [library which can compose functions of arbitrary arities](https://github.com/gelisam/adicity), based on a reddit comment by Conor McBride explaining that composition and application can be unified if the arities of everything is known in advance: :t \g f x y -&gt; runAdicity_0_1 (adicity_1_1 g &lt;.&lt; adicity_2_1 f &lt;.&lt; adicity_0_1 x &lt;.&lt; adicity_0_1 y) (c -&gt; d) -&gt; (a -&gt; b -&gt; c) -&gt; a -&gt; b -&gt; d 
One useful thing I use LTS for is to test my packages against a particular baseline I can expect others to have. But a nightly on the first of each month would satisfy this use case. Another use of LTS I have is to use LTS-2 to test against GHC 7.8. As far as I know, there is no nightly stream that uses 7.8.
This is my licentiate thesis, describing the design and implementation of the Haste compiler, the Haste.Foreign FFI and the Haste.App programming model. The FFI and App chapters are based on [previous](http://ekblad.cc/ifl15.pdf) [papers](http://ekblad.cc/haskell14.pdf). For those who aren't intimately familiar with Swedish higher education, the licentiate degree is kind of a "halfway to PhD" milestone. Sometimes it's used as an early out for those who don't intend to complete their PhD but still want to get a postgraduate degree out of their efforts. In other cases its meaning is more like "wow, I've been going at this a while without getting a new degree now, better get myself a pat on the back".