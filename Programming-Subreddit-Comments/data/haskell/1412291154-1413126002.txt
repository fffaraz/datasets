Scotty is super simple to get started with. Yesod was way too big and confusing for most of what I wanted to do. 
&gt; What is the answer to those critiques? Having had this discussion with more semantics-minded folks the discussion usually tends to boil down to foundational design assumptions of the language. Haskell has always been a general purpose language ( i.e. it's an industrial strength language, not a vehicle for type theory research ) that had two core ideas: laziness and type inference. Adding full SML style modules severely complicates the type system and you need a core calculus at least a powerful as System Fω to fully embed them. This gives you more expressivity at the cost of making certain typing problems much more difficult, namely that inference is intractable in many cases which diverges from Haskell's design substantially. At which many people in the Harper school of thought will claim that we should just do away with type inference entirely, just add toplevel declarations to all functions and do bidirectional elaboration and typechecking. Which still ignores the fact that full inference is still extremely useful in practice and part of the reason why many people like myself use Haskell in the first place. If there's one thing that that bothers me about Harper's arguments is that they tend to have this *one language presupposition* baked in. That somehow exploring multiple avenues in the type system design space and having specialized languages is not productive. Some of his posts he just flat out [declares by fiat](https://existentialtype.wordpress.com/2011/03/27/the-holy-trinity/) that any type system design isn't in his chosen research areas isn't worth doing and "cannot be a manifestation of the divine". tl;dr Harper's argument is right is if you buy into his assumptions that expressivity of modules are more important than the complications they bring. If you don't buy into that, then typeclasses are a natural solution that maintains inference and gives practical overloading but aren't able to express the full generality of modules. It's a function of which power/weight ratio you're willing to buy into. There isn't one right answer and having languages which explore both is a good thing!
`(x:xs)` and `['\n'] ++ xs` have the same length, so if the former equals 0 mod 10, then so does the latter! Your program keeps chopping off and adding the same `'\n'` character. Try something like then ('\n' : addNewlines xs)
Was the transition from Haskell98 w/o hierarchical modules to Haskell2010 really just mere polishing compared to burning bridges?
That this has been the status quo for 16 years doesn't mean much to me. Just for the sake of the argument, 16 years of inaction could arrive at a similar outcome: it doesn't mean much. In any case, I understand your concerns. My comment was a bit more general and less about the specific change. In fact, if we consider that there are so many different stakeholders with a vested interest in haskell, maybe the obvious course of action is to remove things from Prelude instead of adding. Make it as thin as possible. That too would be a change and people should be open to that possibility too, imo. 
That's really hopeful, thank you. 
I see why Depth isn't a Monoid, there's no neutral element. But it does seem to be associative?
Your reply seems to be off topic. Perhaps it's just too oblique for me I am asking for the experiences of others after sharing mine. I am asking experienced Haskell users "is your tool less productive than this tool that is in some ways similar that I assume many of you also have experience with". I am explicitly asking for these generalizations I want to continue on and would prefer to end up mostly using Haskell. I am having doubts about its relative productivity. Do you have an answer for me about your productivity in the languages? Or about general things, more objectively done, that assuage my doubts?
Ha I've read that page several times. It's one of the reasons I've kept powering on in haskell (also doubts I might be accidentally benefiting from that earlier lisp experience) Thank you for it
Your usage of the length function makes your code O(n^(2)). Haskell strings don't keep track of their length.
Reading Harper's blog posts is like swimming through a sea of bitterness and snark to find pearls of wisdom. Not to turn the technical into an ology, but I'll pass on this discussion in favor of more positive avenues.
Hell, I would almost be happy if some of the too specific functions were removed, if only because I find Haskell's import hiding mechanism to be kind of crummy. Though that would be much more backwards incompatible, which I actually think would be a bad thing; the main reason I personally support the Foldable/Traversable change is that as far as I can tell it's backwards compatible up to ambiguous types, so in the worse case at compile time you realize you need to add a few type signatures. I suspect in most cases the compiler will still be able to figure out the types; it's not like it causes the type of ambiguity that something like show.read does. Though maybe there are issues I'm not aware of. It seems to be a more contentious issue than I expected.
Just figured it was either a borrowed or convergent idea, and I guess it's the latter. I do like the idea, but I think using new syntax versus a pragma would be better, simply because it's something that the type checker should be verifying for correctness. In my example, I would want a compiler error if I wrote default instance FooClass a where a = Int because `Int` does not have an instance for `FooClass` (in scope, someone could define one later and then override the default). It also is similar to the `default` syntax, so there's that.
It also makes the function stricter than it should be.
It's noteworthy that diagrams use finger trees for trails and paths.
i think your title might put people on the defense. it is probably better to ask, and not in a haskell forum, "having developed in both, do you find you work faster in clojure or haskell?"
Yeah, appears so. Can't change titles sadly. Not sure that question gets the type of info I'm looking for either It sounds like some people from the small answer set find it no slower to write at least. That's hopeful!
Use Snap!
also check out this: http://bitemyapp.com/posts/2014-04-29-meditations-on-learning-haskell.html especially his own experience at the end, as it is re: clojure
I think Nix may have done this in the past, but doesn't anymore. Not 100% certain on either point.
I'm an ex-lisper myself. It helped with some stuff, but not a ton. ML would've been more to the point but that has its warts too. I'm glad my guide helped! We have an IRC channel if you ever want to chat. :)
Whoops, my bad. Now I can't find the paper. Will fix it tomorrow.
&gt; I'd be plenty happy with &gt; &gt; {-# LANGUAGE NoImplicitPrelude #-} import Prelude.Polished Why can't you do this right now? Or are you saying that this is what you are doing? But then what is the problem?
I learned Haskell after Clojure as a way to better understand functional programming. I much prefer Haskell. I still do a little Clojure, but for most things I reach for Haskell first. In my experience, * Most of the time, I'm roughly equal getting a system up and working, maybe slightly slower in Haskell, but that's more than offset by my other points. * I regularly spend a lot of time tracking down bugs in Clojure that the type system would catch in Haskell. * I am much more effective at refactoring in Haskell. * Haskell seems to also be more maintainable in the long term. I can get back into a code base much more quickly. 
&gt;I spend a fair bit of time just writing type signatures and exploring where I'm making bad assumption by it failing to type check before I write any code. I am more productive in part because of this. I am now thinking about the problem instead of thinking about all the annoying details of how to not mess up writing the code. I will try this approach. I think I may need to be primarily considering all the type transformations more to be super productive in Haskell. &gt; I started with C and that experience put me into a mindset of being super concerned about all the myriad of ways I can mess up to catastrophic consequence That's one of the reasons I'm feeling Haskell is the better option longterm, even though Clojure is easier now. Just testing the hell out of things feels like a cop-out when provable (or at least more provable code) is accessible in a similar order of magnitude of effort.
With GHC's bazillion extensions, I think Haskell is now on par with ML in terms of the functionality offered by modules. Consider the example from [Wikipedia](http://en.wikipedia.org/wiki/Standard_ML#Module_system). By turning on `TypeFamilies`, we can get a good approximation: class Exception (QueueError t) =&gt; QUEUE t where type Queue t a data QueueError t empty :: proxy t -&gt; Queue t a isEmpty :: proxy t -&gt; Queue t a -&gt; Bool singleton :: proxy t -&gt; a -&gt; Queue t a insert :: proxy t -&gt; a -&gt; Queue t a -&gt; Queue t a peek :: proxy t -&gt; Queue t a -&gt; a remove :: proxy t -&gt; Queue t a -&gt; (a, Queue t a) data TwoListQueue instance QUEUE TwoListQueue where type Queue TwoListQueue a = ([a], [a]) data QueueError TwoListQueue = TwoListQueueError empty _ = ([], []) isEmpty _ (ins, outs) = null ins &amp;&amp; null outs singleton _ a = ([], [a]) insert _ a ([], []) = ([], [a]) insert _ a (ins, outs) = (a:ins, outs) peek _ (_, []) = throw TwoListQueueError peek _ (ins, a:outs) = a remove _ (_, []) = throw TwoListQueueError remove _ (ins, a) = (a, ([], rev ins)) remove _ (ins, a:outs) = (a, (ins, outs)) instance Exception (QueueError TwoListQueue) where ... insertTwice :: QUEUE t =&gt; proxy t -&gt; a -&gt; Queue t a -&gt; Queue t a insertTwice inst x = insert inst x . insert inst x We use a sort of 'witness' to determine which module we're calling, e.g. rather than `TwoListQueue.empty`, we call `empty (Proxy :: Proxy TwoListQueue)` (this can easily be made nicer). The actual queue type, `([a], [a])`, is defined as an associated type of `TwoListQueue`, so we can declare as many instances as we want for `([a], [a])`.
I actually find Haskell to be pretty verbose compared to some other things I've worked with (F# probably most noticably, owing to how similar it is). It's less verbose than other languages intended for programming in the large, like Java, but it's one of those beasts for the most part. Although it'd probably only take some script-friendly libraries to change it I do share the feeling that Haskell makes small projects with partial, fragile solutions more cumbersome than they really should be, with more of the emphasis placed on being able to write bullet proof, reliable libraries.
a codementor is the categorical dual of the dementor
weirdly enough, the first thing I thought of: addNewLines l = concat . zipWith (:) l $ cycle (replicate 9 "" ++ ["\n"])
That is what I'm doing essentially. The problem is that new users will have to go through the same thing I did: essentially trip over the fact that lots of functions in the prelude are inconsistently not general, try to work around it for a while, eventually get fed up and look for a new prelude, find one, realize after a while that their code can't interoperate with other libraries because the new prelude decided to redefine monad, look for *another* prelude, ad infinitum. My eventual solution was to make my own (essentially base-prelude with the unsafe functions removed), experience some issues with cabal sandboxes being able to see it, eventually fix those issues, ... I still don't know if I'm setting myself up with bad habits, because as much as I understand all about monads and comonads and arrows and endofunctors, my experience with the Haskell ecosystem itself has left me very underconfident. I basically feel like if it's not in a type or stated as a law, I can't rely on the language for anything. And since plenty of useful behaviour isn't stated as part of the types (hell, most useful behaviour, or programming just be unification), it's an unpleasant feeling. I don't think new users should be expected to deal with that, but as soon as I started to write any actual code I started to experience the problem. Is my prelude any good, or am I going to find a bunch of issues later on? I have no idea, but for some reason I'm expected to design the defaults of a language I'm still learning. Punting the issue to another library like classy-prelude or base-prelude or basic-prelude... doesn't really help either, because I'm still a new user being expected to choose which library is best, when I don't have the background to make that choice. The problem isn't that you have to workaround defaults, it's that you feel the need to go looking for new defaults at all. Then you've also got the issue of library dependencies. If I were to release anything I'd have to change stuff to use the default prelude anyway, since for better or worse people don't like having alternative preludes as a dependency. We need sensible module in base. It doesn't have to be called Prelude, but it should definitely exist. So I think defaults are very important; the standard libraries have a privileged position, and if enough people have felt they're lacking to go out and re-design it then maybe that's evidence it should give up that position, or at least share (since backwards compatibility is a legitimate concern). I might be able to design my own little prelude, but that doesn't mean I can design a good one, and it doesn't mean I can put it in base. If I can't rely on the prelude as being a reasonable minimal set of what I should know for Haskell, then I can never rely on myself being sure I understand enough to consider myself more than a beginner. I recently came across readMaybe, after spending a bunch of time using try.readIO (and being tempted to use unsafePerformIO.try.readIO). Tomorrow I'm sure I'll learn something else I've been relying on was similarly terrible. The standard should encourage people to follow good practices, but it doesn't. Maybe if I learn everything in base I'll be able to confidently say I understand how to write reasonable good Haskell code. I'm still sticking with Haskell despite what I feel are warts, but my current policy is to explicitly not recommend new people learn the language unless they have a particular need for STM or some other thing that Haskell does exceptionally well, or they've got someone to hold their hand and walk them through the ecosystem. Coming from python, I feel like I've traded a language which encourages good practices with the standard libraries but bad practices with dynamic-mutable everything, for a language that has the opposite problem. And seeing as standard libraries can be changed, I think they should, as long as they are indeed part of the standard. Anyway, that my 2c. I don't think I have much more worthwhile to add, so I'm bowing out of the discussion.
 chunk n s = chunk' s where chunk' s = case rest of [] -&gt; [c] _ -&gt; c : chunk' rest where (c, rest) = splitAt n s addNewLines = unlines . chunk 10 Maybe?
this depends on the users definition of `better`.
Are there any examples of using the fingertree implementation that's on hackage? 
Co-dementor. Puts happy feelings into me.
Perhaps hash trees? Haven't thought about it deeply.
It took me about 8 months to rejigger my thinking to become productive again after switching to Haskell. Afterwards there was no real comparison, I'm night and day more efficient at writing Haskell code than I was at working in other languages before. I do construct code differently than I used to. I just write down a type for a part of what it is that I'm working on, then write out all the instances that flow out as consequences of that type before moving on to the next part, bottom up organically trying to evolve the best possible pieces from which a solution can construct itself. As a consequence I don't write down the solution to one problem, but solve the whole space around it, then get the solution I want as an obvious case.
I know you asked to disregard the post, but in, consume (pieces, str) = (pieces ++ take 10 str, drop 10 str) Note that, take 10 str :: String pieces :: [String] Therefore `something ++ take 10 str` implies `something` is a string. In this case, you expect it to be a list of strings `[String]`. So you probably wanted `pieces ++ [take 10 str]`. After that, your `split` should work just fine. Now a minor kink in `addNewLines`: addNewLines = concat -- :: [String] -&gt; String . intersperse '[\n]' -- :: [String] -&gt; [String] . split -- :: String -&gt; [String] You probably meant to write `['\n']` or `"\n"`.
Hmm, you can always just use lists for everything in Haskell, and then it is suddenly Clojure...
snap-server has been around longer and has better test coverage (the latest development code is at 100%). Beyond that, it's probably more a question of which ecosystem you prefer. We have several public facing websites running snap. One of them has been serving more than a million hits per day on a mission critical site for over two years. It's quite solid.
Wish I had a time-lapse video of this process. I see it working for libraries, but can't wrap my head around how this would work* for end-user applications. Alternatively, I'd take a git repository with tags annotating exactly when each bit happens. Maybe I'll dig through some of your smaller projects' revision history and see if I can recognize such patterns. * Note, I'm not arguing that it doesn't work: one need only look at your github repositories to see that it does! Rather, I can't properly visualize going from a simple type, to instances for that type, and as a natural consequence, the proper answer.
Add a bumper sticker like "My other car is a cdr"
I switched from clojure to haskell a while ago. And now i am writing code in haskell just as fast as in clojure. Add to it that now haskell helps me catch much more mistakes so my productivity is actually higher than used to be with clojure. But yes, it will take you time to get there. 
Once seen cannot be unseen.
Not as efficient as random access would be, obviously :)
Kmett is sort of the Grothendieck of Haskell, I would say…
It depends what you mean by efficient :) They have very good asymptotic properties. The constant factors are quite high (this is why `Data.Sequence.Seq` is highly specialized on the measure rather than using a generic finger tree structure). My personal view is that this structure is ok for moderate-large scale datasets that don't require really high performance. For stuff that you're trying to tune to the utmost, it's probably not a good structure. Although as a data point, my splaytree package uses exactly this technique (http://hackage.haskell.org/package/splaytree), and for some specialized code it was almost as efficient (within 10%) as a very highly tuned, purpose-built 2-3 tree.
It's actually quite easy. I made sure you can compile the `base` package separately (i.e. w/o having to build GHC first, but you need a compatible GHC from somewhere, for instance from my PPA); e.g. for writing patches for `base-4.7.0.1` you'd need a GHC 7.8, for `base-4.8.0.0` you'd need `ghc-head`). You need to clone at least `ghc.git` as `base` is part oft that repo now. Then you can can simply `runghc Setup.hs {configure,haddock,...}` and start your edit/haddock/view iterations. And when you're done, you can submit your patches directly to https://phabricator.haskell.org (which will verify for you your patches don't break the GHC build) Now that I think of it, I really need to write this up somewhere, I guess, as the process is rather easy if you know the few steps involved, but it's non-obvious otherwise (and before someone asks: sorry, no, GitHub PRs are not supported for `base` as they don't fit our current workflow w/ Trac &amp; Phabricator; however, most other core libraries use and encourage the GitHub workflow over at https://github.com/haskell/) 
I trust them, as many trust them. Since they are in Haskell, and usually serve an app written in Haskell that is statically compiled to it, I "expect" them to be a lot safer out-of-the-bow then the very popular Nginx/Apache + dynamic language setup. But what more can I say then "expect" :) I'm not a security researcher, though quite a few paper have been written on the topic of how Haskell can help to increase the security of a piece of code.
The point is that finger tree can be used to implement pretty much *any* data structure that is traditionally implemented with trees: priority search queues, interval trees, `Data.Map`, ... It is by no means restricted to random access lists. Also, even in the case of linked lists, the finger tree implementation has two properties that an array has not: 1. It supports concatenation of two lists in `O(log n)`. 2. It is *persistent*, which means that any old version of the data structure is still available. In contrast, arrays are usually used *ephemerally*, which means that once you update the array, the old version is gone forever.
I'm confused about the notation used at 'Constructor application' section: Heap: C x y (downarrow cost) Heap: C x y AFAIU it should mean that constructor C is applied, which has a cost, but why are the two sides of the operation the same? Shouldn't have something changed during the application? Or is this the cost of just having C x y on the heap?
These are persistent data structures. There's a lot of value in that. 
Depends on the use case. These have the property of persistence. An array-based implementation would not.
What purpose do the `proxy`s serve here?
Yes. My reply was implying that they rewrite the same algorithm in vimscript.
How does the splaytree package compare to containers?
Thanks for writing this. Simple, practical and not so trivial little problem. Out of curiosity, how is this handle by other web server that lives in a (J) virtual machine ? Simply by relying on the JIT ? Would it make sense to rely on a JIT LLVM optimization for this ? PS: If I am not mistaken the [RecordWildCards extension is not needed anymore](https://github.com/yesodweb/wai/blob/master/auto-update/Control/AutoUpdate.hs#L2)
Would you mind elaborating on some papers that I can read to understand what 'MTC style unification of type classes with modules' means? EDIT: MTC as in modular type classes like [this](https://www.mpi-sws.org/~dreyer/papers/mtc/main-long.pdf)?
i guess to make the specific `QUEUE` to be used easily changeable in one spot (when having a `where proxy = Proxy :: Proxy TwoListQueue` somewhere and not rely on typeclass resolution.
re 2. it is implemented, at least it was on hacberlin.
You can actually render the routes in a type-safe matter, the functions for that are just not exported yet. The function is already defined in our underlying routing package: http://hackage.haskell.org/package/reroute-0.1.0.0/docs/Web-Routing-SafeRouting.html (renderRoute). The reason why it's currently not exported is that you would need to pass around the routes all the time. We're still looking for a good solution for that... But you're right - you can render roots that don't have a handler defined, too.
Exactly.
In the current implementation if parsing fails the route just doesn't match. This is important because you might want the framework to try a different route before showing the user an error.
For me the trouble here is that I have to define a type for every handler I use. Of course in some cases this might be desirable, but I wanted something more lightweight.
To Question 1: Yes - but as mentioned above we wanted a framework without any template-haskell or quasiQuote magic and we didn't want to define a new type for every route/handler. To Question 2: Yes! There's already a function "renderRoute" in our routing framework ( http://hackage.haskell.org/package/reroute-0.1.0.0/docs/Web-Routing-SafeRouting.html ), but we still need to find a convenient way to plug that into Spock itself. 
HLint can confirm that and suggest it for you. Just see the bottom of http://lpaste.net/111999 (where I pasted the code), which says: 2:1: Error: Unused LANGUAGE pragma Found: {-# LANGUAGE RecordWildCards #-} Why not remove it. Note: you may need to add DisambiguateRecordFields
Originally I had trouble getting it working, but I've played with it a little. I need to invest more toward that. I'm not sure that will really help, though. I think I'd end up banging my head against a wall long enough to realize I need the core.typed's help. Add annotations. And then hopefully get help. But I'd still be spending a lot more time with those bugs than I would in Haskell. Or maybe I should always use type annotations from the beginning, but then why not just use Haskell from the start? ;) 
&gt; i.e. it's an industrial strength language, not a vehicle for type theory research I think you got that the wrong way around. GHC Haskell is only "industrial strength" because of its age and because of the high volume and quality of research that went into it.
The rust language has some nice ideas with their borrowing semantics, which are verified by the compiler, but I never programmed anything in rust, so I can't tell how good it works in real projects.
The `chunk` function is also part of the `text` package as: chunksOf :: Int -&gt; Text -&gt; [Text]
I use the template-haskell-free core of the yesode router for http://hackage.haskell.org/package/route-generator -- it's very nice, and more efficient than most routers
&gt; There are two fundamental problems with type classes. The first is that they insist that a type can implement a type class in exactly one way. For example, according to the philosophy of type classes, the integers can be ordered in precisely one way (the usual ordering), but obviously there are many orderings (say, by divisibility) of interest. This isn't a bug, but a feature. First of all, if you have multiple implementations for a typeclass, you can implement all of them using newtypes. Second of all, if you can have multiple instances for a type, you usually have to care about that. For example, if you know two tree sets must have the same ordering, then you can use an efficient hedge merge to union them. If you lack that guarantee, the best you can possibly do is to insert all the items from the second into the first, which is asymptotically slower.
Are you commenting on LiteApp, or proper type safe Urls? If the former, you actually don't have to define a data type, but the type safety appears to be stronger than Scotty. If the latter, you're absolutely correct.
That comment is incredibly high praise, I truly appreciate it, thank you.
Haskell has [arrays](http://hackage.haskell.org/package/array) if you want arrays. This data structure is not an array. (Assuming that your measure incorporates the size,) it is O(log n) instead of O(1) for accessing the ith element, but also O(log n) for deleting the ith element or inserting an element after the ith element where an array would be O(n). apfelmus mentioned other properties of finger trees that are not shared by arrays. Which data structure you choose should be determined by what operations you need and what performance you need them to have, not by implementation language.
Haha, I totally saw `Depth` and assumed that bss03's instance was this one, oops!
Ah, I see. My bad.
Fingertrees are sweet to have in your toolbox. Like rwbarton says: &gt; Which data structure you choose should be determined by what operations you need and what performance you need them to have, not by implementation language. And you can view fingertrees as "data structure constructors", so to speak. Something really good to have in your toolbox of persistent data structures. 
From a math point of view, yes, but it might be hard (or impossible?) to compute with the "monoidization". For instance if I have a magma structure on a type m with decidable equality it doesn't seem like there would be a way in general to decide whether two values of type m become equal in the monoidization (how would I know whether there exist `m3`, `m4`, `m5` such that `m1 = (m3 &lt;&gt; m4) &lt;&gt; m5`, `m2 = m3 &lt;&gt; (m4 &lt;&gt; m5)`?) So there still might be some use for this from a computational standpoint...
The amount of magic in Yesod is on a whole different level, though. I don't have any experience with Snap, but it seems to be **much** less complex than Yesod's TH settings, routes, templates, models and the 100 file scaffold.
I know very little of ML, but my understanding is that you would instantiate a module with a key type and a comparison function, and the resulting module would contain a tree type that could be efficiently and type-safely combined with itself.
&gt; I think we're disagreeing about the meaning of the word prevent Well, yeah. I prefer to use the word "prevent" only when the programmer is indeed incapable of expressing something (without resorting to functions like `unsafePerformIO`). For instance, `Data.Map.Map` prevents us from modifying the internal structure of the tree.^* ^* A notable exception would be the function `Data.Map.fromAscList`, which, however, does come with an informal precondition that we have to check. &gt; I just mean that using Text makes it more natural to do things the right way, whereas the plethora of list-processing functions that will accidentally make mistakes with textual data makes [Char] more error prone. It's a stylistic debate then: whether `newtype Text = Pack { unpack :: [Char] }` makes it more natural to avoid unicode text mistakes than a plain `[Char]`. Personally, however, I don't think that the current `Text` makes a good trade-off here: on one hand, it does have a correct `toLowercase`, but on the other hand, it just reimplements many list functions verbatim, and some not even adapated to Unicode (`break`). I think I can get a better bang for the buck by using `[Char]` for the latter list functions and `toLower :: String -&gt; String` for the former unicode things. I general, I think that type synonyms work for this purpose. Another prominent example would be `FilePath`. The `System.FilePath` module encourages correct use, even though we can still apply all list operations on the type `FilePath`. 
I don’t know — I think the snark and polemic, while obviously completely over-the-top, often lets him make his points fairly well. He ends up not having to hedge them around with all the mostlys, sometimeses and so on that the rest of us might use, and not wasting time on apologies when he says why he thinks his way is better than every other way. You have to read them with a *huge* pinch of salt, and mentally add back all the perspective and qualifying restrictions that are missing. But he gives one point of view, and gives it clearly and vividly, and as you say, it’s a point of view that often does offer pearls of wisdom and insight that you can take away afterwards.
Two reasons: 1. The type class instance can't be determined from a `Queue t a` because it's a type family, so the proxy aids instance resolution. 2. Alternatively, I could declare `data TwoListQueue = TwoListQueue` and have each function take in a `t` instead of a `proxy t`. However, the latter guarantees that my module implementation will always behave in the same way.
It remains to be seen if community events will drive me to renounce all citizenships and move to a monastary in the mountains of France, while trying to get everyone to destroy all my previous work. Hopefully, the GHC 7.10 release cycle will be kinder to my psyche than that. ;)
Do you mean Scotty or Spock? Scotty doesn't really provide any compile-time typing of routes. Spock(Safe) makes sure that your handler only get's called when all types of the params are correct. I'll have to take time and look into LiteApp before I can say anything about it. At first glance I find that the "syntax" is not simple enough...
So it's an apparition that distributes souls to depressed people, causing them to become criminals? EDIT: apparently, /u/semigroup already made the joke [later](http://www.reddit.com/r/haskell/comments/2i3ttm/tutorial_implementing_highly_efficient_data/ckz3mnw) in the comments.
i meant Spock, sorry. It may be that I misunderstood the scope of the safety in this new setup, it may be just as safe as LiteApp.
Yep. You're right. 
Am I missing something or how is the readIORef thread safe? How is it ensured, that during the writeIORef a requestor of the result of the update action isn't seeing something that isn't a Nothing value anymore, but also not the complete right Just value? 
I said it wasn't a vehicle for type theory research, not that it wasn't a vehicle for compiler research. In my experience these two groups intersect very little, most pure type theory people in the Harper school of thought don't seem to care if they can write practical programs that can compile to efficient code at all. They care about proving theorems and modeling math and logic. I would disagree that GHC Haskell is *only* industrial strength of it's age, and assert that it reached fruition while other languages of the same era died (SML) because the people involved didn't prioritize compiler engineering at the level of GHC. 
writeIORef is supposed to be atomic in the sense that the write itself is atomic and the object pointed to by the new value is visible to any reader that can see the new contents of the IORef. From the [documentation](http://hackage.haskell.org/package/base-4.7.0.1/docs/Data-IORef.html#g:2): &gt; The implementation is required to ensure that reordering of memory operations cannot cause type-correct code to go wrong. In particular, when inspecting the value read from an IORef, the memory writes that created that value must have occurred from the point of view of the current thread. How (and whether) GHC actually implements that correctly on every architecture is another matter :) x86 does not reorder stores past other stores, so then the only problem is making sure the stores are not reordered in the compiler. But many other architectures do (e.g. PPC, ARM).
There's a tutorial for the web-routes approach [here](https://github.com/lukerandall/snap-web-routes/blob/master/README.md.). In your example, if you had this: get "/regex/{number:^[0-9]+$}" and changed it to this: get "/myregex/{number:^[0-9]+$}" You would have to go and change all of the links like &lt;a href="/regex/..."&gt; Web-routes unifies this so that you won't end up with broken links like this when you change a route's URL because all your URLs will be generated from the same definition that defined the routing table.
Sure. Spock didn't even exist when I first wrote that :)
Ok, thanks, but why is there then an atomicWriteIORef at all? And reading the docs of atomicWriteIORef regarding multiple IORefs feels a bit strange ...
I spent most of my time when writing Haskell code running things in ghci, so I wouldn't say it feels "big-bang slow", no.
atomicWriteIORef is for situations in which you want ordering guarantees between operations on multiple IORefs. Ordering between constructing a value on the heap and updating an IORef to point to it is the compiler's problem, and writeIORef should be sufficient for that.
Thanks for this.
In the Scala implementation of immutable Vectors, the branching factor is 32, so indexing is O(log_32(n)). For many practical purposes, that's constant; log_32(2^(35)) = log_32((2^(5))^(7)) = log_32(32^(7)) = 7.
Good idea (and this is how the automated builder did it too). I've updated the script.
&gt; So you are saying that you could also have [...] and the `proxy` disambiguates. Exactly. &gt; Would it be sensible to do the proxy plumbing with a `Reader` monad? That would work too. But instead of tying ourselves to the `Reader` monad, I'd rather put in a little extra work to emulate something like a scoped module import. e.g. in one module (assuming we have record punning turned on): import qualified ThatModuleIPutQUEUEIn as QUEUE data OpenQueue t = OpenQueue { empty :: forall a. Queue t a, isEmpty :: forall a. Queue t a -&gt; Bool, singleton :: forall a. a -&gt; Queue t a, ... } open :: QUEUE t =&gt; proxy t -&gt; OpenQueue t open inst = OpenQueue { empty = QUEUE.empty inst, isEmpty = QUEUE.isEmpty inst, singleton = QUEUE.singleton inst, ... } -- Example usage: twoSingletons :: QUEUE t =&gt; proxy t -&gt; (Queue t String, Queue t String) twoSingletons inst = let OpenQueue { singleton } = open inst in (singleton "hello", singleton "world") It's a little tedious, but we can probably automate it with some `TemplateHaskell`.
Thanks for the insightful reply!
Do you mean twoSingletons :: QUEUE t =&gt; proxy t -&gt; (Queue t String, Queue t String) ? This looks like an interesting way of doing things. I'll have to play around with it. When I was reading about Lennart's complaints about replicating ML modules in Haskell I came up with something similar based on `ImplicitParams` but the type inference is weird. http://h2.jaguarpaw.co.uk/posts/modules-for-lennart/
Also, if you're going to do this it's unclear what purpose the `proxy` serves. Why not just pass the `OpenQueue` instead? twoSingletons :: OpenQueue t -&gt; (Queue t String, Queue t String) twoSingletons openQueue = let OpenQueue { singleton } = openQueue .... 
I wish I could comment inline on github, but I'm worried about (not that any of these are necessarily correct, just that it would be great if the comments guided me through them): - possible race condition at lines 126 and 104-106 when readMVar is non-atomic (ghc &lt; 7.8), - async exception safety ( `mask_` and handlers on blocking operations), - possible assumptions about visibility and memory ordering in IORef reads/writes (are barriers needed?) 
Ah, maybe you need the `QUEUE t` constraint to get access to the `Queue` type family. Hmm ...
I like this approach. It is simpler than the more heavy weight approaches in Yesod or Snap or web-routes, and has worked really well for me in the past. I was working on something similar using variadic functions, but was concerned about performance and thinking of using HLists instead. Glad to see someone beat me to it! Another interesting possibility would be to allow `var` to carry around a name, so that it might be rendered as a URI template. 
&gt; Ah, maybe you need the QUEUE t constraint to get access to the Queue type family. Hmm ... That's not necessary. I could declare the type family outside of the type class, and it would work just the same. You're right in that we could just pass some `OpenQueue` around instead. However, there's no longer anything preventing someone from passing around different implementations of the same module. For example, this is valid: impl1 :: OpenQueue TwoListQueue impl1 = ... impl2 :: OpenQueue TwoListQueue impl2 = ... With type classes, I know that each module function (e.g. `empty`) will behave the same regardless of what `proxy` I pass to it. If someone passes me a `OpenQueue TwoListQueue`, then I have no idea whether I've been given `impl1` or `impl2`. In ML, this would be akin to declaring two modules with the same name (note: the module would be called `TwoListQueue`, not `impl1` or `impl2`).
&gt; That's not necessary. I could just declare the type family outside of the type class, and it would work just the same. My experience as detailed in the article I linked makes me think that declaring the type family outside the type class won't work so well. You seem to lose some instance resolution and the inferred types become strange. I think this exact approach (i.e. `twoSingletons :: OpenQueue t -&gt; (Queue t String, Queue t String)`) is one of the exact things I tried, or maybe Lennart tried it before me. Perhaps I'll have to have another play. Anyway, your confidence on the matter gives me hope this can work out! &gt; However, there's no longer anything preventing someone from passing around different implementations of the same module. That's certainly a decent reason. 
But, beginners are not only students! They might be software engineers which are already familiar with other functional languages.
It... works. After starting at it for a few minutes, I can even see why. That's some impressively twisty thinking. Not so sure about the readability.
&gt; Anyway, your confidence on the matter gives me hope this can work out! Well, I'm a little less confident now that I've actually tried my code and found out it doesn't work! The signature `empty :: proxy t -&gt; Queue t a` is problematic because there's nothing to determine what `a` is. This can be solved by changing `type Queue a ` to `type Queue :: * -&gt; *`, but then we have to declare new types for our implementations, since `([a], [a])` isn't `* -&gt; *`... Sigh, back to the drawing board. (That being said, my claim that moving a type family outside a type class doesn't change anything still stands). EDIT: On second thought, we just need the `a` parameter to be injective. We can define an extra associated `type QueueParam t s` (where `type QueueParam TwoListQueue ([a], [a]) = a`), and add the constraint `a ~ QueueParam t (Queue t a)` to our functions. Alternatively, we wait for injective type families (which might be in 7.10!).
I just drafted http://barratt.us/codercorner/2014-10-03-ZipWith to make an effort at explaining this example. To be fair, I came up with that function after unfortunately deciding to check reddit on my phone while falling asleep, but I think it demonstrates some Haskelly things.
I caught a cold and my head is killing me but I intend to adapt the code suggested by chris when I get better. If only I could understand lisp a bit better. Hoping i can read it better without a mean headache though ;)
I don't know if it helps, but a common pattern in Haskell is to actually not construct an end-user application. You make a library for creating end-user applications, and then the actual end-user application comes almost as an afterthought, being just a thin interface to your library.
I think I'm getting better at reading Haskell these days. I'm not so intimidated by seeing (:) out of contexts like 1:2:3:[] but I know I would have been as a beginner. My metal decoding process when something like this: 1. zipWith. Expect function-list-list. 2. What are the lists? One is l (the font tricked me into thinking that was a pipe until I copied to ghci, but that's reddit) and the other must be 3. Copy cycle (replicate 9 "" ++ ["\n"]) into ghci. See that it does something quite simple. Stare at it. Realise it is quite simple. 4. Mentally put (:) between corresponding members of l (of type character) with a member of of cycle (replicate 9 "" ++ ["\n"]). Mentally node that the type would be list for each element. 5. Not that. concat just sticks everything together at the end. Actually, it's kind of interesting thinking about the differences between how I decode both haskell code and mathematical equations vs. imperative code. With imperative code I often mentally find myself starting up an "interpreter". "Variable i is 3, then subtract 1 from it and dereference the array" but with Haskell it's often more about about deducing facts. 
When I program in Python, I feel waaay faster then in Haskell. And when working on programs that are in the "few hundred lines of code" range, this might even be the case. But as soon as I need to actually write longer, complex, algorith heavy code, the type system is a perfect crutch that keeps me from messing up, and lets me push through faster and with fewer errors. 
also: intercalate x = concat . intersperse x 
To be cliche... *this*. Monad, Applicative, and Alternative/MonadPlus are the biggest problems, because they generally result in fairly complex behavior, are critical to the usage of a type, and their implementations are totally undocumented.
Huh, that is quite an awesome prelude. I am gunna have to start using that.
Yes! The exact method described here is used in Data.Sequence: http://hackage.haskell.org/package/containers-0.2.0.1/docs/Data-Sequence.html
The code can deadlock, though not likely, if the thread requesting a value doesn't get scheduled before the delay is over: -- Thread A do takeMVar needsRunning -- Blocks -- Thread B do mval &lt;- readIORef currRef case mval of Nothing -&gt; do void $ tryPutMVar needsRunning () -- Unblocks thread A -- Thread A a &lt;- catchSome $ updateAction us writeIORef currRef $ Just a void $ tryTakeMVar lastValue putMVar lastValue a threadDelay $ updateFreq us -- Blocks, but doesn't gaurentee that Thread B runs. writeIORef currRef Nothing void $ takeMVar lastValue -- lastValue is now empty takeMVar needsRunning -- Blocks! -- Thread B readMVar lastValue -- Blocks!
Good catch. I believe one option here is a [condition variable](http://en.wikipedia.org/wiki/Monitor_\(synchronization\)#Condition_variables). Thread B (and others threads like it) wait on a condition variable that indicates if currRef is ready. Thread A does signalAll on the condition variable whenever it updates currRef. Thread B runs a loop that waits on the CV, and when it's ready, checks currRef again (to see if it's Nothing or Just).
And intercalate "\n" = unlines
Can you create a ticket on the bug tracker so the developers get to see it? It would be sad if you spotted the bug and it gets missed.
Hoogle has a bunch of weights for how the type differs, figures out the correct weights, and then orders the results. I suspect by tweaking the weights you could get the results you are asking for. 
That does help, a lot, actually. Thanks!
Would you give up global coherence of typeclasses to get an ML-ish module system, and if not, why not?
You can get into the car, but you can't get out, except to get into another car.
The API is mostly incomplete because I've only implemented what I've needed (it's on github though, and I'd implement missing functions if people requested it). Also splay trees are a little annoying because you get a new tree from lookups. Or do you mean performance-wise? I should run some benchmarks and report back.
Why the O(n²) complexity?
First, although it's popular to blog about mathematics using lots of Haskell notation, it typically obscures the interpretation of things. Haskell doesn't have a canonical denotational semantics, and programs don't have any more meaning beyond what your compiler outputs. I don't want to discourage people from reasoning informally about things in code. But if you're going to talk about topology, you should work within a traditional set theory, where all the definitions make sense and all the classical theorems hold. Here, the reason no topology exists is going to be due to the fact the author is silently "floating" definitions between set theory and Haskell code. While `type Set x = x -&gt; Bool` seems like a reasonable definition for subsets of `x`, it is going to give you a nice approximation. In a close analogy, equality is not the exact same as Haskell's `==`. Equality is a notion that exists in the "metatheory" of Haskell, whereas `==` is part of Haskell itself. Haskell's `==` (when well-defined) gives us a merely decidable notion of equality. But we freely speak of equal functions, even though functions in Haskell do not have a proper `Eq` instance. (Speaking of which, the article mentions `instance Eq (a -&gt; b)`. I can't imagine what that it even does...)
&gt; I can't imagine what that it even does... I was hoping the word *extensional* was a clue?
&gt;But if you're going to talk about topology, you should work within a traditional set &gt;theory, where all the definitions make sense and all the classical theorems hold. I assume you mean point set topology here? Even then, it should be fine to work in a Sets-like topos ([edit] not that this necessarily helps a lot here), but basically I think lenses give the way (or at least are *incredibly closely* related to the way) to wrap things into a diagrammatic algebra/topological duality in a way that's mathematically canonical. 
Ah. Does that mean it's just defined as `f == g = undefined`, so that, while it has no meaningful reduction, it's still well-typed?
No. [Wikipedia has a good explanation of extensionality.](https://en.wikipedia.org/wiki/Extensionality) As applied to function equality, it simply means that f and g are equal iff for all x, f x and g x are equal. I've always thought it a pretty intuitive approach to equality of functions, tbh.
Here's an example. Note the first pair of functions are intensionally different but extensionally equal. Prelude Data.Searchable&gt; (\x -&gt; if x then 3 else 4) == (\x -&gt; if not x then 4 else 3) True Prelude Data.Searchable&gt; (\x -&gt; if x then 3 else 4) == (\x -&gt; if x then 3 else 2) False 
Here's a rather more interesting example: Prelude Data.Searchable&gt; (\f -&gt; if f 7 then 3 else 4) == (\f -&gt; if f 7 then 3 else 4) True Prelude Data.Searchable&gt; (\f -&gt; if f 7 then 3 else 4) == (\f -&gt; if f 6 then 3 else 4) False 
&gt;Here, the reason no topology exists is going to be due to the fact the author is silently &gt;"floating" definitions between set theory and Haskell code. This happens a lot with mathematics. 
The statement I was making isn't really tied to any particular kind of topology. What I meant was, if you're going to use some technical notion, you should work in a context where it's well-understood. A topology is something you endow a set with in classical mathematics. It's part of the standard mathematics undergrad curriculum to study these things. Everyone agrees on all the results (Heine-Borel, Tichenoff's Theorem, Brouwer's Fixedpoint theorem). So since topology is something you define on a set, what are the sets we're working with in this article? Quick side note: Perhaps we can agree that System Fω is a sufficiently close approximation, since it has a formal specification (whereas "Haskell" is a culturally-defined term). There are no proper sets in sight. Haskell (System Fω) is a type theory, not a set theory. And a type is not the same thing as a set. Similarly, `type Set x = x -&gt; Bool` is not the set of subsets of `x` (because `x` is not a set, and it's not quite clear how to interpret things when a function `x -&gt; Bool` maps some `a : x` to `⊥`). What the author has done here is to transfer these notions into Haskell. But to transfer results always requires care. 
I've never really found Haskell to be a verbose language. Quite the opposite actually. Granted I've never used F#; my background is primarily C++ and Python and I find Haskell to be leaner than both due to the ease at which you create and use various abstractions. (Not quite as dramatic for Python though.)
Sounds like we agree on this. :-)
&gt;But to transfer results always requires care. For sure! Hence Tannakian formalisms, etc. I suppose GHC does some form of this "reconstruction", but for some reason (I wouldn't know) there's not a good way to keep track of the "laws of reference"?
I think his criticism is related to a "choice of coordinates" (so to speak) which makes it tough to interpret this as a "Haskell law". So if you want referential transparency of equality, you need a way to relate combinators to combinators at some fundamental level. [edit] Nevermind! haha 
I am not sure if it is *ever* correct to use any cabal commands outside of a sandbox. Perhaps a first step would be to automate this, and make sandboxes even nicer? Cabal tells you that you are not in a sandbox, and you should create one, and only lets you do it if you really mean it. Build caching to make it less painful, etc. It comes down to a nix-style solution, really.
&gt; Regarding Control.Arrow, the types would be easier to grok if they used type operators. I think that works well. The Scalaz library in Scala does something similar. For example, *** has the following signature: [def split[A, B, C, D](f: A =&gt;: B, g: C =&gt;: D): ((A, C) =&gt;: (B, D))](https://github.com/scalaz/scalaz/blob/series/7.2.x/core/src/main/scala/scalaz/Arrow.scala#L46) 
This is an awkward consequence of modelling Haskell types in Haskell.
Follow the 'getting started' guide on that website by running through the sections on the left - you'll have to install Vagrant and VirtualBox, but after that it's just a matter of running some shell commands! To set up the most recent GHC and stuff, you might want to look at [this repo](https://github.com/begriffs/haskell-pair) as an example. The great advantage of Vagrant, of course, is that you can edit in your native platform, though that doesn't help your IDE issue. Though you can always use Vagrant to manage a VM with a UI, and edit in that like a regular desktop VM.
Isn't lazy evaluation enforces purity? 
&gt;Or maybe I should always use type annotations from the beginning, but then why not just use Haskell from the start? ;) For me it's the simplicity. I find Clojure is a much smaller and simpler language and there's a lot less syntax. Personally, I actually like the parens and I really enjoy having a structural editor that's semantically aware as opposed to working with dumb text. While core.typed is not as sophisticated as HM, it does have some nice things like union types. [Cursive](https://cursiveclojure.com/) now even started adding support for core.typed so you get editor support as you code as well. 
Right, but then you've lost overloading. Typeclasses give you `insert :: Ord a =&gt; a -&gt; Set a -&gt; Set a` so you can write `insert "hello" stringSet` and give it a principal type. 
I see. Thank you for clarifying. 
Thanks for pointing it out I'd seen that page. The strict either or on lots of dimensions with no middle ground didn't seem that effective at transmitting opinions informed from people in a place to judge both, especially without good evaluation of the populations (lots of it could be the fact there are more clojure folks). Maybe there is some methodology section I'm missing Interesting idea though
I like that! Makes me wish that we could have that instance, perhaps if we did more of Prolog style instance search.
&gt; , "real world" program from scratch. It really helps you to learn things instead of just reading about them. If you're stuck on any problem then you can ask on IRC or SO. Keep up with reddit as it gives you a lot of opportunity to learn about material you otherwise wouldn't by just 'doing'. (e.g. the blog post on Phantom types from a day or two ago was really Hi Can you give some examples of what you consider a "real world" program. The reason I am asking is that someone with 0 experience in programming getting a good sense about of what good real world app is would be very helpful. 
FWIW, LYAH (which I'd expect programmers migrating from other FP langs to Haskell to at least skim over) also covers `Foldable`
No prob - I did need the "explosions" to be classifying diagrams, so I should say thanks for the great example! :-) [edit]Also think it's amazingly neat work in terms of both mathematics and programming, and very well communicated! 
Links to types/functions from other packages don't seem to work. E.g. http://hackage.haskell.org/package/direct-sqlite-2.3.14/docs/Database-SQLite3.html References to `Text` should link to `text` package's docs. This used to work when Hackage built the docs. E.g. see the same haddocks for the previous direct-sqlite version: http://hackage.haskell.org/package/direct-sqlite-2.3.13/docs/Database-SQLite3.html
I love Haskell for a lot of things, but there is one thing I really don't like and that's the unwieldy nature of GHC(i). GHCi still doesn't work on ARM (and most my devices, including laptops, are ARM), it is *really* hard to change backends (it is rather normal for a modern language to generate Asm for x86/ARM, JS, JVM &amp; CLR) and instrumenting the (interactive) environment is far harder than with the CRuby, CPython, V8, JVM, .NET. It won't be fixed soon I don't think, so I always have to be on the look out for something that feels Haskell but doesn't have this. I really like Mercury, but that has the same issues as Haskell, besides the backend generation (it does have Java, C, C# &amp; Erlang backends). 
The lens documentation links correctly across packages like this. The issue is that you don't have text documentation built locally.
Math dropout here: but what if one of the functions never halt? How would you test them for equality? Or are we talking about equality on a theoretical level instead of a practical level? 
To give an actual example: in a strict language, you know what the following will print: xs = [print(n) for n in [1, 2, 3, 4, 5]] It will print the numbers in ascending order. In a lazy language, it will not print anything initially – the list is never evaluated, just created. Then when you use the value of xs[3] somewhere it will suddenly print 4 and you have no idea why!
Like being at a wedding and seeing the words "Just Married". One can't unsee that either.
&gt; Since setting does not have a meaningful return value, we can just return `()`. According to this, the return type of `set` should be `State ()`, not `State Int` as listed.
I think you're overthinking things here, maybe due to the way that Haskell plays two different roles in this blog post. The first role is as object of study. The author identifies three behaviors that evaluating a Haskell value of type `()` can have: termination with value `()`, nontermination (as in an infinite loop), or raising an exception. For a function `f :: () -&gt; ()` that can be defined in Haskell, the behavior of `f x` is determined by the behavior of `x` in a way that depends on `f`. (For instance if `x` and `y` are both nonterminating expressions, then we cannot have `f x` terminate but `f y` raise an exception.) Thus each Haskell function `f :: () -&gt; ()` determines a *classical* function Beh(`f`) : Poss → Poss where Poss is the *classical* three-element set {T, N, E}. I think the author implicitly considers two Haskell functions to be observationally equivalent (i.e., metatheoretic equality, not `==`) when they have the same image under Beh. However, not every function Poss → Poss is in the image of Beh. We would like to characterize the image of Beh, since it corresponds to the set of Haskell functions up to observational equivalence. The question is now: is there a (classical!) topology on the (classical!) set Poss such that the image of Beh consists of exactly the functions Poss → Poss that are continuous with respect to this topology? Now Poss is a (classical!) set with just three elements, so this question is amenable to brute-force search as soon as we have written down which (classical!) subset of Poss → Poss the image of Beh is. After all there are only 2^(2^3) candidates for a topology on Poss, and for each one we can consider each of the (at most 3^(3)) functions in the image of Beh, and for each of the (at most 2^(3)) open sets in Poss, check that its preimage under the function is also an open set in Poss. By coincidence, the author uses Haskell to actually perform this search. This is the second role of Haskell in the blog post, as a computational tool. But there is no deep relationship implied between the denotational semantics of *this* Haskell program and the (classical!) mathematical objects we are studying—just that the program does in fact compute the correct answer to the question. The program could equally well have been written in C++ or Python, it would make no difference at all, but for obvious reasons the author prefers to write it in Haskell. tl/dr: Classical mathematics is the best thing ever and we can study it using Haskell without turning into constructivists
You're also going to need a bigger beard.
&gt; GHCi still doesn't work on ARM (and most my devices, including laptops, are ARM).... It won't be fixed soon I don't think I think 7.8 and definitely GHC HEAD support GHCi on ARM, so at least that part will hopefully be coming soon to an ARM distribution near you.
I just want to warn people that this is both evil and dangerous. This is relying on an implementation detail of `ghc` which may change at any time. You also risk getting uncatchable `IO` exceptions in the middle of pure code if you do this.
Type holes have really dramatically increased my practical abstraction ceiling. I use them as a wonderful way to decompose problems that are big enough I can't fit all the steps in my head at once.
I think that certain theorem provers *might* use hash-consing, but I can't find any references off hand. I understand that because of the huge proof object size in tools like Agda or Coq, hash consing could be beneficial, but I dont think they presently do anything of that sort. 
For one of the most canonical Haskell libraries it's surprisingly difficult to find a tutorial for. It exists scattered around several places, but it's not entirely obvious what to read to actually learn the library initially. It's kind of embarrassing actually. Perhaps someone with the Parsec commit bit could redirect the [Hackage](http://hackage.haskell.org/package/parsec) homepage from Daan Leijen's otherwise 404'ing [homepage](http://legacy.cs.uu.nl/daan/parsec.html) to something more helpful?
Seconded, I'd really appreciate that. I'm not somewhere right now where I can write that one down.
But wouldn't an ML-style module system allow us to make, for example, a ML-style functor to produce a Set or a Map module from a given type and order for that type, thus negating the need for global coherence of the type classes? I guess what I'm asking is, wouldn't a good module system allow you shift those places where you use global coherence to the module system, but still let you use type classes to do type-directed implicit dictionary passing for those cases where that feature is convenient but where global coherence isn't required to maintain invariants? Edit: Oh nevermind, I found your thread in https://www.reddit.com/r/haskell/comments/2hif58/edwin_brady_on_idris_in_the_type_theory_podcast/
Links from "resources" slide (for the lazy): * GHC user guide: http://haskell.org/ghc/docs/latest/html/users_guide/index.html * STG Paper: http://research.microsoft.com/apps/pubs/default.aspx?id=67083 * GHC core syntax highlighting for vim: http://hub.darcs.net/dolio/vim-ghc-core * These slides and *exmaples* (sic): http://hub.darcs.net/dolio/optimization-talk-demo
One of the greatest things about Haskell is that it's a LIVE language and it has continued getting better for 25 years. These changes are positive and needed in the long run. And we have to pay the cost if we want to avoid the fate of Java, where it takes forever to get a new language feature. Haskell 2014 seems like the best solution to me.
Also, if there's legacy code that is truly useful, it'll be updated. 
Sure, but understanding why this works helps you understand the ghc evaluation mechanism, which is useful.
I started using Parsec this year and if there is one thing I've learned it is to always write lexeme parsers, that is parsers that consume all trailing whitespace.
Oops. Fixed that.
Will you guys be using NixOps for deployment?
This is exactly right. Thank you.
Out of curiosity, why does Idris also have tuples/Pairs when it has these? Heterogeneous vectors seem to be much more versatile and expressive than nested Pairs, and I don't see any benefit of having (String, Int, Day) instead of HVect [String, Int, Day], other than perhaps performance.
Would love you to expand on both of those!
We don't have a nice proper string handling **at all**. Even all the stuff in `Data.Text` is codepoint-level.
&gt; Now as the other thread about Searchable shows, it is possible to implement Haskell equality between functions whenever the input type is finite, Bool for example. And certain infinite types, such as "Integer -&gt; Bool". (i.e., we have "Eq ((Integer -&gt; Bool) -&gt; Integer)").
I thought the Parsec paper was relatively easy to read and that's where I learned it from. I'm glad you found a guide that works for you though.
I think this page in particular is really useful and should be linked/mirrored somewhere where it is easy to find: http://web.archive.org/web/20070711034016/http://legacy.cs.uu.nl/daan/download/parsec/parsec.html
The TH in the main function is for dynamic code reloading. Which is a feature that I'd assume that Spock just doesn't have. If you want a more minimal starting point, just use the barebones template (which is 24 lines, including imports and blank lines, https://github.com/snapframework/snap-templates/blob/master/project_template/barebones/src/Main.hs, with no TH), or no template at all. Hello world is: import Snap.Core import Snap.Http.Server main :: IO () main = quickHttpServe (writeBS "Hello World") So I think this argument is just crap. Snap is as minimal as you want it to be. The fact that it has more in it, like Snaplets, don't prevent you from not using that (indeed, if you stick to the snap-server and snap-core packages, you'll avoid all that code). It's there because it's quite useful, but if you don't like it, ignore it. With that said, I tried to use Yesod once and I was completely thrown off by the amount of TH magic going on. That I couldn't follow imports to find the files I needed to work on was totally off-putting, and the amount of TH present (for routing and type definitions, for example) really left a sour taste in my mouth. Not because I have anything againts TH on principle (using it to generate Lens boilerplate seems completely sensible, for example), but because it makes it harder to refactor / restructure code. Granted, this was several years ago, and perhaps it has become easier to work with since then.
My point wasn't that you *should* use the wrapper, it was that you *should not* walk the list twice. I even proposed using maximumBy if the OP didn't want to use the wrapper. I'm also not sure that your should avoid things in [**base**](http://hackage.haskell.org/package/base-4.7.0.1/docs/Data-Ord.html#t:Down) for a beginner tutorial. Heck, I'd like people to get familiar with the whole haskell-platform as part of their education. NB: I'm pretty sure the Lisp code walks the list twice, but we really shouldn't teach bad habits when we can easily avoid it.
rather than a pojo you have an ADT. It is usually simpler to just have a field for each object at the lower level that re-slot every field. data UserWithAccount = data UserWithAccount { uwaUser :: User, uwaAccount :: Account } instance ToJSON UserWithAccount where toJSON (UserWithAccount u a) = extendObj u ["account" .= a]
i had a similar experience when learning [mparser](https://bitbucket.org/cakeplus/mparser/) - it just makes life a lot easier to have the whitespaces consumed by the token match
I am sure there is a lot of good work in Snap. But I think this just highlights the whole problem of having a scaffolding tool. See, I only get to run a scaffolding tool *once* for a project. I can't run the scaffolding tool in the "barebones" mode, write my minimal viable product, and then at that point decide that I am going to get serious about deployment and re-run the scaffolding tool in non-barebones mode. That doesn't work, *unless* you have done things the *right way* which is moving the key functionality out of the request-response framework and into a web framework-agnostic (but possibly not database-agnostic) library. This is not an architecture i have seen *any* tutorials or scaffolding templates promote (but maybe I haven't been looking hard enough). So I am sort of forced to run the scaffolding tool with the most advanced options, even when I am starting out with the framework as a beginner. Unless I am just having fun and building toy projects, which I don't have time for. I want to learn the framework as I am building the project I want to build. This is *not* the case when you have a solidly designed library. This may have multiple entry points with different levels of configuration details, and I can at any point over the development cycle switch my calls to the more detailed entry points. 
And my point wasn't that you _should_ stick to `base` for beginners, it was that you _should not_ write code that makes you squint -- even if that's what you would do in a real application. The original implementation is beautiful, readable, and short -- I can understand it as fast as my eye moves. Yours, on the other hand, is clever. Even as an experienced Haskeller, it took me some time to understand, and in the context of a tutorial, I think would need quite some explanation. I agree with your overall point that one should pay attention to good habits early, but it's not so clear to me that this is the only criteria worth thinking about.
You mean those crazy "[seemingly impossible functional programs](http://math.andrej.com/2007/09/28/seemingly-impossible-functional-programs/)"?
I usually end up with a pTrailingWhitespace combinator: pTrailingWhitespace :: Parser a -&gt; Parser a pTrailingWhitespace p = do x &lt;- p takeWhile isSpace return x and then wrap my other parsers in that. 
http://math.andrej.com/2008/11/21/a-haskell-monad-for-infinite-search-in-finite-time/ The associated mathy trick is called *Stone-Cech compactification*. [edit] Basically it's a "double-dual" point set topology. Dualize once over Z/Z2 to get the *Stone space* of characteristic functions. Then dualize over this by using a notion of *uniform structure*, which is more like a norm in a vector space than it is like a metric. Could say good "view update" properties can be had with a uniform structure.
Thanks for this, great work! I've been intending to add something like this into Hoogle itself. If you'd like to try integrating it into Hoogle that would be great... Otherwise, do you mind if I borrow some of the code/ideas (with attribution, of course).
Yes, I understand this, and it's essentially what I do by passing around a tuple -- unfortunately, haskell only has global type class instances, so we need either newtypes of the tuple or a new record as you have provided. Either way, back to what I was originally trying to figure out: how would you deal with libraries that want to own the data model, either by means of templates or generics. Would you consider the advisable approach to be to have an internal model and use something like Convertible to translate the data between the models as we go between the layers?
This one? http://research.microsoft.com/en-us/um/people/daan/download/papers/parsec-paper.pdf Looking at it I'm thinking, sure, if I were a computer scientist or even a professional programmer who already knew a lot about parsing and Haskell this would be good. But where I'm coming from it's really helpful to see the development process step-by-step with all the terms explained. I really appreciate that this tutorial explains how to write a parser, not just what's good about Parsec.
I think performance is a big reason! Better to chase 1 pointer to get to the Day in (String, Int, Day) rather than 3 in HVect [String, Int, Day]. If you do not ever plan on destructuring your tuple in a "list-like" manner, then the HVect is more flexible than necessary, and its performance is accordingly worse.
[I found this one pretty helpful.](http://book.realworldhaskell.org/read/using-parsec.html)
Right. Before I'd hit a "wait wtf do I type here" moment and go and fiddle with a bunch of different notions, typing and deleting while checking in my head. Now I just toss in an `_` and hit `:r` in ghci the moment I feel those gears starting to crank :-)
No problem, it looks like someone added it as #294 . 
&gt; Better to chase 1 pointer to get to the Day in (String, Int, Day) rather than 3 in HVect [String, Int, Day]. I'm a little dismayed about the use of the term "vector" to mean linked-list with type-level size. It took me a little while to get used to "vector" being used in C++ and Java to mean a dynamically-sized array, so I'll eventually get used to it but I won't be the last programmer to see the name and expect constant-time indexing. Why not some up with a new term or qualify an existing term? Surely this will get confusing where you start trying to manipulate geometric vectors in Idris. I've spoken with a plural number of programmers that had this confuse them for a while having use the C++ vector and then needing deal with OpenGL vectors.
No, we have no such plans. We've been moving to a solution based on Docker to provide matching environments in dev and production.
These are good catches, thank you for reporting them! Let's discuss these separately: &gt; possible assumptions about visibility and memory ordering in IORef reads/writes (are barriers needed?) I believe this is not a problem, see [/u/rwbarton's comments below](http://www.reddit.com/r/haskell/comments/2i5d7m/updating_autoupdate/ckzdlvq). &gt; async exception safety ( mask_ and handlers on blocking operations), I was debating whether I should have been more explicit about this in the first place, I clearly should have. Please see [this commit](https://github.com/yesodweb/wai/commit/6a2ab194df138308df2205374d5358e4a1703f3f). Essentially, what all this boils down to is that there are three classes of async exceptions we could worry about: * Explicitly thrown exceptions can never happen, since no one has the ability to get our ThreadId. * BlockedIndefinitelyOnMVar can happen, but it simply indicates that no one will ever request the value again, so simply exiting is the right thing to do. * Any other RTS-generated exception (like a stack overflow) would indicate a bug in our implementation, and therefore giving an error is the right thing to do. This commit makes the errors in that third case much more meaningful to an end user. &gt; possible race condition at lines 126 and 104-106 when readMVar is non-atomic (ghc &lt; 7.8), You're correct, but I think it may be worse than you imply, since it seems like it can even happen with GHC 7.8. (I think that's why /u/fryguybob is indicating in his comment.) I've [pushed another commit for this](https://github.com/yesodweb/wai/commit/a72225e33985c7bf29b2efa751fcbd88ad956daf). The idea is that each time the worker thread returns to its blocking an requester state, it generates a *new* MVar, and it only ever fills MVars, never emptying them. I think this makes the semantics simpler (even if the mechanics of getting the MVar into place makes the code a bit more complicated), and AFAICT avoids the race condition, even on ghc &lt; 7.8. Review of this code is certainly appreciated!
It looks like you're trying to use both persistent and protobuf, and I think I can understand your question. I think eegreg is right that the Haskell way is to have an ADT, but using these two libraries together there is no way to independently declare that ADT. Ideally you could do data X = ... mkPersist ''X mkProtobuf ''X and be on your way, but that's now how the libraries are designed. (For all I know it's impossible.) Persistent, at least, generates an ADT for you while also creating the plumbing to make that ADT work with the library. The protobuf library could probably be smarter. All the constructs that can be made with [the protobuf language](https://developers.google.com/protocol-buffers/docs/proto) seem to map to Haskell types, which makes me think well-formed ADTs could automatically work with protobuf. But for now that's not the case, so it looks like you'll be manually converting the persistent-generated ADT into the kind of type that fits protobuf's Encode/Decode classes.
In terms of categories, ever notice how the *universal* arrow is always the unique arrow filling a "hole" in a diagram? Interesting to think about that in terms of their "sketches". 
Actually, I found the Real World Haskell chapter on parsing to be very good. I know it's outdated in many respects now, but I should think that chapter has held up fairly well.
I'm not sure that the type checker should be checking the alternatives; doing so would mean that compiling the module would probably involve bringing in modules that depend on the module where the function is defined. There are plenty of places where this feature would be used where you want to give canonical uses in modules which import the module. `mapM` is probably a good example; you could show `Monad m =&gt; (a -&gt; m b) -&gt; [a] -&gt; m [b]` as an alternative type without importing the definition of list.
If you use a GenTokenParser then Parsec already has a function lexeme for that http://hackage.haskell.org/package/parsec-3.1.7/docs/Text-Parsec-Token.html#v:lexeme
I always do this, saves so much time. 
I'm pretty sure it uses the deprecated API of Parsec, like most tutorials do. 
I strongly support this and I urge the library committee to take this approach. The Prelude cannot be changed willy-nilly, so please exercise restraint. 
Free market makes everything better amirite
Not long ago I began a project, which aims to provide the most advanced prelude based solely on the "base" package. Here's [a thread accompanying the release](http://www.reddit.com/r/haskell/comments/2ahq11/announce_the_most_complete_prelude_formed_from/). The project has proven to be extremely useful in my practice.
I think the streaming IO stuff shows that for libraries it actually might... Although it does take a lot of developer time. 
The problem of step 1 (writing the prelude in a package) is : until is accepted as a new prelude , it's just a new prelude package among others. Nobody will use it until it is stable and it will never get stable if nobody use it. 
[I have no strong feelings one way or the other](https://www.youtube.com/watch?v=ussCHoQttyQ) on the particular Foldable/Traversable changes.¹ The whole point of a Prelude is not about convenience of writing imports. One can easily swap in whatever Prelude they want or have no Prelude at all. The whole point of a Prelude is that everybody uses the same one. It's something assumed: shared across the whole community. When I see `map` I know it's `(a -&gt; b) -&gt; [a] -&gt; [b]`. So both the focus on writing import lines and on having separate preludes for beginners and non-beginners both seem rather odd to me. What other languages have different standard libraries for people learning and people not learning? What could be a greater way to confuse learners, waste their time and make them think this language is a joke than presenting them with n different standard libraries? That said I agree with Neil's assessment on the *process* for this. I had no idea this proposal was even made, never mind being implemented.² His post generated a lot of heated discussion which proves his point. The Applicative—Monad transition was nice and smooth. GHC even told me when I defined a Monad instance without an Applicative instance. --- 1: I write code that uses sets and vectors as much as lists, so the generalization makes sense. On the other hand when I look at lens code I feel the same alienation that Neil feels when he looks at `***` and `&amp;&amp;&amp;`, so I empathize with aversion to over-abstraction. 2: I'm vaguely reminded of the `Monoid` class. For a long while, everyone was onboard that generalizing `(++)` to `mappend` was the right way to go. I saw it in Trac tickets, I saw it in people's code, on IRC, it just made sense. Then one day I learn that `(&lt;&gt;)` (“not equal to?”) has been defined and is now part of base. A bunch of people were nonplussed. Now we have two operators, and I don't use `(++)` anymore.
Final stability and polish refinements might come after merging, as we are still doing to the official Prelude. But I think we should try and do as much refinement before merging - and I don't think lack of users is a problem when we don't yet have the code available to use. 
I think the ideal here is let Persistent generate your models and use them everywhere possible. If you have to use Protobufs then certainly you will have data structures dedicated to this and I certainly wouldn't use them all over my application even if I weren't generating structures with Persistent. There may be other reasons but the use of the Utf8 newtype is enough by itself for me, I'd rather use Text in my models and encode only at the application boundary. Is Protobufs really a requirement - e.g. is this used by external clients not written in Haskell? If you are just using the data in Haskell then SafeCopy is a much better alternative - you can still safely persist this by providing migrations when your models change. For an external API you can use JSON or MessagePack which will work with native types and provide generic and template haskell to generate the instances. My first Haskell project started with Protobufs and I abandoned it after a couple of months precisely because I did not want to be converting it everywhere. 
&gt; (&lt;&gt;) (“not equal to?”) "diamond" 
Ok, here we go: Internally, the Maple language is an algebraic data type with exactly 62 cases (though a couple more might have been added recently). This includes all the terms in the language (plus, times, and, or, variables, numbers, etc), but also all the **statements** (like assignment, if-then-else, return, etc). Everything, include Maple's own source code, is hash-cons into one giant "simpl table", which is actually a big hash table backed with a splay tree. Pros: * it makes equality checking *fast*. * when you are doing a lot of symbolic computation, like solving integrals in closed form, you are manipulating 'almost the same' expressions all the time. Hash-consing saves a huge amount of memory that would otherwise have to be allocated and garbage collected [there is still memory allocation and gc going on, but this reduces the pressure]. Cons: * your hash table gets filled up with code. This slows things down. * you spend a lot of time hash-consing stuff which will never be re-used, thus slowing the interpreter down. But fundamentally, the **biggest problem** is * since Maple is an imperative language with side-effects, expression evaluation is done *as a tree*, so that side-effects work as expected. Which diminishes many of the gains of using hash-consing to begin with.
Do you have an example sequence of commands which triggers the problem? I downloaded hslogger and put their [example program](http://hackage.haskell.org/package/hslogger-1.2.1/docs/System-Log-Logger.html) in a file, loaded it with ghci, ran `main`, hit CTRL+D, and ghci exited without a fuss.
I updated the web page as suggested last week and this weekend I have been trying to make a 32bit version. I have been trying to get 32bit Haskell Platform 2014.2 to play nice with all the C libraries from Fedora's mingw32-* packages (this is the only sane way I can find to get WebKitGTK for Windows). Unfortunately I think the gcc used in 32bit Haskell Platform is just too old (64bit HP comes with a slightly newer gcc). I'll try again once [this](https://ghc.haskell.org/trac/ghc/ticket/9218) is fixed or once someone writes instructions on how to build a mingw32-ghc cross compiler on Fedora (which would be amazingly cool if it is possible).
For small tweaks, yes. For significant rewrites, I would disagree. The debate between : and :: as the cons operator was long and hard. What finally swayed it was that if you use :: you tend to put spaces on either side making it 4 times as long, not his 2 times as long. No amount of thought would have shown that. 
As an alternative answer, trying it will almost certainly lead to refining the ideas, and improving it - it almost always does. I don't think we want to rewrite the Prelude regularly :)
Keep in mind we had a massive thread on this that led to the formation of the core libraries committee in the first place explicitly to address _this_ particular issue, in which 90% of the comments were positive, and the community clearly indicated the direction they wanted to go, but was unable to collectively articulate a vision for how to get there. Why has it taken so long? The core libraries committee was formed near what appeared to be the end of the 7.8 lifecycle, and decided not to act heavily during 7.8. The goal of changes for 7.10 is limited for the most part to changes that can be made in such a way that code can be written to compile before and after without CPP for folks who have longer support cycles. We've been studying the changes and how we'd need to move the code around to enable even the possibility of supporting them since the middle of 7.8. (This is why `MonadFail` is on a longer timer. It necessarily breaks code, and I'm still holding out hope that Richard Eisenberg's work on better superclass defaulting can let us make the change less painful for users.) A secondary motivating principle is that I'd personally like to see it that `base` as a whole only really ever exports one thing with a given name. Why? New users to Haskell usually don't see the difference between `base` and `Prelude`. This is the same sort of motivation as Neil, not wanting to see the same name used for different purposes, but at a package rather than a type class level. What we're currently doing is working out what such a change impacts. Consider this a deep dive. We've known about `Foldable` and `Traversable` as abstractions since 2006. They've slowly infected the culture, but they are highly non-discoverable in their current form. Almost _all_ of the breakage from 7.10 will actually come from AMP -- the "carefully considered and deliberated" change talked about in the post. Half of which we had to refactor, because it wasn't implementable as proposed due to unforseen interactions with the way folks write `(*&gt;)` in terms of `(&gt;&gt;)`, the way adding `join` to the class breaks GND in the presence of role annotations, the way it breaks the `haskell2010` package, by making it impossible to build new monads if you use that package, etc. The `Foldable`/`Traversable` changes are such that they can't effectively be implemented as a library. If I we built it as a library, you couldn't see the lack of impact on nofib, or on actually building a good chunk of hackage by hand, etc. Along the way we found some cases where the orientation of operations like `sum` changes, and rather than change semantics on existing users we elected to enlarge the class to permit more efficient implementations. We opened this up for discussion on the libraries mailing list to mostly positive feedback as well. The current direction we're heading is somewhat in line with what Neil is proposing as having two Preludes, but rather than two preludes trying to resuscitate the `haskell2010` package into something users could perhaps actually use for teaching, and if the *AMP* related issues can be worked out, then perhaps even for building atop if they want a more standards compliant way to write Haskell. I hope you can see that both styles are options, and it can be argued that a more package oriented approach or a more module oriented approach would be preferred, but the difference is we *have* the `haskell2010` package already. Why create stratification along yet another dimension? The request that everything be worked out as an alternate `Prelude` is almost disingenuous in one respect. From a POSIWID perspective it is asking for effectively no changes to happen to anything ever. (On the other hand, I suppose in all fairness the same argument might be applied to my appeal to the basically dead `haskell2010` package!) Now that said, I freely admit we need to be more transparent about what is going on, why, the thought process involved, and the way the different needs of the different groups are being considered, and open up for deeper discussion the fate of things like the `haskell2010` package, which is being affected heavily by the AMP in ways that fragment the two candidate sets of users it has: teachers, and users who seek closer standards compliance. I personally disagree with Neil that the default should be the monomorphic version of these functions and that base should continue to export conflicting definitions of so many core functions and if you look at the ratio of upvotes on individual comments in Neil's previous thread it does heavily indicate that there is a lot of passion on both sides of the aisle here. However, I ask you to please consider that far and away the vast majority of the feedback folks on the committee have received *is* in favor of generalizing over `Foldable` and `Traversable` in `Prelude`.
One technical point: Neil suggested writing import Prelude() import OtherPrelude to use an alternate `Prelude. A better way is to put extensions: NoImplicitPrelude in the cabal file, and import OtherPrelude in each module. Or {#- LANGUAGE NoImplicitPrelude -#} ... import OtherPrelude in each module. Otherwise, you still import all the type class instances defined in the old `Prelude`.
The original motivation for it was a form of parser we now know can be handled via an `Applicative`. There is some pretty rich theory you can build up to get to the notion of `Arrow` we have now, but almost all the intermediate steps seem to prove more useful from a code-organizational perspective than the final result that we have baked in sugar to support today.
I don't necessarily see any issue with using a tuple over a record, except maybe there are global instances of, for example, ToJson for tuples that I would want to override. Also, a record can provide more information as to what the data is as it is named -- it's not always useful to provide a name though. The issue I have is that I want to be able to isolate my database layer from the rest of my internal structure -- ideally, I should be able to rip out the db stuff and replace it something entirely different -- and similarly for my various external layers (rest api, protobuf api, etc). For the rest stuff, fortunately I can just provide a ToJSON instance, unfortunately instances are global, so I will probably have to create newtypes for each endpoint that wants to format the information differently (for example, I may have an administration endpoint that returns more information about the data that the public endpoints should not expose). As I'm writing this, I'm realizing that I feel like what I really want to do is emulate the "service abstraction" that I'm using in Scala/Java, and because I want to be able to swap out each component, I don't want to be dependent on any one component for the structure of my data. But I've had somewhat of an epiphany! I'll structure my application with my models at the core (Message, Comment, User, etc), then if I have a layer/component that needs to manage it's own data structure, I'll just convert to their models within the layer (their interface depends on my models, but their implementation may use their own). And now for the realization: I can create the service interfaces as records containing the implementations for each operation: data MessageService m = MessageService { fetchById :: ID -&gt; m Message, deleteById :: ID -&gt; m () etc... } Then I can provide implementations dbMessageService = MessageService dbFetchById dbDeleteById .... and pass those around to my external layers.
A bit out of topic, but has someone though of removing `*` from `Num` so dimensional number (with units) can be instance of `Num` ?
Agreed. I spotted this proposal from the GHC weekly update, e.g. saying what had happened, not what was going to happen.
All of these points are well taken. This was good work. However, there are two things to note: 1. The committee did not seem to consider the possibility of having one deprecation cycle before making this the default. A deprecation cycle is especially important here, because it is a fundamental breaking change, and because having something like this as the default `Prelude` has never actually been tried. If it weren't possible to try it first, I would say let's go with the committee's decision and take the plunge (even though I'm not happy with it); but it is possible to try it. 2. While I think most people were aware of the existence of some discussions about various ways to improve the `Prelude`, I was quite surprised when I heard that the result of this discussion would become the default Prelude in 7.10, and that it would introduce huge amounts of new polymorphism. If I had known that, I would have jumped in and spoken out strongly against that. It seems clear from these discussions that I am far from the only one. From experience in other languages, fundamental changes to the flavor of a language work when they can happen gradually, gain widespread acceptance, and become a *de facto* standard before they are enshrined in the basic definitions of the language. When they are done as forced breaking changes, they are usually more damaging than helpful.
&gt; But for now that's not the case, so it looks like you'll be manually converting the persistent-generated ADT into the kind of type that fits protobuf's Encode/Decode classes. That's exactly what I needed to know: if this was the advisable/accepted approach. Thanks for the info.
The "nonplussed" pun made me laugh. Not sure if intentional. ( `&lt;&gt;` instead of `++` )
I think productivity can be viewed in a few ways. For me, I enjoy languages that force me to think _first_, well before I have a working prototype. Examples of this would be haskell and rust. I consider this to be a bit more productive, for me. The reason for this is twofold. First, I understand the problem domain better leading to better coverage of all possible states in a particular domain. Second, the code will almost certainly require little to no maintenance because of that upfront thought. In my experience, this tends to lead to more correct code, given equal development time (dynamic languages tend to even the time out with the necessarily excessive unit testing and such). The obvious trade-off is getting something to "just work", giving that sense of progress that we all have come to desire as programmers. For me, it took a bit of retraining my brain, but once I came to consider the aforementioned pros and cons, I've found that I appreciate the approach of "stricter" languages for productivity. 
A-ha! Stupid me, I wish I had understood this earlier :) If I can get it to work reliably, next release will have ligatures with proper unicode points, and custom unicode points so they'll work with minor tweaks to haskell-mode. Thanks tailbalance!
I clicked through a number of the linked examples. I saw web sites, but I didn't see API documentation. Is this supposed to compete with Haddock?
I'm not really arguing here about breakage, I think that's a separate issue to whether the change at the end is a good idea or not (and a bit of pain to get somewhere better is sometimes necessary). I also agree that the same name from multiple packages is non-ideal. When it's mapM, it's pretty bad, when it's fromList (e.g. Set/Map) it's the pattern, and then not so bad. I understand the haskell2010 is slightly what I'm proposing, but at the moment haskell2010 is totally broken. I worry it's going to be "the beginner alternative", but no one is going to do the work to fix it, so it's still going to be totally broken. To say we already have the haskell2010 package is only accurate if you omit the word "working". Even then, it seems making "normal" beginner friendly, and making experts hop through a hoop, is a better default. The haskell2010 package had users in mind, but doesn't actually have real users - I personally think it doesn't suit even it's theoretical users. I don't think prototyping the code outside the base library means we can never have an alternative Prelude. I think we'd have discovered some of the issues we've already discovered (not all, but some). We'd also have discovered some of the issues we haven't yet discovered but are going to. Having a prototype before you merge is not the same as never merging. While I'm sure a lot of the feedback of people who are interested in changing Haskell is positive about changing Haskell, I worry the silent majority haven't had their viewpoints taken into account. It's impossible to effectively poll them, but most Haskell users will never have sent an email about Haskell to a mailing list or visited IRC. That's why the libraries proposal list was never about absolute votes.
I like the idea, but I think you've skipped a step. Step 1 is announce we are going to have a new Prelude. Step 2 is pick what that Prelude should be from the candidates suggested in step 1, and advertise it. Step 3 is merge it. There are several alternative Preludes, picking the only one that hasn't been implemented and blessing it in advance seems wrong.
Most people who've learnt Haskell recently have never been on the #haskell channel, and I'd argue it attracts exactly the kind of people who are likely to be in favour of this change. When I used to frequent the Haskell channel changing the associativity of $ was popular, despite being a bad idea.
If the whole point of this proposal is to avoid having to hide method with the same name defined in both `Prelude` and standard Modules ,wouldn't it be better to add a sort "auto-hide" or priority mechanism to resolve name conflict. Something like Prelude.hs: module Prelude where ( ... foldl {-# PRAGMA Hidden by Data.Foldable #-} ... ) or module Foldable where ( ... foldl {-# PRAGMA Hide Prelude.foldl -#} Or import Data.Foldable {-# Win in case of conflict #-} etc ... 
The name confused me because there is already a service called apiary (apiary.io) for API documentation. The automatic api documentation generation is really great though! The examples are for url parameters. Have you thought about how to generate API documentation for JSON bodies? It seems that this is missing documentation for the structure of the response and it would be great to be able to document what is needed in a POST.
Oh, no. I can exit ghci fine. It is run ghci, run main, decide to make code change, CTRL+C, :r, main. Then *** Exception: logs/App.log: openFile: resource busy (file is locked) I have to leave ghci to get it to release the file handle. 
&gt; The committee did not seem to consider the possibility of having one deprecation cycle before making this the default. Actually, this is very much on our minds. The committee isn't entirely sold on whether we should have a deprecation cycle or not. _This is still on the table_. This is still an open question, largely being answered by a.) whether such a feat technically even makes sense and b.) measuring the level of expected breakage. Deprecation is ultimately about breaking code. The proposal as being enacted purposely _breaks_ as little code as possible. So let's talk about what could possibly break. There are four avenues for breakage of user code I'm aware of in this change: 1.) The biggest concern going in was that you can theoretically have code that explicitly depends on the monomorphization of `[]` because nothing else fixes that constraint during production or consumption. e.g. by producing a list entirely with `Monad`/ `MonadPlus` and then consuming it entirely by `foldr` which previously made the instance selection. Measuring the amount of code broken in this way is something we're actively undertaking. I've personally yet to actually find any, but it was definitely the biggest fear that this would be a blocker. 2.) You can have code that breaks by the fact that it doesn't have top level signatures and makes use of a now generalized combinator and runs afoul of the MonomorphismRestriction. This legitimately led to 2 lines of changes in the GHC test suite. 3.) We might change semantics of an operation during the change. This includes performance regressions. We're deliberately seeking to minimize this risk, and it remains a going concern, but preliminary results are looking good. In fact the only regression we've seen so far actually technically stems from AMP's impact on Traversable, not Foldable/Traversable itself. 4.) We might take names in `Prelude` that would collide with user code. Alternately, by adding methods, explicit imports of these types using Foldable(..) may get more methods than they previously expected based on efforts to mitigate #3. Of these, #1 and #2 aren't really amenable to a warning. It isn't clear how such a warning could even be implemented. Item #3 is being actively addressed by the changes to `Foldable` from the most recent proposal at the expense of some increased risk on the import side of #4. Item #4 is the one we're exploring now. One side of it can be partially mitigated by not exposing the entire API through `base` at the expense of a possible warning cycle. The biggest open question for 7.10 is how much gets deferred to 7.12, and how much we can and should limit the impact of #4 on existing libraries. There are components of the change that may wind up having to be deferred to 7.12 for just that reason. e.g. deciding if it makes sense to expose some of the members of `Foldable` now, and expose the rest in 7.12 after a deprecation cycle is _very much_ on the table. There is a flip side to that debate, however, which I'll illustrate through the AMP, rather than Foldable/Traversable. Let's consider (&lt;$&gt;), which really is needed to properly use Applicative's (&lt;*&gt;) sugar sensibly. Bringing it into Prelude will break libraries like `pretty`, which uses `(&lt;$&gt;)` for its own ends. There is a need to balance the pain to users of `pretty` against the pain of an entire deprecation cycle for one combinator, and the engineering fact that there is currently no way to talk about 'hey we're taking this name away from you' in the current deprecation toolbox. &gt; From experience in other languages, fundamental changes to the flavor of a language work when they can happen gradually, gain widespread acceptance, and become a de facto standard before they are enshrined in the basic definitions of the language. This *is* more or less my experience with that has happened with the adoption of `Foldable` and `Traversable`. Adoption of them has grown to be the norm rather than the exception, despite the fact that they've required an explicit import that needlessly collides with a number of combinators in Prelude all this time. (I'm not alone in considering a data type that can legally support them that doesn't broken and unworthy of incurring a dependency upon, as it encourages anti-modular orphan instances.)
web service API, not a library API. http://best-haskell.herokuapp.com/api/documentation
You are very welcome to play! We have a match on going. I'm very interrested to know how to simplify the game to make it more accessible. But that's not easy because it's a Nomic game: you can modify the rules of the game. To modify/propose new rules you have to write them in some language. So I don't know how to avoid the language part. The language could be simplified but it will always be some sort of programming. As for the win conditions, well it is a game so there got to be an objective... Of course this objective can change, since it's decided by the players :) 
Large structural changes to the numeric hierarchy aren't really being all that seriously considered at the moment. The impact on everyone is far too high and we have no tooling to support refactoring class hierarchies in the language design.
&gt; The haskell2010 package had users in mind, but doesn't actually have real users - I personally think it doesn't suit even it's theoretical users. One thing that I'm very very interested in, is seeking feedback upon is how we can make something like the `haskell2010` package viable. I very much want something viable we can offer to teachers to teach more monomorphic code. However, even if we couldn't get to where we can offer such a sop for acceptance, I do ask you to consider at what point `Traversable` in particular enters into the teaching pipeline. For folds, even the prescriptivist Python folks overload `__reduce__`. By the time students get to `mapM`, they've already been introduced to `Monad` (and nowadays to `Applicative`, if its being inserted into the teaching pipeline) -- the type already involves knowing about typeclasses. As a teaching guide it helps show that you don't need the full power of a `Monad` to walk a list and serves as a segue into talking about the context-sensitivity of monadic computation, the power to pick what to do next based on what came before. The fact that we're even talking about `mapM` rather than just `traverse` is _indicative_ of how bad the current teaching approach is.
Join us in the online game, a match is ongoing! http://www.nomyx.net:8000/Nomyx All the infos here: www.nomyx.net
FWIW- I'm generally very much against silent changes to the semantics of existing code -- The `($)` ship has sailed. =) There are lots of generalizations that made their rounds on #haskell, mostly due to `Cale` and `ski` back in the day. Generalizing `flip`, `(.) = fmap` etc. I'm not advocating for all, most -- or even many of them. I do, however, feel that `Traversable` and `Foldable` on the other hand have stood the test of time sufficiently well to be taken seriously. They dropped on us out of academic literature, they showed that the previous `FunctorM` classes were missing the point and have withstood lots of academic exploration in terms of their properties, have eloquent laws, and they have also demonstrated great practical utility.
There's a visualization trick for this [here](http://ncatlab.org/nlab/show/end#SetCoendsAsColimits), where the models of sketches would correspond to enrichment. Or you could say container for that. As it says on the nlab page, this is naturally isomorphic to an over category C/r, where the arrows connect a diagram \/ by composition (arrows directed upward) . In which context does an *integral* fill a *hole*? Well, that context is called [cohomology](http://ncatlab.org/nlab/show/end#related_concepts). Would be cool to do this as a *formal* "de Rham" cohomology, wonder if there's an interpretation of the integral laws like on the nlab page? Sounds complicated, but really it would reduce to *intuitions* from calculus... y'know? Like you integrate over a nice boundary to get the form of measure over the interior, and that's a law when you have a nice boundary. Anyway here's the jist of this formal game idea: Consider a diagram d, two arrows /\ directed downward. This is like a summary of a cocone just to simplify things for now. d0 and d1 are the boundary arrows, and we want to fill in the _ between them with an integral. Anyway, just interpreting the bit about sketches into a different notation cause it *just looks* like two things are meeting at a common "T" here. [edited for an approximation of sanity lol]
Ok, I can now reproduce the problem. Simplifying the repro steps, I notice that I don't need CTRL+C (because the hslogger example doesn't run an infinite loop), and then I notice that I don't need `:r` either. I then change `main` to run its code twice, and now I can reproduce the issue without ghci, so the problem is probably some missing uninitialization code at the end of the first execution. I also notice that the first few messages print just fine the second time around, it's only the "Some useful diagnostics" line which errors out. I trim out the earlier messages, and I continue trimming code until I get this minimal example reproducing the problem: -- | -- &gt;&gt;&gt; main -- *** Exception: debug.log: openFile: resource busy (file is locked) main = do h &lt;- fileHandler "debug.log" DEBUG h &lt;- fileHandler "debug.log" DEBUG return () Let's look at the documentation for [`fileHandler`](http://hackage.haskell.org/package/hslogger-1.2.4/docs/System-Log-Handler-Simple.html#v:fileHandler): &gt; Create a file log handler. Log messages sent to this handler will be sent to the filename specified, which will be opened in Append mode. Calling `close` on the handler will close the file. So it looks like we simply forgot to call `close`: -- | -- &gt;&gt;&gt; main -- (no error!) main = do h &lt;- fileHandler "debug.log" DEBUG close h h &lt;- fileHandler "debug.log" DEBUG close h Now, your situation is a bit more complicated, because you have an infinite loop which you're killing with CTRL+C, so there is no obvious place in which to put the call to `close`. Again, a minimal example reproducing the problem: import Control.Concurrent -- | -- &gt;&gt;&gt; main -- CTRL+C -- &gt;&gt;&gt; main -- *** Exception: debug.log: openFile: resource busy (file is locked) main = do h &lt;- fileHandler "debug.log" DEBUG updateGlobalLogger "MyApp.BuggyComponent" (addHandler h) let loop = do threadDelay 100000 debugM "MyApp.BuggyComponent" "Some useful diagnostics..." loop loop We need to somehow detect the CTRL+C and call `close` in the handler. I think pressing CTRL+C causes a particular exception to be raised. While wondering how to detect whether the exception is the CTRL+C one, I realize that I don't care: I want `close` to be called whenever the control flow leaves the region in which the file handler is used. The standard method for registering this kind of cleanup code is [`bracket`](http://hackage.haskell.org/package/base-4.7.0.1/docs/Control-Exception-Base.html#v:bracket): import Control.Exception.Base -- | -- &gt;&gt;&gt; main -- CTRL+C -- &gt;&gt;&gt; main -- CTRL+C -- (no error!) main = do bracket (fileHandler "debug.log" DEBUG) close $ \h -&gt; do updateGlobalLogger "MyApp.BuggyComponent" (addHandler h) let loop = do threadDelay 100000 debugM "MyApp.BuggyComponent" "Some useful diagnostics..." loop loop It works!
Sorry... this name is my fault... Yes. but this problem is hard. for example: data E1 = E1 { foo :: Int, bar :: String } (E1 12 "field") encode to {"foo": 12, "bar": "field"}. I want to document it like: return JSON Object has fields: name| type | description foo | Int | description of foo bar | String | description of bar I don't have any smart idea to provide descriptions.(If I focus on ghc-7.8 or higher, I think that I can use ANN pragma.) Sum type is harder. data E2 = A { foo :: Int, bar :: String } | B { baz :: Text, qux :: Text } How do I encode it? aeson has 3 options. but I don't have any way to recognize it. So, it don't have auto API documentation for JSON. I'll implement it when I hit upon a good idea:-)
Unfortunately, if any of your dependencies use the old `Prelude`, you'll get the instances anyway.
How expensive is the associated S3 bucket load?
This is actually a worse idea than it appears at first blush. `liftM` isn't just "redundant" but it is also a viable default definition for `fmap` in terms of `return` and `(&gt;&gt;=)` and is used as such across a large fraction of the ecosystem. If you define `liftM = fmap` then large chunks of existing code just become infinite loops. This isn't a viable transition. Removing it just means it needs to get hand-written by a lot of users over and over in the legitimate usecase it does have. `mapM` is more of a sticking point. As knock-on consequence of the AMP it is getting a simpler definition in terms of `traverse`, but removing it from `Traversable` to just move it to the top level is problematic. Why? It turns out there is a corner case where a form of `mapM` can have better stack behavior than `traverse` for a form of `mapM` written in terms of a manual reversal under the bind. This form doesn't extend to the infinite case, but it extends to a 'very large case' in a more scalable manner than the more direct definition on GHC today. We may have to keep `mapM` in the class for such usecases. It is being considered, but most of the things that seem obvious don't actually work for reasons like those above, and, despite the appearances from the current discussion, we're not actively seeking things to break just now. =)
Well, I'm open to contributions. I don't think I'm gonna implement this on my own however, since I don't have to deal with Safe Haskell much.
I think these are fair points, but it looks to me like Snaps scaffolded app is much smaller and easier to understand than Yesod's. It seems to me like it'd be much easier to start from barebones and just build back up to the scaffolded app if you needed to in Snap. That said I'm currently using Yesod because I wasn't confident I'd be able to figure out Snap given the current state of the docs and lack of activity on the mailing list. I'll probably switch for my next project, hopefully after 1.0 is released. 
The class I learnt functional programming didn't include Monads, or Applicative, or Functor, or Monoid. I think it was an excellent course in functional programming, rather than a course in Haskell.
Ah, I see. Nice!
Cool. Thanks for the help. I didn't realize CTRL+C caused an exception to be raised.
I imagine it's based more on "vector" from mathematics than from other programming languages. 
Giving it its own Monad class is a possibility. Hacking the compiler to do something else is an option. Implementing some flavor of Eisenberg's superclass defaulting proposal (or Conor's) could also work. The implications of a separate `Monad` class is that it becomes even less viable than it already is to say you can use the packages you build on `haskell2010` with the rest of the ecosystem.
`&lt;&gt;` was chosen because it's the shape you get when you hit `+` or `x` with a hammer. Since `+` and `x` are the two canonical examples of monoid composition operators, it's very fitting. (I just made that up).
It's not about whether people have adopted `Foldable` and `Traversable`. It's about whether those have essentially replaced the monomorphic `Prelude` list functions in common usage. I don't believe that they have. I like `Foldable` and `Traversable` very much. I use them often. But not nearly as much as `map`. The tiny cost of the one-line qualified import of `Traversable` is well worth it for me to be able to keep `map`. I believe that monomorphic functions for conceptually fundamental types such as lists make code cleaner, more readable, and more elegant. I do not want them replaced by polymorphic functions from `Foldable` and `Traversable` by default. But my mind is open; I'm willing to give it a try. And I'm even willing to bite the bullet and switch to that style if that becomes the consensus of the community. What I don't want is to burn the bridges before we have a chance to try it.
Importing the instances from Prelude should be fine, though. If OtherPrelude reuses the same classes, then you probably want the same instances anyways. If OtherPrelude uses different classes, then the Prelude instances are harmless because they do not apply to the different classes. The only time you might want to not import Prelude's instances is as an exercise of providing the instances yourself.
It's an implementation of Nomic: http://en.wikipedia.org/wiki/Nomic I cannot find a link for parlour games? Anyway from your description it looks quite like Nomic/Nomyx! 
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Nomic**](https://en.wikipedia.org/wiki/Nomic): [](#sfw) --- &gt; &gt;__Nomic__ is a [game](https://en.wikipedia.org/wiki/Game) created in 1982 by philosopher [Peter Suber](https://en.wikipedia.org/wiki/Peter_Suber) in which the [rules](https://en.wikipedia.org/wiki/Law) of the game include mechanisms for the players to change those rules, usually beginning through a system of [democratic](https://en.wikipedia.org/wiki/Democratic) voting. &gt;Nomic is a game in which changing the rules is a move. In that respect it differs from almost every other game. The primary activity of Nomic is proposing changes in the rules, debating the wisdom of changing them in that way, voting on the changes, deciding what can and cannot be done afterwards, and doing it. Even this core of the game, of course, can be changed. &gt;*Nomic* actually refers to a large number of games based on the initial ruleset laid out by Peter Suber in his book *The Paradox of Self-Amendment*. (The ruleset was actually first published in [Douglas Hofstadter](https://en.wikipedia.org/wiki/Douglas_Hofstadter)'s column *[Metamagical Themas](https://en.wikipedia.org/wiki/Metamagical_Themas)* in *[Scientific American](https://en.wikipedia.org/wiki/Scientific_American)* in June 1982. The column discussed Suber's then-upcoming book, which was published some years later.) The game is in some ways modeled on modern [government](https://en.wikipedia.org/wiki/Government) systems. It demonstrates that in any system where rule changes are possible, a situation may arise in which the resulting laws are contradictory or insufficient to determine what is in fact legal. Because the game models (and exposes conceptual questions about) a legal system and the problems of legal interpretation, it is named after νόμος (*nomos*), [Greek](https://en.wikipedia.org/wiki/Greek_language) for "[law](https://en.wikipedia.org/wiki/Law)". &gt; --- ^Interesting: [^RIGblaster ^Nomic](https://en.wikipedia.org/wiki/RIGblaster_Nomic) ^| [^Eek-A-Nomics](https://en.wikipedia.org/wiki/Eek-A-Nomics) ^| [^-nomics](https://en.wikipedia.org/wiki/-nomics) ^| [^Gnomic ^poetry](https://en.wikipedia.org/wiki/Gnomic_poetry) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cl1c86e) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cl1c86e)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I do this when using Parsec, but use `pTrailingWhitespace` when using Attoparsec. For example : https://github.com/yesodweb/yesod/blob/master/yesod-test/Yesod/Test/CssQuery.hs#L63
I think your api generation code would need to be informed of the JSON rendering options by the programmer. This isn't that horrible if we just assume everyone is using aeson and that the same options are used everywhere. Given that you just released the web framework, requiring ghc 7.8 for description annotations doesn't seem like an issue.
&gt; I think the ideal here is let Persistent generate your models and use them everywhere possible. If you have to use Protobufs then certainly you will have data structures dedicated to this and I certainly wouldn't use them all over my application even if I weren't generating structures with Persistent. There may be other reasons but the use of the Utf8 newtype is enough by itself for me, I'd rather use Text in my models and encode only at the application boundary. I want to avoid using Persistent's models everywhere because I want to be able to swap out the db service if need be, but otherwise, I agree with you. Protobufs are just an example, I'm considering using them for a project to communicate with an external device, but upon finding it's requirements for the structure of my models, I needed to figure out a better way to put together my application...
 language: haskell ghc: 7.8 before_install: - travis_retry sudo add-apt-repository -y ppa:hvr/ghc I believe there is a mix of using proper ghc packages provided by Travis infrastructure and @hvr'ed packages from a self-titled ppa repository. First configuration is pretty suitable when you're not bothered by HEAD, 2014-full cabal and working alex/happy packages. What's faster - using default ppa and installing fresh cabal/alex/happy via cabal-install or using ppa by @hvr? And finally, all these beautiful workarounds sounds like Travis infrastructure is not in the best condition now and it could be tuned better. Otherwise I cannot understand the reason why @hvr's ppa exists as a non-any-part of a TravisCI
Actually, trying to recognize auto-encoding won't work when there is custom encoding. I do a fair amount of that where 1-3 fields are custom and other things are based off the auto-encoding. You would have to force me to always use a Haskell type before the final form of JSON rather than using something such as object. Rather than documenting a Haskell data type you can document the final JSON, but the final JSON is always Value. The only way to document it seems to be to parse the encoded JSON.
FWIW- We are not replacing `map` in this process. ;)
Negligible -- last month was $0.03, and that includes S3 hosting and traffic for the binaries I load up in my heroku buildpack.
That is the way functional programming should be taught. First learn to program in a pure functional style, before bringing in effects via Applicative, Monad, etc. 
Fair enough. I may have to look into this for my packages.
I think we're all in agreement on that front. I have absolutely no objection to teaching (especially a first semester Bird-style course) in that style, and in fact think it is a very good idea! That said, if that is the stated goal of the course, objections to things like `mapM` and `traverse` on expository grounds are greatly reduced. ;) The one function being impacted that the `Prelude` exports that really falls within the scope of that teaching discipline is `foldr`, but we also generalize `(+)` on users so by the time they start folding lists, they've at least been probably been exposed to some notion of typeclasses, just so that notions of things like `length`, `sum`, etc. can make sense -- and if they haven't because you are being hardcore about teaching just from universal principles and have denied them numbers, then you are probably building the primitives you use from scratch anyways, and what is happening in `Prelude` isn't much of a going concern.
(I know I'm a bit late to this discussion, but it was recently linked, so may get a few readers anyway.) &gt; In this case I prefer tools because a library obscures where the symbols come from. I don't find this argument convincing, because ghci isn't fooled. For example: BasePrelude&gt; :i guard guard :: MonadPlus m =&gt; Bool -&gt; m () -- Defined in `Control.Monad'
&gt; we need an easy way to get a single list in one place of all of the symbols being imported, together with their type signatures. &gt; :browse BasePrelude
Parlour game is just a term for games played in person inside, I think. http://en.wikipedia.org/wiki/Parlour_game
On further reflection, i think calling the current work rewriting the prelude is false. It is a small set of generalizations that attempts to move forward while being maximally noninvasive.
I haven't used Clojure very much, but I've done a lot of Scheme (mostly Racket), and I learned the two at the same time. For most things over a couple hundred lines, I find Haskell significantly faster; for shorter problems, they're roughly comparable overall. Sure, for some things, Racket might be faster, but it's not a significant trend. One of the big differences is how much non-coding work I have to do when I'm not using Haskell. I have to spend a lot more time playing around on the repl and testing as I develop. Thanks to Haskell, I'm used to loading the code and immediately getting a list of things to fix (type errors, warnings and the like); with other languages I actually have to run a few examples each time, even just to catch really trivial mistakes like typos. Another thing that Haskell really improved was my planning ahead of time. The types naturally constrain what I can do, generally pushing me in the right direction. They prune the search space. Getting the hang of using types protectively like this really improved my productivity. So I'd say that Haskell easily became my most productive language after a bit of practical experience to understand the philosophy, idioms and libraries. (I still find picking up certain abstractions, ones that come with their own idioms and philosophies tricky. Conduit and lens, for example, slowed me down at the beginning. Now, I think, there are many problems I'd solve faster than before with lens! It's the same experience I had with Haskell proper, in miniature.
In texts they often use ○, `&lt;&gt;` is the prettiest approximation, I guess. 
&gt; What other languages have different standard libraries for people learning and people not learning? Racket is a notable example because they do this rather well.
Can *type erasure* be done at all in dependent languages? I read somewhere that true dependent language don't actually have a phase distinction between "compiling" and "running". Edit: found [this](http://eb.host.cs.st-andrews.ac.uk/talks/plpv13.pdf).
I've seen an actual diamond used sometimes too, though
&gt;(.) = fmap ???
This is exactly how I feel - And I've only been using it for about a year.
&gt; Maybe someday when we have a kick-ass Haskell IDE that will show you the inferred type of any subexpression anywhere in your program This is called [ghc-mod](http://www.mew.org/~kazu/proj/ghc-mod/en/), it works on both Emacs and Vim.
(.) and fmap are equivalent for functions. I did wonder why (.) wasn't generic to Functors when I first realized that, though obviously I hadn't encountered Category at that point.
Tell me how well this worked for Python or Perl.
It makes the category theorist in me cry as well. Cale used to argue (and still might) it was a more useful definition to a Haskeller than generalizing to Control.Category given the ubiquity of Functor instances and relative paucity of Category instances, but fortunately the proposal never gained any traction. =)
; A;
RE last point: &gt; More importantly, the wiki helped summarize and explain a discussion that was extremely laborious to read and impossible to understand by looking through a mail list. Yes yes yes yes. I really think this is needed more that I noodle about it. I believe it has helped immensely with GHC and I think it's crucial for several reasons to help keep things moving. Look at the Python Enhancement Proposals (PEP) index: http://legacy.python.org/dev/peps/ These are beautiful, straightforward, have a relatively standard format, are indexed for historical records, are revision controlled, and go through a standardization process like ours. Some are big, some are small. As it stands, we have a 'process' (the last part), but it falls flat in several ways. We frequently hit points like: - What proposals occurred before? - What status is a current proposal in? - Did a specific proposal occur before - why, and what happened to it? - What discussion started the idea? - What objections or concerns do people have about it? - What refinements did a proposal have? A mailing list just doesn't scale for recording *these particular things* in an accessible way, honestly. `libraries@haskell.org` is great for discussion and open forums, when we want to have an open discussion about what might happen, and let people voice concerns. But it is awful as a historical index/catalogue, and an awful place to get an overview - many times the discussion happens *first*, but rarely do refined abstracts of a proposal appear. Reading the full mail thread is almost always required to get a 'full, updated understanding' of what state something might be in, just as you said. I think adopting a process like the PEPs - with the primary motivation being a better way to document and evolve proposals - would be a great idea.
I would love a PEP process. It also gives new users a way to see how the language has evolved, which can be very informative to get a good understanding of how to use the language.
I don't think "untagged union" is the right name for what you're describing. An untagged union is like a C `union`: union { struct { uint8_t red; uint8_t green; uint8_t blue; uint8_t alpha; }; uint32_t color_code; } color; That is, the same four bytes in the memory can either be interpreted as four individual 8-bit channels, or as a single 32-bit color code. It isn't possible to pattern-match or otherwise examine the value to determine which of the two it is, as the value is both at the same time. Nevertheless, I think you might be interested to read more about the research on untagged unions. Pierce, for example, has a [few papers on "union types"](http://www.cis.upenn.edu/~bcpierce/papers/index.shtml#Intersection and Union Types), as they are called in the academic world. I think you'd be interested because you suggest that sum types would implicitly create subtypes for each term, and that's precisely how union types are represented. In the paper "Programming With Intersection Types, Union Types, and Polymorphism", for example, Pierce writes (when translated into Haskell syntax): &gt; The injections `Left : a -&gt; Either a b` and `Right : b -&gt; Either a b` are replaced by *implicit* coercions represented by the subtyping laws `a &lt;: Union a b` and `b &lt;: Union a b`. It is important to note, however, that those subtypes are not like child classes in an OO language. In your text, you are implicitly assuming that given a value of type `Union a b`, it would be possible to figure out whether the value is actually an `a` or a `b`, via pattern-matching or `instanceof` or otherwise. This is not possible in the language which Pierce is describing: &gt; Whereas each element of `Either a b` contains a tag indicating which of the two summands it comes from, elements of the union type `Union a b` are untagged: the only operations that can be applied to values of type `Union a b` are those that make sense for *both* `a` and `b`. That being said, in a language with `instanceof` (so, not in Haskell), I think it would be perfectly fine to have union types in which the summands could be distinguished. For example, I found a language called "Ceylon" which [already has this kind of union types](http://ceylon-lang.org/documentation/1.0/tour/types/): void printType(String|Integer|Float val) { switch (val) case (is String) { print("String: ``val``"); } case (is Integer) { print("Integer: ``val``"); } case (is Float) { print("Float: ``val``"); } } And it looks like [union types in TypeScript](https://github.com/Microsoft/TypeScript/issues/805) are being discussed this very week: var p: string|Point = /* ... */; if(typeof p === 'string') { console.log(p); // OK, 'p' has type string in this block } else { console.log(p.x.toString()); // OK, 'p' has type Point in this block }
You may want to check out ocaml's polymorphic variants. You can do kind of what you want but they're "very unsafe"... Rather it's the other way around - you don't need to declare the types at all but only functions which operate on the "anonymous sums". The code below runs, without defining `Cons`, `Nil`, or `List` anywhere let rec (sum : ([&lt; `Cons of int * 'a | `Nil] as 'a) -&gt; int) = function | `Nil -&gt; 0 | `Cons (x,xs) -&gt; x + sum xs let (value : [`Cons of int * 'a | `Nil] as 'a) = `Cons(1,`Cons(2,`Cons(3,Nil))) let (summed : int) = sum value note that neither type annotation is necessary but without the annotation on `value` its type will be fully expanded as [&gt; `Cons of int * [&gt; `Cons of int * [&gt; `Cons of int * [&gt; `Nil]]]] which can be monstrous for large data. Because polymorphic variants form open sums (with the natural subtyping relationship), we could also define a function `f` that only works on the `Cons` constructor, and pass a nonempty list to either function
&gt; I would love a PEP process. If we need a quirky name we can just go with the acronym HIP/HOP = Haskell Improvement / Optimization Proposal / Project.
YAAAAAAS
Probably true, but I'd still like a `Vector n a` or `HVect ts` to be implemented as something that gives indexing performance closer to C++ vector. Even in Haskell, the term is often used for array-ish structures instead of list-like structures. I suppose doing something like a RRB-Tree would make proofs more intensive, but the name just strikes me as a little misleading.
I don't really see how. The compiler could be smart and do some pre-fetching in certain cases, but you'll still have to do the pointer chasing eventually, unless you change the underlying data structure and (possibly) lose sharing.
Polymorphic variants seem like an almost exact match, except maybe for handling recursion less implicitly. Nominal subtyping in general tends to make type inference undecidable, but row-variable polymorphism works fine. This is just the dual to extensible records: track possible constructors and their types in a similar way, but a sum `[fields]` is made from just one constructor instead of a record having a value for each field.
This article is well written and persuasive - nice! There are two concerns: 1. The code was not proposed, only a guiding design principle. That seems dangerous for something so critical. You may have insights that show their interpretation of the design is flawed. You have actually done the work that I am concerned has been skipped, so I want your opinion! 2. The plan for haskell2010 to contain the previous Prelude and tell beginners to use that is the current plan. It seems few think it is a good idea, and I don't think it will even work. 
With regard to `parallelOr` being definable in Haskell, we can use `Data.Unamb` (which, to my understanding, uses `unsafePerformIO` internally and this may disqualify it as a solution) to define an equivalent `parallelOr x y = unamb (x || y) (y || x)`. `unamb` is safe when both of its arguments always equal the same thing for non-bottoms, so this is "okay" because of the commutativity of `||`.
Thanks for sharing this :)
Why don't you think it will work? The only obstacle is the Applicative Monad proposal. The worst case scenario there is "infecting" `haskell2010` with `Applicative`. It has no impact on keeping currently monomorphically list functions in the prelude as monomorphically list functions.
&gt; This article is well written and persuasive - nice! That means a lot coming from the person I was quoting, I am glad that the debate is not creating animosity. 1. It would be good to solicit input from authors &amp; users of prelude replacements that have taken a similar approach, but right now I don't know the details of what is being changed. 2. It doesn't make sense to me to have a Haskell standard that is inoperable with current code. Am I wrong that the starting point for this approach would be a 2015 language revision with just the changes needed to make the standard work (Applicative/Monad)? Does it then make more sense to create a Prelude replacement for these purposes ahead of creating a new standard? 2. I don't know if the plan is to tell beginners to use something else so much as the plan is just to have that option available. 2. I think a more promising solution to the problem of generic type complexity is making specialization of type signatures easier in code, documentation, and compiler error messages. Unfortunately these are fairly new concepts and there is not a lot of time before the 7.10 release is supposed to be made.
Standard process seems so dead, could even just the Applicative/Monad haskell2015 actually happen? It certainly should. Regardless, I don't think Applicative/Monad should wait for the standard any longer, so many years have passed for that wart to finally disappear. And now its in HEAD!
At some point (when enough avoid-success-at-all-costs-failure has been accumulated) adopting a more formal PEP-ish process will be unavoidable.
... with a similarly viral reference/pun as PEP :D
When did they last make a breaking change? As far as I've seen a lot of the development is on LANGUAGE pragmas isn't it? This seems like the best of both worlds (although it wouldn't work here) because you get to keep advancing but you don't break people's code. Haskell isn't going to be use in very many places if you make breaking changes like this. Other languages do quite well with a proposal system. 
&gt; The page for Foldable/Traversable would explain &gt; 1.) what is being changed &gt; 2.) which changes create a breakage &gt; 3.) how type errors have changed &gt; 4.) how library code is affected &gt; 5.) how user code is affected &gt; 6.) best practices for using Foldable/Traversable We have committed to post a page on the state of the `Foldable`/`Traversable` proposal, including the parts where we feel we need direction from the community about how best to proceed. This stalled somewhat when `hvr` pointed out the scope of the breakage caused by AMP in [#9590](https://ghc.haskell.org/trac/ghc/ticket/9590). Unfortunately, the timing of things wound up a bit off. Austin posted the release notes for what was happening in GHC and Neil's first post on this topic went up while I was stuck at IFL 2014, and concerns rightfully raised by folks in the first 2 threads Neil started on this topic further changed what it was that I had intended to say in the writeup. As a result i've been flailing around here writing up piecemeal replies to folks rather than putting together the coherent document I'd originally promised to Herbert. =/ I'll pull together a frank summary of the current state of things and open design options, and importantly how and why they entangle with issues for AMP and Lennart's candidate `MonadFail` proposal shortly. &gt; Right now we are stuck in a loop of repeating the same points that were already made in the original discussion of the proposal. Given a wiki page, Neil and others could point out the down-sides of the proposal with actual code and have their voice heard in a productive way that builds up our body of knowledge. Hear, hear.
I said persuasive, not that it persuaded me :) - but it's the kind of thing that might. Certainly no animosity on my part! I also note that you made 38 minor tweaks to the Prelude, and that once bundled with GHC, that kind of thing becomes massively harder. 1. Wholeheartedly agree! We need to know what we are getting into, and then people who have travelled this road need to comment on it. You have 38 releases worth of minor tweaks, I don't want us to reach feature parity in GHC 7.86. 2. Not sure that's necessary, the code behind the standards isn't as important, since the haskell2010 package is basically unusable. 3. Beginners need clear and unambiguous guidance so they can focus on the language. Having a beginner Prelude is a bad idea. Having a beginner Prelude they aren't expected to use is an even worse idea. 4. Yep, that's part of why I wish things were moving slower. Get feedback. Make improvements. Do better.
Usually in Python they write the proposal before committing the code into the repo.
That sounds a lot like what is is called "policy based design" in C++
FWIW, I found testing monadic code in QuickCheck surprisingly painless. Check out [this StackOverflow answer](http://stackoverflow.com/a/2946515/2393937) for an example. Also, to narrow down your input space, you can always * Create a newtype wrapper for which you define your own Arbitrary instance OR * Write a custom Generator and use [Test.QuickCheck.forAll](http://hackage.haskell.org/package/QuickCheck-2.1.0.1/docs/Test-QuickCheck.html#12) Obviously, this is not quite reverse . reverse == id. Nevertheless, it is still no more complicated than defining a bunch of test cases manually.
You can do that (kind of) by creating a custom datatype for your AST. Then you define an Arbitrary instance for this datatype &amp; a toString method. Your properties take the AST as their input, convert it to a string and then check whatever the property is you want to verify.
&gt; The plan for haskell2010 to contain the previous Prelude and tell beginners to use that is the current plan. It seems few think it is a good idea, and I don't think it will even work. It was more that if we say that a more monomorphic teaching Prelude is a thing we want to offer, `haskell2010` is already in place so if possible it'd have been nice to be able to employ it for its stated purpose. I agree that the `haskell2010` plan does not appear salvageable at this point. The AMP looks to have killed it. **haskell2010** Given the tools we have in the bag today, at best that package can live in limbo, near to 2010 with either... 1.) `Applicative` bolted in, pushing it further away from the language standard 2.) its own Monad, rendering code written with it incompatible with hackage. 3.) or something like a horrid infectious overlapping, incoherent `instance Monad m =&gt; Applicative m`, which also effectively renders it incompatible with Hackage. I was holding out hope that at Hac Phi in a couple of weeks I could work with Richard Eisenberg on his version of [InstanceTemplates](https://ghc.haskell.org/trac/ghc/wiki/InstanceTemplates), and see if we could prepare it in time for 7.10. If we had that in place we could put in a default superclass default instance for Applicative in terms of Monad, which would resolve a number of issues. In particular there is a largish portion of hackage that hasn't acted on the Applicative-Monad Proposal warnings, so it'd enable us not to have the option to not break their code, but it'd also enable the `haskell2010` package users to still be able to define new monads without having to pull us any further away from the standard if we put a default definition for Applicative in in terms of the Monad. If we're forced to make a decision with the tools we have today then that fourth option ceases to be an option. **Prelude alternatives** If we abandon that package, which we basically have done, anther option seems to be to go with something closer to your original option of a second Prelude, possibly inverted, with an explicit `Prelude.New` (or `Prelude.Old` depending on selected direction), alongside with a passing proposal made by Simon Marlow a year and a half ago that if you import `Prelude.Foo` then NoImplicitPrelude would get set automatically. This would make alternate preludes easier for folks to push. An argument for it being `Prelude.New` with the new behavior deferred for a year as Yitzchak Gale suggested is that we wind up with Prelude already somewhat deformed from AMP, so it isn't really the 'Old' `Prelude` anyways. An argument for having the monomorphized `Prelude.Old` be the second class citizen is that things are set up to avoid systemic breakage and that the candidate user base is basically a subset of the user base for `haskell2010`, which has seen very little adoption. However, there is a *huge* caveat with both of these... they can't be implemented as such. Why? `Control.Monad` re-exports the monomorphized `mapM`, `mapM_`, etc. `Control.Monad.Foo` from `mtl` repeats this sin. `Data.List` re-exports monomorphized versions of everything, and is commonly imported unqualified as it exports the same version of the combinators from `Prelude`. etc. These are the very things that make importing `Data.Foldable` and `Data.Traversable` such a chore for users today. They have to hide the existing combinators and then keep on hiding them! `Prelude` isn't just one module being affected, but rather a large chunk of `base` (and `mtl`) that redundantly exports the same combinators is also affected. We could explore this option further if we also then go back and remove these re-exports -- rather than simply reform them to match the other types as we'd been planning. However, *that* increases the scope of breakage for 7.10 over what is strictly necessary to implement the raw original proposal. [Edit: I changed the link for Richard's proposal from DefaultSuperclassInstances to InstanceTemplates to get the right one]
&gt; I was holding out hope that at Hac Phi in a couple of weeks I could work with Richard Eisenberg on his version of DefaultSuperclassInstances, and see if we could prepare it in time for 7.10. &gt; If we had that in place we could put in a default superclass default instance for Applicative in terms of Monad, which would resolve a number of issues. In particular there is a largish portion of hackage that hasn't acted on the Applicative-Monad Proposal warnings, so it'd enable us not to have to break their code, but it'd also enable the haskell2010 package users to still be able to define new monads without having to pull us any further away from the standard. what is the status of that proposal anyhow? I gathered the https://ghc.haskell.org/trac/ghc/wiki/IntrinsicSuperclasses and/or possibly https://ghc.haskell.org/trac/ghc/wiki/InstanceTemplates superseded that particular design, based on the note on the top of the article? So is Intrinsic Superclasses the current contender? I've hoped for something like that for ages, and that design seems reasonable, possibly modulo some syntax bikeshedding... 
We have a proposal. It is the Foldable/Traversable Proposal / Burning Bridges Proposal a year and a half ago. It garnered over 100+ comments, across 2-3 threads, heavily biased in the positive on the topic of Foldable/Traversable generalization. A large part of the reason for the formation of the committee was to manage the sense of frustration that folks in the community had that nothing could ever change without complete universal agreement on any package that didn't have a dedicated maintainer. As more and more of the platform fell nominally into GHC HQ's hands there was nobody there who felt responsible for making decisions. Mind you, the proposal itself just said "swap out the monomorphic versions of combinators in base with the ones from Foldable and Traversable" and after SPJ formed the committee, and once we collectively figured out how to work with GHC HQ we did. That part is done. It was easy except for a few knock-on effects that we found when we went to implement it. Data.List and Control.Monad re-exported monomorphic functions, and we had to restructure parts of base to put the code in the right place to allow the re-exports. Finally, since we wanted to do it without changing semantics in user code, we needed the raw material from the "extra members in the Foldable class" proposal from a few weeks back. If we just want that proposal, it is summarized in that sentence directly, and already implemented. Anything around `haskell2010` or trying to make things better for folks who would prefer a more monomorphic Prelude is actually an extension to the scope. If you want us to go farther and try to make it easier for folks to try to work with a monomorphic Prelude, these are the things we need help with, but they _are_ technically out of scope of the original proposal. I personally want that smoother upgrade path, but we *could* live without it.
IntrinsicSuperclasses is Conor McBride's preferred version of things. DefaultSuperclassInstances [edit: actually, InstanceTemplates] is Richard Eisenberg's. Ultimately I think both of them contain good ideas and they could probably stand to cross-pollinate better. Trying to see if there is a viable timeline here is a large part of why I'm going to Hac Phi this year.
&gt; As far as I've seen a lot of the development is on LANGUAGE pragmas isn't it? Coincidentally I created the [LanguagePragmaHistory](https://ghc.haskell.org/trac/ghc/wiki/LanguagePragmaHistory) wiki page recently. Curiously, as it stands now, GHC HEAD (and thus 7.10) has only one new language pragma landed so far... 
Btw, there's a minor downside to using default-extensions: NoImplicitPrelude Tooling (editors, linters, etc) may get confused if it only looks at the `.hs` files and doesn't know `NoImplicitPrelude` is active, as opposed to self-contained `.hs` files which declare all `{-# LANGUAGE ... #-}`s explicitly in each module.
&gt; DefaultSuperclassInstances is Richard Eisenberg's. hm are you sure about this? goldfire created the InstanceTemplates trac page, while pigworker created both IntrinsicSuperclasses and DefaultSuperclassInstances pages ?
I may have linked the wrong page. I was running off a memory of the title from talking to him about it at Hac NYC and Hac Boston earlier this year.
You can, but it turns out to be a bit of a pain to get a nice distribution of generated strings. When I did that naively to test my own parser, most of the generated strings were way too long, which makes sense because most of my grammar's rules were not terminal. I played around a bit with the weights of each rule and got something that performed reasonably, but then the generated strings were too short and not really representative of realistic inputs. Generating good test cases from a grammar is tricky.
What's unsafe about polymorphic variants?
&gt; Then one day I learn that (`&lt;&gt;`) (“not equal to?”) has been defined [..] and I don't use (`++`) anymore. I'm in the same boat. I only use `++` if for whatever reason I want to force the type-inference to use `[a]`, otherwise I like the diamond-ish `&lt;&gt;` ASCII representation more than `++` to represent (generalised) concatenation. 
...post-exposure prophylactic?
Did you use [Test.QuickCheck.Gen.sized](http://hackage.haskell.org/package/QuickCheck-2.1.0.1/docs/Test-QuickCheck-Gen.html)? I do agree though, that actually producing realistic inputs is not easy. One could argue, though, that that's why QuickCheck should not necessarily completely replace regular HUnit-like testing but rather complement it. In that case "unrealistic" inputs can be a good thing because it allows you to catch unexpected edge-cases.
yup, post-exposure prophylaxis :)
We do have a PEP process for the Haskell platform, modeled after the Python PEP process: * http://trac.haskell.org/haskell-platform/wiki/AddingPackages * http://trac.haskell.org/haskell-platform/wiki/Proposals I must say it hasn't been a major success however.
When I last spoke with Richard we'd talked about including such a component in the proposal. I'm unsure if its absence is an act of omission or commission. The ability to split a class is particularly dear to me, if we ever want to have the ability to refine our class hierarchies without doing so on the back of every user.
I just want to remind everone, that if you're on Debian or Ubuntu, you can easily install nightly GHC HEAD bindists and try how the BBP/FTP/AMP-lified Prelude feels like, and see for yourself where it causes *actual* problems: http://deb.haskell.org/ ----- PS: and don't panic if GHCi starts up less verbosely, as we've [changed the verbosity for the RTS linker chatter](https://ghc.haskell.org/trac/ghc/ticket/7863), so GHCi starts up tersely as GHCi, version 7.9.20141003: http://www.haskell.org/ghc/ :? for help λ:2&gt; Control.DeepSeq.rnf () () it :: () λ:3&gt; as opposed to GHCi, version 7.8.3: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. λ:2&gt; Control.DeepSeq.rnf () Loading package array-0.5.0.0 ... linking ... done. Loading package deepseq-1.3.0.2 ... linking ... done. () it :: () λ:3&gt; PS2: and in case anyone wonders about the prompt, I've got :set +t :set prompt "\955:%l&gt; " :set prompt2 "\955:%l| " set in my `~/.ghci`
Ideas like this have been looked at since the early 80s under the heading of "intersection types". People began looking at applying them to functional programming in the early 1990s, when Freeman and Pfenning investigated adding "refinement types" to ML, with the idea that you could define subtypes of a datatype (eg, nonempty lists as a subtype of ordinary lists, or values as a subtype of terms). This quickly led to the idea that you could index types by arbitrary (but decidable) predicates on data. Hongwei Xi and Joshua Dunfield's PhD theses are both about this, and they both have implementations -- Dunfield implemented the Stardust language, and Xi and his students have built the ATS language. For Haskell, similar work was done by Dana Xu under the name "static contract checking". Ranjit Jhala and his collaborators have integrated these lines of work with the LiquidHaskell system. All of these systems are "research grade" (though ATS is mature enough to be used for production code if you're brave). Since people haven't yet found the "sweet spot" that gives the best balance of expressiveness, efficiency, and understandable error messages, there's still a lot of experimentation to be done, and so the emphasis is on building systems for hackability rather than robustness. 
&gt; In particular there is a largish portion of hackage that hasn't acted on the Applicative-Monad Proposal warnings I have the theory that `cabal install foobar` quieting down the build-process may have had an impact here, as the warnings would not be visible anymore unless you explicitly use `-j1`. Otherwise I guess people would have started nagging the respective maintainers to fixup the AMP warnings more actively by now. Next time we'll simply add a `threadDelay 60000000` right after such a warning is emitted. That should catch everyone's attention (just kidding... obviosly) :-)
For completeness, [digestive-functors](https://hackage.haskell.org/package/digestive-functors) is a formlets implementation by jaspervdj that works with Snap out of the box.
I should probably mention that [yogsototh](https://github.com/yogsototh/install-haskell) has been a source of inspiration to me 
I fail to understand this. I thought you wanted it as a teaching package, not as a package for students to _continue_ to use. What is there in `base` but not `haskell2010` that you feel is necessary for _teaching_?
You neglect that `classy-prelude` is in fact an ambitious change of the prelude. The changes under discussion here are relatively minor and involve the generalization of a few functions.
I just tried this with my [ace](http://hackage.haskell.org/package/ace) package. The sign up and creation process was surprisingly straight-forward and well-designed. The build and tests completed in 2m52s. You have to be logged in to see the log it seems, so [here's a screenshot.](https://dl.dropboxusercontent.com/u/62227452/screencapture-app-wercker-com.png) A few minutes is pretty spiffy.
Thanks. This is a really useful answer. It sounds to me like the lesson is that lightweight _opt-in_ hash-consing might be a huge win, but using it pervasively can be problematic. But I do wonder, in a pure context, if even without that we might get somewhere interesting. Relatedly, I was pondering if hash-consing not at runtime, but just at codegen time, might not yield a suite of optimizations -- in particular just allowing us to inline much more freely and recover sharing opportunistically.
The proposal "Swap out the monomorphic instances in base with the ones from Foldable and Traversable" doesn't say anything about exporting new names from Prelude like foldMap, mappend, and traverse. I don't see how it is logically necessitated by generalizing the functions which are already exported by Prelude. Someone who actually wants to write a Foldable or Traversable instance can import the necessary modules. Was this simply a mistake? If not it looks exactly like committing the code into the repo before writing the proposal.
GHC has Typeable, which could be used in the way instanceof would be used.
I updated my post, now there are links to the logs &gt; All the requested packages are already installed don't you just love when cabal tells you this :) &gt; build and tests completed in 2m52s the interesting thing is, that *build and tests* takes only 57s. most of the time (1m45s) is spent on unpacking the *haskell-stackage-preinstalled* box anyway, thank you very much for interrest 
Wait, the other names are going into Prelude? Why? We don't export all of Control.Monad from Prelude, why should traverse be any different?
The more a box is used, the bigger the chance is that it's in the server's cache, which should decrease the time spend in the "setup environment" step.
That's quite true! [Here is a quick and dirty implementation of union types in Haskell](https://gist.github.com/gelisam/5968c688d1df47e981f5) I made using your suggestion.
I don't want a teaching package, I think a teaching package is a terrible idea. I want a package for general use. I think teaching something and then having a cut off point 4 months in where you say "you're a big boy now, you get to play with real Haskell" would alienate everyone (and I can't imagine any teacher doing that, because they'll skip the teaching package).
Actually, `Monad(..)` + a couple of non-methods out of `Control.Monad` are currently re-exported from `Prelude`, notably `sequence` and `mapM` (but no `forM` nor `when` for whatever reason). I think it makes perfect sense to have the more fundamental `traversable` and `sequenceA` exported as well, as a logic consequence of the AMP.
One option left out is the [Free Monad](http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html) approach. Currently at work without time to provide example in your case. I'd love to know the downsides of using `free`, though.
&gt; AMP, which is pretty much still happening now. Sorry, I meant before recent changes. It seems to me Haskell has been pretty stable for quite a while in terms of breaking changes and that has coincided with a build up in popularity. I can't think of anything in the late 2000s that was a breaking change and I wasn't around for the early and mid 2000s. &gt;Look at what a mess python and perl are both in by trying to never change anything and then do one massive "fix everything in one go" version. To be fair the fix it in one go version was fixing syntatical annoyances for Python. Most of the development in libraries and syntax have been backported - async io comes to mind along with threadpools. I think this shows you can have backwards compatibility and development at the same time, as long as you are careful about it. Also it's not like everyone hated Python 2 because of these slightly messy bits of the language. It's a minor annoyance and dealing with a minor annoyance seems a lot better that not being backwards compatible.
I would argue that it was a decision. We have a [Libraries Submission](http://www.haskell.org/haskellwiki/Library_submissions) process. &gt; The maintainer is trusted to decide what changes to make to the package, and when. They are strongly encouraged to follow the guidance below, but the general principle is: the community offers opinions, but the maintainers decide. The core libraries committee acts as a collective maintainer for the portions of the Haskell Platform that aren't maintained by anyone else. We had a nuance to the proposal that required a decision, and so we made one. Admittedly we also have, &gt; API changes should be discussed on the libraries mailing list prior to making the change, even if the maintainer is the proposer. The maintainer still has ultimate say in what changes are made, but the community should have the opportunity to comment on changes. However, unanimity (or even a majority) is not required. &gt; Changes that simply widen the API by adding new functions are a bit of a grey area. It's better to consult the community, because there may be useful feedback about (say) the order of arguments, or the name of the function, or whatnot. On the other hand few clients will actually break if you add a new function to the API. *Use your judgment.* The precise space of names we want to add to the Prelude is still open for discussion. It is in the space of things we need to talk about -- we have as-yet unmerged changes into GHC around this question, but I do personally think the balancing act of utility vs. complete non-breakage is best served by making the classes that are exported from the Prelude implementable as given to you.
I don't want to speak for the person you're replying to but the reason I think it is different is because I believe there is a time and a place for Foldable and Traversable and they are not needed or beneficial in everyday use - the normal functions are.
Ok, thanks
Haskell report uses BNF IIRC. I'll get you a specific section once I'm at a computer. **update:** Hmm, [this](https://www.haskell.org/onlinereport/haskell2010/haskellch10.html) is what I was refering to, though it's not a BNF. I was mistaken in that the Haskell 98 report says that ["BNF-like syntax is used throughout"](https://www.haskell.org/onlinereport/haskell2010/haskellch10.html). Sorry to get your hopes up like that.
Funny, I have the opposite experience with `Foldable` and `Traversable`. They work over a much wider range of types that I use every day day, including lists. Functions like `foldMap` get quite a lot of use. I also like having `Functor` instances for datatypes that are functors, and `Monoid` instances for monoids. This particular flavor of Haskell might not be the one you prefer, which is fine, but does that really mean it's not needed or beneficial for everyday use? It meets my bar.
When I wrote about [small languages](http://blog.chewxy.com/2014/09/03/small-languages/), I noted that Haskell's BNF can actually be found (and easily parsed out of) [Haskell Report 2010](http://www.haskell.org/onlinereport/haskell2010/haskellch10.html#x17-17500010). As for the contexty bits, it's elsewhere on the report (BNF doesn't cover semantics well)
At last check there was a notion that you could get many things to typecheck that would be normally disallowed as long as the type ran through a polymorphic variant, but I haven't looked recently.
&gt; :set +t &gt; :set prompt "\955:%l&gt; " &gt; :set prompt2 "\955:%l| " nice
I think it's most likely a combination of issues, one of them is that it's not widely known.
This is basically what Scala does. Scala implements sum types as case classes which extend the the union class and you can pattern match on them like sum types. I personally prefer the Haskell way of doing things, though. It's much less confusing and easier to infer the types of things.
&gt;Disclaimer #1: you might need more than 1 cup. Perhaps every download of lens should include a free [Sports direct mug](http://i.imgur.com/6sbR6Qh.jpg)!
I didn't know about `sized`, but I ended up hacking together something similar. It's hard with grammars, because you can't arbitrarily stop at any point: you can only stop once you hit terminal symbols. 
http://pottery.dynamicpatterns.com/graphics/decaf-mug-baby.jpg
Nixos?
&gt; ...and they are not needed or beneficial in everyday use - the normal functions are. I beg to disagree. Unless you limit yourself to using only lists, the Foldable/Traversable methods and combinators help me reduce the degree I need to sprinkle my code wih `Map.`, `Vector.`, `Set.`, `Array.`, `HashMap.`, `Seq.`, `DList.` etc prefixes for many of the operations, instead of simply focusing on what I want to do with the data-structure and not being always reminded which flavor of a data-structure I'm currently dealing with (which often changes over time).
So your argument is you want a package for general use that is less powerful than what others want? My understanding is that you had raised two concerns -- one the complexity for beginners, the _other_ the complexity for practitioners. If you are arguing that a teaching package is besides the point, then we should just stop talking about it, since even a great one obviously won't alleviate your concerns. This just leaves the argument that _you_ prefer a more monomorphic prelude, while many others do not. That's something we, frankly, can just vote on. Having a "simple" practitioners prelude is in fact a much easier task than a genuine beginners one. We don't even need `haskell2010` for that -- you just can import your own more monomorphic prelude over the existing `base` and have things "just work". So the only issue is then, what shall the default prelude be, and the answer is "the one that is closer to what most people want." At which point, if you're in the minority, you just have to jump through a few hoops -- easier hoops than in fact everyone now has to jump through to program in a more Foldable/Traversable heavy fashion.
I've heard about Nixos. How does it work? Can someone send me a "Nixos expression" with GHCJS on it?
I was there for that. Great talk. Note that CRDTs and LVars are not quite the same thing (last time I checked).
I don't think it's possible to have every package on Hackage installed at the same time: with the number of packages out there, it's likely that there is at least one pair of packages where every version of the first says that it absolutely needs libfoo &gt;= 2.0, while every version of the second one says that it needs libfoo &lt; 2.0. Have you tried stackage? The fine folks at FP Complete picked a subset of Hackage where all the packages are compatible with one another, and they published a cabal configuration which guarantees that everything will be compatible with everything else. There is even a [debian-bootstrap.sh](https://github.com/fpco/stackage/blob/master/debian-bootstrap.sh) script which starts from a blank debian or ubuntu installation, installs all the external dependencies, downloads GHC, configures Cabal, and builds all the packages; it it works for you (my attempt in a VM failed mysteriously during the "install all the packages" phase), it sounds like you should be set.
And it shows we're [hep cats](http://dictionary.reference.com/browse/hep?s=t&amp;path=/).
&gt; Is there a list of libraries it includes? The first few lines of the script apt-get all the libraries you should need. I had to add debian squeeze to my `sources.list` in order for some of them to be found. &gt; But how could it fail for you? Didn't you use an empty Debian? I suspect that my VM ran out of memory or something. I haven't figured it out yet.
If it's NP-complete anyway, maybe you could use brute-force search? The list monad is quite succinct at expressing backtracking search: -- | -- &gt;&gt;&gt; findPytagoreanTriples [1..6] -- [(3,4,5),(4,3,5)] findPytagoreanTriples :: [Int] -&gt; [(Int,Int,Int)] findPytagoreanTriples xs = do a &lt;- xs b &lt;- xs c &lt;- xs guard (a*a + b*b == c*c) return (a, b, c)
On Debian, run `sudo apt-get install $(apt-cache search libghc)`. Haven't used `apt` in a while, but that should work and there is a HUGE collection of compiled binaries from Hackage in the Debian repos. Beware that this is a very massive amount of packages (since that will include the `-doc` and `-prof` packages as well).
That would not work as they are not all compatible. What you are asking for, made realistic, is probably Stackage.
and that baby never slept again
The fact these discussions have been so alive the past few days indicates that maybe we already have reached that point, and it's time. :)
&gt; "Nixos expression" You should say "Nix expression". Nix is a package manager written in a purely functional language named Nix (nonrec/not-a-typo). NixOS is a nix-based distributive. Take a look at NixOS wiki, it is extremely informative https://nixos.org/wiki/Main_Page Also here it is your expression https://github.com/NixOS/nixpkgs/blob/master/pkgs/development/libraries/haskell/ghcjs-codemirror/default.nix If you're interested in some other links, here is a number of blog posts that seemed pretty helpful for me when I has been diving into all that stuff https://gist.github.com/dmalikov/613fa59063604e51d323 And final note. There is no need in installing NixOS to get this feeling of nix usability.
&gt; I would argue that it was a decision. With no offense intended, adding new exports from Prelude, which I believe has no precedent in any released version of GHC since Haskell 98, is not the sort of decision to make without community involvement. Let's consider specifically the change to export `traverse` from Prelude and what happens to someone who writes a program that uses only standard Prelude functions plus `traverse`—presumably the exact kind of person you're seeking to help with this change. That person has three options: * Don't import anything, since `traverse` is exported from Prelude in 7.10. They save one line of imports, but their program is now incompatible with 7.8 for no good reason. * Import Data.Traversable so their program builds with 7.8. However if they now build with 7.10 with -Wall, they get an annoying warning "The import of ‘Data.Traversable’ is redundant". So they are actually worse off with the change: they still have to write the import line, but now they have this useless warning (and if they turn off that warning, then they later won't see other redundant import warnings that may actually be of interest). * Import Data.Traversable conditionally on `__GLASGOW_HASKELL__` &lt;= 708. Now finally they are back to the functionality they would have had without the change, but they had to write 2-3 extra lines at the top of their file to get there. You're actually hurting the exact set of users you want to help, except for people who just give up on supporting 7.8, which we don't want to encourage users to do! I urge you to remove all these extra exported names from Prelude *now* and, if you see fit, put together a proposal to add them back for community discussion. 
I bumped into this with Blacktip and I learned something valuable about IO in the process :)
cabal sandbox add-source ${u-c-src}
&gt; Import Data.Traversable conditionally on __GLASGOW_HASKELL__ &lt;= 708. Now finally they are back to the functionality they would have had without the change, but they had to write 2-3 extra lines at the top of their file to get there. For however long their support window is, at which point the overhead goes down to zero.
&gt; ...adding new exports from Prelude, which I believe has no precedent in any released version of GHC since Haskell 98... The AMP also brings new symbols into Prelude during this release.
this sounds like a (further) formalization of the libraries list with even more process around it...
&gt; proposal made by Simon Marlow a year and a half ago that if you import Prelude.Foo then NoImplicitPrelude would get set automatically. This would make alternate preludes easier for folks to push. That is a really nice idea. 
Recursive polymorphic variant types can be inferred, so you get some of the same problems as `-rectypes` -- many common errors with recursive functions that would otherwise be caught by the occurs check instead result in well-typed infinite loops or impossible-to-construct arguments. It's also possible to write many small functions that are supposed to act on the same polymorphic variant type but due to an error get different variant types inferred, and rather than finding out as soon as you construct/pattern match the wrong thing instead everything will typecheck until you try to use the various functions together. In both of these cases the code you wrote will run without runtime errors, it just might not do what you expect or compose the way you want. They're not unsafe in a strict sense, but they convert some cases of wrong code not typechecking to wrong code typechecking with crazy types. 
Good overview. "Liquid types" is another name for predicate-based refinement types in Haskell. They use an off-the-shelf SMT solver to be able to embed quite expressive predicates in Haskell types that are checked at compile time.
nowadays apt-get is just a joke
You might google "esoteric programming languages" but will probably have to decide which of them are turing tarpits for yourself. No. 2 sounds like a neat project.
That's exactly the issue that I was half-remembering. =)
My point is that we have no experience yet with how this kind of change turns out in practice. Also, we at least had the AMP warnings in place to tell people that some new names were about to appear in Prelude. If anything, that sets a precedent saying that we are not going to dump new names on people without warning, much less do so in the same version of GHC!
There is tension here because while ghc is not base, a specific version of ghc is tied to a specific version of base. &gt; You already lose compatibility by using bleeding edge packages, so lets keep moving ghc forward as well. I don't see how having to change my code because library "foo" changed is ok where having to change my code because library "base" changed isn't. ghc moves forward with * bug fixes * better infrastructural features (e.g. dynamic libraries, linking together multiple versions of the same package, debugging compiled programs) * better run-time performance (hopefully :)) * better compile-time performance (see above) * wider platform support (iOS, Android, GHCJS etc.) * shiny new toys, I mean, language extensions Many people want these bug fixes and new features but don't want their programs to stop building due to changes in base. Those aren't incompatible things to want. They're only incompatible because of the way base versions are tied to ghc versions.
have a look at this * https://github.com/sideeffffect/install-haskell-stackage if you would like to test/deploy Haskell in cloud * /r/haskell/comments/2ifu66/announcement_wercker_box_with_stackage_packages/
It's in Ruby but the O'Reilly "Understanding Computation" book has decent coverage of these things: http://shop.oreilly.com/product/0636920025481.do
What's the alternative on systems based on Debian?
Make sure if you pull some functions into the class and do this, you pull EVERY function into the class. We will lose a lot of performance doing this initially, and probably list fusion too. Maintainers need ways to override and buy into algorithmic complexity.
Well, note how in the first interview Simon talks specifically about his interests as a researcher. Research is almost by definition bleeding edge, and researchers must be willing to invest a lot of effort into keeping up with the technology anyway. However, these interests are opposed to those of industry people who don't want to rewrite some software every couple of years, or library writers who would like their code to break only if they break it themselves. As far as I know, the big mainstream languages go out of their way to maintain backwards compatibility, at the cost of far greater ugliness than what is being discussed here. I used to be quite fond of Haskell for introducing breaking changes slowly, through LANGUAGE pragmas and very incremental revisions of the standard. Reading in some of these comments that GHC is apparently not even really trying any more to offer a standards-compliant mode of operation therefore worries me quite a lot.
List fusion was a concern going in. It surprisingly hasn't manifested in practice as a benchmarking problem. The extra components about what to fuse into the class was described in a separate thread on libraries@ a couple of weeks ago. The thread for that didn't advocate putting in "every function" but only just enough that the things that can be reasonably cached / calculated could be handled by individual containers as they saw fit.
My understanding is that CRDTs embody the idea of having merge function that works for any two values inhabited by that data type (this is one of the implications of data type being join semilattice) and not something domain-specific as in Riak 1.x. Are LVars any different?
Excellent article! I'll take 10.
To complete /u/jberryman's answer, [Esolang](http://esolangs.org/wiki/Main_Page) is a good place to go. There are a lot of [categories](http://esolangs.org/wiki/Esolang:Categorization), of which [Turing tarpits](http://esolangs.org/wiki/Category:Turing_tarpits) will be of interest for you.
My recommendation is when starting from scratch cabal install foo bar baz quux quaffle all of the different packages you need to have work together at a consistent version. If you do that high enough up on the package pipeline, and with things that are preferably part of stackage, so they are getting built regularly together, then everything just works. It is mostly these piecemeal installs that get you.
&gt; But fixing a slightly messy bit of the prelude doesn't seem as important as that. I don't particularly wish to have any mess in the language forever unchangeable due to mere industrial inertia. Sure, time needs to pass between consolidations, but it is way, way past on things like AMP. Whatever happened to avoiding success at all costs? We've been failing in that mission as a community for far too long, time to see if breakage due to simple cleanup might help :P
is this a [synchronicity](https://en.wikipedia.org/wiki/Synchronicity)? :) for others, this post explains it in more detail: /r/haskell/comments/2ifu66/announcement_wercker_box_with_stackage_packages/ thanks for the free advertisement 
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Synchronicity**](https://en.wikipedia.org/wiki/Synchronicity): [](#sfw) --- &gt; &gt;__Synchronicity__ is the [experience](https://en.wikipedia.org/wiki/Experience) of two or more [events](https://en.wikipedia.org/wiki/Event_(philosophy\)) as [meaningfully](https://en.wikipedia.org/wiki/Meaning_(non-linguistic\)) related, where they are unlikely to be [causally related](https://en.wikipedia.org/wiki/Causality). The subject sees it as a meaningful coincidence. The concept of synchronicity was first described by [Carl Jung](https://en.wikipedia.org/wiki/Carl_Jung), a [Swiss](https://en.wikipedia.org/wiki/Swiss_people) psychologist, in the 1920s. &gt;The concept does not question, or compete with, the notion of [causality](https://en.wikipedia.org/wiki/Causality) (however critics state that the causality, statistics and probability theorems, is enough for explaining cases of "synchronicity", which are in fact __"normal events of low probability"__ ). It maintains that just as events may be connected by a causal line, they may also be connected by meaning. A grouping of events connected by meaning need not have an explanation in terms of a concrete sense of cause and effect. &gt;In addition to Jung, [Arthur Koestler](https://en.wikipedia.org/wiki/Arthur_Koestler) wrote extensively on synchronicity in *[The Roots of Coincidence](https://en.wikipedia.org/wiki/The_Roots_of_Coincidence)*. &gt;==== &gt;[**Image**](https://i.imgur.com/tgR0svC.jpg) [^(i)](https://commons.wikimedia.org/wiki/File:LewisCarrollSelfPhoto.jpg) --- ^Interesting: [^Synchronicity ^\(The ^Police ^album)](https://en.wikipedia.org/wiki/Synchronicity_\(The_Police_album\)) ^| [^Synchronicity ^\(Olivia ^Lufkin ^album)](https://en.wikipedia.org/wiki/Synchronicity_\(Olivia_Lufkin_album\)) ^| [^Synchronicity ^\(Bennie ^K ^album)](https://en.wikipedia.org/wiki/Synchronicity_\(Bennie_K_album\)) ^| [^Tuner ^\(radio)](https://en.wikipedia.org/wiki/Tuner_\(radio\)) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cl2964h) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cl2964h)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I'm a little skeptical that the examples in that SO post look simplified... Testing plain old IO actually seems quite a bit easier than testing the monad stacks that I'm dealing with for web programming. In particular, the startup time to run actions, at least for snap, is non-negligible (as startup involves reading config files, loading templates into memory, etc), and for large test suites to be viable, that needs to be amortized (otherwise you're looking at, best case, a couple seconds per test... which if you are using QC to generate 100 per property, is going to get pretty ridiculous pretty soon). This has really been the hardest thing when doing this kind of testing. I can't say I've pushed through the code to get QC working with this, and the only other person I've seen do this is essentially using QC just for unit testing (ie, they've restricted the domain so much that they are basically just getting the same thing every time), at which point, I'm not sure if _any_ hassle is worth it. I'd be curious to see examples of monadic QC that aren't just simple IO functions. "No more complicated" sort of demands examples :) It doesn't have to be web code; any monadic action with a non-trivial startup time would be interesting. As randomized testing _is_ really cool... and I'd love to do more of it. (But all testing is also cool, and unit testing gets a really bad rap from people who think it means checking that java accessor functions return the proper attribute).
Doesn't seem to me this would ever generate a prelude revision. The entire point of a prelude is that it is common ground. People won't depend on a new however slightly incompatible one that just does away with a dozen particular import ... and import ... hiding incantations one repeats throughout his/her modules. Yet while made least likely by this proposed procedure, those are precisely the kinds of changes - fairly noninvasive and simple ones - that should rather stand the best of chances for being standardized. By that metric you can prob find quite a few packages in the wild already manually creating the changes here proposed: basically any using foldable/traversable, basically any creating functor, and applicative instances for their monads ... If a more substantial breaking reorganization were to be considered, prob a trial period like described would make more sense, though the exact migration path for any particular set of changes should not be fixed but rather a matter of maintainer judgement - and THAT is the current process for changing prelude, and any other package. And the right one IMHO.
Nope, the report deals with that in a separate, [algorithmic](http://www.haskell.org/onlinereport/haskell2010/haskellch10.html#x17-17800010.3), phase, translating it to explicit braces. Note that doing that in an actual implementation is a bad idea, it leads to seriously awkward error messages. As evidenced by Hugs.
the delta between the official standard and haskell in the wild has been steadily growing for some time now, to fairly unsustainable levels. by 2010 there was significant hope much of the extension will be incorporated into it. Multiparameter typeclasses and an extension to make them workable like fundeps or type families were a blocker at the time - some provisional lists of likely extensions to be incorporated would include the former but not the latter etc. This failed to deliver, mostly because of the dominance of the GHC compiler making the standard largely irrelevant, and hence few people willing to sink their time into the tedium of precise deltas for the spec nobody really uses. This makes GHC haskell the defacto standard. Its pretty much impossible not to at least implicitly use a language with a fair number of them enabled, through the libraries one depends on at least. Trying to compile anything of significance on an old standard compliant compiler should easily convince you of that. Haskell in the wild is so saturated with extension-enabling incantations that its fairer to consider a fair bulk of them part of haskell proper. Admittedly this rarely necessitates breakage, beyond reserving some obscure keywords - such is the nature of extensions. removing N+K patterns comes to mind, or not generalizing local let definitions (in the presence of gadts and similar at least), or removing contexts from datatypes, or the few breakages from roles. Ofc various libraries themselves are not too shy of breaking their clients. But certainly one library - the prelude - effectively had no maintainer in this GHC-haskell world, so nobody could take charge in acting similarly. until now, when SPJ suggested creating a collective maintainer for such cases in order we could move on there as well. So they did.
It's definitely not fundamentally unreasonable. All it would take is decoupling the user-facing parts of base from the parts which are linked to implementation details of ghc. Yes it would be a big project, but it's possible in principle. 
There's actually a little trick to avoid redundant import warnings: GHC only considers an import redundant, if the previous imports already provided everything a given import statements would bring into scope that's needed by the current module. So simply add an explicit `import Prelude` to the end of your import list, as e.g. import Data.Traversable import Prelude -- ... code that uses stuff from Traversable... and you'll get code that doesn't need any CPP that compiles with `-Wall` w/o warnings in both GHC 7.8 and GHC 7.10
That is kinda gorgeous.
There have been a number of requests for overviews of FRP—this one is far from "complete", but provides a particularly nice view on AFRPs.
I actually just came here to ask if someone could pre-do the steps in this post against Stackage, then we could get a Stackage Travis box.
This is awesome. Can you please explain how you write a wrecker.yml file (I figured it out, but nothing in the post makes it super trivial). Thanks for doing that!
A slide deck by the same author with roughly the same topic: http://www.slideshare.net/edwardamsden/introduction-to-functional-reactive-programming
Is there any research work on implementation of co arrows?
You're welcome. And yes, hash-consing at codegen time probably is a very good idea. Although I would not be surprised if GHC already did something equivalent! It does have a CSE pass after all.
GHCJS specifically has been pretty difficult to install for me in the past, this advice is good for people to know but probably won't help in this case.
It's true that it's *ad hoc*, but after all, `Prelude` is already an *ad hoc* magical name, so it fits. Sorry about your breakage though.
Are typeclasses with laws really mutually exclusive to precise denotational semantics?
I confess, I've never managed to get GHCJS installed personally, so perhaps I shouldn't be dispensing advice on the topic. ;)
No. Indentation is a de-sugaring. It's not actually part of the grammar.
A few months ago I wrote a [blog series](http://fluffynukeit.com/installing-virtualbox-for-nixos/) about getting set up with NixOS for haskell. It should give you a flavor of what's what. At first it's tricky to get into the nix mindset, but after some experience I now usually have no problems installing packages or developing with different versions of libraries. I am by no means a nix or even a linux expert, but as long as I can work, I call that a success.
For installing packages, I prefer aptitude.[1] Contrariwise, I use apt-get for updating the package indexes.[2] Either will probably treat you well; both have been the recommended tool for a inter-release upgrade in the past. [1] In particular, I use the TUI to resolve conflicts, and audit recommendations and suggestions. [2] I switched to apt-get because aptitude was not giving me the exit code I was expecting for certain package index update failures and I was scripting some things.
The box has a sensible default build-test procedure: default-build: haskell: priority: 50 detect: - default: true text-to-append: | # Build definition build: # The steps that will be executed on build steps: - script: name: install dependencies code: | cabal install --enable-tests --only-dependencies - script: name: build package code: | cabal configure --enable-tests cabal build - script: name: test package code: | cabal test all you have to do is add `wercker.yml` to your repo with this content: box: ondrapelech/haskell-stackage-preinstalled@0.0.1 you can look at Chris Done's ACE * https://github.com/chrisdone/ace * https://app.wercker.com/#applications/543228274d7a367e23000cf0/tab and there is an official documentation (I haven't actually read it much): * http://devcenter.wercker.com/articles/werckeryml/ does this answer your question?
&gt; there is a HUGE collection of compiled binaries from Hackage in the Debian repos. It's significantly larger than the number of packaged hackage libraries in other distributions. It's a small selection of all the packages on hackage. Also, unfortunately, mixing Debian packages with non-sandboxed packages installed directly from hackage can be a bit of a pain point. :/
.. and it gets there once the current version rolls out of whatever support window the application developer chooses. This argument can be waged against changes to Typeable to make it polykinded, etc. and any sort of change that we've made to base over time that has caused users to ever write CPP to manage a transition, except here its not even CPP. You are effectively advocating for a world where we stay with the extra noise we do have now forever, because there is no transition where this doesn't happen in some release.
While the amount of mentions of NixOS on this subreddit do hint that their might be a organized marketing approach driven by NixOS maintainers, I feel like at least some of the mentions are mainly just happy Nix users. We just get more happy Nix users here because there is a parallel between some of the ideas of Nix and some of the ideas of Haskell; so there's more overlap between the communities than between (e.g.) Nix users and Rust users or (e.g.) OpenSUSE users and Haskell users.
&gt; debian squeeze Make sure you are using the LTS repos or you are likely to be eaten by a security bug. Wheezy is the current stable, and Jessie is currently being prepared for release. (Installer betas are out, and the sid to testing migration delay has increased.)
Thank you.
Thanks. I loved that wiki, but seems like they don't have much other than the known SKI, Jot and Iota, though. The point is, there are probably a lot of languages similar to SKI that are, too, turing-complete. Right? I guess how one could make a list... Specifically I'm looking for the most well suited language for program search using brute force. Any idea?
Nice book, thank you!
&gt; ... and it gets there once the current version rolls out of whatever support window the application developer chooses. Adding a line and then removing it sounds twice as bad as just adding the line, I'd say. But yes, new code written in 2017 will be able to save one line, if it happens to use just the functions that you happened to decide to export from Prelude. How lovely! I'm also distressed by the idea that a library necessarily has a "support window" at all (what happened to being able to write code to a standard?), but never mind. &gt; This argument can be waged against changes to Typeable to make it polykinded, etc. and any sort of change that we've made to base over time that has caused users to ever write CPP to manage a transition, except here its not even CPP. These changes give the user considerably more power than the ability to skip one or two lines of imports. I'm all for making changes that will actually let me do more things! &gt; You are effectively advocating for a world where we stay with the extra noise we do have now forever, because there is no transition where this doesn't happen in some release. Yes! That sounds great. :) Anyone who really cares about removing a few lines of imports can just create a module AllOfBase and import that in place of Control.Monad, Data.List, etc. That would be be far more effective than waiting for the functions they want to be exported from Prelude in dribs and drabs, and then they can take on the burden of dealing with changes in base, without the burden being imposed on everybody. Ultimately I'm frustrated by the fact that this kind of change is happening behind closed doors. This is a matter that ought to be decided by the community at large, and frankly I'm baffled that you don't see it that way.
uhh, well 1 line, and the relevant hiding &amp; qualified imports necessitated by name conflicts ...
&gt; what happened to being able to write code to a standard? probably died along with the standard, about 4 years ago :D
I thought that this would more be "union types" rather than intersection types. My recollection is that refinement systems are nice but tricky. On the other hand, opening up more unions means that nearly everything typechecks :-(.
I am talking about specifically the change to export `traverse` from Prelude, assuming that the Foldable/Traversable generalization changes have gone through.
&gt; I think we should just make clear that the core libraries committee should have the freedom to work out details of proposals without the massive noise in that massive libraries@ correspondence I think one of the mails concluded, after some 85% support for the change - that it is a bit ridiculous to demand higher consensus for a change in prelude than what is needed in democratic societies to start World War III :D 
Your "continuization" is more commonly known in mathematics as "dualization".
oh, right, sry for the noise
My understanding was that because of the scope of work that needed to be done, and the need to experiment and collaborate that the process here worked differently. Certainly communication has been not handled well, as these threads indicate, and that's something that is a definite lesson from all this!
Sorry, I do not understand. How do I use that expression? I put that in a directory, now what? How do I actually call ghcjs to compile something after that?
I gather BBP as such is considered approved; the technicalities of how much to expose exactly, and on what schedule look to be up in the air still. But whatever ends up sitting in HEAD, if not causing trouble, is prob out in 7.10 no? You think there would be another round of seeking community input on whether that exact export list is what needs to go into the wild?
ghcjs does things that ghc was never intended to do and has its own cabal patches, etc. It is kind of crazy that it works.
I've fairly recently gotten part basic lensing, so I don't know whether I've lost my beginner eyes, but this post seems to be fantastic. It contains explanations of all the bits and pieces I cobbled together to get to my current understanding / the point where lens really clicked for me, and then some. I can't wait to see more.
Dualization in math is a very fuzzy, broadly used term. I'm not even sure continuization is a form of duality.
I appreciate your appreciation of Yesod, etc. However, you have completely mis-characterized how I program libraries. There is very minimal usage of Foldable or Traversable in yesod, persistent, or any other of my libraries that I contribute to. Everything except for mono-traversable and classy-prelude defaults to using minimal abstractions, and almost all of the interfaces specify using lists. I only get to use mono-traversable and classy-prelude in my application code.
Additionally, I'd argue that the AMP is somewhat defanged by leaving Traversable out of the Prelude. If it's appropriate to have mapM (and I believe it is), it's important to have traverse as well. It seems that there's wide consensus that Applicative is an important abstraction the use of which we wish to encourage. Including it in Prelude on the one hand (which I think is a good idea) while not providing traverse undermines that goal. Having traverse suggests that we should probably have Traversable (the class, not necessarily the whole module) available as well. It would be unfortunate to be unable to instantiate Traversable without a separate import for Foldable. Foldable brings in Monoid. This is all to say that while the AMP necessarily brings in Applicative, Alternative, and their methods, taking full and (if you'll pardon the pun) idiomatic advantage of them strongly suggests that Foldable, Traversable, and Monoid come along.
The `protobuf` library uses text, `protocol-buffers` has the Utf8 type and utf8-string dependency.
If you have ideas on how to improve `protobuf`, please file issues in Github, complain here or send pull requests. I'm totally open to exploring other designs and providing better interoperability, especially if these come as pull requests with tests :-) I couldn't figure out a better way to associate the field tags with a record field.. so all fields end up with those `Nat` tagged wrappers that make conversions harder. Automatically converting a protobuf message to a regular ADT is possible, but the field tags and packing need to be written down somewhere. This might be in the form of a pair of ADTs: the persistent generated one, and a matching version with the protobuf field tags (and an auto-generated conversion between the two), or perhaps an associated type that just contains the field tags/types to apply to the original ADT. Layered on, if you will.
Well don't mind me, I'm just a schmuck on the internet. :) I haven't used protobuf myself. Glad you're on top of things though!
+1
As a Haskell beginner I got the sense that while Functor, Monad, Applicative, Profunctor, even Traversable though it has more complex constraints, have a certain logical inevitability about them while Arrow seems more like a collection of constraints that might be useful together. Is there a better road to FRP in your judgement?
Arrow arises from the same sort of "logical inevitability", but it is much further along the path, so it has a lot of stuff bolted in.
I'll be there too! 
Yep! :)
Also see Elm creator Evan Czaplicki's presentation: https://www.youtube.com/watch?v=Agu6jipKfYw
Looking forward to meeting everyone again!
&gt; Ultimately I'm frustrated by the fact that this kind of change is happening behind closed doors. It may appear so due to suboptimal communication (which was admitted to a couple of times already), **but** the changes per-se were *not* kept hidden from you "behind closed doors": - The GHC Weekly news mentioned them (which are also broadcasted to https://planet.haskell.org/ to reach a wider audience) - The GHC 7.10 status page mentioned work going on in that direction - Most of the commits went through https://phabricator.haskell.org and were each reviewed by at least two parties - Most of the commits have an associated ticket, so it creates notifications - The commits end up in various mail-archives, and most have meaningful commit messages (at least I tried to write them in such a way) - The CLC list archive has been opened up for the public recently (where you'll see there was no behind-closed-doors talk there either) So there were plenty of opportunities to tap into that information stream, IMHO.
Great lens exercises! (They reminded me very much of the "20 intermediate Haskell exercises".) I copy/pasted them to School of Haskell so you can fill in the blanks and typecheck it in your browser. https://www.fpcomplete.com/user/DanBurton/pastes/lens-exercise
I've updated my article to give an implementation in `Free` complete with intermediate representation. I've talked about down sides and ways to mitigate some tl;dr: closed set of requests, changes require modifying interpreter code. perfomance problems in left-associated binds: codensity transform and type-indexed datastructures are ways to make this faster
Click on "Open in IDE" for even more fun. I think using type holes to help get through these is a pretty cool approach.
Actually, the whole reason I started writing this was /u/tel's [Lensmaker](http://www.codewars.com/kata/lensmaker) challenge on Codewars. *That* is what I call “great exercises”.
ICFP talk this year on indentation parsing: http://youtu.be/bwEHh0BZc3Q?list=PL4UWOFngo5DXuUiMCNumrFhaDfMx54QgV I believe it discusses parsing Haskell.
Sorry, my wording was imprecise. The Prelude export change itself did not happen behind closed doors, but the decision to make the change did. 
Hey the link said decaf
Sandboxes! This will probably solve many of the OPs problems. Also understanding dependency version bounds and how cabal's solver selects.
&gt;Curry-Howard is an interesting mathematical equivalence, but it does not advance any argument about how to write software effectively. Just one sentence to cover that whole subject. Wouldn't type inference let you write software more efficiently by not having to declare the types everywhere while still benefiting from them? EDIT: So I did confuse Curry-Howard with Hindley-Milner in my haste. But I did not think that Fetishising Curry-Howard was a thing in most typed languages. I would have agreed to Fetishising Hindley-Milner which would have made more sense.
The *decision* to make the change was made in full view across several polls. The *details* were hashed out by building and building and building GHC over the course of a couple of months to work out the kinks. If we cannot collectively enact a change when you can have a thread on the libraries mailing list with over 100 responses, a separate poll with 110 replies, 90% acceptance, and a committee formed explicitly to be empowered to enact it then the language is dead and we're all standing around the corpse.
Are you confusing Curry-Howard with Hindley-Milner?
&gt; Wouldn't type inference let you write software more efficiently by not having to declare the types everywhere while still benefiting from them? It would, marginally, although whenever anyone mentions "efficiency" or "productivity" in this context we should always remember that fingers-on-the-keyboard is not the bottleneck in programming. But, Curry-Howard is isomorphism between programs (in languages with strong-enough type systems) and a proof of some theorem. Which is very nice, of course. TFA's point is that however nice it is, in the day-to-day practice of programming it's *no help* (so shut up about it, for now).
Wow, this kind of response is far beyond what I could have hoped for writing that! Thanks so much for putting this exploration down in words—I am certain it will help people enormously as they approach these trickier concepts and libraries.
`united` and `devoid` form final and initial objects in the (moral) category of `Lens` devoid :: Lens Void a devoid inj v = const v &lt;$&gt; inj (absurd v) united :: Lens a () united inj a = const a &lt;$&gt; inj () which is nice from a theoretical point of view—these properties are inherited from (moral) Hask.
They only reason they can't be better understood as a Haskell type is that they have ad-hoc polymorphism tossed in (which we could do, but would require some typeclasses) and also they have pervasive effects. That said, the model of transducers in haskell types is pretty nice, and if its a rehash of anything, its only a rehash of stuff we've developed over the past two years, in terms of the precise formulation you get out of it. Just because you don't like the vagueness of how transducers are described, I wouldn't be so dismissive about the idea that they're an interesting slightly different twist on our common FP set of ideas.
Ah, somehow I just found it easier to imagine a number without multiplication and addition. But there aren't typeclasses for those properties, are there?
See [my `foldl` library](https://hackage.haskell.org/package/foldl) which is the Haskell version of transducers.
If someone wants to make those arguments that's fine by me. I don't program to prove that I'm a grown-up, I program to make computer programs that do things. Similarly I don't walk when it is more convenient to take a train, I don't solve big integrals longhand when I can toss them into mathematica, and I don't use a hand screwdriver for big jobs when I have a power driver lying around.
The problem is that the family of flat tuples isn't algebraic, you need a separate case for each one. It seems like there should be a way around this considering how easy it is to convert between HVec and flat tuples in our heads, but I have never seen it done and suspect it's harder than it seems. 
No, your library isn't the Haskell version of transducers. It bears relationship to the reducers work. You can think of a transducer as a type: type Transducer a b = b -&gt; [a] And the key thing about them is the idea that you can _preappend_ them to realized folds such as in your library. So you give premap :: (a -&gt; b) -&gt; Fold b r -&gt; Fold a r Importing the idea of transducers gives: prebind :: (a -&gt; [b]) -&gt; Fold b r -&gt; Fold a r Which now lets you also e.g. `prefilter`, etc. Of course you can now add this to your library. But doing so is _importing_ a neat idea that came from elsewhere, so lets give credit where credit is due.
I think you mean the Curry-Milner system.
The article isn't meant to be a criticism or critique of static typing, it's mean to be an illuminating critique of the way people speak on behalf of static typing. And it says so: &gt;the theme here is rhetoric [...] advocates of type systems often try to sell them as a miracle cure, without acknowledging their limitations and undesirable side effects" TFA suggests that when speaking in favour of static typing one should * [avoid] Not distinguishing abstraction from checking * [avoid] Pretending that syntactic overhead is the issue * [avoid] Patronising those outside the faith * [avoid] Presenting type-level programming as a good thing * [avoid] Fetishising Curry-Howard * [avoid] Equivocating around “type-safety” * [avoid] Omitting the inconvenient truths in the interests of clarity and honesty.
&gt; statements about "we're all adults" and how we "don't need to be protected" from doing bad things by using "bondage &amp; discipline languages" I've honestly never heard that before. I find the Haskell "I'm not smart enough not to use it" far more common. &gt;Of course such statements imply that the people who do use static typing aren't really adults I don't think it does imply that. It implies that because we are all adults we don't need static types. They're trying to free you of the burden of static types if you will more than saying you're using static types because you're an adult. I also don't really like the idea of "look, them lot do it as well". 
&gt; But, Curry-Howard is isomorphism between programs (in languages with strong-enough type systems) and a proof of some theorem. Which is very nice, of course. TFA's point is that however nice it is, in the day-to-day practice of programming it's no help (so shut up about it, for now). I don't know if you're being ironic or not, but being able to prove correctness of your program is a pretty big help when programming IMHO.
http://esolangs.org/wiki/Funciton
Did you read the article? It says: &gt;Curry-Howard is an interesting mathematical equivalence, but it does not advance any argument about how to write software effectively. the important word there is *effectively*. TFA also says (correctly): &gt;Proof has a cost, and the appropriate expenditure depends on the task. It's far from a no-brainer. and &gt;Type systems *cannot* be a one-stop solution for specification and verification. They are limited by definition. They reason only syntactically, and they specify only at the granularity of expressions. They're still useful, but let's be realistic. [emphasis in original] For most of the programs that my clients will pay to have written, a proof of correctness would be exorbitantly expensive.
If you write code in Agda or Idris you use this correspondence all the time to prove certain (but probably not all) properties of your program. Of course there is a trade off between how much you want to statically prove and how much effort it takes to write the proofs (i.e. compared to writing tests instead), but to generalize and say that's it's not helpful to be able to write proofs is an exaggeration.
The last time I had asked this question, the consensus was that show and read are not the right tools for this. If you want efficient serialization/deserialization, Haskell has other libraries that are better suited. If you don't care about efficiency, JSON is a more commonly used format and Haskell has an excellent JSON library. `show` and `read` are purely for convenience/debugging, so adding a law like this would make them tougher to use.
In my experience, being able to prove properties of your programs is not often helpful and even more rarely economically justified. So rarely that touting it as a benefit of static typing is a red herring. As TFA suggests. &gt;If you write code in Agda or Idris If. I prefer to get paid for writing code ;)
One example of this is some people define a `Show` instance for functions (such as `const "&lt;lambda&gt;"`) that would obviously break the `read . show == id` law.
The article at least has the courtesy of supposing that being able to formally reason about programs is a good thing. If you question that, then there's really not much grounds for discussion.
That entirely consisted of absolute statements without any evidence or reasoning backing them up. What a waste of time.
A `Traversal'` is a special case of a `Fold`, so it seems that the signature `sclv` gives for `prebind` is strictly more general. Given that your implementation of `pretraverse` only uses `Fold` functionality I don't see why you wouldn't just have it accept a `Fold`. http://hackage.haskell.org/package/foldl-1.0.7/docs/src/Control-Foldl.html#pretraverse
&gt; In an untyped language, arbitrarily complex polymorphism is expressible. This is not clear to me. Typeclass polymorphism seems to provide a sort of polymorphism that you simply cannot have without types because you need types to do typeclass dispatch. &gt; Type systems cannot be a one-stop solution for specification and verification. True. The question is, when does having a strong type system get in the way of doing things you would have done without it? I can think of * Effect typing. Manipulating monad transformer stacks can be fiddly. * Mutating large object hierarchies Anything else? What is the real complaint here? I'm not going to claim that static type systems are a panacea, but if there are real use cases where a strong static type system would generally be a hinderance rather than a help then I'd like to hear about them.
Those suggestions are fine by me. However, I have two unaddressed issues: 1. If syntactic overhead is not the issue then what is? I don't buy the transducers example unless someone can demonstrate that it is genuinely not typeable in Haskell. 2. *"If everyone would just use a modern language with a fancy type system, all our correctness problems would be over, right?"* Please show me one Haskell proponent who claims this. 
"types are abstractions that get in the way of real-world programmers."
The OP is a researcher at a university, so I don't think it's unreasonable to suggest research languages as articles to be taken under consideration in the discussion.
`read` and `show` form a [retraction and section](http://en.wikipedia.org/wiki/Section_%28category_theory%29): * `read :: String -&gt; y` * `show :: y -&gt; String` * `read . show :: y -&gt; y` = `id` `show` is a section, `read` is a retraction. However, `show . read = id` is not true. So it's not an isomorphism. For example: `show (read "Just (2)" :: Maybe Int)` /= `"Just (2)"`. I would prefer it if it was an isomorphism. And if `show` was a function, not a method, defined in terms of a `showP :: a -&gt; P`, like data P = List [P] | Integral Integer | Floating Double | Alg String | Rec [P] | Tup [P] | String String and there would be a predictable `P -&gt; String` or `P -&gt; ByteString` or `P -&gt; Text` or `P -&gt; Builder` or whatever. Likewise, `read` would be a function implemented in terms of `readP :: P -&gt; a`, but you could also have `readBS :: ByteString -&gt; P`, etc. That would be perfect for tooling and having predictable printers. Your IDE, or REPL, or IHaskell, could then easily pretty print your data structures or make them collapsible and expandable or even do custom presentations based on the `Alg` constructor. Show a tree as a tree, etc. As it stands, tooling this is hard. Show is popular, but unreliable as discussed above. Data is more reliable (ish, some data types, Vector, Text, ByteString have useless instances), but in any given project there isn't an instance for most data types. My last recourse to eventually get something like this maybe be to use the GHC API to inspect types, or template-haskell, as I'm quite sure both of these are able to break encapsulation but I can do a check to see if there is an instance for Presentable or whatever, so that Text is printed as text and not pointers.
I think you have it the other way around: a `Fold` is a special case of a `Traversal'`: type Traversal' a b = forall f . Applicative f =&gt; (b -&gt; f b) -&gt; (a -&gt; f a) type Fold a b = forall f . (Applicative f, Contravariant f) =&gt; (b -&gt; f b) -&gt; (a -&gt; f a) The reason that I use `Traversal'` instead of `Fold` in the type of `pretraverse` is to avoid an unnecessary `contravariant` dependency. `foldl` tries to stick to the Haskell Platform. Edit: Oops. I just realized your point. What you're saying is still true, though, even if you swapped your intermediate step: `prebind` is more general because you can't take a function of type `a -&gt; [b]` and turn that into a traversal, but you can turn it into a fold.
&gt; I don't use a hand screwdriver for big jobs when I have a power driver lying around. Although, stripping screws is really annoying.
&gt; For most of the programs that my clients will pay to have written, a proof of correctness would be exorbitantly expensive. Everyone says this, but no one looks at the data. seL4 cost about $400/LOC and is estimated to cost about $200/LOC if it were to be done again now that the tool support and expertise have been established. This is a fraction of the typical figure of $1000/LOC used for _unverified_ high-assurance (EAL6) software development, and only about twice as much as low-assurance software engineering. Getting it down to be cost-comparable with low-assurance development is still a work in progress, but it's not nearly as prohibitively expensive as people think it to be. 
&gt; The reason that I use `Traversal'` instead of `Fold` in the type of pretraverse is to avoid an unnecessary contravariant dependency. Fair enough. Maybe you could provide /u/sclv's `prebind :: (a -&gt; [b]) -&gt; Fold b r -&gt; Fold a r` too, since it's equivalent to taking a `Fold` (I think)?
It would actually add unreasonable demands for their implementation, such as that `show` would have to be lossless (which would seriously hamper the human-readable-ish purpose of `Show` instances), and it would tie `Show` and `Read` instances into a mutual dependency, even when both are defined in separate (potentially independent!) modules.
Ok, then if you don't mind, what attracts you to hang out on the Haskell reddit?
I often had a dumbfounded stare from non static types believers when I explain some kind of errors they protect from. Then they say something like "but this is only useful for stupid people that write stupid bugs !" :)
I don't get involved in any high-assurance work, but I do a lot of what I think you'd call "low-assurance software engineering". I struggle to parse exactly what you're claiming here, but it's something like "low-assurance software engineering costs about $150/LOC" yes? I have a team in the field now that costs the client about $10,000 per day to run, which would suggest that they write about 65 lines of code a day, which as luck would have it is a bit less than *one line of code per programmer per day* for that team. Does that strike you as at all credible? As it happens, tomorrow is an iteration boundary so I'll find out from git how many lines of production code they wrote in the last two weeks—and that's code with unit, functional, integration and exploratory tests done. Watch this space.
For things like this vim's '`virtualedit`' option and normal-mode `r` command come in handy. I quite often use unicode box-drawing characters in my notes (it helps to learn vim's systematic digraphs for them), so I don't think it's too bad from a typing/editing point-of-view actually.
I'm not a researcher into software engineering, but $100 per line of code is definitely an industry rule of thumb. I'm not sure where it originated, but if you google you'll find hundreds of reasonably credible sources citing that figure. They're talking about total money invested divided by the number of lines of code in the final product. Of course, this is for low-level systems software, and a line of code in such software may be worth more than in your regular business CRUD app.
Proofs are nice, I don't have that much faith in *specifications*. When I started working as a programmer it was all about the specification, all about the ∀'s and the ∃'s and what I learned is that getting specifications right—demonstrably right—is really, really expensive. 
Ok, so my hope is as you soak up more of the stuff here you'll see that lightweight semi-formal reasoning with types is _incredibly_ cheap, and so the tradeoff is not nearly as onerous as you imagine :-)
But "lightweight semi-formal" anything is cheap. As you can see from my [comments elsewhere](http://www.reddit.com/r/haskell/comments/2ijwwz/seven_deadly_sins_of_talking_about_types_cross/cl338qb), I'm no stranger to formalism.
&gt; This is not clear to me. Typeclass polymorphism seems to provide a sort of polymorphism that you simply cannot have without types because you need types to do typeclass dispatch. How does python's duck typing not solve this problem?
&gt;so I don't know whether I've lost my beginner eyes, but this post seems to be fantastic. It looks *great* to me, I'm going to be testing it with the next person that wants to understand lens. :)
How do you write `return` in Python, where you have to specialise on the return type, not an argument type?
Well lightweight semi-formal reasoning _with_ types is cheaper than lightweight semi-formal reasoning _without_ them then :-P
Ok, fair enough. Forget about that point. My thinking is mainly driven by the fact that in every single significant software project (in any language) that I have worked on since I got out of school gigantic lists of probably 30+ imports at the top of source files are the rule, not the exception. This suggests that it is a fundamental reality of software engineering that will always be with us. Therefore, when I hear people complain about needing too many imports at the top of their modules, it feels to me a bit like a child complaining about having to look both ways before crossing the street. Is it inconvenient? Yes. But it's a fact of life and there are good reasons for it. It is also a problem that has been solved. (Notice I said "needing too many imports" not "having to type too many imports".) When I was doing commercial Java development I almost never thought about imports. Eclipse's organize imports functionality was a 100% solution to the problem. What we need is something like that for Haskell. Now maybe there is some use in consolidating some of the most common imports. But that approach clearly doesn't scale. Why not solve the problem once and for all with something that is tried and true rather than proposing a new core piece of our language?
Not only that, but to fully understand how a dynamically typed function is polymorphic, either very good docs have to be written or you have to understand the function and the places it's called. Static type systems like Haskell's help you encapsulate the behavior entirely in the signature, creating their own documentation.
I actually prefer expressive static types because I don't have enough confidence in myself without a logic providing some verification. If you're a god among programmers, you could probably write beautiful, understandable, and safe Bash. But none of us are.
&gt;Those suggestions are fine by me. However, I have two unaddressed issues: &gt;1. If syntactic overhead is not the issue then what is? I don't buy the transducers example unless someone can demonstrate that it is genuinely not typeable in Haskell. His arguments is that static type systems necessarily force you to write around their syntactic reasoning, rather than our own intuition. I don't see this as necessarily a problem though.
He's right anyway that it doesn't advance the conversation, but only because it's been so well established that you can actually leverage the "neat" result of Curry-Howard in a sensibly statically typed language. ;)
He's right anyway that it doesn't advance the conversation, but only because it's been so well established that you can actually leverage the "neat" result of Curry-Howard in a sensibly statically typed language. ;)
I worry about it, but I don't have any good ideas :)
Damned professors always looking for new ways to make their books more expensive. 
&gt;In my experience, being able to prove properties of your programs is not often helpful and even more rarely economically justified. Perhaps you should take a look at seL4 then? Just because it's not part of your experience doesn't mean there isn't much demands for even limited verification. &gt;&gt;If you write code in Agda or Idris &gt;If. I prefer to get paid for writing code ;) Heh fair enough, but many of us use Agda or Idris because it's so damn fun and beautiful, and in my case it's helped me grow as an programmer by changing how i think of the pre/post/invariant conditions of my code.
Least controversial statement ever. :)
I may have triggered this discussion by objecting to /u/ThreeFx and others using `Show` in [this SO question on printing mathematical expressions](http://stackoverflow.com/questions/26169469/taking-data-and-turning-it-into-a-string/). I posit that `Show` instances should generally mimic the derived instances, for ghci and debugging purposes; a "moral" equivalence, perhaps. If the `Show` instance helpfully illustrates the structure of some expression or data structure so that a new user can better understand how to use or implement something, all the better. But my original objection was that people were writing ad-hoc `Show` instances just to expose a function of type `a -&gt; String`. `read . show` is at most a side bonus. (I never use `Read` except for, say, wiring up a stupid GUI example.)
Worth more or cost more? Because those are very different things. And I hope that reference to "your regular business CRUD app" isn't you getting your No True Scotsman ready to dismiss my data.
Try using cabal to install tf-random. Sometimes I find that cabal is stupid that way. 
I think every programming language community chooses what to be worried about -- and then promptly dismisses all other problems. Haskell is no exception. I think most Haskell developers are aware of the problem you mention here. And in cases where the typeclass is based on some mathematical algebra, the laws are specified somewhere. (Although, in many cases, they are stated in an ass-backwards way, say with `Monad` and `Applicative`, whose laws look nothing like the simple form they take in category theory). I think the major source of problems tends to come up when a typeclass appears in a more practical context. Something like `MonadPlus` is probably a good example. Someone was working on a parser, they knew that parsers formed a monad... but they also wanted a monad where you could "take alternatives". They managed to work out just enough of the problem to throw together an implementation, it got seeded into the standard library, and we have never been able to get rid of it since. The laws aren't specified because no one ever took the time to actually figure out what the laws ought to be. Informal reasoning is cheap, and so you can only naturally expect to see more of it than the other kind. Perhaps as a culture, the Haskell community should be more discriminating against entrenching vaguely-defined typeclasses. But it's (unfortunately) hard to argue against "hey it works, doesn't it?" 
I'm no expert on using cabal (or OSX for that matter) but the line which says ``` cannot satisfy -package-id primitive-0.5.2.1... ``` appears you're missing a dependency for tf-random-0.5 called primitive. [This package appears to be it](http://hackage.haskell.org/package/primitive-0.5.3.0) You may be able to download it with `cabal install primitive`, but like I said, I have no experience with cabal. Hopefully that points you in the right direction. 
You are right sir, you were the catalyst of this question ;)
&gt; I am not here to push an inevitable march towards dependent typing. I am, as I think it is really the best way to document properties like this. Sure some we get as free theorems (like, IIRC, the traversable laws), but for others, it would be nice to at least write them out using the same language we are already in. There some things I don't like about the push towards `{-# LANGUAGE DependentTypes #-}`, but I feel that direction is where we want to go, if the goal is high-reliability (if not verified) programs.
I know about seL4. Nowhere have I said that there is never any need for any formal verification.
Huh, found this late. Thanks everybody for the kind words :)
&gt; I find this happens more among those who are in favor of dynamic typing Check out any thread about Go in /r/programming, and you'll see plenty of condescension from those who prefer a more expressive type system. (And Go isn't even dynamically typed.)
&gt; I might even risk contributing to a meme by saying "every sufficiently complicated Haskell library has an incomplete and informally-constructed Agda program in the blog posts expounding on it". Is this actually a thing? Because I like it a lot.
How do you run this program without a type system? let x = def in case x of Nothing -&gt; putStrLn "Oh." Just p -&gt; putStrLn ("Yay! " ++ p) As an author of the Fay compiler I would like to know, because Fay has no type checker, and I couldn't determine a way to resolve instance dictionaries without it, so Fay has no type-class support. :-) Also, this would also exclude type-classes which have no methods, presumably? Because they are a purely type-checking concept, but can be used for constraining functions and checking invariants. Like this: class Subset sub super instance (HasField field super,Subset sub super) =&gt; Subset (RecCons field typ sub) super instance Subset RecNil super Now my database update function ensures that the fields I update are a subset of the relation I'm updating: update :: (Subset s r,ShowLabels s, ToPrimExprs s) =&gt; Database -- ^ Database. -&gt; Table r -- ^ Entity. -&gt; (Rel r -&gt; Expr Bool) -- ^ Predicate. -&gt; (Rel r -&gt; Record s) -- ^ Updates. -&gt; IO () How can you express this without type-checking?
You might be interested in the "Testing Type Class Laws" paper by Johan Jeuring et al. http://paginas.fe.up.pt/~niadr/PUBLICATIONS/LIACC_publications_2011_12/pdf/OC26_Amaral_Haskell2012.pdf
You have to remember that while the notation is the same, the "expression" and the "value" side represent very different things in a running Haskell program. The former would be encoded using a code pointer plus a stack, while the latter refers to a closure on the heap. To make the representation change clear we'd essentially have to introduce distinct notations for "constructor expression" and "constructor value" terms. However, Launchbury et al chose not do this, so we follow their lead here.
Does `ghc-pkg list` show that you have a version of `primitive` installed? I don't think it would display the hash unless you had once installed it but somehow it has been removed or cannot be found but is still registered. It might be necessary to do `ghc-pkg unregister primitive` (and with it the things that depend on it, like `vector`, as `ghc-pkg` will tell you before doing anything.) If you are using the Haskell Platform, `primitive` and `quickcheck` will both have come with it, so I'm not sure what's going on then. `tf-random` is being required by a conditional in http://hackage.haskell.org/package/QuickCheck-2.7.6/QuickCheck.cabal ; it might be possible to evade this. 
No, the laws aren't specified because we have two conflicting sets of them, and it took us a while to notice this :-P
Well its more the case we can reason about our code because of the types. Laws are an abstraction barrier that is a bit leaky to the extent they're not enforced. But, thanks to parametricity and other tools, there are lots of things we can say about types _without_ recourse to typeclass laws :-)
On `MonadPlus` – doesn't the documentation mention the laws? mzero &gt;&gt;= f = mzero v &gt;&gt; mzero = mzero edit: more understandable, I can into english
Amazing post, thanks! You should get a bitcoin address for the donations.
As far as I'm aware Idris doesn't normally require proving type class laws either, even though it does have dependent types. Instead it has separate `Verified`* subclasses which do require them. This suggests that even having dependent types doesn't necessarily make it practical to do so. (Disclaimer: not an Idris programmer.)
&gt; force you to write around their syntactic reasoning, rather than our own intuition Unless I am interpreting this wrong, that is entirely the point. If our intuition were always perfect we'd never need a type system...
I'm just going to cross-post my moderately controversial comment from HN. It espouses a strong opinion, but that's entirely appropriate because I am arguing *for* strong opinions :). Suffice to say, though, I largely disagree with this post. The conversation around static typing *can* be improved, both broadly and in the little details, but largely not in the ways—and, more importantly, *not for the reasons*—espoused in the article. The specific improvements are an interesting topic unto themselves that I'll have to write (and think!) about some time later. &gt; This whole article is a reflection of a belief that I really don't like but is all too common these days. It's the idea that nothing is better than anything else, everything is a trade-off and strong positive are inherently wrong. &gt; I don't buy it. &gt; It's a tacit assumption that, somehow, everything is on the Pareto frontier. Profoundly implausible. In the background, it's basically a hybrid appeal to moderation (all extreme positions are wrong) and popularity (if so many people are doing something, it can't be wrong). Never mind the fact that popularity is almost meaningless, driven by fashion more than anything, and that what's "extreme" is arbitrary and largely driven by popularity itself... &gt; This is how we get "worse is better"; this is how we get standards that are compromises ideal for nobody but still widely adopted because they're standard or because that seems like the thing to do or because that's what everyone else is doing or because " engineering". &gt; The worst part is that this all goes unsaid, assumed correct by default. Just the fact that we're having the discussion the article wants, and not this meta-discussion, implicitly elevates its premise! &gt; Some things are better than others. A strong debate between strong positions is healthier than an artificial social force reverting everyone back to mean. &gt; Now, all this is not to say the usual message about types is perfect. It could definitely be improved in many different ways. But not by muzzling, weakening or qualifying: not by most of the argument's suggestions. &gt; Especially ironic is the rhetorical techniques the author uses to argue against rhetoric. The one I personally find the most grating—beyond the ambient implication that everything is equal that I've just written about—is establishing cheap credibility by aligning himself with the camp he's arguing against. "I like X, but..." Or, in this case, "I use OCaml, but..." &gt; Then again, my pointing this out is probably ironic in the same way. Or is it?
&gt; This suggests that even having dependent types doesn't necessarily make it practical to do so. Or, that there is a use for the law-less versions / non-law-abiding instances of the type classes. I can see using the Monoid typeclass when I meant Magma since the former is in base and the other isn't. In this case, I'm just really using the type class as ad-hoc overloading; i.e. I want one name that works across multiple types. I can also see using a Monoid that's not verified because it follows the laws for *observational* equality (or another equality-like relation) instead of *propositional* equality. If I want to do whole-program verification I might need to eventually convert this, but correctness of the program as a whole might not depend on the correctness of the Monoid instance. There are some things that dependent types makes *possible* but not necessarily *simple*.
&gt; "every sufficiently complicated Haskell library has an incomplete and informally-constructed Agda program in the blog posts expounding on it" It is a variant of [Greenspun's Tenth Rule](http://en.wikipedia.org/wiki/Greenspun's_tenth_rule). If any blog posts about the library speak of "equational reasoning", yeah there's probably an Agda/Idris program hiding in there.
&gt; Although, stripping screws is really annoying. Version control is your friend. ;)
It wouldn't have happened if the typeclass was properly specified to begin with. But it was a case of specification-by-implementation. 
Many libraries assume more laws than this. This page has more details: http://www.haskell.org/haskellwiki/MonadPlus 
Yep. Thanks!
It is easier to write JSON deserialization in a typed language, because you know what the types the members are supposed to be. This where dispatch helps you.
I'm pretty sure sociological studies generate some fairly hard evidence, or at least a lot more and harder than what the article provides.
Yay! I've been missing this every once in a while. 
While there's been no comprehensive studies, anectdotes from several verified projects indicates that verified software has 3-5x the upfront cost (prior to release, but may involve several changes to the original specification), but virtually eliminates maintenance costs. TCO for verified software should actually be lower.
Done: `1AkLcbVUYz1WJwedoj88mzR4vZ8eLFbida` (also added to the bottom of the page).
Donation made, thanks.
Excellent article! I specially liked the transition to `Getting` and `Setting`, it finally clicked for me! I do have a somewhat unrelated question, though. I don't mean to be facetious, if this is a sensitive topic, but English is not my first language: why did you refer to Edward Kmett with the pronoun *ne*? (you stated at the top of the page that it is indeed not a typo, but the link is way to hard to click on my mobile with my huge sausage fingers) *Edit:* How does exactly Patreon work? If I pledge X$, would you get that amount every article, say, when you make part 2 of the series?
Solution: power driver with torque control. (although it's not perfect)
An appropiate retort for that is "Well, I don't write those bugs, but it helps keep in check the code of other people I work with." Since people who bring the "that's just for stupid people" argument usually think of themselves as smarter than others (at least that argument gives the impression) this might be an angle that's closer to their point of view. The other alternative is of course educating them that "programmer" and "stupid person writing stupid bugs" are basically synonymous.
It doesn't worry me, because it doesn't really prevent me from writing ok software. Informal reasoning is mostly fine and always better than no reasoning, like in most other languages.
Also interested in this, especially in the context of [this](http://www.reddit.com/r/haskell/comments/2i4tmh/is_haskell_ever_going_to_feel_faster_to_write/ckz49lq).
I believe this is mostly because what is viewed as the 'good' use of typeclasses: Typeclasses are used to introduce overloading for *operations* not for data-structures. So when you have an abstract structure with a (hopefully small) set of operations over that structure, you define a typeclass for that structure with the member functions being overloaded over instances of that class. It's usually seen as bad form to introduce typeclasses as a way to form an API for a specific structure, since then you're not actually doing any overloading! 
Lenses solve mutating large object hierarchies really well.
`MonadPlus` is basically a holdover from the early days of Haskell. I think we got much more rigorous about this as we moved forward...
the interesting thing is i think it looks really good in your screenshot, and in the monochrome examples on the page, but not as good as bitstream vera sans mono when you add in colours. not sure why i feel that way.
I guess you don't necessarily have to refactor every single piece of code when you change the structure, if you just export Image type and operations on it but don't export its constructor.
What is it that you're changing? The Image type? You can change its implementation. Or provide an alternate one in and different module. It's not too bad. But it's also not good. That's why there is work on an SML like module system for Haskell called backpack. 
And I'm glad you've brought it to reddit for a chance at broader discussion. I didn't like to just throw out unfounded accusations of bad practice.
Thanks a lot for taking the time to answer. I wanted to leave the link for tomorrow, but I can't resist me some satire. &gt;:) I supposed it had something to do with gender neutral pronouns (or my browser wasn't rendering the ascender of the `h` properly), that's why I was somewhat hesitant to ask. It is the first time I have seen it done for a language reason, however! I have never thought of it that way. That's the good thing from Spanish, you can omit the pronouns completely most of the time (gender still shows when declining adjectives (not as bad as in German or Finnish, though)). Did you come up with this? It is certainly interesting, will give me something to think for the next couple of days. ^(in my opinion the most jarring thing about English is the spelling) P.S. I think you are *really* cool for publishing the article, and then answering a lowly Reddit comment from a worthless peon. P.P.S. &gt; Just wondering: does this begging-for-money look pathetic?. I actually appreciate it. Many times I have wanted to support the people writing the articles from which I mercilessly profit, without knowing how to do it. I pledged an amount in Patreon (first time using it, hope I didn't break it), it seemed more motivational/supportive than a one-off donation. ^(sorry it's a small amount, I'm a poor college student) P.P.P.S. everyone knows that sheep is the plural of a single shoop. 
Would it be helpful to explicate the expectation that `show`/`read` should be used for convenience and debugging? Furthermore, should general-purpose libraries implement those classes for their data types? Consider for example, `Identity`, which formerly didn't have such instances.
I have a related question for everyone. General advice is "type classes should only be small, and with well-defined laws." But what about `Vector`? It doesn't satisfy either category, but it needs type classes/families to allow its storage structure to change based on the type.
&gt; I think most Haskell developers are aware of the problem you mention here. Damn. I knew I was an outlier.
Yeah, I really would like Edward to submit `profunctors` and `contravariant` to the Haskell platform.
But we build abstractions which correspond to our intuitions. If what is intuitive and what is typeable are in tension, this is a real problem, not one that can just be resolved by saying "ditch intuition" or "ditch typing". This is where a gradual typing approach is useful: take the type system as far as it can go, but don't commit yourself to it; as the structure of your application crystallizes, reinforce it with statically checked assertions, but don't slow your dev cycle down elsewhere. I don't exactly endorse the gradual typing approach; my current research project is a language which is ubiquitously typed, but with a type system that stays out of your way as much as possible. But I sympathize with the contrast to Haskell-style typing.
God I wish.
My power driver has torque control, it's not just that, it's the smoothness and consistency of force applied. It might sound ridiculous, but if I wanted furniture that lasted a lifetime, I'd want it to have been built with quality wood and no power drivers.
There's possibly the `ClassLaws` package but I haven't used it http://hackage.haskell.org/package/ClassLaws And it's not that *nobody* worries about it: http://www.reddit.com/r/haskell/comments/2foggq/why_does_david_turner_say_type_classes_were_a_bad/
&gt;one line of code per programmer per day for that team numerous numerous studies have shown that that is an accurate number for a large project. shit it was even in Mythical Man Month.
Which editor do you want to use? You should be able to set the editor to the command that you type in bash to open a file in your text editor. For me, that's `:set editor subl` (for Sublime Text). Note that some text editors need you to do something to setup a shell command to open the editor. 
As far as I know, the origin of the "adults" bit is Python's "consenting adults" stance toward language-level enforcement of access modifiers. So Python basically lets you hint to a user of your code that you consider something to be private/internal/whatever, but doesn't let you outright *forbid* them from making their own choices about using it anyway. Which is a very different thing from what you seem to be complaining about.
You nailed it with: "nothing is better than anything else". It's a poisonous belief. It's a *frustrating* form of CS cultural relativism! 
&gt; My problem with such approach is that, after your code is finished, if you decide to change the used structure you will have to **refactor every single piece of code using it**. What are you thinking about when you say this?
This only handles `ccall` compatible functions at the moment, but its a good start and I'm very excited by the approach, which follows in the same line as the "Foreign Inline Code in Haskell" stuff from Manuel Chakravarty (https://speakerdeck.com/mchakravarty/foreign-inline-code-in-haskell-haskell-symposium-2014). I'm sure Max would welcome more contributors to build out `vtable` support, etc. 
Github page: https://github.com/zaxtax/hakaru Hackage: https://hackage.haskell.org/package/hakaru
I'm using Sublime. I got it set up and got my .ghci file set up as well. Anything else I might need as a fresh OSX programmer? Thanks for the help.
I wasn't familiar. I'll look into it. Thanks for the suggestion.
If only backpack were like the sml module system!
Something is getting you confused... There is absolutely no reason why Image cannot change its implementation without everyone who is using it having to change. More importantly, Image could very well be a nice abstraction in itself. For example, the Image library could define Image like this: {-# LANGUAGE RankNTypes #-} data Image = Image (forall a. EditableImageCanvas a =&gt; a -&gt; a) Here, Image is just a wrapper around "generalized rendering functions". Instead of rendering direclty, it constructs a function that modifies some arbitrary canvas. In this way, the implementation is abstracted within the Image library, but hidden from users. Or it could be record, containing the functions to manipulate which can be swapped out as desired, which is basically what Typeclasses desugar to anyways. And even if it is some kind of simplified Array of pixels, you wouldn't neccessarily know or have to deal with, since the datatype exports neither the structure or internal structure.
I see how you can read my post that way, but it's not really what I meant. Or, at least, I haven't put together some sort of Shelly-alike in Idris, yet. I might have some time to fiddle with this, inspired by work though; I do have a scripting task on my plate. (A small CI/build system to hold us until Jenkins is in Debian stable and I can get it set up.) What I mean is that while I take a lot of care in writing safe bash code, I find myself making very inconsistent oversights. I know many of the safe-shell-coding practices and I follow them like 90% of the time without thinking, and 98% of the time when I'm paying attention, but 1-3 times out of a hundred, I'll just forget to apply or overlook a potential application of one of them. With something like Idris, I can write a proposition and have Idris ensure that I have satisfied the proposition, like guaranteeing that the 1st argument in a 4-argument call to `[` doesn't start with '-'.
I regularly write code based heavily on [mono-traversible](http://hackage.haskell.org/package/mono-traversable) which is completely agnostic, for example, to which implementation of a dictionary it uses (HashMap vs Data.Map) or which sequence-like structure it uses (Data.Vector, Data.Vector.Unboxed, or Data.Sequence).
&gt; I don't exactly endorse the gradual typing approach Me neither. I really do appreciate Haskell's (or even Java's) static typing over Python's (or PHP's) lack of types. That said, I can see it both as a middle ground AND as sort of a RAD-like tool. You could start with whatever level of typing you are comfortable with at the time, and have them turned into contracts (run-time tests) at the borders between your team's code and libraries or the client's code. You could write propositions as types; types would be statically checked and a warning (or error) emitted when they can't be statically proven and are instead compiled into contracts. You'd write more types/contracts to be blamed for fewer faults. You'd engage the proof assistant to improve contract performance (turning run-time checks into compile-time checks) and be blamed for fewer performance issues. We'd need rules for deciding which side of the border to charge/blame for the run-time overhead of contract checking, but it could be a way toward incremental and viral dependent types. Sometimes it really is worth it to just test-and-blame in order to get the patch out with a minimum of programmer time, even if it has a large performance cost. Concrete blame numbers are also way managers can see technical debt (and plan to pay for it) without having to worry about "bugs that will never happen" (i.e. bugs detected by static analysis that haven't occurred in test or production, yet.) It could be really interesting to see something that a full M-L (or even HoTT) type system that also integrated gradual typing, contracts (with blame), some of the new profiling/performance (with blame, including of contracts) techniques. I suppose there's still some research to be done there, too.
&gt; His arguments is that static type systems necessarily force you to write around their syntactic reasoning, rather than our own intuition. I understand that, but I'd like a concrete example. Personally I don't know any examples where typing forces you to write in a way that is unhelpful.
&gt; Did you come up with this? “ne” was stolen from [here](http://genderneutralpronoun.wordpress.com/) – or were you asking about the “fitting the language to suit your needs/desires” thing? People are doing it all the time, except that they call it “slang” or “memes” or “business-speak” or “Simple English” or “politically correct English” or whatever... it just happens that, for most people, “making the language more logical” isn't a goal in itself (the same way as most programmers are interested in -producing code which solves a task-, and not in -making code more elegant for the sake of elegance-). I suspect you would find a lot of “language designers/engineers” among writers, tho (for instance, here's a [rant by Mark Twain][awful german] about German). [awful german]: http://en.wikisource.org/wiki/A_Tramp_Abroad/Appendix_D &gt; ^(in my opinion the most jarring thing about English is the spelling) Yeah, it's pretty twisted. I also miss Russian's morphology system – for example, in English “ask away” means “go ahead and ask”, but you can't form “tell away” or “open away” (and “google away” is questionable); however, in Russian you can often manipulate prefixes and suffixes (of which there are hundreds) to give a word pretty much any shade of meaning without having to change the root. [As a consequence, up to 30% of words used daily by an average Russian peasant are derivatives from just 4 “obscene roots”, since those roots can serve as wildcards in word construction.] &gt; Many times I have wanted to support the people writing the articles from which I mercilessly profit, without knowing how to do it. Well, people who don't have Bitcoin addresses and donation buttons all over their blogs usually don't need money that much (...I think?), so you can donate your time instead. E.g. help them write docs for their open-sourced libraries (if they have any) – writing documentation can be boring, but for someone who has written the code and -knows what every line and every function does- it's *much* more boring. &gt; ^(sorry it's a small amount, I'm a poor college student) ^(it's not a small amount for someone who eats ~4 cookies/day on average so) thanks a lot
Google Jurgen Schmidhuber.
&gt; I specially liked the transition to `Getting` and `Setting`, it finally clicked for me! I've just realised that I haven't stated clearly enough^(because I myself wasn't aware of this clearly *enough*) that type Getting r s a = (a -&gt; Const r a) -&gt; s -&gt; Const r s is the same as `(a -&gt; r) -&gt; s -&gt; r`. So, I've updated the text (and also fixed an accidental use of the old 2-parameter `Getting` in the recap, which hasn't been reported by anybody yet).
I think a lot of the "well-defined laws" people take an overly restricted view on how type classes should be used, based upon how they want to use them. If you think about it, `vector` only works because all the instances do in fact have well-defined behavior. Most of that specification is written in the haddocks, e.g. at http://hackage.haskell.org/package/vector-0.10.11.0/docs/Data-Vector-Generic-Mutable.html, for the methods of the `MVector` class (I don't know if that's a complete specification, but it may be). That sort of specification isn't what people mean when they talk about "well-defined laws" for type classes, but operationally it's perfectly valid. Anyone could implement a new instance, and if it followed the spec it would work with the framework. I think a lot of users reject such type classes in general because a complex spec like that is difficult to work with. With vector it's essentially an implementation detail so it doesn't typically escape to users, but that's not often the case with classes like this. As another example, consider classy lenses. What laws are associated with the derived classes? Simply that a method produces a valid lens on the correct field as derived via the given name mangling? It's not really that mathematical of a structure, but in practice it works pretty well. (I'm expecting ekmett to post some nice derivation in Hask now)
&gt; That is completely unacceptable in 2014 What? You are getting upset over something silly, I think.
I'm excited.
What do you mean by "maintanence" here? I'm happy with the idea that verified software, used in its intended environment for its intended purpose, will have a very low rate of latent defects manifesting as failure—becuase there are almost no latent defects. For the systems I usually work on "maintance" more often means "the requirement changed", or "the behaviour of this up (or down) stream system changed". In those cases, it would be interesting to know which costs lest—repeating the verification, or regression testing. Particularly if the regression tests are almost all fully automated. 
What about building up a larger polymorphic expression?
I believe that unless you are writing a library that you expect many other people to use, you should refrain from thinking that way. Go for the simple solution, you will eventually see if you need something more complex, and, when it's the case, it will be a lot less work to refactor everything than to have lived with an overengineered solution in the first place. If you need a stable API, there are other tricks, as can be seen in the other threads. There even are proposals to export lenses instead of data structures ;) When you push the OP's approach to the extreme, you end up with the "java business application developer" archetype, where every change comes with ten files : factory, business interface, business implementation, technical interface, technical implementation, a few GoF patterns. Development, testing and maintenance are now 10x more expensive. And the next week a new use case is found that will require the developer to break everything anyway.
&gt; I think a lot of users reject such type classes in general because a complex spec like that is difficult to work with. &gt; It's not really that mathematical of a structure, but in practice it works pretty well. But this is exactly what I'm wondering about. They work very well in practice, but people seem to be of the opinion that they're an anti-pattern. Especially with the vector case (specifically unboxed vectors), I'm not sure how else it would be implemented with type classes/families. The usual suggested alternative to typeclasses is records of functions, but that makes little sense for vectors.
&gt; It's the idea that nothing is better than anything else, everything is a trade-off […] I don't see that in the article. Rather, the article seems more to be saying that *if* static typing is superior that $WHATEVER *then* advocates should make that case honestly and transparently. &gt;[…] and strong positive are inherently wrong. Again, I think the point is not so much that strong positives are inherently wrong, but that claims of *unalloyed positives* are suspect. Are we really to believe that static typing (of the kind that haskell supports) is an *unalloyed* good? It has only benefits and no drawbacks, of any kind? Not even a little bit? And, you know, the further away you get from the researcher's office the more life, even in technology, is about tradeoffs and compromise. In industry the question is rarely “What is the one unique technically most superior solution? Let's do that!” It's more like “How much imperfection can you tolerate in the interests of meeting your timescale and budget?”. Just to be very clear: in a small number of cases the answer really is “I can tolerate zero imperfection”, and that's fine, bring on the big guns. But arguments in favour of $REALLY_GOOD_THING, if they are honest, need to recognise that different context require different tradeoffs—or else they come to resemble the preaching of a religious zealot. 
"pattern matching", nuff' said.
Shake to replace make and Bake to replace Jenkins. Shake and Bake?
Can't GHC Haskell call COM Apis through a v-table on windows? 
Your comments are quite topical, given the discussion about List functions in the prelude upgrading to Traversible. There are arguments on both sides of that debate. But I don't think people are suffering the bad consequences of concrete data types that you think they might. Classes are very heavyweight and lesser techniques usually suffice. For example, as others have said, two modules may export the same API without classes getting involved. In java, I see all classes having a corresponding interface and this dramatically obfuscates these programs (not that you're supposed to do that in Java, it's supposed to be role interfaces). Actually, talk of role interfaces reminds me: In the limit, small role-style interfaces become functions. So a really well factored Java program may resemble a Haskell program. This probably doesn't convince you. You're thinking (in Java terms) use List instead of ArrayList. For that particular scenario, Haskell is a bit limited in the sense that there is no comprehensive unified set of interfaces abstracted away from collections. We do have Foldable and Traversible, which cover many of the most important use cases. As I mentioned, these may be about to get a lot more common.
As an unhappy Jenkins user, I am looking forward to what this can turn into.
The pair of names is not a coincidence :) Bake doesn't rely on Shake in any way (it reuses the command line sugar from Shake, but none of it's dependency bits), so they are separate but complementary.
Oh, I didn't know anything about `Default` until now! If monomorphism restriction is enabled, then `x` would have a specialized type and the example could be implemented without dictionaries. If we don't have monomorphism restriction, I guess you would have to resort to instance dictionaries. I would imagine something like this: // instance Default (Maybe a) where // def = Nothing instDict_Default_Maybe_a = { def: undefined }; // x :: forall a . Default a =&gt; a var x = function(instDict) { return instDict.def; }; // x_0 :: Default (Maybe String) var x_0 = x(instDict_Default_Maybe_a); I don't really know what are the constraints of Fay's implementation. I'm assuming this isn't possible because of the lack of type-checking information? (Could you somehow siphon this from GHC's type-checker?)
&gt; var x_0 = x(instDict_Default_Maybe_a); How do you decide to pass `instDict_Default_Maybe_a` to x?
Can I use it just as one of the "checks" in an external code review system ? It seems it's designed to merge as soon as tests pass.
I guess there's no way to obtain that using GHC's API then?
&gt; And of course most of the comments in /r/haskell are about how their way is much better or the dynamic lot are worst than others. Really? Where? Post links to a few such comments.
The central Oven type provides the abstraction for running stuff, and one of the members is ovenUpdateState - see http://hackage.haskell.org/package/bake/docs/Development-Bake.html#v:ovenUpdateState. The default ovenGit does the merge in ovenUpdateState, but you can write a replacement to ovenGit or apply ovenGit then override just the ovenUpdateState method. That said, the idea is very much to merge lots of patches together to check them. That might not work with a code review system, where you presumably want to accept/reject each patch in isolation.
Could you share your negative opinions on Jenkins? I mean – what's wrong with it? I'm curious.
Note that it's a long way off being ready to take over standard Jenkins tasks yet - but hopefully one day.
This is called *union types*. It's a great idea, but unfortunately, nobody has yet figured out how to combine it (or, in general, subtyping and polymorphism) with type-inference in a nice way. The main issue is that type inference works well on *equations*, i.e. `'a = 'b`. For example, in Elm, records are structurally typed (it's technically not subtyping), and if you have an equation `{x : int | 'a} = {x : int | 'b}`, you can immediately conclude that `'a = 'b` and this is the unique, most general solution. On the other hand, if `|` denotes a type union, and you have `int | 'a = int | 'b`, there is no way to relate types `'a` and `'b`, since you could have `'a -&gt; bool` and `'b -&gt; bool`, or you could have `'a -&gt; int | bool` and `'b -&gt; bool`, and so on.
Stack traces everywhere? I got annoyed by Jenkins so I also wrote a CI server in Haskell for my own use. Honestly, writing Haskell code is much easier than learning how to configure Jenkins.
Have you tried buildbot? I really like it.
Personally, what I dislike most in Jenkins (and also Travis) is that it seems designed to build the latest state (in a branch) of repository, rather than building all commits. This is one thing that I liked about the design of [Bitten](http://bitten.edgewall.org/) (whose development has stalled). Luckily, Phabricator also supports the build-every-commit concept by default, which is quite useful for GHC development, as it reduces the amount of `git bisect` needed to pinpoint a breakage.
cool. Just in case people do not know my rather heavyweight way of foreign function interface to C++, I introduce fficxx here : http://ianwookim.org/fficxx http://github.com/wavewave/fficxx This fficxx is used for generating binding for ROOT (called HROOT: http://ianwookim.org/HROOT ). 
Note that Bake takes this concept to the extreme, it skips many commits, deliberately. Its designed for where you have a 100's of commits a day and 5 hours of testing to check something. I'd be much happier if I could get test every commit from Travis for my small-scale stuff.
Actually, having build (but not test) every commit should be feasible on top of Bake without much difficulty at all.
You should summarize what's missing to close the gap, so that others know how they can contribute
Great type names !
I question your ability to distinguish between type classes, and the word class from the imperative world.
 data ImageOps a = ImageOps { fillRect :: Color -&gt; Bounds -&gt; a -&gt; a } image :: ImageOps Image image = ImageOps { fillRect = ... } You can use records to define abstract interfaces that can be instantiated by multiple types. Then to program generically over the interface, you just program over polymorphic records: someAbstraction :: ImageOps a -&gt; ADerivedInterface a
The vast majority of maintenance programming is concerned with one of (a) crashes, (b) security issues, and (c) correcting program behavior to match existing requirements. This has been confirmed with multiple studies. In fact, if requirements change after release, most studies will consider it a separate project. I know our company considers it a new project and negotiates and bills it separately. Requirements changes before release are considered part of the up-front / non-maintenance costs.
I assume you mean that it tests all those 100 commits together. What happens if a test fails? How do I figure which of those 100 commits had the problem?
&gt; Are we really to believe that static typing (of the kind that haskell supports) is an unalloyed good? A static type system where all types are inferrable (H-M)? I'm pretty sure that's the closest to an unalloyed good I've seen in programming. Haskell brings some baggage with it though. I like laziness by default, but I'm still not as good at reasoning about it at I was about my C/C++/Java code. The record system is just barely there, although maybe that will get better soon with record fields basically becoming lenses. Modules are namespaces, not modules. Etc. Even type classes have their rough edges and it wouldn't be Haskell without them. An M-L type system or even just opting-in to some of the GHC extensions is another case entirely. I think it is, overall, a good, but it comes at a cost.
Bisection, presumably. Which also makes sense: If one commit fixes a previous commit you're probably not interested in seeing a report that the first one fails, as long as the last one works.
I originally had ovenPrepare named ovenPreheat, but I decided that was stretching it too far.
&gt; "Foreign Inline Code in Haskell" stuff from Manuel Chakravarty Read the paper and associated blog posts and saw the talk at ICFP. I'm still not sold on this idea, mainly because the right way to marshal `void **` even just `signed char *` is not something that can be derived from the C header file. Obj-C and C++ have the same issue. Last time I checked the project GitHub, they still hadn't addressed "out" / "in/out" marshaling. Still, this does seem to be an improvement on the other way I'd seen C++ called from Haskell. (Compile the C++, dump the symbol table to get the mangled name, add to Haskell as if it were a foreign C function.) Never let the perfect be the enemy of the good; kudos!
Each "patch" in Bake is typically a set of commits. Usually a "patch" corresponds to typing "git push". So when there is a failure it rejects one patch, it doesn't chop up inside a push on the individual commits, but it does point at which push failed. Of course, it's also parameterisable, so you could make each patch a commit if you wanted and it would identify the offending one.
Really? And if your intuition says that you're constructing a list (well, ordered collection) of functions which, despite having arbitrary domains and codomains, are guaranteed to chain correctly, what do you do? Because in Haskell, you had three options: * Restructure your code so that you're not doing it (either by dropping down to some kind of dynamic type or by turning a linked list inside out and avoiding more efficient structures. * Wait for Oleg to write a research paper on it. * Come up with an efficient representation of free categories that satisfies the type checker on your own. The point isn't that I'm good; it's that the type checker really does get in my way sometimes, and it's not the one writing code.
Aha! I thought you meant that if Alice wants to merge feature-foo and Bob wants to merge bugfix-bar, then the tests would be run for master + feature-foo + bugfix-bar; so Alice and Bob wouldn't know whose code caused the tests to fail.
The main thing is using Bake for real, which I'm sure will show up lots of things, and also teach us what a standard Bake configuration will look like. There are a few really obvious places, in particular: * The stdout of every process ever run is stored in memory on the server as a String (search for aStdout). That should be persisted to a file and the aStdout on the server changed to point at the file. * The links you can click on from the main page give the information dumped with Show. That needs improving. Both those things are maybe half an hour to fix, so certainly not showstoppers.
I doubt anyone will see this now, but I'd like to put in a good word for qualified imports. They're common in Python, mandatory in Go, and they make import management much less fussy while still making the provenance obvious.
&gt; A static type system where all types are inferrable (H-M)? H-M only handles equational types, so you have to sacrifice subtyping (so no Java or OO in general). Granted, there are systems which handle subtyping without losing inference (e.g. the original Aiken &amp; Wimmers paper contains one), but you get into other problems. I don't buy that SML-NJ is an unalloyed good.
So the tests run for master + feature-foo + bugfix-bar, and if all is well, they both get merged. If one fails, it figures out which one it is. Looking at https://raw.githubusercontent.com/ndmitchell/bake/master/screenshot-part.png the top three patches were submitted almost concurrently. Patch a68 did one test before the next came in, then it tested all three together, until it found a failing test, then rejected one patch and continued with the other two. When run to completion those two both succeed.
How did you measure it?
&gt; It's a tacit assumption that, somehow, everything is on the Pareto frontier. Profoundly implausible I may be misunderstanding the concept of a Pareto frontier, but to me the opposite claim, namely that there is anything in wide use that is not on a Pareto frontier, is the implausible one. My reasoning is as follows: * The Pareto frontier is a set of feasible solutions which cannot be improved in one dimension without sacrificing other dimensions. * A naive greedy search will hit a point on the frontier because it will improve certain dimensions until it can't anymore and then stay in its local optimum. * Social adoption is at least as good as a naive greedy search.
&gt; H-M only handles equational types, so you have to sacrifice subtyping I see it as an unalloyed good above (e.g.) C, which also doesn't have subtyping. When mixing subtyping with inference, you do get into problems, but I feel those are problems with subtyping, in particular subtyping where there is some Top or Bottom type as either can turn what should be inference/checking failures into a result that violates the principle of least surprise. We see such odd behavior in Scala in practice -- Option[Any] and Option[Null] get inferred too often and (at least) push around compile time errors that makes them harder to fix correctly.
Perhaps you'd like to [compare performance](http://benchmarksgame.alioth.debian.org/u64q/benchmark.php?test=all&amp;lang=ghc&amp;lang2=python3&amp;data=u64q) on real-ish problems with tuned implementations of specific algorithms. Your test is probably one of the least meaningful comparisons of performance I have ever seen.
Thank you this helped clear the difference up for me. 
Did not take any benchmarks just running it from console the difference was easily perceived, but thanks Ikcelaks answered my question. 
&gt; I'm still not sold on this idea, mainly because the right way to marshal void ** even just signed char * is not something that can be derived from the C header file. Eh? In /u/chak 's library, you specify the marshalling you want to use along with the inline code. It's not deriving anything from headers.
If you run sum(range(10**8+1) vs. foldl' (+) 0 [1..10\^8] they seem about the same. I think ghci is just slow at printing the result, because it interleaves evaluation and printing, and to display the result it uses show, which produces a String, which is also slow. In general it is not fair to compare the speed of a primarily interpreted language against ghci, because interpreted performance is not a priority for Haskell.
Link to tutorial: http://learnyouahaskell.com/starting-out My background in programming is very limited and I would barely proclaim myself a coder. The majority of my experience has been collegiate and Python for offensive security purposes. In no way was my mention of the performance variations a benchmark. The question should have been "What is the difference between haskell's [x..x] and python's range()?" Albeit given my vague question gelisam &amp; Ikcelaks were able to figure out what I meant and answered my question despite the poor phrasing. 
Thank you for this clarification there is still a vast majority of pieces that I am missing when it comes to actual programming principles. 
I have no idea what linguistic means, but (2) there are appropriate uses for typeclasses, but what you've applied here is a generally powerful solution where the principle price will be having to wire your own "dictionary passing style" which may very well be a good thing and (3) since this is more or less a manual dictionary-passing transform you can see it as being one "desugaring" step applied by hand that the compiler would be doing anyway. So, it's a good approach when used judiciously and it probably won't suffer any performance penalties above a typeclass version since the compiler will just use inference to convert the typeclass versions into something not dissimilar from this. Please don't entirely eschew typeclasses if they're a good fit for a particular problem, however. They do make Haskell what it is an afford considerable power.
GHCi writes each character of output individually to the terminal, which accounts for the slowdown. This is useful in a language where String is a lazy data type: you get to see the first n characters of the output without having to wait for character (n+1) to be computed. Python is strict and range(100000) is a list, so what happens in Python is the entire list is computed in memory, and then the entire list is printed using buffered IO. There's no way to print a list as it is being computed in a strict language (and in Python, you could even have a list of mutable objects where the computation of later objects modifies earlier objects), so Python doesn't try. If you run `ghc -e 'print [1..100000]' &gt;/dev/null` (or add a 0 or two to get a more accurate measurement) you'll find that it is close to the speed of the Python version, and with compiled code the Haskell version becomes slightly faster than Python.
Why are you avoiding typeclasses for interfaces?
Why does your Map only allow Ints as keys? Why does it only allow Ints as values? This really limits it's use cases. Instead, you could have a Map parameterized by a key and value. data Map k v = Map {get :: k -&gt; v, set :: k -&gt; v -&gt; Map k v} What if you want your Map to implement more then one interface, and you want to be able to use them in whatever order you want? What if, say, you also want to be able to get the length? Maybe, you can separate the data, and the interface used to access it. newtype ListMap v = ListMap [v] data Map m k v = Map { get :: k -&gt; m -&gt; v, set :: k -&gt; v -&gt; m -&gt; m } mapLM = Map { get = (\k (ListMap m) -&gt; m !! k) , set = (\k v (ListMap m) -&gt; let (before, after) = splitAt k m in ListMap $ before ++ v : tail after) } Now, mapLM descibes an interface with which we can modify ListMaps, which we can pass in to any function that needs to map over it. So, a function with the `Map k v m -&gt; m -&gt; m` could take as a parameter ANY datatype, so long as you also pass in a way to map over it, and it can be defined purely in terms of those mapping functions. Only at this point, we basically have typeclasses. What I described is basically the same as class Map m k where get :: k -&gt; m v -&gt; v set :: k -&gt; v -&gt; m v -&gt; m v instance Map ListMap Int where get k (ListMap m) = m !! k set k v (ListMap m) = let (before, after) = splitAt k m in ListMap $ before ++ v : tail after ... and now the compiler will pass around the record containing the interface for you to any function that needs it. ------ Side issue, but worth mentioning: Getting and setting values from lists is a really ineffective way of dealing with lists. Lists should generally be used instead as iterators in other languages, a way to stream data from one place to another. Generally, the focus to think about isn't "what is the thing" but "what do I do with it". The reason collections are useful in imperative/OO languages is that they encapsulate stateful manipulation. In functional programming, your primary focus is gluing together actions into transformations over state. Thats why all the main typeclass abstractions that people have come up with are things like Functor (mapping), Applicative (parsing), Monads (sequential actions), Folds (combining), Traversals (collection-driven actions), and so on. Its a different approach to the abstraction. ----- You may also be interested in in lenses. They are like a way more generalized version of getters and setters, with a lot more flexibility, and can be chained easily to fetch or modify more deeply nested values. However, they do rely heavily on typeclasses. EDIT: I made about a million mistakes (not sure how that was even possible), which I found and fixed. Its what I get for haskelling without compiling. 
It seems like the "performance" issue I was noticing was strictly in relation to GHCI and not actually Haskell thanks for the edification.
In general I think this is a reasonable thing to want to do.
Maybe this is my inexperience showing, but since when can you do this sort of dependent typing in Haskell? I had thought that it's typing system was strictly less powerful than dependent typing, with the tradeoff that inference was now possible. 
For me `Read` is just for primitives like `Int`, `Bool` or `Double`. For complex/composed/compound types `Read` is more a hack than a sane solution. Other techniques should be prefered in any way to parse these structures, e.g. JSON with aeson mentioned above. And in my opinion the other solutions are way easier to utilize.
That's promising, but I can see no way to deserialize that way. I'm planning to use cereal for serialization, so something like this could do: data Characterizer = forall p . Serialize p =&gt; C { matchingScore :: Loc -&gt; Float , payload :: p } and making `Characterizer` an instance of `Serialize`, but that can't work because of `matchingScore`. There must be a way to encode characterizers in the type system, its matching function that is, in order for this to work because we can't serialize the function. 
You don't have to serialize the function—just let the `payload` be the serialization exactly. It'll be deferred until needed by laziness. In some sense you just need to "preserialize"
Modern versions of Python have the `range` function return a generator. These are in some ways similar to linked lists. They could be slower, but they don't allocate an entire list when you just want to consume one element at a time, once.
Ah, so it would be possible to sneak in together some patches where master+foo+bar work but master+foo or master+bar fail? I don't see how to do any better without unrealistically strong assumptions about independence of patches.
It is always going to appear happen more in the group you are not associated with due to bias. You are more sensitive to attacks on things you agree with then things you disagree with.
With serialization I mean the whole store-it-in-a-file-and-load-it-from-another-process thing, e.g. storing a signature file some time before a new version of the executable is published, which I can the use to compute scores and find the most similar function. How could I preserve `matchingScore` across processes? I think I'd need `payload` and a function `fromPayload :: p -&gt; Characterizer` for that and that's pretty much what I got in the OP. Edit: Just to clarify, from the type of `payload` we cannot infer the `matchingScore` to use... Edit2: Or maybe we can somehow if every `payload` is a `newtype`...
What you think you say: "Static types help prevent these classes of errors." What they think you said: "Your bad choice in language means you write buggy code, and a shit programmer." What they think they say: "The code I write in this language is quite fine, thank you!" What you think they say: "You are a moron for needing a compiler to catch bugs like that!"
If you care about reasoning informally about code, I think it's worth it to know what a completely formal proof looks like, even or perhaps especially if it's clearly not worth the cost. Then you know if the stuff you're thinking about when you're just thinking about code would be enough if you filled in all the little details, or if there are all kinds of crazy preconditions you never really think about, or more realistically if there are some modest conditions you don't mind assuming (like nobody messing around too badly with `unsafeCoerce`). Whether or not you are talking about languages that even have a static type system I would expect easier formal reasoning to correspond to greater reliability of informal reasoning, even if full formal reasoning isn't practical in any of the languages. Is "well affords informal reasoning about code" a very meritorious characteristic? (whether or not you buy my argument that it correlates with affording formal reasoning).
&gt; linguistic In context, I think it means "idiomatic for this language".
While I'm sure there at least a grain of truth to this, I also think there's quite a bit of Dunning-Kruger mixed in this whole debate -- in that most programmers are introduced to dynamically type checked languages first and regrettable probably never get to experience what a powerful type system can do. (Where I'd count ML and Haskell as "sufficiently powerful to get the point across". Scala is almost there, but inference is so bad that it's not really good enough in practice.) Being novices they may end up overestimating their own skill and suffering from biases which tend towards the "but I almost never make mistakes like that!" rebuttals, when in fact they *do* often make mistakes like that, but their biases just cause them to tend to underestimate how much time they *actually* spend doing the "fast cycle time" thing between a browser and editor. (Assuming e.g. JS.) That's not to say many non-novices don't use dynamically typed languages. Examples would include Rich Hickey who is clearly an expert programmer. Given the sheer number of programmers such people are probably just outliers and one can't conclude anything about static vs. dynamic type checking from their existence... Or maybe that's just my bias speaking ;).
Yep, it would. Unless you have the resources to run every test on every patch, that will always be the case. Most CI systems assume you do have that many resources, but Bake assumes you don't (for certain realistic examples, you often have orders of magnitude less resources).
I've tried something similar. I used the type of an implicit parameter and phantom types to select which type class instance to use. For example, you could write a `mappend'` that works on many different `Monoid` implementations. It has a type signature something like: ``` mappend' :: ( ?param :: proxy a , Monoid (f a) , Coerce (f a) (f b) ) =&gt; f b -&gt; f b -&gt; f b ``` I found it really syntactically awkward in practice to have to include the `let ?param = Proxy :: Proxy SomeType` everytime i wanted to use these functions even on the default instance. I really wish implicit parameters could be defined globally. This would let you specify a "default" instance but override it with a local let binding.
That's a bizarre assertion. Every time you encapulate a thing (OOP style), you're (informally) using the type system to prove properties of your programs. It's simply unavoidable.
Where *is* your data? Have you published it anywhere? (I'm not asking to be snarky, it could be valuable for researchers in this area.)
How to add `merge :: Map -&gt; Map -&gt; Map` method to the interface?
Installation (which you appear to be past, but just in case): https://github.com/bitemyapp/learnhaskell#mac-os-x Development environment: https://github.com/bitemyapp/learnhaskell#development-environment
Nice one :) Unfortunately, the quote I wrote is verbatim !
And is this equivalent in power to a full Dependent Type system like Coq or Agda? 
Also, benchmarking a compiled language in an interpreter isn't exactly useful or representative.
Deserialization is always a partial process and will be something like `ByteString -&gt; Maybe Characterizer`where the serialization must know the desired data to construct the score function.
Nobody's keeping you from using qualified imports (if you want them). The not so nice thing would be to force everyone to use them instead of giving them a choice to opt-in.
The same workflow has been used for [the gdiff library](https://hackage.haskell.org/package/gdiff).
IIRC the singleton-types and full-spectrum dependent types approaches are very similar in power. Of course having the same language at disposal at both term and type levels makes things much less verbose.
It is mostly a matter of taste, but I strongly believe you shouldn't ever add language feature that can be replaced equally well by something existing. I like the minimal approach of FP, that's why I'm here after all. Typeclasses add complexity to the language, are something else to learn, other functional languages do perfectly well without it, I'm not convinced of its benefits and I'm convinced of [its downsides](http://www.haskellforall.com/2012/05/scrap-your-type-classes.html). Haskell code, as it is, feels like a mixture of type classes and lambda calculus. Without type classes it feels like a single thing; much more homogeneous, much easier to read. So I'm out of type class on my code until I figure out I absolutely need them for something. Lambda calculus with sugars (Haskell) is perfectly fine without it. At least for me, for now.
I think someone mentioned python's transducers were a good example, but I doubt know anything about that. If you want to go to the realm of dependent types, I can give you plenty of examples of struggling to say what I mean. I give DTP the benefit of the doubt (I.e. I'm a dummy) but I can see how others could get frustrated.
The problem I see is this: There might be two characterizers, both having an `Int` as payload (say, one might compute a hash, the other count all instructions of the function). Thus, the types and the serialized data can't uniquely determine the scoring function. The `ByteString` alone doesn't suffice to reconstruct a `[Characterizer]` I assume. We need to incorporate the specific order and matching functions to be able to so. Maybe I just don't see the obvious solution, so given 2 characterizers, how would I serialize (probably `BS.concatMap serialize`) and more importantly afterwards deserialize them (e.g. `foo :: ByteString-&gt; Maybe [Characterizer]`)? How would I reconstruct `matchingScore` which wasn't serialized?
no - you run into limitations (eg no GADT promotion, type families need to be saturated, limited unifier doesn't deal with injectivity); you cannot write everything doable in Agda, yet. see the ICFP 2014 keynote linked in the article for an overview of where it's at and where it's going http://www.youtube.com/watch?v=rhWMhTjQzsU . also HIW 2014 "Dependent Haskell" talk http://www.youtube.com/watch?v=O805YjOsQjI , and, for lifting some of the restrictions right now, eg the saturatedness of type functions, the haskell syposium 2014 talk on the new additions to the singletons library http://www.youtube.com/watch?v=J47OTYArG08 - where they enable promoting functions including unsaturated ones to type level. further work on the matter linked there - https://ghc.haskell.org/trac/ghc/wiki/DependentHaskell some more: https://ghc.haskell.org/trac/ghc/wiki/InjectiveTypeFamilies https://ghc.haskell.org/trac/ghc/wiki/ExplicitTypeApplication EDIT: oh, and not sure where this is linked, but making the TypeNats solver a proper SMT solver to support reasoning on the type level is mentioned in https://ghc.haskell.org/trac/ghc/wiki/Status/May14 . 
&gt; And is this equivalent in power to a full Dependent Type system like Coq or Agda? In 7.4? No. At the very least, functions can't be lifted to the type level. ClosedTypeFamilies brought us closer. But, DataKinds still can't lift GADTs and I don't know if that's going to change soon. I think there's also be some sticky bits if you wanted to write a proposition/type spanned more than two universes (terms, types, kinds OR types, kinds, sorts, e.g.). There's still a lack of "expressivity", because the type and term levels are distinct so you have to use explicit calls to move values from one to the other, IIRC. Lifting functions to the type level with the aid of ClosedTypeFamilies is particularly troublesome, but I believe there's a TH library for it now. Also, Idris / Agda / Coq have things like a totality checker and proof assistant, which is decidedly lacking when doing dependently-typed Haskell.
&gt; I really wish implicit parameters could be defined globally. This would let you specify a "default" instance but override it with a local let binding. I think that would be a good addition to the behavior of implicit parameters. We should avoid making [implicit resolution too complex](http://www.scala-lang.org/files/archive/spec/2.11/07-implicit-parameters-and-views.html#implicit-parameters) though, as it is not good to require an IDE to figure out what implicit is in use. Searching for the first lexical scope with a type-compatible implicit (leading '?') binding and choosing the unique binding from that scope seems like a rule I could apply visually, most of the time. If not unique, compiler error.
Wow, I won't debate this :-) Maybe I mixed it up with `ConstraintKinds`. EDIT: And according to http://blog.omega-prime.co.uk/?p=127 that was in 7.4 too.
&gt; It is mostly a matter of taste, but I strongly believe you shouldn't ever add language feature that can be replaced equally well by something existing. This is not so simple as it might seem. For instance you are using typeclasses if you use: * numeric literals (with OverloadedStrings/OverloadedList string and list literals) * case statements with numeric literals. Additionally do you choose to avoid (,) since you can do the same thing with `data`? Do you use do-notation? Many ways to achieve the same thing. I don't this means is eschew one over another entirely. I think people do not like typeclasses because they are unable to plan on how to use them, because instance selection is poorly understood. IMO Gabe's blog is quoted too much to support typeclasses are bad. Typeclasses are a integral part of the language, have been used in many capacities successfully.
Yes I am indeed against the treatment of numeric literals, it is one of the things I hate most about the language, so many inferences lost. (,) is a different case, it is just a data structure. It is not part of the language, other than the syntax sugar. Same for `do notation`. It is just a syntax sugar, and you can easily see what will be the result of unsugaring. It isn't nearly as profound as the changes caused by type classes, which are almost a language on its own. The point is: it is just a matter of taste, after all, isn't it? There aren't many people who'd agree with me on this. But that is how I do it.
You mean having the scoring functions around before serialization and then plugging in the payload which we can deserialize due to the type information? I'm not sure if that's what you are getting at, but I got a working solution now utilizing a type class: class Serialize c =&gt; Characteristic c where characterize :: Location -&gt; c score :: Location -&gt; c -&gt; Float newtype Hash = MkHash Int deriving (Show) newtype Counter = MkCounter Int deriving (Show) instance Serialize Hash where get = MkHash `fmap` get put (MkHash i) = put i instance Serialize Counter where get = MkCounter `fmap` get put (MkCounter i) = put i instance Characteristic Hash where characterize _ = MkHash 123 score _ _ = 0 instance Characteristic Counter where characterize _ = MkCounter 5 score _ _ = 1 loc :: Location loc = undefined fingerprint :: (Hash, Counter) fingerprint = (characterize loc :: Hash, characterize loc :: Counter) bytes :: BS.ByteString bytes = runPut (put fingerprint) fingerprint2 :: (Hash, Counter) fingerprint2 = case runGet get bytes of Right fp -&gt; fp Left e -&gt; error e totalScore :: Float totalScore = (score loc . fst) fingerprint2 + (score loc . snd) fingerprint2 I should replace the `(Hash, Counter)` pairs with `HList` or another viable alternative though... 
&gt; One of the Pros mentioned was "friendly community" that has been at least challenged already, thanks. Wait, what? Challenged how?
I think you'd like Agda's instance arguments
`PolyKinds` in 7.4 was just considered a "preview" or something of that sort, and not recommended for use. (And I did in fact run into bugs with it.) It also didn't have explicitly annotated kind variables.
... thank you for proving my point.
That's a massively unfair characterization.
Are you being sarcastic?
I interpreted it as pointing out that the sentence "your test is probably one of the least meaningful comparisons of performance I have ever seen" could have been phrased in a friendlier way. That being said, I think it's clear that the sentence was never meant to be offensive; if it was, "least meaningful" would probably have been replaced by something more, er, flavourful :)
That problem has nothing to do with the type checker. You get the same problem with monadic effects in an untyped language. It's just trickier to see the solution with more types in the way :-)
&gt; Wait for Oleg to write a research paper on it. "But why can't we just [etc.]?" "Well, it turns out that's something that can only be done in O(Oleg) time."
Hi there. I'll be installing the umm-hip image type in nfdata tomorrow as well as fixing up a few other odds and ends. Although it will still probably not do very well in the benchmark. It's more of an educational tool than an industrial strength image processing library. There were plans to build it out more . . . But I got lazy :(
From the original post: &gt; ...we still end up with needing to use module qualified names. This isn't really an acceptable way to program. I have not seen another language where this extra line noise is considered good style. I was surprised to find this, since qualified imports are common and well-accepted in other languages. There's definitely some advantages to having things unqualified, but I don't think the qualified version is as unworkable as the article suggests.
To be perfectly explicit, I think that using type classes that form nice mathematical structures tends to make users think that's the only way type classes should be used, which is IMHO incorrect. I sometimes think that current prevailing attitudes towards type classes are really just a shorthand way of saying "type classes aren't appropriate for your use case", where that use case is typically trying to implement an OO-idiomatic design. But that's probably just me being really cynical.
We wrote this as an alternative to the scheme library used in this course: http://www.cs.unm.edu/~williams/cs522f13.html
The CV library I have forked on github should also be usable.
Sadly, adding a solver for nats just exacerbates the problem, namely that for some reason Haskell's type-level naturals do not come endowed with an induction principle.
The Bullet Physics binding was done by generating a C wrapper from the C++ headers. This works nicely across all the platforms and doesn’t require any ugly hack in the build process.
&gt; How long did the manual porting take? Two days (25th-26th September) - see git logs :-)
http://xkcd.com/927/ Especially as there seems to be a "official" way to generate opencv bindings that is used for the python and java bindings.
[Image](http://imgs.xkcd.com/comics/standards.png) **Title:** Standards **Title-text:** Fortunately, the charging one has been solved now that we've all standardized on mini-USB. Or is it micro-USB? Shit. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=927#Explanation) **Stats:** This comic has been referenced 888 times, representing 2.4396% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cl4jf4w)
Here's a class pulled out of the Pharo Smalltalk image: GenericUrl subclass #MailtoUrl instanceVariableNames:'' classVariableNames:'' poolDictionaries:'' category: 'Network-Url' it has two instance methods: MailtoUrl»activate "Activate a Celeste window for the receiver" MailSender sendMessage: (MailMessage from: self composeText) and MailtoUrl»composeText "Answer the template for a new message." ^ String streamContents: [:str | str nextPutAll: 'From:'. str nextPutAll: MailSender userName; cr. str nextPutAll: 'To:'. str nextPutAll: locator as String; cr. str nextPutAll: 'Subject:'; cr. str cr]. and one class method MailtoUrl class»schemeName ^ 'mailto' I chose this class because it is small, and because it is reasonably obvious what it is meant to do. It's not obvious (to me, anyway) what it proves. If it does prove anything, it's that *if* you had an instance of such a class and *if* you sent it ````activate```` (which might never happen) then some system–wide ````MailSender```` and ````MailMessage```` classes *if they exist* (they might not) will receive messages ````sendMessage:```` and ````from:```` respectively (and so on). What happens when that happens is anyone's guess, unless we have and go read the code of ````MailSender»sendMessage:```` and ````MailMessage»from:````, and the rest, (````String»nextPutAll:```` and so on) *at runtime*. If this were Java then we would know a bit more. We'd know, from the compiler, that classes with those names existed on the classpath for the compilation, and that the methods in those classes existed and took the right number and type of arguments, but nothing more. And we would have proved nothing about runtime except that *if* classes with those names turned out to be on the classpath of the JVM at runtime and *if* those particular classes have those methods defined then they'd get called with the right number and type of arguments—and nothing more. What has been “proved”, however “(informally)”? *edit: grammar, presentation*
Thanks - by one of the 8 guys :P
The phase Shake and Bake originally comes from http://en.wikipedia.org/wiki/Shake_'n_Bake, but it's now a fairly well known English phrase, but not particularly used to mean anything (I knew it was a food, but had no idea which until I just visited Wikipedia)
While it's true that JuicyPixels doesn't do any image processing per se, everything in this article but `mean` could be done with a quick call to pixelMap.
Well the only two operations it benchmarked are threshold and mean...
Friday creator here. The results you got for the blur are very strange, this operation should be way faster. Could you send me your original code, compilator command and test image? Thanks! 
To go further, lists in haskell tend to act a lot like iterators, but with the ability to peek ahead or push back items if you want to. So [x..y] acts more like xrange(x,y).
the th library is [singletons](https://hackage.haskell.org/package/singletons) btw.
yes, that's really unfortunate. many libraries roll their own `Nat`s because of it and can't use type literals that way.
Hey, you just need to sign in (it's free) to access the video. 
Even when programming in Python, I prefer to `import * from Foobar` but Python makes this harder to the point forcing me to use qualified imports, as symbol clashes are reported eagerly rather than when actual ambiguities arise (as it's done in Haskell) Moreover, while qualified imports in Haskell may be tolerable to some degree (but I'd rather avoid them), they become rather ugly/confusing when you need to disambiguate infix operators.
Coq can export to Haskell, but it's been labelled as experimental for some time now so I doubt it's too high on their priority list. 
That doesn't seem to be good advice, as like e.g. Erik Meijers goes to great length to explain in his C9 Haskell lectures, using direct recursion makes it significantly harder to prove termination or other properties about an algorithm, than expressing it in terms of basic high-level `Foldable`/`Traversable` combinators such as `foldr` et al. It's a bit like telling ppl to use Gotos (which is kinda Haskell's goto to some extent) instead of using high-level control-structures such as for/while/until iterators.
and that's not so nice.
Any indication of when the rest of the videos will be available?
Well, the `coerce` function is for users. If `C` is a newtype constructor you should no longer write `fmap C`, but instead `coerce`. It's not zero cost for the user, it's only runtime zero cost. 
Nothing wrong with laziness. Just do it when its time to do it. 
It's a shame their password reset button doesn't work :-/
Ok, so there is the code [running](https://github.com/phischu/haskell-image-processing-benchmark/blob/master/src/Main.hs) the benchmark, the [friday-specific](https://github.com/phischu/haskell-image-processing-benchmark/blob/master/src/HaskellImageProcessingBenchmark/Friday.hs) code, the [image](https://github.com/phischu/haskell-image-processing-benchmark/blob/master/koblenz.png) and the compiler flags in the [cabal file](https://github.com/phischu/haskell-image-processing-benchmark/blob/master/haskell-image-processing-benchmark.cabal). Thank you for looking into this, the results look strange for me as well and I would love to know where I have made a mistake.
One of my goals was to use only libraries from hackage, to make the whole thing reproducable. I could drop that goal though.
I've been practicing this work flow for some time now. For certain kinds of problems (e.g. data structures with some invariants to maintain), it works great. I would even skip Agda if haskell-mode gained some of agda-mode's features. 
That technique does allow for some [interesting approaches](http://www.gamedev.net/page/resources/_/technical/game-programming/haskell-game-object-design-or-how-functions-can-get-you-apples-r3204) (and more [interesting approaches](http://hackage.haskell.org/package/parsec-3.0.0/docs/Text-Parsec-Token.html#v:TokenParser)) to code reuse and will protect you from changes of implementation. However, unless you need the data type to act as an interface to multiple implementations that you will be using _together_, you may be better off using separate modules for the separate types so that you do not take the performance hit from not being able to exploit the internal structure of the types when operating on them. For example, if you had both your `ListMap` implementation and also a `VectorMap` implementation that you needed to be able to compine via the common `Map` interface, then you would be stuck with this approach and also with the performance penalty of having to combine `Map`s only using `get` and `set`, instead of being able to operate more efficiently on the underlying data structure. If you do not need multiple implementations to work together frequently, you can take the more performant and easier-to-read approach taken by the [text package](http://hackage.haskell.org/package/text-1.1.1.3), which provides lazy and strict packed strings via different modules and provides functions for converting between them when necessary. If you only need one implementation at a time, but that implementation may change, then simply using modules to hide the implementation has the best power-to-weight ratio among the options. 
This article would stir so much shit on /r/programming that it's probably irresponsible to cross post it there.
Of course Haskell programmers aren't second-rate! But there might be some truth in the fact that people who focus too much on the programming language (which mostly is a rather low-level tool), might not be the best overal engineers. Building proper software is much more than picking the nicest language and knowing all the ins and outs. Engineering is all about difficult trade-offs and design choices, for which a broad mind is very useful.
Doesn't anything that lets people feel superior to others stir shit on /r/programming ? 
Hi, Thank you very much for this inovative piece of code. Traditional UI libraries are a PITA to use at the moment, and I know what I'm talking about. Hope 3penny will turn into something stable and complete as soon as possible.
Today and tomorrow. Just released Bryan's talk - https://skillsmatter.com/skillscasts/5466-bryan-o-sullivan
Sorry!
Send me an email at theo dot england at skillsmatter.com if you still can't get access. Theo
The main issue I have with "right tool for the job" is that it's vague enough that most people can apply it to most things. "Yeah, sure C++ is the right tool for writing this compiler. I can structure my AST into objects, which I can't do cleanly in C"
Simon Peyton Jones came and gave a guest lecture at my uni while I was there. It was one of the most interesting lectures I had there. He went through so many ideas in that short space of time though, I had to completely focus to keep up with his thought process.
It's ok, we have mailinator.com for situations like this.
&gt; It's not deriving anything from headers. While this is is true[1], it doesn't reduce the problem, for me. Maybe it's just lack of documentation. But, [the design](https://github.com/mchakravarty/language-c-inline/wiki/Design) seems like automatic marshaling is only suitable for "in" arguments, and I've yet to see an example of how to do manual marshalling. In fact, looking at [the C marshalling code](https://github.com/mchakravarty/language-c-inline/blob/master/Language/C/Inline/C/Marshal.hs), I don't see a place where I can put in some manual marshalling. How would I call [strerror_r](http://pubs.opengroup.org/onlinepubs/9699919799/functions/strerror_r.html) using this library? What about [readv](http://pubs.opengroup.org/onlinepubs/9699919799/functions/readv.html)/[writev](http://pubs.opengroup.org/onlinepubs/9699919799/functions/writev.html)? [setsockopt](http://pubs.opengroup.org/onlinepubs/9699919799/functions/setsockopt.html)? [fcntl](http://pubs.opengroup.org/onlinepubs/9699919799/functions/fcntl.html)? [nftw](http://pubs.opengroup.org/onlinepubs/9699919799/functions/nftw.html)/qsort/bsearch? [iconv](http://pubs.opengroup.org/onlinepubs/9699919799/functions/iconv.html)? It seems difficult to have a language any less complex that Haskell to cover all these cases. If that's true, why are we writing in this TH embedded language rather than writing more traditional FFI code? [1] And I knew that when I wrote my previous comment. I should have said "from the types" or something. I know this approach intentionally avoids reading the headers, leaving that to the programmer (since the programmer will have to know that stuff anyway, at least for "low-level" / C-like bindings). In any case, thanks for the correction.
Arjun Comar has done the work for automating the generation of low level bindings. The reason there are still several libraries is that the design of a high level interface is an interesting open question. The tricky part is that automating generation of the high level interface doesn't seem to be so easy: different parts of OpenCV benefit from different wrapping styles when going to Haskell. For instance, in Haskell we make extensive use of partial application, which suggests that one ought to think rather carefully about order of arguments to support function chaining. In my bindings, for example, I mutate intermediate images if that mutation can not be observed. Arranging for the opportunity to perform in-place updates took some care when wrapping each function. I don't think mine is a final answer to how to do this, but it was a satisfying exercise and served me quite well as it let me program in a mostly functional style without giving up the performance you'd expect from the imperative API.
Thank you very much for making the videos available!
http://bugmenot.com/view/skillsmatter.com
Thanks to /u/dstcruz.
Coincidentally, [today's commitstrip](http://www.commitstrip.com/en/2014/10/08/the-other-me/) is about this very sort of situation.
Except that type classes imply a global confluence of instances; i.e. there is only one instance for a given type. Explicit dictionary-passing style doesn't imply that and (for GHC) neither does implicit dictionary-passing style.
this has very little to do with haskell, clojure or programming itself, its just michael projecting his passive-aggresive response to someone at some point failing to recognize his brilliance....the underlying theme for anything he writes but it should surprise no one that blogs are basically primal-scream therapy for the new century. get it all out michael...there, there
Why have an Int / Integer type? Peano numbers are easy to define and use. Why have an if statement, it's just the application of a Peano Boolean. I wrote a functional language like this an an undergrad. It had a print statement (with string literal included), and lambdas. I even wrote a program to print out the Fibonacci series with it. It didn't even have a libraries / include syntax, so I had to implement church-encoded everything in the same file. I was fairly obsessed with language minimalism at the time. IME, it's not worth it. I like understanding what is happening behind the scenes, but once I understand it I'll take my language with as sugary-sweet syntax and as many features as I can get.
&gt; implementation inheritance IME, this is usually better done via composition instead of inheritance. &gt; multiple [implementation] inheritance Few languages get this right even among the ones that have it. I've only ever wanted to do this when dealing with a framework that prefers implementation inheritance over composition.
Is that counting lines of proof too? How about extra lines because the code may have to be restructured to be more amenable to proof? To compare apples to apples you have to have the same amount of lines per unit of functionality. If verified software takes e.g. 2x as many lines per unit of functionality it's not really fair to say: look! it only costs a little bit more per line of code.
I found it very refreshing that he didn't bitch about closed allocation until about 3/4 of the way through. I think MOC does have useful insights but he often buries them.
I don't believe you. If I had an untyped language with monads, I could just use one of the native efficient list constructs and put `a -&gt; m b` functions in the list and then fold them together with `(&gt;&gt;=)` when I need the results, whereas with Haskell's type checker, I need to create a new list construct that propagates the type information.
Continuing with /u/willIEverGraduate's criticism, I feel like your piano comparison is a better analogy to "A great Haskell programmer can write great code even when restricted to Haskell98". A better music analogy is trying to construct a band out of theremin or dulcimer players.
Uh... come on peeps. We need some @remembers or direct mail to myself for the "quotes" section.
Hmm, really? This sounds contrary to modularity and hiding of implementation details. What if the implementation of `C` changes to something else? What was previously a no-op may then become rather expensive.
I'm not fond of the phrase "right tool for the job" because it implicitly assumes that that somehow there's a clear-cut decision procedure for choosing the right tool. It's a really vague phrase that seems to be a popular mantra, but it really doesn't have any information content.
&gt; A skillful engineer can write something that works pretty well in any language, true; but that in no way implies that all languages are somehow equally valid to him. That may be true but there is no language that is good for everything.
Are Haskell engineers second-rate? Yes, of course, given that the quality of being first rate is very uncommon. Mediocrity forms most of the things in the Internet of things. As for the blog posting, he's obviously venting online and as result, it's too intellectually flawed to respond to other than in flippant ways. 
This is really an unfortunate interaction with GHC (and I'd argue GHC's bug but moving on). Do you not tend to write composition chains in your work, such as `apply filter1 . apply filter2`?
This ends up being a bit equivocal really. Doesn't seem there's much of substance here, unfortunately, since it started well and I often like reading Michael's blog posts.
Sorry, haven't been on IRC in quite a while. I usually drop by when I'm hacking Haskell at home, but graduate school on top of a full-time job is keeping me busier than I want to be, so there's no time for that.
If I'm getting it right, then coerce only works if the data constructor of the newtype isn't hidden.
That is my understanding too, and I think my objection about modularity still applies nonetheless.
It won't work efficiently. But I plan to make `apply` an internal thing to create new filters and only expose filters as functions of the kind `Image -&gt; Image`.
Yep, Python's guides consider the * to be bad style: https://docs.python.org/2/howto/doanddont.html#at-module-level The problem with import-all is that it assumes that name conflicts are rare between different modules. This can work out when the libraries have been designed to work together -- like the stdlib, or the scientific-computing toolkit in Python -- but it breaks down when you have a rich and varied ecosystem like we have in Hackage right now. Of course, it's certainly very convenient when things do work out, enough so that it's worth taking a little time to avoid egregious conflicts -- particularly so for operators, since they're especially hideous when qualified. After a point, though, I do feel that namespacing is the least of evils.
This is a highly intelligent response. Your mother must be so proud. 
&gt; There's not much you can say with certainty about the diverse group of people who call ourselves Haskellers. I agree. I obviously intended the answer to the blog post to be "No". It's [Betteridge's Law](http://en.wikipedia.org/wiki/Betteridge's_law_of_headlines). &gt; Preferring Haskell is rarely an opinion that's uninformed by having not worked in other languages and paradigms. Very, very true. I was attacking a *perception* that is common among IT middle managers and most tech CTOs that "languages don't matter" and that a strong preference with regard to PL is a sign of a weak engineer. This isn't an attitude you'll come across on /r/haskell but you deal with it a lot in the business world. To a typical IT manager (we're not talking about people at top startups) there are "hard" languages (C++, Java) and "easy" languages (Python, Ruby, PHP) and the good engineers use only the "hard" ones. That's a total bullshit dichotomy, of course, but any language that programmers actually *like* will be judged to be one of those "easy", "watered down", "inefficient" languages. 
Something I've had to learn is that, while I feel unproductive on Java as compared to Haskell or Clojure, I'm still *acceptably* productive. A good programmer can write good code, at a reasonable rate, in C++ or Java. He won't necessarily be happy, and he'll probably suspect that he's getting too little done. That said, some Java codebases are so bad that success in working on them is inversely correlated to engineer quality. That's not the fault of the language, though. 
How is it different from any other breaking change to an API?
As a recovering fan of Java, to me this smells a lot like excessive abstraction. It's not always necessary to abstract over every possible implementation of some type of structured data.
My experience is similar, I think a lot of the middle management MBA types espouse a "commodification" of the programming craft, such that programmers are replaceable cogs and the tools they use are just fungible machinery. What's kind of more alarming to me is that this seems to have pervaded down into the programming profession itself and we get this "nothing is better than anything else" idea about tooling and languages. 
Thanks.
The visibility of implementation details depends on the visiblity of the data constructors. coerce doesn't seem to change anything in this regard.
They are comparatively smaller and fix less of the implementation detail (Foldable does not, for example, require the thing being folded to have a notion of an enumerator, it just requires it to have a correctly typed fold operation). AFAIK there is no reason why this should be the case for some typeclasses like Monoid, Java could implement those in terms of its interfaces and it's be mostly just like Haskell, just un-idiomatic and a bit verbose. Java interfaces (for some reason) cannot be higher-kinded though, so you can't write functions like msum or mapM. For these cases, there is a good reason to make Java interfaces bigger and more complicated; these functions have to be implemented on a case-by-case basis, so you may as well add them into the interface itself. Actually, you have to, there's no other way to be polymorphic on types of higher kinds!
I think your comment misses the point the article intends to make and only sets up a straw man point that is then easily beaten. The point cannot be that "nothing is better than anything else", because the article only argues against a list of often repeated arguments in favor of static typing. That does not imply, nor does the article suggest it, that no favorable arguments are left. This is also evidenced by the fact that the author himself prefers the use of ML. In short: arguing against some pro-P arguments does not mean you are anti-P, nor does it mean that P and not-P are equally good/likely. He is not arguing against static typing and if you interpret the article that way, it tells us more about you than about him. I think his arguments are undeniable: they are all arguments generally made in favor of static typing that are *wrong*, because they are not advantages of static typing or not obviously advantages at all. Repeating those arguments is actively working against the cause, because they lead to irrelevant discussions about only tangentially related subjects. You have not engaged the arguments, only the form in which they were brought. That is also telling.
It's interesting that this is possible. I can't say it's exactly what I'd like, but it's quite cool never the less.
You know why that was the only actual argument *against* static typing? Because he was not making arguments against static typing. He was making arguments against bad/wrong arguments in favor of static typing. If you interpret this post as an attempt to argument against static typing, then you are suffering from as much blind fanaticism as the inquisition that considered every critical theological question as evidence of satanism. You really need to step back and wonder why you didn't give the article an honest chance.
It's ok, I used bugmenot.
This is great! I'd really love to combine this with `vty-ui` and create a nice little command-line HN app. Would be a cute project :)
When I log in using my real account, I get: &gt; Because of its privacy settings, this video cannot be played here. Any ideas what's up?
I wonder how long this stays valid? https://pdlvimeocdn-a.akamaihd.net/46690/880/295448207.mp4?token2=1412884300_59ba837984e6e33b7a4e5b1ac7004dbd&amp;aksessionid=3e8a34142a239ff8
Thanks! I think your idea is a great one! Would be cool to browse hacker news from emacs or vim :)
Cool. I've started implementing it just few hours ago. Well, you was faster. I've skimmed through the code. Seems like there is no way to make several responses within single session. IMO, the bindings can't be complete without it. Did you try wreq library, using it would make the code way shorter, the price are some heavy dependencies like lens. What do you think about it? Also, I've noticed that the field deleted is not used anywhere, from my understanding of documentation it appears only if the item is deleted. Anyway, would you mind if I contribute?
http://en.wikipedia.org/wiki/Betteridge's_law_of_headlines
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Betteridge's law of headlines**](https://en.wikipedia.org/wiki/Betteridge's%20law%20of%20headlines): [](#sfw) --- &gt;__Betteridge's law of headlines__ is an [adage](https://en.wikipedia.org/wiki/Adage) that states: "Any [headline](https://en.wikipedia.org/wiki/Headline) which ends in a [question mark](https://en.wikipedia.org/wiki/Question_mark) can be answered by the word *no*." It is named after Ian Betteridge, a British technology journalist, although the general concept is much older. The observation has also been called "__Davis' law__" or just the "__journalistic principle__". In the field of [particle physics](https://en.wikipedia.org/wiki/Particle_physics), the concept, referring to the titles of research papers, has been referred to as __Hinchliffe's Rule__ since before 1988. &gt; --- ^Interesting: [^Sensationalism](https://en.wikipedia.org/wiki/Sensationalism) ^| [^List ^of ^eponymous ^laws](https://en.wikipedia.org/wiki/List_of_eponymous_laws) ^| [^Sport ^in ^Birmingham](https://en.wikipedia.org/wiki/Sport_in_Birmingham) ^| [^Ashford, ^Kent](https://en.wikipedia.org/wiki/Ashford,_Kent) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cl4yj0t) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cl4yj0t)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
You're reading too much into my post. I'm engaging a specific argument in the article (which, incidentally, you requested elsewhere in this thread).
 &lt;lambdabot&gt; ddarius says: "use the right platitude for the job"
Yep, that's a great idea. I'll probably take a try. If someone is going to hack on it to, please tell me, so that we wouldn't do same thing twice. However, I was thinking that it would be nice to have an api with support for live data, so that the app could get updates on-the-go. I had an idea to write a library for working with firebase and then build HN API on top of it.
Oh, wow. That site is awesome. Thanks.
Haskell programmers are first class citizens.
Yes please contribute! It is true, I did skip over the deleted field, this won't cause failures though (UPDATE: deleted is now added to everything). We should definitely add it as a Maybe Bool or just a Bool that is set based on the field's existence and value. I'm not sure why using wreq would be preferred over http-streams in this case (or vice versa). Is there something wreq can do that http-streams cannot? There is no reason why we couldn't create an agnostic request backend and use CPP flags to turn on/off different http clients (http-conduit, http-streams, wreq, etc). so cabal install hackernews -fwreq. I'm considering doing that for another library, is that something you would like to have? Making several requests concurrently can be done by using the async library. I'm assuming most requests will be made synchronously though. For example, you have to retrieve a Poll before you can know which poll options can be retrieved. It wouldn't be a bad idea to use async to grab all the poll options at once though... Adding a dependency on async wouldn't be too nuts in this case. In cases where you want to issue many requests at once, mapConcurrently could be used for now. I think you're onto something though with multiple requests in one session, how do you propose we should do this? I'd like to keep the API simple as possible. By all means contribute! 
Haskell News has IRC quotes for this week: http://haskellnews.org/grouped And you can click them to go to the conversation!
There are significant undercurrents of commoditization in programming right now: * the idea that languages are basically equal * the idea that experience doesn't matter * pervasive anti-intellectualism * the prevalence of Blub languages * reducing programming to duct-taping open source components, whereby the programmer becomes more of an integrator It feels like tech sold its soul to business (who promised success), and got nothing in return. Classic Faustian bargain!
[The last section of this paper](http://www.nicta.com.au/pub?doc=7371) may interest you: &gt; An obvious issue of verification is the cost of proof maintenance: how much does it cost to reverify after changes are made to the kernel? This clearly depends on the nature of the change, specifically the amount of code it changes, the number of invariants it affects, and how localised it is. We are not able to quantify such costs, but our iterative verification approach has provided us with some relevant experience. &gt; The best case are local, low-level code changes, typically optimisations that do not affect the observable behaviour. We made such changes repeatedly, and found that the effort for reverification was always low and roughly proportional to the size of the change. &gt; Adding new, independent features, which do not interact in a complex way with existing features, usually has a moderate effect. For example, adding a new system call to the seL4 API that atomically batches a specific, short sequence of existing system calls took one day to design and implement. Adjusting the proof took less than 0.25 pm. &gt; Adding new, large, cross-cutting features, such as adding a complex new data structure to the kernel supporting new API calls that interact with other parts of the kernel, is significantly more expensive. We experienced such a case when progressing from the first to the final implementation, adding interrupts, ARM page tables and address spaces. This change cost several pm to design and implement, and resulted in 1.5–2 py to re-verify. It modified about 12% of existing Haskell code, added another 37%, and reverification cost about 32% of the time previously invested in verification. The new features required only minor adjustments of existing invariants, but led to a considerable number of new invariants for the new code. These invariants have to be preserved over the whole kernel API, not just the new features. &gt; Unsurprisingly, fundamental changes to existing features are bad news. We had one example of such a change when we added reply capabilities for efficient RPC as an API optimisation after the first refinement was completed. Reply capabilities are created on the fly in the receiver of an IPC and are treated in most cases like other capabilities. They are single-use, and thus deleted immediately after use. This fundamentally broke a number of properties and invariants on capabilities. Creation and deletion of capabilities require a large number of preconditions to execute safely. The operations were carefully constrained in the kernel. Doing them on the fly required complex pre- conditions to be proved for many new code paths. Some of these turned out not to be true, which required extra work on special-case proofs or changes to existing invariants (which then needed to be reproved for the whole kernel). Even though the code size of this change was small (less than 5% of the total code base), the comparative amount of conceptual cross-cutting was huge. It took about 1 py or 17% of the original proof effort to reverify. &gt; There is one class of otherwise frequent code changes that does not occur after the kernel has been verified: implementation bug fixes. 
Cool beans! How are you harvesting those? Are you just pulling the @remember(ed) ones?
I wasn't addressing his specific points about static typing—which weren't the main thrust of the article—but against the high-level point, that we should acknowledge dynamic types as reasonable just because people use them. I was responding to the context and where the article was coming from, not the to the specific details. Most of the actual points *are* deniable, but that would be a completely different point and would put too many different topics into a single post. The reason I wrote it was because nobody else (on the HN thread) was commenting on the "meta-point" that's taken for granted by the article, except to *support* it. If you think that misses the point of the article, you are both correct but, in turn, missing the point of my comment: it's not a response *to the conversation of the article*, but a response to *having it at all*, on *those particular terms*.
Everything you said is absolutely right, and we need to fight it. It's bad for us individually (since we accomplish less, hamstrung by the worthless idiots who've made themselves a filter between our work and the world, we also make less money) and it's bad for society. Venture capital isn't a way out; in fact, it's a well-oiled machine for shitting out yet more of those anti-intellectual, talent-hostile businesses. I'm at the "ready to unionize the tech industry" stage. Unions have their problems, but individual free agency has failed catastrophically. I don't want a union setting salaries or instituting some horrible seniority system like the airlines have; the models more interesting to me are professional athletes' unions and the Hollywood actors' and writers' guilds, which have demonstrably *not* produced the mediocrity that people fear would come about following unionization. Unions would: * bring programmer salaries to a level more in line with what they're actually worth. The end game is to make it possible for average people (not ladder climbers who either luck or weasel their way into equity) to save money and bootstrap again. * destroy stack ranking, which never belonged in technology in the first place. * allow individual programmers to have representation (legal and career-coaching) when dealing with management, negotiating terminations and references and introductions, and in negotiating away evil "we own your side projects" provisions. * increase transparency in compensation for programmers. * limit the power of middle managers within companies over their direct reports. * provide programmers with critical cultural and project-specific information as they plan their careers. I think we might also want to consider a union or guild for Valley *entrepreneurs* in order to demolish anti-entrepreneur practices like back-channel reference-checking and, more generally, the culture of social proof and note-sharing among people who are *supposed* to be competing over the best deals. If there was a political body with the funds to actually start suing VCs for collusion (private-equity market manipulation and insider trading) it would do a world of good. 
Are bloggers intellectually inferior?
&gt; We should definitely add it as a Maybe Bool or just a Bool that is set based on the field's existence and value. Indeed. &gt; I think you're onto something though with multiple requests in one session, how do you propose we should do this? By doing multiple requests within one session, I mean making requests without reoppening the connection. Using the current api you will need to open 20 connections in order to fetch 20 items. Opening a connection is very slow and resourceful operation, especially with ssl, so the difference is dramatical especially if you do a lot of requests. The solution is to open the connection once and share it among the requests. With *http-conduit/http-client/wreq* it is done really simple by sharing the connection manager which manages all the connections and makes them *keep-alive*. I haven't used *http-streams* but the documentation to *openConnection* says that it's possible to use opened connection for several requests but it warns about some problems and the stuff like counting the requests can get complicated as it most likely would require *IORefs*, so in the end you'll have your own connection manager. In fact *http-client/http-conduit/wreq* manipulates the connections on a higher level, you just create a manager and it opens/closes/shares the connections. The api wouldn't get much more complicated it just that we need a function *getItemWithConn* which takes a connection/manager alongside with normal *getItem*'s arguments. The main feature of *wreq* is that it works on higher level. I've already outlined the session management. Plus there are some small features like aeson integration and lenses. All in all the code would be much simpler. We can support multiple backends but is that really necessary. I think I can impliment it with wreq/http-client and let you take a look at the result.
Is there a way to watch without logging in?
&gt; the high-level point, that we should acknowledge dynamic types as reasonable I don't think that is the point at all. At most the point is that the various arguments presented are insufficient to show dynamic types are unreasonable. However, there are other arguments. &gt; [it's] a response to having it at all, Only the 'it' you are responding to is something different from the 'it' that is actually presented. Your first 6 paragraphs describe a conversation with the underlying idea that 'nothing is better than anything else'. And I've argued why that cannot be the underlying idea, because the form of the arguments doesn't support it. There is a whole score of things mentioned that are obviously deemed 'good', although the arguments are presented as in favor of something else. We should argue languages and supporting platforms, not typing paradigms. All kinds of arguments concerning each of these points remain; argument that are much more specific and whose merits can be discussed without devolving into vague generic assertions of superiority. * Haskell enables and invites data abstraction in ways that other languages don't, which results in better data abstractions. Compare fair examples. * Type specifications are good, because they serve as documentation and force you to think about what types go into, and come out of, your functions. This is a form of 'test first': if the implementation doesn't match the type, the compilation fails. ... 
Ok sounds good, definitely implement it and let me take a look. Another reason for using http-streams is that it is based on io-streams, much like the snap web framework, so it would lessen the dependency overhead in snap apps. Another reason why it might be a "nice-to-have" feature to allow people to swap out backends (for the yesod community). I could just use withConnection $ do { .. } and use a ReaderT IO to execute IO actions and close afterwards. I'll have to see how wreq manages connections. In a web app scenario you'd have to use an IORef or MVar to store the connection, since new threads are forked to handle each request obviously. In that case I'd persist the connection with a snaplet and use the pool library to store the connection. There's no reason why we can't do withConnection $ \c -&gt; do { .. } with http-streams though. I see the documentation for withConnection mentions being careful when issuing multiple requests from the same connection due to pipelining since we'll have to manage the request / response lifecycle manually. from docs: "HTTP pipelining is supported; you can reuse the connection to a web server, but it's up to you to ensure you match the number of requests sent to the number of responses read, and to process those responses in order. This is all assuming that the server supports pipelining; be warned that not all do. Web browsers go to extraordinary lengths to probe this; you probably only want to do pipelining under controlled conditions. Otherwise just open a new connection for subsequent requests." Also, firebase supports this efficiently it seems: "If you're on a supported platform, the Firebase SDKs handle all this efficiently (in reference to pipelining) and can even provide real-time change notifications" https://news.ycombinator.com/item?id=8423055
Heh, this I knew, was wondering if that movie had anything to contribute. The answer is negative it seems.
I "know" Haskell because... I'm a PL geek. Some programmers are interested in machine learning, others in security, while PL stuff is my "thing", more or less. 
Not to my knowledge, but there's a bugmenot. http://bugmenot.com/view/skillsmatter.com I used my Twitter login. They didn't ask for any permissions.
&gt; Using the current api you will need to open 20 connections in order to fetch 20 items. Ok I updated it, now the connection is shared in a reader. It's quite fast now :) I made a hacker news monad. FromJSON a =&gt; ReaderT Connection IO (Maybe a). Probably could be a MaybeT too, but we should talk about that. http://lpaste.net/112397 It was broken, then I added the "Keep-Alive" header and voila magic, many requests one connection. To see the usage check out the tests. basically its just hackerNews $ do {..} http://lpaste.net/112398 What do you think? 
HTML and String are representationally isomorphic, but that doesn't mean that they are semantically equivalent. That's actually the reason why we have type families in the first place: so that we can define relations between types that aren't just based on their shape. I don't have an example that comes to mind for HTML and String, but here is one for several `newtype`s over the same underlying type String: newtype HTML = MkHTML String newtype PNG = MkPNG String type family ParsedValueOf a type instance ParsedValueOf HTML = XmlDocument -- some tree representation of Xml type instance ParsedValueOf PNG = PixelData -- some binary representation of an image
This is why SPJ says you have to be careful about what you mean by "=". HTML and String are different types - that's the whole point of newtype so that you can't pass HTML to a function expecting a String (or vice-versa). They just happen to share the same representation, i.e. they are representationally equal - a coarser equivalence class. 
But putting functions in lists is standard in FP (it may take newcomers a bit to get used to, but it's not surprising to long-time users), and picking the right efficient list is something that most languages (including Haskell) have already done in their standard library.
I'm an engineer and I would love to program in Haskell, but I'm not a software engineer.
I'm preaching to the choir here, but focusing on creating a loosely organized professional representation of developers would be a great start. Instead of focusing on all of these objectives at once, I'd couch it in more direct benefits for prospective members -- help in career development, hackathons, mentor/mentee (short-term?) opportunities, networking, etc. The real goal is to be an actual support network for professionals that transcends company boundaries. It has to feel more 'real' than a meetup. If this can be put into place, I expect growth will follow, and with that, the possibility of collective action. But I don't expect buy-in will occur until people feel invested in an institution that feels like it looks out for them. It can't be drawn along any sort of language boundaries (as languages are ephemeral), but more along the lines of mutual respect for people who command the machines...even if they write in PHP. In other words, how do you grow a professional community?
Some of the slides didn't have comic sans on it. It felt weird.
Amazing talk, as always! SPJ is the best speaker I've ever had the pleasure watching a talk of. I wonder about one thing: is it possible to only export the constructor or only the deconstructor of a newtype, and have that work with coercions but only one way? For example a module may use a newtype to hide secret strings. A client may create a new secret string (String -&gt; Secret) but it may not open an existing secret and get the string out (Secret -&gt; String). So (coerce (x::String))::Secret would work, but the other way around wouldn't. Similarly, if you have a function Secret -&gt; Int, you can coerce that to String -&gt; Int, but you can't coerce a String -&gt; Int to a Secret -&gt; Int. Another thing I wonder is what would happen with a dependently typed language. There you don't have type functions directly on types, but you need to work on type descriptions in order to do type functions. Would the naive version of coerce work in that setting i.e. without all the role business? Essentially the difference between a type and a type description would automatically handle the nominal vs representational distinction, right?
I'm an actual engineer (aerospace + EE) and use Haskell. It's great. I'm working on making it a viable alternative to Matlab + Simulink!
In my pre-haskell days I had let myself get _very far_ from algos and _very far_ from the machine. Having a language that gets out of the way has let me connect again with research all the way from generated assembly to core systems stuff to very high-level algorithmic arguments.
any kind soul care to summarize what's covered before I dive in to the video?
Deleted field is now added
 func :: Int -&gt; Int -&gt; [a] -&gt; [[a]] -&gt; [a] -&gt; [[a]] func _ _ [] res _= reverse res func n k (x:xs) res ys | n&gt;0 = func (n-1) k xs res (x:ys) | n==0 = func k k (x:xs) ((reverse ys):res) [] backwardsConcat :: Int -&gt; [a] -&gt; [[a]] backwardsConcat n l = func n n l [] [] i didnt test it, and i havent slept for 30hours so its atrocious but maybe it does the job?
http://hackage.haskell.org/package/split-0.1.1/docs/Data-List-Split.html Data.List.Split.chunk is what you're looking for, if you don't mind the dependency. Otherwise it's simple enough to write yourself
Thanks, that's helpful!
I'm getting a "parse error on input ‘_’". I'm a bit short on time and I think I'm gonna go with http://hackage.haskell.org/package/split-0.1.1/docs/Data-List-Split.html , but thank you for taking the time to write that! Also, you should get some sleep. :)
Yes! That's it, thank you!
Yeah i'll try to get some soon, i promess! The code was pretty ugly, you dont lose much ;)
streaming yes let's do it, https://www.firebase.com/blog/2014-03-24-streaming-for-firebase-rest-api.html
What I like about the videos on skillsmatter is that they the slides are always shown while the presenter is in a little box at the lower right corner of the video. I hate videos where the camera constantly pans around, many times showing the presenter when I want to see the slides.
Why don't you use Brainfuck or Unlambda? Or even better, [Iota and Jot](http://semarch.linguistics.fas.nyu.edu/barker/Iota/). Sure, a language should be minimal, but in order to be practical, it also should allow easy abstraction, which is why typeclasses are there. They reduce boilerplate. Otherwise you end up in a [Turing tarpit](https://en.wikipedia.org/wiki/Turing_tarpit).
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Turing tarpit**](https://en.wikipedia.org/wiki/Turing%20tarpit): [](#sfw) --- &gt; &gt;A __Turing tarpit__ (or __tar-pit__) is any [programming language](https://en.wikipedia.org/wiki/Programming_language) or [computer interface](https://en.wikipedia.org/wiki/Computer_interface) that allows for flexibility in function but is difficult to learn and use because it offers little or no support for common tasks. The phrase was coined in 1982 by [Alan Perlis](https://en.wikipedia.org/wiki/Alan_Perlis) in [Epigrams on Programming](https://en.wikipedia.org/wiki/Epigrams_on_Programming) &gt;*54. Beware of the Turing tar-pit in which everything is possible but nothing of interest is easy.* &gt; --- ^Interesting: [^Turing ^completeness](https://en.wikipedia.org/wiki/Turing_completeness) ^| [^Esoteric ^programming ^language](https://en.wikipedia.org/wiki/Esoteric_programming_language) ^| [^Iota ^and ^Jot](https://en.wikipedia.org/wiki/Iota_and_Jot) ^| [^One ^instruction ^set ^computer](https://en.wikipedia.org/wiki/One_instruction_set_computer) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cl5gcni) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cl5gcni)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
COM interfaces can be used in C, no need to bring C++ into the picture. Some big chunks of MS Office are (were?) written in C and do a lot of COM.
Recursive data structures using `Fix`, algebras for folding (and coalgebras for unfolding in passing at the end).
The problem with "engineer driven" vs "business driven" companies is that in 30 years of work experience, the only "engineer driven" companies I got to work with, were either startups or research institutes. So unless one is willing to uproot and move away just to use language X, the result is a job at a "business driven" company.
SkillsMatter are very generous to record these videos and post them for free right after the conference. If for whatever reason they want you to register in return, I'm sure you can manage it ;)
I wish there was a book and/or documentation of sorts that explained the connection between Haskell and concepts from category theory just like this talk did. Something like LYAH only for Categories with respect to Haskell. 
I honestly don't see the problem of comparing the minimum execution time when benchmarking two (or more) functions. With enough repetitions this should be a pretty accurate estimation of the fastest possible execution time of the function. Any other measurement will be dependent on cache hits, process scheduling etc. which are basically never gonna be the same in your benchmark program as in a real application.
Hey. Is it still playing up? You should be OK if it runs through the SM site. If you're still running into problems send me an email at theo dot england at skillsmatter.com. Theo
Indeed. They are also generous in that they host London Haskell (as well as many other meetup groups) for free. 
Can't it be an artifact of your timing system ? 
commonly called `chunksOf`
Sadly, this is very true. Even though programmers have quantity when it comes to available jobs, quality is thin on the ground. Business-driven companies mean you've decided to let the passengers take the pilot job away from the actual, you know, pilot. 
Sure, I understood it - I was just pointing out that I hadn't seen that approach before. I'm curious where the left function to bimap comes in though, as we only saw `id` used to implement folds.
For comparison of algorithm implementations minimum execution time is quite useful, but I agree that it doesn't show the complete picture. But I don't think any benchmark shows the complete picture, profiling is often a better option.
I have to agree, while I'd prefer if they e.g. would use Youtube (as that would allow a more convenient way to download the mp4 videos for later consumption on my smartphone), I'm happy those videos are offered at all (which is quite a bit of work, as the quality is quite good, and includes slides), and I can bear giving them my email contact in return.
I'd like to see the full version of the slides, as I'm also interested in what the bifunctor gives you and I think that was in the longer version.
Ah, good call. I forgot Google seems to just turn up random versions of stuff on Hackage.
These type of functions are very easy to write with explicit recursion: fun :: Int -&gt; [a] -&gt; [[a]] fun n = go where go [] = [] go xs = take n xs : go (drop n xs) 
I find his opening statements remarkable, in the sense that this still needs to be told, even though this has been shown numerous times, as far back as 2007, see e.g. http://itkovian.net/base/files/papers/oopsla2007-georges-preprint.pdf, http://users.elis.ugent.be/~seyerman/Micro2008.pdf and of course, criterion itself.
This has been shown to lead to misleading or wrong conclusions. If the difference is sufficiently large, you might assume it tells you something, but even then you may have encountered several outliers. The correct way to do this is to use statistics. See e.g., Georges et al, Statistically Rigorous Java Performance Evaluation (OOPSLA 2007) and Kalibera and Jones, Rigorous benchmarking in reasonable time, ACM SIGPLAN Notices 48 (11), 63-74.
Project doesn't seem to be dead, I'm about to jump on board.
The entire point is that asking about the minimum time is a not-even-wrong question, because it gives you almost no predictive power about what to expect. Even if you're writing a hard real-time system, which is one of the few cases where you care about a single number, you need the maximum time.
The idea is that using the best measurement is simply not correct. It tells you nothing. Saying that A is better than B because A's best beats B's best will not lead to good conclusions.
Coerce doesn't let you do anything new, it only allows operations that you could already do manually. The difference is that coerce may be more efficient (e.g. coercing a [HTML] -&gt; [String]), and it chains coercions automatically if necessary.
&gt; reducing programming to duct-taping open source components, whereby the programmer becomes more of an integrator This last bit is why the famous MIT emigration from Scheme to Python is so upsetting. At any other school, sure. Water down the curriculum. But MIT should be teaching actual computer science in their classes.
&gt; It's Betteridge's Law. "The reason why journalists use that style of headline is that they know the story is probably bullshit, and don’t actually have the sources and facts to back it up, but still want to run it." ...maybe not the best link to drop. ;) Though admittedly I have seen it used [to good effect](http://cdn2.damnfunnypictures.com/qwsk5cy-WasDarwinWrong001.jpg) in similar context.
I know *you* did :)
Well, their email registration link is broken, 404. After registering with twitter, instead of Brian's talk, I got: Because of its privacy settings, this video cannot be played here Tried VPN from a couple of other countries, and got the same message on all of them. Then I noticed Privacy Badger had blocked some web bugs ; after enabling them I was finally able to see the talk.. which is actually hosted on vimeo. From there it was not hard to find a url to download it, using firebug, so I can watch it later. Anyway, the larger problem is that, assuming this is a valuable and insightful talk (which seems likely because bos..), I may want to share it with other people, who might be less inclined to try to jump through every hoop necessary to see it.
We think of SICP as a hardcore text, but to an unbiased, young mind it's probably a better representation of the beauty of CS than anything more 'practical.'
Even then, you will need some form of confidence that new measurements will not go over the threshold and even when you take 1K measurements, you cannot be certain (or have some confidence) if you only look at the largest value.
There's much more that can impact it, both in the hardware as in software. There is the cache, address prediction, etc. but execution time of any part of your code can depend on external factors as well, such as link order, environment size, etc. See e.g., Todd Mytkowicz, Amer Diwan, Matthias Hauswirth, Peter F. Sweeney: Producing wrong data without doing anything obviously wrong! ASPLOS 2009: 265-276.
For those of us without a login (and therefore cannot view the video unless I'm mistaken), can you summarize whether this is in fact REST or if it's simply an API exposed over HTTP?
The idea is also to have real world applications as benchmarks. Kernels can help pinpoint things, but for real conclusions, they do not suffice. We've shown in the past that results in the literature actually measured the JVM rather than Java code, see e.g., How Java programs interact with virtual machines at the micro architectural level by Eeckhout, Georges, De Bosschere, OOPSLA 2003 (http://users.elis.ugent.be/~leeckhou/papers/oopsla03.pdf)
Perhaps there's some confusion of whether the question is existential or universal. I parse "Are Haskell engineers second-rate?" as a universal question, not an existential one. In other words, there may be second-rate Haskell engineers but not all Haskell engineers are second-rate.
Someone pointed out to me a prior project that seems to have somewhat related goals: http://graydon.livejournal.com/186550.html There might be some ideas there that can be lifted :-)
Another simple solution using just take and drop: chunksOf _ [] = [] chunksOf n xs = take n xs : chunksOf n (drop n xs) 
No, it is not useful, even though it has been used (and still is) in many benchmarking studies. Allow me to give an example. Suppose the execution times are as follows (ordered): A: 3,3,3,3,4,5 B: 2,3,3,4,5,5 You'd conclude that B is better than A. However, mean(A) = 3.50, mean(B) = 3.67. So now you'd conclude that A is better than B. However, there really is no difference between them, as a dual-sided Student t-test shows, at say the 95% confidence level. p-value is 0.7879, which is higher than 0.05 and thus the null-hypothesis of A == B is _not_ rejected (the 95% confidence interval for the difference is means is [-1.52; 1.19], which contains 0. You would have preferred B. In reality there is no measurable difference (given this data set). 
This is certainly better than most things that call themselves REST! Though it seems from a quick glance that HATEOAS is still a manual thing, unless I'm mistaken (please prove me wrong!) So it's probably best called HTTP. Very faithful HTTP, but HTTP nonetheless.
Sweet . Great work. 
I've used a very similar system, but with GADTs instead of type families i.e. data SubChannel request response where Classes :: SubChannel ScheduleArgs [Fynder.Class] Locations :: SubChannel BusinessId [Fynder.Location] forSubscriptionChannel :: SubChannel request response -&gt; (request -&gt; m response) -&gt; m () Compared to type families I find that the GADT version gives slightly better type inference (since it's closed) and clearer type errors and it's less verbose. Of course, you then have to define all your channels in one place which makes it less modular.
Thanks :)
Thanks so much!
Hi, the starting point with these implementations is to put all contention on a fetch-and-add instruction which we get as an IO function from the atomic-primops library. I thought about trying to use the counter in an `unsafeIOToSTM` to distribute reads and writes over, say, an array of dequeues, something like: `Array (TVar [a], TVar [a])`. If I could ensure the IO counter increment was called only once, and then the index reused on retry then this *might* work... really I haven't thought too much about it, but I suspected I didn't quite have what I needed.
Judging from the discussion [here](https://ghc.haskell.org/trac/ghc/ticket/8767), it looks like `fmap coerce == coerce` rewrite rules can be added on per-instance basis and the plan is to implement this in the standard library. There can't be a catch-all rewrite rule that works for all `Functor` instances because some of them may not obey the `Functor` laws.
What's depressing is that a "bifunctor" is an "ordinary functor".
&gt; Bake scales up so that even if you have 5 hours of testing and 50 commits a day it will not require 250 hours of computation per day. In order for Bake to prove that a set of patches pass a test, it does not have to test each patch individually. Do you mean that Bake won't necessarily run the test suite on every single commit, as an optimization? Doesn't that mean broken builds could sneak into the project history? EDIT: Whoops, someone asked a similar question directly on the blog. Here's Neil's answer: &gt; Yes, it hurts bitsectability a bit. But already most people only CI test on a push, not on each commit, so there are plenty of untested commits. Even with my small projects testing each commit would be infeasible. So it removes a few known good points, but hopefully it's not significantly worse in practice.
Very simple and elegant.
Are the slides available somewhere? The video shows Simon many times when I wish to see the slide he is talking about.
Don't worry, it takes a while for your mind to get in sync with the type system. It took me a few months before I could write code continously without getting type errors at almost every compilation. Keep going at it.
&gt; I wonder about one thing: is it possible to only export the constructor or only the deconstructor of a newtype, and have that work with coercions but only one way? For example a module may use a newtype to hide secret strings. A client may create a new secret string (String -&gt; Secret) but it may not open an existing secret and get the string out (Secret -&gt; String). You can hide the constructor and expose a function that does this: module Secret (Secret(), secret) where newtype Secret = MkSecret String secret = MkSecret 
Certainly! The question is can it be made to work with coerce?
It's all worth it when it dawns on you that the reason you can't seem to fix the type errors isn't just you misspelled an indenter but that your approach to the problem is fundamentally flawed. This has happened to me a few times now (I think the last one was a case when I was using Map instead of MultiMap or via versa I don't quite remember).
Erik Meijer is awesome and I am looking forward to this.
Hell yeah, thanks for this!
HATEOAS is hard to put into libraries. do (general) you know of any serverside apis to generate one (hal, hydra, whatever)?
not very elegant, because it has to traverse each chunk twice (`take n` and `drop n`). better use `Data.List.splitAt n`.
Thanks for the response. I'm not smart enough to say anything more than it would be very handy to have the composability/convenience of STM with better performance! I think it's something people would find useful, if you ever had the time to pursue it further.
http://channel9.msdn.com/Series/C9-Lectures-Erik-Meijer-Functional-Programming-Fundamentals
It seems Rosefold3 satisfies the homomorphism equation and is considered the "right" and the most general version. But I'm wondering why fold has to satisfy the homomorphism equation to be "good". What is homomorphism anyway? What's so special/good about it? I'm pretty new to category theory, so any futher explanation on this will be appreciated.
I'd like to be able to declare a type class argument to be representational. Then the compiler should check that types with the same representation to have the same instance. (F.e. by only allowing derived instances for newtypes.) Then `Map` and `Set` could use a representational version of `Ord`, because the actual ordering is not relevant, and then `Map` and `Set` would be completely representational too, giving more options to coerce them, i.e. more performance!
I did explore this type of implementation originally, but ran into troubles as I expanded the code. The slides aren't literally all the code we have, so there may be other subtleties that have influenced my decision to use type families. But as I (think?) I say in the questions - there's certainly more than one way to do this.
Thanks, I hear you. I think we're in the same boat smartness-wise
I would be very interested in your alternative to Matlab and Simulink. I work in ultrasonics, and I'd love to have a Haskell library for quickly prototyping new algorithms.
I would really like to here about the problems you ran into, if you're still aware of them. :)
It feels like that initially, but don't overthink it. All the monad/comonad stuff is one of the things I feel most confident about, but whenever I've tried to read any of the Category Theory for Computer Scientists books and hear about how arrows are morphisms and yada yada, I feel overwhelmed pretty quick. You really don't even really need to know basic category theory to be able to use Haskell. The most you need tends to be stuff like "this operator is associative, and has this type signature". For instance, Maybe is a monad because you can write a function (&gt;=&gt;) :: (a -&gt; Maybe b) -&gt; (b -&gt; Maybe c) -&gt; (a -&gt; Maybe c) which is associative and has an identity (Just :: a -&gt; Maybe a). Doesn't tell you how to use monads in practice necessarily, but that's more a matter of practice than theory anyway.
Wow thank you! Was just about to ask a question about the content but now I can see for myself.
This looks a bit like what we did in Haxl, having the request be a GADT indexed by its result type.
There are certainly lots of ways of doing this sort of thing. One downside of the GADT approach is that you need to index the GADT up front by *every* type that depends upon the choice of SubChannel, whereas with a singleton GADT + type families, you can easily add another type family at a later stage. Moreover, the singleton GADT can be mechanically constructed from the plain ADT (e.g. by Richard's excellent singletons package). It is a fairly direct encoding of what we really want, namely proper dependent products in GHC Haskell...
or on one line, chunks n xs = takeWhile (not.null) $ unfoldr (Just . splitAt n) xs or as recently seen on SO, (less clear), chunks n xs = unfoldr (\xs-&gt; listToMaybe [splitAt n xs | x &lt;- xs]) xs = unfoldr (listToMaybe . (map =&lt;&lt; const . splitAt n)) xs 
"Old lectures from *years* ago" - as in from 2009? Ancient! Seriously, I wouldn't expect much change in presentation or philosophy from 2009.
I signed up last week. Maybe a study group is in order on r/haskell?
Wow, this is exciting! I really want to learn Haskell as a branch out from C++ and Java. I hope I can find time to keep up with this course in between keeping up with uni courses. Thank you for sharing this information. 
Perhaps. Maybe IRC or some google groups type forum might work. I don't know how everyone would feel about a weekly recurring thread.
Indeed, it can't at present, and this would require something rather more complicated. Coercible is always symmetric. One could certainly imagine an asymmetric coercibility relation, but as you identified, then one has to worry about the variance of type parameters: (-&gt;) is contravariant in its first argument and covariant in its second argument, so we should get something like (Coercible t s, Coercible u v) =&gt; Coercible (s -&gt; u) (t -&gt; v) and in general we would need to infer/record the variance of the parameters of datatypes, as well as their roles.
How much does this cost?
There was one important change, that brought the grammar in line with how all the compilers already did the parsing: operator fixity levels are no longer included in the BNF'y grammar. The older rules meant that Haskell could in principle not be parsed correctly without chasing fixity declarations, making it wildly non-context-free even if you ignored indentation.
Nothing!
Signed up, would participate.
You'd probably be interested in [Plush, the comfy shell](https://github.com/mzero/plush)
LYAH is a nice introduction to the language, if you haven't already looked at it. 
I was actually just reading it, haha. 
I'm signed up. Beginning Wednesday, I will leave the office earlier. 
not google, not facebook. reddit or irc channel. I am signed up currently doing another course on programing with edx
Not POSIX, but [shell-conduit](http://chrisdone.com/posts/shell-conduit) is worth a look.
There are a few main problems which still need to be tackled: * The prelude sucks for numerical/scientific stuff (e.g. Num, the Category/Functor classes etc) * Arrow syntax * A fast, principled matrix library * Visualisation I'm working on the first two; /u/cartazio is working on the third, and I'm not aware of anyone working on the fourth. 
Something like [those one-liner reimplementations](http://www.haskell.org/haskellwiki/Simple_Unix_tools), or was it more complete programs?
For visualization i assume you mean stuff like graphing and plotting? Would a library to interface with graphvis be useful to get the ball rolling?
Sort of - I mean more 3d/time varying type stuff, but yeah. I know of some good static/2d ones though.
Thanks. At 14 pages, though, it's more a nutshell than a cheatsheet ;-)
If you want a certificate you pay a minimal fee of 50 USD (but are free to pay more).
CoffeeScript and Elixir also have only one cheat sheet each on the internet, meaning that Haskell is part of the "one cheat sheet" league of languages.
Gotcha. Maybe ill take a crack at it when i have more free time. Could be a fun side project!
i've some thoughts on visualization (thanks for the shoutout as always!) BUT, i'm deferring working on that rabbit hole till life and income get a bit saner for me :) 
on the matrix lib front, there *are* some fast matrix libs, but theyre just as terrible/good as the ones in every other stack (so meh). I *hope* what i'm working on can raise the bar but still be easy to use, but only time will tell :) (though the wip code is public, its not like any other array lib i've seen as yet :) )
&gt; Why don't you use lambda calculus? &gt; Performance. Seriously and honestly. If I could just use lambda calculus and get arbitrarily fast programs, I'd encode everything on it and live happy. There is absolutely nothing you can't do in Haskell that you can't do with pure LC easier and faster.
This is great. One more thing down for a fully native experience on Windows. After this, only need: * link.exe compatibility * ability to use MSVC/ICC to build the runtime Make was the biggest pain point though... Rest should be relatively easy.
Unless you're used to lazy strongly typed functional programming in other languages, a cheat sheet won't help you much: it's a fundamentally different way of programming, so you (mostly) can't just learn the new keywords and names of common library functions and hope to be productive.
Sounds good to me. Presumably ghc would carry a copy of Shake, like it currently has Cabal and ghc-cabal? 
That's a detail we will figure out much later. Perhaps, but not necessarily - you don't need to build ghc using the ghc you are compiling, you could just cabal install it for the one you use to bootstrap it. 
The POSIX shell is essentially the Korn shell modulo some features. In the process of standardization they managed to make the ksh93 incompliant, the only shell that currently implements POSIX completely is a patched variant of ksh88. Not even bash is in compliance. It's quite difficult to reimplement the POSIX shell as the specification contains some ambiguous things that have to be implemented in the way the Korn shell does and because POSIX mandates a shitload of features.
half a decade
afaict, no. it won't optimize it away.
Support for both would be nicer. If you do it right, you can probably restrict yourself to a few lines of CPP on a couple of files.
You may like Scheme, it is a very minimal dialect of Lisp.
You seem to have experience with that... can you point to some code examples which get away with a tolerable amount of CPP?
On my wiki page, I said &gt; This all takes a couple of minutes, and if I weren’t so lazy I could probably automate it. What I didn't say is that I figured if I left it long enough, someone would probably do it ;) Thanks - this looks really handy! Ultimately, we should probably get this into `cabal2nix` proper, but this is very convenient stop-gap.
Yes I have some experience, but it's all closed source stuff, so nothing I can share. You can do it with lots of CPP but I hope GHC HQ would reject any such patch. 
All code can be found here: https://github.com/silkapp/rest
With Haskells lazy evaluation and the garbage collector a `[a .. b]` would not allocate the full list if you're traversing it once, one element at a time. (Please correct me if I'm wrong)
They can do it the way OpenBSD does OpenSSH and LibreSSL - code it up for one OS, #include shims.
This looks great, but it gives me the impression that whenever you'd add something to your path, you'd have to recompile your shell... Which would be a bit of an annoyance.
Isn't dash compliant? Well.
Oh, we should definitely get it into cabal2nix proper: if nothing else it would be much more robust.
The first question, of course, is whether it's *worth doing at all*. I imagine doing it would be quite a lot of work, so there needs to be a clear set of things to be gained from doing so, with people willing to maintain it. It would at least require: - At least a rewrite of all the x86/amd64 assembly inside GHC to use MSVC assembler syntax (this alone is annoying enough). - At least a rewrite of the x86/amd64 code generator's pretty-printer, because it uses GNU AS syntax (I imagine this is easier than it sounds, perhaps). - Probably a lot of refactoring and restructuring the RTS to support MSVC features, which generally are anemic for things like C99. We also use several GNU features and attributes too. - A lot of work on the driver pipeline for things like `link.exe` compatibility. - A lot of Cabal work, I'm sure. This stuff is all pretty cut and dried, but it is a lot of changes, plenty of unknowns (how will it perform? what's the feature parity we can achieve, and what new features do we get?) and it's territory we should discuss before moving on it.[1] If people actually care about this, I'd honestly look more forward to Clang's MSVC frontend/link.exe compatibility work, which promises to hopefully bring a modern compiler to the MSVC toolchain, as opposed to trying to get all this working with MSVC itself. That'll at least make a few things easier, and get you a better compiler to boot. **EDIT**: But none of this *really* changes the fact a lot of Haskell packages are simply hostile to platforms like Windows, which can't utilize things like `autoconf` or other things Haskell programmers expect (like certain C compiler features or behaviors, and yes, we do depend on them). MSVC support for GHC is never going to fix the usage of `autoconf` in the network package, for example, or arbitrary packages like `unagi-chan` using GCC-specific builtin functions. It will always fail in non-unix/GCC environments. The issues to actually make Windows more tolerable for Haskell are a bit more broad, I think. [1] And I'm really not being obtuse here, I swear - I don't know what we'd gain, but I'd really like to know! Can someone explain the benefits?
No. Dash isn't in compliance either.
Because Cygwin/MinGW in a production environment is just some kind of cruel joke that never works. It's really hard to sell Haskell to management if it's so reliant on something which is just so bad.
The CPP is not much more than a minor detail, although it is indeed the devil as far as I'm concerned. A lifetime ago I spent months refactoring some C++ into sane cross-platform code with minimal `#ifdef`. But the sticky bit is you effectively have to double the chosen code (at minimum) no matter what when you do this, which is what requires the actual maintenance. But less CPP is always better than more, of course. :) But it's not just C code, parts of the code generators (pretty printers) and driver at least are going to need to be reworked or rewritten to support MSVC-centric features, on top of any RTS changes needed. And you'll have to change Cabal too. I think the path is probably more feasible using a Clang-based MSVC toolchain (see elsewhere), but it's still not just a matter of `#ifdef`'s. I do agree with the OP that `shake` is probably a better starting point than `make` for such work, however.
With recent enough `cabal2nix`, you don't even need to use sed/awk to manually patch your default.nix. You can just do: my-project: $ cabal2nix ./. &gt; project.nix Which will generate a project.nix like this: # This file was auto-generated by cabal2nix. Please do NOT edit manually! { cabal, dlist, doctest, either, filepath, free, lens, mmorph , monadControl, mtl, optparseApplicative, randomShuffle, split, stm , tagged, tasty, temporary, text, transformers, transformersBase }: cabal.mkDerivation (self: { pname = "hcltest"; version = "0.3.4"; src = ./.; buildDepends = [ dlist either filepath free lens mmorph monadControl mtl optparseApplicative randomShuffle split stm tagged tasty temporary text transformers transformersBase ]; testDepends = [ doctest filepath ]; meta = { homepage = "http://github.com/bennofs/hcltest/"; description = "A testing library for command line applications"; license = self.stdenv.lib.licenses.bsd3; platforms = self.ghc.meta.platforms; }; }) Then, you can use the following generic `default.nix`: { haskellPackages ? (import &lt;nixpkgs&gt; {}).haskellPackages }: haskellPackages.callPackage (import ./project.nix) {} And `shell.nix` stays the same. Btw, you can even make it so that you don't need to hand-generate `project.nix`, but have it be generated every time you `nix-build` or `nix-shell`! For that, you'd need to use the following `default.nix`: (But you might not want that if you own the project) { haskellPackages ? (import &lt;nixpkgs&gt; {}).haskellPackages) }: let projectNix = (import &lt;nixpkgs&gt; {}).runCommand "project.nix" { LANG = "en_US.UTF-8"; LOCALE_ARCHIVE = "${(import &lt;nixpkgs&gt; {}).glibcLocales}/lib/locale/locale-archive"; } "${(import &lt;nixpkgs&gt; {}).haskellPackages.cabal2nix}/bin/cabal2nix ${filtered} &gt; $out"; in haskellPackages.callPackage (import projectNix) {} I added the feature to cabal2nix in https://github.com/NixOS/cabal2nix/pull/59.
In practice I rarely add new things to my PATH, but YMMV. You don't have to recompile the shell, you just have to `cabal install shell-conduit` again.
[Hell](https://github.com/chrisdone/hell) is based on shell-conduit. 
Is there a screenshot or example or anything of that project? Is it vapourware? What gives?
Just in time! started looking which frp library to use before I implement such functionality implicitly :) - this will definitely help me 
I signed up! :)
You are correct. And the behaviour of range() in Python 3 is similar to the one in Haskell, which is not the case with Python 2. (Unless you use special versions of the ordinary functions: xrange, ifilter and so on.) 