&gt;...has been created FTFY.
I have a theory that the reason for the poor optimization may be because `transformers` does not add `INLINABLE`/`INLINE` pragmas to `MonadTrans` instances. The easy way to test this is to create a small module with those two `ReaderT` variations you mentioned, but with your own custom `ReaderT` defined within the same module. Then compare the performance of that.
Good luck!
Alright, I have set up a [repo on github][0] for the toy example. If you could help me with that, that would be awesome. When using a two-level `ReaderT`, GHC produces [expensive closures][1]. However, what we really want is a tight inner loop, which [we get][2] when using a single-level `ReaderT`. (There is only one loop in the Core, search for the `Rec` keyword.) The task is to add INLINE or whatever pragrams to the [`ReaderT` implementation][3] such that GHC will produce the second, more efficient core even in the case of a two-level stack. Is that actually possible? [0]: https://github.com/HeinrichApfelmus/optimize-monad-trans/ [1]: https://github.com/HeinrichApfelmus/optimize-monad-trans/blob/3ec605eca579c25743bf2893fcd46251625c11b4/OptimizeMonadTrans.core1.hs#L1022 [2]: https://github.com/HeinrichApfelmus/optimize-monad-trans/blob/3ec605eca579c25743bf2893fcd46251625c11b4/OptimizeMonadTrans.core2.hs#L838 [3]: https://github.com/HeinrichApfelmus/optimize-monad-trans/blob/3ec605eca579c25743bf2893fcd46251625c11b4/Reader.hs#L11
You are right, it does indeed have dynamic linking on OSX in this release. As you can see in the comments below, there are some minor speed issues. I'm sure the problem will be solved soon enough. It's already solved for OSX 10.8 https://ghc.haskell.org/trac/ghc/wiki/Status/GHC-7.8 &lt;-- Has nice tables and explanations.
It's just as I suspected. When you have all the code within a single module the core becomes identical for both versions. This means that the problem is probably fixable using `INLINABLE` or `INLINE` pragmas. I will continue working on this. I sent you a pull request with the relevant code and core for each version [here](https://github.com/HeinrichApfelmus/optimize-monad-trans/pull/1).
Actually, I attempted to derive core for your original Eval1.hs and Eval2.hs and I got identical results, using both `-O` and `-O2` for both modules. Both of them eliminated the closure. I'm using `ghc-7.4` if it helps. You can find the generated core in this commit that I added to the pull request: https://github.com/HeinrichApfelmus/optimize-monad-trans/pull/1 That core was generated using `-O`, the same as yours and there is no closure in either version and they are both identical. What version of `ghc` are you using?
&gt; The main reason for shipping now is that it is able to build on GHC HEAD, unlike lens 3.10. For those trying to `cabal install lens`, they will probably hit [GHC #8696](https://ghc.haskell.org/trac/ghc/ticket/8696). But that can be workarounded by `cabal get lens`, cd-ing into the src-folder, and `cabal install`-ing twice.
Look at [JMatch](http://www.cs.cornell.edu/Projects/jmatch/) for a (vast) generalization of this. Going further you could say that logic programming is the ultimate of this; all you ever do is define pattern synonyms. This is one of those situations in programming language design where you have a clear trade off between simplicity and power. It is in these situations that I'm always hoping that somebody will come up with a solution that's both simple and powerful.
The RC was initially planned for September 2013, so that's what people were planning their bugfixing phases for. Since then I don't think big new and unstable patches have been merged; the biggest change in the period was probably adding pattern synonyms, which is also pretty stable from what I hear. In other words, there has already been a long bugfixing period, so unless something awkward is found release should be soon-ish. I don't think Hackage has been built with 7.8 yet though, and that's probably where the most new bugs will be discovered. Before that any timeframe is kind of arbitrary.
Actually, the question is not whether the modules `Eval1` and `Eval2` have similar core output -- they do. The question is what the GHC inliner will do when it encounters the recursive loop in the module `OptimizeMonadTrans`. The GHC documentation suggests that cross-module inlining will use the original definitions, so the optimized core that was generated for the exported functions is not relevant in this case. Note that the `Makefile` documents the specific compiler options and output files used. The commands $ make core1 $ make core2 create the core files that we're interested in. (I'm using GHC 7.6.3.)
When I used Xmonad it seemed to be at the start of a resurgence in popularity of tiling window managers. I wasn't aware of any before that point and only heard of Xmonad because of involvement in the Haskell community. 
I used to use it before I joined the Dark Side and bought a mac. I preferred it because it was written in Haskell. Before I used Haskell I used dwm.
I guess I can say that I use XMonad because of Haskell. But also because of the concept. I have used AwesomeWM before, and I liked the idea of a tiling WM, but I did switch to XMonad for the reason that I can configure it using Haskell. I would say that my knowledge of Haskell, unfortunately, makes it way easier for me to configure my WM. You have to know at least Functors and Applicatives in order to configure XMonad (and being able to understand your config).
I used it until I switched to a Mac. The fact that it was in Haskell let me comfortably tweak settings and extend it very easily. I've since played with `Config.Dyre` a few times for exposing that style of configuration in my own projects.
I cannot say that I did some major project - but GHC with your usual toy-projects runs fine on the RasPi.
I use XMonad both at work and at home. I first heard about it because it was written in Haskell, but would not have stuck with it if I didn't really like it.
I'm a bit fuzzy of what happened first. I do remember of hating both the new gnome and unity, and then looking around for a less bloated window manager. I think I chose Xmonad because it was in Haskell, but my knowledge was quite rudimentary at the time, so I just ended up copying and pasting configuration bits and pieces. Some time later, after being annoyed that some things were still not just as I liked them, I found that suddenly I knew what I was doing... It was quite nice :)
https://ghc.haskell.org/trac/ghc/wiki/Building/ARMLinuxGnuEABI indicates that there is some (albeit very spotty) support for ARM with EABI.
Does this break `At` as well then? If not, does `ixAt` still function?
First was XMonad which I liked. Then was Haskell which I had to learn to configure XMonad. I liked Haskell too.
Clang will always suck for Haskell cpp. For the rcs continue building with GCC if perf matters, and you want working cpp. The final 7.8 release may wind up bundling something like cpphs to resolve the cpp matter once and for all 
I use Xmonad. I switched to it around the time I was starting to learn Haskell. Haskell only played in to my decision to use Xmonad because of its reputation for being good for writing correct software (which incidentally is also a big part of why I was interested in learning it), the fact that it was configured in the new language I was just starting to learn ended up being a nice bonus. 
The speed issue isn't solved for 10.8 or any OS X version. See the ticket me and `audreyt` linked to below for more details. I updated that page since both of those issues are 'fixed' for the RC.
This is correct.
Same here. I read an article on LWN about the web browser uzbl, and how they did not want to implement tabbed browsing because any proper window manager should already implement tabs. They mentioned Xmonad. I had never heard of Haskell so Xmonad looked bizarre as hell. That's what got me stumbling around in Haskell. Now I'm decent in Haskell and I do all my hobby programming in it because I get more done in Haskell than in any other language. Ironically though, now that I actually know Haskell I don't use Xmonad. I long ago moved to fvwm and now to i3.
From my limited research into it over the last year or so, it seems possible, but it's very much wild west, best of luck, and god speed. iOS seems to be the best supported ARM platform currently and if you can get XCode to behave it seems like a fairly stable process. I *think* last time I checked there was a copy of GHC available on the ARM branch of Arch Linux, but ghci was conspicuously missing from the package. So yes, it's possible to run Haskell on ARM (with some caveats), but it's not exactly the relatively smooth process it is on x86, and there's nothing like Haskell Platform available yet.
Have you seen [osxmonad](https://github.com/xmonad/osxmonad)? There's a few impedance mismatches that still need to be fixed, but at least Mavericks now has per-monitor virtual desktops (IIRC, this was previously one of the bigger issues).
Not really interested.
http://ianyh.com/amethyst/ This seems somewhat popular
Arm Linux should be a tier 1 supported platform. Ben gamari and others have spent a lot of time getting GHC HEAD in working shape. I know there should be good support for armv7 boards, and I believe v8 and v6 arm CPUs too, but I could be wrong
iOS is supported, but ARM moreso. iOS can't th, and currently we can't run th with a cross compiler (yet)
Get a pandaboard or beagle bone or something in the mean time! :-)
I didn't measure it down to that level. But if you have an inclination for micro-optimization, I'll put it to you this way: it's the difference between saying[1]: movq %rax, %gs:752 and: leaq 265(%rip), %rdi callq *(%rdi) The first snippet stores a value directly into a segment register at a specific offset, which is the precise location of the TLS slot for a thread - that is, every OS thread keeps track of its local `%gs` segment, and it contains (in part) some of the TLS slots for every thread. The second assembly snippet loads RIP-relative value into a register, then does a dynamic indirect call to properly do this operation. This indirect call is opaque. If it's anything like the `pthread_getspecific` source code I saw in OS X 10.8 when I read it, it essentially iterates through the set of pthread TLS keys registered for the given thread in order to find the proper TLS slot entry, which it will then store the value into. This is a lookup in a thread-allocated data structure in the background (so it's not handled by the OS.) For the inverse (loading values from TLS slots,) it's basically the same idea. So yes, the overhead difference is huge between these two when you look at it from a 'micro' point of view. The reason this is so bad is because we *read* the GC thread descriptor structure *all over the place* when doing garbage collection, and set it too when doing things like marking roots. This quickly adds up *a lot*. Do a search for references to the `gct` variable [in this file](https://github.com/ghc/ghc/blob/master/rts/sm/GC.c#L1470) - we use it all over the place to do lookups and whatnot. That `gct` variable is marked `__thread`, meaning every single occurrence could potentially result in a register load, indirect call, TLS set lookup, write/read entry, and return. Versus just a single store/load from a single location. It's a lot more expensive! On Linux, `__thread` devolves into a single `load/store` into the `%fs` register segment on x86, which is the optimal case #1 above.[2] On OS X with Clang, it devolves into #2, which is not optimal. That's the basic idea. Sorry for the long-windedness. [1] These are snipped from the relevant GHC ticket. [2] Actually, Linux/amd64 is even *better* off than this - with GCC, we can directly tell the compiler to dedicate an *entire register* to that single GC descriptor. We do this using a [Global register variable](http://gcc.gnu.org/onlinedocs/gcc/Global-Reg-Vars.html), something not supported by LLVM. This is truly the optimal case, and that structure is so performance critical it's worth dedicating an entire register to it whenever possible. Linux/i386 still uses `__thread` due to the pitiful number of available registers.
Perhaps what I should have said is that iOS is the best documented so far as I have been able to tell. I found lots of mailing list activity, wiki pages, and articles talking about setting up GHC to compile against iOS, but almost nothing about targeting other ARM platforms. The documentation might be out there somewhere, but I haven't been able to find much of it.
I always really liked minimal, light-weight user interfaces that could be customized easily. I started using fluxbox, and used it for a while. It was okay, but .. not quite there. It felt messy. I tried a few tiling WMs after that, maybe ratpoison or some such, but nothing stuck. I had already started learning Haskell a long time ago and was really fond of it, and kept hearing about this XMonad stuff. I decided to try it out and found that it was infinitely nicer to configure, incredibly easy to set up and customize. I've never gone back. 
I have used tiling window managers for ages. The reason I use xmonad and don't spend notable time fiddling with my wm any more is Haskell.
Ah! Thank you for the correction! I am not an OSX user, so I do not follow those specifics very carefully.
When I was on OS X I managed to get by with the fake tiling you get from [SizeUp](http://www.irradiatedsoftware.com/sizeup/). It's not nearly the same thing, of course, but it integrates very well with the native window management. Now there's apparently also a similar, but open-source, project called Spectacle.
I use XMonad because it is simple enought and works well enough. I also happen to use Haskell at work, but I think this is unrelated. Regarding configuration, Haskell or not, I am too lazy to tweak XMonad finely, so it becomes from time to time a source of frustration : Gimp windows are a mess to manage, LibreOffice hangs on certain dialogs and others anoying stuff poping here and there. Sometime I'd like to run a "minimalist gnome-like" window fullscreen in XMonad, and then run these problematic applications from there. Do you know how feasible this is ?
I've not had much trouble with GHC on ARM, but template haskell and ghci don't work as of now.
`ixAt` is still there, but `containsAt` is gone. If you were using a `Contains` instance that was read-only foo^.contains x then you can still use has (ix x) foo so no power was lost overall, it just became more sane to be parametric in the instance and easier to write.
There's no AArch64 support in mainline GHC yet. There's not even actual AArch64 development SoCs on the market right now, so frankly I at least am not worried about it at all (and I simply can't tolerate the slowness of emulated hardware.) I imagine they'll pop up later in the year. I can also almost guarantee those platforms are not going to be completely bug-free or rock solid out of the gate when they're released - we've hit several ARM/Linux problems in the past, and the ARMv6/v7 line has had a much, much longer lifespan. In general, ARM/Linux is an extremely diverse platform, and that shows in the extreme variety of software/hardware packages that have odd combinations which need support. At minimum it'll happen in the 7.10 timescale, and even then it might not be a 100% solid port (ARM took a long time to get properly polished, because there's really not a lot of people doing the work. Ben Gamari, Karel Gardas, and to a lesser extent me are pretty much the only people who work on ARM/Linux on our machines.) There *is* relatively solid ARMv7 and ARMv6 support in GHC HEAD these days. Including GHCi and Template Haskell support. I'm hoping to still release an official ARMv7 binary (thanks to the hard work of Ben Gamari) for 7.8.1.
That page is ridiculously old. On any modern ARMv7/Linux distro (and even the RPi) you should be able to just install a copy of GHC directly from your package manager and use that. It should handle both hard-float and soft-float configurations (as well as the RPi's ridiculous combo of ARMv6+VFP3 hard float.)
Same story for me, except s/Awesome/dwm/
I initially tried XMonad because it is written in haskell. Unfortunately, it was noticeably slower than dwm, as well as having one major bug related to the size of fullscreen windows that has been known about for half a decade and remains unfixed. Other than those two things, the flexibility and core defaults are awesome. Particularly having a single tagset across multiple monitors, and deterministic monitor switching hotkeys built in. dwm has neither of those and have to be added :(
What I've done in the past is spin up a linux VM and then basically run it full screen on one of my monitors under Xmonad. The benefit there is if I also run an Xserver on the host I can just retarget to the other server when I need to run something that doesn't tile. 
I don't currently use it daily; multiscreen support is really important to me. Looking at the commit log, it hasn't been updated since October. It's a fairly small project, though, it basically just makes XMonad work with Quartz instead of X. After updating it to work with Mavericks (and Maverick's new per-monitor Spaces), it should be pretty good. Brian Mckenna would probably be able to comment further, though.
I use xmonad. I guess I started using it because dons was working on it and it was in Haskell, but then it wasn't much of a paradigm shift for me, as I'd been using ion as my wm before that. It's nice, I like it.
I use it, because it is in haskell and it is fast
Learning Haskell has been a pet project of mine for a few years now (and I'm hardly any better :P), but looking for things written in it turned me on to xmonad. I use it now, though, because it's the easiest tiling WM to integrate with the rest of xfce (and I *love* that setup for hardcore getting-work-done days!)
I became interested in it early in my Linux "career" because it had both tiling and was written in Haskell (which at the time interested me but couldn't code well in). This was during a lot of hopping around different WMs, and I shelved XMonad for a while for other things. Once I learned a bit more Haskell and came back to XMonad, I felt no need to ever switch to anything else. The functionality is fantastic, it's stable, and the configuration is comfortable and more powerful than anything else I've found.
Do you ever feel like you asked this question on the wrong sub? Unless your goal was to collect a large body of maximally biased opinions, of course :P
I use both Xmonad and sometime Unity under Ubuntu. I looked at Xmonad because it was written in Haskell but I found that liked the tilling window manager philosophy for many (if not all) of my needs. 
Been awhile since I've looked at Xmonad, so maybe my impressions are stale. But the best thing about i3 is the underlying model. Windows are in containers, which can have one of four states (split horizontal, split vertical, horizontal tabbed, vertical tabbed). You can nest containers an unlimited number of times. It's an incredibly simple and easy to understand, yet very powerful, paradigm. I also like how i3 bakes in things that Xmonad separates out. In particular, some of the status bar features are better integrated, which makes them easier to use. For instance you can click on your workspace name to move to it.
I began using it as a replacement for Awesome, because my .awesomerc kept breaking after breaking API changes in newer versions. I didn't know Haskell at the time and I found the config syntax a bit different, but no more challenging than the Lua I was using to configure Awesome.
You're not wrong, Walter, you're just an asshole
&gt; i3 i3 is batteries included Awesome. The tiling functionality is a thing of beauty, have about 20 terminal windows open on my laptop, split into a quadrant, each panel consisting of several vertically tabbed terminals. Among other things this is extremely handy when managing a bunch of remote servers, can pipe in server load status to each terminal's title bar and get a nice bird's eye view of what's happening in the data center. Floating dock for Skype, Rhythmbox, Gnote, etc. is also a serious nice-to-have. Status bar takes Conky input so you can get your CPU/GPU temp, load average, date/time, and currently playing song eye candy. It's really absurdly good, hotkey move windows to other monitors/desktops (or automatically open apps on certain virtual desktops). p.s. I know neither Haskell nor Xmonad, sorry for crashing, just had to throw out the i3 thumbs up. p.p.s the tiny "xcalib" screen inversion package is also a computing life changer of sorts if you have a strong aversion to white backgrounds.
Most of the things you mention are common features in tiling window managers. But I appreciate you sharing, anyway!
https://ghc.haskell.org/trac/ghc/wiki/WorkingConventions and https://ghc.haskell.org/trac/ghc/wiki/Building 
A coworker of mine a few years ago used it. He was interested in Haskell so it was probably his version, but for me it just seemed like an efficient way to use a computer. Later I could probably say that this exposure was what eventually got me interested in Haskell though. I say later because my literal first exposure to Haskell was cursing its name and spending 3 hours trying to figure out how to configure XMonad. 
I use it because unlike previous tiling window managers I've used, it is maintained, and works nicely with desktop environments.
[Generalized newtype deriving](http://www.haskell.org/ghc/docs/latest/html/users_guide/deriving.html#newtype-deriving) 
This was my first version, but the program was very slow. As I blogged [here](http://lpuppet.banquise.net/blog/2014/01/30/version-0-dot-11-dot-0-now-with-a-full-serving-of-unsafecoerce/), this was probably not the reason, and this will go away in the next version.
Current NoSQL is about scalability and availability and they achieve this with a distributed architecture, giving up consistency to have eventual consistency (hence losing the C in ACID). There is also a focus on map/reduce operations, since they can be done in a distributed fashion. Had COBOL such features? I think that NoSQL systems would outperform COBOL solutions simply by throwing more machines into the problem. (Hence why Google and Facebook developed software like MapReduce and Cassandra, instead of using some COBOL thing)
What would happen if you nested them?
I see. Anyway, what I suggested is still boilerplate, so hopefully one of the other answers will help you out with that.
Thanks, works like a charm !
I can't imagine how wrapping and unwrapping could impose a significant performance penalty: newtypes share their representation with their underlying type and the wrapping and unwrapping is essentially just cosmetic.
/u/kamatsu and /u/edwardkmett both mentioned that they used to use XMonad, but switched to a Mac. My question to both, and anyone else that also made this switch: why?
I'd assume it'd be a bit like lexical scope, where the inner scope shadows the outer scope.
You can shard a COBOL table. Mainframes have been doing that forever.
COBOL doesn't exactly have the facilities to do a generic map/reduce framework. It'd have to be hand coded each time which is why the language sucks. Map/Reduce is a pretty small subsection of what you get out of NoSQL though. My general point was people are essentially just reinventing the way data was before SQL.
Why don't we have a suitable Haskell impementation of CPP that's portable and does what we need? I seem to remember there being one, hpp perhaps? Is there any reason we have to rely on the local C compiler's CPP?
&gt; There's not even actual AArch64 development SoCs on the market right now What's in the iPhone 5s then? [It's definitely a 64 bit ARM...](http://www.phonearena.com/news/Apple-iPhone-5s-performance-review-CPU-and-GPU-speed-compared-to-top-Android-phones-benchmarks_id47739#cpu)
I use XMonad because I fool around in Haskell. Otherwise, ghc dependency for compiling the config would be unacceptable to me for a lightweight wm, but since I have it anyhow, doesn't bother me. And it is awesomely configurable, and fairly simple.
I said development SoCs, i.e. dev boards which replicate the functionality you can *basically* expect for similar production chips - the iPhone 5s does not qualify (least of all because it's hardly a full development platform similar to Linux, suitable to port GHC onto a server - which is what the AMD ARMv8 announcement was relevant to.) Furthermore, the AArch64 backend which exists in LLVM is [not the same one used in the iOS compiler](http://lists.cs.uiuc.edu/pipermail/llvmdev/2013-September/065480.html), and it's unclear when that will be merged upstream. So we can't even target that yet.
There are some gems in the "Zen of Python" which I agree with. One is: "In the face of ambiguity, refuse the temptation to guess". Name resolution can and should be unambiguous. The current solution of using a qualified prefix (`V.` or `P.`, etc) is IMO better. An even better solution would be to generalize common operations. For example, use "fmap" instead of "map" (or one that generalizes to monomorphic containers too). Use Foldable for "foldr" (ditto). 
That container demo looks seriously good. I think I need to switch to i3 for a while to try it out.
I don't prefer it because it is configured in Haskell, but it is a plus.
Or perhaps local imports? foo = bar where import Data.Vector Agda has this, it's kinda nice.
I have ghc and hugs installed on my RPi. My senior design group are going to be using Haskell on the RPi this semester for our project. I haven't run anything besides some simple code, but [this guy](http://alenribic.com/posts/2012-08-06-running-haskell-on-raspberry-pi.html) wrote a nice blog post about making a simple ping server on the Raspberry Pi.
You don't have to go through the `Free` monad if you are comfortable defining the complete data type. There is an advantage to using the free monad approach, which is mainly that if you separate out the base functor, then it makes it easier to decouple different terms and their interpreters. See this [StackOverflow question](http://stackoverflow.com/questions/21416561/data-types-a-la-carte-vs-nested-freet-transformers) and [this paper](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.101.4131) for more details.
VisualBasic and Pascal have that :-)
I think these things cause general mass confusion around open-world typeclasses, though.
wrt [2]: Has there been any effort expended in adding global register variable support to LLVM?
One of the reasons I left XMonad (albeit, eventually for XFCE) was the lack of a tabbing functionality (although it may have just been something I couldn't figure out).
I just use Xmonad because it tiles, and it doesn't crash. Even if you don't touch the configuration (I haven't modified mine in years) it's worth using just for the stability.
I have no idea of this is possible, feel free to ignore me, but could you write a rewrite rule for `m &lt;== k` (with appropriate type/class annotation to make it only apply to the monad instance of interest) to force GHC to generate the non-sharing version?
Tried it, fussed with configuring it, realized I just wanted it to work like ratpoison anyway.
That's great to hear. The less external tools we need to rely on the better.
I met someone that did, but he didn't do much Haskell. I do not see myself using it. 
It is probably useless here, but not in the general case. See [this comment](http://www.reddit.com/r/haskell/comments/1mqni1/edward_chands/ccbt8af).
[The New Parallel I/O Manager](http://www.reddit.com/r/haskell/comments/1k6fsl/mio_a_highperformance_multicore_io_manager_for/) will be in GHC 7.8.1. Totally looking forward to this!
Yes you are right! But like you said "GHC 7.8 branch is created" sounds incomplete (and hence incorrect). Things like "...is created here/when etc" are just fine. If I were the OP I would have probably said something like "FYI a GHC 7.8 branch has been created".
That's really nice, thanks!
I love tiling window managers, and I love that I can script Xmonad using Haskell because it is written in Haskell. I love how easy it is to figure out. But I don't like to fiddle around with my desktop environment anymore. I used to like that kind of thing, back when I was a student. But these days I just don't have time to screw with every little detail. I just want a stable desktop environment that works out of the box without me having to do any configuration at all. For now I just use Ubuntu Unity with just a few key-bindings changed around, and I configure it with their graphical control panel apps. I would like to see someone build a Haskell graphical widget toolkit built on top of a functional reactive programming API like Ractive Banana. It should be like what GTK+ is to C, what Qt is to C++, but instead written in pure Haskell. Once a project like that takes-off and becomes popular, then people can build desktop environments around it. You could have the "Haskell Desktop Environment" which uses the "Haskell Graphical Widgets Toolkit" to do graphics, and Xmonad as the window manager. The hardest part would be to get large applications like Firefox or the Chromium browsers, which link directly to GTK+, to work against the Haskell graphical widgets toolkit. You would probably need a GTK+ bridge library, a library that exports all the symbols of the GTK+ libraries but internally they call the Haskell widget toolkit library routines, then get Firefox or Chromuim to link to it, which would fool it into running smoothly in the Haskell desktop. 
You're welcome!
You could have to import the module normally (presumably `qualified`). Then the local "import" would just change or remove the qualifier you used. Even without being allowed to introduce ambiguous names by unqualifying modules locally, you'd get away with many fewer prefixes in pieces of code that don't use two conflicting imports simultaneously.
I personally think a better idea might be to do what Rust does (but better of course). Move things like foldl/foldr into Foldable, implement accumulate etc in terms of those and then power everything with type classes. So, for example, List.empty, ByteString.empty and the such don't exist anymore and everyone starts using mzero/mempty instead. Then figure out what strange types can be generalised over for things like length, head, tail etc. A lot of imports become instances instead, and Existential types suddenly becomes so much cooler because you can have lists of differing structures. A common interface for many different structures makes swapping them out a lot easier and there's less documentation to read. Also fixes missing monad implementations (grr, these are annoying) in some libraries, since these can be generalised automatically over the non monadic variants. (Yes, you lose some static typing, but I think we'll all agree that having to append BS./List./M./IS./S. everywhere gets kind of old...) Edit: Interestingly, Data.Set seems to support a lot more folds through Data.Foldable than exported from Data.Set... that's a little unintuitive in my opinion.
No, and I don't know if they're interested in adding it any time soon, or ever. It's a relatively rare extension all things considered and from the looks of it, seems like a relatively large change to LLVM. (I'll also go ahead and say I personally have absolutely zero interest or time to dedicate to such a project.)
My very amateur understanding is that lens's `Plated` is an encoding of the usual Scrap Your Boilerplate stuff in a more lensy API, but there's otherwise not a big difference. But, as I said I'm very amateur, and would love to see someone who knows what they're talking about addressing your question. I think it's a very interesting one.
I love this Little Schemer style approach to teaching lenses! I find myself quite comfortable with `lens`, but I was still gripped throughout this tutorial. Great work. Edit: I just saw "But for now, go get yourself a peanut butter and marmalade sandwich." so I see that similarity isn't just a happy accident ;)
I liked the concept, but changed to Awesome because had too many interference between GUI apps and Xmonad. Apparently the XMonad position is that those apps don't implement whatever model correctly, but it really doesn't help me use those stuff (Java Swing apps, some gnome/kde stuff too what I don't remember).
Just curious - is mac's default wm behavior so good you don't need tiling wms anymore? Or what is the reason? Thanks.
`XMonad.Layout.Tabbed` from XMonadContrib provides a very easy to use tabbed interface.
The thing you have to keep in mind is that XMonad is more of a library you can use to write your own WM (and a DSL for doing that) then it is a fully-featured WM in and of itself. For example, I added clickable workspaces to my config with one function: wrapWorkspaceClickable :: String -&gt; String wrapWorkspaceClickable s = wrap ("^ca(1, xdotool key alt+" ++ show (romanToInt s) ++ ")") "^ca()" s Using the fact that dzen supports actions on click. So stick a call to that function wherever you want a given workspace to be clickable (like in the pretty printer) and you've got your clickable workspaces. Of course, you'll need to modify it to get the correct key combinations to fire for your workspace naming scheme, but that's the idea. As for containers, I'm not familiar with it, but it sounds like something you could add with the right layout. For example, I looked into and liked bspwm's default tiling mode which looks like windows put into a tree with some nice manipulation tools. I'm writing up the layout that will provide that feature. 
These days a *huge* amount of patterns are codified and made first class by [lens](http://hackage.haskell.org/package/lens-4.0), including functoriality, stateful update, traversables, pattern matching, generics ... There's a huge amount in there. In some ways it is like a programming methodology all of its own!
You might want to take a look at [Data.Data.Lens](http://hackage.haskell.org/package/lens-4.0/docs/Data-Data-Lens.html). It provides traversals similar to SYB. 
Note that `AutoDeriveTypeable` is important in conjunction with `PolyKinds`, because it will also generate instances for `Constraint`-kinded things i.e. classes. The manual alternative I believe is standalone deriving. (Also I think `NullaryTypeClasses` should've just been folded into `MultiParamTypeClasses`.)
I feel like this is the sort of thing that would benefit from first-class language support. Of course, wantonly adding syntactic sugar wouldn't really be in the style of Haskell, but maybe the F# compiler front-end could admit something like this.
I know. I've just scratched the surface of lens, and yet any time I'm working in another programming language I find myself missing a lens library! I was just thinking that whether or not you access these patterns with lenses or the "common" Data.Traversable shouldn't matter for how they are used, so if I learn to use them with Data.Traversable I can later on leverage that power with (lens) traversals as well!
Yes, but then I run into tons of crap with ARM instructions and that was it :) I get stuff to compile but not work. Or is that normal? I think to understand why it does what it does I need to understand quite a bit about what GHC does internally and ARM as well.
There is a lot of code in `lens` for dealing with generic programming and traversals. We piggyback on the existing `Data.Data` and `GHC.Generics` infrastructure though. `Data.Data.Lens`, `GHC.Generics.Lens` and `Control.Lens.Plated` include a port of Neil Mitchell's `uniplate` library. These give a lot of generic programming functionality. When you use `Data.Data.Lens`'s `biplate` and `uniplate` traversals you can use `Data` and `Typeable` to go find say, every `Salary` in a `Company` and increase it by 10, just like with `Data.Data`. &gt;&gt;&gt; let foo = ("hello","world",["generic","programming"],()) &gt;&gt;&gt; foo &amp; biplate %~ toUpper ("HELLO","WORLD",["GENERIC","PROGRAMMING"]) &gt;&gt;&gt; foo &amp; partsOf biplate %~ (reverse :: String -&gt; String) ("gnimm","argor",["pcirene","gdlrowolleh"],()) The implementation of `biplate` and `uniplate` include constructing a hitmap based on the target type, this gives you the ability to avoid "walking everything even irrelevant things" even when using generic programming with `lens`. The `Plated` combinators have been upgraded to work with arbitrary traversals, so they need no longer be limited to the structural updates. More or less `lens` is trying to do more in a slightly different area, that happens to overlap with `SYB` and generic programming. To your comment about speed, most of the uses of `lens` can avoid those generic programming combinators and thus enjoy a great speed benefit, yes.
Shot in the dark, but might this be a use case for [reflection][1]? I recall edwardkmett saying that one of the motivations was being more efficient than `Reader` wrt sharing, but I don't remember the details, or if this particular case is a thing that it solves. (I also have no idea whether your code can work with `reflection` instead of `Reader` at all.) [1]: http://hackage.haskell.org/package/reflection
The patterns are in some sense abstract concepts, whereas lens and co. are first class values implenting those abstract concepts.
Actually, the dynamically sized stacks in GHC *are* a source of slowdown. The Rust guys just care about this penalty more than most of us Haskellers do. We also do benefit from it, though, since it means most threads can start with much smaller stacks. This is partly where GHC's reputation for having very scalable threads comes from. The sequential performance of a single thread can still suffer a bit without most people caring.
I'm really glad to hear that! Perhaps unsurprisingly, I found it really fun to write it as well. That style is just really fun.
Hm, I'm not entirely sure, but it's definitely an intriguing idea. Read through the [tutorial for `reflection`][0] and it appears that the first part doesn't help, because you still have to carry around a function parameter -- the source of our problems in the first place. However, it looks like this function parameter can be [replaced by a type parameter][1] and that's where things start to get interesting, because GHC will likely lift type applications around `let` statements. [0]: https://www.fpcomplete.com/user/thoughtpolice/using-reflection [1]: https://www.fpcomplete.com/user/thoughtpolice/using-reflection#turning-up-the-magic-to-over-9000
I don't like peanut butter and marmalade sandwiches. Can I still use lenses?
As a Haskell-newbie, I could understand the concepts in the tutorial, but I find it kinda hard to see the benefits. Something like "flatten a list" or "return first element of list/tuple" is easy enough without lenses. The JSON example seems interesting, though. I'm curious how elegant error handling would be for this example. (Let's say there is no "version" property on some objects)
I'm pretty sure this is not accurate at all. First off, the "hot-split" problem described in the OP's link, in Rust or Go, happens with a code path like: // performance-sensitive loop for(;;) { ... code ... func(); ... code ... } where 'func' will allocate stack space. In the optimal case, the current stack chunk will be big enough to service that. But if it's *not*, and the boundary of the stack is hit in the middle of this loop, `func()` must allocate a new stack segment, link it into the list of segments, and then use it. Then upon return it must immediately deallocate it. For a hot loop like the above this is pessimistic in terms of performance, and it's relatively unpredictable as it depends on where the stack boundary lies per iteration. That means changing code is sensitive since you can easily cause the boundary to move around. Why? [Because it uses a tricky instruction sequence to dynamically link and then unlink a stack entry, precisely when needed](http://gcc.gnu.org/wiki/SplitStacks). The core of this is the `__morestack` routine which will expand things on entry, and it sets up a clever epilogue to deallocate the chunk. This is just how GCC does it for C code when compiled with `-fsplit-stack`, and how Rust used it (essentially.) Second, the situation is completely different with GHC. For us, the stack is (perhaps confusingly) a heap allocated data structure. We never immediately deallocate them - this naturally occurs as a part of a young-generation GC or an old-generation GC. This 'hot split' problem doesn't occur precisely because our allocation is stupidly fast, and the deallocation is put off until later. In other words, the GC amortizes this cost, where Rust has a much more imperative and memory-sensitive model, and immediately pays for it. And finally, these stack chunks as we call them *are a performance benefit for GHC.* Why? Well, GHC's stacks have *always* been growable. It's just a matter of how they grow. Previously, when GHC needed to expand the stack to grow it, it merely had to double the size of it and copy it to create a new one. For large stacks that gets extremely expensive quickly, and it's a source of bugs to manage as you must forward 'old stacks' to the new copied stacks, basically with a forward pointer. The only way to combat this is making the default stack larger - but this impacts L1/L2 cache. Stack chunks are a win because the GC doesn't have to traverse or copy unmodified parts of the stack, and can selectively copy only the things that need it during GC. When overflowed, you just allocate a new chunk. The chunks are then linked together as expected. This means we no longer have to worry about forwards for old objects (brittleness gone,) and it's cheaper since we save time by not copying or traversing the whole thing. In fact it's quite a nice win across the board. There are more details here: https://ghc.haskell.org/trac/ghc/blog/stack-chunks Ultimately Haskell and Rust are just different languages with vastly different implementations. Stack chunks really do operate differently in Haskell than they do in Rust. So I don't really think the problems the Rust team encountered are as much of a concern as we might think. &gt; We also do benefit from it, though, since it means most threads can start with much smaller stacks. This is partly where GHC's reputation for having very scalable threads comes from. The sequential performance of a single thread can still suffer a bit without most people caring. We could still scale to a very impressive amount of threads before we had stack chunks. The core reasons we can actually scale with so many threads are different: safe-points and the scheduler tie into allocations, allocations are fast and common, the scheduler is fast, and the overhead of an individual green thread is incredibly small (less than 20 machine words.) A real OS thread is vastly more expensive to manage, stack chunks or not.
That's a great principle for sustainable language development, but I can't help but feel like the functional world needs its version of PHP - quick, dirty, unsustainable, but practical for common real-world use cases.
&gt; You will collaborate with our innovation IT team based in Trento, Italy and will work either on site or remotely. I will do no such thing. Also, this is obnoxious and spammy. Keep this shit off r/haskell. If you have a job posting, link to your site.
Prev discussion http://www.reddit.com/r/haskell/comments/1obpxb/a_little_lens_starter_tutorial_school_of_haskell/
Ah, I haven't thought about manipulation in this sense. thanks!
My guess: because xmonad isn't working nicely in Mac OS X ;-)
&gt; Experienced haskell developer &gt;short term contract of three months. &gt;Working schedule: Full-Time Choose 2. No one will quit their paying jobs to get 3 months work and then hang in dry. 
Yes, definitely good advice. Lenses are a generalisation *on top of* existing abstractions, and it's much more accessible to start with the simpler (and more common) versions.
I would say that what you have described is Scala. It's definitely piled feature on top of feature in its design, but as you noted, the benefit of this is a lot of practicality.
But... we already have Scala.
What kind of first-class language support is it you miss for lenses? They work very well as they are, I've found. The only change I can think of would be to make the default accessors lenses instead of getter functions. Besides that, lenses already have all the support they need. (In GHC, at least. I suppose lens libraries for other platforms might be a little weaker.)
Aww come on. There aren't *that* many Haskell jobs that posting an ad here is unwelcome, surely?
This is just called freelancing. You generally get paid more to compensate for having to spend time finding work.
Posts about how your company is hiring is fine. But no one on this form wants to see an ASCII-ized laundry list of skills, and I certainly don't for a company I've never even heard of while I'm already employed
That takes a little more work -- you have to combine two separate layouts and add some keybindings to make it work nicely, but it's very doable. My coding setup is gvim with a terminal below it, and a tabbed set of documentation (web browser and otherwise), all on the same workspace. Looks neat, works better.
see the classy prelude. one problem is, that type errors get pretty beginner unfriendly. (i still think there should be a somewhat more generalized prelude by default and another beginner prelude with one pragma. but that will never happen i guess).
&gt;make the default accessors lenses instead of getter functions Hell yeah. Really all I'm complaining about is the syntax, and how the type errors look.
Scala's the closest thing, but I find it WAY too verbose. F# has a lot of nice stuff for plumbing, things like type providers (essentially type system plugins) but deploying .NET or Mono is another can of worms.
Well, surprisingly you don't seem to use Applicatives in your configuration, in addition, your config is quite small. Well, I am only glad if it suits you! :) 
I worked a lot with Tikz (a very common LaTeX library doing similar things in C) and wonder if it's worth to learn an alternative. After looking at your examples and reading the first two tutorials I have to admit there seem to be some really good arguments for diagrams. Pro: - it Haskell, that's always good - since that it is much more powerful when some of the maths is done while drawing the picture - there definitely are some mathematicians in your team. Otherwise you wouldn't have come up with the idea of differentiating points and vectors. As a mathematician myself I love it. Cons: - the Syntax doesn't look very Haskell-like. The #-thing is nice but in Tikz it looks more natural to me: \draw [thick, dashed, red] (0,0) circle (2cm); I will watch this very carefully. Have you ever worked with Tikz?
skew mentioned the hysteresis used by GHC to avoid the issue mentioned in the first bullet point in that email. GHC also isn't affected the second two issues (which are really the same I think) because of the way stacks work in a Haskell program. As I understand it, there are two (kinds of) stacks in a running program, the Haskell stack (one per Haskell thread) and the C stack (one per capability/OS thread, so just one if not using the threaded runtime). The C stack(s) are set up in the normal way as for a plain old C program; they aren't especially small, or allocated on the heap, or anything like that, and on x86 the `esp`/`rsp` register points to the top of the C stack. The Haskell stack is allocated on the heap as a linked list of small chunks and during the execution of Haskell code, some other register (I forget which) is used to track the top of the Haskell stack. For unsafe FFI calls and LLVM instrinsic fallback function calls, GHC simply emits a `call` instruction, using the C stack. This is okay because GHC's cooperative multitasking triggers on heap checks that appear only in Haskell code. There's no way for a single capability to ever be in the middle of two unsafe FFI calls, so we just need one "large" stack (the C stack) per capability. My guess is that Rust uses the C stack (as in, a stack pointed to by `esp`/`rsp`) for its native code, so it needs every Rust-thread stack to be large enough to meet the space requirements of FFI/LLVM intrinsic function calls. (Using the hardware stack pointer register does have some big advantages: you can use the `call` instruction to do a native function call and get smaller code size *and* take advantage of the hardware branch prediction for `call` (the processor is much better at pipelining execution across a `ret` than an arbitrary `jmp *rax`). Probably the gain from doing so exceeds the benefit of segmented stacks. As I understand it, though, GHC currently has other reasons that it can't put the Haskell stack on the C stack anyways.)
What keeps me away from F# is that frankly I pay all the price of ML syntax and get none of the benefits of ML functors, so I can't abstract over most of the things I need to abstract over in my code. 
Essentially, it's more concise than deep pattern matching.
Well it's a great idea, so I hope it happens.
I'm not sure what's the attraction of ML-style functors over, say, OO-style interfaces + records. Do you have any particular use case in mind?
Correct. [ctrl-f 'buffer' in this message to confirm that assumption](https://ghc.haskell.org/trac/ghc/changeset/f30d527344db528618f64a25250a3be557d9f287/ghc) &gt; Perhaps rust programs frequently stack-allocate large data structures? Yes, this is a pretty typical programming style for Rust - lots of things in seem to be stack allocated (even closures,) and you pass pointers around or carve up parts of the allocated memory to hand out - like when you do intrusive-styles of C programming. &gt; As for the rest, I'm pretty sure rust programs call into C functions more often than most Haskell programs. Possibly (it'd be interesting to see,) but the bigger issue is they share the stack with C. GHC keeps a separate Haskell stack aside from a C stack for foreign calls, as rwbarton said. This makes foreign calls much harder for Rust with these dynamic stacks since it needs to be big enough.
Yes, this is all of this is correct as far as I understand, including the bit about Rust sharing its stack for foreign calls (so they're in a tougher spot here.)
ML functors can be used for many of the same roles that we can use typeclasses for, but they have other uses. I'll ramble a bit and hope I get the idea across. =) One issue is that the 'OO-style interfaces and records' approach as limited by the common language interface of the .net ecosystem doesn't give you the ability to talk parametrically about parameterized types in such a way that you can write code that is parametric in the monad. e.g. You can't define a 'class' for `Monad` properly in F# in such a way that you can properly abstract over the choice of `Monad`. Now, they give you hacks that let you pretend by inlining untyped code into the call site and expanding it, but you are stuck pretending, a fact that is made much clearer to you the moment you attempt recursion under those conditions and the whole thing breaks. What ML Functors are good at is making global system choices. I'm going to compare them to Haskell typeclasses rather than traditional OOP classes for a bit. With a typeclass I can make up a new instance of something and enable new behavior on top of what I had before. However, if I realize after the fact that an entire module should be parameterized over, say, the choice of random number generator backend, I have to a.) rewrite the entire module to add a type parameter somewhere and plumb it through b.) revert to 'OO-style interfaces and records' and plumb it through and lose the pretty mathematical style or c.) I can parameterize the module I'm working on, on a module signature for a module that supplies my RNG functionality. The benefits there is that it lets me 'curry out' the parameterization for the entire module and all classes and data types in the module. I can actually write code this way in Scala today. One of the reasons I miss ML functors in Haskell is that I occasionally find that if I didn't guess right about what things users will want to abstract over, I have to do a big global refactoring to rebuild the API to make it possible for users to plumb it in. Haskell typeclasses work well when you can consistently 'point' the selection of the instance via the positive or negative type arguments. Consider, say, Monoid. class Monoid a where mappend :: a -&gt; a -&gt; a mempty :: a mempty introduces a monoidal value in positive position. mappend preserves that selection. On the other hand consider Eq class Eq a where (==) :: a -&gt; a -&gt; Bool There we strictly consume in negative position. Now, we could make a class that had both of these in it. class (Monoid a, Eq a) =&gt; EqMonoid a but inference for `mempty == mempty` is going to fall down on the job. It'll be forced to turn to defaulting, which is a grim scenario indeed. ML Functors win over typeclasses in scenarios where they can 'pick the point in the existential' when you are dealing with cases like that. Haskell typeclasses win over ML functors (and the weaker record-passing for dictionaries) when the selection is obvious and you don't want to plumb it through by hand, which is about 90% of the time.
That's very interesting! Thank you so much for the write-up.
I've similarly used QuickCheck to generate data that is used for testing beyond simple properties. The test suite for my git-repair project generates arbitrary Damage which is applied to files in a git repository, and then requires that git-repair manage to repair that well enough for git-fsck to pass. data Damage = Damage DamageAction FileSelector data DamageAction = Empty | Delete | Reverse | AppendGarbage B.ByteString | PrependGarbage B.ByteString | CorruptByte Int Word8 | ScrambleFileMode FileMode {- To select a given file in a git repository, all files in the repository - are enumerated, sorted, and this is used as an index - into the list. (Wrapping around if higher than the length.) -} data FileSelector = FileSelector Int So have similarly found myself using sample' and wishing there were a better interface. Not only is it clumsy, but the random data generated is biased. For example, fmap head (sample' arbitrary :: IO [Int]) is always 0, 1, or -1. That makes sense when quickcheck is exploring a space with many tests, but not when sample' is only returning 11.
It has seemed to me that "merely" expanding the Typeclassopedia wouldn't be enough to be a book, but a Typeclassopedia + how to use Lens in practice might be. Could probably find another couple of topics to cover too without trying too hard. (An examination of how types can help guarantee practical properties wouldn't be amiss.)
Pet peeve - don't do this: -- (passwords will be quoted, and generated without "" marks or control chars) ... , T.concat ["\"", password profile, "\""] Sure, it may only be for debugging now, but code has a habit of lingering. And then all it takes is one user with a password like `foo","bar` to break your output format. Just do it the right way - use `show (password profile)`. It already adds the surrounding quotes, and does the escaping for you. It's even shorter! (and if this is really supposed to be CSV, just use a CSV printer)
Great read! I got inspired and added a pull request to add samplesOf :: Int -&gt; Gen a -&gt; IO [a] https://github.com/nick8325/quickcheck/pull/9/files
Gen exposes its constructor, so you can always make your own sampler: import Test.QuickCheck.Gen (Gen(MkGen)) import System.Random ( split, newStdGen, StdGen ) import Control.Applicative ((&lt;$&gt;)) import Data.List (unfoldr) samples :: Gen a -&gt; StdGen -&gt; [a] samples (MkGen m) rnd0 = zipWith m rnds [0,2..] where rnds = unfoldr (Just . split) rnd0 samplesIO :: Gen a -&gt; IO [a] samplesIO g = samples g &lt;$&gt; newStdGen And now you can just do  take 10 &lt;$&gt; samplesIO (arbitrary :: Gen Int) [0,-1,1,6,-27,8,-62,-91,762,-1452]
I always get enthusiastic when I see a job post on /r/haskell, but I feel like a large fraction of them are spammy. I wonder why?
Regarding error handling and nested pattern matching, this is the problem that the `Maybe`, `Either`, `MaybeT`, and `EitherT` monads solve. I'll use `Maybe` as the example, but the other ones are not much different: safeDivide :: Double -&gt; Double -&gt; Maybe Double safeDivide x y = if y == 0 then Nothing else Just (x / y) uncons :: [a] -&gt; Maybe (a, [a]) uncons xs = case xs of [] -&gt; Nothing y:ys -&gt; Just (y, ys) -- Here's how you combine functions that can fail: example :: [Double] -&gt; Maybe Double example xs = do (y, ys) &lt;- uncons xs (z, _ ) &lt;- uncons ys safeDivide y z The `example` function divides the first element of a list by the second element, returning `Nothing` if the list has fewer than two elements or if the second element is zero. Here's how it plays out: &gt;&gt;&gt; example [] Nothing &gt;&gt;&gt; example [1, 2, 3] Just 0.5 &gt;&gt;&gt; example [1, 0, 3] Nothing Notice how I combined three function calls that could fail (two calls to `uncons` and one call to `safeDivide`) into a single function call that could fail. This is the standard Haskell idiom for reducing N error checks into a single error check. Notice how `example` can itself be used within a larger `Maybe` monad, too: contrived :: [Double] -&gt; [Double] -&gt; Maybe Double contrived xs ys = do m &lt;- example xs n &lt;- example ys return (m + n)
Monad tutorials were never the problem. The stupid monad metaphors were the problem.
&gt; emailDomains :: [(Int, Gen Text)] &gt; emailDomains = map (\ (i, t) -&gt; (i, pure t)) [ &gt; (50, "yahoo.com") &gt; , (40, "hotmail.com") &gt; , (30, "aol.com") &gt; , (20, "gmail.com") &gt; , (10, "sbcglobal.net") &gt; , (8, "yahoo.co.uk") &gt; , (6, "yahoo.ca") &gt; ] Eeek. No, please, don't make your random profile generators generate real email addresses. They will find their way into a test database somewhere. This test database will get used by a careless tester who runs the application with a live email sending configuration. And you will spam the addresses, many of which will turn out to correspond to real people. Instead, use the reserved domain names like example.com and example.org.
Lens really shine when updating deeply nested structures. For example, let's say that I have a `Molecule`, which is a list of `Atom`s: {-# LANGUAGE TemplateHaskell #-} import Control.Lens import Control.Monad.Trans.State data Point = Point { _x :: Double , _y :: Double , _z :: Double } data Atom = Atom { _point :: Point , _elementType :: String } type Molecule = [Atom] makeLenses ''Point makeLenses ''Atom Now let's say that's my running state and I want to update the `x` position of the third atom in my molecule. All I have to write is: example :: State Molecule () example = element 2 . point . x += 1.0 If you try to implement that without lenses you will become a lens convert immediately. See also [this lens post I wrote](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html) which gives even more clever examples of things you can do with lenses that you can't do with simple record syntax.
Before reading further: avoid "herp derp" stuff. It has connotations for certain groups that aren't desirable.
 test :: Int test = (\Bar -&gt; 1) (Bar 1) I don't know what that's supposed to mean.
I had no idea there was a meaning, thought it was like foo, I apologize
that was a typo it should have been fixed. It should be &gt; test = (\Bar -&gt; 1) Bar 
Aha, yes, I just saw that you changed it. Ok, now I'm still not sure how the end quote relates.
Fair enough, but what about the material? I care less about the quote, and more that I understand the other stuff correctly
The content is fine.
Woot!
Where did the quote come from?
https://twitter.com/HaskellTips/status/429369349871124480
Well at least I'm not alone in being oblivious that there was meaning. 
See also: * [Scrap your type classes](http://www.haskellforall.com/2012/05/scrap-your-type-classes.html) * [Haskell Anti-Pattern: Existential type class](http://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/)
I was unaware as well until a fellow Haskeller let me in on the possible connotations. Given the abundance of stock names in the CS domain, it's no great loss to avoid them.
Not sure how the existential type class anti-pattern relates but the first does. 
The second post also shows how to translate (some) type classes to records of functions
ah Probably most relevant is http://people.csail.mit.edu/dnj/teaching/6898/papers/wadler88.pdf
The quote isn't anything significant. Its showing that both C++ utilize implicit parameter passing, albeit for different reasons and in different ways.
The accessor/traversal side of the lens world looks pretty good, but some way to combine the advantages of prisms (generality, composition) and native patterns (syntactic privilege, ability to bind names) would be lovely.
Why not embed that tweet in the post instead of just giving an unattributed quote?
Sure, how do I do that?
re: dynamic stack overflow checks: https://ghc.haskell.org/trac/ghc/ticket/8703
Adding Linux support was a lot of work. Adding Mac support should be not too much additional work, since mechanism for build scripts are in place now. It is not clear, if I will have time for it, though. Maybe somebody else can take over. Sources contain all neccessary tools and binding generation configuration mechanisms.
https://dev.twitter.com/docs/embedded-tweets
I still maintain that it's not an antipattern; existentials are quite handy in some case. Previous discussion: http://www.reddit.com/r/haskell/comments/18rcc6/easy_in_oop_harder_in_fp_list_of_animals/c8hd33p
I agree. I mainly consider it an antipattern if the record solution works better.
So monad transformers are not only unwieldy to use, but also terribly slow? Wow, Haskell never ceases to "amaze" me. It's like a reinvention of the (imperative) wheel, except the new wheel is crooked and turns like 6 times slower.
Warning: This post got a little longer than I intended it to, but it shouldn't be too tough to read. I don't expect you to understand what every operator does and so on. It is hopefully not necessary to get the gist of the idea. Especially the lens library is known for it's massive amount of operators, which are very nice to have, but it takes a while until you learn them well. (And I certainly wouldn't expect someone halfway through LYAH to have any idea of how they work.) The equivalent Haskell code: data Person = Person { _name :: String, _age :: Int } -- We need this next line to tell GHC that we want lenses for the Person makeLenses ''Person p = Person "John" 57 p &amp; name .~ "Dave" or even: p57 = \n -&gt; Person n 57 sameAge = map p57 ["John", "Dave"] Is there really any more apparent pain? The only additional cost in this example for using lenses to do this is the TH line where we say we want lenses. ---- (NB: When I in the following text talk about "lenses", I usually mean lenses, prisms and traversals, which are built on similar principles and work really well together, so much so that they are all included in Kmetts excellent lens library.) But then while these basic operations with lenses are nice, it's not where the magic is. This will be quite a large example, but most of it will be setting up the data required, so don't fret. The following bit is written in literate Haskell so you should be able to execute it. Imagine for this example we are writing some sort of social network, where users enter things into their profiles. First, we need to enable TH for convenience, and we import Control.Lens. &gt; {-# LANGUAGE TemplateHaskell #-} &gt; &gt; import Control.Lens Then we define a Location data type, that stores a country and perhaps a city. Not all users will want to store a city, so doing so is optional. &gt; data Location = { _country :: String , _city :: Maybe String } deriving Show We also need to create a Profile data type, that stores an age, a birth year and perhaps a location. Not all users will want to store a location at all! &gt; data Profile = Profile { _name :: String , _birthYear :: Int , _location :: Maybe Location } deriving Show Finally, we need to create lenses for these types. This is where the TH comes into play. We don't *need* Template Haskell for this, but it makes it very convenient. &gt; makeLenses ''Location &gt; makeLenses ''Profile Then, for the sake of readability, we will define a type for profile IDs, which will just alias to Int. &gt; type Id = Int Next on our list of things to do is to create a bunch of profiles. Dave has entered both a location and a city, so he has all fields filled in. &gt; dave = Profile "Dave" 1967 (Just (Location "Brazil" (Just "Braslia"))) John feels a little more paranoid and entered only a country. &gt; john = Profile "John" 1967 (Just (Location "Canada" Nothing)) Kate really values her privacy, and didn't enter a location at all. &gt; kate = Profile "Kate" 1976 Nothing Finally, Sarah, Eric and Anna are just variations on a theme. &gt; sarah = Profile "Sarah" 1983 (Just (Location "South Africa" (Just "Johannesburg"))) &gt; eric = Profile "Eric" 1991 Nothing &gt; anna = Profile "Anna" 1974 (Just (Location "Denmark" Nothing)) We have a bunch of people now! Let's make a list of their information connected to the profile IDs. &gt; people :: [(Id, Profile)] &gt; people = [(27, dave), (318, john), (121, kate), (9823, sarah), (743, eric), (483, anna)] Now, imagine we want to make a list of all the cities people have entered. First, try to figure out what you would do in a non-functional language like Python. Perhaps it would look something like def cities(people): for id, profile in people: if profile.location and profile.location.city: yield profile.location.city In Haskell, we could do something crude with pattern matching like import Data.Maybe cities = do (_, Profile _ _ loc) &lt;- people Location _ cit &lt;- maybeToList loc maybeToList cit (Please note that this is clearly not the optimal way to do it. I just put something together and I have done no attempts at refactoring it to make it better.) What this does is extract each location with pattern matching from the list of people, and then it uses the `maybeToList` function and repeated pattern matching to pick out the cities, and return those. Even though this can probably be made a little better, it's still clumsy. And what's worse, you have to write one of these loops for every part of the profile you're interested in! We can do better with less effort. First, we create a lens to pick the city out of a profile. (Never mind the type signature. I'm not sure why it needs that, and why it has to look like that. I'm no expert lens user by any means. GHC asked for it so I just put it in there. I'll figure it out eventually.) &gt; locationcity :: Applicative f =&gt; (String -&gt; f String) -&gt; Profile -&gt; f Profile &gt; locationcity = location . _Just . city . _Just I'm sure you can read the definition. In a profile, go to the location, extract the location from the Just value, go to the city and extract the string from the Just valuethere. It's *that* easy. Now we can query profiles for their cities. &gt; sarah ^? locationcity Just "Johannesburg" &gt; anna ^? locationcity Nothing And there is no magic in the definition of `locationcity`. We could just as well use the composition of already-existing lenses. &gt; dave ^? location . _Just . city . _Just Just "Bras\237lia" All the magic lies in how well lenses compose. What's even cooler is that we can use this `locationcity` lens to change profiles when people move! &gt; sarah &amp; locationcity .~ "Cape Town" Profile { _name = "Sarah" , _birthYear = 1983 , _location = Just (Location { _country = "South Africa" , _city = Just "Cape Town" }) } The output is a little long, but we can see that Sarah is suddenly based in Cape Town! Very convenient. But it doesn't stop there! We can also create a list of cities from the list of people. &gt; cities = people ^.. each . _2 . locationcity This takes the list of people, goes to each element, picks out the second element of the tuple (which is the profile) and then picks out the city whenever possible, and compiles the results to a list. So we can print this list of cities. &gt; main = mapM_ putStrLn cities and we get an output that looks like $ ./profilecities Braslia Johannesburg $ the two cities that are specified in the profiles. Hopefully, this illustrates how composable and flexible lenses are. They can be used for the same things you use getters and setters for in OOP languages, but they are much, much more powerful than that. Any time you have a problem where data is nested more than one level deep, at least consider lenses.
Thanks, that was a very helpful glance into the world of applied lenses. Also, the Haskell equivalents of the Scala code examples are totally palatable (was worried that basic things in Haskell involved a world of pointless hurt).
What's so good about lenses is that once you know how they work, nested data stops being a load and starts being... almost fun. You just write the names of the fields separated by periods and everything will just work  as opposed to getters and setters in an OOP world where everything can break if you try to dereference a null pointer, or reference multiple things at once. You don't even need lenses for the first example. Here's how it's written without lenses. data Person = Person { name :: String, age :: Int } p = Person "John" 57 p { name = "Dave" } This is what's called "record syntax", and it's a feature of the Haskell language. lens is a library. I don't think the record syntax is nearly nearly as nice as lenses, but to each their own.
Careful what you wish for. Separating side effects from pure functions is essential to `reactive-banana`. My use of monad transformers is an internal affair, and they are quite elegant as they allows me to decouple code in ways that would be impossible in an imperative language. And requiring that my use of monad transformers is fast is equivalent to asking the compiler to perform code transformations that could dramatically change run time asymptotics. This is a no-no for any language.
Choosing software by the language it was written in is a sign of fanaticism and fanboyishness. It's all machine instructions, ultimately. And configuration is pretty much the same in XMonad and its kin. That said, XMonad is really a good WM and has been working faultlessly for me (the only bugs I've encountered were hang-ups, but those were due to the 'xmobar' toolbar and not XMonad). I've tried Awesome WM too, but it struck me with unexpected and buggy behavior in the first 10 minutes, so I stuck with XMonad, yes. I do believe that a WM should provide an in-built toolbar with system clock, keyboard layouts, volume control and notifications. Not having one is definitely a fault of XMonad, albeit a tolerable one.
My apologies, but I just cannot conceive how having to use two monad ReaderT's just to have two pieces of shared environment is elegant. The same goes for having two StateT's just to have two pieces of state, two ErrorT's just to be able to throw two types of exceptions, and so on. Add to that the manual lifting that transformers force unto a programmer, and the performance - it's a pity, but using monads to create imperative DSLs looks like Stone Age.
Isn't it the case that while this forms a useful mental model, GHC actually often avoids inlining dictionaries? I thought I remembered reading that in the Lazy with Class paper. *Edit:* On second glance, it was Gofer which first innovated on removing runtime dictionary construction, but that lead to inconsistencies in between Gofer and Haskell. Then, SPJ later translated that technique to genuine Haskell in what LwC calls the "first dictionary-free impl of type classes". I have no idea whether that's what's living in GHC today, but it's worth noting that people have written compilers that behave that way at least.
class Burrito b where (&gt;&gt;=) :: b filling -&gt; (tomatoes -&gt; b newFilling) -&gt; b newFilling
You will even have time to install and play with perl6 while it compiles ;)
Scrap your type classes looks related to an idea I was mulling around. Of course I found someone else had already thought it up and implemented it in agda. The basic idea is taking these 'type class' records and having a module system that can import them as implicit arguments to functions. They are calling it instance arguments. Here is the paper: https://lirias.kuleuven.be/bitstream/123456789/304985/1/icfp001-Devriese.pdf I'd be interested in hearing what you think about it.
You can do things like this, but... What kind of operation is `ReadFirstVar`? I'm concerned I don't really understand your goal, so I can't really answer your question the best possible way. The current implementation provides an untyped (as far as the user can tell) stack that just explodes into tiny bits if they treat the element removed from the stack as the wrong type. This doesn't seem like a good way to handle anything. It has no static typing, and dynamic typing is notoriously hard to make debuggable in a language like Haskell. After all, there's a ridiculously powerful static type checker to lean on, so you might as well do so. Forget implementation for the moment. What functionality do you actually want to expose in the DSL?
Yea, there are a couple of different approaches. The easiest is probably to use layout combinators and keybindings to adjust the layouts being combined on the fly, and separate keybindings to control the individual layouts. The problem is that of course, the number of keybindings you'd need to set up to make it truly controllable would be kind of silly. I find it easier to just mess with it till I find a setting I like and then just use that one setting rather than trying to mess with it dynamically.
I want the user of the DSL to be able to create variables of the type of their choice. The created variable will be stored in the game state. To use static typing, I could restrict the choice of the user to some types, but well... I don't like the idea because in the game the user is also able to create and use new types of their own. So it's difficult to predict what will be the types of the variables.
Ok, but what you say you want doesn't match up with the DSL you provided at all. I'll try sketching out something that matches what you say.
Yeah its just one way to implement them. I think JHC did something differently too. 
When I search for "template haskell" on Google this is number 4. http://stackoverflow.com/questions/10857030/whats-so-bad-about-template-haskell
IMHO, the first thing to check when you think you have a heterogeneous list is to see if you can turn it homogeneous. If you want to store "ints", "strings", and "heterogeneous arrays": data Value = I Int | S String | A [Value] And now you have homogeneous lists again. There are various ways of expressing things like arrays of the same type and such vs. arrays that are themselves heterogeneous. GADTs can be very useful here. This is the [variant type](http://en.wikipedia.org/wiki/Variant_type). On the other hand, if you really do want to wrap arbitrary Haskell types, well, that _is_ what Data.Dynamic does. If you are doing that, the type is going to indicate that somehow, irreducibly. (`#include &lt;modulo_unsafe&gt;`)
So, the traditional way to solve this is to provide reference cells. The user can create new reference cells, read the contents of a cell, or write to a cell. The type of the reference cell specifies the type of its contents. So let's provide a DSL that does this. -- I like the operational approach to building this sort of DSL. -- This is from the free-operational package. import Control.Monad.Operational -- a typed reference to a stored value. It provides the necessary -- type safety in reading and writing stored values data Ref a -- no details yet - it could depend on the interpreter -- a data type representing the operations the DSL provides data Op a where NewRef :: a -&gt; Op (Ref a) WriteRef :: Ref a -&gt; a -&gt; Op () ReadRef :: Ref a -&gt; Op a -- A type synonym for the DSL, using the free-operational Program type type DSL a = Program Op a -- wrappers for embedding the operations into the DSL type newRef :: a -&gt; DSL (Ref a) newRef = singleton . NewRef writeRef :: Ref a -&gt; a -&gt; DSL () writeRef ref x = singleton $ WriteRef ref x readRef :: Ref a -&gt; DSL a readRef = singleton . ReadRef -- a stupid example of an exceedingly imperative fib function fib :: Int -&gt; DSL Int fib x = do res &lt;- newRef 1 flip fix 2 $ \loop i -&gt; when (i &lt;= x) $ do r &lt;- readRef res writeRef res $ r * i loop (i + 1) readRef res The idea is that a Ref is typed index into *some* data structure that happens to hold the value in question. The exact data structure is up to the interpreter. Nevermind the interpreter details yet. Is this sort of API going to solve your problem? It's almost entirely unlike what you started with. On the other hand, it actually makes sense as an API.
Do you mind if I ask you about an example that popped up? It's a little more complicated than these basic ones, which has me lost. takeRequests :: Configuration -&gt; RequestChannel -&gt; ResultChannel -&gt; IO () takeRequests config requests results = do request &lt;- readChan requests case request of Right contents -&gt; do result &lt;- handleRequest contents case result of Right reply -&gt; writeChan results (Right (request, reply)) &gt;&gt; recurse Left err -&gt; writeChan results (Left err) &gt;&gt; unless verbose recurse Left "done" -&gt; writeChan results (Left "done") where recurse = takeRequests config requests results Here, the function is supposed to continuously read requests from the `requests` channel, then run an IO computation on those requests and if it gets a reply, put the reply with the request in the results channel and take a new request. If it gets an error response, it should write the error to the channel and then if `verbose` is false, take a new request. When it gets the "request" `Left "done"` it should write that to the results channel and stop evaluation. Is there a cleaner way to write this, or is the problem complex enough that this is the best way of writing it?
Think about something JSON like to represent your variables. How would you represent a JSON object in haskell? (hint: you don't need Typeable)
The machine I'm on right now doesn't have ghc on it, so this is untested. Let me know if there's anything wrong with it: hopscotch [] = 0 hopscotch (x:xs) = x + (max (hopscotch $ drop 1 xs) (hopscotch $ drop 2 xs)) Making it return the sequence of steps is left as an exercise to the reader.
Is capital `B` in `\Bar` legal?
I have no idea how they are implemented in Control.Lens, but your implementations sure seems like they work. If you want to check out other implementations, `set` and `get` are called `over` and `view` in the lens package.
To turn it around, after reading two articles telling me how type classes are bad, when would I ever want a type classees? Let's assume that of a simple language feature is missing, I'd rather extend GHC with a principled solution, instead of hacking it with a type class. Also, re your blog post, type class instances are "dynamically scoped" during the compilation process. It is known that Set does not enforce a consistent Ord instance, (becuse the type class dictionary is chosen at function call sites, the dictionary is not propagated from function agument to result) in the case of a Set constructed as `A.insert x (B.insert y s)`. If modules A and B have different instances of Ord in scope, the resulting Set is incoherent (duplicate elements, etc)
Here's a quick take \[edit: This is the directly recursive approach, which is straightforward, but inefficient for large inputs. See [rampion's take](http://www.reddit.com/r/haskell/comments/1wr4nc/having_fun_with_haskell_anyone_got_any_clever/cf4vw2r?context=1) for the [corecursive](https://en.wikipedia.org/wiki/Corecursion), [dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming) approach, which is less obvious, and much more elegant and efficient\]: import Control.Applicative import Data.List import Data.Ord -- Calculate all the hopscotch paths for a given sequence. paths :: Alternative f =&gt; [a] -&gt; f [a] paths (x:_:y:xs) = (x:) &lt;$&gt; (paths (y:xs) &lt;|&gt; paths (xs)) paths (x:_) = pure [x] paths [] = pure [] -- Find the "best" hopscotch path, for the given choice and metric functions. hopscotchFind choiceBy metric xs = (best, metric best) where best = choiceBy (comparing metric) $ paths xs -- Find the hopscotch path with the highest point total. hopscotch :: (Num t, Ord t) =&gt; [t] -&gt; ([t], t) hopscotch = hopscotchFind maximumBy sum This is similar to [kamatsu's](http://www.reddit.com/r/haskell/comments/1wr4nc/having_fun_with_haskell_anyone_got_any_clever/cf4nc3x), but the `paths` function performs more exhaustive pattern matching in order to output each unique path only once: &gt; paths [7,4,5,9,6,1,2,3] :: [[Integer]] [[7,5,6,2],[7,5,6,3],[7,5,1,3],[7,9,1,3],[7,9,2]] The `hopscotchFind` function generalizes the original problem to allow finding the minimum or maximum (or any other choice function) of the metric applied to the hopscotch paths: &gt; hopscotch [7,4,5,9,6,1,2,3] ([7,5,6,3],21) &gt; hopscotchFind minimumBy sum [7,4,5,9,6,1,2,3] ([7,5,1],13) &gt; hopscotchFind maximumBy product [7,4,5,9,6,1,2,3] ([7,5,6,3],630) &gt; hopscotchFind minimumBy product [7,4,5,9,6,1,2,3] ([7,5,1],35) &gt; hopscotchFind maximumBy minimum [7,4,5,9,6,1,2,3] ([7,5,6,3],3) &gt; hopscotchFind minimumBy maximum [7,4,5,9,6,1,2,3] ([7,5,6,2],7) &gt; hopscotchFind minimumBy id "supercalifragilisticexpialidocious" ("seafaiiieiico","seafaiiieiico") &gt; hopscotchFind minimumBy reverse "supercalifragilisticexpialidocious" ("seafaiscpadco","ocdapcsiafaes")
What do you mean "constructor" in a lambda parameter name? The other `Bar` on that line is a constructor, it the one adjacent to the `\`.
I'd strongly suggest deleting this and resubmitting it with a title that means something. The only informative word in your current title is "functors", and even that doesn't have _all_ that much information content in the local context.
You already answered part of your own question: incoherent instances. Also, type class dictionaries are automatically supplied by the compiler, but I think I've heard people mention that Agda has a more principled solution to that.
Also, use a link post rather than a text one.
Pattern match
This is how I would write it: takeRequests :: Configuration -&gt; RequestChannel -&gt; ResultChannel -&gt; IO () takeRequests config requests results = do request &lt;- readChan requests (message, continue) &lt;- case request of Right contents -&gt; do result &lt;- handleRequest contents return $ case result of Right reply -&gt; (Right (request, reply), True ) Left err -&gt; (Left err , not verbose) Left "done" -&gt; return (Left "done" , False ) writeChan results msg when continue (takeRequests config requests results)
This problem is similar to calculating the nth fibonacci number in that its solution depends on the solution of two smaller instances, which can cause a lot of duplicate work being done if you implement it naively. Not to mention a ginourmous slowdown for bigger problem instances. One way to circumvent this is to have a store for already computed solutions aka passing around a dictionary. import Prelude hiding (lookup) import Data.Map type Solution = ([Int], Int) type Problem = [Int] -- naive solution just as reference naive_hopscotch :: Problem -&gt; Solution naive_hopscotch [] = ([], 0) naive_hopscotch (x:xs) = let shortjump = naive_hopscotch $ drop 1 xs longjump = naive_hopscotch $ drop 2 xs in combineSolutionsWith shortjump longjump x combineSolutionsWith :: Solution -&gt; Solution -&gt; Int -&gt; Solution combineSolutionsWith (jumps1, sum1) (jumps2, sum2) x = if sum1 &gt; sum2 then (x:jumps1, x+sum1) else (x:jumps2, x+sum2) -- quick and dirty solution, can most likely be cleaned up to look a lot nicer map_hopscotch :: Problem -&gt; Solution map_hopscotch xs = fst $ map_hopscotch' empty xs map_hopscotch' :: Map Int Solution -&gt; Problem -&gt; (Solution, Map Int Solution) map_hopscotch' m [] = (([],0), m) map_hopscotch' m p@(x:xs) = let key = length p in case lookup (length p) m of (Just solution) -&gt; (solution, m) Nothing -&gt; let (longjump, m') = map_hopscotch' m (drop 2 xs) (shortjump, m'') = map_hopscotch' m' (drop 1 xs) solution' = combineSolutionsWith shortjump longjump x in (solution', insert key solution' m'') Looking at the type of map_hopscotch' this can probably be written a lot cleaner with the State monad, I will leave that as an exercise for everyone whos eyes are scarred by seeing this. (Aka I am too lazy right now.) An interesting point here is, that you can identify a subproblem of the given problem instance by its length which lets you dodge using the complete subproblem as a key.
There's still a bunch of case expressions, but I see the overall flow is nicer and a bit of other kinds of repetition is gone. Thanks. I'll try to think more in terms of passing data and less in terms of performing actions.
Classic dynamic programming
Ohhhh because the constructor doesn't have any arguments. I am slightly less dumb when I am not on my phone and can see more than 2 Ines of code at a time.
Is there any relation to Haskell or is this just pure math?
Not criticizing, was just curious if there was some connection that I just didn't understand.
Short and sweet using the same corecursion technique as the canonical `fibs = 1 : 1 : zipWith (+) fibs (tail fibs)`: hopscotch xs = last ys where ys = z : z : z : zipWith3 go (reverse xs) ys (tail ys) z = (0, []) go x (a, xs) (a', xs') = if a &gt;= a' then (a+x, x:xs) else (a'+x, x:xs') 
Like PasswordIsntHAMSTER says, it's classic dynamic programming. Given the optimal hopscotch path for all proper tails of the list, we can compute the optimal hopscotch path for the list itself in O(1). So if we compute the optimal paths for each tail of the list by length ascending, we can compute each in O(1), since we can reuse prior results. Therefore we can compute the optimal hopscotch path for the entire list in O(n). Knowing that, it'smerely a question of implementation. Traditionally dynamic programming is implemented with arrays, but in Haskell we can use corecursion instead in this case. So we build the partial solutions in reverse order. Traversing the input in from back to front (`reverse xs`), we see whether skipping two or skipping one leads to the bigger total. The key is in how we jumpstart the corecursion with three base cases `z -= (0, [])`. This offsets the elements generated by the `zipWith3` by three from the front of `ys`, so that for `0 &lt;= k &lt; length xs`: ys !! (k+3) = go (xs !! (length xs - 1 - k)) (ys !! k) (ys !! (k+1)) Which means `go` can make the decision to either step 1 or 2 after picking up the given element of `xs`.
We can adapt this to do pjdelport's generalized `hopscotchFind` fairly easily: hopscotchFind cmp op a0 xs = last ys where ys = z : z : z : zipWith3 go (reverse xs) ys (tail ys) z = (a0, []) go x (a, xs) (a', xs') | cmp a a' == LT = (op x a', x:xs') go x (a, xs) (a', xs') | otherwise = (op x a, x:xs) hopscotch = hopscotchFind compare (+) 0
Thank you, that is a very nice solution!
With your definition, `hopscotch [1, 10]` returns 1. The optimal solution may involve skipping the first square.
Probably a stupid question, but is there a way to make `universe` and co play nice with infinite types? For example `data DList a = DList (DList a) a (DList a)` and a method to *tie the knot* is it possible to define a `Lens` and/or `Plated` instance that is aware of the circular structure? I tried using `makeLenses` but this will (as probably expected) only create lenses that will change the focused on entry but fail to "un- and retie the knot".
You should be able to make a "distinct"`Traversal` or version of `universe` that collects a `HashMap` as it walked of stable names / stable pointers then used them to fill in subsequent occurrences of the same thing. This can't be the default, as when there isn't a cycle it induces a space leak for the map. Ideally it'd be a mixin combinator taking `universe` or the `Traversal`, but it likely can't be.
This is a followup to [this older article](http://www.reddit.com/r/haskell/comments/luyf4/quick_and_dirty_reinversion_of_control/), which describes using continuations to implement a quick and dirty `yield` operation.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Hopscotch hashing**](http://en.wikipedia.org/wiki/Hopscotch%20hashing): [^(**NSFW !**)](http://www.reddit.com/message/compose?to=%28This%20is%20a%20placeholder%29&amp;subject=1%20NSFW%20words%20are%20present%20in%20this%20comment:&amp;message=fill%0a%0aIf%20you%20think%20any%20of%20word/s%20above%20is%20SFW,%20forward%20this%20message%20to%20/r/autowikibot%20%28keep%20the%20subject%20unchanged%29) --- &gt; &gt;**Hopscotch hashing** is a scheme in [computer programming](http://en.wikipedia.org/wiki/Computer_programming) for resolving [hash collisions](http://en.wikipedia.org/wiki/Hash_collision) of values of [hash functions](http://en.wikipedia.org/wiki/Hash_function) in a [table](http://en.wikipedia.org/wiki/Hash_table) using [open addressing](http://en.wikipedia.org/wiki/Open_addressing). It is also well suited for implementing a [concurrent hash table](/w/index.php?title=Concurrent_hash_table&amp;action=edit&amp;redlink=1). Hopscotch hashing was introduced by [Maurice Herlihy](http://en.wikipedia.org/wiki/Maurice_Herlihy), [Nir Shavit](http://en.wikipedia.org/wiki/Nir_Shavit) and Moran Tzafrir in 2008. The name is derived from the sequence of hops that characterize the table's insertion algorithm. &gt;The algorithm uses a single array of n buckets. For each bucket, its neighborhood is a small collection of nearby consecutive buckets (i.e. one with close indexes to the original hashed bucket). The desired property of the neighborhood is that the cost of finding an item in the buckets of the neighborhood is close to the cost of finding it in the bucket itself (for example, by having buckets in the neighborhood fall within the same [cache line](http://en.wikipedia.org/wiki/Cache_line)). The size of the neighborhood must be sufficient to accommodate a logarithmic number of items in the worst case (i.e. it must accommodate log(n) items), but only a constant number on average. If some bucket's neighborhood is filled, the table is resized. &gt;In hopscotch hashing, as in [cuckoo hashing](http://en.wikipedia.org/wiki/Cuckoo_hashing), and unlike in [linear probing](http://en.wikipedia.org/wiki/Linear_probing), a given item will always be inserted-into and found-in the neighborhood of its hashed bucket. In other words, it will always be found either in its original hashed array entry, or in one of the next H-1 neighboring entries. H could, for example, be 32, the standard machine word size. The neighborhood is thus a "virtual" bucket that has fixed size and overlaps with the next H-1 buckets. To speed the search, each bucket (array entry) includes a "hop-information" word, an H-bit bitmap that indicates which of the next H-1 entries contain items that hashed to the current entry's virtual bucket. In this way, an item can be found quickly by looking at the word to see which entries belong to the bucket, and then scanning through the constant number of entries (most modern processors support special bit manipulation operations that make the lookup in the "hop-information" bitmap very fast). &gt;==== &gt;[**Image**](http://i.imgur.com/6bihCDC.gif) [^(i)](http://en.wikipedia.org/wiki/File:Hopscotch-wiki-example.gif) - *Hopscotch hashing. Here, H is 4. Gray entries are occupied. In part \(a\), the item x is added with a hash value of 6. A linear probe finds that entry 13 is empty. Because 13 is more than 4 entries away from 6, the algorithm looks for an earlier entry to swap with 13. The first place to look in is H-1 = 3 entries before, at entry 10. That entry's hop information bit-map indicates that d, the item at entry 11, can be displaced to 13. After displacing d, Entry 11 is still too far from entry 6, so the algorithm examines entry 8. The hop information bit-map indicates that item c at entry 9 can be moved to entry 11. Finally, a is moved to entry 9. Part \(b\) shows the table state just after adding x.* --- ^Interesting: [^Hash ^table](http://en.wikipedia.org/wiki/Hash_table) ^| [^Hash ^function](http://en.wikipedia.org/wiki/Hash_function) ^| [^Cuckoo ^hashing](http://en.wikipedia.org/wiki/Cuckoo_hashing) *^\/u/barsoap ^can ^reply ^with ^'delete'. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less.* ^| [^(FAQs)](http://www.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.reddit.com/r/autowikibot/wiki/modfaqs) ^| [^Magic ^Words](http://www.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/) ^| [^flag ^a ^glitch](http://www.reddit.com/message/compose?to=/r/autowikibot&amp;subject=Glitched comment report&amp;message=What seems wrong: (optional description goes here\)%0A%0A---%0A%0AReply no. 40419:%0Ahttp://www.reddit.com/r/haskell/comments/1wr4nc/having_fun_with_haskell_anyone_got_any_clever/cf51baa)
`(^. picture)` may read better as `view picture` 
from OP's description: &gt; You start in the first square (and collect the marker on that square) all the time.
The problem with your approach isn't the implementation, it's the algorithm. Your complexity is (roughly) O(2^(n/2.5)) for a list of length n. You can generate this from the recurrence relation: cost(n) = 1 + cost(n-2) + cost(n-3) But you can also see it just by looking at how many times you compute `hopscotch $ drop k xs` for a given input `xs`: k number_of_times 0 1 1 0 2 1 3 1 4 1 5 2 6 2 7 3 8 4 9 5 10 7 11 9 12 12 13 16 14 21 15 28 16 37 17 49 18 65 19 86 20 114 ... So the total number of steps for a list of a given length grows like: length number_of_steps floor(2^((length + 1)/2.5)) 0 0 1 1 1 1 2 1 2 3 2 3 4 3 4 5 4 5 6 6 6 7 8 9 8 11 12 9 15 16 10 20 21 11 27 27 12 36 36 13 48 48 14 64 64 15 85 84 16 113 111 17 150 147 18 199 194 19 264 256 20 350 337 21 464 445 ... For a list of length 100, it will take 1.5 trillion (1,559,831,901,917) steps to compute the optimal path! For a list of length 1000, it will take over 2^405 (127103905765224044119359109116821385799025177984373365422584551276381823069321712418507801229472324130201129561147327365920) steps!
Is it possible to restate this so that the partial solutions are built up from the head (inits) of the input, instead of the tails, to avoid the requirement that `xs` is finite?
Notice that `ReadAccount id` has exactly the type you want: readAccount :: ExpF r Int readAccount = ReadAccount id
Even if it's straight-up math, you should motivate the topic in the first post of the series by explaining some applications it could be used for.
An infinite xs would correspond to an infinitely long sequence of squares and since the problem states that you are only done once you reach the end, you would be hopping forever in every infinite instance of it. Basically, if a solution terminates when given an infinite sequence, then you probably have made an error somewhere. Unless you are an optimist, then you accidentally solved a different problem ;)
Well, yeah, obviously, it's a naive implementation. You could get around this with memoization.
We can get that corecursion with loeb very nicely. lookupDef :: a -&gt; Int -&gt; [a] -&gt; a lookupDef d n [] = d lookupDef _ 0 (x:_) = x lookupDef d n (_:xs) = lookupDef d (n-1) xs loeb :: Functor f =&gt; f (f b -&gt; b) -&gt; f b loeb x = fmap ($ loeb x) x hopscotch :: (Num a, Ord a) =&gt; [a] -&gt; a hopscotch = head . loeb . map step . zip [1..] where step (i, val) vals = val + max (lookupDef 0 (i + 1) vals) (lookupDef 0 (i + 2) vals)
Anyone have any insight yet on whether this is a good book? It's by Apress, which is promising, but I've certainly seen at least one Haskell title in the past year or so that just seemed to be trying to cash in on the buzz around the language.
The Amazon page lets you see a lot more of the text. Glancing at the contents, I noticed that one of the bold-printed section headings(!) contains a misspelling of the word "trees," which I took as a bad omen. (Then again, the author may be a non-native English speaker.)
`step` is O(i) not O(1), though, so this is quadratic where the earlier solution was linear.
This nice solution of rampion seems kind of magical if you are not familiar with dynamic programming. To illustrate how it works, it is helpful to combine this approach with the more straightforward approach of enumerating all possible hopscotch paths as a tree. Each node of the tree is a single hopscotch square, with the children being all of the possible squares that can continue the path from here. This kind of tree is called a "trie". The trick of rampion is to enumerate the paths from the end rather than from the beginning. By starting from the end, we will only need to follow one branch of the tree at each level when searching for the best solution. While it may seem that enumerating all the paths will be much less efficient than rampion's solution, it turns out that laziness actually makes this solution equivalent to rampion's, with `O(n)` complexity. Another key observation of rampion, in addition to the idea of reversing the list, is that when starting from the end there are three possible endings, not just two, for the paths. After that, there are only two possible continuations of the path back towards the beginning at each step, as expected. Our algorithm is three steps: 1. Build the trie, starting from the end. 2. Replace each node of the tree with a tuple of the hopscotch path from here to the end (i.e., a list of the node labels from this node up to the root of the tree), and the score so far. 3. Search the tree for the best leaf node. We will use one more interesting technique, which could be skipped but fits in perfectly here: When replacing the tree labels with tuples, we need to traverse the tree while keeping state, with the state being updated at each node for the traversal of its children. It turns out that you don't need the State monad for that - the Reader monad, together with its "`local`" function, is just what you need. So we'll also illustrate that technique here. Of course, you could just skip the monad altogether and pass the state manually as parameters to the traversal function. EDIT: Just to avoid confusion: Our use of the `traverse` function is *not* the traversal of the tree; it is a traversal of a list. We really could have used `mapM` in place of `traverse`. But since we are using applicative notation, `traverse` seems to fit in stylistically better than `mapM`. Here is the code: import Data.Tree (Tree(Node, rootLabel), unfoldForest) import Data.List (maximumBy, tails) import Data.Ord (comparing) import Control.Monad.Reader (runReader, local, ask) import Data.Functor ((&lt;$&gt;)) import Control.Applicative (pure, (&lt;*&gt;)) import Data.Traversable (traverse) hopscotch = maxHopscotch . addScoresAndPaths . hopscotchTrie . reverse take3NonNullTails = take 3 . init . tails hopscotchTrie = unfoldForest go . take3NonNullTails where go (x:xs) = (x, drop 1 $ take3NonNullTails xs) go _ = (0, []) -- never reached addScoresAndPaths = flip runReader ([], 0) . traverse go where go (Node x ns) = localNode x $ Node &lt;$&gt; ask &lt;*&gt; traverse go ns localNode x = local $ \(p, s) -&gt; (x : p, x + s) maxHopscotch [] = ([], 0) maxHopscotch ts = maxLeaf $ maximumBy (comparing $ snd . rootLabel) ts where maxLeaf (Node x []) = x maxLeaf (Node _ ns) = maxHopscotch ns 
And doesn't loeb need to tie the knot? loeb x = fix $ \y -&gt; fmap ($y) x
Possibly a [Trie](http://en.wikipedia.org/wiki/Trie)?
I started using XMonad this weekend because I upgraded Ubuntu 13.10 which broke both Gnome and Unity. But I've been meaning to try it out anyway and so far it works OK. I'll probably have to fiddle a little with the layout as I'm not 100% happy with the way it out of the box. This is my first venture into tiling wm land and the fact that XMonad is written in Haskell is a big part of why I'm trying it out.
A little typo in the first edition is hardly cause for alarm. It's awfully easy to type "tress" when you mean "trees". If anything, the TOC gives me hope that this will be a good bridge from LYAH to "serious programming". Apress is at least a little more trustworthy than Packt.
Ah okay, didn't know that.
It's great to see some of Dan's wonderful posts coming back closer to programming in Haskell, from the world of philosophy and quantum mechanics that they have been in recently.
Only time will tell, but I think discussing cabal is smart, because package management is a major hurdle for new users once they get past the basic syntax.
I really haven't run into cabal hell since 7.4ish. Maybe I'm just not working hard enough. :D
Oops, I stand corrected.
The typo is "Binary tress for the minimum price"
He mentions dependent types!
One positive review of it here: http://jpmoresmau.blogspot.com/2014/02/beginning-haskell-book.html
I love seeing more pragmatic introductions to Haskell to balance the wealth of more "principled" approaches.
&gt; I was prompted to write this (a little late, I know) after reading this article and Tekmo's post on reddit. I was about to say: "this is a free monad in (a not very good) disguise!" &gt; I think ultimately continuations may perform better than using ASTs. If that's true it sounds like a good opportunity for some compiler optimizations. 
2 chapters named for "Monads" , nothing about Functor or Applicative or any other type class --&gt; suspicious.
What gives me pause is that the typo is so conspicuous--if the proofreader(s) can miss that, then how many errors might there be in the code? Edit: I don't want to be Debbie Downer here--I *am* happy to see an up-to-date introduction to Haskell.
And Idris, even!
Ok I just realised I could use rampion's solution as well, building up from the beginning. I just need to choose between the last 2 solutions instead of taking just the last one. Changes needed minus choosing the best sequence: -- well, don't use last here, take the last 2 instead hopscotch xs = last ys where ys = begin ++ zipWith3 go zs ys (tail ys) (zs, begin) = first3' xs go x (a, xs) (a', xs') = if a &gt;= a' then (add a x, x:xs) else (add a' x, x:xs') -- first 3 items are deterministic first3' :: Num a =&gt; [InfNum a] -&gt; ([InfNum a], [(InfNum a, [InfNum a])]) first3' [] = ([], []) first3' [x] = ([], [(x,[x])]) first3' [x,y] = ([], [(x,[x]),(MinInf,[])]) first3' [x,y,z] = ([], [(x,[x]),(MinInf,[]),(add x z, [z,x])]) first3' (x:y:z:xs) = (xs, [(x,[x]),(MinInf,[]),(add x z, [z,x])])
Recently he hasn't been posting much at all: his last one was october, and 2013 had four blog posts.
Just ordered mine. $35 on [amazon](http://www.amazon.com/Beginning-Haskell-A-Project-Based-Approach/dp/1430262508/ref=sr_1_1?ie=UTF8&amp;qid=1391377456&amp;sr=8-1&amp;keywords=A+Project-Based+Approach+beginning+haskell)
This is very cool, but for the "beginners" you mention, what they want is `Debug.Trace`: anything else is overkill.
The GitHub should explain the basics, but for reference here is: * [The package on Hackage](http://hackage.haskell.org/package/haskelm-0.0.3) * [The discussion on the Elm Mailing List](https://groups.google.com/forum/#!topic/elm-discuss/4Fbk0DYFc0g) * [The Elm programming language main site](http://elm-lang.org/)
The GitHub contains documentation and examples, but for extra reference, here is: * [The package on hackage](http://hackage.haskell.org/package/haskelm) * [The discussion on the Elm mailing list](https://groups.google.com/forum/#!topic/elm-discuss/4Fbk0DYFc0g) * [The Elm language's main site](http://elm-lang.org)
Maybe have two levels of piping? So a `Producer String (Producer String IO) ()` and use `log = lift . yield`.
Out of curiosity, what's the other book?
Sounds like what you want is an abstract data type over the output of `yield` and a constructor for your pipe that takes a record defining what to do for certain inputs. For the simplest case, you do something like: log :: Monad m =&gt; String -&gt; Producer (Either String a) m () log = Pipes.yield . Left yield :: Monad m =&gt; b -&gt; Producer (Either a b) m () yield = Pipes.yield . Right It seems like it should be fairly easy to then limit only certain constructors (in this case `Left`) and with the mechanism for isolating effects, hide them within a `Pipes` chain. 
it's really good. worth every penny.
[This exists!](https://github.com/switchface/helm)
Indeed, this was one of my reasons for not naming it as such! Although it is nice that there's only one letter difference from Haskell to Haskelm.
This is the suboptimal (IMHO) solution I referred to in the 2nd part of my message :-)
I thought about that, and should give it a go... Wasn't sure about the resulting interleaving order of effects.
What is the ideal solution for making it a bit more composable (many different types of output, and such)?
For what reason(s) is it suboptimal? I can't think of a solution that is more composable and when chaining to another pipe, you should be able to rewrite it with the same `for` technique as above.
I don't know if it's related, but I have done this approach with [my "Imperative Game Library"](https://code.google.com/p/smartlight-haskell/source/browse/trunk/SmartLight/src/GameLoop.hs). Maybe you're interested. 
Something like Financial Modeling in Haskell. It's a Packt book.
So the problem as stated gives the squares in order, from first to last, with the question being what's the best path. Consider the different ways a hopscotch course could stretch into infinity. * We could stand at the end of a hopscotch course. We can't see the true beginning of the hopscotch course, since it is infinitely long. However, we could walk backward along the hopscotch course, and as we pass each square, ask ourselves if we were to begin here and play a game of hopscotch towards the end of the course, what would the optimal path be? We can answer this using the dynamic programming approach from above. Note that since we're walking from the end of the hopscotch course towards the beginning, there's no need to reverse the input: tail_paths :: (Num a, Ord a) =&gt; [a] -&gt; [(a, [a])] tail_paths reversed_course = drop 2 ys where ys = z : z : z : zipWith3 starting reversed_course ys (tail ys) z = (0, []) starting x (a, xs) (a', xs') = if a &gt;= a' then (a+x, x:xs) else (a'+x, x:xs') Answering this question for each square we pass takes constant time. No matter how far toward the beginning of the course we walk, we've only taken linear time to answer all the questions we've asked so far. We can see too, that this gives us the expected answer:  tail_paths $ reverse [7,4,5,9,6,1,2,3] [(0,[]),(3,[3]),(2,[2]),(4,[1,3]),(9,[6,3]),(13,[9,1,3]),(14,[5,6,3]),(17,[4,9,1,3]),(21,[7,5,6,3])] [] ( 0, []) [3] ( 3, [3]) [2,3] ( 2, [2 ]) [1,2,3] ( 4, [1, 3]) [6,1,2,3] ( 9, [6, 3]) [9,6,1,2,3] (13, [9, 1, 3]) [5,9,6,1,2,3] (14, [5, 6, 3]) [4,5,9,6,1,2,3] (17, [4, 9, 1, 3]) [7,4,5,9,6,1,2,3] (21,[7, 5, 6, 3]) * We could stand at the beginning of a hopscotch course. We can't see the true end of the hopscotch course, since it is infinitely long. However, we could walk forward along the hopscotch course, and as we pass each square, ask ourselves if the course ended just after this square, what would have been the optimum path to complete the course? We can also answer this question via dynamic programming, if we answer a slightly different question first - what's the optimal path from the start that lands *on* the given square? If we know the optimal paths to land on the last three squares we've passed, then we can answer what the optimal game would be if the game ended just after the current square. init_paths course = zipWith3 ending (drop 0 ys) (drop 1 ys) (drop 2 ys) where ending p1 p2 p3 = second reverse . fromJust $ choose [p1, p2, p3] choose = maximumBy (compare `on` fmap fst) ys = Just (0, []) : Nothing : Nothing : zipWith3 landing course ys (tail ys) landing x p p' = fmap ((+x) *** (x:)) $ choose [p, p'] Note that `ys :: [Maybe (a, [a])]` because not all squares on the course are reachable (`course !! 1` in particular), so not every square has an optimal path that lands on it. Both `ending` and `landing` run in constant time, so again answering this question for each square we pass takes constant time. No matter how far toward the end of the course we walk, we've only taken linear time to answer all the questions thus far. Again we can see we get the expected answer:  init_paths [7,4,5,9,6,1,2,3] [(0,[]),(7,[7]),(7,[7]),(12,[7,5]),(16,[7,9]),(18,[7,5,6]),(18,[7,5,6]),(20,[7,5,6,2]),(21,[7,5,6,3])] [] ( 0,[] ) [7] ( 7,[7] ) [7,4] ( 7,[7 ] ) [7,4,5] (12,[7, 5] ) [7,4,5,9] (16,[7, 9] ) [7,4,5,9,6] (18,[7, 5, 6] ) [7,4,5,9,6,1] (18,[7, 5, 6 ] ) [7,4,5,9,6,1,2] (20,[7, 5, 6, 2] ) [7,4,5,9,6,1,2,3] (21,[7, 5, 6, 3]) 
So the best solution in theory is probably to define an `ArrowChoice` instance for pull-based `Pipe`s. Unfortunately, there is no `Arrow` instance for pull-based pipes (only push-based pipes can implement `Arrow`) and `ArrowChoice` has `Arrow` as a super-class, so I cannot implement it without leaving `first` undefined. However, if that were acceptable then you could use the `Arrows` language extension to automatically thread values for custom sum types. It automatically desugars case expressions on pipe outputs to the equivalent `ArrowChoice` operations. This works for arbitrary sum types and it's very slick and easy to use. That would probably be the most ideal solution if it weren't for the undefined `first`.
Great! Using `Maybe` is the key to making the forward corecursion work. This is obvious in hindsight (as hindsight tends to be), but eluded me before. This is an interesting asymmetry introduced by the puzzle's starting-point requirement.
why is that suspicious, especially since that judgement is given on a very sparse table of contents of a book you haven't read?
Is there rebindable syntax for `proc` notation? Could you plug in your own typeclass / operation there?
Hm, is Raskell lazy at all? I needed five Chars to break it: [1..]
Would a something like outputting an Either or a type collection be sufficient?
That's awesome! I've been hoping for a book that digs into Foldable and Traversable a bit. I'll definitely have to grab a copy.
Looks very nice, from what I can tell! LYAH is a fantastic book, but at the end of it there's still so much practical stuff missing from your arsenal (networking, vectors, graphics, parallelism, profiling, FFI, etc.). RWH is also great and talks about a lot of these subjects, but it's also from 2008 and many things like conduits and async were not around. Hopefully this book is useful for beginners.
Seconded, this is how I usually solve the problem. You can also declare a type class that can convert to and from "Value" data types in the manner that Data.Dynamic.toDyn and Data.Dynamic.fromDynamic work: {-# LANGUAGE FlexibleInstances #-} class IsAValue a where toValue :: a -&gt; Value fromValue :: Value -&gt; Maybe a instance IsAValue Int where toValue = I fromValue v = case v of { I i -&gt; Just i; _ -&gt; Nothing } instance IsAValue String where toValue = S fromValue v = case v of { S s -&gt; Just s; _ -&gt; Nothing } instance IsAValue [Value] where toValue = A fromValue v = case v of { A a -&gt; Just a; _ -&gt; Nothing } 
The `Arrow`/`ArrowChoice` type classes are probably the closest you will get since `proc` notation desugars to them. Are you asking about rebinding it to desugar to a different type class?
The Profunctor hierarchy seems to be finer-grained than that of Arrow. In particular, a profunctor can be Choice without having to be Strong and implement first'.
hmm..I know some of those words.
Which changes to State?
I think `doSomething`'s type is isomorphic to `piped`'s type. These are the isomorphisms I'm thinking of: {-# LANGUAGE RankNTypes #-} import Pipes fw :: MonadIO io =&gt; Producer String IO () -&gt; (String -&gt; io ()) -&gt; io () fw piped_ k = runEffect $ for (hoist liftIO piped_) (lift . k) bw :: (forall io . MonadIO io =&gt; (String -&gt; io ()) -&gt; io ()) -&gt; Producer String IO () bw doSomething_ = doSomething_ yield I haven't proven the isomorphism laws, though.
My main motivation is being able to share code between a Haskell server and an Elm client. It also will help with developing Elm libraries, since we can translate existing Haskell libraries. 
MTL switched from having separate `State` and `StateT` types to having type State = StateT Identity So now instead of you having the constructor `State :: s -&gt; (s, a)` you have `state :: s -&gt; (s, a)` which is just a plain old function. This is great for code reuse but did break some code, a not uncommon trade off.
 perl -e '$foo = "Haskell"; print ++$foo'
Very nice; we need more books! Haskell is hard to learn...but not because it is intrinsically hard to learn; any language is hard to learn. But other languages have shelves and shelves of books available, while Haskell has...maybe five? What I would really like is a book that goes into the higher level type theory and extensions. I've got the LYAH and RWH sort of stuff down fairly well. I avoid nearly every GHC extension because I just don't understand them.
&gt; Are you asking about rebinding it to desugar to a different type class? Yes (i.e. `-XRebindableSyntax`). So for instance if the (sugared) uses of `|||` point to an `ArrowChoice`-like class without the super-class, is that helpful at all?
Are there any currently practical alternatives to Monad Transformers? I'll cop to some fuzziness in the term "practical", but I know there's been a large number of theoretical proposals; I don't know if anyone is using them or indeed if any of them are useful since it seems like MTs are still what gets used in practice (eventually followed by their removal, it seems). Practical here would probably look something like "useful for a significant subset of problems, doesn't completely bork composition even if it might break some edge cases, exists today". (One such example is the all-in-one RWS monad, which meets a very specific need.)
Certainly diving in head first. I would've just used SDL and put most of my game logic in the IO monad. Trying to learn effective FRP at the same time as produce a game in 48 hours, well, I'm amazed you produced anything at all.
&gt; The deeper your stack, the more expensive &gt;&gt;= is, which is a tax paid by every line of your program. Is there a good reason the layers of the stacks are not easy to optimize away?
The user guide says that, with [RebindableSyntax](http://www.haskell.org/ghc/docs/6.8.1/html/users_guide/syntax-extns.html#rebindable-syntax): &gt; Arrow notation uses whatever `arr`, `(&gt;&gt;&gt;)`, `first`, `app`, `(|||)` and loop functions are in scope. But unlike the other constructs, the types of these functions must match the Prelude types very closely. Details are in flux; if you want to use this, ask! So I guess if the desugaring ended up using `first` somehow, it would fail if no `first` was in scope, rather than silently introducing `undefined`.
Nice writeup! Have you looked at netwire? It seems to be a library that does exactly what you're describing with Autos.
Certainly, I don't really feel like I know what I'm doing yet, so any feedback is useful. Using it for the game jam was more of a "what haven't I thought of yet" for the underlying engine. I like the idea of keeping my game logic outside of the IO monad for the simple fact that it's more pure in some sense. IO really only needs to be used for rendering (sound, geometry) or loading assets, which are orthogonal to game logic. I still don't know how to properly abstract away the two ideas, esp. in a FRP setting. I like the *idea* of FRP, but I still haven't really seen any big projects that use it. At least, all of the demos have some giant arrowized game logic loop, which seems to go against the idea of having small composable elements. So, I'm not totally sure what a good way to abstract away a lot of things that game developers need would be (like input handling, loading/rendering, simple control schemes like first-person movement, camera systems, particle systems, etc). Definitely open to suggestions though. :)
This.
thanks! actually, as i mentioned in the post, the entire series is going to be leading into netwire :)
I see, in fact it's more or less what I have at the moment. As a reference, I use the name of the variable (provided by the user). The variables are then stored in an association list. But then, this list is heterogeneous, and all sort of problems ensues (mainly, casts). How would you store it?
It's a bit sad that the request parser is now [written in terms of pointer operations](http://hackage.haskell.org/package/warp-2.0.3/docs/src/Network-Wai-Handler-Warp-RequestHeader.html) -- admittedly the code is not particularly hair-raising, but wasn't `attoparsec` supposed to make this sort of thing unnecessary?
I had recently been thinking about letting types and functions live in the same namespace. Anyone know why this might not be such a good idea? (I haven't watched the entire thing yet, so maybe he tells us.)
Why not just use hslogger?
Haha, I skipped the "Why FRP" section because I already knew why. ;)
I think it's a good idea. 
There is a meetup on Idris tomorrow in London if anyone is interestered. [meetup info](http://www.meetup.com/London-Haskell/events/163456892/)
I don't think it's such a bad idea. Personally, I'd stick everything in the same namespace with the caveat that it is illegal to use a value as a type (but using a type as a value definitely is legal!) You can investigate Agda's hierarchy of universes to learn more.
Mine is on a slow boat... 
The AST vs continuations comparison is the free monad vs codensity comparison. If you're only going to use the results once, then codensity is much faster because it avoids re-walking the AST on every bind. However, if you're going to use the results repeatedly, then you want to cache the result as an AST so that you don't need to recompute it every time. It'd be nice to have the compiler exploit this tradeoff, though it's not entirely clear how well that could be done. There's definitely some low hanging fruit here ( la fusion/deforestation), but how far can we push it? and, is the development/maintenance cost worth the benefits?
Take a look at Yampa and this asteroid implementation http://hackage.haskell.org/package/Haskelloids-0.1.1/src/src/Haskelloids/ also cuboid: https://github.com/pedromartins/cuboid
Exactly. I'm also not saying that library-level monads are a bad thing either -- see parsec/attoparsec, snap/happstack, stm, Par monad, etc. You get a monad and you get a function to run the monad that is either pure or in IO. This idiom is usually understandable, easy to use and works anywhere (because everyone implements `liftIO`). Typically if I'm going to implement something like this these days and my newtyped monad implementation starts looking like `FooT (BarT (BazT q u u x))`, I'll flatten it and handwrite Monad/MonadState/MonadReader/etc instances. This almost always yields a speedup. Once you decide you want to implement your library as a monad transformer, you tend to get headaches: figuring out exception semantics being the biggest one. The solution du jour for this is the `monad-control` package, but that's really a piece of folk wisdom at the moment and there's no way a random person who has "Learn you a Haskell" and "Real World Haskell" can figure that out from browsing Hackage. Complicating the situation, you have `MonadCatchIO`, which some libraries still use (including Snap 0.x) except it's slow, doesn't implement mask semantics, and is suspected to be unsound these days. Of course, if your monad has continuation-passing semantics then you're really screwed and you either have to punt the issue outside your monad and do the exception handler bookkeeping by hand with IORefs (i.e. ResourceT) or you have to thread exception handlers through your continuations and be very very careful that you're masking and restoring exceptions at the right times. The latter is very difficult to get right.
Oh, I thought it was more a comment about continuations being faster than a condensity transformation of a free monad, because somehow in the continuation case all the fusion has been done "by hand" in your encoding. But I don't claim to really know what I'm talking about!
Off the top of my head, a few reasons: - You're making the inliner and strictness analyser work harder. - If you hit a function that is `NOINLINE` or too big to inline then you're back to the typeclass bind. - Lots of monad transformers have state that gets threaded through the computation. This costs registers, which causes more stack spills, and the monadic bind often causes heap allocation. Re: that third one: let's consider StateT, WriterT, and ReaderT. To review: - `StateT s m a` is isomorphic to `s -&gt; m (a, s)` - `WriterT w m a` is isomorphic to `m (a, w)` - `ReaderT r m a` is isomorphic to `r -&gt; m a` So if our monad is `ReaderT r (WriterT w (StateT s IO)) a`, that's equivalent to `r -&gt; s -&gt; IO ((a, w), s)`, and that's two registers lost and about 48 bytes of heap allocation, and those tuples probably have thunks inside. The inliner and strictness optimizer might get rid of some of this extra overhead sometimes but if the inliner is configured to inline stuff up to some complexity threshold *X* then logic dictates that feeding it a bunch of extra nested monadic binds is going to reduce that capacity to *X - * for some **.
These things are only used in the context of `runState`, `runWriter` and `runReader`. Doesn't that help at all? Honestly, I really don't have a clue, but it's interesting to learn about and I like to ponder how to make the evaluation of lazy functional programs similar to imperative evaluation, even when the code looks very different! 
Attoparsec is an amazing labor-saving device and is really fast for what it does, but you are not going to beat a hand-tuned pointer-twiddling parser for speed. I wouldn't go to those lengths in typical application code (especially if the grammar might change) but HTTP is pretty fixed and it's a domain where you really just want to get out of the way and spend your CPU cycles running business logic rather than shoving bits back and forth in the web server.
For me, the most exiting are the [typed holes](http://www.haskell.org/ghc/docs/7.8.1-rc1/html/users_guide/typed-holes.html). I suspect this is a life changer when programming in Haskell...
There's lots of really exciting changes in the release notes, I can't wait for release!
Don't wait! Try out the RC! :-) (and report bugs, fix up your packages &amp; pester the authors of your deps)
I don't have enough time to write much Haskell at the moment, so I feel I'd be a poor tester of the RC. Is there a rough time frame for the release? Obviopusly any bugs caught in the RCs will push it back, but can we expect days, weeks, months?
Same, this will streamline development so well.
It's more of a post for how to avoid the `WriterT [a]` anti-pattern. I just used logging as the running example.
You can optimize them away, but you must do so manually. This requires taking advantage of the monad morphism laws for `lift`. For example, let's say that you have the following computation: do lift action1 lift action2 lift action3 You can optimize that manually to: lift $ do action1 action2 action3 Now you are using a cheaper bind. Similarly, if you have something like: forM_ xs $ \x -&gt; lift someAction You can optimize that to: lift $ forM_ xs $ \x -&gt; someAction I do this all the time in my code to use the cheaper bind whenever possible.
Most of all i'm exited about ghcjs. It is time for me to settle on haskell-&gt;js library. 
Is ghcjs actually being included in this release? I googled a bit after the last post, and I couldn't find anything definitive.
Damn, I'm still on 7.4 (Debian stable)
ghcjs is installed via cabal and requires ghc 7.8 
And no systemd. That must hurt :) 
Mind you, I prefer to stay one something extremely stable (I just need a recent browser and Emacs 24 to be happy).
 [13 of 24] Compiling HCraft.Renderer.Program ( HCraft/Renderer/Program.hs, dist/dist-sandbox-ee7ad166/build/hcraft/hcraft-tmp/HCraft/Renderer/Program.o ) HCraft/Renderer/Program.hs:30:14: Not in scope: `createShader' Perhaps you meant `glCreateShader' (imported from Graphics.Rendering.OpenGL.Raw) HCraft/Renderer/Program.hs:30:27: Not in scope: data constructor `VertexShader' HCraft/Renderer/Program.hs:31:14: Not in scope: `createShader' Perhaps you meant `glCreateShader' (imported from Graphics.Rendering.OpenGL.Raw) HCraft/Renderer/Program.hs:31:27: Not in scope: data constructor `FragmentShader' HCraft/Renderer/Program.hs:32:14: Not in scope: `createShader' Perhaps you meant `glCreateShader' (imported from Graphics.Rendering.OpenGL.Raw) HCraft/Renderer/Program.hs:32:27: Not in scope: data constructor `GeometryShader' HCraft/Renderer/Program.hs:33:14: Not in scope: `createShader' Perhaps you meant `glCreateShader' (imported from Graphics.Rendering.OpenGL.Raw) HCraft/Renderer/Program.hs:33:27: Not in scope: data constructor `TessControlShader' HCraft/Renderer/Program.hs:34:14: Not in scope: `createShader' Perhaps you meant `glCreateShader' (imported from Graphics.Rendering.OpenGL.Raw) HCraft/Renderer/Program.hs:34:27: Not in scope: data constructor `TessEvaluationShader' HCraft/Renderer/Program.hs:35:14: Not in scope: `createShader' Perhaps you meant `glCreateShader' (imported from Graphics.Rendering.OpenGL.Raw) HCraft/Renderer/Program.hs:35:27: Not in scope: data constructor `ComputeShader' HCraft/Renderer/Program.hs:41:5: Not in scope: `shaderSourceBS' Perhaps you meant `shaderSource' (imported from Graphics.Rendering.OpenGL) HCraft/Renderer/Program.hs:55:18: Not in scope: `createProgram' Perhaps you meant `glCreateProgram' (imported from Graphics.Rendering.OpenGL.Raw) HCraft/Renderer/Program.hs:61:7: Not in scope: `attachShader' Perhaps you meant one of these: `attachedShaders' (imported from Graphics.Rendering.OpenGL), `glAttachShader' (imported from Graphics.Rendering.OpenGL.Raw) HCraft/Renderer/Program.hs:136:15: Not in scope: data constructor `UniformLocation' Failed to install hcraft-0.0.1 cabal: Error: some packages failed to install: hcraft-0.0.1 failed during the building phase. The exception was: ExitFailure 1 Code designed around an older/newer version of the library than was actually specified in the cabal or what? JFC there aren't even any version qualifications. build-depends: base&lt;5, transformers, bytestring, containers, GLFW, OpenGL, OpenGLRaw, monad-loops, mtl, filepath, random, vector, StateVar whyyyyyyyyyy
Update dependencies using `cabal install --dependencies-only`.
Wow, that's exactly what I do. However it will be tempting to backport GHC 7.8.
Well, I've built something just like this for 'fmap while retaining sharing' before. It actually worked fairly well for smallish structures with tons of sharing. With a `Traversal` it is just a bit more hazardous as you have peskier laws and visibility into the number of targets.
If you nuke away cabal and try and upgrade to 1.18 using git, don't forget to clone a fresh cabal (and checkout the 1.18 branch). I reported an issue this morning that the upper bound on HTTP was incorrect and it was promptly fixed.
No, I needed a specific version that the code was implicitly relying on but that the utterly useless build-depends clause wasn't documenting. I've fixed it and I'm PR'ing it.
like the concept. hopefully you can craft lambdas in the game.
(Author here) The book doesn't go into the actual theory of lambda-calculus, type theory, semantics and so on. But it discusses Generalized Algebraic Data Types (GADTs), functional dependencies, (closed and open) type families, singletons and even a bit of dependent types. Thus, you should get a good understanding of the type-level functionality available in GHC (and a general idea of Idris). The missing parts about type programming would be in the realm of datatype-generic programming (GHC.Generics), which is introduced in the examples but without explanation of how to develop actual generics.
FYI, **GHC 7.8.1RC1** is available in the [Multiple GHC Versions for Travis-CI](https://github.com/hvr/multi-ghc-travis) repository.
(Author here) The case with monads is that there are four parts to understand about them before being able to use them with its full power: - What is the general concept of Monad (and MonadPlus), - Which are the functions and constructions available when working with monads (sequence, mapM, foldM... and also monad comprehensions), - How to create monad stacks using transformers, - How and when to use the actual monads: Maybe, List, State, Reader, Writer, Logic... This last part is specially important, because Maybe, State or List encode very different meanings and have their special primitive operations to work with them, which must also be explained. Functor is a much more small type class, with a much easier interpretation (think of what is the list functor vs. what is the list monad). As mentioned before, Applicative is also discusses in great length: the chapter 10 introduces the idea via parsing, but then the book uses Applicative when working with aeson and digestive-functors libraries. It even discusses how to make your code more general (and sometime more beautiful) by using Applicative style instead of do notation.
Is the 64bit version of ghci/TH/ghc -e working?
IIRC the plan is to have a new 7.8 based HP in March/April. There have been some delays for the HP independent of GHC, and it was decided to basically scrap the planned 7.6.3 based release as there were no changes to the compiler and no major library additions. So it'll be a bit late, but you also don't have to wait another 6+ months for the next one with 7.8 as you'd have otherwise. Good decision, IMHO. Have to look at the discussion on the HP mailing list for more specifics!
&gt; These things are only used in the context of runState, runWriter and runReader. Doesn't that help at all? I don't understand the question. Go back to the definition, e.g. of StateT: newtype StateT s m a = StateT { runStateT :: s -&gt; m (a,s) } Here "runStateT" is just the projection function removing the newtype constructor. And since newtypes are representationally equivalent to the underlying type (except for the type tag, of course), "runStateT" is basically "unsafeCoerce". StateT values really are just closures of type "`Monad m =&gt; s -&gt; m (a,s)`". So when you write foo :: Monad m =&gt; StateT Integer m Integer foo = do a modify fib x &lt;- gets (/10) y &lt;- b x return (y+1) this is the same as if you wrote: foo :: Monad m =&gt; Integer -&gt; m (Integer, Integer) foo s0 = do (_, s1) &lt;- a s0 (_, s2) &lt;- modify s1 fib (x, s3) &lt;- gets s2 (/10) (y, s4) &lt;- b s4 x return (s4, y+1) For this example the optimizer will probably boil everything inessential away but you get the drift. BTW if you are interested in high-performance Haskell there is much more information in the GHC commentary, for example this chapter on heap layout is essential reading: https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects P.S. study that doc and then dump some Haskell code to assembler using "ghc -S". There will be some "a ha" moments!
somebody already asked in the youtube link, but a URL with the slides would be welcome.
Does GHCi work reliably? I'm thinking mostly about [#8094](https://ghc.haskell.org/trac/ghc/ticket/8094). I've hit that bug myself and it's a bit of a show-stopper IMO, but I simply don't have any Snow Leopard machines left to test with. That's why I named the builds this way. If it does work, great! I would really appreciate anyone else trying it too. (Hm, actually this working might not surprise me now that I think about it more! Previously (7.6 etc,) we always used the homegrown Mach-O linker inside GHC's runtime system, but the new 7.8 RC1 binaries now use the system dynamic linker. It's entirely possible #8094 was caused by some strange bug that the Apple linker can easily handle.)
Hm.. I've been thinking one can turn on TypeHoles with a language pragma in 7.6 already.
You can get a very crappy approximation of it when using `ImplicitParams` - it's something of a trick that's been known for a while. Replace any hole with an unbound implicit parameter, and GHC will spit out the type that implicit param should have, in a similar way. Typed holes are better because they'll also tell you the types of the surrounding terms, making it easier to understand at a glance (technically `ImplicitParams` will now also mention the types of the surrounding items if you have an unbound term, but I don't see any reason to use it anymore when `-fwarn-typed-holes` is on by default.)
Note that RC1 was built against glibc 2.15 (Ubuntu 12.04.) RC2 will be built against glibc 2.13 (Debian stable.) This was an error on my part and I'm sorry about that, so you should be able to try the next one more easily. But we'd still really like people to try the RCs so I'll try to get it out soon. :) Related: I will also very passively mention that earlier this week, a new DNS entry for http://deb.haskell.org appeared, and someone is working on it. Watch this space soon for good news. :)
With such an easy setup, I bet GHCJS really takes off.
I can really recommend [Typeclassopedia](http://www.haskell.org/haskellwiki/Typeclassopedia) for this purpose.
I've put them at http://eb.host.cs.st-andrews.ac.uk/writings/idris-lhug0113.pdf Mostly I just use slides as a brief introduction then dive into the live hacking though. The code is at https://github.com/edwinb/idris-demos and https://github.com/edwinb/Protocols
Well, another big advantage ghcjs has is the GHC name. I always forget Fay and Haste even exist. I've not used any of them and am kind of an outsider to the compiling to JavaScript stuff, so hearsay and memorability is all I have to go on. 
Would it be more or less efficient to implement `Auto` like this? data Auto i o = forall s. Auto s (s -&gt; i -&gt; (o, s)) 
I've been trying to get FRP forever and this is one of the better tutorials I've read. Looking forward to future installments!
Oh man, if there were something like http://mozilla.debian.net/, that would be **so** cool!
I should look into it. But last time I tried ghcjs was generating megabytes of js for hello world (even after google closuring). So it's Haste for me, 40K.
I'm using 10.6.8, and the lion binary, and can confirm that ghci runs reliably.
You do?! I thought you were pretty undecided about it... :-)
Sweet! I'm excited to ditch Fay.
Separate name spaces is the Lisp-2 mistake all over again. 
All I know is that the rewrites improve performance. I think `ghc`'s behavior in optimizing recursive calls is the culprit there, but I haven't proven it.
I've never managed to get haste to work, installation has always been too hard. 
Can I build current HP from source with 7.8 compiler?
Then I'll try to implement something along this lines and hope that it'll work for several hunderted megabyte datasets, too.
I actually didn't, but now that you made it seem like I did it's pretty funny.
From the release notes: This release mostly improves robustness and efficiency. The user visible changes are: - Agda-style equational reasoning (in Syntax.PreorderReasoning) - case construct now abstracts over the scrutinee in its type - Added label type 'name (equivalent to the empty type). This is intended for field/effect disambiguation. name can be any valid identifier. Two labels are definitionally equal if they have the same name. - General improvements in error messages, especially %error_reverse annotation, which allows a hint as to how to display a term in error messages - --ideslave mode now transmits semantic information about many of the strings that it emits, which can be used by clients to implement semantic highlighting like that of the REPL. This has been implemented in the Emacs mode and the IRC bot, which can serve as examples. - New expression form: with [name] [expr] privileges the namespace name when disambiguating overloaded names. For example, it is possible to write with Vect [1,2,3] at the REPL instead of the (Vect _ _) [1,2,3], because the Vect constructors are defined in a namespace called Vect. - assert_smaller internal function, which marks an expression as smaller than a pattern for use in totality checking. e.g. assert_smaller (x :: xs) (f xs) asserts that f xs will always be structurally smaller than (x :: xs). - assert_total internal function, which marks a subexpression as assumed to be total, e.g assert_total (tail (x :: xs)). - Terminal width is automatically detected if Idris is compiled with curses support. If curses is not available, automatic mode assumes 80 columns. - Changed argument order for Prelude.Either.either. - Experimental neweffects library, intended to replace effects in the next release. Internal changes are: - Faster parsing and elaboration - Smaller .ibc files - Pretty-printer now used for all term output 
How about just running `cabal update` via a cron job? That is if there aren't any issues with running multiple update commands at the same time. 
Good point. I hadn't considered The Simplest Thing That Could Possibly Work. :) That would solve my problem of forgetting to update from time to time. Thanks! More on brainstorming - the cron solution wouldn't allow for push-based updates. I can't imagine a scenario off the top of my head where the local index would need to be kept as close to in sync with the hackage index as possible, so this would really just be a thought exercise beyond this point.
You could also use `ImplicitParams`, upon compilation GHC will complain about "unbound implicit parameter". Alternatively just define `data Hole = Hole`, write it in place of undefined, and the type error will tell you what was expected.
As I watch this this morning it makes me sad that I'm about to go to work and code in Java and Coffeescript. 
Great News!
Any advice on how I can get cabal-install working with the RC? I downloaded the latest release from the website, but had issues getting it installed as the version of HTTP that it requires is incompatible with the latest version of base (or at least the build deps say it is) Edit: Never mind comment below says that this is fixed with a git checkout of the 1.18 branch. This worked for me as well.
Oh wow, that.. is amazing..
As to the why - this is for a personal development machine. I tend to like staying close to the bleeding-stable edge. Cabal sandboxes generally reduce my pain in this area, though I agree about 'extra work created by conflicts'.
Ahh, but I was installing this on a new machine. Thank you though.
Awesome! I've been excited to see some more tutorials around Nix, especially for developing Haskell.
or just don't delete cabal-install :) 
This is really cool (and timely). I missed the original Nix post but I'm heavily considering swapping out one of my old laptops (mbp) to a Nix machine. Either way I'm getting a new computer this Friday and I might see if I can get into using Nix/Vagrant for hs dev rather than hsenv.
That is awesome, I was actually working on exactly the same thing myself (loosely based on https://github.com/fogleman/Craft). But you shipped, congrats! Looks great so far.
Good stuff. Finance is already an area with many Haskell users.
I'm very close to jumping ship to NixOS at this point.
I don't know about others but I find this very disappointing news.
Ineffective psychopaths using unsound languages sounds scarier to me. 
You just disparaged a huge portion of the Haskell community.
I'm happy to hear about this. There's some exciting opportunities for seeing all that accumulated Haskell data processing knowledge trickle down to everyone else.
Better than sandbox? Binary package distribution for Haskell? Yes please! 
What is --ideslave for? Is it a replace for the --client stuff the vim mode uses?
How so, there's a distinction between the haskell community and douche-bag financial firms that hire from said community or douche-bag companies, like FP Complete, who want to write vaporware for them.
As the founder of FP Complete, frankly I'm excited. As we've said since our founding, a big priority of ours is to increase the adoption of Haskell in applied settings. In our analysis this will (1) improve the overall state of software, (2) create more opportunities for smart people to use great technology, and ultimately (3) improve the productivity of lots of hard-working organizations that need software to function. Currently many of Haskell's most enthusiastic users -- as well as many enthusiastic evaluators who are considering Haskell -- are in finance. So it seems wise to do additional work to support this growing community.
I'm glad you care what we do at all. :-) We will of course be continuing School of Haskell and FP Haskell Center, and continuing to support several major open-source Haskell projects, so I hope your worries will subside.
Why is that?
IMO, it was a mistake inherited from LISP. It was by no means invented for Common Lisp. 
Quite a few of the more prolific Haskellers work in the financial-tech space in one form or another. Which quite frankly, isn't that much different from the rest of the software industry.
It is a bit disappointing that in order to actually work with Haskell professionally you need to either do defense contracting or work in the finance sector. Though there are obvious exceptions, but I wish Haskell had a broader base.
NixOS is probably the only distro that seriously made me consider switching after I settled down on Arch some years ago. I've been toying around it in a VM for a while and hopefully will finally make up my mind to switch my main systems over. (or well, you know, btrfs has excellent subvolume support I could use) 
Can someone explain to me why there's so much hate for finance here? Surely at this stage it makes the most sense to push Haskell in an industry it's already got a toe in, rather than try and force it into completely new ones.
Those were Haskell's first big toeholds, but it seems to me we've been branching out more and more lately. So hopefully this state of affairs is only transient.
It's because GHCHQ doesn't want to maintain the seperate ghci linker anymore (because it's a pain) -- and so the only way forward was to use dynamic linking for everything so that ghci can use the system linker. Of course, this makes deployment and distribution a pain -- so it's a bit of a tradeoff. I still configure all my builds to static and use an old GHCI :P
4.1 according to the code.
Hmm, it's possible. I missed the comment about continuations in the original article, so was just extrapolating/guessing
I have some questions regarding nix: * is it slow? Do you find yourself waiting for compilations more than you'd like? * the package files look like they share some of the functionality of cabal files. Do you have to duplicate any configuration between these two files and if so are there tools to make that nicer? * you say that I could use Nix alongside another package manager but I'm not quite sure how that would work. Haskell [isn't awesome](http://www.reddit.com/r/archlinux/comments/1f26hg/why_does_haskell_suck_on_arch/) on archlinux with pacman. Are you saying I can use Nix for haskell packages and projects, and pacman for everything else? Thanks for sharing by the way. I was interested in Nix and NixOS several years ago but I was in no way comfortable enough with my linux knowledge to consider getting involved (and it sounds like it has more of a community than it used to).
We do like the finance sector, and we continue to work on bringing Haskell deeper into additional industries. Clearly it can be useful in several fields, and I expect we'll be making further announcements this year. Often when a technology makes big progress in one industry, other industries take notice -- that's one way things spread.
https://github.com/topos/hray For those too lazy to cut and paste. -- I was almost too lazy myself. ;)
I think science is another area where Haskell will take hold in the near future
I think there is hate for finance all over reddit and other places. Not especially here. People are angry at the way our institutions let us down in recent years. It's human nature for people to look for scapegoats and want to criminalize behavior. It's much harder for them to accept that the crisis was mostly caused by people showing up to work and simply doing their jobs in a flawed system. There are several cures, some of them quite simple, but the voters, regulators and legislators are pretty much blind to them. As a former Wall Street quant (nearly 20 years) I can sincerely recommend finance as a fascinating engineering discipline and a great application area for Haskell. I say that knowing full well the institutional issues were barely addressed and the industry remains a risk. We can use some more Haskellers there to help pick up the pieces next time! 
Yeah, I certainly understand that people don't like the financial institutions. But as you say, as an industry it's just like most others - people doing their jobs, and interesting ones at that. It isn't the engineers making the decisions that people don't like. It's only going to harm the Haskell community if we shun potential users, let alone a whole industry.
I hope so. Keep it up!
Well, that might be a one-time up-front cost. Also, could the shared JS that all GHCJS applications use be cached in a single shared file?
You mention setting up NixOS on an MPB. How is the hardware support? It's a very tempting proposition.
Please could you give an example of a piece of software that is unappreciative of the non-standard locations ? What does it do ?
Steam. It's a binary package with paths hard-coded in. To get it working is very complicated and usually involves setting up a chroot environment so that it can find everything it needs in the right place.
Interesting. Though the `undefined` approach means I never have to exit the editor (granted I can probably get compiler messages in there). Also, it has another use case: use it as a function to figure out what you're trying to implement when the context is slightly complex. Simple example: newtype EitherT e m a = EitherT { runEitherT :: m (Either e a) } instance Monad m =&gt; Monad (EitherT e m) where return = ... (EitherT m) &gt;&gt;= k = EitherT $ m &gt;&gt;= \ea -&gt; case ea of Left e -&gt; ... Right a -&gt; -- Huh? What am I trying to do here? undefined a `F1` over the `undefined` tells me I have to implement a -&gt; m (Either e b)
Is there a reason why you're a 'former' Wall Street quant? (No snark intended, genuinely curious.)
I love ghc-mod for just this reason.
Yes. I saw it all coming and left in 2004. :-) Actually I left to get a PhD in Machine Learning. Now I run a startup that uses Haskell for computer vision. (Insert plug for Alberto's easyVision package here.) We're launching an app in the US that lets users share and compare grocery prices. Financial transparency at its finest. 
Does anybody know, is Nix suitable for performance with long running tasks, like running a Yesod/Warp server? Or is it primarily designed for development, with speed taking a lower priority?
The new sandboxing options really help alleviate this. All of my projects have separate sandboxes. Makes the initial setup time a bit longer as you have to compile a lot of dependencies the first go around. This is only a problem if you continually bork your sandbox and have to nuke the .cabal-sandbox and cabal-sandbox.config.
Fair enough, I've added a "README.md."
Looking forward to reading part three. Also that web page styleis totally daft/great. 
Nix is absolutely suited to this sort of stuff! In fact, we have software called [Nixops](http://hydra.nixos.org/build/8593510/download/1/manual/manual.html) which is used to provision servers. This is going more into NixOS territory which I didn't cover yet (maybe I'll do another blog post on that), but the idea is that you can re-use the same single system configuration to configure multiple machines.
&gt; is it slow? Do you find yourself waiting for compilations more than you'd like? For compilation of my own single projects, it's no slower than `cabal build`. When I use `src = ./.`, `cabal build` caches results in `./dist` so when I change one module only that one module will have to recompile. When there are cross project dependencies, things get a bit more complicated. A change in project *A* will cause project *B* to entirely rebuild it (there's no incremental building to help here). However, overall Nix is *faster*. Because we have binary substitution and a very powerful build server, installing most Haskell libraries becomes a case of downloading binaries and dropping them into the right place. When you're installing things like Snap and Lens this is a huge win - you can save a lot of time. When you start having dependencies that are common between projects you don't even have to download anything, Nix realises that you already have the binaries so it only has to make a symlink. &gt; the package files look like they share some of the functionality of cabal files. Do you have to duplicate any configuration between these two files and if so are there tools to make that nicer? I do have to duplicate configuration, which is a bit of a shame. There is [`cabal2nix`](https://github.com/NixOS/cabal2nix) to automate the creation of these Nix expressions from `.cabal` files. &gt; Are you saying I can use Nix for haskell packages and projects, and pacman for everything else? That's right. Everything that Nix uses lives in `/nix/store` - it will never refer to `/usr/lib`, so you can carry on using other package managers and the two will not interfere. You can use Nix for more than just Haskell too - if you want you could install Emacs and Git from Nix, if for example you wanted those tools to be part of your per-project sandbox.
A lot of things that expect files to be in `/usr` (which we don't have at all) will usually explode pretty fast, and fail to start. 99% of Linux stuff tends to configure well these days and not make these assumptions. With proprietory software it's a little harder, but we have a tool called `patchelf` that rewrites paths inside the executable metadata to do the right thing. Here's how I package Pianoteq, which is a proprietory piano sampler: { requireFile, stdenv, p7zip, libX11, libXext, alsaLib, freetype }: stdenv.mkDerivation { name = "pianoteq-stage-4.5.4"; buildInputs = [ p7zip ]; src = requireFile { message = '' Please download Pianoteq and then use nix-prefetch-url file:///path/to/pianoteq.7z ''; name = "pianoteq_stage_linux_v454.7z"; sha256 = "17d0q4j8ccygya5rml2q5m49bqphd28w0vz48plxdyf3hbz6crif"; }; unpackPhase = '' mkdir $out cd $out 7za x $src ''; libPath = stdenv.lib.makeLibraryPath [ stdenv.gcc.gcc stdenv.gcc.libc ] + ":" + stdenv.lib.makeLibraryPath [ libX11 libXext alsaLib freetype ]; installPhase = '' patchelf --interpreter "$(cat $NIX_GCC/nix-support/dynamic-linker)" \ --set-rpath $libPath \ 'Pianoteq 4 STAGE/amd64/Pianoteq 4 STAGE' ''; phases = "unpackPhase installPhase"; } 
Well, you have to find a business plan. I suspect that the Finance "industry" was already a big target for their IDE. Maybe they did not advertise it directly because they did not want to hurt the feelings of some people. But it make sense for your own credibility to have a specific marketing target. And in the end, if it helps to grow a mature haskell ecosystem, it will be good for everyone.
They have their web based IDE for Haskell, which is quite slick.
You're right, it was not difficult to convert to `Tagged`. I have [implemented a `Reader` monad][1] using an implicit dictionary argument. Unfortunately, it doesn't look like the [GHC Core][2] for my toy problem optimizes as well as I had hoped it would. In the end, you still have to pass around an argument, one way or another. [2]: https://github.com/HeinrichApfelmus/optimize-monad-trans/blob/master/results/OptimizeMonadTrans.coreRefl.hs#L2205 [1]: https://github.com/HeinrichApfelmus/optimize-monad-trans/blob/master/ReaderReflection.hs#L2
&gt; whyyyyyyyyyy WIP?
The PLS group at UNSW (which produced Repa, Accelerate and DPH) are hoping to make Haskell good enough to replace fortran for scientific computing in the long term. 
Well that about does it. I'll give it a shot and see how things work. Maybe if I can get things working well enough I might even be able to safely give ghc-7.8-rc1 a try.
If you encounter problems, please complain loudly - that we can try and smooth things out for the next potential convert :) Good luck, hope you enjoy it!
Sorry for that. It seems to work now with the version numbers provided by /u/Mob_Of_One.
&gt; people showing up to work and simply doing their jobs in a flawed system Are you aiming for the quickest way to a Godwin point?
&gt; people doing their jobs Same question as above: Are you aiming for the quickest way to a Godwin point?
We are very active in #nixos on FreeNode, but the mailing list is equally active. I tend to prefer IRC, but you can choose whichever mode of communication suits you best.
I assume you mean a Macbook Pro? I haven't used NixOS on such a machine, but I imagine the hardware support is as good as whatever the kernel gives you. I have a ThinkPad T440p and it seems to work quite well, though the track pad is shitty. I'm not sure whether that's a NixOS problem or just lack of support in the kernel (I run 3.12.9).
 Right r -&gt; Hole r -- Either this, ?dunno r -- or this. :-)
&gt; one-time up-front cost It's too big Anyway it just doesn't have a single advantage over haste which is already working and producing much less (even without minimizing). 
If I install Nix within my existing Ubuntu/Debian/whatever install, try it out for a bit, then decide I don't like it, is uninstalling it as easy as `rm -rf /nix`?
So, Steam was my main hold-out probably, but I recently saw the Nixers got Steam working as ocharles said. So my FTL/Starbound don't have to wait (that's a pretty big requirement I'm afraid.) I also do want my copy of VMWare Workstation, but I'm not convinced this is undoable and I'm willing to work at it. I've been using Nix/Nixpkgs on my Ubuntu machine. It works great for development and whatnot as laid out in the OP, and Nixpkgs is very easy to contribute to. For the most part I'm very happy. On a related note, and a first stepping stone, I just used [NixOps](http://hydra.nixos.org/build/8593510/download/1/manual/manual.html) to recreate my Hetzner machine as a brand new NixOS instance, and so far it's working smoothly (I've also gotten EC2 deploys to work.) The main reason I want this is mostly because as I've been doing more system automation and work on Haskell.org and my own machines, the idea of a unified language to specify my system configurations and do deployments from is my dream. It obsoletes a lot of other tools in one fell swoop. Furthermore, the ease of use, hackability of Nixpkgs, and the surprisingly pleasant config language make expressing configurations and keeping them up to date actually tolerable. At this exact moment I have other reasons to not switch (and nix/nixops are fine on Ubuntu,) but the list of reasons seems to be getting smaller very quickly.
The kinds of tools that are good for data analysis in finance will also be good for data analysis in science. 
That's easy. Finance is all about making money from money, without producing any concrete value. If someone manage to pull 100 bucks from the economy that way, then someone else in the system will have to produce real work that is worth 100 more bucks that what he will actually earn. It is really *that* simple: although legal, finance is mostly economy parasitism. I am also saddened to see great talents spending intelligence there. They won't make this world a better place that way.
Every Nix user is a Nixpkgs developer. There simply aren't enough maintainers for an ordinary user to be able to avoid writing his/her own packages a lot of the time.
I'm using Arch too. I previously used Gentoo but I found it too high maintenance. Something like NixOS interests me, but I don't know if I would bother tweaking Nix expressions forever. In-place modification irks me, but I'm afraid I will use NixOS features a lot and then kind of forget it and neglect maintenance (this perhaps wouldn't be a problem if the community weren't very small -- it seems that every user needs to write Nixpkgs sometimes).
"sin2 is no longer a pure function" Isn't this what unsafePerformIO is for? Or would it not work in this case?
Answering my own question: [it's nearly that simple](http://nixos.org/nix/manual/#idp24454512) (two commands instead of one).
&gt; I am also saddened to see great talents spending intelligence there. They won't make this world a better place that way. Yeah, they should all go to instagram instead. Or Zynga, or facebook. Why do all the moralistic arguments about how one's work should be profound and meaningful and improve humanity stop at finance? 
&gt; As the founder of FP Complete, frankly I'm excited. Really? How unexpected _
Is there any sort of roadmap being followed? I'd love to know what the major axes of improvement are in this respect.
You too! Hopefully you manage to convince the rest of Erudify that NixOS is a good choice ;)
Well, I don't have commit rights or anything. I'm really just a conntributor to the project - nothing higher up than that.
I think the idea isn't so much that one's work should be profound and meaningful as that it should *not* be actively harmful.
I do have to cringe when reading an advertisement like this. Then again, I am probably not the target audience. (I wonder who is, though.)
No, they are not the same. Zynga or Facebook will not ask for a bailout to the tune of $700 billion dollars when they fail or put each other at risk when one fails. Moralistic arguments apply because what they do effects the lives of the vast majority of the population in material ways. http://www.nytimes.com/interactive/2009/02/04/business/20090205-bailout-totals-graphic.html If 2008 economic disaster is an example of what investment banks are capable, they make the world a worser place. And FP Complete wants to join in and get some of that public handout. Why? Because they have nothing. 
(That's not really my point, but) Pandas: http://en.wikipedia.org/wiki/Pandas_%28software%29
Sorry, but how else do you propose that the savings of millions of people get invested in public works projects? How would you protect retirement funds from inflation? Capitalism is about putting your money to work for you; the people who best figure out how to do that deserve a cut.
You cut out the "mostly", affecting the context. I'd prefer not to write a dissertation here on my theories of cause and cure for financial crises. It's better heard over a beer.
I tried to play with NixOS last year but I failed to make it work in VirtualBox. So that put me off exploring it further. But just running Nix seems like a nice option. I'll try that out.
I am also one of those people who think that investment banking or the NSA are intrinsicly bad things that threaten our democratic systems. The fact that Haskell is used successfully in these areas makes me a bit sad and alienated me from parts of the Haskell community a long time ago. I am glad to see this controversial discussion here, because it restores my faith in the critical thinking potential of Haskell users.
Those of us on MacOS X may be interested to know that some Nix packages seem to work/build on Mac. I haven't had any time to work on it lately, but currently I have 31 packages from Nix, and 55 from Homebrew, the hope being that to be able to eventually cover all my needs with Nix. The prospect of being able to share a package tree between Linux and Mac (each being customised accordingly) is really attractive! Raises the hope of being able to give people fairly reliable install instructions or packages that will work on both platforms. Haven't tried the Haskell stuff though; tend to use HP binaries for that.
&gt; about putting your money to work for you But money does not work. People work. "Putting your money to work for you" is just a nicer way to say "putting people to work for you". Investing people savings in sane and healthy projects has been done for centuries in all societies. That does not necessitate any math or FP skills or superfast computer, which are used for very different kind of "investments" ;)
Elm taught me about FRP: http://elm-lang.org/learn/What-is-FRP.elm They have a lot of examples: http://elm-lang.org/edit/examples/Intermediate/Mario.elm http://elm-lang.org/edit/examples/Intermediate/Pong.elm More examples here: http://elm-lang.org/Examples.elm Here are some more full featured games/examples: https://github.com/Dobiasd/Breakout#breakout--play-it https://github.com/jcollard/elmtris https://github.com/GoranM/bluepill There is another more complex game I'm forgetting, but check out the mailing list for more: https://groups.google.com/forum/#!searchin/elm-discuss/game Elm's very close to Haskell and if nothing else will teach you the patterns. EDIT: Also check out these papers: http://www.haskell.org/haskellwiki/Research_papers/Functional_reactive_programming and this video by the creator of elm: http://www.infoq.com/presentations/elm-reactive-programming
Why would Finance necessitate math? Calculating risk involves complicated math, and is crucial in ensuring that your savings don't disappear overnight. Why would Finance necessitate FP skills? Well there will always be software written in every industry, and we all tend to believe that FP has its advantages. What's wrong with the application of FP in Financial software? Why would Finance necessitate superfast computers? Your average investor has access to a wildly more diverse set of investment instruments with lower overhead than in any point in history. Using the best possible computing hardware and programming talent for this infrastructure is prudent resource allocation in my opinion.
code maintenance is def an accurate selling point. modularity, purity, and types are vital here.
[Pandas](http://pandas.pydata.org/)
It would be great to get a really simple tutorial-style walkthrough of getting started with Haskell development using NixPkg... Like "first, you need to install NixPkg. Then you run this command to add Haskell Platform and Lens to a new project"... 
Plenty of investment banks have advertised their use of functional languages.
Use the same unsubstantiated claims as every other language/tool promoted in the last 40 years? I actually agree with much of this but it's delusional to think it can convince anyone who hasn't already seen it for themselves.
Technically she *is* probably an actress, just like anyone in any stock photo, but why should she be less likely to be a haskell programmer?
Well, I haven't had any issues with ghc 7.6 on that OS X 10.6 machine, and I have used ghci a ton and template haskell many times. Maybe not all OS X 10.6 machines are affected?
&gt; get to market in half the time I assume this doesn't include the time to learn Haskell?
How about: "I know you've heard this before, but using Haskell really does reduce the time spent maintaining your codebase because (1), (2), and (3)". Would that be better? We on this subreddit all know what (1), (2) and (3) are, so I have omitted the details :)
It's not entirely zero sum. Taking the simple example of a bank: Banks make their money from interest on loans (or returns on other investments) which they make with money given to them by other people as savings. The people with the savings derive obvious benefit as their savings now keep up with inflation due to the small interest payments the bank makes to them. The people with the loan do have to work more to pay off the interest, but they can derive immense value from having the capital _now_ - as anyone who has started a business using a loan can tell you. If you cut out the middle-man and got the person with savings to loan directly to the people needing the capital, then there is no "parasitism" going on, but most people just don't have the lending power that a bank does.
It is very useful to learn Haskell. I don't know how useful it is to do real development on it. When I say very useful, I mean it is very very very very useful. Beginners should just use fpcomplete IDE as a starting point.
Really the main things they need to get done are: 1. Performance improvements to generated code, particularly numeric code (LLVM backend came out of this area) 2. Extending Accelerate to support other hardware. 3. Extending Accelerate to support nested data parallelism 4. Improvements to fusion optimisations in repa (see series expression work) 5. Improvements to vectorization transformations for flattening nested into flat data parallelism 6. Generalisation of parallel programming techniques to irregular arrays 7. Most importantly, discovering more applications and examples and techniques that can be used to drive future improvements. Most of these things aren't necessary if you just want to write Fortran-in-Haskell, but they want something a bit more abstract than that - writing nice array combinators and have it be competitive with hand-written fortran code. (I'm not actually part of that group, but I attend their meetings)
Even better now that we can defer type errors to runtime, when we're trying to figure something out.
Could be! I have two 10.6 machines where every single release including &amp; after 2012.2.0.0 shows this bug. 32bit always works, 64bit ~50% chance of a segfault.
Considering the title, the target audience of the *arguments* in the article would be management types.
Most of the arguments for Haskell from the article are the indirect result of working with Haskell; good programmers. Kind of how Linus Torvalds thinks C is a better language for developing Linux because it is harder to use. I will not deny that Haskell is very powerful, expressive, and can help reduce bugs etc. But bad programmers will write spagetti code in any language. 
Pros: * Multiple versions of the same package can co-exist together, and it is handled automatically by Nix. Arch's way of handling this is having different packages, so you have `gcc` which is the latest stable version, and `gcc47` for GCC 4.7 release, `gcc46` for GCC 4.6, etc. All of them and their dependencies have to be maintained separately to ensure they work. In the worst case (e.g. you want gcc 2.95), you just end up dumping everything in its world into `/opt/&lt;package-name&gt;` which is basically what Nix does, except that it is done automatically. * Users can install packages locally, which is something that `pacman` does not support at all.^[1] This can be useful for sysadmins of workstations so users who need specific (or specific version of) certain packages can just install them themselves, without affecting the system or other users. Users who share the exact same packages will share the same files as well, so there is no problem of wasting space. * Nix at its core is a source-based distribution much like gentoo, meaning that you can easily tweak each package's source and build flags. While it is true that Arch can also achieve this to a certain degree with ABS, but it is not really as convenient when you drift too far away from the official PKGBUILDs. One case is that if you change a core library, you need to manually find out whatever that depends on it needs to be re-built, while Nix can figure out all of them automatically. This happens regularly with AUR when a package depends on a library in official repository that was recently upgraded, but the maintainer hasn't bumped `pkgrel` yet. * In NixOS, everything can be built from Nix expressions, and binary packages are basically cached build results to help reducing the horrible compile time typical in source-based distro's. Most importantly, they are hashed with every possible input to ensure that the binary package is really the same as if you build it by yourself. Nix can do this reliably because of its focus on pureness. * NixOS offers a declarative configuration system. Every aspect of your system can be configured in a centralized file (`/etc/nixos/configuration.nix`) using the same syntax. You don't have to worry about each application putting their configuration file everywhere around the filesystem, each with its own syntax and rules. The configuration file is also written in a purely functional language so you can easily abstract parts of your configuration. Testing and switching between configurations is also super easy. * Did I mention that Nix has a super awesome feature that you can import/export packages and all of its dependencies (a closure) and transfer them to other machines over ssh, so you can effectively reproduce an environment that is exactly the same as the origin? Just think how useful this would be if people attach their closures to bug reports! Nix will also only transfer what is missing on the destination, because if two packages are identical, they will have the same hash and can be safely skipped. Cons: * Nixpkgs at this stage still lacks many less popular packages, so you have to prepare yourselves to learn some Nix and write your own. This is not a particularly big deal if you are already comfortable with Haskell, but this is more than enough to scare away most users. * The central declarative configuration sometimes do not provide the option you need (for example, I want to handle some ACPI events like volume buttons manually, but NixOS currently only provides options to handle ac/lid/power events), or is not flexible enough, so you often have to end up writing your own modules that can put off many people. I mean, what was equivalent of copying the sample configuration, changing a few lines and save turned into writing or modifying full-blown module in Nix! Also, it's probably a violation of Arch's principle of simply packaging upstream provides and not alter them. * A lot of packages related to normal desktop usages are (semi-)broken or simply missing. For example, GTK+3 support was almost non existent until recently, and many things like IM module still doesn't work properly, which I'm tackling at the moment. * Dealing with proprietary software which likes to hard-code paths or programs that do not follow the conventional build/install process with little documentation is generally asking for trouble. * I miss `netctl`. :'( * (More of a complaint) Why the hell did they make the ISO in a format not bootable from a USB drive? Does anyone really burn ISOs to CDs to install a new distro these days? That's what I could think off the top of my head right now. Generally I believe NixOS is the future, but it is still quite far from its prime time. Even if you may not be switching to NixOS today, make sure you keep an eye on it. ^([1] I actually tried to hack `pacman` to do this at some point, and it kinda worked. It was too hacky for my taste however as I believe it was probably never designed to support this. For example, there are many checks simply hardcoded in `libalpm` that require certain actions to be performed by root IIRC.)
I said this in another forum where this article was posted, but why is that lady eating a giant pop tart?
Best way to sell Haskell so far is "Avoiding success at all costs". It seems to work much better than this article.
Working in finance doesn't mean you're getting bailouts or are even "screwing the little guy," despite popular perception. I wouldn't want to work for many large financial institutions, but there's a lot of interesting CS work in finance that doesn't touch any of that crap.
Yet some languages make writing spaghetti code harder, where others consider some kind of spaghetti to be the idiomatic way to tackle things.
Have you seen Jane St.'s participation in the OCaml community? Sure, prop trading firms aren't investment banks, but I don't think the financier hate is specific.
A lot of high assurance research funding (including my own) comes from the US security-intelligence machine. I'd be more ethically comfortable if my work wasn't used for military drones but I take the position that I don't care where the funding comes from provided my work is available for anyone to use.
Jane St and INRIA keep the OCaml community alive. Without either of them, it would be as unused as SML. Jane St for commerce and INRIA for research. 
Very interesting, thanks for taking the time to summarise all that!
I put in my vote, it seems like a really nice site aesthetically. 
Sure, it does have you no memory of TARP? John Stewart, Steven Colbert, and even South Park had a field day with this. Here's a relatively recent article on TARP: http://www.csmonitor.com/Business/Latest-News-Wires/2012/0127/132.9-billion-Remember-TARP-It-still-owes-you What it comes down to is simply this: do you want to build systems for asshole investment bankers?
Does the Dunning-Kruger effect mean anything to you?
Parent was mocking the notion of running partially broken code.
When someone tells me they switched languages in order to go multicore on an embarrassingly parallel problem like computing the fingerprints of thousands of separate images, I don't trust their technical judgment.
&gt; do you want to build systems for asshole investment bankers? No. And I don't. 
From reading [this paper](http://gsd.uwaterloo.ca/node/537) it seems like a tool that you use to help extract domain knowledge from a subject matter expert. It has two basic inputs into the system: abstractions and specific examples. You can then build on these inputs in two ways: * You can use examples to guess at potential abstractions that explain them and ask the domain expert if the abstraction is correct. * You can use abstractions to generate specific examples and ask the subject matter expert if the examples are correct. If they are not correct, the subject matter expert can then clarify by providing you with more abstractions or examples that would resolve the ambiguity. So in other words, it's a useful tool for extracting domain knowledge and/or specifications from people who are not used to reasoning as rigorously and methodically as programmers and converting that knowledge into a rigorous format.
However, a very large percentage of profits sustaining the huge firms in the finance industry are trading profits. The economists only explanation for those in an otherwise fair market is unequal distribution of information (which makes the market unfair). Oh, and the huge fortunes built up in finance buy considerable political power, which makes the whole country less than fair. Consider First Lady Clinton's successful effort on behalf of the little guys to persuade her husband to veto the bill that would have removed most of bankruptcy protection for the non-rich vs Senator Clinton's failing to even show up to vote for or against the next generation of the same bill when she represented and was funded by the financial industry of New York.
Getting the same issue on OSX with a Macbook Pro that supports OpenGL 4.1 (Radeon HD 6750M)
510 people now.
Is this like `pandoc`? [[Link](http://johnmacfarlane.net/pandoc/)]
Sort of. There are significant parallels, but generally there isn't much overlap: `pandoc`besides being an order of magnitude more mature &amp; sophisticatedis aimed at the interaction/conversion between various static file formats, whereas `neat` attempts to do something similar for template systems (which themselves can produce any kind of file.) Offhand, a rough analogy may be the difference between kinds and types.
Jewelry that looks nice, but isn't, really. Sorry for making my first post a joke, I'll try to be more constructive in the future.
Thanks for the explanation! That makes a lot of sense
They are not. Managers worry about results and do not always understand or care for the "how". If anything, they are more sensitive to brand; "Go was created by Google" is considerably more powerful an argument even if management was looking for a tech stack change. The easiest way to introduce Haskell to a company is to hire a Haskeller under a different title (we went for "data scientist"), somehow get authorization to deploy Haskell apps, and then let the advantages of the language result in you taking over more and more of the work. 
I'll admit that I'm a little worried about the rampant law violations, but I can hardly judge because I like to write non-proper prisms all the time. For example, I quite enjoy having things like `prism' printer parser` which certainly does not round-trip back to where you started (unless you feed in pretty printed text in the first place). However, when you start to use improper lenses/isomorphisms (`iso encodeUtf8 decodeUtf8` for instance), I wonder how much worse things are when you combine that with improper `zooms` too. Maybe this is all FUD though, because the API is lovely. Great work nonethless, Gabriel! :)
I'm a complete beginner at this point, most likely won't have a project and can't attend all full days (i.e. can't take Friday off). Can I still come and check it out?
Thank you! I wish there was a way to encode in the types whether or not a lens was law-abiding. Lens sort of has a system like that right now (subtypes of lenses respect subsets of laws), but I couldn't find an appropriate subtype for the laws that `pipes-parse` needed. This is what I was referring to when I mentioned trying to find a lens-like type that models `Getter` and `Focusing`, but not `Setter`.
That looks like a cool idea... After some search, it does seem a somewhat crowded market, altough most of them seem to focus mainly on giving _you_ the best prices, but seem to fetch those prices either online or from 'selected' partners. So I'm guessing you also focus on receipt scanning (like cart crunch) or other means of digitalizing non-web prices?
718 :)
Sometimes running partially broken code is exactly what you want, during development. Sometimes.
Woah, that went up fast. I voted this morning when it was around 100.
That's a great summary. Domain modeling is one of the applications of Clafer. Others include variability modeling in product-lines, architecture modeling, multi-objective optimization, and meta-modeling. There are many example models on the [Clafer Model Wiki](http://t3-necsis.cs.uwaterloo.ca:8091/). 
Let me try to explain Clafer from Haskell's point of view. Clafer serves roughly the same purpose as ADTs or GADTs - you can define a static hierarchical data structure. The biggest difference is that you have 1) inheritance in an object-oriented sense, 2) references, and 3) a very powerful constraint language. Let's use a simple example: abstract Person // this is an abstraction of a Person age -&gt; integer [ this &gt;= 0 ] xor maritalStatus // xor means that exactly one of the children must be instantiated neverMarried married [ age &gt;= 18 ] // can only be married when 18 or older spouse -&gt; Person // when married, one has a spouse [ spouse != Person ] // who must be different than yourself (this Person) [ spouse.maritalStatus.married.spouse = Person ] // and who must be married back to you divorced [ age &gt;= 18 ] Alice : Person // this is a partial instance specification [ married ] // we assert that Alice is married Bob : Person // another partial instance specification Carol : Person [ age = 5 ] // she cannot be married nor divorced when you give this specification of abstractions and instances (model) to an instance generator, you'll get all possible instantiations that satisfy it. One of them will be === Instance 1 === Alice age$1 = 29 maritalStatus$1 married$1 spouse$1 = Bob Bob age$2 = 31 maritalStatus$2 married$2 spouse$2 = Alice Carol age$3 = 5 maritalStatus$3 neverMarried Or, if your constraints are inconsistent, you'll get an UnSAT core. You can run that model in the [Clafer IDE](http://t3-necsis.cs.uwaterloo.ca:8094/). So, the model is roughly what you write as a ADT or GADT + constraints, and the instance is what you'd potentially have in memory when running the program. Here's the abstractions and the generated instance expressed in Haskell. Note, that you cannot express the instance specifications in Haskell as they are specializations at the type level. -- the abstractions data Person = Person { age :: Int, maritalStatus :: MaritalStatus } data MaritalStatus = NeverMarried | Married { spouse :: Person } | Divorced {- the instance specifications data Alice "inherits from" Person "such that her maritalStatus is Married" data Bob "inherits from" Person data Carol "inherits from" Person "such that her age = 5" -} -- the generated instance alice = Person { -- should really be alice = Alice {...} age = 29, maritalStatus = Married { spouse = bob } } bob = Person { -- should really be bob = Bob {...} age = 31, maritalStatus = Married { spouse = alice } } carol = Person { -- should really be carol = Carol {...} age = 5, maritalStatus = NeverMarried } So, in this case, the `xor` constraint is naturally expressed in Haskell by alternative constructors. However, the other constraints cannot be expressed. Also, Clafer has other group constraints, such as `or` (one or more children), `mux` (at most one child) - you could use `Maybe`, and explicit `n..m` (at least `n` and at most `m` children). Also, a Clafer instance generator can be thought of as a QuickCheck, which takes the constraints into consideration. Hope that clarifies. Does anybody have any ideas how to express Clafer-like constraints in Haskell? 
Here is a [nice post](http://r6.ca/blog/20110808T035622Z.html) by roconnor about *-semiring. Am I correct that the difference between a seminearring and a semiring is that it is distributive only on one side?
Funny, at my research meeting yesterday I brought up the possibility to move to a lens-based parsing solution. It was ultimately decided against because, apparently, lenses can be "prohibitively complicated". I would love to argue against that, but then when I read this blog post I'm a bit lost D:
&gt; unless you feed in pretty printed text in the first place I consider those arguments to be pure pedantry. Equality up to isomorphism is enough for everyone. There's no real improper isomorphism, there's only not enough round-trips to have hit the fixpoint.
Is it the types or the implementation that confuses you?
That's really the question though: what does your isomorphism mean? These are really sections and retracts, but we can see the stability on one side. The Prism itself is an assertion that we care about the parsed structure. I actually like all the pedantry because it serves as a reminder to think about it. I don't think there's anything deadly about building "improper" lenses with good documentation though.
I'm not up to date on the Haskell-ese. I don't know what a pipe or a producer is, I don't know the lens laws by heart, etc. I've been working on improving my understanding of the ecosystem, and at the same time I've been learning German, and it seems like German is easier. I have the same relationship with category theory - it's not wizardry, it's just a different, highly abstract language.
You might be surprised. Learning Haskell at home in your off hours is one thing. Learning it while contributing to an existing product while having full access to people who can help (in person!) is vastly easier. In practice, it's not much more than the spin up cost you already pay to teach someone how to be productive in your particular software stack. 
You know that a language is mainstream when... the authors of open source libraries in that language start flamewars over whose library is better. The Hask-ecosystem is really on the rise, hehe.
There are lots of shopping list apps but none of them let you share and compare grocery prices at your local store. Stores don't share this data, mostly for good reason! So we focus on letting users point the phone at the shelf label. Haskell gets involved shortly after that. Receipts are a partial solution, but have some serious issues of their own. 
Nice... there are lots of ways, and I think an intelligent combination is the best solution. But indeed the 'current picture' is always more 'updated' than any other means. 
Kudos to Alex McLean who seems to be getting quite a bit of notice these days.
We've talked about adding aliases for the various kinds of 'improper' optics, but without extending the normal typeclass hierarchy in untenable ways, you can't enforce the discipline. On the other hand, if you work through everything operationally, `lens` itself doesn't care about the lens laws. They are strictly for the user and to ensure that the operations `lens` does do are canonical. If we didn't have the laws then more of the operations in `lens` would have to have an explosive number of variants for the number of passes taken, etc.
Programming languages are tools, not religions. If a tool is used by the baddest, evillest criminals, then it's a mighty good tool. Simple as that.
Yep. The left (sic) distribution law from `MonadPlus` / `Alternative` is the only distributive law for a _right_-seminearring. 
I mentioned the relation between semirings and seminearrings in emails to the list, but probably should've included it in the post as well. We have that `S` is a semiring just in case all the following hold: 1. `S` is a right-seminearring 2. `S` is a left-seminearring (flip the direction of the two laws) 3. The multiplication for `S` has a left-/right-identity element 4. The addition for `S` is commutative ----- So if you just have distributivity on both sides (i.e., only the first two conditions hold), then we get a seminearring without the side qualification. However, in practice, people usually use "seminearring" to mean a right-seminearring similar to how people often say "(semi)module" when they actually mean a left-(semi)module. And then once we have a semiring, we need to add the asteration operator in order to get a \*-semiring. I don't know off hand whether anyone has looked much into adding asteration to unital seminearrings. Seems like it should make sense though. However, we might only get one of the two asteration laws (i.e., either `x^* = 1 + x.x^*` or `x^* = 1 + x^*.x`; most likely which one depends on the chirality of the seminearring). For more on how this menagerie of ring-like structures are related, [see this old post](http://winterkoninkje.dreamwidth.org/80018.html). In particular, seminearrings are near the top of the diagram.
Yeah, if we don't generalize the distributivity laws then this really only requires `Alternative`. Though I firmly believe that `Alternative` and `MonadPlus` should agree when both are defined. Notably, the only laws currently required for `Alternative` is that `empty` is the left-/right-identity for `(&lt;|&gt;)` it doesn't even require that `(&lt;|&gt;)` be associative! So saying that `Alternative` should form a right-seminearring is still making progress IMO. If we accept that `Alternative` should form a right-seminearring, then IMO we should either (a) abolish `MonadPlus` entirely, or else (b) have `MonadPlus` mean that the generalized distributivity laws hold.
Well, I know `Maybe` is fine with the non-generalized laws. So if there're counterexamples, then it's gotta be one of the other usual suspects...
I'm not big fan of custom templating languages. I'd rather use Haskell to structure my code instead of having to learn a new way to do loops, conditionals, subroutines, etc.
What about distributive laws of List over a monad, how does that relate to MonadPlus? Also right modules of List.
This doesn't really have anything to do with lazy evaluation. You're putting the action `accumulate (return ())` in the dictionary, when I think you meant to run the action `accumulate (return ())` and put its result in the dictionary.
Sure, though `neat` doesn't attempt to introduce a custom template language; the goal is to reuse existing template languagesstarting with Django'sto the extent possible, so that they can be used (and understood) by anyone familiar with them. Some times it can be clearer to express an output specification in a different syntax, and `neat` gives you that option (including the ability to mix-and-match, if desired.) Or you may work on a project with someone on the front-end who's more comfortable with template languages, for instance.
&gt; The pipes-parse API now improves on the conduit parsing API by providing stronger guarantees for equational reasoning and improved leftover support. Can you rephrase that in words that make less of a value judgment and provides more actionable information? Something like "pipes-parse API lets you declare (in the type system) whether transformation preserves or drops leftovers". 
This music (if you wanna call it that) is mesmerizing as hell.
I'm getting search results from 5 years ago when I search for Yesod. From my sample queries it looks like just searching Reddit and Hackage, both of which have their own search engines (and Hackage's is displayed much better than this one). Which sites are you using as "big"? It seems like it would be a good idea to remove Hackage from the results, or at least include an option to search specific sites. Right now it's very noisy and not useful (to me) at all. Maybe indexing blogs and such would be better than indexing Reddit/Hackage? EDIT: also, hitting enter doesn't execute the search, which is really annoying.
I agree that `Alternative` and `MonadPlus` should always agree when both are defined. I also agree that I like the idea the `Alternative` can model a right seminearring. I often use it that way, even when I know it can be a lie. The current lack of `Alternative` laws are a reaction to the fact that folks can't agree on them, not because we don't want more. It is just that the design space is annoyingly large, and the number of existing usage scenarios to cover is even larger still. Worse, most users never 'feel' the pain of there being multiple candidate sets of laws with different instances having wildly different semantics, because it all 'feels' right when they go to use it in the current ad hoc situation. However, that said, I've had to be careful in my code not to make that assumption as many monad transformers are not `MonadPlus` transformers, in that they take control of the `mzero`/`mplus` and assign it different semantics about where and how it can backtrack. This is unsatisfying. As a pragmatic measure, I'm unsure how much traction you can get here or precisely what it is you are advocating for. We've had various `MonadPlus` reform proposals running around since before I joined the community. A weakened `MonadPlus`/`Alternative` law that doesn' talk about `(&gt;&gt;=)` at all is profoundly unsatisfying. The existing situation is also unsatisfying. The possibility of losing existing instances without a story for how to move forward is _also_ unsatisfying. Hence I hesitate to sign up until I know more about what I'm getting myself in for, and I find it hard to get motivated to solve a problem that hasn't moved in so long -- and especially one for which most users really don't notice is an issue, despite using `Alternative` and `MonadPlus` on a daily basis.
Awesome idea! Thank you! Some remarks. I want to press enter to search. Also, the results could be a bit more relevant. "liftM" brings up tons of permalinks to posts in a haskell-cafe thread.
I patch for enter key. I can add a checkbox to search on "hackage". Or I can look how I can make best index.
This is some nice work, but I'm sorry that I won't be using it. It's easier (for me personally) to use existing search engines. For example, in duckduckgo I can append !hoogle or !hackage to do the search I want, and it will already be more tailored results than what you provide. I don't see how this will make my searching any easier.
&gt; As a pragmatic measure, I'm unsure how much traction you can get here or precisely what it is you are advocating for. The email thread started off with someone noting that the purported law `m &gt;&gt; mzero = mzero` doesn't actually work, and some discussion about how to deal with that (just drop it, add new laws to cover parts of it, etc). In that thread I mentioned seminearrings being probably the thing we want/mean. Then I thought, "hey, noone knows what those are; I should write about them." So I did. The blog post is more about what seminearrings are, and the `MonadPlus` parts are mainly for why you might care. As far as proposals, I do think we should say that `Alternative` forms a right-seminearring; and we should have some other type class for dealing with those that satisfy catch-like behavior but not distributive behavior. However, it's worth pointing out that I'm not really willing to spend too much time advocating for this since, as you say, there's a tremendously long history of people not willing to budge on the fact that there are two distinct things going on here. I'll just be sure to apply my proposal whenever I get around to forking my own language ;)
Incidentally, it may be worth noting that the failure of the `(m &gt;&gt; mzero) = mzero` "law" is also what's causing the problems with distributivity, at least for monads like `IO` which have more than one "failure" value hence don't even satisfy the weakened law.
For the moment : http://book.realworldhaskell.org/read/* http://hackage.haskell.org/package/* http://hackage.haskell.org/packages/* http://haskellnews.org/ http://learnyouahaskell.com/* http://planet.haskell.org/* http://search.oreilly.com/?q=haskell&amp;amp;x=0&amp;amp;y=0 http://snapframework.com/* http://www.apress.com/9781430262503 https://www.fpcomplete.com/school/* http://www.haskell.org/pipermail/haskell-cafe/* http://www.reddit.com/r/haskell/* StackOverflow i add http://www.yesodweb.com/* Maybe make a form to submit your site to be indexed?
Perhaps I'm the only one, but I don't like the fact the `lens` is creeping into everything. I understand that it's a great, useful library, but it's a difficult library for an intermediate user, let alone a beginner. `lens` is quickly becoming required knowledge for anyone interested in Haskell. I see blog articles posted here or articles on the School of Haskell that are intended as tutorials for something, but halfway through they chuck in some `lens` stuff. Many Haskellers, especially the library producers and those on /r/haskell, are very into the theoretical stuff. This isn't necessarily a bad thing, but libraries like lens are starting to require more and more theoretical knowledge to use and understand. People that work with category theory by necessity are able to handle very abstract ideas, but the average user will generally not be as good at this. I think this is my main problem with including lenses everywhere - the people writing the libraries are disillusioned with how much of a barrier lens is to a beginner without any exposure to category theory. I know the community used to hate the idea that "you need a PhD to write Haskell", and the community worked hard to remove that stereotype. But now it's starting to feel like we're willing walking right back into it. &gt; This library is very minimal This is why I always liked `pipes`. It was simple, and you only needed to know about monad transformers to understand and use it. Perhaps `pipes-parse` is still minimal in terms of provided features, but by throwing Lenses into the mix you're vastly increasing the mental prerequisites. Lenses are a deep, dark rabbit-hole that will scare new users away from your library. --- In the Pipes.Parse.Tutorial page, all of the links to lens functions are broken. I also don't like the constant comparisons to `Conduit`. I realize that the libraries are very similar in their goals, but list your features and if they're better they will stand on their own. Constant comparison to a competitor detracts from an introductory article, it feels like you're trying to sell me something.
Wasn't mentioned in the article, but he's the author of the [tidal](https://hackage.haskell.org/package/tidal) library on hackage. Very interesting work.
Make libraries, not war!
Yep. That one bites even `Error`. It can only be viewed as somehow holding if you view all 'failures' as indistinguishable -- a gross generalization of the view of how we handle all _|_'s as being the same at least informally, but it is a terrible statement of that viewpoint.
I work in finance for a ridiculously ethical and awesome employer. I acknowledge that there are tons of greedy people in the industry that do evil things for their own gain, but that is not all that finance is, and I think it is very illogical and shortsighted to assume that just because there are people who have manipulated the system to gain enough power that they are basically immune to the law that the entire industry is useless or even actively harmful. If you don't actually understand how finance works, please don't criticize the whole thing and everybody in the industry. It's blatantly ignorant and disrespectful. I used to work in defense contracting. I found that industry, overall, much more ethically compromising than finance, and yet I don't see so many people criticizing it. Again, that doesn't mean the whole industry is bad (heck, one of the Haskell community's favorite companies is in it, and the company I worked for was actually pretty great, too), but I think it's worth pointing out the apparent contradiction.
How about something like: "The `pipes-parse` API lets you propagate leftovers upstream, encode leftover support in the type system, and equationally reason about code with several theoretical laws"? Edit: I fixed it to use the above sentence.
For that matter, many roles in finance serve to balance power in favor of the little guy versus the big guy. For example, a market maker allows the little guy to get the price he expected when he tries to make a successful trade; without market makers, only the big guys would have enough power to arrange deals with such low risk.
Im not sure what the point of this is: I can just use google if I want to search for anything. 
"Putting money to work" is actually just a way of saying "paying people to be productive." That's what investing is. Putting money into promising businesses helps the business and yourself. It's not something that only benefits the investor. Conventional economic theories (yes, they largely suck, but are the best we have for now) mostly predict that a profitable business is going to also be beneficial to a productive economy. Finding businesses with the potential for the most profit is a very computationally expensive job, so it totally makes sense to use expensive computers and hire bright minds to do it. It actually seems kind of silly that somebody would suggest that it would be more efficient to find good investments "by hand." Also, computers aren't only used for finding good investments, but also for facilitating more efficient investments.
My opinion is that lenses only appear complicated because they are implemented within the language, so all of their semantics are exposed to the user. My recommendation is to approach lenses the same way you would approach learning about a language built-in: you read the documentation/tutorials for that language to learn how to use that feature, and if you are a beginner just treat the lens type errors the same way you would treat parsing errors (i.e. go back and read some more documentation instead of working through the type). Also, using `lens-family-core` helps for beginners because the types are much simpler. There are three reasons I compared to `conduit`'s parsing API: A) The `pipes-parse` API scarily resembles the `conduit` API. This was not intentional and the resemblance is remarkable. B) Getting leftover propagation correct is not trivial at all, and without that comparison most readers who don't live and breath parsing would not realize how neat of a feature this is. C) In a couple of cases `conduit` has used features from `pipes` without acknowledgement, and I've heard people remark on several occasions that `pipes` took something from `conduit` when it was the other way around. Making noise about this sort of thing is how I protect the `pipes` brand for being a cutting edge streaming library.
&gt; My opinion is that lenses only appear complicated because they are implemented within the language That's not why they seem complicated. It's the generality of them that scares people. It's like monads - they aren't all that hard, but the generality of them makes them hard to grasp for a beginner. I know we all say "monads aren't hard!" but that doesn't change the fact that they cause trouble for many people. &gt; you read the documentation/tutorials for that language to learn how to use that feature That's how most people would approach Lens - by reading the documentation. But that doesn't automatically confer understanding. This is what I meant in my post above - people that are experienced with the concepts usually aren't the best judges of difficulty.
Then maybe the issue is that the documentation for `lens` could be improved.
Nope, sorry. :( None of the Haskell I work on is open source. 
When I search for `pipes`, three of the top four results are posts from Michael bashing `pipes` and the sixth entry is the actual `pipes` library. Compare to the search results for `haskell pipes` on Google.
I don't think documentation quality is the issue. It could probably be improved, yes - some of the tutorials read more like a textbook than a tutorial - but there's only so much you can do to make a hard topic easy. I don't think this is a *problem* with lenses, but it certainly changes the barrier for entry, and that's something we, as a community, must keep in mind.
If you want to use lens advanced features then yes it is a hard topic. However, if all you do is use them as simple getters and setters then I don't see how they are more difficult than the equivalent feature in another language.
A few days behind the official announcement as we had some last-minute fixes to do to the binary distribution system for cross-compilers. Super excited to have it out, as it's about 1000% easier to get things going now. If you have any interest in writing Haskell for iPhones/iPads/iPod touches, please give it a whirl!
Once again, I'll draw from the monad metaphor - sure you can use `IO` without knowing anything about monads, but is it going to work well in the long term? You'll be doing fine, then someone chucks a `replicateM_` in and you're back to square one. I guess my thoughts are: if the library is only using lenses as getters and setters, then why use lenses at all? And if the library's using them for more than that, then the difficulty is there, and users will have to deal with it. I'm not trying to debate how difficult lenses are, I'm discussing their difficulty in the context of them creeping into otherwise simple libraries.
I agree with you on the monad point. The simple interpretation doesn't prepare you for the complex version and the complex version is idiomatic. You make a great point there.
I think the higher salary you can command as a result of spending a decade or more learning to write high quality Haskell is unequal distribution of information. Clearly, Hillary ought to lobby for outlawing or reducing this unfair advantage that the Haskell community's most prominent members have on the US software engineering job market. I'll write to her now.
I found this helpful: https://github.com/gsdlab/clafer/wiki
It's a little toy search engine made by one guy vs. Google (one of the largest companies in the world). Are you expecting him to implement PageRank? Also, have you read those posts? He's not just bashing pipes. His complaints are valid, and I've personally experienced many of them. 
No, I don't expect him to implement page rank. I was just pointing out a side effect of adding blogs to search results. I wouldn't be happy with him adding my blog either, which would similarly screw with search results for `conduit`.
Lucky you :)
Here are the numbers now: 1. Python (1514) 2. Haskell (1047) 3. C# (735) 4. Java (702) 5. C++ (621) 6. C (551) 7. PHP (429) 8. Clojure (415) 9. HTML/CSS (374) 10. Go (339) 11. Scala (268) 12. Objective-C (220) 13. Erlang (160) 14. Perl (115) I have no idea what this site is for, but I voted Haskell anyway.
Do you have any examples that use this? Can you do interesting things such as use a frp library to manage gui elements? 
Yes, you are right. There is unequal information in the real world, and the distribution of information is one of the things that decide who wins and who loses in the market. And that means that the theorems demonstrating that free-market economies produce optimal results are not about the real world. Economic losers are not unworthy, neither are they simply the necessary collateral damage of completely unpredictable market fluctuations. They lose a game played on a tilted field, hoping that someday they will win, believing the propaganda that the winners pump out touting the game as the one true engine of freedom and happiness. 
Works great, although you can't search for stuff like (&gt;&gt;=) or (=&gt;). (not sure if this would benefit but might be good if you don't know the names of them)
thank you, this is a bug. search web service not interpret well the query. I will try to solve it.
Tangential, but how is catenation different to concatenation? 
No ; I'm asking whether arguing along the lines of "just doing your job" in a context where the morality and the negative externalities of the industry are discussed is wise. Because [hint here at famous example].
They're synonyms. You can read [_catenate_](https://en.wiktionary.org/wiki/catenate) as "to chain" and [_concatenate_](https://en.wiktionary.org/wiki/concatenate) as "to chain together", but it's generally not a meaningful difference: it makes more sense to just use whichever one is used more commonly in the context of discussion.
Playing around with it! Great work! 
yes, yes, yes, can't wait to try it!
I went to create a YubNub command for this, and discovered that it's JS-only? No way to initiate a search from the URL?
The HN article is here: https://news.ycombinator.com/item?id=7196287 and it references this PDF file: https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/269031/New-Year-Honours-2014-PM-list.pdf Unfortunately I could not find SPJ's name anywhere in the file. 
Me neither $ &lt; New-Year-Honours-2014-PM-list.txt grep -i simon | grep -i jones $ 
Sirmon Peyton Jones!
At the risk of oversimplifying, would you say that the two main prohibitions regarding these improper lenses are: * Never use them as setters. * Never use them with *zoom* to construct a monad morphism that might be passed to *hoist* ?
Yes, everything is in js for reasons of simplicity. I coded in one evening so ... If I have time, I can do it with ws. Ex /?q=%s
Definitely the second one. Actually, you can use them as setters in some cases, like `over`, but then the following law does not necessarily hold: over lens f . over lens g /= over lens (f . g)
What I find amusing is on /r/scala there is an article "In defense of Scala: Part x". 
Not yet. What a pity.
A standard parsing `Applicative` isn't a right-seminearring. Take the following example: (return '_' &lt;|&gt; char 'a') *&gt; char 'b' By left catch, this is equivalent to `char 'b'`. However, if we were to use the distribution law, we would get a parser matching `"b"` and `"ab"`.
This isn't really the time for it? Honours generally get issued at New Years' and the Queen's Official Birthday (in June).
I couldn't either, but the words "Simon" "Peyton" and "Jones" do appear in the file. Someone did a faulty search, I think.
Sounds unlikely. If he were to get an honour, MBE or OBE would be the place to start with CBE and KBE (knighthood) coming later in life.
Your Algolishness.
this is real good. But how do I do it? I did `ghci` and it loads modules and I get a prompt `&gt;&gt;`. &gt;&gt; import Intro &lt;no location info&gt;: Could not find module `Intro' It is not a module in the current program, or in any known package. 
As well as his Haskell work, SPJ has been the chair of the Computing At School working group developing the school computing curriculum. Possibly this work would supersede his research work in the eyes of those who give out the awards. So maybe not that unlikely. 
I see a couple of small improvements: * You use a `get` followed by a `put` in a few places (in `push` and `compile` at least). That can be replaced with [`modify`](http://hackage.haskell.org/package/mtl-2.1.2/docs/Control-Monad-State-Class.html#v:modify). * `lookupWord` is a redefinition of [`lookup`](http://hackage.haskell.org/package/base-4.6.0.1/docs/Prelude.html#v:lookup). * There are a few repetitions of `getNextExpr &gt;&gt;= \expr -&gt; case expr of`. Would it be better to define those with pattern-matching function definitions instead of `case` expressions? At least for `accumulate` you can just move the `getNextExpr` out to where it's used. *Edit:* Never mind the last one; I missed the recursion.
One thing that I'd like to see expounded upon is what your ocharlesCommon file looks like. I'm trying to figure out getting local package dependencies in place, but I don't really understand the syntax very clearly for nixfiles yet.
Why wheb over scotty, then? Assuming you're doing it for actual use, and not just learning.
It's always nice to see new projects. I'm curious as to where the TH is in Snap that was giving you issues. The only line I can think of off the top of my head is a call to `makeLenses`.
Make sure you got it from https://github.com/tonymorris/course not the fork linked here. Follow the instructions given. Edit: I'm wrong.
How does this compare to Happstack then?
An impedance to beginners is a fair point; Haskell in general isn't particularly beginner-friendly (I'm trying to solve this for Snap actually) and that is definitely something worthy of attempting to improve. Saying you need to understand TH and the Lenses library to write a Snaplet is a bit dishonest. The only thing that is absolutely necessary to get started is an understanding that `_mysnaplet` turns into `mysnaplet`. Even if we threw out the Lens library, and used record syntax, we'd have to write getters/setters manually, which isn't too beginner friendly itself. "Why not use simple record syntax" is an argument against Control.Lens in general, not just in the context of a web framework, to which the Lens docs have the motivation: &gt; This package provides lens families, setters, getters, traversals, isomorphisms, and folds that can all be composed automatically with each other (and other lenses from other van Laarhoven lens libraries) using (.) from Prelude, while reducing the complexity of the API. Arguably, Lens' reduce complexity. &gt;I don't think there is a practical use case that nesting Snaplets and using lenses would be better than explicitly defining them on the top level except maybe to save an instance declaration. One use of Snaplets is for encapsulation of state. A possible use case for the one you're describing is nesting a database Snaplet under an authentication Snaplet, which gives the auth Snaplet access to the state (a connection pool) of the database Snaplet. Of course, it's completely possible to throw out Snaplets and just write all of the pooling/etc code directly with manual getters/setters, but I don't really see how that would be easier for beginners unless we're also throwing out encapsulation. Arguably, not having to deal with encapsulation and Lens' could be simpler to start with (and there's a place for a simple framework, which is what I assume you're working on here), but I don't think I'd write a large application in such a way. So yes, there are simpler ways to do things, but IMO we'd have to throw out a decent amount of solid architecture to do it. I'm looking at the issue as a documentation failure, rather than a failure of the framework (Snap is sub-1.0 release, so I wouldn't really expect the docs for beginners yet).
&gt; I've been working on improving my understanding of the ecosystem, and at the same time I've been learning German, and it seems like German is easier. I think this is my favorite comment all week. (Also! Ich muss mention [dieser Artikel by Mark Twain][1] jeder Time diese Subject comes up.) [1]: http://www.crossmyt.com/hc/linghebr/awfgrmlg.html For learning category theory people tend to mention mathematics textbooks of greater or lesser sophistication, and I've tried a few and always bounced off after the first chapter or two. The paper that finally built some intuitions for me about what the whole thing is about was [Physics, Topology, Logic and Computation: A Rosetta Stone][2]. (As for `lens`, I'm still looking.) [2]: http://math.ucr.edu/home/baez/rosetta.pdf
How does this compare to Happstack? As far as I know, Happstack also doesn't use TH as much.
Thanks a million times for the recommendation!
Yep - to me that sounds perfect - good summary of the utility with zero value judgment. This is a tricky issue. I have to give a talk in a couple days about some hardware I'm working on that is very similar to hardware my labmate is working on, and it's tempting to make a table of places where my thing is better or his thing is better :) But no human could deliver such a table without seeming biased, and everyone's bias sensor is calibrated differently. So instead I decided, I'll only mention his system by name when I'm recommending it over mine for a certain application. Applications where I'd be inclined to recommend mine, I'll just describe mine, not make any recommendation. Well, we'll see how that goes :) Also - thanks again for these great posts. I use pipes-binary in a couple places, and I was surprised to see decodeGetMany disappear and types change from Producer to Parser. It's great to be able to have the docs, ghc-mod, and a blog post all helping me update that code!
Good points. What I was getting at was that manually writing getters and setters more closely resembles what someone probably learned in their beginning Haskell class or Learn you a Haskell. Even if I know that my record _someLens creates a lens called someLens, it is still considered an advanced technique. There is still some magic that is happening behind the scenes that a beginner doesn't know. I want to lower the friction point between beginning to learn Haskell to building web apps. Not that other frameworks are poorly designed, but just that if you made things even 20% simpler and 20% more people went from reading a tutorial to getting started, that is 20% more people gaining confidence in Haskell. 
Is learning Haskell inherently harder than learning any other language? You say you've been learning Haskell for 6-7 months now. How long were you programming imperatively before you were able to make sense out of "writing an application that streams music from this API"? (if you are a prodigy that went from no programming experience to being able to understand how to do this, then my case falls apart here haha ) Perhaps this tautilogically-defined "real world" approach that seems to make things easier for you was only here because it was conditioned into you by months and years of training? And that someone who had never had any experience with imperative or object-oriented programming would have found OOP or Imperative's approach even more disorienting than you find the functional approach? The "easier" mapping for you might just be because if your years of familiarity with it, and might not be inherent in the technique itself, perhaps. Of course I am not here to say that you don't find learning Haskell hard :) Just maybe trying to shed some light on whether or not learning Haskell is inherently harder than learning anything else, compared directly and in a vacuum. (And of course, nothing is in a vacuum)
Not a prodigy, sadly :P Had a friend comment there that I possibly didn't explain this well enough - but my point is misunderstanding that I wasn't so much learning Haskell, but relearning how to actually engineer an application. It's quite easy to appreciate the differences between functional programming and imperative programming when it comes to any sort of blackboard example, but I think something I definitely didn't realise when starting was just how big an effect this would have on the overall structure of the application and how you approach the "blank page" problem at the beginning. You're absolutely right in comparing the lengths of time spent learning different paradigms, I was trying to get across how I didn't appreciate exactly much of an investment learning a new paradigm was in the wider, non-toy-example sense in the beginning :)
I think functional programming becomes easier when you start modeling problems denotationaly, i.e. thinking about values rather than control flow. This is not so different from OO, except than objects are usually mutabe and values are immutable. My advice would be to start with simpler modeling that isnt the textbook reverse/fibonacci. For example: modeling the rules of a board or card game. Avoid thinking about control ("i want a program to play chess") and think denotationally instead ("i want a function to list valid moves given a board position"). This will lead to explore data types, pattern matching and eventually lead to a better program.
Definitely agree with this, writing Sudoku from scratch in Haskell when I was learning taught me a lot about thinking about how to design a program. Definitely more than I ever learned in the intro to Haskell class I took in university last year (the same one /u/NSNO just completed) despite the fact it used a lot of the same principles.
Yes! Happstack is great and straight forward. However, I personally found it wasn't very clear how to share resources between ServerParts except passing them explicitly as arguments or rolling your own Transformer.
I actually teach Haskell in a second course on programming course at the university of Porto and use some simple games to examplify functional decomposition. Another challenge is to take some Haskell program that was derived in class and recode in Java --- this usefull to show that a lot of the design can be re-used in a more mainstream language (albeit the Haskell is generally a bit clearer).
Strong agreement from me. The alien-technology feeling of doing everything with pattern matching and recursion for the first time was kind of hard, also really fun. But trying to map a whole real-world problem into a haskell program for the first time: total mental blackout. Some things that got me past that (after LYAH and RWH of course): * Simon's Marlow's book. Tons of real-world problems worked there. * Making up some really simple problems to solve (eg: my first was a program that looks in its current directory for .jpg files, and writes out an index.html with those file names put in html img tags). * Playing with gloss. Snap. Working on problem where the domain and library determine the structure of the application. * Seeing if I can write a library instead of an application, in a way that makes writing the application trivial. Often * Categorizing the application: streaming data or bounded? One source or multiple? Big state-object in the middle modified by all the inputs, or no? Main function is to draw on the screen once, or animate, or write a text report out? Being exposed to different problems with different combinations of requirements and building up intuition about what tools to reach for (I often reach for STM and pipes first). * Refactoring - like the textbooks say :) Realizing I wrote a mostly-imperative program, and seeing how much I can squeeze the guts out of the main loop. After lots of practice, I just... stopped feeling like I don't know where to begin. I think this could potentially be an interesting haskell wiki page: hints for getting intuition about structuring your application? Or rules of thumb: for type-of-application-X, go for structure Y?
"When you start a CS course, you learn how the machine works." -- worst CS program ever
Hi jprider63, not yet! Schell Scivally is working on a Tetris clone but I don't think it's done yet. https://github.com/schell/blocks-ios/tree/master/Blocks You could absolutely do something incredible with OpenGL and FRP and barely touch Objective C at all  Stephen Blackheath (creator of GHC iOS) is working on a series of OpenGL iOS games using his Sodium FRP library. You can also definitely control Cocoa GUI elements, but you'd probably want a richer bridging system between Cocoa and Haskell than the raw C FFI. I'm updating a simple one called ObjectiveHaskell but we're most excited about https://github.com/mchakravarty/language-c-inline/ for this once we find a way to use Template Haskell. Creating some more examples would be an excellent way to contribute : )
Assuming that the OP is thinking about an imperative assembly language model, the irony is that it is *not* how the machine works but a programming abstraction anyway. A real CPU will typically re-order instructions, pipeline the execution, sometimes even translate instrutions into micro-code. I don't get why is the ilusion of sequential execution abstraction be considered by some as more fundamental.
&gt; "When you start a CS course, you learn how the machine works." -- worst CS program ever -- most constructive comment ever
Thanks for catching that. I'll add it to my checklist. :)
We formerly imperative programmers are all gone through this. The point is just that you have it engraved in your brain how to attack a problem imperatively. Your brain automatically "compiles" a problem in a language where memory cells are being mutated in a certain order. It's not Haskell, that is hard, it is translating from this your internal language into Haskell.
&gt; a Haskell equivalent of the Knights of the Lambda Calculus Knights of the Typical Class? Wielders of the Curry, Defenders of the Weak (Head Normal Form)?
Speaking as an attendee of the same degree program as /u/NSNO, we: 1. Start with digital logic design and build simple systems with NAND gates (most complex thing being a pong-like game made of LEDs and SR latches, MUXes, etc. 2. Assembly programming on an ARM7TDMI 3. Object orientated programming in Java 4. C and C++ 5. Hardware design in VHDL 6. Compiler Design 7. Revisit C++ with an eye towards mechanical sympathy, out-of-order superscalar processing, micro-optimisation, etc. 8. Haskell and functional programming 9. Prolog and symbolic programming. (roughly in order and all spread out over the course of 3 years) After that point students are allowed to choose their own classes and get to specialize in subjects of their choice. So the subjects you touch on definitely get time invested, just not right away. I agree that the illusion of in order sequential processing is just as much a fiction as declarative programming, etc., and that it's no more fundamental, but a lot of the effort in developing those systems (particularly superscalar processors and so on) has gone specifically into maintaining the fiction of in order sequential processing from the perspective of the high level programmer's model. Not that I'm really disagreeing with you, we're essentially saying the same thing, I just think that it's not *so* surprising that this is the structure that things take.
Also, catenation is more hip to say.
As someone who got some years left before university, may I ask if that is what a general CS programme looks like? I've thinking of aiming to get into a CS programme after high school but I honestly thought it was more theoretical (e.g. mathematics and such) in contrast to that which sounds more practical. Not that I favour one or the other (I basically don't know what either is about, Haskell is only languages I know and only programmed in it for a year). (sorry for being off-topic)
yah. too hard. i give up.
Surely, though, the point is still that the CPU at the most basic level reads a single instruction, modifies the state of its registers accordingly, then reads the next instruction. Bringing pipelining and so on into it vastly increases the complexity of the process, but it's still a sequential process. I'm no expert - I'd love to hear otherwise. Maybe my point of view just comes from experience with very simple hardware (i.e. programming microcontrollers) which conform much more closely to the imperative model. I'm not aware that modern processors are _fundamentally_ very different, though - just orders of magnitude more complex.
I can only speak for my own experience. That's by no means fully comprehensive list of things we study in our programme, there are also five semesters worth of mathematics classes, a semester of physics (electromagnetics primarily), two semesters of information theory and databases, etc., the above list is purely programming related and makes up about 1/3 to 1/2 of the classes we take. I feel like our syllabus is quite abnormal in terms of structure and order of presentation in topics but as I've already stated I've never studied CS anywhere else so I have no grounds for comparison in that.
Microcontrollers definitely are more simple, and yes you're basically right on all counts. It's a data flow problem; http://en.wikipedia.org/wiki/Instruction-level_parallelism has a pretty good summary of the kind of black magic processors are performing to optimise running code without breaking the sequential semantics (the bullet list at "Micro-architectural techniques that are used to exploit ILP include:")
I am in the situation now where coming up with solutions to small problems is not hard. For example, a triangle of primes in an array and then summing each row. Practically the problem defines the solution, which is where functional is a no-brainer. But then for applications.. it's like that frustration like forgetting where you put your wallet or keys. It stares at you in the face but you just don't see it!
Also, I think maybe I just haven't been exposed to the benefits of Lenses enough. Could you provide a concrete code example in Snap where the nested lens design offer benefits? Wheb does have plugin dependencies like Snaplets (the Auth wheb plugin depends on the Session plugin). It would help me out if you look at examples/Main.hs to see the Auth and Session plugins in action and compare them to their snap equivalents. I actually based them off Snap's so they are pretty similar.
I would love something I could run on my local machine that would scrape my ~/src/ for haskell projects and build documentation if it's not already there then let me browse the generated list via a web browser.
Not necessarily. Different universities do it differently. For example, at Berkeley (in the US), we started out at a *high* level with Scheme then went to Java and only then did C/asm/architecture. I personally like this approach much more than the bottom-up version because, ultimately, the most important idea in CS is *abstraction*. Also, our program is far less structured. Beyond those three courses (and discrete math and 2 EE courses) we can do pretty much any CS courses we want in any order. This includes basically any combination of theory and practice that you want. There are some major differences between US programs and others: we have 4 years rather than 3, we usually have more flexibility and we take more classes outside our major.
Not to understate how big the differences between US programs and elsewhere are, but Irish degree programs are also 4 years, even if everything else you mention is right on the money.
It's just like any other Haskell package - it takes `haskellPackages` as input and calls `cabal.mkDerivation`.
Right, good point. I was mostly thinking about programs in the UK and France--I don't know anybody who was educated in Ireland.
I really like the way you use Streams to build up to the idea of an Auto. This post was really helpful for me, and I'm looking forward to the next one.
Sure, I'm just giving some more context on myself and /u/NSNO, not meaning to go off topic from your point at all.
The first language that I learned when I began to programming seriously was Scheme (I took a Pascal class in HS). I don't think learning it first helped learning Haskell that much. I still had a ton of trouble with the type system and understanding Monads (though to be fair, I didn't really understand call/cc until long after beginning to learn Scheme).
`decodeGetMany` is still there: it's just now the `decoded` lens. Use `view decoded` to get back the old behavior. The reason it's a lens is that in some cases you can also use `over decoded` to take care of decoding and reencoding the values. So instead of writing something like this: for (view decoded producer &gt;-&gt; foo) encode ... you can now just write: over decoded (&gt;-&gt; foo) producer I will ask Renzo to add a comment explaining this to the documentation.
Amen.
What does investment banking mean to you? Can you elaborate on why it has to be bad?
I thought the NICTA version was the canonical version. I'm pretty sure Tony was passing around the link to this one the last time it came up. I can't imagine they're too different in either case.
Optimus Prime, ruler of the transformers. 
the large-scale structure of functional programs feels a lot like the large-scale structure of well-factored imperative programs to me, honestly. typically, you have a main loop, and then you have subroutines. it just so happens that more of the work in these subroutines happens in a pure context.
As for Lenses, [here](https://www.fpcomplete.com/school/to-infinity-and-beyond/pick-of-the-week/basic-lensing) is a good place to start. It conveniently starts off with an example involving getters/setters and moves into composition. Which works well with isolated concerns, such as Snaplets. Snaplets don't just offer encapsulation but also file-based configuration (through configurator), snaplet-local routes and the ability to initialize them with a root path (such as might be done for a blog snaplet), reloading, etc. Snaplets are composable units of application rather than just modules. In addition, there is a difference between your example's global state and Snap's default local state, [as described here]( http://snapframework.com/docs/tutorials/snaplets-tutorial): &gt; The Snap web server processes each request in its own green thread. This means that each request will receive a separate copy of the state defined by your application and snaplets, and modifications to that state only affect the local thread that generates a single response. Not only that, but creation of multiple instances of Snaplets is possible by naming them differently, in case an application needs to have different authentication mechanisms for different sets of routes, for example, or access two different postgres databases. In that sense a comparison isn't really fair either way but [here](https://github.com/ChristopherBiscardi/Snap-Databased-within-Auth-Snaplet-Example/blob/master/src/Site.hs) is a basic example of using Auth, Session and snaplet-postgres-simple. To that end, I'm not really sure what the auth/sess plugins in the Wheb Main.hs were designed for and as such it would be hard for me to make a just comparison. It looks to me that they don't encapsulate state, don't have configuration, don't have reload mechanisms, etc and are therefore not meant to be compared to Snaplets as it seems they are designed for different purposes.
Thanks for your response. I see the difference now. Wheb plugins are designed to be an interface to manipulate the main application's state and not carry the state themselves nor be standalone applications. You are indeed correct, Wheb and Snap are designed for two very different purposes. As a side note, I actually love snap and even wrote a project to help me get started on Snap projects easier: https://github.com/hansonkd/snap-boiler-plate
I think the intent isn't for you to load the module into ghci (that would be where you can play around with the changes you make), but to read through the module itself (i.e. Intro.hs, in the src/Course directory, in this case). Working through the exercises (which start, according to the readme in List.hs) involves replacing the body of the functions with something that actually does what the comment above the function describes. The readme tells you how to run the tests on them to make sure it works as expected. Hope that helps! edit: Sorry, I completely missed the issue. Yeah, I have to agree with the below that the Intro module doesn't seem to exist. Maybe it meant the Course module which just re-exports everything else? In any case, you can pretty much just read over the Id, Optional, and Validation modules (the .hs files) and then get started with List.hs, etc.
this is true about CPUs also
Cool. I was actually thinking of creating a leiningen-style project creator but that idea is it's infancy right now. The idea is to be able to run: hinit new snap --compiled-heist --snaplets auth,postgres or templates like: cabal install hinit-angular hinit new angular
They're basically the same thing. I tend to use "catenation" for the binary operation and "concatenation" for the `n`-ary operation; thus, `cat :: a -&gt; a -&gt; a` vs `concat :: [a] -&gt; a`. But I don't know that anyone else makes that distinction.
It goes without saying that catch and distribution are going to give different errors. However, I would expect that term to match both "b" and "ab" since it's essentially the same thing as the regex `/(|a)b/`. Why anyone would expect this to match only "b" I have no idea
In his talk on lens, SPJ predicts that lens would slowly creep into everything. Resistance is futile! :)
According to the readme, this is the code for a course, not a self-contained course where a program acts as the teacher anyway, try https://github.com/tonymorris/course/blob/master/README.markdown and the section about "cabal test".
Since the docs you have quoted don't say that the auto-specialization is transitive, and your tests demonstrate that the auto-specialization is indeed non transitive, I think we can safely say that the answer to your question is No.
Ooh, do you have some specifics? I'm actually really interested in this. At the very least, it supports my belief that the world is inherently evil and out to get me. 
I started with Scheme, and I never found OOP modelling easier than Haskell programming.
I know you speak in jest, but that's exactly the problem I'm talking about. That's fine for SPJ, because he's a super-genius, but it's potentially hostile for those of us who aren't quite so smart.
A lot of questions on how. After downloading the repo, run ghci. The first exercise you should do is `src/Course/List.hs`. If you open it, you will notice it is full of error / TODOs. Running `cabal test` runs doctest, which will test if you implemented the functions correctly. Then try `Course.Functor`, `Course.Apply`, `Course.Applicative`, `Course.Bind`. It is listed in the read me. You can find answers on https://github.com/tonymorris/course
&gt;"When you start a CS course, you learn how the machine works." The machine you're referring to here isn't _the machine_, which behaves quite differently from the _abstract machine_ you're using to mentally track the execution of a program. When you learn an imperative language, you learn the semantics of an _abstract machine_ which evaluates your program, you don't learn how the compiler is producing assembly code (for 99% of use cases anyway). Haskell programming uses an entirely [different machine](http://research.microsoft.com/pubs/67083/spineless-tagless-gmachine.ps.gz) which explains why the mental transition is difficult. Not only that, but the entire idea of reasoning about programs _operationally_ (as in, according to their evaluation on a machine) is discouraged in haskell, preferring instead a _denotational_ reasoning (where expressions are mapped into mathematical objects). Although I've never made this transition from imperative to functional, having started with Scheme, I found learning to reason denotationally incredibly helpful for programming in any language. If you learn how to do static reasoning for imperative languages (hoare logic, wp, etc.), I suspect you will find your mental process changing from "What does the machine do here?" to "What do I know about my state at this point" -- an infinitely superior way of thinking. When you toss purely functional programming into the mix, hoare triples become trivial equational properties, and it's a joy to work with if you think _statically_ rather than _dynamically_. So, my point is: Learn about programming language semantics and formal reasoning. It will completely change the way you think about programs and you'll produce better code as a result.
Is there something like `wai` for pipes?
Well, the lens laws say that it doesn't matter how `lens` works internally w.r.t to say, reading from a `Lens` and putting the answer back. It could just as well do it in one pass (as `over` does) or two by performing a `view` followed by a `set`. If you have a legal lens you don't care. Both implementations must give the same answer. On the other hand, if you don't, _you_ need to care about the implementation details of `lens`, as you might for some reason want the other version. We of course do everything we can in `lens` to do everything in one pass, we do that as a matter of course across all combinators, so it'll typically do the right thing -- if what you expect is that we'll do everything we can to do things in one pass! but the fact that you have to care how we implement things is the price you pay for disregarding the laws. 
While definitely possible to do what you say, I (who relate with the author) want to avoid doing that. Making everything an `a -&gt; IO b` wouldn't teach me anything about how to properly structure a program declaratively. And that's really the problem: learning to structure programs declaratively. Doing the "appropriate sequence of equational manipulations" is surprisingly difficult when you're locked into a process-centric world view, as opposed to the data-centric view Haskell encourages.
Ask any universities you're interested in going to! They'll give you all the information you want.
If you consider your written program to be a permanent, unchanging product then you're right. If you consider it a starting point for a refactoring into a more idiomatic solution then it can be useful.
You could alias cabal install to cabal update &amp;&amp; cabal install? Also, check out [Cab](http://www.mew.org/~kazu/proj/cab/) by Kazu Yamamoto. It has a lot of useful features like uninstall, outdated packages and reverse dependencies. I love it.
Not yet, but we're working on it. Jeremy Shaw is leading this effort with his `hyperdrive` project. However, if Michael were to release a `conduit`-independent version of `wai` then I'd also be willing to write a `pipes` layer on top of that. I already released `pipes-http` on top of his `http-client` and `http-client-tls` libraries (announcement post coming soon), and I have no problem doing so for `wai` if he can factor out the `conduit` bits, perhaps for a `wai-core` library. However, regardless of whether `pipes` supports `wai` we will still be working on `hyperdrive`, too.
This is the reason why people make a big fuss about lenses. They make it really really easy to mutate state, even easier than imperative programs. [Here is a post I wrote](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html) showing how much nicer state manipulation is using lenses.
Perfect answer, thank you :)
&gt; a problem like I want to make an application that streams music from this API and going and writing an application that does that I'd like to know more about this. If I wrote this in Haskell I don't think the *large scale* design would be significantly different from one I wrote in Python. The small scale design would be completely different though, but the OP says he has no problems with the small scale.
Typed holes look awesome! I can't wait to get my hands on GHC 7.8 and try them out.
You can use http://www.open-search-server.com/ in "local". It's pretty simple to use. You can make it to index local files. Then you can inspire you to "haskell-search" for the front.
On a side note, I think this is a situation where a private teacher/mentor might help. Some are available [on Codementor][1]. (Disclaimer: I am one of them.) Learning Haskell is a bit like learning guitar: It's perfectly possible to teach yourself guitar (Haskell) by watching Youtube videos or studying tabs (tutorials and source code). Some people pick it up very quickly, some struggle with specific concepts but breeze through others, some don't make progress even after years of playing. I managed to pick up Haskell by myself just fine, but for guitar I'm now seeing a teacher, mainly because it's much faster and I don't have to figure out the curriculum myself. [1]: https://www.codementor.io/haskell-experts
Even if you do learn that, it's not like you're reasoning about a C program in terms of the assembly it produces. A course like that would show just how nontrivial the mapping is!
Okey, this might be a bit biased since I've not really written any real-world programs yet, but thought it might be interesting anyway. Haskell was (practically) the first language I learned how to program in. I started learning it in the beginning of last year. My previous exposure to programming have been a few failed attempts learning some other languages. This time I tried learn it for real. The things I founded hard when learning Haskell was the following: * Recursion * Fixing type-errors (due to not understanding monads fully) * Solving problems (e.g. what is needed to connect to an IRC server? what kind of data structure should I model a pong game as? how do I add this feature to a existing program?) The first was a real pain in the butt to learn, but after a lot of reading and writing different kind of recursive functions I started to get used to it. I also had a bit of hard time understand monads, first I learnt how to *use* them and that was enough to work with (later on I would learn how they worked/what they was). The third I started to finally get only due to reading existing code, after copy-pasting and reading cook-books like Write Yourself a scheme and the IRC bot tutorial on the wiki I started to understand more how to think. I remember even struggling thinking kind of imperatively although I hadn't any experience in such a language. Anyway, when you "get" Haskell it's just feels normal to program in, you think of a program, you divide it into small problems, then try solve each individual problem.
I added the function. It's still full of js for reasons of simplicity : http://haskell-search.org/?q=%s
About the only things I wouldn't use Haskell for these days are tasks with some sort of technical limitations making it extremely hard to do (such as if it would require me to port GHC to a new architecture).
The way I approach application design is I ask myself: "How do I anticipate that my application might vary?" The answer to that basically dictates the category I will use to structure my application. Plugins for my application are morphisms in some category, and the composition operator of the category is how you combines plugins together. Let me give a specific example. Let's say that I am writing a concurrent application and I anticipate that number of concurrent input sources of my application may vary. That means that the input sources need to be morphisms in some category, and if I figure out what category that is then I will be able to easily combine them. In this case, that category is the `Input` monoid, where the `Input` type is from `pipes-concurrency`: data Input a = Input { recv :: STM (Maybe a) } instance Monoid (Input a) where mempty = Input (return Nothing) mappend x y = Input $ do (i, ma) &lt;- fmap ((,) y) (recv x) &lt;|&gt; fmap ((,) x)(recv y) case ma of Nothing -&gt; recv i Just a -&gt; return (Just a) The morphisms are `Input`s, the composition operator is `mappend`, and the identity morphism is `mempty`. Now I have an easy and composable way to add new `Input`s to my system. The beauty of this is that now I can write my system to just take a single `Input`, and I can `mconcat` as many (or as few!) `Input`s together into a single `Input`. The nice thing about a monoid is that you can always concatenate any number of values into a single value, so you have a uniform entry point. Same thing for a category: you can always compose any number of morphisms into a single morphism (as long as the composition type-checks). If `Input`s have different element types, I can unify them to agree on a common element type using the functor design pattern: inputStream1 :: Input A inputStream2 :: Input B totalStream :: Input (Either A B) totalStream = fmap Left inputStream1 &lt;&gt; fmap Right inputStream2 This is the sort of thing that is easier to teach from specific examples than in the general form, so if you can suggest some sample applications I can give more examples of useful categories for structuring these applications.
First you ask "what do I want my program to do, in what order." Then you write down something that does those things, in that order, asking as you go "what is the type signature of the function that would do this thing" and now you have a bunch of type signatures without implementations. So then you go and try to implement those type signatures. Maybe sometimes you write your data structures first and your function type signatures second. Or you write your data structures such that they contain enough information that they can actually work with the type signatures you have. That's it.
There is one more thing that I think might be difficult to grasp that is pretty important: one of the biggest advantage of Haskell is that it is a stateless language which promises referential transparency. However that does **NOT** mean stateful programs are impossible or undesirable. You may have heard that fact before but there are some important implications in that fact. Most all software projects are necessarily stateful. The power of Haskell is that it is a pure, referentially transparent **language that easily models stateful programs.** It isn't like C or C++ where you call a function, you have no clue which variables it changes. In a stateful Haskell program, you control exactly what the state can and cannot be, and what can and cannot be modified. The state of the program is not an inherent property of the Haskell language, the language makes that impossible. Instead you define the state of your program **explicitly and in a well-typed way** using Haskell libraries like monad transformers (in the "mtl" package) and this is what gives you full control over the state, and what makes the static type checking so thorough. So don't be afraid if you rely often on State, or ReaderTs lifting IO that contain lots of IORefs. At first you will probably end up using these as a crutch to make your programs more like imperative programs, but with experience you will start to realize which stateful variables you need and which you do not. You may see an IORef and think to yourself, "I could do that without an IORef." Many values you may compute and store in an IORef could just be computed with a pure function taking other data you already have in other IORefs. So this is how most large projects develop, around a central monad that contains the exact stateful values necessary and nothing more. After that, organization becomes much more common-sense, much more similar to OOP projects but without all the nasty side-effects of OOP (pun intended).
Very cool. I'm interested to see where this goes. Maybe I'll try making an example. 
!hoogle or !hackage won't help if you want to search, for example, for articles about pipes, rather than just for documentation. This search engine seems to index everything that's Haskell related, which I think I'll find rather handy myself.
One big thing that I'm not sure the author here may realize is the vastness of the Haskell API that allows you to deal with the real world. I've run into this problem too, but where I feel I've reached a point that I can reason about a functional program, I can't for the life of me figure out what functions, hooks, and data structures to use to interact with the world. I've been trying to dig through the API and the basic modules to fully cement my understanding. 
So, I guess my question boils down to "how do I gain this intuition?" You're able to pull an example out of your pocket and demonstrate exactly how you would go about piecing it together; this is exactly what I'm looking for. Perhaps the answer is just "read other people's project and try to find patterns," but that seems inefficient. By your advice, it seems like the idea might be to learn a little bit of category theory and try my best to view applications through that lens. The whole point of the post, though, was to have an idea *in the general* of how to approach different classes of problems. These are handled with (arguably) hacky "Design Patterns" in Object-Oriented languages; I guess I'm looking for a detailed discussion of robust program design in a functional setting. I didn't give an explicit example for that reason (and partially because I don't have one); I'm looking for intuition when I *do* have an example, not project-specific advice, if that makes sense.
I've found the biggest strength of Haskell is in the refactoring. So I feel more free to just whip up a prototype and then iterate, because I know iteration is possible.
Well a large scale haskell program that does things will have an imperative aspect, unless there's an FRP-like interactive component. By this I mean, you will have sequencing, so that everything doesn't happen all at once. There's nothing to fear here. Subcomponents of your program (like manipulating sort of state object, performing calculations, transforming data, actually computing anything) are where more pure FP principles will shine. I guess there's the usual "avoid global state, avoid mutation, use concurrency primitives carefully and sparingly" sort of advice. edit: I suppose the one thing here is if you're doing GUI stuff. Then you have some big set of stuff you're binding to, and you have to adapt to that, or write an adapter for that, to make it tractable. So this is where continuation monad style tricks or the like tend to pop up, "re-inverting" a callback-heavy API. Alternately, you need some concurrency constructs with e.g. a control thread and worker threads, or an event model of some other sort. But that's really specific to the domain. It would be interesting to document a few different high-level architectures that different types of programs would tend to fall into.
I'm sure both Tekmo and sclv can give more sophisticated answers but... I have two suggestions: 1. It's not necessarily that bad as a starting point. Refactor using the Haskell.org advice on removing IO. 2. Separate the calculation/evaluation of what you want to happen from the actual execution. If you can work out what you want to happen, it's usually easy to execute it. 
I suppose one other piece of advice is that you can write components and test them from the "ground up" at the repl, only later designing the high level thing that actually puts them all together.
That's not bad per se. Haskell is after all a great imperative language. It's common for bigger program to have lots of do-notation in their top-level machinery.
It truly is, and I understand that. It's just that when I read about other peoples solutions to the same problems, I feel like I'm missing out on some really intelligent solutions that aren't imperative in their nature.
I think you have made some leaps of logic here. You write "I anticipate that number of concurrent input sources of my application may vary. That means that the input sources need to be morphisms in some category." The sense I can find in this is that if something may vary in number, it should be represented as morphisms in some category. I do not think this is true. It is true that category theory is very general, and about morphisms more than objects, so in general, we can _always_ represent anything as morphisms in some category. But that does not mean that things varying in number are especially suited to be thought of in this way. We proceed to now not be talking about morphisms in general, but morphisms from a single object to itself -- a monoid category. So why did we go all the way to categories if we really just need monoids? Monoids are simple enough on their own that category theory doesn't help. So now we have monoids and our leap is that because things may vary in number then they should be monoids! But _why_ should they be monoids. Other things are combinable! Perhaps they should form a group, or a groupoid, or a semigroup. Perhaps you have a ring! Or maybe you don't even have a monoid, because associativity isn't important. But a monoid is I suppose suitable for something which can vary in number (including zero) _and nothing else_. But why is that? Consider what it means for something to be a monoid. Well the universal monoid is a list. So we have invoked a bunch of mathematical machinery to say "if we can have more than one thing, then it would be nice to have some listlike notion". But intuitively this was obvious from the start! (edit: and in fact, we get basically the same semantics, if different efficiency, perhaps, from simply writing `data Input a = Input [STM (Maybe a)]`.) It seems we have gone very far afield to gain very little, at least in this case.
Haskell is a terrible imperative language. Monads are hard to refactor, transformers are painful to use, and all monads introduce a huge performance overhead that increases faster than linearly. For instance, 10000 actions in the IO monad are 10 times slower than with a pure recursive function, while 100000 are already 50 times slower. I wouldn't exactly call a clumsy and inefficient imperative language "great". That's just a marketing hoax.
If $ing Hask to RealWorld seems hard to you, it'll probably be helpful to look at the sources of some games written in it. http://stackoverflow.com/questions/952841/exemplary-haskell-game-code
&gt;Monads are hard to refactor What do you mean by this? &gt; transformers are painful to use Granted, sometimes there's a lot of book keeping there. &gt; all monads introduce a huge performance overhead that increases faster than linearly. WTF? The IO monad performs about as well as the equivalent program in any imperative language. I have no idea what you mean by this: &gt; For instance, 10000 actions in the IO monad are 10 times slower than with a pure recursive function, while 100000 are already 50 times slower. How can you even compare IO monad and pure recursive functions when they're quite different programs? Even if they compute the same thing, the IO monad version would have to stand in the way of some optimisation that GHC is doing in the pure case for any such discrepancy to be observable.
hey, how do you know when to stop refactoring? =)
That is much harder ;)
I'd like to, but it usually doesn't show up in library code and all the applications I've written are private. I really think that looking through Oleg's finally tagless work was one of of the biggest inspirations.
&gt; Granted, sometimes there's a lot of book keeping there. And `mtl` can help a lot there.
1. Suppose you want to add effects to a pure function, e.g. to make it print debug messages. You'll have to change its return type to a monad, and the return types of its callers... and then change all of those return types back once you don't need them. Rewriting the types of everything is NOT fun. 2. Need to add an effect like throwing exceptions? More layers to the transformer stack, plus more refactoring to the lifting functions. Want to remove a layer? More screwing around with lifting, hoisting and whatnot. In a good imperative language you have all the effects for free, no extraneous pain. And don't start about "safety", a stack with an IO at the bottom is no more safe as just an IO. Oh, and what if you want to have two StateT's? You can only reference them by cryptic lifting functions, it's like de Bruijn index hell. Damn, I want to tear my hair out just from writing the unwrappers for monad transformer stacks. Much useless pain. 3. Since you were talking about Haskell's qualities as an imperative language, and since Haskell is imperative only in the monads, then it is meaningful to compare computations in non-monadic imperative languages with monadic computations in Haskell. A pure recursive is not much slower than a C/C++/what-have-you loop, I trust, so the comparison is justified. I made a little benchmark to see just how much overhead monads introduce and it's at least 10x for IO and 16x for ST.
&gt; to make it print debug messages. For the case of debug messages, there's Debug.Trace. For actual effects, you should state so in your function type. &gt; Need to add an effect like throwing exceptions? You can throw exceptions from pure code: throw :: Exception e =&gt; e -&gt; a If you want to use either values instead, you can, but there's no requirement. &gt; And don't start about "safety", a stack with an IO at the bottom is no more safe as just an IO. Sure, no one said it wasn't? &gt; Oh, and what if you want to have two StateT's? You can only reference them by cryptic lifting functions, it's like de Bruijn index hell. Or, alternatively, just have one stateT and use lenses, so you can give nice names to each component of your state. Or, if you're on top of IO anyway, just use an IORef and there's nothing lost from the imperative world. &gt; Since you were talking about Haskell's qualities as an imperative language, I was not the original poster. &gt; and since Haskell is imperative only in the monads, then it is meaningful to compare computations in non-monadic imperative languages with monadic computations in Haskell. OK, sure, but this: &gt; I made a little benchmark to see just how much overhead monads introduce and it's at least 10x for IO and 16x for ST. Is just totally false -- I don't doubt you've managed to get that sort of result, but certainly has nothing to do with monads or IO. Anyone more familiar with GHC internals can probably post more, but I have implemented a basic mini-haskell compiler in the past and it was quite easy to compile monadic IO code without overhead. GHC, however, knows quite a bit about deforestation optimisations, so it could be that whatever benchmark you're writing has triggered some rewrite in the pure case that is not triggering in the IO case. Mind posting your benchmark program? I suspect it's flawed and that's why you're getting this sort of inaccuracy. 
Please post your benchmark program so we can know what patterns to avoid, fix it, or help you improve performance of your Haskell programs. The pains you talk about seem pretty annoying, I'm curious to see why it is you are having them.
This showed me how lenses are useful as an abstraction over getters/setters with examples: http://www.haskellforall.com/2012/01/haskell-for-mainstream-programmers_28.html It kind of shows what the template haskell magic does, and how you can use lenses without TH. I had a bit of an epiphany after reading it :)
I think this would be perfect! I'm guessing it would be aimed at those who have finished LYAH and RWH?
We have extensible-effects now instead of transformers.
It is called Debug.Trace.
Great idea (and also a very nice font).
The trick to having code bases that scale is writing lots of small modules that can be used together, but do not depend on each other. A lot of the ideas in Large Scale C++ apply to other languages like Haskell http://www.amazon.com/Large-Scale-Software-Design-John-Lakos/dp/0201633620 . Also the specific issues affecting really large code bases just don't manifest in small to medium projects, so they're not even worth worrying about. Something that I don't see discussed much from practical perspective is how to build "fakes" as described here http://www.martinfowler.com/articles/mocksArentStubs.html The two approaches that I have seen are the data type a la carte approach used in IOSpec http://hackage.haskell.org/package/IOSpec And then there is the finally tagless approach, or it's simplified version, which I call, "using a type class" If you write total functions with fairly precise types, then as singpolyma said, you could always refactor. It takes a fair bit of knowledge to right really evil code in Haskell, so I wouldn't worry and just start writing code.
You are arguing against a straw man by reading too much into my choice of a Monoid as a specific example. Also, the alternative semantics you gave for `Input` as a list of `STM` actions is not correct. For example, the semantics you gave would allow me to reorder the priority of `Input`s even after combining them, whereas the semantics I gave would not.
What is Oleg's work? Is there a post or anything about this?
I don't know what will pedagogically work best for others, but I can share what worked for me. In my case I absolutely loathe releasing libraries that may have bugs because dealing with bug-related issues from users is the most unforgiving and thankless task that sucks all the joy out of programming. I want to build new code, not spend all my time maintaining old code. This led me to learn how to [equationally reason](http://www.haskellforall.com/2013/12/equational-reasoning.html) about code to prove certain properties held. However, my early attempts to do this grew difficult in a hurry. The reason why is that there are countless arbitrary properties that one might be interested in proving about a library, and proofs are very time-consuming. I then discovered that certain kinds of proofs were more useful than others, specifically laws from category theory (i.e. category laws, functor laws, monad laws, etc.). When I say "more useful", I mean that they concisely summarized my vague intuition of correctness in a surprisingly small number of equations. This greatly reduced my proof burden and allowed me to deliver libraries more rapidly with a greatly reduced maintenance load. My obsession with equationally reasoning about code is how I built and exercised my intuition for design patterns inspired by category theory. By structuring my proofs around laws from category theory I began to more frequently spot how to structure my code the same way.
Here's Oleg's post index: http://okmij.org/ftp/tagless-final/
&gt; The machine you're referring to here isn't the machine Funfact: your machine isn't *the machine* either. Just consider how modern CPUs rip apart and reschedule the instructions you carefully assembled ;-)
Fair point!
I've given commit access on almost every repository I have to several other people. If I ever die or become unavailable for a protracted period said committers are hereby obligated to fight in the arena for the right to become the new package maintainer on a project by project basis. This may, however, be rather rough on Shachaf Ben-Kiki, whom I think has commit rights to something like 30-40 of my projects. In the event of ambiguity in these terms, disputes should be resolved by the core libraries committee. ^^^That ^^^said, ^^^they ^^^could ^^^also ^^^just ^^^talk ^^^it ^^^out ^^^amongst ^^^themselves.
&gt; The main responsibility of BM is thus to make it easy for such a hypothetical person to take over. Also, elimination of waste material.
&gt; I made a little benchmark to see just how much overhead monads introduce and it's at least 10x for IO and 16x for ST. Are you telling GHC to optimize your code?
&gt; How it works and how to improve it is an exercise left to the reader. Here's my solution: import Control.Exception import Data.IORef import Pipes newtype Fiber a = Fiber { runFiber :: IORef (Producer a IO ()) } new :: Producer a IO () -&gt; IO (Fiber a) new p = fmap Fiber (newIORef p) resume :: Fiber a -&gt; IO a resume (Fiber ref) = do p &lt;- readIORef ref x &lt;- next p case x of Left _ -&gt; throwIO (userError "FiberError: dead fiber called") Right (a, p') -&gt; do writeIORef ref p' return a main = do fiber &lt;- new $ do yield 1 yield 2 print =&lt;&lt; resume fiber print =&lt;&lt; resume fiber print =&lt;&lt; resume fiber Here is the output: $ ./fiber 1 2 *** Exception: user error (FiberError: dead fiber called) However, `pipes` can actually go one step further and also mirror how Ruby lets `yield` return a value: import Control.Exception import Control.Monad (forM_) import Data.IORef import Control.Monad.Trans.Class (lift) import Pipes.Core import Pipes.Internal (Proxy(..), closed) newtype Fiber i o = Fiber { runFiber :: IORef (Server i o IO ()) } new :: Server i o IO () -&gt; IO (Fiber i o) new p = fmap Fiber (newIORef p) next' :: Monad m =&gt; i -&gt; Server i o m r -&gt; m (Either r (o, Server i o m r)) next' i p = case p of M m -&gt; m &gt;&gt;= next' i Respond o fi -&gt; return (Right (o, fi i)) Request x _ -&gt; closed x Pure r -&gt; return (Left r) resume :: Fiber i o -&gt; i -&gt; IO o resume (Fiber ref) i = do p &lt;- readIORef ref x &lt;- next' i p case x of Left _ -&gt; throwIO (userError "FiberError: dead fiber called") Right (o, p') -&gt; do writeIORef ref p' return o main = do fiber &lt;- new $ forM_ [1, 2] $ \int -&gt; do char &lt;- respond int lift $ putStrLn $ "Fiber: " ++ show char forM_ ['A', 'B', 'C'] $ \char -&gt; do int &lt;- resume fiber char putStrLn $ "Main: " ++ show int This outputs the following: $ ./fiber2 Main: 1 Fiber: 'A' Main: 2 Fiber: 'B' *** Exception: user error (FiberError: dead fiber called) This is because `pipes` is internally implemented in terms of a bidirectional communication protocol. `respond` is just `yield` with a more general type.
Before clicking through, I actually wondered for a moment if this had anything to do with Euromaidan, even though I didn't really think it did. :)
&gt; Suppose you want to add effects to a pure function, e.g. to make it print debug messages. You'll have to change its return type to a monad, and the return types of its callers... and then change all of those return types back once you don't need them. Rewriting the types of everything is NOT fun. Before I actually used much of Haskell, I wondered how a Haskell developer can stand to deal with such a situation. I soon learned about `Debug.Trace`, which allows you to do precisely this without having to sprinkle `IO` everywhere, but I still found it unsatisfactory. Now that I'm quite experienced, though, I can honestly look back and say that I have only rarely actually even wanted to use `Debug.Trace.` When a function is pure already, it's just so easy to debug it using QuickCheck and GHCi that `Debug.Trace` doesn't even occur to me. There's simply little point in `printf` debugging a pure function. &gt; Need to add an effect like throwing exceptions? More layers to the transformer stack, plus more refactoring to the lifting functions. Want to remove a layer? More screwing around with lifting, hoisting and whatnot. In a good imperative language you have all the effects for free, no extraneous pain. And don't start about "safety", a stack with an IO at the bottom is no more safe as just an IO. Oh, and what if you want to have two StateT's? You can only reference them by cryptic lifting functions, it's like de Bruijn index hell. Damn, I want to tear my hair out just from writing the unwrappers for monad transformer stacks. Much useless pain. I hardly ever use monad transformers anymore, at least not in this style where one determines what pile of effects he wants and smashes them together into one supermonad. I think monad transformers as they tend to be used by beginners are kind of a crutch that allows you to revert to old habits instead of thinking more functionally. &gt; I made a little benchmark to see just how much overhead monads introduce and it's at least 10x for IO and 16x for ST. I think you just failed somewhere along the way. `IO` and `ST` don't actually add overhead. I echo the request to see the code.
no, you presented a poor contribution by using a terrible specific example. (and i didn't give an alternative semantics for Input -- I gave a specification and claimed you could equip it with the same semantics, which you can!)
I find I use `Debug.Trace` when I have complex functions with one or two key pieces of data that I'm maintaining invariants on, and I just want to check, visually, what is occuring with this data as the function progresses.
you will be coding on the moon shortly. +/u/dogetipbot 100 doge verify
__^[wow ^so ^verify]__: ^/u/staque ^-&gt; ^/u/prototrout __^100.000000 ^Dogecoin(s)__&amp;nbsp;^__($0.109026)__ ^[[help]](http://www.reddit.com/r/dogetipbot/wiki/index)
As far as the problem of starting things up, it'd be better/easier to do this: newtype ReacT i o m a = ReacT (m (Either a (i -&gt; (o, ReacT i o m a)))) This says that we only get our `o` values after passing in the preceding `i` value; whence, no need to fabricate an initial dummy `o` value. Of course, there are other solutions as well, depending on what semantics you want.
That's a good catch! I was following the literature, and didn't think of structuring it that way. Like a lot of Haskell stuff, your way seems so obvious when I look at it now, though. Thanks!
Wow, such thanks.
These are some great thoughts on algebraic vs abstract data types. What I find really interesting is that, to me, it seems to resemble very closely the idea I've heard in OO languages (Ruby specifically), of having a "Functional Core and Imperative Shell", where the "imperative shell" is message passing between OO objects. It seems like 'functional core' is to algebraic data type as 'imperative shell'/OO is to abstract data type
For what it's worth, the back-and-forth-and-back-again pattern is the essence of an adjunction. 
If I recall correctly.. you should be able to copy your code into another directory in the VM, and it should work. If that is the case.. then it is because cabal is trying to do some symbolic linking, which is not supported on the shared / remote file system.
I was more concerned with whether the inheritance would be lazy or strict.
&gt; Certainly foreign import javascript is a HUGE advantage for code integration, What advantage? Haste'c `foreign import` are pretty much the same thing &gt; From what I've seen, it also has more active maintainership, which is its own advantage. I'd love if more people look into haste. 
The hackage permissions are much more important, actually. That's what gave me the most headache when taking over other people's packages. Repos can be forked easily (although the issue tracker can't, sadly); but you cannot fork a hackage package without changing its name. Ditto with quick fixes in my absence. It's no good to fix the git version if it's still broken on hackage. (Oh, and by the way, `reducers` on hackage still doesn't build with 7.8!)
I was going to say it's trivial to revoke (there's a remove button across every maintainer). Then I decided to try it out, and it turns out the button doesn't work. (I guess it requires higher permissions.) So now you're in the maintainers group of tasty, too =) (Not that I mind.) I tend to trust people. What's the worst thing that can happen? The NSA will bribe you or Ollie to install a backdoor in tasty? They should've talked to me directly instead ;-)
Thanks!
I'm also rather far on the trusting side of the spectrum. My main reason for lazy evaluation here is just that, laziness, the others are just excuses for the same. ;)
+1, symlinks and permissions are the main reason why stuff fails inside vagrant/VirtualBox. If cabal is using a directory tmp, you could try to configure it to use a different directory. Failing that, you could try to mount a tmp file system over your mounted directory. By the way you should give `ansible` a try, which is way easier for provisioning vagrant than puppet.
Well I was able to use cabal-install successfully inside a vagrant VM. I doubt anybody would be able to diagnose your problem without any logs though.
That actually reminded me of your poor-mans concurrency via coroutines (implemented using Free monad). I bet the Fiber interface can be implement on top of that.
 &gt;If I ever die That seems an optimistic way of phrasing things. &gt;This may, however, be rather rough on Shachaf Ben-Kiki, whom I think has commit rights to something like 30-40 of my projects. Experience in the arena is valuable. 
What is going on in this code? titles = json ^.. _Right . key "data" . key "children" . values . key "data" . key "title" . _String I'm not used to this lens stuff.
That's basically what is happening. `pipes` is internally implemented as a free monad very similar to the one in the post.
It's composing `Traversal`s. Think of a `Traversal a b` as a way to navigate from an `a` to an unspecified number of `b`s. Let's call these `b`s "children". For example, the very first part in that chain is `_Right`, which you can think of in terms of the following narrower type: _Right :: Traversal' (Either a b) b In other words, it's a way to get from an `Either a b` to an unspecified number of `b`s. In this case it could be one `b` (i.e. the `Either` is a `Right`) or zero `b`s (a `Left`). The `key`s are a `Traversal`s, too. They point to either 0 or 1 child values indexed by that key. `values` is a traversal from a JSON array to its individual elements. Again, there could be any number of elements in the array. When you compose traversals, you keep chaining these children lookups, returning all possible paths. It's conceptually a bit similar to how the list monad works, if you squint a bit. In the above case, the reason there is no error checking is that these traversals are treating errors as "zero children", so if any step fails and returns zero children, then the whole traversal fails and returns zero children and you would get an empty list. `(^..)` just takes a `Traversal' a b` and applies it to an `a`, returning all `b`s that it points to. If you didn't want to be so lax with error handling, you can do something a bit more careful like: fmap (toListOf (key "data . key "children" ...)) json That would at least preserve the outermost `Either`, while still converting everything else to a list. Sorry for not explaining that example more, but I didn't want to digress too much into `lens` for that post. However, I'm more than happy to answer more questions here in the comments (and write up a post on this subject later).
Wait a second. How isn't that backwards? Usually the first action in a function composition chain would be in the end...
`lens` essentially implements lenses by using something related to continuation passing, so that reverses everything.
ah, thanks then
UK, four years in Scotland too.
You're defining a bad CS course on what they *do* teach rather than what they *don't* teach? Computer Architecture was part of my degree.
Heh, okay, good to know. Maybe I should just have said "Cambridge" instead of UK :P. I don't actually know too much about--or anyone at--any of the other universities there. 
I was complaining about what they *start with*
But these are all ad hoc plugs, not real general solutions. Debug.Trace is but unsafePerformIO in disguise, clumping all state into one StateT can lead to bad memory performance, etc. Ah well, just believe what you want to, and here's the (obviously flawed) benchmark of the (obviously great) imperative language Haskell: http://ideone.com/0dULzX If I choose a (+1) worker function, then 10^5 iterations take 2.8 ms in IO versus 54.5 us for a pure function. And if I choose a "cons a letter to a Text", then the StateT version just hangs outrageously despite IO performing quite well. Not to mention RWST stack always performing badly (I suppose it's because of the mistake of making WriterT lazy, despite the fact that it's never used!). Monads just have an unpredictable and possibly bad overhead, I guess. 
This is awesome, I'm really excited to see it!
I do this, but still often feel like things get out of control at some point, because I've inevitably missed something. I think it's an excellent tool to be able to write type signatures for functions and define flow of control by linking them up, making sure that the types fit, but it's not the end-all-be-all of design. For example, when developing a recent project, I was using a monad transformer that I thought would work for basic flow of the whole application, but it ended up that I had the spots of two of the monads flipped and couldn't access what I wanted to at some point. The whole program compiled and ran as I wanted it to, until I realized something was missing. I was stuck for a while, but it was such a simple fix that I feel like the whole thing could have been avoided. Maybe that's just a (beginner's?) mistake and it really is as simple as you say, but I feel like long time Haskell users have their ways of doing things pretty much laid down. I'm not at that point, and still struggle with things like the aforementioned problem occasionally.
Your last sentence describes precisely the document I'm looking for, and judging from this thread, I don't think it exists at the moment. :)
The first programming language I wrote a serious project in was Visual Basic, the first programming language I seriously learned was SML/NJ. My experience over time has been that every language had its hurdles when I was learning it, but Haskell has definitely been the easiest one to learn to use, balancing simplicitly and consistently nicely with usefulness in the real world. The result has been that I've learned it better than anything else (except maybe for C#). I can say that the functional approach was more intuitive for me, but I picked it up early in my development as a programmer. I've also sort of become stuck on Haskell because I really struggle to learn anything more difficult than Haskell, now. JavaScript or C++ by comparison to Haskell are byzantine, difficult to learn languages. Things you take for granted like "there is one type of string" or "values fed to me are not null" just go out the window and complicate everything.
Surprisingly, I've searched through some non-trivial open-source Haskell projects and they hardly ever use monad transformers too. But that goes to show that Haskell is good at discouraging imperative style rather than being good at it.
Can I use equational reasoning, QuickCheck, etc, to convince myself that TLS/HTTPS is implemented with proper handling of encryption? Let's assume I have a (non-Haskell) implementation I trust, and can access it over FFI or command-line Obviously not looking for 100% certainty, but I would like to be more confident in this than for example some Ruby grem mentioned on /r/programming (Free blog post idea!)
&gt; An early application of separation principles for reasoning about arrays using a notation called partition diagrams that Reynolds invented for this purpose. The material is also covered in Chapter 2 of Craft of Programming. A technique that Reynolds often used for motivating this material was to ask students to write a binary search program and let them discover for themselves how it is hard it is to get it right. With Reynoldss technique, it is almost impossible to get it wrong. One of my lecturers brought up some study about assigning the task of implementing binary search on arrays to a sample of programmers. IIRC most or half of them got it wrong in some sense. A 'solution' implementation was given. Some number of years later, that implementation was found to be wrong and fixed. 
They will, but it will be a much longer process than if you elect someone to act on your behalf if you fall off such an edge.
This. The fact that so little is written about this makes me wonder what "large scale" systems are actually in use. So many of the IO libraries in Haskell need to be rewritten in a modern style. They are like old C-libraries that are untestable and unusable in large systems without testable abstractions bolted on top. IO should never be in the signature of a function. It is untestable, unmockable, unfakeable and unfit for large scale software systems. As a community we really need to learn from Java how to build interfaces. 
Please don't die in the Ukranian revolution. It's soo not worth it!
it looks as though the output hasn't been un-htmlified [*] I wrote a simple &amp;lt;200 line Forth interpreter. Does...
Makes sense. Thanks for all the good advice. :)
Yes, your benchmark is quite flawed. Here's [my version](http://lpaste.net/99698). One of your problems was not forcing your accumulators, and the other was putting the blame on `IO` and `ST` when in fact the blame should be put on `IORef` and `STRef`. As I demonstrate in my version, unboxed versions (even when implemented as an ugly hack like mine) of these perform just fine. I didn't bother with the transformers because I don't feel like taking the time to really defend transformers, but I note that your benchmark uses them very unidiomatically by lifting every `IO` action individually. I would not expect that to perform well in the first place.
At first I was surprised not to see `lens` but then I realized that it has more than 30 dependencies and therefore can not possibly make the list.
lens have 148 reverse deps .. and counting :)
`ScopedTypeVariables` anyone? foo (x :: Type) = y
Those counts are not transitive. They only include direct dependencies.
I'm not sure what the best unescaper is, but with say `fast-tagsoup` you would add import Text.HTML.TagSoup.Fast and then add . to unescapeHtmlT to the sequence of lenses after `_String` Oh, you need to add `to` to the imports from `Control.Lens`
Oh I see, my mistake.
tldr: If you try hard enough you can even in Haskell write ugly code. 
Rather tangential, but I didn't know there was a `bert` package. I used bert quite a bit in my Erlang days!
I would really love this as well. Last I checked it's hard, however, as the tls package assumes an IO base.
Yep. I never claimed it was original :)
OK, well I elect the Core Libraries Committee.
I see `mtl` is high up there, but I keep on hearing it's kind of antiquated, should I be looking at other packages instead for monad transformers?
Ideally in any library any function that is exported should have at least one associated example besides a description of what it does. [Mathematica](http://reference.wolfram.com/mathematica/guide/Mathematica.html) is probably the best example for that. In Haskell too often it is assumed that simply by looking at the types it is clear what a function does, this is not really the case.
Possibly, but it would be a sizeable effort. The [full TLS 1.2 specification is pretty goddamn long and complicated](http://tools.ietf.org/html/rfc5246), with lots of knobs (e.g. cipher suites) and many moving/complicated parts (x509, etc.) There is much, *much* more to TLS than just "proper handling of encryption" - encryption is a small, configurable piece of the pie. It's entirely possible [you messed it all up anyway](https://crypto.stanford.edu/~dabo/pubs/abstracts/ssl-client-bugs.html). There are unfortunately plenty of examples of this (e.g. Python's `urllib` modules [don't even verify that certificates are properly signed](https://lwn.net/Articles/582065/), making any kind of secure HTTP communication trivially MITMable.) On the note of equational reasoning, there have been efforts to create a [formalized and verified TLS implementation](http://www.mitls.org/downloads/miTLS-report.pdf), so it's certainly possible. But it's definitely not Haskell, and not easy to do today, either. As for the real question: does the Haskell TLS package today provide the necessary stability and security we would expect from such an offering, and how can we test and be sure of that that? Well, there will naturally be some bugs. But a lot of people are using it - for stability what you can do is just try to talk to a lot of different SSL libraries - PolarSSL, NSS, OpenSSL, GnuTLS, etc. Write simple dumb servers and dumb clients to push through lots of data and ensure it comes out clean with checksums. Unfortunately to get decent coverage you must also test many different ciphers and many combinations of options. But feasibly I think a lot of that could be automated, and a lot of the actual Haskell code could probably be factored and made 'pure', too, even the packages are somewhat impure: http://www.cs.nott.ac.uk/~txa/publ/beast.pdf &amp; http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.118.9528 As for security, I don't know - auditing those libraries is a minefield, and there could be several problems there (based on the fact *every other implementation ever* has had security problems, I speculate this is highly probable). In the end, if you want to use the same SSL implementation that literally everyone uses you're also free to use [http-client-openssl](http://hackage.haskell.org/package/http-client-openssl), even if OpenSSL makes me want to vomit in my mouth.
Maybe make it in literate haskell and compile it into html? At least for me that would be easier to read.
There already is a package for this: `lens-family` and `lens-family-core`. They are mostly lens compatible and I use them for their smaller dependencies and narrower types.
I'll make a note that zippers are a conceptual pre-requisite, sort of. Thank you.
Haha yeah, I saw that article (as well as [this one](http://en.wikibooks.org/wiki/Haskell/Arrow_tutorial) ) after writing mine without even realizing they were there. The article is amazing :) I might link it. And at least I now know what I can focus on so I don't overlap too much.
&gt; Well, as it happens, our Focuser' type is equivalent to the type... I feel like when I was learning lens *that* was the big question.
Interesting, but the complexity alone makes me want to run in fear. 
You have good instincts UPDATE roche made some great simplifying suggestions. The type signatures are much better now, take a look!
Yeah, turns out Cabal likes to do its work in `/root/.cabal`, even placing binaries in directory only root can read, `/root/.cabal/bin`. Works on my machine! https://github.com/mcandre/mcandre-ubuntu/blob/master/manifests/default.pp#L195-L233
I use `transformers` exclusively for everything and I have never had problems. It's a little more verbose (and not much more verbose once you learn several tricks), but it provides excellent type inference and type errors. The other feature is the sparing use of type classes, which teaches good functional habits.
You are right! I totally forgotten about it. Thanks for pointing that out, will update it soon! 
Yes, indeed. Why is it equivalent, and how would you stumble across this equivalence. Well, I'll see if I can figure out why, but the witness to the iso should at least satisfy the desire to know *that* it's true. But I think it's definitely beyond the scope of the tutorial to explain how you'd discover it. Especially since that is basically just "knowing a lot of category theory".
I was just wondering whether they actually did anything or if it was just a way to [sneak `m a y b e` into your code](http://hackage.haskell.org/package/generic-maybe-0.1.0.1/docs/src/Generics-Maybe.html#G) :p
I agree with the first half of your comment. The current solution is to use a type synonym if you want a quick and dirty way to document the parameter's purpose, but it's not the best solution.
&gt; Haskell's syntax definition is hypercomplicated and full of surprises just to make it look like 'mathematical textbook notation'. There is sugar in the syntax to make certain things easier for some people, but most of that sugar can be avoided simply by using the regular, sugar-free version. Indented do notation can be written as `do { line &lt;- getLine , putStrLn line }` or as `getLine &gt;&gt;= putStrLn`. Case statements can be rewritten as explicit pattern matches in seperate functions, and vice versa. `case fact 5 { 5 -&gt; True ; _ -&gt; False }` is also a perfectly valid one-liner. Finally, there's nothing stopping you from writing your haskell like you write your lisp. factorials = (cons 1 (cons 1 (zipWith plus factorials (tail factorials))))
Not surprising at all that most of those packages are part of the **Haskell Platform:** base, containers, bytestring, mtl, directory, text, transformers, process, QuickCheck, network, parsec, random, template-haskell, stm so most of those packages are installed by default, most people will use them. Maybe you should only include packages in hackage that are not part of the Haskell Platform to see which ones are used as dependencies for most projects.
 liftState = lift . lift Now I just use that exclusively for all StateT actions and if I rearrange my stack I only need to update the definition of `liftState`. Same thing for the other layers.
These aren't full of surprises and hypercomplicated, these are actually consistent and intuitive and I have nothing against it. Also, like I said, Lisp syntax is terrible for Haskell, Lisp syntax takes advantage of variadic functions, explicit braces only gain you something if they supplant grouping and are used purely for application such that grouping isn't necessary any more. Haskell functions are not variadic so Lisp-style syntax uses its value. The counter-intuitive and hypercomplicated things are far weirder than mere whitespace which is consistent except in some few cases. - The minus sign is overloaded in bizarre ways which leads to unexpected behaviour. One would assume that (- x) is simply an operator slice but no, it's a unary negation. But the function used for that can't be captured by (-) because that _is_ an operator slice of the subtraction function. There is no way to capture this function, - is just in some weird cases syntax for `negate`. - 2 years back I tried to define an infix operator `(..) :: (c -&gt; d) -&gt; (a -&gt; b - &gt; c) -&gt; (a -&gt; b -&gt; d)` for 'two argument function composition' if you will. and `(...)` for 3 argument, couldn't do that. It took me half an hour to figure it out, again counter-intuitive behaviour. I was under the impression that an infix operator could use the character `.` and in fact it can, except two dots because my guess it conflicts with `[n..m]` notation. The error message was thoroughly unhelpful for that back then. Why is `[n..m]` notation needed? Does something like `range start end` and `rangeByStep start end step` not suffice? Why does this super specialized thing which quite frankly is not that common need its own specialized notation when a simple function suffices. (Also, Haskell breaks tradition by making ranges inclusive on both ends, most languages make the lower bound inclusive but not the upper bound). - The indentation rules after the `module &lt;name&gt; where` are unique. One would assume that the indentation rules require you to indent one level further after the where but they don't in this case. This is again the one exception to the rule. - Function composition is right to left, but monadic binding and monadic composition is left to right. This is more part of the standard library than the actual language but this inconsistency is annoying. Someone in mathematics once set in stone the principle that `f . g` should mean that f is composed on g rather than in reverse. Pretty much _everything_ in programming languages goes in the opposite direction and if we view functional composition as certain forms of non commutative multiplication the tradition is also typically to reverse the order. From sequencing to piping to do notation. In programming languages we tend to work from left to right, not right to left. it sort of follows from that we read from left to right in English. The very type of the function `(.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c)` shows how skewed this is. Does `(.) :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; (a -&gt; c)` not look more natural? - Haskell has _one_ infix type constructor (-&gt;) which they don't consider a type constructor but I'm not sure why not, maybe someone can tell me why (-&gt;) isn't a type constructor? The syntax is deeply special and magical and heavily overloaded to mean different things. They could've said that infix type constructors have to start with `-` or even `-&gt;`. In fact, any infix operator can probably serve as a type constructor because the namespace is different. - Lists enjoy special literal notation that other things don't. I'm some-what okay with special literal notation provided it is absolutely needed to make code readable. The point is, here it isn't. : is not that much more work to type than a comma, the added Nil or [] at the end is really not that much work (a : b : c : d : e : f : Nil) is really not that much more outstanding working than [a , b , c , d , e , f]. You save the Nil in work, that really does not in my opinion justify specialized notation. - In fact, by freeing [ ... ] from lists. You can use it for a far more general construct. let's say Haskell defines variadic function syntax. Let's say you can define variadic syntactic-sugar yourself. Say you can define [: a b c d c] as a syntactic layer which the compiler then rewrites statically to (a : b : c : d : c : Nil), the head of this application has to be a name, not an arbitrary expression obviously but suddenly you got lists, vectors, hash tables, all in literal notation with a single _consistent_ principle. It doesn't look like mathematical textbook notation no, but on an objective level it's better in every single way. Propagating mathematical textbook notation is propagating historical mistakes. I'm sorry but everyone knows that if someone many years back did not decide to use the same symbol for subtraction and unary negation that would have saved us a lot of time and money right now, it's a mistake. You shouldn't give a unary and binary operation the same symbol. I personally don't really like infix notation but at least Haskell keeps it consistent and non magical unlike other languages. But is `neg x` or `0-x` really that much effort to write so that `(-x)` has super counter-intuitive behaviour?
I have a criticism of this: you are talking about lists in Haskell as if they are simply data structures, which you could call the "naive" understanding of lists. Actually, in a lazy language like Haskell, lists are control flow structures. You use can lists to implement loops, like with function from the "Control.Monad" module like forM, replicateM, sequence, and msum, as well as map, concat, and fold. Lists are also monads, so when you want to find the best possible answer in a list of all possible answers, you can do that using the ordinary monad operators, or also using the imperative syntactic sugaring for monadic computations. The Monadic bind operator (&gt;&gt;=) is defined as "concatMap". So anywhere you use concatMap or (concat . map) can be replaced with the bind operator. 
Now I am starting to understand why generics never really took off and everyone is talking about lenses now instead.
That would make it more practical. But that wasn't the main goal. I wanted to see what the GHC.Generic solution looked like, and how usable I could make it. I think it is actually usable, whether you want to use something this complicated for such a simple problem well, yeah I'm not sure where I stand on that myself. The thing I like about this you don't have to do any extra work for your types. I usually derive Generics anyway, so that's it your done. Even adding one `instance MaybeLike Foo` is work I don't want to do. 
Lenses would not help by themselves. The alternative solution is to use a type class.
I'll take a look in Control.Category. Just to help clarify, I'm aware that each of the items listed have their own composition method. I was more thinking about how awesome (though probably impractical) having a means of abstracting over all forms of composition would be. Also I should probably note that I'm not a terribly advanced haskeller. For some reason I just never end up needing more complicated stuff.
Could you forall them away? None of the functions seem to talk about the extra types.
Probably, but I took one shoot at it add it didn't immediately work. If I get rid of the extra variables, then type signatures would look more like the Maybe ones. I was hoping someone might look at it and go, "ah use RankNTypes on your type syn this way ... " problem solved
This is an entirely wrong assumption. This is not at all an idiomatic way of using generics and the way this library looks has nothing to do with generics taking off or not. Also, lenses solve a different problem than generics do. Both can be really valuable.
For me, I just decide what information needs to be seen and/or updated by all parts of the program and place that data into a single data type which becomes the state of the program. Then I write a monadic API functions to provide controlled access over how the state is modified. If necessary, I write "subordinate" stateful monads which modify particular parts of the main monad state data. So for example if I am making like a text editor with multiple tabs, I might just have a "Tabs" data structure that contains all the documents to be the state of the program. I would write one main monad 'Editor' for modifying the tabs, and one subordinate 'EditDoc' monad for modifying a document stored at a specific tab: {-# LANGUAGE GeneralizedNewtypeDeriving #-} module Editor where import Control.Monad.State import Control.Monad.Reader import Control.Monad.IO.Class import Control.Applicative import Data.Sequence import Data.Map import Data.IORef import Data.Unique import Data.Monoid import Data.Text import System.IO type Tabs = Map TabPath Document data TabPath = NoTabPath Unique | TabPath FilePath deriving (Eq, Ord, Show) type Document = Seq Text -- a sequence of lines of text -- This is the editor monad, which controls tabs and which documents are loaded newtype Editor a = Editor{ editorToReader :: ReaderT (IORef Tabs) IO a } deriving (Functor, Applicative, Monad, MonadIO) instance MonadReader (IORef Tabs) Editor where ask = Editor ask local f (Editor ed) = Editor (local f ed) -- This is the function you would use in your "main" function. runEditor :: Editor a -&gt; IO a runEditor (Editor ed) = newIORef mempty &gt;&gt;= runReader ed -- This is a subordinate monad, it only operates on single documents, not on tabs. You could -- think of it like a monad that focuses on a document, and any functions of this monad may -- modify the document it is focused on. newtype EditDoc a = EditDoc { editDocToState :: StateT Document IO a } deriving (Functor, Applicative, Monad, MonadIO) instance MonadState Document EditDoc where state = EditDoc . state -- This function will modify a 'Document' using a function of the 'EditDoc' monad. runEditDoc :: EditDoc a -&gt; Document -&gt; Editor (a, Document) runEditDoc (EditDoc ed) = liftIO . runStateT ed -- This function takes a 'TabPath' to select a 'Document' from within the 'Editor' monad state, -- and it takes an 'EditDoc' monad and uses it to modify the selected 'Document'. This ties the -- main monad and the subordinate monad together in a meaningful way. editDocument :: TabPath -&gt; EditDoc a -&gt; Editor a editDocument path editDoc = do ioref &lt;- ask tabs &lt;- liftIO $ readIORef ioref case Data.Map.lookup path tabs of Nothing -&gt; liftIO $ hPutStrLn stderr ("Document not loaded: "++show path) Just doc -&gt; do (a, updatedDoc) &lt;- runEditDoc editDoc doc liftIO $ writeIORef ioref (M.insert path updatedDoc tabs) return a -- And here is an example of an 'EditDoc' monadic function which simply appends the given 'Text' -- at the end of the 'Document' that it is focused on. You would write many more functions like -- this to insert text, perform regex search and replace, delete lines, and so on. appendLine :: Text -&gt; EditDoc () appendLine text = modify (|&gt; text) 
If you want this to be a drop-in replacement for maybe 'fromMaybe' should probably not return a maybe(like) again.
Well now that I know I have at least one user that changes things :). I could easily drop the lens dependency. 
why?
I'm curious about what is idiomatic GHC.Generic code. Is there a particular piece of code that comes to mind?
One possibility would be to provide MaybeLike maybe a d u m b y =&gt; Iso' maybe (Maybe a) and then all your other combinators come for free. [EDITED to correct type sig] 
Because that's not what `Data.Maybe.fromMaybe` does. fromMaybe d = maybe d id
ah crap my bad. That was dumb. New version pushed.
Please do :) For my packages I try to support GHC starting from 7.4 when possible. But IIRC 7.4 doesn't have `ConstraintKinds`, so the `MaybeLike`constraint synonym won't work there. A class-based approach would work, though. It'd also give nicer error messages when trying to use generic maybe functions on something that is not a maybe.
I'm not very `lens`-literate, but are trying to say that `a` and `Maybe a` are isomorphic?
Somehow, hopefully. A type class is probably the most sensible thing I bet.
Generic programming is usually used to make functions that are generic over all structures of sums &amp; products etc. This means functions that work for all datatypes. Like generic equality, or generic serialization to and from JSON/XML/binary etc. This is doesn't mean your code is wrong, it is just a less common approach to target specific structures of sums products. I do think it's an interesting approach though.
I don't think Tom's type signature is correct. Probably something more like this: MaybeLike maybe a d u m b y =&gt; Iso' maybe (Maybe a) 
Haskell functions tend to be incredibly short, so why does it matter? Furthermore, there are a multitude of instances where single character names *are* the most descriptive names possible (or, rather, a longer name would merely be equally descriptive.)
This is actually "nice" Generic code, compared to want goes on in say Aeson (for good reasons though).
I used to think that `veryLongVeryDescriptiveVariableNames` were universal best practice in software development. Then I met Haskell. I learned that long names often make code harder to read, understand, and maintain. Sometimes even *much* harder. Haskell (once you have learned it and are comfortable with it) is very expressive. It's true, though, that you also need human-readable comments, and sometimes mnemonic variable names, for code to be easily understandable by a human reader. It takes some work to find the optimal balance point between brevity and expressiveness to maximize readability. And of course, the location of that optimal point depends on who you expect your readers to be. But no - it's not just CS academics who use short variable names.
I do the same, but one thing that's going for the MTL approach is that functions have a nice specification of the monadic features they use in terms of typeclass constraints. Instead of having a specific type of the whole monad stack, part of which isn't even used, it just says: "I need monadic state, and monadic continuations". On the other hand, the semantics of these monadic operations are going to wildly differ according to the stack arrangement which is not specified in the type. Therefore, it is also a bit dangerous, since the function likely relies on a specific semantic which it cannot express in the types. Another problem with the transformers approach, is that not everything is as easy to lift as "lift . lift ...". Sometimes, you need to use "mapStateT . mapReaderT . ..." (for example, when lifting "local") and the mtl makes lifting all these various operations easy. 
Why is that?
Thank you for the very detailed response! This is similar to what I would typically do as well, but put together a bit more nicely.
How do you compose typeclasses?
A generic means of composition is precisely what category theory is all about (that and yoneda) 1 - Functions - Functions are morphisms in the Hask category. 2 - Actions in a Monad - Functions (a -&gt; M b) are morphisms in the Kleisli category of M 3 - Monads - Monads don't compose necessarily. Do you mean monad transformers? Monad transformers are morphisms in the category of monads. 4 - Types (sum types) - There are many ways to compose types, all of which can be explained categorically. Monoids and Groups and Cartesian Closed Categories (all of which explain types in various aspects) can all be viewed categorically. 5 - Typeclasses - How do you compose typeclasses?
I'm not sure about the exact rules GHC (7.6.3) has, but it doesn't allow it. E.g. it accepts these definitions type GMaybeLike' maybe a = forall d u m b y . GMaybeLike (Rep maybe) (G d u m b y a) type MaybeLike' maybe a = (Generic maybe, GMaybeLike' maybe a) But as soon as I use them in a function definition fromJust :: MaybeLike' maybe a =&gt; maybe -&gt; a fromJust = fromJust' . view gsimple it says [1 of 1] Compiling Generics.Maybe ( src/Generics/Maybe.hs, interpreted ) src/Generics/Maybe.hs:220:13: Malformed predicate `GMaybeLike' maybe a' In the type signature for `fromJust': fromJust :: MaybeLike' maybe a =&gt; maybe -&gt; a I do not know whether the `constraints` package will help here either, due to the functional dependency. 
Oh, of course - `MaybeLike` is a constraint not a type. I completely missed that.
The invariants are listed in [the source](http://www.haskell.org/platform/doc/2013.2.0.0/ghc-api/src/OrdList.html).
You could define it as two data types: data OrdListNonEmpty     where One  a  OrdListNonEmpty a Many  a  [a]  OrdListNonEmpty a Cons  a  OrdList f a  OrdListNonEmpty a Snoc  OrdList f a  a  OrdListNonEmpty a Two  OrdListNonEmpty a  OrdListNonEmpty a  OrdListNonEmpty a data OrdList  Emptiness     where None  OrdList Empty a Some  OrdListNonEmpty a  OrdList NonEmpty a 
Ah you were talking about a different way to implement it, yeah that is better. I'm going to remove the lens dependency which makes that approach less attractive considering the logic is written already.
Got the first case to compile with type family (x :: Emptiness) :+: (y :: Emptiness) :: Emptiness where Empty :+: a = a x :+: y = NonEmpty 
It doesn't seem to work with closed type families, possibly because of what it says in your link GHC has no internal notion of inequality, so it can't use previous, failed term-level GADT pattern matches to refine its type assumptions. So when writing (+++)  OrdList f a  OrdList g a  OrdList (f :+: g) a None +++ None = None None +++ b = b *None* is the only possible case of *OrdList 'Empty* so it must follow that *b  OrdList 'NonEmpty a*, same as the return type but I suppose GHC can't tell without an explicit witness or proof that it must be *NonEmpty*?
Please compare and contrast to [AwesomePrelude's MaybeC](https://github.com/tomlokhorst/AwesomePrelude/blob/master/src/Generic/Data/Maybe.hs)
How long after 7.8 will 7.4 still be worth supporting?
Looks like you get the same functionality, but you would have to derive an instance of MaybeC yourself. generic-maybe requires the least amount of programmer effort, if you ignore the wasted calories looking at the type signature ;)
And the first four cases with type family (x :: Emptiness) :+: (y :: Emptiness) :: Emptiness where Empty :+: e = e e :+: Empty = e NonEmpty :+: e = NonEmpty e :+: NonEmpty = NonEmpty --... appOL :: OrdList (e :: Emptiness) a -&gt; OrdList (e' :: Emptiness) a -&gt; OrdList (e :+: e') a None `appOL` b = b a `appOL` None = a One a `appOL` b = Cons a b a `appOL` One b = Snoc a b _ `appOL` _ = undefined {- a `appOL` b = Two a b -} But I'm still erroring out when trying to do the last case, ```a `appOL` b = Two a b```: OrdListGADT.hs:28:27: Couldn't match type e with 'NonEmpty e is a rigid type variable bound by the type signature for appOL :: OrdList e a -&gt; OrdList e' a -&gt; OrdList (e :+: e') a at OrdListGADT.hs:23:10 Expected type: OrdList 'NonEmpty a Actual type: OrdList e a Relevant bindings include a :: OrdList e a (bound at OrdListGADT.hs:28:1) appOL :: OrdList e a -&gt; OrdList e' a -&gt; OrdList (e :+: e') a (bound at OrdListGADT.hs:24:1) In the first argument of Two, namely a In the expression: Two a b OrdListGADT.hs:28:29: Couldn't match type e' with 'NonEmpty e' is a rigid type variable bound by the type signature for appOL :: OrdList e a -&gt; OrdList e' a -&gt; OrdList (e :+: e') a at OrdListGADT.hs:23:10 Expected type: OrdList 'NonEmpty a Actual type: OrdList e' a Relevant bindings include b :: OrdList e' a (bound at OrdListGADT.hs:28:15) appOL :: OrdList e a -&gt; OrdList e' a -&gt; OrdList (e :+: e') a (bound at OrdListGADT.hs:24:1) In the second argument of Two, namely b In the expression: Two a b 
I don't see any connection (but maybe it's just me). AwesomePrelude simply "lifts" all operations to work on `j X` instead of `X`. This is useful for DSLs, but that's about it. It doesn't even provide a `MaybeLike` class, let alone allow to derive it.
It's what Lennart Augustsson does when parsing a string into a monomorphic representation and then typechecking that into a GADT: [More LLVM](http://augustss.blogspot.se/2009/06/more-llvm-recently-someone-asked-me-on.html)
"Mutate" but not mutate. Lens Setters won't modify the value pointed at by another reference to the same object, so it isn't a drop in replacement for all mutative algorithms. But for algorithms that don't rely on that (or can easily be modified such), yay, lenses!
If you remove the lens dependency you could take the approach based on a pair of maps MaybeLike maybe a d u m b y =&gt; maybe -&gt; Maybe a MaybeLike maybe a d u m b y =&gt; Maybe a -&gt; maybe Exposing that functionality will make it most reusable from the point of view of your library's users. Additionally you can export the `Iso'` with only a dependency on `profunctors`, but that's still maybe too much.
This is actually a nice twist on the boundary between functions and values. Usually a literal function never looks like a value-- not needing an argument means too is a value (possibly a thunk at runtime) not a function. But foo being "constructor" doesn't imply too is a function! Tricky name!
It compiles given the following definition :) I suppose the closed type families cut out a few cases but it's still not very acceptable (+++)  OrdList f a  OrdList g a  OrdList (f :+: g) a None +++ b = b a +++ None = a One a +++ b = Cons a b a +++ One b = Snoc a b Many a as +++ Many b bs = Two (Many a as) (Many b bs) Many a as +++ Cons b bs = Two (Many a as) (Cons b bs) Many a as +++ Snoc bs b = Two (Many a as) (Snoc bs b) Many a as +++ Two xs ys = Two (Many a as) (Two xs ys) Cons a as +++ Many b bs = Two (Cons a as) (Many b bs) Cons a as +++ Cons b bs = Two (Cons a as) (Cons b bs) Cons a as +++ Snoc bs b = Two (Cons a as) (Snoc bs b) Cons a as +++ Two zs us = Two (Cons a as) (Two zs us) Snoc as a +++ Many b bs = Two (Snoc as a) (Many b bs) Snoc as a +++ Cons b bs = Two (Snoc as a) (Cons b bs) Snoc as a +++ Snoc bs b = Two (Snoc as a) (Snoc bs b) Snoc as a +++ Two zs us = Two (Snoc as a) (Two zs us) Two xs ys +++ Many b bs = Two (Two xs ys) (Many b bs) Two xs ys +++ Cons b bs = Two (Two xs ys) (Cons b bs) Two xs ys +++ Snoc as a = Two (Two xs ys) (Snoc as a) Two xs ys +++ Two zs us = Two (Two xs ys) (Two zs us) correctly giving the type ghci&gt; :type Two (One 'a' +++ None) (Many 'b' "cdef") Two (One 'a' +++ None) (Many 'b' "cdef") :: OrdList 'NonEmpty Char 
Got it, using a proof type. {-# LANGUAGE GADTs #-} {-# LANGUAGE DataKinds #-} {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE TypeOperators #-} module OrdListGADT where data Emptiness = Empty | NonEmpty type family (x :: Emptiness) :+: (y :: Emptiness) :: Emptiness where Empty :+: e = e e :+: Empty = e NonEmpty :+: e = NonEmpty e :+: NonEmpty = NonEmpty data OrdList :: Emptiness -&gt; * -&gt; * where None :: OrdList Empty a One :: a -&gt; OrdList NonEmpty a Many :: a -&gt; [a] -&gt; OrdList NonEmpty a Cons :: a -&gt; OrdList f a -&gt; OrdList NonEmpty a Snoc :: OrdList f a -&gt; a -&gt; OrdList NonEmpty a Two :: OrdList NonEmpty a -&gt; OrdList NonEmpty a -&gt; OrdList NonEmpty a data MaybeEmpty :: Emptiness -&gt; * -&gt; * where IsEmpty :: MaybeEmpty Empty a IsNonEmpty :: OrdList NonEmpty a -&gt; MaybeEmpty NonEmpty a check :: OrdList e a -&gt; MaybeEmpty e a check None = IsEmpty check a@(One _) = IsNonEmpty a check a@(Many _ _) = IsNonEmpty a check a@(Cons _ _) = IsNonEmpty a check a@(Snoc _ _) = IsNonEmpty a check a@(Two _ _) = IsNonEmpty a appOL :: OrdList (e :: Emptiness) a -&gt; OrdList (e' :: Emptiness) a -&gt; OrdList (e :+: e') a None `appOL` b = b a `appOL` None = a One a `appOL` b = Cons a b a `appOL` One b = Snoc a b a `appOL` b = case (check a, check b) of (IsNonEmpty a, IsNonEmpty b) -&gt; Two a b otherwise -&gt; error "None should be caught by earlier cases" This compiles cleanly on ghc 7.8.1-rc1 
Good idea. [An explicit proof](http://www.reddit.com/r/haskell/comments/1xiurm/how_to_define_append_for_ordlist_defined_as_gadt/cfbrinr) worked.
I found a way around the junk type variables (never mind the names): class MaybeLike0 rep any where maybelike0 :: Iso' (rep p) ((U1 :+: Rec0 any) p) instance MaybeLike0 (G m a y b e any) any where maybelike0 = clean instance MaybeLike0 (M1 m a (C1 b (S1 e (Rec0 any)) :+: C1 y U1)) any where maybelike0 = iso invo invo . clean where invo = over m1 commuteSum type MaybeLike1 maybe any = (Generic maybe, MaybeLike0 (Rep maybe) any) fromJust1 :: MaybeLike1 maybe a =&gt; maybe -&gt; a fromJust1 = fromJust' . view (generic . maybelike0) 
Great answer! You can get rid of the error case with this (the *check* function isn't any different): check  OrdList e a  MaybeEmpty e a check a = case a of None  IsEmpty One{}  IsNonEmpty a Many{}  IsNonEmpty a Cons{}  IsNonEmpty a Snoc{}  IsNonEmpty a Two{}  IsNonEmpty a appOL  OrdList f a  OrdList g a  OrdList (f :+: g) a appOL xs ys = case (check xs, check ys) of (IsEmpty, IsEmpty)  None (IsEmpty, IsNonEmpty b)  b (IsNonEmpty a, IsEmpty)  a (IsNonEmpty (One a), IsNonEmpty b)  Cons a b (IsNonEmpty a, IsNonEmpty (One b))  Snoc a b (IsNonEmpty a, IsNonEmpty b)  Two a b
Now `(Generic maybe, MaybeLike (Rep maybe) any)` doesn't look too bad, it's even self-explanatory (except for the `Rep` occurrence), so maybe it doesn't need a synonym. Or you could craft one using the single instance trick (I think you referred to it when you mentioned UndecidableInstances).
Oh awesome. I'll try that tonight.
Cool, can Haskell represent type constraints as a monoid? (I would assume not since they aren't first class, but I wouldn't mind a theoretical discussion of what it might look like anyways.) Note, I'm asking about representing Haskell's type constraints as a value and compose them, not how to compose constraints normally since I've already shown that.
[constraint kinds](http://www.haskell.org/ghc/docs/7.4.1/html/users_guide/constraint-kind.html) gives you a way to compose constraints by bundling them up as regular old type synonyms.
&gt; Then again, Haskell coders who come from a CS academics background still think its acceptable to use non-descriptive single-character names for parameters in a non-academic environment Do you have any clearer variable names for something like map f (x:xs) = f x : map f xs I suppose you could do something like map someFunction (firstItem:restOfList) = someFunction firstItem : map someFunction restOfList but those names are exactly as descriptive and about an order of magnitude longer. Sometimes, code is abstract enough that there's no such thing as a descriptive name.
Interesting... so (,) is used as constraint composition in constraint kinds. Though, without first class types, it can't be unified with (.) or any or the monad binds.
As a suggestion (unrelated to your problem): You could use graphics-drawingcombinators to make use of OpenGL for simple 2d graphics much easier. As for your problem, are you sure the primitives drawing itself is heavy? You could use [TimeIt](http://hackage.haskell.org/package/timeit-1.0.0.0/docs/System-TimeIt.html) to easily measure the time it takes to run the primitive drawing action. As usual, make sure you compile the code with high optimizations (-O2), etc. If you see that with optimizations, drawing lots of primitives is still expensive, you could try to use glLists that record the drawing operations and then you could replay those. As far as I know, the implementation of this actually sends the recordings to the graphics hardware to run it there locally, much more efficiently. Another option is to use shader code -- draw the whole screen as one big rectangle, and use per-pixel shader code on the GPU to decide how to color that pixel. I am less knowledgeable about this option though.
Thanks, I'll try that out. I've heard about the shader thing but... ick, that means I have to translate my (fast but) complicated drawing function into a language I don't know =P (Timeit says drawing operation is taking 0.1 seconds. Ow!)
What do you mean? They're already a monoid. They're not an instance of the monoid typeclass. If we had kind-classes, it would be possible.
If you just want to put some pixels from an array onto the screen, look at gloss-raster.
Well, you could hide the emptiness by using existential quantification: {-# LANGUAGE ExistentialQuantification #-} {-# LANGUAGE RankNTypes #-} -- ... data OrdList' a = forall e . OrdList' ( OrdList e a ) concatOL :: [OrdList' a] -&gt; OrdList' a concatOL [] = OrdList' None concatOL (q:qs) = case (q, concatOL qs) of (OrdList' x, OrdList' y) -&gt; OrdList' (appOL x y) Or you could build up the emptiness at construction time by having your own list type data LL :: Emptiness -&gt; * -&gt; * where LL_End :: LL Empty a LL_Cons :: OrdList e a -&gt; LL e' a -&gt; LL (e :+: e') a concatOL' :: LL e a -&gt; OrdList e a concatOL' LL_End = None concatOL' (LL_Cons x xs) = appOL x $ concatOL' xs 
Oh cool, I'll look through that package's source!
Wouldn't lens' reverse dependency count be skewed by the fact that you can create lenses compatible with lens without depending on lens?
I more or less meant what constraint kinds does but in a first class sort of way. The idea that I'm trying to explore is what it would look like to unify all forms that composition may take. So that you could use  to represent: * h = f  g * OptionalList = Maybe  List * IntOrChar = Int Char * foo :: Show  Read a =&gt; a as the exact same entity (instead of being different entities with the same syntax.) In a type classy sort of sense: class Composable ? where  :: ? instance Composable ?Monad? where  = ?? and so on. I'm not saying that type classes are what I would want to unify using, but hopefully this portrays the intent; to have all types of composition be the same thing. Also, I'm not saying this would be useful; just that I wondered whether or not it was doable or to what extent.
0.1 seconds for 4800 primitives is far enough from a reasonable time that I'm pretty sure you're unwittingly hitting a degenerate case many, many times per frame. The most likely culprit is that you've got an OpenGL call that requires polling state from the GPU, which requires data transfer from the GPU and is very slow. This includes pretty much every `glGet...` function. For people new to OpenGL, this often manifests as lots of calls to `glGetError` every frame, which will kill your performance. This, of course, is all speculation since we've got no code to look at. I'd still recommend moving to using shaders simply because that's how things are done now so you may as well not rely on the deprecated API. You'll probably want to make it draw rectangles (modeled as a pair of triangles) for each pixel. With a bit of care it should be pretty easy to push all the relevant data such that the shader programs do very little.
If my understanding is right then the unified  would have to be a monad transformer. m  Monad, m  Identity Meaning I should probably actually go study monad transformers... Which I should do anyways, but just haven't needed to.
Implemented and pushed. Thanks!
Code to look at is always good! So is github :) https://github.com/MagneticDuck/PixelGame (run the (hastily written and crappy) Main.hs in the example directory after installing the package to recreate a problem situation)
What do you mean?
[This line of code](https://github.com/MagneticDuck/PixelGame/blob/master/src/PixelGame/Graphics/Pixels.hs#L54) sticks out at me. I'm not sure if getting the window size requires a round trip, but in any case you're fetching the window size for every vertex you draw, which is clearly extra work you don't want to be doing. Instead, you should be getting the window size once in `drawGraphicsNoFlush` and then passing it in to the rest of your drawing functions. I can't test it out to ensure that's the problem, but that's my guess as to what it is.
I have installed the release candidate in a folder which comes before ghc-7.6's in my PATH, and I change that PATH whenever I want to compare the behaviour of the two versions. Is that what you wanted to know?
Take it to /r/ProgrammerHumor
Take a couple of Aspirin pills, remember that monads are just warm, fuzzy things and continue. I can't guarantee that the headache is going to end soon, but it will eventually.
You could simply write (subtract 1), now couldn't you? 
This picture shows that you're on the right way! The false idols called "Mutability" and "Side-Effecting" must be wrung out of your brain. 
This is 3 months too late, but I had the same issue starting out. I wrote a [guide on web dev](http://adit.io/posts/2013-04-15-making-a-website-with-haskell.html) in Haskell if that helps you.
This site is bull, the example ruby problems are both syntax errors
sorry if this is against the rules folks, it's been a long day of studying
Nobody knows that's why they're looking in to it.
If you are like me, you may be confused by how easy this is. I thought it would be a lot more work than it was. Just download the tarball and install it according to the README - the only option I gave configure was a prefix of /usr/local/ghc7.8rc1. This will install ghc, a cabal and base libraries. Then I added it to front of my path. I use hsenv but it isn't necessary - you can just play with paths like gelisam says. Then I go to my project directory and cabal config etc. I had only one dependency I had to monkey patch to relax a constraint on, of course YMMV.
Note: monad transformers are complicated explicitly because they aren't like functor composition. You need more, like a distributive law, an adjunction to sandwich between, etc. 
Did that change help? Watch out, Haskell's immutable sharing doesn't carry over to foreign/IO APIs ;-) an FRP framework with signal for window size change would have saved you ;-)
I mean why aren't you the OP of this post?
Oh, I usually wait to submit my posts to see how much interest there is in them. An easy way to judge if I wrote a dud is that nobody submits it to /r/haskell. If nobody submits it after a day, then I submit it myself.
Regarding his last point about automating effect mixing using a type class hierarchy, would this be a distinct type class hierarchy from the one that `lens` already uses?
Do you know of any work that would be more suitable then? (i.e. that does have those properties)
&gt; In the case of sum vs. product, there's no way for the compiler to figure out which one you mean. So, overloading all forms of composition to use the same operator doesn't just seem useless, it seems harmful to me. IMHO here we should have Semigroup be a MPTC: class Semigroup o a where -- Ideally we wouldn't need o as an argument op :: o -&gt; a -&gt; a -&gt; a data Sum = Sum data Product = Product instance Num a =&gt; Semigroup Sum a where op Sum = (+) instance Num a =&gt; Semigroup Product a where op Product = (*) a +! b = (op Sum) a b a *! b = (op Product) a b
I suspect it's more like the ones `mtl` uses.
I have several versions of ghc installed to different directories, installed with: ./configure --prefix=/path/to/ghc. I little shell script that takes in a version that switches versions by changing a sym link, which I have on my path. #!/bin/sh rm /Users/jfischoff/ghc if [ $1 == "7.6" ]; then if [ $2 == "32" ]; then ln -s /Users/jfischoff/Library/Haskell/ghc-7.6.1-i386 /Users/jfischoff/ghc else ln -s /Users/jfischoff/Library/Haskell/ghc-7.6.1-x86_64 /Users/jfischoff/ghc fi elif [ $1 == "7.7" ]; then ln -s /Users/jfischoff/Library/Haskell/ghc-7.7-x86_64 /Users/jfischoff/ghc else ln -s /Users/jfischoff/Library/Haskell/ghc-7.0.3-x86_64 /Users/jfischoff/ghc fi hash -r 
FYI typo: "We could give an console interpretation"
`hsenv` has also worked wonders for me whenever I need to toy with alternative `ghc` versions.
There is a guide for *cross-compiling* AArch64 GHC on x86/x64 machine: https://ghcarm.wordpress.com/2014/01/18/unregisterised-ghc-head-build-for-arm64-platform/ But if you have access to AArch64 machine please volunteer it as *GHC build slave*: https://ghc.haskell.org/trac/ghc/wiki/Builder I hope that astounding success of new Opterons will make AMD produce 16-core or 32-core version.
Don't forget the Cabal `-w` flag for specifying what GHC to use!
Rendering 4800 primitives should be OK even with the very low-end Intel integrated graphics cards. However, you should use vertex arrays instead. Another option is to make a raster image with the CPU, and use that a single big texture. The problem with that is that diagonals will be ugly unless you use a very high resolution texture. This can be also slow. A third option (probably the best) is to make a 40x60 texture, and do the "sub-pixel" (diagonal etc bisections) from a shader. The shader fetch the corresponding pixel from texture, which contains the two colors and the type of bisection (if that's two much info to cram into a texel, use 2 textures), then decides based on the screen coordinates which part of the pixel you are in.
Why would anyone downvote it? I didn't see anything controversial in there.
i think it is merely not interesting. no need to down- (or up-) vote.
Well, it's sometimes annoying to see how some of these 'celebrities' dismiss Haskell so quickly one way or another. In this case, regarding Haskell as just a highly theoretical language with focus on correctness, somehow implying that there is a huge trade-off, might not be entirely fair. I guess many people hear about Haskell for the first time from this kind of blog posts. Like with the xkcd story, I still think it's positive for Haskell to have this exposure.
Fair enough. I just don't want to be banned from /r/haskell for submitting links of what famous bloggers say about Haskell :)
Different people have different favorites - I would have expected a much more negative reaction to Haskells "purism" - what I would not have expected is the "not real world enough" joke(?) (... is ML that more common in the real world?) ... anyway - not much in that blog post - I guess there is more to come ;)
On a similar note, I install all my GHC's under `/usr/local/hs/ghc-X.Y.Z` and then have this convenience function in my `.bashrc`: setghc() { if [ $# -eq 0 ]; then echo $PATH | sed 's%.*/usr/local/hs/ghc-\([0-9.]\+\)/bin:.*%\1%' elif [ $1 = '-l' ]; then ls /usr/local/hs/ | sed 's/ghc-//' else local GHC_DIRECTORY=/usr/local/hs/ghc-$1/bin local CLEAN_PATH=$(echo $PATH | sed -r 's%/usr/local/hs/ghc-[0-9.]+/bin:%%') if [ $1 = '-d' ]; then export PATH=$CLEAN_PATH elif [ -d $GHC_DIRECTORY ]; then export PATH=$GHC_DIRECTORY:$CLEAN_PATH else echo "requested ghc version is not installed" fi fi } I do actually have a reason for the specific `ghc-X.Y.Z` directory name, and that is because I have my cabal config set up to install global packages under each ghc subdirectory instead of in a common area, which makes cleaning up easier. And, IIRC, there isn't really a way of specifying a path fragment that's specific to a ghc version that isn't `ghc-X.Y.Z`. I also symlink all the executables with versioned names in `/usr/local/bin` and also set the unversioned symlinks to whatever ghc I'm using most at the time. It's not perfect; I really should modify this to put my `~/bin` directory ahead of the ghc path, but it works well for me most of the time. 
Nice to see!
Isn't this just explicit dictionary passing? I.e. where you would normally have a computation of type `MonadReader r m =&gt; m a`, you now have a dictionary data ReaderOps r m = ReaderOps { ask :: m r, local :: forall a. (r -&gt; r) -&gt; m a -&gt; m a } and a computation of type forall m. Monad m =&gt; ReaderOps r m -&gt; m a Not that this isn't useful, as the handle example shows; this would not work if you'd have a `TeletypeMonad m` type class.
I'm glad our paper was helpful! 
It was, thank you! Easy to understand and implement, and I could follow the claims made about it. Two thumbs up. 
I actually looked using fsnotify (cross platform file system watching, http://hackage.haskell.org/package/fsnotify-0.0.11) for this, but it doesn't seem to allow me to watch single files, only directories. Pointers are very welcome!
Woa you can be banned for that ?
This bug asks for it: https://github.com/haskell-fswatch/hfsnotify/issues/32
I doubt it.
i have not looked in detail in the blog post or the paper, but the code looks similar to tekmo's post "scrap your typeclasses". http://www.haskellforall.com/2012/05/scrap-your-type-classes.html
immediate mode is not ideal, but should be fine for 5000 polys even on low-end hardware
I was thinking more like `IOSpec`.
Excuse my ignorance, but who is this famous blogger guy?
&gt; "But in practice, correctness isnt always the highest priority, nor should it be necessarily." It isn't?
Because classes are product types. I'm imagining a class class Monad m =&gt; TeletypeIO m where {- No additional laws! Freedom! -} getChar :: m Char putChar :: Char -&gt; m () Then the type of the vanLaarhoven free monad for teletype operations becomes `forall m. TeletypeIO m =&gt; m a`. I really have no idea if this is practical, or even necessarily a good idea. The passing of a `Handle` doesn't work when you use classes. On the other hand, perhaps both the classy and non-classy styles might work well in harmony together. To start with, someone ought to reimplement `IOSpec` with the van Laarhoven approach and see how things pan out.
No. Lots of downvoted submissions may make future submissions more likely to get flagged by the spam filter and need manual intervention to post, though.
He carries on to give a justification for this view. Is there part of that that you disagree with?
Once I submitted another blog post from a Python programmer picked by the official Python Planet and got some negative votes. To be fair his arguments were a bit lame, but I thought, like in this case, it'd be interesting to hear what people with expertise in other languages think when they try Haskell. That's why I tried to joke about being banned collecting negative vote, heh.
Just me who interpreted that xkcd comic differently? I thought the one saying "..because no one will ever run it?" was in fact the Haskell expert. (playing on the false meme that Haskell is only useful for functional problems)
But then as Sjoerd hinted, this is already what the mtl does. The isomorphism you describe doesn't really provide us with new functionality, as far as I can see. It just provides a, perhaps clearer, explanation of what's going on.
This PStore thing looks like a container where the, so (badly) called, position set isn't dependent on the, so called, shape set, i.e. PStore S P X = [[ S &lt;| K P ]] X? Which makes me wonder if one can generalise it further; something along the lines of: forall F. Functor F =&gt; ((s : S) -&gt; F (P s)) -&gt; F X? If that's interesting, then an obvious next step would be indexed containers...
And so you think that instance Apply (Bazaar1 p a b) where Bazaar1 mf &lt;.&gt; Bazaar1 ma = Bazaar1 $ \ pafb -&gt; mf pafb &lt;.&gt; ma pafb or insertLookupWithKey = go where go :: Ord k =&gt; (k -&gt; a -&gt; a -&gt; a) -&gt; k -&gt; a -&gt; Map k a -&gt; (Maybe a, Map k a) STRICT_2_OF_4(go) go _ kx x Tip = (Nothing, singleton kx x) go f kx x (Bin sy ky y l r) = case compare kx ky of LT -&gt; let (found, l') = go f kx x l in (found, balanceL ky y l' r) GT -&gt; let (found, r') = go f kx x r in (found, balanceR ky y l r') EQ -&gt; (Just y, Bin sy kx (f kx x y) l r) is readable? When reading code in the wild, short meaningless variables definitely hurt readability. Real World Haskell gives nice examples of readable snippets, and the variables are carefully named. you think map is readable because you already understand its meaning.
I pulled hackage index and create a special index for hackage ( http://hackage.haskell-search.org/ ).
Did you discover a way to properly handle type classes in higher-rank types?
The main difference with the mtl is on the type of the operations. Here they are required to be algebraic (of the form A -&gt; m B), while in the mtl they can have any type (for example, the "local" operation of the Reader monad) which leads to a lifting problem. Operations like local can be implemented in a "van Laarhoven" monad as a handler of algebraic operations, i.e. by providing an appropriate instance of the dictionary.
TIL that https://twitter.com/copumpkin is famous. He'll be amused. =)
They actually run most of their in-house code over there in Haskell. It was definitely tongue-in-cheek. http://www.reddit.com/r/haskell/comments/uved7/waldo_the_haskell_powered_codebase_behind_xkcds/ 
&gt; Traveling Salesman is NP-hard but there are workable solutions using approximation and heuristics that are 99.x% correct. That's not about correctness. That's about approximations vs optimal solutions. Totally different definition. Correctness is when the algo got coded wrong and segfaults for corner cases. In hardware, correctness is when the CPU has no FDIV bug. &gt; If time to market is crucial then a small amount of bugs may be acceptable. Too many variables for such a sweeping statement. Sample rebuttal: Of what use is time-to-market if the product still reeks of the same-old brittleness? But this won't get anywhere because of all the unspoken assumptions. &gt; If a bug costs the company $100 a year but it takes a developer a day to fix then his time is better spent elsewhere. If only such numbers were so easily obtainable. You cited Cook about probabilities of errors. Even error probabilities and expectations of dollar-valued consequences are difficult to analyze. Most shops don't, resulting in a severe case of not knowing what's not known. For shops that care about knowing what they do not know and take pains to crunch numbers about the consequences of bugs, guess what? They care about (gasp!) correctness. 
You usually define type class with the operations of the algebraic structure. class Group g where unit :: g binop :: g -&gt; g -&gt; g inv :: g -&gt; g and then declare an instance for it. For example for the additive group on integers: instance Group Integer where unit = 0 binop = (+) inv = (0-) 
This reminds me of Pipes. You can define: type Proxy a' a b' b m r = FreeT ((PStore a' a) :+: (PStore b b')) m r And then the usual push and pull compositions. Now how do you generalize to more than two channels of communication (not only request and respond) and arbitrary compositions of those (not only push and pull)? I also noticed that you can peel off layers of effects one at a time like: handle :: (Monad m,Functor f) =&gt; (i -&gt; m j) -&gt; FreeT ((PStore i j) :+: f) m a -&gt; FreeT f m a handle handler freeT = do freeF &lt;- lift (runFreeT freeT) case freeF of Pure a -&gt; return a Free (InL (PStore i ja)) -&gt; lift (handler i) &gt;&gt;= (\j -&gt; handle handler (ja j)) Free (InR f) -&gt; FreeT (return (Free (fmap (handle handler) f))) Very obvious probably. I just wanted to hear your thoughts.
He already knew that. And was decidedly unamused.
I still don't really grasp the difference. You can implement `local` as a handler if the underlying monad is a free monad, and there's no reason you can't use the mtl typeclass interface with free monads. I guess I'd like to see an implementation of what roconnor is suggesting, because I don't really understand what it is. Algebraic effects are certainly an interesting area of research.
Partial answer to myself: http://lpaste.net/99779
Reddit works by people voting, either up or down, if they feel strongly that something is or isn't worth reading, not if they like or dislike the submitter. If your posts are voted down, please don't see it as a personal attack against you or feel unwelcome to keep submitting content you find interesting. It's just the community's way of collectively trying to get the most interesting content featured here.
I think it is funny, knowing hardly anyone downvotes in this subreddit, that reddit tries to vote fuzz the points.
`local` is not of correct form and therefore the theorem does not claim anything about how to represent `forall m. Monad m =&gt; ReaderOps r m -&gt; m a` in a first-order way. The point of the van Laarhoven free monad representation is that if you are already planning to use a free monad to [connect to a foreign function interface](http://comonad.com/reader/2011/free-monads-for-less-3/), or for [testing "I/O" functions](http://hackage.haskell.org/package/IOSpec), or any other time you might want to use a free monad of the requisite form, then you may want to consider the van Laarhoven free monad representation. I wouldn't use the van Laarhoven representation for any of the monads from the MTL because none of those are free monads. 
If you ignore non-algebraic operations then the mtl signatures look like class MyMonadState m s | m -&gt; s where get :: m a put :: a -&gt; m () and so actions have the type `(Monad m, MyMonadState m s) =&gt; m a`. I believe this type is already a free monad. It's equivalent to Monad m =&gt; MyMonadOps m s -&gt; m a where MyMonadOps m s = { get :: m a, put :: s -&gt; m () } 
This is really interesting. I've been stuck on `PStore` as a parameterised comonad because of it's relevance to lenses, the relation to containers seems more relevant here.
* Client requirements * Existing code reuse * Technology stack (this one is weak) * Developers (this one is the one that stops me from using Haskell) * Interoping with other systems
I suspect there is a van Laarhoven free monad transformer, but we haven't worked out what that would be.
I to write a function getSubgroups :: Group -&gt; [Group] Which finds subgroups of another group. I'm trying to start from the basics and build a system around AlgebraicStructure. 
Thank you! Is there a way to extend another class? If I have class AS g where binop :: g-&gt; g-&gt; g how would I extend it to be a monoid or group? Also, your class group isn't necessarily a group, how do you define a constraint on the binary operator to be associative? EDIT: Also, once I have the class, how do I apply it to finite sets? example: type moduloTen = [0..9] instance AlgebraicStructure moduloTen where .*. g q = (q+g)%10 Or better yet how do I use it once defined?
Fixed.
Of course, I see your point. If you think I've wasted your time by clicking on that link and you think it was not worth submitting I see no problem in downvoting me. I tried to clarify why I submitted the link even if I don't agree with the author.
Why do you think longer names would make either of those more readable? The `Apply` instance is self-explanatory and longer names will only make it harder to read. I'd actually consider (taken out of context) shortening the name of `pafb` because whatever it might mean is irrelevant to the type class. `insertLookupWithKey` is slightly obtuse for reasons of algorithmic efficiency and would very much benefit from comments explaining what it's doing and why it does it that way, and while more descriptive names might help in a few places they wouldn't clarify anything that isn't already pretty simple and would make the code longer which is, all else equal, counterproductive to readability. Yes, it might "look" more readable at first glance to someone trained to think long names are better, but how hard it is to *actually read and understand the code* has very little to do with how "readable" it looks. There is exactly one thing being defined in all of that which has significant meaning it *needs* to communicate via its name, specifically `insertLookupWithKey` itself, which you will note is in fact named appropriately--its name, along with its type, tell you 90% of what you need to know about the function. Look, I'm not sure how much pre-existing real world code you've actually *worked on* over a long period of time, but in languages and codebases where "descriptive" names are the norm, my experience is that names which aren't part of a public API (and even some that are) usually do one of five things: 1) communicate useful information using three times as many characters as necessary 2) provide information irrelevant to their use or purpose 3) restate the obvious 4) actively mislead you about their use 5) my personal favorite: actively mislead you by restating a seemingly-obvious-but-wrong assumption about the code Maybe things are better in open-source code where there's more chance to have people you don't know reading the code; my experience is mostly from working on closed-source Java and C# codebases. This "long names are inherently better and more meaningful" notion is unfounded nonsense. Cargo cult engineering, nothing more.
&gt; To start with, someone ought to reimplement IOSpec with the van Laarhoven approach and see how things pan out. Something like this? http://hackage.haskell.org/package/base-io-access
While I agree that correctness isn't always a priority, I don't agree with what he implies. He seems to say that because Haskell has a focus on correctness, it's giving up in other language features like flexibility, agility or productivity. I think Haskell touches a nice sweet spot between correctness and productivity. Of course you have to invest heavily upfront on learning Haskell, but once you have a decent level, you productivity may be even higher than what what you were used to with a dynamic language.
This guy gets paid to work in Haskell, despite having no experience or knowledge of it at all? How did he manage that?
Good to see kinds getting explained. There not that difficult and help out with understanding why certain things can be functors and other things can not. One small thing, I think your use of sort is incorrect. Expressions, types, and kinds are all different sorts.
&gt; As described above, in pipes-parse you need to convert the entire stream. I do not understand Snoyman's argument for the simpler example. I pointed out on a gist where he origneal posted the simpler example that a simple change to his parser removes the the conversion problem as results in the same traced output as hist conduit code. The gist: https://gist.github.com/snoyberg/8888998/ The modified parser, the only change is adding `Pp.splitAt 1` to delimit `Pp.peek`. let parser = (,,,) &lt;$&gt; zoom (Pp.splitAt 1 . piso ab) Pp.peek &lt;*&gt; zoom (Pp.splitAt 3) Pp.drawAll &lt;*&gt; zoom (Pp.splitAt 3 . piso ab) Pp.drawAll &lt;*&gt; Pp.drawAll results in: ("atob",1) ("btoa",1) ("atob",4) ("atob",5) ("atob",6) (Just (B 1),[A 1,A 2,A 3],[B 4,B 5,B 6],[A 7,A 8,A 9,A 10]) Which is the same output as the conduit code in the article. Soyman has commented in the gist and as a comment on this stackoverflow answer: (http://stackoverflow.com/a/21650705/128583), but I do not understand his objections to the approach I have presented above.
Can you clarify that last part? I've always understood Haskell to have a unique sort which people call `BOX` (or the shinier unicode equivalent). &gt; where BOX is the (unique) sort that classifies kinds. Note that List, for instance, does not get sort BOX -&gt; BOX, because we do not further classify kinds; all kinds have sort BOX from the GHC docs on datakind [promotion](http://www.haskell.org/ghc/docs/latest/html/users_guide/promotion.html)
&gt; the mtl library doesn't provide any free monads Ah but it (kind of) does! `MonadState s m =&gt; m s` is a free monad (at least if you ignore the `state` method, and maybe even if you don't.)
Sorts are just the different forms of syntax in the language. 
Hm, I think the problem here is that sort is overloaded. When I sorts I mean the "level" so to speak above kinds. In Agda it's called `Set1`.
So BOX is the sort above kinds, but there is no reason to represent it directly because there is only BOX. I think you are right that sort is used informally. I find the use here the clearest. http://www.cs.cmu.edu/~rwh/plbook/book.pdf
For the simple example, it *does* solve the problem. My point was that I was specifically trying to test the functionality of leftover propagation, and giving a fixed size for the input stream simplifies the problem too much. Look at the more complicated example I described, where there's a UTF8-encoded sequence of numerals. You wouldn't know in advance how many bytes to read from the stream.
The problem is you do not give a full working example where pipes is breaking in the more complex case, and it does not break in the simple case. The only full working example is the simple case and that seems to work fine in Pipes. &gt; Look at the more complicated example I described, where there's a UTF8-encoded sequence of numerals. You wouldn't know in advance how many bytes to read from the stream. It is not obvious to me that it breaks in that limit with only a description and the working simple example. To explore the issue further I was planing on working through an example where you convert between the types: data A = A Int data B = B Int Int It seems like the problem you are presenting exists when the underlying streamed types do not have an isomorphism. So `A` and `B` above would be an example where no isomorphism between the types being streamed and should expose the problem you are talking about if it exists. 
There's some ambiguity about what "sorts" means. Sometimes people use it to mean the next level above kinds, but this is the less common meaning nowadays. More often, people use it to refer to syntactic categories; thus, "Term" is a sort (often ignored), "Type" is a sort, "Kind" is a sort, etc. In non-uniform languages, these are all different sorts because they have different syntactic natures. In uniform languages like Agda, it's harder to describe since sorts are not the same thing as levels. This latter meaning can also be seen in PTSes. The STAR symbol is a sort, the BOX symbol is a sort, etc. If some expression `e` satisfies `Gamma |- e : STAR` then we call `e` a type; if it satisfies `Gamma |- e : BOX` then we call it a kind; etc. However, it's nonsensical to ask things like `Gamma |- STAR : BOX` because STAR is the name of a syntactic category not an expression in the language! This is in spite of the fact that people enjoy punning and so will often have an expression "Star" which internalizes STAR, or an expression "Box" which internalizes BOX.
This is good short introduction, but I have a couple of wee niggles, which I think can be easily addressed. Firstly, the definition of red-black trees demands that the children of a black node have the *same* colour. If I understand correctly, they should be free to be any colour, so you end up with 2,3,4-trees, rather than 2,4-trees: is a tree with two nodes representable? Repair with NodeB :: a -&gt; Tree a c -&gt; Tree a c' -&gt; Tree a Black or, more boldly, just do 2,3-trees. NodeB :: a -&gt; Tree a c -&gt; Tree a Black -&gt; Tree a Black The other niggle is a bit of pedantry about type synonyms. It's misleading to suggest that `Cons` has kind `* -&gt; * -&gt; *`, because `Cons` standing alone is not a thing: you can't use `Cons` where a thing of kind `* -&gt; * -&gt; *` is expected. Sure, uses of `Cons` are kind-checked, so that `Cons a b` is a thing of kind `*` if `a :: *` and `b :: *`. Contrast that with `Either`, which can be used wherever a thing of kind `* -&gt; * -&gt; *` is expected.
[i see what you did there](http://www.reddit.com/r/haskell/comments/ti5il/redblack_trees_in_haskell_using_gadts_existential/). :) I should update that gist to use `DataKinds`. I do love them so.
*"If you want to express equations in the type you'd need to use a language with dependent types, such as Agda."* I wonder if it could make sense to decorate classes with statements of their laws that could be quickchecked for every instance. It would probably need some means of saying "No, this applies, trust me" for things where actually running quickcheck might not terminate - but at least then the instance writer would have to individually mention (and thus more likely individually consider) the various laws.
This starts to get a little bit ridiculous, but it's actually a fair simplification of the kind of parsing example I was demonstrating. Code is up at: https://gist.github.com/snoyberg/8943391. The idea is that we have two constructors for the `A` type: `A` and `AEnd`. We want to parse the stream of `A`s into a stream of `B`s by counting the number of `A`s until an `AEnd`. If the stream has an `A` as the last element, it's an invalid sequence (similar to ending a UTF8 sequence with a continuation byte). The example here is to peek at the first `B`, take three `A` values, take two `B` values, and then get the rest of the `A` values. The stream ends with an `A` constructor, which is invalid for conversion to a `B`. But we never demand the end of the stream after conversion to `B`, so it should be accepted. The conduit version follows that behavior: it only converts as much of the `A` stream as necessary, and therefore the `IncompleteSequence` exception is never thrown. Also, `btoas` is only called once. The pipes version, however, calls `btoas` twice and then throws an exception when it encounters the end of the stream. (Fair warning: I'm *not* a pipes expert, there's probably a better way to structure that code.)
I haven't tried adding a type class system yet.
tldr; an idea for pipes-parse -- use conduit.