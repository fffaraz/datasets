&lt;3 `streaming`.
Wire is hiring a Senior Backend Operations Dev in Berlin. https://wire.softgarden.io/job/1166533?l=en 
 #2 404s.
Let me be somewhat contrary and say that it's by no means clear that dependent types will make it into Haskell and if they do it's by no means clear that they will be worth it.
&gt; It also doesn't have qualified imports Idris doesn't have qualified imports?! How do Idris users survive?
You mean those are things that *Haskell* has?
Yeah we need `-fdefer-syntax-errors` or similar! The first suggestion of this that I have found was by Michael Sloan: https://github.com/haskell/haskell-mode/issues/294#issuecomment-74753524
https://dl2.pushbulletusercontent.com/YdMeXL7PSU59aKzXrJBSzfRYac2F36Eq/quantitative-type-theory.pdf
I added the file writing on-purpose to make sure actual work gets done and there is absolutely no chance that laziness/optimisation gets in the way of the benchmark. The file writing part is a constant overhead in all the benchmarks, so shouldn't be much of a problem. Let me fix the `IsString` stuff.
&gt; The file writing part is a constant overhead in all the benchmarks, so shouldn't be much of a problem. I would not be willing to assume that writing to a file has deterministic performance. Seems like the sort of thing that can take more or less time fairly randomly. `deepseq` ought to be all you need
It is great but sadly types like unpack :: Monad m =&gt; ByteString m r -&gt; Stream (Of Word8) m r are far more general than necessary.
And now that I look at it, I'm puzzled why data ByteString m r = Empty r | Chunk {-#UNPACK #-} !S.ByteString (ByteString m r ) | Go (m (ByteString m r )) rather than type Bytestring = Stream (Of S.Bytestring) NB data Stream f m r = Step !(f (Stream f m r)) | Effect (m (Stream f m r)) | Return r and data Of a b = !a :&gt; b Why the need to use `fromChunks` and `toChunks` to implement this trivial isomorphism? EDIT: Mentioned the wrong type definition.
Unable to use `deepseq` because `NFData` instance for `Html` is not defined.
Have fixed the `IsString` stuff and while the string interpolation is now faster, it is still 2x slower than blaze!
`NFData` has an automatic instance for `Generic`, can't you just declare it without body or use `StandaloneDeriving` and `deriving instance NFData Html`, the same for `Generic`?
The 'Chunk' constructor of the streaming 'Bytestring' has its byte contents unpacked. 'Of' doesn't give you that. Less indirection for the sake of efficiency. Maybe that's the reason? 
True. Why couldn't `Of` be unpacked, though?
&gt; stress the PG query parser/optimiser) The Postgresql query planner gets stressed? How big are your queries? 
The hand-written query is actually not that large, just some left joins on 7-8 tables. The Opaleye generated query, on the other hand, is a monster! Related issue - https://github.com/tomjaguarpaw/haskell-opaleye/issues/284
I'm also a little bit puzzled by the idea that Haskell preprocessing can improve the Postgres query optimizer, which is highly engineered and tuned.
I think you would lose polymorphism, both in the elements produced and in the functor that parameterizes the stream. Being able to use different functors allows you to express grouping very easily (notice that streaming-bytestring uses Stream for grouping). It also lets you use Stream as a FreeT replacement in many cases. 
Interesting, 60ms to analyze is quite a while. 
For me the biggest issue with Haskell records is that they can only be nominally typed. This makes e.g. typing a database query much more cumbersome than it needs to be. Library solutions for structurally-typed extensible records typically roll type-level sets with type families and are syntactically noisy with limited type inference.
What about adding prepared statements as I ask here? https://github.com/tomjaguarpaw/haskell-opaleye/issues/284#issuecomment-318633278
Since you keep mentioning that, I’m now convinced that I need to give it a shot ;) (currently I’m a pipes user)
I haven't looked at the code but isn't Blaze supposed to be super fast ? I mean, it uses a builder which makes concatenation fast. How do you append hard-code string ? 
How is `unpack`'s type signature too general? Do you have a more monomorphic version in mind, or did you mean that you don't like that it has the return parameter `r`?
Are they persistent? 
You can just encode the Html to a lazy bytestring, and then force the length of the bytestring. This ends up forcing the bytestring to materialize.
&gt; Do you have a more monomorphic version in mind, or did you mean that you don't like that it has the return parameter r? Neither of those. In the pipes world it would be unpack :: Monad m =&gt; Pipe ByteString Word8 m r Every inhabitant of that type naturally gives rise to a Monad m =&gt; ByteString m r -&gt; Stream (Of Word8) m r but you can't go the other way. Therefore unpack :: Monad m =&gt; ByteString m r -&gt; Stream (Of Word8) m r is too "loose" a type. (I said too "general" but perhaps "general" was not precise enough) 
Ah, right. You can't unpack something polymorphic? That's a bit of a shame. It feels like you should be able to unpack stuff that's `Storable`, or something. EDIT: Now that I've said that, I don't see how you can unpack a ByteString, since it is not a fixed size. EDIT2: Ah, but a ByteString is a fixed length, because it's a payload, offset, and length, as this shows https://hackage.haskell.org/package/bytestring-0.10.8.2/docs/src/Data.ByteString.Internal.html#ByteString
It's a fine idea.
Slightly off topic, but I love what you are doing with respect to benchmarks and usability. Some people drive by with some random criticisms, and you've stuck around and have been digging in with some committed good-faith engagement - it's excellent. And you're very sharp as well. I'm equally excited and scared of the day you start to poke around in areas I'm familiar with :) 
Are you doing repeated left-associated appends on your strings?
Thanks for sharing your experience! &gt; I made it through LYAH, but was left with the feeling of 'well now what? I still don't know what to do'. I partially read LYAH online already, so now probably there's no point in buying a physical copy (except to support the author, which I'd better do with a donation). I got the same feeling as you on this point! It's meant to be just a "quick" and fun overview of the language overall, but it really misses some practical/challenging stuff.. 
&gt;I started with Learn You A Haskell (LYAH) and thought it was one of the best tech books I've read. I stopped at chapter 13 mainly because I heard Haskell Programming from First Principles (HPFFP) was a better way to get started. I already read most of LYAH online, was a great introduction but I now want to move on to something more serious &gt;As far as format, I prefer reading LYAH online versus HPFFP's PDF format. LYAH is also nice as a reference. I guess it can't hurt to read both :) Yeah I'm actually looking to spend some of my company's educational budget on a physical copy, it's a better way to commit myself on actually learning it :D 
&gt; I strongly recommend going with Haskell Programming from first principles. It's very thorough and lots of exercises help you really understand how to write and read Haskell Seems that everybody agrees on that, I think I'm sold at this point! :D
Thanks. Now I understand what you mean, and I can see why that bothers you. Although, what I like about `streaming` is that the types are more "loose". Things like [group](http://hackage.haskell.org/package/streaming-0.1.4.5/docs/Streaming-Prelude.html#v:group) are much more elegant with `streaming` than with `pipes`. I guess it just depends on what you find yourself needing more often.
One issue is that explicit splits will be desired for a huge amount of types. Such a suggestion would make such a thing harder. Implicit splits won't be desired much.
One of the lessons of `streaming` seems to be that bidirectional pipes are rarely needed. On the other hand, we don't need to throw the baby out with the bathwater. Yes group :: (Monad m, Eq a) =&gt; Stream (Of a) m r -&gt; Stream (Stream (Of a) m) m r is very nice. But group :: (Monad m, Eq a) =&gt; Stream (a :-&gt; Stream (Of a) m) m r would also be very nice, where data (a :-&gt; b) r = Await (a -&gt; r) | Yield b r and we can even have some helper function like stream :: Stream (a :-&gt; b) m r -&gt; Stream (Of a) m r -&gt; Stream (Of b) m r to make the medicine go down more easily. Now `Streaming.group = stream TomejaguarStreaming.group`, and very little of value was lost! EDIT: I corrected the definition of `a :-&gt; b`. 
Largely depends on whether he is familiar with benefits of immutability. If he is, then next thing that would be helpful is type system. If not, then somehow combine those 2. How does he feel about code you write? Since I became familiar with functional style, I tend to use it more and more in code that I write in procedural/OOP languages. Honestly, I would not go too deep in everything Haskell offers, main benefits are purity and immutability, and Haskell just provides a convenient system to code in that style. Show him data constructors, pattern matching, explain how types limit stuff you need to think about when working on a function. Show him how easy it is to manipulate data (map, filter, perhaps even fold, fold is not so intuitive for people, they need some time to process it and see similarity to foreach). My dayjob is PHP/Magento and if I wanted to sell Haskell to anyone at company who understand how Magento works my main focus would be limiting size and complexity of application state and providing you guarantees that some other random bit of code will not clash with your bit of code. If you feel that Haskell will be impossible to sell, try something like Hack, that will probably go significantly easier. I do understand benefits of Haskell, but I also understand benefits of going wild with tech stack. What happens if you leave and there is nobody to provide guidance?
Well the advantage of type safety is its internal consistency, and the less monolithic your system is, the less that guarantee matters. If you're trying to interact with a bunch of PHP microservices, your Haskell experience is likely going to consist largely of debugging foreign API calls, not of the fabled "if it compiles, it probably works!" Not that I'd recommend against it, just make sure to have realistic expectations about what Haskell can and cannot help you with.
That's pretty cool. I'm going to have to think on it a little more to make sure I actually grok what's going on.
Most of the situations in which I use qualified imports would probably be solved by type-directed overloading, so that probably helps.
The problem ist not unpacking the `ByteString`, it is unpacking `f (Stream f m r)` in the `Step` constructor. You can't unpack some thing of which the shape you don't know. And since GHC doesn't specialise data types, we can't even rely on `Step ("" :&gt; undefined) :: Stream (Of S.ByteString) m r` (hope I got that right) to be unpacked. Fun fact: GHC has `-funbox-small-fields` on by default, so the `UNPACK` pragma isn't even needed in the `Chunk` constructor. 
I like [`structured-haskell-mode`](https://github.com/chrisdone/structured-haskell-mode) quite a bit for this.
Ah, I didn't know there was a physical book available. You may also want to keep an eye out on https://intermediatehaskell.com and https://joyofhaskell.com although I'm not sure when either will be released.
&gt; The problem ist not unpacking the ByteString, it is unpacking f (Stream f m r) in the Step constructor Surely it's both? 
By unpacking the bytestring, I mean the constructor exposing the primitive pointer. There are at least 2 levels of indirection involved, prior to optimisation, IIRC. Edit: Ah, you posted the relevant constructor and it also contains slicing information, my bad. And this is not considered a small field by GHC, so no unpacking just by giving it a `!`.
Oho, thank you mrkkrp, and for your excellent work with this package.
I did this awhile back: https://engineering.imvu.com/2014/03/24/what-its-like-to-use-haskell/ Nobody really talks about Haskell from a maintenance perspective, but it's totally in a class of its own. Haskell programs flex and grow as they age like absolutely nothing else out there. You can throw 20 people at a project for 2 years and *still* retain the ability to make the changes you want quickly and safely. Even if some of those 20 people don't know Haskell very well, or if they tend to be the sort of person who likes to get the feature out fast and leave the mess for others to clean up.
It would be free in the common case where the tail can be fused away. And even where it can't, it is just an ephemeral bump-allocation of 4 words or so; hardly expensive and much much cheaper than carrying around an extra 3 words for every list element in memory forever.
In the best of all possible worlds, somebody who already is intimately familiar with the RTS would just implement this and give me all the credit. ;) In our fallen world, I've just started reading through the source to get an idea how hard it will be and whether some parts could usefully be split off to volunteers. But thanks for heeding the call!
I don't think so. Here's what the quasi-quoter compiles down to: https://raw.githubusercontent.com/vacationlabs/monad-transformer-benchmark/d32511c9348afd648028c3302c5debd0d2d255ed/ConstantStringMarkup.dump-splices
Fair enough. Maybe it's just a String vs ByteString thing.
Without a great deal of additional and less efficient code, vectors--boxed or unboxed--are not a viable alternative for many of my codes which largely deal with conceptually infinite lists; no structural laziness, no infinite vectors. Sometimes data laziness, too, comes in handy. Finally, a vector most of which is no longer needed nonetheless continues to use its entire memory. Finally, let me stress that absolutely nothing in either proposal changes the semantics of lists or requires the change of a single line of Haskell code. Perhaps you thought otherwise.
I think you'll have a hard time convincing your lead to switch the codebase to another language, especially one which the team doesn't already know well. Even if you convinced him that Haskell is great, you would have to train your team on the language, tooling and practical industry approaches like which frameworks and libraries to use and avoid, how to deploy Haskell, etc. This is exactly the kind of thing [we do at FP Complete](https://www.fpcomplete.com/training); we provide training and advice on developing, deploying, optimizing and hiring for or migrating to Haskell, including dev work full time or part time. Companies get in touch with us because they don't feel comfortable taking the risk of using Haskell (and there is some risk compared to popular languages) even if they were sold on using it already before contacting us. So if you're trying to convince your team to use Haskell, I'd say that you *at least* need a consulting company like FP Complete to provide the assurance of succeeding. Also, even the existence of companies like ours gives you a fallback. **EDIT**: I have written a couple blog posts that argue the technical side that you can pass to a manager e.g. [Mastering Time to Market](https://www.fpcomplete.com/blog/2016/11/mastering-time-to-market-haskell) and [Maintenance matters](https://www.fpcomplete.com/blog/2016/12/software-project-maintenance-is-where-haskell-shines), though I think that the OP has already got his food in the door on the technical side. The real hurdle is making a compelling case for the business decision.
Perhaps something that might be interesting to him is that Haskell and Ocaml have a good industrial user base in the financial sector, whereas PHP is mostly known for brittle code, wat moments, and security vulnerabilities (yes, PHP users will tell you it is all modern and wonderful now, [no true Scotsman](https://en.wikipedia.org/wiki/No_true_Scotsman)-like). Another strategy would be to just rewrite (part of) that complex logic in Haskell and show how it is readable/easy to refactor/almost like magic. In the few use cases where I tried that, I found that good data types and pattern-matching made some tangled if/then/else trees immediately readable. Finally, you can just start rewriting the application and not tell anyone about it. It works better if you are on the ops side :p
**No true Scotsman** No true Scotsman is a kind of informal fallacy in which one attempts to protect a universal generalization from counterexamples by changing the definition in an ad hoc fashion to exclude the counterexample. Rather than denying the counterexample or rejecting the original claim, this fallacy modifies the subject of the assertion to exclude the specific case or others like it by rhetoric, without reference to any specific objective rule ("no true Scotsman would do such a thing"; i.e., those who perform that action are not part of our group and thus criticism of that action is not criticism of the group). *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
Good thing you weren't around when they first proposed fusion optimizations. "One tiny change without any semantic effect and suddenly my program needs three orders of magnitude more memory and runs at a third of the speed? And if I want to find out why, I need to read something called 'Core'?" What a ridiculous idea! :P
... Yes. On re-reading, I can see how that's not clear.
Where is the linearity?
But blaze also has `renderHtml :: Html -&gt; String` which I'm using.
Are purescripts extensible effects a concrete monad (transformer) or an mtl style typeclass? I expected it to be the (more elegant) equivalent of foo :: (MonadEffect b '[EffConsole, EffBytestringFile] m) =&gt; m () And then have the compiler/a type family translate this into a bunch of constraints like `(MonadBase b m, MonadConsole b, MonadBytestringFile b)` so that the base monad can be swapped for testing while avoiding exponential instances for the split typeclasses. Your description almost makes it sound like a concrete base monad, though.
If persistent refers to the pipe-line of transducers, then yes, there are no mutations involved in the process. You could (in theory at least, as I cannot find good use of it right now) use that as a feature and replay a transformation, from a given state of the pipe-line, with different inputs. As far as I know, most Clojure's transducers use volatile! reserved word (which maps to Java's volatile) and use mutations instead inside the pipe-line of transformation to track state.
I don't quite grasp how your definition of `group` works. Could you explain it a little more? What advantages does it have over the original formulation?
It looks like you lack acceptance test, so you can be confident in your code modifications. You could write acceptance tests in Haskell, so that people get used to it, without having the fear to send Haskell code in production. After that it should be easier to show how easy it is to test Haskell code at the "unit testing level", show Quickcheck too ! It should give you an opportunity to show the strong points of Haskell, on tangible cases.
You could move from PHP to Hack, which isn't a giant stretch to Haskell. Small steps. Also, proof of concept. Code wins arguments in the end.
`Stream (a :-&gt; b) m r` would be isomorphic to `Pipe a b m r` from `pipes`, that is, it reads `a`s and emits `b`s. The advantage it has is that it is a "tighter" type (I'm not quite sure of the best terminology).
&gt; Are purescripts extensible effects a concrete monad (transformer) or an mtl style typeclass? Neither. It's literally just a labeled form of `IO`
Do you have a log of recent bugs? You could simply go through them and show how Haskell would have prevented each one.
And to flesh that out a bit group :: (Monad m, Eq a) =&gt; Stream (a :-&gt; Stream (Of a)) m r reads `a`s and emits `Stream (Of a)`s, that is, it emits the groups. EDIT: I think this may be nonsense. 
Haskell got me as a user. Writing a BOSH Kubernetes CPI in Haskell. Don't regret it yet. https://github.com/SAP/bosh-kubernetes-cpi-release/ BOSH: release engineering and lifecycle management for distributed systems. See bosh.io for details. CPI: cloud provider interface to allow managing distributed systems on multiple clouds Kubernetes: container scheduler. See kubernetes.io for details. 
For grouping, in my view it has the disadvantage that it uses an extra concept, `:-&gt;`. Also, it seems difficult to stack (admittedly, an infrequent case). **Edit:** about the corrected definition of a :-&gt; b. streaming has two functions [separate](http://hackage.haskell.org/package/streaming-0.1.4.5/docs/Streaming.html#v:separate) and [unseparate](http://hackage.haskell.org/package/streaming-0.1.4.5/docs/Streaming.html#v:unseparate) that transform layers of Stream transformers. I think something like `a :-&gt; b` can be obtained by "unseparating" a layer of `Of` and a layer of `(-&gt;) inputtype`. I use that trick in my [streaming-eversion](http://hackage.haskell.org/package/streaming-eversion) package. 
Make sure you check my correction of the definition `a :-&gt; b` before you think too hard! 
paging /u/ephrion
Actually this is very wise. We have a lot of type errors indeed. "cannot call getId from null|boolean|integer". I have a big log of them. Plus all those phantom errors like corrupted fields in the database, because the php accidentally added a string to an int and didn't alert at all. Those are hard to fix in PHP.
&gt; it seems difficult to stack What do you mean by "stack"?
That's basically my job right now! I'd be happy to share my experience with him; PM me if you want to get contact info etc. Here are the concrete benefits we have moving from PHP to Haskell: 1: Performance. The Haskell code that replaces PHP code is typically between 10 and 400 times faster, and costs that much less. The Haskell code can easily take advantage of concurrency and parallelism, while the PHP can't. The Haskell runtime is also vastly superior to the PHP runtime. The libraries in Haskell for batching and streaming are extremely good, too. 2: Simplicity. The PHP code is littered with checks against weird exceptions, strange bugs, and unreliable code/RTS. As a result, the PHP code needed to be calling to Redis to store job progress and a ton of other state. The programs are extremely fragile and literally untestable. The Haskell code, being far more reliable, can omit nearly all of this testing, state, and checking, and is typically vastly simpler and more clear. As a result, bugs are super easy to diagnose and detect. 3: Developer speed. There has been one or two Haskell developers on the Haskell projects at my job. We consistently get things out the door faster and with fewer bugs than the PHP team when they initially implemented this stuff. We don't have to write nearly as many tests, and thanks to QuickCheck, the tests we do write mean much more. It has almost consistently been faster to do a clean rewrite in Haskell than a moderate refactor of the original PHP code. 4: Correctness I somewhat covered this in the previous few points. It's really hard to communicate how important this is. PHP, JavaScript, Java, etc. are all constantly trying to undermine you. They're trying to be clever, but they're really just like a wet fish that just slips out of your hand the moment you stop caring. Haskell, in comparison, is just a reliable tool. Sure, it has some quirks, but the language and ecosystem almost never undermine you with stupid choices that slow you down and cause mysterious bugs. That isn't to say that bugs never happen -- they do! It's just *really* easy to track them down and identify them, even with limited context or information. There's typically one place in a program where anything can go wrong at all, and tracing that through is typically very easy. The type system and immutability helps a ton here. Making it rain `newtype`s has been tremendously useful for ensuring i don't mix up two `Text` values, and since problems tend to happen early and are very close to their source, I don't ever need to debug weird issues where a problem has bubbled thousands of lines away from it's source. --- It would be dishonest to not also mention the negatives! 1: Hiring No, not like you think! Our company hasn't decided to switch to Haskell 100%, so we still need to hire PHP developers. When we advertise for Haskell developers, we get a ton of highly qualified candidates that just don't want to write any PHP ever. When we advertise for PHP developers, we get... almost nothing. So it's been hard to find Haskellers that are willing to write PHP and JavaScript. (Incidentally, we *are* hiring, if anyone wants to write PHP/JS for now and possibly write Haskell in the future when it's sufficiently eaten the company). We hire remotely, so we have access to the entire USA of Haskell talent. If you're not hiring remotely, then you're going to be very limited -- Haskell's biggest scenes are SF and NYC, and if you're not there, you're going to have a harder time finding people that know the language. 2: The bar is kinda high As a Haskell developer, I've had to dig a lot deeper into the guts of libraries than I've had to in Ruby or JS (though, *not* more deeply than in PHP! I've had to dive into the Laravel library code quite a bit). The sheer size and focus on TDD in Ruby means that OSS is often more of a "please fix" and itll actually get fixed. The smaller size of the Haskell community means that most libraries are developed with a single purpose in mind. Some libraries partially support other uses cases. The database library Esqueleto purports to work with PostgreSQL, MySQL, and SQLite, but I've run into quite a few bugs with MySQL -- I suspect we might be some of the first real intensive users of Esqueleto for MySQL! So, you need to be comfortable reading library code, and also submitting fixes for it. This requires a decent level of expertise from at least one person on your team. Companies like Well Typed and FPComplete help out with this a lot, and it's getting better every day as more-and-more people are using Haskell in production. 3: The library availability is not first-class Haskell just isn't a language most API vendors consider to write bindings for, so you'll either have to rely on community bindings or write your own. We've had to write bindings to push EKG metrics to Amazon Cloudwatch and Bugsnag integration. The lack of an off-the-shelf OAuth2 server caused us to end up picking Laravel instead of Haskell for a project (something I sorely regret). I will say: for the last two points, PHP hasn't been noticeably better. One thing that's great about Haskell is that, if a library exists, it tends to be pretty high quality (at least, for the use cases it exercises: database bindings strongly prefer postgres). There are a lot of garbage PHP libraries that are impossible to figure out, just don't work, or are literally the dumbest possible way to do something (gonna call out [eloquence](https://github.com/jarektkaczyk/eloquence) here -- the query generated for full text search is possibly the dumbest thing that would conceivably work). There's a *very* strong and reasonable temptation to discard libraries in favor of writing your own tools. --- I do think that the positives *more* than outweigh the negatives, especially if you're coming from PHP. Honestly, PHP to Ruby is a significant upgrade. PHP to Java would be huge, and PHP to Haskell is just a ridiculously huge jump in quality.
Haskell does have a huge learning curve, and it's not likely that a weekend is enough to sell him on the concepts. If he's *interested*, ask to implement a prototype of some core logic that needs to be correct. Hook it up to a Yesod server so it's available as a web app, and then your existing PHP code can talk to the web server to get the Right Answers it needs. Eventually, more and more logic can be moved into the Haskell as it becomes expedient.
&gt; streaming has two functions separate and unseparate That's *really* interesting! I'll have to look further into that.
I'm glad I'm not the only one who found LYAH to be a solid tour, but unhelpful for truly learning the language. I'll check out *Haskell Programming from First Principles*.
my dude i'm so on top of this
I meant having more than one level of grouping, something like `Stream (Of Text) m r -&gt; Stream (Stream (Stream (Of Text) m) m) r`. For example, dividing a stream of text into lines and paragraphs (without having to keep whole lines in memory at any point).
I'd highly recommend Programming in Haskell by Graham Hutton. I was lucky enough to be taught Haskell by him at university and he is an excellent teacher, and this really comes through in the book. Everything is explained clearly and builds progressively on previous topics.
Hi, the theme is not set yet, This was just my "first idea"! Anyway, looking at the last two years, not a single thesis about improving a already existing library. I do understand that this would be a lot more valuable to the community, I'll have to talk with the professor and see if it is acceptable to improve an existing library. This is probably related to our local culture (Not saying that this is good). I got interested in the type-checking aspect of HaSQL, is anyone already working on this? If I choose this as my theme, I'll probably need a Haskell "mentor", is there anyone that could eventualy help me? Also, this is a long term commitment, our final project is divided in two segments, the theory and report+practice, this semester I'll be working more into the theory aspect, doing very little on the implementation side.
Last weekend I did a small project with `Megaparsec`, parsing a really simple SQL Grammar (minimum required for an ODBC driver) If I was to make compile time checks, based on the DDL... I have no idea how to do this in Haskell.. I'll share how I think this work: - I the DDL section will generate a ADT (at compile time?) - This ADT will be used as return from select statements for instance - Said select statement might have a where clause, doing something like: `name = 1`, wich is syntactically correct, but looking at the ADT it is clear that the types are incompatible. How can I can I check this?
the primitive "forward" in pi calculus was introduced for the proof term for the identity axiom for the sequent calculus linear logic. On its own, it has no linearity, and would be unrelated but for its primary obvious use being implementing a HOAS typed embeding of that logic.
We have a new stack release. New platform is pending on a new cabal-install coming real soon now as well.
Fixation on `zip`, I guess?
That should probably be fixed upstream.
I've never tried it, but there's also [deepseq-magic](https://hackage.haskell.org/package/deepseq-magic) which doesn't need `NFData`.
The last time I tried using it, it wasn't the greatest and it definitely doesn't work well at all with evil mode which is another big downside for me.
To be clear, you can use the qualified name to disambiguate, but you can't not dump everything from the module into your namespace. So you can't control if you'll get ambiguous type errors, which can handicap the usual type-directed workflow, but you can disambiguate by passing explicit types or using module qualification: http://docs.idris-lang.org/en/latest/reference/syntax-guide.html#syntax-imports Maybe the users survive by not being very many, and not yet writing very large programs... but C has even less namespace control and people write even larger programs, so much for that theory.
Eh, maybe. I find that the type system helps push errors to the boundaries, and actually assists in documenting and detecting breaking changes early.
Break things into phases. Your initial pass over the SQL will simply parse it into a data structure representing the query. You'll want another pass over it to do semantic analysis: do these tables exist? do these fields exist, and if so, are they unambiguous? For this, you'll need the DDL compile-time parser to emit something that the SQL type checker can use to verify these sorts of things. The simplest thing it could emit would be a top level value: tables :: Map Text (Map Text SqlType) which would allow you to lookup whether or not `"users"` exists and whether or not it has a field `name` with a SqlType that is compatible with `1`.
Correct. See: https://github.com/fpco/stackage/issues/2449
You suppose correctly. The number manually dropped from nightly is looking somewhere around 300-500. The number transitively dropped will be more... not sure exactly how much more. 1000 might be a good guess. We'll see. See: https://github.com/fpco/stackage/pull/2669
Next time around, I would add hiring entry level as an additional column. Also, I would add major Haskell libs the companies are contributing to. Besides IOHK, for most other companies, the libraries they are making large contributions to are more meaningful and impressive than the libraries they have built locally from scratch (not a comment on quality, just how much time people have in a day).
&gt; available time to train after work, No, honestly, the company needs to sponsor this training as part of the employee's work hours. If they rush ahead on their own time, cool, but this is an investment the company is making that will pay huge dividends in terms of productivity and efficiency. Employees shouldn't sacrifice their own time for the benefit of the company.
Linear and uniqueness type systems are precisely dual to each other (linear = ⊤, uniqueness = ⊥ in a lattice whose points are the cartesian product of {no weakening, weakening, I have not yet used weakening} and {no contraction, contraction, I have not yet used contraction}), so I'd be surprised if this wasn't the case. IIRC Ed's argument was about ergonomics: it's easy to "forget" uniqueness; linearity ties your hands as it is a requirement, not a guarantee.
If `split` is available, I imagine it would end up being implemented for any type where it's possible. This is generally the way with class instances.
I understand what you are saying, but idk how this is achieved. What would be the keywords for this weekend's reading material? 
Indeed, sounds like a PR should be filed for that, given that deepseq -- in part due to being bundled with GHC &gt;= 7.8 -- is almost universal at this point.
How does your implementation of forwarding enforce linearity in the type system?
I think it's a good approach, but be prepared for the usual push-back: "Yeah, but those bugs could be caught by tests!" The answer to that is: "Yes, but we didn't write those tests, did we?" (evidence: these bugs are in your bug tracker). With Haskell you don't need these absolutely pointless and trivial tests. For managerial types it usually helps to contrast "how much trivial testing do we need to do" vs. "how much obviously bad code should have made it past the compiler". Both require some kind of *budget*. I can also personally attest to another poster's point that Haskell code is *absurdly* maintainable. It's really unbelievable until you've tried it. EDIT: You'll hardly notice it. It's just a bit of bad grammer or syntex.
It's a compilers exercise, essentially. Unstructured text -&gt; Either ParseError SqlSyntax SqlSyntax -&gt; SqlEnvironment -&gt; Either TypeError SqlExpression `SqlEnvironment` is going to be something that contains all the known tables, their fields, and their types.
please please preserve the serif branch! personally, it is much harder for me to read in sans-serif. 
It does not. The intended use case is for a package I'm working on (slowly) embedding a linear type system in Haskell.
The only existent descriptions of this concurrency primitive outside of this package (that I know of) are in descriptions of linear logic. That is the only relation. 
1) Better Yesod templates and ORM. This is the 80% use case in today's developer market. The "Shakesperian" templates and Persistent were a good first stab, but we need better. Haskell needs it's "Rails". 2) Compiling to SIMD. 3) Succinct (Combinatorial Species) data structures.
I mean yeah that is probably what will happen, and it makes sense. But the existence of that class should absolutely not lead to it ever being used implicitly. I can't see a good way to allow for that implicit stuff. Maybe have both an `ImplicitSplittable` subclass of `Splittable` AND an extension that must be turned on in order for implicit stuff to happen?
&gt; We don't have to write nearly as many tests In your Haskell codebases, what is roughly your ratio of "test lines of code" vs. "non-test lines of code"? In my experience, I'm finding I'm still writing a lot of tests in Haskell. Sure, I no longer have to do checks against NULL and such as in other languages because of type safety, but I still have to write a lot of tests.
According to my production codebase I just checked, our Haskell test:non-test ratio is about 5:16.
My work codebase has roughly 1:12 test:production code.
Second the maintainable part of Haskell code. Yesod app that I love working with.
Again just wanted to second this ... love maintaining my Yesod app!
I had hack in mind reading about this, then when hack's value is technical and business value is demonstrated I think you have a better case for Haskell. If not, at least you reduced the php code in this world for something sane!
&gt; Hack I don't know how it compares to Hack but I've used Haxe and it was quite pleasant. I however gave because interfacing with PHP code was doable but not straightforward.
Thank you for the suggestion, but no, no way! Plus they are about to release their Reflex language for the HHVM.
&gt; Dependent types can be pretty bad for this, since libraries can end up with many datatypes which are conceptually similar, identical at runtime, but with slightly different proof obligations. Doesn't HoTT offer a way to deal with this? (Basically, by automatically "transporting" code and proof obligations across type isomorphisms, like a less ad-hoc version of type casts.)
I get a normal error message. What exactly do you mean by "a library", and how are you invoking GHC? I tried both `ghc --make Main.hs` as well as creating a cabal project with an executable containing Main and a library containing Testing. Both produce the error below and no panic: • Couldn't match a lifted type with an unlifted type When matching the kind of ‘(# Bool | Bool #)’ • In the second argument of ‘($)’, namely ‘func 1’ In the expression: print $ func 1 In an equation for ‘main’: main = print $ func 1 | 8 | main = print $ func 1 | ^^^^^^ 
Can you use this library instead? http://hackage.haskell.org/package/judy-0.3.0
One way to fix this is to write to tmpfs. Or to use something else (send to socket, send to stdout)
 `show` and `print` are not *levity polymorphic* but they [can](https://github.com/ghc-proposals/ghc-proposals/pull/30) be cheered up class Sh (a :: TYPE rep) where sh :: a -&gt; String pr :: a -&gt; IO () where the default instance for `Type`s uses the default show, directed by the `LiftedRep` representation that `TYPE` is indexed by type Type = TYPE LiftedRep instance Show a =&gt; Sh (a :: Type) where sh :: a -&gt; String sh = show pr :: a -&gt; IO () pr = print and for unboxed sums instance (Show a, Show b) =&gt; Sh (# a | b #) where sh :: (# a | b #) -&gt; String sh (# a | #) = show a sh (# | b #) = show b pr :: (# a | b #) -&gt; IO () pr pair = print (show pair) And you can define `main = pr ((# True | #) :: (# Bool | Bool #))`.
should that be `pr pair = print (sh pair)`? 
Actually I meant to write `pr pair = putStrLn (show pair)` The `Sh (# a | b #)` instance only works for lifted types `a :: Type` &amp; `b :: Type`. The following code is fine, in an attempt to make it levity polymorphic instance forall (a :: TYPE rep1) (b :: TYPE rep2). (Sh a, Sh b) =&gt; Sh (# a | b #) where sh = undefined pr = undefined but function arguments cannot be levity-polymorphic, so adding arguments will fail instance forall (a :: TYPE rep1) (b :: TYPE rep2). (Sh a, Sh b) =&gt; Sh (# a | b #) where -- A levity-polymorphic type is not allowed here: -- Type: (# a | b #) -- Kind: TYPE ('SumRep '[rep1, rep2]) -- In a wildcard pattern sh _ = undefined pr _ = undefined To the best of my knowledge if you want to print `(# Int#, Double# #)` you must duplicate it for each representation (here I use a preprocessor) #define SH(rep1,rep2) \ instance forall \ (a :: TYPE rep1) (b :: TYPE rep2). \ (Sh a, Sh b) =&gt; Sh (# a | b #) where { \ sh (# a | #) = "(# "++sh a++ " | #)"; \ sh (# | b #) = "(# | " ++sh b++" #)"; \ pr str = putStrLn (sh str) } SH(LiftedRep, LiftedRep) SH(LiftedRep, IntRep) SH(LiftedRep, DoubleRep) SH(IntRep, LiftedRep) SH(IntRep, IntRep) SH(IntRep, DoubleRep) SH(DoubleRep, LiftedRep) -- ...
I thought nippy means a little cold?
Both!
Am I missing something major about compiler design here? It seems like GHC does an extremely amazing job of modularizing language features. What, sans extensions, is seen as a major hurdle to industry that is in conflict with research efforts? Is there some camp of GHC maintainers that shoot down pull requests that would help industry concerns, or is there something about the concrens of one group that make it difficult to provide for the concerns of the other? From a language stability perspective, it doesn't seem like Haskell is changing at such a break neck speed that code I write today sans some of the fancier extensions won't work 2 years from now.
I don't understand the obsession with monads. IMHO, we should treat it like a particular abstraction among infinitely many.
Yeah, I disagree. Lens is great and all, but it's like killing a cockroach with a shotgun a lot of the time. The end experience of dealing with deeply nested data is nowhere near as straight forward as dealing with the same sorts of structures in Perl, JS, or Python. Figuring out how to compose lenses when dealing with intermediary traversable components in a tree of records is not a pleasant experience for a novice haskeller. I feel like the trade-off in other areas is still a net gain compared to those languages, but it took some pretty serious dedication and effort to get to that point, and I have no doubt the community loses a non-trivial number of developers at that stage due to frustration.
unless it's opaque, you can just write an orphan instance, right? even if it is, you might be able to drive it generically if they've instantiated Generic or Data. 
i work for tvision insights; we are hiring. our GitHub is [here](https://github.com/tvision-insights), our sector is television/advertising.
confer health (also in boston, ma) uses haskell. their GitHub is [here](https://github.com/ConferHealth). their sector is healthcare; i don't think they are hiring.
Thanks for the bug report! Note that in order to reproduce the issues you were experiencing, I had to compile each of the files with `-O2 -c`: $ ghc -O2 -c Testing.hs $ ghc -O2 -c Bug.hs ghc: panic! (the 'impossible' happened) (GHC version 8.2.1 for x86_64-unknown-linux): getUnboxedSumName 513 Call stack: CallStack (from HasCallStack): prettyCurrentCallStack, called at compiler/utils/Outputable.hs:1133:58 in ghc:Outputable callStackDoc, called at compiler/utils/Outputable.hs:1137:37 in ghc:Outputable pprPanic, called at compiler/prelude/KnownUniques.hs:104:5 in ghc:KnownUniques I've filed this issue as [GHC Trac #14051](https://ghc.haskell.org/trac/ghc/ticket/14051).
I don't understand why throwing away useful constraints would be a good thing.
That there is very heavy magic.
Not as haskell dev (unfortunately) but refactoring is MUCH easier using a statically types language, especially haskell
&gt; The end experience of dealing with deeply nested data is nowhere near as straight forward as dealing with the same sorts of structures in Perl, JS, or Python. How so? I mean all Python has for dealing with nested structures is `.`, how could that possibly be better than the equivalent in Haskell / lens. Python can't even deal with intermediary traverseables AT ALL, besides with manual for loops and such which can trivially be replicated in Haskell. So having it be not completely trivial to do it in Haskell is still a huge win. All Python can handle well is a series of nested lenses, it cannot deal with folds or traverses at all, and dealing with a series of nested lenses in Haskell is very easy.
Care to expand? You've very much piqued my curiosity.
Did the try CallHaskellFromAnything?
Thanks!
Digital Asset also has offices in NYC, London, and Budapest. Most of the management is actually in New York, with enough folks scattered elsewhere that it isn't just a hub and spokes.
It's nice how in python (or JS or ocaml anything else really) you can write x.x.x.x without having to import all of those x's into scope and coming up with an overloading scheme to disambiguate them. Or hand-write lenses or invoke a heavyweight macro system (I guess there's a Data.Generics way, but maybe it's slow?). My impression is the DuplicateRecordFields might help once complete, but that we'll still have to to add every field to every import list, and still have to hassle with shadowed or ambiguous variable names when they collide with a field name. I've been experimenting with conventions like trailing _ or leading ', but I think most people just use bare names.
I've got an idea along these lines: a free Monad/MonadPlus/MonadZip that compiles down to SQL queries. Then one could use `MonadComprehensions` and related extensions to implement something like this: `http://www.cs.ox.ac.uk/jeremy.gibbons/publications/ringads.pdf`
I think having a newer version of `containers` than 0.5.7.1 would have been nice...
&gt; We don't have to write nearly as many tests, and thanks to QuickCheck, the tests we do write mean much more. Could you please elaborate on this. What kind of app are you writing? What kind of tests are you writing in QC? I've been struggling to come up with a sensible development workflow involving QC in a webapp.
When I do that, I'm getting this error: jeff@jbb-lenovo:~/code/hask-stack-experim$ stack build No compiler found, expected exact version ghc-8.2.1 (x86_64) (based on resolver setting in /home/jeff/code/hask-stack-experim/stack.yaml). To install the correct GHC into /home/jeff/.stack/programs/x86_64-linux/, try running "stack setup" or use the "--install-ghc" flag. jeff@jbb-lenovo:~/code/hask-stack-experim$ My stack.yaml contains only the portion relevant to Linux: setup-info: ghc: linux64: 8.2.1: url: https://downloads.haskell.org/~ghc/8.2.1/ghc-8.2.1-x86_64-deb8-linux.tar.xz sha1: 3e52527de6f1cad6d7f1c392ddde8148116d4e36 content-length: 126809836 compiler-check: match-exact compiler: ghc-8.2.1 resolver: ghc-8.2.1 (I'm running Ubuntu 16.04. I had installed stack using apt-get; I uninstalled that (using "apt purge") and then reinstalled stack using wget.) 
Author here. In this article, I make a suggestion to use `-Wmissing-import-lists`, which gives a warning when using implicit import lists. For instance, take the following code: import Control.Monad.Trans.Reader myFunc :: Reader Int a -&gt; a myFunc x = runReader x 3 This will produce the following warning if you turn on `-Wmissing-import-lists`: example.hs:1:1: warning: [-Wmissing-import-lists] The module ‘Control.Monad.Trans.Reader’ does not have an explicit import list I'd be interested in hearing people's practices/thoughts/opinions on when to use explicit vs implicit imports lists.
Besides the readability benefit of explicit imports (either by import lists or via `qualified`), it it's one of the conditions under which the [PVP](http://pvp.haskell.org) contract works best. For more discussion of explicit imports, see also https://wiki.haskell.org/Import_modules_properly
Me pasas el link de los slides?
For those who want to make their code forward compatible in advance, I'd like to point out the `-Wcompat`/`-Wnoncanonical-monad-instances`/`-Wnoncanonical-monadfail-instances` warnings which do a little bit of static analysis to detect code that is likely to break or become suboptimal with future `base` releases. See also [GHC's migration guide recommendations for forward-compatibility](https://ghc.haskell.org/trac/ghc/wiki/Migration/8.0#Recommendationsforforward-compatibility).
&gt; but not so great if you want to get some work done. Am I the only one who gets a deja-vu from this about static and dynamic languages? "How am I supposed to get anything done if I have to prove to the compiler that my types are correct?"
Is there any particular reason for that? Sounds easy to solve.
Actually, I've been down a deep rabbit hole and I couldn't work out how to do this in the way I wanted! Perhaps the `streaming` way is the best way after all.
Can we get `-Wincomplete-uni-patterns` and `-Wincomplete-record-updates` into `-Wall` please?
If you run `stack build` it won't install the compiler for you automatically, so you need to tell it that you want to install the newer compiler. Try one of the two suggestions you got in the error: `stack setup` and then `stack build`, or `stack build --install-ghc`. If it still doesn't work, I'm not sure what's going on: [this](http://taylor.fausak.me/2017/05/17/testing-ghc-release-candidates-with-stack/) article might help.
In our company we use explicit import lists for everything except custom prelude. It really enhances readability of code. Though management of such import lists become tedious at some moment... I agree with your thoughts on IDE features. Good IDE can do such mechanical job as import management automatically. We are working on ways that improves experience of explicit import list management. But haven't announced them yet, they're extremely WIP. Unfortunately, nowadays we live in the world where no such good and popular IDE with Java-like support from Intellij IDEA for Haskell exist. I've worked with several big projects. And it's extremely hard to dive into big codebase and understand code when all imports implicit. You just don't know where functions come from. It slows your productivity a lot. And not only at the beginning of work but even during whole work. Because code is constantly refactored, things change.
&gt; EDIT: You'll hardly notice it. It's just a bit of bad grammer or syntex. Aha, yes, very good.
@graninas - anything that those of us who want to see this project come to fruition do? Did you check out Patron / BTC possibilities? There seems to be a ton of interest in this.
Maybe, but the amount of work it takes to write a type annotation is insignificant compared the amount of work it takes to write a proof.
Using your own example: &gt; in a language with dependent types, you can express your exact preconditions quite easily, say (xs : List A ** sort xs = xs) to express that you expect your input to be sorted, so you are likely to do so. But this imposes a burden to all your callers, who must now prove to the compiler that this precondition is satisfied. Here you have the call-site including proof: # Proof (the following diverges if xs is not sorted) assertSorted xs # The following now type-checks: # Since if xs is not sorted this will never execute requiresSortedArg xs &gt; the amount of work it takes to write a type annotation is insignificant compared the amount of work it takes to write a proof. I don't see how the amount of time it takes to write any type can be "insignificant" compared to the amount of time it takes to write `assertSorted`. If what you mean is that most of the time you don't need to write any types at all, the same does apply to proofs. There is nothing preventing a type system with "proof inference" in the same way that we have "type inference" (e.g. if you call a function that requires `sorted`, that means that its argument must be sorted, which can be propagated to the last time the argument was modified / constructed, and a single run-time check can be inserted there). EDIT: Note that if `xs` is known at compile-time that program can be rejected at compile-time. Also note that one can insert between the `assertSorted` and the `requiresSortedArg` function call other mutating function calls on `xs` as long as they preserve sortedness. 
A ghc panic is always a bug (if the program is invalid you should get a normal compiler error)
I think GHC's warning for implicit imports should provide you with the code for an explicit import list because it already has that information. This is what PureScript's compiler does. Then your editor can just apply the suggestion automatically. Intero can remove or add from/to explicit import lists when GHC suggests that, though.
I think import lists are a usability disaster. Your editor should provide that info in context.
Wouldn't it be better to make the editor produce import lists? Ads code is generally written using editor support specific to the language, but may be read in less advanced editors (think for example browsing the source on GitHub). So placing the complex part on the writer seems more useful. 
Great idea!
Yeah, if your editor can work out which module identifiers come from then it can make explicit import lists to help out those whose editors can't.
Out of functor-monad-applicative-alternative are there any other way to compose effecful computations?
I allow implicit import lists for local modules (in the same program) and also "common" modules (mostly stuff from `base`)
If you import without a list and without `qualified`, then it's possible that a dependency can break source-compatibility with you by *adding* a new symbol. Since *removing* and *changing* a symbol are already binary-incompatible, this means that your code is not forward-compatible *at all*. Sometimes I even give an import list for *Prelude* when I'm being paranoid.
I would also love some ideas for taking advantage of QuickCheck for a web app. (Free blog post idea for someone!)
I assume you are writing to "/dev/null" then? Otherwise, you are going to get all kinds of data noise from the file system and physical disks.
I agree that the time it would take to write this kind of asserts would be similar to the time it would take to write type annotations. In fact, we can do this today, using smart constructors, we don't need dependent types to use an assert-based approach, and that's exactly the kind of "not-quite-dependent" approaches I am advocating. However, I wouldn't call such an assertion a "proof". If a proof is a type-correct program, then asserts are more like a call to `unsafeCoerce`. When I say that proofs are time consuming, I am talking about a proof which involves reasoning about the implementation, and conveying our reasoning to the compiler so that it can check that our reasoning is correct. For example, if I want to prove that the merge step of merge-sort produces a sorted list, I need to say that 1. given two empty sorted lists, the algorithm returns an empty list, which is trivially sorted 2. given one empty sorted list and one non-empty sorted list, the algorithm returns the non-empty sorted list, which is still sorted 3. given two non-empty sorted list, the algorithm compares the heads and picks the list with the smallest head. the tail of the list is still sorted, see lemma A. the algorithm then recurs with the tail of the chosen list and the entire other list. by induction, the result will be a sorted list, which we will call the intermediate list. moreover, the elements in this intermediate list will all be at least as big as the chosen element, see lemma B. the algorithm prepends the chosen element to the intermediate list, and the result is thus still sorted. This is a lot more work, but has the benefit that the proof will fail at compile-time if you've accidentally swapped the `then` and `else` branches after comparing the heads. Regarding proof inference and type inference, we currently have a very good understanding of the type systems which can be inferred and the circumstances under which a type annotation is required. We already have proof inference, but it's a lot less clear when inference will work or not. I am advocating for many "not quite dependent" solutions which trade some power in exchange for a guarantee that the compiler can always infer the proof. This way, we will improve our understanding of the boundary between the kinds of proofs which can and cannot be inferred.
I'd say that `groupBy` is a more fitting name for `repSort`, and viewing it that way you can attribute `r` a map-like structure. The list monoid models partiality, e.g. that we don't have an `a` for each `Rep r`. I think representable functors are good at modelling containers in general. In absence of partiality, `Rep r`, denoting the slots in the container, gets a little complicated, though. That is essentially the idea behind the theory of [containers](http://www.cs.nott.ac.uk/~psztxa/publ/fossacs03.pdf) https://en.wikipedia.org/wiki/Container_(type_theory)
The real question might be how to abstract your code from the persistence mechanism and http request, where your implementation is something that receives json and arguments, performs persistence actions, and responds with json and a status. People have discussed this issue a bit, using transformers or extensible effects (e.g. Lexi lambda). It is also discussed outside of Haskell, with topics like hexagon architecture, etc. Some people approach building a mock persistence mechanism by using sqlite. I have been curious about using an ioref or acid-state for the mock. The question is usually how to do it with the least amount of extra code and cruft, and avoiding over architecting. Once you have abstract action handlers, then you can use quickcheck with generators for typical json input and initial state of the persistent store, and test for expected final persistent store state, response body, and response status.
From Java we have a pretty good example of what happens if you design around the assumption that certain editor features are available, and it's not great.
I'm not! How do I join?
Because far too often these pre-emptive upper bounds are too restrictive, and the package maintainers are asleep. There are a lot of bitrotten, "broken" packages on hackage which would compile and work perfectly well with a simple change from `base &lt;= 4.8` to `base &lt;= 4.9` or even `base &lt;= 4.10`. Lots of packages that compile perfectly well with the latest `aeson`, yet insist on having an upper bound that's a couple years old, too. Instead of keeping these "useful" constraints which are preventing perfectly-usable packages from compiling, the Stackage folks have decided to see what they can salvage for us, while we wait months for package maintainers to realise that GHC 8.2 _is now officially A Thing._
Edward Yang and I believe also Simon Marlow worked out a primop for that. It uses information kept around for garbage collection to reduce values recursively.
Interesting. I've come to prefer qualified imports in most cases, which keeps the benefits of disambiguation while avoiding the nuisance of constantly updating the explicit import lists. I just wish more packages would name their identifiers with short names that assume they will be imported qualified.
In the use-a-separate-datatype example, data Foo = Bar RealBar | Baz data RealBar = RealBar { barInt :: Int , barString :: String } `Foo` should likely be written data Foo = Bar {-# UNPACK #-} !RealBar | Baz This gives it the same run-time representation as the unsafe version. While I'm at it, I should also mention that `RealBar` should *probably* use `barInt :: !Int` to unpack the `Int` and improve strictness analysis. You should only use a lazy `Int` if you have a really good reason; it's very rarely what you actually want.
Apart from the `base` version changing, levity-polymorphism now means that quite a few packages have missing `LANGUAGE` pragmas. For example, `TypeRep` is now a type synonym, and Haskell's deriving mechanism needs to be coaxed into working with anything but simple types. These are mostly very minor changes, though. Trivial. 
With Applicatives, an action can't depend on a value "returned" from a previous action. So you do need Monad. (**later edit:** Sorry, I misunderstood the question. The anwers below aren't actually what was requested.) To wait until a certain total, you can use [untilJust](http://hackage.haskell.org/package/monad-loops-0.4.3/docs/Control-Monad-Loops.html#v:untilJust) from **monad-loops**. If you need more control, another way of seeing it is as a streaming problem. You could generalize `roll` to work like this: roll :: Monad m =&gt; StateT StdGen m Int roll = state (randomR (1, 6)) And define a stream of 3-roll results using [streaming](http://hackage.haskell.org/package/streaming): import Streaming import qualified Streaming.Prelude as S rolls :: Monad m =&gt; Stream (Of [Int]) (StateT StdGen m) r rolls = S.repeatM $ replicateM 3 roll Using [Streaming.Prelude](http://hackage.haskell.org/package/streaming-0.1.4.5/docs/Streaming-Prelude.html), you can manipulate the stream of results a bit like a normal list: filtering, taking, and so on. λ gen &lt;- getStdGen λ print . runIdentity . flip evalStateT gen . S.toList_ . S.take 1 . S.filter (\rs -&gt; sum rs &gt;= 18) $ rolls [[6,6,6]]
I am the maintainer of `plugins`, `web-plugins`, and `plugins-ng`. I took over `plugins` when the original author stopped responding to pull requests, etc. However, I know almost nothing about the internals and have no desire to do any real work on it. My entirely role is to respond to pull requests and upload new versions. One reason that I do not work on `plugins` is that I suspect it is the wrong approach. It predates the GHC API. I hacked up a prototype [plugins-ng](https://github.com/Happstack/plugins-ng) which uses the GHC API. `plugins-ng` is definitely incomplete -- though I do not remember how incomplete. I know I implemented enough to show that it is a viable option. `plugins` and `plugins-ng` do not really provide any sort of API for writing plugins -- just some low-level dynamic loading code. `web-plugins` aims to provide an API which can be used for creating plugins for web applications. It can be used either for staticly linked plugins or dynamically loaded ones. There is a proof-of-concept dynamic loader -- but you have to manually recompile the `.hs` files to provide the `.o` files for loading. At present I only use statically linked plugins. In order to provide one-click install of plugins into clckwrks I need some sort of sandboxed environment where plugins can be compiled and hopefully even support downloading of precompiled binaries. When I started those libraries I think `cabal sandbox` was not even a thing. But even so, that only handles the sandboxing of Haskell libraries, but not other dependencies. So now I am thinking that the `plugins-ng`/`web-plugins-dynamic` stuff ought to build on top of nix since nix nicely solves all those issues. Alas, I have not yet gotten to the point where there is an economic imperative to have solid dynamic recompilation and reloading, so everything remains stagnant. My time is currently invested in work on an unreleased client-side web UI library. 
You're right, it wouldn't be a good thing. But there's nothing useful about those constraints so nothing of value is lost when you remove them. In the contrary, version bounds are harmful as they block new versions from entering Stackage snapshots for no reason other than there being an arbitrary bound. And to literally add insult to injury, you have the Hackage police going around and [coercing package maintainers to comply with silly policies](https://www.reddit.com/r/haskell/comments/6jv15h/haskell_infrastructure_swift_navigation/djitlho/). So getting rid of version bounds would really kill two birds with one stone. No more Hackage police, no more revisions, and no more waiting months for Stackage to upgrade package versions due to phony version bounds. A clear win-win for everybody.
It would be nice to have an "everything else" operator for import lists. Then I could say `import MyModule (stuff, things, ...)` where `...` means everything else. This would allow me to import everything when developing a new feature, and then after the feature was implemented and I knew exactly which functions I needed I could clean up the import list. Although a sophisticated editor can eliminate the need for this, we shouldn't assume people are always using a sophisticated editor.
I'd agree, GHC could help make that easier to automate. 
Don't mock the database. That's the wrong level of abstraction. Test the functions that *use* the data coming from the database, and ensure they're correct. Who cares where `[Entity User]` comes from -- could be QuickCheck or the database. The function should work the same either way. If you really need to mock something, then mock one level up: the data access layer. Given foo :: UserQuery -&gt; Handler User foo userQuery = runDB (getUserMatching userQuery) you can choose to mock this by either 1) mocking the entire database, or 2) mocking just the one function/query you care about. It's way easier to provide a limited mock than it is to mock the entirety of SQL. So, you can either try to provide a mock for `SqlPersistT` (don't), or you can hide your data access layer behind another level of indirection: class DataAccessLayer m where getUserMatching :: UserQuery -&gt; m User with an instance instance MonadIO m =&gt; DataAccessLayer (SqlPersistT m) where getUserMatching q = selectList (convertQueryToWhere q) (convertQueryToOpts q) this recovers your database access. Then, to test, you use: newtype MockDb a = MockDb (StdGen a) instance DataAccessLayer MockDb where getUserMatching q = generateArbitraryUserMatching q
Looks like I forgot to run stack setup. Could have sworn. Anyway it works now, thanks!
Most web apps are basically: 1. Receive a web request 2. Do some logic 3. render and return the response For our Servant API, we can use QuickCheck to verify that our `ToJSON` and `FromJSON` instances for our request/response types satisfy our `ToSchema` Swagger instances: https://github.com/haskell-servant/servant-swagger/blob/master/src/Servant/Swagger/Internal/Test.hs This ensures that our documentation and clients don't get out of date. That handles points 1 and 3 as far as I care about -- most of the correctness of a web request and rendering should be the responsibility of the libraries and not the application. For 2, this doesn't have anything at all to do with a web application. Sure, your handler is like handler :: UserId -&gt; Handler Html or, in Servant, handler :: UserId -&gt; Maybe SomeQueryParam -&gt; PostBody -&gt; Handler Foo But, ultimately, your *domain* logic has nothing to do with the web. It should be a library that can be just as easily called from the command line, or a native GUI app, or from some other program. 2, then, is often "just a function" -- something `a -&gt; b`, or maybe `a -&gt; m b`, or maybe something a little more complicated. In those cases, you use the same techniques you'd use to test anything else.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [haskell-servant/servant-swagger/.../**Test.hs** (master → 22eef92)](https://github.com/haskell-servant/servant-swagger/blob/22eef921d3706a263b0077bd3abd4c35cd67cbfd/src/Servant/Swagger/Internal/Test.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dkw6m0f.)^.
Really, you can't have your cake and eat it too if you aim to accommodate the lowest common denominator: you would logically end up choosing qualified imports for everything. Otherwise you have to pick a side between writer and viewer. If you aim for editors with no features (basically, `ed`), then open import lists are better than explicit import lists, because you need editor support to make adding/removing those lists anything other than obnoxious. If you aim for viewers with no features (basically, `cat`), then explicit import lists are better than implicit export lists, because then you need viewer support to see where things come from. Qualified imports have a write and read overhead that is equal for both sides.
I think that qualified imports are readable, if ugly. Explicit import lists aren't "readable" in the same way, because the name's origin is most of the time too far away. You can't read a page without flicking to the start of the book. That's not "readable" to me. 
By the way, PureScript is a compiler that assumes good editor support, because it badgers you about writing explicit imports for everything including `Prelude`. You end up with a screen full of warnings to scroll through. It was awful to program in (you just spend all day jumping back and forth from the imports and your code) until I updated my editor with psc-ide-mode which has a way to apply suggestions in an automatic way.
And while you're at it, `-Wredundant-constraints`! This was part of `-Wall` in GHC 8.0.1. But sadly was removed from 8.0.2 onwards...
Dunno, some day I'll go trawl mailing list archives to see if it's been asked before, and if not, ask myself, but I was going to wait until I had more experience with it.
This would fix a whole bunch of partial IsString instances in my code. And we could have typesafe printf, safe regexes, all manner of lightweight DSLs, not to mention simple things like multi-line string literals. It's some of the stuff that dependent types can do too though, but lightweight macros might be simpler to implement and with better errors.
I think the biggest difference is for modules that are "algorithm-like" imo. For example placing elements in a grid or getting authentication logic right. You specify invariants and QC will generate a lot of different input scenarios that might break those invariants. For example in a grid the bounding boxes of elements may never extend over the side edges, there should be no holes in the finished layout, and no duplicates. 
That limitation is really quite annoying. It seems to me that it should be possible to make it work in relevant cases by ensuring that everything is specialized to a sufficiently representation-monomorphic form before actually being used (i.e., producing only an unfolding, and not object code, for a function that takes a representation-polymorphic argument). Some care would probably be required to prevent unacceptable polymorphic recursion from throwing the specializer or simplifier into a loop, but letting the compiler loop on bad code isn't the worst thing in the world.
That says more about the shortcomings of the PVP than it says about the advantages of maintaining explicit import lists. Both the PVP and explicit import lists lead to a treadmill of tedious maintenance. 
Surely under the heading Function Types, the definition should include f' a', and not f a'?
For `-Wmissing-import-lists`, I'm waiting for some tool that will do the work of filling in the information for me automatically. Until then, it's not worth it to get this right for every symbol.
What's bad about the ergonomics? Do you mean you don't like the config language?
Spellchecker tells me modularization is not a word and suggests "Popularization". I think it's a little premature at this point.
&gt; The sheer size and focus on TDD in Ruby means that OSS is often more of a "please fix" and itll actually get fixed. I'm not sure if I have the same experience. Sure, TDD is absent in Haskell (and sorely needed IMO), but for me "please fix" often leads to things being fixed in haskell libraries. I have a very low threshold for filing bugs (on github), and people are very responsive.
Make sure you include one that Haskell *wouldn't* have prevented, so you look like you are being fair and balanced. 
The other thing is that qualified imports make a tool to automatically manage the import list trivial, which almost eliminates the write overhead.
It seems you're right, and yet the docs still claim it's on by default: https://downloads.haskell.org/~ghc/8.2.1/docs/html/users_guide/using-warnings.html#ghc-flag--Wredundant-constraints 
&gt; Aren't most webapps/webservers stateless to begin with? Are they? You have to startup the web server after re-compiling without using rapid. I guess the server state would be in the foreign-store? I'm not sure about it internally though, I've mostly used rapid for data analysis and keeping large data in memory while making 100s of small changes. 
Claro, se las pido al speaker y te paso el link
I recently started work on intero support for Yi. If you want to help but don't know where to start, I have made a todo list with details in [the yi-intero readme](https://github.com/yi-editor/yi/blob/master/yi-intero/README.md#development). Don't hesitate to reply here, open an issue on github or join our irc chat at #yi on freenode if you encounter problems or unclear documentation.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [yi-editor/yi/.../**README.md#development** (master → 33306ea)](https://github.com/yi-editor/yi/blob/33306eab672c9f8fff268a92852e295459ea67c8/yi-intero/README.md#development) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dkwfyzd.)^.
Which modules are the emacs' `haskell-mode` equivalent in Yi? I see an Intero package and a Haskell one, what are their differences? Also, why the choice of alex, instead of another package?
You're right, thanks! Fixed!
Follow this link: https://discord.gg/YmjzmN
I don't really know what the emacs haskell-mode does, but the yi-mode-haskell package does the highlighting and indentation work. yi-intero is very new and not quite ready for real use, but it aims to provide ide-like functions using intero. I don't know why alex was chosen. It might have something to do with that yi is pretty old. The decision to use alex was made over 10 years ago, so I guess it was a pretty good option at the time.
Is there a detailed guide on how to configure and use yi? I'm full time spacemacs right now, but I'm tired of getting bitten by elisp. I've tried yi a couple of times over the years, but I get stuck after building it and running it. There aren't a lot of comments in the example configs or core code, so I couldn't figure it out for myself. One of the great things about emacs and vim is that they have extensive help docs built into the editors. I realize yi is still very young by comparison, but laying the groundwork for this would go a long way towards getting more users and contributors.
&gt; I get stuck after building it and running it. Could you expand on where exactly you get stuck? So for example you are familiar with vim, you build and run yi-vim-vty-static, what happens next?
Looks like a patch was just posted, that's some fast turn around! https://phabricator.haskell.org/D3805
Separating pure logic from the effectful parts I agree with but parametrizing all of the effectful functions is kind of pointless. It's exactly what mtl classes do and they save you from having to pass in implementations manually.
&gt; As far as I know, most Clojure's transducers use volatile! This is correct, see for example the source code for `take`: https://github.com/clojure/clojure/blob/clojure-1.8.0/src/clj/clojure/core.clj#L2752-L2772 It's just for performance reasons, there's no real need for immutability here afaik
I really wish there way a compiler flag to add `HasCallStack` to every function in a package. That would also make is easy to remove them in release builds if performance mattered.
Yes, exactly. I've given a talk on mtl style a few times to coworkers and at a small, local meetup, and this is actually how I *motivate* using mtl style classes for testing purposes. Manually threading a bundle of functions through your program isn't very readable, and it's basically what typeclasses are designed to do.
 partition' :: (Representable r, Foldable f, Eq (Rep r)) =&gt; (a -&gt; Rep r) -&gt; f a -&gt; r [a] partition' pred = repSort pred (\x -&gt; [x]) looks like a generalization of [`partition`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-List.html#v:partition) partition :: (a -&gt; Bool) -&gt; [a] -&gt; ([a], [a]) Compare it to `partition' @Pair @[]` (using the `@` syntax from `TypeApplications`) partition' @Pair @[] :: (a -&gt; Bool) -&gt; [a] -&gt; Pair [a] &gt;&gt; partition odd [1,3,5,6,5] ([1,3,5,5],[6]) &gt;&gt; partition' odd [1,3,5,6,5] :: Pair [Int] Pair [1,3,5,5] [6] only `partition'` will let you partition into any representable functor, which gives us weird stuff like partition' @((-&gt;) _) @[] :: Eq w =&gt; (a -&gt; w) -&gt; [a] -&gt; w -&gt; [a] &gt;&gt; partition' (`elem` [1..5]) [10,4,66,1,2,-5,5] True [4,1,2,5] &gt;&gt; partition' (`elem` [1..5]) [10,4,66,1,2,-5,5] False [10,66,-5] It can even create a **2 x 2** truth table if we [compose](https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Functor-Compose.html) two `Pair`s partition' @(Compose Pair Pair) @[] :: (a -&gt; (Bool, Bool)) -&gt; [a] -&gt; Compose Pair Pair [a] &gt;&gt; partition' @(Compose Pair Pair) (even &amp;&amp;&amp; (&gt; 10)) [5..14] Compose (Pair (Pair [12,14] [6,8,10]) &lt;- Even (Pair [11,13] [5,7,9])) &lt;- Odd ^ ^ (&gt; 10) (&lt;= 10) I'll leave it to others to understand `partition @(Cofree _)` :) such an interesting use of `Representable`
You should almost never need to do that, because simpler techniques already have you covered. `mtl`-style can be rather burdensome with the plethora of `newtype`s and instances, so they should be reserved for only the most difficult of effectful business logic.
Nice writeup! A few comments: 1. `:type` doesn't show the `HasCallStack` constraint by design. There were concerns about overwhelming newcomers, especially since we added it to `error` and `undefined`. `:info` does show it along with other "lower-level" details like the levity parameters. 2. The fact that a single function without a `HasCallStack` constraint in the call stack can break the chain is indeed a big limitation. Unfortunately, since the `CallStack`s are passed around like regular arguments, I don't think there's a way around this without fundamentally reworking them. 3. There's actually an ongoing discussion about attaching `CallStack`s to GHC's exceptions at https://ghc.haskell.org/trac/ghc/ticket/12096. IIRC the main question is how to best handle catch-and-rethrow situations, please chime in if you have any thoughts!
Unfortunately doesn't work with vectors from [*linear*](https://hackage.haskell.org/package/linear) ([`V2`](https://hackage.haskell.org/package/linear-1.20.6/docs/Linear-V2.html) is isomorphic to `Pair`). While `Rep Pair = Bool` the *linear* library chose the more complicated `Rep V2 = E V2`, `Bool` and `E V2` are isomorphic but there is no `Eq` instance for `E V2` &gt; :t repSort @V2 @_ @[] &lt;interactive&gt;:1:1: error: Could not deduce (Eq (E V2)) arising from a use of ‘repSort’ from the context: Monoid w bound by the inferred type of it :: Monoid w =&gt; (a -&gt; Rep V2) -&gt; (a -&gt; w) -&gt; [a] -&gt; V2 w at &lt;interactive&gt;:1:1
There's also an issue about this. https://ghc.haskell.org/trac/ghc/ticket/13360 I agree it would be nice to have, but there would still be limitations, e.g. if you're using partial functions from `containers`, you would have to have compiled `containers` with the flag as well to get a full stack trace. In the limit, it starts to feel like adding another version of the `-prof` way. The main difference is that with `HasCallStack` you wouldn't have to recompile the whole world, just the packages where you need stack traces.
This is quite close to [unliftio](https://www.reddit.com/r/haskell/comments/6nr9ya/announcing_the_new_unliftio_library/). I wonder if the author often needs to use control operations like `try` or `bracket` in `IO`.
I'm actually planning on a library that wraps exceptions in something like data LocatedException where LocatedException :: Exception e =&gt; e -&gt; Callstack -&gt; LocatedException along with a `$(checkpoint)` function that catches exceptions, annotates the location, and rethrows as a `LocatedException`. My company needs this for a BugSnag integration, so I might as well do it myself :D
In those cases, I use `MonadBaseControl`, though switching to `unliftio` is definitely on my todo list.
Same here. Qualified imports aren't as ugly as so many people seem to claim. It's certainly no worse than all these long names people come up with to avoid forcing others to import their modules qualified. Also, qualification allows you to choose your own prefix. 
If we were not so concerned with the lowest common denominator then I feel like we would have done a better job of exploring non-text file programming by now. 
`partition' @(Compose ((-&gt;) _) ((-&gt;) _))` is so fun and weird, let's give it a name satisfying :: (Eq b, Eq b') =&gt; (a -&gt; (b, b')) -&gt; [a] -&gt; b -&gt; b' -&gt; [a] satisfying pred xs = getCompose (partition' pred xs) `satisfying ((isUpper . head) &amp;&amp;&amp; length)` allows us to find strings based on capitalization and their length. &gt;&gt; -- Finds capitalized words of length 5 &gt;&gt; satisfying ((isUpper . head) &amp;&amp;&amp; length) ["Hello", "fine", "World"] True 5 ["Hello","World"] &gt;&gt; -- Finds non-capitalized words of length 4 &gt;&gt; satisfying ((isUpper . head) &amp;&amp;&amp; length) ["Hello", "fine", "World"] False 4 ["fine"]
No, applicative doesn't support branching control flow based on the *value*; that's almost precisely the extra power of the monad, and why applicatives are easier to statically analyze. (Of course the applicative itself might implement a choice-style control flow.)
Yeah. I wrote a library for effects of type `a -&gt; m b` since those can be lifted through any transformer, and the library provides an overlappable instance so I don't have to worry about newtypes or writing instances manually.
Honestly, if it only worked for the package I'm currently working on I'd be fine with it.
Would you like to hear a joke about lazy evaluation? Answer "Yes" or "No".
A good story for database migrations could be really valuable as well, especially if your database types were the result of applying all declared migrations to the null schema or something like that. A good way to reference stored procedures could be nice too, but is probably too small for a thesis project.
&gt; That's the wrong level of abstraction. I was mostly trying to seed the idea of simplifying dependencies/effects to get closer to pure functions, with no clear endorsement of which method I would use. I don't think a consensus has emerged, nor do I expect one to. Layering, how to test, and what to test varies widely based on people's personal preferences and where they want safety and other factors. Ideally, for most tests we write, we should be thinking about how we can make dependent types work harder for us instead. But I definitely appreciate having a complete explanation of each person's perspective, so we all know of more options available to us.
Yup that's unfortunate, for the deeper tries you could use [Vector Arrays](https://hackage.haskell.org/package/vector-0.12.0.1/docs/Data-Vector.html) for performant access, you'd just have to wrap them to ensure its a fixed size. Would do the trick for string indexed Tries.
Glad you found some uses for it, I think it's pretty cool, but I'm biased! As functions get more general they gain more utility, but also get harder for the uninitiated to use. I enjoy seeing how the machinery built up all fits together, in this case Representable and Monoids.
What sort of problems have you had with Emacs Lisp?
This is fundamentally monadic. You need to make a decision about what to do next based on what you've done before. Applicative isn't enough to allow that.
Could you please give an example of rolling until a certain total with `untilJust`? I can only do that with `unfoldrM`: rUntilSum2 n = unfoldrM (\acc -&gt; fmap (\d -&gt; if acc &lt; n then Just (d, d + acc) else Nothing ) roll ) 0 
\_|_
The answer on when to use them is simple: If a function can error due to some bad input (for example when it is partial), then it should have `HasCallStack`.
type errors
Thanks for the Rasa shoutout! I'm hoping to put some more time into it soon. This was a nice survey of the text editor ecosystem, it's tricky to write these things, but I'm confident we'll eventually get it all figured out!
side effects
I think you're missing a -&gt; in the type signature of prodRel.
It's been a while, so I don't remember the details, but I do remember typing `:help` and it just opens a pane with a list of functions and typing those into `:` does nothing. I then dug into the code to see about changing some keybindings, and I got turned off by the lack of comments and documentation. I think it would help if there was some documentation that focuses on how *do* things, like: * How to override a keybinding * How to change the "mode-line" (or whatever you'd call it in yi) * How to build an interactive buffer It hadn't occurred to be to look at your [configs](https://github.com/ethercrow/yi-config/blob/master/Main.hs) which would have been helpful, but there still aren't any comments or justifications so it makes learning difficult. The [example configs](https://github.com/yi-editor/yi/blob/master/example-configs/yi-vim-vty-static/Main.hs) have gotten *smaller*, which I think makes it even more difficult because there is nothing to tweak. My first thoughts are "What is `ConfigM`? How do I change it? What else can I add?" The [`~/.spacemacs`](https://github.com/syl20bnr/spacemacs/blob/master/core/templates/.spacemacs.template) config is a great example for how to teach users about the system to make it their own. Also, it would be nice if yi had a way to annotate functionality with help text. Something like [which-key](https://github.com/justbur/emacs-which-key) is a huge help for making functionality discoverable.
There is a [`V`](https://hackage.haskell.org/package/linear-1.20.6/docs/Linear-V.html) type in *linear* that wraps `Vector` and is represented by `Rep (V n) = Int` which is obviously `Eq` newtype V n a = V { toVector :: Vector a } repSort @(V 3) @m @[] :: Monoid m =&gt; (a -&gt; Int) -&gt; (a -&gt; m) -&gt; [a] -&gt; V 3 m &gt; .. a binary tree is isomorphic to `Cofree Pair a`, etc. I think you mean `Free Pair a`.
Good catch :)
In my opinion, we really need a sensible default between “pedantic” and the current default which allows incomplete pattern matching. I think it would make sense to categorize each warning as either a runtime error-warning or a compile time-error warning, by whether or not the warnings can result in runtime errors. For example, something like orphan instances will make a pedantic build fail, but it is not a source of runtime errors, whereas ignored incomplete pattern matching-warnings will make your function crash for some input. In some cases orphan instances are too pedantic, while incomplete pattern matching-warnings are never irrelevant (just define as `someFunc Nothing = error “someFunc got Nothing”` if you want your function to be non-total).
My preferred ones are often along the lines of snark and odd-ball sentences that are only tangentially Haskell. Consider some past laughs that have made the rounds: * As we all know, elephants are integers. * "God I love lazy evaluation." ... "It will love you back as soon as you force the right thunk." * Shooting yourself in the foot with Haskell * Evolution of a Haskell programmer Heck, there's a whole [wiki page](https://wiki.haskell.org/Humor) with some good stuff, more on topic than mine. If we widen the topic to more than Just Haskell to the kind of things one might see on Planet Haskell then I'd say the Ex Falso prover and Imperfect Stitch Compiler are good ones.
&gt; Miso's templating language is just a newtype around JSVal, so the end user is as close to js as possible. Users embed pure Haskell functions onto the virtual dom while templating. I prefer to stay as far away from it as possible, thank you very much. :) That being said, I've had a lot of issues with frameworks doing this to the point that they force you to write raw JavaScript for every edge case that isn't covered by the framework. So it's nice to have a FFI to as fallback, even though I resent the very existence of the problem. And I don't see how total coverage of JS functionality is that hard of a problem to solve. What else is there beyond basic programming constructs and a bridge to the browser's API? &gt; Another interesting consequence is isomorphic javascript. Since the view type and templating combinators are shared, we get some assurance that what the server sends to the browser will be what the client expects, this allows us to traverse our virtual dom and the real DOM together (since they will be identical rose trees), populating references from one into the other. I don't buy into the JS community's usage of the word isomorphic, so I'm going to need some additional clarification here. Does the virtual DOM exist on the client or the server or both? I know Elm is all client-side, but you mention the server sending DOM related things to the client, so I'm assuming it must hold some representation of the DOM. &gt; (shown in https://haskell-miso.org) What exactly is this demonstrating? Yes, it's seamless, but how is it anymore seamless than any other framework that uses a virtual DOM but isn't isomorphic? And as for demonstrating the idea, a visualization of how user events are routed through the application would be useful. &gt; Otherwise, Elm and Miso look very similar from the outset. I'm using Elm for now but am interested in migrating to Miso down the road. But I'd like to know more about its scalability are before investing too heavily into it. Pre-optimizations, how many concurrent users is a medium-sized Miso app backed by an HTTP API running on the same server capable of supporting? And what are the bottlenecks? I know this is a rather vague description, but I haven't quite worked out the specific technologies it will use. Still at the preliminary stage of surveying what Haskell has to offer.
Ascending the ambitions of select Haskellers such that improvements like dependent types are scoffed at as Little Things for little souls. Dependent types are just catch-up work, they aren't even anything new. How about topological representations of dependencies? Or a wholesale replacement to file-based development? Those are Big Things. Integrating what other languages already have is not.
What even are the motivating examples? I can appreciate the abstraction well enough, but not enough to justify the hype and investment.
Got a link to those last two?
Hey, thanks for chiming in! May I request you to please put a notice on the hackage docs for `plugins`, which clearly indicates the state of the project. I think you should clearly state your opinion on its (un)suitability and the fact that it pre-dates the GHC API. If possible, do indicate a few links which talk about more modern approaches (one of them could be the `hint` package, which works really well for cases were intpretation of HS files is acceptable). It would save others a lot of time. There's a very real problem of abandonware in the Haskell ecosystem. Regarding `plugins-ng`, would it be possible to share a single gist which uses the current GHC API to load a pre-compiled `.hi/.o` file? I'm very invested in this problem, and given the right direction, may be able to help take the `plugins-ng` package forward.
Hey, thank you for your kind words and encouragement. I'm just doing my bit, and it helps to have skin in the game! (we're pushing really hard to get Haskell into production)
Length-indexed lists are cool, and more generally, shape-indexed containers are cool. The problem is making a reasonable syntax for all this stuff and getting your type system clever enough so it automatically does all proofs/calculations you need on these things.
&gt; Our company hasn't decided to switch to Haskell 100%, so we still need to hire PHP developers. When we advertise for Haskell developers, we get a ton of highly qualified candidates that just don't want to write any PHP ever. When we advertise for PHP developers, we get... almost nothing. So it's been hard to find Haskellers that are willing to write PHP and JavaScript. Why would you hire Haskellers to write PHP/JS? It doesn't give sense and that's why nobody is applying !!! Commit 100% to Haskell + PureScript (or even Elm) and then they will come !!! I have many friends that are Haskellers that aren't allowed to show their talent because *imbeciles* (aka CTO's, tech leads, and so) tell them to code in languages of lesser form so they can feel better about them selves. If you know the pay-cut many haskellers would make JUST to work with the language, you will probably get surprised and probably fire the hole PHP squad ...
A few equivalent ways: λ gen &lt;- getStdGen λ let roll3 = replicateM 3 roll λ flip evalStateT gen . untilJust . fmap (\rs -&gt; if sum rs &lt; 18 then Nothing else Just rs) $ roll3 [6,6,6] λ flip evalStateT gen . untilJust . fmap (\rs -&gt; guard (sum rs &gt;= 18) *&gt; pure rs) $ roll3 [6,6,6] λ flip evalStateT gen . untilJust . fmap (mfilter (\rs -&gt; sum rs &gt;= 18) . pure) $ roll3 [6,6,6] There's a slightly more powerful `untilJust` in module [Control.Monad.Trans.Iter](http://hackage.haskell.org/package/free-4.12.4/docs/Control-Monad-Trans-Iter.html) of the free package. It also requires the auxiliary `retract` function, but it lets you specify a maximum number of tries before giving up: λ import qualified Control.Monad.Trans.Iter as I λ gen &lt;- getStdGen λ let roll3 = replicateM 3 roll λ let maybeOver50 = fmap (\rs -&gt; if sum rs &lt; 50 then Nothing else Just rs) roll3 λ flip evalStateT gen . I.retract . I.cutoff 7 $ I.untilJust maybeOver50 Nothing It also lets you interleave other actions: λ let his = forever (I.delay (liftIO (putStr "Hi!"))) :: MonadIO m =&gt; IterT m r λ flip evalStateT gen . I.retract . I.cutoff 7 $ I.untilJust maybeOver50 `mplus` his Hi!Hi!Hi!Hi!Hi!Hi!Nothing **Edit:** Argh! I'm dumb. I understood you as wanting to roll a number of dice *each time* until the sum is above a certain amount. Disregard my previous posts! Your unfoldrM solution is ok. I would do it with the [scan](http://hackage.haskell.org/package/streaming-0.1.4.5/docs/Streaming-Prelude.html#v:scan) function of Streaming, to produce a stream of increasing sums: λ let rolls = S.repeatM roll λ flip evalStateT gen . S.head_ . S.take 1 . S.filter (\acc -&gt; acc &gt; 20) . S.scan (+) 0 id $ rolls Just 22 
Hey hey, there's no need for flipping people off here! I guess it kind of does make sense that the symbol for bottom looks like that, though. 
If you'd said "Yes": "OK, well I'll start thinking of one." If you'd said "No": "OK, well I won't bother thinking of one."
is there `-Weverything` that has more than `-Wall`
Oh. Sorry. I thought it was * Yes -&gt; "Ok." Then, Ask the same question again. * No -&gt; "Ok." End of conversation. Sort of the "bad" laziness that you can see in: aleph_0 = S aleph_0 or data PartalF f a = Step (f a) | Done a type Partial = Fix PartialF eventually = Fix (Step eventually)
I could have used ⊥, but I find it harder to type on reddit.
I always use [Evolution of a Haskell programmer](https://www.willamette.edu/~fruehr/haskell/evolution.html). I hadn't read [Shooting yourself in the foot with Haskell](https://wiki.haskell.org/Shooting_your_self_in_the_foot) before.
Couldn't you just import everything during development, and then only import specific functions afterward. An everything else operator would just do that.
 fix ("ha " ++)
Why is Sir Mixalot bad at fast &amp; loose equational reasoning? Because he can't ignore bottoms.
I'm really excited about the pickup in Yi development since all developers need an Emacs replacement that allows you to write the modules in a language that's more amenable to writing parsers for all kinds of languages. While the mention of the regex based mode is nice, regex is the major limitation and problem in Emacs and Vim and more editor major modes. The origianl approach of Yi is better and will be faster. You will write a parser for major mode language either way, whether it's purely as a sequence of regexes or with a parser that also utilizes regexex but it's inside a framework like parsec. Notable projects that tried this before are OCaml's https://github.com/zoggy/chamo and http://camelia.sourceforge.net/Camelia_Guide.pdf
Now that modularization is being tackled, would you consider joining efforts with Yi?
I'm excited for Yi to provide support for Haskell editing just like Eclipse does for Java by actually reusing the Haskell (Java) libraries to process the code without spawning external commands and trying to parse their output. What LSP and Intero do should be easier to build without the context switch of vimscript and elisp. Eclipse has its own Java compiler and for Haskell it may be enough to have a mini-compiler that doesn't do more than parse and validate, which can probably be achieved without writing it but reusing GHC's API. Similarly the same can be done for a C/C++ mode that uses LLVM. But this isn't the only model of supporting a language. For most languages it's enough to have a proper parser written in Haskell for a syntax without the fragile Emacs/Vim regex web for highlighting.
Is there a guide on how the new build process (static without dyre) can be used? I will have to port my old config over to that.
But with mtl-style approaches you tie a given interpretation of something to a monad, and this may or may not be a problem. It has been many times in the past for me. The approach described by /u/ephrion (with a `UserQuery` type to act as the frontier between the app and some interpretation of "user queries") has been very good to me, with the little twist that I usually make it a GADT, so that I specify what arguments it takes and their type, but also the expected return type, as the different queries that I have usually don't return the same thing. This approach makes it trivial to mix and match all kinds of strategies depending on whether you're putting together the `main` for the production program or just spinning up the appliction with "dummy/mock interpreters" for testing, etc, without the need for changing the monad or anything like that.
&gt; May I request you to please put a notice on the hackage docs for plugins submit a pull request. &gt; Regarding plugins-ng, would it be possible to share a single gist which uses the current GHC API to load a pre-compiled .hi/.o file? Apparently I got as far as actually watching the .hs files and automatically recompiling and reloading when they change, including dependencies. Check out: https://github.com/Happstack/plugins-ng/blob/master/Example.hs It has a simple loop that gets a string from the user, applies the 'filter' function' and then prints the result. The filter function comes from `Filter.hs`. You can run `Example.hs` and then modify the `Filter.hs` and it will automatically recompile and reload that module. `Filter.hs` loads functions from `Filter2.hs` and if you modify the functions in that file it will also trigger a rebuild even though `Example.hs` only directly refers to `Filter.hs`. You can look in here to see how the magic is done, https://github.com/Happstack/plugins-ng/blob/master/Plugins.hs There is not much too it. 
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [Happstack/plugins-ng/.../**Plugins.hs** (master → c35f6c2)](https://github.com/Happstack/plugins-ng/blob/c35f6c215963b20c8b5afb5350ca37ea5881a770/Plugins.hs) * [Happstack/plugins-ng/.../**Example.hs** (master → c35f6c2)](https://github.com/Happstack/plugins-ng/blob/c35f6c215963b20c8b5afb5350ca37ea5881a770/Example.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dkx568l.)^.
The unexpected \_|\_ gave me a good laugh.
You can always write `instance MonadState s (ReaderT (s -&gt; m ()) (ReaderT (m s) m))`. Now you're free to choose your implementation at runtime.
[removed]
I feel like if there would be anything that would get me to use Yi, it would be killer Haskell support, so very much looking forward to this! :)
I can't disagree with that!
Unfortunately intero doesn't expose a haskell API so I have to do the same external command spawning Emacs does.
Maybe the Intero fork that's been refactoring it, forgot the name, does? Also, I suppose collaboration with Alan on LSP and all the underlying bits like Brittany and such would provide API. The most important feature is that a haskell editor mode uses a real parser that can handle partial code and if it fails to indent or highlight, for instance, does not crash and burn. Haskell's syntax is nice but it imposes more work on fancy editors. I think we can get there, and there's a need for a syntastics/flymake style quick syntax validation parser mode.
One thing that I'm really glad about is that Yi embracing modularization but is no longer over zealously splitting every package into its own git repository. I took a serious look at Yi a year ago and was very fearful for Yi's feature if that trend continued. Now, Yi's development looks quite sustainable and all it seems to need is contributors. I wonder what is blocking Yi from earning a dozen or so regular contributors. At that point it would really be a serious contender of an editor. Also, have the performance problems finally been solved? I recall Yi slowing down from time to time, and consuming a ridiculous amount of memory.
A comathematician is a device for turning cotheorems into ffee.
In theory HoTT should help, since the "univalence" principle allows isomorphic types to be treated as equal. This would let us treat list-like types as lists, but we'd still have to transform the extra bits (e.g. proof obligations) if we want our guarantees to hold for the results too. That's what the "ornaments" work mentioned in a sibling is about. One problem with univalence is that it doesn't yet (AFAIK) have a computational interpretation. In other words, given an expression which uses univalence, we have no general method to reduce it to a result (like we have for, say, beta-reduction).
Cartesian Closed Comic is good too!
I am unsure, but it might be useful to have db libraries offer two versions of each db action - one that raises exceptions exclusively, and one that uses a combination of an error monad and exceptions. The hybrid version would treat errors like "violates unique constraint" as errors in the error monad and errors like "unable to connect" as exceptions. (I am not sure if an existing db library already offers such a hybrid). One could test that the error monad errors are handled as one desires.
Loved the absurd confidence of the Elephant one.
A novelty account in /r/haskell?
Who's we?
Similarly XMonad and xmobar have according to `top` put a memory pressure of around 1TB during a couple hours of running while they do not exceed 16MB peak resident memory. Does that mean there's a lot of churn (allocations) happening in the runtime?
That last one isn't just a joke. It's one of the best resources currently available for showing how to decouple recursion. The last couple of entries predicted the Free Monad craze years before it happened. And with minor fixes all that code still runs on modern GHCs.
I don't consider rasa to be any serious competitor, it doesn't have undo/redo or syntax highlighting or anything like that ; Rasa is my first ever Haskell project which I wrote to learn the language, I've learned so much working on it! So no, I don't plan to stop working on it since my goal was always educational and not just to build a good editor. Even if absolutely no-one uses it, if I'm still learning by tinkering with it then it's worth it to me. I'm all in favour of Yi and their efforts though, and I have plans to eventually contribute to our shared tools, like yi-rope! :)
&gt; Why would you hire Haskellers to write PHP/JS? It doesn't give sense and that's why nobody is applying !!! Part of being an employee at a small polyglot company is that you have to touch every part of the system occasionally. There's some PHP code that needs maintenance while it's being replaced, and there's some PHP that probably won't ever be replaced. The front ends are entirely in JavaScript at this point, and while I'm a proponent of Elm/PureScript, we don't have the functional expertise on the front end to convert a project at this point. &gt; If you know the pay-cut many haskellers would make JUST to work with the language, you will probably get surprised and probably fire the hole PHP squad ... This isn't really something I've observed. Most Haskellers where I'm aware of salary numbers are expert programmers and are in the 95th percentile for salary. For them to take a cut from $130-150 to $100-120 doesn't make them appreciably cheaper -- they're still on the higher end of developer salaries. Furthermore, it's not like PHP developers are expensive -- they're some of the lowest paid developers on average.
For Falso I had to use [archive](https://web.archive.org/web/20160402134351/http://inutile.club/estatis/falso/) since the original is down at least right now. The [Imperfect Stitch Compiler is here](https://galois.com/blog/2016/04/galois-announces-isc-the-imperfect-stitch-compiler/).
Not sure if events will be useful. Maybe a link to a directory of local meetups, or events could consist of conferences and events meant for people to travel to.
&gt; will also reuse the test suite dependencies in the same way that new-build reuses build dependencies? I assume that by "test suite dependencies" and "build dependencies" you simply mean satisfying the respective build targets' `build-depends`/`build-tool-depends`/`build-tools`? As to the reason that `cabal test` doesn't work is that you need to run `cabal new-test`. Which requires at least cabal 2.0 (better yet cabal 2.1 if you care about `new-build`, as Francesco is [making quite some progress](https://github.com/haskell/cabal/projects/4)), then you can simply do something like $ cabal get -s parsec git: clone "https://github.com/aslatter/parsec" Cloning into 'parsec'... remote: Counting objects: 857, done. remote: Compressing objects: 100% (34/34), done. remote: Total 857 (delta 27), reused 48 (delta 22), pack-reused 799 Receiving objects: 100% (857/857), 336.73 KiB | 0 bytes/s, done. Resolving deltas: 100% (517/517), done. Checking connectivity... done. $ cd parsec/ $ cabal new-test --enable-tests -w ghc-7.6.3 Resolving dependencies... Build profile: -w ghc-7.6.3 -O1 In order, the following will be built (use -v for more details): - HUnit-1.3.1.2 (lib) (requires build) - hostname-1.0 (lib:hostname) (requires build) - parsec-3.1.12 (lib) (first run) - regex-posix-0.95.2 (lib:regex-posix) (requires build) - xml-1.3.14 (lib:xml) (requires build) - test-framework-0.8.1.1 (lib:test-framework) (requires build) - test-framework-hunit-0.3.0.2 (lib:test-framework-hunit) (requires build) - parsec-3.1.12 (test:tests) (first run) Configuring hostname-1.0 (all, legacy fallback)... Configuring HUnit-1.3.1.2 (lib)... Configuring library for parsec-3.1.12.. Configuring regex-posix-0.95.2 (all, legacy fallback)... Configuring xml-1.3.14 (all, legacy fallback)... Building hostname-1.0 (all, legacy fallback)... Building HUnit-1.3.1.2 (lib)... Building xml-1.3.14 (all, legacy fallback)... Building regex-posix-0.95.2 (all, legacy fallback)... Preprocessing library for parsec-3.1.12.. Building library for parsec-3.1.12.. [ 1 of 25] Compiling Text.Parsec.Pos ( Text/Parsec/Pos.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/Parsec/Pos.o ) [ 2 of 25] Compiling Text.Parsec.Error ( Text/Parsec/Error.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/Parsec/Error.o ) [ 3 of 25] Compiling Text.ParserCombinators.Parsec.Error ( Text/ParserCombinators/Parsec/Error.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/ParserCombinators/Parsec/Error.o ) [ 4 of 25] Compiling Text.Parsec.Prim ( Text/Parsec/Prim.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/Parsec/Prim.o ) [ 5 of 25] Compiling Text.Parsec.Combinator ( Text/Parsec/Combinator.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/Parsec/Combinator.o ) [ 6 of 25] Compiling Text.ParserCombinators.Parsec.Combinator ( Text/ParserCombinators/Parsec/Combinator.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/ParserCombinators/Parsec/Combinator.o ) [ 7 of 25] Compiling Text.Parsec.String ( Text/Parsec/String.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/Parsec/String.o ) [ 8 of 25] Compiling Text.Parsec.ByteString ( Text/Parsec/ByteString.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/Parsec/ByteString.o ) [ 9 of 25] Compiling Text.Parsec.ByteString.Lazy ( Text/Parsec/ByteString/Lazy.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/Parsec/ByteString/Lazy.o ) [10 of 25] Compiling Text.Parsec.Text ( Text/Parsec/Text.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/Parsec/Text.o ) [11 of 25] Compiling Text.Parsec.Text.Lazy ( Text/Parsec/Text/Lazy.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/Parsec/Text/Lazy.o ) [12 of 25] Compiling Text.Parsec.Expr ( Text/Parsec/Expr.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/Parsec/Expr.o ) [13 of 25] Compiling Text.ParserCombinators.Parsec.Prim ( Text/ParserCombinators/Parsec/Prim.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/ParserCombinators/Parsec/Prim.o ) [14 of 25] Compiling Text.ParserCombinators.Parsec.Pos ( Text/ParserCombinators/Parsec/Pos.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/ParserCombinators/Parsec/Pos.o ) [15 of 25] Compiling Text.Parsec.Char ( Text/Parsec/Char.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/Parsec/Char.o ) [16 of 25] Compiling Text.Parsec.Token ( Text/Parsec/Token.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/Parsec/Token.o ) Configuring test-framework-0.8.1.1 (all, legacy fallback)... Building test-framework-0.8.1.1 (all, legacy fallback)... [17 of 25] Compiling Text.ParserCombinators.Parsec.Token ( Text/ParserCombinators/Parsec/Token.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/ParserCombinators/Parsec/Token.o ) [18 of 25] Compiling Text.ParserCombinators.Parsec.Char ( Text/ParserCombinators/Parsec/Char.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/ParserCombinators/Parsec/Char.o ) [19 of 25] Compiling Text.ParserCombinators.Parsec ( Text/ParserCombinators/Parsec.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/ParserCombinators/Parsec.o ) [20 of 25] Compiling Text.ParserCombinators.Parsec.Expr ( Text/ParserCombinators/Parsec/Expr.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/ParserCombinators/Parsec/Expr.o ) [21 of 25] Compiling Text.Parsec ( Text/Parsec.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/Parsec.o ) [22 of 25] Compiling Text.Parsec.Language ( Text/Parsec/Language.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/Parsec/Language.o ) [23 of 25] Compiling Text.ParserCombinators.Parsec.Language ( Text/ParserCombinators/Parsec/Language.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/ParserCombinators/Parsec/Language.o ) [24 of 25] Compiling Text.Parsec.Perm ( Text/Parsec/Perm.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/Parsec/Perm.o ) [25 of 25] Compiling Text.ParserCombinators.Parsec.Perm ( Text/ParserCombinators/Parsec/Perm.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/build/Text/ParserCombinators/Parsec/Perm.o ) Configuring test-framework-hunit-0.3.0.2 (all, legacy fallback)... Building test-framework-hunit-0.3.0.2 (all, legacy fallback)... Configuring test suite 'tests' for parsec-3.1.12.. Preprocessing test suite 'tests' for parsec-3.1.12.. Building test suite 'tests' for parsec-3.1.12.. [1 of 7] Compiling Util ( test/Util.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/c/tests/build/tests/tests-tmp/Util.o ) [2 of 7] Compiling Bugs.Bug35 ( test/Bugs/Bug35.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/c/tests/build/tests/tests-tmp/Bugs/Bug35.o ) [3 of 7] Compiling Bugs.Bug9 ( test/Bugs/Bug9.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/c/tests/build/tests/tests-tmp/Bugs/Bug9.o ) test/Bugs/Bug9.hs:45:9: Warning: Defined but not used: `reservedOp' [4 of 7] Compiling Bugs.Bug6 ( test/Bugs/Bug6.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/c/tests/build/tests/tests-tmp/Bugs/Bug6.o ) [5 of 7] Compiling Bugs.Bug2 ( test/Bugs/Bug2.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/c/tests/build/tests/tests-tmp/Bugs/Bug2.o ) [6 of 7] Compiling Bugs ( test/Bugs.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/c/tests/build/tests/tests-tmp/Bugs.o ) [7 of 7] Compiling Main ( test/Main.hs, /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/c/tests/build/tests/tests-tmp/Main.o ) Linking /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/c/tests/build/tests/tests ... Running 1 test suites... Test suite tests: RUNNING... Bugs: Control Char Parsing (#2): [OK] Look-ahead preserving error location (#6): [OK] Tracing of current position in error message (#9): [OK] Quality of output of Text.Parsec.Token.float (#35): [OK] Test Cases Total Passed 4 4 Failed 0 0 Total 4 4 Test suite tests: PASS Test suite logged to: /tmp/parsec/dist-newstyle/build/x86_64-linux/ghc-7.6.3/parsec-3.1.12/c/tests/test/parsec-3.1.12-tests.log 1 of 1 test suites (1 of 1 test cases) passed. 
[I've written quite extensively about cross compiling macOS to iOS, Android and Raspberry Pi.](https://medium.com/@zw3rk). Especially about the implications with Template Haskell. I would suggest to use a full stage2 compiler for macOS -&gt; macOS and linux -&gt; linux. Even though macOS -&gt; linux should be possible quite similar to how the [raspberry pi cross compiler](https://medium.com/@zw3rk/a-haskell-cross-compiler-for-raspberry-pi-ddd9d41ced94) works. I have not yet tried to target windows, though I believe that it is possible to setup a windows toolchain on linux or macOS. Cross compilation (with TH) will likely work with GHC 8.4+.
Aaah. It seems cross-compilation remains equally troublesome. Is some easier way planned? 
It's true of `:type` in general. 
You picked a good bias. Btw this can be useful, a function where we pin down the representation probe :: forall r. (Representable r, Rep r ~ (Bool, Bool)) =&gt; [Int] -&gt; r [Int] probe = partition' @r @[] (even &amp;&amp;&amp; (&gt; 10)) making `probe` polymorphic in which “partition `Functor`” we want to operate on. This means we can freely jump between various ways of representing our partitions probe :: [Int] -&gt; Compose ((-&gt;) Bool) ((-&gt;) Bool) [Int] probe :: [Int] -&gt; Compose Pair ((-&gt;) Bool) [Int] probe :: [Int] -&gt; Compose ((-&gt;) Bool) Pair [Int] probe :: [Int] -&gt; Compose Pair Pair [Int] probe :: [Int] -&gt; ReaderT Bool Pair [Int] **Edit**: even Day convolutions probe :: [Int] -&gt; Day Pair Pair [Int] 
I started using Yi today. Tip: read the configs of Yi users, like /u/ethercrow, for instance. https://github.com/mrkgnao/yi-config
But your evaluator presumably doesn't evaluate optimally!
This is probably true, but I think the evaluator is mostly important for establishing the expressive power of the restricted lambda-terms
When you write a paper, you are an author. Therefore, someone that reads your paper is a co-author.
Now we only need to figure out the optimal way to evaluate arbitrary terms inside the oracle-free evaluator! (Seriously though, obviously the purpose of this is just to show the oracle-free language is very capable.) 
Hey, this looks really cool and relevant to something I'm working on, so I'll see if I can free up some time in the next few weeks to contribute. I'm working on a custom database and I want to be able to access it from a text-editor (ie in a similar way to Emacs' SQL Mode, or the ComintMode that it's based on). Since I don't want to murk about with lisp, I wanted to eventually use Yi for this purpose. From your knowledge of the codebase, how difficult do you think this modification would be? I have an additional use-case for this, which might be a bit tricky. Suppose I evaluate the query "list the columns 'name, age' of table 'person', which outputs the following table: name | age --------|-------- John | 22 Bob | 36 And then I change the table, say by changing John's age to 25 name | age --------|-------- John | 25 Bob | 36 Then I want to be able to run a command that'll parse the table into a data-type, turn it into a new query, and send that back to the database, updating it. Would this sort of use-case play well with Yi? 
I have implemented it just recently. Take a look: [T1005/Main.hs](https://github.com/nlinker/playground/blob/7b8fb33b980bd7158dc4e8a857b0fb9d07ba4f87/problem-set/src/T1005/Main.hs) I have to use MinHeap implementation in the same file, because the environment to run the file is hostile and doesn't allow third party libraries, but in normal environments you can use some good heap implementation, e.g. https://hackage.haskell.org/package/PSQueue
Really cool news, thanks for your continued work on Yi! When I saw Modularization, my first thought was of [Backpack](https://ghc.haskell.org/trac/ghc/wiki/Backpack), which was recently added to GHC. I don't know if you're aware of it, but it might help with the continued modularization! 
The [haddocks for the `compact` library](http://hackage.haskell.org/package/compact) seem to have all the information you really need in order to use the library, including a tutorial. And the research paper is pretty straightforward.
Added step by step instructions https://github.com/commercialhaskell/stack/issues/2822#issuecomment-318892816
I know that's not cross compiling but that can help. For standard environment like this you use Virtual Box and install all the OS you need.
Thanks. It's awesome!
&gt; I prefer to stay as far away from it as possible, thank you very much. :) Agreed :) &gt; That being said, I've had a lot of issues with frameworks doing this to the point that they force you to write raw JavaScript for every edge case that isn't covered by the framework. So it's nice to have a FFI to as fallback, even though I resent the very existence of the problem. Found this true of elm's ports, didn't enjoy that part either. &gt; And I don't see how total coverage of JS functionality is that hard of a problem to solve. What else is there beyond basic programming constructs and a bridge to the browser's API? There's a lot of stuff out there. If you want type-safe browser APIs, ghcjs-dom is a good example of just how many there are. And they all have varying browser support (https://caniuse.com/#search=websql). But agreed in that "most commonly used" could be supported. &gt; I don't buy into the JS community's usage of the word isomorphic, so I'm going to need some additional clarification here. Does the virtual DOM exist on the client or the server or both? I know Elm is all client-side, but you mention the server sending DOM related things to the client, so I'm assuming it must hold some representation of the DOM. The virtual DOM exists on both client and server (on client as newtype over js, that gets turned into a js tree, and diffed / patched -- on server as a rose tree, but there is no DOM on server so there is nothing to diff / patch against -- so we just serialize to ByteString and send to client). In regards to the term, both rose tree structures (the one in js in browser, and one on server) are isomorphic to each other, we can get from one to the other, but I doubt there is any kind of rigorous proof of this. JS community seems to have moved to the term "universal". &gt; And as for demonstrating the idea, a visualization of how user events are routed through the application would be useful. Better docs are in the pipeline (i.e. *UserGuide.md*) &gt; Pre-optimizations, how many concurrent users is a medium-sized Miso app backed by an HTTP API running on the same server capable of supporting? And what are the bottlenecks? Hard to know how best to answer this, but as long as each user has their own browser, the bottleneck should be the server, and it's ability to hold websocket connections.
[Here you go](https://docs.google.com/presentation/d/1PRDX08tzbrG0SghKw4mfymUaZfXmsGUpIeJpGQ71pJw/)
Gracias, muy buena!
We can shoehorn this into a boring real world problem where we categorize products in three ways: data Category = Good | Broken | Expired data Products a = Products { good :: a , broken :: a , expired :: a } instance Representable Products where type Rep Products = Category but let's instead model Hogwarts data Hogwarts a = Hogwarts { gryffindor :: a , hufflepuff :: a , ravenclaw :: a , slytherin :: a } To partition incoming students into their respective houses we need a `Representable Hogwarts` instance where the `Rep Hogwarts` is the `House` data House = Gryffindor | Hufflepuff | Ravenclaw | Slytherin deriving Eq instance Representable Hogwarts where type Rep Hogwarts = House index :: Hogwarts a -&gt; (House -&gt; a) index Hogwarts{..} = \case Gryffindor -&gt; gryffindor Hufflepuff -&gt; hufflepuff Ravenclaw -&gt; ravenclaw Slytherin -&gt; slytherin tabulate :: (House -&gt; a) -&gt; Hogwarts a tabulate create = Hogwarts { gryffindor = create Gryffindor , hufflepuff = create Hufflepuff , ravenclaw = create Ravenclaw , slytherin = create Slytherin } So if we want to sort the students we only need to supply how to sort a single student `a -&gt; House` sortingHat :: (a -&gt; House) -&gt; ([a] -&gt; Hogwarts [a]) sortingHat = partition' @Hogwarts @[] sortStudent :: String -&gt; House sortStudent "Harry" = Gryffindor sortStudent "Hermione" = Gryffindor sortStudent "Snake" = Slytherin &gt;&gt; sortingHat sortStudent ["Snake", "Harry", "Hermione"] Hogwarts {gryffindor = ["Harry","Hermione"], hufflepuff = [], ravenclaw = [], slytherin = ["Snake"]}
You may find this of interest: https://tweag.github.io/HaskellR/.
I heard a different version of that. "What do you call the readers of a journal on category theory? Co-authors." (It's two jokes in one!)
No, that's just GHC asking for 1TB of virtual memory it doesn't end up using. The same will be true if you just run `ghci`. It doesn't put any memory pressure on modern operating systems anyway.
We used to pass callbacks for all our effect mocking at work, but it quickly turned into a big mess passing around all the arguments everywhere, guessing the order etc, MTL typeclasses seem like a much nicer solution to the problem. Create a typeclass with all the operations you want, and then just pass it as a constraint instead. Then initialise it to whatever you want in your tests.
I’ve often been a little mystified by Clojure’s notion of transducers, since they mostly just feel like partial application and function composition combined with lazy streams to me. In Haskell, the equivalent seems to simply be the `Traversable` typeclass. I realize the blog post is about Idris, but this is /r/haskell, after all, so I feel alright in asking a Haskell-centric question: is there something special about transducers that makes them meaningfully different from `Functor` / `Foldable` / `Traversable`? If so, can someone explain it to me?
If you want to do something without Template Haskell: 1. Get a C cross-compiler for the platform you want to target 2. Follow the instructions [here](https://ghc.haskell.org/trac/ghc/wiki/Building/CrossCompiling) to build GHC. If you want to cross-compile Template Haskell, it's a huge pain in the butt.
It's definitely possible, but AFAIK there is no framework for interactive buffers. There is a dired-like file browser so interactivity is not impossible. I have also been thinking about a way to open GHCi like prompts in Yi. That would be straightforward if you know your way around the code. The editing of records is kind of like the renaming of files in the dired-like file browser. So that is also possible. With intero I am started with just a small piece of code that runs the intero server process in the background and now I'm slowly adding functionality to it. That strategy is working quite well for me.
Programming languages are ways of thinking and talking about problems. If you learn an idea in English, that doesn't make you unable to express the idea in French, though it might be a little more clumsy, and some of the idioms may need to be translated. Learn stuff in R, or Python, or whatever the best statistics material is presenting it in. Then translate that knowledge to your FP language of choice.
It's worth noting that &gt; We can make this abstract by taking concrete terms and making them function parameters. The literal definition of lambda abstraction! Basically *is* the approach advocated by `mtl`. What's done in this post is essentially explicitly passing a dictionary of methods, while in `mtl` the dictionary is within the type class. This is also closer to how I think `mtl` should be used. We *shouldn't* have classes with things like `getDbHandle :: m DbHandle`, but we should have classes that describe the API of *your* programs. Then you just implement that in terms of a database connection, an in memory database, a pure value, etc.
I have similar experience with reading large code bases and struggling to find out where function comes from. But where I work we use a bit different approach to this problem with seems to work just as good. I would be interested to hear your opinion on the alternative. Namely, why not make the editor solve problem of finding out where function comes from rather than solving problem of automatically managing import lists? Isn't it true that what function does is the motivation for introducing import lists in the first place? I think that jumping to function definition is ultimately what's needed when one faces unfamiliar function call. This will give both function's signature and, with some luck, its documentation. Currently problem of jumping to definition can be solved pretty good using tags. Some tag generators can index large code bases pretty fast so it doesn't cause significant delay. Yet tags, with some editor support for disambiguating similarly named functions, are actually pretty good at locating functions by their name.
I do see use for explicit import lists in order to help understanding where function comes from. But I think they just slow you down too much. Where I work we use a bit different approach to the problem of finding out where function comes from. Namely, we use tags to navigate our code base and jump directly to the definition. This is pretty fast, gives you function's type signature and, possibly, it's documentation and avoids the need to chase reexports in order to reach the definition site.
&gt; Learn stuff in R, or Python, or whatever the best statistics material is presenting it in. Then translate that knowledge to your FP language of choice. This. 
I think you're right for applications, though I disagree with you for libraries. On the app side, I think we see stuff like `MonadReader` and implement a similarly "low level" class, like `MonadHttp` or `MonadDb`, and then try to mock that. wrong! we want to instead make classes like `SpecificApiService` or `GetSpecificModel`, which are easy to mock. On the library side, it is intractable to try and provide the standard newtype MyEffectT m a = ... class MonadMyEffect m where ... $(generate a million instances) that `mtl` provides. This is where classes like `getDbHandle :: m DbHandle` come in real handy, as they play very well with polymorphic `ReaderT`-like stacks. I usually want to write stuff that interacts with a low level service as essentially myUtilityFunction :: (MonadReader r m, HasSomeThing r, MonadIO m) =&gt; foo -&gt; m bar I'll write an instance for `ReaderT SomeThing` and let the library clients write their own instances for their own app stacks.
Since the subset that works with Lamping's algorithm is turing complete, would it be feasible to implement some intermediate compilation that turned an arbitrary term into an equivalent representation in the lambda subset? If that's the case, then wouldn't Lamping's algorithm take care of ensuring optimal evaluation? (Or is this idea essentially the 'oracle' mentioned in the link?)
Ah that's useful to know, thank you! Would you have a link to the file browser? That makes sense as an approach, I will do likewise. 
Amen!
"We" being people designing or providing feedback about programming languages.
I was just thinking that getting [LSP](https://github.com/Microsoft/language-server-protocol) into Yi would be a force multiplier, as it immediately adds support for all languages having an LSP server. And the haskell one is coming on nicely. See https://github.com/haskell/haskell-ide-engine
Yes and it should be easier to implement and less fragile since it's all Haskell. No friction crossing any boundaries. Alan, let me say I appreciate all the work on LSP, even though it's rough and I couldn't make it work yet. I'm extra happy I didn't have to install NodeJS or anything like that, and once Yi has built-in LSP, it might reduce the effort needed to support more than Haskell.
Join the dataHaskell gitter ask about existing implementations of things that interest you. 
Btw, you can find the historic context (i.e. why it was kept out of `-Wall` at the time) regarding the `-Wincomplete-uni-patterns` warning in [GHC #4905](https://ghc.haskell.org/trac/ghc/ticket/4905)
Btw, GHC has the `-ddump-minimal-imports` facility which gives you a way to figure out which module an identifier came from. This is e.g. used by [`packunused`](http://hackage.haskell.org/package/packunused) in order to detect packages whose modules don't get `import`ed.
I think it is very likely that such a function exists. It would receive a λ-encoded λ-term and convert it to a native graph which includes the oracle machinery. Yet another reason not to have a hardcoded oracle, it can absolutely be implemented on top of the existing language without loss of performance. But, again, I suspect most terms that need the oracle are using resources improperly and should be fixed rather than work sub-optimally...
You will probably need Python and it is quite easy to learn. I think it has advantages over both R and Matlab - it is a good general programming language, and Matlab is mostly used in engineering disciplines and not something that is good to run on a cloud server or computing cluster. For industrial use, I think Scala is good, it has a very pragmatic approach but is also a big and complex language; a bit of a kitchen-sink approach to idioms. As an easy functional language with excellent support for numerics and plotting, [Racket](https://khinsen.wordpress.com/2014/05/) might be a good choice to learn pure FP concepts first time. To know Haskell is without doubt a win but might be a bit much at once. (Personally, I like Clojure a lot, I think it is a great language but by a number of well-motivated technical choices, it is better for programming web applications than for explorative numerical analysis. Going from Racket to Clojure later should be little difficulty though. ) 
I just realized while looking over the code again that the file browser doesn't really work like I thought it did. I think a better and simpler example would be a part of [my interoUses command](https://github.com/yi-editor/yi/blob/e0ac8207f5487865f1a4057f3ef5b036db0a6c54/yi-intero/src/Yi/Intero.hs#L139-L144). In the highlighted code you can see that a buffer is opened (`res` is the result of the query, that would be the a string representation of the table in your case) then also two keys are bound (`KEsc` and `KEnter`). In your case you could bind a key to a function that read the contents of the buffer (for example `readUnitB Document`) and then parsed it back into a table. That table could then be send to the SQL database.
Since `sequence . fmap f = traverse f`, you can simplify it to: \f -&gt; traverse join . traverse f
`Foldable` / `Traversable` are limited to finite containers. They don't cover (possibly effectful) processes. Say, a socket from which you constantly read data. I believe transducers roughly correspond to "functions that transform streams" in Haskell streaming libraries. Something like `Stream (Of a) m r -&gt; Stream (Of b) m r`. One possible difference is that transducers are usually [presented](https://clojure.github.io/clojure/clojure.core-api.html#clojure.core/transduce) as transforming *sinks* (reducing functions) rather than sources, but I think they can transform sources as well. 
&gt; The virtual DOM exists on both client and server (on client as newtype over js, that gets turned into a js tree, and diffed / patched -- on server as a rose tree, but there is no DOM on server so there is nothing to diff / patch against -- so we just serialize to ByteString and send to client). Can DOM events trigger Haskell functions that are run on the server and then update client state? And can they also trigger Haskell GHCJS functions that are run on the client because they only depend on client state? If both, how are local function calls differentiated from remote function calls? I assumed that I'd have to setup a RESTful API to interact with all the state and heavier logic that belongs on the server, just like in Elm. I'd love to hear that I'm wrong in that assumption. But I'd also be concerned about how much session state persists on the server, even if the session state is just a virtual DOM for each active user. Or does the virtual DOM on the server get regenerated with each request, using some representation sent by the client? Edit: I realize my language might be off since I'm coming from an OO background and only just getting started with functional. Hope I'm being clear enough.
I think the clearest way to do this would be to use `do`-notation and pattern matching: example :: Maybe A -&gt; Maybe (String, Int) example m = do A (string, Just int) &lt;- m return (string, int)
That's a fun example! I appreciate all the thought you put into reading my blogs; do you have a twitter or something I can follow? I tend to bounce my ideas off of my followers while I'm writing them up! [@chrislpenner](https://twitter.com/chrislpenner)
Slides here: https://cs-syd.eu/assets/easyspec/public-presentation.pdf
But OSX? Slightly painful to get OSX images. 
Is there any plan so that cross compilation is as simple as its for Go now (1.5+), i.e: just pass target OS and Arch on command line?
This joke is awesome
Why not : Either String (String,Int)? There are ways to do what you want, but maybe you've just chosen the wrong model for your problem? Certainly done that before myself...
This may rely too much on the less-than-intuitively-obvious Functor instance of tuples, but maybe Nothing (sequence . a) x also does the trick here.
[removed]
GHC is very far from being a multi-target compiler. I *think* it would take a pretty large amount of work to make it capable of being multi-target.
Hey, can you explain in detail? The way I understand it, LLVM is a common representation. All we need now, is: LLVM -&gt; Target Platform specific binary. And for cross compilation it would be required to download libraries for other Target platforms. Maybe this is an oversimplification.
/u/angerman can definitely answer this better than I can. But my understanding is that the main obstacles are ld/cc flags, and the primitive packages. GHC does a bit of logic depending on the target platform to determine what flags to give to the cross compiler toolchain, since it's not exactly consistent. Some of these things are configured at build-time of GHC. And primitive packages (base, Cabal, bytestring, template-haskell, etc.) are compiled at the same time as GHC and distributed with it. This expectation and distribution process would have to be reworked.
I've submitted a new ticket here: https://ghc.haskell.org/trac/ghc/ticket/14064
Isn't this just the `ST` monad? You could simply wrap all those primops in `ST` and use do notation
Using lensspeak, `x &gt;&gt;= sequenceOf (_a . _2)`
Parenthesi^/s ))))))
_Resilient Haskell Software_ is in favor of them, it's an interesting read: https://www.gwern.net/Resilient%20Haskell%20Software
If the consumers of your code can be relied on to do implicit imports, adding a new function to your lib becomes a non-breaking change instead of a breaking one. This isn't really "tedious maintenance" like duplicating the same information in multiple places in one project (which is definitely bad), instead it's just being precise. Admittedly being explicit does take some work, but that's OK as long as it's making the code safer (and hopefully the work can be automated). Say your code depends on `FooHtml` and `BarRegex`, and you're using `FooHtml`'s `escape` function. If you update your deps and suddenly `BarRegex` is providing `escape` instead of `FooHtml` this is never going to be what you want. I don't mind boilerplate as long as it's not redundant. EDIT: A much more common situation than I've described here is one where `BarRegex` adds an `escape` function, but `FooHtml` doesn't rename its `escape` at the same time, so the two conflicting names are caught at compile time. That's better than runtime issues, but still isn't great.
This seems like a totally reasonable attitude. We'll get there, Haskell editor tooling is improving fast! . . . thank you to everyone that's been making that happen, by the way =)
This is a really useful article, and deserves to be turned into a long-lived resource instead of just a blog post. If the original author reads this, is this something you'd consider doing? I'm thinking of stuff like incorporating the suggestions from this thread.
Video cuts out at 29:52
I hope that we eventually will end up with a multi-target cross compiler. But that is still quite far out. I'd love to work on this if someone would be able to sponsor it, but there is a lot that still needs to be done. For the time being, I hope that GHC 8.4 will provide a much better cross compilation story (with TH support via an external process on the target).
I've noticed. The recording musts have failed somehow. Sorry about that.
The repo README looks to inform more effectively what this is all about: https://github.com/NorfairKing/easyspec
I think you are talking about LLVM IR, the LLVM intermediate representation. While you can carefully hand roll LLVM IR that might be multi-target capable. The LLVM IR however that GHC generates contains all kinds of hardcoded (most obvious: word size) values and is therefore not really portable across architectures. The GHC LLVM pipeline is essentially `C-- -&gt; LLVM IR -&gt; Assembly -&gt; Mangled Assembly (LLVM Mangler) -&gt; Object Code`. Assuming we would drop the Mangler part (and we will for the foreseeable future not do this; it is more likely going to be expanded), GHC could maybe be taught to produce LLVM Bitcode, which is basically just LLVM IR in a binary format, that is a bit more stable across LLVM versions. You could then use that bitcode and try to retarget it with `llc`. This is what I believe apple hopes to be able to do eventually. If the applications uploaded to the apple store contain architecture independent bitcode, apple could simply retarget (or optimize) an existing app based on the embedded bitcode. Assuming for a moment we had this. This would only equip GHC to produce bitcode, but we would still lack all the tooling around it. All tooling would still need to learn that bitcode is some form of final object code. However if our tooling would know about bitcode, and we could produce architecture independent bitcode, we could likely just retarget the produced bitcode and obtain object code for a different architecture. Interacting with foreign libraries though will open yet another can of worms.
It does not go into implementation details, if that's what you mean :)
A very similar positive/negative position pattern shows up with proxies, but it goes the opposite way. If the proxy is in *negative* position, it should be polymorphic: f :: proxy a -&gt; q If the proxy is in positive position, it generally must be monomorphic (or higher rank, but that's silly): g :: (Proxy a -&gt; q) -&gt; r Edit: Whereas we have absurd :: Void -&gt; void we have reproxy :: proxy a -&gt; Proxy a It's very much an initial vs. terminal thing, I believe.
`containers` has no `HashMap`. That should be `unordered-containers`.
The example is just an example. In real life the records has more fields.
Thanks! Fixing it. I actually need to change mentions of Hashmap to Map.
Without using lens, can I still use `&gt;&gt;=`?
Here it is [@Iceland_jack](https://twitter.com/Iceland_jack), what I do is also spread over comments and [gists](https://gist.github.com/Icelandjack/) (brain dump written for myself, may be interesting). I look forward to other ideas from you
The example is just an example. In real life the record has more field and the value can be Nothing
I'd never thought about that, very interesting!
The funniest thing on the original thread is the branching and extensive discussion of whether the infinite loop joke is funny.
Hey thanks for writing this! I really enjoyed the adventure xD. 
 pure $ last $ sortOn snd (M.toList m) looks awfully fishy for performance. That's O(n log n) with substantial allocation to do what you could do in O(n) with no allocation. I don't know how much it matters in the grand scheme of things, but you can clean it up easily. The simplest approach is probably to use `maximumBy`.
I just tested with GHC 8.2.1. GHC 8.0.2: -O1: 2.94user 0.16system 0:03.11 -O2: 1.84user 0.16system 0:02.00 GHC 8.2.1: -O1: 2.87user 0.05system 0:02.93 -O2: 1.84user 0.06system 0:01.90 Hm, I remember hearing a lot that `-O2` didn't matter. It seems that might not be the case?
&gt; This is a really useful article, and deserves to be turned into a long-lived resource instead of just a blog post. If the original author reads this, is this something you'd consider doing? I would be willing to do this. What did you have in mind? &gt; I'm thinking of stuff like incorporating the suggestions from this thread. This is a good idea. After the comments die down I'll update the article to link to some of the good ones. Are there any comments in particular that you think should be rolled back into the blog post?
This change using GHC 8.2.1 and -O2: - pure $ last $ sortOn snd (M.toList m) + pure . maximumBy (comparing snd) $ M.toList m In terms of runtime bought me roughly 0.03 to 0.05 according to unix time, and in terms of allocation 200,000 or so bytes. In prof summaries: -- pure $ last $ sortOn {- Mon Jul 31 01:29 2017 Time and Allocation Profiling Report (Final) haskell-version +RTS -p -RTS total time = 4.85 secs (4852 ticks @ 1000 us, 1 processor) total alloc = 16,199,891,192 bytes (excludes profiling overheads) COST CENTRE MODULE SRC %time %alloc processFile Main src/Main.hs:(13,1)-(19,39) 40.5 58.8 processFile.m.\ Main src/Main.hs:16:25-134 35.6 36.8 readDecimal_.start Data.ByteString.Lex.Integral src/Data/ByteString/Lex/Integral.hs:(260,5)-(265,32) 11.4 2.1 toInt Main src/Main.hs:10:1-20 2.6 0.0 readDecimal_.loop0 Data.ByteString.Lex.Integral src/Data/ByteString/Lex/Integral.hs:(268,5)-(274,32) 1.6 0.0 processFile.m Main src/Main.hs:(15,7)-(18,17) 1.6 0.0 readDecimal_ Data.ByteString.Lex.Integral src/Data/ByteString/Lex/Integral.hs:(246,1)-(335,59) 1.5 0.0 processFile.m.\.\ Main src/Main.hs:16:97-105 1.3 1.0 readDecimal_.loop3 Data.ByteString.Lex.Integral src/Data/ByteString/Lex/Integral.hs:(292,5)-(298,54) 0.5 1.0 -} -- pure . maximumBy {- Mon Jul 31 01:30 2017 Time and Allocation Profiling Report (Final) haskell-version +RTS -p -RTS total time = 4.65 secs (4648 ticks @ 1000 us, 1 processor) total alloc = 16,199,608,408 bytes (excludes profiling overheads) COST CENTRE MODULE SRC %time %alloc processFile Main src/Main.hs:(13,1)-(20,47) 38.9 58.8 processFile.m.\ Main src/Main.hs:16:25-134 36.8 36.8 readDecimal_.start Data.ByteString.Lex.Integral src/Data/ByteString/Lex/Integral.hs:(260,5)-(265,32) 12.1 2.1 toInt Main src/Main.hs:10:1-20 2.9 0.0 processFile.m Main src/Main.hs:(15,7)-(18,17) 1.8 0.0 processFile.m.\.\ Main src/Main.hs:16:97-105 1.4 1.0 readDecimal_.loop0 Data.ByteString.Lex.Integral src/Data/ByteString/Lex/Integral.hs:(268,5)-(274,32) 1.3 0.0 readDecimal_ Data.ByteString.Lex.Integral src/Data/ByteString/Lex/Integral.hs:(246,1)-(335,59) 1.2 0.0 readDecimal_.loop3 Data.ByteString.Lex.Integral src/Data/ByteString/Lex/Integral.hs:(292,5)-(298,54) 0.3 1.0 -}
I'm glad you enjoyed it!
Great post
Does Stack have an easy way to build all libraries with the LLVM backend? I know in previous builds, LLVM has not usually been noticeably faster. But I've heard that join points in 8.2 work a lot better with LLVM. It'd be nice to have a benchmark for stuff like this comparing it.
Why do we need objects to be multi target? Why can't we just recompile everything for every target, but using the one compiler rather than having to rebuild GHC? Seems like making a GHC that works for many platforms is a lot easier than making a GHC whose outputs work for many platforms. Also, is there any detailed resource on the mangler? Is it different for each target? Why would it expand?
Yes but in the `Maybe` monad, a failed pattern match in `do` notation does not produce a runtime error, it produces `Nothing`. This is because partial pattern matching in `do` notation returns `fail` rather than `error`, and the `Monad` instance for `Maybe` has `fail _ = Nothing`. Personally, I think it's an abomination the `do` notation is the only place in the language where partial pattern matches are treated differently, but it can simplify things sometimes.
So a Google found: https://github.com/commercialhaskell/stack/issues/1742 And in that: stack --nix-packages llvm_35 build --ghc-options=-fllvm So I'm willing to bet that if llvm is the version expected and in the expected location that even without nix you could just run: stack build --ghc-options=-fllvm
I *think* you want llvm_37 (maybe 39). And I'm not sure if that will recompile all the dependencies with that GHC option, which would be necessary, since a lot of the work that might have gotten optimized is hidden away in those libraries.
Can `traverse f . traverse g` be simplified further using e.g. `Compose`?
&gt; Can DOM events trigger Haskell functions that are run on the server and then update client state? And can they also trigger Haskell GHCJS functions that are run on the client because they only depend on client state? If both, how are local function calls differentiate from remote function calls? DOM events are routed to pure Haskell functions (miso's event handlers) which create actions that update the model, causing a redraw. These actions can optionally construct effects that will be evaluated asynchronously. In an effect you can do things like SSE, XHR, or use websockets. So to answer your question, yes, you can update the model on both client and server using a variety of protocols / transport mechanisms. If you wanted to, you could send the entire model back and forth from the client to server using a websocket. This might be ideal for realtime communication scenarios. &gt; I assumed that I'd have to setup a RESTful API to interact with all the state and heavier logic that belongs on the server, just like in Elm. I'd love to hear that I'm wrong in that assumption. But I'd also be concerned about how much session state persists on the server, even if the session state is just a virtual DOM for each active user. If you'd like, you don't need to persist any state on the server, it can all live on the client. &gt; Or does the virtual DOM on the server get regenerated with each request, using some representation sent by the client? W/ isomorphic js, it only gets sent on the first page request, all subsequent navigation uses History API. If the user refreshes her browser, it will re-render from that point and send the new page down. 
`forall a . a` and `Void` are isomorphic -- one may always be used instead of the others. What this blog post is about is: in which case can a use of `forall a . a` (for example, `IO (forall a . a)`, or `IO (forall a . a) -&gt; IO b` be turned into prenex quantification (moving the forall quantifier at the root of the whole type) to avoid having to use a language extension? The answer is that this is valid when the `forall a .` quantifier occurs in positive/covariant/mappable position. Indeed, if `F` is a functor, you can always turn a `t : F (forall a . tau)` into `fmap id t : forall a. F tau` or, more precisely in System F notation, `Λa. fmap (\x -&gt; x[a]) t : forall a . F tau`. What I didn't see in the post is justification for why using `forall a . a` would be better than `Void`. It is "worse" when the occurrence is not positive, by the metric that it requires enabling a language extension. It seems that the justification for using it at all was to "avoid having to use `absurd`", but one could argue that (1) `absurd` actually improves code readability and (2) in the post itself, the examples kept using `absurd` even when they did not need to -- `withSideThread` could have just `Left x -&gt; x` if `x` can be instantiated to take any type. It would be nicer if Haskell had some notion of absurd patterns, like Agda's [absurd patterns](http://agda.readthedocs.io/en/v2.5.2/language/function-definitions.html#absurd-patterns) or OCaml's [refutation clauses](https://caml.inria.fr/pub/docs/manual-ocaml/extn.html#sec241) which would allow writing the code in a nicer way than explicitly using `absurd`. Using `{}` for an Agda-style absurd pattern (a pattern that matches on a type without constructors; when it is used in a clause, no right-hand-side is needed) Left {} Right x -&gt; x (in OCaml this would be written `Left _ -&gt; .`, with an explicitly empty right-hand-side.) Note in particular that `{}` should be a match-failure error if there are valid head constructors at this type (although you would hope that the type-checker prevents you from writing it in this case), so in particular it is clear that it forces the `Left` argument. Matching `Left undefined` should diverge, and this pattern matching is exhaustive.
I always forget that.
The justification was exactly what you quoted: avoiding having to use `absurd`. I didn't want to get too philosophical in the post, but you're right (and a reviewer of the post pointed this out to me as well): it's fair to say that forcing use of `absurd` could be considered a good thing. So let me put it this way: * I personally think using the type variable where appropriate, and thereby avoiding forcing usage of `absurd`, is a good thing * People can definitely disagree on this point * Nonetheless, the blog post points out when you are able to avoid usage of the concrete `Void` datatype, which some people have expressed confusion about
I don't think so, the `Traversable` instance for `Compose` uses `sequence . fmap sequence`, and not their direct composition. Also, `sequenceA . sequenceA` has type: (Traversable f, Traversable g, Applicative f, Applicative g) =&gt; f (g a) -&gt; f (g a) so it's hard to see it going through `Compose`.
After reading all your answers my best shot is `(sequence . a =&lt;&lt;)`
I would also love an answer to this. 
For an even simpler example, consider `()` as the categorical dual of `Void`: putStrLn :: String -&gt; IO () -- () in positive position const :: a -&gt; b -&gt; a -- b in negative position
I develop on MacOS, so I use Docker to compile linux binaries and my own machine to produce the MacOS executable. If I didn't own a Mac, I would use Travis to build the binaries
And unit :: unit -&gt; () as [/u/benjaminhodgson hints](https://www.reddit.com/r/haskell/comments/6qmcsd/to_void_or_to_void/dkyidns/).
What do you mean?
Can you use `Identity` to get around the `fmap`?
It's probably a very good idea to forget that!
Interesting. No discussion whatsoever. So is the historical reason "because one person once asked for it"?
Why is higher rank polymorphic proxy silly? :)
It's quite high-powered technology to do something that can be done simply with a monomorphic type.
Here's my one word answer: benchmark. All of the approaches are equivalent, and the choice between them comes down to performance. Others can give you guidelines and guesses as to what will be fastest in different circumstances, but ultimately the only real answer will come from testing out the alternatives and choosing the fastest. Recommendation: keep your data types abstract so you can change the internal representation without affecting your users. Here are my guesses from my own performance experience: * Test out CPS and see if it speeds things up. I never had luck with that in conduit, but whether it speeds things up or not seems to almost be a crap shoot * If codensity doesn't hurt performance in the common case, use it. Its purpose is to help users who may use monadic binding in the wrong association, and you may as well protect against that quadratic case. IME, the constant factor overhead of providing codensity is negligible, but you should confirm that before subjecting everyone to it. * While reflection without remorse is certainly a nice approach, I didn't find any cases within the conduit codebase where it was necessary, and it contains a significant performance overhead from my understanding. Conduit is always able to either fully consume the `Pipe` value, or immediately turn it back into CPS after performing a transformation on it. To use the dlist analogy, this would be like a case where every operation on a dlist either turned it into a list and consumed it, or turned it into a list, mapped over the list, and the mapping function converted back into a dlist at the same time. Again, just my own guesses, you'll need to benchmark to be certain!
Yes, that's what it looks like... ...but maybe there was some additional discussion not documented on the Trac ticket (IRC, emails, phone-calls or other in-person communication, ...), but it's difficult to figure this out 7 years later. And just because it was decided 7 years ago to keep it out of `-Wall`, it doesn't mean we can't re-evaluate that decision now, based on additional experience/information that has been gained since then.
In this case you compose `f` with `Identity` and finish with something isomorphic to `f`, which kinda kills the purpose.
Pretty vague question, but I love to see (in no particular order): * Open source contributions * Conference talks/blog posts * Non-Haskell skills (devops, web dev, data analytics, etc) The last bullet is especially important. Someone who knows the basics of Haskell and has a specific domain skill can be _extremely_ valuable on a team.
You should not need the `toList`, as maps are foldable.
A related improvement would be to exclude modules in the same package from it, since you normally only care about having explicit imports for external dependencies.
I agree with you on editor features. What we want eventually is to increase our productivity by understanding code faster. Go to definition, especially relevant with going to proper instance of polymorphic function. Show details of functions in context inside some hover (like type, documentation, implementation, etc.). But as author said, we not always work with code from editors. Sometimes from Github. Yes, my dream is to have support of all IDE-like features everywhere (with sensible tradeoffs of course). But now it's not really possible... And we live with what we have and now should do anything possible to make code more readable. `qualified` imports are even better for code readability and understandability than explicit import lists. Because they provide even more context information. But they look ugly especially with operators... Indexing code with some tags and using those tags for features like type-directed autocompletion, go to definition is what basically every editor does. Intellij IDEA does this very well and fast, their indexing algorithms are really mature. Also, having explicit import lists is not good only for readability but there exist other reasons to write import lists explicitly. See this style guide for example: https://github.com/tibbe/haskell-style-guide/blob/master/haskell-style.md#imports
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [tibbe/haskell-style-guide/.../**haskell-style.md#imports** (master → 0d4fea6)](https://github.com/tibbe/haskell-style-guide/blob/0d4fea64a87195192faa376696ea094d1aed4baa/haskell-style.md#imports) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dkym5bo.)^.
Isn't the output of the go benchmark tool telling that each run of the loop takes 252327565 nano seconds? which is roughly 0,25 seconds. If I'm reading that right, there is still some more work to do before claiming victory over the hand optimised go code.
Right! I got carried away with the LLVM as universal IR idea. If you look at [D3352](https://phabricator.haskell.org/D3352), this is pretty much what I'm trying to do (slowly), to abstract out all target dependency such that they become a function of the target. And then GHC can dispatch based on the target. A large chunk that's still missing though are the configure generated target dependency includes. Now why would I prefer the target independent IR? Simply because that would allow GHC to do the compilation once, and only once for multiple targets. Recompiling for each target would result in doing the almost identical work (typchecking, ...), multiple times over the same source code. Regarding the [LLVM Mangler](https://github.com/ghc/ghc/blob/master/compiler/llvmGen/LlvmMangler.hs), I believe the source is the best to read up on. In the discussion of D3352, some of the reasons why the Mangler might be extended for Proc Point Splitting in the mean time.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [ghc/ghc/.../**LlvmMangler.hs** (master → 49e334c)](https://github.com/ghc/ghc/blob/49e334c8ea98cd5ecc81cfe10827538182815723/compiler/llvmGen/LlvmMangler.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dkyn8cr.)^.
Looks like it. See: https://www.reddit.com/r/programming/comments/6qmhuy/faster_command_line_tools_with_haskell/dkym7vl/
It's a bit weird that you're measuring the Go code from within Go (using a benchmark) but you're measuring the Haskell code outside of Haskell (with `time`). I would use something like [bench](https://github.com/Gabriel439/bench) to make sure that you're measuring the same thing for both programs. 
I was being lazy and it ended up biting me. I also didn't want to put a criterion primer in the post since it was aimed at a larger audience and was already getting pretty long.
Yeah :/ &gt; Retraction/Update : The Haskell program at the end of this article was not faster. Friends don’t let friends benchmark after dark. I seem to have made a benchmarking mistake and measured the total time of the benchmark in the Go program vs one iteration of the Haskell program. I was in a hurry for this entire post and rushed through a few pieces of it due to time constraints. If I’d had more time, I would have written a Criterion benchmark for both the Go and Haskell program. In the future I think I’ll do so to reduce the possibility of mistakes like this. - https://codygman.github.io/posts/2017-07-30-faster-command-line-tools-with-haskell.html
However, I had at least one more trick up my sleeve to speed things up: Use judy-arrays I'll see about writing an updated version that does that. Perhaps one that goes into even more detail about why the Haskell version isn't as fast.
What I was missing is the Template Haskell part, now I can understand better what you are saying! Another question... Can I type-check for example this: `let code = 1 in [query|select code from user where code = $code|]` will `reify` have enough information to help me check the types during compilation?
I love it. Reminds me of the C++ [function pointer trick](https://www.youtube.com/watch?v=6eX9gPithBo) int add(int x, int y) { return x + y; } int main() { int (* fp)(int, int) = add; int three = ( ******************************************************************************** ******************************************************************************** ***/***************************************************************************/ ***/* NOTICE: WE ARE CALLING A FUNCTION VIA A FUNCTION POINTER HERE */ ***/***************************************************************************/ ******************************************************************************** ******************************************************************************** fp)(1, 2); }
Usually when you want to define `foo = (&gt;&gt;= asd dsa)`, I'd say it's better to define `foo = asd dsa` and use `&gt;&gt;=` or do notation manually at the use site.
You can't write an expression of type `ST s Int#`. I'm interested in finding a good coding style for guaranteed zero-allocation functions, and if I'm using boxed ints, I don't have that guarantee.
I believe these are big factors that got me my first Haskell jobs.
Can I ask, but where did you find out about your Haskell job? I'm looking for Haskell (or really any other functional languages) jobs in the UK, and can't find any other than some in London.
Yep. It means King/Queen. ("nol" is a combining form of "nobli" which means noble (as in a hereditary aristocracy), "rai" means most.) I was fairly into Lojban junior and senior year of highschool, but don't have a way to play around with it the way I do math and CS, so never got past knowing 20-30 words and a little grammar.
My first Haskell internship was discovered via Reddit. My first Haskell job was found on Twitter, though there were a number of other companies I was talking to. I was willing to relocate for the job (first one out of college), which expanded my possibilities a lot.
Yes, but that might require messing up the import list that's already there. Or at the very least you'd need to remember the order of the existing import list or you'd see a random shuffling of your import lists on every commit.
Nice. I never got fluent, but I did read _The Complete Lojban Language_ front to back and followed the more recent grammar standardization attempts on the mailing list.
Reminds me of Ed's [lens video](https://www.youtube.com/watch?v=cefnmjtAolY).
Practically, ghci will go cry in a corner and take all your memory with it, if you try very many more than that.
"practically"
In terms of primitives we now have a decent portable C implementation of blake2b and blake2s. In terms of internal changes we have 1. Windows support (thanks mistuke) 2. The chacha20 based library prg is now hardened with "fast key erasure" technique as described in https://blog.cr.yp.to/20170723-random.html and implemented in OpenBSD/NetBSD. 3. Some work toward liquid haskell integration 4. Some work towards cpu capability checks at runtime. Useful for selecting the appropriate implementation at runtime. As always raaz is highly experimental still. Use it at your own risk.
Judging from the upvotes, it seems like your suggestion has a lot of support. If you want to push this further, I'd recommend to submit this via https://github.com/ghc-proposals/ghc-proposals, which is the designated process for getting such user-visible changes passed (and the README specifically mentions changes to `-Wall` as being covered by the GHC proposals process).
But what if the package WOULDN'T work perfectly well with said simple change, and instead would have runtime behavior changes not caught by the compiler. There could potentially be pretty disastrous consequences if something like that happened.
Cool colours on the Hackage page.
And a lot of packages were blocked from the build plan in order to get here. Many but possibly not all maintainers have been notified at this point. We will continue to push out communications about this and we encourage the Haskell ecosystem to upgrade! If you notice that some of your favorite packages are missing, then start opening PRs and/or checking for issues on their issue tracker.
Yep. I suspect that most non-trivial software will fail to build fully with this. But figuring out how come, and alerting the package authors, is part of the fun (and an integral part of the open-source social contract)! So, let's start breaking some builds!
for example atomic-primops does not compile due to the upper limit on base which is pretty artificial. since nobody can predict the future. Can we have a --ignore-upper-versions flag ?
&gt; I would be willing to do this. What did you have in mind? Totally up to you. The difference between a "blog post" and a "resource/article" is mostly in attitude. I think the main things would be: 1) put a note on there about when the article was last updated (so people know it's a living work), and 2) put a note on there that you're open to suggestions, no matter how late after the original article was published. This way people don't wonder if you want to hear from them if it's a year later and they have a suggestion they think might be helpful. If you wanted to go all the way, you could turn it into a repo of its own like State of the Haskell Ecosystem or WIWIKWLH. That's nice because you get issues and PRs, but this is a much smaller project so it's not at all necessary. Really I'm just excited to have a reference for what warnings it's best practices to turn on. This is really nice, because while I'm sure this will keep getting discussed, at least new discussions can start with this article which should keep them from repeating themselves so much. So thank you! &gt; Are there any comments in particular that you think should be rolled back into the blog post? Not in this blog post, but after our twitter discussion about `-Wmissing-import-lists` it might be nice to have two different recommendations at the top of the article: one for people who use `NoImplicitPrelude` with an alternate Prelude and one for those who don't. I think alternate Preludes are going to keep becoming more common, and you want your main suggestion to work for everyone.
This will give the OCamlers one less thing to give us a hard time about: https://blogs.janestreet.com/what-do-haskellers-have-against-exhaustiveness/
stack has allow-newer
Is is possible to see why a specific package has been removed? For example, list-t is gone yet it builds fine.
I always forget how positive and negative position work, and found this to be a great explanation: https://www.reddit.com/r/haskell/comments/1vc0mp/whats_up_with_contravariant/cer11jw/
I am not an expert in the topic, but my thoughts: - CS degree or someone who has taken some CS courses (Coursera or local university) and cares about mastering fundamentals - A few pull requests adding an interesting feature to a Haskell library (not necessarily accepted). Even better if the PR is to stack, cabal, hackage, stackage, intero, hlint or some other internal tooling, as that means the person will be more familiar with basic Haskell tooling - Experience building the type of software my shop is working on (typically one of Compilers/DSLs, REST APIs, distributed consensus, crypto, ...) - Be able to passionately talk about Haskell, possibly have attended a conference or regularly attends local meetup - Speed and able to dive deep in programming interview, or mature, well-reasoned tradeoffs in take home programming samples - Demonstration of being able to dive deep into some complex part of Haskell, and apply the topic, through a blog post or programming sample in gist/github 
...while cabal has both, [`--allow-newer`](http://cabal.readthedocs.io/en/latest/nix-local-build.html#cfg-field-allow-newer) and [`--allow-older`](http://cabal.readthedocs.io/en/latest/nix-local-build.html#cfg-field-allow-older) with a richer and more fine-grained syntax.
You can check the build-constraints.yaml file of the [stackage repository](https://github.com/fpco/stackage). In this case there's just a comment that says GHC 8.2.1, which suggests it was bounds on `base` or one of the other libs that ships with ghc (`time`, `process`, etc.). Note that list-t builds fine [as of today](http://hackage.haskell.org/package/list-t-1.0.0.1). Prior to this release, there was a constraint of base &lt; 4.10. So it could probably be safely added back into the build plan now that it has been fixed.
TIL! Cool. Stack's purpose for allow-newer is mostly just to say "ignore the constraints on the package, I know what I'm doing", because most of the time you aren't using stack to calculate a build plan, in the way that you are when using cabal.
https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/Backends/LLVM/Installing ghc 8.0 -&gt; llvm 3.7 ghc 8.2 -&gt; llvm 3.9 ghc head -&gt; llvm 4.0
I didn't know this, either. That's actually pretty nifty! 
These days most deployments use immutable infrastructure in some cloud environment, so spinning up the previous version, dumping in a portable format, and loading into the next version is a relatively trivial operation.
I probably wont use `(sequence . a =&lt;&lt;)` but in my case`sequence . a =&lt;&lt; x`
&gt; Many but possibly not all maintainers have been notified at this point. In [the issue for time-1.8.0.2](https://github.com/fpco/stackage/issues/2678) some maintainers that you intended to notify were affected by GitHub's rate limiting: The first @-mentioned users are in bold font, but several at the bottom aren't, starting with @entropia. I also wish you would automatically re-add those packages that have been excluded transitively, for example `threepenny-gui` which is excluded now because of its dependency on `snap-server`. **EDIT:** [The issue for base-4.10](https://github.com/fpco/stackage/issues/2670) also contains a lot of unsuccessful notifications.
Indeed. But Haskell programmers rarely write functions that ignore an argument *and* it's type--that's just too boring (well, they do it to make a "convenience" function, but whatever). But they often write functions that take an argument only for its type; that makes `proxy`/`Proxy` more practically important, I think.
I just go `import MyModule -- (stuff, things)`
&gt; Learn stuff in R, or Python, or whatever the best statistics material is presenting it in. Then translate that knowledge to your FP language of choice. This is true. I did the same when doing Numerical Methods.
You can use explicit braces, if you find that any better. case primOpA# s1 x of (# s2, y #) -&gt; { case primOpB s2 y x of (# s3, z #) -&gt; { case primOpC s3 z of (# s, w #) -&gt; { ... }}}
I've seen people do this before, but having to explicitly close all those curly braces is pretty unfortunate. Possibly, I could find a vim plugin for counting the number of unmatched braces before a certain point in the document. It seems like the best solution available, but it's still not as good as what I wish could be done.
Rainbow parens and/or mashing `%` to see where you're at work well for the editor part.
I recommend filing an issue on haskell-vim-now's github page.
See [#14064](https://ghc.haskell.org/trac/ghc/ticket/14064).
Also, `id id id ` type checks with any number of ids.
I love this XD Guys announce new cryptographic library and you complement the colours on Hackage page :D
If it works it's not a trick. If it works and isn't undefined behaviour it's magic!
What do you mean? VRay is written in Haskell, or the chaosgroup website is written in Haskell?
I think VRay is written in Haskell. Here's a talk on it in Bulgarian: https://www.youtube.com/watch?v=c4rO9r5I0Ow The description Google translates to "Why would anyone choose Haskell for his project? Why Haskell was the chosen language for V-Ray Bench? How Haskell Improves the V-Ray Bench Development Process? Daniel will answer these questions for 40 minutes, showing real examples from the project and talking about the various problems that have arisen during the work."
...for some value of "sensible".
No thanks: sign-in required to download, no source-code link, absolutely nothing of interest to me here. Thanks for wasting my time.
Thanks for the heads up! I had the same problem, reinstalled gcc5 from the ports (could not wait for the package to be updated) and the error went away.
My criterion primer would probably be "here's some benchmarking code! I'm gonna pull a hand-wave at this point and skip to the..."
But using Void is strictly more type safe, right? As you said, there's no chance that the function can change the return type and that my code still compiles.
It got me about .2 seconds: cody@zentop:~/faster-command-line-tools-with-haskell/haskell-version$ stack exec -- time haskell-version (2006,22569011) 1.76user 0.04system 0:01.81elapsed 99%CPU (0avgtext+0avgdata 216108maxresident)k 0inputs+0outputs (0major+6324minor)pagefaults 0swaps Edit: Here's the truncated prof cody@zentop:~/faster-command-line-tools-with-haskell/haskell-version$ cat haskell-version.prof Mon Jul 31 18:44 2017 Time and Allocation Profiling Report (Final) haskell-version +RTS -p -RTS total time = 4.71 secs (4705 ticks @ 1000 us, 1 processor) total alloc = 16,199,608,408 bytes (excludes profiling overheads) COST CENTRE MODULE SRC %time %alloc processFile Main src/Main.hs:(13,1)-(20,47) 38.4 58.8 processFile.m.\ Main src/Main.hs:16:25-134 36.5 36.8 readDecimal_.start Data.ByteString.Lex.Integral src/Data/ByteString/Lex/Integral.hs:(260,5)-(265,32) 12.1 2.1 toInt Main src/Main.hs:10:1-20 2.6 0.0 processFile.m Main src/Main.hs:(15,7)-(18,17) 1.8 0.0 processFile.m.\.\ Main src/Main.hs:16:97-105 1.6 1.0 readDecimal_ Data.ByteString.Lex.Integral src/Data/ByteString/Lex/Integral.hs:(246,1)-(335,59) 1.4 0.0 processFile.m.\.key Main src/Main.hs:16:29-66 1.2 0.0 readDecimal_.loop0 Data.ByteString.Lex.Integral src/Data/ByteString/Lex/Integral.hs:(268,5)-(274,32) 1.2 0.0 readDecimal_.loop3 Data.ByteString.Lex.Integral src/Data/ByteString/Lex/Integral.hs:(292,5)-(298,54) 0.5 1.0 
Thanks. Btw, did just passing `--ghc-options=-fllvm` successfully recompile dependencies? Or was there more to it than that?
Hm, actually I'm not convinced that my libraries were compiled correctly after changing to use llvm. Let me retry. Edit: I wasn't using `apply-ghc-options: everything` with stack, which is necessary to compile libraries with `-O2`.
 flip flip flip = flip flip flip flip fmap fmap fmap fmap fmap fmap = fmap fmap fmap fmap fmap fmap fmap fmap fmap fmap These are the earliest fixed points of each
&gt; I also wish you would automatically re-add those packages that have been excluded transitively We try to do this by hand when we can, but it's still not automated. I think to accomplish this we'd need to enhance the format of build-constraints.yaml to record machine-readable reasons for why we disabled a package, so that a tool could make recommendations to re-enable stuff as the build plan changes over time.
I think just the existence of the project is interesting. Haskell is somewhat rare in industry and very rare in end-user products, so this is a nice addition to the list of examples for people who ask if Haskell is useful in the real world.
Which part of it is Haskell? I can't help but think it's just create-an-account bait.
That's the V-Ray bench application, which is calling V-Ray. I doubt V-Ray's rendering core has to do anything with Haskell. They have an api that let's you to embed the core into multiple products, they are most likely just using that through FFI. Edit: They support a bunch of ways to write shaders for V-Ray, like OSL and GLSL, some of those compilers *could* be written in Haskell, but that's a very small part of the core.
I don't know because their talk is in Bulgarian: https://www.youtube.com/watch?v=c4rO9r5I0Ow Maybe just the UI, but that's the most novel part to me. I'm not sure I know of a single other commercial desktop application written in Haskell.
This is great, and the suggestions in this post are good. But I think that if we really want to tighten up the release schedule, we have to put forth some level of commitment to a real schedule. The easiest way to do this is how a variety of Linux distros do it: We pick two months, six months apart (e.g. February and August), and we set a goal to make a release in each of these months every year. Of course, the goals for these releases will often not be met in time, so it's ok for releases to be delayed and miss the schedule. But that does mean that less time (and therefore less work) will be budgeted to the following the release, in order to keep that release on schedule if possible. In addition, it would be useful to set a generic goal for what the timeline should be for the process described in the OP. Knowing how long the process is expected to take after the release cut helps the community understand when things are delayed. Finally, I think having timelines rigidly defined (but loosely adhered to) helps keep things moving, and ensures there is a constant effort to set realistic expectations for releases and to make those releases happen. As we tune the process, we will get better at budgeting for the unexpected and expecting to go over budget. EDIT: Also, I'm not at all sure of the six month cycle. But of course the time interval in this process can be changed.
To be fair, the two shortest releases there: 7.2 shipped as more or less a preview of coming attractions for 7.4 and 7.4 shipped with a number of show stopping bugs which caused a bit of concern that things were going too fast. e.g. it shipped with PolyKinds that don't work across module boundaries. (Then again it seems like we're seeing that again with unboxed sums?) 7.6 came out quickly in part to address those bugs. The the long 7.8 release time table at the time seemed more like a reaction to back-to-back breakneck fast releases full of problems. So, yes, the release schedule has gotten longer by a bit, but not quite so long as it seems if you look at those numbers uncritically, once you consider 7.2 and 7.4 all but the same release and 7.6 more or less as a bug fix round.
Please add a link to the youtube talk slides, if you have access to them. Most of the talk slides are in English. There is some live coding from midway to near the end of the talk. The talk seems to mostly be showing how to write brief, introductory code in Haskell for people unfamiliar with the language.
Silly question: what exactly is a left associative bind and what makes it so much worse than a right associative bind? 
Couldn't we do something like Ubuntu's .04 and .10 versions each year eventually? The predictability of new versions is definitely appealing. &gt; How many times per year do you envision upgrading your compiler before the process becomes too onerous? With Stack's `--install-ghc` switches, it's not really upgrading my compiler so much as having whatever version a project requires be installed automatically. I think a significant portion of the userbase, especially relative newcomers, use Stack to the exclusion of all else. &gt; Would you feel more likely to contribute to GHC if your work were more quickly available in a release? Likely so.
Maybe SpecM in hspec, or ActionT in scotty?
Yes.
* Left associative: `(foo &gt;&gt; bar) &gt;&gt; baz`. * Right associative: `foo &gt;&gt; (bar &gt;&gt; baz)`. Personally I think this is easier to see if we replace the `&gt;&gt;` operator with a function called `next`: * Left: `next (next foo bar) baz` * Right: `next foo (next bar baz)` In the right associative case, GHC is able to evaluate just two identifiers (`next` and `foo`) before it gets started on work. In the left case, GHC will need to look at the first next, then the second next, and _then_ finally look at `foo`. If we have a really long call chain, this would be even worse. If that seemed abstract: I prefer thinking about it in terms of list append. Comparing the runtime behavior of these two snippets: * Left: `((foo ++ bar) ++ baz) ++ bin` * Right: `foo ++ (bar ++ (baz ++ bin))` (And yes, I win an unnecessary parens award.)
Unfortunately not all colours are good. Some of them show build failures :-(
It's interesting to hear Linus' argument about why they moved to short release cycle for the kernel: there were too many integration issues caused by a flood of changes. The kernel releases are like a clockwork now, very predictable, and regular users of, say Ubuntu, get a new kernel all the time without any hiccups. 
Regarding receiving frequent updates, I wouldn't mind updating my compiler version frequently. Implementing adapters for existing library versions to expose upcoming features before the GHC upgrade might smooth the process a bit. 
Just a humble user here, but I don't mind waiting a while for a quality release. I try to avoid the newest fancy whizbang features in production code anyway.
I remember reading that GHC 8.2.1 has colorized output by default but that using it with Stack will disable the colorized output. Is there any way to fix that if that's the case? I'd like to be able to run `stack build` and get pretty colors.
It would interesting if there are enough SaaS providers one day that we could use Haskell SaaS's for most of these - https://builtwith.com/fpcomplete.com
One of the good thing about returning a `Maybe`, though, is you can evaluate the key lookup without evaluating the table entry. There should be a function, something like lookupM :: (Ord k, Monad m) =&gt; Key ph k -&gt; Map ph k v -&gt; m v It'd work like the `indexM` functions in `Data.Vector`.
Could you give some examples of what this wins you, in practice?
When I was learning monad transformers I started with a bunch of the [dumbest possible examples] (https://github.com/danidiaz/haskell-sandbox/blob/master/MonadTransExamples.hs) (now obsolete, as they use Error instead of the more modern ExceptT) in a file and toyed with them in ghci, with lots of **:t** and **:reload**. Anyway, speaking of State + Except, here's a brain teaser of sorts: {-# language FlexibleContexts #-} module Main where import Control.Monad.Except import Control.Monad.State foo :: (MonadError () m, MonadState Int m) =&gt; m () foo = (put 1 &gt;&gt; throwError ()) `catchError` return main :: IO () main = do print $ runExcept $ flip execStateT 0 $ foo -- Right 0 print $ flip execState 0 $ runExceptT $ foo -- 1 return () **Hint**: It has to do with the fact that effects in "inner monads" have priority over effects in "outer monads". "foo" is defined in mtl-style, which leaves the ordering of the monads up to the "implementation". I'm not sure this is a good thing.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [danidiaz/haskell-sandbox/.../**MonadTransExamples.hs** (master → 1ed15f0)](https://github.com/danidiaz/haskell-sandbox/blob/1ed15f0b7c5b7138acb0e1b4b1965cacfab58d2d/MonadTransExamples.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dl02jjd.)^.
try `-fdiagnostics-color=always`.
&gt; waiting a while for a quality release It is not obvious that overall quality goes up with less frequent releases. There is even some evidence in the opposite direction.
We've got a new release but AFAICS regressions like #13426 and #13535 don't look like they're resolved, so I'm guessing current release still have these issues, correct? As a commercial user and contributor; I'd love to see more frequent releases. These days I can only contribute minor improvements (e.g. improvements in error or warning messages) / fixes (bugs in parser etc.) but my last bug fix took 7 months until I could use it, and my last warning message improvement will have to wait until next release. Those improvements or bugs are usually realized at work during development so being able to have those fixes/improvements faster would be really helpful for us.
It somehow randomly started working magically ¯\\\_(ツ)\_/¯ I'll keep that in mind if it breaks in the future. (Now if only I could get my emacs prettify-symbols to not break sporadically while I'm at it)
Sounded to me like the biggest setbacks in the current release were correctness and performance bugs. I'm happy to wait a bit to make sure we don't have regressions in those areas.
&gt; Sounded to me like the biggest setbacks in the current release were correctness and performance bugs. It will always sound like that, but there is a difference between bringing 12 months of work to release quality and bringing 6 (or less) months of work to release quality. It sounded to me like some of the work with correctness and performance bugs were related to new features that entered the codestream late in the release cycle.
Why does `lookup :: (Ord k) =&gt; Key ph k -&gt; Map ph k v -&gt; v` mean you have to evaluate the table entry? It's just giving you a computation that gives you a `v`, it's up to you to pattern match on that and force evaluation - no?
Presumably any time you want to store keys and later look them up. I can't think of anything off the top of my head, but I've certainly done that in the past.
What's the advantage over just looking up the corresponding value and storing that? What do I gain from having a guaranteed key in a map?
if we want more opportunities to use haskell in the real world and a bigger circle of users/contributors, we're going to have to live with commercial development, some of it closed source.
Looks good. As a user, in the description, I'd like to see whether this is a zero-cost abstraction or not.
Isn't there one in the README already? And it says you can find even more details in [`Data.Map.Justified.Tutorial`](https://github.com/matt-noonan/justified-containers/blob/master/src/Data/Map/Justified/Tutorial.hs)
[removed]
This is a solid question. One immediate answer is that you can use `adjust` or `reinsert` to change the value associated to the key in the map.
As a user, I don't have a particular problem with the current cycles, but.. With more automation, I think release cycles can basically go arbitrarily down. But as release cycles go down, the releases must be ranked according to LTS-ness. Developers that are on a 1-year compiler upgrade cycle should all end up choosing the same release, and those that are on a 1-month upgrade cycle should all end choosing the same release. The cost of flipping from one stackage release to another is very low, lower than, say testing new `npm` packages which will arbitrarily break (I experience that all the time these days). I'd like to try full stackage releases though, not only the compiler, and my back-of-an-envelope calculations, assuming stackage can be built in 500 EC2 vCPU-hours is that it would cost ~5USD to do such a release (spot c4.xlarge instances), so it's worth doing this automatically at regular intervals. That doesn't mean it's a high priority for me to be on the latest compiler. It isn't a high priority, but that doesn't mean that _it's a hassle_ which is what /u/bgamari wanted feedback on. I just flip a switch and let the CI system figure out if it will work or not (my highest priority is GHCJS compatibility). 
But you can always insert the value at any key, and `adjust` is just an insert since I already have the old value.
One of the main problems with the Linux kernel was that as the 2.6 development dragged on, distributions started backporting 2.6 features on top of the ostensibly stable 2.4.x series they were using. Which occasionally created unstability, as the new shiny features weren't really ready, and effectively forked the kernel as each distro had it's own extensive set of patches. But AFAIK this is not really a problem the Haskell community has. That being said, it's certainly possible to create high-quality releases with a short release cycle, e.g. Linux, firefox, chrome, rust.
For the love of God, can people stop using that word "evangelist." There is nothing more cringe-inducing.
Very much looking forward to see the Liquid Haskell work; how long a development period do you foresee before we can trust the library in production-critical applications?
/u/ElvishJerricco, that's one thing on your wishlist addressed, if not solved, here. :)
Probably worth mentioning prominently that you can't delete anything from this container. Perhaps there could be an addition to the API that allows deletions if they're wrapped in another `forall` trick.
&gt; Manuel M T Chakravarty | 1 August 2017 ... [7 paragraphs] ... I am thrilled to announce that I am joining Tweag I/O :-O Burying the lede, much? Amazing news!
Hard to tell. The problem is review of the code. It has largely been effort from some students here and myself so I do not think I am comfortable recommending it, at least right now. My overall plan is to fix all the pending primitive implementations for version 1. Then take up programming an actual network protocol say ssh/noise. The second step is to clean up the api and see where we can improve on the security through formal guarantees like liquid haskell. When such a protocol is implemented, I think I would be confident to say that it can be used in production and we will mark it with the release of version 1. That said, I would welcome users of this library may be for not so critical software. The implementations that are currently there are fast and I believe secure. For what it is worth, we have a reasonable collection of cryptographic hashes and a fairly good implementation of chacha20 (though no authenticated encrypted mode). I am also pretty happy with the CSPRG. So if some one wants some limited kind of low level crypto, raaz is very much an alternative. In fact such uses can accelerate our approach to version 1 and may be get unbiased reviewers.
Transducers are meant to be decoupled from the source of the data, and the destination as well. Although they are used in this post with "transduce" (which consumes a Foldable), you can use them in other use case too. In Clojure, transducers are often used with asynchronous channels for instance, to automatically apply a transformation (or filter) elements that go a channel between two green threads.
&gt; "inner monads" have priority over effects in "outer monads" Is this definitively true? Surely it depends on the transformers in question. 
You can think of it as a new kind of program invariant that can be checked at compile time. For example, the directed graph type type Digraph ph node = Map ph node [Key ph node] documents and enforces the property that each neighbor's node key *must* also exist in the map. If you can't prove that the property holds, your program won't type-check.
~~Very true.~~ I couldn't figure out a good API for deletion that didn't require you to re-validate each key. The natural function to covert a key from a map to one with fewer valid keys would be like downgrade :: Key ph k -&gt; Maybe (Key ph' k) which is not much better than get extracting the underlying map and doing another `withMap` operation. --- **Edit:** No longer true! Based on the comment below, the package now includes key deletion, intersection, filtering, and differencing of maps. Proofs that keys belong to the resulting map can be converted into proofs that the keys belong to the original map, dual to how key insertion / map union works.
I'm pretty sure there is no way to avoid revalidating each key. (Maybe linear types will have an answer, one day!) &gt; `downgrade :: Key ph k -&gt; Maybe (Key ph' k)` I don't see how this helps remove keys. You need to "downgrade" the map, not the keys!
Cool. That example does seem like something you can't have otherwise.
Pun intended?
Good point, I'll add something to that effect up front, since all of the types involved are just `newtype`s of standard containers. That said, I'm not sure if the `newtype` wrappers could inhibit rewrite rules from `Data.Map`? 
I'm not sure TBH. I would be interested in possible counterexamples. Here's another example: `WriterT` over the [megaparsec](http://hackage.haskell.org/package/megaparsec) monad only preserves the log of a successful parse. megaparsec over `WriterT` also gives you messages emitted in branches that backtracked.
Well, it's both. There is actually an interesting pattern here... The library does already have the `inserting` function, with signature inserting :: Ord k =&gt; k -&gt; v -&gt; Map ph k v -&gt; (forall ph'. (Key ph' k, Key ph k -&gt; Key ph' k, Map ph' k v) -&gt; t) -&gt; t The "what to do with the new map" continuation here has kind of a weird signature. But if you think about what the key-existence-related invariants of updating a map `m` with a key `k` are, you could say: - The key `k` should be present in the new map - Any key that existed in the old map should still exist in the new map The continuation in inserting gets a tuple of the new `Map ph' k v` itself, plus *evidence* proving that the two points above hold. The evidence is of the form newKey :: Key ph' k (the inserted key is present in the new map `m' :: Map ph' k v`), and upgrade :: Key ph k -&gt; Key ph' k (any key that existed in the old map also exists in the new map) Oh! I think I've got it now! For deleting a key, the invariant is "any key that exists in the modified map should exist in the original map", so the signature should be deleting :: Ord k =&gt; k -&gt; Map ph k v -&gt; (forall ph'. (Key ph' k -&gt; Key ph k, Map ph' k v) -&gt; t) -&gt; t The continuation gets evidence that all keys in the deleted map are valid in the original map, reified by the function with type `Key ph' k -&gt; Key ph k`!
I didn't actually notice that haha.
True. Although leaving that thunk unforced will leak a reference to the map, whereas matching the `Maybe` from `Data.Map.Lazy` will yield a `v` thunk that doesn't retain the map.
The main difference between `adjust` / `reinsert` and `Data.Map`'s `insert` or `Data.Map.Justified`'s `inserting` is that the former guarantee that they set of keys is unaffected, while the latter can potentially change the key set. So you can keep using evidence that a key existed in the original map to directly access values in an `adjust`ed or `reinsert`ed map, but a little extra work is required in the `insert` case.
What link you're looking at? Do you mean the slides of CG^2 conf?
V-Ray is written in C++. We have wrote V-Ray Benchmark in Haskell. Latter one uses V-Ray AppSDK to access V-Ray for rendering. UI uses wxHaskell library.
V-Ray Benchmark is a fully written in Haskell, but uses V-Ray AppSDK to render using V-Ray. V-Ray Benchmark is free, but closed source industry product. But it's still a Haskell product of a commercial company with great popularity. We just give an example of real world product written in Haskell.
I had to do a stack clean first before it took effect.
The terrible regression associated with #13426 is indeed resolved. All that remains of that ticket is a bit of implementation cleanup (and perhaps a bit more performance, we shall see). However, #13535 is indeed with us. I'll get back to it eventually. Thanks for the feedback!
Why?
First, let me be clear that I'm fully aware that Tweag and Manuel do not intend to or use it in a negative way. They're just doing what was done by others in the tech space for years. Like you, I wish the use of "evangelist" as a title for someone who does community outreach and pr as main activities would end. The tech space is happy to use terms loaded with negative associations: - PC Master Race - Evangelist - Zealot I can't tell if the tech space's use is meant to re-purpose the words and give them a positive meaning or simply being unaware of what the rest of the population associates. Are "master race" or "evangelist" used in a positive way outside tech? To me it's not cringe but disappointment in the fact that (computer) scientists of all people are happy to use a term that means &gt; a person who seeks to convert others to the Christian faith, especially by public preaching &gt; synonyms: preacher, missionary, gospeler, proselytizer, crusader; But I'm 100% confident that neither Tweag nor Manuel mean this when they use it. We should still point it out and discourage its use.
Thank you for clarification. As someone working on the raytracing industry, I'm happy to see more Haskell in this domain. One day we'll write our integrators in Haskell ;)
I'm also an evangelist. Not because I like but simply because people refuse to see the light.
FWIW, I'm in favor of shorter release cycles. And even if that would mean that we had a tick-tock cycle. Cut a release every 6mo, offer it for another 6mo as public beta, and then release. I don't think that just increasing release cycles will prevent bugs. GHC is already hard to build and the build system is rather opaque. If that wasn't bad enough it also takes quite some time to actually build GHC. I'm also afraid that long release cycle will just result in cramming in more and more, because if it doesn't make the cut, you are going to wait a whole year for anything to be publicly available.
Really interesting, I just understood the original comment. Is there any good reason why we need `m` to be a `Monad`? I.o.w. beyond `pure`, do we need `bind`? Sounds a lot like https://ghc.haskell.org/trac/ghc/ticket/13875.
&gt;Are "master race" or "evangelist" used in a positive way outside tech? Without a doubt, if you said to someone in the street "I belong to the (PC) master race", or "I am a tech evangelist" you would get funny looks. I don't know what it is, but so many people in tech are not just bad at naming things, they are good at naming things badly.
Why is it cringe-inducing? The term, at least in the UK, typically refers to people who are pushy and obnoxious in their attempts to convert others to (usually) Christianity.
I have never seen concerns being raised in other communities about the use of this term, yet you make an interesting point. What other title would you rather see?
I wasn't aware of Manuel's Swift work he's referring to here &gt; I used the experience I gained in developing this application to help others adopt functional programming in Apple’s new hybrid language Swift Does anyone know what the reference is?
Seems a bit harsh.
How about "advocate"?
I dislike the other terms you listed but, personally, I do like "evangelist". To me it has mostly a "bearer of good news" sense, as opposed to mere zealotry. Also "evangelist" carries with it a certain expectation of travelling, unlike the more sedentary-sounding "propagandist" or "publicist". It fits better with the itinerant life of a tech conference speaker. 
The term by itself means the same thing here in the US with the same connotations, but terms "tech evangelists" and specific instantiations of them like "Solaris evangelists" have been used so much over the years. I've heard them so much that I didn't even make the connection to religion. Maybe they aren't so common on that side of the pond? Or maybe I'm just weird in my mental compartmentalization or something.
Let me validate that I understand this correctly. Let's say I have a variable `v0` which holds a thunk to an expensive computation returning an `Int`. I can insert `v0` in a `Data.Map.Lazy`, and since I've picked the lazy version of Map, the thunk won't be forced. Later on, `v0` goes out of scope but I still have the Map. So I can use `lookupM` to get a `Maybe Int`, or I can use `lookup` to get an `Int`. At this point, whichever approach I choose, the result is a thunk to a computation which walks down the Map to find the key and returns the result, be it an `Int` or a `Maybe Int`. In both cases, I have at least two choices: I can return this thunk unevaluated to my caller, or I can deepseq the result, thereby performing both the walking-down-the-Map part and the expensive computation, and return the Int to my caller. The difference is that with the `Maybe Int`, there is a third possibility: by pattern-matching on `Just v1` and returning `v1` to my caller, I can force just the walking-down-the-Map part of the computation. `v1` will hold a thunk to the original expensive computation I had in `v0`, still unevaluated. But there is an obvious solution: define data Lazy a = Lazy a newtype Strict a = Strict a -- same as Data.Functor.Identity and insert `Lazy v0` into the Map instead of inserting `v0` directly. This way, even with `Data.Map.Strict` and the `lookup` which returns an `Int`, I will still be able to keep the expensive computation unevaluated by pattern-matching on `Lazy v1`. And isn't this solution much more composable, too? Instead of having every container provide both a Strict and Lazy variant, or having justified-container provide an extra `lookupM` variant, wouldn't it be simpler to only have a single (strict) API, and to sprinkle `Lazy` and `Strict` wrappers when we want more control over whether a container's values should be lazy or strict?
Yeah, it's a good fit. In the Latin origin it meant to be called to someone or something's aid. Given that there's a proliferation of contenders for popular languages that solve pressing issues and Haskell has to defend itself in the space, I find the word fitting for someone who goes around and campaigns for and defends it against accusations of, say, space leaks :).
Indeed when I put together that history I was somewhat surprised to find that the 8.2 slip wasn't quite as terrible as I had thought. 
I don't hear it that much over here in the UK. It's mostly giant tech firms that I hear it from - "digital evangelist" is one I remember from Google. Outside of its conventional meaning, I just don't understand why it is used. I mean, surely we aren't trying to "convert" people to digital (whatever that means, it's not like people use analog much anyway), or to functional programming (do we really want to convert people? - what do we do when we can't convert them: start the programming inquisition?). I don't know, maybe I'm just being picky, but it just seems like the wrong word. 
Are you referring to the scheduled releases?
It was meant as a joke. :)
I don't even think the `lookupM` solution actually works unless implemented in a rather specific way. If it's just: lookupM :: Applicative f =&gt; Key ph k -&gt; Map ph k v -&gt; f v lookupM k m = pure (lookup k m) then pattern matching on `Just` will not yield the thunk of the value, but rather the `lookup k m` thunk. You'd have to rewrite `lookup` itself to be able to `seq` its tree walking with respect to some context.
Those are all tongue-in-cheek hyperbole. The humor derives from the negative/extreme connotations. Compare "Grammar Nazi."
Hey Ben, first, that looks like a good direction and I find it motivating to read. It seems obvious to me that all release tasks (except from maybe writing final release notes and tipps on how to upgrade smoothly and) should be fully automatic to not take time away from developers, I'm very glad you're tackling this. From a GHC contributor's perspective, I'd find it great if you went all the way with this automation: I don't even want to know how to build or test GHC on FreeBSD, or even worse, OSX or Windows. I want to push my change to a branch and have a robot tell me if I broke something on those platforms, before I even ask anyone to look at my change. (OK, maybe I have to tell you where I live before so that you can beat me up in case I push something that takes over Jenkins and starts mining Bitcoins, as Jenkins isn't designed to build untrusted changes, but you get the idea. Alternatively this could also be adressed by having it build in a VM that gets killed after a timeout. I think GlusterFS has such a setup that is also used to make it easy for everybody to test changes in a standard VM.) And further, I'd like that, given capacity, my proposed changes are built all the way into bindists that I can give others to try out my changes. That will decrease the chance that my changes introduce bugs. I already follow this practice by manually putting bindists on https://github.com/nh2/ghc/releases, but it's an effort and I can't easily do it cross-platform, while Jenkins can. If hardware capacity is a limit to implement that, I'm sure we can address that. Hosting companies seem eager to sponsor build / infrastructure hardware for important Open Source projects these days (and IIRC GHC already makes use of that somewhere, was that right), and if we've exhausted our possibility for sponsing from that side, the people on the [List of companies that use Haskell](https://www.reddit.com/r/haskell/comments/6p2x0p/list_of_companies_that_use_haskell/) most likely has an interest to remove unwanted surprises in GHC upgrades by sponsoring some 20 Jenkins slaves or so if called for on Reddit (being a commercial user of Haskell I'd happily sponsor one personally in that case). Regarding release frequency, I think roughly-6-months releases would do GHC good.
Yes, the link in the thread below.
I'm talking about the Ubuntu-style twice-yearly releases, yes.
Thank you for writing this, your thinking on the issue seems on-target to me! &gt; Do you feel that it takes too long for GHC features to make it to users' hands? Yes. What I think is most problematic about this is that the authors of a change live with it for too long (and write papers!) without contact with the enemy (friendly, constructive enemies in this case). This leads to a situation where gross infelicities are missed, but pushback can be awkward since the feature is long-since a done deal in the minds of the authors. I think addressing this will require a more explicit notion of experimental features. E.g., we have `LANGUAGE` that in reality is used for effectively standardized things as well as things somebody wants people to try out. Perhaps we should have `EXPERIMENTAL` that does the same thing as `LANGUAGE`, but is for things that may be taken out or promoted to `LANGUAGE` pragmas (perhaps with changes) after a release cycle. &gt; How many times per year do you envision upgrading your compiler before the process becomes too onerous? Not more than twice. 
&gt; first, that looks like a good direction and I find it motivating to read. Glad to hear it. &gt; I want to push my change to a branch and have a robot tell me if I broke something on those platforms, before I even ask anyone to look at my change. Right, there is of course limits to what we can do with the resources we currently have. As I mention in the post, I don't believe we can currently support the hardware costs to validate every commit on `{OpenBSD, FreeBSD, Darwin, Debian, Old Debian, Windows} x {amd64, i386, AArch64, ARMv7, SPARC, PPC}` (note that not all of those combinations make sense, but you get the idea). However, I think we can at least do nightlies for a large fraction of these configurations, which makes it far easier to identify the culprit than only realizing the issue months after merge. &gt; (OK, maybe I have to tell you where I live before so that you can beat me up in case I push something that takes over Jenkins and starts mining Bitcoins, as Jenkins isn't designed to build untrusted changes, but you get the idea. Alternatively this could also be adressed by having it build in a VM that gets killed after a timeout. Indeed the trust issue is a hard one but one that we already deal with. The assumption is that if you have a Phabricator account then you are trust-worthy. This is admittedly a pretty low bar, but there have been no abuse problems yet. We could indeed isolate builds a bit more and Jenkins will facilitate that, but I don't think that's an immediate priority. &gt; And further, I'd like that, given capacity, my proposed changes are built all the way into bindists that I can give others to try out my changes. Right, the nightly bindists will address this. The structure of the Jenkins automation is such that we always build a bindist. The only question is whether we archive it or not. Perhaps we could also keep a few weeks worth of per-commit artifacts as well. &gt; If hardware capacity is a limit to implement that, I'm sure we can address that. Hosting companies seem eager to sponsor build / infrastructure hardware for important Open Source projects these days Indeed, Rackspace already graciously supplies Haskell.org with all of the infrastructure which runs `*.haskell.org`, our Phabricator infrastructure, and Hackage. I think getting coverage of `{ FreeBSD, OpenBSD, Linux, Windows } x {amd64, i386} + (Darwin, amd64)` is feasible with the resources we have. The problem arises when you bring in the more unusual/computation-limited platforms (think SPARC, ARMv7, etc). Hosting providers with this hardware are relatively few and it requires a non-negligible amount of hardware to keep up with GHC's churn rate. &gt; Regarding release frequency, I think roughly-6-months releases would do GHC good. Lovely.
&gt; I'm also afraid that long release cycle will just result in cramming in more and more, because if it doesn't make the cut, you are going to wait a whole year for anything to be publicly available. Indeed this is a major concern today. I find myself pushing back against contributors who (understandably) want to sneak their changes into minor releases more often than I would like.
It's probably the kind of joke we should be careful with in this community because, deserved or not, we have a certain reputation for arrogance.
&gt; It sounded to me like some of the work with correctness and performance bugs were related to new features that entered the codestream late in the release cycle. Unfortunately this isn't really true of 8.2.1; the most difficult issues that held the release back were really long-standing but latent correctness bugs. However, this was very much the case in 8.0.1. In particular, the pattern match checker was merged quite late and ended up introducing some rather serious compile-time regressions that needed to be sorted out before the final release could be cut.
It's a good start. Doing even more frequent releases like every 6 weeks or so will lead to a streamlined process. Knowing your branch will be included two months later if you miss QA this cycle, you're less stressed and the release won't be delayed. I think it has served Firefox, Chrome and Rust well and is worth considering as the next step.
&gt; Likely so. I'm glad to hear it!
Yesod is too frameworky for my taste. You need to learn a ton of shit just to use it, and then a ton more shit to use the "blessed path". No idea about Snap.
&gt; Perhaps we should have `EXPERIMENTAL` that does the same thing as `LANGUAGE`, but is for things that may be taken out or promoted to `LANGUAGE` pragmas (perhaps with changes) after a release cycle. This is an interesting idea. What specific features do you imagine this new pragma might have been used for in the recent past? Looking back at recently introduced language extensions: It seems to me like `UnboxedSums` and `PatternSynonyms` had a fair amount of support prior to implementation and I've not perceived any pressure to remove them. `TypeInType` is invasive enough that it's hard to imagine removing it, even if we wanted. I think in an ideal world we would simply fold the "stable" extensions into the Haskell Report. Of course, we have seen time and time again how difficult maintaining the momentum of the standardization process can be. In light of this perhaps your suggestion is a reasonable alternative.
For what it's worth: Where I work we use TypeScript for front-end web code. The TypeScript team ship a new minor version, typically with a small-ish number of incremental improvements, every month or two. It's usually me who does the upgrade. I don't find it too difficult to keep up to date. The installation process is very simple and the migration path tends to be smooth. Occasionally I have to update some third-party packages, or change a few lines in our codebase due to adjustments to the type system. On the whole I prefer to keep up to date and make frequent small fixes, than to get behind and have to contend with lots of changes at once in a big bang upgrade.
What is your go-to tool then?
[Suave](https://suave.io/). Probably not what you wanted to hear.
It's not their fault, they just don't know better
Yesod is great. Comes with everything you need to run a web app. I've heard Snap is great too, I've just never worked with it. Do an experiment with both and see how you like them.
I'm looking more for an avenue to anticipate the likelihood of breaking changes than a way to remove things. `PatternSynonyms` is a good example, and I think perhaps `ApplicativeDo` could have done with a round of public usage. My aim with this suggestion is to support the opportunity provided by the proposed shorter release cycle. Say a contributor is able to put a little time into a change or extension. A shorter release cycle is appealing as, 1. They will see their change more quickly 2. Getting their change merged is hopefully less of a marathon 3. When they finally get some public feedback, design considerations are fresher in their mind But all of these things, to me, would benefit from a classification somewhat different than `GADTs` or `FlexibleContexts` that GHC users take as "standard". Since we have trouble updating a standard, we are left with many de facto standard extensions that then muddy the waters when a truly experimental language feature comes along. I think that the working assumption should be that the first release likely has serious problems, and fixing those problems shouldn't be stymied by backwards compatibility concerns. So the contributor can tag their change as EXPERIMENTAL and not feel guilty when discussion reveals it needs changes. I am motivated here by the Rust convention of shepherding experimental features to stable. I feel like we have more of an Athenian springs forth fully formed approach that encounters unpleasant friction with reality. Longer release cycles make this more tenable since authors do live with the change for longer, but I'm often struck by the effectiveness of a few days of public use in teasing out problems.
Don't know about Snap, but Yesod is fairly cumbersome if you're only making an API for a SPA. I recommend Servant, it can also generate a JS client for your API, and is more typesafe and composable. 
Fair point, but `Lazy` or [`Box`](https://hackage.haskell.org/package/vector-0.12.0.1/docs/Data-Vector-Fusion-Util.html) introduces an additional indirection. As long as we don't have data types that can be specialised, and thus have `StrictMap (Lazy a)` just unbox into `LazyMap a`, I wouldn't be willing to sacrifice that performance. At that point we would have the same situation as now anyway, but with the compiler having done the hard work of replicating the implementations.
Yesod has pretty much everything you need and has a great documentation : the [Yesod book](https://www.yesodweb.com/book). However, I'm planning (if I ever have the time) to replace Hamlet with Lucid. Hamlet is great to start, but the fact that code slices doesn't accept any valid haskell expression is a bit annoying.
What would be your approach if you had to convince a software company to switch from enterprise Java to pure FP, and start using purely functional languages?
I suppose it is indeed implemented differently, and `lookup` is simply implemented in terms of `lookupM @Identity`.
I would suggest considering servant or Scotty, if not then snap sounds a lot lighter than yesod for your needs. For front end I would suggest either [react-hs](https://github.com/liqula/react-hs) (fork of the unmaintained react-flux) or reflex coupled with GHCJS. 
How good are with Lens? If it's alright for you, then Snap might be a good fit. But tbh, I would pick servant for an API. 
I wrote a library called yesod-elements that basically gives you lucid-like operators for build yesod widgets.
Though I believe the list should fuse in this case, so it should be the same as folding the map.
It basically means that, if I am taking the lookup value from one map and putting it in another, then the value placed in the other map will either be an unevaluated `lookup` thunk or a fully WHNF-evaluated value. In the first case, it means that the second map holds a reference to the first; in the second, it forces unnecessary evaluation. Both of those are undesirable.
Cool !. How does it compare to yesod-lucid ? Apart from the fact that both are not on stackage ?
I am also quite interested in this, because I am doing a lot of Web-Application-development in other languages (Ruby+Rails and a little Elixir+Phoenix), and would really like to use a ML-style language for these problems. I have looked at Yesod, Spock and Happstack so far. Note that this was a _very_ brief glance during one evening, so YMMV: - Yesod comes including its own brand of 'Kitchen Sink', or maybe actually a wardrobe of kitchen sinks. It attempts and does a lot, but to be able to use it seems like such an extreme investment that I probably won't be able to get through it all (let alone convince my coworkers that programming (Web-)Apps in Haskell is fun and that we should do it at our company, which is one of my goals ^^'). - Spock seems nice, but there are only few tutorials on it right now. Its scope is a lot smaller than Yesod's or Happstack's, which is probably its main strength but this also means that you might have to (re-)invent a lot of things yourself. My main gripe with Spock right now is that I have not seen any code for larger(ish) applications written with it, so I have no idea how well its chosen separation model holds up when your application grows. - Happstack does not build on my computer for some reason which I have been unable to debug thus far. It seems to have a lot less 'blessed path' restrictions than Yesod (But I haven't read enough to say this with absolute certainty). Just like with Spock, code separation in Happstack is still a mystery to me. About single-page-apps: I recently came across [miso](https://github.com/dmjio/miso), which looks very promising :-).
Yes, evangelist does have positive connotations outside of tech, mostly amongst Christians. In other areas I'd consider it neutral. It's been used in print, for at least 30 years to describe extolling virtues of a subject other than Christian religion. It's not an insult or even a word that implies excess regardless of consequence, like 'Zealot.' At worst, it could imply a conviction of faith rather than reason. But this is as an American English speaker - Not sure if the connotation is different in other English speaking countries. However, the phrase as it pertains to tech was coined by American tech companies / journalists, so I think it's fairly safe to say that at least in this context, the word use is appropriate and has plenty of precedence. 
&gt; It's probably the kind of joke we should be careful with in this community because, deserved or not, we have a certain reputation for arrogance. Maybe I should have marked it as sarcasm. It's easy to blow off some steam on reddit. If people disagree they can just downvote. But people pushing their java-agenda is also pretty harsh :)
My experience with Yesod, is that there pretty much zero investment to use it if you use the scaffolding. Just read the book and get started. 
If you know what the word means. It's fine and generally describes the job. People just have some poor associations. It's literal meaning is from Greek. Meaning "To Spread Good News". Meaning this persons job is to spread the news of functional programming to the masses. Christian Evangelism is simply an instance of it. Spreading the news of the gospel. 
The point of evangelism(in the general and literal sense) is to spread good news. To the Christian Evangelists. The good news is the Gospel. To a FP/digital evangelist the news is knowledge/underaranding of their expertise. 
&gt; It's usually me who does the upgrade. I don't find it too difficult to keep up to date. The installation process is very simple and the migration path tends to be smooth. Occasionally I have to update some third-party packages, or change a few lines in our codebase due to adjustments to the type system. On the whole I prefer to keep up to date and make frequent small fixes, than to get behind and have to contend with lots of changes at once in a big bang upgrade. That is an interesting data-point, thanks!
And with stack it's `stack build --ghc-options=-fdiagnostics-color=always ...` And for those wondering: the colorized output you're looking for appears in compilation errors. You'll also now see the line and problem expression displayed after the error message (with or without color).
the application is probably haskellformac. 
That's not how words work though. They don't have predetermined meanings. They very literally mean whatever you use them to mean, so the fact that the Greeks used word X to mean Y has nothing to do with how it is used now. Now, over time the tech sector may well give the word "evangelism" a new meaning - but for now, in most people's minds, we are stuck. Also, I really don't like this idea that we are spreading Good news - it's has such a judgemental tone to it: This is the Good News, embrace it and have your programming sins washed away. No thanks. But this belies the point. We already have many words for what you are describing: *educator*, *teacher*, *enthusiast*, and so on. Why do we need a new one, just because Google says we do?
You seem to have inadvertedly posted the same comment 5 times. Would you mind deleting all of them except this one?
- If GHC moves to time based release schedule, as industrial user I'd expect time-based support policy. [*Three-release*](https://prime.haskell.org/wiki/Libraries/3-Release-Policy) to *two or three years*. Maybe for selected releases: "should work with last two *long time support* GHCs, which are released every two years" (something like Ubuntu?). - Disclaimer: I do like Debian release cycle way more than how Arch Linux works. - As a (co-)maintainer of some libraries on Hackage, I think that even as long as 6-months release cycle will force me to synchronize updates to my libraries when adopting for breaking change in other libraries. I.e. "your library doesn't support http-client-0.6" - "I'll fix that after next GHC release". I don't know if this is good or bad thing. - Decoupling `base` from GHC might help with that, as then one would need to adopt only to language and GHC-internal changes (e.g. `template-haskell`, not also `base` and `containers` and ...). - Release cycle changes won't affect how I contribute to GHC, I made few changes for 8.2.1, but I really don't have *real* use case for them.
You know "gospel" means "good news", right? So an evangelist always spreads the gospel.
haskellformac helps people adopt functional programming in Swift?
idk :p 
&gt; These days, everybody knows what functional programming is, everybody knows that it would be better if they were using it. I know 10 years ago, people would say "don't say the M word, you can't say Monad in front of ordinary programmers because they get scared", and that is certainly no longer the case, because now it's a cool word. If you know what a monad is, you get a certain status, even among Java programmers. So you can stop calling them WarmFuzzyThings now!
I honestly don't even know if that's feasible with the manpower Haskell commands. It's hard to forget since Haskell is getting trendier but it has maybe 2 full-time-ish workers on it with a handful of part time contributions and very little finding compared to, say, rust. Rust can do a 6 week release because it's a very modern language with corporate backing and several full time teams with very little "real world" usage to think about. I'd like to see progress towards a shorter release schedule, but even with shortening the "dead time" as much as possible, I don't really see sub 6 month releases being super feasible anytime soon.
I didn't know that. Good to know. Thanks
Dammit! I think it's a Reddit mobile UI bug: it looked like it hadn't done anything, so I hit the button several times.
/shrug. To each their own i guess. If you don't believe that the technology you're advocating for is better than it's alternatives you shouldn't be advocating for it. Creatibe destruction necessitates the destruction of formerly useful ideas. The problem is that for the people that hold the role of Evangelist. They're doing everything you describe. They are educators, enthisists, community members paid for the purpose of being a mouthpiece of the expertise and an advisor those those who create it. The other words don't adequately encompass the scope of the job. Evangelist does.
Haskell for Mac is a MacOS application so presumably its developed in Swift (either that or Objective C).
At present we've barely demonstrated the ability to maintain one current release of GHC, let alone multiple. Long term maintenance releases require a big community investment and hinges on a number of critical folks in our release cycle, like Ben, taking on twice the workload. Ubuntu has a lot of commercial support, and a heck of a lot more bodies behind it than GHC. Back when Austin was doing the releases, he kept saying he'd love to do it, but couldn't figure out where to find the specialized resources and time.
I've used `yesod` and `snap` and `scotty`. While I originally liked the "minimality" of the latter two, and was wary of the TH used by Yesod, pretty quickly I grew to _love_ the fact that `yesod` type-safe routes banishes the various "stringly" typed errors that arose in the other two (unless I was using them wrong...). Thus, `yesod` lets one to just update the model/routes file, and then follow GHC's type errors to fill in the remaining code in the type-directed fashion that many of us like. 
I use yesod every day and while it has a lot to it you can include only what you need which is nice. Haven't used the other ones so idk.
I agree with this, and another point to support it is that I usually don't upgrade to the latest ghc until it's been out for a bit. Partly because it can be a disruptive change but also because usually it has to be out for a month or so before the dependencies are updated. Then I have to wait for a lull to try out some new features, and only then would I have practical feedback. For instance, it took me quite a while to update to 7.10, or whatever the first one with CallStacks was, and a bit longer to start using it, since I had to tear down some custom hacks that were taking its place. At that point I (and others) discovered the brittleness with directly using implicit parameters and made a type alias, but it had to wait until 8.0 to make it in. Also I noticed that the stack callee info is off by one, such that you can't get the calling function's name. The original author I think basically agreed it would be better to get the caller's name, but at that point he was blocked on another bug and I think now doesn't have so much time to work on it (https://ghc.haskell.org/trac/ghc/ticket/11686). So this might be an example of the implementor to user gap being so long that by the time the user feedback comes back the implementor has moved on. I don't know if a differently-named pragma would have helped, but a shorter release cycle might have. Maybe both together would help: the EXPERIMENTAL is a psychological nudge toward "not done yet, don't move on yet", and the shorter cycle makes the nudge seem more reasonable.
Recompiling gcc5 from my updated ports tree worked like a charm.
I remember trying Spock a fews ago. Everything was fine until I started adding things to make it real website : authorization, js minification etc ... I'm not saying it's not doable with Spock, but it seems easier to strip down Yesod to what you need, rather than extending a Spock app. Also, one advantage of Yesod: it comes up with lots of choices (libraries) already made for you, which saves lots of time to get up and running. For example you don't have to chose and investigate between pipe and conduit, just use conduit ;-) (I'm not saying conduit is better than pipe, but sometimes is better to have no choice). Some will see this as a disadvantage but if you know what you are doing you still use the component/package that you want. 
*Miso* does sound promising! 
I understand. But if currently a single release is maintained for about a year, it's about twice as long as 6 months! 
GHCJS currently (for a while now) doesn't work on Windows.
Is the `INSTALL.windows` file out of date?
It's worth noting that the scaffold includes a bunch of stuff that you CAN just factor out if you decide you want to use something else. I know some people don't like yesod-auth so they take it out, I like it so I keep it in. Etc etc.
There are several issues about it on GitHub but I think this is the first one https://github.com/ghcjs/ghcjs/issues/548
&gt; Warm fuzzy things are the small crispy things in the big box of thingy things.
I find it funny that people are downvoting you for being honest. Haskell frameworks are awesome but the majority of the world uses js and php for the web and the overall ease of integrating with the web ecosystem reflects that.
`miso` builds on Windows, with `stack` and `GHCJS`, the 7.10.3 version, https://ci.appveyor.com/project/dmjio/miso
After a brief personal experience, Scotty and Snap are very easy and convenient for simple web service work i.e. REST services with few endpoints. I think front-end work is made easier by more complete frameworks, but I wouldn't know what to suggest here since I'm not an expert.
&gt; Spock seems nice, but there are only few tutorials on it right now. Its scope is a lot smaller than Yesod's or Happstack's, which is probably its main strength but this also means that you might have to (re-)invent a lot of things yourself. My main gripe with Spock right now is that I have not seen any code for larger(ish) applications written with it, so I have no idea how well its chosen separation model holds up when your application grows. I've created a website with Spock which might not be larger but isn't that small either. I've tried to document the code so it'll be easier for others to read as there aren't many tutorials or examples available for Spock. - This is the website: https://gathering.purescript.org - And the [source](https://github.com/soupi/gathering) - Edit: and also my experience report: https://gilmi.xyz/post/2017/04/25/building-gathering 
Yeah, but 7.10.3 is two major versions behind by now. 
I have a personal related rant: - One data-point is `hashable-1.2.6.1` release. The diff from `1.2.6.0` is very small: http://hdiff.luite.com/cgit/hashable/diff?id=1.2.6.1&amp;id2=1.2.6.0. `1.2.6.0` was supposed to be compatible GHC-8.2 release, - On Jan 26 (6 months before GHC-8.2 actual release!) the GHC-8.1 [compatible release was requested](https://github.com/tibbe/hashable/pull/132). - Johan settled to make a release on when release candidate is out, and `1.2.6.0` was released on Mar 17 (though RC1 was only at Apr 10!) - Everything looks good, [except on Jun 20 - RC3 breaks](https://github.com/ghc/ghc/commit/9649420a05e6417c05a46a3079b253bd69d03724) `hashable-1.2.6.0`. Luckily Ben fixed the issue right away, but that caused confusion as the GHC-8.2.1-RC3 weren't yet released, so [two](https://github.com/tibbe/hashable/issues/142) [issues](https://github.com/tibbe/hashable/issues/143) were opened because `hashable-1.2.6.1` didn't compile with GHC-8.2.1-RC2. - Another data-point is Cabal. It introduced (not strictly necessary) breaking changes into 2.0 branch during RC period. It broke `build-type: Custom` packages, which had "speculative GHC-8.2 compatible" releases. That's unfortunate. I understand that packages were released to make it possible to test GHC release candidates, but it's almost impossible to predict the future. I'm all for fine-tuning APIs etc, details are important. --- To fix the issue, I already half-promised to /u/hvr to hack together a curation-builder-environment (a bit like Stackage, but different), so we - Can test unreleased GHC against big-enough to be representative package-set - Be able to make patches to fix them (possibly even if the author doesn't care, yet) - So **we don't need to release speculative packages** on Hackage, which are supposed to work with soon-to-be released GHC, but sometimes aren't In other words, I volunteer to help with *b) regularly test GHC against user packages in between releases*.
There might be a few extensions you'll miss out on. That's about it. You don't need to build all of hackage on the front end. Sharing a few types and aeson instances is about all you need.
I think it's a good idea not to release speculatively on Hackage. It's probably a better idea to say that anyone wanting to use a release candidate should expect to pull many dependencies from GitHub.
&gt; Everything looks good, except on Jun 20 - RC3 breaks hashable-1.2.6.0. Luckily Ben fixed the issue right away, but that caused confusion as the GHC-8.2.1-RC3 weren't yet released, so two issues were opened because hashable-1.2.6.1 didn't compile with GHC-8.2.1-RC2. Indeed this is a very fair critique. I definitely messed up on this one. The minor (and perhaps arguable) improvement that this change brought wasn't anywhere near the chaos that it brought. 
&gt; pull many dependencies from GitHub. This is where [Stack leaps ahead](https://www.reddit.com/r/haskell/comments/6mzx7s/stacks_really_awesome_new_extensible_snapshots/dk5nfk9/)! 
After seeing /u/algebra4life's [intro to brick](https://www.reddit.com/r/haskell/comments/6jsxrc/an_introduction_to_brick/) on here the other week, I wanted to have a play, and I'd wanted to build a CLI typing app for a while since the interface on all the online ones leave a lot to be desired. It's my first "real" Haskell application, so be nice :)
I agree that putting releases speculatively up on Hackage is probably not a good idea. However, I also don't think we can expect pre-releases to see much testing if users are forced to pull dependencies from GitHub. It seems like an environment like /u/phadej suggests would be a great step in the right direction.
Isn't the slogan of Haskell "Avoid success at all costs"?
Oh, yes, that would also be great! I just meant to agree about speculative Hackage releases =)
I really like Yesod. But when you want client-side rendering (preferably renderable on the server as well), then you want more then Yesod provides. You need UI-routing that works both client- and server-side. You also need an API (probably REST*ish*; see comment thread for the reason of the *ish* suffix), for data requests (that need their own routing). Servant seems to have the "server-side-only API for data" covered really well. It also allows one to generate docs and API-wrapper libs! But then there is [the JS problem](https://wiki.haskell.org/The_JavaScript_Problem); you need JS for modern UIs, but it kinda sucks. Several solutions are out there, all compiling a strongly types languages to JS. I list a few: * Servant + PureScript + {Pux, Thermite, Halogen} (a bridge exists for front-to-end type safety) * Servant + Elm (bridge also exists) * Servant + GHCJS/Miso * Servant + GHCJS/react-flux * Servant + GHCJS/reflex-dom 
The main difference is there is one master to cut releases off of and very little back-porting of patches. GHC's internals change quite a bit from release to release.
This is definitely a place where using stack shines. It is pretty good for this sort of pre-release work flow.
&gt; You need to learn a ton of shit just to use it I found this true for Haskell as well. Still glad I learned it.
Glad my pedantry helps :)
&gt; our visionary leader I think this causes you to be down voted. Otherwise you are correct. Snoyman did some amazing things for the Haskell community.
When it comes to the function *producing* the impossible result, you're getting into some sort of meta-safety argument. "The function should have type `... -&gt; Void` because otherwise maybe I'll be tempted to change the type of the function later and make it do something else." That's really not necessary! If the consumer wants to be defensive, they can always just use `absurd` (or empty `case`) anyway.
Haskell insights are mostly transferable, but a lot of what you have to learn about Yesod is syntax and idiosyncrasies.
Good points which, if I'm right about shorter cycles forcing the introduction of more automation, should make it more feasible and help the 6-month cycle too. What is "dead time"?
There's a difference between learning about algebraic structures that change the way you think about programming and learning the specifics of how to make a framework work.
I confirm. I'm not even half way into being professional in day-to-day haskell, but I already started building a small website with Yesod from ground up and it's working out nicely for me, no major roadblocks.
You are right about how words work, but "technology evangelism" (aka secular evangelism) is an established and widely used term: https://en.wikipedia.org/wiki/Technology_evangelist
**Technology evangelist** A technology evangelist is a person who builds a critical mass of support for a given technology, and then establishes it as a technical standard in a market that is subject to network effects. An evangelist promotes the use of a particular product or technology through talks, articles, blogging, user demonstrations, recorded demonstrations, or the creation of sample projects. The word evangelism is taken from the context of religious evangelism due to the similarity of relaying information about a particular set of beliefs with the intention of converting the recipient. There is some element of this although most would argue it's more of showcasing the potential of a technology to lead someone to want to adopt it for themselves. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
I like this alternative.
I hear what you are saying and point taken. However, the use of the term "evangelism" in a secular meaning and specifically in technology also has a solid history by now and is pretty main stream — see https://guykawasaki.com (who popularised it).
Check out this video for an example: https://vimeo.com/191467217/3f45cbd32e As others mentioned, Haskell for Mac, is written in Swift and Haskell: &lt;http://haskellformac.com&gt;. In fact, I believe, it was one of the first Swift (as opposed to Objective-C) apps on the Mac App Store.
There is no general approach IMHO. First of all, you need to ask people and understand what their problems are. Then, you can try to solve them.
You all make good arguments for the use of it and maybe it's more accepted in the Anglosphere with a secular meaning so I won't argue against it any further. But to be true to myself I must say it would be nice if the Haskell community held itself to a higher standard and tried to use a term that's more likely to be perceived in a secular meaning across communities/cultures around the world, just like we have higher standards for correct code than other language communities. In any case, today I've learned a lot about the term and the large number of evangelical branches of Christianity.
I used yesod daily for 6 months, and didn't love the experience. Much of the framework is very opinionated, it's hard to avoid orphan instances, and the subsite functionality was hard to work with. Servant is far and away my favorite backend, although I end up writing front ends in raw HTML and JavaScript, which is fine for the one page apps I usually develop. 
What I've observed to work for new languages is to present convincing solutions to existing problems, favorably in an improved fashion. This usually means better code and/or better runtime performance. Success stories seem to work well too. Outside the Haskell community, there's a perception that some Haskellers are smug or elitist and it's not a practical language. Just recently I read a comment on Hacker News that was discussing languages and mentioned Haskell as that weird language nobody used but your "intern friend in the basement". More success stories will hopefully deal with this perception.
You are right that success stories are a good way to get people interested. However, to actually get somebody to use, say, Haskell, in my experience, the most effective is to solve one of their problems. For example, we can point at many banks and other financial companies as major users of FP today. However, somebody who develops mobile apps will only take that as an interesting data point, but not as something that is directly relevant to them.
Regardless of this one piece of terminology, I completely agree that we must reach out in a friendly manner and not give the impression of being arrogant know-alls.
Looking at the "typical release process" to me, dead time would be: * a few months after the previous major release) A set of release priorities are defined determining which major features we want in the coming release * Having to coordinate with library authors, prepare release documentation * Finally, do preliminary testing. (This is the "go back to" step) * On top of preliminary testing, produce tier-1 builds and send to binary packagers, wait 1 week to make sure tree is buildable. If they can't build, go back to the preliminary testing. * Make a release candidate and then wait several weeks for testing. If there are any issues, go all the way back to prelim testing and start over from there. * Send final release to binary packagers, wait a week for builds, and then call it a day. Tons of dead time in there that can be shortened or outright eliminated if critical feed-back loops are shortened (which will likely happen with CI builders and such). That being said, even if all of that is eliminated as much as it can be, there's still a lot that Haskell has to care about which will lengthen its release time cycles quite a bit. The main one probably being that Haskell tends to remain closer to the edge of research and so sometimes we don't know what the next feature really is going to be or the best way to accomplish it yet.
not to worry, we are still very far from "success".
The problem with bank success stories is that it's seldom in the open. Interestingly the OCaml financial companies are doing more open source work than the Haskell ones, if we ignore Neil's many one man contributions. That said, I agree, so we need more Haskell in browser and Haskell on iOS stories and code out there. Doing more in the "data science" field would also help gain users that have to deal with Python right now.
Schedules can be adjusted once the process and tooling is in place and should also be beneficial to research features. What we need the most is CI and automatisms that keep Hackage building as completely as possible before a release is done. Maybe we could have something like `ghc convert` for applying forward fixes and use it on Hackage, since there doesn't seem to be an effort to avoid code breakage as a priority.
I like advocado even better :-)
I just want to say that this is an awesome question! It must have taken a lot of effort and thought to lay out the alternatives so clearly. I'll be back in a week or so once I've read all the background to see what the conclusions are ;-)
Did you forget the word "you" in your question?
I can imagine that the users comment history might also be a factor. 
[removed]
He gives talks very frequently at the Sydney CocoaHeads meet up around applying high level FP concepts to Swift. 
Really amused by intern comment (:
Unfortunately it doesn't even seem to compile with stack LTS-8 on linux either. https://github.com/tolysz/prepare-ghcjs/issues/10
I think the best kind of evangelism is one that's also brutally honest about shortcomings, readily admitting tradeoffs and straight-up mistakes. Silver bullet evangelism is very much cringe inducing.
Whet appetite, setup familiarity with the very simple stuff (Maybe/Optional), immutable objects, immutable collections, common list functions. Pick some random elements of Guava that are functional. Ease familiarity with alien concepts, have them work through Elm tutorials (even if they only know back end libs usually). Force exposure to ADTs. Get used to model and view being immutable transformations. Find a java library that provides very simple exposure to Applicative and Monad, to reduce intimidation. Then, after you have made Haskell fundamentals less alien, start to provide examples of useful CLI or REST API programs that use very few Haskell concepts, possibly using Scotty, mysql-simple, aeson, and lucid. Gloss over Monad transformers, typeclasses, just try to use the subset of ideas the person has been exposed to in steps prior. Write some unit tests and integration tests. Repeat this a number of times, until everybody feels very comfortable. Try to get one or two people inspired enough to read Haskell First Principles book, at least some chapters. Pair with one of those people, when implemting another API or CLI program. Pounce, produce something large and complicated, still trying to minimize number of Haskell concepts used. Get people to understand the code. Wait, then propose Haskell for a major project. Repeat until Java is dead. 
We're using Java 8 so some things are already possible. Also Elm for frontend is something I intend to share as well. I'm also thinking of approaching it with some category theory concepts from the beginning so that the names make more sense. Thanks for your input. 
Overall, you can teach people who are willing to learn, but going through the political fight with senior tech leadership who is entrenched or risk averse doesn't seem worth it. Even as you overcome technical objections, you will have to wage a bunch of other battles. Seems easier to sharpen your resume and public Haskell portfolio and land a Haskell job in an environment where politics isn't stacked against you.
I mention Elm purely as a learning tool. As safe place to explore with almost a subset of Haskell, for people who are learning alone with the compiler. For actual front end, I would probably choose one of the other options.
Hey /u/piyushkurur, it would be good when announcing this to include in the title that Raaz is a cryptography library, since it's not as well known as say `cryptonite`, and the name doesn't give any clues about what it is either. Also, I was wondering if using `inline-c` would be useful, particularly with the new additions to GHC 8.2 which make compiling inline C code much easier. I also note that you're not using arc4random on macOS, any reason not to? I believe macOS has had arc4random for quite a while, and trying to support anything older than whichever version added it is probably futile, since the install base will be miniscule.
Pretty much. It's hard to sell FP as a main paradigm because of the learning curve. I'm aware of this but hope at least I can make a case on the math is more useful area. At worse I'll make people look stupid because they haven't changed patterns since java 6, which could be what they really need (i kid). 
The most convincing example there was the graph that guaranteed all target nodes were present. I'll buy that one for sure.
I use Scotty and can say it's great. Minimal and easy to learn. I prefer minimal lib like Scotty compared to a framework like Yesod, though. 
I'm impressed, fun typing practice app. Minor issue, my default color scheme obscures the text I have not yet typed. Do you think you could set the color of the text you have not yet typed as the inverse of the terminals current color?
Yes, the secrecy of the banks doesn't help. Hence, recent deployments, such as at Facebook, with talks, blog posts, and some OSS (Haxl) is extremely helpful. Personally, I have high hopes for data science. We are still missing some infrastructure (libraries, visualisations, ...) that works out-of-the-box, but this is definitely something I'd like to help improve if possible.
Thanks /u/Axman6 for your response. Yes I should have mentioned it is a crypto library. Some responses. inline-c: I am not yet familiar with it and might move to this. A rule of thumb for backward compatibility that I have been following is to support the ghc available with Debian stable (which we know moves very slowly). So if we want to use it right now, it will involve a lot of CPP magics which probably is not good. Regarding arc4random on macOS: Since I am not a macOS user I was not familiar with it. I was under the impression that it still uses RC4 whereas OpenBSD/NetBSD has moved to chacha20. Same with FreeBSD. If you think that arc4random is more suitable then please raise a ticket and I will fix this (just a minor change in the cabal file will do it). May be other MacOS users can also comment. 
Have you read my favorite paper: "Fuzzies for the hungry thing-a-ma-bobber"?
I'd advise against `inline-c` if you care about portability not the least because it requires TH which isn't available everywhere. Also note that `raaz` currently has a rather light dependency footprint, but if you start depending on `inline-c` you'd ironically inherit [quite a few additional dependencies such as cryptohash and thus cryptonite](https://hackage.haskell.org/package/inline-c-0.6.0.2/dependencies). ;-)
Good catch – I'll definitely try and do something along those lines. At the very least I should make the colours configurable... Thanks for the kind words!
Servant has served me really well. Its great to have your API (or website path structure) laid out and explicitly programed against as a type. Also, Lucid is a quite nice DSL for describing html, and you can integrate it with servant via the servant-lucid package and providing a typeclass instance specifying how to render the data types corresponding to your pages. That said, it may not be the *best* for everything when you consider starting from scratch. The approach does take a some wrapping your head around, although once you do its fairly simple and very pleasant to use (proviso: the type errors can be huge). I can't speak to integrating with JS frameworks like Angular. I also had a reasonably nice experience with Scotty for [a small website project](https://github.com/BlackBrane/shiva), also with Lucid. To be honest, thats probably the best answer to your question, for a simple and nice web framework. But I mentioned Servant first because its such a gem, and I would probably use it even for small projects now (but I also know it well now). 
thanks /u/hvr_ . Sorry I did not realise it involves TH. I thought it was some thing cleaner built into GHC. One option is to make use of hsc more but in the case of raaz i did not find it that useful.
I see you have implemented replicate via untyped replicate rather than a type class as Jeremy has in his paper. Is that for performance?
Several of my family members happen to be clamoring for one of these, right at this moment. Looks like this is it! Thanks! EDIT: I opened github issues for you for the issue of /u/Aoi32 and a few other things. Thanks for a great app!
I'm wondering why `insert` doesn't have the signature insert :: Ord k =&gt; k -&gt; v -&gt; Map ph k v -&gt; (Key ph' k, Key ph k -&gt; Key ph' k, Map ph' k v) Why the indirection through continuations in this case? BTW, I think this library could benefit from a type level operator: type (~&gt;) ph ph' = forall k. Key ph k -&gt; Key ph' k 
iirc, no, but idk http://hackage.haskell.org/package/compact-0.1.0.1/docs/Data-Compact.html 
If you're looking at guaranteeing valid graph structures at compile time, take a look at the algebraic-graphs library on hackage and the accompanying paper (alga). It's interesting.
Seconded. We've found it's really easy to set up and code a yesod app, from simplest app to major product. It's even easier if you've previously used any of the other MVC-style web frameworks in any language, but most people find that pretty easy to pick up. In any case, the documentation is generous in quantity and very helpful. TH is a big plus for yesod, not a minus as some people claim. It is a powerful way to create the simplest possible type-safe DSL's, which is exactly what you want for the various parts of an MVC framework.
I haven't tried, but I'll guess that it works fine in WSL.
Agreed. If you've already done MVC in other languages, you'll find yesod to be the same except easier and type-safe.
Misleading title, I thought about "typing" in the "programming type" terminology. My mistake ;) I submitted a small PR for nix integration. The run on the `README.md` is really difficult due to the number of symbol used. ;) WPM 72, Accuracy 98% for me ;)
I think the intended use case of hamlet is that you do essentially all of your programming logic in the Haskell backend code. The code snippets in hamlet are only meant for setting up simple loops and the like. We've actually found the code snippet feature to be *too* general. It is tempting to allow logic to leak into the templates, and that drives our non-Haskell-programmer frontend web developers batty. So if you want to use a style where you generate HTML in Haskell code, you are right that lucid is probably a better fit for you.
Well, nothing related to GHC "works fine" in WSL at the moment but they're slowly fixing it.
It's standard MVC stuff, similar to what you have in MVC frameworks in other languages (except nicer imho). So not really idiosyncratic, but specific to MVC and not general Haskell.
I discovered Miso the other day after Nik from haskell.do mentioned it, it's great stuff!
In my mind the word has already incorporated this new meaning, so no cringeness.
I thought along the same time initially! "An app to help practice writing types for haskell expressions? To do Type Driven Development?"
&gt; ... held itself to a higher standard and tried to use a term that's more likely to be perceived in a secular meaning across communities/cultures around the world, just like we have higher standards for correct code than other language communities. If it wasn't for Haskell code's propensity for single-letter variable names, single letter type arguments, and fairly esoteric mathematical language, I'd agree. But Haskellers are as far from using mainstream language for things as almost any other language, so _evangelist_ seems like a perfect fit for the community.
 $ git clone https://github.com/hot-leaf-juice/gotta-go-fast $ cd gotta-go-fast $ stack install Those installation instructions are nice, but shouldn't `stack` be able to do something like this: $ stack install https://github.com/hot-leaf-juice/gotta-go-fast ?
Why not use a custom snapshot for this, based on some nightly/lts? Broken packages can be replaced by fixed ones, just by overriding the packages location.
Great, thanks for the issues! I'll look in to them asap :)
haha yeah, I can see how that would be misleading! Thanks for the PR, I'll take a look as soon as I get a chance :)
This would be cool! I didn't know that that functionality exists. That being said, this doesn't work for me as given. I get Error parsing targets: Unknown local package: https Will look in to it further though.
Sorry, I didn't mean stack *could* do it, but it would be nice if it could, so we could install haskell software in 1 line, not 3. That's a 70% simplification (take or give a few%)!
Established and widely used does not mean we should use it.
ah I see! Totally agreed. That would be awesome.
I was wondering if there were some parts of the stackage machinery that still needs automation? E.g. from other comments it seems notifications, re-adding transitive dependencies, including machine-readable reasons and such are at least some areas that still involve a lot of manual work. It would perhaps be beneficial to keep such a list that people could take a stab at (or does one already exist?)
I understand the separating programming logic and rendering logic argument. However, I prefer when it's the programmer responsibility to decide what's ok or not, rather than an arbitrary decision. I think Heist makes a clean separation between the two and that's why I'm using Yesod ;-) Anyway, the type of things I often do, which is my opinion has nothing to do with programming logic but is not valid Hamlet is : I'm rendering a table and need to either display the row number (or use the row number to generate id or classes). The way I would do it is &lt;table&gt; $foreach (row, index) &lt;- zip rows [1..] &lt;tr&gt; &lt;td&gt;#{index} .... But no. Hamlet doesn't like `[1..]`. So what I have to do the zipping in my handler, or have somewhere in my handler `indexes = [1..]`. Of course, I can survive with that, but it is just unnecessarily annyoing. Also, I think splices doesn't accept `$` and things like that. Again, I probably have bad taste but I didn't invent the `$` operator. 
With regards to the steps needed to compress the release cycle (i.e. automation of steps), it would be cool with perhaps a more detailed outline of what was necessary, so that we as a community could help tackle some of these problems. One idea that immediately stuck in my head for example was if “regularly test GHC against user packages” could be done with something like checking what packages break by switching only the compiler in the latest LTS of stack? Or am I missing something that wouldn’t make this a good idea? I think I just mean to say: I think a lot of people would be interested in helping in these processes if the steps could be thoroughly outlined and perhaps divided into small steps that’ll allow us to iteratively get closer to a smoother process in all :)
It would be cool if language extensions (e.g. in the form of `EXPERIMENTAL`) could be pulled in by cabal/stack directly much as we do with libraries. This would allow for a much quicker feedback loop at least.
I don't know, to be honest. It would probably be a trivial thing to add, since it would most likely just be shelling out to run the commands above.. but since it's so trivial, it could be considered unnecessary. 
I've not used Yesod, but went with Opaleye+Servant. As time passes, it seems that I am reinventing the thought &amp; design that has gone into Yesod. If you're cool with the slow recompile cycles, go with Yesod.
Oh, this isn't about `Reflex` it's about `Reflex` :( Disappointing.
&gt; educator, teacher, enthusiast, and so on Educator/Teacher is even worse. An "evangelist" is telling you something they consider good news. An educator/teacher is "curing your ignorance" and educating you on the proper way to do things.
&gt; Servant seems to have the "server-side-only API for data" covered really well. It also allows one to generate docs and API-wrapper libs! That sounds interesting, but in this case I wish people would stop claiming it has anything to do with REST.
Why?
Because if you're documenting *URIs* you're not doing REST. If you're generating the client side that couldn't get more opposite to REST (in fact, it's ~~SOAP~~ HTTP-RPC).
So true! I actually managed to make a small [CMS](https://github.com/Tehnix/HsCms) back when I was starting out with Haskell, without even having a clue about Monads, Functors and a ton of other things. The book gives a nice intro to the various things, and you can just browse to the chapters you need to know :)
There has been a lot of improvement in the reload and I think there were also several ways to get it down to real quick with GHCi and reloading there :) I think it was /u/chrisdoner (hope I didn’t misspell that) that did the GHCi trickery.
Any time a type like X -&gt; Y -&gt; (forall ph. F ph -&gt; t) -&gt; t appears, it's really an encoding of the type X -&gt; Y -&gt; exists ph. F ph This makes use of the isomorphism between the types ∀Y. ((∀X.F(X) → Y) → Y) and ∃X.F(X), and is only there because AFAIK you can't write the latter directly in Haskell, but you *can* write the former with RankNTypes. So wherever you see one of the continuations at the end of a type, it's saying something like "I (the compiler) am going to select some specific `ph` to use in this type, but I'm not going to tell you what it is and you'll never be able to find out". The only thing you could do to such a type is pass it into a function that is polymorphic in the phantom type. From that perspective, the library version of `inserting` basically has the type inserting :: Ord k =&gt; k -&gt; v -&gt; Map ph k v -&gt; exists ph'. (Key ph', Key ph k -&gt; Key ph' k, Map ph' k v) while your proposed version is inserty :: Ord k =&gt; k -&gt; v -&gt; Map ph k v -&gt; forall ph'. (Key ph', Key ph k -&gt; Key ph' k, Map ph' k v) (note the change of quantifier). So with your version, the **user** gets to instantiate `ph'` at whatever type they want. You'd basically be writing them a license to "prove" that any key they like is valid. Operationally, here is how that plays out if I add your version of `insert` to the library as `inserty`: λ&gt; let m = M.fromList [ (1,"a"), (2,"b") ] :: M.Map Int String λ&gt; :type withMap m $ \table -&gt; let (k,_,_) = inserty 3 "c" table in (table ! k) :: String λ&gt; withMap m $ \table -&gt; let (k,_,_) = inserty 3 "c" table in (table ! k) "*** Exception: Data.Map.Justified has been subverted! CallStack (from HasCallStack): error, called at justified-containers/src/Data/Map/Justified.hs:362:17 in main:Data.Map.Justified while if you try the equivalent thing with `inserting`, you won't get past the type checker: λ&gt; :type withMap m $ \table -&gt; inserting 3 "c" table $ \(k, _, _) -&gt; table ! k &lt;interactive&gt;:15:81: error: • Couldn't match type ‘ph'’ with ‘ph’ ‘ph'’ is a rigid type variable bound by a type expected by the context: forall ph'. (Key ph' Int, Key ph Int -&gt; Key ph' Int, Map ph' Int [Char]) -&gt; String at &lt;interactive&gt;:15:32 ‘ph’ is a rigid type variable bound by a type expected by the context: forall ph. Map ph Int String -&gt; String at &lt;interactive&gt;:15:1 Expected type: Key ph Int Actual type: Key ph' Int • In the second argument of ‘(!)’, namely ‘k’ In the expression: table ! k In the second argument of ‘($)’, namely ‘\ (k, _, _) -&gt; table ! k’ • Relevant bindings include k :: Key ph' Int (bound at &lt;interactive&gt;:15:61) table :: Map ph Int String (bound at &lt;interactive&gt;:15:23) Thanks for the question, I'll add it to the README and tutorial.
Not sure I understand you, or you simply got some things mixed up. REST is an alternative to SOAP that uses the URI and HTTP standards to get rid of a lot of the annoying enveloping that SOAP mandates. It does not dictate the data format (though JSON and XML are quite common). SOAP is an XML vocabulary for making web-API calls. &gt; If you're generating the client side that couldn't get more opposite to REST (in fact, it's SOAP). Many REST API web-interfaces provide (generated) "API wrappers" for the convenience of the programmer. It simply reduces some boilerplate, and possibly allows less room for errors. They go perfectly hand-in-hand.
It seems that this technique lets you change the value associated to a particular node (say, adding a monster somewhere in a graph of connected rooms). Would alga allow that?
:3
Isn't "teacher" more: *here is how you do x*, whereas "evangelist" is more: this is the One True Way?
Representational state transfer. If the API doesn't return a representation of all the possible state transfers, it's not RESTful. All possible states need to be accessible via hyperlinks starting from the representation of the base resource at the initial URI. Hence why the client shouldn't need an enumeration of all the URIs beyond the initial one, nor even what HTTP methods they support. Yes, it's still providing representations of the resources enumerated and the means of changing it, but it's not representing those means of *state transfer*, which is an altogether different thing.
Hi, The caching primitives of TCache execute the conputation again when the time has expired. For example: cachedByKey "key" 10 comp So `comp` will be executed when `cachedByKey key` is invoked after 10 seconds since the last execution of `comp`. To invalidate the memoized value and force `comp` to be executed in the next invocation of cachedByKey , use `flushCached key` flushCached :: String -&gt; IO ()
I understand that, my question is, does flushCached just invaldate the object, or does it remove it from the memory as well ?
Given all that's shared between the server and client, how far is Miso from being able to call GHC modules from GHCJS, eliminating the need to map client functions to URLs to server functions? I know that Servant auto generates most of the glue code for what essentially amounts to RPCs, but you still have to define URLs and handlers manually. And I see know reason why the gluing can't be automated. What I want is a Monad that lets me treat RPCs as if they're local, to the extent that the function being called is imported from the exact same module in both the client and server, without the client needing to know the actual definition of the function when it's called remotely.
Hi, Thanks for your appreciation of Transient. With simple types I mean that the effects of a computation are not reflected in the types. For a beginner, this is IMHO much better, since this avoid cryptic error messages with long type signatures. I think that effects at the type level are useful for two things : to manage state and to execute the appropriate "runner" of the monad. For example the state runner also assign an initial state: runStateT &lt;computation with the "state a" effect&gt; &lt;initial value of a&gt; But runners and state initiaizations destroy composability. I don't find a way to combine computations with different effects which use runners and manage state in such a way. So the advantage of effects at the type level are precisely the reasons for not being composable. In transient all effects can be used anywhere without the need of effect runners. 
How's that different from running a filter on a container with the properties of the shape as its predicates? Or is the benefit in expressing these shapes at the type level?
Would the same problem occur if proofs for keys from two different maps become intermingled? Is there a way to have a unique ID generated for each map so that keys only work with their parent map?
So it would be better to say "Servant allows for REST", but you can still neglect the HATEOAS principle by not caring for it in the response bodies. Fair enough, i'll change my original comment to to reflect this new knowledge. Thanks.
Not very nice to re-use a name that is bound to very much the same niche. Hmpf.
Yeah, not cool at all.
No, this is completely wrong. Fielding must be very unhappy with how confused people have become over what REST is. What you're describing is [HTTP-RPC](https://jimmybogard.com/swagger-the-rest-kryptonite/). The entire point of a REST interface is that client and server can be developed independently of each other. The only thing they agree on is the data types they will work with. What you just described is the opposite of that: as soon as any change happens on the server all clients are broken because they had the URIs hard coded in them. If Facebook changes all their URI's will you suddenly be unable to check your Facebook with your web browser? No, you probably won't even notice because the Web browser is the best REST client ever made and completely resilient to server side changes: so long as the agreed data types remain the same all is well.
Actually I would say it's more like: An Evangelist believes they have the one true way but others see them as someone with an opinion. A teacher is someone who probably sees themselves as having an opinion of how to go about things but are generally seen as knowing the correct way by their students (i.e. a teacher is in a position of authority).
The latter. Yes, the benefit is you know the structure statically from the type.
They're guaranteed to only work for their parent map by the ST-like rank-2 trick.
I must confess I was frustrated more than amused since there's a lot of cargo cult in tech. But I didn't respond.
This `Reflex` is some dialect of Javascript right?
Maybe investigate and take a lesson from Elixir's major success and decline in new pure Erlang projects. For reasons I don't follow myself many developers have been attracted to Elixir because it looks like Ruby. This has resulted in many more projects and companies using Erlang/OTP via Elixir. I find Haskell's syntax more aesthetically pleasing than Rust or Clojure and even somewhat nicer than OCaml, but I'd like to understand if it's pure marketing or familiarity with Ruby on Rails that gave rise to many Elixir users in such short time. Clearly, I wouldn't want to see a Reason-for-Haskell project since I enjoy Haskell's syntax, but there must be a lesson to be had here somewhere. Similar story with GHCJS vs Elm/PureScript/TypeScript and ocaml_of_js vs bucklescript/Reason.
Data science is the perfect fit for a language that allows you to express invariants and be more confident about the computation results. Nobody wants to make the wrong choice during an election campaign because of bugs in Python or R code. We already have a nice REPL and can use it in data IHaskell, but your haskell-for-mac is something I'd like to see in a cross-platform version and IPYthon isn't really confidence-inducing compared to just Haskell. There's a window of opportunity we might miss and lose to Julia or another new language. I guess if we had haskell-for-mac as open source cross-platform, it would have the same effect as Elixir gaining more users for Erlang/OTP.
How can I store one of these maps in, say, a MVar so that other threads can work on it? I was thinking of the graph example in particular.
I agree. I was thinking recently, that it would be fun to have a reflex-build package that would let you describe a build system graph in terms of reflex's DSL. Like make or shake. Someone go do that and I'll claim all the glory.
That, or also other maps that are known to have the same key set. For example, if `key :: Key ph k` and `table :: Map ph k v`, then any of these are fine and will type-check: lookup key table lookup key (reinsert otherKey 17 table) lookup key (adjust f otherKey table) lookup key (fmap f table)
Might be worth emailing them honestly.
For those environments, I get most excited to use a quickcheck clone in the language we are stuck in.
I imagine you could follow most of the normal advice for testing in Haskell, i.e. following the same steps as if you wanted to make `stack test` run some Haskell unit tests. (I'm not familiar with stack, but the steps are probably similar to [something like this](http://taylor.fausak.me/2014/03/04/haskeleton-a-haskell-project-skeleton/#tests)). The difference would be that your tests will need to run `IO` actions, rather than just return values. Depending on which test framework you use (or if you use one at all!), this may be called something like "monadic testing" (e.g. in [QuickCheck](https://hackage.haskell.org/package/QuickCheck-2.10.0.1/docs/Test-QuickCheck-Monadic.html))
I still can't even keep the words "reflex" and "react" straight in my head, so I think I'm just not cut out to work in this space.
I would say the same. Look at the test suites for some larger Haskell tooling (stack, cabal, ...). Your first attempt at this can use the `process` library inside of bare hunit, hspec, or tasty.
Oh. In understand. It removes the object from memory.
Hey Ryan! FB thinks you came up with a cool and marketable name! 
Isn't REST a subset of HTTP-RPC? And the breakage is avoided by putting an API version in the URI somewhere.
Some release manager at Facebook has dropped the ball.
Do you know examples of actual APIs in use that are properly RESTful? I like the idea but it seems that they would require smarter client libraries, among other things.
Ok, but I need to explicitly flush the key or clear the cache using `clearSyncCache` ? (will work even on memoized object )? Also, I've seen there is no commit since 2014. Is the project still maintained ?
I'm very glad we can now have nice stack traces! Since they do have overhead any chance there will be an optimization level / flag to turn them off? I don't want to have to redefine built in functions without the `HasCallStack` constraint when performance is critical.
tasty's [withResource](http://hackage.haskell.org/package/tasty-0.11.2.3/docs/Test-Tasty.html#v:withResource) can be used to ensure that something is running in the background during a test or set of tests.
The only thing you have to do is create your `MVar` and forked threads in a context where a `Map ph k v` is in scope. Here's an example where two workers add and remove edges without violating the "all edge targets present" guarantee: {-# LANGUAGE ScopedTypeVariables, RankNTypes #-} module Main where import Data.Map.Justified import qualified Data.Map as M import Control.Concurrent import Control.Concurrent.MVar import System.Random (randomRIO) import Control.Monad (forever) type Adj = M.Map Int [Int] type Graph ph = Map ph Int [Key ph Int] type Node ph = Key ph Int randomNode :: Graph ph -&gt; IO (Node ph) randomNode m = case keys m of [] -&gt; error "barf" xs -&gt; (xs !!) &lt;$&gt; randomRIO (0, length xs - 1) maker :: MVar (Graph ph) -&gt; IO () maker g = forever $ modifyMVar_ g $ \graph -&gt; do src &lt;- randomNode graph tgt &lt;- randomNode graph return $ adjust (tgt:) src graph taker :: MVar (Graph ph) -&gt; IO () taker g = forever $ modifyMVar_ g $ \graph -&gt; do src &lt;- randomNode graph tgt &lt;- randomNode graph return $ adjust (filter (/= tgt)) src graph withNodes :: [Int] -&gt; (forall ph. Graph ph -&gt; t) -&gt; t withNodes ks action = either barf id (withRecMap initial action) where initial = M.fromList (zip ks (repeat [])) barf = error "should not happen" adjacencyMap :: Graph ph -&gt; Adj adjacencyMap = fmap (map theKey) . theMap main :: IO () main = do myGraph :: Adj &lt;- withNodes [1..100] $ \graph -&gt; do g &lt;- newMVar graph workers &lt;- mapM forkIO [maker g, taker g] threadDelay 10000 mapM_ killThread workers adjacencyMap &lt;$&gt; readMVar g putStrLn (show myGraph) 
Honestly I'd be fine with just disallowing records on sum types altogether. If at some point we allow people to create lenses through something like record syntax then it would be cool to have them. But for now they just seem dangerous and too likely to be enabled by newer users who don't realize that they are now using `fromJust` all over the place.
I usually use `snap-server` (that is Snap without all the framework stuff).
I agree.
I've done something similar with template Haskell using Cassandra, but I only generated the types. I also need to update the Cassandra library to be based on user provided types instead of tuples.
We needed to abstract this to multiple db drivers I think.
On that topic, I dislike how you can't really pass around the information that you have one specific constructor of a type, so you end up having to do things like pass around a tuple of the values contained by that constructor. Or do things like replacing: data Foo = Bar Int Bool | Baz String Char With: data Foo = FooBar Bar | FooBaz Baz data Bar = Bar Int Bool data Baz = Baz String Char You can see very strong evidence of this when dealing with lenses, as with the former code a `_Bar` lens will have to be of type `((Int, Bool) -&gt; f (Int, Bool)) -&gt; Foo -&gt; f Foo`. This has numerous downsides, such as performance, since you have to copy all the fields out of the (potentially tightly packed) constructor into a tuple, and also type safety / reasoning since once extracted a `data Foo = ... Bar Int Int ...` would be identical to `data Foo = ... Baz Int Int ...`. It seems like it would be much much nicer to be able to pass it through with only the type modified but it still residing at the exact same spot in memory. Perhaps something like having: data Foo = Bar Int Bool | Baz String Char also create types `Bar` and `Baz` (or with some sort of prefix) and creating something like `Bar.X` and `Baz.X` constructors / destructors that have identical run time representation to the equivalent types in `Foo`. Now you get all the conciseness that you normally get when defining data types, but with the niceness and performance of the more verbose definition. Then you would also allow GHC to convert `foo (Bar a b) = myFunction $ Bar.X a b` into `foo b@(Bar {}) = myFunction $ unsafeCoerce b` to make things like the previously mentioned lenses perform better. You could also allow `coerce :: Bar -&gt; Foo` although obviously not the other way round. You could even also allow doing something like `foo (Foo.Bar b) = myFunction b` where `myFunction :: Bar -&gt; ...` for when you do just want to pass on the constructor with additional type information (at no runtime cost besides just making sure you actually have a `Bar`). So `Foo.Bar` would essentially be equivalent to `FooBar` from above, `Bar.X` would be equivalent to `Bar` and `Bar` would be equivalent to basically an invertible `(FooBar .) . Bar`. But actually with potentially even better performance than even the more verbose version due to the whole "same spot in memory and internally unsafeCoerce of sorts" thing.
In that case are you depending on `A.B.C` of every package you are importing implicitly? because depending on `A.B` violates the PVP contract, since going from `A.B.x` to `A.B.y` could make your code stop compiling.
yes, `flushCached` explicitly flush the key Yep it is actively maintained. use the hackage version. The last upload to hackage is from 2017. 
Thanks
Thanks for the reply. Your explanation makes sense to me. A follow up question comes from this. So the argument is that transient, due to not making types for effects explicit, allows for arbitrary composition of "runners" and effects. I could be completelt misunderstanding because I dont know transient; However I disagree with this. Transient only gives the *appearance* of fully comparable effects - the order of composition of effects is actually well defined. In its global "runner" implementation. When it sees a nesting of effects it can globally view the "effect stack" and chose the order to compose the effects in so that it makes sense and the code works. So a user can't really say compose e1 . e2, and e2 . e1. Instead it can say compose effects (e1, e2) and transient will use it's built in order for combining them correctly. [Note this is an assumption of mine that I could be very wrong on]. So I believe benefit transient has is am internal global well ordering of effects which is in its intetnals. If it didnt, then transient and it's flexible compability could be easily implemented with types representing effects (which it cant). If so the solution would be disallow nesting of effects in types. Allow only flat lists (or sets) of types. Replace all "runner" implementations with a call to a "global runner" which selects which effect to run next based on knowledge of the full effect list and choses and order which will be successful. It's not 100% flexibility in comparability as you must define the set of supported effects first to provide an ordering, but once you've done that any composibility variationd within that set should now work. Again, I may be 100% off base, but do appreciate you taking the time to explain this. 
Then that's not an implicit parameter. Just use a normal variable.
I'm guessing what they want is to be able to do: f = ?a :: Int ?a = 5 print f ?a = 10 print f With that said I'm not a big fan of implicit parameters as a whole so I would suggest just doing `f a` explicitly instead.
The problem is that they are immutable values once the program is RAN. IO is still occurring and should be marked as such. Also, a function that uses getArgs/getProgName/getExecutablePath wouldn't always return the same output for a certain input. This breaks the purity that Haskell is built around, which is why the functions return values in IO.
Unfortunately, things like System.Info.os violate this principle. 
&gt; The problem is that they are immutable values once the program is RAN. IO is still occurring and should be marked as such. But IO isn't occurring is it? I'm pretty sure the arguments are passed directly into the runtime so they are actually available to the Haskell program without any true `IO` necessary. &gt; Also, a function that uses getArgs/getProgName/getExecutablePath wouldn't always return the same output for a certain input. This breaks the purity that Haskell is built around, which is why the functions return values in IO. But it would for any single execution of the program, and IIRC that is the purity that people actually care about. I'm pretty sure CPP and TemplateHaskell and similar can all violate the much more restrictive definition of purity you have there.
So I guess this partly comes down to personal preference, but it should probably be as consistent as possible throughout the Haskell ecosystem. Should purity hold across a single execution of a program. Or should purity hold across a compilation of a program and an arbitrary number of executions. Or even more strictly should purity hold across all compilations as long as the source isn't changed. In my view since the very idea of executing a program at all is deeply grounded in impurity that having purity hold throughout a single execution of the program is more than sufficient. So I would say `System.Info.os` got it right. Unless I am missing something.
if you think they're safe, you can wrap them in unsafePerformIO. but idk what happens when getArgs is executed, there's probably a reason. does it read from some handle that might not be available and thus can fail, or that's actually mutable? does it read from some cached / fully evaluated memory location? if the latter, the difference is more philosophical. it's constant within a running program, but not across program runs. as mentioned, operating system and architecture are similarly available as "even more" deterministic constants, but they're not in IO. 
`System.Info.os` is a value added at compile time and can be deduced from analysis of the static binary. It is tantamount to declaring a constant in-code. `getArgs` read mutable memory passed to program execution on the base of the stack as program execution begins. It's not found in the static binary. Hence it is IO. Contrast/compare a `peek` of an FFI call which returns the `argc` passed to `main()` in C — that's more or less what happens.
&gt; but IO isn't occurring is it? A program could technically (via FFI to C, say) modify argv. (At least, I think that's well-specified, but now that I think about it, I'm getting less and less certain... ) EDIT: I think I might have the *actual* real answer: In e.g. [C++](http://en.cppreference.com/w/cpp/language/main_function) (that's a link) the argument isn't taken as "const", which means that you are *technically* allowed to modify it. Regardless of what the actual OS does, it'll at least have to supply (mutable) buffers to support that, and as Haskell is basically subservient to POSIX/WinXX it adheres to that, for practical reasons, I suppose.
Good question. I've wondered the same.
&gt; I'm pretty sure the arguments are passed directly into the runtime That's how it looks like in C, with `int main(int argc, char** argv) {...}` receiving the arguments directly from its caller. But if we look at [the implementation](https://www.stackage.org/haddock/lts-8.19/base-4.9.1.0/src/System-Environment.html#getArgs), we see that it performs C calls and then some pointer manipulation and `peek`s, and the latter are definitely IO operations. That being said, I believe that the value of tagging some computations with IO and not others is that it makes debugging easier by indicating whether a call may or may not have some mysterious interactions with other parts of the program. Calling `getArgs` and friends never has mysterious consequences, so I believe wrapping them in `unsafePerformIO` would be morally correct.
&gt; does it read from some handle that might not be available and thus can fail, or that's actually mutable? If it does do something like that (which it very well might) then IMO that should be added to the documentation to avoid anyone in future having the same question I currently have, and it should of course stay in IO. And on the philosophical side of things IMO executable a program is so inherently impure that I personally think purity only matters when you are talking about a single execution of a program. I'd be interested to hear arguments against this philosophy though.
agreed
That's a good point, if that is true then that answers my question, and preferably should be added to the documentation.
I do see how its a little bit more subjective. In my view you could morally see arguments passed to the executables as somewhat of a second compilation stage. As in `./foo bar` and `./foo baz` are fundamentally two different programs that happen to share a binary. You are correct that you can analyze the `System.Info.os` value by looking at the binary. But you could also argue that you can analyze the value of `args` by looking at the binary and looking at the argument you are about to pass into the executable.
I guess one argument in favor of having functions like this (and also `System.Info.*` functions) in `IO` is the idea of predictability and consistency. It is generally annoying to have behavior change depending on the environment, like for example having to do a bunch of `CPP` stuff to make a project that you can compile with both GHC and GHCJS, and a lot of people assume that `Int` is 64 bit even though technically that can vary. Having a write once, compile anywhere, and obtain the same resulting program (by program I am talking about what there is prior to GHC's runtime actually executing the IO actions, I realize that the actual RESULT will change no matter what when doing things like `getArgs` and `os`) is nice. So perhaps it could be argued on consistency and predictability grounds that anything that varies between environments should be in `IO` (or some other `IO`-like monad that can be converted into an `IO` monad that models the notion of variability between environments).
Would be interested in that too, but seems like Haskell ecosystem is too complicated for that. For example is there an easy way to build binary distributions? Like build a set of packages (Haskell libraries) on first PC and somehow install/make them usable on a second PC for building projects using those libraries. Cabal sandboxes are not simply copyable AFAIK, as well as .stack-work directories or stack's global snapshot cache, because they all use absolute paths internally.
After further thought I am not so sure. Technically using `System.Info.os` you could have something weird such as a module that exports the variable `five :: Int` that is actually only on windows, and 4 on other platforms, without any `unsafePerformIO`. Likewise you could have an `add :: Int -&gt; Int -&gt; Int` function that doubles the result whenever the `--double-all-my-addition` flag is passed into the runtime if `args` is not in `IO`. So maybe purity should be something that is tied to any piece of source code + dependencies combination, and should be runtime argument and environment independent. That way we maximize predictability and consistency, at the expense of flexibility that probably isn't that crucial anyway. Things like `Int` sometimes being 64 and sometimes being 32 bit, the huge amount of `CPP` that is needed for hybrid GHC/GHCJS projects, and similar are good indications of how annoying things can be when stuff outside of `IO` varies between environments. Maybe `System.Info.*` should all be in IO, I'm honestly not sure which way to go.
The values returned by these actions can vary at runtime via withArgs and withProgName, both found in System.Environment. 
Ah, ok yeah that answers my question. Thanks!
This isn't js or php though? 
I think it would have to be let f :: (?a :: Int) =&gt; Int; f = ?a But I do not know why subsequently one gets Prelude&gt; let ?a = 5 Prelude&gt; f &lt;interactive&gt;:16:1: Unbound implicit parameter (?a::Int) arising from a use of `f' In the expression: f In an equation for `it': it = f Why is `?a` not in scope?
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [AHartNtkn/Dependent-Binary-Lambda-Calculus/.../**DBLC.hs** (master → b18533c)](https://github.com/AHartNtkn/Dependent-Binary-Lambda-Calculus/blob/b18533c8963cec545ae28325cb7c0c76b3bf7781/DBLC.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dl2vfmm.)^.
Re: the "sysinfo" thing. I'm not actually sure that a program *can* take advantage of 'dynamically' changing things like the MAX_FD (or whatever it's called). Can they? I'd love to hear examples!
One area where the community could certainly help is to participate in the sort of pre-release curation system suggested by /u/phadej. One of the biggest issues GHC has is finding a buildable yet comprehensive set of packages for regular testing. Having even a small set of maintainers track GHC HEAD would allow us to put in place automated testing of nightlies which would help quite a bit.
They should be arguments to `main`, just like in C
Some of the caching you are talking about is covered by Nix, I suspect. I think there might be some way to implement an initial hack using file copying, if the developer's machine and the cluster of spot instances are also using the same linux distribution and have haskell build tools preinstalled. I don't know enough about the guts of how GHC and Cabal build to say for sure.
I remember one case when modifying argv was used to hide password. After launching `sybase -u user -p password` doing `ps` will display `sybase -u root -p ********`. The trick was to modify argv to replace each character of the password by `*`. Those modifications will then be picked up by ps. I'm using the past because I don't think it works anymore. That probably depends on the OS and the version. (This was in the 90s on UNIX. It probably matters at that time because some people where still sharing computers using terminals).
I remember one case when modifying argv was used to hide password. After launching `sybase -u user -p password` doing `ps` will display `sybase -u root -p ********`. The trick was to modify argv to replace each character of the password by `*`. Those modifications will then be picked up by ps. I'm using the past because I don't think it works anymore. That probably depends on the OS and the version. (This was in the 90s on UNIX. It probably matters at that time because some people where still sharing computers using terminals).
It's not about purity but referencial transparency. Having a function behaving the same across executions is fundamental for testings. Let's say args is pure, then I can write add x y = x + y + lengh args - 1 I can write some test and verify that add 0 0 == 0 add 1 3 == 4 etc. But If run my program and pass a parameter , add 0 0 is now worth 1 ???? If I come back and run the test suite, add 0 0 equals to 0 again. So no, holding purity through back single execution is not sufficient. It's the same for environment variable, configuration files, locales etc .. 
That would be nice! But how would one generate random well typed expressions to the user? ---- EDIT: Actually that's maybe not a problem if you know the grammar, but how would one generate random well typed expressions with known difficulty? I guess this would have to use some kind of item-response measure of difficulty. That's actually a very cool idea. ---- EDIT 2: There's also the problem of how to generate random well typed expressions of known difficulty that are actual useful practice examples and not just some mush of operators that one would never use in actual code. That can also be solved I think... 
&gt;So I believe benefit transient has is am internal global well ordering of effects which is in its internals. It is simpler than that. Really the effects are created by stand-alone primitives. in Transient the state effect for an data type (x :: a) is stablished by `setState x` and run upto the end of the computation unless `delState`deletes it. But, for the rest of the statement that do not use this effect, this state effect is transparent. What transient has is what I call "high level effects" like parallelism, concurrency and distributed computing that are created by primitives that use simpler effects, like state, continuations and Maybe. They are not like the effects of a monad transformer or in an effect monad, which need a runner or an interpreter for each effect and each effect is independent of the other. These kinds of effect systems do not compose using binary lawful operators like applicative, alternative and bind unless both operands have the same effect stack. In transient all the effects are implemented over a single monad with a single type, so every term combine with every other no matter the effects that each one implement. well, there are three monads, but the other two are auxiliary.
I'm not sure a 70% reduction in effort can be considered trivial. I'd call it a monumental speedup!
Yes, that's the one! Hide the password and/or cmdline arguments and/or ENV variables specified before the command (oh, wait, you can't! Oh, no!). It's absurd as far as *actual* security goes, yet there we are. In 2017... still working against C "calling conventions".)
No, it various depending on where you run the executable. It is not a compile time constant. 
This is no more fundamental than the preference /u/Tysonzero was talking about. It depends on what the scope of the environment in which you're running your tests is. We're already not just testing the logical validity of the written program since we also run tests when deploying to different platforms, running different versions of the compiler etc. Saying that command line parameters are a "constant" in the scope of your testing environment is perfectly legitimate. Tagging `args` with IO, in my opinion, doesn't make it's type more honest. It's just a choice to describe a different characteristic of the value. `args :: [String]` implies that the value is a constant (albeit in one run, of one version of the program, on one platform, etc...) and that lets us do some things; like let floating for example. `args :: IO [String]` tells us that the value will not be the same every time, but we lose the information that it WILL be the same during the execution of the program.
They are mutable values. That works in C but would not in Haskell (perhaps with an unwieldy type like `IORef String`?).
A program can modify basically anything via FFI.
No reason for them to be mutable. Everything is a mutable buffer underneath. Pass the arguments in as pure values and don't provide an API to mutate them.
IIRC, this still *works* on Linux, but it's still very much encouraged not to put secret information on the command-line.
Funny enough I recently rewrote how getArgs works [on Windows](https://ghc.haskell.org/trac/ghc/ticket/13940) to match Linux behaviour . AMA. The code is [pretty simple](https://github.com/ghc/ghc/blob/master/libraries/base/System/Environment.hs) if you want to take a look. Since we need to load these values from the C parts of the runtime these do actual IO and dont just carry the type for no reason. The System.Info definitions don't actually do IO so there really would be no advantage to make them use IO 
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [ghc/ghc/.../**Environment.hs** (master → 884bd21)](https://github.com/ghc/ghc/blob/884bd21a917f607b5a44e038e06f78d0b765ea63/libraries/base/System/Environment.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [ghc/ghc/.../**Environment.hs** (master → 884bd21)](https://github.com/ghc/ghc/blob/884bd21a917f607b5a44e038e06f78d0b765ea63/libraries/base/System/Environment.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [ghc/ghc/.../**Environment.hs** (master → 884bd21)](https://github.com/ghc/ghc/blob/884bd21a917f607b5a44e038e06f78d0b765ea63/libraries/base/System/Environment.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
Yes... but is UB, DB, or just B? (As it turns out, semantics for actual processors is quite difficult, esp. if you have to adhere to some of the absurd conventions-turned-pratice from C... but whatever. Rise up!)
But it CAN differ during execution. (eg withArgs).
Sure, and that's a definite reason why it should be in IO. Lets imagine that it can't.
&gt; The other reason we don’t have a shortage of libraries is that we’ve stayed so close to Haskell syntax that the vast majority of packages on Hackage (Haskell’s package repository) compile out-of-the-box! Is Eta intending on tracking GHC/Haskell, or is it expected to diverge significantly?
Words acquire meaning by the way they're used. "Evangelist" has been used in many non-religious contexts such that I no longer associate the word with religion at all unless the context clearly suggests it. 
&gt; don't provide an API to mutate them That failed. See [this comment](https://www.reddit.com/r/haskell/comments/6r6t08/comment/dl2uc8o).
That wouldn't be a problem with what they're saying. Eliminate getArgs and withArgs, etc, and just make the type of main [String] -&gt; IO () or something.
Yes. This. You can do it today with a shim (main = getArgs &gt;&gt;= realMain)
Stack is brilliant. Try Haskell for Mac if you just want to try things or need an IDE. Otherwise if you use vim there's a load of great plugins.
I would argue one advantage of tagging them with `IO` is to indicate that they vary between environments, and thus only meet some peoples definition of pure. I'm still a bit on the fence about the whole issue but that is one thing to consider.
I'm not sure how receptive /r/haskell is about Idris posts, but I consider it basically a side language of this community so I thought I'd post it here. This is something I've been waiting for since a long time and am extremely happy it exists now!
My vote for supporting this in haskell is to build support within stack to read / write projects to a zip archive, and let build scripts handle moving that between machines. Without better cross compilation support, this seems like the sort of feature that would rapidly spiral into a nest of complexity better addressed by CI task runners. 
I have mixed feelings about Idris content here. I'm definitely pro-Idris, but they have their own subreddit and I follow it in addition to Haskell. On the other hand, I like that talking about Idris here from time to time increases visibility of Idris.
I dev on a mac, and would suggest: Remove all global ghc installs and use stack exclusively. Learn the stack api - everything you need is there. stack build --file-watch, for example, is how you get a tight compile for those quick scripts. stack templates are how you quickly create them. I use emacs, via spacemacs, with intero as the ghci replacement, and its a solid ide that pretty much works out of the box. In a typical session, I have several ghci's open, mostly in terminals, one just for checking types, one for holding state, emacs, a compile loop watching file changes, and then travis checking if committed code is linux friendly. Happy days. Sounds like a sweet gig you have. 
I develop in haskell exclusively on OS X and I second all of this. Are you planning to do regular Haskell or, given that you're coming from the frontend, perhaps GHCJS? If so... let me know if you get it all working nice :)
You're thinking a bit too generically, I think. It should be pretty easy to make a few hundred exercises manually and label them with certain difficulties. All that remains after that is to fuzz them (add a random type, change types so people learn to not associate the idea with certain type variables, a converter between concrete and generic types...). Even a very simple fuzzer can turn 500 exercises into 2,000+. Maybe at some point in the future that could be extended to generating full blown problems, but I don't even think that would be needed (just fun)
You're thinking a bit too generically, I think. It should be pretty easy to make a few hundred exercises manually and label them with certain difficulties. All that remains after that is to fuzz them (add a random type, change types so people learn to not associate the idea with certain type variables, a converter between concrete and generic types, adding small things that change the code in "trivial" ways like (,) vs curried signatures, etc...). Even a very simple fuzzer can turn 500 exercises into 2,000+. Maybe at some point in the future that could be extended to generating full blown problems, but I don't even think that would be needed (just fun)
What's the story for Template Haskell and Eta? Does it just work?
I concur; It goes great in burritos and we all know Haskell people *love* their burritos
I just noticed today that Idris 1.1 release recently. Glad to see the language progressing. I had to grioan (both a grin, and a groan) that the Javascript backend for Idris received a lot of improvements while I didn't see any improvements to the native backend specifically mentioned. Javascript is taking over even Idris! ;) I'm mostly joking here. I would like to know what, if any, improvements are desired for the reference runtime though. Seeing reports of how good the Javascript codegen is makes me think the native runtime could do some pretty amazing things as well.
One thing I should point out is that `react-flux` is completely unmaintained and has bugs that will not be fixed. The main fork that I know of and that is well maintained is [`react-hs`](https://github.com/liqula/react-flux).
HTTP (the first one on your list) is the oldest and I think it's safe to say you don't want to use it. wreq and req are built around http-client, which I think indicates that you want one of them and not http-client :). I use wreq, and I hadn't heard of req until today. I didn't know lenses when I started using wreq, and I found it easy to use still. That'd be my recommendation.
I've only seriously used `Wreq` but I think it's about as close as you're going to get to `requests` in Haskell (which is why I picked it).
According to the [FAQ](http://eta-lang.org/docs/html/faq.html) they &gt; want to extend the type system to support row-type polymorphism and new syntax for anonymous record types as they have shown to be very successful in Elm and PureScript. It sounds like they do plan on contributing their charges back to GHC though.
My inference from the [FAQ](http://eta-lang.org/docs/html/faq.html) is that Template Haskell is not currently supported and not a priority.
Honestly, I would just stick with Atom for the moment. You can catch type errors on save and get types on hover, plus get code completion. Enough to keep me happy.... though I don't use Haskell professionally :)
Hi! Sorry for the late response, I wasn't checking reddit this time, and it refuses to send a notification to e-mail. I'm glad to receive your feedback :) I'm collecting feedback on Google Docs, and will update Redditors on my plans. I'm starting to write the 6th chapter these days, and will also be correcting previous chapters according to the comments you gave here and on Google Docs.
Yeah I find a lot of value in only having one editor so I'm trying to keep Atom around. Unless I find a superior editor for both JS and Haskell, and even then I place a lot of stock in using JavaScript tools when possible ;) Any haskell packages you would recommend for Atom?
For now, I changed my place of work, and this will allow me to continue writing the book. I'll post some update on Reddit these days.
&gt; A program could technically (via FFI to C, say) modify… …any bit in the accessible memory space. Therefore not even Haskell 'Int' and friends are immutable and access to those values should only happen in IO. Right? At some point you have to say "don't do stupid shit". Should modifying argv etc be considered "stupid shit"? I'd say yes. But I guess other people's opinions differ…
Yes, `System.Info` should be in `IO`. Consider code that automatically migrates between nodes of a distributed system as another argument.
I am using the following packages - https://atom.io/packages/ide-haskell https://atom.io/packages/haskell-ghc-mod https://atom.io/packages/autocomplete-haskell https://atom.io/packages/language-haskell Hope that helps :)
Not sure how reliable this is but it should be a start. Warning this is not an easy thing. https://github.com/nh2/call-haskell-from-anything/blob/master/README.md I'd strongly recommend working out if Haskell is a good fit for your project, if so it might be worth converting it over. It might not though. Good luck
I for one appreciate it, because I like the cross-pollination og ideas and Idris feels fairly close in spirit to Haskell :) It’s also interesting to hear from other Haskellers that wouldn’t otherwise follow Idris, and if they have any “you can achieve something similar with ... in Haskell”.
I use docker for that. All team members have access to the same base image, and it's also the one used by the CI build.
Thank you for the advice :)
For that particular use-case, I'm not sure if a *distributed* build - i.e. a build where multiple machines cooperate - will help. When you change a few lines, it's probably a fairly serial dependency chain of work that needs to be done. So what you need is a powerful machine for serial execution, and you can certainly use a spot instance for that (given that you can wait a few minutes to get it online). I use docker for everything, and for this particular use-case I'd probably do something like: - I already use docker for CI, so there will be containers lying around for each branch. I can pick one of those, sync the code, and build. - Always prefer one powerful machine to multiple weak ones. In this case, one machine should be enough. But basically, if you need fast builds when you have multiple developers, tying docker images to branches and commits, and some scripting should make it possible to basically do quick builds starting from any commit. For the build-from-scratch use-case, `stack` can already build in docker, so surely some hooks and scripts can be added to make those builds be distributed. all that is needed is shipping docker layers around. 
Stay away from `http`. It's too low-level, and does not support SSL/TLS. `http-client` also does not support HTTPS, but `http-client-tls` does. I would use this combination in library code. It works quite well. `wreq` is very end-user-friendly, and has a very beautiful syntax, etc. It *does* have `lens` as a dependency (to facilitate very easy manipulation of JSON payloads and HTTP headers), so I'd advise against using it as a dependency in library code, for well-known reasons. I ain't never heard of `req` before... 
How would you start cleaning up a computer that didn't use stack or cabal sandboxes? Such that, as you say, all global ghc installs are removed and all work happens through stack?
You could describe the API using `servant` and then generate the client with `servant-client`.
It's true, because I have some C++ code doing exactly this, since it's the only posix portable way I could find to change how a program's name appears in the output of `ps`. So yes, argv, the program name and similar things can be mutated during a program's runtimes. As for updating the documentation, feel free to clarify the behaviour and send a patch to either github/GHC issue tracker/Phabricator. Documentation patches are always welcome and easy to get added :)
I second this. You will need to stack install the dependent pacakges and put ~/.local/bin on your path
It looks like someone has one here, though I'm not sure how complete it is: https://github.com/sebastianmika/pyduckling
There's an LLVM backend in the works, btw: https://github.com/idris-hackers/idris-llvm
Woot ! &lt;3 I am going to have a look at it !
Sure, but there *are* programs out there that actually do this. (E.g. to wipe a cmdline-provided password). I'm just *sayin'*...
`./foo bar` and `./foo baz` are, canonically, different invocations of the same function, with differing arguments. (Indeed, `mv foo quux; ./quux bar` is just the same, c.f. `argv[0]`.) Implementation details matter here (c.f. the POSIX ABI.)
... While I can accept that one might be able to transfer an excecutable between a Linux Mint and a Ubuntu box; you will very quickly run into a wall called ABI differences.
`MAX_FD` is either a compile-time constant in C, variating over target architecture, or an environment variable (and `getEnv`(?) is `IO`.)
Start deleting and keep trying `which ghc` and `which cabal` till they're not there anymore. Check and remove anything on the path that says ghc or cabal, and remove ~/.cabal and ~/.ghc This is *mostly* psychological as stack will ignore your global ghc, but then the temptation to runghc, cabal install or whatever can get you all mixed up. Once every major lts version change I reinstall all of my haskell executables (hlint, pandoc, intero): `stack install hlint` etc in global, and stack installs everything in ~/.local/bin. I'm doing this now using lts-8.24. A major thing that can bite a fresh install is a weird version of c compiler that hang around (installed by homebrew usually). Out of the box, macos c compiler is clang which is at `/usr/bin/gcc` but ghc (and stack) is hard coded to look in `/usr/local/bin/gcc`. `ln -s /usr/bin/gcc /usr/local/bin/gcc` is the direct fix. 
It's a sysconf variable (for anything remotely POSIXy), but if your program relies on the old static MAX_FD then you're in for a fun surprise.
at hackage: **Reflex is a high-performance, deterministic, higher-order Functional Reactive Programming system** https://hackage.haskell.org/package/reflex at facebook: **Reflex is a new programming language for reactive programming** ??? Otherwise it's interesting.
Please, please, please fix the records problem in Haskell!
Use `wreq` for the HTTP stuff and `aeson` for the JSON stuff.
Tnx. This helps. Maybe it should be mentioned in the README or on the Haskell wiki (JS problem page)
So why is not System.Info updated to export at least something like `arch' = pure arch :: IO String`?
&gt; `args :: IO [String]` tells us that the value will not be the same every time, but we lose the information that it WILL be the same during the execution of the program. Indeed. Everybody agree that `IO` is somehow too broad. However, this problem (of things constant during execution) is called sometimes the *configuration problem* and some solutions can be found [here](https://www.joachim-breitner.de/blog/443-A_Solution_to_the_Configuration_Problem_in_Haskell) and [there](https://www.schoolofhaskell.com/user/thoughtpolice/using-reflection). Having said that, it is probably a legitimate use of `unsafePerformIO`. I would however personally prefer all functions using programe named passed as a parameter ... 
The very best example we have is the web itself. All your browser comes with, fundamentally, is knowledge of specific data types (HTML, CSS, JavaScript, usually). There are plug-ins that teach it new data types (e.g. Flash, Java Applet, PDF, etc.). It knows no URIs at all. The user gives a URI in, the browser knows to do a "GET" on it. The HTML datatype has the concept of hyper links, forms, etc., that can cause additional HTTP actions to happen depending on what the user does. So I think it's less that the client libraries have to be smarter and more that the focus is on the data contracts and not focused on invocation, if that makes sense. Having said all that, I think a critical thing that a lot of people miss, that probably contributes heavily to the overall confusion, is that REST isn't for everything. It has specific attributes, specific trade offs, etc. and should only be applied in those cases. Trying to make every possible API into a "REST" API is probably what caused this mess to begin with.
&gt; But IO isn't occurring is it? Haskell `IO` covers more than input and output in the traditional teletype/disk drives/line printer sense - as another example, it lets you examine bottom "values" (as used in my [acme-smuggler](https://github.com/benclifford/acme-smuggler) package). From that perspective, `IO` is perhaps the wrong name and `KITCHENSINKOFEVIL` would be better...
Related, in the past I've wanted to be able to reason about a broader environment outside of the current Haskell process: * will this code always behave the same given that I'm not changing a particular set of files that are deployed alongside the executable? * or when using some database restored from a particular backup? * or compiled for a different CPU/runtime? Those different classes of immutability have been relevant in production projects before, but they aren't addressed well by the Haskell notion of immutability.
Wonderful. Anything I can do to help then? 
Relevant paper: Mirai Ikebuchi, Keisuke Nakano "On repetitive right application of B-terms" 2017 https://arxiv.org/pdf/1703.10938.pdf Something serious is going on.
&gt; Isn't REST a subset of HTTP-RPC? No, RPC is invocation-focused. REST is focused only on the data. Any invocations that can/do happen are data specific (e.g. clicking a hyperlink for the HTML type or submitting an HTML form). The client doesn't know or care what the URI is, it simply knows that for this data type if the user wants to click on "reply" I do a POST using the well known key/value POST format to whatever URI is in the form's "action" field. As a partial aside: REST [doesn't even require HTTP](http://restcookbook.com/Miscellaneous/rest-and-http/). &gt;And the breakage is avoided by putting an API version in the URI somewhere. Sure, that's what you would do for an HTTP-RPC application. For REST, I would be wondering what kind of clients you have that hold their copies of data so long that it's actually surviving multiple URI-space rewritings [1]. REST, like all architectural patterns, is a set of trade offs but one of the things it buys you is that you can completely change where all URIs are on the server and the clients don't even need to be restarted, much less recompiled. [1] And further wondering why you can't just leave the old URIs and have them return current data types. When you see in the logs that no clients use the old, unreferenced URIs anymore then you can retire them.
"Haskell for &lt;other platforms&gt;" would be great, but Haskell for Mac itself is rather closely tied to macOS (i.e., that vast majority of the code depends on macOS frameworks). That is the only way in which you can get truly great UI/UX IMHO.
That is an interesting question.
Cool trick to make the monad linear to basically simulate linearity on the type itself, instead of on the arrow.
Does `wreq` allow you to control the `Manager`? That's pretty critical. I only skimmed the tutorials, but I didn't see anything about it.
I like when someone talk about composition, modularity and separation of concern, so here you have meant that the code is modular in term of component where each component has its logic and view. IOW, each component is a function that has the logic and view embedded and then those functions can be composed to build the whole application
I prefer the coinductive record approach for modelling this kinda thing, but that's just me :)
Is this what you're looking for? http://www.serpentine.com/wreq/doc/wreq-0.1.0.0/html/Network-Wreq-Types.html#v:manager
I was planning to use hspec, but only because that's the only testing framework I used so far (in combination with QuickCheck of course). It seems tasty requires another testing framework on top anyway (like HUnit, QC, or SmallCheck). I wonder what's the added value of tasty wrt plain hspec...
Yes it does (not used it though). But for this particular newbie use-case isn't an http network manager an overkill?
Wonderful news. Congrats /u/chak and Tweag!
There's a lot of discussion in here about "`IO` actions that always return the same result". I think it's worth noting that *any* expression of the form foo = unsafePerformIO (whatever ...) always returns the same result, because it's memoized. This should make us suspicious of taking something out of `IO` just because it "always returns the same result". My personal preference would for anything that has different values on different runs on the same system to be in `IO`. If it's different runs on *different* systems then I'd lean to `IO` but I realise this is a bit awkward, for example with `INT_MAX`. 
Worth comparing this approach to [Paykin and Zdancewic's *Linearity Monad*](https://www.cis.upenn.edu/~jpaykin/papers/pz_linearity_monad_2017.pdf) - they discuss some of the differences in section 7.4
He said REST, but you're describing [HTTP-RPC](https://jimmybogard.com/swagger-the-rest-kryptonite/).
Oh sure. Your comment just sounded like a much more general-case recommendation. You're right though.
Roughly speaking, yes. But I would argue that you still have view separation because of the HTML/CSS split. That's kind of the whole point of CSS in the first place, right? HTML defines the structure of the document and CSS defines how it looks (the view).
What is "coinductive record approach" ?
Is it actually faster than using all those command line arguments in Stack or Cabal? Seems like it's the lack of such arguments that probably takes more time, not Stack or Cabal.
Yes, Alga allows to manipulate graphs, guaranteeing that the consistency invariant holds. Specifically, `Graph a` is a Functor, so you can `fmap addMonster` over it, where `addMonster` can examine and modify vertices as if we were `fmap`-ing over a list.
Coinductive records are great, but if we encode state-carrying types at the level of the type system then it would allow us to detect state-related bugs at compile time. 
Wow!
Coalgebras are used to model state as "coinductive data", which are similar to frp-like streams that change over time. So a coinductive record would be a record whose internal values change over time. http://www.haskellforall.com/2013/02/you-could-have-invented-comonads.html
I think it possible to do REST with servant. It let's you define arbitrary HTTP APIs. In fact they mention REST on their homepage. Servant would be my recommendation as it is a very elegant way of defining an api server and client. If however OP is looking to build a client for an existing API then perhaps some of the other suggestions may be more appropriate (though I think it would still be possible to go the servant-client route).
Mapping over the whole graph seems inefficient if I just want to target a particular room. Say, the monster only appears at the monster's den. If for that purpose l assign unique ids to each room, then I have the problem that justified-containers tries to solve. 
&gt;I think it possible to do REST with servant. Of course it is. &gt;In fact they mention REST on their homepage. Irrelevant. Swagger claims to document "REST" APIs when, in fact, using it means [you're not doing REST at all](http://blog.howarddierking.com/2016/10/07/swagger-ain-t-rest-is-that-ok/). &gt;Servant would be my recommendation as it is a very elegant way of defining an *api server and client*. And this is where it stops being REST. You can do HTTP-RPC this way, which is totally fine. Just please don't call it REST because if you're documenting the URI then you're not doing REST. If you're *generating client side code* from those URIs you're not doing anything even remotely related to REST. Doing REST would be documenting the data types that the client and server agree on. &gt;If however OP is looking to build a client for an existing API then perhaps some of the other suggestions may be more appropriate (though I think it would still be possible to go the servant-client route). If I were building a REST service in Haskell I would likely use Servant for my server because I like the type safety. I just wouldn't be generating client libraries with it, unless I wanted HTTP-RPC instead of REST.
Is the yesod-minimal template from stack to high level? I would think it's a simple REST project. 
That honestly just sounds like pedantics to me.
Well if there are different arguments then it seems as though there is no requirement that the results (I.e the Haskell code itself) have to be the same. I do think that for consistency putting all this stuff in IO makes sense, but just trying to get a satisfying answer. 
Hehe unfortunately I don't work on the project. In the meantime, there's the [first class records library](https://nikita-volkov.github.io/record/).
yes you are right. just curious to know how do you tackle the modularity: - choosing to make the component stateless so it can be reusable and the state is external - choosing the state to be modular so a component has its own state without needing to know about the whole application state
Why not generate client libraries? This seems preferable to me as they can be derived from the api spec thus removing a load of boilerplate. It seems to me that whether or not the client libraries generated are REST or RPC would come down to the architecture of the API and the code you write for the handlers. In fact from my understanding of REST and RPC it would be possible to have an api that is both (and maybe hence neither). Although I may be misunderstanding something about client generation being incompatible with REST. 
I try to keep as much state as possible local to the widget...i.e. minimize the scope of that state. The next question is about the widgets inputs and outputs. I approach this by thinking of the application in three layers: +------------------+ | Application Glue | +------------------+ | Business Widgets | +------------------+ | Reusable Widgets | +------------------+ Widgets in the bottom layer are reusable across applications, completely general, and adhere to the principle of least context. Ideally they live in a separate package ([reflex-dom-contrib](http://hackage.haskell.org/package/reflex-dom-contrib) for instance). Widgets in the second layer are composed of widgets in the first layer, but add on business logic specific to your application. These widgets still conform to the principle of least context because you might want to reuse them in multiple parts of your application. The top layer is where you glue everything together. In that layer you often don't want to use the principle of least context; you want to have uniformity of interface because you know that things will be very in flux as you build your application and you don't want to have to be changing type signatures all the time every time you add a new feature.
It's not, it's a trade off with large ramifications. The example I used in another thread was that if Facebook changes where all their URIs point, not only do you not have to recompile Firefox, you don't even need to restart it. So long as the underlying datatypes they use (i.e. HTML, CSS and Javascript) don't change, everything works fine. HTTP-RPC, by contrast is completely different. It's not data-centric anymore, the focus is on "method calls" and their signatures. If you add a new URI with some new feature, every client will have to be rebuilt to take advantage of it. If Facebook adds a new feature tomorrow we can have it with no modifications to our REST client (the web browser) at all.
&gt; `IO` is perhaps the wrong name We should rename it to `RO` for referentially opaque. KitchenSinkOfEvil has a negative connotation and doesn't roll off the tongue as well.
I find I sometimes wind up with a few types that are used most places throughout my project. When I make a change to one of the containing files, there often *is* a lot of parallel rebuilding going on. On a sufficiently large project distributing that might be a win.
I'm curious how it compares to GHCJS in terms of performance. I would actually expect it to produce faster code.
&gt; Why not generate client libraries? This seems preferable to me as they can be derived from the api spec thus removing a load of boilerplate. What would you be generating exactly? What the server and client have in common is (1) the data types (which you can generate, of course) and (2) the semantics of that data type (how do you generate this?). You're probably thinking about the URIs but this is just an implementation detail of the server and of no interest to the client. The URI is to REST what the client-side TCP port is to e.g. an FTP server. &gt;It seems to me that whether or not the client libraries generated are REST or RPC would come down to the architecture of the API and the code you write for the handlers. But with REST the API *is the data types and nothing else*. What WWW gives us is HTML, CSS, JavaScript, Flash, etc., etc. It doesn't give us URIs, these are part of some data type (e.g. the links that Google returns for a query) [1]. REST isn't a silver bullet, it's a specific architectural pattern with specific things it's good at doing. One of those things is decoupling client and server in such a way that they can radically evolve independently. Think about how different Facebook acts now than 4 or 5 years ago. Did you have to recompile your web browser for any of that? No, because HTML is still HTML, JavaScript is still JavaScript, etc. That is the only contract a REST server and client have with each other. [1] Before someone says it, yes I know URIs are part of the protocol but it isn't mandatory that REST be implemented on top of HTTP (which does care very much about URIs).
There's only way to find out :)
Oh nice, the last time I looked at it development of it seemed pretty much dead so I’m very happy to see that someone has picked it up.
Using servant I would define Haskell data types, a type safe api, and write handlers for each route of the API. JSON marshalling to and from Haskell data types can be derived, the server is derived from the API, clients can be derived in Haskell/JavaScript/arbitrary-languages that call the API. This leaves you with the non derivable work of writing handlers that provide server side logic. Whether or not your client application needs recompiled upon API updates seems like a separate issue from all of this including the REST/RPC debate. e.g. you add a route to your api -&gt; this generates new JavaScript client code -&gt; your UI team can then import this and use it in the client. I don't see how this would require any browser recompilation as its just more JavaScript as you say. I also don't see how you can get around having to recompile for example android/ios applications if you want to add your new api functionality. The REST/RPC architectural preference seems perhaps irrelevant to OPs original question which although mentioning REST I interpreted more as a request for API tooling recommendations. Although all of this REST/RPC chat is certainly philosophically interesting and I appreciate it :)
I'm pretty sure Idris is much faster given that it uses native JS closures. GHCJS implements them (with the STG runtime), but it is pretty much impossible to implement functions in JS itself in a way that is faster than the native implementation. Only WASM could change that, I guess.
I've read that post but I still have no clue what a coinductive record is. What am I missing?
&gt;, the server is derived from the API, clients can be derived in Haskell/JavaScript/arbitrary-languages that call the API. What you're calling the "API" is an RPC view of the API. The REST view of an "API" would simply be the data types that can be encountered (HTML is probably the most common of these). &gt;Whether or not your client application needs recompiled upon API updates seems like a separate issue from all of this including the REST/RPC debate It's not separate, it's the entire point of why Fielding wrote his original paper to begin with: to give a name to this pattern of decoupling clients and servers. &gt;you add a route to your api -&gt; this generates new JavaScript client code Fine. But this is RPC, not REST and we used to use SOAP/Corba/etc. this way. &gt;I don't see how this would require any browser recompilation as its just more JavaScript as you say. Don't you see how this is cheating? You don't have to "recompile" because JavaScript doesn't have the "compile" phase the way Haskell does. If your client is Haskell then you absolutely must recompile. &gt;I also don't see how you can get around having to recompile for example android/ios applications if you want to add your new api functionality. I'm not sure how to be more clear here. Does Facebook ever add new features? Yes, all the time. Do you ever recompile your REST client (aka: your web browser) to use them? Not at all. And Facebook isn't cheating by using JS to get around recompiling the clients, they are using data types that the web browser knows how to deal with so it can trivially handle changes in what you keep calling "the API" with no modification to the client (i.e. web browser). &gt;The REST/RPC architectural preference seems perhaps irrelevant to OPs original question It might be, but if we knew if the OP wanted RPC then we could recommend things like client generators that we can't recommend if OP actually wants REST. &gt;for API tooling recommendations You're talking RPC again. &gt;Although all of this REST/RPC chat is certainly philosophically interesting and I appreciate it :) For me the issue is one of "if everything is REST then nothing is REST". I can never tell if a server is actually a REST server, as it will no doubt claim, without a lot of inspection because this term is so horribly abused and REST so misunderstood.
Wow, cool idea, and well executed! Works great for me. FYI if you want to unobtrusively add styles without affecting a user's given terminal theme, your best bet is to use existing vty styles such as [`reverseVideo`](https://hackage.haskell.org/package/vty-5.16/docs/Graphics-Vty-Attributes.html#v:reverseVideo), [`dim`](https://hackage.haskell.org/package/vty-5.16/docs/Graphics-Vty-Attributes.html#v:dim), etc. I say this because actually querying for the current user's default colors is difficult and not provided by brick.
I guess now would be a good time to port the [idris atom plugin](https://github.com/idris-hackers/atom-language-idris) to idris, all the cool kids ([Typescript](https://github.com/TypeStrong/atom-typescript),[Purescript](https://github.com/nwolverson/atom-ide-purescript)) are doing it these days ( ͡° ͜ʖ ͡°) 
Every example I've seen of comonads so far has been a wrapper around a catastrophic space leak waiting to happen. Is there some literature out there talking about how to avoid that, or some use of comonads that's more constrained then shuffling along an entire universe of possibilities / former states in memory?
&gt; My understanding is that, VCache will computes the value when needed, but will not recompute if it has already been done "recently". That's the `auto-update` model as well. &gt; Another approach would be to compute the value at regular interval so it's ready when needed. That's very simple, just fork a thread to periodically perform some action :) The motivation behind `auto-update` is that if no one currently cares e.g. what the current time is, why waste time computing it?
Aha, you are right. There is no efficient vertex update at the moment. I've raised an issue to address this: https://github.com/snowleopard/alga/issues/20
~~It's like an FP version of an Object from OOP.~~ EDIT: [Counterargument to the 'comonads as objects' view](http://gelisam.blogspot.co.uk/2013/07/comonads-are-neighbourhoods-not-objects.html)
May be best then to just accept that REST (now) means something else. There are plenty of other properties that you can contrast between RESTful APIs and non-RESTful ones.
Well hopefully this thread will prevent some future mislabeling of APIs. You've certainly improved my understanding of the REST/RPC idea spaces. I get the incompatibility with client code generation now! TLDR: 1 - REST API = virtual state machine where operations correspond to state transitions 2 - RPC API = calling a function over http
I've never understood how people implement these from comonads though. While there are many comonad dissertations of varying levels online (and boy do they love to talk about life simulations), I see almost no discussions about how IO works in that world. In particular, what duplicate means in that context. I don't suppose you have a resource handy to help clear that up?
I disagree. We already have a perfectly good, and very descriptive name for the "something else": RPC. To be even more specific we can call it HTTP-RPC so we specify the protocol. If we simply accept that everyone uses the word wrong and go with it then we just gain yet another word to describe a very old pattern and lose a way to describe a different architectural pattern.
I started in java and used to use atom, intellij, eclipse, emacs but after going through all those editors and becoming a haskeller I'd recommend vim. Its old skool but its faster (doesn't feel as laggy to me), has less clutter and it does everything you need. There are some Haskell editor projects in the style of vim that might become usable in the next while but until then... Stack for builds unless you go the nix route in which case cabal. And finally, good luck in the new job and congrats on becoming a Haskell dev! Its an awesome language and community :)
&gt;I get the incompatibility with client code generation now! Wonderful. I'm really happy it clicked as it's a nice pattern to have in the toolbox when you need it. I hope you're right that this thread will prevent some misunderstandings, but it's really difficult as there is so much bad information out there. I guess it's a similar situation to what Haskell already endured with Monads! :)
Definitely a parallel; I had a similar click there too :) Once I got the state machine connection it all made sense! Thanks again mr alien!
Very glad to be of help and I hope I never came off as snarky. I'm a bit passionate about this. :)
But RESTful as it's used today almost always implies stateless, something RPC doesn't imply at all.
In my experience, people who claim to have "RESTful" APIs don't follow the stateless constraint any more than they follow anything else. In any case, I'm not sure that you need to specifically indicate that your API is stateless. For me, trying to understand some else's architecture, the first thing I need to understand is the system architecture (i.e. service bus, REST, RPC). After that, I can look into the finer details of the interaction within that architecture.
I can tell! Its an interesting and subtle topic that I hadn't appreciated the depth of before. I'm now wondering if the REST concept could be encapsulated in some way so as to provide guarantees of REST - e.g. by looking at a cabal file you would have proof of a servers restfulness...
&gt; That's very simple, just fork a thread to periodically perform some action :) I thought that was was auto-update do. &gt; The motivation behind auto-update is that if no one currently cares e.g. what the current time is, why waste time computing it? Well, that' from the auto-update package description &gt; &gt; This library allows you to define actions which will either be performed by a dedicated thread or, in times of low volume, will be executed by the calling thread. I understand that in time of high volume, the action will be performed by a dedicated thread. My point is, yes it will be performed in a dedicated thread, but if the thread wait to be kicked in when needed, then you don't save any time my having another thread (as you have to wait anyway). My understanding was that in time of high volume, the thread update every n seconds, and if nobody cares, then thread eventually die out. It seems for me that the actual behaviour of auto-update and it's implementation differs, so my question is, is this normal, is this a bug, or is it that I'm misunderstanding the code and auto-update behaves as I expect?
I've been thinking about this for a while myself. It would be nice to provide more automated guidance some how. The other major architectures all have something (i.e. RPC, Queue, Pub/Sub, etc.) that eases development of client and server.
&gt; Well, how *else* would you build Haskell projects? Hugs? ... &gt; Oh. This is really interesting! I'm curious about the speed benefit -- could you do a `time` difference on a moderate project?
It's worth noting that GHCJS gets GHC's entire optimization pipeline almost, which may change the equation. (That said, I would also be surprised if GHCJS were as fast as Idris).
The approach doesn't seem to work if the file depends on any hidden modules, or if any modules that it depends on depend on hidden modules in the same package. It doesn't appear to be any faster than `stack build --fast` for those files. Seems like the major benefit is to get only a single file compiled at a time, where `stack build` has no method of targeting just one file.
&gt; Tasty is a modern testing framework for Haskell. It lets you combine your unit tests, golden tests, QuickCheck/SmallCheck properties, and any other types of tests into a single test suite. So maybe the advantages only become clear once you're using 2 or more testing frameworks?
Perhaps instead of 'handlers' for Servant routes and alternate framework e.g servant-rest could use another abstraction 'rests' which enforce the use of a state machine abstraction. Perhaps the solution could require 'rests' to be written in the monad specified by the verb in the api definition. E.g. Get, Put, Patch, Post, Delete monads instead of the one ExceptT monad being used for all methods... It's unclear to me at the moment whether this in itself would provide any type safe guarantees of restfulness though perhaps in combination with a state machine framework accessible from the method monads it could be a step in the right direction!
Fair enough, I've been surprised with how fast GHCJS is in some fases... but I never managed to get it close to native JS speeds, only ~5-10x slower. Idris being faster than my handwritten JS code was absolutely surprising.
It probably depends heavily on the kind of code you write.
Can we mix linear and nonlinear operations in `IOL`? Like treating the socket resurce linearly but freely duplicating some other resource? How does this approach compare to Idris' [ST monad](http://docs.idris-lang.org/en/latest/st/state.html)?
Yes, we can mix linear and non-linear operations. (But I'll admit it's buggy in the current implementation). Regarding Idris's ST, it's somewhat similar, although with much simpler type machinery: indexed monads have a form of linearity. Also, implementation-wise, I suppose that we don't have to rely on some implementation of a resource heap (in Idris's ST every resource with a typestate is part of the ST's state, so we need a way to query this state. We could implement that with STRefs, I suppose). There is more on both these questions in the paper linked in the blog post.
I'll try to open a bitcoin and/or Patreon channel this weekend, but I believe, the most valuable help will be an information spreading. Maybe I could ask you to send a link to this post to your friends? Anyway, thank you for your interest!
Previously I shared [servant-auth-and-elm-example](https://github.com/mchaver/servant-auth-and-elm-example) here. For various reasons I've decided to try PureScript instead. I went ahead and translated the elm version into PureScript Pux. The website is nothing beautiful, but the code should be enough to get you started building something. If you have any questions or suggestions, please share here.