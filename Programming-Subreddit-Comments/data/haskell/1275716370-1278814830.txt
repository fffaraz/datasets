Ugh, I'd hate to have anything that large and ugly on any of my machines, but I would love something like [these](http://schwag.archlinux.ca/product/badge/).
Oh no, this is the summoning call for jdh. Since he's probably already banned from this subreddit, I'm sure he'll be putting up a blog post sometime soon about "dissatisfaction in the Haskell inner circle" or "the floodgates are open" in no time now.
I think the "code used" measure should not be logarithmic.
 def fac(n, acc): ... seems to be translated to: \ [_s_n, _s_acc] -&gt; But this seems like a bad idea, how do you get the "TypeError" exception semantics from this?
&gt; [This] actually somehow beats the C version (slightly) in my test setup.
Are there any practical and/or surprising consequences of this isomorphism between categories and databases? The presentation ends just after the introduction of the basic idea.
I like the idea of loop fusion though, because it (ideally) allows you to write it more idiomatically, i.e., in a high-level way, and still get the compiler to convert it to this type of control structure---that's how it should be, imho. If you can't express it differently when translating between languages, it seems rather pointless..
More informatively, a database *schema* is a category with *tables as objects* and *joins as morphisms*. Lots of things are categories, but the semantics of the arrows are generally the only reason for that to be interesting. The article goes into a bit more detail. It'd be nice to have a bit more discussion of the structure of the resulting category, though. As it stands, I'm not entirely sure what definition of "database" is being used. Just abstract relations with directional cardinality? The formal relational algebra on sets of labeled tuples? SQL in all its inconsistent glory? The choice does complicate matters--in SQL, inner and outer joins produce morphisms with distinct properties, and both SQL and the relational algebra permit Cartesian products (sometimes "cross join")--is that a morphism? Or would the cartesian product perhaps make more sense as something else, such as a functor between schema categories?
I'd prefer something even more understated--maybe just the Haskell logo symbol? (Incidentally, is there a name for that? `&gt;λ=`? "lambinda"?)
I needed to download the ghc tarball to compile the platform. It was on Centos 5.4. By the way, Centos 5.4 is on glibc 2.5 and it looks like the binary (6.12.1) works. 
Haskamtrak perhaps? Modern case badges are all thin aluminum anyways. I've been looking for a place to order custom ones from, but they're probably only available in minimum quantities of 100 or so.
And the moral of the story is: For best results, use Don Stewart as your GHC backend code generator. Is there a compiler flag to do that automatically?
I suspect that is still a topic of active research.
Not necessarily; I know engineers who would prefer to write C-like code in Haskell, because even when they need to do it low level they still get more powerful typechecking and a set of abstractions that are no performance impact.
As I recall, the relational algebra is effectively a re-phrasing of many-sorted first-order logic (perhaps with some restrictions on the use of negation and recursion) I wonder if the category they're talking about is the same as this logic, considered as a category with proofs as morphisms? or whether in some different sense.
This may be a bit naïve, but "Programming in the large" in Haskell feels a great deal to me like programming in the small, but with more cycles. Start programming a project, and when you encounter a problem look for a general solution. Then switch gears, and program a module/library that provides that general solution, or at least the necessary subset of it. Rinse, lather, repeat. Typeclasses, hofs, and parametric polymorphism ftw! (Oh yes, and data structures not functions, functions not data structures, records not classes, and classes not records. That is to say, there are lots of representations that are roughly dual, but differ in the amount of information they expose -- so take advantage of that, and switch between them as necessary.)
Actually I'm not sure your characterisation is correct. Objects appear to be the the fundamental data types of the database in this example ('domains' in relational algebra IIRC, 'entity types' in some modelling jargon); morphisms between them are binary relations; composition of morphisms correspond to joins between those relations. At a first glance this appears (unless I'm missing something) just to be the plain old category 'Rel' (or technically a full subcategory of it), and hence not super exciting. http://en.wikipedia.org/wiki/Category_of_relations More interesting perhaps would be that category of multi-sorted first-order logic with propositions as objects and proofs as morphisms. You can translate between FOL and relational algebra by saying that an expression with free variables corresponds to a relation, with each variable as a column of that relation (amongst some other details).
Hm, you may be right; I didn't write very clearly, anyway. To clarify, I was reading the arrows as representing foreign keys (in SQL terms) or shared attribute names (in relational algebra), which aren't actually joins, but rather define where certain kinds of join are possible. Objects in this case would be tables/relations. What you're suggesting is instead regarding columns/attributes as the objects, tables/relations as morphisms (with no trivial decompositions), and relationships between tables as composite morphisms?
Is someone working on this over the weekend?
in that case, perhaps the ct jargon should be dicarded for less esoteric names. eg, "iterator" not functor, "combinable" not monoid, "sequencable" not monad... 
Ah gotcha. Yeah maybe that was what he meant. Although suspect that would also prove roughly equivalent to the structure of Rel, if you think of the tables as being their primary keys, and the foreign keys as being binary relations on the primary keys. Should probably read the original paper, I guess :)
Re what I'm suggesting, pretty much yeah. I guess I'm relying on the fact that it's possible to normalise a relational schema into one based entirely on binary relations. (Which is true, although sometimes you need to introduce surrogate primary keys to do it). In particular the way he talks about joins as composition of morphisms makes it sound like the morphisms he's talking abotu must be binary relations. Although I may have gotten the wrong end of the stick.
This point comes up somewhat regularly it seems. The problem is by changing the name in such a manner, you are attempting to make the idea appear less general than it actually is. For example, data Foo a = Bar | Baz a instance Functor Foo where fmap _ Bar = Bar fmap f (Baz x) = Baz $ f x (This is actually the instance of Functor for `Maybe`, btw.) I'm not sure it's directly obvious how using `fmap` over my `Foo` type of "iteration" or how my `Foo` type is an "iterator" - the term "iterator" to me brings to mind the idea of iterating over a collection or sequence of values, but in no way does the `Foo` type really represent a collection to me either. A monoid is also more than a type with with some function that "combines things" - a monoid has the important property that the binary operation be associative and there be an identity value. When you say some type is "combinable" as opposed to a "monoid", *you actually lose information*, because it's no longer clear to me this type has an identity nor its operation for combination is associative. And because I don't know that, I also am not aware of the other useful properties you can derive by simply knowing something is a monoid. Things like monoids, functors and monads all have laws your type have to abide by in order to be called such. If they abide by these laws, they qualify, no matter what their more concrete semantics may be. Many things can be a monoid because the rules are so simple: you need a type with an associative operation and an identity value. Nothing more than that, yet already we can describe a large class of objects as monoids and derive useful properties for them just from knowing that. But renaming it and calling it 'combinable' is not more clear, it is actually more of a restriction in the way we can then think. People generally have a problem with seemingly esoteric names and operators, but there is a good reason they are there, in mathematics and in haskell - because names like 'combinable' carry with them ambiguity, and raise questions about semantics and meaning, and that brings about confusion when you ask "my type abides by these rules, but it's not really something that's an 'iterator' or a 'sequenceable'". It is the plague of using human language to describe such general ideas. To quote Alfred Whitehead: *"By relieving the brain of all unnecessary work, a good notation sets it free to concentrate on more advanced problems, and, in effect, increases the mental power of the race."* So, no. I don't think the CT jargon should be discarded for something else. A monoid is not really a category theory specific thing - you've used monoids in almost *every* single piece of code you've written in every other programming language. I can guarantee it. You just didn't have a sufficiently general name for these "things" which had such properties, and now you do.
I went the "make your own" route. http://www.flickr.com/photos/jeremynicoll/4441032543/ 
True.
Why? Those are confusing, ambiguous names that don't actually describe the type classes very well. The whole reason for ganking mathematical terms is that they provide clear, well-defined descriptions of abstract concepts that are pretty close (though not completely identical) to the way they're used in Haskell. Less "esoteric" terms are undesirable because they carry extraneous and irrelevant extra meanings. I would argue that using terms like those would make it *harder* to get the hang of using their respective type classes, because people would draw incorrect conclusions based on those existing meanings. In fact, I'd even go so far as to say that explaining the current names by analogy to such concepts is harmful and detrimental to understanding (cf. [monads are burritos](http://byorgey.wordpress.com/2009/01/12/abstraction-intuition-and-the-monad-tutorial-fallacy/)). The purpose of jargon is to form a common basis for communication in a field by making it easy to express specialized concepts, not just to be an arbitrary stumbling block for newcomers. Muddying the jargon to be less scary for outsiders is missing the point on multiple levels.
Amen!
That's the plan, yep. Have to finish some other writing tasks first.
-fvia-stackoverflow
all great points. though i'd argue that if names like "combinable" carry with them ambiguity, names like "monoid" carry with them a general "wtf??". i actually think "monoid" is a cool sounding name, but probably because i'm into sci-fi :-) anyways, i think a name for a monoid, functor, etc., that's more readily accessible, and that gives at least some clue as to what's going on, even if incomplete, would go a long way towards helping the programmer gain intuition. instead, it's the opposite: start with a completely foreign name, say "wtf", find some link to something called "category theory", say "wtf", try to learn the concept behind it, try to use it a little, *then* get a little light bulb of intuition ("hey, most of these monoid thingy's are combinable somehow..."), then use them some more, gain a deeper understanding, circle back round to the category theory, use them some more, etc. etc. if instead we had started with a type class called "combinable" or something like that, it would have served as an immediate stepping stone to deeper understanding and the mathematical notion of a monoid. it's ok for a name to be incomplete. most names for deeper concepts are. it happens all the time in the OO world; until you look at the API of a class or interface, you don't know what it's really all about... but the creators of those classes typically come up with some sort of object name that will at least give you a clue as to what it is. if we called everything "snarfeldingles" and "woznobblers", we'd likely find little audience for our library or API. 
I'd rather have to learn a new word than learn an incomplete or even blatantly wrong idea of what something is.
We routinely do code reviews in #haskell on Freenode. You should stop by. :)
Be a man and put a David Haskellhof sticker on there.
This is an interesting discussion, but I don't agree that your original attempt was a very idiomatic translation into Haskell. The C code was only keeping two variables as loop state because of idiosyncrasies of C; it's really an iteration over a single sequence of values. Same for iterating in backwards order - it was only to "save" a heap allocation by destructively updating the passed parameter. Even in C that's silly in this case since the compiler optimizes away all the heap allocations anyway. But that's the C coding style.
I am only adding the following comment because this blog is ultimately about creating web pages, so I thought you might be interested. :) This blog page looks bad in Safari 4.0.5 (current latest version) on Snow Leopard. The bottom part of the letters in each heading is cut off. The amount cut off depends on how fast I move the scroll bar up and down - by moving it quickly, I get the whole thing to display. That makes this sound like it may be caused at least partially by a bug in Safari. But it doesn't matter - I have never seen this before on other web pages, so the bottom line is that you are doing something unusual that looks bad in Safari.
I am doing something unusual ;). It's Google Web Fonts, and I hadn't realized that they had this effect. I'll see if others are experiencing this and if there are any solutions... otherwise, I'll just disable the fonts.
It is a bit strange that you are abandoning Data.Text. UTF-8 in raw bytestrings is a kluge; the whole point of Data.Text is to replace that. If Data.Text doesn't do the job it is supposed to for Unicode, then it will soon be fixed, or if unfixable, replaced. It seems like it would be more efficient to focus on fixing Data.Text rather than re-inventing that wheel for one specialized application. You feel that the whole reason Data.Text is too slow for you is because of using UTF-16 internally. Let's assume that you are correct. There are two approaches to that issue. Firstly, how prominently do CJK languages feature in your benchmarks? And do you factor in the extra slow-down when a browser needs to download and render a CJK page in UTF-8? Judging by the current demographics of the Internet, a quickly growing majority of web pages are served in those languages. My intuition tells me that web apps optimized for UTF-8 are not going to provide the best user experience for those pages. I have no first hand knowledge, but I have heard claims that UTF-8 is not popular in many countries where non-Latin scripts are used, and I could understand why. Secondly, let's ignore the demographics and assume that your aim is a web framework optimized for Latin scripts. Then perhaps Data.Text ought to have a variant, Data.Text.SingleByte, that is optimized for that special case of text that is mostly single byte in UTF-8.
I think I'm only having that problem with Cardo. That's what I see on your blog, and that's also what I see in the [Google Font Directory](http://code.google.com/webfonts).
I'm much more interested in working, functional, fast software than software that doesn't use "kludges." Can you give me a concrete example of a problem introduced by my usage of ByteStrings directly? All of this occurs under the surface; users should never be touching the bytestrings directly. Now, as far as your CJK issue: how is having the default encoding as UTF-8 any different from having the default encoding as UTF-16? Either way, you'll have to re-encode at the end of the day. The advantage of UTF-8 is that you *know* with absolute certainty that a significant portion of any HTML document will be Latin characters. I'm optimizing for the usual case. I'm also not convinced that modern web applications supporting CJK are avoiding UTF-8. My understanding is that Gzip compression addresses most of the space issue involved. But as I said, there's nothing stopping you from re-encoding the data. Now I'd be all for the text package providing UTF-8 support. But if you read my post closely, you'll see that I'm using some tricks that avoid any run-time encoding of the data. As the text package stands right now, there is no way to feed it raw bytes. I suppose I could go ahead and write my own text package to address this issue, but I fail to see why I should do that when I'm only using features exposed by the bytestring API.
Then I suppose I'll be dropping Cardo ;)
We need IO that is general but still simple
Aaarg! So many people suggesting complicated non-standard things! [It's really not that hard](http://stackoverflow.com/questions/2981582/haskell-lazy-i-o-and-closing-files/2984556#2984556).
This is a [known bug](http://code.google.com/p/googlefontdirectory/issues/detail?id=5). It can be fixed by installing locally the [latest version of the font](http://scholarsfonts.net/cardo99.zip). Google's response to the bug is: &gt; We haven't pushed the newest version of the font to production yet but plan to soon. It does seem like it fixes the problem.
It was compromised, right? Don't fret too much, people are working on solutions, and we're likely to move from Yale soon.
Isn't this what Iteratee-based I/O was designed to solve?
I partially agree... for this toy example, that solution (putting the print inside the action that's mapped over the list of files) is fine. In general, though, it's not appealing. The choice of what to do with the MD5 hashes once they were computed is embedded pretty deeply into the code. Modifying that code to do something else with the MD5 hashes instead means basically changing the whole thing. If that matters, then the various "non-standard" solutions are trying to address that shortcoming.
oh yeah it did bug me (chrome on OSX.6), but scrolling around fixed it for whatever reason so I never complained. Thanks yitz for bringing it up
The other solution I suggested using `withFile` would let you preserve the original structure of reading the files in one place and writing the results in a different place. Something like: getHashes :: FilePath -&gt; IO [Hash] getHashes p = mapM getFileHash =&lt;&lt; getRecursiveContents p where getFileHash path = withFile path ReadMode $ \hnd -&gt; do c &lt;- hGetContents hnd evaluate (hash c) -- this is the only slightly subtle bit
The key point is that you *actually compute* the hashes, i.e. evaluate them to normal form before moving on. Duncan does that by printing them, but you could as well write hashes &lt;- mapM (\path -&gt; evaluate . (`using` rnf) . fileLine path ... for full generality.
Thanks for posting that, very informative. I have a basic grasp of Haskell, so it's been only par and seq for me all this time :)
Just the other day we had a discussion about this, and you said to only use "unsafeInterleaveIO" (directly or indirectly) if and only if you don't care about the order of the effects relatively to any other effects in the program. Here, you do care, but you use "evaluate", which means you're not considering unsafeInterleaveIO to just introduce non-determinism, but to be driven by evaluation. This approach is really not in the Haskell spirit, IMO, and even if a problem initially seems hard (purity) we shouldn't give up, cheat and just allow I/O to be driven by the evaluator.
Yes.
Yes exactly. Note that we are implicitly relying on the hash consuming the whole file so that the file will get closed (admittedly a reasonably safe assumption for a hash). In my other example using `withFile` we guarantee the file is closed but we still have to make sure that the hash has been evaluated before the file is forcibly closed. That's what the `evaluate` is for in that example. In my opinion this gives is a reasonably easy to use combination of safety and the performance and convenience of lazy IO. Note that if you get the bit about evaluating what you need before the file is closed wrong then you will get an exception about the handle being closed. That's a very upfront and visible indication, compared to silent resource leaks you might only discover much later.
&gt; All of this occurs under the surface; &gt; users should never be touching the bytestrings directly. OK. If fixing this later will be transparent to users of the framework, then it's certainly much less of an issue. &gt; The advantage of UTF-8 is that you know with absolute &gt; certainty that a significant portion of any HTML document &gt; will be Latin characters. I'm optimizing for the usual case. My point is that it's not the usual case. China is now the largest Internet user base in the world, and growing quickly. Is your framework meant for general use? &gt; I'm also not convinced that modern web applications &gt; supporting CJK are avoiding UTF-8. Don't know. I've heard that most CJK-speaking people do not use UTF-8 as their default encoding. I don't know if that's true for truly localized web apps. It's worth finding out. &gt; Now I'd be all for the text package providing UTF-8 support... &gt; As the text package stands right now, there is no way to &gt; feed it raw bytes... &gt; I'm only using features exposed by the bytestring API. Are you passing those bytes through as opaque binary data, or are you sometimes treating it as text? If the former, then your use of bytestrings is correct. If the latter, then you should be using Data.Text, and if it doesn't provide an API that you need, you should add it. It shouldn't be a big deal. &gt; I suppose I could go ahead and write my own text package &gt; to address this issue Oh no, why re-invent the wheel? Just provide a patch, or an add-on package. This UTF-8 issue may not be the only issue. I only focused on it because you claimed that it is the reason Data.Text came out slower in your benchmarks. But I'm not sure you really know that. If you are treating that data as text in any way, I think it would be worth it for you to take a closer look at Data.Text. 
`rnf` isn't needed here, since the result is just a String. `evaluate` alone does the trick.
It is designed to solve the "general" part. But it certainly doesn't solve the "simple" part.
That we have. System.IO.Strict, Data.ByteString.Strict, etc. are simple. But then if you need lazy processing, you'll need to do it entirely by hand. I'm asking for *lazy* IO.
Perhaps the poster didn't see this previous [link](http://www.reddit.com/r/haskell/comments/ca4ls/wwwhaskellorg_has_been_down_for_about_24_hours/).
I did see it -- I even linked to it above. I just thought it was troubling (to say the least) that the site was down twice in the space of a week. EDIT: Yitz, I apologize: I saw the link, but I didn't read down far enough to see all of your comments. I appreciate all the hard work you and the other admins are doing on what is basically a volunteer effort. 
The problem is that lazy IO is *a trap*
&gt; I've heard that most CJK-speaking people do not use UTF-8 as their default encoding. I don't know if that's true for truly localized web apps. It's worth finding out. For curiosity: in Japan, I'd say the most used encodings are Shift JIS and euc-jp, and your impression rings true. I rarely see stuff in Unicode. China also has popular national standards like gb2312. I wouldn't say they're actively avoiding UTF-8, but the old standards are 'the default choice'. There's not a lot of incentive to change. 
Does anyone have actual numbers on the bandwidth used by haskell.org? Would be interesting to see how much a virtual server would cost... setup a fund to pay for it.
&gt; My point is that it's not the usual case. China is now the largest Internet user base in the world, and growing quickly. Is your framework meant for general use? My point is that you're making that assertion based on some pretty big assumptions. I have not seen any literature implying what you're claiming, and all my experience has been to the contrary. (And yes, I've done multi-lingual web development, including languages that take 3 bytes per character.) &gt; Are you passing those bytes through as opaque binary data, or are you sometimes treating it as text? If the former, then your use of bytestrings is correct. If the latter, then you should be using Data.Text, and if it doesn't provide an API that you need, you should add it. It shouldn't be a big deal. In the current version of the API (remember, it's not finalized), the user sees bytestrings in only two places: the final result is a lazy bytestring, and the HtmlContent constructors take a bytestring as their argument. I'm still on the fence for the latter one, but for the former case you should be passing off that lazy bytestring to an output function anyway. &gt; This UTF-8 issue may not be the only issue. I only focused on it because you claimed that it is the reason Data.Text came out slower in your benchmarks. But I'm not sure you really know that. If you are treating that data as text in any way, I think it would be worth it for you to take a closer look at Data.Text. I'm not sure what other issues you might be alluding to here, but I think it's fairly clear from the benchmarks I ran that the text package was the culprit. As I said previously, I'd love to see that I was simply misusing the library, but it was a pretty trivial usage and the numbers were so vastly different. If you're curious about these numbers, you can [read the post](http://www.snoyman.com/blog/entry/bigtable-benchmarks/)
No point in going with a VPS when these guys[1] offer nicely specced dedicated servers for 49€ a month. However, I suspect the hard/expensive part is to actually run said servers, not the cost of hosting itself. [1] - http://www.hetzner.de/en/hosting/produkte_rootserver/eq4/ 
Work is already underway to mirror the site, but it is not easy, as the Mediawiki instance used has an old database, and the last db migration failed. Once Yale's live (looks like Monday sometime) we'll know more.
[Cafe Press - $4](http://www.cafepress.com/+rectangle_sticker,395776257)
&gt; &gt; The advantage of UTF-8 is that you know with absolute certainty that a significant portion of any HTML document will be Latin characters. I'm optimizing for the usual case. &gt; My point is that it's not the usual case. I assumed that he was referring to the actual HTML tag characters. Whether this is "significant" or not obviously depends on the page; I'd be interested to know if anyone has good numbers on the distribution of tag/content proportions in CJK-land.
I suppose my question would be whether *new* web projects are continuing in old encodings. I know that here in Israel, it seems that modern sites use UTF-8 and older sites are still on Windows-1255. And Hebrew characters also require 3 bytes in UTF-8 encoding.
Nothing to add, but thank you for your dedication and work.
No, according to the [Unicode consortium](http://www.unicode.org/), the [Hebrew](http://www.unicode.org/charts/PDF/U0590.pdf) script occupies code points within the range `[0x590..0x5FF]`, which encodes in UTF-8 (see [chapter 3 of the Unicode standard](http://www.unicode.org/versions/Unicode5.2.0/ch03.pdf)) to the range `[0xD690..0xD7BF]`. Two bytes each, not three. Anyway, that still doubles the previously used Windows-1255 (extended ISO-8859-8-I) encoding. The reason UTF-8 took over so easily is that since the Hebrew web is a very small market, support in applications for a specialized Hebrew encoding has always been sketchy at best. Support for UTF-8 is becoming quite universal. Whereas huge markets like China, Japan, and Korea are far more capable of supporting the infrastructure for local character sets. So that particular incentive for change probably doesn't apply. But there may be other reasons. Perhaps on the web there is a trend towards UTF-8. I'd be interested to know.
&gt; Does anyone have actual numbers on the bandwidth used by haskell.org? I've been told "an average of around 1MB/s (including the backup pulse)."
&gt; In the current version of the API... &gt; the user sees bytestrings in only two places: &gt; the final result is a lazy bytestring, and the HtmlContent &gt; constructors take a bytestring as their argument. So then suppose the user has a huge amount of content in, say, the Shift JIS encoding, to be rendered as HTML. Doesn't that mean that in Hamlet this content needs to be first decoded to Unicode, then encoded to UTF-8 thus increasing its size by 50%, then marked up and sent over the wire in that bloated form? Looking at your benchmarks, they seem to contain nothing more than digits in tables, a tiny subset of 7-bit US ASCII. How can you base your decision about whether to use Data.Text, a Unicode library, exclusively on such benchmarks?
And much much more during releases of GHC (and now the Haskell Platform).
This looks good. How much of a performance boost can we expect from this? I'm pretty sure immix would only be applied to simple first-generational objects that are quickly collected, right? Then again, there are alot of those in haskell.
You seem to be missing the main point of UTF-8: you can put *any* character on your page. It's really that simple: using a specialized character set locks out people not speaking your language. Thanks for the information about Hebrew only taking 2 characters, I was sure I had seen it taking up 3 before...
You seem to be forgetting that *every* system currently uses some encoding internally. String uses (essentially) UCS-4, ByteString whatever you want (UTF-8 in my case) and text UTF-16. No matter what, the current general approach is going to incur that overhead. Maybe you have an idea of how to avoid this in general. Perhaps we can have a text type that specifies its encoding and then only converts at the last minute. However, I have a very strong suspicion that it will end up slower in general, because every web page still contains a significant portion of Latin characters (the markup). I'd be willing to try out other approaches, but for now switching from text to bytestring seems like an obvious win for most cases.
Awesome work!
I am looking forward to this!
No. `String` is a compound data type and `evaluate` will only evaluate it to weak head normal form, i.e. to the first cons cell. &gt; null `liftM` evaluate (undefined:undefined) False &gt; null `liftM` evaluate (undefined:"test" `using` rnf) *** Exception: Prelude.undefined You have to use `rnf` to get it evaluated to the full normal form.
Cause has been found. Back up shortly.
The post said that it would only kick in for the last generation.
&gt; In my opinion this gives is a reasonably easy to use combination of safety and the performance and convenience of lazy IO. Agreed. Not to mention that other solutions basically enforce the same usage pattern; for instance, Iteratee corresponds to a `foldl'` over the string right away, and strict IO corresponds to fully evaluating the string before doing something with it. It seems to me that the troubles with lazy IO are actually a sign that a firm understanding of lazy evaluation in general is not as widespread as we would like. 
Back up now. Mon Jun 7 11:13:49 PDT 2010
Good to know, thank you! I'm curious, though, as to what caused the outage. Hackers from without, or instability from within? 
Past companies represented at CUFP include: * ABN AMRO * Aetion * Amgen * Antiope * Bluespec * Credit Suisse * Deutsche Bank * Eaton * Facebook * Galois * Linspire * RENCI Stand up and be counted!
GHC's GC already is quite similar to Immix in many ways. The main features that Immix provides but GHC's GC doesn't are: 1. Both allocate memory (in the old gen) in blocks (GHC: 4K, Immix 32K). However GHC can only reclaim completely empty blocks. Immix in turn subdivides blocks into lines (compile-time parameter, default: 128 Bytes) which can be reclaimed separately. 2. When a block becomes too fragmented, Immix performs *opportunistic compaction*, that is, it copies objects from the fragmented block into another block. This improves memory utilisation (due to less fragmentation) and possibly locality (because the copied objects are likely contiguously allocated). I'm not sure whether using Immix would make much sense in the first generation. Currently, the first generation is special in that it only allocates into completely empty blocks. If the nursery allocates into only partially empty blocks, more heap checks will fail and this may slow down some programs (but improve memory utilisation). Immix is not that much better than Mark Sweep in a generational setting, but using Immix in non-generational mode can mean more efficient memory usage (since you don't need the second space) at the expense of a bit performance (3% according to the Tech Report).
I am Ben Hur!
About time that somebody noticed that lol. +1
I'm using Haskell at work, but you don't even want to hear the things I'd say.
I do! What?
http://okmij.org/ftp/Algorithms.html#tree-annot
I wish I did.
Here is the [specification](http://wiki.secondlife.com/wiki/LLSD) of LLSD. Conceptually, LLSD is similar to JSON. One of its three serialization formats is nearly identical to JSON's, but Linden has hardly ever used that format. They use the XML and binary serializations.
I wanted to see a stem plot of `visitCounts`! I guess it was beyond the scope of the video, but it's not that hard to do: mapM_ putStrLn . map (uncurry $ printf "%2s | %s") . map (init . head &amp;&amp;&amp; intersperse ' ' . sort . map last) . groupBy ((==) `on` init) . map show . sort I just slapped that together ad hoc in ghci like in the video, so it probably could be improved if it was placed in a source file and played with a bit. I tried to make it such that it would have been suitable for a screen cast like this, but I'm not sure that I succeeded. It required a couple additional imports, for one. Edit: Hmm... half the data points in this case were less than 10. Maybe a stem plot would be inappropriate.
Same here.
I'm considering a follow-up video with some more sophisticated analysis/visualization. I'll keep this in mind. If you have any other suggestions or code, drop me a line on IRC or email.
Another thing I do a *lot* when playing around with data in GHCi is to take a list of numbers (called, e.g., xs), and do: mapM_ (print . flip replicate '*' . (`div` scalingFactor)) xs
Just in case people were interested in seeing the code, it's here: http://github.com/snoyberg/hamlet/tree/blaze
Don't forget the Chart and gnuplot libraries.
lotsa haskell at work.
A really interesting read. We should run this survey on Haskell programmers.
everyone i know working in industry on the jvm uses clojure
I'd be curious to know the uptake of Haskell by people using dynamic languages such as Ruby and Python.
Makes me really wish we had Haskell for the JVM.
I know I was one such person
I'm starting work on a similar project in Agda! Good luck to all
I *told* you, jdh! Still not believing a lowly intern now?
On that note, I'd love to see work on a Coq library to assist in formal amortization analysis of algorithms. In the paper [A Persistent Disjoint-Set Data Structure](http://www.lri.fr/~filliatr/ftp/publis/puf-wml07.ps) by Conchon, et al, they use Coq to verify the correctness of their algorithm, but they don't address either formally or informally how their use of implicitly shared mutable state affects Tarjan's original amortization argument for disjoint sets. I am in fact nearly certain their data structure doesn't have the claimed amortized O(1) performance, so the formal proof of correctness feels almost like a smokescreen that draws attention away from a fatal flaw.
Jon Harrop does not think highly of your [interns](http://harrop-highly.blogspot.com/).
but it was posted by dons and he doesn't even have phd... oh, and hashtables!
those papers are usually available only after the conference, right?
Damn! I was wanting to do something similar to this in Matita.
Sometimes the authors will make them available on their webpages.
I'll be making mine available as soon as I've submitted the final version, I've already read at least 5 or 6 of those papers - just go to the authors home pages. And sadly those papers are only behind a pay-wall after the conference, so finding them on home pages is usually best anyway.
&gt; 5 days ftfy
It's a real shame that blog has not been kept up.
I thought the current consensus in the Haskell community is that Lazy IO is convenient but not safe at all, and causes problems. Are you saying that we don't need Iteratee? If so, that's great, because I find it terribly confusing to deal with Iteratee and haven't successfully written anything but a toy with it so far.
Oh how I wish it were more simple :-). I've seen some nice examples, but nothing to make me feel like the problems of IO in Haskell have been dealt with in a way that is easy to reason about or code once and for all. Maybe uniqueness typing really is better?
That's a LaTex file, not a pdf.
&gt; I'd love to see work on a Coq library to assist in formal amortization analysis of algorithms. [There is such a library for Agda](http://www.cs.nott.ac.uk/~nad/publications/danielsson-popl2008.html).
&gt; Damn! I was wanting to do something similar to this in Matita. There is still plenty of work to do. According to the author's slides, he has verified 825 of the 1,700 lines of ML code in the book. There are also other functional data structures than those in Okasaki's book, such as those designed by [Kaplan &amp; Tarjan](http://www.cs.princeton.edu/research/techreps/TR-584-98), [Brodal et al.](http://www.cs.au.dk/~gerth/papers/esa06trees.pdf), and [Blandford &amp; Blelloch](http://www.cs.cmu.edu/afs/cs/project/pscico/pscico/papers/fingertrees/main.pdf).
I don't really understand why so many people want their language running in the JVM.
&gt; I am in fact nearly certain their data structure doesn't have the claimed amortized O(1) performance I am grepping the [PDF version of the paper "A Persistent Union-Find Data Structure"](http://www.lri.fr/~filliatr/ftp/publis/puf-wml07.pdf) and the only references I find for "O(" or "amort" or "constant" are referring to the access and modification times for persistent arrays. Is this the bound you are doubting? A few years ago, a [draft circulated claiming that union-find could be done in O(1) amortized time](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.86.6020), contradicting Tarjan's famous lower bound of inverse-Ackermann complexity. I believe the draft was withdrawn, and that the accepted wisdom is still that Tarjan's lower bound is accurate.
[Functional Pearl: Every Bit Counts](http://research.microsoft.com/en-us/people/dimitris/every-bit-counts.pdf) [The Impact of Higher-Order State and Control Effects on Local Relational Reasoning](http://www.mpi-sws.org/~dreyer/papers/stslr/main.pdf) Google is your friend!
[Matching Lenses: Alignment and View Update](http://www.cs.princeton.edu/~jnfoster/papers/matching-lenses-tr.pdf)
Well, I think it would be cool to write a Haskell web app and run it in App Engine. That's really my main motivation.
The author interprets the results as meaning that the availability of good web frameworks and tools was a critical component of Clojure's success. I think that underlines the importance of the hard work that some people are currently putting into Haskell's web toolkit.
People want to leverage existing code. Haskell's usefulness would increase rapidly with a JVM version.
Though with all the guys coming from Java, it seems JVM support is crucial for them.
Did you even try to compile the program and time it using different runtime parameters? Curiosity needed!
Appears to be a PDF to me...
Well [that link](http://www.reddit.com/r/haskell/comments/cdchl/monads_in_potential_the_potential_programming/) wasn't very informative! Perhaps you intended to link to something like [this](http://intoverflow.wordpress.com/2010/05/21/announcing-potential-x86-64-assembler-as-a-haskell-edsl/)?
Another anecdata point from me--found mainstream static typing to be tiresome, got enthusiastic about Ruby and Python, got tired of making mistakes that a type checker would have caught, then discovered Haskell and, with it, fleeting glimpses of enlightenment.
I don't know why people talk about jdh as if he thinks Haskell doesn't get used in industry; that's clearly false: &gt; I'm trying to draw an objective comparison and you can't do that without pitting languages against each other. From my point of view, *...* the references about ... *[Haskell]* ... in industry are credible, significant and recent. -- [Jon Harrop](http://lambda-the-ultimate.org/node/2491#comment-37916) Take the man at his word, I say.
but the OP already posted that [here](http://www.reddit.com/r/haskell/comments/c6tbj/announcing_potential_x8664_assembler_as_a_haskell/)
Sorry, I meant `O(alpha(n))` with `alpha` the inverse of `\n. A(n,n)`, i.e. constant for all intents and purposes. They don't actually talk in any real way about amortized complexity in that paper I linked, but the implication throughout is that they are presenting a persistent version of the usual data structure with `O(alpha(n))` amortized complexity. My point was that they don't prove this, and I don't believe it's actually true.
Praise be! And this after: &gt; Multicore is the elephant in the room. JVM- and CLR-based languages, PolyML and maybe GHC are ok but all other functional language implementations are dead in this context GHC is maybe OK! &gt; Haskell may be beautiful but it is completely ineffectual at multicore. Their performance results are dire: real Haskell code rarely scales beyond 5 cores. The reasons are obvious. OK and dire! What should I do? I don't have 5 cores :(
No no, that only applies to *real* code--as long as your code has a nonzero imaginary component, you'll be fine, so if some code isn't scaling well, just make it more complex until it does. His problem is that he's only familiar with very simple (i.e., not complex) Haskell code. (I hear multi-core scaling in Agda involves quaternions...)
Good point!
After years of Ruby I decided to jump into something else, something different, and Haskell seemed pretty hardcore while having all sorts of interesting math around it. It was while porting some Haskell code to Ruby that I started thinking that Haskell may be better at some Ruby things than Ruby is, and that the type system was making certain things easier. (You can hear me recount my heretical adventures into Haskell when my MountainWest RubyConf Hubris finally goes up on the confreaks site. http://mwrc2010.confreaks.com/)
There's a possible stealth factor as well. When pitching JRuby to skeptical Java people, I like to point out that JRuby is just a jar file. There's nothing to install. You can just drop it into a Java project and use it. Your Java app can now load these text files that "just happens" to have text that follows Ruby's syntax. :) Being able to mix Haskell into existing Java projects might make it more palatable to people. You don't have to make a complete switch. 
Now it is, it wasn't before.
I don't understand it either. I switched from Clojure to Haskell because I didn't want to use the JVM, and I especially didn't want to have to deal with all the Java libraries.
I don't see why the person you're replying to thinks that gaming has anything to do with multicore work
Sure, you're right, but does it really matter to answer that kind of trolling. Here, I wouldn't even see your answer: it is buried under the heavily downmoded comment you replied to. Well at least you're right to submit your comment itself for all its nice links.
Not so much JVM as all that useful library legacy. Problem of all new languages: no libraries. 
I'm not so much interested in the original question (not interested at all), but more in how widespread the knowledge of all the work going on in parallel haskell is. Did Haskell people know about this stuff?
Yes. And I damn you all to hell because my box is single-core. Apart from the GPU, that is.
Possibly because he's a gamer, and possibly because there's a tendency among imperative game programmers to believe that the most you can get out of multicore is to spread the paint and update loops to two cores. Which is bunk, of course, but still. Paralellising the update loop itself would require work in impure languages.
They seem to basically just implement Tarjan's algorithm on difference arrays and then hope you don't access old versions very often. You can provide the Tarjan complexity bounds only if you use them ephemerally, or if you can amortize a little bit of backtracking over a large number of union/find operations.
Yes, we do need to keep up the buzz in /r/programming. And it's a great post, full of great links, as noteed pointed out. But I'm not sure if a room full of shouting people is the best place to get the word out.
Yeah, I'm not trying to get the word out here. Just pretend this is a private wiki page. 
Don't worry. There are people [addressing your concerns](http://www.cse.chalmers.se/~joels/writing/dccpaper_obsidian.pdf) as well.
Honestly I have no idea why he's getting downvoted for this. I generally thought the Haskell crowd was level headed enough to looks past a few remarks in an incendiary tone. His core premise is right. There are not a lot of examples of real applications solving real problems as yet. Most of your list amounts to: "but everyone is really excited about this!" Let's be honest, the potential sounds awesome, let's hope it becomes something more.
I think there's a lot of applications in gaming that could benefit from parallelisation that doesn't go all the way to the main update loop. Gaming is a good example of an application that pushes the hardware hard and in a diverse range of ways, I don't see why it couldn't potentially benefit a lot here.
Damn them to hell, they require CUDA, I only got GLSL. But on another scale than whining about my hardware, I'd like to see [SIMD primitives](http://hackage.haskell.org/trac/ghc/ticket/3557), my CPU can, after all, add 8 bytes to 8 other bytes individually in a single instruction...
I actually saw this post in the original thread and it got me excited to try Haskell again. I had messed with it in the past for about a week straight but when I got to things like Maybe and Just in the Real World Haskell book I feel like I must have missed something because I had no idea what was going on. Hopefully a second shot will do me well because I'm really interested in the purity and checking out it's concurrency features.
I think that we now have license wording that allows Unity apps while banning Flash apps.
Make sure to try the IRC channel if you get stuck. It's #haskell on irc.freenode.net. Beginner questions are welcomed. :)
If I'm not mistaken, the LLVM backend does generate SSE instructions, so it might be in the next big release.
hahaha... so in order to use haskell on iPhone we have to use an interpreter (in a limited way) instead of compiling to native code?
And the [mailing lists](http://www.haskell.org/haskellwiki/Mailing_Lists), in particular beginners@haskell.org .
This iOS thing is going to confuse me mightily as it has nothing to do with (Cisco) IOS.
&gt; Haskell! I love you! But we only have 7 days left to submit a talk! &gt; &gt; (dun dun dun dun dun dun dun) Haskell! &gt; &gt; (dun dun dun dun dun dun dun) Haskell! Apologies to Queen and everyone involved in the Flash Gordon movie.
You know, I'd love to make a replacement for a graphing calculator for the iPhone. But this isn't possible, as it would require a user-facing interpreter. It just doesn't make any sense at all. Screw Apple.
I have become increasingly disenchanted with Apple over the past year or two. Apple in 2010 acts a lot like Microsoft in 1998. "Meet the new boss, same as the old boss."
TBH stuff like Maybe and how clean it makes error handling is what keeps bringing me back towards looking at Haskell. Algebraic datatypes and pattern matching are a definite win on their own.
There are lots of opportunities to parallelism game code. Practically any tree structure is a viable target for concurrency. A LOD heightmap can be concurrently rendered. An AI decision tree can be concurrently evaluated. Your scene graph can be concurrently walked. Also the maths used by the physics engine will use a lot of algorithms that could be made concurrent. I'm sure I haven't scratched the surface. There is a lot of room for concurrency in games.
I imagine that to be true but first I must grok those concepts. Haskell was my first dip into Functional Programming, followed by Erlang. Erlang had a much lower barrier of entry and I really liked it but there were some warts I really didn't like and fell by the wayside. I'll definitely go through and try again though and make use of more resources this time.
I lurked in IRC for a bit but I'll definitely do some more active participation this time. The next trick is to find a project and to try to slip some code in at work while no one is looking. ;)
Ah thanks! This resource hadn't occurred to me when looking at Haskell for some reason. I'll definitely check it out.
There's a couple things I didn't know.
Both HD consoles are moderately multi-threaded. Far more so than, for example, most desktop machines. Also, they rely more heavily on threading to hide memory latency since they're in-order CPUs.
This is a step in the right direction.
Congrats for your degree!
I'm sorry you have trouble reading what redditnoob actually said, the question which dons answered excellently. It also became obvious very quickly as redditnoob lifted the goalposts and ran around the park screaming "you guys wit' your book learnin' can't touch me!" that this was not a sensible discussion.
I wish I delimited continuations well enough to program with them. Enough with the monad tutorials. Anyone want to start a new trend? 
HTML5 works nicely on iPhone (and elsewhere), so there's your UI. There are are also persistence features. "All we need" is to target javascript. I noticed there is a project HsWTK related to this. Anybody know about this or have opinions on this approach? 
One thing I've found helpful, but not mentioned often enough, is that if you are confused, start working the equations out by hand. Haskell permits equational reasoning to a degree most languages don't. If a statement is confusing you, go find the original function definitions and substitute them in by hand. Like, literally on a sheet of paper, you can't skip this part. My epiphany came when I finally did this with the state monad, which I could freely use but was having trouble wrapping my head around, until I sat down with a quick usage of "get" and "put" and a couple of other monad statements and just substituted the hell out of it until I got back down to primitives I was comfortable with. (On that note, don't let anyone tell you the state monad is "impure". Not at all.)
The iPhone is an interpreter of binary code. &lt;/grasping at straws&gt;
This is great. One could now write a "gesture Lisp" interpreter for the iPad.
Sure, so, good multicore work is advantageous for game development, but I don't see why having not many good games made on the platform is a sign of poor multicore work.
No but success here would be a sure sign that Haskell is achieving its potential.
Well, I wrote one [a while back](http://haskell.org/haskellwiki/Library/CC-delcont). I'm not sure how good it is, but it has links to other, better stuff as well.
so, will you pick where you left, or start with the current news?
Probably gonna start with current news, I'll try to dig through the archives and find super good stuff though. TBH, I never expected I'd have to stop for as long as I did, so I didn't come up with a good plan for how to handle coming back.
Thanks for writing it. That's been one of my primary resources in learning about delimited continuations. Other sources include, a paper co-authored by SPJ and Oleg's website: http://research.microsoft.com/en-us/um/people/simonpj/papers/control/ http://okmij.org/ftp/Computation/Continuations.html#zipper-fs 
No! Say it ain't so! A Galois tech talk finally appears on video! Hopefully this will be the first of many --- there have been a fair few that have seemed very interesting but, well, I'm several thousand miles away... My sincere thanks to dons and whoever else at Galois who was responsible for the production of the video. Cheers!
We've had 160 tech talks in the past 5 years, so yes, a lot aren't recorded. But the last 3 have been, as will all future ones. Stay tuned.
This really needs [the slides](http://code.galois.com/talk/2010/10-06-spivak.pdf) as the camera is on the speaker. Does vimeo support synchronized video &amp; slides like infoq does?
HsWTK is defunct and the compiler that it was built from is also defunct. My thesis project will be a JS backend for GHC. Sit tight.
Nice work. I'd like to see them try SMT. 
And Yi, of course!
Oh, *blush* :}
Yeah, the only reason that I knew about some of those was your post on StackOverflow describing the best IDE's. I hope that somebody with experience in all of them takes it onboard.
Unfortunately yi doesn't currently compile against scion, because of conflicting instances in conflicting monad template libraries. Nasty issue which someone needs to fix!
Link?
http://stackoverflow.com/questions/734309/haskell-ide-for-windows/734768#734768
Thanks!
What's the advantage of doing all those checks at compile time, when the tool is running at code generation time, anyway?
You can debate whether it counts as an IDE, but personally I use [Sublime Text](http://www.sublimetext.com/) for my Haskell programming.
the vim haskelmode-plugin is quite usefull, but there are some more additional plugins one might want to add: - nice new syntax file: http://www.vim.org/scripts/script.php?script_id=3034 - and some unicode-cuteness: http://www.vim.org/scripts/script.php?script_id=2603 - little indentation script for haskell: http://www.vim.org/scripts/script.php?script_id=1968 - cabal syntax file: http://www.vim.org/scripts/script.php?script_id=2625 - hlint support: http://www.vim.org/scripts/script.php?script_id=2907
I use TextMate and a Terminal window with ghci running.
I think Emacs + [ghc-mod](http://hackage.haskell.org/package/ghc-mod) is a wonderful combination. It adds a lot of nice autocompletion stuff, AND it automatically has a built in flymake mode for GHC, so you can see on the fly what errors/warnings GHC reports your code might have. :) Kazu deserves some praise for this app, but I haven't really heard it mentioned anywhere. Is anybody else using it?
ghci, hoogle, hlint, and a text editor 
nice... appreciated by a hard core vim user
Could you give more details about what the error is? Is it `mtl` vs `monads-fd` vs `monads-tf`?
I do. Lovely bit of work. Kazu is very responsive to feature requests and bug reports, too.
I like the idea of applying Category Theory to RDF / Semantic Web, seems like a natural application of this. One thing I wasn't clear on in the talk: Is this something that you would be able to implement today in Haskell (manipulation, composition, etc of databases using Category Theory)? Would it require additional support at the database level? Would it be something you might be able to implement within a library with drivers for different databases?
Yes. The error message is src/Yi/Eval.hs:93:32: Overlapping instances for Monad (Either Yi.Interpreter.Err) arising from a use of `addMakeAction' at src/Yi/Eval.hs:93:32-44 Matching instances: instance (mtl-1.1.0.2:Control.Monad.Error.Class.Error e) =&gt; Monad (Either e) -- Defined in mtl-1.1.0.2:Control.Monad.Error instance (Control.Monad.Trans.Error.Error e) =&gt; Monad (Either e) -- Defined in Control.Monad.Trans.Error In the first argument of `(=&lt;&lt;)', namely `addMakeAction' In the second argument of `(=&lt;&lt;)', namely `addMakeAction =&lt;&lt; rename env =&lt;&lt; parse s' In the second argument of `(=&lt;&lt;)', namely `interpret =&lt;&lt; addMakeAction =&lt;&lt; rename env =&lt;&lt; parse s' BTW, I think [JPMoresmau's tree](http://github.com/JPMoresmau/scion) is the latest - it has fixes that yours doesn't have, but it hasn't fixed this issue because it doesn't affect EclipseFP. Also, I had to make some tweaks to yi: hunk ./src/Yi/Scion.hs 17 -import Scion.Types hiding (gets) [_$_] +import Scion.Types hiding (gets, io) hunk ./src/Yi/Scion.hs 77 - let Just (grp, _, _, _, _) = renamedSource mod + let Just (grp, _, _, _) = renamedSource mod hunk ./yi.cabal 382 - build-depends: ghc == 6.10.* - build-depends: ghc-syb + build-depends: ghc &gt;= 6.10.1 + build-depends: ghc-syb, ghc-syb-utils 
[Bugs fixed](http://hackage.haskell.org/trac/ghc/query?status=closed&amp;order=priority&amp;col=id&amp;col=summary&amp;col=milestone&amp;col=status&amp;col=type&amp;col=priority&amp;col=component&amp;milestone=6.12.3&amp;resolution=fixed) in this release.
well, never heard of it before, looks great!
And the [Budapest FP group](http://www.meetup.com/fp-bud) also exists :)
Arch tracks [the Haskell Platform specification](http://code.haskell.org/haskell-platform/haskell-platform.cabal).
I built 6.12.3 from source on an Intel Mac, Snow Leopard, with target arch x86_64. Worked perfectly on first attempt. (Binary download from the site only provides the i386 version for the Mac OS X.)
Was it in Macports? The reason I ask is because I am behind a company firewall right now and won't be able to check for another 8 hours.
No, not from Macports. Direct build from source available at http://darcs.haskell.org/download/download_ghc_6_12_3.html Built using following: - Existing GHC -&gt; macports ver 6.10.4 (x86-64) - Cabal 1.8.0.4 - OS = Mac OS X 10.6.3 - Xcode 3.2.1 (x86-64) 
Thanks!
It would be nice if we could finally fix that make error on regex-dna... The old one was complaining about not having haskell-regex-pcre-builtin. However it now seems to be a parsing problem with the use of seq... Is there something prohibiting the use of haskell-regex-pcre-builtin??? 
The library is fine. The problem now is the use of the old parallel package API. It has to be ported to use the different version of `rnf` now available.
Is it possible to get the slides of those talks, if not the videos?
Click on the link at the bottom of the video -- it takes you to a blog post which hosts the slides and abstract. In general we have slides and abstracts.
Thanks for the info, I am attempting to do this also, so it's good to know it works for someone.
Did you make framework-pkg?
[Release notes](http://darcs.haskell.org/download/docs/6.12.3/html/users_guide/release-6-12-3.html)
Wow, does your audience usually spend more time talking then the speaker?
Yes, exactly as per these instructions http://hackage.haskell.org/trac/ghc/wiki/Building/MacOSX I ran just "make framework-pkg" as non-sudo.
Judging from the example you referenced, asynchronous IO seems to be a solution to the problem that threads in F# are very expensive. But since threads in Haskell are very cheap, you don't have that problem in the first place. Second, in case you are dissatisfied with a particular monad, like IO, you can always implement your own. I advertise my package [operational](http://projects.haskell.org/operational/) for that; in particular, have a look at the [WebSessionState.lhs](http://projects.haskell.org/operational/examples/WebSessionState.lhs.html) example that shows a web session that looks like it needs to be executed in a running process, while in fact it's just a CGI script. This is similar to your F# sharp example; the point is that you can write something that looks familiar while it has very clever behavior internally. 
Haskell is doing IO in async way, but exposes sync API for that. If you trying to read from socket in Haskell's thread, runtime will suspend this thread until data will be available.
I assume this was using the macports version of 6.10.4?
Great! I thought nobody was working on this. I'm glad to see that was not the case after all :)
In haskell, threads are extremely light. You can spark hundreds of thousands of them without any trouble. They are in the runtime, not OS threads, so libev has nothing to do with it. Is that what you mean?
right
An ICFP paper just came out. 
Note that in GHC threads are very, very cheap -- 1M + threads on a laptop is feasible -- and they are based over an underlying async mechanism -- epoll, kqueue, select etc. In the new IO manager design both the epoll async layer, and the synchronous layer are available, though the regular synchrnous layer is far easier to program in. An example of an async model in Haskell instead of inverting control via MVars and threads: http://donsbot.wordpress.com/2010/01/17/playing-with-the-new-haskell-epoll-event-library/ So you get both in Haskell, and unlike F#, you also get millions of lightweight threads, transactional memory, data parallelism, and speculative evaluation via sparks :)
Link? Can't find it...
It works perfectly well for me. (Linux, Chromium).
I have the same problem with Firefox on Linux. Though, as it was stated, Chromium works flawlessly.
Now what other language has a package manager that lets you install a new compiler?
Thanks for the reminder. Will take another look.
Are you using GlimmerBlocker (a proxy based ad blocker) or something similar? If so, try turning it off, I cant access research.microsoft.com when using GlimmerBlocker.
Works fine for me (Safari 5/OSX 10.6.3)
It doesn't load for me too (Chromium and Firefox, Ubuntu 10.04). I use private mode in those browsers as a workaround. Opera is able to load part of main page, but not others. [Possibly relevant bug report](http://www.mail-archive.com/universe-bugs@lists.ubuntu.com/msg126760.html). It seems the server responds when I write manually HTTP headers. Edit: Disabling cookies in Chromium makes it work without going to private mode.
I can't access MSR from home, but using the same machine and browser, I can access it from the university network. Really strange. Edit: What the heck? It works in private mode, but only if i paste the link, not if I click the google link. Oh, it works in Safari. Only doesn't work in Chrome, Mac SL.
The paper is: * Rethinking Supercompilation * Neil Mitchell
&gt; This release contains a number of bugfixes relative to 6.12.2, so we recommend upgrading. Is that also true for people using the Haskell Platform? This GHC announcement says that everyone should upgrade. But the [download page](http://darcs.haskell.org/download/download_ghc_6_12_3.html) says clearly *not* to upgrade. On the [Haskell Platform](http://hackage.haskell.org/platform/) download page, no mention is made of anything other than 6.12.1, and the version has not been upgraded there. But it doesn't say not to upgrade. Thanks. P.S. I actually happen to know the answer. I'm just pointing out that from the point of view of users, it's important to be crystal clear and consistent here. 
Interesting. No, I'm not using it. Certainly not in Safari. But that may be another clue about what's happening.
I wrote to the developer of GlimmerBlocker, and his said the fix is to zap some specific cookies, so it might be worth a try to disable cookies, and see if the site loads.
Thanks for the link to the Ubuntu bug report. Summary: the HTML on that site is so malformed that many browsers can't recover from all the errors and choke on it. It can depend on the specific build of the browser and, as we have seen, various seemingly random settings in the browser.
why ghc devs wont add more unicode alternatives (http://bit.ly/amFZfR), at least λ?
λ is a problem. It's a letter in the modern Greek script. If we give it the current magic syntax for lambda, Greek speakers writing Haskell could never again use any word in their language containing that letter. It would also be technically wrong according to the Haskell Report, because we would be treating it as symbol while Unicode defines it as a letter. If we change the parser so that just λ by itself is a reserved word, it would be a major change to Haskell's grammar.
MIDLINE HORIZONTAL ELLIPSIS (…), which had been supported as a Unicode alternative for .., is a problem, and has now been removed. See [GHC bug #3894](http://hackage.haskell.org/trac/ghc/ticket/3894) and the [discussion on the GHC Users list](http://www.haskell.org/pipermail/glasgow-haskell-users/2010-April/018692.html).
Oh, it's slides for a talk by Keegan McAllister. Wow, that was pretty thorough. Where did he give this talk?
&gt; allows Unity apps provided that you get written permission from Apple.
I've had the same problem for at least 2 years on Linux and from two different ISPs.
Thanks, I see ☹
There's a [programming] game in the App Store that does act as an interpreter.. http://itunes.apple.com/us/app/robozzle/id350729261?mt=8 .
Works fine for me, under linux using both Firefox and Chromium
Very true. I wonder if it will be enough for Unity to get the approval or if each developer will need it. The wording seems ambiguous enough to allow both.
I second Chromium. Just install that. At the risk of a flame war 'It's better anyway'. Either way it sure beats restarting your OS.
Boston Haskell, last week, http://groups.google.com/group/bostonhaskell/browse_thread/thread/57394f37403659eb
Works for me on the following configuration: * Firefox 3.6.3 (32 bit x86 version) from mozilla.com * A number of Firefox add-ons, none of which are supposed to affect the rendering of pages (except Greasemonkey, which has no user scripts active for that site). * Ubuntu Lucid Lynx (10.04) 64 bit distribution. * Linux kernel 2.6.32-16-generic (64 bit). 
Thanks, that clears up that issue. I look forward to the epoll work coming into a Haskell release in the future.
If you don't just want simple lists that are repeated (like 'cycle' in roconnor's answer), you can look up 'tying the knot' for the general approach: * http://www.haskell.org/haskellwiki/Tying_the_Knot#Migrated_from_the_old_wiki * http://stackoverflow.com/questions/357956/explanation-of-tying-the-knot/393953#393953 * http://en.wikibooks.org/wiki/Haskell/Laziness#Tying_the_knot * http://www.mail-archive.com/haskell@haskell.org/msg10687.html * http://www.haskell.org/pipermail/haskell-cafe/2009-July/064254.html 
&gt; I'm using a circularly linked list as an example, as it is one of the simplest non-DAG 'types'. You can use the regular list type. There is even a prelude function for making circular lists: cycle xs = xs' where xs' = xs ++ xs' &gt; Creating a circularly linked list, from which one may create new circularly linked lists by applying 'insert' and 'remove' functions seems impossible without jumping through hoops: http://stackoverflow.com/questions/2363264/how-do-you-manage-an-object-graph-in-haskell In Haskell we generally don't "build" data by "inserting" and "removing" things. We construct immutable object with their final values in one go, as done with `cycle` above. BTW, this "tying-the-knot" technique may make your brain hurt if you try to think of the operational semantics of what is happening (though after a long while of thinking you can make sense of it). It is much easier to think of the denotational semantics.
In a way the question does not make sense. Haskell has no concept of pointers, cells, cycles, or anything like that, so the question isn't meaningful about Haskell. A Haskell implementation may use a graph with cycles if it has the right semantics, so for an implementation you can ask the question. Most implementations use cycles to implement certain recursive definitions. I don't know of any implementations that try to preserve these cycle when giving a cyclic argument to a function. For example, xs = 1 : xs is cyclic in ghc, but ys = map id xs is not (I'm ignoring what RULES may to map). 
There is no link yet, but will be as soon as I've done the corrections - maybe next weekend.
This package is highly experimental, feel free to play, but I'm hoping to revise it and make it easier to use in the next month or so.
There are others as well: Jason Reich: http://optimusprime.posterous.com/ Max Bolingbroke: http://www.cl.cam.ac.uk/~mb566/ Peter Jonsson: http://staff.www.ltu.se/~pj/ 
Yes. Sorry about not updating the blog in a while. I'll get our workshop paper and slides up one day... Don't worry though! There has been progress.
Has it been updated since we last communicated?
I don't think so, I just bundled it up and threw it up. If I have updated it, it was almost certainly to break it.
&gt; Is it a fundamental feature/limitation of the maths behind Haskell which prevents types with cyles? no, it's chosen to disallow infinite types for pragmatic reasons.
I gave a talk on this topic several years back: http://web.comlab.ox.ac.uk/oucl/work/duncan.coutts/papers/recursive_data_structures_in_haskell.pdf It explains how you can make cyclic data structures. It also makes the point that you cannot update them incrementally, you make them all in one go, so it's not appropriate for an "object graph" use case as in the StackOverflow question.
&gt; BTW, this "tying-the-knot" technique may make your brain hurt if you try to think of the operational semantics of what is happening (though after a long while of thinking you can make sense of it). It's not too bad if you remember that the Church-Rosser theorem applies to WHNF reduction. So, if you can think of any well-founded evaluation order for the code in question, then WHNF reduction on the code is necessarily also well-founded. After I had that insight, thinking of knot-tying via operational semantics became quite easy.
&gt; I just bundled it up and threw it up Interesting release process!
Ok, I didn't really understand much of that, but do I need to? What kind of thing (excuse the pun) can knowledge of this sort be used for? (in a practical sense; for a mere hobbyist, ie. not someone who's going to be hacking at ghc internals etc.)
Strange. That's exactly the case that does *not* work for me. That's what made me think originally that perhaps it's a problem with my ISP or one of their upstream proxies. But this thread has shown me that there really is a wider problem with MSR's site. Pinning down the exact problem is another matter.
Is there any chance of Supero being integrated into GHC so that we can have all the GHC goodness and extensions + supercompilation? Either way, its fantastic and very impressive work. 
I am revising the paper and wanted to get something up on hackage so I've got a URL I can point at. After I've got the paper finished off, then I'm going to go back to the code. I have to have the paper finished in a month, but the code can then be updated over a much longer time period.
It helps you understand GHCi's output from the `:kind` query?
Actually, I was just entertained by the image of someone throwing up Haskell code :)
If you're only working with typical Haskell this won't help at all, except to indicate that if ghc tells you a kind that includes ? or ?? you can just pretend to read them as '*'. If you're working with unboxed values of any sort, this is key information to understand where they can and can't be passed.
What is up with this site? I'm trying to 'page down' and the text just jumps all over the place.. (using Chromium) oh wait, i see it works much better in firefox..
Must have been designed in Front Page.
Update: Simon has submitted a bug report to Microsoft. Let's hope this bears fruit. In the meantime, we're looking into setting up a mirror.
This seems to be allowed with the new terms, since interpretation of user commands is the express purpose of the app.
[This is the link the bit.ly link redirects to](http://www.haskell.org/~simonmar/papers/strategies.pdf)
That's one excellent paper! Beautiful abstraction and an acute discussion of the why and how and rules of thumb; I love it.
It's the type system of the type system! Without properly understanding it or even having any use of it for quite a while after first reading/hearing about this, I've always found it pretty awesome that such a thing even exists.
To answer your PS: &gt; P.S. I must apologise for not knowing the mathematical terminology for &gt; the question I'm trying to get across. Would spending a few days &gt; (weeks?) learning category theory be a good investment for someone &gt; learning Haskell? Granted, I am nowhere near the level of math competence that most, if not all, of the shining stars of Haskell possess, but as a...Padawan, if-you-will, I would give you this advice: DO NOT look to learn Category Theory in an attempt to understand more Haskell. * For one, Haskell does not strictly follow CT, it just has many type classes with names and concepts inspired by similar forms in it. * Secondly, CT is an immense, and immensely abstract subject - if you want to learn CT, awesome! Learn it - but learn it to learn it, not to learn something else. * Thirdly, Haskell is a semi-difficult language; other languages that are like it are also known as "tough" languages - so, it'd be kind of dumb to help yourself learn a tough subject, by simultaneously learning two tough ones. * Fourthly, the sooner you learn Haskell, the sooner you can feel the HUGE ego rush of using a language that gives you so much power in such a tidy package, and use it to do good! There are plenty of Haskell projects out there that need doing, and we can't let the post-doc academics have all the fun, can we? ;) Now, if CT just sound like spooky fun, and it catches your eye, then go ahead, look up a good book or four, find some Math guru that is only a little grumpy that ten-dimensional math may actually be useful in the Real World, and study your brains out! It's a weird, fun topic, and it's amazing to see how topics you thought were totally different can be tied together in neat little mathy packages. Just don't expect it to make you a Haskell super-hacker in three weeks - Haskell is a little "off" from most other languages out there, and makes you think about what you're doing in unexpected ways. Both subjects are interesting in their own light, and both deserve to have your full attention, when learned. 
Don should know better.
I think we might be missing the point here. Perhaps the blogger goes on about cheap threads but I don't think that is the point of async IO in F#. I was at an [Erlang conference the other day](http://www.erlang-factory.com/conference/London2010/tracks/Give-me-a-break) and Don Syme gave a nice talk about F#. It was pretty clear that the point of async actions is to help you structure your code nicely. I think we should look at the F# API and see if there is a nice pattern we can stick in a library. Of course the implementation would just use normal lightweight threads.
Neat paper, but I'm concerned by the dismal `MatMult` performance. There are lots of memory- and cache-related issues involved with making a fast `MatMult`, but it's an "easy" kernel for parallel scalability. I would be interested to see some analysis to see if it's purely a memory issue or if there is something else limiting scalability. Note that this benchmark runs through 13.7 GB of heap despite a max of 32 MB resident. I also wonder how this interacts with process and memory affinity on NUMA architectures (where accidentally ending up with all pages of a certain temporal locality on one socket kills parallel performance).
&gt; Don should know better. [Blame Twitter, or SMS, or the FCC, but not dons](http://twitter.com/simonmar/status/16156681121)
If your hobbying involves optimizing code for performance (hey, some folks enjoy that sort of thing) then it could be helpful. The actual kinding system itself isn't too interesting, but the reasons for why it is the way it is are what would be helpful. For instance an "unboxed tuple" isn't a tuple at all. What it means is that, rather than returning a tuple we can instead just pass the components in registers. This is great for when the calling function is just going to take the tuple apart anyways, but it's an optimization most users don't have to think about. But, that particular implementation strategy explains why you can't pass unboxed tuples in as arguments to functions: it just doesn't make sense. If your hobbying were to involve learning about the guts of compiling lazy functional languages efficiently, then you may also want to check out [jhc](http://repetae.net/computer/jhc/jhc.shtml) which takes this sort of thing and runs with it. But other than that sort of thing, you can generally just think of everything as having kind `*`, `*-&gt;*`, etc. In terms of the type theory, there's nothing significant lost in that approximation. The only (type theoretically) interesting kind other than `*` is the kind used for GADT indices. However, GHC doesn't distinguish them from `*` AFAIR. 
The solution for me on firefox 3.6.3 is to delete all cookies from "microsoft" using Preferences-&gt;Privacy-&gt;Remove Individual Cookies
Well I did my part, took Haskell from 69x C to 3.8x C.
Great question, would have answered it sooner but I was in the forest this weekend! The tool is running at code generation time, so checks can be performed there. For now, though, everything I've cared about checking can be encoded in types, and we've already got a typechecker (thanks, GHC). If I wanted to do the checks at code generation time, I'd need to both model the problem *and* write code to check the model. This way I only need to focus on the first half of that. Of course, at any time it's possible that there is a problem lurking around the corner where implementing a non-typed check will make more sense. If one shows up, you can bet I'd rather implement a check than try to shoe-horn the problem into types.
We didn't get around to tuning that benchmark. It's a different matmult from the one we used in the ICFP paper last year, which achieved better speedups. In any case, absolute performance wasn't the goal here - I wouldn't be surprised if our matmult is 100 times slower than the best you can do. It represents the matrix as a list of lists, after all. Locality is a huge factor in performance, which is why we have per-CPU nurseries and are careful to trace local roots in the parallel GC; see the ICFP''09 paper.
Upload to vimeo?
I did try this, but free vimeo accounts have a 500MB limit.
Hey nice work. I'm going to take a read of this. Though I have seen news of this around before, is this just the first time that the paper has been released or something? (P.S. UNSW ftw)
The work was originally described in David Terei Honours thesis, but the paper includes our progress since the thesis and up to date performance numbers.
Ah, well in that case, that is important information. Thankyou for sharing it with us.
I'm curious about the calling convention GHC uses. I see that it passes the first couple of arguments in STG registers; how are the other ones passed? Also, the tables-next-to-code optimization: is the function's info table used during the call for anything aside from getting the code pointer? If it's not, then why isn't the code pointer simply stored in the payload instead of the info table, eliminating the need to double indirection?
The info table is needed by the garbage collector. It contains information about the size of the object and which fields are pointers. Note that [dynamic pointer tagging][1] removes quite a bit of overhead introduced by following the info pointer (both with and without TNTC optimisation). Storing both code pointer and info table pointer could increase memory usage &lt;del&gt;substantially&lt;/del&gt;&lt;ins&gt;notably&lt;/ins&gt; if the program allocates a lot of small objects. [1]: http://research.microsoft.com/en-us/um/people/simonpj/papers/ptr-tag/index.htm
Makes me wish I knew how to program Haskell.
The LLVM backend just got into the HEAD (but a slightly older version than the one we benchmarked — more patches will follow): http://www.haskell.org/pipermail/cvs-ghc/2010-June/054437.html
I'd be interested if the measured performance decreases as opposed to the NCG are due to TNTC not implemented for the LLVM backend. The GHC wiki/documentary states [something akin](http://hackage.haskell.org/trac/ghc/wiki/Commentary/Compiler/Backends/LLVM/DevelopmentNotes).
I would like to see more sophisticated numerical integration methods. (yeah I'm a geek)
Fancy seeing you here.
You can! Have a look at the Learning section on the left of [haskell.org](http://haskell.org/).
The 'Righteous Mangler' is not a bad name for a single line of Regex but if it grows any bigger we will have to call it something else and it'll probably be known as the 'Lesser Mangler' as in the lesser of two evils mangler. ... And I just got around to finishing; nice paper, I enjoyed seeing the benchmarks.
Either way I think that it is impressive that even so early an attempt is showing comparable results with the NCG. I would not have predicted that myself and it speaks many positives for the LLVM backend.
Quickchecking for conservation of energy is pretty cool. Conservation laws are sort of like the "free theorems" of the physical world.
The paper also on haskell reddit right now clains they've found a way to do TNTC with LLVM. The nofib performance gap is then said to be 0.1%. So that looks like a yes.
If you didn't have to worry about the NCG and C backends, would you slowly get rid of the cmm intermediate step?
The new paper has corrected many of the performance gaps with TNTC on LLVM.
Could anyone say a few words what is it suppose to be about? I heard about well-founded induction which can be used to proof the termination of recursion functions but searching for well-founded recursion in google didn't find any results.
In fact, conservation of energy follows from time invariance via Noether's theorem, so the analogy may be closer than you think.
That's exactly what I meant. Noether's theorem is the coolest thing about physics that I can claim to at least sort of understand. Unfortunately, you need manifolds and so forth to give a real treatment of Noether's theorem, so it leads you into lie groups rather than a treatment purely in terms of functors and naturality, I think.
Makes sense, thanks for your answer.
You want to do more that not mix up units. The position data type isn't a vector space; it is a Euclidean space. It makes no sense to add positions and subtracting positions ought to yield a vector, not a position. Just like you want vector operations to be invariant under change of basis, so too do you want position operations to be invariant under change of origin.
I'd hold off on it for the moment, even though publicizing it seems reasonable. There's a broader mtl-transformers migration plan underway already, which will eventually be much better.
It's a surprise to read that fib(n) equals fib(n-1) + fib(n-2) **+ 1**. Nice to see improvements of Haskell's deterministic parallelism, though. 
As Simon mentioned this is a simplistic example designed to test our high-level programming constructs on a common data structure. In particular, we want to combine non-intrusive parallelism with clustering, to gain improvements, and test compositionality of the programming model. I have done a bit of tinkering with the tuned parallel matrix multiplication, and initial results give relative speedups btw 5-6 on 7 of 8 cores, with the new strategies. So, the lower speedups in the paper are no limitation of the algorithm neither a limitation of GHC's parallelism. In the tuned version the separation from strategy from alg is not as clear as in the simple version, though. 
Recursion and induction are related. "Inductively defined" types are defined by what constructors they are closed under. Proofs "by induction" are based on the principle that if you show a predicate respects the constructors of an inductively defined type (`forall &lt;v&gt;. &lt;P(v)&gt; -&gt; P(C&lt;v&gt;)`, etc.), then the predicate holds for all values of the type (because all values are built out of those constructors). And functions are defined "by recursion" when they are given for each constructor by appealing to the definition of the function at fields of the constructor. In a dependently typed language that unifies types with propositions, and proofs with programs, proof by induction and function definition by recursion become more or less the same thing. So, with well-founded induction/recursion, it's the same correspondence. You have a relation `&lt;` on a type. For induction, you show that if you can prove that `P(x)` by appealing arbitrarily to `P(y)`, so long as `y &lt; x`, then `P` holds for all values of the type (or at least, "accessible" values). For recursion, you're defining the value of `f x` by appealing arbitrarily to `f y`, as long as `y &lt; x`. And in a suitable dependently typed language, these are one and the same.
I'd just point out that if someone already wrote both of the MonadIO instances themselves, this code will break it. However, maybe if you include OverloadedInstances it would be ok... the whole thing feels a little fishy. But trust me, I feel your pain and see why you'd want it ;).
http://learnyouahaskell.com/chapters
I'm upvoting this on the principle that if I were a (much?) more advanced newbie, it would be helpful: it looks like he took the time to find a good, concrete example. It did make me feel quite stupid, though :)
Probably not. The phase that generates Cmm (from STG-code) compiles many Haskell features (such as laziness) down to imperative code. We could of course try to generate LLVM IR directly, but Cmm is more compact and doing it in two phases keeps each phase simpler.
It won't get any bigger. On the contrary, we hope to be able to get rid of it eventually.
I just made a post outlining how to install haskellmode for vim and I might expand it; does that count: http://massaioli.homelinux.com/wordpress/2010/06/16/haskellmode-guide-awesome-haskell-with-vim/
ah I see the connection now: being able to instantiate a parameter with any type takes the role of being able to transform the coordinate system along a real-valued parameter. 
waited for this so long ... :) thx 
Perhaps. I don't see anything terrible with viewing a Euclidean space as the vector space of translations from the origin. It does force you to pick a particular point as being the origin, which can sometimes be ugly, but in this model there happens to be a natural choice for that.
The original post in the thread is [here](http://www.haskell.org/pipermail/libraries/2010-May/013688.html).
&gt; Lazy ByteStrings should be used for all input data No, they shouldn't. While it's certainly ok to provide such an interface, the real primitive one is one that gets fed an arbitrarily-sized chunk and returns a continuation awaiting further ones (not necessarily implementing the iteratee (as in the hackage package) interface because of dependencies, wrapping such a function into an Iteratee is more than trivial)
I recently got this to build on Snow Leopard. 1. run 'sudo port install gtk2hs' 2. repeat 1 a couple times (the build will fail but this ensures all dependencies are present' 3. cabal install gtk2hs-buildtools (make sure 'ghc --version' spits back 6.10.*). 4. cabal install threadscope 5. reinstall ghc. at present, there exists a binary for ghc 6.12.3. 6. modify your path so that 'which ghc' points to the new ghc. This should do it hopefully. After this is done the threadscope executable will (normally) be in ~/.cabal/bin which should be in your path. Then, make programs with the '-eventlog' flag and run with +RTS -ls -RTS (plus others, e.g. -N). cheers
Indeed, a look into the paper does help, heh.
So what's the answer? If I have the Haskell Platform ... 1) should I upgrade? 2) if so, should I install inside the same folder than the Haskell Platform? Thanks!
yes, and Adam already commented on how LPS is the wrong way to go. While I still believe Lazy ByteStrings should be usable, the primitives being in strict bytestrings is the right way to go - this is precisely the conversation in the link, the main point behind this revision.
It occurs to me that these notes might be a bit too advanced for those who aren't already familiar with Curry-Howard, and a bit basic for those who are. So please let me know your comments!
Perhaps, but as a novice CH-isomorphismer it was a nice refresher. There were a few details I hadn't thought of before or forgotten.
Glad to see Tagged getting more use.
Doesn't changing the calling convention (first few args are in registers) break ABI compatibility? BTW, this paper gave a very nice overview of GHC's pipeline.
Glad to have been introduced to Tagged.
Dude. Asserts.
[TakeoffGW](http://takeoffgw.sourceforge.net/) is a Cygwin-like [project](http://sourceforge.net/projects/takeoffgw/) for Windows, except based on MinGW. In order to add GHC to the project, they need to get it to work in SUSE's cross-compiling system for MinGW. Simon M. is helping them. There is a [follow-up post from Simon](http://www.haskell.org/pipermail/glasgow-haskell-users/2010-June/018908.html). EDIT: added links to TakeoffGW
Hi, To help with your effort, I've created [HOWTO: mingw packaging for openSUSE Build Service](https://sourceforge.net/apps/mediawiki/takeoffgw/index.php?title=Mingw_packaging_on_OBS) based on my own experience. I've created two new packages: - mingw32-pidgin and - mingw64-pidgin
There are a few cross compiling people around (including me) targeting platforms like ARM and such. Much of the work would be similar, so talk to us. Some of us (iPwn Studios &amp; others) have been talking about getting some of this work integrated properly into GHC. The most advanced work I know of to date is at http://www.alpheccar.org/en/posts/show/94 See also http://projects.haskell.org/ghc-iphone/ Sorting out GHC is not especially difficult, but one trouble you'll run into is how to get hsc2hs to work properly. Actually the way to do it would be to use Wine. If you have not done cross compiling before, here are a few things to note: 1. The GHC people use "cross compiling" to refer to the process of porting GHC to a new architecture. Don't be confused by the same word having two meanings. 2. To cross compile, you need to be a masochist. Heed the voice of experience.
You really need a debugger for this kind of programming. When something segfaults the right answer isn't "oh crap, wonder what went wrong, guess I'll have to try some things randomly until I figure it out", it's to fire up the debugger and look at the callstack and all the variables in there and see what's *actually* going on. Also, static analysis can help - maybe it's a silly mistake. Valgrind, perhaps.
Great summary of the whole situation regarding various kinds of hash tables and alternatives. But another important comment in that thread is: "Don't feed the troll." I would like to see Don's post copied elsewhere, so that we can link to it without also giving link karma to the troll.
thank you so much!
sometimes it's important to refute the troll too. He said that this was a "fundamental design flaw in GHC's garbage collector", yet it took me one afternoon and a couple of hundred lines of patch to fix it. 
please do work with us to get this stuff integrated, I'd hate to see the fork diverge too far. We're definitely interested in having your work in the mainline. As usual with things like this, if you can chop it up into smaller pieces that will make it easier to digest...
nice introduction, I didn't know that defunctionalisation had a name, I just presumed it was a matter of course, then again maybe I've subliminally absorbed it somewhere along the way. p.s. I forgot about the flickering 10 minutes into the video, also you could probably experiment with trimming the filesize using ffmpeg. eg. &gt; ffmpeg -i machine.mp4 -vcodec libx264 -vpre normal -cropleft 90 -cropright 90 -croptop 90 -cropbottom 60 -b 450k -r 25 machine-cropped.mp4 Crops the video to the whiteboard, uses about 1/4th the bitrate fixed at 25 frames per second and also reduces the bitrate of the audio, appears to encode at about 4MB/minute, so I would presume it should end up at around 250MB.
-fvia-#haskell also works but the output often winds up interspersed with a lot of other messages from the compiler.
That may mean you fix fundamental design flaws in afternoons, though :-)
I'm curious as to why this was pegged as "a problem with Haskell", as it seems to me (and the article seems to confirm) that, say, Java would have exactly the same issue.
As long as you're explaining: [MinGW](http://en.wikipedia.org/wiki/MinGW) is a distro/version of GCC (and related tools) for Windows, which has a less restrictive license than [Cygwin](http://en.wikipedia.org/wiki/Cygwin); and unlike Cygwin, MinGW strives to be "native" (for example Microsoft's libc "MSVCRT" is used rather than GNU libc).
Because Jon Harrop is a troll.
&gt; I read [...] Jon Harrop's website Well *there's* your problem.
I may be demonstrating profound ignorance here, but couldn't the LLVM back end be leveraged here to make cross compiling less painful?
Well sure, but he is practically orgasmic over F# and wouldn't F# have exactly the same sort of problem?
Through the efforts of my colleague, Chris Poskitt, my most recent talk and preprint paper have found their way onto the research group site. http://www.cs.york.ac.uk/plasma/wiki/index.php?title=Current_events#376
Some more statistics: at this point, there are [2135 Haskell projects](http://github.com/search?type=Repositories&amp;language=&amp;q=language%3AHaskell) on github. __edit__: [1813 projects that are not forks](http://github.com/search?type=Repositories&amp;language=&amp;q=language%3AHaskell+fork%3Afalse)
Those are repos. So all forks of a same project are counted? I guess there are also quite a few tests/throw away/scripts among those project. Edit: well, the number of forks is showed under each project, so maybe this is indeed a project count.
Hey, not too bad. 400 ocaml projects, 1909 Clojure projects (huh!), 2405 Erlang ones (!), 0 F#, 647 scheme.
Right. I stand corrected.
F# is not pure. I know that's no reason, but to Harrop it is.
are there any advantages of code.haskell.org over patch-tag?
I think it's telling that the questioner was worried because he read about the problem on a website, rather than actually encountering the problem himself. Goes to show how dire of a problem it is (and to refute the claim that people avoid the hashtables because they're slow; rather, people don't even know that they're slow first-hand, because they don't use them). That's not to say it isn't good that the problem was fixed. One less thing to complain about.
Because github doesn't recognise f# codes ;)
I'm guessing there's not a lot of open source F# either. But I'd be happy to be shown to be wrong.
Others have handled cyclic values. Cyclic types are allowed, but have to go through a newtype (this is just a marker at compile time, with no runtime cost) newtype Lists = Lists [Lists] defines infinitely nested lists. Allowing infinite types without limiting the recursion is technically easy, but in practice leads to lots of bogus programs being accepted with silly types.
I think there are more F# projects on Microsoft's www.codeplex.com (mercurial hosting site).
Nice. Looks to be only a few under [the F# tag](http://www.codeplex.com/site/search?query=&amp;sortBy=PageViews&amp;tagName=,F%23,&amp;licenses=|&amp;refinedSearch=true).
JVM has used card marking for ages. I don't know what .NET uses, but it doesn't have the performance problem Haskell (used) to.
Honestly, as somebody who has hacked on and with HPDF a bit, I think this is the wrong approach. I think what we really need is a better HPDF, with a cleaner API and better (i.e. any) documentation.
Definitely will submit nice small, neat patches. It may take a few months before anyone can find the time to do it, but we have a need for it, so it will get done.
I am a *functional* alcoholic. I think I've just hit, [puts on sun glasses] rock _|_ Yeah!
how does one get *into* haskell?
[Learn You a Haskell](http://learnyouahaskell.com/) is one way in. The [Haskell Wikibook](http://en.wikibooks.org/wiki/Haskell) is also highly recommended (it's even a featured book on wikibooks). You might check out their [Functional Programming book](http://en.wikibooks.org/wiki/Computer_programming/Functional_programming), too; the foundations in it really do help. 
The issue is in finding clients that are interested in investing in unconventional tools. This is risky (who can they hire to do maintenance if things go south?), so there will need to be a lot of benefits. The easiest way that I can think of to illustrate the benefits is to do a real project and document the results. E.g., do a website pro bono for a local charity and approach it like you are doing a full commercial system. If you can clearly show great benefit then companies are more likely to seriously look at Haskell as a viable option. Even so, I suspect that there is little chance of success. A more viable alternative may be to join a startup. Funding (and pay) will be more of an issue, though.
It's an introductory article about the general abstraction you get from typeclasses, although with a title like that I thought maybe this was type-level sorting :^)
This is the sort of worthless -cafe thread that makes people unsubscribe.
awesome, i've seen learn you a haskell, but not the other two. think i should learn a smidgen of category theory, too? :)
http://www.create-net.org/
Well, I have yet to learn how to make a good title :D
Programming in Haskell, by Graham Hutton is a great introductory book.
code.haskell.org offers bug trackers and mailing lists in addition to space for projects. And they're not mutually exclusive; I have some projects with the repos hosted on patch-tag and a mailing list and bug tracker on the community server.
It's not really SPJ's trick, this pattern has been independently reinvented by pretty much everyone writing a compiler in an ML-family language. The first compiler I ever wrote in OCaml had the monomorphic version of this: type node = { src : src_info; data : exp } and exp = Id of string | App of node * node | ... The only real alternative I've come across is making the annotation nodes part of the AST-- which has the annoying downsides listed in the above post. It would be interesting to collect more of these functional compiler design patterns-- it's not always obvious where the design tradeoffs are when implementing things like an AST, control flow graph (or other intermediate form), polymorphic cache, optimizer, etc... 
I don't understand this concept of C# jobs, Java jobs, PHP jobs. Surely the customer has a problem and seeks a solution, something so technical and arcane as what programming language is used is purely a matter for the craftsman and his team to decide, just as decisions based on what algorithms and data structures to use. 
No need for category theory if you just want to use Haskell.
Oh and he's definitely not trolling http://www.mail-archive.com/haskell-cafe@haskell.org/msg73269.html
Wrong analogy. No matter what tools craftsman uses, client gets the result that any other craftsman later can come and fix/maintain/replace with whatever tools he is comfortable with. Not so with the code. Paying client is getting not only the program itself (what runs), but also it's code (what needs to be maintained). So smart client wants its program to be written in the language for which he can later find craftsmen that are 1: cheap, 2: easily found, 3: replacable. 
yes absolutely
As is often pointed out (or perhaps not often enough?), you don't really need to know any category theory in order to learn Haskell. Some of what's in Haskell has come from CT, but you don't need to know the intellectual background in order to be able to use the tools. Don't get me wrong, CT is beautiful and fascinating and well worth studying. But if you're going to study it, you should study it for its own sake not as an adjunct to learning something else.
Made my day. Thanks!
i laughed at &gt; #scala is full of Haskell lurkers. They bring a lambdabot with them to annoy Java people with what the types of things should be.
The craftsman analogy stands: You can't really hire a carpenter to maintain and develop your stone sculptures.
and here i am going sailing, like a chump
I play alot of roguelikes, and this one tops it for unplayability and unapproachability, but not a great deal else, from what I can see.
This is the first year I actually have the time to compete!
possibly of interest: http://haskell.org/communities/05-2010/html/report.html#sect6.6.1
Can you submit over the Internet?
the irc-channel dedicated to this contest is: #icfp-contest on irc.freenode.net 
You can *only* submit over the internet.
Feel free to improve existing JACK packages, e.g.: http://code.haskell.org/jack/ You may report your progress at http://www.haskell.org/haskellwiki/JACK and join the haskell-art mailing list http://lists.lurk.org/mailman/listinfo/haskell-art . However real-time audio in Haskell is not bound to JACK. I have good experience with ALSA, others use PortAudio. jvoigtlaender kindly pointed to my Haskell synthesizer project. I'm able to perform signal processing in real-time, but I got much more speed when using LLVM as portable inline assembler. 
&gt; Don't get me wrong, CT is beautiful and fascinating and well worth studying. But if you're going to study it, you should study it for its own sake not as an adjunct to learning something else. This is funny advice, since most mathematicians learn what category theory they know as an adjunct to algebraic topology, homological algebra, algebraic geometry, etc. I like category theory for its own sake, but would never tell someone else to avoid learning about it unless they want to learn it for its own sake. In fact, I got into category theory through other fields of math, and think it was a fine way to do so.
category theory seems like a nice complement to OOP as well.
Oh, how does a chump sail? :-)
ICFP is the worst time to start learning a new language
Is it really the smart client who wants the kind of programmers who are cheap, easily found and replaceable? I suppose it depends on whether the project is one that is easily achievable by these types of programmers.
Although I really like jack, I see more use of some abstracted multiplatform sound library for haskell. Since all sound libraries are quite similar in the end, it should actually be possible to do without too much work. Some simple setup functions (get device list, supported formats, etc.), a buffer callback, and a teardown function. Something like that. Guess I should do that myself instead of convincing someone else =)
This is an interesting topic to me. I'd love to do my audio programming and other real-time programming in Haskell, but as I see it, it is currently not possible, because Haskell doesn't have any real-time guarantees. This is mainly due to the fact that the garbage collector is hidden from the programmer and can kick in at any time without bounds on the time it takes. In contrast, in a typical C audio program, the signal processing callback is (or should be anyway) designed to avoid any kind of memory allocation / deallocation, and perform no unbounded operations. The memory determinism is a real killer, as it is opposed to the ethos of most functional languages, including Haskell. This is generally why any modern functional language (or any VM-based, GC'ed language really) cannot (reliably) be used for real-time audio programming. Laziness in Haskell is also a big challenge, in that it can be particularly hard to reason about space and time behaviour. On the other hand, Haskell as a natively compiled functional language is interesting, in that there are clearly demonstrable cases, particularly with loop fusion, where you can look at the compiler output and see that in a particular section of code it performs no memory allocation and never allows the GC to kick in. This kind of post-compilation analysis could allow Haskell to be trusted to perform real-time work. What would be interesting to me would be to have the compiler do this analysis by looking for signs of real-time-incompatible compilation results. That is, the language would actually support the idea of time-determinism guarantees by tagging sections of code. For example, to register an audio callback: registerAudioCallback $ realtime audioFunction where `realtime` is semantically a identity function, but indicates that compilation should fail if `audioFunction` compiles to code that would potentially trigger the allocator or GC. (I'm assuming `audioFunction` is a function that takes a block of signal input and calculates the corresponding signal output, perhaps using an IORef to maintain internal state.) While such a feature would obviously drastically limit the functionality and language features available to `audioFunction`, it would allow a programmer to trust that the compiler doesn't allow unexpected non-time-deterministic behaviour--work is needed to develop solutions for functional approaches to programming that can be used within this RT "bubble". (For example, RT-friendly wait-free FIFO-like "channels" for communication with the non-RT portion of the program.) I should mention that such a feature would also be useful for things like device driver interrupt routines. Another helpful solution would also to add a real-time garbage collector to Haskell, but I think that it is just as interesting to think about how to avoid memory non-determinism entirely. Additionally, there is typically only a small part of a program that needs real-time behaviour, so changing the entire run-time's GC to accomodate 10% of the program isn't really a good trade-off imho. Currently my solution to using Haskell for audio-related programming is to write the real-time portion in C and talk to it through the FFI. This works fine, although lately I've been thinking of looking into Atom to see if it might be an interesting option to replace the C portion of the program. 
Thanks for the detailed answer !
It's true; there is actually a lambdabot in #scala. It's in the default lambdabot config to join that channel among others.
Well, if you're cool with the fact that you won't "win" and just approach it as a fun programming problem that happens to suddenly become available, then it might be fun. Alternatively, it seems to me at least some of the [previous contests](http://icfpcontest.org/) have enough stuff available online to take them on instead, with no time pressure at all. Given the scope of the previous problem that might not be a bad idea, the ICFP contests have been "getting real" lately, it seems to me.
Sure, as I've said the idea of using FP for real-time processes is a topic that I find really interesting, because it seems so challenging and yet would be extremely useful. I know there is some work being done on the topic, with things like Atom as well as with some FRP research, but if we are to ever use Haskell for things like operating systems device drivers or for doing control and simulation, etc., more effort needs to be put into RT research. I'd love to hear from someone knowledgeable about GHC on whether such a "realtime" tag would be feasible to implement. I can imagine it being non-trivial.. for example the tagged function would have to trace any of its dependencies for similar problems. I wondered once whether "realtimeness" could even be an attribute of the type system, but then realized that a function may be real-time in some instances but not others, so maybe the type system is not the appropriate way to propagate this information. (For example, one might want to do offline audio processing using the same code block.)
I'm just reading the problem, and I hope I'm reading it wrong. Is it really the case that people starting late in the weekend will be at a severe disadvantage because everyone else has been accumulating points all weekend?
I guess I could have googled that myself (I didn't know that card marking was the official term), thanks. So I guess that Harrop had a point. Not that it's a fundamental problem, but that it was a genuine problem in Haskell (whether it's more of a problem with the implementation of mutable arrays or of the GC is not something I'm capable of determining). Glad it's been fixed.
&gt; but I got much more speed when using LLVM as portable inline assembler. Are you saying you used an LLVM binding to generate assembly, or that you tried the new LLVM backend for GHC and got a speedup?
Best time, you have motivation and a deadline.
like dan akyrod and eddie murphy at the end of trading places, but with no money and no girls
Building a web site or order processing CRUD system is not exactly a rocket science. No one asks those programmers write parsers or generics library. 
Category theory is absolutely not necessary for any of those. But it is fantastic in taking constructions from one and applying it to the others, so in that guise it has nearly revolutionized all of them.
perhaps more useful would be a good interface to wkhtmltopdf - something that can convert html + css to pdf
psst REDDIT, what's the answer to 1?
I'm sorry but at the end of the day the problem is the problems you have to solve, if you spend 6 months in a problem domain you're not interested in and find solving these problems boring then it doesn't matter if you use your favorite language or not it's going to suck. Anyone with a long enough commercial experience will know this.
This made me curious. How'd you do this? (Although I'd first rename the "Position Vector" to `Point`.)
Check out: http://en.wikipedia.org/wiki/BitC http://www.it-c.dk/research/mlkit/index.php/Main_Page http://www.haskell.org/haskellwiki/DDC
Well sure :) But mathematicians are taught things differently than other scholars of mathematics (e.g., programmers, logicians, philosophers, linguists,...) I wouldn't suggest *avoiding* CT. Rather, there's this conception out there that one needs to learn it in order to grok Haskell, and that's totally wrong. You shouldn't learn CT just in order to learn Haskell. Since Haskell has been influenced by CT, if you already know one of them that'll be helpful for learning the other. But both subjects are sufficiently large and sufficiently different that, if you don't already know CT (or associated mathematics) then you're better served by just learning Haskell directly instead of trying to learn CT as a mere stepping stone to Haskell. One doesn't need a PhD in mathematics to learn Haskell. That's a reputation that really needs to be squelched. IMO, you should learn CT because you're interested in CT--- either for its own sake, or as a foundation for mathematics and as an introduction to universal algebra, abstract topology, etc. Historical development aside, I don't really see much of a difference between CT itself and categorified versions of these other specialties. Instead I see CT as an alternative to traditional (i.e., set-theoretic) mathematics. I think all too often mathematics is taught in a way that recapitulates its historical development, but that historical development is often at odds with presenting things in a natural order for understanding the connections and generalizations/specializations between ideas. Then again, I always seem to learn things in the opposite order from how they're usually presented.
Errh I don't know why the video is getting embedded here (actually it's my video lol) but this isn't correct. EDIT: Forgot to mention this is not my blog.
I can probably dig up some of my example code if you like.
By the way, if you want to do some non-Haskell functional programming for audio, check out [FAUST](http://faust.grame.fr/). A very useful language to describe signal processing block diagrams, and the compiler generates C++ code that can be used to generate Jack programs as well as LADSPAs, PureData plugins, and VSTs for example.
You seem to be diving in to potential solutions too fast. You have not laid out the actual problem that you want to solve. And by 'actual problem', I mean a problem which is phrased in terms of requirements only, and avoids all solution-space talk. You'll get much better answers if you ask that question (at least you will fro me!) The other obvious thing to say is that you're still thinking way too OO. To do things 'right' in Haskell frequently requires a radically different division of concerns, and the only way to do that is to go back to basics.
I second this totally. HPDF shows what is possible, but it is difficult to comprehend. It could do with some sort of refactoring and some sort of support for different fonts - and unicode. It does actually incorporate the TeX algorithm for typesetting text - only it's way faster of course. The PDF format is only slightly more complicated than Html (where it is a question of writing text in any case) only it's a million times better of course. Haskell is in fact an ideal language for keeping track of the several dimensions of a PDF document. It's strange there's a million libraries full of HTML combinators but only one attempt at PDF. 
I'm really just submitting this because I want to show it off at work on Monday and PasteBin's the easiest way to transport it, and then Reddit is the easiest way to make sure I don't lose the PasteBin link. But maybe other folks will find this vaguely amusing too. :-)
You win the last 5 years of [obfuscated Haskell](http://haskell.org/haskellwiki/Obfuscation) code contests. An internet for you! You also get the Haskell Reddit logo in honor! Hint: `view-source:http://localhost:8000/`
HPDF already supports multiple fonts, this I know for sure. I also believe it supports unicode, but I may be mistaken. It has some kind of typesetting algorithm prototyped, but I don't know whether or not it is the same as TeX. Comparing PDF to HTML is a bit apples-to-oranges, and PDF is much more complicated than HTML. I've used HPDF for a few things, and contributed a few patches, but looking through the source code there are large swaths I don't understand very well, with some very ugly idioms.
HPDF supports exactly the fonts mentioned in the original PDF specification: data FontName = Helvetica | Helvetica_Bold | Helvetica_Oblique | Helvetica_BoldOblique | Times_Roman | Times_Bold | Times_Italic | Times_BoldItalic | Courier | Courier_Bold | Courier_Oblique | Courier_BoldOblique | Symbol | ZapfDingbats deriving(Eq,Ord,Enum) The characters and encodings supported are summed up here: -- | Create a PDF string from an Haskell one toPDFString :: String -&gt; PDFString toPDFString = PDFString . S.pack . map encodeISO88591 -- . escapeString encodeISO88591 :: Char -&gt; Word8 encodeISO88591 a = let c = fromEnum a in if c &lt; 32 || c &gt;= 256 then 32 else fromIntegral c 
I'd love to see it.
One of this guy's points is that Haskell doesn't have some multi-core advantage over other languages. From what I've read, software transactional memory is such an advantage. Specifically, transaction rollbacks are far more expensive to the point of being a net loss for imperative languages, but STM is still a net advantage in a pure language. Is that correct?
Yeah, except at least my Opardum server project (99.9%) haskell, github decided to tag it as JavaScript and give me no way to override :/
that's really nice ! I will have a look at this.
My love affair with Haskell was brought into check by implementing and *maintaining* a significant, real world problem with it, and comparing the experience to other languages. My area is web development, and compared with Django/Python, there are some pretty big disadvantages to using Haskell, including: * difficulty of debugging (especially post-mortem): with my Django process, any time any uncaught exception occurs, I get an e-mail containing a full strack trace and request data. This is usually enough to find the bug very quickly. Try doing that with an arbitrary Haskell process. Adding debug symbols is not a solution - I need this on the *live* server, and without a performance hit. * difficulty of controlling space leaks and memory consumption: my Haskell blog software uses a large amount of memory. Each 1k comment on a post requires an additional 300k to render. I strongly suspect the culprit is the template engine I used - HStringTemplate, which seems to be nicely written using obvious techniques (parsec), by someone more competent than me. I haven't begun to see if I could fix it, and even if I can find the problem, the solution might be a complete rewrite of HStringTemplate. * difficulty of understanding code: compared to Python code, Haskell can often be impenetrable. Lots of things contribute to this, but it is not simply a case of "you can produce bad code in any language" - Haskell is particularly good at allowing you to express yourself with wonderful flexibility, which matches your thought patterns at the time, but might be nearly impossible to match to how anyone else thinks. * difficulty of adding side effects: sometimes you just need to add some side effect at a certain point - like logging etc. Or this [wonderful/awful hack for tracing a SQL statement back to the URL that caused it](http://chris-lamb.co.uk/2010/06/01/appending-request-url-sql-statements-django/). Haskell makes this kind of thing very painful compared to other languages. If you are already in a monad stack, it is better, but monad transformer stacks are hardly the nicest thing to manipulate. What is a small change in another language becomes extremely intrusive in Haskell, because it propagates through all the type signatures. * can you do exploratory programming from deep within a program execution in Haskell, like [this](http://lukeplant.me.uk/blog/posts/exploratory-programming-with-ipython/), easily? All these are reasons why I am *so* glad that some projects I maintain are in Python and other languages, and not Haskell. Of course, I still really love Haskell, but seeing these disadvantages allows me to get motivation back. You see that "practicality beats purity", and you can take pride in your work using other languages on that basis.
Very nice. But let's face it - Haskell will never beat Perl at this. Line noise syntax, together with the "There's More Than One Way To Do it" principle, give Perl a varied palette with unmatched programming power.
In theory, they are terrible. ;) In practice sometimes they are unavoidable, as later libraries added to hackage that expose properties of a large number of built-in types have to choose between providing instances for everything and the kitchen sink out of pre-existing libraries that are out there, thereby bloating their import list and limiting the number of scenarios in which they are applicable, or provide a series of packages which exist solely to provide orphan instances, and thereby risk accidental non-inclusion and the host of problems that orphan instances can entail. 
I have been wondering about how best (or most standardly/idiomatically) to organize tests (specifically HUnit tests, but could also apply to QuickCheck), versus non-test code. For large-scale Java development, we often partition these at a top level, for example, having separate src and test directories, perhaps with parallel directory structures underneath. One benefit of this approach is that after your build process runs the unit tests, it can package up the src directory, and not have the final executable bloated with tests. Other options I see are to put the tests in the same source file with the code being tested (I imagine this is popular with QuickCheck, although maybe less so with HUnit?), or to put the tests in separate files in the same directory. Any input about existing standards in the Haskell community and pointers to related docs, would be greatly appreciated, thanks.
I think a mechanism to explicitly control instance visibility would be great. Keeping track of where instances are coming from can be quite difficult.
I'd love to see an instance graph, showing package boundaries, with arrows from classes to the packages that have instances of that class.
I think the most common approach these days is a separate tests directory (with its own cabal file) that tests the visible API of your library. For tests for internal components, the use of cabal -ftest and CPP to enable the tests, which are written in each module, is probably the only way.
That breaks the whole point of the instance mechanism, as it allows two bits of code to act with divergent opinions on the same data. Instances are great for enforcing invariants. For "well known" type classes, the data definition should provide the instance, (so long as there is only one way for the data to be part of the type class). The problem is for new classes, where you can't alter the data definition. Naturally, either the type class provider has to do it, or the user of both the data type and the type class. must provide it. If the type class provider can provide it, great -- the data type provider need not, and nor does anyone else. (Unfortunately this breaks if the typeclass and the data both add an instance in the same release time frame). If the data type also isn't "well known", this isn't possible, and it falls back on the user to do so. Once the user has, then problems show up when you try to use multiple libraries possibly causing dependence on multiple instances. The solution (as the solution to multiple ways a given data type can be a member of a typeclass) is judicious use of newtypes, and they can then "own" the instance. In practice most of the problems would go away if GHC could compare definitions for multiple instances and complain if it couldn't show they were equivalent.
Your project was split between Haskell and Literate Haskell. I've added code to count the latter as the former, so it'll be marked as Haskell shortly.
orphan instances are bad, mmkay...
I've got about 2+ intense years and 67,000 lines of code of real world Haskell experience, plus many years' experience of mainstream languages. I really love Haskell, and would not use anything else unless I had to, but I think if you wanted to list Haskell's weak points for commercial use, you've done a good job. The last three are valid points, but they have not been practical issues for me. Unlike your experience I've found other people's code generally easy to read, and I've never had a situation where I wanted to add a side effect but the effort was prohibitive. The ability to control side effects at all is so brilliant, that the extra effort needed when I haven't got one where I need it seems trivial in comparison. I have worked on a large Python project. It was not pleasant, and I would never repeat the experience. Re-factoring is extremely risky in Python, to the point that I was afraid even to rename a method. I agree that Haskell's debugging support is not good, but since you need to do it so rarely in comparison, overall it is hugely less onerous. I definitely agree about space leaks. It's certainly possible to deal with them, but the learning curve is quite significant and the frustration can be high. I've found the way to mitigate this in a commercial situation is to consider them in the design, and to deal with them early. They generally only affect certain areas of the code, and if they're designed correctly, the problems can be eliminated from the start, but it requires a bit of experience to get this right. My area is video games, which are quite a "state-heavy" application, so I've had to think about this a lot. Even with the space leaks, I still think Haskell takes most of the pain out of programming. I'm only talking about huge programs here, and that's exactly where Haskell excels. 
A separate `tests` directory seems to be the most common practice indeed. As for building the tests, people use either Cabal (using either the the project's .cabal file or a separate file used only for tests) or make. The advantage of using make is that you can test internal modules without any `-ftest` flag polluting the package specification. The downside of using make is that it doesn't integrate seamlessly with Cabal's dependency resolution. My Google Summer of Code student is working on adding test support to Cabal so by the end of this summer you should be able to specify test stanzas in your Cabal file which will give you both accurate dependency tracking and a principled way of defining your test suites, without polluting the library with `test` flags. It will also make it easier to run the tests using a continuos build system. 
Ok, I stand corrected. I've pushed a fork of HPDF to GitHub [here](http://github.com/lpsmith/HasPDF), it needs a better name and a lot more work. 
Thanks, tibbe and dons, for the recommendations. It sounds like the advantage of make here is only a temporary one. Can you point to one or two modules that you consider good examples of the "definitive" project/test organization? It's not clear whether you advocate an "src" directory at the same level as "tests". Also, in situations where each of several groups of tests have their own battery of data files driving the tests (e.g., good &amp; bad input test cases, and expected results, manifests pointing to these, etc.), would these typically be placed subdirectories inside of "tests", and referenced with a filename relative to the location of the test .hs? Or relative to "tests"? What's the default working directory when these tests kick off from Cabal? Thanks again!
I've just added F# to GitHub. It was originally being counted as OCaml. http://github.com/languages/F%23
Here's a project I'm working on: http://github.com/tibbe/event We have a `src` and `tests` directory at the same level and build our tests using make. As for the organization of the `tests` directory I usually start with a flat structure until I get enough tests that more sub-directories are needed. I'm not sure about data files. Cabal will build the test binary inside `dist` and run it from there with the Cabal root directory as working directory. I think you could use Cabal's "path" feature, at run time, to figure out where your data files are located. (Cabal generates a `_paths` module which lists your data dir.)
You've got a lot more experience in Haskell than I have, and other languages too, so perhaps if I really mastered the language the points I mentioned would be less of an issue for me. But there is still the practical point that mastering the language is harder, which means that getting a capable team together is harder, making it less attractive and less practical from the commercial (or even open source team) point of view.
Well, the changes so far focus on improving Graphics.PDF.Coordinates, and treating points as first class values (specifically, as complex numbers...), not as separate x and y coordinates to a function argument.
&gt; That breaks the whole point of the instance mechanism It certainly does not. Classes are for increasing the usefulness of data types with ad-hoc polymorphism, not for limiting their usefulness. &gt; it allows two bits of code to act with divergent opinions on the same data No, it allows two different applications to re-use the same bit of code in an appropriate way for each context, polymorphically. &gt; Instances are great for enforcing invariants. It would be great to be able to use instances in that way. Invariants are enforced by exposing a consistent API using the import/export facilities of the module system, but that is woefully lacking for instances at the moment. Of course, in some cases you are right. If the instance expresses an inherent quality of the data type, not a specialized use of it, then there really needs to be one and only one instance. That instance needs to be defined in the same module as the data type or the class declaration. And it's not a good idea to block the import of such an instance. The problem is that sometimes a mistake is made, or the proper perspective only becomes clear later on when it is too late to change. The obvious example is the Monad instance for Either in mtl and transformers. At the time it seemed like a natural Monad instance for that type. But in retrospect, that instance is actually very specialized and should indeed be blocked except when Either is being used for exception handling. So this is something that needs to be left to the judgement of the programmer. It is a mistake to tie the hands of the programmer unconditionally. &gt; The solution (as the solution to multiple ways &gt; a given data type can be a member of a typeclass) &gt; is judicious use of newtypes. Sometimes. Other times it is a messy work-around. Other times it is so unwieldy as to be totally impractical. This is really an issue of namespace control. For many well-known reasons, global names are bad. Currently, the only way to define an instance is as what is effectively a global name. That is bad. It should be fixed. &gt; In practice most of the problems would go away if GHC could &gt; compare definitions for multiple instances and complain if it &gt; couldn't show they were equivalent. That would be a misfeature, I certainly hope it never appears in GHC. Just give me a way to block instances that I don't want. I'll decide for myself. You can decide for yourself, too - if you don't want to, you don't ever have to use it.
Additionally some packages separates the instances. For example chp provides CHP monad and chp-mtl provides instances for transformers in CHP. IMHO there could be blessed instances: -- Package chp module Some.Module where data CHP a = .... blessed instances Some.Other.Module blessed instances ... -- Package chp-mtl module Some.Other.Module where instance Class CHP where ... Now: - chp-mtl may depend on chp - chp does not depend on chp-mtl - there is no warning about orphan instances (in this case - not in general) 
As Gour pointed out on the blog, there is also [hakyll](http://hackage.haskell.org/package/hakyll). Further alternatives: * [yst](http://hackage.haskell.org/package/yst) * [halipeto](http://hackage.haskell.org/package/halipeto)
The [wiki page](http://www.haskell.org/haskellwiki/Practical_web_programming_in_Haskell) about web programming in Haskell has some great descriptions about CGI programming, and a few links to other things, but obviously there so much more that that going on now. I think that page should give a short, simple overview of the various approaches available, with links to a sub-page for each approach. But anyway, start with at least links on that page to your favorite web frameworks and apps. I just gathered a couple links for static site generators, so I'm pasting them there too. 
+1 for Hakyll, very good. I added Yesod to the web frameworks section, but I think the real problem with that wiki page is the page itself: it's completely unorganized and spends more time discussing nitty-gritties than actual programming. For example, it discusses how to make a static binary before showing any code! I think I'll start a thread on the web-devel mailing list to discuss how to fix up the Wiki page.
&gt; I think I'll start a thread on the web-devel mailing list to discuss how to fix up the Wiki page. Sounds good. Thanks!
&gt; Classes are for increasing the usefulness of data types with ad-hoc polymorphism, not for limiting their usefulness. The key difference between instances of type classes and other means of binding functions and data together (e.g. dictionary in the data) is that type class instances can force this to binding to be globally unique. Instances of type-classes let me use the type system to statically guarantee that rather than passing possibly inconsistent ordering functions into a map, I force the use of the same exact comparison function. Oh, just put the ordering function in the creation of the map, you say? What if I want to union two Maps? If you don't want that, don't use type classes! You can even pass around the dictionary separately from the data in its own function argument for even further flexibility. &gt; No, it allows two different applications to re-use the same bit of code in an appropriate way for each context, polymorphically. So does any polymorphic function -- for ad-hoc polymorphism that isn't type-global, we have these wonderful things called "higher order functions". &gt; Invariants are enforced by exposing a consistent API How do I construct a consistent API that prevents the combination of objects whose internal state depends on two different implementations of the same interface? How do I guarantee it using the type system, but not globally unique instances? &gt; The problem is that sometimes a mistake is made, True, hence my small caveat: (so long as there is only one way for the data to be part of the type class), which I did not emphasize nearly enough. &gt; This is really an issue of namespace control. For many well-known reasons, global names are bad. Classes are not global names. They follow the module namespace like anything else. You can have a type class Data.Concatable and a different Text.Concatable, with the same function names even. And the normal module imports (including qualified) will sort them out. Instances aren't names at all but linking two things -- the class and the type. They are global _type properties_ which is a different matter entirely. 
I'm sorry I didn't get to the interesting problem either. Poked at the circuit format. Wrote a function to generate random circuits. Wrote what I thought was a simulator, and tried to brute-force the gate function and input stream from a few input/output pairs. After a comprehensive search, no gate function matched. So I figured there was a bug in my simulator, and it wasn't worth sorting out, compared to enjoying my weekend. Would have been nice to hobble together something that at least yielded some score at all, though.
I totally agree. HPDF was my first big Haskell library to understand what it means to write an Haskell library for someone coming from the imperative programming world. And the result is exactly what an Haskell library must NOT be ! But at least it allowed me to understand a lot of things about what is a good Haskell API. If I had some free time, I would be the first to start again a new implementation which should, at least, abandon the current monadic API (too many monads in HPDF) and be compositional. And it should support Unicode. I would also make the typesetting and hyphenation as separate libraries and more generic. So, my advise : I think it should be simpler to start from zero. At least you could reuse the low level part of HPDF which is generating the PDF code. 
If you are curious, the gate function was (a,b) -&gt; (a-b,ab-1) (mod 3).
I wish I'd been able to give the contest a proper go--I registered late on Friday, and spent a few hours here and there working on it over the weekend, but I already had other major time commitments. Figuring out the circuit format and logic and writing a simulator took a stupid amount of time (wasted several hours due to a misguided notion of what counted as "backwards"), and while a stupid-but-effective algorithm was obvious for constructing a large, ugly circuit to produce an arbitrary ternary output stream, coding it was a bit time-consuming, and by that point it was late on Sunday and I gave up. Never even got as far as designing cars or fuel...
I signed up, started reading the problem description, found out that I was expected to guess at the problem rules, then lost all enthusiasm for the contest. Started playing TF2.
I wasn't planning on participating, but usually I at least read the rules and regret it. This year I was glad to turn to my personal projects instead. A programming contest should be about receiving a task and figuring out the best way to solve it, not about working out what the task is in the first place. I get enough of that from work.
Well, I'm not sure that a monadic interface is necessarily a bad thing. :) I think that [Haven](http://haskell.org/haven/) has some interesting ideas as far as structuring the API is concerned, although it's pretty throughly bit-rotten and largely undocumented. Personally I excel at smallish problems that are deep, but really get bogged down in largish problems that are more shallow. Thus I've found it hard to get a good start on a clean implementation, and working on HPDF has been easier. At least at this point, I've read the Adobe PDF reference manual enough that it's starting to make sense to me. 
This was the first year I could have spent the whole week-end on participating but I was really disapointed by the discover-the-rules (and less importantly also by the continous and degressive scoring). I just built a circuit to generate the key prefix then lost interest. I think the whole thing would have been much more enjoyable if they provided smaller steps, to make it easier to find out if our assumptions were good or bad. Overall I spent more time with a pen and paper, and trying to get parse errors from their server, than programming. But as it appears, some team were pretty fast to come up with something; maybe it was not my day.
In 2006, the contest featured some sort of guesswork/reverse engineering component for the 1st time. Some people liked that... And in 2007 the contest was also a lot like that. I had the feeling that it was generally despised, and we'd be free of such crap for a long time. But there it comes back again.
This was my and my team-mates' first year, and we found it brutally difficult. We only worked out how to create circuits that could create arbitrary ternary streams by the end of Saturday, and it was the end of Sunday by the time we'd figured out the ternary encoding schemes for cars and fuels. That only left Monday morning to attempt the meat of the competition - creating fuels that satisfied the constraints - by which time I was back at work and had to let my team-mates carry on without me. So the vast majority of the weekend was spent mostly debugging and reverse engineering, with only small amounts of programming in support (scripted interface to the webserver; circuit parsing, simulation and visualisation). It's a real shame the obfuscation was so difficult to overcome, since the real problem (termination problem) turned out to be one that researchers are actually interested in, and I suspect very few teams got far enough to give it much thought at all. On the bright side, I now know OCaml, since that's my team-mates' current language of choice. I was thrown in at the deep end, but was reasonably pleased with how quickly I picked it up, due to my Haskell knowledge. It was both strangely familiar and unfamiliar at the same time. I only used one mutable variable all weekend, but I washed my hands after just to be safe.
I admit I never figured out what the proper notion of "backwards" was. I assumed backwards meant any edge that would result in a cycle when topologically sorting the fuel graph. This resulted in circuits that I could easily evaluate ( just in order of their topological sort ) but since I also never figured out the cell operation ( :-( ) I don't know if this was correct.
Yeah, last years problem I thought was more reasonable. The guessing for the circuit creation was just annoying. As someone else all ready said, it would have been better to give us a task. If there had been more than 72 hours, then I probably wouldn't have been annoyed about guessing the circuit creation rules.
"backward" just meant any wire from a gate that appeared later in your circuit description, to a gate that appeared earlier. I agree here. This is part of the "petty" bit. It would have been very easy, and detracted from none of the interesting challenge, to describe the circuit (minus the gate function) in detail. Instead, they throw a bunch of vaguely suggestive text and say "figure it out." I lucked out on that one. I had the right intuition from the start, and my first attempt as a simulator was correct. But I know other people I talked to today who just naturally figured the numbers in the circuit description were wires, and they puzzled over it for hours before realizing it was completely the wrong interpretation of the description format. Those people aren't dumber than me; just didn't get as lucky. That's the problem with making the "challenge" arbitrary. When someone was writing the task description and realized that they needed to use words like "forward" and "backward" without being able to define what they mean (and the quotes in the task description suggest they did explicitly realize this), surely something in the back of their minds called out "hold off a bit. You aren't creating a clever problem. Rather, you're just writing jibberish."
Your assumption is precisely the mistake that cost me several hours. The correct notion is as cdsmith explains. Irritatingly, in hindsight, the correct interpretation is actually *simpler* in many ways, by not requiring any analysis of circuit topology! Nevertheless, on some level the "cycles in a graph" makes more *sense* to me still.
&gt; Instances of type-classes let me use the type system to statically guarantee that rather than passing possibly inconsistent ordering functions into a map, I force the use of the same exact comparison function. Oh, just put the ordering function in the creation of the map, you say? What if I want to union two Maps? That is an excellent example. It shows that using two different versions of an instance in the same program is bad design, and should be avoided. But that doesn't mean I must be denied the ability to choose which single instance to use in my program, just because the author of a library implemented a different instance than the one I need. &gt; How do I construct a consistent API that prevents the combination of objects whose internal state depends on two different implementations of the same interface? How do I guarantee it using the type system, but not globally unique instances? When you are forced to define an orphan instance, block the import of any other. &gt;&gt; global names are bad. &gt; Instances aren't names at all but linking two things -- the class and the type. They are global type properties which is a different matter entirely. Not really a different matter. They break encapsulation and module-level composability just the same. And that can be the elephant in the room for building large systems in Haskell. 
The 2006 contest was amazingly fun. It started out with a VM you had to implement, then the whole contest involved solving puzzles on a computer system simulated within the VM. But all this stuff was adequately defined. I don't remember there being any reverse engineering.
&gt; They break encapsulation and module-level composability just the same. And that can be the elephant in the room for building large systems in Haskell. Global instances don't break encapsulation and composability, but ensure that if you compose and the composition is allowed, that it works. This is square in the vein of Haskell's static type system guaranteeing soundness, but possibly rejecting some programs that could work with dynamic types. &gt; But that doesn't mean I must be denied the ability to choose which single instance to use in my program, just because the author of a library implemented a different instance than the one I need. Unfortunately, it does. If library A implements a type class for a common data type one way, and and library B implements it a different way, they really should not be used together in the same program. Similarly a client using a type class differently than a library does. You really should blame the authors for writing bad code that's hard to use, because it essentially "makes a new type" -- something that should be reserved for (a) the type implementor, (b) the class implementor, or (c) an API "end user" who has no further clients. The last is, of course, an orphan instance, but it's usually pretty trivial to wrap in a newtype and use that. &gt; When you are forced to define an orphan instance, block the import of any other. But how do you keep other places in the code that import or define an instance "walled" off so as to not interact with other uses of the same instance defined differently? And guarantee that statically by the compiler?
Ah thanks -- that's what I got wrong. It didn't make sense to me that one wire from the external gate would run backward, and the other forward.
You had to dig your way in to the actual problem description, solving puzzles. In that 2006 contests there were the seeds of the big annoyances of 2007 and 2010. (Obfuscation, small step stones to get to the actual problems) In 2006 they got the "right" balance of puzzle/programming -- it seemed that some people enjoyed that any way. I see the 2007 and 2010 contests as reminiscent of the 2006 one, only worse.
Here is a start. I haven't done the types for lines, planes and volumes. {-# OPTIONS_GHC -fglasgow-exts #-} module Clifford3 where infixr 7 /\ infix 7 .: infixr 7 &lt;*&gt; infixl 6 &lt;+&gt; infixl 6 &lt;-&gt; infixl 6 .+&gt; infixl 6 &lt;+. infixl 6 .-. class Group a where zero :: a (&lt;+&gt;) :: a -&gt; a -&gt; a opp :: a -&gt; a (&lt;-&gt;) :: a -&gt; a -&gt; a a &lt;-&gt; b = a &lt;+&gt; (opp b) class Wedge a b c | a b -&gt; c {-, b c -&gt; a, c a -&gt; b -}where (/\) :: a -&gt; b -&gt; c class Contraction a b c | a b -&gt; c{-, b c -&gt; a, c a -&gt; b -}where (.:) :: a -&gt; b -&gt; c class GeometricProduct a where (&lt;*&gt;) :: a -&gt; a -&gt; a epsilon :: RealFloat a =&gt; a epsilon = encodeFloat 1 (fromIntegral $ 1-floatDigits epsilon) {- Boosted from Boost http://www.boost.org/boost/math/special_functions/sinc.hpp -} sinc :: RealFloat a =&gt; a -&gt; a sinc x | (abs x) &gt;= taylor_n_bound = (sin x)/x | otherwise = 1 - (x^2/6) + (x^4/120) where taylor_n_bound = sqrt $ sqrt epsilon type Scalar = Double {- basis e1 e2 e3 -} data Vector = Vector Scalar Scalar Scalar deriving (Read,Show) e1 = Vector 1.0 0.0 0.0 e2 = Vector 0.0 1.0 0.0 e3 = Vector 0.0 0.0 1.0 {- basis e12 e31 e23 -} data BiVector = BiVector Scalar Scalar Scalar deriving (Read,Show) e12 = e1 /\ e2 e13 = e1 /\ e3 e21 = e2 /\ e1 e23 = e2 /\ e3 e31 = e3 /\ e1 e32 = e3 /\ e2 {- basis e123 -} newtype TriVector = TriVector Scalar deriving (Read,Show) e123 = e1 /\ e2 /\ e3 {- Definition of addition (&lt;+&gt;) -} instance Group Scalar where zero = 0.0 a &lt;+&gt; b = a + b opp a = -a instance Group Vector where zero = (Vector 0.0 0.0 0.0) (Vector x1 y1 z1) &lt;+&gt; (Vector x2 y2 z2) = Vector (x1+x2) (y1+y2) (z1+z2) opp (Vector x y z) = Vector (-x) (-y) (-z) instance Group BiVector where zero = (BiVector 0.0 0.0 0.0) (BiVector z1 y1 x1) &lt;+&gt; (BiVector z2 y2 x2) = BiVector (z1+z2) (y1+y2) (x1+x2) opp (BiVector z y x) = BiVector (-z) (-y) (-x) instance Group TriVector where zero = (TriVector 0.0) (TriVector a) &lt;+&gt; (TriVector b) = TriVector (a+b) opp (TriVector a) = TriVector (-a) {- Definition of wedge product (outer product) (/\) -} instance Wedge Scalar Scalar Scalar where a /\ b = a*b instance Wedge Scalar Vector Vector where a /\ (Vector x y z) = Vector (a*x) (a*y) (a*z) instance Wedge Scalar BiVector BiVector where a /\ (BiVector z y x) = BiVector (a*z) (a*y) (a*x) instance Wedge Scalar TriVector TriVector where a /\ (TriVector b) = TriVector (a*b) instance Wedge Vector Scalar Vector where x /\ a = a /\ x instance Wedge Vector Vector BiVector where (Vector x1 y1 z1) /\ (Vector x2 y2 z2) = BiVector (x1*y2 - y1*x2) (z1*x2 - x1*z2) (y1*z2 - z1*y2) instance Wedge Vector BiVector TriVector where (Vector x1 y1 z1) /\ (BiVector z2 y2 x2) = TriVector (x1*x2 + y1*y2 + z1*z2) instance Wedge BiVector Scalar BiVector where x /\ a = a /\ x instance Wedge BiVector Vector TriVector where x /\ y = y /\ x instance Wedge TriVector Scalar TriVector where x /\ y = y /\ x {- Definition of Contraction (inner product) (.:) -} instance Contraction Scalar Scalar Scalar where a .: b = a*b instance Contraction Scalar Vector Vector where a .: (Vector x y z) = Vector (a*x) (a*y) (a*z) instance Contraction Vector Vector Scalar where (Vector x1 y1 z1) .: (Vector x2 y2 z2) = x1*x2 + y1*y2 + z1*z2 instance Contraction Scalar BiVector BiVector where a .: (BiVector z y x) = BiVector (a*z) (a*y) (a*x) instance Contraction Vector BiVector Vector where (Vector x1 y1 z1) .: (BiVector z2 y2 x2) = Vector (z1*y2 - y1*z2) (x1*z2 - z1*x2) (y1*x2 - x1*y2) instance Contraction BiVector BiVector Scalar where (BiVector z1 y1 x1) .: (BiVector z2 y2 x2) = -z1*z2 - y1*y2 - x1*x2 instance Contraction Scalar TriVector TriVector where a .: (TriVector b) = TriVector (a*b) instance Contraction Vector TriVector BiVector where (Vector x y z) .: (TriVector b) = BiVector (z*b) (y*b) (x*b) instance Contraction BiVector TriVector Vector where (BiVector z y x) .: (TriVector b) = Vector (-x*b) (-y*b) (-z*b) instance Contraction TriVector TriVector Scalar where (TriVector a) .: (TriVector b) = -a*b {- Definition of the reverse operator -} {- rev (x /\ y /\ ... /\ z) = z /\ ... /\ y /\ x -} class Reverse a where rev :: a -&gt; a norm2 :: a -&gt; Scalar norm :: a -&gt; Scalar normalize :: a -&gt; a inv :: a -&gt; a defaultnorm2 x = x .: (rev x) defaultnorm x = sqrt (norm2 x) defaultnormalize x = (recip (norm x)) /\ x defaultinv x = (recip (norm2 x)) /\ (rev x) instance Reverse Scalar where rev = id norm2 x = x^2 norm = id normalize x = 1 inv = recip instance Reverse Vector where rev = id norm2 = defaultnorm2 norm = defaultnorm normalize = defaultnormalize inv = defaultinv instance Reverse BiVector where rev = opp norm2 = defaultnorm2 norm = defaultnorm normalize = defaultnormalize inv = defaultinv instance Reverse TriVector where rev = opp norm2 = defaultnorm2 norm = defaultnorm normalize = defaultnormalize inv = defaultinv {- Points -} newtype Point = Point Vector (.-.) :: Point -&gt; Point -&gt; Vector (Point p) .-. (Point q) = p &lt;-&gt; q (.+&gt;) :: Point -&gt; Vector -&gt; Point (Point p) .+&gt; y = Point (p &lt;+&gt; y) (&lt;+.) :: Vector -&gt; Point -&gt; Point x &lt;+. (Point p) = Point (p &lt;+&gt; x) {- Rotors are unit quaternions -} data Rotor = Rotor Scalar BiVector deriving (Read, Show) expRotor :: BiVector -&gt; Rotor expRotor x = Rotor (cos phi) (x/\((sinc phi)/2.0)) where phi = (norm x)/2.0 instance GeometricProduct Rotor where (Rotor a1 (BiVector z1 y1 x1)) &lt;*&gt; (Rotor a2 (BiVector z2 y2 x2)) = Rotor (a1*a2 - z1*z2 - y1*y2 - x1*x2) (BiVector (a1*z2 + a2*z1 + y1*x2 - x1*y2) (a1*y2 + a2*y1 + x1*z2 - z1*x2) (a1*x2 + a2*x1 + z1*y2 - y1*z2)) instance Reverse Rotor where rev (Rotor a v) = Rotor a (opp v) norm2 _ = 1.0 norm _ = 1.0 normalize = id inv = rev 
There were 872 registered teams. 215 scored some points. That means they would have crowdsourced much more by giving better explanation about their encoding. I guess the vast majority of participants had no fun being at a state that gave 0 points.
I think it's fairly natural to assume that the order given in the description is arbitrary -- real circuits aren't evaluated in the order the components are placed on the breadboard.
They couldn't even be bothered to provide citations for how a car is a rewriting termination problem.
Not to mention those of us who didn't even start because the reverse engineering portion didn't sound at all interesting, novel or fun.
The sad fact is that orphan instances do exist in Haskell, and always will; we are way too far down the road to stop it. As a result, encapsulation and composability are indeed very badly broken at the module level. Furthermore, Haskell projects currently face the risk that a critical commonly-used package may suddenly turn out to be unusable in the project, which is a totally unacceptable risk in any real-world project. So there is no choice - the inability to block the import of instances must be viewed as a serious bug in any Haskell compiler. Once this critical capability exists, there are two ways you can view it: You can take the view that Haskell's type system in principle does provide the global static guarantee that you would like it to. Then you will view this instance-blocking capability in the same category as unsafePerformIO - an evil made necessary by circumstance to enable Haskell to be used as a practical programming language. In this view, instance-blocking should be used only with the same fear and trepidation as unsafePerformIO. Or you can say that while in theory, a language with a HM type system extended by a Haskell-like class system could provide the additional static guarantee that you describe, in practice the Haskell type system itself sadly does not provide that guarantee, due to the existence of orphan instances. Programmers must manually verify that their programs are sound in the presence of orphan instances. Once we have given up on your static guarantee, we might as well use orphan instances as a convenient tool for separation of concerns and package dependency control, as the original SO poster has done. It seems that you would take the first view, and I believe that a majority of experienced Haskellers agree with you. But there also seem to be experts who take the second view. So I personally am not sure which side of this fence I am on. EDIT: Expanded a bit on the consequences of the second point of view.
Ditto. I'm put off by the too-cute-for-words obfuscation word-problem they're hiding behind. Obfuscation is the wrong impulse for organizers in this setting. I'm reminded of group beach houses on Fire Island, where flocks of five houses would form just to "walk" somewhere. I chatted up one organizer, describing to him this guy who built an ultralight plane to train geese to fly behind, eventually getting them to migrate with his help. Then I asked, "But why would you do this with geese, if you could do this with people?" It's a privilege to lead a game of Simon Says with such a strong and large gathering of programmers. One needs a Zen discipline to ridding one's mind of any impulse to obfuscate, or be cute.
I've started the work on the wiki "Web" page: http://haskell.org/haskellwiki/Web . First task: get a good format for this page. Second task: get it up-to-date. Third task: try to get rid of other unnecessary pages as much as possible.
I had the same reaction, but, on the bright side, it was a quick decision to make. At least they were up-front about that aspect of the challenge.
CMU's 2006 was crazy cool. Those guys really knew their games (all the way back to infocom and beyond). One major factor in their favor was that they knew how to design a long tail of scores. It was easy to score a few points and then slowly get hooked. Just like how a good game should be. Man, you just wanted to explore the darn (addictive!) thing. Yup, Utrecht's 2007 was a wannabe and so was Leipzig's 2010. This year was especially steep just to score any points whatsoever. 
From what I have read so far, it seems it was a fiasco i.e. a lot of people where put off by the exercise.
I don't really see that as obfuscation. There definitely were multiple stages you had to get through, and unlocking certain stages depended on solving other stages. But as I recall all of the puzzles were well-defined, or at least obvious.
Even after reading a few post-mortems (I understand the ternary encodingj, etc), I still do not quite understand the problem. 
I hated CMU's 2006; but 2007 and 2010 were even worse (by far).
Same here, a quick look at the spec told me I wasn't interested. Which in retrospect is a real shame, because termination is my thesis topic. But on the other hand I would never have gotten as far as reaching the core problem and figuring out it was a termination problem. Perhaps if they had thrown in an VM and some cute pictures, people wouldn't be so upset. 
I don't see any way to avoid orphan instances in practice. Consider the following simple example: There is a package providing 2- and 3-dimensional vectors. Then there is the OpenGL package. Clearly, the vector package shouldn't depend on the OpenGL package; on the other hand, its author wants to provide OpenGL support for the convenience of its users. It seems to me that only way to do that at the moment is the provide a third package, which gives instances of the OpenGL type classes for the data types defined in the vector package. 
very awesome!
Very excited for the Widgets support
Here's the short version: "Read our minds."
The problem itself was relatively clearly specified, and it is completely independent from the ternary encoding or the circuits. You got pairs of sequences like ABBCABCABBBACA and BCABAAABC, and you have to fill A, B and C with (nonnegative) matrices such that if you multiply them according to the sequences, the difference of the results will be nonnegative. Of course using the zero matrix is not allowed, that would be cheating.
&gt; Furthermore, Haskell projects currently face the risk that a critical commonly-used package may suddenly turn out to be unusable in the project, which is a totally unacceptable risk in any real-world project. Unusable is a bit of a stretch. If the project isn't a trivial one, keeping a fork of some package (and one as minor as a removal of an instance) is probably a much cheaper alternative.
Really enjoying the competition in the Haskell web framework space nowadays. Keep up the good work, guys!
 result = let a' = f0 in let b' = f1 a'' in let a'' = f2 b' a' in a'' 
Crash as in segfault, or crash as in exception thrown and program terminated? If it's the former then it's a bug in the port of ghc to the iphone.
It's not much better, but you could leave off some of the lets if you like: result = runIdentity $ do let a' = f0 b' = f1 a'' a'' = f2 b' a' return a''
I think the ultimate haskell web framework is going to be the one that is built on solid theoretical and safe foundations and yesod seems to be heading that way. Keep up the good work.
I would much prefer general recursions to be quarantined somehow. At first sight, I hated "letrec", but now I think it might be a good idea.
Why not: result = a'' where a' = f0 b' = f1 a' a'' = f2 b' a'
If the last definition is left off, GHC will complain immediately: result = let a' = f0 b' = f1 a'' in f2 b' a' Another option is to choose more meaningful names, so that the error becomes more obvious.
The where' clause, just as the 'let' in the blog post, introduces recursive bindings, so you can make the same mistake (write a'' in the b' = ... line).
Ah, thank you.
Unfortunately, you don't get the sequencing this way. GHC will compile this without any problem (with loop problem in runtime).
This is a nice one. I thought that the "let" has to be aligned with the "in". 
Good catch, my bad ;)
How much would this hurt performance? Not just because of the monad, but because the assignments are sequenced, and ghc can't do some of its optimisations as it would if it had free reign over evaluation order.
Hasn't zvon.org been out of date for about 8 years? or is it new?
Has to be said, however, that the ICFP contests are just brutally hard. Ideally, one should not get zero points, but it would also be sub-optimal if everyone were very close in ability to complete the problem. I do agree, however, that this year's obfuscation did not help things at all. I recognised the core problem that was being asked (the termination thing) but our team never got far enough to start in earnest to solve that problem. In the end, we just about knew enough about fuels to just spam solutions at the cars.
But then you lose the ability to put the definitions in any order you like, you'd always have to sort them topologically.
I have been bitten by the same bug as the OP and I think a forced order is better than a random/arbitrary order, even if it's a bit of a pain. Of course, a good Haskell IDE could take care of the order
I don't find arrow very useful, because of the existence of "arr". Is there any case arrows are useful for that couldn't map to the simpler Applicative class?
I'd go further and say this is the right one; the identity monad has nothing to do with the problem you're solving. It does have its uses, but this is not one.
I'd disagree on that, I frequently define variables in reverse order or mix them up completely because this grouping feels nicer to me. The problem with the number of ticks can be ameliorated by using digits instead of ticks. let a1 = g a0 a2 = g a1 b a3 = g a2 in ... That's what I usually do. Relatedly, I also intentionally shadow names in scope, like in case n of Just n -&gt; ... This way, I'm guaranteed that I always use the right one, because there is only one. For the original problem, this would mean to write result = let a = f0 in let b = f1 a in let a = f2 b a in a ... Hm, this only works in the identity monad, but you get the idea. 
Looks like the same old out of date index that pops up first whenever I google for something that was included in Haskell 98. ;)
By the way, Haskell could allow arbitrary orders while still disallowing [co-]recursion, unless you use "letrec".
This is an excellent screencast. I learned more in &lt; 10 min of watching this than I could have grok'd in 30 minutes of reading. And the code really does work!
I've adopted the convention of prefixing maybe values with mb like case mbN of Just n -&gt; ... which avoids shadowing and makes it unambiguous when I use a maybe.
&gt; How much would this hurt performance? None AFAIK. GHC is still free to reorder whatever it wants.
I'd make my own Identity type newType Id a = In { out :: a } then I'd write result = out$do let a' = f0 let b' = f1 a'' let a'' = f2 b' a' In a'' 
How can we kill it?
## **Did you mean: _unintelligible_**
I think it will be the one that is most Rails-like - providing an easy to use full stack solution for web development with minimal cruft, particularly an easy to use persistence layer. Being type-safe will be the motivation to use it instead of Rails.
It looks like it just had a visual redesign... Also, the "new" site doesn't seem to include the index of functions by type signature, which was the one part of the old zvon.org site that I found useful. It's still archived here: http://zvon.org/other/haskell/Outputglobal/index.html
You can still allow definitions in any order in a nonrecursive let, you just ban recursive definitions. I've done it, it works well. 
There's no sequencing. Using the identity monad is the same as using nested lets. 
Excellent work!
As I mentioned in the introduction my primary domain is XSLT and XML; I have not received any feedback for a long time so I have tried to solicit some feedback publishing here. I could not devote any time to Zvon for quite a few years but it has changed recently. I did not realized that Haskell changed so significantly as I judge from your comments; I will look into it more closely over weekend Mila
BTW: Would be someone so kind to submit a feedback at zvon.org@gmail.com what are the major "sins" ? :)
Oh, good to know it is up to date. You might want to engage more with the Haskell community, as things have changed significantly over the years.
Yep, I'm doing that too. Sometimes I like having exactly one "version" of the value in scope, though.
Ah, indeed. I'd be happy with that, I guess. :-)
Internship at MSR, ftw!
As I am using Haskell only sparingly nowadays I am afraid that I do not have enough time to be really engaged but I would like to keep the reference up to date (I am a regular user of it if I need some Haskell programing) - could you please give me some indication where I should concentrate my attention in my attempt to have it really current - where are the main problems with the old one? While with XML stuff I really redone the references with Haskell I took the old reference, put it into the new system and did some random checking but being out of touch I missed the problems this discussion brought to light. 
Very cool stuff. We'd spotted the performance headaches in Data.Graph pre/post-order traversals and talked about the redundant operations in balance on #haskell before, but everything else there was new to me. Are these changes being merged into containers? In particular, having a decently performing Data.Graph.topSort (which uses the aforementioned methods) is key to a number of my current projects to avoid having to turn to fgl or roll my own.
Hell yeah. Or open source doesn't work.
The main consideration is that your zvon.org reference indexes the modules defined by Haskell 98 which are, well, quite honestly not really used much any more and export outdated versions of the APIs that have since been scattered across the hierarchical namespaces. As for how the hierarchical module system is employed in practice, you might explore: http://www.haskell.org/ghc/docs/6.12.2/html/libraries/base-4.2.0.1/index.html which is the current base package, and as you can see differs considerably in layout from the backwards compatibility package: http://hackage.haskell.org/package/haskell98 which is largely documented on zvon. Then again, base is just part of what is now known as the haskell platform: http://hackage.haskell.org/platform/contents.html which contains a much wider array of stable APIs. You can think of the haskell-platform as Haskell with "batteries included". Documenting the portable bits of base in your current style might be useful, but you can see it'd probably be quite a bit more work and for the most part the documentation you've been providing up until now is provided automatically through haddock.
Heavy use of Criterion+Progression, nice :-D
This is great! From the conclusion: &gt; Even though it would be very unsatisfactory, part of the &gt; package could also we rewritten to C, which would definitely &gt; result in further improvements. Is it really 'definitely' true? *That* would be very unsatisfactory.
Thanks a lot. I will look into it (hopefully this weekend). If it proves to be to much work I will at least put a warning on the reference pages that it should be used with care and redirect to the http://www.haskell.org/ghc/docs/ My experience with students has shown me that there is some use for Zvon like references even in presence of an automatic doctool :) 
The [location of the patches](http://fox.ucw.cz/papers/containers/) when the eventually appear. (There's a placeholder at the moment.)
another solution to string problem: http://blog.moertel.com/articles/2006/10/18/a-type-based-solution-to-the-strings-problem Mixing these two and redoing Interpolique with type classes to support support XML, SQL and others may give a nice "secure string" library... oh and ByteString support would be great too...
Releasing the benchmark code to hackage, thus preserving it for future optimizations would add even more value?
I'd be interested to know which parts (if any) would benefit from being rewritten in C. Is this referring to the slow hash function implementation mentioned in the paper?
That's a feature I've been suggesting for years. Excellent work.
Awesome work! The performance of Hashsets vs. tries is quite surprising to me, but it sounds like I've got a new go-to datastructure :-)
&gt; The sad fact is that orphan instances do exist in Haskell, and always will; we are way too far down the road to stop it. Right. But I'm not against judicious uses of orphan instances, just against tolerating incoherent instances in programs. By judicious I pretty much mean "API end users only". Though I do think that even that is not essential, given `newtypes`, just very handy. I could actually even see keeping all the instances in separate modules, and having a global "we use these instances in this project module" that imports the correct ones, and is imported everywhere. Of course that screws up the dependency graph something horrible. &gt; As a result, encapsulation and composability are indeed very badly broken at the module level. That's like saying "encapsulation and composability are very broken at the function level" because you can't compose a function of type `a -&gt; b` with one of type `c -&gt; d`. I want to know when this sort of failure to compose happens rather than silently allowing it. Which doesn't mean "no orphan instances ever", but "fails to compile when incoherent instances are in play". &gt; Furthermore, Haskell projects currently face the risk that a critical commonly-used package may suddenly turn out to be unusable in the project, which is a totally unacceptable risk in any real-world project. Well currently, pretty much all Haskell libraries are distributed as source. And they pretty much have to be given the inlining and rapid ABI changes. If you can't remove an instance or switch to a `newtype` somewhere amongst the the various pieces of code, then that's a strong indication that there's enough leakage to be a problem. &gt; So there is no choice - the inability to block the import of instances must be viewed as a serious bug in any Haskell compiler. But blocking the import doesn't fix the problem, it just papers it over... Okay, yes, I can see where you're coming from -- the `unsafePerformIO` is a good bit rhetoric that helps. You want a way to tell the compiler "shut up, I know what I'm doing." I really don't think that's necessary, and prefer that people fix their broken code. I don't entirely buy the "separation of concerns" argument. Given my interest in build systems, the dependency management argument I am highly sympathetic to though. I should only care about `instance A B` if I use data type `B` and the methods of class `A`, and `A` shouldn't have to depend on `B`, nor `B` on `A`. EDIT: minor grammar issues.
I wonder what parts. Purely function things would need to be written in C--, so surely this means hashing algorithms. Detailed attention to the generated Core should remove this need, though, in my experience.
This sort of seems like treating compilation errors as compile time exceptions. Is this a totally horrible way of thinking about it or just a useless way of thinking about it. C++ compiler writers should take note.
Nice post and damned you; I guess I will have to read your master thesis.
Nice work, nice paper. I hope the author doesn't mind unsolicited advice, but here are some suggestions for the future: - The bar charts are often very confusing due to inconsistent colour/pattern and axis labelling. For example, from the bars for "Set: lookup" it looks like AVL is up to 5x faster. Then you look at the axis and the summary, however, it's not quite as dramatic. Similarly confusing is that the same implementation gets different colours in each benchmark. - You only tested with one version of GHC (6.12.2, 32bit). It would be interesting to compare with other versions. If the results are comparable that would at least deserve a mention. - I see no mention of memory usage. I suppose all these structures are comparable in memory usage, but that should be mentioned. I assume the reviewers had/will have similar comments. Benchmarking is certainly a delicate matter, but there are a number of simple rules that work. See also: http://evaluate2010.inf.usi.ch/ Anyway, nice work!
As the cofree comonad is spotted once more
Seconded on the memory usage. I'd be interested to see that especially for the tries. Edit: I'd also be curious about how the HashSet does under significant pressure -- say holding 3+ times (maxBound :: Int) elements. Of course with 64 bit ints, that doesn't seem like a real possibility/issue :-)
You can get part of the way there with only one version of the functions, but a type level either.
I wonder how this fixed point solution compares in terms of practical usability to passing functors to the Expr type: data ExprF f = Num Int | Add (f (ExprF f)) (f (ExprF f)) | Sub (f (ExprF f)) (f (ExprF f)) | Mul (f (ExprF f)) (f (ExprF f)) | Div (f (ExprF f)) (f (ExprF f)) type BareExpr = ExprF Identity type PosExpr = ExprF ((,) SrcSpan) While this is less generic, it does not require a newtype wrapper.
are there any papers on this gcminor?
Can someone explain to me why this is getting down votes? 
I've noticed very weird downvotes on the haskell subreddit in the past; it seems some people just enjoy that.
Well, it seems as though after a few votes are cast, Reddit adds a random quantity of upvotes and downvotes for whatever reason. Either that or they use an approximate algorithm, perhaps sacrificing consistency, that seems to vary quite a lot. Also, I think people have downvoted meetups, hackathons, etc advertised here, perhaps because they think it isn't a general enough interest for /r/haskell. If this hypothesis is true, I don't agree with it, because it's nice to see this kind of effort put forth. So personally, I upvote every one because I don't think they get enough upvotes. 
What I am not clear on is, why mix them? What are benefits of Interpoliq over Moertel's solution by itself?
why is this less generic? imho it's actually more generic. It allows you to annotate "heterogeneously typed trees" too, like this: data BoolExpr f = Eq (f (IntExpr f)) (f (IntExpr f)) | And (f (BoolExpr f)) (f (BoolExpr f)) data IntExpr f = Num (f Int) | Add (f (IntExpr f)) (f (IntExpr f)) | IfThenElse (f (BoolExpr f)) (f (IntExpr f)) (f (IntExpr f)) 
Interpolique is all about escaping and (as originally presented) not (explicitly) about types. It uses a highly neutral form of escaping (base64) which should work in many, many different contexts (whereas traditional escaping requires you to decide between "&amp;amp;amp;" and "\\&amp;" based on who you are escaping for). The Moertel article describes a way to use types to ensure that some form of escaping has taken place, but doesn't address the right way to do escaping. In some sense, the code I posted is a hybrid (read: I want to be clear that I'm not claiming to have anything original). It uses types to ensure that escaping has happened, while using the QQ extension to support the Interpolique syntax, and using Interpolique-style string escaping. Incidentally, yesterday I put up [some more code](http://github.com/intoverflow/InterpoliqueQQ/blob/introducing-taint/TestTaint.hs) along these lines. Inspired by the ST monad, it uses rank-2 types to prevent user-data from leaking out of a context before it has been scrubbed (ie escaped, Interpolique'd, whatever). The Interpolique code I posted doesn't yet have explicit support for that, but it is certainly forthcoming.
&gt; ghc can't do some of its optimisations as it would if it had free reign over evaluation order It still does. Monads specify *execution* order, not *evaluation* order. And what "execution" means is a property of the specific monad in question -- here, it means almost nothing.
For some reason I didn't even hear about the contest this year until after the fact. In past years it's been obvious to me that the contest is starting. Was it advertised differently this year?
We have a paper on it appearing in ICFP this year. If we decide to make it available before then, I'll post a link. -- Tim (Andrew's student)
They do this in order to prevent spammers, i don't know the details
Dante's Inferno from the inside out.
So you'r saying there are no side effects in Hell?
Not in Limbo, at least.
In math and physics, the tradition is to post at http://front.math.ucdavis.edu/ (ArXiv) where source is required. In CS the tradition is to obey editors who aren't dead yet, and post only a PDF in two column format. A perfect storm rendering the paper only readable on dead trees, not (for example) on a laptop. Alas.
I guess that makes Agda like the devil or something..
I am a bit surprised about the types chosen for HashSet and HashMap (section 5, p.9). Usually, hash-based containers are optimized for the case that the hash-buckets remain small. For that case, it's hard to imagine that the given types wouldn't be slower than the more straightforward types: data HashSet elem = HS (IntMap [elem]) data HashMap key val = HM (IntMap [(key, val)])
Dante's Coinferno?
Yeah, from the codivine medy.
 import Text.Printf nl = 10 :: Int q = 34 :: Int s = "import Text.Printf%cnl = 10 :: Int%cq = 34 :: Int%cs = %c%s%c%cmain = printf s nl nl nl q s q nl" main = printf s nl nl nl q s q nl
That's cool, by any chance was it a port from something like C?
Yes, exactly what it was. This was actually one of those fun/short questions for a computations course, someday I'll have to do a blog post on the fun parts of graduate course work (grrr, course work!). EDIT: A funny part about this quine was when I ported from C it wasn't a quine. I had to run the program a couple cycles, compiling the output, before my whitespace mistakes settled into a quine.
Haha, I wish I could write haskell for my coursework.
I'd really like to see this code as well as I plan to look into improving container performance further.
I had the same thought. One could save a little more space by defining a data type for lists of at least one element as the common case when there are no collisions would be `Singleton a` instead of `Cons a Nil`. The former saves one pointer per element.
The central class of functions (those expressible in Agda) should not IMHO be called "pure total functions", but "functions which can be proved total under Agda rules". That is a strict subset of total functions.
&gt; Soon I switched to using HTTP 4000’s form support. The form support is experimental, undocumented, but is pretty trivial and does work. Now my solver could upload solutions directly to the server. How does this compare to download-curl, which would be my first choice for these kinds of tasks?
&gt; ICFP **programming contest** 2010 Post-Mortem FTFY :-)
Once, I fed something like Berengal's quine into lambdabot on #haskell: yitz: &gt; text.ap(++)show$"text.ap(++)show$" lambdabot: text.ap(++)show$"text.ap(++)show$" Someone (sorry, I forget who) responded: someone: &gt; "" lambdabot: ""
and then you gained enlightenment?
I have written a spidering tool in Haskell. I initially used HTTP 4000, but the lack of SSL support became a dealbreaker. download-curl also did not meet my needs, so I ended up just piping in the results of running curl from a shell.
That's a really good point!
&gt; With ghc 6.12.1, -O2 -fasm: &gt; 1.752 &gt; With ghc HEAD (june 26), -O2 -fasm **-msse2 ** &gt; 1.708 My understanding from seeing that is that SSE2 support enables less than 3% improvement in running time, even in microbenchmark of extremely vectorizable code. Am I wrong? Where's the catch?
I'm pretty sure all `-msse2` does is tell the backend to generate SSE2 code for floating-point arithmetic rather than x87 code. There's no auto-vectorization going on; the generated code is still effectively scalar even if it uses the vector instruction set. The main benefit is that it bypasses the quirky stack-based x87 instruction set, which is notoriously difficult to target for code generators.
Exactly.
Makes sense now. Thanks.
Your picture is nice, but your (evidently quite successful) attempt to convince the OP and the world at large that everything is OK with unsafePerformIO and Haskell is disingenuous and you know it. unsafePerformIO isn't just unsafe, it is **unsound**. There is no justification for introducing unsound constructs anywhere in any language that aspires to be taken seriously, not even in the FFI. The fact that this has been done in Haskell counts as major language design FAIL in my book. 
what is so 'prolog' about mptc+fundeps? writing f x y instead of f x = y?
It's more that the evaluation proceeds like Prolog. "Execution" at the type-level is actually type unification, which roughly matches Prolog's concept of unification. Plus, Prolog is founded on the idea of a backtracking execution model, where you search for the existence of terms that match the call you made. Type-level programming with type-classes works in a similar way, searching for type-class instances that match the type-class constraint required.
there's no backtracking. that unification isn't so different than searching for the correct case in pattern matching, and I don't think I've seen it described as 'prolog'-like.
No comments on this? r/haskell surprises me here.
I stand corrected on the backtracking. On the matter of unification, I think it's because Haskell type inference can have unknowns on both sides in arbitrary places: "m Int" can be unified with "IO a" to make IO Int. In pattern-matching you can't express "I want to match a specific value inside a non-specific constructor" (which is the closest analogy I can make).
ok, that's true. but that's a limitation of haskell, and I don't think it would turn haskell into prolog. for me, prolog is about backtracking, and defining relations that can act as two-way (or more) functions (eg. parser+pretty printer inside one relation). I don't see any of that at the most obfuscated haskell type-level. 
You can go both ways in Haskell type-level programming, if you specify that in the fundeps (with three or more type parameters you can have all sorts of ways of getting outputs from inputs). E.g. class Swap a b | a -&gt; b, b -&gt; a instance Swap (a, b) (b, a) swap1 :: Swap (Int, String) a =&gt; a swap1 = undefined swap2 :: Swap a (Int, String) =&gt; a swap2 = undefined The fundeps mean you can go either way in the Swap relation; swap1 uses one direction while swap2 uses the other. GHCi confirms the type results of swap1 and swap2 are valid, and the same: *Main&gt; :t swap1 swap1 :: (String, Int) *Main&gt; :t swap2 swap2 :: (String, Int)
swap is trivial, what about append?
YAFE.
This while blog post is pretty much completely wrong
I think that tail recursion is almost always just a trick, simulating imperative programming to achieve better performance. to understand any tail recursive function, you're asking yourself questions like "when will it stop?" "how do parameter values change between calls?". these are the same questions you must answer when reading imperative code. and very often you make the same mistakes (e.g. off-by-one iteration count). the only good thing about it (in general, in haskell there's the benefit of not using IO monad), is being specific about what variables are actually modified and also it's clear that these are local side effects so they don't make the wrapper impure. Recursion is usually presented as a simpler concept than imperative programming, but the (simpler) examples aren't tail recursive, because it's not natural - it's just an optimization.
The DDC homepage is here: http://haskell.cs.yale.edu/haskellwiki/DDC. Seems like an interesting variant of Haskell. Summary of differences: * Strict Evaluation Order is the default, laziness is introduced explicitly. * Type directed Field Projections complement type classing. * All data objects support Destructive Update. * The Effect System tracks what computational effects are being used in a program, without the need for state monads. * The Class System ensures that effects and destructive update play nicely with laziness. * Closure Typing is used to track data sharing, and to preserve soundness in the presence of Polymorphic Update. (I wish it had a logo more on the lines of this: http://www.progarchives.com/progressive_rock_discography_covers/191/cover_3439917102008.jpg)
This looks like it's going to be an interesting series! I wonder what his project is about; what lies beyond MPTC+fundeps/typefamilies?
Is they're any chance we'll get direct access to SSE instructions so we can manually do 4-way adds, multiplies, etc... ? http://www.reddit.com/r/haskell_proposals/comments/7jyfn/library_to_provide_access_to_sse_instructions/
My sense is that this is about proper support for some of the ideas implemented here: http://personal.cis.strath.ac.uk/~conor/pub/she/
Exactly!
Speaking really hand-wavy here, but this should let me define a kind HBool inhabited by the types HTrue and HFalse, correct? At which point then I could write an effectively closed type function of, e.g., kind `* -&gt; * -&gt; HBool`? And with that in hand, we should be able to do HList with type families, finally? Because that would be great. 
Most of us have just been replying in the cafe. ;)
It would be a relatively straightforward summer project.
Not really a well formed question: the language here of type expressions is not itself typed (kinds only really have a syntactic role) and in particular there is no "metatype" of lists to append. Found the original article very helpful, but I can't see what it can be used for yet... need more examples!
Writing instances for append that meet the fundeps class Append xs ys zs | xs ys -&gt; zs, xs zs -&gt; ys, ys zs -&gt; xs is likely to be difficult, although I wouldn't rule it out completely. Prolog has numerous features that Haskell type classes do not---for instance, ordered alternatives, backtracking, cuts, negation-as-failure, etc. The correspondence between type classes and Prolog is at best a metaphor.
I think "like Prolog" refers to the fact that a multi-parameter type class defines a general relation, rather than a function. Heck, today I learnt why FDs are called *functional* dependencies!
Prolog-like refers, at best, to the style of programming people use. The notions of programming with relations is much more general. [Functional dependencies](http://web.cecs.pdx.edu/~maier/TheoryBook/MAIER/C04.pdf), for instance, originated in the database community.
Because, e.g., the Add typeclass can also be used for subtraction. This reversible computation is typical for Prolog. 
what about overlapping instances?
you're right. it is possible (e.g. Sum class from http://okmij.org/ftp/Haskell/PeanoArithm.lhs is almost as powerful as prolog one), but it's useless. to use it at the value level, there would have to be 3 methods, and defining them would be as hard as defining three different functions. so I still think, that type-level code has as much in common with prolog as erlang. which is syntax.
Well, having skimmed the thread, I'll say it isn't what I expected. I was expecting it to be about Fillinski's monadic reflection stuff, where you have a monad capable of expressing the effects of all other monads using its own effects (roughly speaking). The stuff in the link is actually about a family of monads (codensity).
Well, sure, but what's the alternative for today, given that Haskell is partially intended to do actual work? (I add the "for today" because everything I can come up with involves rather a lot of software that does not currently exist.)
&gt; There is no justification for introducing unsound constructs anywhere in any language that aspires to be taken seriously, not even in the FFI. So which languages should be taken seriously, and are any of them capable of developing real systems without violating your rule? 
From a type-level programming standpoint, overlapping instances are an ugly hack to enable some useful tricks, which are unfortunately necessary for a great deal of stuff one would like to do. Mostly it all ends up being isomorphic to writing the TypeCast and TypeEq classes found in HList, i.e., type equality assertion and predicate, respectively. Try playing around with those until you understand why they work and why you need them, and everything will make a lot more sense. Closed type functions would solve the problem in a more direct manner, meaning that ideally I'd never have to use overlapping instances again which would be *awesome*.
The lack of backtracking is why it's not a *good* logic language. Prolog is to GHC type programming as C is to brainfuck.
Thanks! You explained it better than I could. In fact for the basic stuff with HList, I think that TypeEq itself is enough, even without TypeCast, right? And I've found that `~` isn't as powerful as TypeCast, but it is in fact sufficient for at least some use cases. Edit: Just wanted to add that if we could implement an HList that didn't cause compile times to go nuts and produced somewhat more reasonable error messages, it would really cut down on the need for a better record system.
But not all uses are at the value level, often the type level is enough. In Bluespec we used type classes in exactly this way. There was Add, but no Sub. The Add and Sub classes had no methods.
Hard to say. I've make extensive use of both, not to mention that in super-happy-fundep-land[0] TypeEq is implemented using TypeCast. That said, the only uses for TypeCast I can recall at the moment fall into roughly three categories: * Asserting known equality in order to use typeCast on terms * Delaying unification of known parameters to avoid fundep conflicts * Forcibly unifying a parameter, as a mock-fundep for providing "output" from a parameter not determined by actual fundeps[1] I believe the first is handled by ~, and the others obviated by type families. There may be other things you can do with it, though, it's remarkably versatile! **[0]** Do not taunt happy fundep. **[1]** At one point I wrote a horrible pair of type classes where the parameters were indirectly determined by a complicated mutual relationship. To find an instance of the first when not enough type variables were officially known to resolve ambiguity for either, it would check the second, then based on the parameters determined by that use a helper type class, which chose the right set of relationships based on which parameters were known, used TypeCast to fill in the first class's parameters based on some conditional dependencies, satisfying the first class's fundeps, unifying the remaining variables, thus resolving ambiguity in order to find actual instances for both classes.
With plain cons-lists, though any two arguments determine the third it seems to unavoidably involve backtracking for one of the pairs. It's easy enough to get two {-# LANGUAGE UndecidableInstances #-} data Nil; data A a; data B a class Append a b c | a b -&gt; c, a c -&gt; b instance Append Nil a a instance (Append l r c) =&gt; Append (A l) r (A c) instance (Append l r c) =&gt; Append (B l) r (B c) Load with -fglasgow-exts (or extend the LANGUAGE pragma with the suggested features in the -X&lt;feature&gt; in error text), play around with things like (undefined :: (Append a b c) -&gt; a -&gt; c -&gt; b). That said, this is also the sort of thing you can do with normalization and higher-order unification in a dependently typed language like Agda, so I don't think any of this is especially strong evidence that "Prolog" is what we want the programmable part of inference to look like.
It it heresy to suggest that the Report include a gloss for "⊥" the first time it is used in section 1.3? Something like: &gt; ⊥ ("bottom") Or &gt; ⊥ (pronounced "bottom") Perhaps readers are expected to be familiar with top and bottom types before reading this, but I could imagine being frustrated every time "⊥" shows up in the text if I didn't know what the symbol was called. I know the Report isn't really meant for beginners, but I can't see what including a gloss would hurt.
&gt; but it's useless. to use it at the value level, there would have to be 3 methods, and defining them would be as hard as defining three different functions. Now why would you say that? class Peano n where num :: n instance Peano Z where num = Z instance (Peano n) =&gt; Peano (S n) where num = S num plus :: (Peano c, Peano t, Sum a b c) =&gt; a -&gt; b -&gt; (c, t) plus x y = (num, num) is :: (Peano t) =&gt; (c, t) -&gt; c -&gt; t is _ _ = num And we're off to GHCi... &gt; let x = (two `plus` two) `is` x in x S (S (S (S Z))) &gt; let x = (x `plus` one) `is` three in x S (S Z) &gt; let x = (two `plus` x) `is` one in x &lt;interactive&gt;:1:4: Couldn't match expected type `Z' against inferred type `S c' &gt; let x = (one `plus` x) `is` x in x &lt;interactive&gt;:1:0: Occurs check: cannot construct the infinite type: c = S c 
all of those can be solved with ~. the only problem I know, is that ~ doesn't play along with IncoherentInstances, it needs many explicite type-sigs, whereas typecasts don't.
&gt; And I've found that ~ isn't as powerful as TypeCast examples?
I do know how they work. but that's like saying that pattern-matching is an ugly hack to enable useful tricks, because you can always check with (==) if the list is empty. Oleg created them long time ago, because he was disappointed with hugs and its fundeps+overlapping instances status. and it's nice to say that you only really need two classes and then, everything is possible. (actually it should be 3, incoherent TypeEq is missing). but it doesn't change the fact, that programming with TypeEq leads to more code, more boilerplate and worse type sigs and type errors. and nobody uses hugs anymore.
I stand corrected. have a big upvote!
Yes, that's right. In fact, it will let you just take the normal data Bool = True | False and have it automatically lifted to the type/kind level.
Precisely, the language of type expressions is untyped, which is what we intend to fix. More in an upcoming post...
`unsafePerformIO` itself is unsound, but particular uses of it may very well be sound. That proof burden falls on the user of `unsafePerformIO` -- which seems like a reasonable tradeoff for the power it adds.
An eminently reasonable request.
Thanks for the honorable mention, good post.
PreludeList? Why not Prelude.List and put the things that are in neither [List,IO,String] into Prelude.Base and leave the module Prelude to be just a re-export of all of them?
In [13](http://www.haskell.org/~simonmar/haskell-2010-draft-report-2/haskellch31.html#x39-28000031).1.3, mallocForeignPtr, is it good practice to mention GHC-specifics in the standard? (I suspect that part was copied verbatin out of the base haddock)
Announcement is on the [haskell-prime](http://www.haskell.org/pipermail/haskell-prime/2010-June/003197.html) mailing list where some of the errors have been pointed out already.
&gt; Haskell is my go-to language, both for scripting, and for getting work done. This is not because of any particular allegiance to the language. Haskell and I have an open relationship, and the moment I find a language that out-Haskells Haskell, you can be sure I’ll move on. `intoverfl0w` seems to think this sets him apart from a mere Haskell addict or cultist or 'fanboy', but isn't this attitude written into the language itself (and the surrounding culture)? --It is aimed at something even greater than itself, something that is to come, etc. etc. -- and in the meantime at transferring bits of this better future into other existing languages, where possible. `intoverfl0w`'s attitude is, for example, the one adopted by Simon Peyton Jones in the "A Taste of Haskell" tutorials. Haskell and Haskelling are always partly the sake of the "language that out-Haskells Haskell". In other words, far from showing himself to be distinct from a mere Haskell addict, cultist or 'fanboy', `intoverfl0w` proves to be the most extreme and archetypal (and thus best!) form of one. 
&gt; Macros written in Template Haskell are actually written in Haskell syntax. The compiler then takes this code, compiles it like it would any other Haskell, and then uses it to expand your usage of the macros. Everything is typed the whole way through, **Objection!** The macro language is typed. The output of running the macro is type-checked afterwards. Macro expansion is *not* type-safe. It wasn't even *grammar*-safe last time I tried it. Type-checking the output mitigates the problem, but it remains that TH produces "run-time" type errors, where the run-time for a macro is considered to be compile-time for the client code. If everything were truly typed the whole way through, any TH functions accepted at the point of definition would be statically guaranteed to never create type errors when run. See also: [Typed metaprogramming languages](http://stackoverflow.com/questions/3037643/typed-metaprogramming-languages) on Stack Overflow, [TH vs. partial-evaluation metaprogramming](http://www.haskell.org/pipermail/haskell-cafe/2010-April/075753.html) on haskell-cafe.
&gt; This says that any data type a which is an instance of Show provides functions with these signatures. (The first and last of these functions are used for implementing a Haskell idiom for fast string construction.) No, the last one is for being able to show `[Char]`s differently from other `[a]`s, in Haskell 98.
I think he's referring to the fact that it returns a ShowS instead of a String.
He's confused by that fact. `showList` is strictly not needed for efficiency reasons. `showsPrec` is sufficient to reach that goal. And here is an excerpt from GHCs library docs: &gt; The method showList is provided to allow the programmer to give a specialised way of showing lists of values. For example, this is used by the predefined Show instance of the Char type, where values of type String should be shown in double quotes, rather than between square brackets. Also, try defining an instance of a simplified `Show` class (which doesn't have this method) for `[Char]` in Haskell 98. :)
I'm aware of all that, and I think it possible the author is as well. I saw it as slightly bending the truth to make things simpler to explain: imagine if instead he'd said: That first one is for efficiency reasons, and that third one is due to the fact that in Haskell 98 you can't have flexible instances. I think the reader would have been distracted. At times, it makes sense for an author to simplify the truth a little bit for the reader's sake.
For the reader's sake it's always better to never lie. If slightly or not. Maybe he should have left these sentences out altogether.
I've yet to reach the point of being impressed by quasiquoting. It certainly makes it easier to mix other languages into Haskell source files, but then these other languages are not Haskell. They have completely arbitrary syntax rules, do not benefit from many Haskell language features, and in general lose the property that they might be used in ways that weren't intended when they were designed... something that's always been one of the motivating factors for me of embedding domain-specific languages into a well-design higher-level language.
I'm for a layered approach -- an idiomatic combinator-style EDSL is great, but often it will still have rather clunky syntactic quirks as a result of mismatch between the target and host language. On top of that EDSL, you can implement a quasiquotation layer that lets folks write much prettier and more natural looking code. At that point, they can still drop down to the EDSL, and even use antiquotation to embed terms from the EDSL in the quoted syntax. 
I went ahead and updated the post after reading this thread.
There's something to be learned from Perl 6 here, I think. Perl 6 has a highly modular grammar, and in addition to being able to overload it, you can _specifically_ overload something based on a previous grammar, including all the ones the language ships with. Quasiquoting would be a lot easier if I didn't have to start more or less from scratch with the grammar. I'm pessimistic about it ever being useful for the "general public", but on the topic of the Haskell more Haskell than Haskell, something that takes the functional principle of composition right into the language grammar would certainly be a candidate.
Thanks! Another one to fix. Keep em coming.
It could be quite a big task I think - you'd have to define new primitive types, which means changes throughout the compiler. A summer project for a very keen hacker, perhaps.
This proposal has made me happier than any proposal on libraries@ in recent memory. Here is hoping that it survives the discussion period!
&gt; The Error constraint is &gt; removed from both instances, and the default definition of fail is used. This alone is worth the change -- then mtl and transformers can update their `ErrorT` types, and I can stop bundling fixed versions!
&gt; Of course, I still hate the ErrorT transformer, both for the super-class requirement and the orphan Monad instance. It's easy to implement your own `ErrorT` -- the actual definition doesn't require an `Error` class, or `Monad` instance for `Either`. Just copy the source file into your project, delete the offending constraints, and continue on.
This has the added benefit that the MTL would no longer being exporting orphan instances for types it doesn't own, and that with the mtl = transformers + monads-fd change coming down the pike that the non-transformer version of ErrorT could be a type alias for ErrorT e Identity, just like State, Reader, and Writer and writer will be. Sadly, due to lack of internal consistency, this would mean either renaming Error to something or giving the Error basic type alias a name that doesn't fit the otherwise global convention across mtl. 
&gt; Sadly, due to lack of internal consistency, this would mean either renaming Error to something or giving the Error basic type alias a name that doesn't fit the otherwise global convention across mtl. What `Error` type? The only thing named `Error` is a typeclass, which (with this change) will no longer be in use.
&gt; The Error constraint is removed from both instances FREEDOM!!!! Finally the shackles released!!!!
`Error` will remain a type class that is a prerequisite for the `ErrorT` monad instance. The non-transformer version of that would be the type `Error`, but types and type classes share a name space, so you can't have both an `Error` class and an `Error` type.
&gt; Error will remain a type class that is a prerequisite for the ErrorT monad instance. What? Why? What possible purpose does this serve? `fail` is a mistake -- just give up on it, and let us use whatever type we want.
Because having a monad that represents actual failure, with potential status information (unlike `Maybe`) isn't a bad thing. It lets you have, for instance, a `MonadPlus` instance for when you want 'just fail', and allows `fail` to do something useful, which _is a good thing_, if the monad in question has such operations, and `ErrorT` can be one such monad. And it potentially lets you write utility functions that can be polymorphic over the error type, knowing only that strings can be lifted into them. Having an `EitherT` that isn't restricted wouldn't be a bad thing, either, because the more general `Either` monad isn't necessarily about errors.
[neither](http://hackage.haskell.org/package/neither), as I say in the next paragraph.
I'm able to accept that I might be wrong, but I'm not sure I understand the problem (but I want to!). If I have some TH like let x_0 = mkName "x" in return $ LamE [VarP x_0] (TupE [VarE x_0,LitE (StringL "foo")]) this will reify to \x -&gt; (x, "foo") What are some examples of what can go wrong? Certainly I can't mess up and use a VarE instead of a VarP, but I'm guessing you have something more important in mind. **Edit** is it that it's possible for TH to output something that (in the 2nd phase) doesn't type check? This is certainly true. If that's the problem you're getting at, then I need to reword my article a bit, since I wasn't *intending* to convey that the opposite is true. I just wanted to point out that TH is much safer to work with than string replacement.
I'd love to see this presentation given again once he learns a bit more about the type system and (hopefully) recognizes the good decision that using bytestrings as opposed to strings was. Type system aside though, have I lead some sort of weirdly sheltered bytestring life where pack and unpack never fail me?
"I'd love to see this presentation given again once he learns a bit more about the type system and (hopefully) recognizes the good decision that using bytestrings as opposed to strings was." So would I. :) I understand the advantages of using bytestrings, but compared to using String in Ruby, which Just Works for nearly all cases, having to deal with bytestrings to do even simple operations feels wrong. I suppose what I want is either some undercover magic going on so allow me (or, say, the average Rubyist moving into Haskell) to write basic code without regard for certain efficiencies, or some way of explicitly saying, "For this code I don't care about the overhead of plain strings." For the use cases I see for Hubris, at least in Web apps, the cost of dealing with plain strings being passed over the bridge is negligible compared to the gains brought by whatever Haskell code is ultimately being called. But given my early-stage understanding of Haskell, I might be completely missing something, and I also expect that bytestring aversion is dependent on habit and experience. 
What actually happened with bytestrings? Was it a package issue? It should be an almost identical interface. Btw, nice talk, and I'm extra motivated to keep working on it so nobody but me has to apologise for its shortcomings.
That's really cool! Mutually recursive datatypes without too much trouble; the other solution doesn't allow that (at least not in its current form). I don't think it's more generic, it's just differently generic. Instead of annotating every occurrence of Expr, you annotate every Expr field. Notice how the Int field in Num suddenly has an annotation; before Num as a whole had an annotation. 
Which among many other things will finally let me implement apomorphisms the way they should be. ;)
Coming soon: the rest of Traversable, some real examples, and some common speculation patterns.
Coming soon: define accumulate in terms of speculate :-)
What does the timeline look like if `f g` takes longer to evaluate than `a`, and `g /= a`? This is what would be ideal, I think (`#` means the computation is cut short): [------- f g ----# [----- a -----] [---- f a ----] [------- spec g f a -----------] Edit: Looks like this would happen using GHC HEAD, at least if the GC catches it.
Yes, your edit is precisely what I meant. TH is certainly an improvement over things like string substitution with the C preprocessor, or [optimizing assembler using regular expressions](http://hackage.haskell.org/trac/ghc/wiki/Commentary/EvilMangler), but it's not "typed macro metaprogramming" taken to logical conclusion, either. TH can produce output that is not well-formed (resulting in "that's impossible!" errors from GHC) or doesn't type check. Fully typed metaprogramming, on the other hand, blurs the distinction between macro and source, turning macro expansion into something closer to an elaborate sort of dependently-typed partial evaluation performed at compile-time. The most prominent example of the latter, at least that I'm aware of, is MetaOCaml. Note that this is in contrast to the more common OCaml metaprogramming system camlp4, which rebuilds ASTs and is loosely comparable to TH.
whatomorphisms?
Yep. Based on what happens in GHC head, the spark won't act as a root, so once the reference to f g goes away, it'll be collected during the next gc cycle. Sadly specSTM doesn't get this benefit since it has to run 'a' in the background, but I started working with simonmar to see if we can get a primop added to check the size of the spark queue to approximate the right behavior.
I think the issue was that any method exposed via the bridge required byte-strings. I wanted to then pass on the given values to some existing Haskell code that did some calculations (specifically, code from the Lisperati Haskell tutorial). But all of those functions expected and returned plain Strings. I think I tried doing some conversion, and got stuck, and gave up. I'll have to go back and try things again and see if anything I've learned in the last few months has opened my eyes, or if any recent changes to Hubris makes a difference.
I'm actually editing the docs now to reflect this. ;) Sadly, with STM this doesn't happen. =/
I know the debian-haskell team is doing an awesome job, so kudos to them. But as a relative noob to haskell, I find it frustrating to navigate the installation of packages and versions between debian and hackage. And then whether to install locally (no biggie on personal machine, but others?) or globally through cabal-install. Makes me feel lost and confused :( Ideally, everything would come through deb proper so that the automated tools can keep me up-to-date, but I keep hitting roadblocks. Mind you I'm not complaining! Just reporting my experience as someone learning. .02
One bumming thing, though is that for example: withFile :: FilePath -&gt; IOMode -&gt; (Handle -&gt; IO r) -&gt; IO r Doesn't let you use monad transformers around IO :-(
http://hackage.haskell.org/package/EitherT
Well, you can do that with a similar strategy: newtype Fix2 f g = In { out :: f (Fix2 f g) (Fix2 g f) } data Ann2 x f a b = Ann2 x (f a b) data BoolExprF b i = Eq i i | And b b data IntExprF i b = Num Int | Add i i | IfThenElse b i i type BareBoolExpr = Fix2 BoolExprF IntExprF type BareIntExpr = Fix2 IntExprF BoolExprF type PosBoolExpr = Fix2 (Ann2 SrcSpan BoolExprF) (Ann2 SrcSpan IntExprF) type PosIntExpr = Fix2 (Ann2 SrcSpan IntExprF) (Ann2 SrcSpan BoolExprF) And of course, you can generalize that to as many mutually recursive types as you need. However, one nice thing about twanvi and yairchu's approach is that you can annotate with an arbitrary functor. On the other hand, that can be achieved by composition of functors as well.
The Haskell string situation is a bit awkward at the moment. I ran into the same sorts of issues when writing my CPython/Haskell bridge, so this post contains some bitter tears of experience ;) First, Haskell's `String` is implemented as a linked list of UTF32 codepoints. This is incredibly inefficient, obviously, but in mathematical terms it's easy to reason about so it was chosen for lack of any better option. The type `String` is actually just an alias for `[Char]`. Haskell's `ByteString` is semantically `[Word8]`, but implemented using an array rather than a linked list. It's much faster and more memory-efficient than `String`, but can't represent non-ASCII characters. There's a third Haskell type, `Text`, which is semantically `[Char]` (like `String`) but is implemented using arrays (like `ByteString`). This is the preferred replacement for `String`, since it's more efficient and can still represent every Unicode character. Ruby's `String` class is actually equivalent to the Haskell `ByteString` type, not `String`/`Text`. It stores bytes, which might be interpreted according to UTF8, UTF16, ShiftJIS, or many other encodings. Therefore, the Hubris developers really have no choice but to use `ByteString` to represent Ruby's `String`. Now, how do all those words help you? The `Text` package also contains functions for interoperating with `ByteString` safely, such as `encodeUtf8` and `decodeUtf8`. You can use these functions to construct Ruby strings using UTF8, which ought to work well: -- The Hubris docs haven't rendered on Hackage, so I don't know -- the type signature to use here. You know what I mean, though. bytesToRuby :: ByteString -&gt; IO RubyString -- Here, we'll assume we want a Ruby string containing UTF8 textToRuby :: Text -&gt; IO RubyString textToRuby = bytesToRuby . encodeUtf8 -- You can also create a function for Haskell's 'String', if you want to work -- with older tutorials which haven't been updated to use 'Text' yet. stringToRuby :: String -&gt; IO RubyString stringToRuby = textToRuby . Data.Text.pack -- And you'll want to define some functions to go the other way around rubyToBytes :: RubyString -&gt; IO ByteString rubyToText :: RubyString -&gt; IO Text rubyToText = fmap decodeUtf8 rubyToBytes rubyToString :: RubyString -&gt; IO String rubyToString = fmap Data.Text.unpack rubyToText
I'm a member of the debian-haskell team. Thanks for the thanks :-). Personally I have found that installing some packages via apt-get/apptitue and some via cabal resulted in all sorts of weird problems. I've basically given up on using cabal on debian systems. If there is a haskell package you'd really like to see in debian, you should post a request to the debian-haskell maintainers mailing list. 
Thanks very much for this. I need to go back to my sample code and see what I can re-work.
in the context of Hubris, you'd need an instance of Rubyable for String - you can read that as "you can bring a Haskell String across to Ruby". The problem with that is that String is just a type synonym for [Char]. We have a Rubyable instance for Char already, and an instance Rubyable a =&gt; Rubyable [a] (which uses an array of VALUE in Rubyland). This means that given a [Char], Hubris expects to bring it back to Ruby as an array of characters, rather than a Ruby string, which is a bit of a pain. Haskell gurus: is it possible to declare a different instance for a type synonym? I suspect it isn't, but would be glad to be proven wrong. (you don't get the problem going the other way, I think - Hubris knows the types on the Haskell side, so should be happy to marshal a ruby String into either a list of chars or a bytestring, depending on what the Haskell function needs.) 
Given that people want to make type-level computations in a Haskell-like language, why don't we build a language with actual type-level computations that is somewhat compatible with Haskell? I can't believe it would take any more effort to build a sane system to do this stuff, then the effort that already goes into working with the Turing tarpit that GHC's typechecker has become. When you're writing something like "add x y", and it doesn't look like "x + y", you must be doing something wrong.
&gt; This means that given a [Char], Hubris expects to bring it back to Ruby as an array of characters, rather than a Ruby string, which is a bit of a pain. Haskell gurus: is it possible to declare a different instance for a type synonym? I suspect it isn't, but would be glad to be proven wrong. You can declare instances for `String` by enabling the `TypeSynonymInstances` language extension, but then you won't be able to instance `Rubyable [a]`. Honestly, if I were you I'd just `instance Rubyable Text` and leave `String` to rot.
an apomorphism apo :: Functor f =&gt; (a -&gt; f (Either (Mu f) a)) -&gt; a -&gt; Mu f is a variation on an anamorphism (unfold) ana :: Functor f =&gt; (a -&gt; f a) -&gt; a -&gt; Fix f where you can also return the entire remaining tail. Which, can be implemented in and of itself just fine, but Either is a monad, in which case. apo becomes just a special case of a generalized anamorphism using the Either monad and its distributive law. g_ana :: (Functor f, Monad m) =&gt; (forall c. m (f c) -&gt; f (m c)) -&gt; (a -&gt; f (m a) -&gt; a -&gt; Fix f This is particularly satisfying, because it works precisely dual to how paramorphisms work with the product comonad. See category-extras: Control.Morphism.Ana and Control.Morphism.Apo
Wow, that's one of the wildest uses of quasi-quotation I've seen. Awesome!
Related : [Multi-Dimensional Analog Literals](http://www.xs4all.nl/~weegen/eelis/analogliterals.xhtml). It is worth noting that C++ does not have a quasiquotation system...
I... I'm speechless. That is insane.
That's one of the scariest and most awesome things I've ever seen. I'm afraid to say it, but you think they could do 4 dimensions? ;)
post this to proggit, before dons steals karma, that you deserve!
This solution is much less elegant because it requires an extra type parameter for each datatype in your family. 
Wow. As a kernel developer... I want this.
Hey Jeremy, it's been a while, but remembering this thread, I have now made a ["not a book" explanation of fixpoints](http://apfelmus.nfshost.com/blog/2010/07/02-fixed-points-video.html). What do you think?
dons, don't be stealing!!!
I vaguely remember a research project where a single person aimed to create a kernel from scratch by defining a tower of domain specific languages on the fly, including such diagrams. I don't remember the URL, though. :-(
I remember something similar -- the neat bit was that the diagrams it read were precisely those in an IEEE (?) spec, so some portions were nearly correct by definition.
Maybe you're talking about the STEP project, investigated by - among others - Alan Kay. One of their goal is to create an OS from bare metal to a user-friendly UI in less than 20K LOC. See: http://www.vpri.org/pdf/tr2007008_steps.pdf. The implementation of TCP is indeed done in a "diagram-style". See this blog post: http://www.moserware.com/2008/04/towards-moores-law-software-part-3-of-3.html. Also created for the STEP project is OMeta, http://tinlizzie.org/ometa/ , which allows for ultra-expressive writing of parsers. Jeff Moser talks about it (OMeta#, an implementation of OMeta in C#) [here](http://www.moserware.com/2008/06/ometa-who-what-when-where-why.html). Look at the code screenshots, it's very impressive.
That's certainly the one I remembered. Thanks!
After reading that stuff (a while ago), I would disagree with the article's assertion that "this is a gimmick". Admittedly, making a diagram where none previously existed is a gimmick, but to the extent that you can take the actual specification, copy it into your code, and have it run, that's not a gimmick; that's in some sense the way it should be done. In the future we'll consider specifications that make this impossible or even hard to be a bug in the spec.
as always.
I feel I should probably comment on this, even though it's a bit off-topic. I'm not particularly invested in my (reddit, ycombinator, mathoverflow, etc) karma. If someone else posts a link to my blog, they're ultimately increasing my readership. I *am* invested in that type of karma. When I write about anything remotely related to Haskell I always come to /r/haskell to publicize it. I've found this to be incredibly effective in two distinct ways: * The /r/haskell reddit can generate a couple hundred hits in a couple hours, which (as an author) is a very satisfying thing. (This isn't an advertising thing: Wordpress sometimes injects ads into my blog, but I don't make money on that.) * Someone else will post it to other sub-reddits and link-sharing sites for me, which is actually pretty convenient. * If I have any technical errors in the post, someone here will catch it and point it out to me in a way which is both professional and useful. It can therefore be very handy to post something up here before posting it everywhere, as this initial feedback serves as a chance to clean things up before more eyes see the post. 
Thanks for the links! I went ahead and updated the article at Potential to link off to this as well. Way cool!
That's the very one, thank you so much!
I'm actually really fond of the proposal that fail s = Left (error s) but it doesn't seem to be gaining traction. It is just sufficient enough that the do sugar could be used in bindings. (It even gives a path for MonadError Exception to be defined for Either, with a little bit of impurity), meeting (for the most part) the needs both camps.
Understanding the history of MVC helps a lot. From what I can see, MVC very specifically came from GUI programs that have multiple views on the same fundamental object on the screen simultaneously. Think a CAD program viewing a model from two angles on the screen at once, which allows you to use either to edit it. There, the split makes sense. It really doesn't travel out of that context very well. MVC is not a universal pattern. It is a particular application of separation-of-concerns in a particular GUI context. The further you stray from that context, the less the original formulation makes sense. Why isn't there a clear place to put parameter processing in a web application built around MVC? Because actually examining the "MVC" breakdown of your web app will reveal that the controller is actually smeared between the server and the client if you do the "right thing" with Javascript validation on the client and double-checks on the server. MVC in a web context is actually a stupid way to conceptualize your web app because MVC has no room to accommodate the client/server split. Of course it doesn't! It came from desktop applications! Of course, MVC got itself enshrined as something beyond even a "design pattern"; it got itself enshrined as _the embodiment of good design_. Therefore, we have hordes of people trying to figure out how to jam their design into the MVC template, and thereby prove that their design is therefore good design. Stop that. MVC !=== good design. It is _a_ good design, a special case of the general good design principle of "separation of concerns" and "don't repeat yourself". MVC is when you have _a_ model, a set of views, and a controller. There ought to be a class or interface or other relevant local design concept that you can point to and say, "_This_ (class/module/interface) is my model, _that_ is my controller set, _that_ is my view set", very quickly. If you're drawing strange lines around your code to declare what the controller is, what the view is, etc, stop. Components of your design are what you naturally draws the lines around, those local language-specific base components of design, not lines that cut through 25 different source files pulling this line and that paragraph but not this thing here to create the "Controller". Functional programming doesn't do MVC automatically. If you need MVC for your particular app, use it, but don't try to use MVC to describe functional programming in general. Instead of trying to convince people about the value of functional programming by showing how it can be jammed into the MVC with a C'thulu-summoning line drawn around the various bits of your source code, tell people about how much easier it is to separate concerns and not repeat yourself in functional code. If you happen to end up with MVC in one case, great, but it's not a goal, it's just a thing that could happen. (The value of the MVC pattern is itself enhanced when we stop treating it as a sheet of rubber to be stretched over whatever design we happen to have handy, regardless of how it rips or how it stresses the underlying design, and limit it back down to its original purpose, where it actually makes sense. Then we can get back to talking about the real point, which is _separation of concerns_, rather than getting tied up in arguing about how MVC something is. This is a losing proposition for functional programming anyhow, you cannot and will not out-MVC OO; again, that's where it _came_ from.)
Cool thanks. I do lurk there. I'll make a point to request in the future. Maybe I could even help out!!
Also note that this is the default for x86-64 targets.
FWIW, the similarities between MVC separation and purity via functional data structures, with effects kept separate, was a strong motivator in the design of xmonad.
The karma ... it does nothing!!
I see you submitted to proggit, but it got caught in the spam filter. Let me know next time.
Thanks for the fix! I would have said something, but I didn't actually notice :/
But we have the famous hnop!!! http://hackage.haskell.org/package/hnop Mon Nov 14 05:34:23 UTC 2006
Admittedly I'd forgotten about hnop, but as Duncan pointed out my version is a bit friendlier :)
Reluctantly upvoted for token nod to my native language.
Of what use is this new function?
No Haddock documentation?
My one concern is that the middle of each result is always the same as the input -- so why give it? -- and that this is often less useful than a split combinator because the results can't be put back together. with intersperse, etc.
About the upcoming 0.3 version: &gt; the new version features a “development mode” which runs your web handlers using hint. This will allow changes in your Haskell code to be reflected instantly in the server output, without the overhead of a shutdown/startup cycle on the server. Oh my, I'm so excited!
I'm a little concerned about the "All Rights Reserved" license. It would appear that all of my Hackage packages are in fact derivative works of hnop; might I be infringing copyright?
Cool! Does anyone know if this is possible with vim?
 permutations [] = [[]] permutations xs = [ y:ys | (l, y, r) &lt;- separate xs, ys &lt;- permutations (l ++ r) ]
All hail Snap
% cabal unpack hnop Downloading hnop-0.1... Unpacking to hnop-0.1/ % cat hnop-0.1/LICENSE HNOP is written by Ashley Yakeley. Permission is hereby granted, free of charge, to any person obtaining this work (the "Work"), to deal in the Work without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Work, and to permit persons to whom the Work is furnished to do so. THE WORK IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE WORK OR THE USE OR OTHER DEALINGS IN THE WORK. ...there's a "License: OtherLicense" field missing in the .cabal. Not the best package to copy from, but then we've got cabal init.
I wish I had more than just one upvote to give this post.
Basic setup for automated tests using HUnit and/or QuickCheck would be nice, too.
Wow, that's a relief. Here I thought the RIAA was going to sue me for more than the national debt.
&gt; Heist is an XML templating engine based loosely on ideas from the Lift Web Framework. No! [Hamlet](http://hackage.haskell.org/package/hamlet-0.3.1.1) please!
I tried using Snap with BlazeHtml instead of Heist and met no resistance, so that shouldn't be a problem.
Alson Kemp had [similar insights](http://www.reddit.com/r/haskell/comments/bfcgi/how_do_you_maintain_typesafety_in_web_applications/) a few months ago about how MVC/functional is not exactly a match made in heaven.
Is something wrong with Hamlet/Blaze ? Edit: Ahh, misread it :-(
Will the 0.3 still stay in the low(er)-level, i.e. no higher-level stuff?
Actually I think MVC is a pretty bad example of separation of concerns. True, it separates out two concerns to some degree, model and view, but controller is nothing but a catchall for the rest you didn't care to separate properly and model really mixes the two concerns of having data types to hold your data in memory and serializing them to a database or disk.
For this find function the middle is the same, but for example with a regular expression matcher or even a find-character-matching-predicate function the match can differ. It is good to have a consistent API, so this find function should also return a triple.
The Show class in Haskell faces a similar problem - we want an instance Show a =&gt; Show [a], which would print brackets and commas, but we want to be able to show [Char]'s with quotation marks and escaping. We get around it by having _two_ functions on the class interface, 'show' and 'showList', where 'showList' has a default implementation involving brackets and commas. The instance "Show [a]" simply has 'show' call 'showList', and then the instance "Show Char" has a non-default implementation for 'showList'. 
The first comment on the OP scares the crap out of me. His function could be completely optimal and still have the code be dog slow because of how he constructed *what he's passing into the function*. For all its advantages lazy evaluation totally destroys local reasoning, it's never been that clear to me before.
The fact that execution jumps around at runtime doesn't affect your ability to reason about performance locally. Laziness defers some of the execution, such that part of the code may or may not run depending on the demand, but the profiler will still attribute the cost to the lexical call stack, not the dynamic call stack. 
make an original puzzle game (then I will enjoy playing it) if you get cabal install you can install a bunch of libraries like GLOSS has some examples of 2D animation with GLUT you can poke at.
If you are really a beginner, I would suggest starting out with a project or two that are a little less ambitious. Asteroids and shooter games introduce too many language-independent side complexities that will distract you from really getting the hang of what Haskell is about. If you are learning Haskell as somewhat more than a beginner, then you probably want to use SDL. I think that is likely to get you the best quality per time spent ratio. It will take you more towards the imperative side of Haskell, though. If you are learning Haskell as an expert - Haskell is something you never stop learning - then try to push the envelope by using FRP. Our understanding of using FRP in practice for complex UI interaction is slowly but steadily increasing. Document your experience in detail, and we'll all be wiser.
I'm using the various SDL libraries, and they are simple to use, and work well. It's basically the same as the normal SDL in C/C++, so you can just use the normal documentation on it. The only differences I have seen thusfar is that instead of flags, you pass lists of the flags, and the events can be pattern matched against.
+1, I was trying to make a rougelike in Haskell before, and it was hard enough.
lol, simple event loop
One small piece of advice I'll offer: If you want to experiment with writing a complete game in a specifically *functional* style, avoid FFI-binding libraries that control the event loop for you and are designed around callbacks. While this style makes for very simple, clean code in a standard imperative language, in Haskell it fractures your program into a bunch of little pieces that can only communicate with each other via the IO monad. Now, Haskell makes a fine pseudo-imperative language if that's your thing, but if you want to try out something like FRP that sort of library will really ruin your day. The downside, of course, is that if the library lets you write your own event loop, that means you'll have to... write your own event loop, which can be a pain for anything non-trivial. Oh well. Off the top of my head, SDL is do-it-yourself style, whereas if memory serves me GLUT uses callbacks exclusively. Personally, if I were going to write an Asteroids clone in Haskell in functional style, I'd use SDL for input/sound/etc., hand-roll an event loop, use SDL to create an OpenGL context, then ignore SDL's video functions in favor of using [Graphics.DrawingCombinators](http://hackage.haskell.org/package/graphics-drawingcombinators).
What is it exactly you are trying to say? That you really must use hamlet but for some reason not yesod?
I think maybe I'm misunderstanding your terms. I'd expect the dynamic call stack to be what changes in response to demand, and the lexical call stack to be what most programmers think of as a normal backtrace. You're saying that the profiler will blame the cost on the lexical stack, rather than the dynamic stack -- which means your function may look really expensive when it's actually the fault of how what was passed in was constructed. Thus local reasoning about performance is destroyed... I don't understand how you come to the opposite conclusion.
Maybe [this](http://www.reddit.com/r/haskell/comments/bqaig/getting_started_with_sdl_in_haskell_sdl_game/) can help. But [basics](http://learnyouahaskell.com/chapters) first! I suggest at least finishing some simple exercies first in order to get the feeling for regular haskell. But eventually you will need some understanding of [monads](http://en.wikibooks.org/wiki/Haskell/Understanding_monads) in order to do a game project.
You got that exactly backwards. If function foo is expensive, then it will get attributed at such, it won't get attributed to bar just because you call bar (foo x). The evaluation of the expensive argument doesn't get attributed to "bar" just because bar is the thing that happens to trigger it.
&gt; rougelike How hard could that be? Just use lots and lots of red.
I don't know about frp, but SDL can be easily used. There's an audio library for it too.
It occurs to me that we should have a tool that searches for good GC flags, exploring the search space, graphing the results, and making a recommendation to the user.
Lazy evaluation gives you nice compositional properties of program semantics, but not so nice for resource consumption. With strict evaluation it's the other way around. There's no way to get both that I know of.
Correct, we'll be starting in on that stuff soon.
just creamed my pants
You'd have really thought I would have learned how to spell that by now...
What type of project would help one learn haskell ? When I came into Haskell I was pointed at the Euler project. When I started solving those, I found that I was (re-)learning high-school math more than actually learning Haskell. I/O based projects are supposedly not good for learning FP languages (especially Haskell). When I started looking into writing small OpenGL programs in Haskell, I found that GLUT uses a lot of state and it looks almost like the imperative program written in Haskell. I am currently writing bits and pieces of code I use in my everyday work in Haskell, I am learning some things but I have no idea if I am going in the right direction. Because mostly what I need are mainly I/O based programs (parse a text file, extract values using reg-exes and print them out etc.,). The answer to this question would be nice to know. 
Yes! Such a thing might give useful insights that could be used to improve the GC's own heuristics too.
If you're going down this road of doing a game then I'd recommend using a state monad otherwise you're going to write repetitive boilerplate code. It's painful if you're doing any complex state manipulation in this way I find that state monads can make your code a lot more clearer and back to elegant code. State monads are still purely functional, they abstract what your doing explicitly without them. If you use monad transformers you can get away from this god World record segregate you "global" mutable state into one record, another for read-only or config data into another, etc. [I have a little experience of this](http://github.com/snkkid)
I made (am currently still making) a ray tracer to learn Haskell. It's suited really well to FP. Also you can start small (like a full ray tracer in 100 loc) and scale up (in complexity) almost indefinitely.
Learning from other games from [hackage](http://hackage.haskell.org/packages/archive/pkg-list.html#cat:game) would be good start as well. Find a game, read the code, tweak it here and there
That sounds pretty cool. Do you know of any good articles or tutorials about how ray tracers work?
I first started with [this tutorial](http://www.codermind.com/articles/Raytracer-in-C++-Introduction-What-is-ray-tracing.html). That said, I didn't follow it religiously, or even for very long. I was constantly googling things and have probably used resources from all over the web. Also check out the [ompf forums](http://ompf.org/forum/). A good (and very detailed) book is [Physically Based Rendering](http://www.pbrt.org/), the 2nd edition of which is to be published soon. Just about all the references you're going to find on the web have code in C/C++, so if you want to make a ray tracer in Haskell you're going to have to translate. What I did was focus on the concepts rather than the code, and then implemented that in Haskell. You can check out HRay or Glome which are Haskell ray tracers on Hackage. If you're feeling brave you can check out what I've done [here](http://github.com/kearnh/HToyRayTracer).
Don't beat yourself up. _Gary Gygax_ wrote a whole series of books that managed to get it wrong...
I recommend using OpenGL+GLUT for graphics. Sound is not that easy, maybe you should postpone it. Though if you only want simple sound effects, OpenAL should work. And you can always seek sound-related advice on the Haskell-Art mailing list. My personal opinion about purity is that you shouldn't be afraid of IO and other "dirty" stuff. Finally, FRP is a wonderful thing, but I would leave that for a second project :)
I recommend ST arrays. You get an immutable, pure interface on the outside and can still mutate in-place. 
StorableArrays See also: * [Data.Array.Storable](http://www.haskell.org/ghc/docs/6.12.2/html/libraries/array-0.3.0.0/Data-Array-Storable.html) * [Modern Array Libraries](http://www.haskell.org/haskellwiki/Modern_array_libraries)
How do ST arrays compare with the arrays in the [vector](http://hackage.haskell.org/package/vector) package? From a casual review, they seem to have similar capabilities.
Those fuse, and actually are ST arrays iirc.
First cut. http://i.imgur.com/m2yKr.png Should have the prototype finished today. Done: http://hackage.haskell.org/package/ghc-gc-tune And written up: http://donsbot.wordpress.com/2010/07/05/ghc-gc-tune-tuning-haskell-gc-settings-for-fun-and-profit/
There is [Data.Bitmap](http://hackage.haskell.org/package/bitmap), which is intended to solve the image problem; but be warned that it's quite experimental at the moment, and the API will change in the future.
lovely? I thought the consensus was that the gameplay sucked.
*[citation needed]* Anyway, I'm referring to the overall package.
waves hand in air, oh back when it was last linked. ok I agree, it's good to see more stuff on hackage.
I concur.
And look [here](http://github.com/luqui/graphics-drawingcombinators/raw/master/example.hs) for an example of how to use SDL to create an OpenGL context and then use Graphics.DrawingCombinators.
Wow, great tool! Could be tricky to run for programs that interact with the environment in non-trivial ways, though.
This might interest you: http://conal.net/Pan/ Also, if you are going to implement your own Image data type I really recommend reading this first: http://lukepalmer.wordpress.com/2008/07/18/semantic-design/ 
Hmm, we can't change GC parameters for the shootout, but we can use this tool to tweak the algorithm to use the GC we've got most effectively.
Here's a graph of [GHC itself compiling a medium-sized module with -O](http://haskell.org/~simonmar/ghc-20100706-simple-O1-cam04unx.svg). Note the second local minimum... very interesting.
last discussion: http://www.reddit.com/r/haskell/comments/a30i2/project_raincat_a_brilliant_haskell_game/
In practice you'll find that with the type class instances that specific that you can't really write most of the general purpose combinators that make monads worth using. You wind up with the only things you are able to right as the intersection between the features of RMonad and the parameterized monads I described here: http://comonad.com/reader/2007/parameterized-monads-in-haskell/ You give a lot up by making your instances that specific. The power you gain in being able to tailor each instance is power you lose in being able to rely on common shapes, fmap, etc.
Typeclass aliases, we need. So that map Monad m to Monad2 m m, we can.
So record syntax fixes didn't make the cut?
which record syntax fixes are you referring to? 
Hmm, last time dons thought it was "brilliant", now it's "lovely". It may well have been a brilliant and lovely learning experience for the students that made it but it's a pretty feeble game.
And once again a local minimum at -A small and -H 256M. Seems to be common.
Looking at the games written in Haskell, rarely do they have such a distinct design element. It stands out, even if there are flaws in the game play.
http://i.imgur.com/smrub.jpg :-)
I agree, re:asp.net. Been working with it for years, and am now building a new RESTful .NET framework based on ideas from FRP and other functional frameworks. Doing my daily work with web forms is painful.
I thought it was a very nice game, even if the UI wasn't that polished. It was certainly fun to play.
When setting "-A 64M" on x86, it would be neat to automatically use hugetlb for another notable performance win. EDIT: Yes, I'm aware of the drawbacks of hugetlb (ex: pre-allocation / generally at boot time)
thank you for this.
Sadly, it was.
There's a subtly in working with type families, owing to their laziness. Here's a trap that bit me the other day. Consider the following code: data True = T data False = F class Decide tf a b where type If tf a b nonFunctionalIf :: tf -&gt; a -&gt; b -&gt; If tf a b instance Decide True a b where type If True a b = a nonFunctionalIf T a b = a instance Decide False a b where type If False a b = b nonFunctionalIf F a b = b useRank2 :: (forall a . a -&gt; b) -&gt; b useRank2 f = f "foo" hasTrouble a = nonFunctionalIf F a (2 :: Int) -- try useRank2 hasTrouble hasNoTrouble :: a -&gt; Int hasNoTrouble = hasTrouble -- try useRank2 hasNoTrouble Because type families are lazily evaluated, we have: *Main&gt; :t hasTrouble hasTrouble :: a -&gt; If False a Int *Main&gt; :t hasNoTrouble hasNoTrouble :: a -&gt; Int So for instance, while useRank2 hasNoTrouble is allowed, useRank2 hasTrouble is not: *Main&gt; useRank2 hasTrouble &lt;interactive&gt;:1:0: Inferred type is less polymorphic than expected Quantified type variable `a' escapes In the first argument of `useRank2', namely `hasTrouble' In the expression: useRank2 hasTrouble In the definition of `it': it = useRank2 hasTrouble The only way around this seems to be to specify the type of hasNoTrouble. In this example that's pretty easy, but for a complex combinator library it can be very labor intensive. It would definitely be handy to have a smoother way of forcing type families to be evaluated.
who's that guy?
What I would be happy to see in Haskell 2011 (in increasing order of being possibly problematic): * (outlaw tabs) * PackageImports * overloaded strings (maybe not by default?) * kind annotations * scoped type variables * GADTs * existential types * rank 2 (or rank N) types where the important part is the last 3 type system extensions. But I'm not really deep into the theory, so please feel free to point out if some of these are in fact problematic in some subtle or not-so-subtle way. I omitted MPTCs and/or type families, even though there is clearly a big demand for that, because even I know that they *are* problematic :)
http://en.wikipedia.org/wiki/Per_Martin-Löf
Someone should sell this poster at Haskell hackathons.
&gt; (outlaw tabs) This seems like the most problematic proposal in your list -- how do you propose code be indented, if not for tabs?
I've tried to go this direction and found that type inference is a lot harder; you either use very specific functions or give up on general combinators. The big problem is ambiguity, which will pop up as soon as you bind into and out of any general combinator, since the general combinator could have worked at many monadic types.
Probably this: http://en.reddit.com/r/haskell/comments/bhygx/there_are_more_than_4000_people_in_the_haskell/ 
If only there were some other character that rendered as whitespace, but uniformly at a single fixed width... nah, that's crazy talk.
&gt; uniformly at a single fixed width So, defeating the entire purpose of indentation?
Perhaps said character could be used to encode greater levels of indentation by repeating it multiple times...
You're a dreamer. A crazy crazy dreamer.
This only works if you use a different character for indentation and alignment. Spaces work great for alignment. They're terrible for indentation. You can't resize spaces, because if you do, then all of your alignment gets out of whack and the code looks terrible. I suppose you could argue that 0x20 be used for one and 0xA0 be used for another, but that seems an awful lot of trouble when there's a character designed *specifically for* indentation already extant. Here's an example. You have the following code (using only spaces): foo :: Foo foo = Foo a b c where a = 1 b = 2 c = [ 123456 , 234567 ] Now, that works fine if you want it to be indented by two characters. But what if you want to display indentation as four, or eight characters? Suddenly, your editor has to support parsing the language syntax and regenerating the file on-the-fly. Contrast that to using tabs: foo :: Foo foo = Foo a b c where &lt;t&gt;a = 1 &lt;t&gt;b = 2 &lt;t&gt;c = [ 123456 &lt;t&gt; , 234567 &lt;t&gt; ] Now, I can indent it however I want. Two, four, eight, eleven, whatever -- it'll come out looking perfect. Using spaces for indentation is a terrible idea, an awkward solution to a problem which hasn't existed for decades.
Yes, using tabs for extra semantic content, e.g. "increase indention level" vs "put a blank space here", is a nice idea in theory, but doesn't work in Haskell because the language spec defines indentation in terms of fixed-width column position with awkward rules for tab stops. Without first changing that part of the grammar, tab indentation will never be a good idea. And judging from tab vs. space debates involving languages where it's not syntactically relevant, most people seem to prefer using spaces to make ASCII art out of their code rather than use meaningful indenting anyway. So, oh well.
Actually, tabs work perfectly in Haskell -- I use them in all of my code, and have never had an indentation-related problem. Ditto my Python code, which is a language even more sensitive to whitespace problems than Haskell (due to dynamic typing).
[Section 10.3](http://www.haskell.org/onlinereport/haskell2010/haskellch10.html#x17-17600010.3): &gt; The “indentation” of a lexeme is the column number of the first character of that lexeme; the indentation of a line is the indentation of its leftmost lexeme. To determine the column number, assume a fixed-width font with the following conventions: &gt; &gt; * The characters newline, return, linefeed, and formfeed, all start a new line. &gt; * The first column is designated column 1, not 0. &gt; * Tab stops are 8 characters apart. &gt; * A tab character causes the insertion of enough spaces to align the current position with the next tab stop. So, no, they're not just eight spaces. Of course, by defining any ordering relationship whatsoever between the "size" of a tab and a space, we've left the land where tabs are used for indentation sensibly.
That rule would actually be more sensible than what's in the spec, which requires tabs to be expanded to spaces until the next column is a multiple of eight. I don't understand how specifying their behavior makes you think tabs have been "banned". Whitespace indentation is specified in terms of brace notation, but the spec doesn't ban whitespace. My ideal language would use *only* tabs for indentation, and treat spaces as comments -- but that would anger the folks still writing code in Notepad.
Actually, I'd say it works *better* in Python: Python uses indentation primarily to distinguish *blocks of imperative code*, where each line stands alone and the only grouping that matters is sequential. Haskell code, on the other hand, often uses indentation to describe *recursively-structured expressions*, where it's often more natural to use vertical alignment and visual nesting, and grouping matters at multiple levels simultaneously. Most arguments against tabs in whitespace-oblivious languages boil down to "people won't use them properly, this is why we can't have nice things". In Python the main argument is "the Benevolent Dictator commands it" which I guess works for them, though I always use tabs for Python code that's a solo effort. But honestly, in Haskell, often I actually *want* to make ASCII art with my code's layout, so I gave up and set my editor to use spaces only just for .hs files.
Having occasionally needed to edit code using Notepad or other very dumb editors with no "insert X spaces instead of tabs" behavior, I promise you that single-keystroke indentation with no errors due to miscounting is the *much* better option.
&gt; But what if you want to display indentation as four, or eight characters? Why is it important to be able to do this? It's mainly nerdy obsessiveness, destined to be sacrificed on the altar of compatibility as described [here](http://urchin.earth.li/~ian/style/haskell.html).
I hate that page -- it was written for morons, by a moron. His chief argument is that it's possible to design a layout that makes tabs ugly, but stretched to a few hundred useless words. And yet, it keeps popping up. Being able to see the indentation of code is not "nerdy obsessiveness", it's vital to understanding at a glance how a function is structured. Given that there's practically zero chance of any two people agreeing on a suitable indentation size, it's much better to let it be customizable than to make everybody use 1-char or 16-char or whatever. As for the "altar of compatibility", bullshit. Every major text editor written in the last 20 years can render tabs. vi(m), emacs, gedit, notepad++, visual studio, xcode, kate -- no matter what OS you use, chances are that whatever you write code in can handle tabs.
Is there a miscommunication here? I'm arguing *against* using spaces for indentation. "insert spaces instead of tabs" is a feature intended for people who indent with spaces, and therefore, not one I feel is at all useful.
In Python, if your indentation is wrong, then some lines will be in different if-else branches or procedures. This won't be caught until the code is actually run. Contrast that with Haskell, which only supports one expression per if-else branch or function -- if the indentation is sufficiently broken that it changes the meaning of the code, the compiler will generally refuse it.
&gt; Being able to see the indentation of code is not "nerdy obsessiveness" I was asking why it's important to be able to *change* the indentation of the code. &gt; Given that there's practically zero chance of any two people agreeing on a suitable indentation size You acknowledge this, yet you think that everyone should standardize on a particular approach to using tabs. Seems inconsistent. Particularly since the approach you're advocating has its own set of problems: for example, either depending on mixing two types of whitespace on the same line, or being incapable of aligning code properly. The reality is that plain text files aren't designed to support the exact requirements of program code layout. The only way around this is to impose a convention, and none of the conventions are perfect. What tends to end up winning is the convention that's most reliable and practical for the most people. That's the altar of compatibility that I referred to. &gt; Every major text editor written in the last 20 years can render tabs You act like there's just no issue with your favorite solution. Was [this page](http://www.emacswiki.org/emacs/TabsAreEvil) also "written for morons, by a moron"? Is everyone on the other side of this issue a moron?
&gt; I was asking why it's important to be able to change the indentation of the code. Because I can't see how code is indented when somebody uses only 1 or 2 spaces as indentation, as is very common among space-indented code. &gt; You acknowledge this, yet you think that everyone should standardize on a particular approach to using tabs. Seems inconsistent. Standardizing on tabs is trivial -- just turn on visible tabs in your editor (some have it enabled by default) and hit a different key. There's no adjustment of preferences needed. &gt; You act like there's just no issue with your favorite solution. Was this page also "written for morons, by a moron"? Is everyone on the other side of this issue a moron? Did you even read that page? It goes into exact detail about *why I'm right*, and links to a second page ([SmartTabs](http://www.emacswiki.org/emacs/SmartTabs)) showing how to configure emacs to do it the way I advocate! The only limitation they mention on using tabs for indentation is that it prevents some styling which violates common conventions. And yes, I maintain that anybody who can't figure out how to press "tab" when they want to indent is a moron.
Okay, the first four versions are pretty mundane--recursion in types allows one to introduce recursion in terms, nothing new here. The `Data.Dynamic` one is, admittedly, a particularly droll bit of obfuscation. But the fifth one is just.... what? ...how does that... gah! How does he come up with this stuff?
I think it's a great game. It'd be nice to get the space leak fixed, though.
Laziness in Ref style datatypes (so IORef and MVar and others) frequently leads to surprising behavior. dons has talked about this in this presentation: http://donsbot.wordpress.com/2010/02/26/evaluation-strategies-and-synchronization-things-to-watch-for/
Tabs are fixed to 8 columns in Haskell. They are a primitive file compression mechanism, nothing more.
So you avoid using layout in a way that depends on the width of a tab? Would you advocate making that illegal?
Clearly I shouldn't have mentioned tabs. The fact that tabs generated 20+ posts while nobody talks about the type system is a harsh reminder to Wadler's law...
do you mean that ST one because of ST? same idea in impure languages seems old (it's present in Oleg's ocaml fixpoints module) - it was one of my assignments in fun prog class, so it can't be that new.
Impressive, I like very much! I probably would have used this in clientsession had it been available at the time. Any chance you might be writing some encryption algorithms of the same caliber?
Very interesting example! In my opinion this is a bug -- when checking for escaping existentials the typechecker ought to see whether expanding any type synonyms gets rid of them. If you have time, I'm sure that creating a bug ticket on the ghc trac (or adding this example to an existing ticket) would be appreciated!
Hehe. I do love type theory and dependently-typed languages. But I think there's still room to push Haskell a bit further towards dependent types without starting over from scratch. =)
I agree. This is good prodding; I'll post it tomorrow. Great article, btw! **Edit:** [#4178](http://hackage.haskell.org/trac/ghc/ticket/4178)
&gt; Because I can't see how code is indented when somebody uses only 1 or 2 spaces as indentation, as is very common among space-indented code. One of the issues here is simply that you're valuing different things than other people. For example: &gt; Standardizing on tabs is trivial -- just turn on visible tabs in your editor It seems to me that a dislike of visible tabs is no more or less rational than a dislike of two-space indentation. The issue is then not which approach is technically superior, but which set of values is subjectively better, and there's more than one answer to that. &gt; Did you even read that page? Yes, just not as selectively. The section you're referring to is "how [editors] *ought* to handle [indentation]". The problem with that "ought" is that it relies on a combination of editors and users behaving correctly, which is a particularly iffy requirement when that involves them being forced to work in some other mode than their preference (and possibly in some other editor.) It just isn't very practical. One reason space-only tends to win is that it's easy to convert a properly indented file to that format, no matter what mechanism you used to do indenting. Lowest common denominator wins, especially when there's no perfect solution. &gt; And yes, I maintain that anybody who can't figure out how to press "tab" when they want to indent is a moron. Oh good, I guess that excludes all the people who just have different taste than you. 
What a talented person!
I had a brain fart when reading the name at the end and felt this sudden spike of confusion and excitement, hurriedly clicking, expecting to see Oglaf taking a *really* weird turn. She and that Abstruse Goose guy should do something together.
There's nothing to discuss about the other points, that's a very nice list imho. The tabs/spaces thing is simple, just require that indentation is consistent between consecutive lines.
What's with the bogus `._HaRe.cabal`? Additionally, the whole package smells like being created on a non-case sensitive file system, in short, it doesn't compile out of the box, at all. Using base 3 _and_ the split base packages isn't a good idea, either. /me gives up after stumbling across the missing include for `./tools/base/Modules/QualNames.hs`. This package never compiled, on _any_ machine or platform, in its current form. Try again, this time testing before submitting. 
&gt; If the imponderables about syntax were amenable to scientific theory, then programming language experts would long since have taken them on board and they would have just as much significance, for them, as type system subtleties do. And it'd be ignored by the mainstream, just as type theory is.
GADTs implies existentials
I guess that Haskell is actually moving "away" from type classes and more towards plain, functional, type-level programming.
Although to be fair we could introduce GADT syntax without existential syntax.
Here I was thinking I had Haskell's evaluation semantics figured out and then I'm reminded of lazy ST.
So what? They're not fixed in the editor. I can choose to render them as *n* spaces, where *n &gt; 0*, and it'll come out looking fine.
Yes. Python has the `-tt` command-line flag, which enables checking of indentation at compilation time. If there's any ambiguity (eg, using both tabs and spaces for indentation), compilation fails. I'd love an option like this for Haskell, especially if it were enabled by default (but I suspect much existing, poorly-written code would fail).
Cool. Please also see the recent discussion [1] on reworking Crypto. I've started making the "crypto-api" package to provide a unified set of typeclasses and helper functions for such operations. This discussion hit reddit about a week ago. [1] Latest e-mail: http://permalink.gmane.org/gmane.comp.lang.haskell.libraries/13195
In practice your editor's tab width must be set to the same number as person who uses the most narrow tab width in the project. If one person uses a 2 space tab setting and another uses 8 the latter will see text overflow, probably way over the 80 characters wide.
That's acceptable -- I haven't had to edit code in a VGA console in years, so an 80-character margin is more advisory than rule.
&gt; Many of the 'superficial' 'irrelevant' syntactic niceties of Haskell contribute immensely to the beauty and (thus) clarity of Haskell source. I fully agree with that. Still, the type system is more interesting, and syntax-related discussions usually never lead anywhere.
Well, skimming the Haskell prime proposals page always make me feel that there are lots of very subtle interactions between the different features, which I'm not at all aware of (I even remember somebody arguing that GeneralizedNewtypeDeriving - which I would think is one of the most harmless extensions - introduces some unsafe stuff, though I'm not sure if I should believe that :). So I can easily imagine that, say, rank 2 types interact with some existing language feature (say, type inference, or the type class mechanism) in a way so that there are good arguments against including it in the next standard. I expected such feedback instead of a tab/space flamewar (of course I can only blame myself for starting it).
Indentation is semantically significant, and tabs are defined to be 8 columns apart. If you make them look different, then either you have to be very careful not to use them in a semantically significant way or you'll get some very confusing errors.
&gt; And yes, I maintain that anybody who can't figure out how to press "tab" when they want to indent is a moron. There is no problem with the *tab key*. The problem is with the *tab character*.
`-f-warn-tabs`
That warns if *any* tabs are used, rather than if the indentation is ambiguous. It's the opposite of my proposed flag.
The correct fix is to define tabs and spaces to be separate, and to refuse to compile files which combine them in an ambiguous way. Defining a tab to be "8 columns" is an ugly hack, albeit one which does not usually cause problems in practice.
http://www.haskell.org/pipermail/haskell-cafe/2010-July/080028.html &gt; There were a few reported problems with building HaRe on Linux systems, so I have now uploaded HaRe, version 0.6.0.1. This version should fix all these problems. &gt; I can confirm that I have tested this version on: &gt; Ubuntu, version 10.04 &gt; Mac OS X version 10.6.4 &gt; and Cygwin. &gt; &gt; I also confirm HaRe works with AquaMacs, Emacs 23.1.1 and GVim 7.2 &gt; &gt; Please let me know if there are any issues. &gt; Kind regards, &gt; Chris Brown (on behalf of the HaRe team). 
hmm, much as I hate to drag this on any longer, I'm still not sure you've understood the issue here: it's not just about tabs and spaces, e.g. main = print x where x = y y = 3 (with tabs used to indent y). So if someone else wrote this code and you load it up in your editor, it's going to look very strange. I'm not saying that writing code this way is necessarily a good idea, indeed I think GHC ought to have a warning for it. But it's what Haskell currently allows. Python's -tt option doesn't have to worry about things like this. 
&gt; with tabs used to indent y I think you mean "align", but why would anybody do this? Tabs are not for alignment, they're for indentation. For example, here's three ways to render that code: main = print x where &lt;t&gt;x = y &lt;t&gt;y = 3 main = print x &lt;t&gt;where x = y &lt;t&gt; y = 3 main = print x &lt;t&gt;where &lt;t&gt;&lt;t&gt;x = y &lt;t&gt;&lt;t&gt;y = 3 Notice that 1) no matter what your editor's tab setting is set to, these will look as expected and 2) in no case does the syntax's definition of tab size matter. Now, here's a case where changing the syntax's definition of tab size has a measurable effect. This code depends on a particular tab size to compile; a flag to enforce proper indentation would cause this code to *always* fail, in the style of Python's `-tt`. main = print x &lt;t&gt;where x = y + z where &lt;t&gt;&lt;t&gt; z = 5 &lt;t&gt; y = 3 &lt;t&gt; z = 4
I can clearly see a LICENSE file in the directory.
It wasn't there seven months ago :)
A feature I always wanted (and should be very easy to implement) is the possibility to tell ghci to use some other function than `show` to print the expressions.
Well, type classes themselves are still useful for all sorts of things and aren't going anywhere soon. But as far as type-level programming is concerned, yes, consensus does seem to be moving towards a more functional style with type families.
Yes, it works fine now, ghc 6.12.3 on gentoo here. ...it was very, very hard to locate a single file in my sources that I could use it on. It'd be great if it wouldn't choke on extensions even when it can't understand them and thus would have to refactor around them, which I think stands and falls with compilers in general exposing an (ideally cross-compiler) interface that does everything HaRe needs from parsing to pretty-printing. While programmatica seems to be a very decent framework, it seems to have trouble (or deliberately doesn't even try) to keep up with non-standards track Haskell, which in my perception is the vast majority of the code out there. 
Type families and functional dependencies are simply two different ways of dealing with multi-parameter type classes. If you squint a little, at least.
I did a similar thing when I was learning Haskell, and I found it to be very rewarding. I used SDL for graphics and sound, and it worked quite well for me. Took me a bit to figure out the sound (didn't have a lot of samples). And I'm not sure I ever got it working on OSX (but I didn't have a machine to test on).
Any record syntax reform at all. Wasn't sure if it was being considered for this iteration of the Haskell Report.
Okay, I've installed it from Cabal...now how do I use it n Emacs?
this (or something like ipprint) would be nice to have in ghci by default
`Show` should really be renamed to something like `Serialize`.
I don't like the syntax for existentials in GHC anyway, so I'd be up for that. Then you could switch ExistentialTypes to being an extension for an actual first-class 'exists' quantifier.
Generalized newtype deriving is a problem when you rely on newtypes simply introducing an equation `New ~ Old` in the type system. Then you can implement the 'deriving' by simply using dictionaries for `Old` as if they were dictionaries for `New`. This can allow instances for `New` that are otherwise impossible to write. In other words, there's no problem with newtype deriving as long as you have some way of ensuring that all the instances you generate could be implemented by fiddling with the constructor. I'm not sure how hard that is to ensure in general, though.
I kind of wonder how hard it would be to hack ghci to do this. (Goes and downloads ghci source...)
Just [RTFM](http://www.cs.kent.ac.uk/projects/refactor-fp/hare/README_28062010.txt) (the scripts should be in `~/.cabal/share/HaRe-6.0.1` or something)
The relevant code is in compiler/typecheck/TcRnDriver.lhs. GHCi appears to be desugaring code into System.IO.print it (using the PrelNames module), which means all you have to do is overload System.IO doink. I'm not sure but I bet changing this to not qualify the variable (not changing PrelNames) would do the trick. Edit: Well, it's not that simple... we'd need to pull out the unique identifier for a locally bound print...
That would imply that it should be used for serialization when it shouldn't. It's only for human reading.
why there aren't any comments? it seems pretty groundbreaking, is it too good to be true?
I think people have to come to terms with their programs being ethereal fuzzy things without normal form, just isomorphic views.
Up to now, I assumed that showList in the Show class was a hack. Now it's a language feature...
&gt; This can allow instances for New that are otherwise impossible to write. Can you give an example for such a situation?
See also http://homepages.cwi.nl/~jve/ for DEMO -- a Dynamic Epistemic MOdel checker by Jan van Eijck in Haskell (the paper's direct link http://www.cwi.nl/~jve/papers/07/tilag1/VanEijck.pdf ), which also supports epistemic action models and their “execution”
Doesn't this post show it's not for human reading? :) Also `Read` is taken to be the inverse of `Show`, so that implies some kind of serialization contract...
It's theoretically interesting, I think. However, on a quick skim it requires having access to / being able to change all the source code. And if you have that, then you've lost half the point of modularity anyway, I think?
I use read/show for quick and dirty serialization all the time. Efficient serialization is case-by-case, not always necessary, and what "efficient" means depends a great deal on the use case.
I've never accepted that separate compilation requirement from expression problem. All those convoluted solutions are so circular that they aren't really compiled - no optimizations take place, so what's the point? they could be interpreted at runtime. and if you want to deliver only binaries without source code, you would only have to expose those matrices (variant*function), every case (e.g. eval-Add) can still be obfuscated inside.
I think some of separate compilation could be preserved with a smart enough system. If you change the view, make and edit and then change the view back, a thing similar to ccache could detect that some modules didn't actually change. You'd be left with more cache files than before, that's all.
I'm not sure I understand. If you don't have separate compilation, then the expression problem just reduces to "I'd rather make more edits in fewer files." And then you don't have an expression problem, you have an expression gripe.
I think the standard example is: class C a where convert :: forall f. f Int -&gt; f a instance C Int where convert = id newtype N = N Int deriving C Now you have `convert :: forall f. f Int -&gt; f N`, but there's no way to write that by hand for arbitrary `f`. If f were some variety of functor (covariant, contravariant, exponential) there would be, but there are absolutely no constraints on what `f` can be. So: type family T a :: * type family T Int = () type family T N = Int -&gt; Int and now for `f = T`, we get `convert :: () -&gt; (Int -&gt; Int)`, but since the newtype deriving is just reusing the `Int` dictionary for `N`, we know that `convert x = x`, so we've coerced and defeated the type system.
Mind Explodes
When all you have is Haskell, every problem looks like a type system in disguise (of course, every problem *is* a type system in disguise). BTW - I believe that B has 20 and C has 30.
With something like this it'd be possible to replace show with a latex renderer so you could build a pretty ghci-based texmacs interface. good luck. =)
gripe is a problem:)
Ah, I see, thanks! Though, for the second part, I cannot get GHC to unify `T Int` with `f Int`.
Yeah, I didn't bother trying to compile it. The following will check, though: newtype Wrap a = Wrap { unwrap :: T a } bad = case convert (Wrap () :: Wrap Int) :: Wrap N of Wrap f -&gt; f 5 -- corrected; I frequently type = instead of -&gt; in cases which is obviously a bad thing.
Some good advice. The main thing to take away for newbies is to un-learn all the imperative/OO stuff you've learnt, and then gradually re-learn it all in the context of Haskell. It makes you understand imperative programming alot better as well.
This is interesting and all, but should it really be posted to the Haskell reddit?
Anyone want to suggest a good "first project" for Haskell? My major problem with new languages is that I need a goal, and just doing calculations isn't interesting enough to hold my attention. 
Learning Haskell is like learning any other language - if you want to get to a good level in it, then you have to spend time in the country where the language is spoken. So start spending a lot of your time in Haskell country, and you'll be alright. 
Some kind of logic game solver. Sudoku, Light Up, Kakuro, Solitaire (the peg jumper), etc. I made a solitaire solver first (finds all solutions, not just one) and I learned a lot of built-ins (such as the (!!) operator, catMaybes, etc.) so I didn't need to code such functions on my own anymore.
Well, we've got to keep an eye on those OOP'ers.
See, that's what I mean when I said boring calculations that don't hold my attention. Finding solutions to game X just seems very dull to me... as an example for Python I wrote a full web spider that did web analysis and later that did basic checks for XSS for me... do you think the learning curve is too steep to start off with something of that level? 
Text or some data format processing or command line filters. In many cases [interact](http://haskell.org/ghc/docs/6.12.1/html/libraries/base-4.2.0.0/Prelude.html#v:interact) is enough for all input-output, and you can write useful code as a pure function from the beginning, without touching more tricky parts of the language at first.
Did you mean: [haskell county](http://www.google.com/search?q=haskell+county)
There is a great tutorial about writing an IRC bot [here](http://www.haskell.org/haskellwiki/Roll_your_own_IRC_bot).
ooh, now that could be fun. I haven't screwed around with IRC in forever... thanks! 
Yep, I saw your answer to mathstuf below. You're welcome, I'm glad it's interesting to you.
Doing calculations works too. I started learning Haskell by risking my M.Sc., using it for all the implementation (of an AI Planning system dealing with hierarchical actions under knightian uncertainty, so there's almost no I/O). It all worked fine in the end, but probably because of the "goal" component of the situation though. PS: Damn, it's been almost 3 years already and I still haven't put it on hackage ... It's called HasPlan by the way, so please save this name for me! ;-) It's part of the Haskell AI Strike Force, under the AI.Planning hierarchy, and the goal now is making it a combinators library for AI planning. Who wants to help btw?
For the very beginning, projecteuler.net is always nice. I got the hang of many things in Haskell trough solving those exercises. They are not as dull as you might think. Edit: link.
Another random nerd makes a blog post about learning Haskell and about how we should all forget our evil imperative habits blah blah blah blah blah. Originality points! Haskell is the greatest and the easiest (yes) language I've ever used. In fact, it the only language that has made programming actually enjoyable for me. But stop saying the same things over and over and make some libraries because we really really really need them.
I wrote a ddns client for afraid.org [link](http://codepad.org/o1X4D2Rw) Then I wrote a script to wake up my computer [link](http://github.com/onmach/Wakeup) using /proc/acpi. Really anything you would normally script you can do in haskell fairly easily.
Something like that should be really straightforward, actually. It's just that getting a webpage is getting a webpage, no matter what language you do it in. Somewhat more complex tasks are where a functional language will really start to shine.
Indeed, it typechecks and segfaults! (well, after replacing `=` by `-&gt;`) Which makes me think, because GeneralizedNewtypeDeriving is such an useful thing; we should probably weaken its scope, for example by emulating some template-haskell style deriving mechanism, and throwing an error if that doesn't succeed. I cannot really imagine that something could go wrong by mechanically writing ordinary Haskell code :) 
Folks in the Haskell world have read many posts like this, but that's not to say that the author's readers have also read many posts like this.
My first program in haskell was a remake of glxgears. The gears had easily configurable teeth. It was also my first opengl project. I never had the courage to dig through the lengthy opengl code, but with haskell it was suddenly easy.
That's the second time a read a comment about an upcoming iteratee release with an improved API. Any grounds for this? Edit: Alright, that would be [JL's darcs repository](http://inmachina.net/~jwlato/haskell/iteratee/).
As an imperative programmer (mostly C++) who's trying to learn Haskell, I find posts like this not to be helpful at all. They're like Zen koans: They're not teaching knowledge, they're summarizing wisdom, so that those who have already acquired the knowledge can nod their heads sagely, while those are trying to acquire it just scratch their heads. The "forget your imperative ways" aphorism needs a follow up: What replaces them? How do I accomplish tasks that were easy with imperative code? What are examples that show where functional programming really shines over imperative programming? And for the latter, I don't mean Euler problems, I mean the stuff I would be writing ordinarily.
Ah, the beautiful country of Haskell--where the maidens are pure, the workers are lazy, and everyone eats nothing but curry.
Well that's unpleasant. Guess I know what I'm doing tonight...
and remember, like any language, haskell has lots of esoteric features you probably don't need to care about right away...and indeed you might never need to care about 
Amusingly enough, it wasn't segfaulting when I ran it in ghci. It produced a function that was effectively `const n` for some `n` I don't recall. But yes, that was my original point. Newtype deriving is useful, and not theoretically a problem as long as you can confine it to generating things that could conceivably be written by hand. The practical problem is deciding whether it's possible to write functions of a certain type by hand.
Premature optimization --&gt; the root of all evil. Well actually I think the misleading wiki article is also partly to blame here.
Been here a [few](http://www.reddit.com/r/haskell/comments/c9oa7/wait_what_berp_an_implementation_of_python_3_that/) [times](http://www.reddit.com/r/haskell/comments/c9osg/the_compilation_scheme_used_in_berp_the_python_on/) before.
Maybe we are using a different GHC version, because it seems to segfault all the time here (this is on 6.10.1)
Great news, especially the haskell 98 and haskell 2010 upcoming compatbility! If the plan if to enable the gc by default, does this mean the region inference will not be used anymore? Or will both coexist?
&gt; If the plan if to enable the gc by default, does this mean the region inference will not be used anymore? Or will both coexist? The latter. http://article.gmane.org/gmane.comp.lang.haskell.jhc/681
&gt; * packed representation of algebraic types, 'Maybe Foo' is actually &gt; represented in memory as a NULL pointer or just a 'Foo' with no tag bits. I'm a fan of this.
Are there a performance advantages of this? Also, if I create a similar custom data type (say `data A x = B x | C`) will it result in similar optimization?
These are indeed 'summarizing' thoughts (cant say 'wisdom') that i gathered in the way of learning haskell. It's kinda personal but i hope it will help others, if any, in a way. It's really not a 'practical starting guide' for learning haskell or something like 'teach yourself haskell in 7 hours or 21 days'. It's hard to say what are the shining points of functional over imperative, more so if by the 'practical usefulness' standard. To me, functional programming lets me start to view problems in different ways -- especially in a more high-level and abstract way. You become a better problem solver gradually maybe not by writing functional program literally but by thinking in a functional way, even when you are writing imperative code. 
Thinking i'm going to cross post it to proggit so that more 'outsider' people who might want to have a try at haskell will see it. -- and btw, you have to wait 8 minutes for submitting another comment? reddit?
Probably, on both counts. A normal layout of an algebraic datatype would be something like (using C notation): struct { int tag; ... } Where the `...` holds pointers for the fields of each constructor somehow. The bullet point says that `Maybe T` instead has an identical representation to `T` (when unpacked). So you save a byte or four (or eight) at least each time that happens. And I doubt `Maybe` has been special cased. There are other similar optimizations you can do. Like for any datatype of the form: data T = C1 | C2 | C3 | ... if you know it's safe to keep it evaluated (i.e. you're definitely going to evaluate it anyway), you can represent it as just a machine integer, instead of (effectively) a pointer to a machine integer (along with whatever other overhead normal datatype representations carry). Anyhow, it's a step toward the sufficiently smart compiler. You write code that's nice to work with, and the compiler finds an optimal representation.
[RWH](http://book.realworldhaskell.org/read/) has some chapters about JSON and Barcode recognition which you might like to take a look at. -- I personally quite like the bloomfilter chapter. :^) There's also a bloomfilter hackage, which i just played with and found 2 bugs. I then reported to the author, who fixed 'em promptly. 
and a lot of ghc extensions and tricks to squeeze out the last possible drop of performance, which i currently choose to ignore. Now just stick to the main language itself and maybe revisit 'em later on.
.. also where the electricity is static but the electronics are functional.
The bigger issue than the API change (IMO) is that it's a totally different style of iteratee, switching to using CPS internally.
Yeah I was wondering already when I had a look at the code. I do find CPS iteratee cleaner to read, but didn't Oleg say something about performance problems here?
I thought I'd read that CPS usually performs as well as the current approach, and sometimes a bit better. But I'm not really sure. I actually have a totally different concern: CPS doesn't mix very well with exception catching. In particular, it seems to be impossible to write a bracket function for CPS code; you can see [this discussion](http://permalink.gmane.org/gmane.comp.lang.haskell.cafe/76262), though it has come up elsewhere.
Quoth the Haskell Report, *apropos* derived instances of `Read` and `Show`: &gt; (...) `readsPrec` should be able to parse the string produced by `showsPrec`, and should deliver the value that `showsPrec` started with. &gt; The result of `show` is a syntactically correct Haskell expression containing only constants (...) &gt; `readsPrec` will parse any valid representation of the standard types apart from strings, for which only quoted strings are accepted, and other lists, for which only the bracketed form […] is accepted. It may not be explicitly described as such, but the *intent* is reasonably clear: `Read` and `Show` together constitute a crude form of serialization, with the additional constraint that the serialization format be *valid Haskell code that, if run in the context of the original program, would construct the same value[0] that was serialized*. If anything, the default semantics for `Read` work out as something like a crude `eval` function for a subset of Haskell that contains only data constructors. This is, in fact, a frequent source of confusion for newcomers to Haskell, who expect `show` to behave like "coerce to string" functions in other languages, then become frustrated that applying `show` to a `String` will *add the double quotes* instead of returning the string unchanged. If we're talking about replacing `Read` and `Show`, how about replacing them with a less crappy parser for Haskell data constructors and a datatype pretty-printer, respectively? [0] Up to some sort of isomorphism, at any rate.
- No support for higher-kinded polymorphism and this is (partly) why monads in F# are not as generic as in Haskell. This is an issue with .NET generics more than F# (although Scala does it) - Type constraints (generic constraints) can't use operators, inherit problem with .NET generic constraints since they use interfaces for constraints but operators are defined as static methods of a class. Still it's a decent language.
Now lets see a comparison the other way...
References to the articles/#haskell topic he mentioned in passing: http://augustss.blogspot.com/2009/02/more-basic-not-that-anybody-should-care.html http://augustss.blogspot.com/2009/02/is-haskell-fast-lets-do-simple.html
this is the BASIC monad he was talking about: http://augustss.blogspot.com/search/label/BASIC amusing stuff
&gt; Still it's a decent language. That's perhaps not as strongly worded as it should be: F# is a *good* language, unquestionably, and one that I suspect most Haskell programmers wouldn't be too averse to working with. These sorts of fine-detail comparisons between similar languages can be counterproductive, as it's easy for fans of one language to read minor complaints as serious criticisms, resulting in silly feuds between people who basically agree on nearly everything. Instead, we should set aside our differences, Haskell and F# programmers alike--and OCaml and Scala and any others--that we might all come together in the spirit of brotherly love, join hands as one, and go run those C++-using bastards out of town once and for all.
&gt; `let` for top-level functions instead of top-level pattern matching Functions are values, values are bound by `let` and pattern matching is introduced by a different construct (`match... with`). Haskell mixes binding with pattern matching which confuses me a little.
&gt; that we might all come together in the spirit of brotherly love, join hands as one, and go run those C++-using bastards out of town once and for all I'll race you over yonder bikeshed.
&gt; cabal unpack basic For a closer look.
Seen on [PDXFunc and BayFP](http://groups.google.com/group/pdxfunc/browse_thread/thread/fe82b13a2bccf4e0).
This is fantastic, but I do wish we could standardise on a packaging system. Cabal has its warts, but having to work out how to get stuff built in a different packager just to try out a compiler is a colossal pain.