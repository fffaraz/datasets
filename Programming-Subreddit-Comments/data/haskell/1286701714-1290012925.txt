Great.
I must sadly agree with this.
Grumble. I don't like the API. I don't like the callback style or the fact that it's all in IO. I suppose I should implement the API I consider to be nice in the zlib package.
The callback style is to try and prevent people from shooting themselves in the foot. And the fact that it's all in IO is kind of the point here: you can easily wrap a pure interface around this mid-level API, but each one of those actions does by itself change the world. That said, I implemented the API that was most convenient for my use case, and I'm well aware that some things are sub-optimal (for example, it would be nice to allow direct passing of Ptr CChar for some low-level uses). I'd be happy to make any modifications, and even happier if an appropriate interface showed up in zlib ;).
&gt; Frankly, most of the work so far has gone into making OpenID 2 a reality It would be quite cool for Yesod to have easy-to-use OpenID, Gravatar, and reCaptcha plugins.
OpenID is already there (see yesod-auth package). I'll probably factor out the Gravatar and reCaptcha code from Haskellers in a bit and put them somewhere. The only downside with my reCaptcha code right now is it uses the OpenSSL library, since there doesn't seem to be a good pure-Haskell AES library that works on 32-bit systems.
You can grumble, Duncan, but several people have asked you for exactly this in the past and you've demurred. The fact is, to write an iteratee transformer to compress something in O(1) space right now we have to kludge it by making a couple of `Chan`s, `forkIO`ing a worker thread, using `getChanContents` to fetch a lazy input stream, feeding that to zlib, and pushing the results back out the destination channel. Or something equally dastardly with `unsafeInterleaveIO`. I would of course be much happer with a saner high-level streaming API, but at least I can switch to this low-level binding now and avoid the unnecessary IPC overhead (low though it may be with green threads...)
Michael, how do you stream with this api? It seems like it expects the entire input at once and I don't see how you're supposed to feed it more chunks.
You can call withInflateInput as many times as you want. Depending on your use case, the buffering may cause you a bit of a problem, but I'm happy to add a function to bypass buffering. The basic compression approach is: def &lt;- initDeflate ... withDeflateInput def rawContent1 sendCompressedData withDeflateInput def rawContent2 sendCompressedData ... finishDeflate def sendCompressedData You can check out the code in [http-enumerator](http://github.com/snoyberg/http-enumerator/blob/master/Network/HTTP/Enumerator/Zlib.hs) to see how I build an enumeratee on top of the inflation API, or [wai-extra](http://github.com/snoyberg/wai-extra/blob/master/Network/Wai/Zlib.hs) for the deflation API. Note that the former uses the enumerator package, and the latter uses the ugly WAI enumerators.
It would be nice to have that example at the top of the haddock for Codec.Zlib, pretty please :). And thanks for writing this!
I partly agree, and partly disagree. It's definitely the case that a few very large projects - for example, GHC - don't work well with darcs. That said, there's a lot that's appealing about darcs. It's slow sometimes, and certainly a nice graphical interface could be a nice benefit; but the way it does things is just... right. That's an intangible benefit, but one I value quite a bit. In any case, we're fortunate that very little of the Haskell community is dependent on darcs. If I choose to use any version control system at all, I can still upload my projects to Hackage, and others can easily install them. That's a nice change from when I started out in Haskell, and the installation instructions for most packages started with "do darcs get http://..." This also argues strongly for building community features like ratings, comments, etc. into Hackage, rather than going the github route.
I submitted the following response on Bryan's blog: &gt; Bryan, I'm a bit surprised at your tone in this post, &gt; despite your somewhat biased position as a long-time Hg &gt; developer. Why not just report issues that need &gt; improvement? That is the value of this post. I am familiar &gt; enough with the high professionalism of the Darcs team to &gt; know that they will appreciate that aspect of your post, and &gt; they will fix every one of these issues. &gt; You understand better than most that revision control is far &gt; from trivial, and that building a new system from scratch &gt; based on totally new first principles is a huge undertaking. &gt; Extrapolating the rate of progress of Darcs along that road, &gt; especially since Eric took the helm and turned Darcs into &gt; one of the best managed open source projects in the world, &gt; the distance until Darcs becomes a full competitor of Hg and &gt; Git is closing fast. &gt; It's great to hear that Darcs is influencing other revision &gt; control systems, just as Haskell had profound influence on &gt; other programming languages even before it was ready for &gt; production use. Just as the full impact of Haskell is yet to &gt; be felt as a production programming language, the full &gt; impact of Darcs is yet to be felt as a production revision &gt; control system. &gt; Whether or not you continue to "soldier along" is your own &gt; personal decision, of course. And I have no doubt you will &gt; vote your conscience when decisions are to be made about &gt; revision control for projects in which you are involved, and &gt; that is as it should be.
Good point, I keep meaning to add synopsis sections to haddock whenever possible, and don't seem to remember to do so very often. I've added the example from above, plus added a [more complete example](http://github.com/snoyberg/zlib-bindings/blob/master/file-test.hs) to the repo.
Why is this interface better than something like: &gt; initState :: Settings -&gt; IO State &gt; inflate :: ByteString -&gt; State -&gt; IO (State, ByteString) &gt; finishInflate :: State -&gt; IO (State, ByteString) Is there an impedance mismatch with zlib that this would run into? This sort of interface it what I would expect for something I would then want to use in a streamy way,
&gt; Why not just report issues that need improvement? That is the value of this post. I am familiar enough with the high professionalism of the Darcs team to know that they will appreciate that aspect of your post, and they will fix every one of these issues. The issues he brings up are pretty fundamental to how Darcs works so reporting them as bugs make little sense. I seriously doubt the Darcs team would appreciate a bug report saying "patch theory makes poor engineering sense".
Well, the initState *is* basically identical to initDeflate/initInflate, so I don't see an issue there. I also don't see the point in returning the State from inflate and finishInflate: zlib handles all of that via ugly mutable variables that we can conveniently ignore. Now to the heart of things: finishInflate *is* essentially the same type signature you give, but finishDeflate can return more than one ByteString. In that case, there are a few different APIs available: * What I've written: Deflate -&gt; (IO (Maybe ByteString) -&gt; IO a) -&gt; IO a * finishDeflate :: Deflate -&gt; IO [ByteString]. Notice that this no longer runs in constant space. * Providing finishDeflate :: Defalte -&gt; IO (Maybe ByteString), which a user would need to call multiple times until getting a Nothing. You may think that 3 is the better choice, but things get even worse when providing input via the withInflateInput/withDeflateInput. The user would need to call, in correct sequence, the functions to set the new incoming chunk, draining out the output, etc. *And* the user would either need to make sure to call all the withForeignPtr stuff correctly, or we'd need to introduce more mutable state to hold onto ByteStrings for him/her. Basically, the callback API allows the user to ignore the gory details under the surface.
I'm mostly faliling wildly trying to find an interface that looks 'Haskelly' but is still low-level enough to be broadly useful :-) The notion of having a 'popper' function to unroll the fold just feels like something that could be wrapped up in a more natural way, but I haven't thought of it yet. The C-metaphor you're wrapping is a common one, so it would be great to have good Haskell metaphors for dealing with it.
I think the proper metaphor for this is enumeratees to be honest: that's exactly how I wrap it up in http-enumerator. The goal of this library is to be the ugly thing that your beautiful enumeratee wraps around.
yitz, I was very careful to couch everything I said in terms of my own experience, and even to say that I think darcs is a good piece of software. In fact, I wrote this post because Eric Kow asked me to, precisely so the darcs team could take whatever they might find useful from it. I’m not sure what more I could have done to pull my punches and maintain a constructive tone, while also describing why I’ve moved on.
To be fair, it's only patch theory that I've got the fundamental problem with. Making merges easier to deal with would be (I hope) just a matter of focusing more on the UI. The other aspects, e.g. the existence of github and TortoiseHg, are still "just engineering", but they do represent rather more work than you could expect a small team to be able to accomplish in a reasonable amount of time.
I guess one question is whether rebase (which I'm working on at the moment, and has already been tested out by Simon Marlow with some positive feedback) would make a substantial difference to people working with GHC? Another recent improvement I made recently that makes a huge difference to dealing with conflicts (and is so simple I wish I'd done it earlier): including the old text in the conflict marking so you can actually tell what change each side made.
&gt; I seriously doubt the Darcs team would appreciate a bug report saying "patch theory makes poor engineering sense". Affective statements like that are exactly *not* the value of Bryan's post. Nor was I talking about bug reports - there aren't any of those. Here are some examples of points Bryan makes that will be helpful for Darcs: About conflict resolution, Brian said that in other RCSs: &gt; I start a merge by hand, most of it proceeds automatically, &gt; and I get dropped into a nice GUI whenever something funky &gt; needs fixing up. That's a nice goal to aim for, well stated. About history, Bryan describes an extremely useful property of Hg: &gt; I can reconstruct what another developer was able to see when &gt; they were writing code. That might or might not be a good goal. Bryan himself admits in his post that in order to achieve that, quite a mess is created in the repo. However, he says, &gt; The rebase command provided by git and Mercurial provides a &gt; nice balance: history is explicit by default, but can be tidied if &gt; necessary. OK, that is definitely something for the Darcs team to think about. In fact, they have been thinking about those issues quite a lot, but I have no doubt that hearing this perspective from an expert like Bryan is very helpful. Bryan states: &gt; TortoiseHg is the best GUI I've had the good fortune to use &gt; with any revision control tool. It makes committing changes &gt; and browsing history a snap. From time to time, the Darcs team takes note of the fact that TortoiseDarcs is bitrotted, and moves on. I agree with Bryan that it needs to take a more important position in thinking about Darcs. I have seen that issue play a central role in the RCS decision-making process over and over again. And there are other gold nuggets that can be mined from Bryan's post. 
Well, Brainfuck is to C as Lazy K is to Haskell, give or take.
I like that in "git", the primitive operations of "reset" and "cherry-pick" allow "rebase" to trivially be implemented on top. Are you taking a similar approach?
And in that tone, it would be really nice if whenever someone discovered that "Hey, this package over/under-specifies its version deps, it also/doesn't work[s] with this version" -- that knowledge could easily be shared with everyone else, so we don't all debug the same silly build issues.
I don't even understand what git reset does :-) (And I have tried to understand, though not very hard) Also, in darcs cherry picking isn't really a distinguished operation from normal merging or pulling of patches. In effect rebase will use the same infrastructure but it also has to take care of changing the patch identity, something which in git happens on every cherry pick (I believe).
Another darcs contributor here. I have made some [wiki gardening](http://wiki.darcs.net) these days, thus the following sentence rings a bell to me: &gt; After enough bad experiences, I have a subconscious view of a darcs merge as a way to lose bits of work. Maybe the author is referring to [this (horrible) bug](http://wiki.darcs.net/HiddenConflicts), which is solved in the darcs-2 patch format (which exists since 2008). Darcs still has many problems as of now, but it also has problems that existed in the past and are no longer. The problem is that people sometimes still talk about those. Which is understandable, this is how reputation works. So… either we improve our communication, or maybe the not-yet-existing "Darcs 3" should just be renamed to something new if we do not want to hear again about exponential-time merges, etc. I find the rest of the post is very relevant: we definitely need GUIs to make darcs attractive and more convenient, and I should add that "hg serve" is a killer feature of mercurial I wish we had.
In git, unlike darcs (afaik), a single repository has a bunch of branches. I find this feature immensely useful because I often work on projects with very large working trees. I'll explain a bit about git using a bit of C syntax, because it's all about pointers and dereferencing them... Git "branches" are just ptrs to some commit (essentially, a branch is a hash). Aside from "single" ptrs (single as in: `typedef commit *branch`), git also has a "double" ptr called `HEAD` (`commit **HEAD = &amp;master`). HEAD points at a branch ptr. Branch ptr points at commit. `git checkout` is a command that re-assigns `HEAD` to point to a different branch. Then, the working tree is updated to reflect the new position of `**HEAD`. e.g: git checkout my_other_branch `git reset` (specifically with the `--hard` flag) is a command that re-assigns the branch currently pointed by `HEAD`. i.e: assigns to `*HEAD` and also updates the working tree to reflect the new position of `**HEAD`. e.g: git checkout my_branch git reset --hard some_hash The above sequence will set the `my_branch` ptr to point at `some_hash`. To "rebase", all you have to do is: git reset --hard &lt;new-base&gt; git cherry-pick &lt;old-1st-commit&gt; git cherry-pick &lt;old-2nd-commit&gt; git cherry-pick &lt;old-3nd-commit&gt; git cherry-pick &lt;old-4th-commit&gt; ... The `git rebase` command just automates this. `git rebase -i` is one of git's **killer features** that opens up a text editor with the sequence of cherry-picks it's about to apply on the new base, and lets you delete some of them, change their order, or squash them together into a single commit. About your question, yeah, in git all cherry-picks change the identity -- essentially copy the metadata and patch of the commit.
i've used darcs in the past and moved to git. i experienced some of the darcs issues that were probably fixed....but i don't really see any value in moving back...git has real network effects now. to be a success, darcs would have to pull ahead of git and hg in some serious way...xmonad did this. it started out as dwm written in haskell...if it had stayed just that, i probably would have just continued with dwm.
Have you guys seen droundy's [iolaus](http://github.com/droundy/iolaus), which aims to provide a darcs-like experience on a git repo by using multiple HEADs as a bag of patches (or patch streams)? This seems to be a very attractive model which implicitly nails down ancient history but keeps current revisions quite dynamic. Not that it's ready yet... and it's written in go, after he abandoned one written in Haskell. 
I've accepted that the current interface is unsuitable for the block-oriented approach you want. It's also of course fair to say that I've not yet implemented the API that I've suggested.
But you cannot wrap a pure interface around it because it is in IO already. The individual actions do not change the world, they mutate a single mutable variable (the zlib stream state). In other words it'd be safe to wrap zlib in the ST monad and then sequences of actions can be done purely.
Isn't [patch-tag](http://patch-tag.com) the (less developed) darcs answer to github?
It would appear that all the black magic in your zlib package's pure interface lies in calls to unsafePerformIO (hidden by the run function in Codec.Compression.Zlib), so it's a little disingenuous to say you can't build a pure interface on top of something living in IO. It might be an interesting exercise to get this stuff to live in the ST monad, but calling out zlib-bindings to letting things live in the IO monad as they naturally do is a little strange.
&gt; I'm mostly faliling wildly trying to find an interface that looks 'Haskelly' but is still low-level enough to be broadly useful :-) So if our decompression was really genuinely pure we could use a nice API like this: data Decompressor = Fail ByteString Exception | NeedInput (ByteString -&gt; Decompressor) | OutputAvail ByteString Decompressor | EndOfStream ByteString decompressor :: Decompressor You'll note that this is very much like attoparsec. We can just unfold the data structure and we are in control at each stage. You could implement streaming on top of this no problem. For the simple lazy pure API it'd just be: decompress :: [ByteString] -&gt; [ByteString] decompress input = unfold input decompressor where unfold in (Fail trailing err) = raise err unfold [] (NeedInput cont) = raise unexpectedEndInput unfold (in:ins) (NeedInput cont) = unfold chunks (cont chunk) unfold ins (OutputAvail out cont) = out : unfold ins cont unfold _ (EndOfStream trailing) = [] -- ignore trailing However since our decompressor internally uses mutable state then this API is not safe, we cannot call that embeded continuation multiple times. So the next best thing is to stick it all in the ST monad. This lets us still implement the pure version but we can also lift it into IO and use it in IO in a block-streaming style. data Decompressor s = Fail ByteString Exception | NeedInput (ByteString -&gt; ST s (Decompressor s)) | OutputAvail ByteString (ST s (Decompressor s)) | EndOfStream ByteString decompressor :: ST s (Decompressor s)
dep versions should be discovered automatically.
There has to be a transition at some point, the question is where it happens and who checks if it is correct. Yes, my current implementation uses unsafePerformIO in the run function but I've also checked that that is ok (beause I check all the things that are lifted into the monad that we later run unsafely). If the lib exposes IO functions how is the user to know what is ok? And yes, we can do better than my current implementation. We could make all the primitives live in ST and then the user can take those low level functions and use them in a safe way without using any unsafeFoo. &gt; It might be an interesting exercise to get this stuff to live in the ST monad, but calling out zlib-bindings to letting things live in the IO monad as they naturally do is a little strange. Actually I think it is a fair criticism. Given just those low level IO bindings, you cannot reasonably implement the pure interface. It is not reasonable for a user of the IO level bindings to know the intimate details of the internals so that they know what is and what is not "safe" to unsafely run, so they can get something that is pure (and I'm sure we all agree that morally (de)compression is pure, it's just the C APIs that are not). Yes I provide a pure interface in the zlib package but that is because the pure stuff is in the same package, so it knows all the details of the implementation.
The substantive issues you raised are good ones. You imply in your post that they can't be fixed because of patch theory, and here on reddit you claim that it is because the team is too small. I believe the Darcs team will prove you wrong on both counts. 
I agree, I'd love for explicit import/export signature to replace the horrible PEP/versioning stuff.. but until we get there (dcoutts says it won't be any time soon...) we ought to at least be able to share our debugging
Also, since branching is so cheap/easy, I create a lot of local branches for working on different features/fixes/etc, and then merge them into master before pushing to svn ( git-svn is the only way to use svn! ). The reason cherry-pick changes the id is the hash id of the commit is based on the parent commit as well as the data in the current commit. So it shows the lineage of the commit, you can't forge a commit tree history.
what do you mean by "explicit import/export signature"? what I want is a tool that will try to 1) typecheck (easy+very fast) installed package with every possible dependency suit combination (the easy variant would assume that the dependencies are independent, correct approach would probably result in a huge search space. anyway some graph version of binary search would nice for this), or 2) compile+run tests (harder), and spit out a list of deps that will satisfy all needs. it should probably also keep a cache, so when run a week later to see if any new deps would qualify, it doesn't repeat all the work.
My use case in zlib-bindings is *not* for a pure interface; it's for streaming. In fact, providing an ST-bound interface would be the wrong route for this: try implementing the file-test.hs file using an ST interface. My point remains: it's entirely possible to build a pure interface around what zlib-bindings provides. That a user would need "intimate details" to build such an interface is perhaps a bit over-reaching, but even so the point is *zlib-bindings isn't trying to replace zlib*. Each package is providing for a different use case. If you have a concrete suggestion of how using ST, or how using something besides callbacks, could improve this library for its given use case, I'm happy to hear it. Unfortunately, all I hear right now is "this should be done differently because it should be."
Fair enough. So my plan is to implement something in the zlib package that covers the streaming case and allows the existing pure API. I'll check it against reality with some example server application.
I mean that when I import packages, it will list all the fully-qualified names I use and also all the types I've used them as. This will become my "import signature". If figuring out the import types automatically is difficult, it is ok to have manual intervention, but it'll probably be ok. A package that has a list of exported modules, each of which has a list of exported names, each of which has a type. This, also as a fully qualified typed name list, is the "export signature". If I explicitly depend on some semantic, and not just on typed names, I could always add an explicit "flag" (fake name or just have import signatures support "flag" deps) into my import signature. Then, I would like "cabal-install" to find potential implementation combinations that have the export signatures that would cover my import signatures completely. This will take "programming to an interface" to its conclusion, rather than what we have now, which is "programming to an implementation". We'd still have dates on implementations, so we prefer to choose a newer implementation that implements an export signature, but we can probably do away with version numbers completely. What you suggest is sort of an implicit test of the above, iiuc. EDIT: The above approach would also allow for things like an mtl-transformers bridge that gives an MTL interface from a "transformers" implementation -- and no magic is needed, all MTL-using packages will just get to choose this bridge rather than the mtl itself to implement their import signatures.
Cool. I'll happily deprecate zlib-bindings if zlib gets an interface that supports my use cases.
&gt; What you suggest is sort of an implicit test of the above, iiuc. if I understand you correctly - you want to recreate haskell types at the cabal level and ask cabal to do the same search as I suggested and cabal-typecheck every combination.
Just as a proof-of-concept, here's a pure API built on top of the current zlib-bindings API. decompress' :: L.ByteString -&gt; L.ByteString decompress' gziped = unsafePerformIO $ do inf &lt;- initInflate defaultWindowBits ungziped &lt;- foldM (go' inf) id $ L.toChunks gziped final &lt;- finishInflate inf return $ L.fromChunks $ ungziped [final] where go' inf front bs = withInflateInput inf bs $ go front go front x = do y &lt;- x case y of Nothing -&gt; return front Just z -&gt; go (front . (:) z) x compress' :: L.ByteString -&gt; L.ByteString compress' raw = unsafePerformIO $ do def &lt;- initDeflate 7 defaultWindowBits gziped &lt;- foldM (go' def) id $ L.toChunks raw gziped' &lt;- finishDeflate def $ go gziped return $ L.fromChunks $ gziped' [] where go' def front bs = withDeflateInput def bs $ go front go front x = do y &lt;- x case y of Nothing -&gt; return front Just z -&gt; go (front . (:) z) x Edit: and these are in no way replacements for Duncan's compress/decompress: his use lazy IO to avoid reading the entire bytestring into memory, mine do nothing of the sort.
Well, that is one possibility for matching import signatures to export signatures. But it would still be different to have those signatures explicitly. Another possibility is for all imported names' types to be specified explicitly. Of course you could ask whatever cabalizer you're using to borrow the types from the current implementation you're using. That way, cabal wouldn't have to type-check, just match exact name/type pairs. That would mean that if an exported name is generalized, though, it would have to get a new name -- which is reasonable because over-genericity can also break code through ambiguous types.
I think we want to solve different problems - you want to import types/values from different packages (mtl etc.), I have a problem with under-specified dependency bounds that result in compilation errors (for the lack of testing), and over-specified bounds, that result in manual attempts to install a package with a new ghc and then notifying the maintainer about the success (and he doesn't respond). I'm very unlucky with software, and this happens to me very often (mostly with gentoo, but that's because I don't use much haskell software)
I don't understand -- why wouldn't export/import signatures be an exact (rather than over or under) specification of a dependency?
the previous comment was a bit off topic. I think we want the same thing: I'd put more work on the ghc (and its typechecker), you'd prefer to help it by providing cabal with the important parts of packages (and their interfaces). so if your solution wins, and it's easy to declare those signatures - I'm sold.
I'd say the dependency field should (as a stop-gap) be editable by non-maintainers, and without the necessity for version bumps. No-one gets the dependency field right at first anyway.
Roman's primitive monad stuff is your friend. Lets people embed your code in IO or ST without fuss, although performance in ghc &lt; 7 is poor.
&gt; the way it does things is just... right. That's an intangible benefit, but one I value quite a bit. This sounds really KoolAid-drinky. I've made similar statements about Git, but I think I can back them up with concrete arguments about its design philosophy, data model, etc. Can you elaborate on what makes Darcs "right"?
Is there a way to create an account without being sucked into Google/Yahoo/FriendFace/OpenID? 
I think this is the solution that everyone wants.
I took a course on declarative languages during a part of my undergrad in England. It covered haskell and prolog, and was taught by 2 teachers who did research in programming language theory and such. In the university where I'm currently doing my PhD in the US, it's not taught, but is used in some research projects.
Haskell was the product of studying a subject. That subject is functional programming, notsomuch haskell itself although the study of Haskell in relation to advancing the state of the art in functional programming was certainly valuable. Here is the program for the 2010 ICFP. Try having a read and see if there's anything interesting to you. http://www.icfpconference.org/icfp2010/program.html There are plenty of papers to read... maybe you can look at citeulike.org to see what other people are reading.
read acm and icfp? even if you don't use haskell at work, i'm sure you could write the occasional one-off tool here and there 
Have you heard of this book? http://www.haskell.org/soe/
I am currently in an undergraduate level Haskell course in the United States. The professor is fairly well known in the Haskell community.
Haskell used to be the first language learned at UNSW, Australia. Now the students learn C but AFAIK some subjects still use Haskell. 
 &lt;interactive&gt;: /dev/urandom: openFile: does not exist (No such file or directory) 
Damned! I was under the (evidently false) impression the os(windows) flag combined with CPP compile time options would work. Thanks, I expected it to get further than this. 
"if os(windows)" should work ok, I've used it a lot in cross platform cabal files in conjunction with defines. I noticed that you use cc-options instead of cpp-options in your cabal file, maybe that's the problem? When I change it to use "cpp-options: -DisWindows" then I get a compile error: [3 of 8] Compiling System.Crypto.Random ( System\Crypto\Random.hs, dist\build\System\Crypto\Random.o ) System\Crypto\Random.hs:42:0: parse error on input `import' So it looks like it's actually getting in to the "#if defined(isWindows)" now.
&gt; Damned! I was under the (evidently false) impression the os(windows) flag combined with CPP compile time options would work. That would work, but you are not using a CPP flag: if os(windows) CC-Options: -DisWindows Note that's "cc-options", not "cpp-options". The cc options go to the C compiler when compiling C code. The cpp options are used for pre-processing Haskell code.
&gt; **Lines can be terminated with carriage return (U+000D), carriage-return/line-feed (U+000D U+000A), or line feed (U+000A)**. Tab characters (U+0009) are equivalent to eight spaces. **A blank line** (which has a syntactic meaning to be described later) **is defined as two consecutive end-of-line sequences** possibly with white space between them. So what do you do with CRLF? Is it a line ending with a CR and another ending with an LF, or one line ending with CRLF? I'm presuming you presume that both lines would want to end the same way, like if you hit enter twice on the keyboard, but I've seen files where two people have used two different editors, and different lines can have different terminators.
I'm not sure I understood the solution, but specifically the "we can detect the race" slide was confusing to me. I understand that the race between a thread working on a thunk that is then blocked, and another thread that works on the same thunk is possible to detect *in most cases*. However, how can we avoid a situation where two CPUs create a BLOCKING_QUEUE at the same time? It seems that this can't be avoided unless an instruction similar to cmpxchg8 is used. How can detect that another thread has written to the thunk? Reading the thunk and checking before writing leaves a window open (between the read and the write) where the two-thread situation isn't detected. I didn't understand whether avoiding the race is critical or just nice from a performance perspective so I don't really know whether detecting 100% of the races is necessary. 
Formula language is fairly similar to Excel. I was expecting Haskell to be the language used. It would give a lot of power that Excel doesn't have.
As noted by a JVM developer who sold their JVM to Oracle: High performance on the server is all about handling strings efficiently, doing IO efficiently, and abstraction (framework) collapsing. Doing Strings efficiently *is a million $ issue* for businesses and is *a key enabler* for doing anything on the server side.
Yes it is ambigious. The standard interpretation is to treat CRLF as one line ending rather than two. That way you can handle mixed line ending files and get it right most of the time. -- | Fix different systems silly line ending conventions normaliseLineEndings :: String -&gt; String normaliseLineEndings [] = [] normaliseLineEndings ('\r':'\n':s) = '\n' : normaliseLineEndings s -- windows normaliseLineEndings ('\r':s) = '\n' : normaliseLineEndings s -- old osx normaliseLineEndings ( c :s) = c : normaliseLineEndings s 
Nice catch. That is a bit awkward. I suppose you could look for two same consecutive EOL tokens from the set {CR, LF, CRLF}. (Would that work or have I just moved the corner case elsewhere?)
That wouldn't work: I bet that "123\n\n456" is supposed to be treated as containing a blank line.
You can record the flavor of the first line terminator you find and check subsequent terminators against that flavor, and raise an error or warning on inconsistent line endings.
Is `\n\n` not two consecutive LFs? Or am I misunderstanding you?
They were writing this just a month after I started learning Haskell. So at the time I was still getting my head around `foldr`, `foldl` and the dreaded foldr fusion rule (no not the simple automatic one, the general one with complicated side conditions that is excellent for making tricky exam questions).
It has a lot to do with avoiding unnecessary complexity. Doing stuff with darcs involves fewer different commands than doing the same things in git. That's because everything is just what it is: a copy of the source tree is a copy of the source tree -- not potentially many branches in one with special-purpose commands set and option syntax for creating and managing and moving changes between branches. A patch is a patch, and stands on its own as such (okay, it has dependencies, but it has the minimal set necessary to apply it to other copies of the source code). Yes, these things are more complex in git because getting the darcs way to run well poses some difficult programming problems... I understand that. But I personally just don't run into those problems often. It's worth it, to me, to use software that is designed first and foremost to keep things simple and intuitive, and then to tackle the difficult problems while preserving the simple and intuitive model of what it's doing. And the really nice bit is, the darcs team is making a lot of headway into getting darcs to run a lot better on larger scale projects, too, so chances are that somewhat larger projects will be equally doable with a small, consistent set of commands in the near future. I certainly wouldn't argue that the Linux kernel should move to darcs, and I understand the motivation for GHC to move away from darcs, too, if they decide that it would be for the best. But for the projects I'm working on, right now, darcs is the better piece of software.
Also, are tab characters really equivalent to eight spaces? Normally they move to the next tab position, so they are equivalent to 1-8 spaces.
Yes, this is what i consider the tedious part. The opaque part is that it's quite hard to find out what is going into the next release. As i see it both HP developers and HP users have a hard time.
No, I was misunderstanding you! So yes, I agree that the same line ending, twice in a row would be a blank line... It still doesn't make clear how LF CR LF should be handled: probably this is still a blank line, and not two blank lines.
Oops, thanks for the catch dcoutts.
Alright, thanks. I've fixed that by adding FFI to the pragma. This is frustrating enough that I'll give kvm another go with on my my windows discs.
Weird, I actually did this exact same thing myself a couple of months ago.
John Meacham has a good explanation of this technique at http://notanumber.net/archives/33/newtype-in-c-a-touch-of-strong-typing-using-compound-literals with code samples and generic macros.
I wanted to do it but worried it would be cumbersome. I guess it isn't!
You are wrong on the subject of block scope, see "let definitions" https://developer.mozilla.org/en/New_in_JavaScript_1.7 Also there's shorter notation for functions: https://developer.mozilla.org/En/New_in_JavaScript_1.8 You can use very simplistic syntactic conversion tool to enhance it even further: http://jashkenas.github.com/coffee-script/ It might hinder debugging, though. Plus even if there is minor annoyances, you can mostly just avoid them.
I've done this in complex C code before. It was indeed nicer than without.
&gt; You are wrong on the subject of block scope, see "let definitions" https://developer.mozilla.org/en/New_in_JavaScript_1.7 &gt; &gt; Also there's shorter notation for functions: https://developer.mozilla.org/En/New_in_JavaScript_1.8 Neither of which are part of the ECMAScript standard, and are only available in Firefox. In other words, unusable for the web. &gt; You can use very simplistic syntactic conversion tool to enhance it even further: http://jashkenas.github.com/coffee-script/ CoffeeScript is awesome, but switching to that would be a huge change. At my work, we rely on the Closure compiler heavily and CoffeeScript won't play nicely with that. &gt; Plus even if there is minor annoyances, you can mostly just avoid them. I'm not sure how I could avoid JS's crappy function syntax of variable scoping.
Can you explain this a little bit more? You state: "As for patch theory, my principal point of discomfort with it is that I have little idea what was in someone's repository when they created a patch." Can you elaborate on this? It does not seem like a fundamental limitation - it seems you could record the exact repository state along with each patch.
Yes, that's an ideal binding to zlib as written in Haskell. I guess it wouldn't be hard to write the equivalent sprinkled with 'IO' annotations. It would miss being able re-use the memory passed into C-land, though.
No, that didn't seem to be important to anyone during the planning stages. There are *lots* of OpenID providers out there though; is this really a problem? If so, I can consider adding support for username/password logins as well.
Reposting my reply. Thanks for writing this up. First, if you’d like to migrate from Darcs, you could think about cabal installing darcs-fastconvert and playing around with the fastexport/fastimport support. To (very partially) address your principle point of discomfort, you can see exactly the state of the repository that somebody else wrote their patch in by using a context file. darcs get repo –context /tmp/the-context-file The question here is how to obtain these context files in the first place, and that’s where I have good and bad news. The good news is that patch bundles sent via darcs send double as context files, so if you somebody sends you a dpatch file and you pass it to darcs get, you see exactly the pristine state (and set of patches) that the bundle was built on. Now, this isn’t as useful as debugging gold as having an explicit merge history. It does the job if you’re about to apply somebody else’s patch, but what if you want to look back to a patch you applied three weeks ago? There’s where the bad news comes in. You either have to hold on to the email and the patch bundle, or you have to ask the submitter to make you a new context file with darcs changes –context. We’ve been discussing a future “patch annotation” feature that allows you to associate optional and perhaps free-form metadata with patches. The sort of metadata I have in mind would be things like “who signed off on this patch?” and “what was the original context of the patch?”, and “when was it applied”. This is the sort of thing that history-based DVCSes do naturally, but that Darcs with its emphasis on the set-of-patches view does not. But we could come to this from the patch-annotation angle. If other DVCS could somehow add a notion of higher-level patches to its commits, the two worlds could converge.
Hang on, is it really patch theory you've got the problem with, or is more the lack of a history of operations (commits X is built by merging Y and Z)? If it's really just the latter problem, what if Darcs found a way to accumulate such a history in the form of [patch annotations](http://bugs.darcs.net/issue1613)? The reason I'm asking is if your beef with Darcs is really fundamental, or if it's something that can be recast in a "just"-a-matter-of problem. Now, I confess I haven't put a lot of thought into how a patch annotation scheme could work and how we would use it to represent repository history. But the rough idea is that Darcs patches could be associated with optional data which wouldn't affect core Darcs operations, nor get in the way of its cherry picking. My thinking is that this sort of data could be used to get back at the kind of history trad DVCS users appreciate while preserving the fundamental flexibility of Darcs. Indeed the history could even say things like "X patch was re-ordered on top of Y patch on this date", or "the context that X patch was originally recorded on is Ys", and so forth, or "Bob was the person that pushed this patch to Y repo" &gt; Making merges easier to deal with would be (I hope) just a matter of &gt; focusing more on the UI. We have a lot of work in store for improving the conflicts-handling UI, which is what I think is the painful part about merging.
after one month of haskell you were interested in foldr fusion rules? I feel even more dumb now...
It'd be nice if the compiler would just issue a warning when typedefs don't match, but I guess that's not how C works. I've indeed used similar tricks in the past (e.g. using enums intead of ints merely to help type checking) but never thought of using single-valued structs.
Perhaps the JS hate in the comments is not completely surprising, but of course the whole point is that I guess now you could write a full web application in Haskell, which is a pretty cool prospect.
Thanks for these tutorials. Enumerators are a very interesting topic and these tutorials were a great introduction to it.
The reason that paper languished for so long was that we didn't come up with a good way to solve the thunk locking problem (section 4.3 in the above paper). The solution was pretty simple in the end - add an extra field to every thunk - but at the time we thought that it might be too expensive to go down that route. More details in [Haskell on Shared Memory Multiprocessor](http://www.haskell.org/~simonmar/bib/multiproc05_abstract.html).
It was the first year CS course, not my explicit choice. And we probably didn't get to foldr fusion until near the end of the first term.
What are the goals of haskellers.com right now anyways? 
I did log in via OpenID, but I ended up in a perpetual login loop. I see no way to update my profile at all :-( (btw, I am user/318 [and via google 319, but I disabled that by "report user" as it is equally unresponsive]). What did I do wrong?
This exact activity seems to be covered in detail [here](http://cdsmith.wordpress.com/2010/03/15/type-level-programming-example/)
wow, thanks. 
If it is just compile time programming you want then use Template Haskell not the type system. Here is an example that uses 'filter' and 'even' from Prelude to build a tuple (2,4,6,8,10) at compile time. evens = $(tupE (map (litE . integerL) (filter even [1..10]))) Just write 'isPrime' in plain Haskell and replace 'even' with 'isPrime' (the definition of isPrime must be in a separate module from its use in TH).
So right on. Happstack's RqData always seemed to me a gratuitous abstraction at best.
It's a fair question. I personally have my reasons for creating the site, but I would not want to set in stone anywhere that these are the goals of haskellers: I figure that people will do with the site whatever they want. Anyway, my goals are: * Provide a single place for Haskellers to create professional profiles. * Allow some level of social interaction between Haskellers. This "some level" is very vague, as is the "social interaction." I consider putting up geolocations an aspect of this, one that I hadn't planned on but was community-suggested. * And perhaps most importantly: allow employers and businesses in general to see that there are a large number of highly qualified Haskell programmers, to hopefully break through the imaginary hurdles to its adoption. I'd be interested to hear what everyone else would want as goals for haskellers.
Thats not a heisenbug :-)
None of the websites I have a login for and login to regularly (ie more than once a month) use OpenID. Proper username/password login would be much appreciated. 
The lambda-bind logo would look good in reverse video, too; white-shifting it a little bit won't hurt it at all. One way or another, we find our products listed on sites -- like Heroku's -- where white-on-black is the order of the day.
I put this question in the haskellers survey. I'll have to prioritize this request with other feature requests I get from there. Of course, the project is open source, so a patch for this functionality will expedite matters.
(meta) Sigh - Why doesn't pipermail wrap long lines when displaying archived messages?
Interesting that it uses wx. Any particular reason for that choice?
[Greasemonkey to the rescue!](http://utr.dk/~ulrik/greasemonkey/pipermail_wrap.user.js)
Here's a workaround: javascript:(function(){with(document.getElementsByTagName("pre")[0].style){width="40em"; whiteSpace="pre-wrap";}})() (Tested only in Firefox 3.5)
This is some great design work! Even the contrast colors "fit" together. I really like it.
Bikeshedding. Yawn.
The problem with "bikeshedding" isn't that bike sheds shouldn't be painted nicely, it's that people tend to argue about the color so much that the shed never gets built. This is more someone offering free cans of paint to people who are already building sheds. So it's pretty much the exact opposite of bikeshedding.
I have a few happstack sites (such as happstack.com) which run for hundreds of days with out issue. The #1 reason for restarting is due to upgrading to a newer version of the site. (Though now that hsplugins is alive again, maybe that can be avoided)
happstack used to support recompilation and dynamic reloading of components. We had to drop support when hsplugins stopped being supported. But now that hsplugins lives again, that feature will definitely be making a comeback. 
No way! Must. Read. Now. :) Thankyou.
There is plenty of room for improvement, but hopefully people will find it useful.
I've put this into my frankly, disgustingly large stack of Amazon books that I'll one day have the money to buy. Well done man; between LYAH and RWH, getting into Haskell became much simpler than I first thought!
Sure you're not looking for guards?
Loving the new reddit and stackoverflow sections. Do they now replace the links to the blogosphere directly? (If so, that's fine). One minor issue: "Now or Updates Hackage Packages" should of course be New or Updated
Despite what it says there don't seem to be any comments on the reddit article about purifying HTML.
that's because the commenters did the right thing and commented on the [authors post](http://blog.ezyang.com/2010/10/the-html-purification-manifesto/), not on the reddit submission linking to the post.
I haven't been reading haskell-cafe and didn't see the 'case of' announcement. That would be wonderful! I'll add my agreement. (As far as syntax, 'case of' is the way I've found myself wishing for it, but I'd adjust to anything.)
Nice work Mark.
Ah, thanks for catching the bug. That should have said "scored 15 with 0 comments" instead of "scored 15 with comments". Missed the 0 case.
I can still include a section with posts from planet haskell during the time period. My reasoning for not doing so is because posts from planet haskell end up in reddit anyway, and here there is a sense of what others think is important. Does that make sense? Does 15 reddit and 5 stackoverflow seem ok? Would different amounts make a difference?
Google Scholar is a great resource. I look at how many citations a paper has received. Also look for literature surveys. These are intended to provide exactly the guide you need. Much later, after you caught up in an area, you can search for the latest papers. 
I wrote a reply to this post: http://happstack.blogspot.com/2010/10/is-rqdata-monad-still-needed.html summary: 1. happstack in darcs actually doesn't require the RqData monad anymore. You can use 'look' in ServerPart as cdsmith correctly suggests would be nice. 2. but if you choose to use the RqData monad you can get better error reporting. The more useful variant is actually the (optional) RqData arrow, which I will blog about later.
Wow, I'm really pleased that you got HWN back. I'll keep you in mind if I ever need to hire someone for Haskell work. 
I'll need more details that "perpetual login loop." Also, next time you have a bug, please email me instead, I only accidently stumbled upon this comment.
What if this HWN post on reddit gets over 15 points? Will it be featured in the next HWN, too? ;-)
Tab characters are a good way to cause havoc in a heterogeneous development environment
On a related note, mathematicians overload function application syntax (juxtaposition) heavily. I came across this expression in a homework: `D(f^-1)(f(x))(R^n)`. There are a lot of things going on here: D(f^-1) produces a new function, which gets applied to `f(x)`. That gives you a matrix. Then this matrix is multiplied by a set: R^n. This means multiplying by each element in the set. This makes me wonder about the capabilities of programming languages for this kind of thing, for example applying a function to a set applies it to every element in the set. And: a matrix of functions is a matrix valued function. Is there a good way to add this to a programming language while avoiding ambiguity?
Hello Michael. First thank you very much for editing this website, I love it. I am one of the 51% who think a redesign may help the site look more professional. I have no skills in design so I cannot help but I am sure someone will. In the meantime, I suggest that you change the logo for the regular one (the grey one, the one that won) and you remove the drop shadow under the "haskellers" word. To me, those don't look very professional and this is something you can do without waiting for a whole design proposal. Of course this is purely subjective and this is just a suggestion :) Again, thanks for the hard work!
I guess I can either include it and make some noise about it ;), or quietly filter it out. I was leaning more towards the filtering option...
Glad you enjoy seeing it back! Do keep me in mind :)
how will the theme be licensed? same license as ghc?
Nice, I was just in need of a new color scheme :P Here's the hex codes (Ordered left to right), to save anyone else the trouble: 3c4569 506982 a3b5ba 353948 b84848 e5eef9
Hmm, OK, but the news feed shouldn't conquer the entire front page. If the main point is seeing real Haskellers, the front page should be dominated by real Haskellers. And by how to find out more about them, and how to find more of them, and most importantly, how to find out what this site is all about. The news blog should occupy at best a few headlines and tiny synopses in a little box or side bar somewhere.
The news feed is a single line on the homepage. Or is it rendering badly in your browser?
Well, if yitz was anything like me - I thought this was a link to the homepage. There was no clear indication to say otherwise. Only after clicking around the menu bar, I realized I was wrong - verified only by looking at the URL in the location bar.
Is the confusion coming from something on the site or the link from reddit?
I was not confused if that helps any.
A little of both. It was combination of me having a lapse of judgement, possibly from reading yitz's comment, and nothing on the page to tell me what I'm looking at. Navigational hints like say tabs or a header might help for the latter the problem. The former problem is mine.
should be creative commons, or better yet, the haskellwiki license (simple permissive: http://haskell.org/haskellwiki/HaskellWiki:Copyrights)
i'm way late on this post because I've mostly given up on using Haskell ... but I'm super interested in seeing the source. My biggest problem with Haskell is how much of a pain it is to manage state for software that does a lot of state - like games. Seeing a good example on how someone else did it would be great. i was going down a path with two StateT tranformers .... such a pain. Might as well use c++.
As a beginner myself I experienced the same things you did. With time things started to feel more natural and, although I still have a hard time grasping some of the advanced concepts in Haskell, I am pretty confident about using Haskell for small to medium sized projects. I think you should try to do your next tool in Haskell and see how that goes :)
My experience too. I'm still pretty much a beginner, but after lots of reading and playing around, I do understand monads and am starting to 'get' ideas from reading the type, which was something I had a lot of trouble with at first. (I would always skip over the type definition and read the explanation, but now I'm starting to actually glean something about a concept from reading the type..) I'm using as many excuses as possible to learn Haskell although unfortunately these are further and fewer between than I'd like. What's encouraging though is that I do always feel like I know a little more every time I do something in it.
Thanks for sharing your experiences! It's always interesting to read these articles, especially when they're balanced and thoughtful. &gt; the functional programming paradigm really is different (duh), and it took me quite a while to wrap my head around it. Yes, and Haskell is very different from vanilla FP. Laziness, purity / first-class IO, and type classes are unusual features among functional languages. I think you can power through with Haskell, but if you're really overwhelmed, you could learn some Scheme or SML. Both are excellent languages for learning core FP concepts; SML will also introduce the fundamentals of Haskell's type system without all the bells and whistles. Or spend some time using first-class functions in a familiar language — maybe Python, Ruby, or Javascript. &gt; the strong typing in Haskell drove me nuts at first, and I spent a lot of time with type errors... the various websites told me to expect this, but it was still very frustrating. Yup. Writing correct code is hard in any language. Haskell makes it also hard to write incorrect code. :) &gt; OK, I’ll admit it, I still don’t really understand monads Try not to worry about the M-word when you're just starting out. "Monad" is just the name of a generic API that's used by many unrelated types. You can get comfortable with some specific types — `Maybe`, `[]`, `IO` — before understanding how the API generalizes. If you specialize the type of `(&gt;&gt;=)` to a specific monad, it's usually obvious what it does. For example, for lists we have (&gt;&gt;=) :: [a] → (a → [b]) → [b] which is recognizable as `flip concatMap`. &gt; the Haskell mailing lists have a wealth of information and there is a great sense of community. Yeah, the community is great. (It's only a little self-serving of me to say that!) It's split between here, mailing lists, Stack Overflow, blogs, and IRC. So do look around if your questions aren't being answered in one particular venue. &gt; Having said that, there really aren’t that many Haskell programmers in Vancouver I hear there's a concentration of Haskellers in Portland, OR. Maybe you can lure some of them up Canada way. :)
For sure I will. Your work with HWN is really important -- let me know if you need any help.
what's 'FM'?
Formal methods?
Frequency Modulation. He probably meant that there's a new FM station broadcasting on 88.3MHz in Portland. Listen in for All The Latest Coq Hits.
A Haskell Podcast would be nice :) But maybe it's a reference to the Haskell File Manager: http://michaelcdever.com/?cat=5
Just wanted to say thanks for this note: &gt; For example, for lists we have (&gt;&gt;=) :: [a] → (a → [b]) → [b] which is recognizable as flip concatMap. I've been learning haskell on and off for a while now, and this took me a while to get, but it's really helpful thanks. 
Thanks for the comment... I'm going to stick with it. I am using Maybe and IO types, so I'm getting some exposure to Monads already, and there are some pieces of my code that I'm really pleased with how much I can get done very simply.
*Less talk, more proofs. 88.3 Coq FM. Your only station for the best proofs from the 70s, 80s and 90s*
Seems promising...
I miss two things from inf-haskell (compared to regular ghci): * tab-comletion (ghci one, not emacs) * C-j = &lt;RET&gt; are those doable in your mode?
...isn't coq written in ocaml?
Certainly. Many of the theorem provers originally developed in the 80s and 90s are written in ML dialects.
I think one of the tricky parts, is perhaps understanding that: (&gt;&gt;=) :: [a] -&gt; (a -&gt; [b]) -&gt; [b] is the same as: (&gt;&gt;=) :: [] a -&gt; (a -&gt; [] b) -&gt; [] b And then that you can replace the [] type *constructor* with a type *variable* `m` to get: (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b With all the syntax sugar in the way, the 1-to-1 mapping is less clear.
This is not really the place to post a question. In the right hand nav-bar there you will see a link that says "Ask a question on Haskell Stack Overflow" and please ask it there in the future. :) As for the actual question I am not sure but there are some nice video resources on the Haskell Wiki that you might want to look at and also some good category theory videos on youtube. Plus there are some good articles in 'The Monad Reader'.
Wow, bitchy comment, yay.
Bitchy? Really? I was aiming for factual and straightforward but I guess perception is everything so I apologise to zzing; I only meant to give an honest response not be snarky or bitchy. I do believe that Stack Overflow (or even the IRC channel) are better suited to asking these types of questions than Reddit; they are question asking communities whereas Reddit is more of a news feed. Especially when the Haskell reddit is treated by many as their daily Haskell news feed: "Daily news and info about all things Haskell related" Thats all I was getting at so sorry again; I'll try and be clearer with my intent next time.
If you're starting from Haskell, a gentler introduction to type theory might be Benjamin Pierce's book, Types and Programming Languages. It'll likely feel math-y enough to satisfy you, but at the same time it deals with type systems from the perspective of things that at least resemble real-world programming languages. It'd definitely be a great idea to spend some time with that book. If you want to get even more math-y than that, your next stop is to learn about System F, and to come to some grips with the Curry-Howard correspondence. *After* making sure you understand untyped lambda calculus fairly thoroughly (i.e., not just have read the definition, but are familiar with basic programming techniques and Church encoding of data), the most helpful book I read at this point was Proofs and Types, by Jean-Yves Girard. Very likely a lot of it will be a mystery, at least on first reading, but it does teach System F, and lots more besides, and gives you a sort of taste of what more formal type theory feels like. Definitely just skim what you don't understand for a first pass. Category theory should probably wait for later. Yes, a lot of the terminology and abstractions you see used in Haskell types come from category theory, and yes it is important, but ultimately you can understand a lot about Haskell's types before bothering to note that they form a category with Haskell functions as arrows. So, yeah sure, keep this in mind, but don't confuse category theory with type theory.
I have [Categories for the Working Mathematician](http://www.amazon.com/Categories-Working-Mathematician-Graduate-Mathematics/dp/1441931236), but it's definitely graduate level. I've heard that [Arrows, Structures, and Functors: The Categorical Imperative](http://www.amazon.com/Arrows-Structures-Functors-Categorical-Imperative/dp/0120590603) is great for beginners, but it's out of print.
TAPL also contains working code, so you know it's not about a magical fairy world where morphisms fly around.
I would reply that with the information I have just received on this post, if it had been me posting that information it would have been information and not a question. But somebody doing a search would find the same information as this question provides, so they are essentially the same. But I do get your position.
All IMO: First, you need to understand the lambda-calculus. Get "Lambda calculi for computer scientists" by Henkin. That has a little bit on types in it, too. For a more in depth treatment, the "bible" is Barendregt's encyclopaedic book. Next, read Lectures on the Curry-Howard isomorphism, which takes you from the simply typed lambda-calculus all the way to pure type systems and the lambda-cube. Girard's (translated) lecture notes also nicely cover System F and his normalisation proof. Those two cover type systems from a more mathematical perspective. For a more pragmatic introduction (i.e. type theory as applied in the study of programming languages), read TAPL. If you're interested in category theory as applied to programming languages, there's either Pierce's "Category theory for computer scienists", or Awodey's free lecture notes (which doesn't suffer from Mac Lane's habit of pulling every example from obscure branches of mathematics).
Many of the newer ones are still.
Here are some papers on the topic. They may be a bit more dense / less tutorial than a book, but I've found that once you've struggled through this you'll have a deeper understanding of the concepts (for me learning something new/difficult can be a no pain no gain situation): * http://www.cs.tufts.edu/~nr/comp150fp/archive/phil-wadler/class.ps * http://www.cs.tufts.edu/~nr/comp150fp/archive/mark-jones/fpca93.dvi * http://www.cs.tufts.edu/~nr/comp150fp/archive/simon-peyton-jones/multi.ps * http://www.cs.tufts.edu/~nr/comp150fp/archive/mark-jones/popl97-fcp.dvi * http://www.cs.tufts.edu/~nr/comp150fp/archive/robin-milner/principal.pdf 
When would you want to know and with how much precision? Obviously we cannot know which new packages will be in until the period for discussion of new packages closes. For packages already in the platform we just update to the latest version, unless there is a problem in doing so, e.g. a new release does not follow policy.
Neat! Although I had to open the PDF to understand it. IT would be nice to have teh Mabe example moved to the haddocks. It looks like it would be easy to tie in to haskell-src-meta for a nice workflow, if that was the intent. One piece that's missing is some replacement for pattern matching - maybe generated 'isC' functions per psuedo-constructor.
&gt;(which doesn't suffer from Mac Lane's habit of pulling every example from obscure branches of mathematics) None of the examples in Mac Lane are obscure. They are all very important concepts from the major branches of mathematics. If you think topology, groups, rings, modules, vector spaces, tensor algebra, (co)homology, etc. are obscure, then Mac Lane's book is not for you. It's in the title: Categories for the Working *Mathematician*. To any second or third year PhD student in Math, almost all of the examples are intelligible. For a working mathematician, certainly all of them are familiar to some degree. If you aren't in this group of people you will get very little out of Mac Lane as a teaching book because all the examples will be almost meaningless to you. If you want Categories for the Working *Computer Scientist*, this is not it. Pick it up after you find a better teaching book because it is an invaluable reference.
I once tried building ghc.. Make sure you have 8 or 16 core hardware. Otherwise it is going to hell lot of time and space. 
Examples? The newer ones I'm familiar with are Haskell-alikes: Cayenne (not exactly theorems), Agda1, Epigram, Agda2,...
Generally speaking, type theory isn't a field of mathematics, it's a field of theoretical computer science. There's a lot of maths that gets used in type theory, but few mathematicians (who aren't computer scientists) know much about it. So are you actually looking for the maths (domain theory, universal algebra, category theory,...) or are you just looking for the type theory? For domain theory as applied to lazy functional languages I'd highly recommend [Geoffry Burn](http://www.amazon.com/Lazy-Functional-Languages-Interpretation-Compilation/dp/0262521601). It gives a good intro to domain theory and also highlights some of the more mathematical issues of what it means to compile lazy languages.
TAPL is a great starting point. One caveat though is that he spends a lot of time on subtyping a la object oriented languages. That's coming from the line of researchers who are trying to apply type theory to the wild and wooly world of OOP. If you're mainly interested in functional languages, feel free to skip those chapters/sections; they don't really matter for the rest of the book. Go back and read them later, after you've finished the rest of the book (including the scant discussion of dependent types).
Haskell hackery aside, category theory is pretty far from most type theory. But if you're really interested in diving into it, [Benjamin Pierce](http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;tid=7986)'s book is a good starting point. It's short, but it's to the point and quite readable for the uninitiated. Also he spends more time on the stuff that matters for programming languages, whereas other texts spend less time on CCCs, monads, etc. If you're looking for something more heavy duty, then [Jiri Adámek et al.](http://katmat.math.uni-bremen.de/acc/) is good for some deep hands-on reading. The examples span from core mathematics (topology, group theory, Banach spaces,...), to the discrete maths that computer scientists are familiar with (automata theory,...), to really basic math (partial orders, monoids,...). You can get it free online, or fairly cheaply in printed format. Though be warned, it's definitely hands-on and you should try to work through the examples and theorems along the way; it's more of a compendium of definitions than a textbook for reading. That said, it does have a lot of really nice theorems and such, so it's an excellent reference.
Matita's in O'Caml. IsaPlanner is Standard ML (has to interface with Isabelle). HOL Light is O'Caml. MetiTarski is Standard ML. etc. etc.
I'd have thought the implicit assumption "obscure for computer scientists" was obvious, to be honest.
Does Categories for the Working Mathematician say enough about types? I thought it's more about algebraic stuff...
Type theory is most definitely a field of mathematics. Why did Girard propose System F and Goedel propose System T?
It says nothing about types. For a computer scientist, it's a pretty poor reference. Awodey's book is better suited. Ultimately, the aim with any category theory book should be to understand enough to be able to comprehend [Introduction to higher order categorical logic](http://www.amazon.com/Introduction-Higher-Order-Categorical-Cambridge-Mathematics/dp/0521356539) if you really want to understand the connection between type theory and categories. 
I'm speaking from my experience in doing type theory, and knowing quite a few mathematicians and computer scientists along the way. As I said, there is most certainly some crossover (as well there should be!) but the majority of mathematicians I've known are not particularly interested in nor familiar with programming languages and type theory. Contrariwise, most of the type theoreticians are quite familiar with mathematics, but it's definitely a one-way street. (And, of course, most computer scientists aren't type theorists.) If you don't believe me, consider these posts on the n-Category Cafe where a bunch of well-known mathematicians and category theorists discuss topics like the [pi calculus](http://golem.ph.utexas.edu/category/2009/08/the_pi_calculus.html) ([pt 2](http://golem.ph.utexas.edu/category/2009/09/the_pi_calculus_ii.html)) and [dependent types](http://golem.ph.utexas.edu/category/2010/03/in_praise_of_dependent_types.html) in exactly the way undergrad computer scientists do when first encountering them. These posts are quite elementary from the perspective of anyone versed in programming languages and type theory, and yet some well-educated mathematicians have apparently never looked at them before. John Baez even speaks (in that first post) about the lambda calculus as if it were some bizarre thing that takes a lot of work to understand. Evidently, "mathematics" as studied by those with positions teaching the subject does not include the elementary topics of theoretical computer science. The same can be said of most topics in logic. Logicians and mathematicians are quite different people, regardless of the overlap between their practitioners over the years. I hear tale there's some kind of connection between [Mr. Curry](http://en.wikipedia.org/wiki/Haskell_Curry) and [Mr. Howard](http://en.wikipedia.org/wiki/William_Alvin_Howard) (and even [Mr. Lambek](http://en.wikipedia.org/wiki/Joachim_Lambek), in more outrageous versions of the tale), but the reason the tale is so intriguing is precisely because it ties together disciplines which would otherwise have no reason to talk to one another.
Haven't heard of those ones before. Thanks.
A bit over 20 minutes on my two year old quad core... Though I still remember how it used to take hours on my old sub-ghz one core.
When first learning Haskell, I found that intoxication made it easier to stop thinking in terms of imperative programming. Now, I find that a pad and paper is much bigger help.
I think 20 mg [2C-E](http://www.erowid.org/chemicals/2ce/2ce.shtml) is more helpful than any amount of alcohol.
Hmm, if I had a twitter name field to Haskellers, that might get you a few extra names.
I tried, i got a few lines in half an hour.
Me, jkff. However, I've been inactive for quite a while, and I sometimes write in Russian, but still.
of course you can, the type system holds your hands and gently leads you towards home!
The time it takes is really ok compared to the time to get all the repositories.
I'm more of an aspiring to be Haskeller, unfortunately been a while since I wrote some. But I'm @travisbhartwell
Not exactly type level, but you can look at how the cmdargs package allows you to annotate record values.
Is there any other way?
I was just about to say that. CmdArgs now has it's annotation function split out in a separate module - http://hackage.haskell.org/packages/archive/cmdargs/0.6.1/doc/html/System-Console-CmdArgs-Annotate.html However, if you are new to Haskell, it's probably not the right way to go.
Thanks, that does look similar to what I had in mind syntactically but as you correctly said, I was thinking of something on the type level.
dons, Thanks for the all Arch-haskell work done till date. I would not be using xmonad/ghc and others if it wasn't for the timely releases of arch-haskell team.
I'd like to express my thanks too. I was using Arch before starting to learn Haskell and I was amazed when I noticed that AUR had so many packages from Hackage. So thanks, Dons :)
I might even argue it's never the right way. :)
I have used Wolfgang Jeltsch's records package to get around this problem. I am no expert, so it is likely a terrible hack and the wrong tool for the job.
I don't think it's the right way. this library has useless type signatures (so looking at examples in docs is the only way to do something) and I've managed to get runtime errors from all that dynamic typing - this is not the haskell way.
You can use phantom types to associate information with record fields. 
No longer languishing :-) Great job guys.
"IT would be nice to have teh Mabe example moved to the haddocks." I will take your suggestion. ;) "One piece that's missing is some replacement for pattern matching." I know it, but I decided to add it in a new version that I'm preparing.
The customer facing fork of our application server has been running continuously since mid-April without a restart,reload, etc. We're using a mix of happstack and our own Hack-inspired framework and postgres on the backend and it's still small enough to fit on a single 512m VPS. That said, we only get about 10k or so individual requests per day, but so far we have had 0 problems. +1 to dons comment about heap profiling. Skipfish + heap profiling for a web application gave us very good results. 
We (wearing my iPwn Studios hat) did the first port using GHC-6.10.4 a year ago, which is here: http://projects.haskell.org/ghc-iphone/ We are using this for our video game and it works well, though not as fast as we would like. Alpheccar did some work last year on ghc-6.12.x. kmc is doing an Android port now for iPwn, and when that's done we may port that back to iPhone, and this might mean we can use the LLVM back end.
Is this the kind of project other people will be able to help with? If so, can you elaborate on what is needed?
i love haskell, but scripting is exactly where i would not use it - recompiling to rapidly prototype is a drag - risking downmod with this, but for most scripts i am less concerned with correctness and more concerned with getting a good-enough answer quickly - execution speed tends not to be a big issue for the scripting domain, nor is parallelism or concurrency. - i'm happy for most scripting projects to be "in IO"...i.e. the abstractions of monads are a cost rather than a benefit when scripting - perl in particular lets me solve many scripting problems with "one liners" for which perl is uniquely well equipped 
&gt; recompiling to rapidly prototype is a drag ghci/runhaskell &gt; most scripts i am less concerned with correctness and more concerned with getting a good-enough answer quickly you have to buy into the theory that the script will need to be maintained or modified at some point &gt; execution speed tends not to be a big issue well... &gt; i'm happy for most scripting projects to be "in IO" sure. so they're in IO. done. &gt; perl. yep. but it doesn't even do function parameters right. ;)
I do all my scripting in Haskell. For instance, I've done * make-like DSL for my website * mingling some RTF into LaTeX * extracting and compressing still images from a video * custom formatting for slide annotations * scripting my iPhoto library All this would have been impossible without parser combinators, and cheap abstraction in general. 
Great work Bryan! Posting well-deserved praises here, because there is already plenty on the blog and here there isn't any.
Again, great work! I'll second tibbe's request to have another look at the `Ord` instance as well, in light of these successes. Thanks! This library is clearly shaping up to be what is was meant to be - the standard Haskell library for text. Congratulations!
How much of that work is still proprietary? What is the status of Haskell on the iPhone/Android for general GHC users?
I always used to start off writing "short" scripts in Perl (admittedly, Perl might be a particularly easy target for this complaint). Short scripts have a tendency to grow larger and require data structures, and by the time they reach 100 lines and I need a hash of hashes, it is time to port the script to a nicer language. That used to be C++ (with STL for collections) back in the day but then it became Haskell, and now I cut off the wasted effort by just starting in Haskell. It does have a slightly higher starting cost, but in the long run I'm always greatful. I actually think Haskell could do better in various areas of scripting by being more true to its strengths. For example: it would be nice to distinguish, by type, between directory paths and file paths. That could eliminate several potential errors. (Maybe there is a library that already does this?). Similarly, having the file extension be part of the type could make for some nice tools (if you know you have a PNG, you can't feed it to the pdfcrop script, for example). That might end up being more cost than it's worth, though.
&gt; The syntax primes.(max) means "return the max'th element of the primes array". The syntax primes.(high) := i means "destructively update the high'th element of the primes array to the value i". Good God, it's the natural child of Haskell and OCaml!
Those extensions tend to skirt the line of dependent types: is "/etc" a file or directory? How about "/etc/hosts"? Though you could definitely do it, if you really wanted to. :-)
One good step is to run hlint on the program (cabal install hlint &amp;&amp; hlint MyProg.hs). That gives some common minor suggestions for cleaning up code, including one I was about to make: "sequence . replicate n" is the same as "replicateM n". Your use of newStdGen all over the place isn't ideal. Firstly, it makes things more verbose than necessary: most of your code is in the IO monad, so just use randomRIO rather than passing the generator around everywhere. Secondly, I have a feeling that split (which newStdGen says it uses) isn't as random as it should be (see [this ticket](http://hackage.haskell.org/trac/ghc/ticket/3575)). rndMutateStr can be written as mapM, I think: rndMutateStr = mapM rndMutateChar Similarly, crossoverStrings could be implemented with zipWithM, something like: crossoverStrings = zipWithM cross cross x y = (\b -&gt; if b then x else y) &lt;$&gt; randomIO (Which contains another point: if you just want a random boolean with 50/50 odds , ask for a random boolean rather than asking for a double and using 0.5 threshold).
Constructing literals would be problematic, but I had in mind functions that were the equivalent of the "find" or "ls" commands. So you might have: findFilesDeep :: Criteria -&gt; Directory -&gt; IO [File] findDirsDeep :: Criteria -&gt; Directory -&gt; IO [Directory] filesInDir :: Directory -&gt; IO [File] dirsInDir :: Directory -&gt; IO [Directory] For literals, I guess you'd have some functions like: directoryExists :: String -&gt; IO (Maybe Directory) fileExists :: String -&gt; IO (Maybe File) This isn't a complete guarantee of safety (if some other program replaces a directory with a file of the same name while you're running, all bets are off!), but I think I would prefer it to no safety. I guess there's nothing stopping me (or anyone else) writing such an API on top of existing bits -- I'll add it to the list of project ideas!
A minor nitpick; you have a habit of writing functions of the form: foo :: a -&gt; IO b -&gt; IO c foo arg1 arg2 = do baz &lt;- frobnicate arg1 realArg2 &lt;- arg2 return wedge baz realArg2 This would be slightly simpler as: foo :: a -&gt; b -&gt; IO c foo arg1 arg2 = do baz &lt;- frobnicate arg1 return wedge baz arg2 Functions where you've done this include mutateChars and crossoverChar
I've been doing more scripting in Haskell lately. Although I'd still consider myself a Haskell n00b, I did manage two more ambitious projects that were scripty in nature but grew a little. First was a graphics generator. I wrote some functions to describe some geometric figures and I generated some SVG with filters and stuff to make them pretty enough for a game. The script outputs the SVG, runs inkscape to generate a png, and a plist file so OSX can easily parse out which graphics are where in the png. Hopefully it will eventually be a profitable iPhone game. Second I like the Indiana Pacers for obviously irrational reasons. The news media is notoriously bad at evaluating player performance but the "Win Score" metric has a nice track record and is easy to implement from a box score. So my script scrapes an nba.com page, parses out the box score, and outputs a little table showing me who really had a good performance. I was really pleased with how both of these came out. The only thing I disagree with the OP about is skipping the top level type signatures. Nothing speeds up my Haskell scripting like providing top level sigs. How many minutes did I burn looking at a function saying to myself, that function is perfect! What's GHC's problem? Oh, this _other_ function with no sig is actually not correct... 
To add to neilbrown's answer: If you are happy staying in the IO monad in your functions, then using 'randomRIO' makes more sense (in which case IO is like your state monad). If you want to keep your functions out of IO, then you can make a single call to 'newStdGen' in your 'main' function, and then pass the StdGen it gives you around as an extra parameter to your functions, using 'split' to (deterministically) get then next pseudo-random generator. Finally if you are feeling like it you could read a State monad tutorial and pass that StdGen around "behind the scenes" rather than as an extra parameter. 
I agree, it would be simpler. The reason for the original form was due to my use of foldr and IO. I've now changed it over to simply use mapM as per neilbrown's suggestion. Thanks anyway, though!
The reason why you have IO all over the place is, as you pointed out, that you are using random generators all over the place. Real World Haskell describes how you can put random generators into the statemonad [here](http://book.realworldhaskell.org/read/monads.html#monads.state.random). This will probably make the biggest difference. Also, I just don't like your chooseIndividual because it just has way too many parameters. If line 84 is let (x:y:restIndividuals) = map (population !!) indices Then line 91 can be let parentStr = chooseIndividual x y and the definition for chooseIndividual can be if (snd x =&gt; snd y) then fst x else fst y Less general but so much more readable. 
Thanks to everyone for the responses; I've updated my code to use the ideas you all presented.
I don't have the time to cover the whole thing, but here's a quick tip. This: do x &lt;- m return (foo x) is the same as this: liftM foo m `liftM` is the same as `fmap`, and if you have `import Control.Applicative`, it's the same as `liftA` and, my usual choice, `(&lt;$&gt;)`, so the above could also be written: foo &lt;$&gt; m
Many thanks to the people that made this possible!
If I ever get some spare hacking time I'm going to look again at lazy bytestring and perhaps lazy text. The current extra indirection is not fundamental. There's really no reason they should be slower than strict bytestring (for reasonable chunk sizes).
Great work. A minor grumble is that they're using the upstream version of the haskell-platform metapackage but not guaranteeing the same versions of the dependent packages. This is a bit disapointing because version synchronisation is one of the major points. Hopefully in practice they will be the correct versions. Something to keep an eye on.
can anyone make a compelling case for ever using plain-ol-Strings???
Simple straightforward streaming calculations over a list of codepoints where performance is dominated by other concerns.
Since I don't have anything better to do I wrote [my own version](http://pastebin.com/1SySYR3s). I didn't want to change the whole program, so this is mostly minor edits from the original. One thing I maybe should have done that I didn't do fully (I didn't hide the range) is that your uniqueRandoms is always called like this: uniqueRandoms (0, populationSize - 1) gen x [], that should so have been it's own function: uniques x = do gen &lt;- get uniqueRandoms (0,populationSize -1) x []
whatever you decide, just don't make the mistake of camlp4/camlp5 situation.
Quick summary for those of us who don't know the story? edit: dcoutts' link explains that there was a fork. my question is more about details vis a vis how the fork took place, to what extent this has caused problems/fragmentation, etc.
In general, functions should *never* take IO actions as arguments, if they always simply bind the value out of them exactly once (Except when you do this to satisfy some externally-specified type, IOW when you don't have a choice :-). The type: `IO a -&gt; IO b` is confusing if you could have written `a -&gt; IO b`. It means that it converts an action to a different action, perhaps by chaining that action, or doing a multitude of possible things with the IO part of `IO a` which isn't interesting at all. When people read Haskell types, they usually expect to see the simplest form of the type. It is legitimate to have `IO a -&gt; IO b` as a type, but if you read it assuming it is the simplest form of the type, you'll have a very different idea in mind of what the function is doing.
I think if Haskell's standard libraries had been better organized into type-classes, using ByteString/Text rather than String wouldn't have had to be such a chore...
&gt; fillPrimes :: Array Int -&gt; Int -&gt; Int -&gt; Int -&gt; () Is it me, or does anyone else think this is a *huge step back* from Haskell?
via Google: http://alan.petitepomme.net/cwn/2008.12.30.html#3
This has actually been a bit problematic when we've packaged Haskell software for internal use, and it needed versions of libraries newer than the haskell-platform; we'd need to sketchily install both versions so the dependencies would resolve correctly; I'm not sure if we could pull this off on Debian/Ubuntu. Some food for thought.
It'd also be nice to see an instance for [Hashable](http://hackage.haskell.org/package/hashable). Users could write one with one of the encoding functions and the ByteString instance, but it should be possible to circumvent encoding and just use the internal representation for hashing on. 
I. JUST. DID. THIS. MANUALLY. LAST. WEEK. Luckily I've also upgraded everything to 10.10 so i'll have to do it again /everythingwentbetterthanexpected
That's not the full type. It's really something like: fillPrimes :: Array %r Int -&gt; Int -&gt; Int -&gt; Int -(write %r)&gt; () Although I just made that up based on memories of my glances at Disciple, so it's probably wrong. Anyhow, the extra stuff gets inferred, so you don't have to write it down explicitly. I'm not sure what the point of saying, "After all, checkPrime and fillPrimes aren't doing any IO..." in the article is, though. They wouldn't need to be associated with `IO` in GHC, either, they could just as easily be `ST`, which is analogous to what needs to happen in Disciple, too.
We'll just pretend I proofread things before I post them.
Polya Counting is lots of fun, especially in Haskell!
What's that? My dad released code on Hackage? That's funny...
Disciple uses more fine-grained effect information than in Haskell. In Haskell all your side-effecting functions just get put in the IO monad, but in Disciple the "full" type of fillPrimes is actually: fillPrimes :: forall %r0 %r1 %r2 %r3 %r4 . Data.Array.Array %r0 (Int32 %r1) -&gt; Int32 %r2 -&gt; Int32 %r3 -&gt; Int32 %r4 -(!e0)&gt; Unit :- !e0 = !Read %r3 + !Read %r2 + !Read %r0 + !Read %r1 + !Read %r4 + !Write %r1 , Mutable %r1 That's why I don't tend to write all the effect terms in sigs manually, I just leave it for the inferencer. You can write them all down if it makes you happy though... 
Yeah, you could use ST, but dealing with the ST variable can be tricky. If you destructively create an array in the ST monad you have to "freeze" it before returning it to get rid of the ST variable. When I finish proper effect masking in Disciple, this "freezing" process should happen automatically (and safely). 
&gt; In Haskell all your side-effecting functions just get put in the IO monad That's one way to do it. There are many other possibilities (ST, specialized DSLs that may or may not be monads, ...) &gt; That's why I don't tend to write all the effect terms in sigs manually, I just leave it for the inferencer. But if you don't write the effects down, then they are not visible to the code writer and reader. How am I supposed to know when to expect referential transparency? How can I know what kinds of reasoning about functions will work? It seems that the difference between this approach and the non-pure (ML/Ocaml) approaches, are that the compiler here seems to have a bit more information from code analysis. Developers reading API type sigs/etc are still at a loss about what the function can do. A huge advantage of Haskell is that: * The vast majority of type sigs in API's are of pure functions * A type sig of a pure function is almost always the "whole story" or almost the whole story. When you have one, you often don't need extra documentation. 
It seems to me that if you don't have to explicitly write the effectful information in your function arrows, that you've lost the majority of benefits of purity. It also seems weird to me to make Monads so special in the language. Why not Applicative? How is restoring `=` to being non-referentially transparent progress?
&gt; It seems to me that if you don't have to explicitly write the effectful information in your function arrows, that you've lost the majority of benefits of purity. I'm not sure why that would be. It's entirely possible to write significant Haskell 98 programs without any type signatures whatsoever, so in that sense, Haskell lacks the benefits of purity. The benefit of purity, in my mind, is that there exists a portion of the language that permits easy reasoning about programs, and that that segment of the language is capable of making up the majority of the program. And that is still likely true in Disciple. It's the segment of the language where the arrows are `-(!e)&gt;`, and `e` is not composed of any `Mutable %r`, or most other effects probably (looks like `Read` is ubiquitous, though). I'm not sure what to say about monads versus applicative. If anything, I'd wager that Disciple is more like Wadler's arrow calculus. Except the arrows in question are generated from much finer grained set of effects, and those effects are combined and inferred automatically. If the question is then, "why does the language assume that all effects (and their combination) give rise to Cartesian closed categories," or something of that sort, it's probably because that's a very useful assumption, and all the effects supported do.
&gt; I'm not sure why that would be. It's entirely possible to write significant Haskell 98 programs without any type signatures whatsoever, so in that sense, Haskell lacks the benefits of purity. I said something slightly different than what I meant. When you *do* give an explicit type signature, and that signature says "this is a pure type", and it isn't, you've lost many of the benefits of purity. &gt; The benefit of purity, in my mind, is that there exists a portion of the language that permits easy reasoning about programs, and that that segment of the language is capable of making up the majority of the program. Wouldn't you want to be able to know which segment of the language is pure? If the explicitly specified types don't distinguish pure/non-pure code, how can you reason about them? &gt; It's the segment of the language where the arrows are -(!e)&gt;, and e is not composed of any Mutable %r, or most other effects probably But the specified types fail to mention this. &gt; it's probably because that's a very useful assumption, and all the effects supported do. It's also a problematic assumption in some cases (Applicative interface can allow more optimizations than the Monad interface, in some cases).
Ok, so the plan is to have two "modes" of signatures. If you specify a sig using "::" it will fill in the effect information for you. Alternatively, if you use "&lt;:" (less than) then you have to say what the effects will be else you get an error. I posted more about this on my blog. I totally agree that it's good when the majority of the functions in an API are pure, at least in Haskell. But consider this: if you read a constant integer then that's pure, but if you read a mutable integer it's an effect. Disciple has mutability polymorphism, so the same function can work on both constant and mutable ints. In Disciple, whether or not a function is "pure" or "referentially transparent" is not always inherent in its definition, it can depend on what data it's passed. Also consider the map function that takes a worker to apply to a list. If the list is constant and you pass a pure worker then that instance of map is pure. On the other hand, if you pass it an impure worker then that instance of map is impure. Of course it's important for an API to be explicit about effects, but when you start being precise, then the question of whether a function is "pure or impure" stops being a binary decision.
&gt; so the same function can work on both constant and mutable ints I am not sure what the meaning of this is. If a function transforms some constant int value to a different value, what would it mean to give it a "mutable int"? Wouldn't it mean the function is applied via fmap on the constant int value that the mutable int has at the time of execution? Wouldn't a better name for this feature be "auto-lifting", implicitly applying fmap and (&gt;&gt;=) to make the types work? &gt; Of course it's important for an API to be explicit about effects, but when you start being precise, then the question of whether a function is "pure or impure" stops being a binary decision. Sure, and this is already true in Haskell, e.g: `IO a -&gt; IO b` is a pure function between effectful actions, etc.
&gt; If a function transforms some constant int value to a different value, what would it mean to give it a "mutable int"? If a function reads a mutable int then it would have a side effect because this operation cannot be reordered with others. &gt; Wouldn't a better name for this feature be "auto-lifting", implicitly applying fmap and (&gt;&gt;=) to make the types work? I'm not sure what you mean by "auto-lifing" In Haskell all ints are constant. We do have (IORef Int), but in this case when you do an "update", it is the IORef that gets updated instead of the Int. You could imagine updating the actual boxed Int object in the heap (assuming there is one) but that would be a different operation, which Haskell doesn't support. Disciple uses constraints on region variables to represent mutability, so the types of constant and mutable ints have the same "shape". The type of a constant Int would be like (Const %r1 =&gt; Int %r1) and a mutable one like (Mutable %r1 =&gt; Int %r1). When you update a mutable Int it modifies the actual boxed object in the heap. We also have Ref types though. A reference to an int would have a type like (Ref %r1 (Int %r2)) and you can constrain either region variable as mutable or constant depending on what part you actually want to update (or not). Consider the function: succ :: forall %r1 %r2. Int %r1 -(!e1)&gt; Int %r2 :- !e1 = !Read %r1 It accepts an integer from any region (mutable or not) and produces an integer into any region (mutable or not). If I then have some application of it in my program (succ x), whether this application has a side effect depends on whether "x" is actually mutable or constant. If succ is passed a mutable int then !Read %r1 is a real side effect. However, if succ is passed a constant int then obviously it still reads it, but the fact that this read is on a constant region means it can be safely ignored. In this case we would say "the read effect has been purified by the constancy constraint". I don't think "auto-lifting" really describes what's going on in Disciple, though it could apply to the Haskell case where "mutable ints" are encoded using IORefs. 
&gt; succ :: forall %r1 %r2. Int %r1 -(!e1)&gt; Int %r2 &gt; :- !e1 = !Read %r1 Consider instead: succ :: Int -&gt; Int and then, if you use: fmap succ :: Functor f =&gt; f Int -&gt; f Int So `succ` is the pure one, and `fmap succ` works on `Ref Int`. This is what I mean by auto-lifting. It is not `succ` itself that becomes "side-effecting" when you use it on a mutable variable. It is the *lifted* `succ` (`fmap succ`) that is side-effecting. In Haskell you lift with `fmap`. It seems that in Disciple it is lifted automatically. But will "succ" work on a Parser monad in Disciple? `fmap succ` will generate IO actions that work on `IORef Int`, as will it work with any `Parsec Int`, for example.
&gt; It is not succ itself that becomes "side-effecting" when you use it on a mutable variable. It is the lifted succ (fmap succ) that is side-effecting. Right, so lifting succ to work on (Ref Int) doesn't make the succ operation side-effecting. It's the Ref which is mutable, not the integer. It's right to say that "lifted succ" is side effecting, but this effect is due to reading the Ref. The integer itself it still constant. In Disciple the integer itself can also be mutable, so there are more degrees of freedom in how you represent the effects. &gt; In Haskell you lift with fmap. It seems that in Disciple it is lifted automatically. But will "succ" work on a Parser monad in Disciple? I don't think it's right to say the succ operation is lifted automatically, because there doesn't have to be any additional structure to lift it into. succ wants an Int, and if you apply it to something of type (Parser Int) you'll get a good old type error. Disciple has it's own version of http://code.haskell.org/ddc/ddc-head/library/Class/Functor.ds the Functor class but I haven't had much experience with it yet because dictionary passing isn't implemented and there are some unrelated bugs preventing me from finishing my parser example. The plan is to support parser monads and fmap in much the same way as Haskell. There are also interesting possibilities in combining effect typing with monads such as described on page 241 of http://www.cse.unsw.edu.au/~benl/papers/thesis/lippmeier-impure-world.pdf. There are holes to fill before I can properly experiment with that though.. 
you could represent 'Parser monad' (type + &gt;&gt;= + return) as regular functions - Parser Int == () -&gt; Int. then, when you want to succ result of a parser p result (succ `fmap` p) you can just write (succ $ p ())
are you arguing about syntax? there's a special place for you in hell...
No, I'm not arguing about syntax. I'm arguing that if only the compiler is aware of which function is pure, and not the programmer, then many of the benefits of purity are lost.
While I suppose it's neat to have 5 ways to do the same thing, I do think it's important to have a consistent style about code. Consider what a newbie learning Haskell goes through, then consider how to maintain a code base, and then realize that too many options is a problem in the real world that needs to be dealt with somehow. I also tend to lean towards Applicative, but most of the Haskell books I have don't even mention Applicative Functors. 
Thank you for this! This tool is made of win and unicorns.
If you continue to provide such witticisms as payment, the tools will continue coming ;).
This really is awesome - I'm very impressed (and not nearly as witty as sclv)
In this particular case, I somewhat agree with you. IMO, the only major options should be the original do notation version, `fmap`, and `(&lt;$&gt;)`. There is no point in the others existing besides the fact that `Monad` doesn't require `Applicative`, and even that doesn't explain the existence of `liftA`. In general, more options gives you the ability to write cleaner, more readable code, and in the long run it is more important that your code be readable by people that know what they are doing, as they are the most likely to be able to gain value from it or add value to it in the first place. The (barely) increased learning curve is merely an unfortunate side-effect, in my opinion, and it's worth overcoming these slightly higher barriers in the quest for enlightenment. If I did not believe this then I would have never looked to Haskell in the first place. The lack of Applicative in introductory material is unfortunate, but the situation is changing. LYAH teaches Applicative before Monad, if I recall correctly.
That was fast... original idea proposed [Oct 20, 1:36am](http://groups.google.com/group/haskell-cafe/browse_thread/thread/b6760caab7229840), tool announced Oct 20, 9:08am with functioning web front-end.
related [work](http://www.cse.unsw.edu.au/~pls/projects/dons-thesis.ps.gz) by some completely unknown haskeller.
I think it may be broken right now or on Chrome. I get a short flash of what looks like xml and then Chrome asks me if I want to subscribe to a feed with no entries.
I think that's correct. It returns an rss feed, and I think if it finds nothing with outdated dependencies, it returns an empty feed. You can subscribe and will be notified if new ones show up.
camelon is correct, that's the expected behavior. It looks better in Firefox, which knows how to handle Atom/RSS feeds properly. That said, I'm not opposed to adding an HTML version of the page with some instructions, since you are correct that it's confusing as-is. Edit: OK, HTML version is live now. For example, you can [see the packages that I need to update](http://packdeps.haskellers.com/feed/?needle=snoy).
So, I think there is a real problem here, but misleading type signatures aren't really it. The problem is that in Disciple, we aren't writing pure functions by default. If we consider `map` in Haskell, we know that: map f . map g = map (f . g) works everywhere. When we want to map with effects, we use `mapM`, and we know that in general: mapM f &lt;=&lt; mapM g /= mapM (f &lt;=&lt; g) Now, I presume that in Disciple, `map` has default type like: map :: (a -(e)&gt; b) -&gt; [a] -(e)&gt; [b] Can we say that `map f . map g = map (f . g)`? _No_. This equational rule does not hold for every instantiation of `e` and pair of functions. We can say that it holds when `e` is instantiated with 'purity', but this is no different than saying that: mapM f &lt;=&lt; mapM g = mapM (f &lt;=&lt; g) if m = Identity And the bad news is that we are never writing genuinely pure functions unless we explicitly say so with a type signature (I could be mistaken). Instead, we are always, by default, at best writing code that could be instantiated to any effect, including no effect. That is, I think, the selling point (or one) of Disciple. But this means that we are never really at liberty to apply much equational reasoning, because some instantiation may invalidate it. So even within the part of your program you'd consider pure in Haskell, you cannot replace: foo f g = ... map f . map g ... with foo f g = ... map (f . g) ... without that change being potentially visible, because by default, we aren't writing pure functions, we're writing could-go-either-way functions. &gt; It's also a problematic assumption in some cases (Applicative interface can allow more optimizations than the Monad interface, in some cases). Well, Disciple isn't eff. I don't think you're able to define new effects by some algebraic specification. The effects that work with the effect system are baked in. So this isn't really an issue. Even in Haskell, by far the bigger advantage of `Applicative` is its combinators for writing in a nice applicative style, like: f &lt;$&gt; x &lt;*&gt; y but being able to write in ordinary lambda terms with effects covers that.
If you are free to write ordinary lambda terms with effects, you have Monads.. A parser monad cannot be implemented as efficiently as a parser applicative (well, parser applicative + some combinators that do not add up to full monadic power).
I had a similar idea recently though it took this one a little further. I was wondering if we could have an automatic build tool that tells us exactly what versions of all the dependancies and compiles and runs the tests against them to see if the code is actually compiles and runs; sort of an automated build system / dependancy checker. Maybe that is the logical extension of this idea. It would be great for the stability of the Haskell codebase as a whole.
I have Hugs running fine on my iPhone. I'm guessing you probably wanted to ask about GHC specifically though.
&gt; Can we say that map f . map g = map (f . g)? No. We can't say that in general, but we can for when f and/or g are pure, yes. As far as optimiser rules go, the plan is to annotate the rules with what purity constraints are required for them to work. We would then allow the optimiser to create specialised versions of a function that assume their arguments are constant/pure, which provides the required constraints. Wrt equational reasoning: in Disciple 'map' is effect polymorphic, so some rules about it don't work in the effectful case, sure. Disciple is more expressive than Haskell due to supporting mutability (modulo holes in the current implementation), and when you have a more expressive language you tend to lose operational equivalences. See Felleisen's paper on the expressive power of programming languages for this. The game then is to write effect polymorphic code, but be able to easily instantiate that to the completely pure case if you want to use more equivalences. One method of doing this is to provide explicit type sigs, there might be others. It's still a work in progress...
this package does *not* depend on the profiling libraries for a number of standard libraries. full list being kept up to date [here](http://www.reddit.com/r/haskell/comments/du3fi/maverick_meerkat_haskellplatform_warning/) as i find more of them.
I know that, but it doesn't matter, because Disciple has no parser effect in its effect system, and you cannot, as a user of the language, add one. You'd just do it the same way as it's done in Haskell, with applicatives/monads.
I feel like I'm still a Haskell noob for asking, but why does it need to have dependancies on the profiling libraries? Are they required to run the platform?
the platform is specified as having the profiling libraries. also it's a pain in the ass to manually install them all and recompile every single library you have just to profile one little bit of code.
&gt; We can't say that in general, but we can for when f and/or g are pure, yes. As far as optimiser rules go, the plan is to annotate the rules with what purity constraints are required for them to work. We would then allow the optimiser to create specialised versions of a function that assume their arguments are constant/pure, which provides the required constraints. I'm not really concerned with how the optimizer would work. I'll grant that it can keep track of all the relevant variables, and apply equational rules if and when it is safe. Where it is a problem is when the programmer has to keep track of all relevant instantiations. The advantage of purity is simplified reasoning about code, but trying to keep track of multiple variables that determine whether my code is pure is not simple. The only way I can currently think of for the programmer to delegate that checking to the compiler is for them to write two versions of the function---one being effect polymorphic, and one restricting things to be pure, yielding easier reasoning. But this is no better than `map` and `mapM` in Haskell. Arguably, it's worse, because we have to make extra effort to write the former. If someone figures out how to have the best of both worlds, that'll be great, though.
The page loads and then is whited out by the Reddit Alien. Viewing in `lynx` works okay, though.
Really weird.
I keep getting comments like this - but I can't replicate the issue. Works fine in Win/Chrome, Win/IE, OSX/Safari. What OS/browser are you using?
Chrome on GNU/Linux.
same problem
I've wanted that as well, but that kind of thing would require a lot more computing power than what I've done right now. Just imagine that I have a package that depends on five other packages, and each of those packages have five versions that fit the dependency range. Then we'd have to compile 25 different times. And there are plenty of packages on Hackage with far more than 5 dependencies.
I'm using Chromium on Ubuntu without this problem. Those having difficulty, I suggest trying Readability, works great for these situations ;) 
Decided to install Readability Redux and attempt this. It was unable to parse the page for content. Chromium on Ubuntu as well, by the way.
I can replicate this on Ubuntu Chrome. But I can't upload a change from work. Damn corporate proxy. I will attempt to fix it tonight.
Yeah, it is exponential and gets that way very quickly but perhaps you would give that code to people themselves to run on their own computers. That way you can amortize the cost of running so many builds, or maybe you can just test the extremes in the ranges which means that you will only have an extra 'times two' builds per dependency range. I don't have a solution but I get the feeling that some workable and useful solution could be reached. At any rate, very nice work on what you have so far. It's a good tool.
One workaround, but pretty annoying, is to use the built-in debugger and pause the scripts. Then, just hit reload and the js won't mangle the DOM.
If I remember correctly cabal does not resolve the dependencies of a profiling package. I wonder why this is the case. That is, if I want to install package A with profiling support why does cabal not install all the packages on which A depends with profiling support? 
don't know why, but you are right. that's what it does. which means you've gotta hunt them all down. =/
This is great for package authors: use your name as needle, subscribe to the generated feed, and be notified when you might consider updating version constraints of your packages.
I appreciate that you discuss record update syntax in the context of state updates.
You'd still have to do this for non-platform parts, wouldn't you?
FYI: Same with Chrome on OS/X. OK in Safari.
not if you configure cabal to build everything with profiling libraries. but if you do that, and the base profiling libraries aren't available, then nothing will build. which is what held me up for about three days.
How do you configure cabal to do that?
uncomment and edit the appropriate lines in ~/.cabal/config on unix. it'll be obvious which ones. on windows, i don't know. check the documentation. or ask in #haskell. like i said, though, you'll have to manually chase down dependencies for all the profiling libs you don't have. and make sure to not try to rebuild base libraries (if you don't know if it's a base library or not, ask, or don't touch it), or else you could end up with a thoroughly broken haskell install. well, it's fixable. but just don't do it anyway. 
Interesting, I had no idea there was a plugin. I was just using the javascript version from here: http://lab.arc90.com/experiments/readability/ Works fine for me.
Having the same problem, using Google Chrome 7.0.517.41 on Windows XP. Edit: I'm having the problem also on Ubuntu 10.04 64bit using Google Chrome 7.0.517.41
He must be using Haskell and the Yesod web framework, or something.
 likely ;)
Can you elaborate on what you've been able to do with hugs? I know in my case I was hoping for a fast expressive language for use with cocos2d. Also I wanted to be able to cross compile and deploy from my linux workstation. And a pony. 
I got lost here: &gt; Also, the wrapper is confusingly written with forall rather than exists. This references the fact that we're declaring an existential type, but the data constructor has universal type:
On windows, it is the same, except the config file is in a different directory. There is now standard home directory on windows too, which I never remember where exactly is; I think it puts the config file there.
Actually, props to Duncan and everyone else who wrote the Cabal package. It was that piece of work that let me write this tool so quickly. (Well, yes, Haskell and Yesod helped as well...)
I hope that over time more and more libraries will ditch String in favor of Text.
Bryan has added a faster Ord instance. See the blog post's comments.
I'm working on a MurmurHash implementation using the internal representation. We can use that implementation for both the Text and ByteString instance of Hashable as they both use ByteArray# underneath.
Haskell already has the facilities to make such transitions relatively smooth, but that potential just sits there...
Say you want to write exists t. [t] in GHC. You cannot do that directly, instead you have to define a datatype which looks like this: data Wrapper = forall t. WrapperC [t] The syntax uses the word "forall", although you're declaring existential type. You can use the type like this: getlength :: Wrapper -&gt; Int getlength (WrapperC x) = length x What is type of WrapperC? On the one hand, **for any** type t, it takes list [t] and gives you Wrapper. WrapperC :: forall t. ([t] -&gt; Wrapper) On the other hand, it takes something that is **for some** type t, a list of t, and gives you Wrapper: WrapperC :: (exists t. [t]) -&gt; Wrapper (This syntax is not available in GHC, it's only a way of thinking.) These two types are equivalent/the same. A function that takes any type of lists is the same as a polymorphic function that for all types takes a list. If you remember functions a-&gt;b correspond to exponentiation b^a, existential quantification to sums, and universal quantification to products, the equivalence is the exponent law: b^(sum i. a_i) = product i. (b^a_i) Is it clear?
Based on what you've said, the way I imagine this is as an implementor, albeit in the less enlightened area on the totem pole of implementors. Functions like getlength may have to be able to distinguish between the various types used at runtime so someone has to own a tag for dispatching to the right handlers. Wrapper gets an implicit parameter representing the forall t. I use that to dispatch. It's because of this implicit parameter that we can't use a newtype. Anyway, that's some slum exposition but it works for me until I get a better mental model. The article seemed to imply that the exists was correct type of something that's implicit in the declaration, hence I can't see it, while forall is the type of the constructor. Does that contradict your statement that the two types are equivalent? 
If only we had something like GWT, but based on Haskell and using reactive functional programming... We just got closer to this dream!
Shouldn't be too hard to modify to output Actionscript, right? Programming in Actionscript is so frustrating.
How close is this to creating a web site?
The types are gone at runtime. However, if there are constraints on the type, like: data E = forall t. Show t =&gt; E t Then the dictionary will be packed in the constructor, effectively. It's also important in such cases for irrefutable matches to be rejected, because GHC expects dictionaries to be strictly evaluated, and an irrefutable match would introduce a lazy dictionary. All newtype matches are irrefutable, though. In the presence of equality constraints, irrefutable matches could even cause unsoundness: newtype Bad a b = forall c. (a ~ c, c ~ b) =&gt; Bad c preCoerce :: Bad a b -&gt; a -&gt; b preCoerce (Bad _) x = x -- a ~ c and c ~ b implies a ~ b, so this is well-typed coerce :: a -&gt; b coerce = preCoerce undefined -- since Bad is a newtype, this doesn't explode Theoretically, I guess you could allow newtype existentials as long as there are no constraints, but GHC doesn't bother doing this. This explanation blends a bit with yours in a type class model like JHC's, which is based on a sort of type case. When there are constraints, it needs to pack the type in the existential for later analysis, but it could be omitted if there are no constraints.
Interesting talk. I didn't get all of it because the microphone was not close enough the the persons who were speaking. While Haskell has brought us closer to proper reasoning about the correctness of the program, it makes us difficult to understand the execution. I don't think this is restricted to Haskell, this probably applies to other functional languages. Laziness and garbage collection are not helping us. This may be the price to pay for writing programs with equations rather than pop, load, jump_eq... Math vs. Mechanics. So what are typical Haskell optimizations? I got: *Remove laziness where it hurts (seq, bang patterns, normal forms) *Fusion (Data.List.Stream) *Unboxed types (-funbox-strict-fields) what else? 
This discussion led to at least one new idea, [-fwarn-performance](http://hackage.haskell.org/trac/ghc/wiki/PerformanceWarnings), though not 100% sure how to implement this.
This is good news. I'm especially interested in warnings about &gt; calling overloaded function with known dictionaries I once heard Bryan O’Sullivan say "if dictionaries are passed through your code then you are doing it wrong" (quoted from memory. bos, correct me if I'm wrong.) It would be useful to know which dictionaries are passed through which functions without having to look at Core. Sometimes passing dictionaries seems unavoidable though (we cannot SPECIALIZE Control.Monad for all possible monads, for example.)
&gt; Warn about unboxed constructor fields that had to be reboxed for passing to a non-strict function This one is another that I'd really like to see. When using a lot of records for accumulators or semantically-named tuples, we often want to unpack things. It'd be nice to see exactly when things need to get repacked.
And I thought I had things down pretty fast with flymake.
This is just as interesting for compiling CouchDB and Riak "queries" as it is for client-side programs. What language could be better for specifying MapReduce jobs than Haskell?
Awesome!
Oooh, shiny! Thanks for your work. I'm currently looking into extending a couple of String-only libs I use into accepting Text and ByteString, too. This sounds very promising - if Text can get us nice performance *and* a cool interface, hopefully libs programmers won't use String as their only I/O type anymore! (like HXT and HaXmL do. HaXmL can at least *write* BS, but I'm sure both would benefit from a Text-backend, too. Hexpat allows for all three.)
Simon PJ has implemented call site specialization so if a function is marked with the INLINABLE pragma a specialization version will be created at the call site and there will be no dictionary passing.
Kinda obvious but: use the right data type for the problem! Too many people use lists (as in linked lists) and are surprised when performance suffers. Use ByteStrings for binary data, Text for Unicode, Vectors for sequences, and Map/HashMap for maps.
Sounds good. Are there some docs for that? I only found a darcs patch adding documentation for INLINABLE and the paper about call *pattern* specialization which you probably did not refer to. Or did you? Maybe a specific dictionary is a call pattern..
Hmm. Is text API/ABI change so fast? If yes is there any update guide?
woot
This is different from call pattern specialization. I thought I saw the documentation patches fly by. Did you check the GHC 7.0 user manual section for pragmas?
It's mostly additions. But some release notes would be nice but they're tedious to write. Perhaps there's a good hg to release notes tool out there somewhere. 
GHC 7.0's users guide has a paragraph on INLINABLE but does not talk about call site specialization. At least grepping the .html's for "call site" did not turn it up.
Is that tilde operator explained in the GHC user guide? I don't know what it means. 
Does it seem that if you don't have to explicitly write the type information in your functions, that you've lost the majority of benefits of static typing? DDC does effect inference, and it is a logical enhancement of type inference. 
ABIs change all the time; there's no avoiding that with GHC. But the APIs are actually very stable: apart from a few new functions, no existing code has changed.
But, but, but: the blog posting *has* release notes!
From a brief look at the source code this seems to be implemented as an STG interpreter in Javascript. To take advantage of modern JIT technology it may be better to translate it directly to Javascript closures. You'd still need some sort of foreign call interface to Javascript, though.
Remember that every time you bump the major version (like 0.9 -&gt; 0.10), every package that depends on 'text' has to be updated.
Inference is fine, but only if it is visible to the programmers -- and if it is possible to program pure functions that can be reasoned about under assumptions of purity.
Sorry. I meant to write "Is text API change so fast to justify versions bump? If yes is there any update guide?". I asked having in mind exactly what jmillikin wrote. [Haskell have package versioning policy](http://www.haskell.org/haskellwiki/Package_versioning_policy) - is text use it.
So his options are either editing code in a browser or waiting just over a minute (if he doesn't need to recompile) to see code changes in browser? And he thinks this is "pretty quick"? That's awful. I hope I'm misunderstanding that.
Typo in article, now fixed: "... if I don't have to *reconfigure*, ...". I'm not sure what you see as awful: A minute to round trip via command line tools? Seems common... Editing source within a web page? For the projects' target audience, that is exactly the aim! (And you should see what some JavaScript code editors can do! They are pretty full-fledged. We'll be incorporating one in the future.)
It's type equality. So when we match on a value of type `Bad a b`, we ultimately get a proof that `a` and `b` are the same type (and that the existentially quantified type `c` is the same, too). For that to be safe, though, we need to actually evaluate elements of `Bad a b`, to make sure we've constructed them legitimately. Treating `undefined` as a real proof of equality is unsound, because `undefined` inhabits every type.
A coworker has implemented some Haskell parsing of lots of debug information (Hundreds of megabytes). Using "text" was fast, but internal use of UTF16 doubled space use. The origin of the text here is ASCII, so UTF8 is more suitable, but using ByteString/UTF8 directly is tedious, it is nice to have an abstraction over this. Is there a way to use Text with UTF8 or is using ByteString.UTF8 the only reasonable way?
Did they try the lazy API? It would be an odd log parsing job that needed the entire file in memory if it's hundreds of megs in size. Text doesn't use UTF8 internally. It would take a few days of work to switch it over. If you try it, there are plenty of correctness and performance tests you can run to ensure you're getting it right. Let me know if you try it?
Careful about overgeneralising. I'm sure Bryan's comment came with some context. In lots of ordinary code passing dictionaries is perfectly ok, it's much like in OO passing around object vtables. It is only in particularly performance sensitive code that this sort of thing starts to matter. You then also need to be aware that you are trading performance for code size.
Weirdly turning script deferment off for the reddit button seems to have solved the issue.
It's one way of doing it. Personally, I'd go with something like substring s = tail . inits =&lt;&lt; tails s though.
Another variation: substring x = [ i | t &lt;- tails x, i &lt;- inits t, not $ null i ] 
http://pastebin.com/yZFGvdkm
Here is another possible rewriting substring [] = [] substring xs = inits xs ++ subst (tail xs) 
I like that you smuggled in an obfuscated identity function ;o) But maybe that was meant to be ([[]]++) or ([]:)
Haha, oopsie. :D Fixed, thanks.
 substring :: [Char] -&gt; [[Char]] substring s = concatMap (tail . inits) (tails s)
I think this is where I'd land too. The list-bind version from FalconNL is cute but I so rarely used List as a monad that it wouldn't be the obvious conclusion. I'm surprised the author didn't choose `substrings` (plural) as the name.
Offtopic, but just wondering... It seems that we often need `(not . null)`, so why doesn't the Prelude contains `notNull = not . null`? I mean, even `elem` has a `notElem` counterpart.
Might as well use (&lt;=&lt;) there :)
Just for fun, a lazier version of the non-contiguous answer. finiteSubstrings s = [[]] ++ concatMap f (tail (inits s)) where f s = map (++ [last s]) (finiteSubstrings (init s))
Apples and oranges. We have `notElem` because `elem` is a two-argument function, so the point-free definition `(not .) . elem` is _much_ more annoying to type than `notElem`, whereas the point-free definition `not . null` is only _a little_ more annoying to type than `notNull`. The point-free definition of `notElem` is also much, much more difficult to come up with on your own than the point-free definition of `notNull`, so most people would probably end up choosing `\e es -&gt; not (elem e es)`, which is _even more_ annoying to type.
Compare the following two: subsets1 :: [a] -&gt; [[a]] subsets1 [] = [[]] subsets1 (x:xs) = map (x:) (subsets1 xs) ++ subsets1 xs subsets2 :: [a] -&gt; [[a]] subsets2 [] = [[]] subsets2 (x:xs) = subsets2 xs ++ map (x:) (subsets2 xs) print $ length $ subsets1 [1..24::Int] print $ length $ subsets2 [1..24::Int] `subsets1` is much faster; however, `subsets2` runs in constant space, while subsets1 uses a *lot* of memory (this is with GHC 6.10, -O). I'm not sure why this happens (does GHC do CSE?).
And note how this version is much clearer than the monad bind version, even though you can desugar one into the other: [ i | t &lt;- tails s, i &lt;- tail (inits t) ] vs tail . inits =&lt;&lt; tails s 
It does need a significant part of the debug info in memory, and the laziness's indirection adds more to the required memory footprint. I doubt we'll have a chance to try it, as we're behind schedule as it is... Thanks for the offer, though :-)
&gt; does GHC do CSE? No, because CSE can introduce space leaks. However, the effects of laziness can often appear similar in performance measurements.
I see this as a disadvantage of currying, and I think it is related to some difficulties we have in Haskell w.r.t optional and keyword arguments. Maybe there could be some language feature that doesn't over-complicate the language on the one hand, but makes composition of function inputs/results more agnostic to the position and number of arguments.
Ah, I was a little lazy in reading your question. Debug information in the form of a binary file?
...I'm not sure what's wrong with me that I actually find the monad bind version more readable...
Yeah, specifically DWARF information from executables.
I came here specifically to suggest `tail . inits &lt;=&lt; tails` and you guys had to spoil my fun.
Hmm. Well, I'd use bytestring for that by default simply because the data is all bytes.
The space-time trade-off indicates that GHC does CSE, indeed. You might want to check the core.
Well, I'm not very good at reading core, but it seems that in the first case it *does* perform CSE (and indeed, it introduces a space leak); while at the second case it does not do CSE (and it runs in constant space). According to my naive understanding, both version should run in constant space; and in fact, if I turn optimisation off, both actually run in constant space. So now the question is, how does GHC decide when to perform CSE? Can somebody check this with newer GHC versions? 
This seems very similar in concept to the CPython binding in the MissingPy package. Some of the apparent differences: * The license - cpython is GPL-3, MissingPy is MIT * cpython defines separate Haskell types for many of Python's built-in types and marshals them, whereas in MissingPy you mostly access them via their methods as Python objects * MissingPy doesn't seem to have been updated in over a year * It's not clear from the hackage page which version(s) of Python is supported by MissingPy, whereas it is clear from the version number of the cpython package which version of Python it supports (though that approach could violate hackage's versioning policy in certain circumstances) One way in which they are unfortunately the same - neither seems to generate Haddock documentation. Any other similarities/differences?
Ah, here's something else: cpython has Haskell type classes to represent some of Python's "protocols", like the "Mapping" protocol and the "Sequence" protocol. Nice! Could we request the "Iterator" protocol, the "ContextManager" protocol, and the "Decorator" protocol, too?
I'd request some examples first
Iterator - [PEP 234](http://www.python.org/dev/peps/pep-0234/) for Python 2, and [PEP 3114](http://www.python.org/dev/peps/pep-3114/) for Python 3. ContextManager - [PEP 343](http://www.python.org/dev/peps/pep-0343/). Decorator - I withdraw this request. They are described in [PEP 318](http://www.python.org/dev/peps/pep-0318/) and [PEP 3129](http://www.python.org/dev/peps/pep-3129/). The "protocol" part of decorators is nothing more than requiring a function to be higher order, which requires no special support in Haskell.
I meant examples of calling python code from Haskell, python has a few libraries (complete garbage, but you don't want to rewrite it in haskell) that I'd like to use instead of writing all those regexen by myself.
No, protocols are not libraries. They are special magical method names which, when their implementations provide the semantics specified in the PEPs, allow you to use special built-in syntax. For example, list comprehensions in Python work for exactly those types that provide the Iterator protocol. All of the built-in container types provide this protocol, as do file handles, sockets, and many others. And your own user-defined classes can provide it as well. We are not so interested in the syntax here. But the author of this package is correct that conceptually, these official "protocols" play a role in Python which would be akin to "built-in type classes" in Haskell. So it definitely makes sense to provide type classes that allow you to reflect that in your code on the Haskell side.
Hm, weird, I'd not expected a minor-version bump to garner any attention. [The reddit thread for the initial release might be of interest](http://www.reddit.com/r/haskell/comments/ajbvb/cpython_haskell_ffi_bindings_to_python/). To answer your questions: * It's not possible to fully comply with the Haskell versioning policy, because Python sometimes makes backwards-incompatible changes in minor version increments. I thought about a scheme like 31.x.y.z, but that would be a bit awkward if there's ever a Python 3.10. * Hackage can't generate Haddock documentation for 'cpython' because the Python 3.1 headers aren't installed. The library itself *does* have Haddock documents -- download the tarball and run "cabal haddock". Most of the docs are adapted from the [Python/C API reference](http://docs.python.org/3.1/c-api/). * I'm working on a website for the library, which will hold examples and other docs; until then, existing Python/C examples are a good choice. The API wasn't changed much, or if it was, it was changed in a consistent way. &gt; Could we request the "Iterator" protocol, the "ContextManager" protocol, and the "Decorator" protocol, too? Iterator: sure, that's easy enough. [cpython-3.1.2.0](http://hackage.haskell.org/package/cpython-3.1.2.0) adds the `CPython.Protocols.Iterator` module. ContextManager, Decorator: these protocols are purely Python-level constructs; they do not exist in the CPython API. Therefore, there is nothing to bind to.
I'm working on some examples, but they won't be done for a while. In the mean time, you might look at the [Python/C API reference](http://docs.python.org/3.1/c-api/) -- especially the [introduction](http://docs.python.org/3.1/c-api/intro.html). As for Python libraries being "complete garbage"; most Python libraries are of higher quality than the corresponding Haskell library. Haskell still has no equivalent to [feedparser](http://www.feedparser.org/), [Genshi](http://genshi.edgewall.org/), [docutils](http://docutils.sourceforge.net/rst.html), or half a dozen other libraries I can't live without and which are too large to easily rewrite.
I was talking about different kind of garbage - libraries that provide nice api over something disgusting (like parsing html from a website) - it's bad in every language, because it was broken already at the other end - there's no nice remote api, just a www frontend designed for humans. writing this kind of things is easy (but I hate this kind of stuff), maintaining is the hard part.
Arguably, Haskell has a number of very good options for template support, including xml-based, though none is exactly genshi. docutils &lt;-&gt; pandoc? Feedparser I'll grant is a tricky one to replicate, because its fundamentally a huge bundle of special cases and regexps.
Most Haskell template libraries are either text-based, or compile-time. Genshi loads XML from a file, processes it, and outputs more XML. I haven't found anything that can replace it. Pandoc handles a small subset of docutils, but is very limited -- for example, it doesn't support custom directives at *all*, so 90% of extant ReST can't be parsed.
Pointfree FTW! All substrings : subs :: [a] -&gt; [[a]] subs = concatMap inits . tails All substrings, hiding duplicate empty substrings : subs :: [a] -&gt; [[a]] subs = ([]:) . concatMap (tail . inits) . tails All unique substrings : subs :: Eq a =&gt; [a] -&gt; [[a]] subs = ([]:) . nub . concatMap (tail . inits) . tails All unique substrings, sorted : subs :: Ord a =&gt; [a] -&gt; [[a]] subs = ([]:) . map head . group . sort . concatMap (tail . inits) . tails
For the former, have you looked at heist (http://snapframework.com/docs/tutorials/heist)? 
Sort of; I haven't used it, but based on online documentation, Heist is much much less capable than Genshi; Heist assumes the application and template logic are closely intermingled (Genshi separates them), doesn't support any logic in the template (not even loops or conditionals), and can't even insert dynamic content into the middle of some text.
live math, die young.
If you're abel.
This is the most awesome thing ever to grace the Internets.
iirc hackage 2.0 was supposed to let you upload your haddocks
&gt; Hm, weird, I'd not expected a minor-version bump to garner any attention. I guess some people agree with you that Python bindings are useful and interesting. :) &gt; It's not possible to fully comply with the Haskell versioning policy In practice it will rarely be a problem. The only way to avoid the problem completely would be to stick with the traditional method: keep the package version numbers independent of the Python version numbers, and document very clearly what Python version you are supporting in the hackage description, haddocks, cabal file, etc. &gt; Hackage can't generate Haddock documentation for 'cpython' because the Python 3.1 headers aren't installed. Oh really? Interesting. I don't exactly understand why - but don't explain it here, instead please post an issue on the hackage trac. &gt;&gt; Could we request the "Iterator" protocol, the "ContextManager" protocol, and the "Decorator" protocol, too? &gt; Iterator: sure, that's easy enough. cpython-3.1.2.0 adds the CPython.Protocols.Iterator module. Wow, great! Thanks. &gt; ContextManager, Decorator: these protocols are purely Python-level constructs; they do not exist in the CPython API. Therefore, there is nothing to bind to. ContextManager - true, it is only Python-level. But still, it would be nice to be able to represent its standard method names `__enter__` and `__exit__` as methods of a type class on the Haskell side, as you have done for the other protocols. And perhaps have something like Haskell's `bracket` function that works for instances of ContextManager. Decorator - agreed. I already withdrew that one in a previous comment. :)
I feel your pain, but I suspect that any change here will break type inference very badly.
I think Paczesiowa just wants some basic documentation of the whole library, in the form of some usage examples :)
I used to do a lot of O'Caml work, but, tbh, I just didn't find it as aesthetically pleasing as Haskell. Reading gobs of O'Caml makes my eyes cross.
We use O'Caml at work, and Haskell has spoiled me. It just isn't a language as nice as Haskell. The libraries are incomplete (basically, the standard library contains enough to write an O'Caml compiler and proof assistant with) and make bizarre choices: strings are mutable (why?) and just a general pain in the ass to deal with. Ocamlbuild sometimes works, sometimes doesn't. Polymorphic variants give incomprehensible type errors, and I'm not convinced that the typing system always computes most general types when using them. The language is needlessly complex: why the difference between "fun" and "function"? Infix operators are a pain: you can only use a limited class of identifiers for naming infix operators. etc. etc. Don't get me wrong: I prefer to work in O'Caml over Java. It's just that O'Caml isn't Haskell.
&gt; strings are mutable (why?) Because there is no separate byte array type. Because it is a legacy feature from a gentler, more low-level era. Because in practice nobody gets bitten by it.
&gt; a legacy feature from a more *violent*, more low-level era FTFY. :-)
&gt; in practice nobody gets bitten by it. ^^ epitaph on the tomb of the unknown developer.
I actually prefer ocaml to haskell. Crappy syntax aside, it feels like a simpler, more orthogonal language. I can wrap my head around the entire ocaml rts pretty easily and the module system causes me far fewer problems than type classes. I see plenty of beautiful ideas in haskell, but I get stuff done in ocaml. The haskell community is much more functional though. In the time it took to build cabal, hackage and the haskell platform the ocaml community still hasn't decided on a standard library, build system or package manager.
What parts of Haskell do you find it hard to wrap your head around?
Great summary, very useful for us OCaml illiterates. Thanks!
&gt; Because there is no separate byte array type. Haskell didn't have byte arrays (or even byte strings) either, originally. They got added afterwards. Why not in ocaml?
Can anyone explain the '_a restriction more detail? What would happen if mutables were allowed to be polymorphic? Would every different type-instantiation create a new unmutated value, which would break the semantics of mutation?
It is the same problem as `unsafePerformIO` and `IORef`. Polymorphic mutable references lead to unsoundness. To avoid this, MLs restrict all value bindings as being monomorphic.
Thanks!
So in summary: "More helpful error messages, Yay!"
That's really great improvement and also the article is very readable (for me not knowing much about type checker theory). I'd really like to read your thesis once it's finished (or even before).
I've heard people who use O'Caml professionally (and did their PhDs in Haskell) say that it is much easier to reason about, performance-wise (both time and space). Do you share this opinion?
Thanks! I will most definitely put it online, once it's finished. I don't currently plan on making unfinished versions downloadable. BUT! You CAN play with the unfinished type checker program itself, available at http://github.com/gergoerdi/tandoori . Be aware though that it is far from release quality.
Is this related to Camarao's [ML has principal typings](http://www.dcc.ufmg.br/~camarao/ml-has-pt.pdf)?
Yes. Especially space.
And so Yesod overtakes Happstack in the race to 1.0... keep up the good work, all of you. highly highly appreciated.
While realizing that you're being tongue-in-cheek, I don't think there's a lot of direct competition between Happstack and Yesod. The two represent very different points in the design space, and will coexist for some time to come. Far more interesting is the question of what will happen between Happstack and Snap. I'm watching that with interest.
Thank you for doing this work. I completely agree that its an immense advantage to be able to ask about the type information of a subexpression. I've seen *so* many new (and not so new!) Haskell programmers struggle with a type error, and finally result to a cycle of "add type annotation and recompile" until things make sense. If this were implemented for that nebulous language "Haskell with various GHC extensions" (a long way off, I'm sure), I'd hope that perhaps we'd even get a way to interactively (in GHCi, perhaps) ask about typing information for various subexpressions of code that doesn't type check. This could be *very* useful from a learning standpoint.
What's the complexity difference between linear and compositional checking? I'd imagine that at the very least, you'd have another O(n) space overhead to cache subexpression types.
I've recently taken over maintenance of MissingPy, and posted a new version (0.10.5) to hackage. * [announcement on haskell-cafe](http://www.haskell.org/pipermail/haskell-cafe/2010-October/085443.html) * [MissingPy on hackage](http://hackage.haskell.org/package/MissingPy) * [github repo (with bug tracker)](http://github.com/softmechanics/missingpy/)
I think that last issue is the biggest problem with ocaml. Yes, the language is ugly as hell, but I can deal with that. The fact that there is still no standard system for building and installing packages is terrible, the process of just trying to get ocsigen installed and working was enough to finally make me give up on ocaml completely.
This is nice in that it gets the incremental streaming aspect. Of course, if you have a fixed-size and can parallelize, then ripple-carry is much less nice than a kill-propagate-generate carry tree. Hmm. Redundant representations can also help in the streaming case, as you can defer some uncertainty to next digits. (Okasaki uses some of these representations for efficient functional data structures).
Maybe it is relevant. But this kind of type checking has advantage only for wrong programs so this could be run in a second phase if the program turns out to be wrong...
Wow that's great news! Thanks, and good luck in your new role. For me, a major advantage of MissingPy over cpython is its MIT license, as opposed to GPL. At least that's what Hackage says. But now that I can easily look at the source code via github (thanks for that!) I see that... it looks like MissingPy is sadly also GPL in fact. If that is so, and cannot be fixed, then I'm afraid you must update the cabal file promptly. Aside from that, I hope one of your first actions as maintainer will be to tell the world something about what versions of Python are supported. I realize that you may not know yet exactly how well various versions work, nor are you likely even close to deciding exactly which versions you would like to support. Nevertheless, please say *something*; it's very difficult to use this package without knowing anything at all about version support.
Since Hackage can't generate the API reference, I've uploaded a copy to my site: http://john-millikin.com/software/bindings/cpython/3.1.2.0/
Let me read that and get back to you shortly:)
Never fear! The next release of Happstack is likely to be 6.0. If I am not mistaken, the package version policy states that any API changes must result in an bump of the second component of the version. like, 6.1, 6.2, etc. So, we need to switch from 0.6, to 6.x, if we want to be able to release bug fixes to 6 that change the API before 7 comes out?
Does somebody have to "win"?
As these platforms develop and mature, we can all be winners!
This is awesome. I looked into to something similar, but my skills are shall we say 'entry level', and the time I could devote to the project short. Awesome. 
Redundant representation is in fact necessary (i.e. there is a proof), if I remember right. 
The only way to win is not to play ;). But in all honesty, I think there's been a huge amount of collaboration between the three frameworks. Snap is providing an incredibly fast standalone server for Yesod. Yesod produced Hamlet, which can be used with Happstack. Happstack begat web-routes, which is central to Yesod. And in the future, there's only more points for collaboration coming up. Jeremy and I have discussed work on packages to support things like reCaptcha and gravatar, where we really need a single, properly done implementation. Even just having the three frameworks in existence forces us all to do things better. WAI ripped off wholesale the CIByteString concept used for headers, and is that much better for it. I think http-enumerator is a very compelling alternative to the HTTP package, and it also has (hopefully) kicked off a more powerful, stream-based API to be added to zlib. tl;dr: If one of us wins, we all win. I don't think any of us anticipate our framework being the One True Haskell Web Framework: it's a big enough design space for everyone. Let's just keep working together where it makes sense.
Those of us using any of the above frameworks are the ones who win.
[tanakh](http://twitter.com/tanakh) has been building &amp; running a number of Games developed in Haskell and gives a short comment. I can see two of mine up there and one of the comments was quite funny for me he said the AI was weak, that's because my AI was random :D
Yes :)
Where's the drama? Where's the he said, she said? Where's the self aggrandizing blog posts pointing out why the other framework will never succeed? Where's the horse race? Everyone's just getting along and working together... typical Haskell community. How are we going to going to achieve popular success if we continually ignore these important keys to attracting attention? It's like someone deliberately chose to avoid success at all costs. (and thank you!) 
I agree. However, for those unfamiliar with the Oleg-workaround for this issue: http://okmij.org/ftp/Haskell/types.html#partial-sigs
I noticed that you only checked the random selection you make in the beginning (individuals in line 109). This is an advantage for the withCrossover because it checks 4 different in every step. But checking just the child at every step doesn't make any statistical difference, and doesn't favor any of them. [Here](http://pastebin.com/gdRKAQxt) is a version that does it this way, I also made some small cleanups. 
With it = undefined, sig = False and val = True you could write one of Oleg's examples as: bar | sig = it :: _1 -&gt; i -&gt; Int | val = \a i -&gt; a ! i GHC then derives the type bar :: (Ix i) =&gt; Array i Int -&gt; i -&gt; Int
Some discussion on mtl / monads-fd can be found on the [HP ML](http://haskell.1045720.n5.nabble.com/Haskell-Platform-Proposal-add-transformers-and-revise-the-mtl-package-to-depend-on-it-td3172040.html)
I never thought this day would come. Let there be much rejoicing in the lands!
Could we please have a wiki page that describes what users that current use either mtl-1.0 or transformers are supposed to do to upgrade?
Looking at the dependencies, mtl-2.0 depends on transformers (which makes sense). Transformers does not use any type-classes with FD or type famillies, those are provided by other packages such as monads-fd and monads-tf. I'm guessing mtl-2.0 is like monads-fd + transformers.
laziness
 Prelude&gt; import Data.IORef Prelude Data.IORef&gt; :t newIORef newIORef :: a -&gt; IO (IORef a) Prelude Data.IORef&gt; :t newIORef [] newIORef [] :: IO (IORef [a]) Prelude Data.IORef&gt; r &lt;- newIORef [] Prelude Data.IORef&gt; :t r r :: IORef [GHC.Prim.Any] Prelude Data.IORef&gt; writeIORef r [""] &lt;interactive&gt;:1:14: Couldn't match expected type `GHC.Prim.Any' against inferred type `[Char]' In the expression: "" In the second argument of `writeIORef', namely `[""]' In the expression: writeIORef r [""]
&gt; I'm guessing mtl-2.0 is like monads-fd + transformers. It is. See [the proposal](http://haskell.1045720.n5.nabble.com/Haskell-Platform-Proposal-add-transformers-and-revise-the-mtl-package-to-depend-on-it-td3172040.html), in particular: &gt; mtl-2 (the current monads-fd) depends on transformers and adds type classes using functional dependencies. It has the same modules as mtl-1 and usage is very close, except for the differences listed below. 
Absolutely. Im hoping that Ocaml Batteries will go someway towards fixing that problem.
Might I suggest http://vagrantup.com/ with chef as a way to provide a reproducible environment that your readers can build for themselves?
The MTL is dead. Long live the MTL!
Do you also compute principal typings? It seemed like it would be difficult to extend Camarao's work to handle explicit foralls.
[Ride the snake: Calling Python from Haskell](http://john-millikin.com/articles/ride-the-snake/) [[reddit](http://www.reddit.com/r/haskell/comments/dxj0e)] Hopefully this should serve as a useful example of how to write basic bindings; please let me know if you'd like anything clarified.
it's probably not that difficult, but often you want to use some obscure library that's not available in your system, so you distribute its copy in your project, and using that probably requires setting up some paths or env vars.
I wish there was a way to guarantee the lists are fused away. Lists are a wonderful abstraction for iteration, even in performance critical code. But they may really harm performance in some cases -- so when using lists you can never be sure if they will get compiled away.
If you want to distribute a pure-Python library, all you have to do is include it in the Cabal data files, and then add the data directory to Python's `sys.path` before importing. [Neil Mitchell has an example of adding Cabal data files](http://neilmitchell.blogspot.com/2008/02/adding-data-files-using-cabal.html)
Check out [oasis](http://oasis.forge.ocamlcore.org/).
Specifically, testing predicates over compact domains.
I've only begun to work in Ocaml, but I definitely agree. I find I get predictable O(n) space usage when using lists in Ocaml whereas in Haskell I get unreliable O(1) space usage which occasionally it turns into O(n).
So to upgrade your code either have it depend on mtl-2 or transformers.
Surely this is strictly better. After all, any function that is O(1) is also O(n), so you are reliably getting O(n) space usage in Haskell as well.
I don't have any problems with the concepts. I just tend to run into problems with typeclasses. For example, when working on [gmap](http://hackage.haskell.org/package/gmap) I had declare all the instances on newtypes to avoid problems with overlapping instances. The test suite for the same library has to go through some type trickery to allow the tests to be defined once for the typeclass and then run on many different instances. On the whole I've found that I run into such problems less when designing things with functors, at the cost of less magic and a little more typing. I also find it much easier to predict performance when using ocaml, not because of laziness but because the optimiser is so much simpler.
Oasis is great but its not the software thats the problem - its the fact that the community doesn't standardize on a single system.
The main difference for me is that the ocaml optimizer is incredibly simple. In haskell, the optimizations are sometimes quite fragile and so break under seemingly minor changes. 
It basically prevents you from doing: let tbl = Hashtbl.make 0 Hashtbl.add tbl "key" "value" (Hashtbl.find tbl "key") + 1 Initially tbl has type ('_a * '_b) Hashtbl.t . The second statement changes this type to (string * string) Hashtbl.t so that the third statement fails to typecheck.
indeed ;)
what's '_1' ?
wouldn't scopedtypevariables extended with an explicit forall over the whole toplevel function fix this?
[UHC][1] has [partial type signatures][2]. A partial type for a function can be `… -&gt; …`, or if you want both types to be the same: `%a -&gt; %a`. The ‘…’ dots are called wildcards and the ‘%a’ is a named wildcard. As the user manual mentions, this feature is convenient when a signature involves a higher-ranked type, which can’t be fully inferred, e.g.: foo :: (forall a. a -&gt; a) -&gt; … foo f = (f ‘a’, f True) This is inferred to be `foo :: (forall a. a -&gt; a) -&gt; (Char, Bool)` [1]: http://www.cs.uu.nl/wiki/UHC/WebHome [2]: http://www.cs.uu.nl/wiki/Ehc/UhcUserDocumentation#3_4_Partial_type_signature
As an author of this, I will say It is not an interpreter for STG it IS a compiler and it uses Javascript closures very much.
if you're using transformers and nothing else changes, if you using mtl, mtl 2.0 is basically transformers + monads-fd which means nothing changes except old code might need to make some minor changes to compile-time errors from small interface changes.
I'm missing. But I write a lot in Russian too. And not only about Haskell. Also missing: @alexott, @shebang.
Just a random name, like 'i', but it signals to the reader that you expect the compiler to fill it in.
Ah, my bad. It was apparently too quick a look... Have you tried a few simple performance tests? I don't expect it to be particular fast but it would be interesting to have a ballpark figure. E.g., 20x slower would still be a useful figure.
This is great! Is anyone working on the same for C++ (I know...)? Because I might need just that sometime in the future.
I feel confident you know about [Language.C](http://hackage.haskell.org/package/language-c)... but on the oft chance it slipped by you there's the link. (Yes, I know it isn't C++, just another related link)
What's the criteria for 0.3?
And just as I was considering writing a DSL for writing Java too. How convenient.
I think this system using arrows is really interesting because the first time I learned about arrows I always thought it was much like the Max/MSP audio programming language in a way.
alas, ListLike-1.1.0 has a dependancy on "mtl (≥1.1.0 &amp; &lt;1.2)"; thus also iteratee indirectly depends on mtl&lt;2, and thus snap-core, ... :-/
&gt; inventory-changing commands (like record and pull) now operate in constant time with respect to the number of patches in the repository This is great news! 
The 0.3 branch has been promoted to git master. Main new features: * a new "MonadSnap" abstraction which allows you to use ReaderT/WriterT/StateT/etc without having to "lift" all over the place * The `snap` project starter executable has been forked out into its own project and now contains a development mode which runs web handlers in an interpreter for rapid app development The main thing that's holding 0.3 back from coming out is that we're still depending on [iteratee](http://hackage.haskell.org/package/iteratee)-0.3.* -- there's some debate about whether we should move to 0.4 or cut over to the cleaner [enumerator](http://hackage.haskell.org/package/enumerator) package.
Depending on both mtl-1.* and transformers is ok actually -- we use transformers but lots of packages transitively depend on mtl-1. I expect this to remain so for quite some time, until everyone in the entire dependency chain cuts over to mtl-2. Want to help this move along? Bug package authors to upgrade to mtl-2 asap, for the good of humanity.
Was formerly linear time. See proggit discussion for details.
Hey that is awesome. I did not know that you could toggle the comment like that. +1
Something similar exists for C-style comments: uncommented: /* var foo = "bar"; /**/ commented: //* var foo = "bar"; /**/
I use the following to avoid the "nested /*" warnings from popping up: //* var foo = "bar"; // */
+1 for enumerator. Keep in mind that there's a growing collection of packages around enumerator, such as Yaml and JSON support, as well as http-enumerator. Using that last one, you could probably create a form of a proxy server easily using Snap (or maybe I'm just delusional).
I'm not getting clicking artifacts by doing it [this way](http://hpaste.org/fastcgi/hpaste.fcgi/view?id=27083). But it feels a bit too hacky for my taste...
ha! Let us know if any trick-or-treaters grok that. 
he should try using MaybeT IO and wipe those stairways of pattern-matches.
Buffer size of 16384 (more than a third second with 44khz) is way too large for anything realtime. I always got clicks with smaller buffers, and looking at your link I basically did the same. Finally I gave up and concluded that OpenAL is simply not suitable for realtime playback...
How about this for C (or Haskell when using the C preprocessor): #if 0 var foo = "bar"; #endif The best part? You can even nest these. Edit: I need to add *four* spaces to have it treated as code? You know what these things cost?
So what do you think the best option is for streaming sounds that are mixed on the fly? I’ve had bad luck (read: random segfaults) with Mietek’s bindings to the PortAudio callback API, while the list-based binding currently used by Hemkay is way too slow.
Hey, nice trick! You have inspired me to try a new way of commenting.
It seems to me that you cannot avoid being OS-dependent. On windows, you have DirectSound; on osx, you have CoreAudio; on linux you have many options, most of which seems to suck; i would probably try ALSA. Of course, if there was a package with a common interface for these, life would be easier.
Neat! This is actually easier then using the excellent Vim plugin [NERDCommenter]( http://www.vim.org/scripts/script.php?script_id=1218) The only problem is it takes up extra lines.
Yes there is. You can use a higher-level API like JACK, or use SDL, which wraps the low-level os-dependent interfaces.
... it seems back on-line now.
SDL may work, I never actually tried. I thought Jack was Linux-only, but now I see it isn't; though it is not clear from a quick google search that it can actually produce sound on Windows. In any case, I was thinking of a thin layer on the top of the OS-level sound services so that you can stream stereo output with low latency on Windows / OSX / Linux(ALSA). Generally I dislike big dependencies. OpenAL would be perfect except it is braindead stupid...
Hi Patai! I haven't had the time to work on the PortAudio bindings since last December. I hope to improve the bindings during the Haskell Hackathon next weekend.
I bet a lot more people think this is a Half-life reference.
Using the [package dependency monitor](http://packdeps.haskellers.com/) can help in this situation.
It's great to explore this kind of thing, but what makes people think that a garbage-collected languages is going to work well for audio synthesis? (Being a use case with real-time constraints.) 
I so wish I had thought of that!
Related question on SO: http://stackoverflow.com/questions/2223866/haskell-audio-output-on-os-x I've been able to output audio without "clicks" using that method. (would had left comment in blog but that requires signing up to stuff)
Ie: http://www.reddit.com/r/programming/comments/dyry3/darcs_25_released_record_and_pull_run_in_constant/
http://www.youtube.com/watch?v=Tmhp_NEH12Y
&gt; what makes people think that a garbage-collected languages is going to work well for audio synthesis? I wouldn't rule it out so fast, but you might call my suggestion cheating: use a Haskell DSL to generate C and avoid the whole garbage collection mess.
I had to do the same thing recently, and wrote [this article](http://jystic.com/2010/10/20/installing-gtk2hs-on-windows/) as a guide for my future self. It also has instructions for getting native looking themes working on windows.
I have a [lambda tattoo](http://twitpic.com/1um42n) on the middle finger of my right hand. Most people think it's an upside-down "y" (though that doesn't actually work, you need to reflect it of course) but the second most common suggestion is indeed Half-Life. :-) 
Oh wow, now I feel silly for making a blog post out of that. I wrote this up about a month ago, but I was moving my junk to a new VPS so I didn't get around to putting it on the web until yesterday... if your post had been there when I was searching for this stuff, I wouldn't have bothered writing this. Thanks for the heads up!
Do you disagree? Post away on SO!
Man, that was quick! From -cafe talk to reality in no time!
Or generate LLVM IL and the you can do just-in-time compiling, and have interactivity. In any case, I don't think GC is the main bottleneck here.
Wouldn't type-level lambdas make instance resolution hilariously difficult to do? IIRC their omission is deliberate, anyway.
Yes. It makes inference in general impossible. For instance, what is the solution to the following unification problem: s -&gt; (s, a) = m b, Monad m In Haskell, it's easy: m = (-&gt;) s, b = (s, a) With first-class functions at the type level, and instances, another solution is: m = \t -&gt; s -&gt; (s, t), b = a And the monad instance is the one for `State s`. Or even simpler: a = m b That's straight forward now (it's already a solution). But with first-class type functions, another solution is: b = a, m = \t -&gt; t And a third is: b arbitrary, m = \_ -&gt; a How do you decide between these? (You don't.)
I thought so, but I didn't want to claim it actually impossible if I wasn't sure. :) Thanks for the clarification.
This is pretty much the problem with type inference failing because of type families being potentially non-injective, right?
These two projects have very different aims, and so I don't think are comparable. Haskell already has a large collection of packages, installable and usable with a simple command line: Hackage via cabal. Haskell Platform, on the other hand, I see as intended to be a stable, consistent base system which other software can base upon. As such, I'm pretty happy with it.
Homebrew and the Haskell Platform look like entirely different things to me, with entirely different goals. The Haskell Platform is intended to be a stable environment for creating Haskell projects. Homebrew looks like one of the many package managers for Unix tools on OS X. If anything it would be more apt to compare Homebrew with Hackage/cabal-install, I think. Hackage has many packages added and updated [every day](http://hackage.haskell.org/packages/archive/recent.html).
Hackage has had more the 80 packages uploaded/updated per week for the last couple of weeks :)
I like the constraints placed on HP. It ensures that the core install of GHC has a useful amount of quality, interoperable libraries that will allow a developer to perform non trivial tasks. Quality control and interoperability are the key parts. That said, I see no reason at all why there couldn't be a less formal glob of useful Haskell libs that aren't found in HP. However, my concern is that things move so quickly in Haskell, and there are different camps which have differing opinions as to optimal approaches (and their corresponding libraries) See OCaml for an example of this problem. There is a set of libs based around microthreading via monads (Lwt?). This is incompatible with Batteries. Both of which are incompatible with extlib. All three of which are incompatible with Jane St's Core and Core-Ext and family. I don't want to see the same thing happen to Haskell. Or, if it does happen, I would like to see it happen in an organized manner. Some camps around which libraries might be built would be: * Applicative Functors vs Monad Transformers * Lazy IO vs Enumerators/Iteratees * FRP vs ?? I don't think these categories are mutually exclusive, I'm just saying that it isn't as easy as lumping in a bunch of processes which run on the command line. 
He must be using Haskell. And the Yesod web framework.
Can you elaborate how they have very different aims? Homebrew does not provide packages but provides packages descriptions which are guaranteed to work together. To me that's not much different than the Haskell Platform.cabal file
Homebrew does not provide packages but provides packages descriptions which are guaranteed to work together. To me that's not much different than the Haskell Platform.cabal file. Can you explain where you see the difference?
Looking forward to it!
I don't know Homebrew, but one of the goals of HP is not only to guarantee things working together but also to guarantee a high level of quality. That is, HP doesn't just say "here, these work together" it also says "these libraries are blessed and are assured of good performance, stability, and usability". If you just want things to work together then use cabal-install to do constraint satisfaction over all of Hackage.
&gt; Some camps around which libraries might be built would be: &gt; &gt; * Applicative Functors vs Monad Transformers There's no reason Applicative and MonadTrans can't live together. They just do different things. Every monad ought to provide an Applicative instance, for completeness; which API client code uses doesn't matter. But there are some Applicatives which aren't monads, so you can't really get around the difference. &gt; * Lazy IO vs Enumerators/Iteratees If the I/O is cleanly separated from the data manipulation, then the lazy IO part can be replaced pretty easily. But, after playing around with iteratees on my latest project, I think the bigger conflict is between enumeratees vs list/stream/array fusion. They both solve the same kind of problem (and in much the same way, ultimately) but there's no quick and easy way to convert between the different approaches. We're already starting to see a balkanization of these different types of stream/sequence manipulation and none of them looks like it'll be a clear winner. I wouldn't be surprised to see a stream-based version of FRP jump into the fray once someone tries to iteratee-ize event processing.
It is not clear at all to me what the video is trying to show. [edit] My mistake, the text comment on youtube is fully explanatory.
The two problems are similar, although I'm not sure they're the same. The problem with type families is one of checking. This is sort of related to inference, I suppose, because GHC's type checking algorithm relies on inference. Eventually it can get to a situation where it needs to verify that `a = b`, and it knows only that `F a = F b`. If `F` is not injective, you cannot conclude the former from the latter. However, checking terms with arbitrary type functions isn't generally a problem if your algorithm for it doesn't involve making up new fresh variables and collecting constraints on them. The inference problem is more general, in that you don't merely have to consider `F a = F b`, you may have to consider `Y = f X` for some expressions `X` and `Y`. That is, it's not just about, 'is this function injective,' but, 'do there exist any functions relating `X` and `Y`?' In ordinary Haskell, the only candidates for `f` are partially applied type constructors (partially applied type synonyms and families aren't elligible), so we see if `Y` is an application `F E`, set `f = F` and unify `E` and `X`. In the presence of first-class functions, though, there are other solutions, and no most general solution. Edit: I should note: if you _do_ make type families more first-class, you have the same problem. You then have to decide if there are any families `F` in scope such that `F X = Y` for some `X`.
Hmmm... so one of the answers is written by a Chris Smith, which is not me. I always hope anyone participates in the Haskell community... but that's gonna get confusing! :)
Cabal configuration file also provides package descriptions with version ranges showing what will work together with what.
&gt; If the I/O is cleanly separated from the data manipulation, then the lazy IO part can be replaced pretty easily. It might actually be impossible. Lazy I/O influences library design. Consider this definition of a parser: type Parser a = [Token] -&gt; (Maybe a, [Token]) This parser only runs in constant space if the token stream is generated lazily. The token stream can only be generated lazily using lazy I/O. Thus, with this definition of the parser type, code that's written using lazy I/O cannot easily be turned into iteratee code. Lots of Haskell libraries have been written so that they can only be used (efficiently) with lazy I/O. In my opinion, this is the biggest problem with lazy I/O. It forces programmers that don't want to use lazy I/O to rewrite a whole bunch of libraries. A solution to the problem could be to use continuation passing style. Consider this alternative definition of Parser: data Parser a = Done a [Token] | Failed [Token] | Partial ([Token] -&gt; Parser a) It allows the caller to incrementally feed data to the parser, without using lazy I/O. It also allow a lazy I/O version to be written on top of this interface. 
I guess I'm a little disappointed to see that the bindings between two language implementations, both of which are under liberal OSS licenses, is itself under GPL3.
Haskell Platform provides a default set of basic libraries which app developers can expect to be available, tested and slow to change. This seems similar to, say, Python which comes with a useful set of in built libraries (and which pre-HP GHC lacked). Homebrew seems much closer to the community supported package set of a Linux distro, except for OS/X. A good thing for those Mac users who have missed out on something like Ubuntu's universe repository. Nothing unusual in the packages of a linux distro being tested to work together. To me, these two endeavours share only a few basic characteristics which are common to any open source infrastructure project. Haskell also has the cabal package infrastructure to distribute community packages (like many modern language implementations), the fact that HP is implemented as a .cabal file doesn't mean that much. The effort is in the selection and management of a core set of libraries.
Exactly! And when there are difficulties with the Haskell Platform progressing slowly, it's often to do with trying to revise packages to improve their quality. That's simply *not* a goal of Homebrew, and as a result, it's useless to compare the Haskell Platform's problems with this project, even though they may share some of the same integration testing goals (which are adequately handled by both projects).
Homebrew is a replacement for MacPorts and/or DarwinPorts. It has no similarity to Haskell Platform. Homebrew is not even in the same league as Hackage/Cabal, as it is used for installing *any* OS X program and not libraries for a particular language. The right comparison would be "Can Haskell Platform learn from Ruby's stdlib", in which case I would argue that it wins outright by a large margin!
I was speaking from my experience where it has tended to be quite easy. To me, that parser definition looks woefully unconstrained and it's not clear exactly what the semantics are intended to be (in terms of the formal class of grammars permitted). If we really allow all parsers of that type then there are no guarantees that it runs in constant space (except in the most trivial sense) since a parser could look at the entire stream of tokens and then reorder the tokens arbitrarily in the result. Assuming the parser only looks at a finite prefix of the token list, then I don't see why lazy I/O is required. It seems like all you need is a predicate for detecting whether a parser would return a result when passed (xs++undefined). Then it's just a matter of writing a converter from parsers to iteratees using that predicate to decide when to reach the done state. You can use the same unsafeInterleaveIO trick as lazy I/O to write such a predicate without I/O actually being involved. Though, granted, this wouldn't be trivial by any means.
This looks very interesting. I can't wait until we can see more than just a blurry YouTube screen capture video.
Hackage has [14 packages (moving average) updated daily](http://www.galois.com/~dons/images/hackage-daily-graph.png) -- there's your homebrew. If you've got ideas on how to ensure QA, without an extensive review process, I'm all ears.
Maybe that was just wishful thinking on my side. When i was still programming in Perl 15 years ago, you could do amazing things out of the box. I had high hopes the HP would provide a similar experience. It seems i don't have to hold my breath, thanks guys
We need to move faster to pull in more things from Hackage -- but it is hard, people have high standards, and day jobs.
I think the slow progress indicates a conceptual problem. And unless this is solved, i don't see how the HP will be relevant to people with Haskell day jobs anytime soon.
How does the concept of "blessed" libraries relate to you and to your work when after 2.5 years there are no "blessed" libraries beyond what came with ghc?
The title doesn't exactly scream it, but this paper is about fixing the unsoundness that arises from combining type families with generalized newtype deriving (http://hackage.haskell.org/trac/ghc/ticket/1496).
If you switch to 720p it shouldn't be so blurry.
thanks for posting - I had to google a bit to find Chris' git repo: http://github.com/chrisdone/shime
&gt; i don't see how the HP will be relevant to people with Haskell day jobs anytime soon Interesting. At least going by anecdotes, and numbers, having the current small HP core is already useful to a lot of people. We have regularly used it at work to flash a solid Haskell environment onto machines, for example (and there have been 700k downloads). However, I would say the biggest problem is the community consensus path is extremely slow. Secondly, only two libraries have been proposed for the HP this round (and both will be accepted). It is hard to grow if no one wants to propose adding anything.
This last 6 months was the first full round of "active" Haskell Platform, where all processes are in place. 2 libraries have been proposed for the January release. It's a step. By all means help out: this is a volunteer effort. Concrete things you can do: propose libraries. Alternatively, identify flaws in the process of adding libraries and reviewing them, and recommend changes to that process.
I had to install transformers-0.2.2.0 to get DRBG to build. In any case, I'm happy we have more crypto-ness! w00!
They're very pretty.
Seeing as DRBG doesn't (directly) depend on transfomers this represents an interesting depencency leak in MTL/Transformers. Could you be more specific about what the issue was? 
That is a seriously cool visualisation.
I don't explicitly mention Haskell in this post, so I'll make it clear that comicbake is written in Haskell and the there's a darcs repository you can check out if you're curious. I'll do a post later on about some more technical stuff when I have something worth saying. :-)
re: bubble types I prefer Bob (thinking): text to Bob: (thinking) text The former is more clear &amp; script-like and doesn't limit the things people can say. What did you use for image processing libraries?
What imports were used in the script? 
Thanks for the feedback! I wrote the parser a while ago and remember it being an unholy mess, so I'll probably have to do a lot of relearning before I add in thought bubbles. Your idea is the nicest one so far and one that I'll probably go for. The text, bubbles, margins etc are all done with GD bindings, which are very sparse at the moment. The original library's large but few of its functions have Haskell bindings. I have written some patches for the bindings to make my life easier, but relying on them directly just no means no-one else will be able to use my code until the patches appear upstream. The stitching together of images is done with the hsmagick bindings. I could have used GD for this stage too, using `newImage`, `copyImage` and pasted each panel into the resulting frame, similar to how I put the margins on the panel frames, but hsmagick has a simple stitch-images-together function that I was more than happy to use. :-) Ultimately there is no graphics library that has the wealth of features I'm looking for, both simple line graphics and higher-level manipulation. My original code was written against Brent's Diagrams package (which used Cairo exclusively at the time). I even added some text support so it would create speech bubbles, but it seemed its grasp of dimensions and co-ordinates was very loose and wouldn't "fix" things to a spot without much coaxing. Well, that was a long rambling answer!
My guess is if you try to build DRBG with MTL v2.0, this version depends on transformers. MTL 2.0 is transformers + monads-fd. I haven't tried this out.
I suspect if instead of just music and lots of text flying by, that if a screencast explaining more of what was going on was presented, we'd be more engaged by the demo. I always appreciate efforts to better integrate some language with Emacs, as that's where I spend a lot of time.
That's the only way to build DRBG. I explicitly made DRBG depends on mtl == 2.*. I'm guessing either transformers didn't follow PVP or mtl has too broad a range in the build dep. EDIT: And it looks like we should blame transformers. 0.2.* is the mtl dep and people say transformers 0.2.1.0 doesn't work while 0.2.2.0 does (but no one has told me why 0.2.1.0 doesn't work!)
if you spot errors or have suggestions, please let me know here so that i can fix them! thanks!!!!!!!! i love you!!!
I was a waiting a looong time for this one :-)
I just wanted to say thanks so much for doing this. Real World Haskell seems to fly over my head, but this is so clearly written and has made learning Haskell a pleasure. As for my question, do you plan on adding to this any more?
Are you planning at some point doing an Iteratees section? Or are you done for good with LYAH? :-o
hey, very glad you like it! probably not planning to add more to this, but i have set up a blog to which i will post various stuff. anything specific you'd like to see?
This is a *great* question. I'm wanting to learn how to use Snap, teach me, please.
No.... Don't become a blogger! You're the Haskell doc god; the world can't take another Joel on Software nut. Continue your master-planned work of making Haskell easier to learn than PHP.
You don't need to understand iteratees to uses snap. You just need them to hack it.
I Agree with Evan, the Blogger approach doesn't have the consistency that LYAH or RWH has, We all know every chapter needs a lot of work (your drawings are awesome), but is way better to the community if you keep it this way :-) Good Job, I already read the Zippers chapter, and I got it right away, I spent like 1 day with papers elsewhere and didn't get the grasp of it, man you have the skill to make the difficult/different look simple.
Any plans to write Learn You An Agda?
*Now we've equipped our trees with a safety-net that will catch us should we fall off. Wow, I nailed this metaphor.* My favorite line :)
I second this idea, an Iteratees and Enumerators section would be cool.
maybe you can mention derivatives of data types, just because they are so cool
ziper -&gt; zipper i love you, too!!
This book is so much epic win.
A well-worn reference to a mediocre webcomic? I'm not sure that qualifies as "brilliant". But yes, LYAH does rock.
Looking forward to buying the book!
The `CRandom` keeps around the same unfortunate assumption from the `base` library that we should assume all of the instances are bounded and ordered. As a result instances for types like `Integer` get artificial bounds and types like encryption keys get artificial orders. Furthermore, I prefer a formulation where there isn't an assumption that there is a generator value. Traditional pure generators can be hidden inside a state monad, but IO sources of random numbers have a harder time of providing a generator value (e.g. reading from /dev/urandom) I have gotten a lot of mileage out of the following typeclasses (and appropriate instances) class Monad m =&gt; RandomM m where getRandomInteger :: Integer -&gt; Integer -&gt; m Integer class Random a where random :: RandomM m =&gt; m a class RandomR a where randomR :: RandomM m =&gt; a -&gt; a -&gt; m a 
&gt; A well-worn reference to a mediocre webcomic? I'm not sure that qualifies as "brilliant". do you find this shallow and pedantic?
This chapter helped me to get intuitive understanding of zippers. I wish I had this comic book when I was 10yr old:) Perhaps it is better to use (|&gt;) as in F# instead of (-:). It feels more "standard".
Bought it. It requires some concentration for the greedy algorithms chapters (at least for me :)). Otherwise, it is highly enjoyable.
The thing with learn you an Agda is that it wouldn't really work as an ongoing series. At some point, it'd have to stop.
Unless it's a coblog.
The "Empty" lines of the Maybe versions of goLeft an goRight aren't quite right.
This is the kind of book you want to buy copies of to all your programmer friends.
Hmm, I don't think blogs are generally suitable for corecursion in Agda, due to not being... *productive*. But anyway I was thinking of another book instead of Agda-themed blog posts, and those generally do end. Not that I expect there's any chance he'd do it.
Prolog?
I'm not sure I follow...
I wrote it after reading the title but not the additional text. In Prolog you program using relations instead of functions. Not sure if this is related in any way to what you are asking though.
ooh, nice catch! fixed!
&gt; Not sure if this is related in any way to what you are asking though. Neither am I! They could be the same or they could be only related in name. That's why I was hoping someone here would know the details :-)
I'm not sure about good primers, but a relation between a collection of sets is a subset of their cartesian product. A function is just a kind of a relation, so in that sense, relations are strictly more general than functions. For example, you might have an equality relation on natural numbers. This is a binary relation (involving two sets, which happen to be the same), so it is represented by pairs of naturals: (0, 0) (1, 1) (2, 2) (3, 3) ... Another binary relation on naturals might be "less than or equal to": (0, 0) (0, 1) (0, 2) (0, 3) ... (1, 1) (1, 2) (1, 3) ... And you can view a function as a binary relation between its domain and its codomain. If we take the function (+5) that adds 5 to its input, we get the relation: (0, 5) (1, 6) (2, 7) (3, 8) ... You can also have ternary relations, for example, equivalence modulo a number: (0, 0, 3) (1, 1, 3) (2, 2, 3) ... (0, 3, 3) (3, 3, 3) ... (6, 9, 3) Anyway, once you have the basic idea of a relation down, you can look at abstract properties of relations. For example, we talk about an equivalence relation as being one that's reflexive (`forall x, (x, x)`), symmetric (`forall x y. (x, y) implies (y, x)`), and transitive (`forall x y z. (x, y) and (y, z) implies (x, z)`). See if you can describe the property that makes a specific relation into a function!
hmm, i gotta admit i don't know iteratees quite well enough right now to be able to write a chapter about them. however, for your iteratee needs, there's this great post http://cdsmith.wordpress.com/2010/05/23/iteratees-step-by-step-part-1/ that helped me learn iteratees
haha, i wasn't planning on being anything like those opinion bloggers. i was thinking of making blog posts that are like small LYAH chapters, along with pictures and everything. sometimes i just want to write (and draw) about some concept that doesn't fit into the LYAH sequence of chapters so well.
it would be awesome if someone made that, i would so buy it
hmm i thought about that, but isn't |&gt; pretty much function composition in F#, whereas -: in my example isn't function composition but function application
&gt; The CRandom keeps around the same unfortunate assumption [that] all of the instances are bounded and ordered. True, but I haven't seen these assumptions really harm anyone. Lack of an ordering makes crandomR less sensible (but not necessarily useless) and same goes for crandom on boundless types. &gt; Furthermore, I prefer a formulation where there isn't an assumption that there is a generator value. Traditional pure generators can be hidden inside a state monad And traditionally such state monads can be made instances of MonadCryptoRandom. As for: class Monad m =&gt; RandomM m where getRandomInteger :: Integer -&gt; Integer -&gt; m Integer class Random a where random :: RandomM m =&gt; m a class RandomR a where randomR :: RandomM m =&gt; a -&gt; a -&gt; m a I dislike this design as it forces all random values to be coerced through the Integer type. What if I want a 256 bit nonce? It seems I must do: instance Random ByteString where random = liftM (padTo 256 . encode) (getRandomInteger 0 (2^256-1)) But that isn't right either as the normal "encode" routine for Integer has overheads, perhaps manual byte-shifting would be used. At any rate, the RNG should produce bytes in the fastest most generic manner possible (ByteStrings are in vogue for now). Note my words might sound more critical than I feel - perhaps the interface should be something like what you suggest: class Monad m =&gt; RandomM m where getRandomBytes :: Int -&gt; m ByteString class Random a where random :: RandomM m =&gt; m a class RandomR a where randomR :: RandomM m =&gt; a -&gt; a -&gt; m a I'd have to think on this but am not feeling there's strong enough justification for the change right now. Perhaps I can be convinced.
Maybe you can start [here](http://www3.di.uminho.pt/~jno/) and see where it leads you.
Nope, F#'s `|&gt;` is essentially this in Haskell: infixl 0 |&gt; x |&gt; f = f x
[The Reasoned Schemer](http://www.amazon.com/Reasoned-Schemer-Daniel-P-Friedman/dp/0262562146/ref=sr_1_1?ie=UTF8&amp;s=books&amp;qid=1288888264&amp;sr=8-1) by Friedman, Byrd and Kiselyov
Just don't disappear off the face of the earth leaving everyone going "wtf".
also btw i got your comments on irc, forgot to mention that! i'm gonna incorporate some of your suggestions along with some other errors and things that we caught during the editing phase.
Ahh, i always thought it was flip (.)
&gt; A function is just a kind of a relation, so in that sense, relations are strictly more general than functions. This seems to be the key fact whenever they are brought in to these papers I've been attempting. Assuming a relation `split` which magically splits a list into two sublists at just the point you want it to in order to solve the problem :-) Edit: Since dmhouse pointed out that the last sentence was a question I guess I should be more explicit with my answer. `split` is returning a specific pair `(xs,ys)` such that `xs ++ ys` is equal to the input *and* that `xs` and `ys` are the *right* sublists for whatever task they're needed, out of the complete set of sublists that could be produced. So it seems a relation produces lots of answers where a function would produce just one. In this case it's easy to create a `splitAll` function which produces the list of all possible sublists but you still have to hunt for the right sublist...
I'd urge you to dwell on it, and make it fit. LYAH is by far the best documentation for Haskell. And, dare I say it, you're treading on the title of "best entry level programming book period." You're certainly setting yourself up to be the next Randal Schwatz. Don't get sidetracked in the world of programmer blogs.
EDIT: I originally had the below in plaintext, but I've just noticed godofpumpkins set it as an exercise, so I [rot13](http://rot13.com)'d it. Fcrpvsvpnyyl, n shapgvba s : N -&gt; O vf n eryngvba ba N k O fhpu gung sbe nal k va N, gurer vf bar, naq bayl l va O jvgu (k,l) n cnve va gur eryngvbafuvc. Va bgure jbeqf, s(k) rkvfgf naq vf havdhr -- shapgvbaf ner qrsvarq rireljurer ba gurve qbznva, naq bayl gnxr n fvatyr inyhr gurer.
Like xkcd?
I didn't notice that the last line was a question, thanks for pointing it out. I think I read it more like "So you can describe the property that makes a specific relation into a function", which in hindsight doesn't add much!
iirc they use `&gt;&gt;` for that in F#
For the sake of completeness: - F# uses `|&gt;` and `&lt;|` for function application, the former working like Haskell's `($)` in reverse, the latter I'm not sure about (I forget what precedence/associativity it has). - If you squint your eyes a bit `|&gt;` is also equivalent (up to isomorphism) to `(&gt;&gt;=)` specialized to the identity monad. - F# uses `&gt;&gt;` and `&lt;&lt;` for function composition, which are equivalent to `&gt;&gt;&gt;` and `&lt;&lt;&lt;` in Haskell (shocking and unexpected, I know). Note also the following equivalence: - `h $ g $ f $ x` - `h . g . f $ x` - `h &lt;&lt;&lt; g &lt;&lt;&lt; f $ x - `f &gt;&gt;&gt; g &gt;&gt;&gt; h $ x - `x |&gt; (f &gt;&gt;&gt; g &gt;&gt;&gt; h)` - `x |&gt; f |&gt; g |&gt; h` 
Relations are also a special case of functions. :) You can encode the relation `r : T ⇔ U` as a function `f : T → U → Ω`, where `Ω` is a type/set/whatever of 'truth values,' assuming something like that exists. In a topos, this is rather obvious from your description of relations as subobjects of a product. Subobjects of a given object `X` are in one-to-one correspondence with functions `X → Ω`.
`getRandomBytes :: Int -&gt; m ByteString` is a quite reasonable choice for a `RandomM` class. I wouldn't disagree with that. I would disagree with making a specific `Random` instance for `ByteString` to support nonce usecases at a hard-coded bit-width, but would prefer a `newtype Nonce256` with a specific instance or perhaps another function like `randomNonce :: RandomM m =&gt; Bits -&gt; m Nonce` &gt; And traditionally such state monads can be made instances of MonadCryptoRandom. Why this is true, my concern is that the `CRandom` methods are exposed to the generator value and are pure in implementation while in this alternate approach the instance is allowed to make as many calls to a hardware random number source as is needed.
&gt; in this alternate approach the instance is allowed to make as many calls to a hardware random number source as is needed. So you didn't really want `random :: RandomM m =&gt; m a` but `random :: IO a`? I'm not clear on how you say this can access hardware RNGs and use any `RandomM` at the same time. Sorry if I'm being dense. I'll look again tomorrow when my head feels clearer.
...and don't forget about Arrows :)
I just requested that my local library purchase a copy.
Foldable gets a brief mention, Traversable none at all, I think. Much more basic and sane to deal with those, which I feel are underloved, than to go further. Also, maybe one could go further with Monoids. The reader monoid (one of my faves) doesn't get a mention. 
No, but by operating in the context of `RandomM` the `Random` instance would be able to make multiple calls to `getRandomBytes` (each of which might have a side-effect).
I had a lot of fun playing with [Alloy](http://alloy.mit.edu/alloy4/).
Bonus, I hope you read this. Did you get my email with corrections that I send to bonus at learnyouahaskell dot com?
Chapter 8 * Switch a&lt;-&gt;c and b&lt;-&gt;d: fmap :: (a -&gt; b) -&gt; Barry c d a -&gt; Barry c d b Chapter 9 * Full stop after first "I/O action", capitalize "if": "It takes a boolean value and an I/O action if that boolean value is True, it returns the same I/O action that we supplied to it." * "sequence takes a list of I/O actions and returns an I/O actions" * "short again" is not shorter than 10 characters :) * "That list of string is then filtered" * s/So the exception thrown in the toTry I/O action that we glued together with a do block isn't caused by a file existing/So if the exception thrown in the toTry I/O action that we glued together with a do block isn't caused by a file not existing/ (reword last part of sentence maybe) Chapter 10 * s/and ["3"] as the item/and "3" as the item/ * undefined is not defined till chapter 11. Chapter 12 * do first &lt;- landLeft 2 start Nothing second &lt;- landRight 2 first I'm not too sure about it, and they yield the same result, but maybe it would be clearer for the reader to change this into (because before we were throwing bananas like this: landLeft 1 &gt;&gt;= Nothing &gt;&gt;= landRight 1) do first &lt;- landLeft 2 start second &lt;- Nothing third &lt;- landRight 2 second
The book ["The Art of Prolog, 2nd Edition Advanced Programming Techniques"](http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;tid=8327) is a very good book on Logic programming not just about learning Prolog. I read the first 5 chapters, you should check it out. As someone mentioned already Logic programming is about writing programs with relations and logic variables, interesting way to solve problems.
a proper definition of a mathematical function (e.g. f(x)=y) says that f is a relation that meets some conditions... so it's just a function.
probably going to buy this when Learn you a Haskell comes out
That definition says that functions are relations, not the other way around.
 type Named a = (String, a) getName = fst stripName = snd Add a little template haskell, and we're set. :-P
I'd have used .: because it's so similar to invoking a method of (on) an object.
What a wonderfully gruesome hack. 
They're related, though maybe not in a way you'd find helpful at the moment. The real take-home message regarding logic programming as using relations instead of functions is that (a) relations can give multiple answers, and (b) relations can be run backwards. For the second point I mean that we could define a ternary relation `add(A,B,C)` which is true when A+B=C. Normally we provide a concrete a and b in order to learn what c is; but we could also provide a and c in order to learn b, in which case we're using the addition relation backwards in order to do subtraction! Of course, we can write functions `f :: A -&gt; [B]` to simulate the multiple answers of `f \in A*B`. And relations are only really invertable when they're defined in the right way, i.e. so that they provide a computable function in both directions (thus `g \in A*B` is `(g,g') :: (A-&gt;[B],B-&gt;[A])`). Since Prolog isn't pure, getting truly bidirectional relations isn't automatic. This is what's known as "modes" in logic languages, where the modes "in" and "out" indicate whether a given argument must be ground or may be unground. As doliorules says, instead of taking the set-theoretic definition of functions and relations which makes functions a subset of relations, you could take the topos-theoretic definitions which make relations a subset of functions. Neither is truly more general, it's just a matter of how you choose to formalize the different ideas.
thank you for the article i am trying to do a project in my class for comparing haskell using llvm and it will be very useful
I'm more curious about when -fdons will ship standard with ghc, causing the core to be forwarded to you for performance analysis perhaps for a small fee. That said, I can understand the concerns that this feature might not scale. I loved the little "Don Stewart" morphism on his optimization and compilation diagram.
Personally I think the bikeshedding relating to text is getting a bit egregious.
Yes, hence the [call today](http://www.haskell.org/pipermail/libraries/2010-November/014868.html) to focus on value to the community, and not excessive perfection-lust. Edit: I'm not sure there was true bikeshedding, however. We had 107 emails from 29 participants, covering 4 main topics. If we ignore the side discussions, it was well focused. Some very thorough reviews took place as well. More problematic is that it was unclear how the process would end, and the lack of involvement of the steering committee. Hopefully the community will get more efficient at this, as we go along.
Is this an official repository? There's no link from the UU web pages to this repository, it seems.
The thing that's giving me a headache about it is that text is already better than 95% of the other libraries we grandfathered in that are lacking in one way or another -- poor or missing api docs, wacky interfaces, no test cases, benchmarks, etc. The question people need to ask themselves is: are the minor discrepancies between text and Data.List/bytestring (which Bryan has reasonable arguments for!) serious enough for you to vote *no* on the proposal? I think that would be ridiculous: quality-wise text is already so far above the median that if we rejected it on these grounds it would mean that something was seriously broken in our process. 
This is lots of fun! However, like a thrilling ride on NASA's weightlessness simulator aircraft, be careful not to have just eaten a meal before experiencing it.
I thought people were trying to deprecate mtl in favor of transformers, or am I missing something? (Edit: trac is down, otherwise I'd look at the trac ticket.)
As the author suffering all the slings and arrows here, I think that the "please speak up only if you dissent" model is quite counterproductive. It gives the strong (but, I hope, incorrect) impression that those who care are all dissenters. Speaking as a former Linux kernel hacker (hence someone with a proven thick skin), this doesn't motivate me to get off the couch and contribute further to a process that I already found both overwhelming in its quantity of feedback and dispiriting in its quality.
I love the "text" library... keep doing what you do!
&gt; And remember : we are aiming for the greatest benefit to the community, not to achieve perfection. I call upon the Haskell Platform developers to abandon the Haskell Platform and instead focus on improving hackage and cabal an ghc so that we don't need a "blessed" set of packages at requires constant shepherding and tending, and slings and arrows.
&gt; Have concerns, but aren't going to object? Stand aside. This seems odd to me. I'm also not a fan of the "Say nothing if you approve" aspect of it. A single reply of "+1" is pretty useless, but longer positive replies of support might still be useful. Also, not sure where to ask this question, but I'll try it here: Why is cgi in the platform, anyway? It seems like a rather niche library. It also uses (as far as I can see) String with implicit encodings. There doesn't seem to be an HTML generator in the platform, so that just makes it stranger.
It's a bit strange to ask people to comment on a library and then call negative comments bikeshedding and excessive perfection-lust. I don't think this particularly encourages participation. I also don't see how things will get more efficient - you will get negative comments for *any* library. The underlying question is: should libraries included in the platform strive to have matching interfaces or not? Perhaps this should be resolved before adding any new packages.
Makes perfect sense to me. For context, consider that there has been ongoing conversation about these proposals for a very, very long time. Everyone has had ample time to voice their concerns. Changes have been made in response to those concerns. That could go on forever, but eventually, if we want any packages added to the platform, the process has to move forward... and people have to be asked to decide whether they have concerns serious enough to try to block including the package, or not. That's what this step is about.
This strikes me as somewhat naive. No doubt Hackage, Cabal, and GHC can all be improved in many ways so that it's easier to discover and use ad hoc collections of libraries. At some point, though, new Haskell programmers are going to want to know what the standard library is. Up until the Haskell Platform, while large parts of the Haskell community pretended the question never came up, the default answer was the GHC boot libraries. That had a lot of problems: the pressure to include more libraries in the set of GHC boot library; the conflict between what's useful to a GHC compiler hacker, and what's useful to average Haskell application developers; and so on. If you say get rid of the Haskell Platform, what that amounts to in practice is returning to having some arbitrary set of libraries that most people can latch onto and think of as the standard library... but throwing away the process for discussion and consensus building about it.
&gt; At some point, though, new Haskell programmers are going to want to know what the standard library is. Why does there need to be a standard library? &gt; returning to having some arbitrary set of libraries that most people can latch onto and think of as the standard library sounds good to me. &gt; but throwing away the process for discussion and consensus building about it. Why does there have to be a process for discussion and why on earth does there need to be consensus? 
Yes, it is an official repository. Due to minor account issues on the wiki I was not able to update the UU pages yet. Currently the GitHub repository functions as a mirror of the SVN repository. Hopefully the SVN repository can be deprecated in favor of the GitHub repository in the (near?) future.
Comments are great, but if we go now 2 years without adding any packages, due to *blocking objections* I will be very sad.
&gt; overwhelming in its quantity of feedback and dispiriting in its quality. This is very problematic. We've attempted to come up with [something that would work](http://trac.haskell.org.nyud.net/haskell-platform/wiki/AddingPackages#Consensus), and it is unclear to me what modifications would improve things. I'm gathering notes for modifications to how we do this, that hopefully will make it more productive.
There will always be a set of standard libraries, the question is how that set is produced: by GHC developer need? by a community review process? by a benevolent dictator?
Also see [github](https://github.com/tomlokhorst/language-cil#readme) for an example of how to quickly get started.
Also, I'm collecting feedback [on how this process worked for the 'text' proposal](http://trac.haskell.org/haskell-platform/wiki/AddingPackages/Discussion), let me know if you have a view, based on your participation.
One big advantage is having the Platform is that there are some very popular but very hard to build (depending on which OS/hardware platform you are on) libraries, like OpenGL, all the GUI libraries, etc. You really don't want to start fiddling with Haskell by venturing into the msys hell land on windows just to have OpenGL... Also it's nice to have a one-click installer and have everything just work. So the platform is indeed a very useful thing even if we disregard all the library standardization thing.
Thanks for the info. One question: Is projects.haskell.org one of the affected servers? Or is that one unreachable for different reason?
projects.haskell.org == code.haskell.org == community.haskell.org == trac.haskell.org This is a cheap VPN host that we are moving to run on new-www.
No, mtl 2 has been released, which is the old monads-fd, and depends on transformers. So mtl 1 has been split into transformers, which contains the transformer types, and mtl 2 (the old monads-fd), which contains the type classes.
Thanks. It can be a bit of an adventure trying to find the right library in Hackage, which is why I have great hopes for the Haskell Platform. (And not to mention that "mtl" doesn't show up in the documentation index, because I installed "monads-fd" first.)
The webserver on code.haskell.org is down but SSH access works fine.
Again, you will always get blocking objections for any library if enough people participate in the process. I don't really understand what this consensus model is supposed to achieve. If the platform is about packaging up the most widely used libraries then surely the main criterion for inclusion should be whether a library is widely used. The concept of blocking objections doesn't really make sense then. If it is about providing a somewhat coherent set of libraries, then it has become quite clear, I think, that nobody really knows how coherent they are supposed to be. Also, design by democracy doesn't work, anyway.
&gt; I don't really understand what this consensus model is supposed to achieve It is designed to ensure that the Haskell community feels some ownership over the set of libraries that are in the emerging standard. I should clarify also: there's not really a concept of true "blocking" objections. Rather, the steering committee determines on a case by case basis whether unresolved objections are indeed serious, and representative of the community's view. It is now up to those guys to decide whether the remaining complaints, if any, would prevent inclusion (which is unlikely, in my view). &gt; Also, design by democracy doesn't work, anyway. Maybe. It has worked for other technical communities, however, we may fall back to benevolent dictatorship if necessary. :-)
Most of the stuff currently in the platform is there because it was in GHC's extralibs originally, i.e. grandfathered in from a time when there was no process etc. Arguably we should be looking to remove or re-validate some stuff.
Although I don't use text directly, it seems to me to be a great library that keeps getting better, especially in terms of performance improvements. Certainly something we should have in the Platform. I do find your attitude to the review process a little off-putting, though. From what I can see, there has been quite a lot of thoughtful feedback on both the API and behaviour of the library, and as [dons says](http://www.reddit.com/r/haskell/comments/e2423/consensus_period_for_haskell_platform_additions/c14omn9) it has been quite well focused. Dismissing it as poor quality seems a bit unfair and makes me feel that you aren't really interested in understanding other people's concerns.
Or by *de facto*, which is my preference. It also involves the least amount of work: none. But maybe we need to define what is a standard library because perhaps we are not talking about the same thing.
I agree that packages should be easy to build, and the more popular the package, the easier it should be to build. But this is something that can can be done on in part on a per package basis, and in part on a all package basis (i.e. cabal-install). There is no need to bundle a subset of packages to achieve your result. You *can* bundle packages to achieve your result, but imagine how much better it would be if *every* package were easy to install with one-click.
You have misread me. I didn't at all dismiss the comments, nor did I say or imply that they were of poor quality. When I said that they were dispiriting, I meant that they have focused repeatedly on the naming of about 10 functions, and that this property of the discussion is getting me down.
Hmm, ok. I'm not quite sure how else I should have read "dispiriting in its quality", though. Anyway, like dons I don't think the volume of comment has actually been that great. If it's only been about naming, doesn't that mean that people are happy with everything else?
I also think, given all the slings and arrows in this particular case, that it would be a good idea to actually get some non-dissent out there. Perhaps it should be done off-list, like an online poll where people can put their names in the "good by me" box. But given how much quibbling there's been over names I think it would be a good idea to simply collocate the responses and see what proportion of those involved really consider that a show-stopping event. AFAIK the *only* outstanding issue is the naming issue, and I doubt that more dissent will resolve that. At this point we just need polls and (as necessary) fiat, not more discussion.
It would be ideal, yes, but how will it be made a reality? The fact is that there are plenty of packages that aren't easy one-click installs, and so all the community pressure that's out there has (evidently!) been unable to change that. By having a set of blessed packages we're providing the social currency for people to want to make their packages better; it's the carrot of bragging rights, instead of the silence of not enough people using the package to stoke the maintainer's ego. And by acknowledging the core platform we're providing the pressure to make that core more solid; it's the stick of making visible libraries get better, instead of the silence of not enough users filing decent reports about it being broken.
Shame I couldn't be there! You'll have to tell me about it next Tuesday. The family birthday in Germany was also good fun, though.
I just use [search keywords in Chrome](http://www.google.com/support/chrome/bin/answer.py?hl=en&amp;answer=95653) (e.g. I have Hoogle mapped to 'h'). In Firefox it's even easier to set up, just right-click on the Hoogle search box and choose "Add a Keyword for this Search".
Very nice, an interesting new approach to .NET programming indeed. But what language is this? Apparently not Haskell, because type names are interspersed with values in the expressions. Ah no, those turn out to be constructors of `PrimitiveType`, defined in `Language.Cil.Syntax`. I find that a bit unsettling.
I deliberately choose to stay as close as possible to the CIL domain with the naming of constructors and functions. I understand that constructor names like `Bool` and `String` can be a bit surprising for someone not familiar with the library. But I think it's very convenient when reading an AST, if you're already know CIL assembly. You can, of course, always use a qualified import to make the constructors stand out more. Or I could just create `bool` and `string` smart constructors.
&gt; This seems odd to me. I'm also not a fan of the "Say nothing if you approve" aspect of it. A single reply of "+1" is pretty useless, but longer positive replies of support might still be useful. You'lre looking at the wrong bit. The first stage after a proposal is made is review. At that stage you are invited to say how great it is, or that you use it and you're totally happy with it. The call for consensus stage is where +1 votes are not needed. The question being asked at that stage is "are we happy for it to go in". It is not a vote.
Ah, fair enough :)
Is there a simple way to specify that a source file content should be perceived as written inside the `sealModule [d| ... |]` bracket (and should be complemented with `import Language.Haskell.SealModule`)?
Why was this link submitted using https as the protocol? The reason I ask is that both Safari and Firefox refuse to connect, complaining about the site certificate.
&gt;but imagine how much better it would be if every package were easy to install with one-click. the sad reality is that it's more-or-less impossible to have a one-click solution to build libraries with external dependencies (I agree that cabal-install works well for pure haskell stuff. The catch is that cabal-install is also installed via the platform :). Especially on windows (but things can be hard osx too, even though it is a standard-compliant unix and has package systems). Hell, for some library/platform combinations it's pretty much impossible at all, for mere mortals...
You can also do this with Opera. For example i type "ig something" (into the address bar) to search images on google; "oeis 2,3,5,7,11,13" to search the OEIS, etc.
I can't answer why the link was submitted as https, but the ceretificate is a CACert community certificate. It is probably valid, but just not maked as trusted by your browser/os. Since browsers don't include this CA by default, you must add it manually at cacert.org
The link is also available as [http](http://www.joachim-breitner.de/blog/archives/443-A-Solution-to-the-Configuration-Problem-in-Haskell.html).
Given the POPL camera ready deadline tomorrow, I just updated the URL with the final version of the extended version of our paper. If you've downloaded the paper already, grab a copy of the new version.
Think of it as a targeting mechanism. Rather than supporters writing in general support, it's more useful if they write to address specific concerns of detractors. That focuses the conversation on the areas that need resolution.
What kind of functionality do you have in mind? A general symbolic computing system, an equation solver, a robot that does your calculus homework...? I think parsing a simple equation string to an AST, solving it, and printing the result would be a great exercise for somebody who wants to get into Haskell.
I find it extremely confusing that there was two completely different packages announced almost the same time, both called "cil" and using the namespaces "Language.Cil" and "Language.CIL", respectively. Though it seems both are legitimate users of the name, I believe having the same name causes more harm than good. For the record, my opinion is that .NET usage of CIL is much more widespread, so that one (this one) should use the name "cil", and the other one should be called something else. [1] [language-cil](http://hackage.haskell.org/package/language-cil) [2] [cil](http://hackage.haskell.org/package/cil) 
Good suggestion. Derivatives would not be difficult to add. A google search will yield some good previous efforts. 
You typically break this task up into create a datatype of actions such as * EVAL opp a b * LITERAL a * SET Str b And, you typically use a simple fold to build the op code. There is a good example of a [RPN calculater](http://learnyouahaskell.com/functionally-solving-problems). And, a really good example of a calculator with variables in the Thompson book.
And both authors are named "Tom"!
yeah, that too :)
I see. That does make sense - though still a bit jolting to the eyes. Nice package!
I'd start by studying closely how Mathematica does expressions. Basically, the top layer is multivariate polynomials on an expression "field" (i.e. The coefficient field are functional, not polynomial, expressions). Finding a canonical representation of polynomials is not entirely straightforward; you might want to take it from there. If you have a library near, you could browse "Polynomial algorithms in computer algebra" by Winkler. 
Relations in the prolog sense are just functions to True/False.
You might wanna check out my [mpl](https://github.com/dharmatech/mpl/blob/master/mpl.org) project. The code is in Scheme so things might not directly translate so easily to Haskell. But at least look into the books by Cohen from which the algorithms are based.
... now you have two problems ;)
Cool! The companion cds include the books in PDF format. You should be able to get through alot of the material. The "automatic simplification" algorithm (volume 2, chapter 3), which is the foundation for everything else in the books, should be within reach.
My apologies. PEBKAC.
This makes me feel uncomfortable - it just feels a lot less pure than doing it with a monad. Haskell tries to make itself accessible to the outside world by using monads, and in my mind this makes it a lot less idiomatic than using a monad.
&gt; the sad reality is that it's more-or-less impossible to have a one-click solution to build libraries with external dependencies Why do you say this? How does the platform help with this? Nix shows that it is possible to have one click installs for packages.
It works in my Firefox (the Debian version of Firefox/3.6.10)
also in .pdf form: http://haskell.org/communities/11-2010/report.pdf
Seems to me any .NET stuff should have module names like Language.NET.CIL. 
They have a [matrix](http://gosu-lang.org/comparison.shtml) for comparing Gosu with other JVM languages, with the usual partisan bias.
Haha, one of the matrix rows is "Not Lisp"
At St. Andrews, we have been developing a Computer Algebra Shell (CASH) for Haskell: a system to link Haskell to Computer Algebra systems such as GAP, MuPAD, MAPLE etc. Cash works by offloading the Computer Algebra to a designated system, and marshalling the results and calls via OpenMath and SCSCP; this allows Computer Algebra to be totally transparent to Haskell. It is also possible to call Haskell from Computer Algebra systems using Cash. So rather than re-implementing a Computer Algebra directly in Haskell, we rely on exploiting one of the many Computer Algebra systems out there, so long as it supports OpenMath for data marshalling. Cash also has support for parallelism in Eden and GpH, allowing Haskell users to write high-level parallel algorithms directly calling Computer Algebra functionality. We hope to get a cabalised version of Cash onto Hackage very soon. You can find CASH here: http://www.cs.st-andrews.ac.uk/~hwloidl/SCIEnce/SymGrid-Par/CASH/ There is also SymGrid-Par: a system for orchestrating parallel computer algebra, by linking various Computer Algebra systems and languages like Haskell together, exploiting the high-level parallelism of Haskell: http://www.cs.st-andrews.ac.uk/~hwloidl/SCIEnce/SymGrid-Par/v0.3/ Finally, there is DoCon, an attempt to implement a Computer Algebra system entirely in Haskell: http://www.haskell.org/docon/ I hope this helps, and provides some insight. Kind regards, Chris Brown. 
You guys made a really great job, everything was so damn well organized. Thanks a lot to you and the sponsors!
I don't think this package should mention .NET. Common Intermediate Language is an actual language, standardized by ECMA. That is why I put it under the `Language` hierarchy. The Microsoft .NET platform is a proprietary implementation of the language spec (plus a whole bunch of other stuff). Mono, on the other hand, is an open source implementation that can run CIL assemblies, then it has nothing to do with .NET. The other CIL package could use `Language.C.CIL`, but that's also less than ideal.
That report was awesome and I really appreciate it.
I dispute that Scala does not have easy transition from Java. It's fairly trivial to translate even a non-trivial amount of Java code into reasonably idiomatic Scala.
CIL (George Necula et al.) dates back to 2001 or earlier, I can't find a reference saying when the name changed but I think Microsoft's CIL was still called MSIL then. Coincidentally, Microsoft Research was a grant or gift donor by 2002 to the CIL (George Necula, ...) project at Berkeley.
Great resource!
&gt; Why do you say this? How does the platform help with this? I say this because in think that's the sad truth. You can try and prove me wrong and have a windows user and beginner haskeller install the OpenGL bindings from source without help (hint: it doesn't help to know that you need mingw/msys when the official msys distribution is completely broken, which happened in the past). And OpenGL is pretty easy compared to say wxWidgets or GTK or some other libraries. The platform can help because some volunteers take the time to prepare a binary package for your OS, at least for the most popular packages. For example it contains OpenGL, and there are plans to include GUI toolkits. When I started playing around with Haskell, it was a rather big factor for me that OpenGL worked out of the box (that was before the platform, but GHC included the OpenGL bindings back then). &gt; Nix shows that it is possible to have one click installs for packages. Nix sounds great but I don't have a magic wand which will install Nix for all users in the world by tomorrow. Not even unix users, and then what about windows? Then we will need another magic wand which prepares a nix distribution for all the libraries in the world.
this is very cool
Impressive results. The builder pattern seems like a nice way to glue libraries together which produce bytestrings, but I think it will be hard to get broad acceptance.
Well, as far as I can tell, the main goal of Gosu is to be the least crappy language possible without frightening the kind of programmers who *do* think that transitioning to Scala is difficult.
Scala also looks more like Java than Gosu.
&gt; but I think it will be hard to get broad acceptance. On the contrary, adding a Builder to the `bytestring` package is on my TODO list. The builder pattern makes perfect sense for types like ByteString and Text that do not have natural effecient concatenation. 
Perhaps a dumb question, but if blaze-builder is so much faster than `binary`, why isn't blaze's builder simply adopted into the standard package? They appear to have compatible licenses and APIs, and it would give every package depending on `binary` a nice performance boost.
This is not an answer to the question of the OP, rather, the same question again. I need for my personal work a CAS which might be very simple, I would not mind that, but it needs to forget the usual setup and tackle things more generally such as non-commutative multiplication and such. I am doing Physics (condensed matter mostly) so I need things like operator multiplication such as Pauli matrices but I *do not* need to do matrix multiplication in concrete form obviously. What I really need is not just to simplify but also to track how that simplification was achieved. Any cursory ideas on that, /r/haskell? I think in a sense I need to follow Conal's idea of semantic editing of an expression.
&gt; Common Intermediate Language is an actual language, standardized by ECMA. How about Language.ECMA.CIL? 
&gt; The platform can help because some volunteers take the time to prepare a binary package for your OS, at least for the most popular packages. I'm super happy with this effort, but why can't these volunteers simply make OpenGL bindings have a one-click install? Why do they need to bundle things together and stamp them as blessed? &gt; Nix sounds great but I don't have a magic wand which will install Nix for all users in the world by tomorrow. You misunderstand; I'm not saying that we use Nix. You said that it was more or less impossible to have a one-click solution to build libraries with external dependencies. Nix is only an example to show it is possible to implement something. The idea would be to make cabal more like nix.
Agreed, much better. I have 'h' for jumping to a hackage package page, 'ho' for hoogle search, and 'ha' for hayoo search.
I don't understand. We already have a page for organizing exactly this effort in the Haskell.org wiki. Why do we need a duplicate effort on haskellers, where people who are not registered at that site can't participate? 
Whatever the case, an "ease of transition from Java" is inversely proportional to "usefulness." Many Scala newcomers go right ahead and commit all the same mistakes that they do in Java. This is a weakness of Scala. On the other hand, there are cases where Scala disallows the "degenerate Java-esque problem-solving mindset" (or whatever you'd like to call it) and this can stump newcomers. There is a balance but I am of the opinion that usefulness should not be sacrificed lightly, even for the purposes of appeasement. The goal of Scala is certainly not.
Haskellers has support for discussions and news feeds, as well as permissions controls that only allow members to mark a topic as resolved. Besides, haskell.org wiki *also* requires registration to participate.
By that logic, are you proposing that the Haskell community abandon the community website and mailing lists (for discussions) in favor of yours? Surely bifurcating all efforts between the two is non-productive.
On the contrary, the features of the haskellers web site have been extensively discussed on haskell-cafe, for example [there](http://www.haskell.org/pipermail/haskell-cafe/2010-October/085635.html).
I think you might be overreacting a bit. If you do not like it, you do not have to use it, but I think Haskellers is neat, and having a forum that's not a mailing list or a wiki (clunky) is nice. Plus it is all powered by Haskell IIRC.
It's comforting to know I'm not the only one who finds iteratee somewhat difficult to use and understand. It makes me think if this is the best we can do to tackle IO "nicely" we are in a lot of trouble.
Any reason this and the SIG at Haskellers can't be "linked"? There's got to be a reason to have a wiki and a discussion group forum I suppose, but I've never found wiki's to be a good place for discussion.
It would leave all the people who are not members of Haskellers out of the discussion. If there needs to be a discussion group separate from one of the existing haskell mailing lists, then I suggest Google groups.
that's a very interesting point, however 1. You need an account for Haskellers 2. You need an account for posting to a Google group 3. You need an account for the Haskell wiki You even need an account for Reddit. What's your hang up about Haskellers? Do you just not like the guy in charge or something? This really smells political in nature to me, and I don't like it. 
Like many, I already have an account for Google, and an account for the Haskell wiki. In both cases, the account is just for accountability (as it were), tying me to an e-mail address. Having an account on Haskellers, on the other hand, is about "The meeting place for professional Haskell programmers". I'm not a professional Haskell programmer, I'm not seeking a job, nor am I proffering my skills. These are the main aims of Haskellers and I don't fit that profile, nor wish to have myself presented that way. I've read the FAQ and recognize that "The secondary mission of [Haskellers] is to provide social networking." But that feels largely secondary, and frankly, I seem to have plenty of social networking opportunities in the Haskell community already. 
If you sign up for Haskellers, you can opt out of job offer requests by indicating that it's not ok to contact you for jobs if you're somehow worried that someone might try to hire you. I think you're correct that it's first and foremost meant for people to list themselves as Haskell capable programmers, and secondarily a social networking style site for special interest groups. My point is that that secondary purpose already has some momentum, around a topic that you seem to genuinely care about. Getting serious momentum around a project to make GHC and Haskell easier to get at on Mac OS X seems a really great goal in general, and it'd be great if everyone could work together. 
By the way, making your account public/searchable is an opt-in action. By default, you won't even show up in the searches.
By that logic, why are you posting on reddit? Surely bifurcating all efforts between the two is non-productive.
On the GUI front, can I suggest that you look at making one portable binding work well enough and one "nice" OSX non-portable one. Some apps are OSX specific and there you certainly want to use the most native thing. Other apps need to be portable and so if we can make Gtk2Hs and/or wxHaskell work well on OSX then we cover all three major platforms. I'm less familiar with wxWidgets but Gtk+ does now have a native OSX backend and some major apps like inkscape are using it sucessfully. I've heard that the Gtk2Hs bindings work with the native backend so it "just" needs some attention to the packaging (e.g. not using macports).
screenshots at: http://www.flickr.com/photos/48809572@N02/
this looks a lot like sections mechanism in coq.
Well it's not exactly that simple, as you get the speedup only for the Builder type and not for the corresponding Writer monad, which is called Put in the binary package. See [comparison_to_binary.txt](https://gist.github.com/664979#file_comparison_to_binary_package.txt) for the according benchmark results. The solution with the nice speedup would be to change the signature of the Binary typeclass to classe Binary t where encode :: t -&gt; Builder decode :: Get t
&gt; The builder pattern makes perfect sense for types like ByteString and Text that do not have natural effecient concatenation. It can also be seen as a generalization of the [difference list construction](http://hackage.haskell.org/packages/archive/dlist/0.5/doc/html/Data-DList.html) to linked lists of packed elements. What is currently a bit unclear to me is how it could be generalized to also support being the result of a function like [zlib](http://hackage.haskell.org/package/zlib/)'s [decompress](http://hackage.haskell.org/packages/archive/zlib/0.5.2.0/doc/html/Codec-Compression-Zlib.html#v:decompress), which needs to represent errors as data in the resulting linked list. To be more precise, the open question is how we could have a single builder type supporting both "error free" bytestrings as well "error full" streams stemming from decoding functions. 
Being delivered today... can't wait!
Looks awesome! This approach to a haskell GUI environment is very pragmatic. Which I think is what is required.
How is its haskell editing functionality?
Oh, I meant the "impossibility" as at the moment, with the current state of the world, not as a theoretical impossibility. &gt; Why do they need to bundle things together and stamp them as blessed? To make life *much* easier? You don't have to use the blessed libraries if you don't want, but it seems that the majority of people actually *want* a blessed set. Part of the reasons for this is that there is (fortunately) a rather large set of Haskell libraries out there, and it can be hard, especially for a new user, to select one from the many competing eg. XML, Unicode, Web framework, parser, etc libraries. Another one is that I would rather click once and have a well-defined working environment than install things one-by-one. &gt; but why can't these volunteers simply make OpenGL bindings have a one-click install? Well for example the author and maintainer of the OpenGL bindings is not interested in providing binaries (this may be not true, but that was my impression anyway). Other package writers may have not enough knowledge or even access to specific OS-s/platforms. So a centralized effort may work better. The Haskell platform has OS/platform-specific maintainers (as far as I know), which seems to be a good idea. So in short: 1) bundled makes life easier for many people 2) the centralized nature of the platform makes it happen, as opposed to binary distributions for each relevant package, which may or may not happen depending on many factors. &gt; The idea would be to make cabal more like nix. But no matter how clever Cabal is, the problem is with external dependencies. Basically binaries are the only option on windows, and source or distro-specific binaries are the only option on unix. But you don't want to bundle external binaries with haskell packages, for many reasons (which most probably include legal reasons). 
You mention Debian. Does it build under Windows (he asks with eager hope)?
See [http://www.haskell.org/haskellwiki/Web](http://www.haskell.org/haskellwiki/Web) for a description and a comparison with other frameworks. Good to see activity in the Haskell web server space!
Dependency on unix package I see. Things look glum.
So is this (going to be?) like Emacs for Haskell? A Haskell execution environment? I was just dreaming of a Plan 9 like Acme thing for Haskell, where Acme is a filesystem service that happens to have a text editor interface, but can be scripted via any old file system utility as it's a mounted namespace. Pretty interesting stuff!
Awesome work by Andy here! I look forward to trying it out.
I'm working on a system for representing and transforming mathematical models in Haskell - it is currently pre-alpha, but it is up on GitHub. It is focused on representing systems of differential algebraic equations (DAEs), but parts of it could be reused as a more general representation of equations. So far: * The core data structures: https://github.com/A1kmm/modml-core * A solver for finding numerical results using IDA: https://github.com/A1kmm/modml-solver * Translating expressions annotated with physical units into ModML core (with units checking): https://github.com/A1kmm/modml-units * Expressing systems of reactions in ModML Units: https://github.com/A1kmm/modml-reactions * A collection of units annotated expressions for physical constants: https://github.com/A1kmm/modml-physicalconstants * An example model of hydrogen reacting with oxygen at fixed temperature: https://github.com/A1kmm/test-reactions-model The representation includes data structures for representing real and boolean valued expressions with constants, variables, and common subexpressions, and so may be helpful for your project.
Someday I'll be able to say "Hey! This is a monoid!" and do something cool like that. Not yet.
not that /etc/passwd still contains much of interest, these days. And your shadow file shouldn't be readable by your web server process. But yeah, this isn't so good :)
The real headslapper is that we got it right the first time, then somehow a patch meant to improve performance got in without anyone noticing "oh hey, a giant security hole."
How do you come to that conclusion? It looks like blaze's put is significantly faster than binary's, from that linked comparison file. BTW I'll ask you about that when we have dinner tomorrow :)
Yes, you are right. I've seen that many of these numbers lately that my memory got messed up ;-) So BlazePut is faster than BinaryPut. However, what I meant to say is: We should take this chance and get rid of the IMHO unnecessary Writer monad in the Binary typeclass, as it costs us a significant amount of performance (compare BlazeBuilder vs. BlazePut) without yielding any benefits in the default case.
Considering that the editor part first appears in screenshot #18, and that it already has a music player, IRC client and web browser, it does look a bit like emacs...
Transitioning to Scala isn't terribly difficult, but the syntax behind the functional stuff is a mess. I had to go back to Haskell to go on with my quest of getting into this Monads thing.
:(
Those headslappers hurt! Have you all made any changes to you MO as a result? Not saying you should, but if you did, I'm curious.
They probably hired one guy to build the system, he built it in Haskell then left, and now everyone is lost.
Opportunity?
It'd be awesome if I could run this on Mac OS X, but I'm not really sure the "right" way to get packages I normally do not need on the mac like GTK on here.
Maybe. I've sent an email asking if there is any chance that they could be willing to continue using Haskell and expressed some interest in a job on that condition. I'm skeptical, but we'll see what Padma says. Update: Padma seems to have said... nothing! No response. Oh, well.
"Yeah, I'm using the language...you've probably never heard of it..." I just can't wait to see how they solve the problem of Java not having very good functional programming...
If this is true, its rather sad that groups choose to rewrite/reinvent rather than learn a new tool and expand themselves in the process. Reminds me of an reddit link to some guy writing a tiling window manager because he liked xmonad but felt it was crippled by being written in Haskell. OTOH, it could be that the tool needs to integrate tightly with another code-base that is large and already in Java.
It requires good experience with testing AND with compiling. I'm out. Seriously? Troll anyone?
I'm not even sure why I'm subscribed to this subreddit. But for me; I ended up writing a new tool once because the existing tool was written in Haskell. It wasn't that I really needed to avoid Haskell, but the first apt-get it looked like I needed to build it was 100mb of crap, and where I live that's several hours of downloading.
How about porting from Haskell to Scala? Might do the job and not be so demoralizing. 
You can easily have to dl 100M or more periodically just to keep a system up-to-date. Sounds like downloading an updated GHC or whatever is the least of your worries. This is not indicative to me of a real-world problem with using Haskell.
[The long version](http://blog.sigfpe.com/2009/01/haskell-monoids-and-their-uses.html) 
It's also possible that *some* applications are better initially constructed in Haskell (Common Lisp, Ocaml, etc) but later ported to Java/C#/C++ for *some* companies. One possible case is that the problem has been solved, and they're looking for something maintainable into the future by more junior developers as the product enters a maintenance phase. On the other hand, this could just be your run-of-the-mill port that whatever to Java quick, it makes us nervous.
whether a language is functional or not speaks little about it's Turing completeness. I love Haskell, but there hasn't been a program I've written that I couldn't port to an imperative language.
"Utter lack of dignity and self-worth a plus."
http://en.wikipedia.org/wiki/Turing_tarpit
I completely agree.
Well, there's at least two reasons that Padma may be wanting to port to Java. One could be that somebody wrote some Haskell code and then left, leaving it in the hands of people who can't maintain it. The other is that they have some Haskell code that needs to interface with something dependent on the JVM.
Screw it, I'm porting all my Haskell code to Brainfuck *right now*.
Google Wave, I've heard, used exactly this strategy. Prototyped in Haskell, and then ported to Java for production deployment.
&gt; I just can't wait to see how they solve the problem of Java not having very good functional programming... I don't think it's that relevant—the data flow of a program has never been very paradigm dependent, the paradigm is how you think of the data flow.
I have a friend that solved this trick a couple of years ago. The idea is to simulate functional programming using a class that explicitly handles *closures*. In a sense you need to compile Haskell to a high level VM written in Java. I'm sure there are a lot of other challenges though...especially if the Haskell code uses tricky GHC extensions.
Still, when I didn't use Haskell yet and once wanted to try xmonad, I decided not to, because installing over 100 MB of packages just for the minimalistic WM is too much (disk space on my laptop was a scarce resource). But the problem with xmonad is that it requires the complete Haskell development environment just to compile own its configuration file; but most Haskell programs are compiled to stand-alone executables or libriaries and do not need GHC to be run.
 , ," e`---o (( ( | ___,' \\~-------------------------------' \_;/ ( comment archived / /) ._______________________________. ) (( ( (( ( ``-' ``-' 
I don't know what to think about anything anymore.
Rofl! I love the spinning logo. :D
no this is the best trolling
...
That was much funnier than the video was. Which should give you an idea how funny I thought the video was.
I don't think padma or someone else really has an idea. They just need some Java-Code because someone says so. That's why this weird job post.
Hope they are prepared for two orders of magnitude more code.
Any source on this claim?
Only rumor mill, though there's a [known reimplementation by one of the original developers in Haskell](http://www.mega-nerd.com/erikd/Blog/FP-Syd/fp-syd-19.html) and some of the [devs](http://au.linkedin.com/in/liamoconnordavis) are known members of the Haskell community.
Thanks, that satisfied my curiosity.
Hypothetically, if someone wanted to locally mirror hackage, about how much space would that cost them? Also, is there an easy way to go about that through rsync or similar?
They are called Scala developers. ;)
http://functionaljava.org/ 
uh, xmonad?
Question: does UHC support multiparameter type classes?
Sell! Sell!
Do you mean [gtk-osx](http://gtk-osx.sourceforge.net/)? I tried it a while ago, and it doesn't support glade (yet), among other parts. This turned out to be a problem, since most interesting programs on hackage using gtk2hs use glade.
Not much space, I suppose. The raw package database is about 150 megabytes, uncompressed tar file, and can be downloaded from the Hackage page.
An rsync mirror would be great.
The entire archive, including all package versions and all documentation is about 900Mb. I happen to know because we were working on mirroring last weekend at the hackathon and we got a mirror set up on someone's laptop using the new hackage-server implementation.
Note that tar file only contains the latest version of every package, not all versions.
The syntax reminds me of the [Io language](http://iolanguage.com/), which is neat, but unfortunately the documentation doesn't appear to mention any of the cool metaprogramming features that Io has.
Hmm, right. For some reason I thought that it is the full archive.
Hosting a gigabyte of data on Amazon S3 is pretty cheap. Would it make any sense to try and do something like that?
Any new language needs to explain in brief why one should want to use it. The website and documentation don't seem to cover that. There are so many hobbyist programming languages, and the most important thing is to stand out from the crowd. At the moment, this one doesn't.
WRT xmonad, this is why people developed xmonad-light and later [bluetile](http://www.bluetile.org/).
It HUUUURTS.
I'd be interested in using this as a functional replacement for Lua. It's not clear from the documentation how easy it is to embed into a Haskell program. Obviously, it's embedded in the interpreter, but I want to see how to provide sand-boxed access to the surrounding environment.
Some thoughts: 1: The world has far too many dynamically typed OO languages. 2: If you absolutely must make another one: 2.1: Why compile to haskell, a statically typed functional language? 2.2: ~~Why not have metaprogramming?~~ Apparently it does.
Well, I really don't like the syntax, that's for sure. Seems like it is different just to be different.
I told it to prove prop_num_add_sym, and after 20 seconds or so it errored out: Fatal error: Allowed memory size of 16777216 bytes exhausted (tried to allocate 16515072 bytes) in /homes/ws506/public_html/tryzeno/runzeno.php on line 25
It does seem to have metaprogramming, from looking at examples from the source.
There seems to be no way to download this thing without Darcs. Is that the case? There should at least be a way to download a tarball.
The [tarball](http://hackage.haskell.org/packages/archive/atomo/0.2.1/atomo-0.2.1.tar.gz) is available on [hackage](http://hackage.haskell.org/package/atomo-0.2.1).
Ah hah, `cabal install atomo` worked.
It is prettier! However it gives me horizontal scrollbars.
The top bar is unmentionably huge. While the color scheme is nicer in this design, just about everything else is worse in terms of usability. If I get time i'll design something better.
good spot, you shouldn't be able to prove this anyway since it is about a typeclass (Num) rather than a concrete type (try prop_nat_add_sym instead), but the program should be able to handle this a little better, also weird that the error is occurring in the php script...
Good start. But: * The bar at the top is way too tall. * The tabs are too tall, also. * The text in the "Find a Haskeller" tab doesn't render in the centre of the tab for me, unlike the other tabs (Iceweasel 3.0.6, Debian). * The logo fonts don't look nice to me at all. It looks very amateurish. Are there no Haskell programmers who have some skill in Graphic Design? * There's too many buttons on the page. It's way too busy: Reddit, Disquus, Vote up/down, etc.
I still find the content area much too busy, confusing and hard to read
&gt; The other is that they have some Haskell code that needs to interface with something dependent on the JVM. In which case a total rewrite seems like massive overkill. There's several ways to make Java and Haskell code communicate. You can use some language-agnostic protocol / RPC (shared memory, plain sockets, XML-RPC, JSON, protobuf, Thrift, I could go on and on). Or you can call directly between the languages with JNI and Haskell's FFI. I've done this on several occasions; it's annoying but not completely impractical.
The code could be simplified to use just one mutex. It is s common misconception that each condition variable needs a different mutex.
Nice article, too bad it doesn't actually work and dead locks often. Only tested the pthread implementation, Mac OS X 1.5.8, compiled using g++ 4.0.1 After compiling, simply run: $ while true; do ./mvar-test; done Which will give you a deadlock almost instantaneously. 
Wouldn't it over-serialize execution to use less mutexes?
Deadlock found ;-) In my use case of condition variables they need to be locked when signaling. Code and article updated, please check.
My motivation was to lock access to _value_ only for the minimum amount of time and on the other places, _put_ and _take_ mutexes can be locked both at the same time. But maybe I was just wishfully optimizing code, which is already very quick in its nature. It's just a pointer swap after all.
I agree, but that doesn't mean it's necessarily not the reason. :\
The value lock is always acquired and released with the other locks. There is no saved time. Instead, time is wasted on unnecessary and expensive mutex operations. But don't take my word for it. Compare the single mutex version with the multiple mutex version in a simple benchmark.
Can't a simple pointer swap be implemented more efficiently without locks using atomic transactions (compare &amp; swap, etc)? I use pointer swaps between threads quite often, without worrying too much about locking, and have never really run into issues.
Parallelism is not the inverse of the number of mutexes :). In this case, the parallelism of his code and the suggested single mutex solution is the same, but the latter uses fewer CPU cycles.
Thanks, you helped me realize (once again), that locking is a syscall, though expensive. Also you're right that no work is done when _value_ is not locked. EDIT: Thanks again. I LOVE deleting code.
I'm not really a designer but I made this little mock up of an idea http://james-sanders.com/d/mock-up-2.png
also this one http://james-sanders.com/d/mock-up-1.png
Nice! Atomic transaction are interesting. If you can make an example, how to use them to implement the MVar behaviour, I would gladly read it. However, I will have to read some more about these atomic transactions before I try to change my implementation.
You've fixed your actual downloadable code, but *please* fix your article too. As it is, at best you're confusing people (when I read it, I was all “WTF?? ” before finally concluding that you didn't really know what you were doing), and at worst you're spreading misinformation about how to use locks and CVs. Not everyone makes it to the end where you finally say that your code is crazy, and for those that do, it's not really cool to say “Oh yeah, all this stuff you tried to make sense of, well, it's crap.” 
I like the penguin haskeller.
The cases where the code doesn't wait (e.g, putting a value into an empty mvar, pulling a value from a full one) can be done with atomic operations, but the times when it waits will need to use some mechanism to wait. Long experience says though that trying to get clever and avoid locks involves deep magic that most people don't really want to think about. For example, you probably need to know about [memory](http://www.linuxjournal.com/article/8211 "Part 1") [ordering](http://www.linuxjournal.com/article/8212 "Part 2"). 
BTW, locking may or may not involve a syscall. You have no idea how `pthread_mutex_lock` works on an arbitrary system, because it's an abstraction. Almost all implementations these days are hybrid schemes that only *may* invoke a syscall. But there is no requirement for any syscalls — it might wait by going to a well-known code location and spinning there. The at the end of its timeslice, the kernel notes that the code is spinning on a mutex, and puts the thread to sleep until the lock is released. No explicit system call. Also, it isn't just the time used, it is also space. A `pthread_mutex_t` is probably not insignificant in size, so having three when you could use one is wasteful.
The author provides code, but the code says: // Copyright (c) 2010, Martin "Trin" Kudlvasr // All rights reserved. which is essentially saying, “Here, have some code that you can't use!”. Thanks dude. 
Right, that's a good point. In the instances where I use pointer swap, it is usually when one of my threads needs to be non-blocking. (e.g. real-time audio callback) So this would differ from the MVar semantics in that respect. Usually my needs are: "is there something there? no, do a transaction. yes, skip the transaction, keep processing." I'd either save up the data for a later transaction or simply allow the data to become "lost". (For instance, control rate data for an audio thread might not require 100% reliability, once a new control value is know, the previous one that wasn't transacted no longer matters.) This is not the same thing as a traditional producer/consumer model.
As Maristic pointed out, I wasn't thinking when I wrote that.. atomic pointer swaps are useful only in the case where you don't want to block. (Or for _implementing_ a semaphore, of course.)
Incidentally, in addition to the bugs this code had with respect to proper use of condition variables (now fixed), the code is also buggy in its use of `auto_ptr`. In fact, it is code like this that has resulted in `auto_ptr` being deprecated in the next C++ standard. The author's `MVar` class doesn't prohibit copying, and allows itself to be stored in data structures such as vectors, etc. Yet `auto_ptr` does strange things when you copy it. (And copying a `pthread_mutex_t` probably isn't a good idea either.) 
Btw, it really depends on the coding style of the ported Python program. If you already made extensive use of a functional style based on map/filter/zip/comprehensions/closures and so on, instead of relying on for-loops, in-place mutations and so on, you'll notice there's only little difference. On the other hand, the interesting cases would be where the Python program makes extensive use of non-trivial generators, co-routines, and advanced OO features such as Meta-classes where things get interesting, imho
The LICENSE file says: BSD3 . Should I copy that into each file? Your desire to use my code is very welcome.
The code and article purpose is to demonstrate the abstraction. I'm sure it can be made better. If you wish to contribute and make it perfect, I'm ok with starting a github project.
Yes, you're right. I fixed the article, too.
If a proof does not finish within the 30 seconds limit do you think it is worth to run zeno "at home" and simply provide it with more time? Or is it quite unlikely that the proof succeeds if it does not within 30 seconds?
I like the idea of extending the Haskell/GHC ecosystem with more languages (like the JVM and CLR). After last weeks discussion on -cafe regarding records I'd love some competition that might increase the pressure for a better record system (and keywords a la Lisp - yay!).
Honestly, I missed the license at the top level since I was down in the `pthreads` directory. But if a file is BSD3 licensed, it's one line to say so in the file. 
Awesome. Thank you! 
I like!
I think that the web article is fine as is. For the code, just disable the copy constructor and assignment operator (google it if you're new to C++ and don't know how) and you'll be good to go. These are operations that no reasonable use of MVars will need. I'd only recommend doing a github project if you wanted to have an industrial-strength implementation. For that, you'd add debugging features, printing, and you might want to think about whether you can think of any sane semantics for copying and assignment (probably still not), and also whether to try to avoid a proliferation of locks and CVs if someone creates a lot of MVars, for example MVar&lt;int&gt; exchange[1000]; In OS X, an MVar&lt;int&gt; is 168 bytes, so that's 168,000 bytes of RAM used there. 
This seems to happen from time to time, and I'm not sure why. While I love haskell, I'm not a volunteer or otherwise 'involved' in the community, so maybe I shouldn't complain, but this really shouldn't happen--at least not without a static page explaining the status. If you're really curious, you can always pop into #haskell on IRC.
Yeah, I can't really complain, but ... what is the hosting situation for the website? Who is it hosted by? A university department? How much would it cost to get a reliable dedicated server from a proper hosting company? Every time I want to consult the website it seems down!
I'm pretty sure it is (or at least was) hosted by galois.com.
IIRC haskell.org is being moved to a new host these days. Maybe that's what we're seeing now. 
No, that's hackage.haskell.org
www.haskell.org is hosted by Yale. {hackage,darcs}.haskell.org are hosted by Galois. {code,trac,projects}.haskell.org are hosted on a puny rented VM. We do not know why www.haskell.org is currently down, but we expect it'll get fixed on Monday. Because of these hosting problems, and for financial reasons the haskell infrastructure team are in the process of moving haskell.org to a new commercial rented server. We have now got the new server, (it it temporarily running new-www.haskell.org) and the migration planning is going on. In addition, the community server ({code,trac,projects}.haskell.org) will be moved to a VM running on the new host.
One problem with the website is it does iterative deepening from depth 1, so it has to search through the entire search space of every smaller depth before it finds the proof. The best measure of completeness I think is which depth it managed to get to, where the depth it will need is related to the number of variables/functions in the property. If it's something complicated with a few variables/functions then I would run it at home, otherwise I would rely on the website.
The UU wiki now mentions the GitHub repository: http://www.cs.uu.nl/wiki/bin/view/UHC/Download
I don't know who is responsible, but regarding new-www.haskell.org: http://imgur.com/mXGfL.png * spacing between "Learn Haskell"/"Use Haskell" and "Use Haskell"/"Join the Community" is excessive, but some are columns are too dense ("Join...", "Events"); solution: uneven columns? * Text wrapping in "Join the Community" is awful, see the "Overflow" word, for example; solution: remove all `tr`s, make it either a normal list or a single text paragraph; * "Events" column is hard to read; too long lines for such a narrow column; solution: use a smaller text, ISO date format, a simple list with an item per even instead of the table, or move below. * "Learn Haskell" and "Use Haskell" lack the single link to start with, the new reader has to choose from 10 similar links (what's the difference between "Learning" and "Books and Tutorials"?). I'd suggest highlighting one of the links in every column in bold or in bigger font, e.g. "Haskell in 5 Steps" and "Get the Haskell Platform".
On the whole I think a rewrite in C is clearer and more to the point (I've left out the boring delete method): struct MVar { void *value; pthread_mutex_t mutex; pthread_cond_t take, put; }; void MVar_init(struct MVar *MVar) { MVar-&gt;value = NULL; pthread_mutex_init(&amp;MVar-&gt;mutex, NULL); pthread_cont_init(&amp;MVar-&gt;take, NULL); pthread_cont_init(&amp;MVar-&gt;put, NULL); } void *MVar_take(struct MVar *MVar) { void *result; pthread_mutex_lock(&amp;MVar-&gt;mutex); while (MVar-&gt;value == NULL) pthread_cond_wait(&amp;MVar-&gt;take, &amp;MVar-&gt;mutex); result = MVar-&gt;value; MVar-&gt;value = NULL; pthread_cond_signal(&amp;MVar-&gt;put); pthread_mutex_unlock(&amp;MVar-&gt;mutex); return result; } void MVar_put(struct MVar *MVar, void *value) { pthread_mutex_lock(&amp;MVar-&gt;mutex); while (MVar-&gt;value != NULL) pthread_cond_wait(&amp;MVar-&gt;put, &amp;MVar-&gt;mutex); MVar-&gt;value = value; pthread_cond_signal(&amp;MVar-&gt;take); pthread_mutex_unlock(&amp;MVar-&gt;mutex); } 
I would really love to have this book in my hands right now.
Neat, I've been hoping for a book like this. I see an issue already: &gt; 1.2. What is natural language processing? &gt; **Stub** I'm not sure my definition will match the author's, but can find out the hard way. Also, will there be more than four chapters? IIRC, RWH posted planned chapters (not stubbed links, just text in the ToC) which was nice as it prevented early reviewers from saying "where are we going from here?" or suggesting a topic get covered that is already planned. EDIT: Wish I could download a PDF
awesome
When people select their location by typing in a city Haskellers resolves this to a lat/lon pair. This lat/lon pair should probably get a random 1 mile vector added (only if a city/state name is used, not if a lat/lon is typed in directly) so not everyone in the same city appears overlapped - overlapping makes the numbers hard to distinguish. Sample code: import Data.GPS import Control.Monad.Random.Class ... addVec maxDist coord = do heading &lt;- getRandomR (0,2*pi) distance &lt;- getRandomR (0,maxDist) return (addVector (heading,distance) coord)
Nice to see some more NLPers in Haskell. I hope the book turns out well.
Wow! Hey and there's a Haskell nlp community that's looking for a maintainer and a mission - http://projects.haskell.org/nlp Right now, it's just website, (unused) wiki and mailing list. Earlier I put a bit more effort into trying to recruit people to the list, but I haven't really had time to turn it into anything. Takers? 
Hi Haskellers, we are sorry for the stubs, and the small amount of chapters. We started working on this book just about a week and a half ago, but are enthusiastic about completing it.
We do have a list of proposed chapters. Adding them as unlinked ToC items seems useful. Thanks!
this is awesome, its nice to see people comfortable enough with the language to stop writing language tutorials (we have enough of them) and start exploring interesting domains
It would be nice if you mentioned viterbi in the pos chapter. Also, awesome work! More NLP and Machine Learning in haskell! :)
I paid EUR 10 for Minecraft; I will pay EUR 10 for Haskell.
There have been a number of significant changes since the last major release, including: * GHC now defaults to the Haskell 2010 language standard * -fglasgow-exts is now deprecated, in favour of the individual extension flags * On POSIX platforms, there is a new I/O manager based on epoll/kqueue/poll, which allows multithreaded I/O code to scale to a much larger number (100k+) of threads * GHC now includes an LLVM code generator which, for certain code, can bring some nice performance improvements * The type checker has been overhauled, which means it is now able to correctly handle interactions between the type systemextensions * The inliner has been overhauled, which should in general give better performance while reducing unnecessary code-size explosion * Large parts of the runtime system have been overhauled, including fixing several instances of pathological performance when there are large numbers of threads * Due to changes in the runtime system, if you are using Control.Parallel.Strategies from the parallel package, please upgrade to at least version 2 (preferably version 3) * The full Haskell import syntax can now been used to bring modules into scope in GHCi * GHC now comes with a more recent mingw bundled on Windows, which includes a fix for windres on Windows 7 * There is a new -fno-ghci-sandbox flag, which stops GHCi running computations in a separate thread; in particular, this works around an issue running GLUT from GHCi on OS X The full release notes are here: http://new-www.haskell.org/ghc/docs/7.0.1/html/users_guide/release-7-0-1.html 
&gt;The full Haskell import syntax can now been used to bring modules into scope in GHCi Is enough to make me happy, but this one: &gt;On POSIX platforms, there is a new I/O manager based on epoll/kqueue/poll, which allows multithreaded I/O code to scale to a much larger number (100k+) of threads is pretty nice too.
Will be interested to see if this does anything on the Shootout.
...I can't decide if this is the best or worst idea for a webcomic I've ever seen.
I have a hunch the llvm backend will be excellent for tight, shootout-style code. But maybe it won't be able to do any extra work over ghc?
It hits some things in vector and DPH pretty well. Like 25% better well.
*Cross posted from thread on proggit.* &gt; GHC now defaults to the Haskell 2010 language standard Does this mean we don't have to include the FFI language pragma anymore? [Along with a few other small changes](http://www.haskell.org/onlinereport/haskell2010/haskellli2.html#x3-5000)? Congrats on the grand release btw! */me sits eagerly awaiting HP release*...
Same. If the haskell.org committee does a fundraiser I'd be more than happy to contribute a reasonable amount (even as a recurring donation) to put into a fund to support hosting and odd jobs.
I know it sounds like carping, but an epub download option would be really nice. More so than PDF - they're awkward to read on tablets.
Seems like a great start - good job, guys! Time to pick up my Haskell book :-) Just a little note... Quotes such as this one: &gt; Since you are lazy (and if not, you should spend more time on your girlfriend!) give our field a pretty poor reputation amongst *the other* gender... You might want to abstain from such language.
Great! Is the committee interested in getting contributions from community members? I would like to contribute to the upkeep of servers and the like. Paypal donations seem easy and could go toward buildbots and hosting. It isn't just big companies like Galois that have an interest in keeping the Haskell cogs oiled and running smoothly :)
Can we have bubbles for haskeller pins on google maps? I want to browse users in my area and clicking normally brings up a bubble on most maps, but it navigates away from the page here. Which means that when I press back I get the world view again, and need to zoom back in to find my location and click on someone else.
...what's the recommended way to teach cabal about multiple installed ghc versions? I'd like to keep GHC 6.12.3 installed in parallel with GHC 7.0.1 for a while...
You don't need to do anything, it should Just Work.
It's in the pipeline, but I think we want to introduce dynamic programming first via Levenshtein distance.
Yes, we are interested in this. To be more specific, we are interested in discussing and deciding policies about this and how those contributions are used. A required first step is getting haskell.org incorporated (or join some organisation).
Use the -w switch to specify which compiler to use.
Also notable, * Pragmas are now reread after preprocessing. In particular, this means that if a pragma is used to turn CPP on, then other pragmas can be put in CPP conditionals.
Yes, the FFI pragma is no longer needed.
love the 'next' and 'prev' buttons.
There's two ways you can help, one is contributing your time, and eventually haskell.org will be able to accept donations. There is already a Haskell infrastructure team and yes they would be glad to get more helpers. The job of the committee is not to *be* the infrastructure team but to support and oversee the infrastructure team by decding policy and paying for servers etc. The committee's remit is also wider than infrastructure. As noteed says, one of their first priorities is the legal status of haskell.org. That will help with the financial side because it will be easier for haskell.org to accept contributions.
This is nice, but I found the title misleading. I was expecting to see a survey of some of the many advanced NLP tools available in Haskell, and some hands-on how-to-use information for experienced programmers. With those expectations, you can imagine that the material was quite disappointing. This is not a book about NLP. Nor is it for working programmers. It is an introductory tutorial to the Haskell programming language, with examples taken from a few of the most elementary techniques of NLP. As such, it is nicely written and pleasant to read. There is currently a great need for a book that actually provides what the title of this book promises but does not deliver. Could you please write that one, too?
Very good point! We will change this. Sorry for this!
The DocBook stylesheets support ePub as well, so we will try and see what comes out.
thx, works like a charm, except for broken dependancies causing packages to fail installing, e.g. *cabal install -w ghc-7.01 cabal-install* fails for me with: Resolving dependencies... cabal: dependencies conflict: random-1.0.0.3 requires time ==1.2.0.3 however time-1.2.0.3 was excluded because cabal-install-0.8.2 requires time ==1.1.* 
I get the same error. Is this caused by cabal-install not being compatible with the newest base?
That's inconvenient. I don't know why cabal isn't able to figure out these dependencies automatically. Basically: random can work with either version of time, cabal-install requires time 1.1.\*. Since time 1.2 is already installed and random builds on that, cabal-install doesn't rebuild time 1.1.\* and then random. Two options seem to be: * Wipe out random and time, then reinstall time 1.1.somethng and random, then install cabal-install. * Change cabal-install to allow time 1.2.\*. This *should* be fine, and should probably be done upstream. I'll send an email to the maintainer about this. __Edit__: escape my asterisks
It usually takes a while for a lot of the libraries out there to work with a new GHC release. In particular, the new version of cabal-install hasn't been released yet.
The wiki DB will be updated again once we switch over. Because of this I kept the contents of the main page largely unchanged. It definitely needs a bigger restructuring.
It already is on the [shootout](http://shootout.alioth.debian.org/u64q/haskell.php)
In the case of cabal-install specifically, it works with ghc-7 but cannot be built against the libs that come with ghc-7, that'll wait for a new release. So you can use the cabal-install binary that you've already got, but you cannot for the moment rebuild it from source. Fortunately you do not need to.
So it is. Now I wish I had checked what was there last night.
It wasn't published until ~6 hours after your first comment ;-)
But the community usually needs to do some clean up work on those programs to get the best out of a new compiler release ...
Have you seen [this paper](http://conal.net/blog/posts/beautiful-differentiation/) on this topic.
Can someone tell me what's the current state with 32bit/64bit code generation? In particular, is it possible to produce 64 bit Windows (resp. OSX) executables, and if yes, what is default, 32 or 64 bit; and if 64 bit is the default on some systems, is it possible to change back to 32 bit? 
Thank you! Good Luck to you too! I actually haven't purchased a book yet. I just read a few things about it basically through Reddit and it seemed so interesting, that I dove into it. Already compiled a few programs and now I am trying to see what I can do that is truly useful. I have some interesting data sets I use to parse through and do some analysis with Perl code I have and I am wondering if maybe I should break out some of that code and do it in Haskell now. I need something to test with. 
Sounds like a good plan. Steer clear of regexps -- use basic list manipulation, or one of the parser libraries, for the full experience.
Have fun!!
Good plan! I have it heard it said many times "I have a problem. I know! I will use regexps!! Now I have 2 problems." so I tend to avoid them where possible and use them where necessary. I am still learning Haskell so tips are appreciated and taken. Check on basic list manipulation. Trying it now. If I can get the hang of Haskell I have a real sneaky suspicion that it will replace my Perl entirely. 
So far it has been great fun! I already lost a day of productivity at work over it :-) 
I suggest to solve some of the 99 Haskell Problems and learn from the solutions. Good luck :) And don't try to get into monads too early ;)
I am brand new to Haskell and functional programming, but an old dog in the industry. I like what I see and would love to contribute money and/or time. I would have to say that being new at Haskell my time might be less valuable than I would prefer, but if there is anyway to wrangle community support and maybe pass this on in one of our local Orlando or Boca Raton, FL meetups I would like to offer that. 
Thank you! That's a great suggestion. Unfortunately I already watched a 2-part video series on monads today and am itching to experiment. 
I agree that community will need to improve benchmarks, but seeing if performance is actually improved out of the box for existing programs, without code modification, is also interesting information.
If you're going to do any actual parsing--by which I mean anything more complicated than splitting on whitespace or simple string substitution--then the [parsec library](http://hackage.haskell.org/package/parsec) might be a good place to start. It's nothing fancy, but it's fairly straightforward and well-known.
I've certainly had loads of fun learning Haskell--hope you will too! If you get stuck at any point, the Haskell folks on [Stack Overflow](http://stackoverflow.com/questions/tagged/haskell) are usually prompt and helpful (also, modest).
"Monad" is just the name of a small, very general API. Lots of unrelated types implement this API. You'll encounter these types individually long before the generic API comes into it. For example, [here's](http://www.haskell.org/haskellwiki/Introduction_to_IO) a nice introduction to IO in Haskell which completely avoids the M-word.
Stack Overflow is indeed a great resource. If you prefer more real-time help, the `#haskell` IRC channel on Freenode is also fantastic.
You mean, you spent a day developing new skills that will eventually tremendously increase your productivity in the future... ;-)
Well, all the benchmarks game data is in CVS so you can track back when the [GHC version information changed](http://alioth.debian.org/scm/viewvc.php/shootout/website/websites/u64q/version/ghc-version.php?root=shootout&amp;view=log), and [**diff the data files**](http://alioth.debian.org/scm/viewvc.php/shootout/website/websites/u64q/data/ndata.csv?root=shootout&amp;r1=1.540&amp;r2=1.541). These are the columns: name,lang,id,n, source code size(B), cpu time(s), memory(KB), status,load, elapsed time(s)
I actually used that excuse :-) Thank you for the suggestion!
If you have any questions, make sure to visit the IRC channel (if you haven't already), which is #haskell on irc.freenode.net. It's very beginner friendly, and there are usually lots of people around to help out. (If you don't feel like installing a proper IRC client, you can just use [this](http://webchat.freenode.net/?channels=haskell&amp;uio=d4).)
Thank you very much. I have a few IRC clients so I can work with that, but the link is pretty good too. I took a small break but I am back at it tonight. 
Nice improvements on threadring (~40% less time taken), solid improvements on spectral norm, looks like regexdna now fails to run, chameneosredux is a disaster (8.379 -&gt; 66.184), knecleotide is very bad (66 -&gt; 113), most other things show about a 3-5% improvement. If I'm reading these lines correctly, and I'm just looking at elapsed time.
Seriously. The time I spent learning Erlang (and now Haskell) has really shown through in my productivity programming in other languages. Forcing you to think another way might not be the *easiest* thing, but it certainly opens your mind.
the reverse-complement benchmark has twice the code of Java! I look code size with equal interest to performance.
The current code for knucleotide is hideously ugly. I've written a beautiful version that should theoretically go fast but it needs a bit of work yet. Here it is. Anyone that wants to have a go, please contact me. http://hip-to-be-square.com/~blackh/haskell/k-nucleotide.hs
Excellent, I'm glad to see my thread-ring code back on top :D
No joke. I was working on a project and there was one particularly tricky bit where I actually thought "This would be so much easier in Haskell". At that point I knew I was lost.
The differences in the timings for that are insane! 9 seconds versus over 5 minutes for Mono...
7 minutes even :)
The Haskell version is not exactly the same as the one in some other languages. Since GHC implements N threads on M capabilities, the token seems to be passed between threads that use the same capability, i.e. underlying thread. So on GHC, the token doesn't pass between CPUs like it does in languages that use 1:1 threads - i.e. where one thread represents one OS thread. 
It only specifies that the threads need to be pre-emptive, which the GHC threads are. If other languages don't support light weight threads then that's their problem and highlighting that is legitimate.
Oddly, I tried running chameneosredux on my 18 month old Core 2 Duo laptop, with the same compiler options passed to ghc-7.0.1 but with -N2 runtime options passed to the resulting executable, and it finished in 3.718s with n=6,000,000. I'm running an up-to-date Ubuntu 10.04 system, gcc 4.4.3, I couldn't tell you what the discrepancy is.
I'm just installing it now. I was a bit chagrined to find that in order to install the Haskell Platform I had to have Haskell installed!? And Mandriva didn't give me a recent enough version, so I had to go and build and install a more recent version of Haskell in order to be able to then install, uh, Haskell.
On the whole, the Haskell programs are slightly more verbose than the Java ones (and the Java ones aren't short, even for Java). In another conversation, someone said this was because they were going for performance, which meant writing, I guess, fairly non-idiomatic Haskell code that was verbose and ugly. I think these attempts at pure speed are misguided, though the way the benchmark displays things, that is to be expected. But, when you look [at the summary shapes](http://shootout.alioth.debian.org/u64q/code-used-time-used-shapes.php), I'd rather see Haskell higher up (slower), but further to the left (more concise).
I just installed the Haskell Platform and it didn't seem to have any issues for me. I suggest that you try that approach first to give Haskell a shot. 
One day into it and Haskell has now replaced Ruby for me as the most elegant language to use and read.
Some of these results are still mystifying though. Why is the Haskell version of binary-trees so slow compared to other functional languages? Both ATS and OCaml are beating Haskell here. Can anybody suggest why that is?
You suggest I give what a shot? Install the Haskell Platform? That is what I did and what my post was talking about. I'm confused about what you think I said.
Haskell is incredibly fun to program in. However, I still haven't wrapped my head around Monads, so I usually just use it for simple number experiments. The most disappointing thing about knowing haskell is that everybody goes "hssssss" when you say you want to use it.
Excellent.
If your distro doesn't have a package for the platform/ghc, then the platform = download and install the generic ghc binary, then download and run the platform installer. I think work has been done to make the documentation clearer on this point, but its still not perfect. You should never have to build GHC from source however, as the generic binary package works just fine on most distros. Also note that if your distro does have a platform package, or if you use mac or windows, then the platform includes everything, including ghc.
I'm trying to understand this. The last time disk space on a personal computer has been a scarce resource was, oh, 1995? These days I simply can't buy a laptop or desktop computer without an unreasonably large amount of disk space. Do other people actually have problems with this?
Mono takes 57 seconds, not 5 minutes.
I don't think the poster above was saying that GHC is cheating or anything like that. He was just explaining why it's faster.
FWIW, the binary tree benchmark is really just a stress test of the garbage collector. 
I thought you actually tried building it from scratch and not the install from Haskell Platform. I apologize. The platform just worked on my OS/X and Ubuntu distro so I am not sure what happened for you. I was only trying to be helpful. 
OS/X just gets a .dmg. Mandriva was not so convenient :-)
Is using vector allowed?