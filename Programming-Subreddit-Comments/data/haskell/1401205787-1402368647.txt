Very excited to see actual work in this direction! It's no mistake, though, that the first place an interesting idea like this is tested is ML: a suitable language with well-understood semantics, mechanized metatheory and a simple core.
Hopefully the percentage will drop quickly soon, once the new Haskell Platform based on 7.8 is released.
It's really nice to see a dependency like this get removed without too much trouble. I actually think there's a little too much emphasis on reuse *between libraries* in the Haskell community. If my library needs a streaming utility, that should be an implementation detail; one really great way to break abstraction is to have the fact that I needed a streaming utility *affect the likelihood that the library will build for another person*. Perhaps a better packaging system would make this a non-issue. But lacking that, I sort of think it's not so bad to recapitulate what's needed internally. Then, compatibility layers can be built (as Michael did) without too much trouble. I sort of came to this point of view when I realized that the degree to which Agda &amp; Coq constructions depend upon the standard basis libraries greatly hurts their ability to be used by other people, or people who are not using *my machine*. My habit has been to just recapitulate all the basic constructions of Type Theory locally. Not optimal, but better than the alternative. I want something people can copy and paste into emacs. Likewise, for Haskell packages, it would be nice if `cabal instal xxx` didn't try to transitively install the entirety of Hackage (or the entirety of the Kmett universe). For instance, I wouldn't hesitate to add a dependency like `lens` or `pipes` or `conduit` to an *application*. But I should have to think very, very hard before doing that in a *library*. This philosophy is one of the reasons that, for instance, Vinyl does not and never will depend upon something like `singletons` or `lens`: and that constraint actually led to my coming up with a generalization which made Vinyl much more beautiful than it would have been, had I relied on all the very cool kit in `singletons`. The solution will lie in a few places: 1. Don't be afraid to recapitulate locally what could be got from another library. One of the great strengths of a type system is that we don't have to rely so strongly on huge testing burdens which are the result of the sheer force of human will: "I could make a mini version here, but the gigantic version is very well tested". 2. Conversations with Edward lead me to believe that a change in the way orphans are handled could also reduce dependencies which are included only to export compatibility layers (like much of the lens dependency bulk).
Oh, good point. Is there some nice way around that?
Hindley-Milner is implicitly typed (and hence requires an inference algorithm to tell you which terms are typed) while System F traditionally is explicitly typed (inference happens to be undecidable for System F with type annotations removed).
Ahh this is quite cool. A while ago I spent a lot of time working on rosalind problems, and in the end I [wrote](https://github.com/Mgccl/mgccl-haskell/blob/master/rosalind/LocalAlign.hs) a dynamic programming implementation of the local alignment algorithm. I used all those mutations and stuff because the lazy DP version is just too slow to solve [this problem](http://rosalind.info/problems/laff/). (Two strings, each with length 10k) How fast are these algebraic dynamic programming things? I'm [implementing some string algorithms in Haskell](https://github.com/Mgccl/haskell-algorithm) for fun, so I probably try to implement some alignment related algorithms with the ADP tool...
i don't really argue about your main point, but `singletons`' dependencies are very few: recursively the following packages are required. th-desugar, containers, mtl, syb, template-haskell, (and through containers, mtl and template-haskell: array, deepseq, transformers, pretty) 
There are the classic definitions of Coupling and Cohesion to be aware of. There has been much previous study in how these attributes affect software quality. In OO, folks tend to be quite mindful of how their design decisions affect Coupling or Cohesion. In FP, I don't hear these terms used in the same way, or at least as frequent, but it strikes me that they should be just as relevant.
The idea that hardware exists to serve a program seems idealistic to me. Development of software is often 'opportunistic', i.e. such that its purpose is to manipulate pre-existing hardware to some end. Correctness, in these cases, is an extrinsic property. Timing errors, out-of-memory errors, or hardware interpretation errors are issues that the software must address. Unless we really can change the hardware (and we should not ignore that possibility) it would be futile to blame errors upon it. Machines exist. Software exists. Neither exists for the other. Purpose belongs to us humans. Philosophy aside, I agree with at least one of your major points: that we aren't restricted to any particular machine, and that Haskell code can be written for other machines. (At least, if we take a subset excluding unsafePerformIO.) 
I've never used ADP in earnest, "just" for teaching. But as far as I know, in the bioinformatics community that group has solved some problems that others couldn't. Much of that was probably more because of the "modelling/programming efficiency" rather than mere "program efficiency". That is, due to the declarative/algebraic approach, they were able to conceptually tackle problems others couldn't. That said, I see no reason why their work wouldn't also lead to efficient implementations applicable to big problem instances. (In particular since, maybe sadly, at some point they deviated from doing everything in Haskell - they then wrote a special purpose compiler that takes something largely looking like the Haskell DSL as input and spits out plain C code. Some of the flexibility of staying embedded in Haskell goes down the drain that way, but execution efficiency benefitted.)
Does it support hoogle's json api? 
There is an extensive chapter on System F in Pierce's Types and Programming Languages.
This case isn't about internal dependencies though. wai currently uses the `Source` type from conduit in its public API, as the way to allow web applications to consume requests and produce responses in a streaming fashion. It seems to me the motivation here is less about avoiding dependency hell and more about providing a simpler and more universal API. As a user of wai, I am currently forced to learn at least a bit about conduit whether I want to use conduit, pipes, or my own hand-rolled streaming methods. I'm sure conduit is a great library for doing stream processing, but why not make it opt-in. That's what the switch to the callback-style API achieves. There's an interesting discussion to be had about what effects packages that export their own types have on composability on a large scale.
I think it's important to credit the authors who did most of the writing: Andres Loeh and Wouter Swierstra. My main contribution was a program I wrote for a bet.
Just a typo, fixed in the next upload, and then fixed again in the second next upload... 
But there is something I don't understand. Where are sum types, product types, recursive types and so on in all that? Are those not part of system F itself ?
There are two easier answers than that language pragma. Option 1: Provide a type signature: g :: Eq a =&gt; [a] -&gt; [[a]] g = groupBy (==) Option 2: Eta-expand so the definition is "obviously a function". g xs = groupBy (==) xs Either of these will fix the problem with no language changes required.
`ListT` from `pipes` has the nice property that you can write code for it without incurring a `pipes` dependency. For example, this is how you write a `ListT` that reads lines from standard input with only a `transformers` dependency: -- `t` will type-check as `ListT` stdinLn :: (MonadPlus (t IO), MonadTrans t) =&gt; t IO String stdinLn = do eof &lt;- lift isEOF if eof then mzero else do str &lt;- lift getLine return str `mplus` stdinLn So if you want a streaming abstraction that is dependency free, you can use `ListT` (which is basically isomorphic to a `Producer`). This lets you provide streaming library code without any `pipes` dependency, and then only application code has to import `pipes` to run it.
You mean *e.g.*, not *i.e.*
Well, I just suggested it, emphatically! If anyone is wondering why many people struggle to get up to speed in the Haskell world, add most of the explanations and workarounds on this thread as some of the reasons üòà
Well its not ML either, its an ML-like language developed in the course of this work. To add this to any full-featured FP system seems to me a mildly non-trivial task.
I don't think so. If I would release this as a library, I'd probably include 2 versions, one on arrays and one on lists.
&gt; A cofree comonad is just that. Given the forgetful functor that takes a comonad to its underlying functor, the cofree comonad is its right adjoint. My bad. Coulda sworn it was still a left adjoint &gt; And practically, from an API design perspective having a distinct name is important Oh absolutely. I would never advocate renaming `Cofree` to `Comonad.Free`. That way lies madness! &gt; We do often talk about, say, universal properties when we mean both universal or couniversal, This is a major pet peeve of mine. I've read far too many sloppily written papers where I have to constantly stop and think whether they meant couniversal or whether they actually mean universal. Similarly when people use "limit" when they mean colimit. If people are actually using "cofree" to mean right adjoints to forgetful functors, that's fine. I've just seen too many newcomers to CT tacking "co-" onto the beginning of everything, without understanding what dualizing actually means. As you're well aware, it's not always the case that a co-(X Y) is a co-X co-Y; there are plenty of times where it's a co-X Y or an X co-Y.
The vanilla System F only has functions, no data types. You can emulate data types in System F, but they're not quite the same thing (i.e., you can't prove certain properties about the emulation). Often when people say "System F" they don't really mean this vanilla version; they really mean "System F with data types". ((Of course, that then raises the question of what we mean by "data types": do we just have polynomial functors, or do we also allow exponentials? do we only allow non-recursive types, or do we also allow recursion? what about mutual recursion? blah blah blah.)) The case for Simply Typed Lambda Calculus is similar. Vanilla STLC doesn't have data types; it corresponds with the implicational fragment of intuitionistic logic. You provably can't emulate products in vanilla STLC, so you really do need to add data types if you want them. Luckily, adding products is a conservative extension over vanilla STLC. And, again, usually when people talk about "STLC" they really mean "STLC with data types".
It looks like you've removed the offending section of the post, so I can't say for sure; but depending on how you need to use the length, you could always try tricks similar to those used in [Data.List.Extras.LazyLength](http://hackage.haskell.org/package/list-extras-0.4.1.3/docs/Data-List-Extras-LazyLength.html).
Hmm, thanks, that's worth a look. Still, since I'm mostly focusing on writing pretty, didactic code, it's probably not a good addition to the actual blog post. Another possibility would be, say, switching to Vector and using `fromList`, although I don't know exactly how that behaves. That might even reasonably fuse away with Stream fusion.
Nice! For reference/comparison, here is a version of pong done by Niklas Haas to demonstrate the `lens` library. https://github.com/ekmett/lens/blob/master/examples/Pong.hs It was actually the first example we shipped with the library. Perhaps we should update it to use more modern lens combinators here and there.
Thanks tel - interesting read and lots of great links out for me as I start learning about memoization, games. Style: love the font :) Something about the layout of the proof I think made it look a little ominous. Wish I knew exactly what that X-factor is in some proofs that make them look likey they're full of rewarding ah-hah moments. Or maybe if I actually buckle down and understand yours here, I'd get those intermediate dopamine pulses. On the substance: I'm wondering a couple of things - like what is it in -O2 that makes the memozation work when -O1 doesn't? I (naively) thought that -O1/2/3 would change constant factors. Of course it was a little demotivating to hear that the final memoization trick doesn't pay off as well as the nimbers thing, and is 'nothing to write home about'. Although yes, it was still interesting to see another worked memoization example. As a beginner, I would also like to have seen how this relates to the automated memoization libraries out there. e.g. - the main novelty here is how to decompose the Kayles game into a memoizable form? Thanks - hope to see more posts! :)
You could probably do it in something else if you wanted, but I think it's definitely worth giving it a shot in Scheme anyway. I say this as someone who generally prefers Haskell to Scheme, but is also currently working through SICP, and I'm doing it in MIT Scheme. I can't imagine the bit where they "write lisp in lisp", which I've heard tell is the highlight of the book, would translate very well. And everyone should have a taste of proper lisp macros at least once in their life. By the way, if you *do* go with Scheme and you decide you want to write your answers in a literate style, you're in luck! I've written a literate programming package for MIT Scheme which you can find here: https://github.com/dpwright/lp.scm Here's my sicp repo which uses it: https://github.com/dpwright/sicp -- I'm still pretty early on in my SICP quest!
Would it be possible to implement a compile-time report about failed fusion by using a function that had a deprecation warning, instead of one that threw an error?
I'd like to second the notion that that book is best worked through in Scheme first. Afterwards, its fun to learn how to replace parts with Haskell, but it takes a lot more boilerplate.
Instant mode is absolutely awesome, and the reason I originally installed a local version of Hoogle (it was disabled on the main website for a while). Not sure if you guys feel like putting up with the server load, having all Stackage docs accessible with Hoogle instant mode sure would be nice ;-)
is it possible to get an `iostream` or a `conduit` out of it as well?
Good idea, but I assume deprecation warnings are emitted much earlier in the pipeline, likely in the renamer.
&gt; Psssst! Hey! Hey kid! Wanna try some lens? It is pretty addicting.
I think we need an instance of MonadPlus for Data.Conduit.Internal.Pipe, respectively Data.Conduit.Internal.ConduitM for that to work.
If you want some optimization tips, you can have a look at the various benchmarks and implementations I did: https://github.com/blitzcode/haskell-gol I found it to be a good problem for learning the facilities Haskell offers for numeric computations and parallelism.
It's a bit more than that. `ListT` in pipes has a different `Monad` instance than `Producer`, and as such. `return` is essentially the same as `yield`. We'd need the same kind of newtype wrapper in `conduit`, and would then need either a `MonadPlus` or `Monoid` instance to take the place of monadic bind.
Not yet, I've just added that feature to our internal issue tracker.
Random question: is there a language extension that could generalize this for N-tuples?
Otoh, doesn't depending on `singletons` effectively make a package depend on recent GHC versions, which would be quite a heavy dependency?
Wouldn't one need a pushback operation to get an iostream out of ListT?
I have updated my post with a brief description of incomplete SSE3 implementation. I pushed the code to [sse3](https://github.com/jstolarek/lattice-structure-hs/tree/sse3) branch on github.
Yes, I guess I'm actually after doing tokenized parsing to some degree. Thanks for the pointers, I'll look into `uu-parsinglib`.
Thanks for pointing me to that paper. Very seems an interesting read. I just hope I will be able to fall sleep after reading that one later tonight... :-)
Thanks for the insight, I will definitely do it in Scheme first. And I will absolutely use the literate programming package, that's awesome.
RIP site.
Also: http://blog.headcrab.info/haskell-optimization-and-the-game-of-life/
Great! I'm sure it still has lots of issues, as I've been pretty much the only user until now, so just let me know on the github issues page if you have any problems!
&gt; This means that my original assertion that access complexity is O(1) isn't exactly true. It is constant time, but the complexity is actually O(2). Constant time is exactly what O(1) or O(2) means. They're identical by definition. If they want to talk about scaling factors they need to drop the big Oh notation. &gt; The Data.Vector.Unboxed API is almost identical to the Data.Vector API. Which means that we can switch them out with almost no effort, just change the references as seen in this Commit. AFAIR you need to write some [typeclass instances](https://github.com/TheBB/hess/blob/master/src/Game/BoardMask.hs#L72-L90) if you want to do this with a type of your own design. It wasn't totally trivial for newbie me at least.
&gt; It is constant time, but the complexity is actually O(2). You need to read up on [big O notation](http://en.wikipedia.org/wiki/Big_o_notation). 
Yeah, I think you would need a conduit equivalent of `ListT` that newtypes the conduit to define a new monad instance and then convert from that to conduits (by just unwrapping the newtype).
Yes, you would. However, if you remove pushback then `ListT` has a lot of parallels to `OutputStream` (not isomorphic, but practically equivalent). Consing an element onto the `ListT` is analogous to a `write` for the output stream.
cabal files were originally haskell files... But we moved away from that since its much harder to statically analyze turing-complete dependency and library specifications. Also, you have to execute arbitrary machine code to check the package when you upload it to Hackage... This way lies madness. For old time's sake: http://www.haskell.org/cabal/proposal/x138.html#AEN226 &gt; Setup.lhs is an executable Haskell program which conforms to a particular specification 
Right. Other practical advantages are easier backwards compatibility (we actually do pretty well on backwards compatibility of .cabal files), and the ability of tools to read and modify .cabal files. That's not to say the .cabal format is perfect. We could make further improvements to reduce redundancy for example.
It's not my article, I was just sharing a link. :)
Completely agree, having worked through some of SICP in Haskell myself.
I think the first 2 chapters of SICP are great. Definitely worth reading to get the motivation for scheme and FP in general. I suggest you read this position paper too: [Why calculating is better than scheming](http://www.cs.kent.ac.uk/people/staff/dat/miranda/wadler87.pdf) It explains the rationale for a lot of the haskell features vs scheme. You could definitely skip most of chapter 3 of sicp as the material does not apply to haskell (the way mutable state is handled) or you get it for free (laziness, infinite lists). I would also skip chapter 5. Chapter 4 - "Metalinguistic Abstraction" is the culmination of the book and worth reading. 
Awesome, I really appreciate this.
It's my own Natural module which is exactly the one from the numbers package, but with the constructors exported. 
This is essentially backporting modules into Haskell. I really like that it can happen, but I wish the syntax were nicer.
You should probably put a couple up on the Readme. I always want screenshots when I read about a project like this...
Cartel author here. Old time's sake? :) heck, you still can write a Setup.hs module which dispenses with having a Cabal file entirely, as long as you're okay just using the "runhaskell Setup" interface. I tried this for a brief time, writing the cabal file using the types in the Cabal library and invoking the right Cabal function straight from my Setup.hs. Then I realized, oops, the cabal utility and Hackage would not like this at all. I think Cabal took the right route with making the format plain text. I'd rather use an additional tool to generate the plain-text file (that's how Cartel works) than allow all Cabal files to be arbitrary executables. Most simple packages work great with the plain text format; it's only the big packages with many pieces that I wrote Cartel for.
I think the advises given above a kind of double-edged. In one hand if your ultimate purpose learn pure FP you could follow them (although I'd rather skip SICP entirely). But if you want to get a wide overview of different computing techniques (where SICP shines like nothing else) you definitely shouldn't skip anything. Especially chapter 3, which shows how to get all the fun running on our ancient von Neumann machines.
Someone in /r/learnprogramming posted this: https://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours Which seems like an awesome way to touch on fp without diving into SICP. 
Oh, it is. I just haven't run into it not being there anywhere except in GHCi. Thanks for the correction.
My biggest issue is redundant bits throughout the file, particularly package dependencies. With a library, some executables, and some test executables, I must specify dependencies for each. There is often much overlap; at a minimum, each component depends on base. When editing for PVP compliance, I must make multiple changes when even a single dependency changes. Perhaps some sort of macro expansion or variable substitution scheme could help with this, and with other redundancies (like wanting "-Wall" in the ghc-options for each component.) That said, I actually think it's a bad idea to add anything to the .cabal file syntax. It's complicated and big enough as it is, and even now there are parts that aren't even documented. Rather than add to the .cabal file syntax, I think it's a better idea to tidy up the Cabal modules so that they are easier to understand so that one could easily use the Cabal AST to generate her own Cabal file. Right now the GenericPackageDescription is a head-scratcher: it includes a PackageDescription, which includes lists of executables and test suites, but the GenericPackageDescription itself also has lists for these things. Which do I use? I know the answer to this question, but only because I pretty printed the result from the Cabal parser to see what it is actually doing. The impression I get is that the PackageDescription module used to be simpler but then conditionals were added, and now it just needs some refactoring. Having one module with the types that result solely from parsing the Cabal file, and another module to represent the package description after resolving the conditionals, would help a lot. If Cabal had a clean AST, it would be trivial to use it to write package descriptions. It could then serve as a base upon which to do things like read a file tree to populate exposed-modules. Of course the developer still would need to manually generate the .cabal file, but I'd be happy with that. If I were dreaming and could actually add something to .cabal, it wouldn't be more syntax; rather I would add a feature to generate the cabal file from a program. It wouldn't be much different from using cpp, hsc2hs, happy, and alex to generate haskell sources, except here we'd generate the cabal file. I understand the concerns about running arbitrary code, but Safe Haskell could be used to manage this (which I also understand that Safe Haskell is primarily a GHC thing, but I said I'm dreaming here.)
SICP won't really work in Haskell‚Äîor any language too different from Scheme. Parts will, other parts won't and the impedence mismatch will just get in your way. Happily, Scheme is one of the simplest and most elegant languages around, so learning it is not a chore! I would definitely do it in Scheme. I started learning Haskell at the same time as taking a course on SICP in Scheme. The two are actually complementary: some ideas from Haskell (like pattern matching) really helped me understand some of the core topics in SICP. A while after SICP, I went through "Write Yourself a Scheme in 48 Hours" (which, I gather, you already know about). That was what really pushed me through on practical Haskell. Highly recommended. Of course, before that, I had written a Logo interpreter in Scheme (as a class project). I think doing that first might be a good idea‚Äîmaybe just follow SICP and implement a Scheme interpreter in Scheme. After doing that, the Haskell version will be easier. It's also really neat to see how Scheme reuses data structures for syntax, which is what makes the "metasyntactic interpreter" in SICP so interesting and also what makes Scheme macros exceptional. (Macros aren't really covered in SICP, but they're worth learning about on your own afterwards.)
Great advice. I'm convinced that I should do SICP in Scheme!
Minor correction: For the DMR to activate, it is not enough for something to be not-obviously-a-function (in a silly way, as if `x = \y -&gt; ..` is not obviously a function!), but it also needs to have some type-class cosntraint. Here is ghci-7.6.3: Prelude&gt; let x = \y -&gt; y Prelude&gt; :t x x :: t -&gt; t Prelude&gt; let x = \y -&gt; show y `seq` y Prelude&gt; :t x x :: () -&gt; () 
It's enabled by default everywhere, except in ghci in ghc 7.8 and above.
Most of the problems arise in ghci, but it wouldn't help those that arise in ordinary ghc compilations, would it?
I published this just before giving a talk to the London Haskell User Group about just this. Hopefully there will be a video to accompany the blog post shortly (though the blog post is a bit easier to follow!)
Oh wow! I enjoyed this soooo much. Thanks! Very much like it - full ack: "Exceptions are the primary error-handling mechanism employed by many widely-used languages. They are also a side-effect that makes a liar of the type system, and makes local reasoning about code far more difficult. They represent an undeclared method result smuggled through a back-channel separate from its declared return type. Furthermore, they transitively become an undeclared result of anything that calls that method, and anything that calls that, and so on. Trying to reason about the correct behaviour of code becomes very difficult, since the return type can no longer give you enough information. Exceptions kill modularity and inhibit composition." - Yes, yes, yes! The best summary about exceptions that I ever read. "If the only meaningful return values are -1, 0 and 1, indicating ‚Äúless than‚Äù, ‚Äúequal‚Äù or ‚Äúgreater than‚Äù, then why would we embed them like specks of dust in a big desert of undefined behaviour?" "Any restriction, structure or constraint in the format of the string means you don‚Äôt have a string at all; you have a URL, a Date, a Name, an Email, a Document, an ID, a Warning, or whatever else that might tempt you to deploy this amazing swiss-army-type." "A couple of years ago, we were keen to avoid over-specific domain modelling, and took care to build our services as dealing in the domain of ‚Äúrecords‚Äù or ‚Äúattribute-maps‚Äù, rather than specifically tie our logic to Listings, Agents, Dogs, Cats, Aeroplanes or what-have-you. This was to prevent the loss of generality, and to keep the application focused on web and infrastructural concerns. [..] In an absolute sense, I can‚Äôt say whether this is a good idea or not." "OO lore has it that pattern matching is evil, and that subtype-polymorphism is the answer to all questions." Half agree: "Primitive values such as integers and strings are often the first tools we reach for, but are woefully unsuited to most use-cases they are press-ganged into. This is because they have an astronomical number of possible values, and most use-cases do not." - I agree with the examples that the author has shown, but have to say that sometimes primitive number types are used in programs like mathematical numbers without limits, where the limits should be considered. "names are totally useless for reasoning about software" - this seems to be true more for static-typed languages, where the type signature tells more than the name. "Treat your types as the only real documentation." - in the end types have as well names, and these have to be chosen wisely. So in the end it is shifting the importance of naming from functions/variables to the naming of types. But even if you have well understood type names, you still need a meaningful function name. Here I've got lost: "Wrapper types start from one line of code in Scala."
&gt; "names are totally useless for reasoning about software" I agree. I've seen many bugs that stem from functions with names like getCurrentPrice Doing ... anything but that.
Useless for reasoning maybe, but excellent for grokking. I still think names really, really matter. The hard part, of course, is that functions often evolve away from their original name quite rapidly (assuming they were well-named to begin with). That's another win for types, of course: there's less fear about introducing bugs when renaming those functions.
Uh, these slides aren't going to be so good without accompanying commentary. There were some people who were scribing the lecture so at some point we'll add text.
Same here, especially from functions which encode predicates. I don't know how many times bugs I've chased down because a function called ``isProperty`` has the resulting Bool flipped either because "Property" is an ambiguous word in English or by simply accident. Boolean blindness is a dangerous beast.
What if the type or its constructors are incorrectly named?
[On hackage](http://hackage.haskell.org/package/lambda-placeholders-0.0.0.0)
I don't follow, are you asking what happens if you two expressions which are identical at the type-level but can be mistakenly interchanged at the value level by a typo? If that's your question, the only answer I can respond with is try not get into that situation!
Done! Thanks for the suggestion.
Yes, please post when you have either a video or transcript to go with the slides! I found the GC section pretty readable without commentary but the threading section had too many confusing bits that needed explanation next to the pictures.
&gt; Haskell has a spectacularly rich static type system, with no runtime-accessible metadata, apart from pattern matching on tagged unions. Unless you count `Data.Typeable`.
I mean that you can still have misleading constructor or type names, just like you can have confusing or misleading function names.
Do you link the source somewhere? I can't get some of it to compile, for example, my ghc refuses to see that it can always find a monoid instance for all cases of mkKeyList Here k = Cons [k] mempty On the other hand, my ghc (7.8.2) does't seem to be demanding `-XOverlappingInstances` for the `Somewhere` instance. By the way, the `Applicative` instance for the first `Querying` type should read Querying ks f &lt;*&gt; Querying ks' x = Querying (ks &lt;&gt; ks') (f &lt;*&gt; x ) where the second `&lt;*&gt;` is just the S-combinator. 
I don't know when you'll have the opportunity, but whenever you can present this to a users group with video, getting the talk recorded and on the internet would be a great resource!
&gt;I know of at least two published books that were written to explain Coq libraries... To be fair those are probably more about general Coq thinking than about their libraries...
This is also [being discussed on r/clojure](http://www.reddit.com/r/Clojure/comments/26ol0g/opinions_on_the_abject_failure_of_weak_typing/).
It's his own blog and he chooses the content. The work we are doing lately is certainly in relevant areas, though: data analysis &amp; mathematical modeling in finance and in the chemical and life sciences. In fact I just posted a new job opening a moment ago.
I do not like neither your mockup, nor particularly any other listed web sites. With the exception of python. Here's how i see the best programming language web site. Top header narrow, not wasting half of my screen. No top menu please, no search boxes, no code samples, no nothing. 2, 3 phrases simply telling what the heck haskell is with nice logo and cool background. Right under it huge Tabs across entire screen with first tab being active and occupying the rest of the screen: Work | Download | Learn | Socialize Work tab: Has all the useful widgets for everyday working haskeller. Package search, hoogle/hayoo search. Recent packages, IRC chat log, all the things you often use when working. Download tab: Has all download info: 3 Huge buttons: Windows | OSX | Linux. Sections for tools downloads: cabal, haskell platform, sections for various IDEs etc. Learn tab: You can have that awesome interpreter window there, panels with links to tutorials, books, wikis, GHC documentation, embedded youtube videos of talks etc. Socialize: Has news sections, active digest of reddit, google groups, google+, selected blogs, funny quotes etc. The web site can remember the last tab you've been. So it will open it for you next time you visit.
I like the colours.
It can be looked at as a piece of art. So beautiful handwriting and pictures :-)
&gt; nor particularly any other listed web sites. With the exception of python. you don't like the ocaml or scala homepages?
&gt; Placeholders in Scala act similar to those used in category theory. Don't you just mean "in mathematics"? It would be nice to have lots and lots and lots of examples!
I've often felt that haskell.org is not serving us well. It's definitely better than what's there before, but otherwise I agree with Chris' critique. We should consider integrating (many of) Chris' changes into haskell.org.
"Its design is broken or just strangely put together and it‚Äôs not responsive." That's a pretty bad first point, in that it is responsive.
'Work' doesn't seem like something I would search for on a language website. I don't use Haskell for work (yet?). I want to learn Haskell, which has the potential to help my work, but for the moment it isn't related to work. 'Documentation' seems more accurate (Hoogle is also just documentation). IRC logs (and anything else IRC related) seems more related to Community.
Vision beats committee. Comments are a good thing, but that's that they are - comments. Good job and good luck!
I can't wait for http://ezyang.com/jfp-ghc-rts-draft.pdf to be fleshed out :-)
I agree in general, but Haskell is *not* a "purely-functional" language. For one, it's a stellar imperative language. Make that "advanced pure, functional language".
I love it that the picture is of SPJ, nice touch!
+1
I really like the mockup, more so than any of the 'example' sites Chris mentions. The (shades of) purple look great. I do think the sections at the top look a bit too square and... sectiony. Perhaps dividing things up according to the [rule of thirds](http://en.wikipedia.org/wiki/Rule_of_thirds) might help. One way to do this would be to move the 'Natural, declarative, statically typed code' bit into the white section below it. That way, you no longer have the strong two-against-two effect. A lot of websites do this; the OCaml and Scala ones for example, as well as [FPComplete](https://www.fpcomplete.com/). Dividing things up into threes works really well in general, it's one of those 'once you see it' things, in my experience.
IMO the biggest issue to learning Haskell by oneself is that almost all the existing library documentation require you to already be a haskell expert before you can use them, nevermind just understand them. It's very difficult to learn "apprentice style". What I'd really love to see is the library documentation significantly enhanced by examples of how to use them that don't require a deep understanding of advanced concepts. For example, I can easily use hoogle to find a tcpip library (several of them) and I can look at the types and functions. But it's incredibly difficult to put that stuff together to make a working program without a steep learning curve. Instead, give me black-box cookbook stuff that I can begin to deploy and understanding will follow over time. Right now, it's way easier to use other languages to get the job done, even with very little knowledge. For a practicing developer, that counts for a lot.
&gt; I can see how reading the resolved package description is easier and how it's easier to write a package description without any conditionals. I wasn't too clear here. As a developer who's writing a Cabal package description, I don't care what data types are used to represent the package description *after* the conditionals are resolved. I only want to write the description *before* conditionals are resolved using the flags, target OS, etc. The problem is that Cabal's GenericPackageDescription mishmashes the "before" and "after" types, so when you look at the Haddocks for Cabal, you can't figure out which is which. The GenericPackageDescription contains a PackageDescription, with the latter being used both before and after conditionals are resolved. I only suggest splitting before and after into different modules because clearly Cabal needs to have the "after" types, but someone writing a package description doesn't care about the "after" types. Splitting it up better serves both uses. I wrote an AST for just the "before" part for Cartel: http://hackage.haskell.org/package/cartel-0.6.0.0/docs/Cartel-Ast.html I just wish Cabal had decipherable types already so that there would be no need to write something like this outside of Cabal. &gt; I don't quite get your point here. Should the cabal file generating tool be executed by the user or should the code be put into a cabal package instead of the cabal file? &gt; Your concerns about security indicate, that the code should be packaged, but this would highly reduce the ability of other tools to operate/modify the cabal file. The Cabal file generator would be distributed in the package tarball. The user installing the package would execute the Cabal file generator, just as the user installing the package now runs hsc2hs, alex, happy, and Template Haskell. Other tools could still modify the Cabal file after it is generated, though I do wonder if automated Cabal file editors are a good idea. 
&gt; natural language text is important for explaining why things are the way they are and documenting intent I think the idea was "when writing your types, pretend they are going to be your only documentation to motivate you to do them right" not "don't write comments" :)
Why bother? Avoid success at all costs! Edit: Wow, some people don't know the unofficial motto of the Haskell community.
I was excited by your initial comments to get the wiki off of Haskell.org, then you appeared to sort of be putting it back in in the end. I'd strongly support just dumping it off of Haskell.org. (It can live somewhere else. Domains are cheap now.) I've been through the "Somebody posts some 5-year-old Wiki page from Haskell.org that is little more than one dude's semi-ranty wish list for a feature or a now very-out-of-date description of how to use a now-defunct library on HN -&gt; huge conversation ensues about how sucky this all is and therefore how sucky Haskell must be -&gt; I have to explain this has little to nothing to do with Haskell, it's just some dude's ramblings from 5 years ago that, alas, happens to be hosted on the official haskell.org site" a few time now. This is not "just" a problem of being out-of-date; it's a problem that (nearly) anybody can use the authority of "haskell.org" to post any ol' thing. Haskell.org's authority ought not to be so available. Or it needs to be _way_ more clearly described on the page itself that this is completely unofficial, the way it's clear that PHP's user comments on its documentation are not the documentation itself, though I'm not sure that this is nicely solvable for a Wiki page the way it is for comments.
This is about where I am with reading xmonad source. More well-documented codebases to ingest would be great.
Why does the dependency specification in Haskell have to be turing complete?
&gt; "My relationship to haskell.org over the years has been one of stonewalling when requesting access, of slow replies, and of bike-shedding and nitpicking when proposing designs." Hi Chris, I'm sorry you feel that way. However, I went through and checked my mail archives and nothing I've seen confirms this, at least in the past few years. We had an issue with getting hpaste (now lpaste) a haskell.org subdomain, but that issue was because we were in the middle of a big admin turnover and vacations of key people and so on short notice we had to scramble with figuring out how to allocate dns records. A number of people did a bit of running around to try to get that sorted out, and I'm sorry that in the end you ended up moving ahead without us (it was a matter of days before we got it sorted out that lpaste went live instead). I checked my other mail archives from you, and saw you requested access to www around october or so? Within a week, johnw wrote you and requested your ssh keys, etc to set you up. I don't think you were stonewalled at all! Certainly not intentionally. When you say slow replies, there's some truth to that. This is because the core infra team is basically all working on volunteered time, carved out from schedules that include jobs and lives outside of core infra. You're a fast dev who gets an idea and just runs with it--I get that, and think its great. But the fact that replies sometimes take a little time shouldn't get in the way of the community _process_. Furthermore, I've never seen you jump on irc into the haskell-infra channel and just try to get things resolved directly (although we've chatted on irc). Now if you were to actually replace the existing haskell.org frontpage there would be discussion -- but that's always the case! In fact there's a discussion going on now, you're just organizing it. And in a discussion, people will of course nitpick and make good and bad points both. And points you agree with and points you find ridiculous (but others don't). That's how discussions work! As you well know, we're a picky community. (but note that's not _haskell.org_ bikeshedding, that's just everyone in the haskell community doing it!) Also, in my archives, I notice no emails at all to the haskell.org committee directly, which is responsible for the haskell.org domain and infrastructure (you can read about it on the wiki!). So again I don't know how you have reached the conclusion that the people responsible for this are bad at communication when you've never written that group an email, directly, attempting to communicate (at least since my tenure on the committee). That said, I agree that there are a lot of good ideas in this that we should work to bring over to the main site, and I like the idea of just knocking out a prototype without the peanut gallery weighing in, and I agree that the haskell.org homepage looks pretty out of date these days.
It doesn't. But if you let people write Haskell programs to generate dependencies, they'll introduce SAT by accident at some point...
I don't really count that as the same runtime magic as java's reflection or C++ RTTI. As of GHC 7.8, it's true that it does have a bunch of compile-time magic. But at runtime? It's just a dictionary being passed around, the same as any other type class.
Your mockup is modern and informative. I really like it and hope this goes somewhere. Very impressive design skills, Chris!
Ah, I wish I found your project sooner. Just last week I have installed Haskell Platform on my new system.
Perhaps this could be done by the community, PHP.net style. On each page of documentation there is a section for comments that provide examples of use. This is great for getting started with a library.
Scala site is the worst. 100% of my screen estate is wasted on static background and pictures. Why on earth would i want to come there SECOND time? Basically the entire home page is pretty "about" page. Useful perhaps for someone who never heard of scala. But for someone who comes to the web site 2nd or 3rd time? No. And that's generally the problem with all those language home pages. "About" pages are boring. That's why no one is visiting them. Compare their layout to the layout of web sites that serve everyday returning users. Like StackOverflow or reddit. What is the first thing you see when you land on a home page? That's right. Not explanations of what it is. Rather things you come here looking for. And that's what makes the difference between buzzing alive web sites and ghost towns like all those listed home pages for languages. 
Agree 100%.
Python (or CPython at least) does ship with an optional cycle detector that is enabled by default on most platforms. See [here](https://docs.python.org/2/extending/extending.html#reference-counts).
I've never seen a Python installation without the cycle detector enabled. The * is probably because if you implement `__del__` on a class, then the cycle detector can't collect its instances due to the semantics of finalization (a reference to the object itself is available). There's a workaround for that using weakref callbacks, but tricky to use.
Wow, that is really good. After viewing it, I have the desire to know more.
I agree with your first two paragraphs, but not the third. Historically, it's been very confusing what's hosted on haskell.org, and what isn't. When I started with Haskell, the site hosted a lot of obsolete, bit rotted projects that reflected poorly on the community. Now we have stale wiki pages offering worse information than SO or a random blog. I agree that the wiki being hosted there is a valuable resource, but I'd like to hear suggestions for how to improve it (eg separate vetted, maintained pages from miscellaneous notes). Some level of sanctioning is suggested by virtue of the domain, and we should treat that with respect. Maybe the best we can do is make it more clear that it is an unofficial wiki, but there seems to be agreement that we could do with some improvement in how we document things, so perhaps the wiki can play a role in that (eg more pervasive linking of wiki pages from/to hackage).
It's *really* unfortunate that a non-standard format was chosen, though. The fact that ghc has a dependency on Cabal is amazingly annoying as is the fact you effectly need to depend on the Cabal library to parse .cabal files for basic information such as dependency lists between co-developed projects.
It really needs a standardized syntax which is easier (read: trivial) to parse from other languages. Having to depend on Cabal (and thus requiring Haskell) for parsing .cabal files is far from ideal for various reasons that I could possibly go into, but I won't bore you with the details.
Yes that's why I was wondering
Chris's comp has a [Try Haskell](http://tryhaskell.org/) instance near the top of the page. It's probably reasonable to think that the snippets could be used right on the page.
What an entertaining pronunciation of `split`.
Totally this. Having a comments section on hackage would be awesome.
Metadata on Hackage needs to be editable by the users. A discussion under each package would help.
Couldn't the same be said of Java's reflection stuff? At runtime it's just a tag pointing to a dictionary.
For implementation, yes. For semantics, no. It's magical stuff you don't have the ability to do yourself. Typeable's only magic is in preventing instances from lying (and only in GHC 7.8+). Everything else it does, you could do yourself.
Isn't this why we're all be using dependently typed languages next year anyway :)
For `time-units`, you should use [`addTime`](http://hackage.haskell.org/package/time-units-1.0.0/docs/Data-Time-Units.html#v:addTime). Compare convertUnit ((5 :: Millisecond) + convertUnit (6 :: Second)) :: Microsecond with (5 :: Millisecond) `addTime` (6 :: Second) :: Microsecond
&gt; Everything else it does, you could do yourself. I'm confused. You said that the difference is semantics, but "you could do it yourself" is just talking about a different implementation.
in what way can liquid haskell be used in production? is it worthwhile to include these comment hints in packages like `text`? (i assume they cross module boundaries but can't find the answer. i guess i read that before though.)
It's actually "avoid 'success at all costs'." We still want Haskell to succeed, but not at the expensive of our purity.
This should mostly be possible. The wiki is 'mounted' on the site at www.haskell.org/haskellwiki, so a redirect for future compatibility should work OK for the most part.
I agree with all of this. I'd really like to get a new design up for the website - we're doing a *lot* of infrastructure upgrades at the moment, and a new design would be really cool. Chris, if you're reading this, drop by `#haskell-infrastructure` on freenode and pester me or `johnw` when we're around, we're more than happy to set up a lot of this new stuff if you need help. **EDIT**: *cough* And if someone would like to touch up the GHC webpage, that would be great too. :)
I agree about the Scala page. It looks like an ad for a car or something.
What is instant mode ? (A google search didn't help much)
Press the link at the left of the top bar of [Hoogle](http://www.haskell.org/hoogle)
If you like this stuff, I strongly recommend reading Vazou, Rondon and Jhala's paper *Abstract Refinements*. This is the paper that convinced me that SMT-based refinement types are an idea that really has legs. The basic difficulty with refinement types is that SMT solvers really want to work with primitive types like ints and bools, and don't really handle anything beyond that very well. So while refinements easily do things like indexing vectors with their length, it isn't clear how you can do things like build abstractions *on top of* vectors, because it sort of seems like you need fancy quantifier instantiations which are sure to defeat an SMT solver. The abstract refinements paper makes the really clever observation that actually, polymorphic types makes life easier, rather than harder! This is because an abstract refinement `P x` can be viewed as a new primitive proposition symbol `P`, with absolutely no special properties, which means the SMT solver can treat it as an uninterpreted term --- which SMT solvers *do* handle really well. This is really slick, and IMO it's honestly a better direction for Haskell to follow than full-spectrum dependent types, because it extends the ML/Haskell style of programming to support verification in a very natural way. 
It is all tongue-in-cheek so there's no need to debate the meaning. Yes, we don't want Haskell to be successful at the expense of purity. We don't want Haskell to be successful to the level where things can't be changed without backlash (see Python 3). Of course, it is a Good Thing to promote Haskell and I do like this proposed alternative design.
ugh, please don't make people reload the whole site just to get another snippet - that sounds awful, and will load the servers
Chris: It is awesome and beautiful! Community: Let's drop the nitpicking and *get this thing up there!* A challenge: Commit to getting it up by mid-July? I'll pledge we'll have a shiny new Haskell Platform to go with it... and as we are re-thinking the HP web site - now's the time to pull it together...!
The reason it says `Maybe a0` instead of `Maybe Int` is that it can't know that `5` is an `Int` ‚Äì it could be an `Integer` or a `Float` too! Or any other member of the `Num` class. It's a weirdness with literal integers in Haskell, not the type system in general. If you used `Just "hello"` instead, it would indeed say Couldn't match expected type `Int' with actual type `Maybe [Char]' In other words: when it infers a polymorphic type, it will say something along the lines of `Maybe a0`. Literal integers in the source code are polymorphic. Why it's `a0` instead of `a` is, I assume, because GHC uses numbers to distinguish `a0` from `a1` and `a2`.
It would be very interesting to see a comparison of the GHC RTS and Erlang's BEAM runtime as they're a bit more similar (FP, GC, cheap threads)
github repo: https://github.com/connermcd/sed Previous episodes: 1. [Introduction](https://www.youtube.com/watch?v=0I90MTip-OQ) 2. [State monad](https://www.youtube.com/watch?v=bMNDBym96mo) 3. [efficiency, ListZipper, Text.Regex](https://www.youtube.com/watch?v=io6D9u_UKVQ) 4. [Text.Parsec](https://www.youtube.com/watch?v=i02SarWmuK4) 5. [command line options](https://www.youtube.com/watch?v=NsTF7RUzd4c) 6. [testing](https://www.youtube.com/watch?v=GKRjVITL_oA) youtube playlist: https://www.youtube.com/watch?v=0I90MTip-OQ&amp;list=PLUQzXLQ6jvHL_k3QOMKXehVoZdk-sKtHd 
Oh my god, this is amazing. This is the most wonderful resource. Reading this has really made my day.
I agree with you that `a0` could probably be simply `a` in that error message, although I can guess why it's easier for GHC to give numbers to each type variable rather than have special cases. Consider `addThree (Left (Left 5))` where there's the outer `Either` and an inner `Either`. In this case it reports `Either (Either a0 b1) b0` in the type error, since it needs to talk about two completely separate variables that were named `b` in the type constructor for Either. The reason why it shouldn't tell you that `Just 5` isn't a `Maybe Int` is because integer literals are polymorphic in the sense that they can be used for any instance of the `Num` typeclass. It would quite difficult for the compiler to guess a concrete type here since no matter what it chooses it won't unify, and even if it did it might be confusing since you could've easily intended it to be `Integer` (or any other `Num` instance) rather than `Int`. It *could* tell you that it expects `a0` to a be an instance of `Num`, although I assume it doesn't because that's not related to the problem and leaving it out makes the error easier to read.
That's true. I find it interesting that `seq` used to be relegated to a typeclass, too.
Yes, and changing that was a mistake IMO.
I wonder if similar type level wizardry could be applied to templating libraries. I'm thinking of a "purer" version of compiled Heist.
Thanks /u/neelk ! http://goto.ucsd.edu/~rjhala/liquid/abstract_refinement_types.pdf
what would have been a suitable "standard format"? 
To clarify: &gt; Is it clever enough to only test those conditions at compile &gt; time and never adds any overhead at runtime? Yes. There is *no* run-time overhead. All checking is at compile-time.
There's a semantic difference between RTTI/Reflection and Typeable. RTTI/Reflection is a specialized mechanism that can do only one thing. It's unlike anything else in the language, totally irregular. You couldn't implement it yourself, it needs special support from the runtime. Typeable is a totally standard type class. If it didn't exist, you could build it yourself from existing mechanisms in the language. It's regular, except for (once again) the compiler support for preventing you from breaking its assumptions in newer versions of GHC. One's an extra tacked-on feature. One's just a natural consequence of features already in the language. I consider that difference fundamentally significant.
&gt; In fact there's a discussion going on now, you're just organizing it. Whether or not he's organizing this discussion (I'd say not ‚Äî it's just that his post has started a discussion), the important thing is that he doesn't depend on its outcome (although he could still draw inspiration from it). This is something I sympathize a lot with. Were Chris to replace the existing haskell.org frontpage, there would always be some vagif who wouldn't like the new design. It is so much better not to have to persuade anyone that what you're doing is worthy. Instead, let the work speak for itself, and let the users decide.
Both of these answers make sense, thanks for responding. Of course now I'm wondering why the error message couldn't include the typeclass (Num, Fractional, etc) Maybe Num a0 =&gt; a0 for example. Guess I'm never satisfied but heck, at least I understand some of this stuff now!
Typeable is not runtime metadata, it is literally a typeclass method dictionary realized at compile time. 
Would you happen to have an example of where this library buys me something over plain Haskell? Your example: curried_foo = foo.$.(2, , ) is only three characters shorter than a regular function: curried_foo x y = foo (2, x, y) Given the requirement for `TupleSections` and an import, I'm not losing sleep over three characters. 
How does this mesh with user input? What if I ask the user which index to read and they give me one out of bounds? The other big error is forgetting to verify user input. Protecting against this is not possible without runtime bounds checking.
User input always has to be checked and/or sanitized at run-time. The benefit of specifying the bounds with a system like LiquidHaskell is that it will complain loudly if you forget to insert the necessary run-time checks :)
&gt; Protecting against this is not possible without runtime bounds checking. Indeed! In this case, the refinement system would essentially check that the programmer had done put in appropriate "input sanitization" before calling the various accessing functions. For example, see: http://goto.ucsd.edu:8090/index.html#?demo=permalink%2F1401418498.hs 
Are you talking about the ??? operator in scala? I think perl has similar ... too.
If `MonomorphismRestriction` is enabled, things are a bit different L&gt; let something = Just 5 L&gt; let y = addThree $ something something will have type `Maybe Integer`
Useful debugging/error messages for users is actually widely considered one of the biggest problems in Haskell for widespread adoption. Especially when you go into deeper abstractions, error messages tend to leak the abstraction and basically make them impossible to debug without understanding the abstraction implementation --- something that defeats the purpose of the abstraction in the first place. There are a lot of proposals, but to this day the problem remains pretty big and pretty embarrassing :/ But, if you used to work in debugging technology, maybe you can perhaps one day be the change you wish to see in the world :)
You want json or yaml or something?
Excellent work, I wonder why the author didn't create a simple data type such as data IsQuiet = Quiet | Loud rather than using `Bool`.
There might be a technical reason it is that way, such that the type checker has already tossed away the information about the typeclasses once it gets to the point where the error message is triggered, but I don't actually know. It would be useful if it included the typeclass.
With a lot of these job postings I find it very difficult to get any idea of how good you actually have to be to make it worthwhile applying. I'm sure I'm not the only one put off by this - and I'm sure there are qualified applicants who don't apply because they think the required standard is higher than it is. /u/FPguy : can you produce code samples at the minimum quality you'd expect for applicants to this kind of thing. Or some other type of benchmark - perhaps a link to a FPComplete blog post saying "you must grok everything in this post without too much trouble" or something like that? Another thing on a completely different point: &gt; Typical tasks will range from a few days to a few months in duration. &gt; as long as you are available 30+ hours a week This sounds a bit like you are expecting people to keep themselves available for piece work. Is that actually the case?
Anyone interested in the reasons for the removal of the `Eval` typeclass should look at page 33 (section 10.3) of "A History of Haskell: Being Lazy With Class" http://research.microsoft.com/en-us/um/people/simonpj/Papers/history-of-haskell/history.pdf Basically the reason is that it was too awkward to amend all the type signatures when you realised something needed to be `seq` deep in a nest of function calls. 
Just a tip, if you want a more precise error message, then specify the type explicitly: `let y = addThree (Just 5 :: Maybe Int)` or `let y = addThree (Just (5 :: Int))`
I think the answer to that is a link to a "snippets" page, where they can see a large number at once.
Btw, here's my short personal wish-list: - Some facility to inherit/share common blocks of properties between targets inside a `.cabal` file. - Expose the package-version as a `$version` variable for use inside the `.cabal` file to be able to use it for string-interpolation in the `source-repository/tag` field. - Some facility to generate the package-version by some external script (think `git describe`) on each `cabal configure/build`, and have it frozen during `cabal sdist` so the resulting tarball has a plain static version field. I'm currently using an abomination of a `Setup.hs` to accomplish this, but this is currently *much harder* than it needs to be. (I can't use an external pre-processing step, as I need `cabal build` to verify/keep the package-version up-to-date) - a conditional predicate `extensions()` to be able to express that one has (optional) support for certain language extensions, but one doesn't depend on them. Having to use `impl(ghc &gt;= x.y) { cpp-options: -DHAVE_DERIVEGENERICS; other-extensions: DeriveGenerics }` is somewhat unsatisfactory to me - Alternatively, a new `optional-extensions: ...` property which would define a CPP symbol as above for each listed extension if supported by the current compiler. - Some facility to better represent licensing. Right now there's a bit of confusion w.r.t. licensing of FFI-binding packages. E.g. to which parts does the stated licence apply to, only the Haskell parts, or also the cbits? 
And that's a reason that I don't buy without evidence. This was the claim of some people (like John Hughes, who doesn't write type signatures anyway :) ), but I wonder how bad it really is. Most uses of `seq` will be at a known type rather than being polymorphic. 
Here is a classic work by Kent Pitman that describes precisely this problem in a scheme context: http://www.nhplace.com/kent/PFAQ/unwind-protect-vs-continuations-overview.html I've found it very frustrating over the years that this result, well known in the scheme community, has been repeatedly forgotten or ignored in the Haskell community, leading to many silly implementations that do the wrong thing and confuse users. For supposedly being concerned about correctness, we really let those untyped folks get the best of us here! In any case, I think this is a nice clear explanation of the issue in Haskell terms, and I hope that this result will now stick around in people's minds.
&gt; And lets face it, nobody uses kilo seconds. Just to be pedantic, we X-ray Astronomers use kilo-seconds all the time - e.g. [The kilo-second variability of X-ray sources in nearby galaxies](http://arxiv.org/abs/1307.0748) - with occasional mention of mega-seconds when we feel the need ;-) 
agreed. try not to introduce new terms. modify existing phrases if necessary.
&gt; I've found it very frustrating over the years that this result, well known in the scheme community, has been repeatedly forgotten or ignored in the Haskell community The same is true for many things of Lisp. :-)
The refinement logic is limited to SMT-solver decidable predicates and is engineered to *preclude* the use of non-terminating computations inside types. Thus, termination has never been a problem in the refinement setting. However, we found that with *lazy evaluation* non-termination indeed becomes an issue, see: http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/blog/categories/termination/ Fortunately, it is quite easy to check termination using refinements themselves and so this is not a problem in practice. A new paper with details will appear at ICFP this year.
It would be nice to collect a list of those somewhere then ...
This is why when asked whether to learn Haskell or Lisp, I [only seriously](http://en.wiktionary.org/wiki/ha_ha_only_serious) recommend people learn Lisp first. It doesn't suffice to simply list good ideas out of context. Learning Haskell or Lisp is like [going to the moon](https://www.youtube.com/watch?v=lw4r0yUmXVg), other languages seem of little interest afterwards, but Haskell is particularly like a black-hole/one-way street in this sense.
Some of those solutions seem so amenable to typing, too‚Äîdistinguishing between continuations which will be used again and those which will not? Or distinguishing a "final" return on a continuation? Have these ideas been implemented in a typed language anywhere?
OK, well, I'm sure you've thought of this, but since here we're trying and failing to prove a statement `forall i. i : Int &amp;&amp; i &lt; len word =&gt; i : Nat &amp;&amp; i &lt; len word`, how about displaying a counterexample: `i = -1`?
&gt; The point of the examples is to explain the usage, not to justify the usage. You might ask the people who complained that Haskell was missing this feature for justifying examples. You seem to imply that there exists a large enough community of users asking for this feature that its existence need not be motivated by non-trivial examples. Given the download count, number of forks, and non-existence of reddit conversations on /r/haskell, that would appear not to be the case. As such, the only position I can take is as a Haskeller who has gotten along just fine with plain 'ol functions and lambdas. Rather than completely disregard this library, I asked for a compelling example so that I could perhaps be enlightened on what I have been missing all this time, having not used Scala. If there is a reddit thread on say, /r/scala about this "missing feature", I'd love to read it, as it might lead me to a conclusion other than "Haskell doesn't need this". As of now, partial application of curried functions seems stronger than this mechanism that seems to allow for arbitrary locations in a tuple to be filled when said tuple is an argument to a function. It may sound like I'm being cynical, but I'm genuinely curious. I spent an hour or so yesterday playing with this to see if I could come up with a compelling reason to use this myself and failed. *PS - the example imports `Language.Placeholders` which doesn't exist, so you'll either need to change that to `Language.LambdaPlaceholders` as in the module itself, or rename the module itself.*
No, he's talking about the [_ syntax](http://www.artima.com/pins1ed/functions-and-closures.html#8.5) in scala, which is linked to in the readme.
Just to give you a sample of an error message from a different compiler: test/T.hs:5:15: Type error: the expression at test/T.hs: (5:15) - (5:21) has type Maybe a but should have type Int The differing types are: Maybe a introduced at test/T.mu: (5:15) - (5:19) Int introduced at test/T.mu: (5:5) - (5:13) Line 5 is y = addThree (Just 5) So the error message says that while the compiler was examining the expression `Just 5` it encountered a `Maybe` that comes from the `Just` and an `Int` that comes from `addThrees`. The source spans should really be highlighted in your editor for maximum convenience.
Our "one shot continuation" monad in Haskell is just Either:-)
Uh, yeah, but if people knew enough (when first learning) to do that, they would most likely not make the mistake in the first place and if they did, the error message would be obvious. The issue is about understanding errors when you're NOT already an expert.
What software did you use to write the slides? They look great!
Depending on your level of proficiency you might like to try reimplementing the standard monads yourself (e.g. Reader, Writer, State, Cont). If that sounds too easy how about the monad transformers? Or free monads and interpreters?
As far as I know, you really need linear types for the type system to enforce the zero-or-one-time use of a continuation. Which is a shame, as there are many things you can do more efficiently if you can be certain that a continuation is only called once.
Yes, the main problem is that the errors are written from the POV of the *type checker*, not the programmer. This should be inverted.
https://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours 
Maybe try reimplementing some of the popular sorts? Not quicksort cause that's a breeze, but maybe merge sort or heap sort? The Caesar and Vigenere ciphers are also fun to try, encryption and decryption. You could also try implementing the Sieve of Eratosthenes as an infinite list or another prime number algorithm.
An indexed monad could also work, no? Too bad it's such a pain to use indexed monads in Haskell.
I really like this. Please make more :)
Excellent suggestion (and one we are in fact working on :) ... There are various devilish details, but we should have something interesting to report soon! 
The best place for pandoc questions is [pandoc-discuss](https://groups.google.com/forum/#!forum/pandoc-discuss). Matthew Pickering is working this summer on an EPUB reader for pandoc through the Google Summer of Code program. Once that is finished, converting from epub to pdf will be a simple matter: `pandoc my.epub -o my.pdf`. If you need something now and want to dive into Haskell, you might look at [epub-metadata](https://hackage.haskell.org/package/epub-metadata). This should help you parse the ocf file. 
http://askubuntu.com/questions/299747/converting-epub-files-to-pdf-format
TL;DR: Calibre. Should've thought of that myself, though I'd prefer a CLI method.
calibre come with cli tools that can do everything you can do with the GUI (as far as conversion goes) see this [help section](http://manual.calibre-ebook.com/cli/cli-index.html)
Write a small compiler.
Here are the project I learned the most from. Choose the one depending of your level and preference: 1. Make a program to draw a ASCII Mandelbrot set bonus: animate it. [Solution here][1]. 2. Draw a Mandelbrot set picture a. make it a command line with argument program, centralize the configuration (position, size of the image, etc...) b. use all your core for the computation (using Repa for example). [Solution here][2]. c. Show the mandelbrot in real time using OpenGL and responding to user inputs. [Solution here][3]. 2. Make a simple web application, typically a minimalistic Reddit clone. [Solution here][4]. Don't forget the source code could be improved as I was learning myself while making these. Also, the "write yourself a scheme in 48 hours" was great. I did it twice. I learned a lot each time. [1]: http://yannesposito.com/Scratch/en/blog/Haskell-Mandelbrot/ [2]: https://github.com/yogsototh/mandelbrot [3]: http://yannesposito.com/Scratch/en/blog/Haskell-OpenGL-Mandelbrot/ [4]: https://github.com/yogsototh/inmanis
A bit-torrent client. Finger trees, splay trees, crit-bit trees.
How is quicksort a breeze? Keep in mind that the "functional" slowsort you often see as an example of Haskell code is not a quicksort.
I'm learning a ton from this series! I'm still going through LYAH, and a hands-on example of a CLI application is incredibly useful. Couple thoughts I had: For testing, since you've already implemented command line option parsing in Main.hs, why not extract some of that out so you can write something like runSed :: String -&gt; T.Text -&gt; T.Text to be more comparable to readProcess "sed" when testing? Then you wouldn't have to separate the -n and -e flags when testing. Edit: I tried this out on my own, then realized that parsing the -f option has to be done in the IO monad. It seemed to me like the default buffer dump when not specifying -n and the Print command could be split, such that Dump runs Print only if the flag is set. And Delete / the stream processor could call Dump instead of Print. Maybe the logic could be a little cleaner that way?
If this is fun, a small Scheme *compiler* is more challenging project and a good follow up. My current hobby Scheme compiler weighs in at a measly 600-700 LoC so it's not too terrible. Edit: Write You a Scheme isn't easy! But if you like Scheme too then this is good fun
probably not what you had in mind as it requires a list of all primes(divisors is just a helper) divisors x = [d | d &lt;-[1..x], x `mod` d == 0] primes = [p | p &lt;- [2..], length (divisors p) == 2] factors n = [p | p &lt;- takeWhile (&lt;=n) primes, n `mod` p == 0] EDIT probably a better answer factors n = [p | p &lt;- [2..n], n `mod` p == 0, [d | d &lt;- [1..p], p `mod` d == 0] == [1,p]]
This is probably not what you are looking for, but wrapping a C library is probably the best bang for the buck in terms of community impact. You might not learn a bunch about FP but you will learn about the glue necessary to connect Haskell to other pieces of infrastructure, which is very practical. 
Read [the Genuine Sieve of Erastothenes](http://www.cs.hmc.edu/~oneill/papers/Sieve-JFP.pdf). It goes into this in detail.
OK, that ... kind of works. The problem I've got with the `ebook-convert` output is that the formatting of the PDF is awful. Which was why I was hoping for a LaTeX intermediary. The command is easy enough: ebook-convert source.epub output.pdf Let's try outputing to HMTL maybe ... Nope, no plugin for the output format. And docs are rather weak. Hrm.... I'll keep playing with this. 
If you were looking for a quick and dirty solution you wouldn't necessarily need to find all of the primes first, you just need to try all integers that might be prime (in order). Only the numbers that are prime factors will divide evenly into the quotient of the last successful test.
Of course --- but the altruistic piece of me would like to make it easier for others! I've suffered enough üòÇ
thanks for your interests, but I couldn't. Can you write a working code?
Sounds like a homework problem to me. 
yes right. I did it using recursive functions. But I need list comprehension method.
Uhhh, lets be aware that this stuff isn't that easy for a lot of other people.
http://github.com/NICTA/course
Edited :) Didn't mean to sound condescending there!
Do you havea list of such libraries?
Which is one of the great things that puts quick in quicksort. Another is that it operates on arrays with indexing instead of deconstructing linked lists.
Since you'd want to make the library idiomatic to use, you could actually learn quite a bit about FP I think.
and/or a guide for wrapping a C library? edit: [Chapter 8 of the Haskell 2010 report](https://www.haskell.org/onlinereport/haskell2010/haskellch8.html#x15-1490008) 
Quick question before I look further. By "writing a compiler" do you mean a program that emits an executable binary? Or is emitting LLVM and compiling that more suitable for a hobby project?
To make a library idiomatic, you need to know how idiomatic code works first. That also depends on the community, and how far research on particular design principles has progressed. 
For a toy compiler, emitting C or Javascript is the easiest path to getting something up and running. For anything I would use in production for like a domain specific language or numeric specializer, there's little reason not to use LLVM if you're building something that resembles a static compiler since the llvm-general bindings for Haskell are a joy to use and there's pretty [good documentation](http://www.stephendiehl.com/llvm/) to get started.
Didn't expect to see this here! If anyone has any questions or feedback, very happy to hear them! The video was filmed about 7 months ago, and the project is definitely "hackathon quality", but it gave me an excuse to slip some Haskell into Computerphile ;) 
Thanks for the suggestion; I'll try to assemble some sample code suggesting a level. Thanks for the clarifying question; our preference is to keep the person busy every week, not just on call, as we have a backlog of things we'd like done.
The nice thing about this project is that you can implement as much or as little as you want. [My version](https://github.com/lfairy/pinkie) features tail call optimization, an [M-expression](http://en.wikipedia.org/wiki/M-expression) syntax, and a simple REPL. All in exactly 350 lines of code! As an aside: I've found the [Racket reference](http://docs.racket-lang.org/reference/) to be quite useful, especially for understanding advanced concepts like continuations and unwinding.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**M-expression**](https://en.wikipedia.org/wiki/M-expression): [](#sfw) --- &gt; &gt;In computer programming, __M-expressions__ (or __meta-expressions__) were intended to be the expressions used to write functions in the [Lisp programming language](https://en.wikipedia.org/wiki/Lisp_(programming_language\)). Data to be manipulated using M-expressions was to be written using [S-expressions](https://en.wikipedia.org/wiki/S-expressions). M-expressions were used for the original theoretical language in early papers about Lisp, but the first working implementation of Lisp interpreted encodings of M-expressions as S-expressions, and M-expressions were never actually implemented. &gt; &gt; --- ^Interesting: [^Lisp ^\(programming ^language)](https://en.wikipedia.org/wiki/Lisp_\(programming_language\)) ^| [^S-expression](https://en.wikipedia.org/wiki/S-expression) ^| [^MLisp](https://en.wikipedia.org/wiki/MLisp) ^| [^History ^of ^the ^Scheme ^programming ^language](https://en.wikipedia.org/wiki/History_of_the_Scheme_programming_language) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+chvahi9) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+chvahi9)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
http://boundvariable.org/ and/or http://save-endo.cs.uu.nl/ They are literally designed to be done in a weekend, well 3 days, 3 days straight, with a team. But you don't *have* to do them all in a single weekend.
Excellent post. My only complaint is that I would add a section on polymorphic types. Even if you don't make essential use of polymorphic types, it is still great to make use of polymorphic signatures. For an interpreter I was recently writing, I made the interpreter abstract over the type for variables (with an Eq constraint). Even though I only ever use `String` as my type for variables, by using polymorphism in my interpreter's signature, I am assured that I never mix up string constants with variables.
Great post! It's awesome to see breakdown of real world code like this. Keep 'em coming! (If you've got time)
Actually, I would say that emitting native code is more suitable for a hobby project, and LLVM more suitable for a "real" compiler. Writing a native code generator that is actually good enough to use is a huge undertaking, but writing one that just uses a subset of the available instruction set (or even a virtual instruction set, like in SICP) is much easier, and has an educational benefit that I don't think you could match another way. EDIT: Related: http://www-cs-faculty.stanford.edu/~uno/mmix.html
And that argument is sufficient to prove the point.
Good point
if you want to get some ideas about building math compute tools in haskell, the fine folks on #numerical-haskell on freenode are probably a good resource. if youre goal is to support HPC computation, the HPC folks i've spoke to who have tried using cloud haskell have been like "ewww, this is painfully slow". there are tools for supporting HPC computations in haskell, but the distributed-process suite isn't one of them. (it might be suitable for other work loads, but not that one)
I am using distributed-process in high energy theoretical physics research. The computations I'm running generally involve a very expensive computation, e.g. expensiveComputation :: (Double, Double) -&gt; Bool and I want to know something like: what is the region of the plane where this function is true? This involves a 2d search starting with a grid of points, refining the grid near the transitions between true and false. Each individual computation is run on a different cluster node, and a master program collects the results and makes decisions about what computations to run next, starting new computations when new nodes become available. A huge advantage of cloud haskell is that I can set up a monad Cluster a containing the state of the underlying cluster, with functions for running things on the next available node. Its monad instance sequences computations and it has an applicative instance that runs them in parallel (similar to Haxl). I can then describe my search algorithm using simple monadic and applicative combinators, abstracting over all the complicated choices about which nodes things run on, how/when things connect to databases, etc. The separation between the description of the algorithm and its deployment on the cluster is incredibly useful for reasoning about code and iterating quickly. I have not had any problems with distributed-process, although its API is a bit raw.
The Cult of the Bound Variable contest still blows me away with its awesomeness. It's like a coding mystery adventure game. 
This sounds intriguing. I would really love to see a sketch of the ideas in some blog post'ish manner if you get time to write one. :)
Great edofic, it works. But, when we call factors 120, the result is [2,3,5]. The output must be all prime factors, namely [2,2,2,3,5]. How must I edit the code? Thank you very much for your reply.
Me too :)
I've found it very instructive to attempt the coding competition problems on acm.timus.ru and codeforces.com. Both accept Haskell (7.6.x). You'll have to solve a math/logic problem, but I've found them really good practice for basic IO, writing performant loops (both pure and monadic), using arrays/maps/sets and memory / speed optimizations. You'll be working in a domain where the preferred language is C/C++, and it is enlightening to understand how Haskell compares on problems which have very well understood imperative solutions. acm.timus.ru has a larger collection of beginner problems (you can sort them by difficulty); on the other hand codeforces.com allows you to look at other people's submissions. Both sites host discussions about each problem which should give you enough hints about how to solve the problem.
Correct (temporary) link at: http://www.serpentine.com/blog/ . Permalink not working for some reason.
I'm glad to see the cursor representation being used! You managed to beat trifecta to a new release. I'm in the process of rewriting it to use that abstraction as well. ;)
I can see two reasons for this to be included in GHC itself rather than as a preprocessor. 1. Refinements could allow extra optimisations to be used; especially when passed to LLVM. 2. A much more expressive type system to better capture the program specification. Yay for approaching a dependently typed language : D
I can't find it on here? 
I doubt GHC will include an SMT solver any time soon. An SMT solver is a pretty heavy dependency and making sure it compiles on all targets is likely too big a burden. Also, the type checker might become unbearably slow. The timings reported in the paper indicate that there is still some optimizations needed before this technique can be used in a production compiler.
https://github.com/ghc/ghc/tree/wip/ext-solver https://ghc.haskell.org/trac/ghc/changeset/d2917c5bffced002c0a5b0c467298f2540166d62/ghc
It is being explored mostly because Iavor is looking for ways to improve the natural number kind. We don't have a good story for the current solver about what cases it can solve.
Oh, so it's not actually going into the core of the typechecker itself? I guess type nats would indeed be the easiest starting point.
You may want to look at Dana Xu's work on Static Contract Checking / Extended Static Checking for Haskell. She used to call out to an external solver (Z3?) for most of the numeric constraints. e.g. to do UTVPI/octagon solving.
Yep, I do too....but I didn't when I started, which is part of my point. And even when I was following "Learn you...", I didn't come across the very useful notion of declaring types for temporary variables. For example, instead of let x = someFunction I can write let x :: Int = someFunction That turns out to be very useful as well, particularly when you need to know what is the type of someFunction
It was the first entry, but has vanished.
Google cache: http://webcache.googleusercontent.com/search?q=cache:d9yVuE-XuxIJ:www.serpentine.com/blog/2014/05/31/attoparsec/+&amp;cd=1&amp;hl=en&amp;ct=clnk
I love how this answer keeps popping up everywhere. Has this becomen some sort of inofficial reference?
Emacs Prelude and Eclipse FP both highlight the error in your editor. For emacs, bringing your caret to the highlighted source will show the type error in the mini buffer (a kind of superpowered status bar). This is incredibly useful for me. I also have ghc-mod installed in Emacs with Prelude, so it's quite possible ghc-mod is providing part of this. For getting started again after a long break from Haskell I found FPComplete Haskell Center and School of Haskell very helpful mainly because they provide rapid in-editor type error feedback.
I wish for a parsec replacement that has a transformer version so I can implement #include directives in various positions in the grammar. All the fancy new parsing combinators regressed in this aspect.
I simply tend to see the patterns in many places. Typically, I needed to somehow compose incremental map reduce in MongoDB. And I immediately discovered I could use a monadic structure to reach this end. In javascript. I would say I use category theory explicitly in my code only when the problem at hand seems a bit too complex to break simply with my intuition. And when I do it, it tends to help a lot.
&gt; I have written to use algebraic type constructors (e.g. a list having a datatype F a = 1 + a * F a) and so on. Now, I don't tend to believe structuring code to explicitly build out of products/sums is often useful, but I do think knowing that you _could_ decompose more complex data types down to simpler products/sums is important. &gt; [..] how much do you code categorically? To what end do you do so? I write a lot of code categorically. Why? Because it gives me a pattern to generalize my results from simple cases to more interesting structures. Category theory is an important tool for me in that it gives me a vocabulary to quickly explore key concepts in a new domain. Find out how to write it as a category. Ask questions about its initial/terminal object if any. Find out if it has products/coproducts. Figure out if you have any interesting monads on it, etc. I spent the afternoon working on a more general form of Profunctors so I could make more powerful lens-like constructions to scale up to working with complex data types in our compiler for Ermine. We could have written a one-off combinator, but this way we know we've solved it in general, and it is about the same amount of code. If it explodes, we at least gain an understanding of the problem domain on a very low level. If it winds up being too verbose down this code path, we may yet switch to one-off tools. When the category theory vocabulary breaks down on you, it is usually a sign you need to rephrase the problem, because it is underspecified or mis-specified. Learning the category theory vocabulary and machinery is powerful in that it lets you pick up your knowledge of programming and plunk much of your intuition for it down in the world of quantum physics through feynman diagrams, trace diagrams in linear algebra, knot theory, topology, database models... When I drag up examples out of these disparate realms people either nod sagely or look at me like I have two heads. The difference between those two camps is mostly whether or not the person has taken the time to learn category theory. The ability to pick up what I know and use it to tackle hard problems in other domains, or spot that all these things are in some sense the same, is why I use category theory.
I don't know what it means to code "categorically", unless you mean using explicit sum-of-product constructions, etc. I tend to think in those terms and reason that way. And writing a few small projects explicitly in that fashion _is_ the best way to get a handle on it. But like Edward I don't like using that style in the large. To me, a big part of refactoring is finding places where data is "too expressive" and getting rid of that ability to store bad values, and also finding places where data is not expressive enough and I need to thread more information around. In both cases, being able to look at two very different types and say "these contain the same information, or the same information _except_ for X" is very useful. as far as recursion schemes on data structures go, map and fold are just fine, and for generic programming, rather than the "zoo" I tend to think one should go for uniplate (or lens' plate stuff if you prefer that) or just raw compos (http://www.cse.chalmers.se/alumni/bringert/publ/composOp-jfp/composOp-jfp.pdf)
I've been waiting for a [SMALL](http://en.wikipedia.org/wiki/SMALL) compiler for ages!
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**SMALL**](https://en.wikipedia.org/wiki/SMALL): [](#sfw) --- &gt; &gt;__SMALL__, Small Machine Algol Like Language, is a [programming](https://en.wikipedia.org/wiki/Computer_programming) [language](https://en.wikipedia.org/wiki/Programming_language) developed by Dr. Nevil Brownlee of [Auckland University](https://en.wikipedia.org/wiki/Auckland_University). &gt; --- ^Interesting: [^Animal](https://en.wikipedia.org/wiki/Animal) ^| [^Small ^forward](https://en.wikipedia.org/wiki/Small_forward) ^| [^Small ^business](https://en.wikipedia.org/wiki/Small_business) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+chvmyds) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+chvmyds)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I don't know that coding categorically is interesting at all. I think reasoning categorically is. Further, once you become practiced at reasoning categorically then as you code you will recognize categorical structure like an old friend. It's a not uncommon experience while coding for me to mentally mark something I've written as "oh, that's just a kleisli arrow". Oftentimes I will manipulate the boundaries between functions to better respect these half-consciously noticed categorical structures. Also, whenever working in the abstract, it's common to get stuck on the proper way to generalize something. CT gives you a vocabulary and process for seeking out the building blocks of a general theory obeying just exactly the structure you care about. It doesn't always work out, but it's often a great exercise to "categorify" things. John Baez is basically making his career out of it, for instance.
&gt;Now, I don't tend to believe structuring code to explicitly build out of products/sums is often useful, but I do think knowing that you could decompose more complex data types down to simpler products/sums is important. Yeah, I can't picture myself actually *using* code written like that. &gt;Learning the category theory vocabulary and machinery is powerful in that it lets you pick up your knowledge of programming and plunk much of your intuition for it down in the world of quantum physics through feynman diagrams, trace diagrams in linear algebra, knot theory, topology, database models... This is exactly why I am learning it :D
Great, thank you.
I will definitely be stopping by. I think some of the people working on ```numerical-core``` hang out in there. It may still be beneficial for me to write something in Haskell and monitor the performance for any future applications.
Try just `cabal install language-javascript-0.5.13`. What exactly is the full error?
&gt; is there a Haskell equivalent of Python's virtualenv Yes, they're called sandboxes. See here http://www.haskell.org/cabal/users-guide/installing-packages.html#developing-with-sandboxes
&gt; Is this what is know as "dependency hell"? It could also be just a case of what I'd call "HP Hell", caused by having packages installed in the global package db. If I were you, I'd only install `ghc` and `cabal-install` via `apt-get`, and then install the rest via `cabal install ...` and maybe use sandboxes if you work on several projects which may have conflicting package deps.
anyone knows why /u/bos took the post down?
You can do this with `pipes-parse`. The `Parser` type is a monad transformer. Note that you would not be able to do this with a backtracking parser (since you can't backtrack from a side effect).
I do backtrack in parsec with IO. I don't expect the IO effects to be backtracked of course..
 palindrate = (++) &lt;$&gt; id &lt;*&gt; reverse palindrate = liftA2 (++) id reverse remember that if it takes you a few moments of thought to make an expression point free, it'll take longer to read it and understand it later. which is a bad thing. take care to only make point free things that are trivial to make point free. 
I think I've encountered this before. Check if you have alex and happy installed. The problem is that cabal doesn't auto install build-tools :(
I think everybody here is reading too much into the algebraic data types part of your question. Here's an example of how I'm coding categorically right now. I'm designing widgets for composable GUIs, and the type that I use for these widgets is: {-# LANGUAGE ExistentialQuantification #-} import Control.Category (Category(..)) import Control.Monad ((&lt;=&lt;)) import Control.Monad.Trans.State.Strict import Data.Monoid (Monoid(..), (&lt;&gt;)) import Lens.Family.State.Strict (zoom) import Lens.Family.Stock (_1, _2) import Pipes import Prelude hiding ((.), id) data Widget a b = forall s . Widget { seed :: s , runWidget :: a -&gt; ListT (State s) b } instance Category Widget where (Widget sL fL) . (Widget sR fR) = Widget (sL, sR) (hoist (zoom _1) . fL &lt;=&lt; hoist (zoom _2) . fR) id = Widget () return instance Monoid (Widget a b) where (Widget sL fL) `mappend` (Widget sR fR) = Widget (sL, sR) (hoist (zoom _1) . fL &lt;&gt; hoist (zoom _2) . fR) mempty = Widget () mempty instance Functor (Widget a) where fmap k (Widget s f) = Widget s (fmap (fmap k) f) In other words, a `Widget` has some initial state of type `s`, and the widget transforms an input signal of type `a` to 0 or more output signals of type `b`, modifying its internal state of type `s` along the way. You can compose widgets end-to-end using their `Category` instance, or you can run them side-by-side using their `Monoid` instance. When you connect widgets in this way, they isolate their states from each other automatically, without any assistance from the programmer.
&gt; is there a Haskell equivalent of Python's virtualenv &gt; Note that you'll need Cabal 1.18‚ÄîHaskell Platform provides 1.16.
Then maybe I need to release a `pipes-parsec` layer to upgrade `parsec` parsers to `pipes` parsers`.
&gt; What exactly is the full error? The bit i quoted was the end of several hundred lines of output by cabal. The previous bit was: Registering yesod-1.2.5.2... Installing library in /home/phil/.cabal/lib/yesod-1.2.5.2/ghc-7.4.1 Registering yesod-1.2.5.2... Downloading yesod-auth-hashdb-1.3.0.1... Configuring yesod-auth-hashdb-1.3.0.1... Building yesod-auth-hashdb-1.3.0.1... Preprocessing library yesod-auth-hashdb-1.3.0.1... [1 of 1] Compiling Yesod.Auth.HashDB ( Yesod/Auth/HashDB.hs, dist/build/Yesod/Auth/HashDB.o ) Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. Loading package array-0.4.0.0 ... linking ... done. Loading package bytestring-0.9.2.1 ... linking ... done. Loading package deepseq-1.3.0.0 ... linking ... done. Loading package containers-0.4.2.1 ... linking ... done. Loading package text-1.1.1.3 ... linking ... done. Loading package hashable-1.2.2.0 ... linking ... done. Loading package scientific-0.3.2.1 ... linking ... done. Loading package attoparsec-0.11.3.4 ... linking ... done. Loading package blaze-builder-0.3.3.2 ... linking ... done. Loading package dlist-0.7.0.1 ... linking ... done. Loading package transformers-0.2.2.0 ... linking ... done. Loading package mtl-2.0.1.0 ... linking ... done. Loading package old-locale-1.0.0.4 ... linking ... done. Loading package syb-0.3.6 ... linking ... done. Loading package pretty-1.1.1.0 ... linking ... done. Loading package template-haskell ... linking ... done. Loading package time-1.4 ... linking ... done. Loading package unordered-containers-0.2.4.0 ... linking ... done. Loading package primitive-0.5.3.0 ... linking ... done. Loading package vector-0.10.9.1 ... linking ... done. Loading package aeson-0.7.0.6 ... linking ... done. Loading package case-insensitive-1.2.0.0 ... linking ... done. Loading package exceptions-0.6.1 ... linking ... done. Loading package transformers-base-0.4.2 ... linking ... done. Loading package monad-control-0.3.3.0 ... linking ... done. Loading package lifted-base-0.2.2.2 ... linking ... done. Loading package mmorph-1.0.3 ... linking ... done. Loading package resourcet-1.1.2.2 ... linking ... done. Loading package nats-0.2 ... linking ... done. Loading package semigroups-0.14 ... linking ... done. Loading package void-0.6.1 ... linking ... done. Loading package conduit-1.1.2.1 ... linking ... done. Loading package base64-bytestring-1.0.0.1 ... linking ... done. Loading package data-default-class-0.0.1 ... linking ... done. Loading package data-default-instances-base-0.0.1 ... linking ... done. Loading package data-default-instances-containers-0.0.1 ... linking ... done. Loading package data-default-instances-dlist-0.0.1 ... linking ... done. Loading package data-default-instances-old-locale-0.0.1 ... linking ... done. Loading package data-default-0.5.3 ... linking ... done. Loading package cookie-0.4.1.1 ... linking ... done. Loading package filepath-1.3.0.0 ... linking ... done. Loading package http-types-0.8.4 ... linking ... done. Loading package mime-types-0.1.0.4 ... linking ... done. Loading package parsec-3.1.5 ... linking ... done. Loading package unix-2.5.1.0 ... linking ... done. Loading package network-2.5.0.0 ... linking ... done. Loading package cereal-0.4.0.1 ... linking ... done. Loading package utf8-string-0.3.8 ... linking ... done. Loading package publicsuffixlist-0.1 ... linking ... done. Loading package random-1.0.1.1 ... linking ... done. Loading package old-time-1.1.0.0 ... linking ... done. Loading package directory-1.1.0.2 ... linking ... done. Loading package zlib-0.5.3.3 ... linking ... done. Loading package streaming-commons-0.1.2.4 ... linking ... done. Loading package http-client-0.3.2.2 ... linking ... done. Loading package byteable-0.1.1 ... linking ... done. Loading package securemem-0.1.3 ... linking ... done. Loading package crypto-cipher-types-0.0.9 ... linking ... done. Loading package cipher-aes-0.2.7 ... linking ... done. Loading package crypto-random-0.0.7 ... linking ... done. Loading package cprng-aes-0.5.2 ... linking ... done. Loading package socks-0.5.4 ... linking ... done. Loading package asn1-types-0.2.3 ... linking ... done. Loading package asn1-encoding-0.8.1.3 ... linking ... done. Loading package cipher-rc4-0.1.4 ... linking ... done. Loading package crypto-numbers-0.2.3 ... linking ... done. Loading package crypto-pubkey-types-0.4.2.2 ... linking ... done. Loading package cryptohash-0.11.4 ... linking ... done. Loading package crypto-pubkey-0.2.4 ... linking ... done. Loading package asn1-parse-0.8.1 ... linking ... done. Loading package pem-0.2.2 ... linking ... done. Loading package process-1.1.0.1 ... linking ... done. Loading package x509-1.4.11 ... linking ... done. Loading package x509-store-1.4.4 ... linking ... done. Loading package x509-validation-1.5.0 ... linking ... done. Loading package tls-1.2.7 ... linking ... done. Loading package x509-system-1.4.5 ... linking ... done. Loading package connection-0.2.1 ... linking ... done. Loading package http-client-tls-0.2.1.1 ... linking ... done. Loading package http-conduit-2.1.2 ... linking ... done. Loading package attoparsec-conduit-1.1.0 ... linking ... done. Loading package blaze-builder-conduit-1.1.0 ... linking ... done. Loading package conduit-extra-1.1.0.3 ... linking ... done. Loading package blaze-markup-0.6.1.0 ... linking ... done. Loading package blaze-html-0.7.0.2 ... linking ... done. Loading package system-filepath-0.4.11 ... linking ... done. Loading package xml-types-0.3.4 ... linking ... done. Loading package xml-conduit-1.2.0.1 ... linking ... done. Loading package tagstream-conduit-0.5.5.1 ... linking ... done. Loading package authenticate-1.3.2.8 ... linking ... done. Loading package base16-bytestring-0.1.1.6 ... linking ... done. Loading package binary-0.5.1.0 ... linking ... done. Loading package email-validate-2.0.1 ... linking ... done. Loading package file-embed-0.0.6 ... linking ... done. Loading package system-fileio-0.3.13 ... linking ... done. Loading package shakespeare-2.0.0.3 ... linking ... done. Loading package hamlet-1.2.0 ... linking ... done. Loading package mime-mail-0.4.5.2 ... linking ... done. Loading package fast-logger-2.1.5 ... linking ... done. Loading package monad-loops-0.4.2 ... linking ... done. Loading package stm-2.4.3 ... linking ... done. Loading package stm-chans-3.0.0.2 ... linking ... done. Loading package monad-logger-0.3.6.1 ... linking ... done. Loading package path-pieces-0.1.3.1 ... linking ... done. Loading package resource-pool-0.2.2.0 ... linking ... done. Loading package silently-1.2.4.1 ... linking ... done. Loading package persistent-1.3.1.1 ... linking ... done. Loading package persistent-template-1.3.1.3 ... linking ... done. Loading package safe-0.3.4 ... linking ... done. Loading package shakespeare-css-1.1.0 ... linking ... done. Loading package shakespeare-js-1.3.0 ... linking ... done. Loading package vault-0.3.0.3 ... linking ... done. Loading package wai-2.1.0.2 ... linking ... done. Loading package entropy-0.2.2.4 ... linking ... done. Loading package tagged-0.7.2 ... linking ... done. Loading package crypto-api-0.13 ... linking ... done. Loading package skein-1.0.9 ... linking ... done. Loading package clientsession-0.9.0.3 ... linking ... done. Loading package shakespeare-i18n-1.1.0 ... linking ... done. Loading package unix-compat-0.4.1.1 ... linking ... done. Loading package ansi-terminal-0.6.1.1 ... linking ... done. Loading package stringsearch-0.3.6.5 ... linking ... done. Loading package byteorder-1.0.4 ... linking ... done. Loading package unix-time-0.2.2 ... linking ... done. Loading package wai-logger-2.1.1 ... linking ... done. Loading package word8-0.0.4 ... linking ... done. Loading package zlib-conduit-1.1.0 ... linking ... done. Loading package wai-extra-2.1.1.1 ... linking ... done. Loading package http-date-0.0.4 ... linking ... done. Loading package network-conduit-1.1.0 ... linking ... done. Loading package simple-sendfile-0.2.14 ... linking ... done. Loading package warp-2.1.5.1 ... linking ... done. Loading package yesod-routes-1.2.0.6 ... linking ... done. Loading package yesod-core-1.2.15.1 ... linking ... done. Loading package css-text-0.1.2.1 ... linking ... done. Loading package tagsoup-0.13.1 ... linking ... done. Loading package xss-sanitize-0.3.5.2 ... linking ... done. Loading package yesod-persistent-1.2.2.3 ... linking ... done. Loading package yesod-form-1.3.9 ... linking ... done. Loading package yesod-auth-1.3.0.5 ... linking ... done. Loading package SHA-1.6.4 ... linking ... done. Loading package pwstore-fast-2.4.1 ... linking ... done. Yesod/Auth/HashDB.hs:182:3: Unrecognised pragma Registering yesod-auth-hashdb-1.3.0.1... Installing library in /home/phil/.cabal/lib/yesod-auth-hashdb-1.3.0.1/ghc-7.4.1 Registering yesod-auth-hashdb-1.3.0.1... Downloading yesod-test-1.2.1.5... Configuring yesod-test-1.2.1.5... Building yesod-test-1.2.1.5... Preprocessing library yesod-test-1.2.1.5... [1 of 3] Compiling Yesod.Test.CssQuery ( Yesod/Test/CssQuery.hs, dist/build/Yesod/Test/CssQuery.o ) [2 of 3] Compiling Yesod.Test.TransversingCSS ( Yesod/Test/TransversingCSS.hs, dist/build/Yesod/Test/TransversingCSS.o ) [3 of 3] Compiling Yesod.Test ( Yesod/Test.hs, dist/build/Yesod/Test.o ) Yesod/Test.hs:113:1: Warning: In the use of `runFakeHandler' (imported from Yesod.Core, but defined in yesod-core-1.2.15.1:Yesod.Core.Internal.Run): "Usually you should *not* use runFakeHandler unless you really understand how it works and why you need it." Registering yesod-test-1.2.1.5... Installing library in /home/phil/.cabal/lib/yesod-test-1.2.1.5/ghc-7.4.1 Registering yesod-test-1.2.1.5... 
Yeah I wasn't expecting it to work. I just wanted to see this specific error because that's where the problem is.
In this case it's failure to have an up-to-date `alex`, so I don't know what circle of hell that get classified under ...
Well, considering I'm the only person working on numerical-core currently, thanks! :-) I'm currently amidst release engineering and helping some folks get ready to use my pending alpha release :-) 
http://www.quora.com/Category-Theory/What-is-the-best-textbook-for-Category-theory?share=1 has the usual list of category theory books I send folks to when they ask. For me it was mostly bouncing off CftWM over and over until I made it through, then going back and fixing my misunderstandings multiple times. I'm given to understand there are more efficient means. ;)
It makes me happy that I was able to help you with my pronunciation list.
Ah! Small world. I saw you at the NY haskell meetup.
This is how I use category theory: http://bit.ly/T1zBmc
While abusing the `(-&gt;)` monad often makes the code less clear, I think learning how to rewrite functions to pointfree style has other benefits. Some combinator libraries, such as [partial-isomorphisms](http://hackage.haskell.org/package/partial-isomorphisms), are inherently pointfree. Thus, if you already know how to read and write ordinary functions in pointfree style, you'll have no problem reading and writing partial isomorphisms in that style. Otherwise, you might have a hard time visualizing what isomorphism-based programs do.
i had the same problem on osx after i've installed ghc with homebrew, you have to delete the old alex from /usr/local/bin (or move ~/.cabal/bin before /usr/local/bin in $PATH)
Done that, it's now working thanks.
Could not recommend the Write Yourself a Scheme in 48 Hour more than enough, in fact it was probably what made programming itself click for me. That said, writing a simple lisp interpreter is pretty trivial, and I'm currently looking for something more advanced to sharper my knowledge, but I have basically no idea how compilers work internally (how do I produce code in C from a much more expressive language?, what is CPS and why does it matter?, how do i implement TCO and various other optimizations?), is there any good tutorial or book about this subject that don't require too much knowledge in advanced? 
It should be noted that point-free style has potential performance drawbacks too, namely that [GHC cannot inline functions defined in point-free style](http://www.haskell.org/ghc/docs/7.6.3/html/users_guide/pragmas.html).
Great! You're welcome.
I don't like "pipe to" for `(.)` as it would be read backwards compared to other operators. I usually just call it "dot" like you would when writing out function composition by hand; but if you must have a descriptive name I'd propose "of" which is consistent with how people tend to read functions out loud (`g(x)` is "g of x", `h(g(x))` is "h of g of x").
I recommend Andrew Appel's books 1. [Compiling with Continuations](http://www.amazon.com/Compiling-Continuations-Andrew-W-Appel/dp/052103311X) A small little book that glosses over most of the frontend of the compiler but talks at length about CPS (a flavor of intermediate languages), Closure Conversion, and code generation. The language it compiles is SML and most of the code is in SML which is pretty readable if you know Haskell. 2. [Modern Compiler Implementation (In ML)](http://www.amazon.com/Modern-Compiler-Implementation-Andrew-Appel/dp/0521607647/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1401556616&amp;sr=1-1&amp;keywords=modern+compiler+implementation+in+ml) A much more detailed overview of compiler design, covering a wide range of subjects/optimizations. Details SSA instead of CPS but since LLVM IL is SSA it's definitely worth learning. I found these books to be a lot more readable than the dragon book and friends and SML is such a joy to write a compiler in :) Honestly, once you have a basic idea of how these things work, it's a good idea to just kinda go for it. It's a little bit frustrating but I learned a *ton* from writing my first extremely broken compiler for simply typed lambda calculus + primops to x86.
when? (cool!) feel free to PM me if you like 
Of course if we had a nice, clean, ground-up treatment of first-class data types ala Epigram 2, we could have all the benefits of building types out of products/sums AND of normal ADTs! Oh Epigram, how we need you!
&gt; `palindrate = (++) id reverse` I guess you didn't try this out.
I've never heard people call it "pipe to". I have heard "after" tho -- `f.g` as `f` after `g`. Better, I feel, than "of", because then it's not clear whether you mean `f.g` or `f g`.
I've been unhappy with that "ridicule" word in my post above. I don't believe the complete question of the OP is ridicule, in fact it is an interesting question. Still, I think that calling `fmap` an "implicit use of category theory" is overblown. Category theory is certainly demonstrably useful in some situations (eg. mostly everything ekmett does), but invoking in excess can be a bit, well, ridiculous. Moli√®re obviously uses "Philosophy master" as a derogatory term in this piece (playing on the stereotype that philosophy is a bunch of uselessly sophisticated contemplations about nothing), some people react to "functional programming" and "category theory" in the same way, and we don't want that to happen too much. Again, not blaming OP: there is a point in the learning curve of any concept/tool/idea where you are on the side of over-using it (you certainly were under-using it before), and that's perfectly natural.
Something which is very simple to parse and not too limiting on the content of a .cabal file. That *probably* means json or possibly s-expressions. YAML is (despite its early intentions) actually quite complex to parse correctly because there's a huge amount of ad-hoc rules AFAICT. In fact I even find it too hard to reason about what a parser will do when writing YAML by hand. I interpret this as a bad sign wrt. complexity. Even XML would be better, because at least the rules about XML syntax are easy to understand as long as you ignore namespaces :).
After is good, I still prefer just "dot".
Slightly off topic: I'm not seeing a practical difference between your `Widget` type and this: newtype Widget a b = Widget (a -&gt; [b]) Is there some reason for pulling the state out like you are?
"Monoidal append". Because the monoidal operation for lists is append, and lists are the most intuitively obvious monoid. Use &lt;&gt; for brevity.
List are the mother of all monoids in the sense that lists are free monoids and all other monoids are lists modulo an equivalence relation. http://en.wikipedia.org/wiki/Free_monoid
A very astute observation: once the compiler accepts your code it has a good chance of working, and conversely if you get too frustrated by the compiler rejecting your code, you're probably too drunk to code anything! :)
heh my phone client took out the operators when I edited it. I guess it thought it was html
The `ListT` function is run multiple times, once for each incoming event of type `a`. I need to preserve the state in between invocations.
Thanks! I can't say that's the best reason for picking a name, but it's at least logically consistent.
&gt; Scheme is a statically typed language with only one type for everything. Even if that's a cute mantra to throw around to get people to think, I doubt that it's useful in a context where OP seems confused about what a statically-typed language is.
Thanks! "A small little book" sounds very promising since my concentration span is sadly a bit low. Also I wanna give a shout out for your blog, not just useful but also inspiring/motivating for another teen playing with functional programming.
But can't that just be stored in the closure?
How?
Yes, it is a big final question. I have already done it with recursive function, but the teacher wants it using list comprehension and/or . (Function composition operator). So I need it. If you can help me I will be so happy.
I don't know why it's called `mappend`, but it's easy to see why it's not called `mop`: a mop is an implement consisting of a sponge or a bundle of thick loose strings attached to a handle, used for wiping floors or other surfaces.
typed racket is a typed scheme http://docs.racket-lang.org/ts-guide/index.html?q=typed 
 {-# LANGUAGE ExistentialQuantification #-} import Control.Monad.Trans.State.Strict import Pipes -- I couldn't find a convenient way to just "run" ListT using the -- Pipes API, but I am pretty confident that I'm not breaking any laws -- or doing anything crazy here. Maybe you know of a better way. import Pipes.Internal data Widget a b = forall s. Widget { seed :: s , runWidget :: a -&gt; ListT (State s) b } newtype MyWidget a b = MyWidget (a -&gt; [b]) widgetToMyWidget :: Widget a b -&gt; MyWidget a b widgetToMyWidget (Widget s0 f) = MyWidget $ go s0 . observe . enumerate . f where go _ (Pure ()) = [] go _ (Request x _) = closed x go s (Respond b k) = b : go s (k ()) go s (M m) = let (p, s') = m `runState` s in go s' p myWidgetToWidget :: MyWidget a b -&gt; Widget a b myWidgetToWidget (MyWidget f) = Widget () $ Select . mapM_ yield . f I do think there is one way that yours is more powerful, although I doubt you meant for it to be so. Yours allows you to reuse an earlier state with a *later* state transition function in the `ListT`, whereas mine basically preapplies the transition functions to their corresponding states.
One *could* create a language identical to Scheme with a static type system and type inference, but that doesn't make it statically typed. You could create a language identical to Haskell but without a static type system; does that make Haskell dynamically typed? A bit less facetiously, you have actually hit upon one important issue: if you want to create a static type system for Scheme that corresponds exactly to the existing valid Scheme programs that do not crash at runtime, you are probably going to have a rather difficult time of it. As another answer mentions, Typed Racket is one attempt to put a static type system on Racket; even without knowing much of anything about Typed Racket I guarantee that it rules out some programs which would otherwise be valid Racket programs that do not crash.
out of curiosity, what does `f g.` do? My current understanding of `f . g`, is that it takes the result of `g` then passes that to `f`. 
[-**fun**box-strict-fields](http://www.haskell.org/ghc/docs/latest/html/users_guide/options-optimise.html) (party time?) It's [app**E**ndo](http://hackage.haskell.org/package/base-4.7.0.0/docs/Data-Monoid.html#v:appEndo), not append**O**. ba dum, tss
&gt; I'm given to understand there are more efficient means. Yeah, there are... like reading Edward Kmett libraries. Sadly, that method wasn't available to you. :-|
You could define a new language on top Scheme with a ML/Haskell style type system but that will fundamentally discard a whole class of programs that are expressible in untyped Scheme. For example (lambda (x) (x x)) is valid Scheme but won't typecheck in any HM style system. So yes you can, but by most definitions you no longer have the language called Scheme anymore it's some distinct typed lambda calculus derivative language.
Getting an idea about the performance vs the hard-coded parser-optimizations in wai would be interesting.
Thanks :) It's always nice to find another teenager! Good luck!
Yeah, thanks for the discussion! :)
How about this? primes = [p | p &lt;- [2..], [d | d &lt;- [1..p], p `mod` d == 0] == [1,p]] factors n = [p | p &lt;- takeWhile (&lt;n) primes, _ &lt;- tail $ takeWhile ((==0).snd) $ iterate (flip divMod p . fst) (n, 0)] The method is almost the same. Sill using a list of primes limited to primes less than `n`. Then I'm using a trick. There's second generator that produces a list of tuples that we throw away, but consequently use it's length to replicate `p` the right amount of times. 
The value of static types is that the type checker statically rejects many bad programs, that is, programs which would fail at runtime. It does this at the cost of rejecting some good programs. Dynamically-typed languages choose never to reject any good program, at the cost of also accepting all bad programs. Since your variant of Scheme computes a valid sum type (actually a union type, since the `Left` and `Right` constructors are ommited) for any Scheme program, your type checker will never reject any program. Therefore, the essence of your Scheme variant is still dynamically-typed, even if you can compute the types at compile time.
Well, it depends on what `f` is! But in generally it's just a higher order thing. For instance, if `f = id` then `f g = id g = g`, or if `f = map` then `f g = map g = ...`. It's not supposed to be something special.
I don't think I've ever seen the tilde lazy pattern match operator before. In case anyone else is wondering, this may save some googling: http://www.haskell.org/haskellwiki/Lazy_pattern_match
**m**onoid **op**eration would be a perfectly sensible name for it if we didn't have `&lt;&gt;`.
GHC implements something more powerfull than f-omega. The apply function can only be typed with dependent types...or something close enough data HList ls where Nil :: HList '[] Cons :: x -&gt; HList xs -&gt; HList (x ': xs) type family Apply (f :: *) (xs :: [*]) :: * where Apply x '[] = x Apply (a -&gt; b) (a ': xs) = Apply b xs apply :: t -&gt; HList xs -&gt; Apply t xs apply x Nil = x apply f (Cons x xs) = apply (f x) xs that is not tested or compiled...there are also numerous other ways of acomplishing the same thing in GHC that make different tradeoffs.
It is statically typed out-of-the-box. There is a single type, which we could call Scheme Expression.
I don't think that is what that's saying. In fact, it looks to me like it's saying that a function will be inlined if it is applied to *at least* as many arguments as it has on its LHS in its definition. Also, if that *is* true, you can use functions sometimes without applying them to arguments so they could still be inlined in those cases (which would still be handy for fusion).
I refer to it mostly to justify saying "left fish" and "right fish".
Congratulations on your awesome speed bump!
A\* and A+ ... is this the origin of the regular expression \* and + ?
When looking for the first attoparsec announce I found out that the older posts are not displayed in Brian's blog.
Of course HPC in cloud haskell is painfully slow! It was never designed for that, and erlang, which it is modeled on has never handled distribution for that type of workload either! For managing large chunky workloads on a cluster it should be just fine. But for any sort of more granular parallelism you need something purpose built for that task.
&gt;I think everybody here is reading too much into the algebraic data types part of your question. Yeah. :( I'm surprised more people didn't mention monads.
There is a [Real World Haskell chapter](http://book.realworldhaskell.org/read/interfacing-with-c-the-ffi.html) on the subject.
I like "smush" as a generic description of a monoidal operation, in terms of giving an intuition. Not sure if I could motivate that everyone use it though :-)
Wow. Thanks for link. Here's github repo of this new plugin: [haskell-idea-plugin](https://github.com/Atsky/haskell-idea-plugin) It's based on older [ideah](https://code.google.com/p/ideah/) IntelliJ IDEA plugin for Haskell, based on ideah. For now provides basic features: Haskell syntax highlight Error checking with buidwrapper to have a lot of cool features for free. Simple completion based on ghc-mod Cabal syntax highlight Build of cabal projects Installation of cabal packages 
Check out /r/cellular_automata, you may find it interesting!
Using `fmap` is (in most circumstances) just using the library support for applying transformations. It can be used as part of a development whose intent is to be categorical, but it often isn't. The reasoning technique used in Heinglein and Hinze's 2013 paper [ Sorting and searching by distribution: From generic discrimination to generic tries](http://www.cs.ox.ac.uk/ralf.hinze/publications/APLAS13.pdf) is categorical (it's explained as a naturality property, interestingly, for a non-endomorphic functor).
This looks like a cool project. Although it isn't especially clear (at least to me) how one might use this, as someone not very well versed in osc.
My understanding is that those regex quantifiers come from [Kleene star and Kleene plus](https://en.wikipedia.org/wiki/Kleene_star); not sure if those in turn came from algebraic notations, or if it was the other way around.
The point here is that you need to highlight two ranges, namely the two that gave rise to the two conflicting types. These two are the interesting locations for a type error. They are also the two location that ghc does not give you, it instead gives you the (much less useful) location where the compiler happened to notice the error.
yes, but only because the "of"s are right nested. "f of (g of (h of x))" but with higher order functions, it's ambiguous. "f of g" could mean `f.g` or `f g`
well said (and better articulated)
no, because if you have simple application you can just omit the "of". you may be onto something but i think you'd need a longer or more concrete example to demonstrate it. when things require parens, one should pronounce the parens anyway. i tend to do so by saying `(reverse y)` as "quantity reverse y". in some cases you might need to say the parens explicitly to make things clear, but typically only providing the open paren is sufficiently unambiguous.
Well now you're inventing a whole new convention, which is missing the point entirely. The *existing* convention is that `f x` is pronounced "f of x", which leads to problems with higher order functions. That's just a fact.
I try to do both. ;)
Probably naming the individual terms/syntax
That's very cool, nice work! Though... why is there a `flip id` in there?
I was thinking more along the lines of infixed type operators, but sure :3
i moved the comment to reply to tomejaguar directly so he's pinged
I believe Kleene algebras were invented to make a class of structures for which regular languages were the free algebra. I'm not sure when regular expressions were introduced in this.
All programmers use category theory to some non-zero extent. The extent to which a programmer is aware of it is proportional to the practical consequences that may be exploited.
That threw me as a Haskell noob off as well. Would love an explanation. :)
I'm excited (my install of SublimeHaskell has broken again and I've forgotten the arcane magic required to make it work) 
What does the equivalence relation refer to?
natural numbers and addition*
It concerns a bit more state, which is what a lot of people struggle with learning functional programming. edit: sure, ```fib``` being optimized concerns memoization, but still not as serious as GoL.
Unless I have missed something, it is valid.
I don't think I understand: can you perhaps give the F-omega typing of `apply` you think exists? F-omega does not have dependency or recursive types, so I just don't see one. GHC is a strictly more expressive language than F-omega. You can embed F-omega into GHC haskell as a GADT or via finally tagless, and it isn't even so terrible in our modern world with polykinds datakinds and closed families. Type level computation is not *total* in GHC which is the reason why the power of F-omega is not even close to comparable to haskell. You might want your language to be total, but GHC Haskell isn't even at the type level. GHC also has type equalities, promotion, and polykinds. You can't parameterize an object of kind `(* -&gt; *) -&gt; *` with a lambda. That is true. But this is because haskell is *more* expressive than F-omega not less. Haskell draws a distinction between the constructor kind (which is written with `-&gt;` even though that is also used in haskell for the function type constructor where it has a very different meaning) and the function kind which has to be emulated using something like type families. Oleg showed us the general strategy long ago type family Apply (f :: *) {- ranges over *codes* -} (t :: k) :: k' The idea is that we can simply encode the untyped lambda calculus into the type system, so encoding the computational aspects of the simply typed lambda calculus is trivial. It is an encoding, and that has to show up in the types....but distinctions like this mean our types are more percise. It isn't, apriori, a bad thing. This idea of a distinction is not unique: it also shows up in papers like [this one](http://www.cs.cmu.edu/~drl/pubs/lh09unibind/lh09unibind.pdf).
&gt; For example (lambda (x) (x x)) is valid Scheme but won't typecheck in any HM style system That is not true. It is actually pretty trivial to built a variant of HM with equirecursive types. So much so that Morris didn't publish the paper where showed how to do it, but it is referenced with a bit of explanation by the classic paper "An ideal model for recursive polymorphic types" by MacQueen, Plotkin, and Sethi. Equi-recursive types mean no SN property, but Milner's original language had recursion at the term level, so that can hardly be the defining feature of HM style systems/
Methinks the names only read as well backwards as forwards in Irish.
It's not bad at all with RebindableSyntax (I use indexed monads in a lot of my code) - the `indexed-extras` package even has an indexed continuation monad ready to use. 
Do these quasiquoters also typecheck the shades? And if so to what extent? 
Here's where `:t` in GHCi comes in handy :) &gt; :t id id :: a -&gt; a &gt; :t flip flip :: (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c At first view, this doesn't seem like it should work. We're passing a one-argument function to one that expects a two-argument function. What? The trick here is to notice that there are no restrictions on `a` in `id`'s type signature. So `a` can be any type it needs to be to make the types work out, including a function! Let's try a simple function, from one type to another: `b -&gt; c`. So now: id :: (b -&gt; c) -&gt; (b -&gt; c) This still looks like a one-argument function. But, the `-&gt;` operator is right-associative! So, we get id :: (b -&gt; c) -&gt; b -&gt; c Now we've got `id` as a two-argument function! So, let's plug that into `flip` and see if it works out. Let's have another look at `flip`'s type signature. flip :: (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c We're passing `id` has the first argument, which has type `(b -&gt; c) -&gt; b -&gt; c`. So `a -&gt; b -&gt; c` &lt;=&gt; `(b -&gt; c) -&gt; b -&gt; c`. So in this case, `a` &lt;=&gt; `(b -&gt; c)`. What will `flip id` look like now? flip id :: b -&gt; (b -&gt; c) -&gt; c And if we type this is GHCi, this is indeed the correct type. If we look at this for a minute, we can figure out what it does. It takes an item of type `b`, a function from `b` to `c`, and returns a `c`. There's only one thing it can do - apply the function to the item we provided and return the result. This does the exact same thing as `flip ($)`, which is much clearer; why the author did not use this, I don't know. I'd regard `flip id` as type system trickery that doesn't belong in real code when a much more obvious, descriptive alternative exists. 
If that were the reason, `nub` wouldn't have this name either.
See [my comment](http://www.reddit.com/r/haskell/comments/26zxav/i_made_an_animated_asciiart_music_visualization/chwccrj) to /u/ch0wn :)
Based on my reading of "real" category theory papers: Saying "Haskell code uses category theory because is uses Functors and Monads" is like saying "School children use calculus because they uses addition and multiplication."
I never said each Haskell program used a lot of advanced category theory :P
Yup - it seems in it's early stages atm. Things which work * Building a cabal project, getting useful error back that you can click on to go to appropriate section of code * Syntax highlighting (kinda) * Jump to definition for cabal-project-local definitions Stuff that didn't work: * **EDIT:** *OS X/I suck at `PATH`s. You do get some completions.* Completions of any kind (although they 'temporarily disabled' atm judging by [this commit](https://github.com/Atsky/haskell-idea-plugin/commit/d1eef331140bf7e56344250e4c8e8e46a8558f6f). This is the killer feature that I want - I never got it to work in SublimeHaskell). * Syntax highlighting is ugly and limited; I can't see any way to change the colours in the options and most stuff is the default colour. If you use `DataKinds` and promote variants by prefixing a quote (like `'Variant`) it thinks everything until the next `'` is a string. * Show type of symbol didn't work, although I am using Mac and it says Ctrl+I and I have changed my keybindings a lot. Stuff I didn't try: * Installation of cabal packages It does look in highly active development though - I may try and help out as I really, really want a decent Haskell IDE plugin. Edit: I was using version 0.2.1 Beta, for future reference.
That's odd, I always thought nub *was* named after the word "nub", one of whose definitions is "the crux or central point of a matter". (I haven't heard anyone explicitly say that's the reason for the name in Haskell, but that operation is also called nub in array languages and I have heard that explanation there.) Why do you think nub is called "nub"?
You missed something: Type synonym `ListF` should have 2 arguments, but has been given 1 In the type declaration for `List'
What's a pain is mixing regular monads with indexed monads.
Awesome! I really enjoyed seeing the transition from low-level C-style code to more idiomatic Haskell.
Ah, of course.
I was wondering how you were doing that in ghci, then I remembered adding some stuff to my .ghci for custom commands. For others who are curious you can replicate it by: cabal-install pointfree echo ":def! pl return . (\":! pointfree \" ++) .ghciEscapeShellArg" &gt;&gt; ~/.gchi Also be sure that this line is in your .ghci file somewhere: let ghciEscapeShellArg arg = "'" ++ concatMap (\c -&gt; if c == '\'' then "'\"'\"'" else [c]) arg ++ "'" 
Sure. Lists are more general. In the sense that list-of is a type constructor that for every set gives the free monoid generated from that set. So of course "lists are the free monoid" was sloppy. Lists are not even a type. Lists-of-something is a type. And [()] is a special case of this, while the more general case is list-of-a always being a monoid.
I kind of like that. [It makes me think `Foldable.fold` works like this.](http://cache4.asset-cache.net/gc/83213505-feet-crushing-wine-grapes-niagara-on-the-lake-gettyimages.jpg?v=1&amp;c=IWSAsset&amp;k=2&amp;d=OirifnDwt3XUfefOIkEphTMZBHglkwvbTP2Y48ONrAo%3D)
Wrap it in newtypes or whatever. :P
BOS, what is your opinion on the approach taken by uu-parsinglib? It uses breadth first exploration instead of depth first backtracking. This means that the parser can be fully incremental and doesn't need to keep the entire input around. What is the reason that attoparsec uses depth first backtracking?
We know that concatenation is a monoid over lists. Now, let's partition all the lists into a number of sets, for example the set of all odd-length lists and the set of all even-length lists. We want to define a monoid operation which acts not on lists, but on those sets of lists, each of which forms an equivalence class. With the odd- and even-length equivalence classes, we are in luck, because concatenation is easily adapted to work on classes instead of individual lists: simply pick one list from each class, concatenate them, and return the class to which the resulting list belongs. This operation is well-defined because concatenating two odd-length or two even-length lists always yields an even-length result, and concatenating an even-length list with an odd-length list always yields an odd-length result. If we think of "odd-length" and "even-length" as `True` and `False`, we discover that our adapted concatenation operation corresponds to a `xor` operation! Furthermore, xor is a monoid over booleans: it is associative, and it has an identity element, `False`. Remarkably, any monoid can be expressed in this way, as equivalence classes of lists to which concatenation adapts well. The general scheme, for a monoid `(&lt;&gt;)` over a type `Foo`, is to create one equivalence class `E_x` for each value `x` of type `Foo`: the set of all lists `[x0, x1, x2, ..., xN]` such that `x0 &lt;&gt; x1 &lt;&gt; x2 &lt;&gt; ... &lt;&gt; xN` results in `x`. You can verify that concatenation adapts well to these classes, that `E_x &lt;&gt; E_y = E_(x &lt;&gt; y)`, and that `E_mempty` is an identity element. *edit*: note that applying the general pattern to `xor` yields the sets "lists with an odd/even number of `True` elements", not "lists with an odd/even length".
... or just use [ghci on acid](https://github.com/chrisdone/goa).
Very cool idea. Is the code on github?
This is such a wonderful example of why I love the Haskell community so much. Thanks a lot for the answer! :)
Could you answer a few questions: * Have you installed ghc-mod? (Plugin uses it for completion.) * What kind of features you need in syntax highlighting? * On mac "show type of symbol" is CMD + I. Is it work? * Is error highlighting during editing works? 
Oops, link is accidentally doubled. This one works: http://sencjw.com/posts/2014-05-19-bayhac-2014.html
Got a list for math books in general?
Why don't you resubmit before it's too late?
I'll do my best to put something together.
'cuz I'm not the OP? :)
Yeah, oops. Sorry about that. I could have sworn it was okay in the submit box. Thanks for the fix (I can't edit the submission). I'm going to re-sumbit.
I did have `ghc-mod` installed - and I found my [ghc-mod completions problem](http://i.imgur.com/r2I4Ogw.png). The problem is on OS X you don't get the same path as your terminal if you start your program via finder. Running IDEA from the terminal actually gives me ghc-mod completions. However, the completions I get don't include possible functions imported from packages installed in my cabal sandbox - I'm not sure whether this is a `ghc-mod` or plugin problem, though. It would also be great to have the types on the right next to the function names (even with haddock doc as well) - this is literally the feature I want more than anything else in a Haskell IDE (completions with type information). Regarding syntax highlighting, I don't know, looks what sublime text does: comparison: [idea-plugin-haskell](http://i.imgur.com/Meg5kVv.png) and [SublimeHaskell](http://i.imgur.com/lKFSFIT.png). Some highlighting of function name declarations and type constructors would be very useful. Also, it [doesn't seem to like promoted data kinds](http://i.imgur.com/11vZyuV.png) but very few people actually write code that needs this :p. Are you lexing the haskell using your own lexer? I would possibly recommend against this as haskell is really complicated and you'll never get it 100% right. I tried both cmd+I and ctrl+I with no luck (nothing visibly happened). Is there any way to access any logging or to check this is actually attached up correct? Error highlighting whilst editing doesn't seem to work for me - the box in the top right changes to yellow when I type but is always green after a 0.5s or so even if the code is wrong. I don't get any error markings anywhere on the code text. Thanks for developing this - with a bit of work it should be fantastic!
Were these BayHac talks videoed at all? I would be really interested in watching them.
The input types at the interface have to match: the elm compiler checks that the types of the record attributes sent to the shaders match the GLSL inputs. The GLSL code itself is parsed by the Haskell language-glsl package so if there's any parse errors, you're given both the line number within that GLSL quote, and the line number in the containing .elm file. Beyond that I don't think there's any type checking of the GLSL. The elm-webgl API also allows you to generate GLSL programmatically as a String rather than giving it as verbatim quasiquotes, to allow some future library to make GLSL combinators. The current way is not perfect, in the end you still have a bunch of GLSL which is sent to a browser, and any errors come up in the developer console with some irrelevant error message generated by the graphics driver.
`palindrate = (&lt;&gt;) id reverse` actually does work, though (using the Monoid instance for functions).
Some were: http://www.haskell.org/haskellwiki/BayHac2014
&gt; Remarkably, any monoid can be expressed in this way, as equivalence classes of lists to which concatenation adapts well. The general scheme, for a monoid (&lt;&gt;) over a type Foo, is to create one equivalence class E_x for each value x of type Foo: the set of all lists [x0, x1, x2, ...] such that x0 &lt;&gt; x1 &lt;&gt; x2 &lt;&gt; ... results in x. You can verify that concatenation adapts well to these classes, that E_x &lt;&gt; E_y = E_(x &lt;&gt; y), and that E_mempty is an identity element. That is pretty awesome. However, I would put an x_n to the end of the list, because it makes it look like an infinite list, which is definitely not the case.
I have no idea how to even configure the haskell SDK in IDEA (or should I just create a haskell module without an SDK?). I installed haskell platform via homebrew, if that's relevant. Any tips? Would love to get my haskell dev going in IDEA
Set the sdk to where you installed haskell e.g. C:\Programs\Haskell Platform\2013.2.0.0
Autocomplete works fine, so does the show type of symbol but when I compile I get this error: Error running Unnamed: Cannot run: D:/Documents/Dropbox/Programming/Haskell/Project\dist\build\D:\Documents\Dropbox\Programming\Haskell\Project\src\Main.hs\D:\Documents\Dropbox\Programming\Haskell\Project\src\Main.hs.exe 
D'oh, obviously `flip id` isn't just `id`. Thanks for that detailed explanation!
Fixed, thanks.
What would the equivalence classes look like for modelling the monoid (*, 1, Z) this way? 
[The documentation for nub](http://hackage.haskell.org/package/base-4.7.0.0/docs/Data-List.html#v:nub) says it means "essence", so there's your reference.
Sorry, no source code anywhere :(
Quick question: If I'm viewing some documentation on hackage.haskell.org, how do I make sure that I'm viewing the latest version. Normally I have to look at the other available versions and muck around with the url to 'guess' where the latest docs will be. This is one of my many frustrations with the current haskell.org. e.g. navigate from: http://hackage.haskell.org/package/base-4.2.0.1/docs/Foreign-Marshal-Array.html to http://hackage.haskell.org/package/base-4.6.0.1/docs/Foreign-Marshal-Array.html
Cool, I wasn't aware of this.
Would be really thankful for more video/slides, especially on Gabriel's Pipes and Shachaf's Lens talks.
&gt; x*y + x*z == z*(y+z) I think this should read x*y + x*z == x*(y+z)
fixed :)
One trick you could try is not binding `(&gt;&gt;=)` etc. globally in the module, but only in a where clause. That way you can mix both, and you only need a few extra lines in each where block. Separate modules also work; I used that when I used indexed monads.
Not really. For any particular domain of mathematics I probably have something I can recommend, but it'd be a long list for the whole discipline and I'm not an expert in the whole field. ;)
I usually recommend LYAH and then the Typeclassopedia http://www.haskell.org/haskellwiki/Typeclassopedia. I could probably recommend a third alternative at that point, but I feel like you're probably ready to start actually making things at that point.
Asterisks must be escaped if you want to use them as multiplication symbols. (Or, indeed, as any kind of symbol.) x\*y + x\*z == x\*(y+z) 
 "double /,+ distribute" forall x y1 y2. (y1 *## x) +## (y2 *## x) = (y1 +## y2) /## x Those should be divisions rather than multiplications I assume? (edit: same mistake occurs in "float /,+ distribute")
&gt; for a monoid (&lt;&gt;) over a type Foo, is to create one equivalence class E_x for each value x of type Foo: the set of all lists [x0, x1, x2, ..., xN] such that x0 &lt;&gt; x1 &lt;&gt; x2 &lt;&gt; ... &lt;&gt; xN results in x Instantiating `(&lt;&gt;)=(*)` and `Foo=Z`, we obtain "one equivalence class E_x for each value `x` of type `Z`: the set of all lists [x0, x1, x2, ..., xN] such that `x0 * x1 * x2 * ... * xN` results in `x`". In other words, two lists of integers are equivalent if the product of their elements are the same. `[3,4]` is equivalent to `[2,3,2]`, `[-1,6,-1]` is equivalent to `[-2,-3]`, `[3,2,1,0]` is equivalent to `[99,0,77]`, and so on.
Since this article mentions the two Functor laws and free theorems: I've recently put together a derivation of the second Functor law in terms of the first here, https://github.com/quchen/articles/blob/master/second_functor_law.md
Any kind of meaningful data on the performance gain?
Small typo: 'conext-preserving'.
Shouldn't `x*y + x*z == x*(y+z)` improve accuracy too? Less operations -&gt; less error?
Not necessarily; it depends on the relative magnitudes. If y &gt;&gt; z or z &gt;&gt; y then the rewritten version could be less accurate.
Don't you need to introduce a new variable for that?
A lot of it was recorded to various degrees. We have just been too busy since then prepare the videos.
How would you even measure that? AFAIK, gcc has no tests measuring their -ffast-math optimizations. At least when I asked on their dev mailing list, no one said so! It's likely to be negligible in your average haskell program. In some naive numeric code I have it made about a 2x difference. That's probably the absolute most you could expect.
By sum I meant summing over a list; but yes you need an extra error term
My point was I don't think you could do that with just RULES pragmas... but you could probably do it with a new data type.
Ahh... That you could.
Fixed, thanks.
Handy link for future reference, given that neophytes often ask for this derivation after reading the allusion to it in the Typeclasseopedia.
You know this will change the answer depending on the optimisation level, right?
As a novice, may I ask how you did even come up with that example? Or is this one of the 'known' counter examples?
No, I just came up with it this morning :) The trick is that `seq` on functions plays havoc with the algebraic structure of the language. Beyond that it is just practice and intution. BTW, "fast and loose reasoning" is a technical term in Haskell. It comes from a theorem that you can ignore these sorts of issues in your reasoning (by working in the total subset of haskell) and if you prove two programs equal this way [you can only be wrong in their termination behaviour](http://www.cse.chalmers.se/~nad/publications/danielsson-et-al-popl2006.html).
"f of g of h of x" is ambiguous, though - `f . g . h $ x` or `f . g . h . x`? Granted, it would be clear if the type of `x` is known, but then you have to carry that around in your head implicitly ;)
That's fine. Criterion will just run it thousands of times until it can be measured reliably. I've used this to measure bitwise multiplication and modulo tricks in my own code. If you don't see any benefit running millions of operations with one-fewer assembly instruction then there's probably not much benefit. **EDIT** actually those tests I did were probably on a larger chunk of code (but e.g. with only a single `unsafeShiftL` or whatever); you might find the kind of test I posted is dominated by boxing/unboxing `Int`s depending on how it gets compiled, I'm not sure. 
... also known as *pointless style*, if you take it too far :)
&gt; but one that only holds up to fast and loose reasoning [Erm](http://www.cse.chalmers.se/~nad/publications/danielsson-et-al-popl2006.html).
in the presentation, they unveiled a new programming language which: "includes type inference and makes large classes of errors impossible" It looked like there is still heavy mutability, but this could be a good day for us in the static typing camp.
Nah, it is another "better" Javascript with ObjectiveC naming. Nothing to see here, move along. 
I don't know what 'to Sherlock' in this context but as far as languages go it still seems to be pretty uninteresting at least from someone accustomed to a modern language being something like Idris/Haskell/Agda. Someone correct me if If I'm wrong, but from reading the docs it seems the type system is some sort of ad-hoc construction with a lot of the types builtin to the language, doesn't have sum types, doesn't seem to support higher kinding, and seems to utilize quite a bit of implicit coercion. Probably is an improvement over Objective-C, but as far as the kind of programs I write it definitely seems like it's 15 years behind the times.
It does have Sum types. they're just reusing the enum keyword to declare. Note that they might not have recursive types directly, but I can see some reasonable ways to encode them
As far as PL interest goes, I'm not sure how much there actually is... On the other hand this looks much nicer to work with than Objective-C and that's where my interest in it is coming from. FWIW, Apple's book states &gt;Values are never implicitly converted to another type. Although I haven't dug too deep yet.
Thanks for feedback! * Problem with ' I will fix in next version. * cmd+I may fail if you installed new version of cabal with *cabal install cabal-install* . I will add some diagnostics to investigate such problems. 
I agree as well. Upon encountering `mappend` for the first time, I saw that it worked, but I couldn't come to grips with why that term should be chosen for the monoid operation. `mop` would make a whole lot more sense to me.
&gt; Probably is an improvement over Objective-C, but as far as the kind of programs I write it definitely seems like it's 15 years behind the times. I'd be surprised if they'd explicitly wished for it to be a vehicle of general-purpose PLT prowess: quite a few of the marquee features do simply seem like ObjC idioms lifted up into more concise syntax forms wrest away from C's own grammar - * ARC is elided entirely in form, and is operational on all bindings * the weak references that presumably keep Apple from wrangling over the infrastructure required for a cycle collector are introduced via a slight syntactical lilt to the binding form, rather than a pick-and-mix list of attributes * they make a great deal of effort to work in New and Improved extensions and protocols into the mix; * and they conclude - and finally, at that - that a default policy of indicating failure in-band on a message to nil solely to encode failure in an if-clause friendly format ceased to feel like a penance taken to prepare the way for the otherwise wholesome-feeling minimalism (that ObjC was once, at the very least; I'm sure they're taking this opportunity to work in some of the extensions that have grown unwieldily through @-declarations.) All the goodness, then, minor buttressing for their own ecosystem - although their "major" work has just a prominent a trail in academia, if I can generalize off the fact that the Interface Builder budded off a Lisp presentation-model UI framework, and the Foundation itself designed to overtly emulate the abstract-interfaces-over-hidden-data-structures taste of late Smalltalk's own base set of classes.
Fine. Done.
Yes, they have sums-of-products: https://developer.apple.com/library/prerelease/ios/documentation/Swift/Conceptual/Swift_Programming_Language/Enumerations.html#//apple_ref/doc/uid/TP40014097-CH12-XID_185
Can you even do equational reasoning when you have strictness (like your use of `seq` here)? I'm pretty sure you can't. 
&gt; it still seems to be pretty uninteresting at least from someone accustomed to a modern language being something like Idris/Haskell/Agda. How could it be! Apple [says](https://developer.apple.com/swift/) it is an ***innovative new programming language*** and ***the result of the latest research on programming languages***! ... and WOW! ***You don't even need to type semicolons***! ^/s
I hacked Haskell for a long time (well over a year) before I really *got* the need for lenses in games. I suggest you take your time, like I did. :-)
you can. You just don't get the eta law. Equational reasoning works perfectly fine in strict languages and even in Haskell (which has `seq` regardless of if we decide to use it). You just have different axioms. Lazy language give up some equations too, particularly about sum types. It really is purity not non-strictness that enables equational reasoning. And, even that isn't absolute. SML remains often easier to reason about than haskell because allthough it isn't pure all (immutable) data is in the form of finite trees and so you can easily use induction. When I prove Haskell code I often have to construct clever logical relations and other tricks (and no, Haskell isn't just co-inductive, it is far more complicated than that).
To be more accurate?
As a trivial example, consider triples. It's very nice to work with terms like `(x,y,z)` because they holistically capture what you mean to say. However, with the usual sum/product implementations you'd be forced to chose between `(x,(y,z))` and `((x,y),z)` since generally n-ary products aren't on offer from these frameworks. Moreover, since the associativity is irrelevant[1] we'd much rather have a "canonical" form instead of needing people to manually remember whether to use `(snd . fst)` vs `(fst . snd)` to access the middle element; etc. [1] That is, the types `X * (Y * Z)` and `(X * Y) * Z` are isomorphic‚Äî modulo bottoms. And we can make the bit about bottoms explicit, if desired; though Haskell doesn't actually do so.
Nice. I thought I had seen them all. I apparently skipped over the fay episodes. Thanks!
Yep, I based some of my code on that.
This distinction seems to be splitting hairs a bit. `(x,y,z)` *is* a product even if it's not a binary product. Likewise `data Sum = One x | Two y | Three z` is a sum. 
Same here, hence the submission
&gt; json or possibly s-expressions JSON doesn't support comments and would be rather inconvenient to edit by hand compared to the current syntax. I'm not sure S-expr would be better. However, using XML would be a huge step backwards IMO. If it's just about parsing Cabal files, we just need a self-contained parser for `.cabal` files with a simple AST. I don't see why we need to change the `.cabal` format to something awkward to edit with a text-editor. I believe /u/dcoutts was working one something like that.
&gt;So for attacks that target kernel vulnerabilities, SELinux is no better than vanilla Linux. I disagree with this. SELinux means most of the tools one normally uses to attack the kernel won't usually be available.
I would like to know the differences in both. The one I posted was listed on IntelliJ's website. This plugin appears to not be fully ready yet(per the readme file)? I appears both are active in development so I will keep an eye on both. 
But calculus is based on addition and multiplication - functors and monads are based on cat theory. So the analogy does not hold.
&gt; http://hackage.haskell.org/package/free-4.9/docs/Control-Alternative-Free.html I was aware of it. Unfortunately I also need Monad and MonadPlus instances. Therefore I took Control.Monad.Free as a starting point. &gt; Yours is too big in that you can distinguish multiple levels of Many's. Could you please ELI5 this for me? &gt; The version I have there is a free Alternative for alternatives that &gt; satisfy left (sic) distribution, but not those based on the alternative &gt; left catch law. You mean two possible MonadPlus laws sets here, right?
It depends on how you look at it I guess. Realistically at a certain point with software like Linux, you have to also start mitigating attack classes, not just enforcing policies. A single mistake in your policy, SELinux, or the kernel could undo any of that. You have to build in other mitigating factors for a project of such size, such complexity. Modern distros still ship with a lot of juicy information and attack surface laying around - things like shipping `kallsyms`, allowing unrestricted `/proc` access, kernels allowing users to induce an oops (without repercussion), users autoloading driver families upon things like `socket(2)`, etc. While you can enforce policies for these, it's just extra attack surface laying around, plenty of people don't do it, and it doesn't begin to scratch the surface of other attack vectors (like memory corruption, or more trivial things like Linux developers forgetting to enforce a capability restriction in a key place, etc). It's happened before, it will probably happen again. (Capability based bugs are particularly silly since as the kernel gets more complex, the needs of capabilities will increase, and so will their attack surface as people forget to enforce them.) SELinux is *definitely* an improvement, but I think if you study the landscape, it ignores a lot of the aspects of advanced, modern kernel exploitation. For Linux today, the best you'll get hands down is grsecurity, which not only features a robust RBAC solution (that's less crazy and more flexible and usable than SELinux IMO), but also takes steps to mitigate userspace and kernel attacks from many angles - and empirically, these kinds of things are necessary to provide a truly coherent defense against modern attackers, it seems.
I do (on more recent Ubu): sudo apt-get install ghc cabal-install libz-dev ruby rubygems cabal update cabal install Cabal cabal-install When trying a already check out project make sure to clean up: cabal sandbox delete rm -rf ~/.cabal ~/.ghc Then cd to the project directory of an (just init'ed) Yesod project and: cabal sandbox init cabal install --max-backjumps=-1 --reorder-goals \ . yesod-platform yesod-bin happy alex It pretty much works for me. (Sometimes the last command needs to be retried). G'luck! 
The notion of a 'Free' thing satisfies the laws and nothing more. Here you can distinguish nested `Many`s or nested `One`s. This gives you more distinguishable things. At best the free construction is a quotient of what you have written there. The `Free Alternative` I linked avoids that by always collapsing into layers of `[]`, `f`, `[]`, `f`. Never [] following `[]` or `f` following `f`. If you need a real `MonadPlus`, use `FreeT f []` instead. That is also free and offers the same structure.
That's really my point about why criterion tests for each individual rule don't make sense. They won't translate over into real applications. I can write arbitrarily bad numeric code that the fast-math rules would provide arbitrary speedup for. The only good tests are taking actual numeric code that you have and importing `Numeric.FastMath` at the top. So a good suite of benchmarks would be to take applications off of hackage and compare before/after run times. But I don't know of any applications on hackage this would apply to. Most people don't post this sort of code I don't think.
Sherlocked or not, about 9 million iOS/Mac developers are going to learn how pattern matching/higher order functions make their lives better. The ambitious few percent of those will seriously look into other languages (ML/Haskell camp, where those features originated) to see what they have to offer. That means tens of thousands newbies for /r/haskell. Brace yourself :-)
That's terrific. Thanks for sharing
Nope, not even close. As a Haskell programmer, I've been asked several times what I thought about some language. After giving the question some thought, I've concluded that there are three things a new/proposed language must have before I'm even interested in looking at it. 1. It must be "functional". And by that I mean that functions must be first-class so you can pass them to other functions and return them from other functions. 2. It must have an expressive strong static type system with type inference. 3. It must be pure. Of these, Swift only has #1. So no, Apple did not just 'Sherlock' Haskell. They're claiming that Swift is "innovative" and "is the result of the latest research on programming languages", but IMO that's utterly false marketing hype. Can you name even one feature of Swift that hasn't been around since the 80s or 90s? This kind of thing cements my growing hunch that we here in /r/haskell are quite literally decades ahead of the mainstream programming world. There's just no comparison.
Ah so maybe my confusion is due to lack of familiarity with the "frameworks" you're talking about. What specifically do you mean?
Another point of distinction between ADTs and sum/product comes in with namespace pollution. Consider the following encoding of your `Sum` ADT: data SumTag = OneTag | TwoTag | ThreeTag type Sum = ‚àë SumTag (\t -&gt; case t of One -&gt; x; Two -&gt; y; Three -&gt; z) One x = (OneTag, x) Two y = (TwoTag, y) Three z = (ThreeTag, z) Sure, this is an accurate enough translation; but we end up with a lot more crap we have to name. In particular we have to distinguish between the constructors for each branch of `Sum` and the tags representing those branches. While there is some amount of value in properly distinguishing these different concepts and giving them different names[3], that's the sort of thing the compiler should be doing, not programmers. Doing it manually gets really old really quickly. If we want programmers to have hooks into what the compiler's doing, then the language should offer some explicit metasyntactic access to what the compiler's doing; which is to say, offering a sound and robust way to pun between the data constructor `One` and the branch tag `OneTag`, etc. [3] Similarly, properly speaking, we should distinguish between the actual data constructors themselves vs the pattern deconstructors associated with them. This is seldom done with ADTs, but it really should be. Just because we do not want clients to be able to construct values freely does not mean we cannot let them pattern match to take them apart.
Thanks, that's a very nice example.
`cabal build -j` (parallel builds) might help a bit (available in the latest cabal-install).
Yeah, but on the slide it says `bind :: (a ‚Üí m b) ‚Üí (m a ‚Üí m b)` which looks wrong to me..
While you're developing, you can turn off optimizations (`-O0` with ghc, or `--disable-optimizations` flag with `cabal install` or `cabal configure`). This speeds things up quite a bit. Just be sure to install your package's dependencies first with optimizations turned on. 
 (a -&gt; m b) -&gt; (m a -&gt; m b) is the same type as (a -&gt; m b) -&gt; m a -&gt; m b Remember, `-&gt;` is right associative. The parenthesized version just emphasizes the 'lift this function into this other domain' behavior, which helps think categorically.
See http://chrisdone.com/posts/making-ghci-fast re `-fobject-code`.
Unless I'm misunderstanding something, usually (&gt;&gt;=) has type m a -&gt; (a -&gt; m b) -&gt; m b rather than (a -&gt; mb) -&gt; ma -&gt; mb (which would probably help people get the similarity to functors / applicatives).
Sorry, I'm not understanding what you're saying. `bind` as defined above (on that slide) has type `(a -&gt; m b) -&gt; m a -&gt; m b`. Applying flip to this gives us something with the right type for `(&gt;&gt;=)`.
In addition to the tips already given, make sure to turn off documentation and profiling builds if you're building in a sandbox.
So cabal install --only-dependencies --disable-documentation --disable-library-profiling ?
While the typical discourse around Monads in Haskell considers `bind` and `&gt;&gt;=` to be identical, the slide deck uses `bind` in the Monad class definition as a parameter-reversed version of Haskell's `&gt;&gt;=` operator. This makes the "arrow-flipping" between Monad and Comonad more apparent. It also afterward presents `&gt;&gt;=` as `flip bind`, which gives it the standard Haskell type, so code using these definitions will look and work just like you're used to. The presentation in the slides makes the categorical interpretation of things much easier; the operator argument order in Haskell, on the other hand, is optimized for code clarity. TLDR: No errors here, just a slightly different presentation of things from how they're defined in the standard libraries.
Is there video of this?
I haven't explicitly tried it, thanks. I usually typecheck via :r in the repl so I think I probably have the low-hanging fruit with quick typecheck sorted out. Now I'm mostly trying to speed up `yesod devel` (which rebuilds our website continuously) and deployment.
Can anyone comment on when and why they've chosen to use (for example) Env vs Reader?
Hmm, this exact same question is on SO at the moment. Common class assignment? Another one of those online programming challenges? http://stackoverflow.com/questions/24022960/recursive-fibonacci-list
Evaluate fibN 4 3 by hand.
Alright then, Ghc agrees and I just don't have a clue. :) Thanks for the explanation! &gt; let bind = undefined :: (a -&gt; m b) -&gt; (m a -&gt; m b) &gt; :t bind bind :: (a -&gt; m b) -&gt; m a -&gt; m b 
TH definitely makes it slower than it would otherwise be. It's still faster than a full build though.
This is a great answer. We need more like this. 
It wasn't just slower for me. It wouldn't build. ByteCodeLink.lookupCE During interactive linking, GHCi couldn't find the following symbol: foozm0zi1_FooziCommon_bar_closure This may be due to you not asking GHCi to load extra object files, archives or DLLs needed by your current session.
You can make a Model directory with several .hs files
&gt; However, the completions I get don't include possible functions imported from packages installed in my cabal sandbox - I'm not sure whether this is a ghc-mod or plugin problem, though. Try running the editor from the command line with cabal exec.
Keegan had an old RE2 binding I quite liked - available [here](https://github.com/kmcallister/haskell-re2/tree/master/Text/RE2). Can I ask if you see any fundamental differences between the two?
Just wondering: how does the interpreter pattern compare to straight code in terms of efficiency, once you take into account Haskell's lazyness and compiler optimizations?
Awesome, that works, thanks!
If you're ok with the extra `data` wrapper, you can do this using just `PolymorphicComponents` instead of `ImpredicativeTypes`, without any additional type annotations. {-# LANGUAGE PolymorphicComponents #-} data List a = List { runList :: forall r. (ListVisitor a r) -&gt; r } data ListVisitor a r = ListVisitor { empty :: r, append :: a -&gt; List a -&gt; r } listLength :: ListVisitor a Integer listLength = ListVisitor { empty = 0, append = \value tail -&gt; 1 + runList tail listLength } listEmpty :: List a listEmpty = List empty listAppend :: a -&gt; List a -&gt; List a listAppend value tail = List $ \visitor -&gt; (append visitor) value tail myList :: List String myList = listAppend "Hello" (listAppend "World" listEmpty) listMap :: (a -&gt; b) -&gt; ListVisitor a (List b) listMap f = ListVisitor { empty = listEmpty, append = \a l -&gt; listAppend (f a) (runList l (listMap f)) } listBuilder :: ListVisitor a (List a) listBuilder = ListVisitor { empty = listEmpty, append = listAppend } main = print (runList (runList myList (listMap show)) listLength) You can also move the recursive call into the implementation of the visitor, essentially identifying a list with its own fold function: data ListVisitor a r = ListVisitor { empty :: r, append :: a -&gt; r -&gt; r } listLength :: ListVisitor a Integer listLength = ListVisitor { empty = 0, append = \value tail -&gt; 1 + tail } listAppend :: a -&gt; List a -&gt; List a listAppend value tail = List $ \visitor -&gt; (append visitor) value (runList tail visitor) listMap :: (a -&gt; b) -&gt; ListVisitor a (List b) listMap f = ListVisitor { empty = listEmpty, append = \a l -&gt; listAppend (f a) l } This makes the various fold functions `listMap`, `listLength` a little simpler, but the form of recursion now has to be uniform across the entire list, which is a little different from the visitor pattern as you'd see it in object-oriented languages.
any cellular automaton
An easy way to remember it is if you have f :: A -&gt; B -&gt; C x :: A y :: B then f x y :: C -- obviously but with currying f x :: B -&gt; C Therefore f :: A -&gt; (B -&gt; C)
1. Mine's on hackage. 2. This package bundles the re2 source, so you don't have to find install re2 first. This is useful because re2 itself doesn't have stable releases and isn't packaged for any major distribution. 3. kmcallister/haskell-re2 only exposes 'match' and a few basic pattern properties, mine's also got replacement and extraction.
Expect an overhead on the order of 10s of nanoseconds for each constructor the interpreter produces. This is roughly the same over head you get for building and consuming a list without any fusion optimizations.
I've heard that sometimes the carrier type is boiled off entirely. Is that not the case, or only rarely the case?
In theory, I think they're isomorphic. Obviously you can define instance SomeAction (Free ActionType) to go the one direction, and you can of course go from a free monad to any other monad. I can't speak to whether one version compiles better or faster or proves more convenient or whatever...
In theory ``someAction`` could work with any type whereas using ``Free`` requires you to create a type and wrap it. If your code does not have a clear type you want to wrap the other approach may work better.
Pretty sweet. I've been fiddling with atom a bit myself, and have found the haskell plugin and atom lint plugin (with hlint installed) to be a nice combo. Oh, and run hasktags to generate project-wide symbol tags, which atom automatically uses for the fuzzy symbol search. I look forward to trying your plugin! On a related note, do you know how atom deals with plugin overlap/conflicts? I.e, will I have to manually disable the other plugins? If so, it might be worthwhile to split your plugin into several parts which provide more specific services.
**GHCJS** (#ghcjs on IRC) Off the top of my head * **Dynamic code loading** - We had a solution with the old code generator and we could just port it over. It had some limitations though. It did not allow for dynamic code (you had to know at link time what functions would be used) and solving this would be an interesting challenge. Another interesting idea is to use profiling results to allow referenced but little used functions to be relegated to dynamically loaded automatically. * **Terminal Emulation** - [HTerm](http://git.chromium.org/gitweb/?p=chromiumos/platform/assets.git;a=blob;f=chromeapps/nassh/doc/faq.txt) support (or something similar). This could be a small project to get basic stdin and stdout support working with HTerm or a big project to make things like [hscurses](https://github.com/skogsbaer/hscurses) work. **Leksah** (#leksah on IRC) Lots to do here too * **Add your most wanted feature** - Add the thing that you would miss most if you switched to Leksah (or the thing you most want if you already use it). * **Yi integration** - It is incomplete and needs a hero to fix it. [Here](https://github.com/leksah/leksah/blob/master/src/IDE/TextEditor/Yi.hs) is where Leksah calls Yi. [Here](https://github.com/leksah/yi) is the work in progress Gtk 3 branch of yi it uses. * **HTML based Log or Module pane** - Long term plane is to move off Gtk to HTML and GHCJS. This means replacing existing UI elements with WebKitGtk based ones. * **Side by side editing** - I would love a side by side view of what I am working on diffed with the git version. 
I've got a video of the talk that I'll be uploading (with David's permission) soon.
Not now, but I'll do it in future. Now the autocompletion in development state. You may create an enhancement issue and I'll not forget to implement it.
You might consider using the PureIO package: http://hackage.haskell.org/package/pure-io-0.2.0
Plugins may be in conflict with each other, but in this case a bug issue appears in plugin project issue tracker and plugin developer has to hack it. As for my plugin, I do not want to split it into several components (at least not now) because these components heavily depends on each other.
Interesting. Not sure if that's a bug or some uses of TH are expected to have that effect.
We (Zalora) are trying to figure out the best way to bring FP to the front end - it's less clear for now that Haskell is as big a win as on the back end, and we haven't gotten round yet to hiring a full timer to solve that problem. Like every other apparel company in the world, we also would love to figure out visual recommendation engines. I personally have a bunch of non-Zalora small projects I'd like to get built in Haskell, but no time or much funding (yet) for them. One you might be interested in: the Mechanical Turk flavour of Kaggle, for smaller projects. Kaggle asks you for a lot of cash upfront to host competitions and prefers big 6 months projects, but I'd love to be able to outsource small problems for 500 bucks ("what does this A/B test result tell me, dear person with a great stats background?") and without talking to humans. That last part is the hard one - how do you host competitions, and how do you automate the judging, when your customer might not have a stats/ML background? 
What you do reminds me of the "finally tagless" encoding of data. There is an awesome [tutorial for it by Oleg](http://okmij.org/ftp/tagless-final/course/lecture.pdf).
Slightly off-topic, but does anyone know what the `mod` acronym stands for in `ghc-mod`?
If you get that, please post it as a separate link to Reddit - I'd hate to miss it!
Yeah, I'm OK with having an extra data wrapper. Thanks! But just curious, do you know if the original code can be made to typecheck? Now trying to figure out the pros and cons of your second approach. Looks like it's called "Boehm-Berarducci encoding", while my original approach is "Scott encoding".
We have lots of nice projects waiting around in Idris, where we do have a JavaScript backend for the compiler. It might be fun to write a web-based IDE for Idris in Idris, which will probably involve adding IDE support features to the compiler, which is written in Haskell.
And I was hoping that I this was about a generic (vim and emacs) editor integration for the [atom language]. [atom language]: http://hackage.haskell.org/package/atom
In general a -&gt; b -&gt; c -&gt; d = a -&gt; (b -&gt; (c -&gt; d)) You put the parens in the right when you are figuring out what it means so a -&gt; b -&gt; c -&gt; d is a function that takes an `a`, and since it is `a -&gt; (b -&gt; c -&gt; d)` it gives back b -&gt; c -&gt; d which is a function that takes a `b`, and since it is `b -&gt; (c -&gt; d)` it gives back c -&gt; d which is a function that takes a `c`, and gives back a `d`.
I've not looked at your code, but the Scott encoding can certainly be made to work very well. 
Idris is quickly getting better tooling than Haskell.
Note: building with `-j` seems to have some problems. If interrupted w/ `^C` you might get the `.hi` file for something but not the `.dyn_hi` file for it, which are treated as one artifact by the build system, then you can't build until you clean. =(
Thanks for the explanation! If I got it right, I can simply solve this by using partial function application. I hope this will stick in my memory. :)
Here's a picture that might help: http://idontgetoutmuch.files.wordpress.com/2014/01/2e04835d5100b2acf559d7423351af50.png?w=640 Also I very much appreciate all the posters who have actually made attempts to explain the relevance of homotopy to HoTT. I asked this some time ago and didn't receive a particularly satisfactory answer: http://www.reddit.com/r/haskell/comments/1usnox/homotopy_type_theory_lectures_by_bob_harper/.
That's really great! Thanks. Is autocompletion is a possible feature to implement?
https://ece.uwaterloo.ca/~dwharder/NumericalAnalysis/02Numerics/Double/paper.pdf
It's possible and will be in next release. At least I hope so :)
We're working on it :-) Ideally, Haskell folks will get jealous and do something even better, so we can all have nice tools. I certainly miss case splitting when working on the Idris compiler. There is a GSOC project this summer to bring Idris- and Agda-style interactive editing to ghci, so the strategy seems to be paying off!
94 pages ... wow and thanks :)
Interesting. I'll have to think about this some more.
Let's look at the type of `(&gt;&gt;=)` vs. `(=&gt;&gt;)`, when specialized to `Reader` and `Env`, respectively: (&gt;&gt;=) :: (e -&gt; a) -&gt; (a -&gt; e -&gt; b) -&gt; e -&gt; b (=&gt;&gt;) :: (e, a) -&gt; ((e, a) -&gt; b) -&gt; (e, b) The second argument of each function is basically identical, since `(a -&gt; e -&gt; b)` is isomorphic to `((e, a) -&gt; b)`. The difference is where each computation starts and ends. The `Reader` computations begin and end on something that consumes a value of type `e`. The `Env` computations begin and end on something that contains a value of type `e`. In other words, the `Env` comonad carries around the `e` value with it, so if I hand you an `Env` computation you already have the `e` you need. You don't need to ask me or anybody else to provide it to you. The `Reader` computation, on the other hand, depends on some external resource to provide the missing `e`.
Here's a working shorter version of what I mean, for a plain, non-recursive sum type: {-# LANGUAGE ImpredicativeTypes #-} {-# LANGUAGE LiberalTypeSynonyms #-} type Visitable v = forall r. v r -&gt; r type Constructor v = v (Visitable v) type SumVisitor a b r = ((a -&gt; r), (b -&gt; r)) type Sum a b = Visitable (SumVisitor a b) sumConstructor :: Constructor (SumVisitor a b) sumConstructor = (flip fst, flip snd) foo, bar :: Sum Int String foo = flip fst 1 bar = flip snd "Hello" The interesting thing to me is that the individual constructors of the sum type are just flipped accessors of the visitor product type. It looks similar to sum/product duality, but I don't understand all the details yet.
Looks good, but I am not prepared to give delete permission to my google docs
Here's a complete module for the usual Scott encoding of lists in Haskell. {-# LANGUAGE RankNTypes #-} module List(List, nil, cons, caseList) where newtype List a = List (forall r . r -&gt; (a -&gt; List a -&gt; r) -&gt; r) nil :: List a nil = List (\ n c -&gt; n) cons :: a -&gt; List a -&gt; List a cons x xs = List (\ n c -&gt; c x xs) caseList :: List a -&gt; r -&gt; (a -&gt; List a -&gt; r) -&gt; r caseList (List l) = l 
Hey, the author here. This is a good point, and one I didn't think through. It sure would be nice if there were some way to ask for permission only for a specific folder. I'm not an expert on this, but I don't see that such a thing is possible. I'll reconsider this. In the interim, you can still use the web site to play around with things without signing in. You'd just have to save your work using copy/paste, which is admittedly ugly...
Awesome! I am wondering if some practical interoperability between Idris and Haskell has been already achieved, i.e, can we call haskell function in idris and idris function in haskell? I know idris is compiled via C, so in principle, we can achieve this, but given similarity between idris and haskell, I expect more tight relationship between them. For me, this will be super-feature, like implementing pretty complex algorithm in idris with proof and use it for some haskell program smoothly without a big FFI noise. 
Very cool
Tangential: you can write a program in Coq and export it to Haskell.
If you're working with a cokleisli Env arrows (and you squint), it looks pretty similar to a Reader value. (Especially if 'a' is not hard to get; if it's a monoid for example.) Specifically, Reader e (a -&gt; b) is very close to Env e a -&gt; b. I guess I'm thinking about those cases. When you bring in ReaderT and EnvT and sequence effects, though, it makes a very real difference which you choose.
I'm so glad this is back. I have referred people to Haskell for Kids several times in the last few months and was disappointed that it had died. I plan to suggest this to anyone I encounter who is interested in learning any kind of programming.
Appropriate response: http://www.reactiongifs.com/wp-content/uploads/2011/09/mind_blown.gif
If you think of (e, a) as the type of the *memoized* function (e -&gt; a) then everything matches again but for memoization. Conal has a nice blog series on this idea [here] (http://conal.net/blog/tag/memoization). This lets us see the comonadic version as providing the same functionality but one that has "seen it all before". A less cute way of putting it is that all the "functions" in the comonadic version are strict where the monadic versions are not.
How does this work? I tried with {-# LANGUAGE OverloadedLists #-} import qualified Data.Vector as V printIntVec :: V.Vector Int -&gt; IO () printIntVec = print main :: IO () main = do printIntVec [1,2,3,4] But that gave me a /tmp/olists.hs:10:17-25: No instance for (GHC.Exts.IsList (V.Vector Int)) arising from an overloaded list In the first argument of ‚ÄòprintIntVec‚Äô, namely ‚Äò[1, 2, 3, 4]‚Äô In a stmt of a 'do' block: printIntVec [1, 2, 3, 4] In the expression: do { printIntVec [1, 2, ....] } (and I've checked I have only `vector-0.10.10.0` installed)
As a vim addict, I'm considering giving it another shot after finding a decent example configuration: https://github.com/yi-editor/yi/blob/master/yi/example-configs/yi-vim2.hs. This looks like more fun than hacking on my vimrc.
Damn... It's because the instance for `Data.Vector.Vector` wasn't backported (it was probably missed because the `IsList` instances in `master` were spread over multiple commits). This means yet another minor version bump release.
Call me when we have kind lenses. 
It is indeed way more fun but last I contributed to the project it still had a number of rough corners. It's somewhat active though so its quite possible it's gotten better. You should report back with your experience :-)
Given that unboxed unboxed vector is equivalent to unboxed 2D vector, what is the (performance or otherwise) advantage of using this v.s. the Repa library?
Good question. They are very similar. Advantages of unboxed unboxed vectors: 1) The interface is more similar to the excellent vector package. If you have code that contains a boxed vector of unboxed vectors, it would be straightforward to convert that into an unboxed unboxed vector. For code that I've written, this is much more convenient than using Repa (but YMMV). 2) Repa has a little bit more overhead than an unboxed unboxed vector, but for most applications this is probably negligible. 3) If you need to pass your multidimensional vector to a foreign function, then you can use a storable storable vector. (This code isn't pushed onto hackage, but it is in the github repo.) 4) Repa doesn't provide any of the supercompilation-like advantages of typeparams. That is actually the main reason why I wrote the library, and unboxing unboxed vectors just seemed like an easy way to introduce all the machinery. Advantages of Repa: 1) Automatic parallelism 2) Much more stable 
it work in windows?
Bah, bring me out of cryostasis when we have sort lenses. 
This looks amazing! Some of the limitations with `vector` (lack of `slice`) make me wonder if there are even better demonstrations to be found. I'll be interested to dig in to why things don't work out, but if you can add more detail regarding incompatibilities with `vector`'s design, I would certainly appreciate it! Aside, I am also surprised that `viewParam _len vv1` returns 1.
Oops, thanks for catching that, Herbert. I must have tested master instead of 0.10 before pushing!
&gt; This looks amazing! Some of the limitations with vector (lack of slice) make me wonder if there are even better demonstrations to be found. I'll be interested to dig in to why cosas don't work out, but if you can add more detail regarding incompatibilities with vector's design, I would certainly appreciate it! It's becuase of the type signature of basicUnsafeSlice :: Int -&gt; Int -&gt; v a -&gt; v a which requires the original and new vectors to have the same type. But if the size is encoded using `Static`, then slice will change the size but cannot change the type. It would be possibly to change the type of all the slicing functions to allow them to change the `Static` lengths, but this would be a lot of work. The key code to look at to understand the unboxed vector will be the [PseudoPrim](https://github.com/mikeizbicki/typeparams/blob/master/src/Data/Params/PseudoPrim.hs) class. This class allows any type to implement the primitive functions associated with unboxing by storing a small amount of run time information. &gt; Aside, I am also surprised that viewParam _len vv1 returns 1. Whoops! It doesn't :) I made the definition of `vv1` a little more interesting and forgot to update that section. The README should be fixed now.
Oh, of course! But one could provide a new slice function that changes the type. It might be worth the trouble as vector slice arithmetic could be a very compelling application.
I run my tests in a REPL. I keep two REPLs running, one for development, one for reloading code and running tests. This has given me fast red-green-factor. YMMV.
I don't... does anyone? But utility is very useful.
Very cool. Could you clarify something that I didn't quite grasp? &gt; The significance of Base will be explained in a subsequent section. Mainly it was this that I didn't get. I'm pretty sure the following paragraph was intended to explain it, but it was a bit dense for me. &gt; In general, mkParams will generate these type lenses for every type param of its argument. If the type param p1 has kind `*`, then the type lens will have type _p1 :: TypeLens p (Param_p1 p) and **the class** will have kind Param_p1 :: (`*` -&gt; Constraint) -&gt; `*` -&gt; Constraint. If the type param has any other kind (e.g. Config Nat), then mkParams will generate _p1 :: TypeLens Base Param_p1 and Param_p1 :: `*` -&gt; Constraint. What's **the class** it refers to? And what's `Base`?
It seems I forgot about the promise to actually explain `Base` :) The form of a type lens is: typelens :: TypeLens p (paramclasses p) This allows composition using the `(.)` in Control.Category. For example: _a :: TypeLens p (Param_a p) _b :: TypeLens p (Param_b p) _a._b :: TypeLens p (Param_a Param_b p) The `Param_XXX` are type classes that indicate where in the type our lens points. So when we build up a complex composition of lenses, like `_a._a._b._b._b._a._b._a._a._b._b._a`, the type is changing to indicate where the lens points. The `Param_XXX` class will have kind `(* -&gt; Constraint) -&gt; * -&gt; Constraint` iff the corresponding parameter has kind `*`. Otherwise, the `Param_XXX` class will have kind `* -&gt; Constraint`. This is because for a parameter like `len :: Config Nat`, it doesn't make sense to compose this with anything. Nothing can go inside of it. The `Base` type is used to indicate that the second parameter of `TypeLens` cannot be composed any further. The len param of a vector cannot be composed, so: _len :: TypeLens Base Param_len But the elem param can, so: _elem :: TypeLens p (Param_elem p) It is called `Base` because it is the base case for the recursive type families.
What does unboxing of unboxed vectors mean? What representation is more efficient than a flat array of bytes (which is what unboxed vectors use)?
You can't create a 2d unboxed vector (i.e. put an unboxed vector into an unboxed vector) using the `vector` package. This package makes that possible. In the `vector` package, you have to create 2d vectors by putting the unboxed vectors inside of a boxed vector.
Right now, happy with Sublime + the Sublime Haskell plugin
We're using gtk+ so it should. I don't think we have anyone using it on windows though.
Better yet, lenses for lenses
&gt; so for those not used to Type Classes... What you have there isn't a type classes, but a definition of a type constructor. Otherwise nice post :)
This is one of the best material I have seen in explaining how one can program pi type that enables to express true dependent type in (future) haskell. 
https://www.youtube.com/watch?v=TSyGryuMh-U&amp;feature=youtu.be
https://www.youtube.com/watch?v=TSyGryuMh-U&amp;feature=youtu.be
I've uploaded a video of David's talk to YouTube: https://www.youtube.com/watch?v=TSyGryuMh-U&amp;feature=youtu.be
Well first fire one and hire me. I'm just kidding of course. [Don Syme just retweeted this course which is in haskell and is just starting] (https://www.edx.org/course/delftx/delftx-fp101x-introduction-functional-2126)
how realistic is it to try to interoperate between a strict and lazy language? anyway, I personally would rather see Idris chart its own course than get bogged down with haskell interoperability
That's an amazing course. Thank you. But it is in October and it is 2 months long! I was looking more for a 2-4 day training courses like the ones fp complete provided. Not sure if they still offer those. The funny thing is, i could train them myself, i offered it to management. But they do not want to "waste" 3-4 days of my productive work. But i feel i'll end up doing just that. 
Interesting! :D
It's about time we had a proper Haskell-based MOOC! This course may have potential to introduce thousands of new people to Haskell. Martin Odersky's Coursera course on Function Programming, even though it wasn't that good, created a tremendous spike in interest for Scala. 
I have really enjoy Brent Yorgey's introductory Haskell course, found here: http://www.cis.upenn.edu/~cis194/lectures.html Good lectures, great assignments.
Signed up!
That is definitely a good way to do it for this problem, but I am wondering if in general it is possible to apply tie-the-knot to lists within a monad. Another problem I am trying to solve, for example, involves generating a two-dimensional array that represents the values of a function f(x, y) with the constraint that it must be monotonically increasing for all x and y and concave down. It would be nice if such complicated relationships between different elements of a list or array of possibly multiple dimensions could be resolved using the fix-point even if the list or array lies within a monad.
Haskell interoperability is not a priority for Idris. You can go via C, but that's about it, and supporting a higher-level interop would be exceedingly complicated due to quite different runtime representations of terms, especially in the presence of detagging, as well as Idris's pervasive erasure.
Thanks!
Well, I should have looked closer myself when I uploaded the fixed up `vector-0.10.10.0` release... :-/
Are you asking for the API or the blog? The API is part of `lens`. http://hackage.haskell.org/package/lens-4.2/docs/Data-Aeson-Lens.html
Aaaah ... thanks! I thought the `lens` interface would be part of the `aeson` package and was mildly confused because I couldn't find it ...
Doctor of Philosophy? o_O Edit: Oh dear, you shouldn't reddit when you're sick.
Found [some more screenshots](http://www.haskell.org/haskellwiki/Yi#Screenshots_.280.3.29), but I'm still not sure. Can I run this over ssh?
Well-Typed certainly offers private training to clients. The page you linked merely lists the *open* courses that anyone can attend that we regularly run. But we run training for clients year-round too as it says later on the page. As it said there, you can certainly [talk to us](http://www.well-typed.com/contact/) if you're interested. (It probably goes without saying I work for WT, but full disclosure and all.)
Yi can run with different GUI frontends. It works both in terminal and as a normal windowed application. The CLI frontend will work nicely over SSH.
Interesting topic. But the prose is so awful, that it is almost unreadable.
There is a `MonadFix` typeclass in [Control.Monad.Fix](http://www.haskell.org/ghc/docs/7.8.2/html/libraries/base-4.7.0.0/Control-Monad-Fix.html); this suggests that it's not possible for all monads.
Came across this on Haskell Caf√© - Looks amazing!
yeah i gave up on that one so i hope this one is better too :x
Me too, I've had it open in a browser window for weeks. :)
go the co-burrito! http://imgur.com/Oo6EpYn
sweetness
I signed up just for the Meijerisms.
I use `-fno-code` successfully with `lens`'s TH lens-generating code so it must execute at least some TH code otherwise the generated lens names wouldn't even be there to typecheck.
cabal install cabal: The following packages are likely to be broken by the reinstalls conduit-1.0.17.1 http-conduit-2.0.0.8 http-client-conduit-0.2.0.1 What should a beginner do?
Yeah I'm not sure how that works. I assume you're using the to-be-generated lenses? You can't type check without first resolving names, but maybe it just assumes `undefined` for anything it can't resolve. The error above is a linking error and not from the GHC name resolver, so I seem to be misremembering here. I used to use `-fno-code` for on the fly type checking in the editor (hdevtools etc) and the main issue was the false positives (ill typed code passing as well typed), rather than any actual errors.
sandbox
In my opinion, even beginners should always use cabal sandboxes.
That's awesome. It feels good to know that my first Haskell project turned out to be useful to someone. Thank you!
As a beginner, I don't know what a cabal sandbox is (although I know what sandboxes are in general). Should I just force-install those three packages, it's not like I've ever actually used them
TODO TODO TODO
If you don't use them, you can uninstall them with `ghc-pkg unregister http-client-conduit` and so on. More likely is that they are used by something else you use (in which case `ghc-pkg unregister` will tell you what, in the form of a message about what would be broken).
Sure, you can try that; it's easy to wipe out your global package DB if it gets screwed up. In new versions of cabal-install (do a `cabal update &amp;&amp; cabal install cabal-install` to get it) you can have per-project sandboxes so your dependencies don't conflict and everything generally becomes easier. Assuming this is a library that I can work with in GHCi, the way I would try out this package might be like this: ¬ª cabal update ¬ª cabal get music-suite ¬ª cd music-suite-1.7.0.1 ¬ª cabal sandbox init ¬ª cabal install # or cabal install --only-dependencies or whatever we're supposed to do now ¬ª cabal repl Then you'll be given a GHCi session where you can play with the library. Alternatively you could create a new cabal project and add `music-suite` as a dependency and use sandboxes in a similar same way. HTH
I've been trying to use Yi for a while in emacs mode, but there was an issue with it freezing when I reload, meaning I can't add functions to my config :/
If this edX course is anything like [Erik Meijer's Haskell Lectures at Channel 9](http://channel9.msdn.com/Series/C9-Lectures-Erik-Meijer-Functional-Programming-Fundamentals) I'm sure it will be much better.
I don't really use GHCi --- I'm using GHC with the Sublime-Haskell combo to edit. But I know how to create a cabal project so I'll go that route. Thanks
I liked going through http://shuklan.com/haskell/
Because I don't have the entire output spew from cabal I can't tell you exactly what is happening here, but I will try to outline it in broad strokes. This usually happens because you already of a foo-version installed that has incompatible dependencies with those needed by music-suite. So cabal can reinstall foo-version linking against the dependencies music-suite wants, but this will break the packages (the ones you listed) that already depend on foo-version with the old dependencies. This is the "one instance of a particular version restriction" some, including myself, would like to see lifted. Until we can move towards a Nix style package management system, you have two options: sandboxes or manual "upgrading". Sandboxes are the easy way for executables, but don't work as well for libraries (IMO), so I will walk the alternative. First grab this script. https://github.com/glguy/GhcPkgUtils/blob/master/Unregister.hs This allows you to recursively unregister packages. No call it with conduit-1.0.17.1, http-conduit-2.0.0.8, http-client-conduit-0.2.0.1 then cabal install music-suite with conduit-1.0.17.1 http-conduit-2.0.0.8 http-client-conduit-0.2.0.1 Which might work, if the solver doesn't fail :p 
Got it, very helpful. Thanks. 
Without seeing the rest of your code this is only a guess. Your Distribution monad looks like an environment monad, which has a MonadFix instance. So, there's a fixed-point operator for it, see here: http://hackage.haskell.org/package/mtl-1.1.0.2/docs/src/Control-Monad-Reader.html#Reader Having said that, I'm guessing the issue you're having is that the environment is not being properly propagated (you might need a judicious call to "local" in the mtl parlance.) Or; the monad you need is actually a State monad, which again has a MonadFix instance. If you post a minimal working example, we can dig further in.
What was mentioned above: you want mfix. All papers from 2000 to 2002 on https://sites.google.com/site/leventerkok/ talk about this problem. Some good reading there.
It's to make sure that ghci runs under a well-known environment. ghci could trust your MinGW installation, but it might be outdated / too new / configured in a way problematic to ghci. Shipping an extra copy of MinGW makes these problems not appear in the first place.
It seems a bit broken... When I try to install it, I get a bunch of errors about missing Functor instances for stuff like ((-&gt;) b). Did anyone succeed in installing it?
Fantastic. I just wanted to keep reading that page of documentation, like a book I couldn't put down. For a long time I've wanted to get into music by way of programming... this might push me over the edge!
That's really awesome. Not sure what else to say!
hmm, we shouldn't be adding it to the PATH, that's bad.
From what I recall it's a full fledge GUI program, similar to emacs, except it has some pretty nifty parsing capabilities (more advanced than the already neat emacs indentation system) giving it on-the-fly subtree reindentation. I remember all the haskell gurus having a slow clap moment when they saw this. To impress a haskell programmer is not something usual.
I'd be more interested in adding a Haskell backend to Emacs. Emacs is well-designed, it's just elisp which is sucky.
That sucks pretty badly.
Dependent types are a pain. Emulating dependent types in a language that doesn't support them from the start is a whole world of pain. What for?
Really? Crap! Well then why even bother getting one? 
Lovely! Very straightforward. Note, you might just use `newtype` instead of `data` here, since there's only one field, the parser function; it probably won't affect performance, but could, so it's the standard thing to do. Adding a minimal bit of record syntax will give you the `parse` function immediately: newtype Parser a = Parser {parse :: String -&gt; [(a, String)]} or with the more standard, but slightly stale, naming practice: newtype Parser a = Parser {runParser :: String -&gt; [(a, String)]} 
&gt; or your project includes C source files Or if you install packages containing C source files (which is inevitable).
As a general rule, never force-reinstall. Recursively unregister and reinstall at once.
Thanks. Also the useful thing i found on your page was instructions on how to install ghc 7.8.2 on ubuntu :). I was just looking for it. 
How many of us need a course that covers architecture, optimization and advanced features of GHC's type system? (Phantom types, GADTs, Type Familes ...) If you someone taught that material on Edx, they'd set a record for student retention.
Here are the slides: https://speakerdeck.com/dmoverton/comonads-in-haskell 
&gt;Alternatively you could create a new cabal project and add music-suite as a dependency and use sandboxes in a similar same way. Do you mean adding the dependency to the cabal file and letting it get and install it in the sandbox? Also, say you grab and compile a library off of github. Is there a way to point cabal at that library and have it use it inside a sandbox for another project?
I've been learning both Haskell and music theory in my spare time, and I'm using this lib to implement some harmonic analysis stuff. Its a great library and its really helping me learn some more advanced Haskell :-)
If you have ghci then you probably also have ghc which uses linker from mingw suite to produce executables. At least this is the case for ghc up to 7.6.
I didn't say I didn't know it, I said I don't use it. I have certainly done the kinds of things you describe but for the most part I find it more convenient to use a regular editor and go through the edit/compile/run cycle. I'm not a fan of the command line. I didn't know about the +m option though --- that's actually good to know. Thanks.
To slowly extend Haskell so you get reasonable type inference and most of the benefits of dependent types. If possible, I thought this was a long term goal of many in the community, but I only have minimal knowledge of the community as a whole. For some people it is also because they have Haskell code bases and would like to benefit from additional type safety without introducing another language to their code base.
S'why I wrote it. Ping me if you need help, get stuck, or find anything confusing! Please! I'm using the feedback I get to improve Yorgey's course, this is critical! My contact info is here: https://github.com/bitemyapp/
Ah that seems reasonable. As a Haskell noobb I didn't even know that this is possible, however I already had mingw installed and although there might be version incompatibilities, I think it's cleaner to use the installed version instead of adding another version to path...
If I have another mingw already in PATH, is it really safe to just add the one that works with Haskell?
Thanks, but wouldn't it be easier to just call it from the Haskell subdirectory instead of adding it to PATH?
I am one of the developers of HaskForce. https://github.com/carymrobbins/intellij-haskforce This is a new plugin designed from the ground up with specific [goals](https://github.com/carymrobbins/intellij-haskforce/wiki) in mind. Note however there are several components that are imported from existing projects, namely the jps-plugin from Atsky's project and bits here and there from intellij-erlang. Right now the plugin works well for mostly editing and auto-formatting with stylish-haskell. There are some invalid red underlining of syntax, but we are working on a new parser which should resolve that as well as provide auto-completion. If you want to try out the plugin you can find the ~~nightly builds here~~ **edit**: [you can find the latest Github release here](https://github.com/carymrobbins/intellij-haskforce/releases). Be sure to temporarily uninstall any other Haskell plugin you have installed to avoid conflicts. This is generally an unstable version so there may be some risk involved; however, I haven't come across anything yet. If you are interested you are more than welcome to help out and submit some pull requests. We're developing this as a practical tool to fit our needs, and we're definitely interested in others who are looking for the same. Also, feel free to submit any issues that you come across via GitHub.
try playing this. http://codekinder.com/music/test.pdf sound familiar?
Wow! Thanks for posting this. I have been working very hard trying to discover the possibilities of using HsQML. I will have another post coming very soon with some details on modeling Haskell data in Qt Quick/QML. In that post I also talk about what I believe are the advantages using HsQML (sneak peak: https://github.com/creichert/hsqml-listmodel). As i mentioned in the post, the code is littered with MVar's and there is quite a bit of redundancy. I am open to suggestions and abstractions to fix some of this. I also want to thank /u/komadori, the author of HsQML. His continued correspondence has been extremely helpful and I'm hoping we can continue to develop and release more practical examples of Haskell user interfaces in HsQML. The Haskell community is one of a kind.
Yes that's what I mean. And to your second: you can use `cabal sandbox add-source local/src/dir.git`. Very useful.
Yes. I found an [open issue](https://github.com/music-suite/music-score/pull/256/commits) about this, and installing [armlesshobo](https://github.com/armlesshobo/music-score)'s version of music-score before installing music-suite resolved the problem.
I also always use sandboxes, but in this case I had to make an exception because their music compilation system is looking for their libraries in the global package-db.
I did succeed, but I didn't encounter this particular issue, only the "Ambiguous occurrence `tabulated'" issue. Maybe you have an old version of one of the dependencies and their version bounds aren't tight enough?
Thanks!
This sounds interesting and is probably worth looking at, buy the website was so annoying and difficult to use on my older phone that I gave up. :/
Well it all installed, but the example program doesn't work. That is kinda disappointing, I'm not sure what's wrong, seems like |&gt; is not defined. &gt; import Music.Prelude.Basic main = defaultMain music music = c |&gt; d |&gt; |&gt; e Seems simple enough, but I get: &gt; my_song.hs:4:19: parse error on input `|&gt;' 
here's a hint: Prelude&gt; 1 + + 1 &lt;interactive&gt;:3:5: parse error on input `+' 
I'd say the median Haskell salary is probably somewhere about the industry average for the experience level to 20% higher. It is a buyers market at the bottom end, but at the top end of the market that relationship inverts.
My biggest problem with Scala is that the compiler is broken and will probably never be fixed. I watched a few Paul Philips rants and was told that he was just bitter and you don't run into the problems he outlines every day. I started a job with a startup this week. Yesterday I sat down to my first day of Scala. Tried to fix some inexhaustive pattern match problems that lead to wanting to use the monad instance of either. For comprehensions do not work if you want to extract a tuple. Compiler bug since 2012. Apparently it's really hard to fix because of injected methods here there and everywhere. I am now scared for my sanity. See bug [here](https://issues.scala-lang.org/browse/SI-5589) Edit: grammar
What ever happened to Leksah?
It's great to see another example! In terms of abstractions, I'm sure you could use one of GHC's generics libraries to generate the field accessor/mutator members for your record types. The cumbersome aspect is that to auto-generate distinct signal keys you would need to change to use the newSignalKey and newClass functions rather than DefaultClass (or use unsafePerformIO, or use Template Haskell). Also, rather than have each field in your record be an MVar, you could just as easily define a QML class for MVar YourRecordType (i.e. use ObjRef (MVar YourRecordType)). That might be tidier.
Yeah, it'd be disappointing if Haskell didn't move to use dependant types some time soon.
Maybe. Arrrgh, cabal. Is the Functor instance for ((-&gt;) r) supposed to be in Prelude? Control.Monad.Instances appears to be deprecated, but that's where it is on my system. Maybe I just need to update my Haskell Platform. I'll check it later today.
&gt; UDP the idea was, that it's easier to punch firewalls with UDP. freenet 0.5/0.6 used TCP
Can you please provide more details of the problems you are having? It was my understanding that oauth is well-supported in the Yesod ecosystem. Here is one data point: one of the default authentication plugins provided by [yesod-auth](http://hackage.haskell.org/package/yesod-auth), and one of the most commonly used, is the Google Email plugin. Now that Google has [deprecated](https://developers.google.com/+/api/auth-migration) the original OpenID interface for that service, recent versions of yesod-auth now use the OAuth2 interface. Are you able to use that successfully? EDIT: Michael just [posted](http://www.yesodweb.com/blog/2014/06/google-email2) about this on his blog.
It works if you use `\/` from `scalaz`, but that means either conversions all over the shop. I also haven't quite gotten the hang of how implicits are managed so it's a hard library to use. I'll get there I'm sure.
Are you selling?
If haskell programmers are more expensive than the equivalent in other languages (which may or may not be the case), letting them loose to write Haskell at work will pay back that premium many times over in my experience.
I didn't chose the code base :) I do have plans of carving parts out and rewriting it in Haskell if I can.
That could be built. The problem is that a reload causes you to lose all of the bindings you have created in the REPL. That issue is wearing out my up-arrow and enter keys, and it makes me think twice before reloading sometimes.
Having used both the IDE workflow and the emacs/vim workflow quite extensively, I can say that it's possible to work extremely efficiently with either one. But for people who grew up working exclusively with IDEs, there is a significant learning curve for the emacs/vim workflow. So in the end, that also boils down to the "hit the ground running" point.
They could temporarily be inconsistent and just give you annoying error noise that doesn't help. Having some way to ask it to auto-reload in addition to a manual reload would be nice though.
Huh. It appears that Ubuntu 12.04 LTS only has GHC 7.4.1 in the package repo... I guess I could always get a newer version of Ubuntu. Seems like a bit of overkill for one Haskell library, but then again, I'll have to upgrade eventually.
&gt; Haskell has far superior library support than Scala and Java. This part is also very debatable. IMO, library support is one of Haskell's biggest weaknesses, and is one of the few areas where Java is clearly superior to Haskell.
It can be built. I have. :) It also retains your bindings when you reload.
I always keep auto-reload on in our REPL. It only reloads when you do a command in the REPL, so you rarely get any noise.
At [Suite Solutions](http://www.suite-sol.com) one of our specialties is technology training and webinars. Our leading products are developed in Haskell, and we have built a solid team of Haskell developers. We'd be thrilled to combine those two capabilities and join your training effort. [Let us know](http://www.suite-sol.com/pages/contact/) if you're interested.
Oh, fair enough then :)
I have 3 whole Haskels to sell 
&gt; Haskell has no IDE to speak of, or at least nothing that comes anywhere close to IntelliJ and ScalaIDE. Why does this regularily come up as an argument against a programming language? Why is it such a bad thing not to *require* any IDE to make a programming language bearable? For languages such as `Java` I can see how an IDE compensates for lack of expressivity and language features, but languages such as Python, Ruby or Haskell are usable just fine with a simple text-editor providing syntax-highlighting and indentation support IMHO. Having additional features (like e.g. provided by Emacs with `ghc-mod`) are nice to have, but I don't depend on them to be productive and get things done.
The use of "guys" does not necessarily imply a certain gender. [wiktionary](http://en.wiktionary.org/wiki/guys)
Well, keeping in mind that a big source of Haskell jobs right now is probably quants, I would expect they make pretty good money on average.
The answer is really going to depend on what domain you work in. If you write compilers or do numeric programming, Haskell has a great ecosystem. If you work a lot with data processing or Java web frameworks then Scala shines. 
Web development is not a "normal everyday stuff" for all programmers. It's multi-disciplinary which makes it a fun activity but honestly, with the exception of some highly specialized domains (compiler architect, operating systems, file systems, data science, AI, etc...) most programming is "down to earth". Many programmers (I see this happen with embedded guys and web developers a lot) think the world of software is flat. I'll be honest, doing web development in Haskell as a full-time job will be hard to find unless you're freelancing (and can choose to use it on your own).
In my mind, I don't hire Haskell programmers for vanilla programming work, I hire one for work requiring a level of sagacity --- *learning and using Haskell productively* is a strong litmus test for such a cognitive quality. In addition to that are all of the benefits of using the language.
Kids are being raised on IDEs now from the moment they start programming. Once they become dependent on them, they never go back. I blame java, personally for being popular and basically requiring an IDE to function.
This is not quite so cut and dry. I attempted some scala stuff at one point and what I noticed is that scala doesn't actually write its own libraries even for basic things most of the time because there are often java ones that more or less work even though they are terrible. The one I had the biggest problem with was just downloading files via HTTP. Scala libraries like dispatch are just wrappers around java libraries. The java libraries are all horribly multithreaded pieces of shit that randomly break all the time, and require tons of customization to support things like not downloading files entirely into memory before flushing to disk, all the customization of which must be done in java style. In haskell I could have used pipes or conduit libraries or even made my own in a few hours, instead I wasted a week trying to do it in scala before deciding it wasn't worth it.
Somewhere between 'lovely' and 'mind blowing'.
Well I was just using web development as an example. Others are sys admin, writing various daemons or networking code, web scraping, internal tools. Things that I code in my day job with bad languages that cause all sorts of trouble. But most jobs I see for haskell are things like super math heavy like financial or stock analysis, or require huge amounts of academic knowledge about some subject like compiler development. I'm not too interested in that stuff.
This is insane. I love it.
"guys" is a form of address for a group of people, irrespective of gender: b : person ‚Äîused in plural to refer to the members of a group regardless of sex &lt;saw her and the rest of the guys&gt; I am not interested in an extended discussion of gender politics. But please do not correct what is not incorrect.
This is rather silly statement: knowing haskell doesn't imply "sagacity." It's one of those blanket statements that imply ignorance. The other counterpoint is that if your environment requires haskell, you're going to hire haskell programmers at all levels, if you can find them. 
We don't really have any open source at the moment apart from the SQL parser. I really hope we can release more code some time this year.
I will pass on the feedback. I don't really understand the website either.
I'm scared.
Cleaner, but any version incompatibility can go wrong in subtle ways. It's happened to me.
&gt; In our next installment, we‚Äôll tackle Applicative parsing with type lenses. Thought the lens package had too many operators??? You ‚Äòaint seen ‚Äònothin yet.
About 9 months ago I applied for a job advertised as a python role, but intended to be large "platform" type of project. I applied and in the course of talking to the founders, I shared my reservations of building a big project in python and "sold" Haskell to them for the following reasons: * Safety - A large project in python would easily become as bug ridden as their exisiting asp.Net project. Haskell is more rigorous and easier to reason about * Fit for the problem domain. We're building a realtime, event base system to model capacity in a network. Haskell eats streams for breakfast. * My belief that Haskellers are a superior community to go hunting for programmers in. Your typical Haskeller has spent a lot of time and effort teaching themselves a non-mainstream technology purely for self education as there aren't many jobs available. Therefore even without the above two point hiring Haskellers and letting them do Haskell makes a HUGE amount of sense. Point three is the polar opposite of the mainstream view of Haskell. I think most people/businesses accept that Haskellers are smart (I know that it's moved my CV up the pile on more than one occasion), but they totally miss that there's a large pool of under served people who would bite off your arm for a Haskell job. This might be why there's a perception that Haskellers are paid badly - that and the fact that a lot of the Haskell jobs are in academia and therefore not well paid. Based on my arguements they agreed and we set out to hire some Haskellers. That belief in an untapped pool of folks who want Haskell jobs was completely validated by the response to the job ads I put out. I had a huge number of exceptional people to chose from and hiring great people was cheap, fast and easy. 9 months on, we've built one of the best pieces of pure technology that I've every worked on. Maintenance and adding functionality is still quick and regressions are much rarer than I'd expect in the likes of python or C#. We didn't scrimp on salary, but I'd still say we got more than our money's worth! As an aside - if any of you find yourselves in a similar position to the one I was in 9 months ago and would like my help in getting Haskell into an organisation, please do reach out - I'd be happy to help!
I 100% agree with this.
He's a witch! BURN HIM!!
Now *that's a silly statement*. I totally agree that opinions and facts backed by good data and analysis is always preferred. I also agree that without said data and analysis, it moves opinions and arguments and debates into the subjective and sometimes anecdotal domain (not everyone has past experience to draw upon). However. Saying it has no real or tangible value to anyone outside of myself is, itself, a blanket statement (with no hard numbers or analysis) that you are so vehemently opposing. Sure, there will be plenty of people (you are a good example) that find my "generalizations" and *a priori* logic absurd or lacking value; there will also be plenty of people that find it valuable or "valid to a degree". The more people that show me other anecdotal examples of counterparts or refutations to my opinions and personal experience, the more it will shape these opinions. With that said, why not offer me a different anecdote instead of saying "you're wrong"? Arguing about the rightness or wrongness of generalizations is now off-topic and it's obvious we both make them.
I really would rather not lose type inference and it is not clear to me that their is an accepted method of getting dependent types with out losing type inference yet. As for the most part only an end user I look forward to using using such an integration.
Most people use "real time" to mean "fast"--choose your definition of "fast"--instead of a hard constraint on how long a computation has to complete. 
*GHC* and *hard* real time are a poor fit, mainly because of GC and the difficulties in obtaining fine-grained guarantees about space and time constraints in a lazily evaluated language. Compilers with different evaluation models and runtimes with different GC behavior, etc, might make the Haskell *language* more viable in hard real time systems. GHC can be a fine fit for *soft* real time systems where some deadlines can be missed. I know that, e.g., Galois does quite a bit with systems that deliver consistent 1-5ms response times. There are also projects like atom, cryptol, etc.
Yes, Haskell is not very good at using Java web framework libraries. Totally agree with you there.
I'd avoid `ImpredicativeTypes` and just use a newtype wrapper with `RankNTypes`. newtype Visitable v = Visitable { visit :: forall r. v r -&gt; r } type Constructor v = v (Visitable v) I've never had much success with impredicative types, I feel like you have to help the typechecker in very unintuitive ways to make it work. Just use newtypes and it's clear where you are giving the typechecker help--everywhere you wrap/unwrap the newtype.
There's no need to get into a knot over this. I simply disagree with your statements. I've laid out my reasoning behind it. You're free to disagree at any rational or irrational level that you so desire. 
These kinds of arguments fit in HN better. Please don't go around "sillying" other people's comments.
Yeah, I get the "Not in scope: `defaultMain'" error too. I can run some of the examples from the github repo, but not the ones on the website...
So, I finally succeeded in installing this thing, and after playing around with it for half an hour, the basic example from the webpage still doesn't work. I managed to get this piece of code accepted by ghci: music = c |&gt; d |&gt; e By adding the following type constraints: music :: (Semigroup a, HasPosition a, Transformable a, IsPitch a) =&gt; a That can't be the intended behavior... On top of that, defaultMain doesn't seem to exist.
Sarcasm noted. But atleast scala is better in data processing, especially big data processing with hadoop/spark etc. Haskell doesn't seem to have as much of a competitive advantage here. Agree?
Its often argued that Scala's strong point is the jvm (and not java libraries, there are a few good java libraries for sure, but most are pretty horrible to work with) Clojure also proclaims jvm as a plus point. By saying jvm is a plus point, i assume they mean that there is lots of profiling and introspection infrastructure for the jvm. I would like to see how people manage haskell clusters of a few hundred machines.
Oh. [This](https://github.com/music-suite/music-score/issues/254) and [this](https://github.com/music-suite/music-score/issues/81) explains why defaultMain isn't there, and yeah, open works.
Yeah, I'm selling.
If this turns out to be a useful Haskell frontend to Lilypond then the author is my hero!
I loved it
There's a simpler solution: import Lens.Family (over) import Lens.Family.Stock (_Left, _Right) over _Left :: (a -&gt; b) -&gt; Either a x -&gt; Either b x over _Right :: (a -&gt; b) -&gt; Either x a -&gt; Either x b
&gt; Sure, Hoogle et al allow you to quickly lookup the meaning of &lt;&lt;+= but with an IDE you simply hover over said symbol, click, and voila And where can i "hover the mouse" to find functions of type [a] -&gt; (a,a) ? 
I have no idea how the type-params library provides supercompilation? edit: aha and this example doesn't. it is a category error to call any single optimization or set of optimizations 'supercompilation'. supercompilation is not an optimization, it is a technique whose use subsumes basically all optimizations. there's some compile-time evaluation and specialization here, but that's not at all the same thing.
Looking forward to an CUFP talk at an upcoming ICFP!
I might be abusing the term because I couldn't find a definitive definition. I've seen people use it to mean something like: the compiler evaluates expressions at compile time rather than waiting for runtime. I've also seen it to mean that the programmer must supervise the compilation steps to get this effect to happen. Both of those things are going on in the Lp-norm example (admitedly in a contrived scenario). If that doesn't qualify as supercompilation, then what exactly does the term refer to?
Top info! Thx for sharing
Yes. First, you define traversals for each constructor of types: _a :: Traversal' (Many a b c d e f g) a ... _g :: Traversal' (Many a b c d e f g) g ... which you can do automatically using `makeTraversals` from `lens-family-th`. Then you would write: over (_b._g._c) length
I think this is a different sense of "Real Time", which is the new opposite of "batched". You used to call the opposite of "batched" "on-line", but there are a lot more people who will get the wrong idea there than know the other meaning of real-time.
We often understand the technical/academic sense of words, yet use them informally in discussions. It's not as if it's a wholly-different meaning, either; the difference between "batched processing" and "real time processing" in the data processing sense is that there are time bounds on the "real time" case and you are willing to sacrifice throughput to meet them. It's the same concept as in real-time operating systems, it just has much longer absolute bounds on processing time.
hmm... I don't understand, but that's probably my own ignorance. What exactly is an open expression and speculative case splitting?
I.e. some examples in ["Supercompilation by evaluation"](http://research.microsoft.com/en-us/um/people/simonpj/papers/supercompilation/supercomp-by-eval.pdf). Not sure this is the best intro to the topic in general, but walks through an example of how you "split" a free variable into the values it could possibly take, and then make optimizations based on that info. For booleans this is simple: if a then (False &amp;&amp; True) else a would probably be partially evaluated to something like this: if a then False else a But supercompilation will split on a and try out the two cases if True then False else True if False then False else False then hopefully simplify the entire expression to False. Partial evaluation can't do this because a is only known at run-time. Things get hairy, though, with fancier types, recursion, etc.
Sometimes it's because of the default size of the garbage collector nursery : it's still the same for 2, 3 ... N capabilities. You can check the time taken by the garbage collector with +RTS - s and change the default size of the nursery with +RTS -A.
Given that `lens-family` and `lens` are essentially the exact same representation, it should come as no surprise that what one can do, the other can also do. Of course it can be another question entirely whether some given functionality has been implemented or not.
[UndecidableInstances?](https://i.imgflip.com/9e3bh.jpg)
&gt;By passing in a lens, you are essentially passing in your own functor dictionary. IDK what you mean. The `TypeLens` doesn't contain a dictionary. It's just a singleton type so that the compiler can select a dictionary. Same as any other type signature. &gt; So I guess my question is, what does this new-functor-typeclass approach give us over the over approach? With `Functor`? Probably nothing. The `TypeLens` is just what I created so that I could easily extract constants from the types for specializing functions. (See the Lp-norm example in the README). Nothing from the `lens` package seemed up to that task (or my understanding wasn't up to the task). I just think it's cute that the `TypeLens` can be used for a bunch of other things as well.
Yeah. I just didn't want people to think they needed to pull in all of `lens` for this neat feature.
You can definitely manually define those traversals, but `lens-family-th` doesn't yet support doing what you are asking. I can give you an example of how to do it for the `c` type variable and you can probably infer how to do it for the other traversals from this: _c :: Traversal' (Many a b c d e f g) c _c k (A a) = pure (A a) _c k (B a b) = pure (B a b) _c k (C a b c) = fmap (\c' -&gt; C a b c') (k c) _c k (D a b c d) = fmap (\c' -&gt; D a b c' d) (k c) _c k (E a b c d e) = fmap (\c' -&gt; E a b c' d e) (k c) _c k (F a b c d e f) = fmap (\c' -&gt; F a b c' d e f) (k c) _c k (G a b c d e f g) = fmap (\c' -&gt; G a b c' d e f g) (k c)
If they don't mind self-study, "Typeclassopedia" and "Parallel and Concurrent Programming in Haskell" (both free).
For something like `return`, there is `Control.Lens.re`. Given the example we've been running with so far: {-# LANGUAGE TemplateHaskell #-} import Control.Lens data Many a b c d e f g = A a | B b | C c | D d | E e | F f | G g makePrisms ''Many We can create a Getter out of a prism using re, a getter which seems rather backwards because instead of going from the whole to the part, it goes from the part to the whole. 3 ^. re _A :: Many Int b c d e f g Let me say that another way. Given a prism generated for a constructor `A`, we can recover that constructor using `(^. re _A)`. Now that we've got analogues for `fmap` and `return`, we just need the analog for `join`, although I haven't quite wrapped my brain around what that would actually look like.
This isn't about the REPL, but I'm sorry that Haskell doesn't have the concept of docstrings. Type signature is not the same thing.
I'm sure it's possible to have type inference remain for non-dependent types while still having dependent types
I really dislike the definition of fixity &amp; precedence in the Haskell standard. IMO operators should be ordered by a partial order, with source specifying the relative precedence of different operators. For example, some of the Prelude's fixity declarations could be replaced by infixl + below * above == infixl - equal + infix ==, /=, &lt;, &lt;=, &gt;, &gt;= above $ With these declarations: w == x + y - z =&gt; w == ((x + y) - z) w == x + y $ z =&gt; (w == (x + y)) $ z x ! y + z =&gt; parse error, no fixity comparison defined between ! and + x ! (y + z) =&gt; x ! (y + z) -- no need to define if it's unambiguous x == y &lt;= z =&gt; parse error, == and &lt;= have equal precedence and at least one has no associativity defined infix ! above * below + =&gt; fixity consistency error, * &lt; ! &lt; + &lt; * infix ! below + =&gt; ok x ! y * z =&gt; x &lt;&gt; (y * z) -- since ! &lt; + &lt; * x ! y == z =&gt; parse error, no fixity comparison defined between ! and == infix ! below == =&gt; ok x ! y == z =&gt; x ! (y == z) 
Is it just that turning on the parallel flags means you pay for extra locking throughout the RTS as you switch on threaded? In single threaded mode you don't need fences/locks for almost anything
What was the motivation for the `export` function? What are the advantages compared to the FFI's `foreign export` and `foreign export dynamic` / `foreign import wrapper`?
Non-answer, inverting the question: why turn on parallel flags for a non-parallel program? :P
That's probably the reason more than anything. The threaded runtime system has some additional locking and latency overhead vs the single-threaded runtime, and some things will perform worse, especially when no parallelism is involved. Also: `-N` by default implies usage of the parallel garbage collector, but it is almost *always* going to be a loss for single-threaded programs I imagine*, due to the fact the work balancing will cause inter-core communication thrashes. If you're only using one core, but the GC kicks in and starts balancing all the work across N cores, does the GC, then goes back to one thread - all you've really done is split up the heap and caused needless intercommunication. \* I say 'I imagine' as I haven't verified it, but I'd speculate this will probably be an issue unless you have gigantic heap-sizes or something, where the parallel GC might offset those costs. /u/simonmar can likely verify this or point out I'm a liar.
The key thing about supercompilation, intuitively, is you just start running the program, and every time you can't figure out what to do next you branch on what your input might be and _keep going_. This is the same thing as the "case splitting" described, but I think it gives a clearer account. So supercompilation is to partial evaluation as a tank is to a hatchback car. Furthermore it evaluates _everything_. It helps to look at futamura projections here: http://blog.sigfpe.com/2009/05/three-projections-of-doctor-futamura.html The "super" in "supercompilation" as I understand it doesn't stand for "awesome" as in "awesome compilation" but for "supervision" as in "supervised compilation" -- so there's something standing above the compiler, directing it and managing it to keep trying to unroll all paths.
It's a practical argument. I'm currently using Scala with MATLAB thanks to the latter's Java/JVM compatibility.
I know a couple guys who might be interested in a remote work position. What's the job? (I'm not a recruiter or anything like that, these are just friends.)
Okay, I've got it. Consider: prism :: (b -&gt; t) -&gt; (s -&gt; Either t a) -&gt; Prism s t a b A prism is basically made up of two functions, a "constructor" (`b -&gt; t`) and a "destructor" (`s -&gt; Either t a`). The destructor either succeeds and gives you the destructed value, `a`, or else it fails and gives you `t`, which is usually the unchanged value, cast as a different, compatible type with the expected output. So suppose we can pull that destructor back out of the prism. getDestructor :: APrism s t a b -&gt; s -&gt; Either t a With that, we can write the generalized version of `aJoin`. flattening :: APrism s t t b -&gt; s -&gt; t flattening prism s = either id id (getDestructor prism s) Notice how usually we have `s t a b`, but in this case `a = t` so we have `s t t b`. That means that the destructor will produce an `Either t t`, and we can just pull out the `t` without caring which side it came from. So there. I believe that `flattening` is the join-like operation we are looking for. See this lpaste for the implementation of getDestructor: http://lpaste.net/105223
That sounds like a pretty cool idea! Can you think of any problems that you might encounter if trying to implement this as a GHC extension? (Not an implementation problem; more interested in semantics issues, such as unresolvable conflicts with the Haskell standard.)
So that's exactly what I'm imagining is going on in the Lp-norm example in the readme. The programmer has to manually provide the values on which to split (this is the supervision), and then the compiler will generate functions that are preoptimized for those decisions.
TIL: Indexed monads and [parametrized monads](http://bentnib.org/paramnotions-jfp.pdf) are quite different.
Wish I could upvote twice. You should really post this somewhere besides a comment -- it's a really great idea and I don't think I've seen other proposals like it. The fixity numbers are so arbitrary. Your design feels much more in line with Haskell and is (in hindsight, of course!) so obvious.
I went to this talk! Maybe the hackers at Haskell can help me a bit more with a question I had? Isn't Tagless Final style simply dual to OOP and existentially quantified over type classes? Tagless Final Style seems like a way to encode open sum types and existentially quantified over type classes seem to be a way to encode open record types. Some pseudo-code for the OOP emulation with type classes I'm talking about: {-# LANGUAGE ExistentialQuantification, ConstraintKinds, Rank2Types, MultiParamTypeClasses, FlexibleInstances, FunctionalDependencies, UndecidableInstances #-} module Main where import Control.Applicative ((&lt;$&gt;)) import Data.IORef -- Demo main :: IO () main = do window &lt;- newWindow print =&lt;&lt; window .? title .? readSource print =&lt;&lt; window .? size .? readSource print =&lt;&lt; window .? position .? readSource window .? position .? writeSink (3, 232) print =&lt;&lt; window .? position .? readSource -- Demo GUI library class Widget self where position :: self -&gt; Object (Varying (Integer, Integer)) size :: self -&gt; Object (Signal (Integer, Integer)) class Widget self =&gt; Window self where title :: self -&gt; Object (Signal String) data WindowImpl = WindowImpl (Object (Varying (Integer, Integer))) newWindow :: IO (Object Window) newWindow = do pos &lt;- newVarying (4, 32) return $ Object $ WindowImpl pos instance Widget WindowImpl where position (WindowImpl pos) = pos size _ = constantSignal (400, 600) instance Window WindowImpl where title _ = constantSignal "window" -- Demo OOP implementation data Object c = forall a. c a =&gt; Object a (.?) :: Object c -&gt; (forall a. c a =&gt; a -&gt; b) -&gt; b Object x .? f = f x -- Demo FRPish fluff class Signal result self | self -&gt; result where readSource :: self -&gt; IO result waitForSourceUpdate :: self -&gt; IO () class Sink result self | self -&gt; result where writeSink :: result -&gt; self -&gt; IO () class (Signal result self, Sink result self) =&gt; Varying result self | self -&gt; result instance (Signal result self, Sink result self) =&gt; Varying result self data VaryingImpl a = VaryingImpl !(IORef a) !WaitQueue newVarying :: a -&gt; IO (Object (Varying a)) newVarying value = do ref &lt;- newIORef value waitQueue &lt;- newWaitQueue return $ Object $ VaryingImpl ref waitQueue instance Signal a (VaryingImpl a) where readSource (VaryingImpl ref _) = readIORef ref waitForSourceUpdate (VaryingImpl _ waitQueue) = waitOnWaitQueue waitQueue instance Sink a (VaryingImpl a) where writeSink value (VaryingImpl ref waitQueue) = do atomicWriteIORef ref value signalAllWaitQueue waitQueue data ConstantSignal a = ConstantSignal !a constantSignal :: a -&gt; Object (Signal a) constantSignal constant = Object $ ConstantSignal constant instance Signal a (ConstantSignal a) where readSource (ConstantSignal constant) = return constant -- Go into an infinite loop and black hole the thread waitForSourceUpdate = waitForSourceUpdate -- Utility code data WaitQueue = WaitQueue newWaitQueue :: IO WaitQueue newWaitQueue = return WaitQueue waitOnWaitQueue :: WaitQueue -&gt; IO () waitOnWaitQueue _ = return () signalAllWaitQueue :: WaitQueue -&gt; IO () signalAllWaitQueue _ = return () 
I typically use `:info` in the repl to match the fixity up against the existing operators.
What about those applicatives that aren't monads? What about those types with multiple valid applicative definitions!? ;)
It's all lenses. Lens can represent all of them. Lens is the silver bullet. It's going to solve everything. Cure to cancer, no more hunger, and world peace are expected by lens 6.0. :P
Would it be fair to describe what I've done as "light weight supercompilation" in the same way that type nats are "light weight dependent types"?
We do many things nowadays not like we were used to. We drive cars instead of riding horses, we use microwave ovens instead of cooking in open fire. That's called technological progress. Of course, some people may not need speed and comfort of cars; or maybe someone doesn't like dependency on electricity that comes with a microwave. But it doesn't matter: the fact is that these things are superior to their predecessors; that's objective reality.
Let's imagine you want to find all usages of function "parse" in your project. You'll probably use something like cscope, but there are 20 different parsers in the codebase, and each one of them has a function "parse". And you care only about one particular parser. You'll have to go through 20 times more snippets of code than you need; someone with an IDE will be 20 times more productive than you (in this scenario).
How do we deal with situations like: - a below b - b below c - c below a for 3 operators a, b and c? Do you build a graph of operator ordering at compile time to test for cycles? If so, how do you deal with cycles only appearing with specific imports (actually if the cycle isn't in default imports, it will turn out to be always hidden until specific imports are done)? Do you load all libraries and check for cycles, or do you accept any fixity definition until a cycle is discovered? If you simply accept cycles, with expressions like: E1 a E2 b E3 c E4 How do you resolve priority?
Here's a handful, off the top of the head: - Scala takes after OcaML in a few ways - mutation is allowed, but discouraged, and laziness is opt-in. There are at least a few people that find this pragmatic, I think. - Scala's case classes and sealed traits have some advantages over Haskell's sum-of-products representation when dealing with complicated structures. In many cases, Haskell has to wrap / unwrap values that could be passed directly in Scala. There are many places where subtyping is a natural way to express relationships, and in those places the Scala representation is often simpler. - Scala's implicit arguments are very powerful. They're often used to implement a Scala-equivalent to typeclasses, but are more flexible and can be overridden depending on context. It also has implicit conversions, which are occasionally abused but can help to scrap a lot of boilerplate transformations. (Roughly what OverloadedStrings gets you, but for all data types.) I think other folks have documented a number of places where Scala breaks down. (And it breaks down a *lot* if you're trying to do serious pure-functional programming.) But overall I find it a quite pleasant language to work in.
Given a cycle they just become incomparable again with a complaint about the cycle.
Presumably the *point* is that Scala is better suited to web development than Haskell, Snap and Yesod notwithstanding (both are of interest here to this current non-Haskeller) Haskell has some catching up to do in this department, IMO. Certainly on the database front there are some glaring holes to be filled. For example, Persistent and Esqueleto lead the way in Haskell type safe query land, yet *no Oracle or SQL Server support*, yikes. In Scala we can just piggy back on JDBC as the foundation of our superior query DSLs (ScalaQuery, Squeryl, sqlTyped, among others), or connection pooling libraries like [HikariCP](https://github.com/brettwooldridge/HikariCP) that b-low away Haskell on the performance front, right out of the box, no tuning/hedging around laziness required. Basically the enterprise has funded great swaths of functionality that you get, de facto, fo' free just by being on the JVM, in Haskell it mostly has to be built from scratch. For example, not sure what the flagship Date/Time library is in Haskell, but I'm probably not going out on a limb when I say that JodaTime is already better. Or, PDF generation, customer invoices look like shite in plain html, right? Right, so we just grab FlyingSaucer and voila, gorgeous on-the-fly PDFs. Contrary to TM's blog posting, there are some JVM diamonds in the rough that we can breezily tap into in Scala. Anyway, yes, Haskell wins the scientific computing battle, the job postings centered almost exclusively around financials point to this fact.
You can also warn about fixity declarations when the declaration affects the comparison of symbols that aren't defined in the current module, similar to orphan instances.
`showsPrec` relies on totally ordered integer fixities to decide on parenthesization, and is usually automatically written via `deriving` clauses. Most instances only care about the precedence of application, however.
Only I have this problem? I am using ghc-7.8.2 in Ubuntu GNU/Linux 14.04 X86_64. https://gist.github.com/eccstartup/ee03e5e1032be5710cc3
I think the problem is that the inner `x` in your `fixM` definition does not get the same random generator passed as the outer one, but a split version. So they're not actually generating the same values. I expect borrowing the definition of `mfix` for `Reader` that others have mentioned will work, though; it takes care to share the environment value.
Typed final tagless form is really interesting. I have given a talk based on a [literate Haskell file](https://github.com/ggreif/kaleidoscope-regensburg/blob/master/ggreif/final-tagless.lhs) at [Regensburg Haskellers meetup](http://www.meetup.com/Regensburg-Haskell-Meetup/events/181549732/).
First, you do not define any fixity 0 operators that clash with idiomatic `$` use. I'm looking at you `Text.Parsec.&lt;?&gt;` ‡≤†_‡≤†
I know about 25 guys that would be interested in remote work. Unfortunately, we're not looking for that right now. If you know someone who's in Norway, or happens to want to move to Norway, let me know! The job is (initially) educational software &amp; game hacking in Haskell. (though if the employee can convince me that using Coq or PureScript *really* makes sense for that one module -- go ahead!) So it's back-end Web stuff.
This is really cool‚Äîhas a Haste-React interop library gone any further than html generation?
Is there a published library or just the code in the lecture notes?
What software do you need to follow this? I wonder if it can be followed by using only free software, no Adobe Flash or Microsoft Silverlight etc. I'm a bit past needing an introduction to FP, but I wouldn't mind folowing this course anyway. I could recommend it to others if it's any good.
It doesn't appear like the naming has been standardised. Certainly Coner McBride's definition of an "indexed monad" is very different to Dominic Orchard's that I linked to.
No, because that is also not apropos of type level nats.
I try to do the same when I can, but, what if the DSL operations don't match up nicely with existing operators in the prelude? I'd love to hear about some specific cases of designing DSLs and how the fixities of the operators ended up the way they are. I really like the idea of partial ordering that /u/ryani posted above, but that functionality doesn't (directly) exist in Haskell right now. But, maybe a solution is to try to define a partial ordering against existing operators (and the DSL-specific ones) and then match those up to actual numbers? I'm just spitballing off of some responses here, though. It'd be cool if anyone had specifics on getting the fixities right in their DSL of choice, I'd love to hear it!
Note that the `foldl` package has something very similar in the `FoldM` type, defined like this: data FoldM m a b = forall x . FoldM (x -&gt; a -&gt; m x) (m x) (x -&gt; m b) It turns out that if you write it this way then you can make `FoldM` an applicative.
Hi, thanks for sharing your work! I'm a really beginner in Haskell and just would like to use your program on my Fedora. Do I have to install the full haskell-platform package, which takes more than 650 Mb once installed? I have a pretty old machine with not much free space‚Ä¶
The partial order idea was discussed even as Haskell was first designed. In the end we went for something simple.
I think that "partial order against existing operators" is a more detailed outline of my typical mode. I usually think about how it should interact with $, ., and the Applicative operators. Then I map it to numbers.
Interesting. Do you remember why the choice was made that you can't specify precedences equal or higher than function application?
So I have a couple of fixity 0 operators in the DSL I'm designing right now, but it seems like this is the only real way that it makes sense. Basically it's a music thing, and, for example, the operation (with fixity 0) `n !&gt; m` sets the tempo of `m` to `n`. Do you think this clashes with `$`? Should I avoid using fixity 0 operators altogether? If so, why? I haven't encountered any issues so far, but I want to follow good practices!
I think having function application highest was just the way most (all?) other functional languages were at the time. 
That's right! It's basically an effectful left fold, with all the individual components preserved so that they can be combined using `Applicative` style.
I would expect protocol-buffers to destroy aeson in a benchmark. JSON isn't designed so much for speed as for readability and JavaScript compatibility. Surely the only reason there has been no such benchmark is because they serve different purposes.
Sorry you found the experience frustrating. Does now knowing about the ``cabal repl`` command fix the problem that you had? As far as adding new dependencies go, modify the .cabal file with the package and version. cabal clean # wipe out temporary files cabal configure # reconfigure cabal install --only-dependencies # install new dependencies cabal repl # reload ghci There a couple guides to the modern cabal workflow: * [Getting Started with Haskell](http://bob.ippoli.to/archives/2013/01/11/getting-started-with-haskell/#install-cabal-dev) * [What I Wish I Knew about: Cabal](http://dev.stephendiehl.com/hask/#cabal)
I've got a basic [TodoMVC](http://todomvc.com/) working with Haste+React. I just need to do some of the obvious low-hanging optimizations. Right now I'm doing very un-clever things like installing every possible event listener on every single DOM element. When that's sorted out, I can put up a link.
&gt; Does now knowing about the cabal repl command fix the problem that you had? If I run 'cabal repl' in the root directory of the project (where I did 'cabal sandbox init' and all that), I get "Preprocessing library &lt;library name&gt;..." and then nothing happens. If I do it in one of the subdirectories of the project it launches GHCi but it is equally unable to find the packages corresponding to the imports. 
``cabal repl`` doesn't do anything when you run it? It should launch a GHCi shell in the scope of the sandbox. Sure you have the ``cabal.sandbox.config`` and ``yourproject.cabal`` in the cwd? You will need to either be at the root of your project to use that command or manually specify the path with ``--sandbox-config-file=FILE`` if you want to use it from a subdirectory.
The only new learnings by putting the benchmark together is concerning the current efficiency of the Haskell PB impl vs the current Aeson impl. Which in and of itself may a sufficient reason to put together a bench suite. There is quite a bit of info out there already regarding PB vs Thrift vs ... vs Json in general and the relative performance differences.
A benchmark doesn't make sense if you don't want to compare certain implementations, that's the whole point of doing a benchmark. I'm actually interested in those specific packages (look at the title :)
I don't see why you can't just call it "directed partial evaluation and specialization" -- this explains what's going on much more clearly, if with a few more words?
&gt; Tagless Final Style &gt; Discovered by Oleg Kiselyov &gt; But don't be scared. lol
TBH, I'm not very knowledgeable about the haskell-platform (most of it is a maintenance nightmare for me) and even less about what package options you have in Fedora‚Ä¶ Good luck ^ ^ 
Very nice! I was able to write these translations between `FoldM` and `Sink`: -- | Convert a 'Control.Foldl.FoldM' fold abstraction into a Sink. -- -- NOTE: This requires ImpredicativeTypes in the code that uses it. -- -- &gt;&gt;&gt; fromFoldM (FoldM ((return .) . (+)) (return 0) return) $ yieldMany [1..10] -- 55 fromFoldM :: Monad m =&gt; FoldM m a b -&gt; (forall r. Source m a r) -&gt; m b fromFoldM (FoldM step initial final) await = initial &gt;&gt;= flip (resolve await) ((lift .) . step) &gt;&gt;= final -- | Convert a Sink into a 'Control.Foldl.FoldM', passing it into a -- continuation. -- -- &gt;&gt;&gt; toFoldM sumC (\f -&gt; Control.Foldl.foldM f [1..10]) -- 55 toFoldM :: Monad m =&gt; Sink a m r -&gt; (FoldM (EitherT r m) a r -&gt; EitherT r m r) -&gt; m r toFoldM sink f = sink $ \k yield -&gt; f $ FoldM yield (return k) return
I had never heard or the `cabal repl` command either. Here's how I was coping. Sometimes I work on a small test project which I don't plan to release, and I don't bother creating a cabal file for it. Instead, I create a sandbox, and then as long as I'm inside the sandbox folder everything I install via cabal will be installed to the sandbox. The other ghc commands (ghc, ghci, runhaskell...) ignore the sandbox by default, but I can force them to use it by adding the flag "`--package-db=.cabal-sandbox/x86_64-*-packages.conf.d`". The only annoyance is that sometimes the flag is "`-package-db`" (with only one leading dash) instead; it's not consistent from command to command.
Interesting, I have will have play with these thoughts more. One thing is that `ContT` doesn't play well with `monad-control`, and yet we retain that capability in the "unrolled" version, so there is at least that difference between the first and last representation of `Source` that you wrote.
Hey, this is the Author. This book is also a learning tool for the Haskell programing language, and an introduction to simple data analysis practices. Use it as a Swiss Army Knife of algorithms and code-snippets. Try a recipe a day, like a kata for your mind. Breeze through the book for creative inspiration from catalytic examples. And most importantly, dive deep into the province of data analysis in Haskell. By the end of the month, I will have all the source code available on GitHub. (Here's a start https://github.com/BinRoot/Haskell-Data-Analysis-Cookbook) If you have questions, please feel free to ask!
Another thing I find interesting is that a FoldM looks a lot like the arguments to `bracket`. I'm not sure if it's actually a good idea, but I was able to write this: foldCatchIO :: Foldable f =&gt; FoldM IO a b -&gt; f a -&gt; IO b foldCatchIO (FoldM step begin done) as0 = begin &gt;&gt;= run as0 where run = F.foldr step' done' step' a k !x = (step x a &gt;&gt;= k) `catch` \(_ :: IOError) -&gt; done x done' !x = done x http://lpaste.net/105242 Basically, at any step, if there is an exception, you just skip to the teardown with the last good state you had. This seems useful for folds that don't have a meaningful result, such as writing to a file. This really highlights `FoldM` for me as the inverse of `ListT`. If ListT is the producer, then FoldM is the consumer. into :: ListT m a -&gt; FoldM m a r -&gt; m r into = undefined -- exercise to the reader
My 2 cents: there are [other reasons](http://blog.codeclimate.com/blog/2014/06/05/choose-protocol-buffers/) why Protocol Buffers (or similar serialization formats, I'm not an expert in them) could be a better choice for certain use-cases other than performance.
Not all fixity 0 operators need to clash with `$`, if they are `infixr 0` and have the "heaviest" argument last, they can chain naturally together. In particular your `n !&gt; m` looks to me like it would be fine as `infixr 0`. The Parsec `&lt;?&gt;` operator that has annoyed me before has both the associativity and the argument order wrong for working nicely with `$`, though. You'd want to be able to write things like (EDIT: reformat better) "block" &lt;?&gt; between (char '{') (char '}') $ do x &lt;- ... ... but instead you need (between (char '{') (char '}') $ do x &lt;- ... ...) &lt;?&gt; "block"
Of course, that's just another reason to prefer them over JSON (in my use case). I intend to replace not only some JSON parts but also an older custom-made protocol that basically works like "&lt;sequence number&gt;\n&lt;COMMAND&gt;\n&lt;parameter 1&gt;\n&lt;parameter 2&gt;[...]" so almost anything is better than that.
Looks like a worthy pursuit! IMHO especially if the new protocol is used for message-passing instead that as a RPC mechanism.
Okay, cool! I was wondering if you were saying "Never use fixity 0 operators, find a better way to do things" -- `!&gt;` is indeed right associative and I think it's the best way to represent the transformation. Thanks for your insight!
Just wanted to say, though it doesn't answer the question outright, this is a pretty freaking cool answer. Thanks for your input, it's a super interesting idea!
Protocol buffers seemed so easy and straight-forward until I saw that one can do RPC with it :P
When you are using sandboxes and find you need to do something beyond `cabal repl`, you can always `cabal exec sh` and get a shell that has the environment set up such that normal invocations of `ghci` and `runhaskell` use the sandboxed environment. I've found this extremely handy!
Well, I suppose you can do RPC with basically anything if you beat it enough :)
Could you clarify what you mean by "and then nothing happens", pastebin the error if there is one?
Would like to know what libraries you are using for the various data analysis and machine learning algorithm that are presented in that book ? Also has performance been considered as a topic in the book ? I find that handling large data sets in Haskell becomes tricky unless you know how to properly use them. Also would like to know if Indian print will be available for that book ? ( I rarely find Indian print of Packt publishers book, and the imported version is too costly.)
For linear algebra specifically, you'll use [dsp](http://hackage.haskell.org/package/dsp) and [statistics-ligreg]( http://hackage.haskell.org/package/statistics-linreg). 
By *nothing* I mean that after the message ("Preprocessing library &lt;library&gt;...") it just returns to the terminal, ie GHCi doesn't start. No error message or anything.
Some libraries include opencv, gnuplot, twitter-conduit, aeson, mongodb, and attoparsec. Yes, benchmarking performance is covered in the "Parallelism and Concurrency" chapter. I'm actually not sure about which translations are supported. Please keep a look out for the available languages when the book comes out!
Thanks! I think I was not clear about that Indian print part. I was not actually asking about the translation of the book. I wanted to know if the book will be printed at Indian subcontinent and available here. Typically other Haskell books like RWH, Yesod from OReily are printed here and available easily rather than the imported version. :)
Alrghit, thanks, I'm gonna try a few thinks in the nexst days :D
Short answer - Yes! Long answer - I believe that there are cases when this is possible and others where it becomes very difficult or impossible. In either case we will use what we can from the parseable source as well as other sources for reference and resolve features.
I don't think it's you. I've been using it for years and really enjoying using sandboxes and cabal repl when developing libraries, but I'm still never 100% sure what any of it does. I usually just try various combinations of clean, build, configure, install until it does what I want.
Awesome! I'm always available to answer any relevant questions. If you have any code suggestions, you can also submit a [pull request on GitHub](https://github.com/BinRoot/Haskell-Data-Analysis-Cookbook) , or just let me know via reddit/email.
I like Protocol Buffers because the C++ integration is awesome. But thanks for the information!
it deemed you unworthy of its power
Check out msgpack, essentially binary encoded json. There is a encoder and decoder in essentially most languages and is great as a json replacement.
&gt; If you know someone who's in Norway, or happens to want to move to Norway, let me know! Ah, nope, sorry.
I read somewhere that Galois uses Haskell to generate C, so that memory management's getting baked in somewhere.
While working on a personal project I needed to group incoming requests based on timeout periods, so I put together this library to help me with that. I am asking for feedback! * Is this useful? Perhaps solutions already exist that I missed * Comments on the package/documentation itself? * Any suggestions on how to easily test with multiple versions of GHC locally? Github link: https://github.com/KholdStare/stm-chunked-queues Thank you for taking the time to check it out! -Alex.
fwiw, the very fact that function application is strictly higher than everything else is a good thing imo. The few languages I've seen that mess around with this are utterly inscrutable. 
Haskell's Thrift implementation used (?) to be very slow, and there was even an open bug on Thrifts bug tracker, which was somewhere along the lines: patches welcome. So I wouldn't use at the moment Thrift for Haskell integration unless you can live with that or able to fix it. OR if this no longer applies :)
Really? I can think of a few examples from Ruby that feel 'right'. Most have to do with structure decomposition. obj.f struct.x # field dereference operator binds higher than application. =&gt; (obj.f)(struct.x) print SomeClass::SomeConstant # scope operator binds higher than application =&gt; print (SomeClass::SomeConstant) higherOrderFunction SomeClass#method # gets a reference to SomeClass.method without calling it, # passing it to higherOrderFunction f x, y, z # multiple arguments bind with , higher than application =&gt; f(x,y,z) f x, y, :first =&gt; 1, :second =&gt; 2 # last argument as a dictionary binds up the whole dictionary # usually used in place of optional arguments. =&gt; f(x, y, {:first =&gt; 1, :second =&gt; 2})
you should add gruntjs in your workflow. it solves all problems.
Put it on github and ask for feedback before releasing it. Doing a schema inference pass before doing proper data analysis is a very powerful tool / idea. But a poor execution of the idea is worse than not doing it at all. In the latter case I'd make up a very strongly typed parser, and if I encounter examples that fail that schema, I can quickly get a counter example to refine my proposed schema. This latter workflow is actually pretty easy to do with tools like cassava. And either way, you need to understand the deduced schema to do anything domain specific The former workflow might actually be pretty cool to make happen, but executing on that style might be ALOT of work to be useful. Point being throw it on GitHub and ask people to take a look. It's a bit hard to understand your description without code. And sometimes a library is no better than writing the thing yourself. We need more Infos! You are touching on an interesting work flow question, sharing the code will make it easier to give feedback 
Take a look at http://hackage.haskell.org/package/protobuf too.
I will absolutely be throwing it on github very soon. This will be using cassava in the future, right now it is simply using MissingH for the csv parsing since bytestring is missing the abundance of nice functions that string has. As for failing parses, there is also an option to generate a sum type with a "failure option", so that it can return a container with the string that failed to parse instead of a Nothing. As soon as I am done with a couple more finishing touches, I'll throw it on github and announce it here. Thanks so much for the feedback! 
theres also the protobuf lib which has a much lighter weight implementation of google protocol buffers http://hackage.haskell.org/package/protobuf
if you wanted to do some off the shelf cluster compute thing tomorrow, the jvm tooling would be handy. I think that gap will shift and switch over the next 2 years though, if the right strategic community efforts move forward. (though some aren't yet public by the respective authors, so i wont name them for now :) )
Right, but Java-style IDEs are not obviously superior to Emacs *et al*. Instead, they offer slightly more functionality in UIs that are easier for beginners but not *nearly* as effective as Emacs is for experts. (Among other things, Emacs can gain a lot of the core language-aware functionality through appropriate plugins for each language.) It's more like using an iPad for drawing or Word for publishing. Sure, people do that, and its easy to start, but it's far less optimized for anyone with experience. If you're going to be doing something a lot, you're far better off with a Wacom tablet or LaTex or whatever.
For me: `:i`, `:browse`, `:t` and `:k` are invaluable.
Hope it helps!
Tekmo, it turns out that Sources make interesting Applicatives and Monads. They effectively turn into ListT, allowing you to bind each element of the fold in turn to build a new Source. See the new `SourceWrapper` instances in: http://hackage.haskell.org/package/simple-conduit
There's also: 5 . Handle asyncronous exceptions properly whenever they happen, and clean up all resources gracefully. This point was a major influence in the design of conduits. It's not too hard to come up with a simpler design if you are willing to be lax on any of those 5 points.
Thanks!
I'm having the same problem as well. When I run `cabal repl`, it goes right back to the terminal. shell&gt; $ cabal repl Preprocessing library foo ... shell&gt; $ # back to the shell. No repl 
Yeah, it seems like what you have is sort of like`ListT` + `EitherT`/`MaybeT`.
I am really surprised no one mentioned this guide yet: http://coldwa.st/e/blog/2013-08-20-Cabal-sandbox.html It is quite extensive, and from a development workflow perspective. Of course, some things are still missing. The fact that for example you have to do: ```cabal install --only-dependencies --enable-tests``` If you also want the test dependencies to be installed, while obvious in hindsight, was not so obvious for me at first. And of course, if you have a lot of slightly modified copies of packages, you'll have to ```cabal sandbox add-source``` all of them manually, which gets kind of boring. I've developed a simple (haskell) script to automate this for myself, but would love to know if there are better solutions (run a local Hackage? seems overkill). On a personal level it also bugs me that ```cabal install --only-dependencies``` is the exact same command for sandboxs and regular non-sandboxed folders. You can end up installing things you don't want to your global installation, just because you forgot you're not on a sandboxed folder.
Looks interesting, however I was more interested about the tooling aspect, and specifically I kept wondering about `haskell-mode`. &lt;rant&gt;There seems to be several overlapping facilities in `haskell-mode` for navigating errors and aiding with the edit/compile/test cycles. There's the haskell-compilation mode, then there's the interactive REPL, the inferior REPL, then there's `ghc-mod` which seems to replace `haskell-mode` builtin facilities, and then there's a Haskell flycheck thing which again does thing a little bit different. I'm sure there's a couple of more ways I haven't listed. But it's kinda frustrating there seems to be no canonical way to use `haskell-mode`. Why don't the various Emacs developers stick their heads together and come up with a unified Haskell support for Emacs? &lt;/rant&gt;
Where in Norway and how flexible would you be with partial telecommute? I live in central Sweden and would consider going to e.g. Oslo a few days a week.
Nice video. My only minor issue was that I found full-screen 360p a little hard to read.
Gah, thanks. I've been playing around with the types some but I'm still a little fuzzy on it all. Do you know a good example of a lens that really only requires a functor instance?
Presumably the *point* is that comparing non-JVM languages to JVM languages based on their Java library support is a bit of a racket. You seem to be pretty sure of your claims to the superiority of Scala for someone who doesn't actually use Haskell. It's also interesting that you're quick to dismiss "scientific computing" while touting Scala's PDF generation tools, as if the construction and execution of complex, mathematically rigorous financial models against staggeringly large amounts of data just isn't quite in the same league as *building a pretty PDF* (for which a Haskell programmer can just shell out to or write a C wrapper for something like http://wkhtmltopdf.org/). Do you have any source for your claims about HikariCP or any argument for the superiority of Scala's SQL query DSLs beyond merely asserting it? Are you aware of GHC's new [Mio](http://haskell.cs.yale.edu/wp-content/uploads/2013/08/hask035-voellmy.pdf) high-performance multicore IO manager, which is so fast that it had to find workarounds because the linux kernel's epoll system was *too slow* and exposed a previously unknown *kernel bug*? This isn't a library, by the way. It's just how IO in GHC works now. Based on this I'm not sure how you can claim that anything blows Haskell IO away, especially without evidence. I notice that you also haven't tried to make any claims about the superiority of the Scala langauge *per se*. Maybe you had already read [Edward's comment](http://www.reddit.com/r/haskell/comments/1pjjy5/odersky_the_trouble_with_types_strange_loop_2013/cd3bgcu) about the deficiencies of the language and didn't want to fight a losing battle. I wouldn't blame you. I get that you're excited about Scala. That's awesome. It's certainly a nicer way to program on the JVM than Java and it is doing a pretty good job of introducing programmers to a more functional style of programming. I like that too, although I wish Odersky wasn't so determined to make it bad at FP. It certainly does have some advantages (like Java library interop) and is generally more friendly to enterprise software requirements (Oracle and SQL Server support, etc). I just don't think your assessment of Haskell is very accurate and I think you might be pleasantly surprised if you give it a chance.
It's available in 720p which should make the text a lot clearer.
This was exactly my workflow until I found out about `cabal repl`.
Are you familiar with free monads? They provide a generalized way to decouple the construction of "monadic computations" from their interpretation. I'd be interested to see how much simpler this library would be to write using `FreeT`.
That's a very good point. I'm curious though, since this library I've proposed uses only bracket and function application -- and Either for early termination -- do you see any way for async exceptions to be mishandled? I'm working on some code examples now to prove that they are not, but I was wondering if you'd already thought of something.
Isn't this a special case of bidirectional pipes?
Sweet. I'm very curious!. As I'm trying to do the same thing at the moment with Hom :)
Looking further has highlighted a lack of expressiveness in this representation: Exceptions cannot be handled that occur when generating elements from a source, as you can with `catchC` or `tryC` in `conduit`. Only downstream exceptions may be handled -- by wrapping catch around the call to the `yield` argument. However, it's impossible to know when you should keep yielding to a consumer that had thrown an exception previously. So it looks like exception semantics have to be baked into the sources and sinks in your pipeline; it may be hard to generalize. For example, `sourceFile` bakes this in by attempting to close the file and then aborting the pipeline when it encounters an exception.
Also, they work on existing JVM systems.
Just went through this guys comment history. It's a gold mine. Node.js 4ever!
Depends on the language. You'd have to use an external plugin to search for things. For example, there is support for [GNU Global](https://www.gnu.org/software/global/) (which is much better than CScope) and [eclim](http://eclim.org/features.html) which uses a headless Eclipse as a backend. There's probably other things for different languages, although I don't know about Haskell in particular. This is just from casual knowledge: it's not a feature I personally use, so I don't know much about it. The point is that you can get those features without sacrificing Emacs's expert-oriented UI, customization and so on.
&gt; From: Apple Inc. ‚ÄúThe Swift Programming Language.‚Äù iBooks. https://itun.es/us/jEUH0.l Also readable [on the ADC](https://developer.apple.com/library/prerelease/ios/documentation/Swift/Conceptual/Swift_Programming_Language/TheBasics.html#//apple_ref/doc/uid/TP40014097-CH5-XID_399). There's also a language reference below the language guide that goes over Swift's formal grammar, if anyone's interested.
I have to say I felt warm and fuzzy inside when I was reading the Swift book. Apple is slowly adding sprinkles of functional programming without people even noticing it.
&gt; a high powered IDE Don't get your hopes up. XCode can't even get type errors right sometimes on Swift code just now. It's ok to work with, but it's not really there yet in terms of tooling at all.
To see what is possible in Swift take a look at this functional programming library for Swift: https://github.com/maxpow4h/swiftz It models some of the basics knows from Haskell, including &gt;&gt;= for Optional (like the Maybe Monad) and Array (like the List Monad). A common Monad interface is impossible because the language has no higher-kinded types yet, but it allows defining new operators and creating many overloads, so at least common syntax for the individual Monads and Functors is possible.
Note that we have that maybe sugar (or close to it), called (&gt;&gt;=).
[Chris Lattner](http://www.nondot.org/sabre/), the LLVM architect and creator of Swift, credits Haskell among the many influences: &gt; drawing ideas from Objective-C, Rust, Haskell, Ruby, Python, C#, CLU, and far too many others to list. They certainly don't claim to have invented to have invented concepts like `Maybe`, though the way they have incorporated so many sophisticated language features into a coherent whole (without becoming overly complex) is novel and very impressive. I have just ported a small Rust program to Swift, and it was surprisingly straightforward. Swift is indeed fast and productive, with a syntax that is easy to learn without being baroque when using advanced features. So far, loving it. The Playgrounds feature is incredibly impressive; the developer is able to interactively explore code, inspect expressions and results in rich ways (not just ordinal types but images, etc), and even scroll through time and animate the effect of loops.
Hmm ... I think ther's some confusion here. Working for all Functors is a *stronger* condition than working for all Applicatives.
C# also has a 'nullable' type (a?) :: Just a | Null, with a fromMaybe ('null coalescing') operator called ?? (Null ?? b = b, Just a ?? _ = a), and a bind operator ('null propagation') coming in the next release of Roslyn called ?. that functions just like Swift's.
There are other Haskellers on the team too, e.g. John McCall, who has studied at PSU and knows Tim Sheard well.
Reminds me of CoffeeScript's [existential operator](http://coffeescript.org/#operators). It's nice sugar, to be sure. Could we have a Lens example, someone? :)
And that C# 6 is about to add that same operator, on implicit nullable types though.
Yes, I don't feel too great about using TH. But I haven't found any other way have first class instances or first class data declarations. What would us use to generate instead?
Go for it.
Another notable feature of Swift is the 'inout' formal parameter. This dates back to ADA. Within the function, stores to the formal parameter do not change any globals; this enables a lot more compile optimization. Think of Swift as a front end to LLVM.
The sugar from the example (chaining with '?.') only works for attribute access. Wouldn't the Hakell equivalent be composition of partial lenses? I think Data.Lens.Partial has the operators for that purpose. Bind for Maybe can be manually defined in Swift (https://github.com/maxpow4h/swiftz), but it's not in the standard library.
It appears though that a lot of very basic things are still missing (or do not compile) * id id * flip id * uncurry id etc.
Yeah it fees quite buggy, the compiler itself too. Looks like it will take a few months to mature.
Not quite the same, though. There is no `null` in Swift. `nil` is an alias for `None`. The `?.` operator is only valid on optional types.
The `lens` package's `^?` operator does exactly that.
Reference types are already nullable...
 john^?residence.address.buildingIdentifier.to uppercaseString 
And to my surprise it has a callee-side annotation to implement call-by-name (i.e. a limited form of laziness) http://appventure.me/2014/06/08/writing-simple-syntax-extensions-in-swift/.
I don't know what augustss would suggest, but you could look at http://hackage.haskell.org/package/haskell-src-exts . This provides an AST very similar to that of TH, then you could use the pretty-printer functionality to output Haskell source. But if you're ok with using ghc-7.8 or newer, I'd probably use TH. The new typed TH expressions allow for some pretty interesting metaprogramming capabilities.
How so? Are you aware of the `default` operator?
Wow, good thing I clicked that link! Was about to write how terrible that library is - last time I checked they had Frankenstein'ed their own custom (and inferior) Maybe type into swift. But it looks way more reasonable now.
I've been toying with something like this with Free, but in the other direction. The idea is to use Generics to take the functor underlying the free monad and tear it apart into corresponding Request and Response objects. I can kind of see how to deal with the simple cases, but I'm going to need to play with some more to see where the limits are.
What do you mean with "it only works for attribute access"? Works fine for functions as well. And `Optional&lt;T&gt;` has a `map` method - which (not 100% fluent in Haskell lingo) does what `bind` is supposed to do.
Or, he can be a girl. Or, he can be not so serious. Or, he can be a not so serious girl. As an exercise, write a mongodb query that can deduce possible answers through unification and substitutions and aggregation. Bonus points if done in gruntjs to fully facilitate event driven workhorse minus parenthesises and json goodness without xml schema correctness enforcement. 
I find it interesting that higher order programmers, ehm, I mean people who can read Haskell finds Swift familiar. I have a background in C++ (even though I haven't coded any C++ for quite a while) and ObjC and I definitely see more C++ than ObjC in Swift. Especially structs/classes and things surrounding that. Could it be that Lattner without knowing it has added in bits and pieces of many languages and thus given a lot of people from different backgrounds something to recognize? Or do you think it was done on purpose?
It has algebraic datatypes and matching
I wish folks would stop posting with the sarcastic headings: "Apple invents ..." The thread content is interesting and constructive, but that sort of title is negative and tiresome.
Union types! Yay.
Swift borrows a LOT from functional languages, and I'm pretty sure they took a page from the C++ books. It's all on purpose.
Its next version contains the same syntax for monadic null chaining, too. It's been on the books for a few years now, actually, so swift taking it is neither surprising nor particularly interesting. Edit, read the last sentence of your post and yeah you covered it ;) 
Haskell didn't invent Maybe types either.
&gt; it's not a feature I personally use, so I don't know much about it. How typical.
You might be thinking of Haskell's *fmap*. 
Right! Thanks.
As far as the syntactically supported optional types and being developed alongside an IDE, JetBrains' [Kotlin](http://kotlin.jetbrains.org/) also fits the bill. "String" and "String?" are distinct types, calling .foo() on a "String?" is a compile time error, whereas ?.foo() will call foo if non-null and evaluate to null otherwise, and is chainable. Furthermore, &gt;The compiler is being developed alongside with an IntelliJ IDEA integration. (IntelliJ IDEA is one of the IDEs they develop. I consider their environments to be "high powered".)
ghc-mod requires more of your system - needs access to a ghc-mod binary, for instance, and it needs to be the same version as the elisp. Emacs hasn't got a great story for maintaining this kind of non-elisp dependency, as far as i know.
Except, just like most everything in C#, it's subtly wrong. You can't use ? on reference (class) types, which all have null as a default element that you can't remove.
Doesn't that wrap it in an extra Maybe? Maybe I'm just not setting up my records properly.
I don't think you need "first class instances or first class data declarations": you just need instances for types that are flexible enough to represent any sensible csv. http://code.haskell.org/~aavogt/lmqq/ex2.hs does something like that.
 residence :: Traversal' Person Residence address :: Traversal' Residence Address buildingIdentifier :: Traversal' Address Strng to :: (a -&gt; b) -&gt; Getter a b to uppercaseString :: Getter String String residence.address.buildingIdentifier.to uppercaseString :: Fold Person String If at any point you need the field to be a Maybe, you can 'traverse' through the maybe: traverse :: Traversal (Maybe a) (Maybe b) a b but traverse is more general traverse :: Traversable f =&gt; Traversal (f a) (f b) a b So if you have lenses to Maybe rather than traversals all the way down, e.g. residence :: Lens' Person (Maybe Residence) then you'd use: john^?residence.traverse.address.traverse.buildingIdentifier.traverse.to uppercaseString and to edit you can either us the same traversals which would edit it if its already there only, or you can switch them out for 'non x' which can be used to supply the default for what editing them should mean if they are missing, which is useful for editing nested maps, nested structures with Maybe's in the middle, etc.
&gt; Swift is probably the first programming language developed simultaneously with a high powered IDE Smalltalk, and from contemporary languages Kotlin, no?
Just following up. I made the change earlier today to store your CodeWorld projects in storage maintained by the CodeWorld system, rather than on Google Drive. Your Google account is now used only as a proof of identity, and the site does not ask for any extra permissions. Hope that addresses your concerns.
The Playgrounds feature intrigues me. Swift isn't a pure lazy language, so how can you move forwards and backwards? I do remember a compiler for an imperative language supporting this capability, so perhaps they used the research from that.
Touch√© ;) &gt; Presumably the point is that comparing non-JVM languages to JVM languages based on their Java library support is a bit of a racket. Sure, but comparing Java library support to C library support is a valid comparison, and the former, funded heavily by the enterprise, provides everything under the sun, while the latter does not. &gt; You seem to be pretty sure of your claims to the superiority of Scala for someone who doesn't actually use Haskell. Not claiming Scala is superior to Haskell, look again, I'm talking about a specific domain, web development, and the library support available to us on the JVM. &gt; as if the construction and execution of complex, mathematically rigorous financial models against staggeringly large amounts of data just isn't quite in the same league as building a pretty PDF It would be absurd to think that Scala's primary strength lies in being able to pull in a PDF generation library. Twitter's handling spikes of 140K tweets per second as of last year with Scala as its primary language, that's non-trivial. Now they've forked the OpenJDK and are working toward the goal of 1ms latency. Anyway, Haskell's usage in the financial domain is arguably more about program correctness than raw throughput. &gt; Do you have any source for your claims about HikariCP Yes, see the [HikariCP](https://github.com/brettwooldridge/HikariCP) benchmarks (scroll down) &gt; or argument for the superiority of Scala's SQL query DSLs In Haskell you have a single option for working with relational databases, Esqueleto. Oracle or SQL Server backend? SOL. Furthermore, Esquelto's DSL provides no interface to work with plain SQL. This is a *huge* problem when you have complex queries that are not easily expressed (or are impossible to express) in the library. Complete and utter lack of documentation (empty readme!) is needless to say, a hindrance. Minor issue: choice of operators in Esqueleto make for a less readable DSL. On the flipside we have Slick, Squeryl, sqlTyped, and Activate leading the way on the Scala front, but we could just as well drop down to Java and use, for example, JOOQ, if so desired. We got options. Don't like Slick's strict adherence to Scala collection semantics? Go with Squeryl's elegant LINQ-to-SQL approach. Converting a plain JDBC based application to Scala and need to get up and running now? sqlTyped provides string based, typesafe queries via macros. &gt; Are you aware of GHC's new Mio high-performance multicore IO manager, which is so fast... No, thanks for the link, that is impressive. &gt; Based on this I'm not sure how you can claim that anything blows Haskell IO away IO can't save you when your connection pool falls flat on its face, which may be why Yesod and Snap performance is [so utterly hideous](http://www.techempower.com/benchmarks/#section=data-r9&amp;hw=peak&amp;test=db) on the database front ;) &gt; Maybe you had already read Edward's comment about the deficiencies of the language and didn't want to fight a losing battle Indeed, that, and Paul Phillip's Scala collections rant on Youtube, are often given as "proof" of Scala's inferiority to Haskell. Well, as I mentioned in another comment, what affects library authors in Scala does not necessarily affect library consumers (nearly every bullet point in Kmet's list are exclusively in the domain of author, not consumer). Conversely library consumers are affected by the choices made by library authors in Haskell. For example, how do I install Haskell Platform? `cabal install lens` Speaking of installing, here on Fedora 18 we have Haskell 7.4.1. Reading about the goodness in 7.8 (and the general hype around Haskell these days) I thought OK, I've read LYaH, let's get our feet wet with the latest and greatest. That was no joy, there's a dep on some ancient library that no longer exists in modern Linux distros (not surprisingly, available binaries are for CentOS and Debian), so the suggested workaround is to build Haskell from source. Welcome to Haskell, it's ever so easy to get up and running. Compare that to downloading the Scala 2.11 binary, modifying your PATH, and firing up a new terminal. Portability, JVM for the win. Cheers 
I think the chunked/draining read functionality is useful without the delayed read functionality. You could clarify that in the documentation. I don't particulary like delayed reads because they introduce a minimum latency. This latency does not compose well and limits the number of processing steps that can be used. Usually it is better that the scheduler is what implicitly creates queues because there is other work that needs to be done. In a system with multiple consumers, a max limit on the chunks I would be willing to accept makes sense. This is the trade-off between invoking the STM machinery and the opportunity cost of not utilizing another CPU. 
Much better, thanks. BTW, the help button pops up a window with 'No handler accepted "/undefined"'
&gt; tiresome Right, Swift is half-way between Objective-C and Haskell, and will gently introduce hordes of imperative programmers to functional concepts. Think of it as a gateway drug. Some will not stop there, though, and venture into more pure languages. Interestingly Swift is not Apple's first foray into functional-objectoriented languages. Sadly the first attempt, with Dylan in 1995 failed because of the inability to back it financially. This time there is a very sound technological foundation (what is LLVM) and sufficient funding available. And several orders of magnitude more programmers to address, too. 
Odd usage of "haskell" in the name of the github project. Your code base from what I can see has nothing to do with haskell except in the remotest sense possible.
Very thought provoking! There is a general trend across most all industries to pigeonhole people. I suspect this is a combination of outdated thinking (creative thinking is different than working on an assembly line) and intellectual laziness (it's easier to cling to defined roles than it is to understand needs and find people to fill them). At the same time, part of the challenge is that businesses can't be agile. If your employer specializes in 3 to 6 month contracts in various technologies, it's not really cost effective to maintain a varied group of employees. I've seen companies make good use of contractors but it requires a level of flexibility that's culturally incompatible with many large companies.
My advice to prospective Haskellers looking for Haskell jobs: 1. If you're a junior or have no chops then your back-up plan is work under small pay, do great work and gain experience, and then after a year demand a pay rise once you have proven your worth. If none is forthcoming, take your newly gained experience, interview at other places and leave. If you are feeling undervalued, you almost certainly are. 2. If you know you're worth something by experience, my advice is to simply reject poor pay. Don't accept silly games (and it is a game) like lowballing and statements like "we're a start-up so we can't afford to pay well yet". Just flat out reject the number, as I have done a couple times, they will come back with better offers. If those offers, if any, aren't good, that's fine. Look elsewhere, your time is worth more than to waste on companies messing you around. Remember, it's actually hard to find quality Haskell programmers who both know their domain and are competent workers. Anyone who has been on the employer side of the interview process knows that. It's also not the end of the world if you can't find a Haskell job that meets your requirements, don't settle for less. More opportunities will appear. Oh, also: don't rely on your job to expand your expertise. But you learned Haskell, so you probably already know that.
Just search for "enumerations with associated data".
I think that functional languages will be seeing large gains in adoption (and salary!) over the next couple of decades as their benefits for parallelization become more and more important. That doesn't fix the salary issue *now*, of course, but the companies that tend to have a lot of money to spend on salary also, at this time, have a large investment in other technologies (notably C++ and Java) that they have to maintain. Over time, though, functional practices are making inroads in these institutions. I also don't feel like my day job is "fish frying," despite the fact that I use Java and Ruby rather than Haskell. I make money for a company that has a legitimate business. Is it the greatest stack? No&amp;mdash;Java is crusty and Ruby falls into chaos the minute it has to do something more complicated than show a web page&amp;mdash;but it's also not COBOL, which I could have been working in twenty years ago. Progress is slow, but it's progress.
Ah, a FireFox incompatibility. Fixed. You may need to refresh the page and the help pane.
There has been talk of implementing something called "stable heap" in GHC that seems to be similar to your proposal. See this ticket: https://ghc.haskell.org/trac/ghc/ticket/9052 
Thanks for the pointer. I think what I am suggesting is different. In the "stable heap" case, the "stable heap" is never used as a nursery. One implementation of `pushMemoryMark` could be to mark a position in the nursery and to treat the remaining space in the nursery as a separate nursery, nursery2. Then after having continued to allocate in nursery2, `popMemoryPool` can be implemented as a mini-gc with only a single root. That root object is then moved from nursery2 to nursery and the original nursery allocation pointer is reset to `mark` + sizeof(root object from nursery2). The cost of this operation is the cost of traversing and copying the result. `poolEval` would be sprinkled around high-performance code similar to how `par` and `pseq` are used today, but at positions where a large intermediate data structure is created in order to produce a small result. The use case is therefore opposite of that for "stable heap". Use `poolEval` to do rapid, cheap, focused gc.
Maybe Monadic Regions is what you're looking for. 
Regarding startups, the 99% that do not have VC funding will not be able to give a competitive cash salary, so if that is what you are looking for, avoid startups. The same is true for an executive position at a startup so it has nothing to do with the programming profession.
I wonder if this isn't the natural course to have this pigeon holing? I have said to people that programming is becoming more important as our reliance on machines increases. Right now we have doctors, lawyers, construction workers, and programmers. But eventually we will have medical programmers, law programmers and robotics programmers. The fact of the matter is that people may not be able to switch core competencies at will like they have due to each technology becoming too deep for one person to entirely learn.
&gt; benefits for parallelization We've been hearing this for years, and the benefits of outright parallelization, for the majority of industries, have not materialized. Concurrency, on the other hand, has given huge gains, but you don't need a functional language to solve the concurrency problem (well enough, that is, for most problems) 
True, but as far as I know chaining maybes (aka optionals) in this monadic way originated in Haskell. 
I'm definitiely reading a lot more academic papers now
99% of the people who are involved in startups don't see it what way.
It is sort of worrying that you think of wai as a labyrinth, mortal haskellers view lens as totally opaque, aeson and text optimizations are magic to the rest of the crowd etc.. ;-) Nobody understands the really good haskell libraries except its authors?
Again, that only works if you're interested in default. Default is not null, as zero is not null.
I agree. If you're taking a pay cut relative to your full market salary, you're a founder and should be treated like one (including in equity). There are a lot of chumps in their early 20s (I was one) who work for "startups" at various levels of legitimacy at a cut rate and non-founder equity. That's because they don't know what they're worth yet. 
I probably should be cross posting /r/wtf
I'm getting confused. I started confused with the first article, but I'm getting more confused too. How many times did you get GHC to crash?
I do my best to confuse GHC and it just laughs in my face. It hasn't even skipped a beat yet.
This is something we've known for a while. Put another way: labor is at an inherent disadvantage under capitalism. &gt;The problem is that we‚Äôve let anti-intellectual, non-technical businessmen walk in and take ownership of our industry. This isn't exclusive to programming. Business is always controlled by people who don't have intimate knowledge of any productive activity taking place. That divide is just a necessary consequence of absentee ownership and the rigidly hierarchical nature of capitalism. &gt;It‚Äôs ridiculous, but most companies demand an astronomically higher quality of work experience than they give out. qft &gt;We have to do it ourselves. I love reading things like that. Workers directly controlling their work is something I believe software development is more readily suited for than other productive activity.
That's true, but even desktop processors these days are four cores; and, to be frank, you may never see Haskell become a "mainstream" language, but other functional languages are probably more likely to "break in" (F#, Scala, and Clojure are the obvious suspects). I recently did an ETL script (at my day job) in Clojure using core.async, for example.
Trade Union!
I apologize if anybody thought I was being negative. I'll plead guilty to overly cute and sarcastic. Swift is a brilliant piece of work - I think we can learn from it.
If you've already got a pairing setup that works for you then this might not be useful, but here is why I like it. ‚Ä¢ Creates a clean environment that doesn't expose the files on your computer ‚Ä¢ You can crank up the CPU power as needed (for big compiles) ‚Ä¢ The pair/unpair scripts to manage ssh key access by your pairs ‚Ä¢ Easy to reset the world if you somehow get into cabal hell ‚Ä¢ Nice configuration between vim and tmux
I'd be happy to do in-house training for you, contact me via www.applied-duality.com.
It took some messing around in GHCi to get what you meant, but I think I get it now. I'm still not sure how the `traverse`less version is supposed to work, but I follow the one with `traverse`s.
I think he got it completely wrong. It is not "haskell tax" it is "new development tax". Compare salaries of developers who are hired to develop new systems to salaries of developers who are hired or even contracted to support existing big systems. The truth is, it is much easier to start a new program that get into huge, complex and ALWAYS undocumented existing one. Especially when original devs left the company. Capitalism is really a very simple system. They pay what they have to, not what you are worth or what you are bringing to the table. That's why SAP and JD Edwards contractors easily make $300,000 for tweaking and generally maintaining existing installations, while the development of completely new systems is usually outsourced to Bangalore. I can assure you, junior java or dotnet devs are being squeezed just as much as haskell or scala devs IF they are hired to start a new project. But look at any company who are suddenly in trouble because they lost their key devs supporting mission critical existing system. Out of desperation they will pay top dollars to get someone/anyone to help them out of that mess. The lesson we developers should take from our interaction with capitalism is this: Hook em up, then jack up the price. Build something with haskell (the most obscure and hard to learn language today) while being paid pennies. Make sure entire business of company is tied to your product. Spend at least a year quietly building a critical mass. Then BAM, resignation letter on the table. Within a day you will be paid TWICE as much as you were getting just to agree to stay. And no, they won't find any replacement for you. Use against the capitalists the same tactic they use against you. You were not born into this huge universe to make them rich. Everyone for themselves. They exploit you. You exploit them. You can be capitalist too :)) 
¬´It seems idiotic now but a year ago I thought it was some interpreted equation solver. I'm not alone in that ignorance, sadly; talking about it to some mates over lunch last year and a friend incredulously burst out laughing, ‚ÄúHaskell?‚Äù as if I was trying to tell him my dishwasher had an impressive static typing system. It saddens me to realise how in the dark I was, and how many people still are.¬ª Yup.
Just to fuel the fire: I myself am contemplating starting a software development cooperative.
¬´Some obviously-smart person spent time making the alphabet soup permutations, why?¬ª This is *exactly* how I felt the first time I read functional code.
I don't think he doesn't understand it. He does and he's saying it is bullshit. You're also not refuting what he said, but just chanting your own business matra with "this will work great" sprinkles on top of it.
There was an [interesting post on Hacker News](https://news.ycombinator.com/item?id=7634152) not too long ago where people began listing them in the comments.
In the previous thread people said you could just use regular lens (the `fmap` you made last time is really `over`) - is it also a similar situation here? (I honestly don't know). 
I also have no idea. I suspect that, at the very least, it will be much more awkward. There cannot be any series of combinators that gives you the "correct" `ap` function, because there are multiple valid ap functions for some type. For fmap, there is only one unique function of the type, so the `over` combinator always works.
&gt; interesting post on Hacker News Great, thanks!
I saw it over at /r/cyberunions, which is a resource you might also be interested in.
Nice tip! Thanks! That, coupled with [this](https://stackoverflow.com/questions/23544266/cabal-and-no-require-sandbox) presents a much nicer workflow, in my opinion.
Hmm... As much as I like the general idea, I'm somewhat bothered by the n^2 growth in number of relative precedence declarations required for n infix operators. Perhaps also have a default evaluation scheme (e.g. all are equal precedence by default)? Are there any good reasons not do do it this way?
Great contributions! I am probably going to use your rados-haskell package. Regarding the architecture, I think I would have used a set of brokers that use etcd or consul for consistency. That means no single point of failure as well as not having brokers be a scalability problem. 
I don't understand what that sentence means. :(
&gt; There cannot be any series of combinators that gives you the "correct" ap function, because there are multiple valid ap functions for some type. `Data.Lens.over` extracts the `fmap`-like function entirely from the *lens* it is given, not from the data it is applied to. Likewise, there could be a lens combinator that extracts an `ap`-like function from the lens it is given. I believe this is where the `lens` library has a leg up over the `typeparams` approach: in `lens` the "dictionaries" are first class values, while in `typeparams` the dictionaries must be written as instances of a typeclass, thus incurring the limitations of Haskell typeclasses.
&gt;Data.Lens.over extracts the fmap-like function totalmente from the lens it is given, not from the data it is applied to. Likewise, there could be a lens combinator that extracts an ap-like function from the lens it is given. Consider the list data type. Unless I'm way off, [there is only one way to write lenses that perform the appropriate actions](http://hackage.haskell.org/package/lens-1.3.1/docs/Data-List-Lens.html), like taking the head or tail of it. But for lists, we might want the ZipWith applicative or the standard applicative. Since there is only one set of valid lenses, we must combine them in different ways to generate the two different functionalities. Thus, I don't see how your statement can possibly be true. ---- Here's a really interesting possiblility: It may be the case that there are a limited number of ways one can take a lens and form an applicative. (The parametricity of the lens and combinators might cause this limit, for example.) This would give us a standard technique for deriving all the new applicatives from data types. If this is the case, then it must also be the case that these applicative-generating-combinators sometimes (but not always) generate the same applicative instance. For example, the ZipWith applicative generator and the List applicative generator would generate the same ap function when applied to a lens for Maybe. There's probably some interesting abstract algebra that would go into categorizing/enumerating these constructions.
Fantastic work! I'm sorry I couldn't be more help... but it looks like you didn't need me anyway :) Dropping `zoom-cache` is probably the most sensible move right now. Having *something* working is much better than nothing!
The link to *Practical and Effective Higher-Order Optimizations* is broken. Get it [here](https://dl.dropboxusercontent.com/u/1620890/website/writings/ho-optimization.pdf).
It's not n^2; the declarations follow all the rules for partial orders. [Haskell98 prelude fixity declarations](http://www.haskell.org/onlinereport/standard-prelude.html) infixr 9 . infixr 8 ^, ^^, ** infixl 7 *, /, `quot`, `rem`, `div`, `mod` infixl 6 +, - infixr 5 : -- apparently this isn't legal syntax? infix 4 ==, /=, &lt;, &lt;=, &gt;=, &gt; infixr 3 &amp;&amp; infixr 2 || infixl 1 &gt;&gt;, &gt;&gt;= infixr 1 =&lt;&lt; infixr 0 $, $!, `seq` Equivalent declarations using partial ordering: infixr . infixr ^, ^^, ** below . infixl *, /, `quot`, `rem`, `div`, `mod` below ^ infixl +, - below * infixr : below + infix ==, /=, &lt;, &lt;=, &gt;=, &gt; below : infixr &amp;&amp; below == infixr || below &amp;&amp; infixl &gt;&gt;, &gt;&gt;= below || infixr =&lt;&lt; equal &gt;&gt; infixr $, $!, `seq` below &gt;&gt; -- infix op1, ops... relativeDecls -- is equivalent to -- infix op1 relativeDecls equal ops... We know that `$ below &gt;&gt; below || below &amp;&amp; below == equal &lt;=` so we have `a &lt;= b $ c == d` parses to `(a &lt;= b) $ (c == d)`. Similarly, adding a new operator between + and *: infixl ! below * above + automatically gives it a relative precedence to every Prelude operator. The difference is that if you have two different operators specified this way, without a specific relative precedence, then it's a parse error to include them both in the same expression without explicitly saying which precedence you want: infixl % below * above + test1 = 1 ! 2 % 3 -- parse error, no ordering between % and ! test2 = (1 ! 2) % 3 -- ok test3 = 1 ! (2 % 3) -- ok `test1` would parse OK as one of `test2` or `test3` if `%` was declared either `above *` or `below +` or anything that could transitively prove those. But any existing declaration could be replaced with an `equal` declaration. I'd propose that each integer represents a class of equal declarations so that the existing syntax continues to work, with the usual total ordering on the integers. infixr 3 &amp;&amp; same as infixr &amp;&amp; equal 3
Agreed, this guy started out with the premise of "capitalism bad" "Marxism good" and then bent everything to fit his existing conclusion. A lot of places use C++/Java/PHP because it's a lot easier to find half-way competent people to develop and maintain that code after you're gone. It's not about mustache twirling fat-cattery
Thanks! Now fixed. [Pull requests](https://github.com/yallop/icfp2014-papers/pulls) are welcome, by the way.
If you do, it'd be great if you would post about it here.
&gt; This article is nonsensical speculation from someone who doesn't understand business. Programming job salaries are driven by what all other salaries are driven by: supply and demand. M. O'Church might watch the world through a particular (biased) lens, but he probably understands simple concepts like supply and demand. He goes beyond that, though, because "supply and demand" is a very crude explanation; the next step is to look at how the actors of the market manipulate it to their own advantage. In this case, how employers manipulate it to their advantage. You seem to just want to focus on what an *employee* can do, but of course you need to consider the other sides' tactics, if you assume that both sides are adversarial (and they are, at least as far as incentives go). You can't play a game against an opponent and only fine tune your own wits and motor skills; you have to consider what the other person will do. Just saying that it is *supply and demand* seems similar to one person explaining that woodpeckers have their current beaks because of the food that they eat and the shortage of suitable, nutritious worms on the outsides of trees, and then another person comes along and says "Clearly this guy doesn't understand nature; it was because of natural selection". But the first person understands natural selection, he was just using it as an indirect reference rather than spelling out every detail of his argument. Meanwhile, the other person is just muddying the waters by replacing the first persons specific arguments with a more broad and generic argument.
`(:) :: a -&gt; [a] -&gt; [a]` is a cons operator btw. 
&gt; If operators in your DSL are analogous to existing, widely-used operators (e.g. built in to Haskell, from other, widely-used DSLs, or perhaps from mathematical literature) If that's the case, be sure to check whether your structure might fit some typeclasses. Then you can just overload the operators by declaring some instances. 
Yep, and more of that same stuff here: https://github.com/ezyang/pldi14-rlimits-aec and here: https://ghc.haskell.org/trac/ghc/wiki/Commentary/ResourceLimits Definitely the kind of thing that could make Haskell very grown up. Having "zones" where all memory in the zone is released in a single operation is a very powerful technique for constraining memory use. Not uncommon in the C++ world, but sadly not typically an option in managed languages such as Java, Haskell, etc.
Yes; the Prelude spec claims that `:` is built-in and thus you aren't allowed to make a fixity declaration for it. I find that kind of surprising :)
The first mistake I believe, is labeling yourself as a "programmer". Many businesses will not see you as a valuable asset, just as a replaceable commodity churning out hieroglyphics. What adds *value* to an *engineer* is what other skills they bring to their job. Strong skills in physics, finance, statistics, performance tuning, etc. Above all: proven experience.
I came from academia and I always thought our code was a little rushed and unstructured for maintainability. Many things often lacked proper testing as well. Then I moved to industry and realized that things were even worse. I had always put those "real-world" developers on a pedestal because I believed they needed to write good maintainable code to survive. 
The entire library seems to basically be (using `Control.Monad.Prompt`, one of the several 'free monad' packages on Hackage) data Req req res a where Await :: req -&gt; Req req res res type RequestT req res m = PromptT (Req req res) m directRequest :: req -&gt; RequestT req res m res directRequest req = prompt (Await req) request :: req -&gt; (res -&gt; a) -&gt; RequestT req res m a request req f = f &lt;$&gt; directRequest req 
This was also my first impression upon reading design patterns. The "software engineering" course at my university was my least favourite by far. I thought I was stupid for not being able to naturally design complex processes in terms of "abstract re-usable patterns". Coming from an engineering background, I always found dataflow and composition far more intuitive than inheritance. Functional programming to me is far more close to systems engineering where the Laplace space rules the day. In the strictest sense, refactoring for us was algebraic. Similar concepts are true in VHDL where we describe processing in terms of functions and groups with state contained to "registers". VHDL is the ultimate concurrency language. 
Unfortunately so. This issue certainly divides many companies I hear about. Business vs. developers. It could easily be a symbiotic relationship but that would mean giving more of decision power to the technical people, and well, business people want to feel useful.
&gt; This dates back to ADA. Bleh, I have a feeling it existed before Ada had it, and also you make it sound like Ada is a dead language. It's definitely alive and well and has plenty of features that languages like C++ and Java still haven't caught up to. Haskell and Ada are in some ways very similar, they're trying to solve some similar problems using quite different methodologies. I'd love to see Haskell support [Ada's range types](http://en.wikibooks.org/wiki/Ada_Programming/Types/range), among several others.
I'd just been looking at Oleg's notes on typed taggless final interpreters. http://okmij.org/ftp/tagless-final/course/ For fun I wrote these folds in the style presented in the paper: http://lpaste.net/956292215259267072
&gt; Data.Lens.over extracts the fmap-like function totalmente from the lens it is given, not from the data it is applied to. What I meant by this was that if you have a `(Int, Char)`, then you could fmap into the first slot with `over _1`, or the second slot with `over _2`. Given an `Either Foo Bar`, you can go `over _Left` or `over _Right`. Thus, the mapping function you get out of `over` depends on the lens you give it.
Or because they enjoy the work. I turned down "better" offers from bigger companies because I knew I would hate the environment.
That's sugar for saying `x:Int? = Optional.None()` (not sure about the syntax). There's no null pointer in a Java/C sense. `x.someFn()` won't compile, `x?.someFn()` will always return `None`. Because `x.map({ v in v.someFn() })` is always None if x is None.