What is the problem that this flag solves? It isn't explained in the blog post.
* `MonadIO` is standardizable in that it doesn't require any language extensions at all. It is however, more limited. * `MonadBase` requires an MPTC and a functional dependency. Neither of which are terribly controversial, but do get in the way of, say, specifying it as part of the language report. There is also the minor annoyance that you get much worse error messages when things go wrong.
Hackage *should not* enforce the PVP. Encourage it as a best practice, sure. Offer to help maintain PVP-conforming version numbers and bounds, sure. But ultimately, just let people upload to hackage what they will. PVP is the only reason I've heard for people not wanting to upload to hackage. For the sake of not splitting the community, let's accept all packages on Hackage regardless of if they follow the PVP. We could always create a stackage-like subset of hackage for PVP-conformant packages. But hackage should be the baseline for open source Haskell, and we should lift as many restrictions as possible so that package authors will have no reason to *not* put their open source stuff on hackage.
I believe this would make supporting Stackage in Nixpkgs a lot harder.
Woah, i'm out of the loop, what are the differing forces behind and haskell.org and haskell-lang.org? 
I think Richard Eisenberg's [Glambda](https://github.com/goldfirere/glambda) interpreter handles contexts using GADTs and singletons, though I may be wrong -- it was way over my head when I saw the presentation two years ago :D
This is abysmal on mobile. 
I think Snoyman is being deliberately obtuse to avoid calling anyone out. I'm familiar with the situation, so I'll just say: A Hackage trustee revised a package to include upper bounds, which required the package author to "fix" their package (that wasn't broken in the first place). 
Sorry if I was imprecise, why are there constraints to this class? It seems that it could be useful for other things too.
The first one certainly, that would be `binaryOp (For :: For Num) (+)`, and will work even if the fields have different types, as long as they have a Num instance. The second one changes the type of the fields, which is not possible in general of course, you'd need a parameter for each field. But maybe you have a class like `class Foo a where foo :: a -&gt; Maybe a`, then you could do `gtraverse (For :: For Foo) foo`. The last one is no problem either: `gfoldMap (For :: For Show) (return . show)`.
Isn't that just `MonadState`?
It is very useful for mutually recursive AST types. You can use either the Data.Data based transformBi or universeBi function or let template haskell generate the functions for you as in geniplate. Then the function signature says it all, right? ;) universe :: a -&gt; [a] transform :: (a -&gt; a) -&gt; a -&gt; a universeBi :: a -&gt; [b] transformBi :: (b -&gt; b) -&gt; a -&gt; a There also exist the monadic variants. But these functions I found most useful for simple purposes.
Page 21, Meet-distributivity: shouldn't a' and b' be bigger than a and b respectively? If a is lowest, b is lowest, and c is biggest, then I don't see how the existence of a' and b' is possible.
That's actually exactly what I was thinking of!
You can get that just fine with mtl-style classes: instance (MonadTrans t, MonadState s m) =&gt; MonadState s (t m) where get = lift get put = lift . put instance Monad m =&gt; MonadState s (StateT s m) where get = Trans.get put = Trans.put Implement that and then wait for Ed to yell at you :)
My vote (symbolic, I haven't had a working twitter account in years) is for allowing alt sources, with git repos as the obvious first choice. The existing alternative is doing the same thing by hand anyway.
I'd love to see someone draw the spaghetti of ContT :) 
Isn't that already not a thing anymore? I mean, ostensibly the set is generated from Stackage but modified quite a lot?
Heterogenous lists, we need a lot of language extensions for those. Or do we? type HL0 r = (() -&gt; r) -&gt; r type HL1 r a = (a -&gt; () -&gt; r) -&gt; r type HL2 r a b = (a -&gt; b -&gt; () -&gt; r) -&gt; r type HL3 r a b c = (a -&gt; b -&gt; c -&gt; () -&gt; r) -&gt; r Empty list is easy: nil :: HL0 r nil = \f -&gt; f () Let's write cons, first the specialised examples: cons01 :: a -&gt; HL0 r -&gt; HL1 r a cons01 x xs = \f -&gt; xs (f x) cons12 :: a -&gt; HL1 r b -&gt; HL2 r a b cons12 x xs = \f -&gt; xs (f x) Looks quite the same. We can define generic `cons`! infixr 5 `cons` cons :: x -&gt; (xs -&gt; r) -&gt; (x -&gt; xs) -&gt; r cons x xs = \f -&gt; xs (f x) And we can make list of three different elements: abc :: HL3 r Bool Int String abc = True `cons` 42 `cons` "Foo" `cons` nil We cannot get too far with those though. Consuming head ins't trivial, yet we can write specialised consumers: -- True42Foo shownAbc = abc (\a b c () -&gt; show a ++ show b ++ c) My point here: there are a lot of different data structures, which is hard to do in a "concrete" way (`data Foo = ...`). But by using functions we can specify a lot if we use "consumer" of the data as the data itself. The functional geometry is one great example, try to come with concrete representation of geometric bodies (but if you only care about testing whether the point is inside a body...) Another example is schedule; How to specify when occuring events could happen? Every day, weekly, first tuesday of the month... data Sched = Daily | Weekly WeekDay | Monthly MonthDay | MonthlyRelative WeekDay WeekOfMonth | JointSchedule Sched Sched | ... Yet with functions it's simple -- type Sched = Day -&gt; [Day] type GenSched a = a -&gt; [Nat] which gives dates after the day. (from around 29:00 of https://skillsmatter.com/skillscasts/6587-programming-with-universal-properties by Gershom Bazerman)
well, the PvP creeping enforcement is definitely not to hackage's favour, but it's really not the only thing. As a positive advantage, I'ld like to see fast "publication" of package by git tagging directly in a repo. this, shipping tarball to some random central server, increasingly feel like a wart in this brave new modern world. In some form you can see this question about modernising hackage [1], not removing it. [1] A simple example would be a github file with a list of (package name, url, commit, version) in one file.
Improving package discoverability would be fantastic, but I don't see how Stackage helps with that. Is there a better way to discover new packages than just Ctrl+F on https://www.stackage.org/lts? If not, then it's no different than Hackage. I primarily discover packages through Google and just poking around the dependencies of packages that I already know. If any other programming language communities have built more effective solutions, we should steal from them!
&gt; But ultimately, just let people upload to hackage what they will. I disagree. If you're going to put code out into the world it should build, otherwise it's mostly useless to people. Not completely useless, though. But that's not what hackage is for. If what you want is simply a registry of Haskell code that exists in the world, that could certainly be done. Just create a site that lets people add a git/github repo (or maybe commit as /u/vincenthz suggests) to a giant list. But I have a strong suspicion that such a list would be nigh unto useless. Making sure things build is exactly what the PVP is all about. 
&gt; I'm not sure that any of the hardcore folks would be looking at what you do and dismissing it with "aww cute" or something similar. I do think though, but who knows... &gt; If you ever want to put together a draft post on an introduction to interaction combinators I have done some things like that, maybe in a future if they really turn out to be as promising as I expect I'll build something more substantial. As you said, in the sea of things to learn and explore, interaction combinators are a very small dot with a small case on its favor.
It was more of a "thats not quite how I'd say it, but I make a lot of these arguments myself to folks" kinda response. =)
/u/linharty has only 2 posts, and the other one was posted 9 months ago. If it's a bot, the only explanation for its behavior is that it's a member of a cluster of reddit bots that try to act like humans by randomly posting and hopefully collecting some upvotes.
The cabal/PVP style is one way to ensure things build: let a solver figure out versions based on constraints. The stack/stackage style is another way: have curated lists of versions that are known to work together. The difference in practice is that hackage trustees often go around adding bounds and tightening constraints when a bad build is discovered, while stackage curators often go around pestering people to loosen their constraints once a new version of one of their dependencies is released. It's an interesting yin and yang and I think these two ideologies complement each other nicely to the benefit of all. I just don't want people to not be on hackage merely because they disagree with the PVP and have their own way to ensure builds succeed. Hackage is where I think we should be congregating as a community despite differing opinions on the PVP. If hackage turns people away for not conforming to the PVP, then it would seem to be hackage that is splintering the community.
Since Haskell makes the benefits of immutability clear, I'm puzzled that it was ever considered a good idea to make packages mutable, which is now inspiring odd workarounds like this. All apparently in the service of version bounds, for which there is no clear semantic and questionable utility.
"Death of the Author": it's the content, not the intent. 
Ahem, some people call them *Hughes lists* in the Haskell world. I never liked *dlist* / *difference list* as name - firstly "difference" already has strong connotation so you might expected it to be a run length encoded list; secondly there were doubts that the functional dlist was actually the same thing as the Prolog dlist that it inherited its name from.
The packages are not actually mutated but a new revision is created. 
&gt; On the face of it this sounds like an awful, awful idea with respect to community cohesion. I agree this is a bad idea. That said, the current situation isn't perfect either. Hackage's mutability makes it pretty unsuitable as a foundation for other infrastructure. My personal opinion is that we should follow Simon Marlow's request here: https://github.com/haskell/ecosystem-proposals/pull/1#issuecomment-266717571 and build mechanism for tracking additional cabal-install bounds outside of the source code of packages themselves. Once that is done it seems to me that we won't need mutable packages in Hackage any more (that function being double covered by both plain old new package releases and by trustees/build infrastructure being able to set additional constraints outside of packages). I believe that the best way to build the core of the Haskell ecosystem is for package `foo-1.2.2.0` to refer to exactly one hash. Not a mutable result like we have now, not one hash if resolved against Hackage and another against Stackage, just one perfect, unchanging hash. Then we can build the rest of the ecosystem upon that base.
As others have said, your main speed boost will come from the appropriate data representation. That is, avoid lists especially if you are making lots of concatenations. If you are, maybe Sequence would be better. It seems the limitation is on speed, not memory, so maybe go nuts with structures and algorithms that take up more space with faster results.
To be more precise, it is, at the same time, the Church encoding of booleans and the Scott encoding of booleans. In particular, Scott encodings give you a very simple method of transforming any algebraic datatype into a lambda-calculus function.
[Mafia](https://github.com/ambiata/mafia) also does this and I think it actually does it better.
I'm not on twitter but I think this would be a bad idea.
Especially, I struggled with shuttling around `Strings`, `ByteStrings`, and `Text`. I thought `{-# LANGUAGE OverloadedStrings #-}` made all those problems go away, but clearly I still have a lot to learn!
&gt; learning data-structural bootstrapping Are there any good links for learning that?
But the package name and version pair (like `example-1.2.3.4`) *is* mutated. 
(Handwavy because I don't have code in front of me.) In a project I'm working on, we're using extensible records as the parameter to a `ReaderT`. The module that sets up the record defines classy lenses for each of the types that might be in the record, and the functions that actually do things have constraints to only require the fields that they need. In particular, the app passes in a connection pool when it sets up the contexts, but most of the code only wants a connection. So I wrote a `withConnection` function that embeds a computation-that-requires-a-connection into a computation-that-provides-a-connection-pool. By doing this with an extensible record, I don't have to worry about writing all these ad-hoc extra types that are morally equivalent to 2-tuples, and I don't have to write classy lens instances for them either.
Hi, i'm doing the same as you. I did the smashthecode contest in haskell. It was very computational oriented so i had to optimize heavily my code. Here it is : https://gitlab.com/vannnns/SmashTheCode/blob/master/smashTheCode.hs I managed to get to rank 247/2400. The optimizations i made boosted speed arround 20 times from the naive implementation. I used STM, bang, inline, STUArray, Seq, Unpack, and so on. Coming from a more permissive languages like C#, it was really hard for me to get things done in time with haskell. And i think the result is quite awful and not very idiomatic :) I didn't count hours profiling everything with the (really good) profiler options/logs and even with a 20x boost, i was still 100 to 1000 slower than other competitors with C++ engines. And I almost forgot to speak about the space leaks due to lazyness, very hard to hunt. To put it in a nutshell, i think haskell is not the right tool for heavy performance oriented tasks if you don't have a tons of experience with it. And still, you'll be 2 times, 4 times or 10 times lower on specific tasks. And being 2 times slower = losing versus higher ranked IA :) 
And [Jenga](https://github.com/erikd/jenga) lets you go the other way. Jenga makes it possible to build packages with a `stack.yml` file but without working version bounds in the cabal file using tools like [mafia](https://github.com/ambiata/mafia) as well as with cabal. &gt; I'd suggest first extending cabal to support Github dependencies so that Stackage Mafia is a very thin wrapper around cabal that supports this. 
inline-c offers 2 great advantages: 1. More type safety. With the standard `foreign imports` you can just write whatever signature you want, there's no check if it aligns with what is in the C header file. Get it wrong (or worse, slightly wrong) and bad surprises will get you at run-time. `inline-c` compiles your C quasi quoters so you get much more type safety. 2. With `inline-c` you can bundle related functions into a single lexically scoped snippet and thus make less mistakes. For example: C.block [| mem = mallocThing(..); other_c_function(haskellArg1, haskellArg1); freeThing(mem); |] You do not have to FFI-bind `mallocThing` and `freeThing` into Haskell functions in that case, which can make things easier to read, faster, async-exception safe and thus more correct.
The gitlab repo is empty.
Seems like assuming ill intent, given the results of the poll.
I don't have twitter, so here will have to do. I use hackage for most of my development and stackage for deployment, a split would result in me not using stack anymore.
IMO it makes sense, the value of a difference list is the difference between the input list and the output list.
OverloadedStrings only helps for string literals, i.e. "these" can be any of those types depending on the context.
I could not agree more. I'm shocked that all downstream distributions aren't flipping out over mutable existing versions. It certainly scares me as a user. 
Not yet, but I don't see any essential obstacles.
These won't be total answers but hope they help. 1. Type safety is really nice. You can enforce a lot of the requirements of your functions with the compiler and make some guarantees about the code working (only to an extent) 2. You can call any script from PHP (including Haskell ones), might not be the best way to doing things but I know of more than one big company that does this with a lot of success. 3. Spock looks pretty good but I don't have nearly enough experience to make promises about it especially if you have a scalability in mine. 4. Spock is a library for producing webpages. It's probably best to considered it a backend (though it serves a front end). Haskell itself is great for crunching numbers or data for APIs, that's where it shines if you ask me.
I'll answer (4) in particular. Haskell shines on the backend. It is incredibly performant, and it is pretty much unmatched when it comes to concurrency (Erlang is the only language that really comes to mind here). There are Haskell libraries for stripe which are very complete. I've used them in the past on a personal side business project, and they work (or at least they managed to put money into my account -- I'm not complaining). Personally, I think type-safety is really important here. I like knowing my payment code will not blow up in my user's faces because of a class and null pointer error ten files away. Personally, I always just use the `servant` package for all my websites because it's real easy to use and get started serving web pages or REST apis. 
So do you think if I'm working on a production site that it wouldn't hurt to just run a Haskell script from PHP if the client is insisting on using WordPress, for example? Or would it be better to just stick to PHP in this case? Also, I'm a bit confused about using Haskell for the front end. I've used front end javascript or CSS libraries... I guess I'm having a hard time understanding what a Haskell-based front end library would do since, being new to Haskell, I see it as a back end language. Could you give me a real world example of the benefits of using Haskell for the front end? Would it be similar to how Jekyll uses Ruby to compile a static site?
I'm pretty sure that Spock is not a front end library. Although if you've been using it in the browser, I'd be fascinated / horrified to hear more about that :)
Isn't there some modern guideline that recommends against writing your own monad tutorial? This is my favorite for the working programmer: http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html
IMO, monad is just the distilled pattern in common among those examples (and all other examples; but those are enough establish the pattern). Sure, there's some categorical tie-ins, but that can be largely ignored in practice, and `Monad` instances don't actually necessarily have those categorical properties. Which one do you recommend?
The main problem to me is that monad means something very abstract, even ignoring category theory. But explaining it by citing a few special examples such as those makes it seem like `Monad` is just a shitty solution for doing imperative programming; as a beginner, my thought when I read stuff like that was: &gt; Why would I even use Haskell if it requires this nonsense to do such simple tasks? I'd rather have a language that balances purity than one that makes you jump through hoops for it. `Monad` *is* a complex thing, no matter what people might try to tell you otherwise. It's not just the common pattern among a select few useful ideas. And the most important problem it solves in Haskell is a very simple one in many people's eyes. So I think it's better to keep things simple and shy away from these very specific and seemingly easy problems, because it doesn't instill any real insight. The only explanation of monads I've really liked is where you start by explaining what functors are in Haskell. Functors are really easy for anyone to grasp, and they're much more obviously powerful and useful. From there, you can simply say "Monads are functors that let you compose functions that look like this: `a -&gt; f b`." It's a very simple extension. And since you've already covered some functors, they'll ask "Hm so is this functor a monad? How about this one?" and they'll get to uncover for themselves just what it means for these things to be monads, rather than assigning meaning to monad from them.
Some of it seems to be done in CmmOpt.h, in `cmmMachOpFoldM`. I only know that because Simon's patch (that I'll be merging tomorrow morning) redid part of it.
Seriously the only reason this isn't done, I think, is performance. Every algebraic data type can be encoded with functions with the Boehm-Berarducci encoding.
I don't think that's a productive way to look at things. *All* things require examples. Even in math. Examples are key to learning about monads, but there are definitely some insights that speed up learning. 
I believe the basic arithmetic proofs should be similar to the standard binary ones, but with substantially more cases. The heap should look very similar, but with one or two binomial trees in each position rather than zero or one. Merging should be basically the same. Even `deleteMin` shouldn't be too bad, I don't think. All-ones is just as legit for zeroless as standard; that seems to fit nicely with the usual algorithm, but I'm not sure how it'll go with the fancy typey zipper and all.
But surely programming language designers are allowed? I mean, I think some kind of monad tutorial is a necessity for the documentation of any programming language. Unless you're going to go the Haskell sink or swim approach to learning the language, anyway.
Well, ok. But how does treating monads like a complicated thing help people understand why they should use Haskell even though it requires this complicated thing to do I/O?
I liked it better when von Neumann said it. "In math, you never understand anything; you just get used to it". But surely programming languages should come with some kind of documentation?
The admonishment about monad tutorials (as in [this classic formulation](https://byorgey.wordpress.com/2009/01/12/abstraction-intuition-and-the-monad-tutorial-fallacy/)) isn't directed at explanations of monads in general, but only at the ones driven by hand-wavy analogies. It doesn't really apply to something like your video.
I don't think that approach works for everyone, while pattern-matching is something the human brain was evolutionarily "designed" for. But, it certainly gives a more accurately abstract view of monads.
I don't see how the distilled pattern requires a side-effectful view of monads. I'm not even sure all the examples provided do.
This sounds like a communication problem that can be solved. If it's not the case already, Hackage trustees who modify package metadata should be required to file tickets against those packages explaining what they plan to do and why, and for the sake of accountability should link to those tickets as part of the metadata update.
Sorry, was thinking in terms of services as backend APIs vs web servers (which provide front ends). Edited to try to be more clear.
I figured I'd ask just in case. I had heist working pretty well with ghcjs at one point, although right at this moment I don't quite remember why :) 
Eff implementations (`freer` etc.) solve this M*N instance problem perfectly. - mtl typeclasses are data type definitions in Eff - mtl instances are functions in Eff So you avoid problems with typeclasses and you can combine an Eff type coming from a library with a handler coming from another library without problems. (I should write about Eff sometime, it's great but encoding it in Haskell has some problems -- we need a good language with built-in Eff support for good type/effect inference and error messages!) EDIT: care to explain the downvotes?
Hackage /is/ immutable. You always have access to all earlier versions of tarballs and metadata (cabal files, identified by their revision).
Ah interesting! Maybe I can make those functions polykinded. 
Full disclosure: I think `Eff` is great! It’s almost certainly a more elegant solution in the majority of real cases than mtl. As you say yourself, though, it isn’t perfect, and it has problems: the type errors are awful, it’s not as performant, and it doesn’t really cope with effects for which composition order matters. I will happily throw away all my typeclasses and transformers when `Eff` becomes both seriously competitive and reasonably well-adopted for the libraries I care about. In the meantime, it’s still worth spending effort improving the mtl-style workflow, since that’s what I’m mostly writing. This is a practically-motivated blog post. Don’t interpret it as anything particularly ideologically pure.
I use a package [string-class](http://hackage.haskell.org/package/string-class), it has simple `toString/toText/toLazyText/toStrictByteString/etc` methods
For me [http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html](http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html) is the best of all monad tutorials.
Woops, fixed :)
I'd be very supportive of improvements even if it didn't become user visible. I am heavily reliant on the end results of the matrix builder - namely the bounds fixes the trustees make to ensure that old versions of packages build. It means I can test things I release against old versions of GHC easily. Before all the various bounds fixes, it was a lot of effort to manually find local constraints to get things working.
Sounds like fun... I think :/
I don't like monad transformers at all. It is one more byzantine complication that the Devil put on of the road to success of Haskell, but I think that your solution is a great relief for the pain.
Couldn't we make an FAQ sticky for monad tutorials etc. ?
Definitely!
` java.lang.reflect.WhatAmIDoingWithMyLife`
Nomic is very much a lawmaking game. It's an experiment about the [Self amendment paradox](http://legacy.earlham.edu/~peters/writing/psa/)
Thanks for that comment! I just did some reading, and both parser combinators and monad transformers "clicked" for me for the first time.
&gt; On the other hand, with centralized package storage on Hackage right now, it's very unusual for someone to keep code only on Github if it's intended to be used by others That doesn't mean that they're not *valuable* to the community, though. Checking [this result set](https://github.com/search?utf8=%E2%9C%93&amp;q=language%3AHaskell&amp;type=Repositories&amp;ref=advsearch&amp;l=&amp;l=Haskell), I can already see some pretty interesting stuff: * dpp/LispHaskellIPad - An iPad app that's a Lisp interpretter... written in Haskel * geekrelief/as3tohaxe - An Actionscript 3 to haXe source converter written in Haskell * jamesdabbs/pi-base.hs - A Haskell-powered modernization of Counterexamples in Topology * waldheinz/ads - A Freenet clone written in Haskell * bmillwood/stepeval - A program for evaluating a Haskell expression step-by-step * patperry/hs-monte-carlo - A Monte Carlo monad and transformer for Haskell. Some of these can be used as libraries, others are just applications. /end of relevance to Stackage But these applications are, in my opinion, just as valuable as libraries - we are heavily suffering from a lack of good documentation, especially examples, of how all these Haskell libraries are tied together into an actual application. To reiterate, Stackage should not preoccupy itself with documenting these applications, but we should definitely organize a crawl through Github to document applications that can serve as examples (or from which we can extract examples) for libraries, as well as finding out about some cool libraries that were not on Hackage / Stackage :)
Developing an AI for Nomic would be definitely challenging. It would need to navigate around a lot of ["halting problems"](https://en.wikipedia.org/wiki/Halting_problem) and ["turing tarpits"](https://en.wikipedia.org/wiki/Turing_tarpit)... A first step would be to simplify the [Nomyx language](https://hackage.haskell.org/package/nomyx-language), making it non turing complete, IMO.
Maybe using some heuristics such as an evolutionary algorithm could be used to generated Nomyx rules!
Sounds to me like you're identifying a highly useful—and completely orthogonal—concept. Perhaps something like a blog, weekly newsletter, or even podcast dedicated to taking these "random" bits of Haskell code floating in the ether and talking about them would be a Good Thing.
&gt; To put it in a nutshell, i think haskell is not the right tool for heavy performance oriented tasks if you don't have a tons of experience with it. You don't need tons of experience with it. You just need a few things: * Understand the difference between laziness and strictness and what strictness annotations do. * Use strict types where ever you can, e.g. `Text` instead of `String`, `Vector` instead of `List`, unless you really know what you are doing. * Almost all good things in Haskell are outside the `base` library. Some of it, like `System.Random`, is outright terrible performance-wise. * Don't use lazy tuples. They are not what you think they are. In my experience they are the biggest roadblock for the strictness analyzer. Use strict tuples `data Pair = Pair !a !b`. * Understand that `IO` is your friend and helps you to cleanly separate impure from pure code. `ST` is more advanced, but `IO` is much easier to use. * Use mutable vectors from the vectors package, not arrays from the base package, especially the unboxed and storable variants. If that's not an option you can always use the `Foreign` module and practically write C in Haskell. Here is an example: Prelude&gt; import Foreign Prelude Foreign&gt; let read = peekElemOff Prelude Foreign&gt; let write = pokeElemOff Prelude Foreign&gt; array &lt;- mallocBytes 32 :: IO (Ptr Double) Prelude Foreign&gt; write array 0 0.5 Prelude Foreign&gt; write array 1 1.5 Prelude Foreign&gt; write array 2 3 Prelude Foreign&gt; traverse (read array) [0, 1, 2] [0.5,1.5,3.0] I hope you won't take this the wrong way, but I can tell just by looking at your code that this is 100x slower that idiomatic C. If you write strict functions with mutable arrays you should at least reach Java performance. TL;DR: write strict code and don't be afraid of `IO`.
Interesting article! This seems like it could do away with a lot of the annoyance when defining a custom monad typeclass. It probably isn't quite enough to solve my biggest gripe with mtl, though, namely that it makes a more fine grained io approach unviable. Actually, would a tiered approach work? Like: class (Monad m) =&gt; ConsoleIO m where get :: m String put :: String -&gt; m () -- ..... instance ConsoleIO IO where get = getLine put = putStrLn -- ..... doStuff :: (MonadThrow m, MonadBase b m, ConsoleIO b) =&gt; m () doStuff = do ln &lt;- liftBase get when (null ln) $ throw (UserError "Line can't be empty") liftBase $ put ("The input was: " ++ ln ++ "!")
There's also `string-conversions`, which just converts to whichever type fits using `cs`.
We do that actually when it makes sense! This is likely not apparent to the casual observer, but it wouldn't have helped in this instance, as such filed tickets have been recently met with a bit of disdain by these two particular gentlemen as [e.g. here](https://github.com/input-output-hk/pvss-haskell/issues/9). Luckily, such unfortunate interactions are the exception rather than the norm. That being said, the incident was resolved swiftly as soon as I got wind of it.
The issue is most programmers come from a background where non-terminating discussion is normal. You can't just take that away from a community which is already dependent on it.
By performance you mean inefficient memory usage? Is it impossible to represent functions in memory in a really compact way? I tried to google lambda lalculus function memory representation but didn't find anything relevant. 
If you think I just now "got" monads, then I've utterly failed.
Hmm. You're still answering someone who's just started using Haskell, and has discovered (supposedly --- of course it isn't really true!) "you need to understand this thing from category theory in order to write Hello, World in Haskell", right? And your answer is "start with understanding pure functions, build several mathematical concepts that don't seem at all practical right now on it, and then eventually we'll get to Hello, world"? I must be mis-understanding you. However: it has been my observation that I/O tends to be chapter 3 or 4 of Haskell introductions. (After several data structures and algorithms and concepts like recursion etc. are introduced). Which, in my opinion, is one of the main reasons Haskell is known as a research toy language with no real-world applications.
I mean it's just harder to reason about church encoded things in general.
Ahh, indeed there is a little constant folder there! Today I learned, thanks!
I hang out in #snowdrift and I'm happy to help anyone interested in the project at any skill level. Seriously, if you want to contribute but can't even read a type signature, just hit me up. We can work through some simple bugs/features together via video chat (or if you're in Boston...).
&gt; A couple weeks earlier there was a discussion on tuple instances on this list that got somewhat out of hand, leading to a meta-discussion on civility. Hm, before I read any further, does anyone know what thread this is referring to so I can catch up?
https://mail.haskell.org/pipermail/libraries/2017-March/027824.html
I didn't have time to watch the video before my first post in the thread. I was just reacting to the title and text of your post. It certainly had the tone and content of a brand-new Haskeller proselytizing their new way to understand monads as simple. I don't read / watch "What is a Monad?" stuff anymore because I'm currently comfortable with my current understanding of them. Sure, there's always more to learn, but right now I'm focusing on learning correctness-through-dependent-types.
Commenting without reading the fine article is, of course, the best way to help reddit as a community. But seriously, haven't you spent more than 15 minutes on this thread by now? In any case, I was really interested in getting feedback from other advanced users on my approach to presenting the concepts, so thanks for your feedback!
I think the difficulty might come in over-specification. The context will often have more bindings than the set of free variables in the term, so the context should really have *two* row variables, one matching the term and the second encapsulating the rest. Exactly matching the term specification would probably be too strong a precondition. I don't think I've ever seen that though.
I guess "polykinded" means that a function can accept a argument with different kind ?
I couldn't read anything you wrote because it is too tiny and there's a reflection at the top of the board. I tried following anyway, but it really was feeling like hard work and I didn't finish. It's a relatively short video. I suggest you watch it yourself and make some notes on what you can improve, then re-make it. Perhaps write a script so you aren't changing direction so much, especially at the start.
A few days ago I've written a bit about my experience building a hobby website with Haskell and Spock ([link](https://gilmi.me/post/2017/04/25/building-gathering)). This might be relevant for you as well.
Could you provide an example? 
I think the join/fmap based explanation is way more intuitive than using bind or examples. You need to first understand higher kinded types like `Maybe` or `[]` - usually fairly early in an intro to Haskell - then you need to understand `Functor` - usually introduced right after - then you really can explain the categoric `Monad`... you just have to use tools you've already introduced.
At the very least, I would expect to be notified when one of my packages is revised by a Hackage trustee. 
Probably very unpopular opinion but abandon Hackage and make Stackage an actual package host. I hate the PVP and especially upper bounds. I hate how Hackage has maintainers who mess with uploaded packages (specifically, add upper bounds to my packages without my consent). Pretty much ALL other languages' package hosts have moderators that only touch packages in extreme cases like malware or copyright infringement. Hackage is Not Like Everyone Else and that makes me feel extremely unwelcome. I don't want to publish Haskell libraries anymore.
Your data types are now Turing Complete functions. Via Rice's theorem, any interesting property is now undecidable on them.
I see what you did there.
It just happen that all of our tooling that we use today respects cabal revisions, and default to always take the latest revision. It doesn't have to though! Tooling (cabal-install, stack) has to explicitly opt into it. If you use the hackage API to get the tarball for a particular version of a package, you always get the original file. The pristine, unmodified tarball, the exact same one that the uploader created on his local machine. The tooling has to go great lengths to then extract the latest revision of the cabal file and replace the original with the updated one. Which is why I don't think snoymans idea of putting these revisions somewhere else than hackage will make the situation any better. It doesn't matter where these 'mutations' are stored.
Not using mtl? Isn't it the solution we always have?
I honestly do not think the page you linked to is a good resource. For one, the pictures showing monad as boxes are very misleading. It gave the wrong impression that Monads are like containers, while they are not. 
&gt; I really wish the &lt;derogatory remark&gt; would &lt;derogatory remark&gt; , and &lt;derogatory remark&gt; where &lt;derogatory remark&gt; I think a code of conduct is great precisely to filter comments such as yours.
Alright, sorry, my example didn't really show what I meant. When working in IO those two are the same as you said: import Control.Monad.Base runInIO :: IO () runInIO = do testDirect "IO" testWithMonadBase "IO" testDirect :: (ConsoleIO m) =&gt; String -&gt; m () testDirect stack = do ln &lt;- getLn putLn ("Directly in " ++stack"++ ": " ++ln++ "!") testWithMonadBase :: (MonadBase b m, ConsoleIO b) =&gt; String -&gt; m () testWithBase stack = do ln &lt;- liftBase getLn liftBase $ putLn ("With MonadBase in " ++stack"++ ": " ++ln++ "!") This indirection is more interesting when working in transformers, though: import Control.Monad.State runInStateT :: StateT String IO () runInStateT = do s &lt;- get -- testDirect s testWithMonadBase s put "done" testDirect is commented out here because it would fail with ```* No instance for (ConsoleIO (StateT [Char] IO)) arising from a use of `testDirect'```. testWithMonadBase just works without defining any additional instances. 
I'm the lead dev that recently had to find new income. I'm still around, ish. I still think this project is interesting and worthwhile. We definitely need as many viable methods for supporting public works as possible. Look at [OpenAI](https://openai.com/about/), whose mission is to make AI free and accessible. Why are they doing that? Basically, to save the human race from being swept under the rug by the singularity. If you don't think that will happen, let Freddie deBoer remind you of the [mindset of people currently leading us toward it](https://medium.com/@freddiedeboer/the-three-hot-trends-in-silicon-valley-horseshit-95cc5a85e8a4). If human individuality matters, then FLO matters, and democratic funding thereof matters. (And never mind AI; that's just an idea I was thinking about recently because of Wait But Why's article on Neuralink. You can add: psuedonymity, free expression, the pursuit of happiness, and [all those pesky ideals](http://www.ushistory.org/Declaration/document/) that have been in vogue for a few hundred years.) Snowdrift's mission isn't The One True Solution, but it's certainly an idea worth trying. The Haskelly nature of Snowdrift is... well... an implementation detail. And maybe not the smartest choice for a public, poorly funded project. However, to the extent that there *are* Haskellers out in the world, and that they (you) can help out, well... welcome to the team. :)
That was indeed a terribly rude response you got! It sounds like they didn't read your message closely and dismissed it as a complaint about a PVP violation, but even if so, calling it spam is completely inappropriate. Speaking of PVP complaints, I haven't had a chance to make the `containers` documentation fix to resolve yours, but when I do I will close it--there are limits to how far I'm willing to go to abide by PVP and that one is over the line.
Semantically there is nothing that can be achieved in Haskell that cannot be replicated in a strict language. But lazy programming in a strict language does incur syntactic overhead. I'm not a Scala programmer so it's hard for me to say how much of an inconvenience this is in practice. In existing dialects of ML, lazy programming is somewhat bothersome, but there are proposed fixes for future dialects.
 def all[A](s:Stream[A], p: A ⇒ Boolean) = s.map(x ⇒ p(x)).forall(identity) 
Well, no real reason for that IO constraint: testState :: (MonadBase b m, ConsoleIO b) =&gt; StateT String m () testState = do s &lt;- get testWithBase s put "done" Then it is possible to add alternative interpreters with a single trivial MonadBase instance: {-# Language GeneralizedNewtypeDeriving #-} {-# Language MultiParamTypeClasses #-} import Data.List import Control.Monad.RWS.Strict main = do putStrLn. pretty $ evalStateT testState "StateT IO ()" putStrLn (replicate 10 '-') exec $ evalStateT testState "StateT IO ()" exec :: IO () -&gt; IO () exec = id pretty :: PrettyPrint () -&gt; String pretty (PrettyPrint p) = intercalate "; " . snd $ evalRWS p () 0 newtype PrettyPrint a = PrettyPrint ( RWS () [String] Int a ) deriving (Monad, Applicative, Functor) instance MonadBase PrettyPrint PrettyPrint where liftBase = id instance ConsoleIO PrettyPrint where getLn = PrettyPrint $ do cur &lt;- state (\i -&gt; (i, i+1)) tell ["Get #" ++ show cur] return ("$"++show cur) putLn s = PrettyPrint $ do tell ["Put: `" ++ s ++ "`"] And running main gives: *Main&gt; main Get #0; Put: `With MonadBase in StateT IO (): $0!` ---------- hi With MonadBase in StateT IO (): hi!
And more generally all "solution space exploration algorithms" are much more elegant in a lazy language, as you can decouple the traversal of the space from the generation of it. See "Why functional programming matters" by John Hughes (https://youtu.be/FGQAP0GxlW8?t=33m15s)
As far as I know, Scala's laziness is weaker. In Scala, a lazy "value" is actually just a function `() -&gt; a`. With thunk'd laziness, you can get way more powerful using recursion. split :: [a] -&gt; ([a], [a]) split xs = (ys, zs) where f a (ys', zs', i) = let (ys'', zs'') = if i &gt; mid then (a : ys', zs') else (ys', a : zs') in (ys'', zs'', i + 1) mid = len `div` 2 (ys, zs, len) = foldr f ([], [], 0 :: Int) xs This function splits a list in half using one pass with `foldr` by lazily and recursively referencing one of the thunks that it produces. With Scala style laziness, recursion like this will loop infinitely and duplicate the work each iteration. But with thunk'd laziness, you can reference things in really weird ways. Plus, with Scala style laziness, you can't efficiently pass the same lazy value to multiple functions, because those functions will duplicate the work when they both call the function independently.
Parsing combinators, simple language: a^n b^n S -&gt; a S b | eps And in Haskell, quite the same s = Parens &lt;$ char 'a' &lt;*&gt; s &lt;* char 'b' &lt;|&gt; pure Eps Here the solution in Scala would be simple as well, we'd use some explicit lazy[A](p: =&gt; Parser[A]): Parser[A] on `s`; but in any non-trivial real world parser I don't want to think where to insert those magic "make it work" combinators.
But it doesn't work. foldl' (&amp;&amp;) True (repeat False)
Because that implementation is wrong. `foldl'` isn’t lazy, and it doesn’t have to do with `seq`; `foldl` fundamentally requires traversing the whole list to produce a result. The correct implementation should use `foldr`, which can be lazy: and = foldr (&amp;&amp;) True
thanks, corrected.
Is there any article that explains, how Haskell decides to stop calculation here? foldr (&amp;&amp;) True (repeat False)
Types of kind `* -&gt; *` covariant in their first argument are all like boxes.
You can figure it out yourself by performing reduction evaluation yourself. Start with the definitions of `(&amp;&amp;)` and `foldr`: -- not the only way to define (&amp;&amp;), but a way to highlight its laziness x &amp;&amp; y = if x then y else x -- we’ll stay specialized to [] to keep things simple foldr _ y [] = y foldr f y (x:xs) = f x (foldr f y xs) Now we can expand `foldr (&amp;&amp;) True (repeat False)`: foldr (&amp;&amp;) True [False, False, False, ...] = False &amp;&amp; foldr (&amp;&amp;) True [False, False, False, ...] = if False then foldr (&amp;&amp;) True [False, False, ...] else False = False The `then` case is never evaluated, since `if ... then ... else` is obviously lazy (like it is in basically all languages), and we just get `False`.
Meh. The REPL's nice, but you're still expecting people to "learn Haskell" for a long time before they can actually write any programs in it. It's not really learning by doing, even, at that point. (Part of our disagreement --- I think --- is that you're still assuming someone is highly motivated to learn Haskell, and is willing to go through whatever learning process is necessary. I'm assuming someone who doesn't get why they would learn Haskell in the first place, and saying your approach is more likely to just turn them off it entirely.)
So... `show :: Int -&gt; String` contains a `String`?
&gt;When discussing laziness, defining it using `if ... then ... else` is easier to grok for those not intimately familiar with Haskell, I think. That does makes sense. I just added that comment because given the OP's line of questioning it seems worth it insisting on the fact that `if ... then ... else` isn't special in Haskell.
Referential transparency (distilled as "Why the type of `putStrLn` can't be `String -&gt; ()`") is one good motivation to bring into such an introduction. I used something in that vein for [Chapter 0 of the monads unit at the Wikibook](https://en.wikibooks.org/wiki/Haskell/Prologue:_IO,_an_applicative_functor) (though there is at least one very obvious difference between that and what we are discussing here). That topic makes even more sense considering that your paradigmatic examples of monads in GlobalScript are, I gather, program/markup building ones. I'm tempted to suggest bringing it more to the forefront, as it almost feels the canonical "A monad is a triple..." presentation in your video gets in the way of the explanation you allude to in the first few minutes. In any case, it's not really a big deal, given that I'm pretty sure you'll come back to that in the follow up videos. (By the way, there is certainly an affinity between these remarks and the discussion between /u/ElvishJerricco and /u/bss03 in the parallel subthread. I'm not sure yet of what else to say about that discussion, as I feel both of them are right :))
I'm not quite sure this level of craziness enabled by laziness is easy to get working well. 
Sure, it contains an `Int`ful of strings. One at `0`, one at `1`, one at `2`, and so on.
FP flame-war 1979 "It is indeed depressing to anyone who respects your truly exceptional talents to see you of late apparently spending a great deal of time on shallow, negative polemics that contribute almost nothing in the way of real knowledge or deep truths to the subject matter they deal with." https://medium.com/@acidflask/this-guys-arrogance-takes-your-breath-away-5b903624ca5f 
&gt; PVP is the only reason I've heard for people not wanting to upload to hackage [Those hating people](https://www.reddit.com/r/haskell/comments/67tufo/poll_shall_stackage_split_up_with_hackage_and_see/dgvbtyt/) maybe just need a hug!
I wish there were more remote haskell jobs :(
Where can I find the .xsd?
Anything with a lazy data structure that doesn't already have an implemented stream analogue, such as a lazy tree or graph. A fair while ago I did a simple AI course where the search graph would have been enormous, but mostly unexplored, so I just defined the whole thing anyway. Parsing... this is an excerpt from something I was writing the other day: pattern = msum [... , parens constrPat ... , parens pattern ... ] constrPat = liftA2 PCase (constructor &lt;* skipSpace) patterns patterns = sepBy1 pattern someSpace These are not functions. They are mutually recursive parser definitions to parse a nested pattern.
and `protolude` makes all of these standard in an alternative`Prelude`.
Any plans on ever having Haskell work in your New York office?
Ooookay. I think you are sort of stretching the normal definition of "contains", but if you stretch it enough, I suppose you are correct. What about `getLine :: IO String` or `Proxy :: Proxy String`? Do they contain `String`s?
How can you "split" the rows of the context with row polymorphism? The context has rows "r", where "r = fvs ++ rest", with the expression having rows "fvs". I'm not familiar with languages with row polymorphism (only checked Elm/Purescript out), so I don't know how the above could be defined. In my work I used a "IsSubSet fvs ctxVars" predicate, like: eval : Context ctxVars -&gt; IsSubSet fvs ctxVars -&gt; Exp fvs You could even do it like so: eval : Context ctxVars -&gt; (e : Exp) -&gt; IsSubSet (fvs e) ctxVars 
Not at the moment, I'm afraid.
&gt; [I]t might help to take teaching advise from people that had a more "normal" learning experience. Maybe. But of course I'm also being advised to listen to the 10% of programmers who *have* successfully learned Haskell, and ignore the objections of the 90% of programmers who think Haskell is an academic toy.
This seems pretty similar to the iterate function, except with a monad. You can write a monadic variant maybe something like this: iterateM :: Monad m =&gt; Int -&gt; (a -&gt; m a) -&gt; a -&gt; m [a] iterateM 0 _ _ = pure [] iterateM n f x = (x :) &lt;$&gt; (iterateM (n-1) f =&lt;&lt; f x) Then something like this would give you what you're looking for: iterateM 3 (selectRandom . Map.findWithDefault ["."] str) =&lt;&lt; selectRandom (Map.keys myMap) You have to ensure that every value is a key in the map also, though, I think.
So there's a fun function called `unfoldrM`. The type is `(a -&gt; m (Maybe (b, a))) -&gt; a -&gt; m [b]`. In English, that means, given a seed value, call some monadic function function that maybe returns the next item in your list as well as a new seed in some monad , and keep doing this as long as you have seeds to work with. You'll need: pair a = (a,a) Now you can tweak getNext to: getNext :: Map.Map String [String] -&gt; String -&gt; IO (Maybe (String,String)) getNext m str = sequence $ fmap pair . selectRandom &lt;$&gt; Map.lookup str m That is, given a lookup table, produce a function that takes a string, and if that string is in your table, returns the next random string as `Just (str,str)` in IO, otherwise return `Nothing` in IO. The first str is the value you're going to output to your list, the second str is the seed value for your next lookup and select random. Finally, your whole value will be: strung m str = fmap ((++".") . intercalate " " . (str:)) $ unfoldr (getNext m) str
Ask and you shall receive: https://functionaljobs.com/ - we're at the very top.
I think every single listing on that board is for a senior position.
Let's guess, 28+ years of haskell experience required?
I've dabbled in Haskell, and never fully grokked monads. The video wasn't a bad first try, but pretty much every aspect bears polishing. On the topic of Haskell &amp; monads, I gather two things: - Haskell is putting a lot of what would be the coding logic in every other sort of language into the type system. This has profound consequences. If everything is a strongly typed function, any impure bits (e.g. reality) that are of concern have to submit to the strongly typed dress code of the language. - Beside that, Haskell's execution model means that pure code is a set of suggestions to the compiler to find the optimal way to execute. To get things to operate less purely and more sequentially, we have to drag the compiler by the nose. The "monad laws" offer enough "tab A/slot B" guarantees to string things together, but you're always defining strongly typed functions. You never touch the variable directly; the program will evaluate it and move the return value along as needed in your program. I think this, but I have no confidence I can say it in code yet. Thanks, Chris
The main reason that I haven't switched `mtl` over to a scheme like this already is that, as you noted in the article, it can't be done in a backwards compatible manner. =(
Yes, it’s unfortunate. :/ Something like hvr’s suggestion in [this ticket](https://ghc.haskell.org/trac/ghc/ticket/7395#comment:7) seems like it might help a little bit here, though I don’t think it’s terribly satisfying. As a small aside, do you know why `reader` is a member of `MonadReader` at all? I cannot come up with a situation in which it could be made more efficient than its default implementation.
Yeah, SmashTheCode was the hardest one. I implemented Simulated Annealing, with number of iterations limited by allowed time. Well, it was up to 150, which is shame for SA, while others made in order of 100K. Still don't know what is better way, will look to your code, thanks!
It seems one needs to create an account in Taiga and then you can see the issues: https://tree.taiga.io/project/snowdrift/wiki/home
&gt; The idea with each of 'reader' 'state' 'writer' etc is to provide the monad homomorphism from the simple version of each monad to its transformer. This makes sense to me, but I’m not sure I understand what putting those things in the mtl classes themselves buys you. It seems possible I’m just not seeing the forest for the trees, but I’m mostly approaching these classes from an engineering point of view rather than a categorical one, and I don’t currently see the engineering usefulness. It seems a categorically elegant formulation would look quite different from mtl, anyway, so I don’t fully grok why they’re thrown in there. &gt; Also, regarding cost, for "small" things everything is fine like your intuition would guide you, but […] requires a full round trip through the monad transformer stack. This, too, makes sense, though it’s unsatisfying. I guess the approach mtl takes is an acceptable compromise, though. &gt; If we do decide to make a major breaking change in the `mtl` any time soon, it'd likely be to break apart MonadWriter and MonadState into their `ask` and `tell` parts, which are algebraic, and their non-algebraic handler-like `local`, `pass` and `listen` parts. As there are instances that could support the former but not the latter. Everything but the last sentence of this is over my head, and truthfully, I think there is something to be said for mtl’s *simplicity*. If you can keep the API accessible, though, obviously I trust you know what you’re doing. :)
Your record types are very close to the `NP` type in generics-sop. The `zipWith` function you propose is generically available as `hzipWith` in generics-sop, and there's also one called `hczipWith` for constraints. A `hsequence` function is also available. Converting a type such as the concrete `Record` into an `NP` is currently not provided out of the box, because the standard conversion goes from a Haskell datatype to an `NP Identity`, and normally you do not have datatypes where every field is parameterized by some type constructor `f`. However, it is reasonably easy to define this kind of conversion, at least in the case where all fields are parameterized by `f`, such as demonstrated for example by the gists https://gist.github.com/kosmikus/11523015ed5224de165354af9a40bec7 and https://gist.github.com/kosmikus/1b40cbc4f0e6abf3f47991f2b22bfaf8 . one-liner does most things generics-sop does, and is probably a bit simpler and more accessible. But if you have cases that go beyond the limits of one-liner, generics-sop might still work for you.
You are right. I couldn't say it any better. 
I'll let you know
Out of curiosity, are companies in the UK or Singapore having problems finding local talent? I also understand that both countries have begun to tighten up when it comes to immigration. Have you noticed the effects of such things?
&gt; Do you also hate it when your packages build successfully for more people? Because that's what the maintainers are doing. Yes :P Literally *all* other package registries I ever published to — PyPI, RubyGems, Clojars, npm, Hex.pm, Crates.io — make me feel like I'm in control. Hackage maintainers make me feel like I'm not. *Stackage* is there to make the packages build successfully. When there's no upper bounds, the build on Stackage fails with an actual error *when something actually breaks*. With upper bounds, the build on Stackage fails with a useless version error *even if nothing in the code actually broke*, and the burden is on me to update my upper bounds! Again, *even if nothing in the code actually broke*. I strongly dislike pessimistic versioning, because I'm used to just throwing the libraries out there, forgetting about them for a few years and coming back to fix *when actual people tell me something's broken*. Or Stackage tells me — but still *about actual errors* not version errors.
I certainly do :) But your dismissive comment is completely unhelpful.
Why do you need multiple types of effects in one function? The is only one type of effects - effects necessary to implement the function. If you want to encode effects in function type (assuming it is a good idea), just introduce new typeclass for this particular use case. It's actually just an instance of interface segregation principle. And yes, often IO is the way to go.
No, you can use any email address. We might ask for proof of enrollment later via email though.
&gt; IO is a non-deterministic one-value container. There's a value inside, but you never know what it is and you're not allowed to look inside so you don't break purity. At that point this is so far from a container that the value you get from this analogy is negative imho. Especially when (in the case of IO), you can say a "computation that returns a value" and talk about its monad instance as assembling bigger computations
As far as I can tell, you can do exactly what you propose.
View patterns + pattern synonyms is what you want
Nope, I will check them. If they do exactly that, is there any reason they are not enabled by default? 
The [paper](http://cs.brynmawr.edu/~rae/papers/2016/patsyns/pattern-synonyms.pdf) is probably a better tutorial.
The latter transformation is dangerous because it may hide a division by zero. 
&gt; The is only one type of effects - effects necessary to implement the function. If you want to encode effects in function type (assuming it is a good idea), just introduce new typeclass for this particular use case. But...that's mtl. (well, the same finally tagless style that is at the core of mtl, see [mtl is Not a Monad Transformer Library] (https://blog.jle.im/entry/mtl-is-not-a-monad-transformer-library.html)). You can use mtl and interpret anything in IO just fine. 
So, my eyes contain dark matter? I mean, there's always none of it in there, so it's the same way `Proxy` contains a `String`. If you are going to stretch the meaning of "contains" far enough to include "does not hold within, ever", then yes all functors are containers. But, then again, all zygohistomorphic prepromorphisms are also containers, as are all comonads, and all kinds, and all words.
Coverage checking. While GHC doesn't do totality checking, when -Wall is on, it does try and warn you when a `case` or a function definition does not have enough patterns to cover all of the input cases. Unfortunately, that's virtually impossible when you allow arbitrary functions to determine a match. Also, Haskell has a very capable type system and it is actually possible to expose the data constructors of many types without sacrificing correctness. You do sacrifice abstraction, but you can get it back easily enough (e.g. representing a list-like structure though a fold/unfold rather than through cons/nil).
&gt; I don’t currently see the engineering usefulness. From an engineering perspective each of them can typically be about twice as efficient as reconstructing them from the other operations and the laws, because they need "one walk" through the transformer stack rather than two.
A container is something that gives you access to its content, so I would not say `IO` is one. By even if we want to accept it is, I still don't think it's a useful analogy, cause people will ask how to access the value inside it (because that's what you do with containers). In my teaching experience, that's not a useful mental model. 
They are still a rather experimental feature. The language itself evolves very slowly. Only a few things have been added to the language proper since 1998. On the plus side, unlike almost every other language out there that has been around that long, other than, say, scheme, it means making a compliant implementation is still viable. Almost everything interesting or at least experimental in Haskell is accessed through extensions, so that other Haskell implementations can implement them piecemeal and we can explore the consequences of one design over another.
 myMap :: Map.Map String [String] step :: IO String -&gt; IO String step = (flip getNext myMap =&lt;&lt;) ioStrings :: [IO String] ioStrings = take 10 . iterate step $ return "seed" strings :: IO [String] strings = sequence ioStrings 
Project:M36 has features like this: https://github.com/agentm/project-m36
Haskell2010 introduced pattern guards, which I use in practise for this sort of thing.
To access the contents of `IO` you "merely" have to provide the state of the `RealWorld` at the moment. Normally we do that by threading `State# RealWorld` given to us in `main`. Or you can cheat, access the `RealWorld` and tell no one, which is what `unsafePerformIO` does.
&gt; Any way, you don't have mtl mentioned in .cabal file, so you don't care whether mtl has all of the N*M instances or not. What I'm trying to say is that the N*M instances is a problem with transformers, not with mtl. If all of your classes translate into IO you only need N instances, even with mtl (one instance per class, like you would expect). &gt; Most of the time I don't even want to encode effects in types, so I can do everything in IO just fine. Sure. I find that hard to reason about in most cases, but it's a solution (I tend to do it quite a lot in Scala, where effect abstractions still don't work as well as in Haskell)
There is one type system extension, `RelaxedPolyRec`, that is not only enabled by default but actually impossible to turn off. That's basically because the restriction it relaxes is (to the best of my knowledge) universally considered a bad idea. Indeed, it's rather difficult, reading the Report, to understand exactly what the restriction is supposed to mean.
I don't think your definition of "container" is necessarily wrong, just not useful, because it's so broad that it does not carry information anymore. It's just a notch above "all monads are things".
&gt; Literally all other package registries I ever published to...make me feel like I'm in control. Hackage maintainers make me feel like I'm not. Maybe if you followed the clearly stated [Guidelines for Hackage Packages](http://hackage.haskell.org/), you wouldn't feel this way. &gt; With upper bounds, the build on Stackage fails with a useless version error That's a problem with Stackage, not version bounds. Version bounds are simply information. Information that can very trivially be ignored. And if you ignore it, you get the exact same behavior you would have had if you leave the upper bounds off. `cabal` even has an `--allow-newer` flag that makes it easy for you to do this. Also, if people are trying to build just your package, the odds that it will fail with version bounds are MUCH lower. It will only fail with a version error if they are trying to force your package to build against a dependency version that is outside the bounds. In that case, you want it to tell you because dependency versions are much easier to solve than random compile errors. And like I just mentioned, they can solve it very easily with `--allow-newer`. This problem of trying to force your package to build against dependency versions outside the bounds will be reduced to record lows thanks to the work on [nix-style local builds](http://blog.ezyang.com/2016/05/announcing-cabal-new-build-nix-style-local-builds/) for cabal. You can already use the prototype today as `cabal new-build` if you're willing to put up with some rough edges since it's a work in progress. Failing with a version error is also not useless because it gives you more information and more options. It could be that you can build just fine with a different version of a package. If you had no bounds and your build failed with a compile error this would be very hard to figure out. But if you have bounds and you get a version error you can choose to try to move back to building with the older version or relax your constraints to try to build with the newer version. If you relax your constraints and get a compile error, you then know exactly what libraries are involved. Without version bounds things fail in very non-local ways. If `foo` depends on `bar` which depends on `baz`, `cabal install foo` can break seemingly spontaneously because of a breaking change in package `baz` if `bar` didn't include version bounds on `baz` even though `foo` didn't change at all. One day `foo` is building fine. The next day it's broken. I've actually had this happen to my packages multiple times. In this way upper bounds serve as a layer of abstraction that insulates you from downstream changes. It allows you to only worry about your immediate dependencies instead of having to worry about the whole transitive dependency tree. If you don't use upper bounds, you're shifting this burden to all your users. It's much less work for the community if each package author handles the dependency questions for that package once and for all by specifying accurate upper bounds. &gt; I'm used to just throwing the libraries out there, forgetting about them for a few years and coming back to fix when actual people tell me something's broken. This comparison is misleading for several reasons: First, those communities are almost definitely larger, have more contributors, and are consequently in a more stable state. This combined with Haskell being much easier to refactor and the Haskell community being more willing to make breaking API changes means that the Haskell ecosystem is likely shifting much more rapidly. (Also, [I'm not the only person to think this](https://www.reddit.com/r/haskell/comments/67u9py/enough_with_backwards_compatibility/dgvdym7/).) I suspect that's the real cause of your difference in perception. Second, Haskell's static type system makes these challenges a lot more visible because so many problems are caught up front when you build. Dynamic language communities like Ruby, Python, and JS have very little in the way of static up front checks, so they're not going to find out about problems until much later. This almost definitely means that those communities have more of this problem than you're aware of because many people probably didn't get as far as executing the code path that was susceptible to the problem. Finally, the relaxation of version bounds is something that can be automated. There are still some technical hurdles to overcome, but it's not hard to imagine a system that automatically builds and runs test suites for all dependencies of a package when it has a major version bump. So the state of affairs that we have today can be improved. And people are actively working on doing so. In the early days of cabal and hackage there was no mechanism for specifying dependency version bounds, and it was a nightmare. We've already seen the world that you're arguing for and it is NOT better than the one we have now where much of the ecosystem has version bounds and trustees can add version bounds when they're missing. The system works vastly better as a result of the trustees' efforts. But cabal never reports "this build would have failed if a trustee hadn't added accurate upper bounds for this package", so very few people realize just how much of a difference it makes.
GHC 8.2 (rc1) offers `COMPLETE` pragmas for pattern synonyms. These don't solve the general problem, but they do solve it for important special cases. For example, `Data.Sequence` can now decree that matching `Empty` and either `:&lt;|` or `:|&gt;` is sufficient. In my opinion, this feature has finally made pattern synonyms *useful*; previously I considered it a (very complicated) toy feature.
[As noted elsewhere in this thread](https://www.reddit.com/r/haskell/comments/683ymx/haskell_vs_scala_laziness_what_simple_example/dgvldux/), everything you can do in Haskell can be done in a strict language with enough care and effort. This is obviously true, given Turing completeness. Remember, though: all practical languages are Turing complete, but we don’t ever write our code in, say, Brainfuck. That’s a particularly extreme example, of course, but the point is that programming languages do ***not*** change what you can or cannot do; they simply make it possible to do certain things easier or harder. Haskell is generally lazy by default. Scala is strict by default. Lazy algorithms tend to compose better than strict ones. In a strict language that supports opt-in laziness, you may sometimes have to think very hard to figure out where you need to add “laziness annotations”, and often it might not be clear until much later. Again, you *can* do anything in any programming language, and Scala certainly makes it easier than most! The point is, though, Haskell does it out of the box, whereas you had to implement your own `foldr` with the appropriate properties. If you are looking for an example of Haskell code that cannot be replicated in Scala, you will be unsatisfied. Haskell is not magic; it cannot do things other languages cannot. It just makes certain things *easier*, and ease is subjective. It’s up to you to decide for yourself whether or not you think the benefits are worth it.
OK, I see what you mean. The problem occurs in mtl because it is based on transformers. Though I think it is mtl's fault because you don't need these instances to use transformers - just lift everything manually. Usually mtl style means accumulating effects. E.g. if function `f` uses `f1 :: MonadReader m = m Int` and `f2 :: MonadWriter m = Int -&gt; m ()`, then `f` should have both constrains: `f :: (MonadReader m, MonadWriter m) =&gt; ...` That is one of those thing I find wrong with mtl style.
&gt;In a strict language that supports opt-in laziness, you may sometimes have to think very hard to figure out where you need to add “laziness annotations”, and often it might not be clear until much later. But isn't it the same situation with Haskell, where you need to add "strict annotations" to reduce memory usage? &gt; If you are looking for an example of Haskell code that cannot be replicated in Scala, you will be unsatisfied. No, I was looking for &gt;Could someone show a simple laziness example (preferable some every day task) that would be hard to implement in Scala compare to Haskell. So, I don't think fold is a good example for that. But after all these discussions I got some sort of understanding of the difference.
doesn't the io random monad update the seed each sample? 
I do believe that humor is a good thing to have in discourse, but it is usually clear when humor is used to denigrate someone and tear someone down. Using humor as a negative force is usually intentional and I hope it is possible to separate humor was a force that tears people down and humor as a force to bring people together. "Don't make jokes intended to tear people down" should not be interpreted as "no humor allowed"... the former refers to only a very very small subset of the latter. 
&gt; Usually mtl style means accumulating effects. [...] That is one of those thing I find wrong with mtl style I'm curious as to why you find it wrong, since that's the value proposition of mtl though, and of every other effect library/abstraction. If you use raw transformers and lift everything manually, your code breaks every time you add an effect. It also forces you into thinking about the order of transformers everywhere, even you don't care (there are definitely cases in which explicitly using transformers is useful, but not all the time).
Also http://blog.haskell-exists.com/yuras/posts/effects-encoded-in-types-break-encapsulation.html
&gt; But isn't it the same situation with Haskell, where you need to add "strict annotations" to reduce memory usage? No, I don’t believe it’s the same, but it’s perhaps similar. Laziness is not the reason I like Haskell and would never use Scala, though. &gt; Could someone show a simple laziness example (preferable some every day task) that would be hard to implement in Scala compare to Haskell. The trouble is that “simple” examples are the easiest to emulate in a strict language. Your question is loaded. [The thread linked elsewhere](https://www.reddit.com/r/haskell/comments/5xge0v/today_i_used_laziness_for/deia53t/) gives a number of high-level examples where pervasive laziness is helpful at a whole-program scale, which is harder to implement in Scala compared to Haskell, but if you rule those out because you demand simple examples, then you won’t find anything very meaningful.
That's really interesting. I thought `generics-eot` superseded `generics-sop` but I was obviously wrong. However, the SOP representation should be isomorphic to the generic ones, so how come such functions can't be defined for all Generic. What I mean is, why is SOP needed at all ?
Yeah, the `StateT` isn't for the random seed, it's for the list extracted from the map.
How does protolude do that? I still find myself running into needing to use :: (string type) pretty often.
For basic haskell development they both support it quite well. But emacs may have a better and more fully featured integration with haskell tooling. 
I'm a huge vim guy. I put vim bindings on everything I can. But when it comes to Haskell development Intero is my favorite plugin, so much so that I use emacs because of it (with vim bindings of course). To make the setup easier I actually tend to use spacemacs over plain emacs. I should say there are a few ways to get vim to work with intero, but they broke often from my experience.
Not enough information to decide. It's certainly a functor, and I can provide a computable decision procedure to decide if it is a container, but some members of that type are containers and some are not. isContainer : (n : Nat ** Vect n a) -&gt; Bool isContainer (Z ** _) = False isContainer ((S k) ** _) = True
Ok. Which "version" of Emacs should I run? There's Spacemacs, Emacs for OS X, GNU Emacs (which is the original thing, I suppose), Aquamacs... Which one would be the bestM
I think Chris Done's Intero project kind of took over.
Alright. Notice, however, that `(n : Nat ** Vect n a)` is isomorphic to `List a` (`[a]` in Haskell), so neither is `List` always a container, and here's a computable decision procedure to determine it: isContainer :: List a -&gt; Bool isContainer [] = False isContainer _ = True Does this sound right to you? Are containers necessarily non-empty?
I think you need to use `instF` and match on `Sub` and call the function argument and match on `Dict` in order to bring the instance into scope, but I haven't ever tried this.
If you're going for Vim, then Haskell-for-vim is probably the easiest package to set up and provides a lot.
Spacesmacs is not a "version" of emacs but a "distribution", ie just a configuration. So, if you chose to use spacemacs (which is basically emacs but with key bindings) you still need to install (and chose) a version of Emacs. Just get the easiest to insall (probably Emacs for OS x). I personally used to use use Vim and switch to Spacemacs because, well, Haskell integration is better on Emacs that Vim and Spacemacs is a way to get both (Emacs Haskell integration + vim key-bindings) without to configure anything. Having said that, I also used to use only Vim without any haskell support.
Great, thanks! I wish there was a better comprehensive "how to do these things" for beginners. I'm walking through Haskell from first principles (the book) but I'm also using stack, protolude, and writing exercises where each chapter is its own file (and hence module). It makes things more complicated than strictly necessary, but once you sort of figure out how to do these things, it's not too bad. The problem for me comes in trying to Google how to do things. I couldn't find a single guide on how the module system works and how to export what, or how to set up the stack project for multiple files. I just had to trial and error it. Same with the string nonsense in protolude and the standard library. The book assumes String and [Char] for everything, which also makes life slightly more difficult; does Text work if you treat things like a list? I don't think so, but I can't really find anything concrete on that. 
The are no date on the post, would it not be a good idea to mark it as a April fool?
Worked! Scoped type variable worked, just needed to refresh in the syntax and explicitly use forall in eval signature. Correct, no fd for Convert in my case. I actually had the pattern match, but forgot to add to example.
&gt; Are containers necessarily non-empty? Per my definition of containers elsewhere under this post, yes. Something that is truly empty doesn't contain anything, so it can't be a container. (Most IRL containers always contain at least the vacuum energy of a bounded area of space-time, and usually much more.) Actually, this is one of the reasons I think the "container" analogy is actually more suitable for comonads than for functors or monads.
Not everyone can (functionally) or wants to (subjectively) use stack and LSP shares functionality with all supported language backends, which means the same stuff works for Java, Haskell, Python, Go, Rust, list goes on. And it also works the same way in Visual Studio Code and Emacs and Vim. You don't have to be aware of Go specific or Haskell specific functions/keybindings.
&gt; writing Haskell professionally for 3+ years now cool, may i ask where? (no worries if you have privacy concerns).
Thanks! I guess for now making Functor a superclass of Bifunctor is not worth it. +1 on that ticket. 
Way to start a flame war. ;)
I love them both, but that is often the trend for emacs. While it is bulkier, it runs with more features. 
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [ambiata/mafia/.../**mafia.hs** (master → 3b964cf)](https://github.com/ambiata/mafia/blob/3b964cfb72100db7f01e181da5489dd05e0626ea/main/mafia.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dgxhot6.)^.
Oh, I'm not claiming any kind of moral high ground here, quite the opposite if anything. Just pointing out that the self-righteous bullies pushing for more policing and censorship has that covered, in spades, and taking a stance against their bullcrap.
There's no reason Intero needs to be limited to stack - it just needs to be compiled with a matching GHC version. Nix, etc. would work just as well. I also wonder if implementing LSP on top of it would result in less duplicated work.
A preemptive upper bounds edit is exactly what occurred in this case, which is what surprised everyone involved and promoted adding this feature.
I'll tell you a secret: I gave up. Processing arguments as a list is easy and a little boilerplate is a small price to pay to avoid an obtuse abstraction. Seriously. Give it a shot. I'll bet you can get something working in less time than it takes to read the useless wiki pages about optparse-applicative.
&gt; And most importantly any spawned process locks vim globally. Not the case in Vim 8.0+/Neovim. Even [asynchronous linting on the fly](https://github.com/w0rp/ale) is now possible.
I used to do that but after learning and working with optparse-applicative, I cannot imagine writing CLI apps without it. The amount of automation that optparse-applicative gives you is tremendous.
With limited experience, I use the following: - Define a record for global options in a shared module - Define a record for each command, put it close to the function being wrapped. Consider using the record in place of the individual arguments for the function being wrapped - Define each option parser individually, in an options module. - Combine the parsers together near the main function - If you plan on writing higher level tests in Haskell, you can write an internal main function which conditionally uses either execparser or something like execparserpure and returns some result data instead of unit
Was it preemptive upper bounds and a fix at the same time, or just the upper bounds? Have the maintainers said this was a one off mistake or is it a change of policy that is likely to be repeated?
a stack-only emacs-only IDE took over the "compiler as an an API" project that provides a backend for different editors? 
No, generics-eot does not supersede generics-sop, and generics-sop is actively maintained and used. The generics-eot library is essentially a simplified and streamlined version of GHC.Generics, with good documentation to make it more accessible, so it's based on binary sums and products rather than n-ary ones. Your question on why generics-sop is needed at all is interesting, and it's difficult for me to give a clear answer right now, because I would have to think about it much more. Perhaps it is possible to fuse generics-sop with the transformation between the GHC.Generics and generics-sop representations in some way. But it would not be easy. First, while the representations between the two approaches are in principle isomorphic, that is not being made really obvious by GHC. The representation type of a Haskell datatype in GHC is just a type, and as such in principle an arbitrary combination of metadata, sums and products. There are many aspects of the generated representations that are left implicit, such as where exactly the metadata occurs, and how sums and products are nested. For example, every type is in fact a sum of products of types. Sums never occur underneath products directly. But this is not explicit, but implicit. generics-sop makes a lot of these assumptions explicit. So it can convert GHC-generated representations into its own, but it does so by using this implicit knowledge. Second, just because two representations are isomorphic, it does not mean that the same operations appear "natural" on them. So, for example, by making the product structure explicit in generics-sop, and by having a type constructor argument wrapping every value contained that is normally instantiated to the identity type, it becomes very easy to define transformation that change this type argument. An isomorphic representation that does not have this extra type argument would in principle contain the same information and allow the same operation, but performing the equivalent operation would still be much more difficult.
&gt; it just needs to be compiled with a matching GHC version This is one of a couple of reason I just can't use `ghc-mod`; it cares way too much about what my setup looks like. It's not uncommon for projects I work on to be on different versions of GHC (i've even got one or two on GHC HEAD), and `ghc-mod` wants to be installed globally, so I can't just throw it in my `nix-shell` or whatever. I want my tools to be far more modular than they currently are. This is why I use [dante](https://github.com/jyp/dante). All it needs to know is where the root directory is, and how to enter GHCi. I can swap in whatever systems I want to get it there. I think LSP is far more promising though. With a good LSP frontend, no one has to lift a finger to get a good UX. And I should have total control over how it starts the backend; meaning I can enter whatever `nix-shell` or `stack` command I need for my project to launch a backend. There's a lot of work to be done here, and it's really easy to screw it up by focusing on features rather than modularity, but I think `intero` and `ghc-mod` are just not the solution I need.
/u/alan_zimm, sorry for responding here instead of the mailing list (to which I'm not subscribed, and can't figure out how to respond to without having received the original email (mailing lists are archaic and should really be done away with); also, your GitHub forks can't have issues made on them because they're forks). But [why can't LSP backends be specified by configuration](https://mail.haskell.org/pipermail/haskell-cafe/2017-April/126922.html)? That seems like a flaw in the emacs plugin, not LSP itself. I don't understand why there can't just be a `projectname.lsp` file that both indicates the project root, and the command I want it to use to start a server. Which leads me to my next issue: The haskell implementation here specifies `hie` concretely. For most of my projects, I need the ability to do stuff like entering Nix shells and whatnot. Basically, no part of this stack should take it upon itself to assume how any other part works. If there were such a configuration file, I could just specify `nix-shell --run "hie ..."`. Finally, is it possible to just tell `haskell-ide-engine` how to get what it needs without a cabal file? This fits into the "don't assume how other parts work" issue; at work, we don't have a cabal file in our project, and just use custom scripts to enter Nix and use GHC directly, mainly so that we can be maximally flexible with how we mess with the compiler and incremental compilation and stuff. I don't really think we've done the right thing here, but still, I feel like we should be able to use editor integration nonetheless. There's no reason Cabal needs to be a direct requirement for this stuff.
Yes, pure vs IO is enough in most cases. I can't provide an example, most of my code is not public. Usually I organize effects in "services" and pass them as arguments. That way if code doesn't take some service as an argument, then it can't perform the corresponding effect, yet I can partially apply services to encapsulate them.
In theory it'd be possible for the compiler to gain support for an internal hom for the category of constraints to get proper entailment and to support higher rank constraints (and the internal hom for those forms, recursively.) These are admissable to add to Haskell's type system -- neither one changes the "thin-ness" of kind Constraint. They just aren't a thing that GHC HQ has gone and implemented.
What are the thoughts you can think if you have that equipment? I can think of a couple of things, but I can imagine you have some more to hand.
I think the author, Chris, is stack/emacs only. A neovim plugin exists in a separate repo. So does a fork that removes the stack-onlyness (Dante). I was answering the question regarding the state of HIE. It seems like it lost some momentum to intero, at least to me it did.
I am preparing a presentation about Scala vs. Thunk evaluation and while exploring the topic I accidentally created something interesting: scala&gt; val ethicList = 1 #:: ??? #:: Stream.empty immutable.Stream[Int] = Stream(1, ?) scala&gt; ethicList.head res0: Int = 1 scala&gt; ethicList.length scala.NotImplementedError Instead in Haskell: λ&gt; :set -XMonomorphismRestriction λ&gt; let ethicList = [1, undefined] λ&gt; head ethicList 1 λ&gt; length ethicList 2 λ&gt; :sp ethicList ethicList = [1,_] `_` means the thunk has not been evaluated yet! I found this pretty simple and cool.
Here's a rule of thumb: if you're writing your own `class` definition there's a 95% chance your design is wrong. Anyway, this makes very little sense to me. It seems like you're designing `CustomFieldType` and `CustomField` upfront without a clear idea of what you're doing with them. This GADT for `Filter` seems terribly complicated and doing everything through typeclasses also seems to be very complicated. What are you ultimately trying to achieve? Creating a value of type `CustomerPGRead -&gt; Column PGBool` through a webpage API? If so why not create a bunch of things like singleLineEquals :: Text -&gt; Column PGText -&gt; Column PGBool dobBefore :: UTCTime -&gt; Column PGTimestamptz -&gt; Column PGBool dobMatches :: Text -&gt; Column PGTimestamptz -&gt; Column PGBool booleanCheckBox :: Bool -&gt; Column PGBool -&gt; Column PGBool and then apply them to fields in `CustomerPGRead` using lenses? 
And what about the design implies that `"full_name.66"` is the emergency contact name? Is that decided by the user of your API?
Does the user of your API determine what the `cfName` and `cfKey` are?
&gt; Does this help in clarifying why the GADT was required? No, I don't think so. Does this help in clarifying why the GADT was not required? tag :: CustomerPGRead -&gt; Column PGText tag = ... bookingCount :: CustomerPGRead -&gt; Column PGInt4 bookingCount = ... customField :: String -&gt; CustomerPGRead -&gt; Column PGText customField = ... x :: CustomerPGRead -&gt; Column PGBool x c = ((tag c .== pgString "Member") .|| (tag c .== pgString "Loyal")) .&amp;&amp; Opaleye.not (bookingCount c .&gt;= 15) .&amp;&amp; customField "location" c .== pgString "India" Why can't you just build up values of type `CustomerPGRead -&gt; Column PGInt4` directly?
&gt; Here's a rule of thumb: if you're writing your own class definition there's a 95% chance your design is wrong. That's just a silly statement. The base classes don't know anything about your domain which means they can only provide a bare minimum of type safety. There are so many things you can encode at the type level to make your programs correct. I'd say that if 95% of your designs really are better without defining your typeclasses you might as well be programming in Elm. I hear this being repeated so many times it's infuriating. Have an effect that is probably best captured by a MonadX typeclass? Well, I guess you'll just have to use MonadIO and make your code completely untestable. Maybe that's a 5% situation for you (or even for most) but I personally define separate effects for everything and I can't imagine that putting them all in the same basket would somehow make my code in any way better. I get the idea behind your rule. Built-in type classes have had huge amounts of thought put into them. An average user probably won't be able to do the same and it might be a better idea to just fit their design around the ones already there. But that's not "95% chance your design is wrong". For such a broad and imprecise heuristic I wouldn't even say there's a 10% chance the design is outright wrong.
You might want to capture it using a type class and an existential quantification. class CustomFieldType f where -- Whatever interface you want data CustomField = forall f . CustomFieldType f =&gt; CustomField f If you want JSON serialisation you might include something like class JSON f =&gt; CustomFieldType f where -- Blah blah blah Having said all this, I smell something fishy in the design. So you might want to reconsider your data model more carefully.
I told you explicitly that I intended to make this email public, you did not tell me not to. On the contrary, as I said previously, I think it's very dangerous to have a private policy like this which is in conflict with the publicly stated policy of Hackage, and these discussions need to occur in the open. The reason this is coming up now is because you added a preemptive upper bound to a package, which caused breakage no one expected to be possible. I'm trying to balance the goals of multiple parties here, and this feature is one such attempt. I wouldn't have bothered linking to this reply by you, but the previous post demanded it. I don't enjoy this back and forth, where I need to modify tooling to work around upstream changes. I would much rather that previous agreements, like not enforcing PVP on Hackage or allowing people to opt-out of PVP issue filings, be followed. __EDIT__ I just reviewed the email exchange again. I guess you don't want me to quote what you said there, but my reading of your response in no way implies that I was disallowed from discussing this publicly. I'll restate for the record that I think this double-standard of public vs private policy is very dangerous. __EDIT 2__ Your comment here is clearly stating that you would rather that email be private, so I've removed the link above and deleted the Gist.
Since the topic has now been brought up in this thread, I may as well provide the missing context: there was a previous agreement in place that packages by some authors (Vincent included) who did not want to be bothered with PVP bounds issues shouldn't have issues filed. The basic agreement as I understood it was: * Authors would upload whatever they wanted, as has been the case in the past * Trustees could fix breakage by editing on Hackage * For people who would never bother adding upper bounds themselves, it was a waste of everyone's time to file a Github issue, and just an ongoing source of tension
What does the PVP have to do with the fact that bounds sometimes have to be changed retroactively to prevent breakage? Isn't the PVP all about how one assigns version numbers to new versions?
I think there are a few answers to your question: - Some people don't care about the dep solver case at all (for the record, I do care, just less than many others) - Many people, and this includes me, believe it's much better to release a new version with fixed code than to ever bother updating a bound in a cabal file on github - And some of us have been battling dependency problems long enough to remember a time when upper bounds could make things worse, not better, by sending the solver into hour long calculations, and sometimes old habits die hard
Instead of an N deep comment in reddit perhaps the Hackage maintainers could write a blog post about what they do and why they do it - which is likely to answer at least the first two points, in a way that everyone knows and can stand behind. Hackage maintainers do an awesome job, but lots of the details are hidden (e.g. which fields can you edit in a revision). (I'm always happy to host it on my blog if they don't feel they have an existing forum where it can go.)
At first this seemed like a terrible idea, but now I think it's brilliant! The reason I thought it was terrible is because [more expressive languages cannot be analyzed as much as restricted languages](http://blog.higher-order.com/blog/2014/12/21/maximally-powerful/). So by allowing your users to use the full power of the Haskell language to write their queries, it seems like you are maximizing expressivity and therefore minimizing the usefulness of your database, because you're preventing it from extracting useful information from your query in order to optimize it. For example, `SELECT * FROM pets WHERE age &gt; 10 LIMIT 5` and `take 5 . filter ((&gt; 10) . petAge) . dbPets` seem quite similar, but since the former is using a fixed structure, the database can figure out a faster way to execute the query. It probably has in index of all pets sorted by age already, so it only needs to load the pets which satisfy the filter. When the database receives an opaque function of type `Database -&gt; [Pet]`, however, it seems there is nothing to analyze, and that the only thing your database can do with this function is to load the entire database into memory and to apply this opaque function to it. That would obviously be quite inefficient. But then I started to think about different optimizations which could be performed on top of this inefficient idea, and I now think that it could work with well. With SQL, there are already different levels of query performance: as a mere mortal, I write naive SQL queries and hope for the best, and I still get acceptable performance most of the time, because databases are smart. And if I do need a query to run faster, I know there are some database specialists who can optimize SQL queries somehow. I think the same thing would be possible with Haskell queries: we’d get to write expressive but possibly-inefficient queries using the full power of Haskell, and if we need more speed, we can write efficient queries using the subset of the language which is known to be fast. In order to make this subset fast, we have a few tricks we can rely on. First, I know that lazy IO isn't popular, but by using similar techniques it should be possible to only load the part of the database which the pure function actually examines. Or we could use a monadic API with explicit actions for consuming rows, that would work too. Next, the way in which Cloud Haskell manages to transmit an opaque function over the network is by not making it opaque at all: it sends the AST of an expression whose leaves are the names of known functions. So it would be possible to examine this AST, to recognize some special functions such as `filter`, and to perform the same kinds of optimizations as the database performs for `WHERE` clauses, at least when the query follows some standard recognizable shape.
Thanks for your work generally!
Is there any way to serialise and de-serialize these values effectively? For communication with the UI and for *storing* in the DB?
Also, what happens when i want to apply the same filter against ElasticSearch, as opposed to directly against the database?
I'm afraid i really couldn't understand the comment about "existential quantification". Why do you feel something is fishy about the design?
Most of the `ghc-mod` exclusive features I find to be less useful then the core editing features that I want which are type checking and completion. And a lot of the more advanced features of `ghc-mod` seem to be in a constant state of half way working. One think I like about `intero` is the simplicity. When Chris told me about it, it was clear that the vision is if `stack ghci` works, `intero` works. You're limited at the moment by that choice with what you can do, but yeah it's about as stable as `stack` is.
That's excellent, though I'm skeptical that one wouldn't have a much rougher time doing the optimizations unless the programmer promised to stick to some cheerful subset, which is sort of the opposite of what the goal of this was. I know that to "optimize" sentences in first order logic (which I believe is basically the same as relational algebra) one can try to restrict quantifier depth, number of variables, and other things which lead to being able to implement it on a smaller device. There's a theorem that tells us FO = CRAM[1] where CRAM[1] is constant parallel time, so I'd imagine these optimizations carry over to this real life setting but I dunno. Really glad for the input. I may open up a repo with some thoughts on this when I'm done with finals and other things. I think I ought to read about traditional databases first and figure out where the speed comes from and how best to keep it. The major problem that I see is that FO = CRAM[1], whereas HASK = R, where HASK is all functions definable in Haskell and R is recursive functions. This clearly presents an issue lol.
Existential quantification I mean the use of `forall f. CustomFieldType f =&gt; `. Such types are called Existential types. An equivalent formulation using GADTs is as follows data CustomField where CustomField :: CustomFieldType f =&gt; f -&gt; CustomField Just google for existential types and you will get all the information. Existential types are one way streets. I can make for example `CustomField 5` and `CustomField "hello" ` all of type CustomField but by the very nature, I can use it only using the interface that the CustomFieldType class allows. This is good if I want to treat the data as some sort of a blob but with some guaranteed interface (ToJSON for example). If this is not your use case, which I think it is not because you seem to want to search on it, I am doubtful of the whole exercise. For example, I would assume that you want your search on this field to be fast and would therefore like to index but what sort of indexing would you do on such a blob-y data type.
Agreed. Stop equating monads and side effects! Any tutorial that does this is harmful. If you just want to teach side effects in Haskell, explain `do` notation and don't use the word "monad."
Rereading it, it still sounds pretty tautological to me: if you have to ask about it, there's a good chance you're doing it wrong. Well... yes. I'm more interested in hearing precisely what ideas constitute good typeclass engineering practice in the first place, rather than "people asking questions about this online are usually doing it wrong." The trouble is people (smart people!) rather disagree on this sort of thing pretty significantly: dare I mention foldable? Or global instance coherence? I think if we are to judge someone's code in this space, we should at least make it clear which design criteria we're judging by, rather than "if you're writing a class, you're probably doing it wrong."
Hi, I implemented a LSP for Haskell based on intero, i wanted the best of both worlds. Source : https://gitlab.com/vannnns/haskero/tree/master/server It's used by a vscode plugin: https://marketplace.visualstudio.com/items?itemName=Vans.haskero The only issue i see for a haskeller is that it's coded in typescript... ;-) 
The company I work for has a project that uses WordPress as a CMS but the entire public-facing site is in Haskell and communicates with WordPress over the API. That's been working well for over a year now. Scriptkiddies attack the Haskell server instead of the WordPress which is nice. We're also trying out other CMSes (like Contentful, Prismic, and Wagtail) using the same API-based templating system. So, I'm not really interested in building a CMS, because we've found a good way to use Haskell with existing CMSes that our clients like and are used to. I do think Haskell would be good for coding a CMS though! And if someone built something with great front-end design that was easy-to-use and also flexible, then I'd be first in line to try it out.
With [hask](http://github.com/ekmett/hask) you could fake this construction back in ghc 7.10, but you had to manually manipulate the environment. There I was able to build lenses, etc. into the category of constraints and use them to re-arrange things. You could probably get rid of about 40% of the explicit dictionary manipulation in that code rendering it almost approachable.
[Clockworks](http://www.clckwrks.com/page/view-page-slug/1/clckwrks-com) is the only one I'm familiar with.
I entirely agree which is why my adjusted rule of thumb takes the fact that it's a question explicitly into account!
yeah, that was one of maybe two i did find. i i know all projects have to start somewhere, but it doesn't look super promising to me.
First, it's worth noting that tagging the individual indices assumes we're only dealing with finite-dimensional spaces. I'm not saying that's bad (my library makes the same assumption), just that it's something to be aware of. One issue that arises with the "just tag the indices" approach it that gets a bit fiddly. Usually we think of the covariant indices as being independent of the contravariant ones, so we'd like to avoid any boilerplate for dealing with different ways of interleaving covariant and contravariant indices. That is, because `V ⊗ W^*` is canonically isomorphic to W^* ⊗ V`, we'd like to treat them as the same type. More particularly, we'd like to not have to worry about the differences between `U ⊗ V ⊗ W^*`, `U ⊗ W^* ⊗ V`, and `W^* ⊗ U ⊗ V`. Another issue that arises is with type checking. For example, we would like to be able to state that `(transpose . transpose) == id`. However, to do so we must ensure that the types match up. If we implemented tagging with a `Co` constructor, then we'd have `transpose :: Tensor a k -&gt; Tensor (Co a) k` which in turn means `(transpose . transpose) :: Tensor a k -&gt; Tensor (Co (Co a)) k`— which doesn't even have the right type to be the identity function! So we need some way of ensuring that *at the type level* we have `(Co . Co) == id`. More particularly, we don't want this to be an equality that requires proof, we want it to hold by reduction alone. Your approach to tagging is a bit different than using `Co`. I'm not entirely sure what you had in mind, since the `Vect` constructor is applied to too many arguments, and you also have an application to the left of the colon. Assuming what you had in mind was `Tensor :: Vect n (Nat,Bool) -&gt; Type -&gt; Type` (or more simply: `Tensor :: [(Nat,Bool)] -&gt; Type -&gt; Type`), then rather than simply having one constructor `Co` you'd have two `(_,True)` and `(_,False)`. That is nicer than the `Co` approach, since it lets you define some things more parametrically (i.e., by using `(_,b)` for some parametric `b`). But you still run into the two problems above. What type would you give `transpose` so that involution has the right type by reduction alone? How would you deal with the annoying canonical isomorphisms via permutation?
They (and isos, etc.) are really mostly useful for just treating Constraint as a particularly er.. constrained.. *. You can write a lot of code for * and have it just automatically lift into Constraint when you use the right vocabulary.
Hm I assumed that since the email I was replying to didn't get quoted in the mail client, that it wouldn't respond to the correct email. But I guess `mailto:` just isn't capable of adding the quoted text. Thanks!
Oh snap, I only have 27 :(
Thanks! those fixed the error. I got the code to work by also replacing " (+1).last list" with "last list +1", but I'm not sure why " (+1).last list" is giving me a type error.
&gt; Improving type errors It would really nice if GHC could do this! One of the most time-consuming parts of deciphering a type error involves mentally tracing how GHC concluded something that at first glance seemed absurd.
I agree. I liked how he develops the ideas and you can anticipate profunctors before he introduces them. It's an excellent talk.
Not yet unfortunately, sorry about that!
You want "(+1).last $ list" there, or "((+1).last) list" to avoid the $. This is because of the precedence of function application. The way you wrote it tries to apply "last list" and then compose that with (+1), which gives a type error. Instead, you want the composition to happen first, then apply the resulting function to the list.
It works better for my brain personally - It helps to read it as 'for these things matching these constraints, there exists a class n..' It's like a type variable declaration, so if you look at it that way it reads a lot better. I agree 10,000 times and several more again that it would be really nice to have dynamic color coding on type constraints, it would improve readability immensely regardless of how you feel about the ordering of the symbols. 
That's an interesting take on the matter. It brings me back to a handful of questions that I don't necessarily have answers for... &gt;Trying to make the leap from "[] is a monad, IO is a monad, ..." to 'I know what monads are" perplexes members of the first camp. [...] In the second camp [...] They think there must be some hidden meaning in these words that they can't see, and find the definition somehow lacking. I wonder if these two camps are not so different after all. The second camp is clearly looking for something that isn't there, and we might try to dissuade someone from that by asking "Are you sure there is a hidden meaning to be found?". Perhaps, however, we might ask of someone on the first camp "Are you sure there is a leap to be made?". Stating it in a different way: exactly what distinguishes a Haskeller who "know what monads are" from one who doesn't? &gt;My take on monads, and why they're hard to learn, and why they're hard to teach, is that they're just a piece of mathematics. Monads are a piece of mathematics, and *also* a piece of Haskell. From a certain perspective, they are just a very general and principled interface in a language that has plenty of other very general and principled interfaces. To put the problem in concrete terms: to what extent does one need mathematics to understand and benefit from something like [this post](http://www.haskellforall.com/2012/08/the-category-design-pattern.html)? &gt;Maybe I just suck at mathematics [...] Well, who doesn't? :) &gt;[...] but I solve the problems of these camps mainly with hard work. I've not understood any real piece of abstract mathematics by example or by definition, I've looked at both, hard, often for weeks before I begin to understand. If "hard work" and "look[ing] hard" don't essentially involve neither understanding "by example" nor "by definition", I guess they must involve some sort of counterpart to the oft-repeated advice about grasping monads in Haskell, which I paraphrase thusly: "Go ahead and just start writing monadic code; after a while doing that, the patterns will begin to make sense." 
Yes the arrow in `class` is backwards. class Foo a &lt;= Bar a would almost work, to at least get the arrow in the right direction, modulo annoyances with naming a typeclass `&lt;=` but sadly, as you note, the ship has sailed. It helps a bit to think of the current usage a bit like the use of arrows in GADT syntax, describing what what "fields" are needed to construct an instance as a record in the backend, with the `where` clause bolted in at the end as a big extra blob. That goes a long way towards forgiving it.
You can think of `=&gt;` as logical implication; `Ord a =&gt; Ord (Maybe a)` means that having an instance of `Ord` for `a` *implies* that there is also an instance of `Ord (Maybe a)`
Worse complexities than needed by the optimal algorithm. Not by this (weird) algorithm. I think writing a different, straightforward algorithm here can be more optimal, and you don't have to worry about the optimizer catching on.
Well even inlining (or whatever the right word is) the typeclass definitions, I'd still argue OP has a problem: that the `Filter` structure entails all sorts of Postgres details. I'm all for making an ADT for filter logic, but I see no reason why it should be bound to Postgres or even a database. That being said, I think there will still be problems ahead: as soon as you have your filter (basically a decidable predicate), you want to say "I have an x which passed this filter", and without the dependent product, that gets ugly pretty quick. You can try to dance around it with private/smart constructors, but even then you've written a "meta" check that the compiler isn't "aware" of since there's no *entailment* of x passing that filter. This will put you in the uncomfortable position of constantly having to handle patterns you know should never happen: e.g., I know `Email` will have a `@` followed by a `.` somewhere because it's checked by my smart constructor, but when I do `lookup '@' email`, I'm going to have to handle the `Nothing` case even though I know that case can't ever happen.
If the UI is going to treat all of the field types equally, you could define a typeclass `CustomField` that your field types implement, and then box up your fields in an existential type: data SomeField = SF { unSF :: forall a. CustomField a =&gt; a } chooseCustomField :: IO SomeField Given a `SomeField`, all you know is that it contains *some* datatype that has a `CustomField` instance. (Hence "existential".) This, of course, means that all of your `CustomField`-implementing types would need to also have instances of some `Render` typeclass (and that would actually have to be a constraint in the `CustomField` declaration: class Render a where renderToUI :: a -&gt; IO () class Render a =&gt; CustomField a where ... is what the classes would look like) so you could then do renderCustomField :: IO () renderCustomField = chooseCustomField &gt;&gt;= renderToUI Essentially, all the field type-specific logic would have to be in `Render`-like classes, one for every kind of behavior the fields should have, if you want to go this route. Getting any data out of the fields (input, say) or allowing "some different behavior" would mean adding an associated type to `Render`: class Render a where type RenderData a renderToUI :: a -&gt; IO (RenderData a) which would necessitate some fiddling with the `SomeField` type to prevent weird type errors. In short, I *think* this won't be easy without messily simulating something like subtyping. Otherwise, it seems you'd want a wrapper datatype that has different cases for your field types, and the logic would be implemented using pattern-matching on something like data SomeField = SLF SingleLineField | ... Edit: I looked at your old post, where /u/piyushkurur also came up with the same idea. It definitely is something you might consider if you're committed to this architecture: simpler solutions might exist, but nothing comes to mind. 
"PureScript: unsailing GHC ships since 2013 (?)"
It coexists with haskell-mode as a submode, providing some extra commands (for refactoring and the like), and becoming the source of diagnostics for flycheck and so on. Things like font-lock and indentation are still handled by haskell-mode. My original limited goal is to make "unusual" commands easily available, driven by my work on HaRe. 
If you are sure they are always there you can use (!), but it will cause a runtime error if the value isn't there. 
&gt; Is there a datatype in Haskell that does this? Yes, functions: fruitname :: Fruit -&gt; String fruitname Apple = "Apple" fruitname Banana = "Banana" fruitname Orange = "Orange" This answers all the requirements you explicitly state though updating values would be tricky of course.
Like I said, how do you serialiaze the `IsEqual :: (FilterEq f v) =&gt; f -&gt; v -&gt; Filter` part?
There's [total-map](http://hackage.haskell.org/package/total-map).
This is indeed the solution to my toy problem but not for my real problem. What I'm really doing is loading html templates for a web app. I have made an enum for all templates, with the show instance giving the filenames. I load each individual template using Text.Mustache and I store them in the Map. In that case, your fruitname function doesn't work, right? 
I mean have you looked at the implementation of `splitAt`? You can't just use a built in function and assert that the solution is straight forward since the built in function does it for you.
So how do you guys deal with `bracket` patterns? 
&gt; Yet what to do if you want composable code? Currently I have `type Rpc a = ExceptT RpcError IO a` which is terrible What's non-composable or terrible about that?
Well, exceptions are hard to reason about, that is true. I don't think anyone will argue the opposite. Yet your example is marginally better. Your `ProcessError` is a giant sum of all possible errors, it is very similar to `SomeException`. When your code grows, you'll have to add more error types. &gt; there is zero indication of what can and cannot fail or how it can fail Well, `IO` indicates that the action can fail. Yes, you don't know in what way it can fail, but `ProcessError` is only marginally better. E.g. `readCatFile :: FilePath -&gt; ExceptT ProcessError IO Cat` doesn't indicate it too, because it may fail (almost) only with `EReadFile`, yet it is not clear from the type. The problem is that whatever you do, you alway end with a giant sum type, equivalent to `SomeException`. Yes, you can structure errors a bit better because `SomeException` is extendable while `ProcessError` is not, but I'm not sure it worth the efforts. Other issue: you actually don't need to know all the ways action may fail. Think about it. The complete list of errors depends on implementation. Do you really what to expose implementation details? The idea of exceptions is to handle *some* of them and be prepared for any unknown exception too.
Also, thank you for the post. I disagree with your point of view, yet I find it important do discuss exeptions vs errors topic.
Pattern matching has exhaustiveness and overlap checking so it gives you a little bit more safety.
If you write `flag None`, for example, it will throw an exception. A simpler example: data Foo = Foo { foo :: () } | Bar ghci&gt; foo Bar *** Exception: No match in record selector foo It's best not to define partial functions if you can avoid it. 
The trouble is that `bracket` doesn't work for `ExceptT`. But yeah, I guess `MonadResource` is a viable work-around but then I worry that resources are not reclaimed as soon as they could be. Of course if your code is heavily conduit based then conduit takes care of that.
&gt; If you write `flag None` But in my case there is no situation where I might write that. In case of `Arguments`, it encapsulates the arguments my program might receive. PS: Really new to haskell, hence maybe I am not getting the details.
Ohhh, makes sense. Thank you!
Another advantage of all those custom error constructors is that it allows me to provide clear precise unambiguous error messages to the users of my program (who are data scientists not software engineers).
thank you for the awesome explanation
That's a good point. Will keep in mind, thanks!
This looks a lot cleaner. Thanks again!
Yeah, toy problems often fall apart when they scale up. For the real problem, I would worry that Enum is too static for modelling a set of files. I tend to view enums as something fixed (and usually small) like states of traffic lights. Without knowing more about what the templates represent, I would see a set of files as a dynamic collection and putting them in a total map feels overly strict.
Why doesn't bracket work for ExceptT?
Because sadly we don't have your MonadBracket class universally adopted yet :)
True, but you can still use the one from lifted-base which uses MonadBaseControl. Or just write a custom bracket that is specialized to ExceptT. FTR, I'm not in favor of the ExceptT pattern, but it is at least possible to do this.
But we put it to good use [here](https://github.com/ambiata/x/blob/master/x-eithert/src/X/Control/Monad/Trans/Either.hs#L236-L238).
There's people that complain that you can't catch exceptions (i.e. panics) in Rust (they just kill the whole thread), and then there's people who know what's involved in error handling: `IO` throws *way* too many exceptions, particularly for things that represent mere status results. "Fine not found" is not a bloody exception, how could it ever be: On any finite file system there's strictly more files that don't exist than files that do. I hope that once we get the transformer story down properly (Eff?) the IO monad can be redone with that in mind. As it is now, the whole exception mechanism in Haskell is just shoddy weak typing causing tons of problems because continuations are hard and if one thing's for sure then that noone (but Oleg) likes to think about them.
&gt; Other issue: you actually don't need to know all the ways action may fail. Think about it. The complete list of errors depends on implementation. Do you really what to expose implementation details? I have never tried this, but perhaps higher-level errors could be parameterized by the type of lower-level errors. (Somewhat unrelated: it seems to me that, when using free monads / effect handlers, the main logic of the program has less control over lower-level errors. It might even lack the ability to catch them altogether and recover; that power is conferred to the interpreter. So no more bracket for example. Does anybody code like this?)
Thanks for the post. Personally I think catching all exceptions and turning them into an error type doesn’t work very well in practise: A lot of async exceptions shouldn’t be caught or at least not in general. E.g. `Ctrl+C` relies on exceptions, `timeout` relies on exceptions and so on. Not being able to use any of these is a dealbreaker for me. Even worse, if I press `Ctrl+C` I now get a `EReadFile` error even though the exception is completely unrelated to the act of reading a file. You say you don’t store errors as strings but you are converting all exceptions to strings. You do add the tag representing the code that threw the exceptions but as I’ve explained before this is not really useful with async exceptions. Also your functions can still throw (async) exceptions outside of `handleExceptT` so your type signature is at least partially lying.
To be fair, that doesn't help with the intuition for class declarations. `class Eq a =&gt; Ord a` doesn't say that having an `Eq a` implies anything about `Ord a`.
I find `intercalate` from `Data.List` is often quite useful. ghci&gt; encloseWithQuotes :: [String] -&gt; [String]; encloseWithQuotes = map (\s -&gt; "\"" ++ s ++ "\"") ghci&gt; import Data.List ghci&gt; toStringList :: [String] -&gt; String; toStringList str = "[" ++ (intercalate "," str) ++ "]" ghci&gt; toStringList . encloseWithQuotes $ ["my", "cat"] "[\"my\",\"cat\"]" 
&gt; "Fine not found" is not a bloody exception, how could it ever be Agree completely! One that was hitting me recently was "Connection reset by peer". In TCP/IP networking this is even more common than "File not found" when doing disk I/O. 
&gt; FTR, I'm not in favor of the ExceptT pattern That doesn't mean I can't try to convince you to change your mind :).
Sorry, I don't follow. Your `handlefun` should return the same time as `readFile`, otherwise types will not match. I don't see how you can have `databasework` inside the `handlefun`. Just wrap `readFile` is a function, that asks for other file and retries on exception until success.
&gt; because continuations are hard Not so hard 
Yes. It is wrong. I was thinking in a more complex scenario. This is easily solvable with exceptions simply letting the exception handler return data2. Now I have to think in which scenario I was thinking ;) This example is still synchronous because file IO (which is asynchronous in the deep) is composable thanks to the Operating System and the Haskell runtime. The scenario I was thinking on is when for the recovery of the execution is necessary an asynchronous operation and the the exception can not return, but execute all the rest of the work. Or when the error condition is not locally recoverable. Suppose that file1 and file2 depend one on the other and another file1 has to be reloaded when file2 fails. Suppose also that there are a lot of work previously done, so aborting the program is not an option. Then, there is no way to resume the error locally neither in file1 neither in file2 handlers an continue the main monadic sequence. It is necessary to rearrange file1 and file2 read in a single blob and altering the simple sequence. Or else, It is necessary an error handler before file1 that apply to all the rest of the computation and perform all the rest of the computation again to recover from the error. In this case since the fix can not be local, it is not synchronous, and the effect is a kind of manual backtracking (from the line of file2 to the line before file1). `handlefun` is like an event handler and the above considerations about asynchronous programming apply better, unless I'm wrong again: workdata &lt;- previouswork handle (handlefun workdata) $ do data1 &lt;- readFile filename1 data2 &lt;- readFile filename2 result &lt;- databasework data1 data2 print result handlefun workdata exc= do ask for new filename1 ask for new filename2 data1 &lt;- readFile filename1 data2 &lt;- readFile filename2 result &lt;- databasework data1 data2 print result In this case, the solution for keeping the simple monadic code without breaks or repetitions is to use continuations, that are hard and would obfuscate the code even more, or better, some simple and specific primitives for handling exceptions with the power of continuations inside (that is what ..Ahem I did in transient)
That's looks neat. I must look into other built-in functions too. I feel like I might have re invented the wheel a lot many times in this project. Thank you!
I should have said that I like your post, and you're representing the other opinion on this well. I just still disagree 😃. Do feel free to try and convince me otherwise though, hopefully some time over beers.
Yes, monad-control is very well designed from that standpoint.
Yes, there are some promising approaches developed recently. Yet they are not mature enough IMO. Lets see whether they will win at the end of the day.
what about just using `show`? ghci&gt; show ["my", "cat"] "[\"my\",\"cat\"]"
As another trustee I can at least give my view. &gt; Do you believe that packages without PVP upper bounds should be blocked from Hackage? If we can automate the process enough that most people who at the moment don't follow it will accept then we could potentially, but as it stands now: no &gt; In the case I refer to above, you added some preemptive upper bounds. Was that intentional, and will you do that again in the future? Trustees should not add pre-emptive bounds to the latest release of a a package. I also think we shouldn't do it for any major series in active LTS's. &gt; Do you have a opinion on whether Stackage should use Hackage as its upstream packages source? Previous conversations have implied you might, but I'm not sure. I think we should stick with hackage. &gt; with new builder tooling in place, are you open to following through with Simon Marlow's proposal around automated PVP bounds management? As I've said many times, I think that's central to overcoming the current tensions. I don't remember that much of the proposal but I'm all for automation. Most importantly we need someone to step up and do it. 
Isn't the composition complaint more about composing the type of the exception? foo :: MonadError Foo m =&gt; m () bar :: MonadError Bar m =&gt; m () x :: (MonadError Foo m, MonadError Bar m) =&gt; m () x = do foo bar We'd like to be able to write this, but GHC can't really do it. Class constraints aren't really designed for this kind of programming. [`ether` gets us closer](https://int-index.github.io/ether/), but it requires a bit of boilerplate. That said, I think you *can* do something like this: data FooBar = Foo Foo | Bar Bar withExceptT :: MonadError e' m =&gt; (e -&gt; e') -&gt; ExceptT e m a -&gt; m a withExceptT f = either (throwError . f) return &lt;=&lt; runExceptT x :: MonadError FooBar m =&gt; m () x = do withExceptT Foo foo withExceptT Bar bar I think that compiles, but of course, it's got its own boilerplate.
You're not the only one to find this a little confusing. In purescript, they [flipped the arrow around](https://github.com/purescript/purescript/wiki/Differences-from-Haskell#arrow-direction) to make it a little more human-friendly. This isn't quite what you suggested, but I figured it was worth bringing up.
I'm assuming you're talking about associated types in type classes. With type families your types can depend on variables in the class head, but that's it. With fundeps you can have dependencies any way you like. You can get something similar (maybe still not equivalent) with type family dependencies but I'd say fundeps are probably a cleaner solution.
Agreed!
Hmm, can you give an example of this?
I enjoyed the linked article about brute-forcing your passphrase! I am forever choosing "memorable" passphrases before going on holiday for two weeks...
&gt; the show instance giving the filenames. You may have heard this before but it's not recommended to use show to 'pretty print' values. `Show` should produce valid haskell syntax to recreate the value that was just shown. Instead you could just use a normal function like above perhaps called something like `templateFilePath :: Template -&gt; FilePath`
This is a work in progress. The intention is that it coexists with haskell-mode, and also that haskell-mode works without it. One of the goals of haskell-mode, as I understand it, is that it should do something reasonable without requiring external executables.
Especially since Foo shouldn't even depend on m... khm khm
For example class C a b c | a -&gt; b, b -&gt; c, b c -&gt; a How would you encode this with type families?
Works just fine, thanks again!
Well `Foo` *doesn't* depend on `m` here, since the type family is not injective. It would look like this: class Monad m =&gt; MonadState m where type State m get :: m (State m) put :: State m -&gt; m () instance Monad m =&gt; MonadState (StateT s m) where type State (StateT s m) = s -- get :: StateT s m s get = ... -- put :: s -&gt; StateT s m () put = ... data Foo = Foo Int foo :: (MonadState m, State m ~ Foo) =&gt; m () foo = do Foo i &lt;- get put (Foo (i + 1))
I mean my main point was just that splitAt isn't trivial itself. Have you tested the performance of the two functions?
It’s definitely a significant improvement but personally I don’t have a problem with exceptions so I’ll just keep using them :P
This is really awesome! I'm gonna have to remember this exists the next time I need it. This can be really useful for people doing eDiscovery, among other things, generating the right search strings for systems that don't have a flexible-enough query syntax or in cases where you have to hand over a list of keywords. Could also be useful in a regex-testing toy app for kids. I hope to see this become a part of the standard command line in years to come.
awesome, Thanks 
Help? When I have code that doesn't type check, Intero pops up a small frame at the bottom of my editor with details, however often times (not always) the message is scrolled down a ways, so I can't see the first part, and if I try to click the message, it disappears. I can find it again in my `*Messages*` buffer, or if I mouse over the problem area, but both are rather inconvenient. Has any one dealt with this before?
also, pointfree.io is super cool to check out. It's my crutch for learning pointfree style
The difference between juggling one and n^m balls.
Great presentation! I really enjoyed his previous talk 'Next Level MTL' as well (https://www.youtube.com/watch?v=GZPup5Iuaqw).
I had never heard of e-discovery, that's interesting. Out of curiosity, could you expand on the use cases for that and the regex toy app?
Either is ideal, heck, it is free, and can be composed with other ideal monads.
That uses `MonadCatch` not `MonadBracket`.
Would you be willing to share more details? This interests me personally. How does the Haskell side talk to the WordPress side? Are you utilizing the WordPress JSON API to shuttle data to the Haskell side?
I've been using Neovim with [haskell-vim-now](https://github.com/begriffs/haskell-vim-now) for a few months (minus a few features I find annoying, like Unicode conceal, and plus a few custom bindings and plugins) and have been very happy with my workflow so far.
Thanks for these great notes! fwiw, I’d definitely be interested in taking a look at the source of `Duet` if you ever get around to releasing it :)
Gah, yes, you're right. It was late.
"I'm not a big fan of [method that makes this really easy and clean], so I wanted to write this in [method that is very difficult], and i'm having problems" :) Not trying to say that this isn't an exercise worth doing...but I do hope that after doing this, you will see why people appreciate do notation so much :)
Oxford maybe?
Perhaps you can rename the Haskell rewrite to "Swan" ? ;)
&gt; How does one construct a value of type `CustomerPGRead -&gt; Column PGBool` from an incoming JSON? Straightforwardly. Why should that be a problem? &gt; To make the JSON type-safe, wouldn't all these type-classes basically move to the Aeson layer, just shifting the problem from here to there? I don't think so. What do you mean? &gt; how does one represent the fact that some columns/fields can have equal-to filters, some columns/fields can have contains filters, some columns can have greaterThan filters, etc? All of those are ultimately just things of type `CustomerPGRead -&gt; Column PGBool` so unless you really need to keep around the type of the filter (for example to serialise back to JSON) the function type is sufficient. 
Interesting. I did some work on a similar sort of string composition function (for the same purpose, ie. generating possible passwords for cracking from a known formula) in JavaScript and Lua (I'm just learning Haskell right now) somewhere around six months ago. Usually, I commit these things to a repository on GitHub that I create before I even venture into constructing them; in this case, though, I don't seem to have the repository handy (I most likely have the workspace for it somewhere under https://c9.io/stuartpb, though: I'm going to go hunting for it right after I finish this comment). EDIT: That's right, I composed my original iteration in, of all places, *the TamperMonkey script editor on my girlfriend's Chromebook*, then copy-pasted it into my https://github.com/alphabi/experiment-a0z1 working copy in https://ide.c9.io/stuartpb/alphabi. I should move that to an actual repo or two. One of the key differences is that the primitives in my system were defined, in Haskell terms, something like this (apologies if I mix up `data` and `type` here, as I said, I'm still learning): type Junction = Either String [Sequence] type Sequence = [Junction] where `Junction` refers to a list of *accepted sequences*, and `Sequence` refers to a list of *concatenated junctions between constant strings*. The main generation interface took an *integer* and returned the equivalent *ordinal traversal* of the decision tree, based on the nested junctions. The whole time I wrote this, I felt like there must be some pre-existing branch of CS that describes this (I looked into everything from tree and graph theory to mixed-base digit systems), but couldn't find any existing terminology.
Whoa, this is very cool! Are there immediate plans to add more dimensions? Specifically I _think_ something like affirmative/negative (e.g. "Yeah", "yes", "uh huh", "nope") would also be really useful to us. If we end up training one ourselves, we'll open a PR.
what's the connection between wit and facebook exactly?
This is really great and looks *really* easy to plug into forms. Also I'm kinda tickled by how many of the rules are just regex's.
PRs are very welcome! An affirmation dimension intuitively just seems like a fuzzy whitelist rather than composition of tokens. Are there more complex cases?
Check out the [fficxx package](https://hackage.haskell.org/package/fficxx). There was a presentation about it at the [Bay Area Haskell meetup in February](https://www.meetup.com/Bay-Area-Haskell-Users-Group/events/236725835/).
Haskell makes the teams working on it very happy. Source: talked to some FB devs
You need to indent your code by four spaces so that reddit preserves your formatting. And please ask a concrete question, tell us what you've tried, [etc](https://stackoverflow.com/help/how-to-ask)., don't just paste code and expect us to figure out what you need!
If you want to keep the error message visible, you could load the module into the REPL (C-c C-l) or compile the project (C-c p c). 
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [Lazersmoke/civskell/.../**Civskell.hs#L145** (master → a9eab4d)](https://github.com/Lazersmoke/civskell/blob/a9eab4d0f92d679d1277985e71f99058c0ec3547/src/Civskell.hs#L145) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dh0jwz1.)^.
Try https://www.google.com/search?q=haskell+dsl
For sure
Are you kidding? Haskell is amazing for creating DSLs.
Thanks! That was precisely what I hoped to accomplish. Any feedback is welcome.
If you're building a binary, sure it might make sense to depend on exact versions for everything. With a library though, it's just stupid.
Neat! Was this inspired by my recent [monad-persist](https://hackage.haskell.org/package/monad-persist) package (which is basically the same thing but for mtl style), or is the timing just coincidental?
Totally coincidental! I'm pretty amazed at how close in time these are popping up. 
You are correct, and to the extent that it's possible, is how I'd recommend approaching this in Haskell.
Bartosz Milewski - Category Theory for Programmers https://www.youtube.com/watch?v=I8LbkfSSR58&amp;list=PLbgaMIhjbmEnaH_LTkxLI7FMa2HsnawM_
And all the rest https://bartoszmilewski.com/
I like the book "Conceptual Mathematics" and Bartosz Milewski's blog / video series as ways to dip some toes into that pool. With that said, I'm not sure you need too much category theory to to get a good intuition about Functor / Applicative / Monad. You can go a long way with Haskell - especially for applications - by treating those (and things like Monoid) as libraries with some great properties around reasoning / composability. At that point you're using the library more than you are using category theory. The line gets a bit more blurry if you start shuffling things around using the functor laws or by spotting monoid homomorphisms to reduce the number of times you traverse a structure. With the functor laws you might be doing category theory, or you might just have read the documentation closely and remembered a few things. You can probably get to that point without spending much (any?) time on category theory, by writing (and reading) a whole heap of Haskell and asking for other people to comment on your code. If you're writing a library then there are some potentially wins to be had - if you can spot that certain patterns apply, you can sometimes do quite a bit of fusing and reworking of code in order to improve performance / composability / the ability to reason about things. In that case, the further you dive into the theory, the more you'll be able to spot patterns / draw on existing stuff when you're putting things together. If category does sound like you're kind of thing and it's been a while since you've put the maths hat on, I'd also recommend Pinter's "A book of abstract algebra" and some type theory, since category theory is normally presented in terms of applications to other areas, so being familiar with a few of them can be pretty helpful. If you need a general boost in the maths department, I'd start the whole chain off with "How to prove it" by Velleman. Lots of handwaving / simplifications / generalisations in the above, but hopefully it's helpful.
This library is exactly what we needed, thanks a lot!
I have interest in crypto currencies. Does IOHK have any existing project ideas?
Not if there's a way for version 1.1 to say "I am 100% behavior-compatible with version 1.0," allowing the solver to treat them as equivalent for a 1.0 dependency. Moving the range specification upstream doesn't strictly solve the problem, but funnels it to a single point where there's far more reliable knowledge of what will work and what won't.
The most immediate problem is depending on two libraries which, in turn, depend on different versions of some third library ("diamond dependency graphs"). AFAIK Cabal and GHC have been creeping towards allowing multiple versions of a package to be used within a single program, which would prevent Cabal complaining about unsatisfiable dependencies. It introduces another problem though, which is trying to pass data around, e.g. type errors when you've got a `Foo` from `foo-1.0` and you need to pass it to a function expecting a `Foo` from `foo-1.1`.
ah yes, I remember writing HTML in Clojure :) What is enhanced? Well, the `do` syntax *feels* much nicer to me than `[` `,` `]` everywhere! Looks like it feels nicer to the authors of these libraries too.
ncatlab + arxiv
I somehow manage to write a rather large amount of Haskell without having to lock everything to a single version of my dependencies. It really comes down to knowing what versions of your dependencies supplied the features you are using. If you want a reproducible build for a production artifact? Sure. We have tools for producing that version set. In the middle ground, we have stack providing lock-stepped dependencies as a sort of continuous integration tool for the whole community. But if you're writing a library? it behooves you to be far more liberal about dependency versions to ensure that you can build in more situations. Otherwise the only person likely to be able to build your library is you.
The main real scenario where we could allow multiple versions of a dependency are when the types from the "private" dependency don't leak out of the API. This sort of vacuously dodges the fact that "foo-1.0:Data.Foo.Bar" and "foo-1.1:Data.Foo.Bar" are completely different types.
the "vanilla" books are IMO quite boring to read - especially when you don't know more than Set/Functions. but I really enjoy [P. Aluffi; Algebra: Chapter 0](https://www.amazon.com/Algebra-Chapter-Graduate-Studies-Mathematics/dp/0821847813) that *builds up* algebra using CT from the go instead of after all the work ---- **remark** I don't know if this will really help you understanding Haskell (I doubt it a bit) but it's a worthy intellectual endeavor all in itself and you can put on a *knowing* smile whenever you hear those *horrible* words after
Yes, it did. Maybe op never committed theirs, which would be on them
Have you had a look at [lucid](https://hackage.haskell.org/package/lucid)?
&gt; mother of all Haskell anti-patterns *sheepishly* I can't say I disagree...
Well the mafia got involved lately...
Whether you think they're a good idea or not, I suspect the options that building on a monad transformer provides are obvious: you could easily interact with a state or IO or whatever. But if you never intend to use those options because you believe they're a bad idea, then, does using a monad get you anything else? I think the intention is to try and de-clutter things by letting you take advantage of the implicit sequencing that do notation gets you. Probably something like: renderUserInfo' :: User -&gt; HTML renderUserInfo' User { name, email } = (div_ [] (do (div_ [class_ "user-info--name"] [text name]) (div_ [class_ "user-info--email"] [text email]))) It seems to me that that actually ends up looking a little bit more like your clojure example than your explicit list. And, of course, you could pass in an additional monadic expression and embed it at least a little more cleanly than I think the equivalent clojure would be. Hmmm, having written all that, maybe your real issue is just that it's not using a list? I suspect that's a consequence of basing it on a builder.
Not that I know of, but you can always submit a proposal for something that's not in the list, or you could try contacting them.
Maybe because I have several GHCs installed via `stack setup` and not system-wide, but I had this error first: ➜ hs2rs git:(master) ✗ gcc -shared -o libinter.so inter.c libhs.so -fPIC inter.c:1:10: fatal error: 'HsFFI.h' file not found #include &lt;HsFFI.h&gt; ^ 1 error generated. For which I figured I can add the `-I` like this: ➜ hs2rs git:(master) ✗ gcc -shared -o libinter.so inter.c libhs.so -fPIC -I/Users/kb/.stack/programs/x86_64-osx/ghc-8.0.2/lib/ghc-8.0.2/include/ Undefined symbols for architecture x86_64: "_hs_exit", referenced from: _fin in inter-78e45a.o "_hs_init", referenced from: _init in inter-78e45a.o ld: symbol(s) not found for architecture x86_64 clang: error: linker command failed with exit code 1 (use -v to see invocation) But now I get this other error. Can you help fixing this one please? Thank you!
Best way to give feedback: email to me, or IRC, perhaps? Adding to/fromNPI or a variant thereof should be possible. Regarding your other comments, it seems I have to take a closer look at Metamorphosis first.
Nice work! Do you have a benchmark comparison?
Typclassopedia is a good gateway from abstract Haskell types to category theory.
Won't initializing the Haskell runtime on every call be horribly expensive? I would think you'd want to initialize it just once and keep it running.
We lacked a usergroup here in Munich for far too long, so here we go! :-)
I actually like it a lot too. It also contains some nice historical notes, which makes me appreciate the subject more.
I had heard that this book was awesome, just ordered on amazon and cant wait: Categories for the Working Mathematician
&gt; AFAIK Cabal and GHC have been creeping towards allowing multiple versions of a package to be used within a single program Pendantic-man is here! GHC has always supported allowing multiple versions of a package in a single program, but it is was always the case that they had to be different versions (foo-0.1 v. foo-0.2). As of GHC 7.10, GHC also supports loading multiple copies of the same version of the package built with different versions of their deps (e.g., foo-0.1 built against bar-0.1 v. foo-0.1 built against bar-0.2). There has been NO code added to Cabal as of 2017 to allow for multiple versions of packages in a single library; however, setup dependencies are a relatively recent addition to the dependency solver which are allowed to select dependencies that are different versions that those used for the main package (but these are never linked together.)
Well, now we know what Bartosz Nitka has been up to. 
How this relates to http://muenchen.haskell.bayern/ ?
Hey, author here, so this was written a while back and some of this is outdated, but you're right doing this is expensive. What gets weirder though is once you stop the runtime you can't restart it again. You would want to start it and then stop it at the end of the program if you plan on running more Haskell code at some point.
Being a linguistic library one could argue. But no, this was a typo.
Yes
&gt; That's what Nix is for. I still find myself always disagreeing with this sentiment. Nix *happens* to have caching (it would be unusable without it), but I don't think relying on Nix *solely* for caching is really a fair pitch. Nix has a huge learning curve and it requires almost total buy-in. It doesn't work on Windows, and it has a lot of rough edges with existing tooling. I personally love and use Nix everywhere. But I have lived in many dev environments and I can't imagine trying to push Nix to all of them. Takeaway: We really do need build caching apart from Nix in our tooling chain somewhere, even if I am not the one using it.
The only thing you can do with a `Foo` is to turn it into a `String` and a `SomeJSON` can be turned into a `Value` and also parse a new `Value` to replace it. You might as well define `type Foo = String` and `SomeJSON = Value`. There is nothing desirable about this design. 
It may indeed be an exaggerated claim (depending on how you interpret the word "pioneer"), but I remember that in the late-90s C++ was making it up as it went along when it came to templates and generic programming (at least while respecting what I called C++'s Big Idea: high level programming with no overhead wrt C, or as the C++ people phrase it "you should not pay for what you don't use"). I don't know of any other language at the time (since there have been more languages like this) which did anything as powerful as C++'s templates. I could be wrong, though, but even subsequent languages often have less powerful generics than C++. Note that C++ is still trying to figure out major building blocks of the whole scheme (such as "concepts", only recently introduced). I do think my main point stands: both C++ and Haskell are comfortable taking cutting edge ideas and putting them into a language which is also meant for "real world" programming.
The Haskell Meeting is a get-together in a bar; it’s having a beer with other Haskellers. If you’re around, I recommend paying them a visit! Discussions are about everything people are currently interested in, giving you a mix of lots of different topics. The meetup on the other hand is geared towards the »technical talks« format, where the topic is mostly chosen by the presenters, and the discussion is more focused. More bluntly, there is beer and Haskell in both cases, albeit in different ratios. ;-)
&gt; which is why the more high-tech solutions exist What are the names of these solutions?
You can't have `|` because it's reserved, but how about `+`? {-# LANGUAGE TypeOperators #-} type (+) = Either readCatFile :: FilePath -&gt; ExceptT (CatParseError + FileReadError) IO Cat readCatFile = undefined 
You could use the [union](https://hackage.haskell.org/package/union-0.1.1.1/docs/Data-Union.html) package. import Data.Union readCatFile :: FilePath -&gt; ExceptT (OpenUnion '[CatParseError, FileReadError]) IO Cat readDogFile :: FilePath -&gt; ExceptT (OpenUnion '[DogParseError, FileReadError]) IO Dog relaxErr :: (Functor m, USubset as bs is) =&gt; ExceptT (Union f as) m x -&gt; ExceptT (Union f bs) m x relaxErr = withExceptT urelax readCatAndDog :: FilePath -&gt; FilePath -&gt; ExceptT (OpenUnion '[CatParseError, DogParseError, FileReadError]) IO (Cat, Dog) readCatAndDog x y = do cat &lt;- relaxErr (readCatFile x) dog &lt;- relaxErr (readDogFile y) return (cat, dog) To throw an error, you'd use throwErr :: (MonadError (OpenUnion es) m, UElem e es i) =&gt; e -&gt; m a throwErr = throwError . ulift . Identity
&gt; I think C++ was comfortable with experimental stuff at that time (hence template exports, shudder). Not so much anymore, I don't think. GHC still seems reasonably comfortable forging ahead with interesting new type-level things. I'm not as much on top of C++ development these days, so I'll take your word for it. &gt; I won't dispute the power of C++ templates (they're Turing Complete after all), but that power comes at a terrible price in compilation time and nigh-incomprehensible error messages (much of the time). This was partially compiler issues which have since gotten (at least partly) better. The whole "concepts" idea was supposed to at least make error messages better. But, yes, very advanced template metaprogramming leads to very advanced metaerrors :)
If you're making a script (rather than a project), you can use [the new `stack script` command](https://github.com/commercialhaskell/stack/issues/2805). You don't even need to specify your dependencies; Stack will figure them out for you! For example, I recently had to normalize some CSV data. You can grab [this file](https://gist.github.com/tfausak/ec68921ed4d9a3aadf6f21c1c0f11acc) and then run `stack --resolver lts-8.13 script normalize-csv.hs` and it will just work! 
Oh nice! I had no idea that existed.
&gt; I'm not in favor of the ExceptT pattern can you explain? (sorry if you already have elsewhere)
Yes, there are plenty. But I would like to know what could be consider as the best solution for this problem at this moment.
 idky you want a map over a function, or why you need to pass around a map versus just calling `show` later. but you could do earlier validation to refine your partial thing into a total thing. e.g. with my (probably unnecessary, you can inline the following function if only using non-product-type `Enum`s) `enumerate` and `enumerate-function` packages: import Enumerate import Enumerate.Function data Fruit = Apple | Banana | Orange deriving (Eq, Ord, Enum, Enumerate) f :: Maybe (Fruit -&gt; String) f = toFunction createMap the `Maybe` has been hoisted up. now you only validate it once, perhaps on startup, and never again. this is just the normal smart constructor pattern https://github.com/sboosali/enumerate-function/blob/master/sources/Enumerate/Function/Map.hs https://hackage.haskell.org/package/enumerate https://hackage.haskell.org/package/enumerate-function 
Really they should steal cargo or something like it. Cabal hell is seriously bad.
Can you expand on why you don't want to use Stack scripts? What alternative are you using? 
When did you experience ["Cabal hell"](https://www.well-typed.com/blog/2015/01/how-we-might-abolish-cabal-hell-part-2/) the last time? If this was recent, can you please inform the [Hackage Trustees](https://github.com/haskell-infra/hackage-trustees) so we can investigate?
Stack also allows you to put the script command in a comment at the top of the file along with a stack shebang and then you can run the script directly: `./MyScript.hs`. Scripting using stack and the Turtle library is a dream come true imo!
Fair enough. I've been using Stack scripts to replace Shell scripts. So far I'm liking it. Here's an example: &lt;https://github.com/haskellweekly/haskellweekly.github.io/commit/d2b0ff6&gt;.
I have no experience with either fortran or repa, which knocks our specific commentary on the detail you've put together. But matrix multiplication is kind of a hobby of mine - it's one of the few bits of math I understand, and a most common computation that needs to be fast - and I've thought a bit about the problem domain in a haskell context. Firstly, fortran has won the speed battle and mind share with respect to array algorithms and performance. Linpack began in 1974 and it's descendents rule the race track, so I don't see a native haskell solution catching up anytime soon. There's really not much gap between blas algorithm speed and theoretical limits. In my experience, it is also rather hard to directly convert imperative algorithms to functional approaches using a bottom up methodology such as equational reasoning. The loopiness is very hard to refactor and it usually takes more insight than I cam muster to jump to a better FP mousetrap. Meanwhile haskell shines in several areas. GHC regularly fires up all cores using algorithms I write without me ever having thought about parallel computation needs. If it's a monoid, ghc will find a way. Embedded domain specific languages are a joy to write - haskell can compile anything with class. And, most importantly, I'm able to code in what is very similar to the actual math I'm thinking of (though maybe this is where I've drunk too much from the well). Putting this together in my current hobby project, [numhask](https://github.com/tonyday567/numhask), here's a matrix multiplication: mmult x y = tabulate (\(i,j) -&gt; row i x &lt;.&gt; col j y) (&lt;.&gt;) a b = sum $ liftA2 (*) a b or a recent tweet: https://twitter.com/stdlib/status/859508835295391744 This is an active area of research! I'm currently grinding through a process of generalising these to n-dimensional arrays (aka tensors), and confident that the inner product of vectors, matrices and tensors will unify on the one algorithm. The way I see it, is that lower-level trickery should be embedded in the `&lt;.&gt;` and `*` operations. Instead of multiplication producing an answer (say), it can also produce an EDSL that defers implementation details. That's what accelerate does. It's what vector does via fusion. [plover](https://github.com/swift-nav/plover) is another example. This then produces a separation of concern between the math specification (eg this is a (type-safe!) tensor contraction) from the machine sympatico (this matrix is too large to fit in the cache so let's bust it up into smaller parts). If it all works (chances might be slim), haskell becomes a killer app for the algorithm sciences. Some very nice peeps hang out in https://gitter.im/dataHaskell/Lobby and converse about this stuff all the time. 
&gt; I suspect the vast majority of uses of `traverse` are as "an effectful `map`" More than that, `traverse` **is** an effectful `map`
&gt; but that power comes at a terrible price in compilation time and nigh-incomprehensible error messages (much of the time). Compile time meta programming doesn't _necessarily_ lead to those problems. The issue is with the particular implementation used in C++ (which wasn't designed with meta programming in mind, thus requiring more or less horrible hacks to do moderately interesting things), and of course with poor handling of these situations by compilers. The latter has gotten better, I'm not sure how much the former can be improved without breaking C++ (which is unlikely to happen). Some languages already provide decent compile time meta programming nowadays, with error messages being good as long as the programmer provides them, and speed not being a huge issue most of the time.
Huh, the [docstring for `traverse`](https://www.stackage.org/haddock/lts-8.13/base-4.9.1.0/Prelude.html#v:traverse) even starts with the word "Map": &gt; `traverse :: Applicative f =&gt; (a -&gt; f b) -&gt; t a -&gt; f (t b)` Map each element of a structure to an action, evaluate these actions from left to right, and collect the results. I'd be okay with introducing the synonym `mapA = traverse`. I do think `mapA` is a better name.
Having three different names for the same function is excessive. It's bad enough that `mapM` exists at all anymore. Personally, I don't think `mapA` is any more descriptive; if you don't know the function already, you're gonna have to look up the docs with either name. And it just pollutes the namespace further.
Not sure about this statement either but I came to Haskell because if C++ template. I was really into template (20 years ago) and Haskell polymorphism was like template on steroids (just using lower case for type parameters instead of the heavy C++ syntax was a trait of genius)
&gt; Of course, the ((Left . Left) ...) functions are annoying to write, Indeed, however you can solve it using pattern synom s and define LLeft, LRight etc ... I use it for combination of Either and Maybe (RNothing, RJust etc ...)
&gt;Really they should steal cargo or something like it Why? Be specific, not vague hand wavy "cabal hell". Tell me exactly what this would accomplish. &gt;Cabal hell is seriously bad. It also hasn't existed in years. Getting an error when you should get an error is not cabal hell, and also happens with cargo.
Interesting, it's like the caddr of sum types!
True, but I'd argue it hasn't done the job very well. It relies on everyone understanding *and agreeing with* it; and it isn't framed as a promise to the tooling, but as a gentlefolks' agreement with the community. PVP also handles many more topics than "if I have a dependency on X, can I reliably build with Y?" You'll get version breaks for new API features which don't affect old ones, and often _won't_ get breaks for bug fixes which bring behavior in line with docs -- which is problematic if someone has already worked around broken behavior. I really think a .cabal field along the lines of `satisfies-dependency-versions:` would be beneficial, particularly since it doesn't break anything in the current ecosystem and can be adopted gradually.
Could you explain how Nix's caching differs to Stack's? I know stack does some caching, but don't know enough about Nix to compare them.
Well, it is more than that though. For example, traversing with `Const` is a fold, and I'd hardly call a fold an effectful map. Or, [you can use it to sort](http://elvishjerricco.github.io/2017/03/23/applicative-sorting.html). Traversing with `[]` as the applicative creates cartesian products in your traversable. It's wrong to equate monads to side effects, and far more wrong to equate applicatives to them.
A common joke among mathematicians is that this book should have been called Categories for Specialists in Category Theory
https://www.reddit.com/r/NixOS/comments/64xyd7/nix_package_manager_works_flawlessly_in_windows/ Well, it seems to work on Windows now.
I think all language ecosystems stabilize over time. The character of a community is mutable to an extent, but in many ways is a function of the ratio of people needing exciting reasons to learn something new, versus people with legacy code bases, or who are forced to use the language for a job or for a convenient library. Haskell is crystallizing, as well. Five years ago, I would have confidently described the Haskell community as being willing to sacrifice other attributes to preserve a sense of clean correctness in the core language. We resisted accepting complexity only until it was absolutely certain that it was the *right* complexity. Of course, at the same time, GHC was always host to all manner of complexity, but GHC and Haskell were not the same thing! In the intervening time, it feels like something fundamentally changed there. That productive tension between Haskell and GHC snapped under too much weight, unable to survive the sheer force of those who wanted for social and engineering reasons to take arbitrary bits of Haskell code and use them ubiquitously. No one cares about the core of Haskell much any more. Once it was clear that the clean core language was irrelevant, we found ways to work in greater and greater complexity. And I doubt the community would ever have the spirit left to remove it, even if outside forces changed. Haskell will become more and more complex, now, until it collapses under its own weight. Not for another 10 or 15 years, most likely; but you can't keep following this road forever, without eventually getting to where it leads. Sometimes, I find this sad, because I definitely am in love with this language. But the alternative, which was to have become irrelevant *before* enough people noticed, would undoubtedly have been worse. But it will be interesting to see what rises from the ashes. Still, it was a glorious time when we managed to balance on the edge of that choice, and we balanced there far longer than most.
A lot of the truly pioneering languages of the era are no longer in common use. In the case of generic programming, C++ had some sources to borrow from, although there are certainly aspects of C++'s templates that were new. Not all of those aspects were necessarily planned or known for a while after templates were added to the language, though! While C++ was being developed at Bell Labs, David MacQueen and (for a while) Luca Cardelli were both there; they were both heavily involved in the development of ML and Standard ML. MacQueen in particular was a major player in the design of the SML module system, which is one of the methods of doing generic programming that SML provides (closer perhaps to templates than the earlier polymorphic data types of ML were). I can't say for sure how much influence was had on C++ by SML, but the opportunity was certainly there. MacQueen cited CLU as one of his inspirations for the design of SML modules, and it was undoubtedly an influence on C++ and the design of the STL as well. CLU, which dates from 1974, has as far as I know the first notion of abstract data types that can be parameterized by types. It also introduced iterators (although they were co-routine style, like Python's generators) and had an exception system. Eiffel and Ada followed CLU in providing type-parameterized generic data modules before C++'s templates as well. So, most of the concepts were well and truly established by the time C++ started borrowing them, but that's not to say that C++ templates didn't do anything new. Templates aren't tied to modules (which have been sadly missing in the C family) and they behave differently than earlier techniques for generic programming, plus they've had a tremendous amount of industry engineering effort put into them.
&gt; The name traverse is therefore not particularly indicative of how the function is used in practice and mapA is much clearer. Is it? I suspect the clarity of "map" comes more from familiarity than any intuitive sense.
Take a look at Data.Map: https://hackage.haskell.org/package/containers-0.5.10.1/docs/Data-Map-Strict.html That's the canonical example of the API that the exercise is expecting for the histogram structure. It's unclear from this example if the point of the exercise is to learn about type synonyms or learn how to make your own data structure that reinvents the wheel. Either way, I strongly recommend taking a look at the source for that module, it's surprisingly easy to follow, and it's a great learning experience about how data structures work in Haskell.
You might consider packaging for .deb or .rpm; I might use this for inode performance testing.
Sure, but at least `mapA` communicates well to people who understand what `map` is. `traverse` does not.
I've had similar issues. I think when it's working it's great but when it's not it's terrible and very difficult to understand what is happening. 
What was the reasoning behind creating anew function `sequenceA` instead of relaxing the constraint on the old one? `sequence` got moved into the typeclass anyway.
Any update? :D :D
`sequenceA` predates AMP and FTP; [it was introduced alongside `Traversable`](https://downloads.haskell.org/~ghc/6.6/docs/html/libraries/base/Data-Traversable.html) (and `Foldable`, and `Applicative`).
Yes. I have finished the separation and the changes are on github. I have also updated the candidate release. The openstreetmap stuff will be put into a new package and will been released after the release of the core naqsha package. Right now I am very busy (chasing deadline) but feel free to have a look at it and send pull requests. I will do the release some time after the deadline. 
Thanks
yup, meant to do that. I'll try to find the time this week.
Somehow I thought custom type operators must start with `:`, is that not / no longer so? Or only if they define/alias value constructors?
How though? Can't I traverse a non effectful monad?
I like it, I tend to use [`(&amp;)`](https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Function.html#v:-38-) and [`(&lt;&amp;&gt;)`](https://hackage.haskell.org/package/lens-4.15.1/docs/Control-Lens-Operators.html#v:-60--38--62-) while working interactively because it lets me progressively add to the end of the line &gt;&gt;&gt; "hi" &lt;&amp;&gt; toUpper &amp; mapM_ print 'H' 'I' and typing `traverse_` quickly gets old, I would be +1 if we didn't have `mapM` and `mapM_` already..
Yeah sorry to be vague. I use cabal because stack doesn't work well with Atom IDE yet. It's been a while, so I can't quite give the details, but I've definitely spent days, possibly a week, figuring out how to get cabal to compile stuff when there are packages that have complicated version number conflicts. I think this is one of those examples where the average user says, "This is horrible I want to die!" and the devs say, "No it's not did you try (insert abstruse incantation) based on (completely indecipherable error message). See it's easy." I still could not tell you what the different parts of the system do? What's the difference between cabal and cabal-install, is there one? From a user experience point of view, cabal and haskells IDE situation are so unhaskell it hurts... sorry end rant. I don't want to hurt anyones feelings but I think this may be an example of open source normalizing a bad experience because no good alternative exists. I'm in no position to do anything about this I know.
&gt;&gt; The really big missing piece is the equivalent of ccache for Haskell. &gt; &gt; That's what Nix is for. I don't think that's accurate. As you say yourself in the other comment, nix caches entire packages. It is thus much more similar to e.g. debian binary builds of entire packages than it is to `ccache`. `ccache` caches on the object file granularity, which is much more fine grained (like 100x more fine grained). When you change code, but many files are unchanged, `ccache` can cache efficiently; nix will do a full rebuild even when you change just a comment in your code. So I don't think it's fair to compare the two. I've been thinking about the idea of a `ccache`-like functionality as part of ghc for a while. ghc knows all inputs to, for example, the codegen, so it could hash those the inputs and fetch the resulting `.o` file from a cache dir if it has seen it before. I think implementing this would need some refactoring though: We'd have to make ghc even more deterministic so that the inputs really look the same before being hashed, and the codegen would have to be pure.
Luca Cardelli also worked in Modula-3, which has generics similar to Ada.
Yeah, these are basically urban legends at this point. People repeat myths about "cabal hell" without understanding them and then any time a user gets an error they assume it is "cabal hell" (which hasn't existed in years) and don't bother even reading the error message much less googling to see what they should do about it. Trying to install an impossible set of packages fails just as much in cargo as it does in cabal.
Nix works on macOS, most Linuxes, and is starting to work on Windows
&gt; I don't think this is quite right - AFAICT Stack caches libraries based on architecture, GHC version and library version, but not resolver. I tested this by bumping a project to a newer resolver than I had locally, and running stack build - most packages displayed using precompiled package. Hm every time I try that, it doesn't work. &gt; How would you work around a slow/limited/unavailable internet connection?Can you download the graph of Nix units before you need them, or force it to build them locally? (On the assumption that the source code is much smaller than the binaries.) Yea you can pre-cache any derivations you want. And you can force it to build things locally, but that's basically never going to take less bandwidth.
I have cellular automata implemented [here](http://github.com/bollu/cellularautomata) using the Comonad approach, in case you're interested
Nope, but only because my scripts usually aren't doing much. `System.Process` gets the job done even though it's not pretty. 
Whaa^aa^aa^aat ...? It can guess the package names from the import statements? It's very nice if that's the case! How does it deal with ambiguity if/when it arises?
Yeah! Since it's using a Stack resolver, it knows all the module names in that resolver and which packages they come from. You can see all that information on [the module listing](https://www.stackage.org/lts-8.13/docs) for a resolver. If you import a module that can come from many packages, like `Control.Monad.Reader`, it doesn't include any of the packages. That forces you to pick one. For example, running `stack --resolver lts-8.13 script` on a file that only contains `import Control.Monad.Reader` gives me: Failed to load interface for `Control.Monad.Reader' It is a member of the hidden package `rerebase-1.0.3'. It is a member of the hidden package `mtl-2.2.1'. It is a member of the hidden package `monads-tf-0.1.0.3'.
There are more OSes out there than just those. Also working on WSL isn't working on Windows for me.
Very nice! I wonder if it would be of benefit if authors of packages namespaced their modules in a way to make them more unique. E.g. `import Mtl.Control.Monad.Reader` or `Control.Monad.Reader.Mtl` Also, in a way I find it a bit weird that there is so many levels of indirection when I want to use a function. I wonder if it would make sense to merge the concept of modules and packages. Node's npm is built that way. I see upsides to that more than downsides. 
I thought so too, but then I remembered type-level `(+)` from [`GHC.TypeLits`](https://hackage.haskell.org/package/base-4.9.0.0/docs/GHC-TypeLits.html#t:-43-), so I tried it and it worked. Experimenting, it seems that value constructors indeed need to start with `:`, but that we can define non-`:` value constructors anyway. Possibly because with `DataKinds`, that value constructor gets promoted to a type constructor, and we know that non-`:` type constructors are allowed? It's weird that the compiler can't decide whether it wants a `'` or not though. {-# LANGUAGE DataKinds, TypeOperators #-} import Data.Proxy data (^^^) a b = Foo foo :: Bool ^^^ Bool foo = Foo data Bar = (:^^^) Bool Bool bar :: Bar bar = True :^^^ False data Baz = (^^^^) Bool Bool -- error: Variable not in scope: (^^^^) baz :: Baz baz = True ^^^^ False -- warning: unticked promoted constructor: ^^^^. -- use '^^^^ instead of ^^^^. proxy :: Proxy ('True ^^^^ 'False) proxy = Proxy -- error: Not in scope ^^^^ proxy' :: Proxy ('True '^^^^ 'False) proxy' = Proxy 
Start with a Map. Store only live cells. Create a second Map for them and their neighbors each round. 10 queries (neighbors plus liveness of cell). O(n*log n) per round where n is the number of live cells.
&gt; simple one-off scripts I have my set of gripes with nix, but I find its nix-shell functionality ([with hashbangs `#!`](http://nixos.org/nix/manual/#use-as-a-interpreter)) to be quite something: #! /usr/bin/env nix-shell #! nix-shell -i runghc -p 'haskellPackages.ghcWithPackages (p: [p.scotty])' {-# LANGUAGE OverloadedStrings #-} import Web.Scotty main = scotty 3000 $ do get "/:word" $ do beam &lt;- param "word" html $ mconcat ["&lt;h1&gt;Scotty, ", beam, " me up!&lt;/h1&gt;"] 
&gt; I can't say for sure how much influence was had on C++ by SML, but the opportunity was certainly there. B. Stroustrup: Parameterized Types for C++. Proc. USENIX C++ Conference, Denver, pp 1-18. October, 1988. Also, Computing Systems, V2 no 1, Winter 1989 https://www.usenix.org/legacy/publications/compsystems/1989/win_stroustrup.pdf can't find any mentions of ML modules 
The issue is that WSL is a kind of its own world inside Windows, don't expect to do Windows development within WSL, and that is what I care about.
This one is certainly the way to go, but perhaps use an IntMap and a pairing function instead, as that will be much faster.
It's only a matter of time before someone figures out how to do it =P
Also, our company (Awake Networks) will be investing in using Nix to do partial builds (i.e. one derivation per object file, for example) very soon
The author makes a good point about documentation: &gt; Hard documentation is where you describe every argument to a function and its effects. It is like a reference work (think of man pages). Soft documentation are tutorials and examples and more descriptive text. Well documented software and libraries will have both 
In this vein, /u/kwef has [ComonadSheet](https://github.com/kwf/comonadsheet) and a great presentation on it if anyone is interested.
You will make a blog post as soon as that's ready for public consumption, right? Please?
I've been wishing people did that (unrelated to stack) ever since I started with haskell, but I'm coming from Python, so that response is perhaps predictable.
Yes, we will
That sounds very interesting.
If all you want to do is create a mapping between a type and its HTML representation, wouldn't you just create a `ToHtml` typeclass and then use Lucid/Blaze/whatever-you-want to actually render your types to HTML? About the monadic interface in Lucid: It's purely for convenience. You can use Elm-style templates as well and I've seen Haskell code that does that. Monads are ultimately about the monad laws. If you can make a DSL for HTML that is made prettier with a law-abiding monad, why not? ;) I have written many templates with Elm-style and also with Lucid's monadic style. I definitely prefer the monad style. It's just less noisy. But to each his/her own. (People like you who are at peace with Lisp's parens would likely have no trouble stomaching the bracket-heavy Elm-style.) I've thought about a type-safe HTML library. But I think the complexity of the types would quickly outweigh the benefit. It'd probably be easier to strap on an HTML validator after-the-fact! ;)
That sounds interesting. Where do I sign up for updates about this? 
The project will be started this summer by an incoming intern, Remy Goldschmidt. We can start the work in the open on our organization's public GitHub, which you can find here: https://github.com/awakenetworks/. I'll also tweet about it to draw attention if people are interested in contributing to that effort.
One thing where GHC is worse is cold startup of GHC and therefore any interactive/scripting use of it. In contrast, ocaml (utop) is instant from a cold cache. Is this caused by dynamic linking of `libHS*.so`? Even with warm cache it's still slower to start ghc(i), but that may be the linking step which now is faster because the `.so`'s are in cache.
Why hasn't anyone mentioned `new-build`?! Seriously, take a look at `cabal new-build`! It introduces a nix-like store for haskell packages; allowing to install multiple versions for a package if needed. And although it's prefixed with `new-` and not the default `build` yet it is working really well. With it the dependency conflicts with already installed packages are a thing of the past. And it also introduces multi-package projects which made me ultimately switch from `stack`. Take a look at /u/ezyangs introduction: http://blog.ezyang.com/2016/05/announcing-cabal-new-build-nix-style-local-builds/
Interesting. Well splitAt does use laziness for sure, just in a different way. And 2.5-3.5 times is significant for sure, but definitely nothing to do with complexity or asymptotically, just a constant factor. 
So Hashlife is over a finite world? Maybe you can build a larger world out of these tiles, patched together as a comonad? (If you want to venture outside of Haskell too, you could do the tile processing on the GPU, perhaps keeping the hashes on the CPU? Work with Repa or something?)
The rationale seems to be along the lines of what /u/andrewthad has suggested (awkwardness related to `*&gt;`), but in a more roundabout way. Following that lead I found [this relevant Libraries list thread](https://mail.haskell.org/pipermail/libraries/2015-September/026157.html).
"Effect" is an overloaded word. You and ElvishJerrico seem to be using it in one sense, which I would call the absolute one (i.e. whatever doesn't fully submit to referential transparency), while tomejaguar and ItsNotMineISwear are using it in another, relative one (i.e. whatever any `Applicative`/`Monad` brings to the table, and what e.g. do-notation helps making implicit).
I haven't used Cabal since I switched to Stack, but `new-build` seems nice. &gt; it also introduces multi-package projects which made me ultimately switch from stack This is one of Stack's killer features. Are you saying Cabal can do this now? 
it's only constructor operators that must start with `:`. Some of this behaviour [changed in 7.6.1](https://mail.haskell.org/pipermail/glasgow-haskell-users/2012-September/022845.html)
My first thought was something like emptyUniverse = (repeat row, repeat row) where row = (repeat False, repeat False) But I never would have thought of it as a comonad. Seems like a way nicer interface, is there a list of common instances that could be used to build some intuition?
Ah I see. For some reason I though it didn't even enter base until AMP was already done
Here's my introductory course that was produced for the Midlands Graduate School in case it's helpful for anyone - assumes no knowledge of categories and aimed at functional programmers: http://www.cs.nott.ac.uk/~pszgmh/cat.html
&gt; [T]he status quo is optimising `(&gt;&gt;)` and forgetting about `(*&gt;)`, resulting in unexpected performance regressions when code is generalised from `Monad` to `Applicative`. This unfortunate situation also blocks us from being able to remove the post-AMP method redundancy in the `Foldable`/`Traversable` classes. From the [Monad of no `return` proposal (MRP)](https://ghc.haskell.org/trac/ghc/wiki/Proposal/MonadOfNoReturn).
I like that `mapA` is four characters shorter than `traverse`. But I don't think it is worth it. Also once [MRP](https://ghc.haskell.org/trac/ghc/wiki/Proposal/MonadOfNoReturn) happend `mapM` will probably become an alias for `traverse` anyway.
https://hub.darcs.net/ross/transformers/issue/33
I have no idea what this is...
Static analysis tool for haskell. Not sure why the tag line of the GitHub is so unhelpful or why the author doesn't want to explain the benefits over say, running linters in vim during writing etc.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [haskell/haddock/.../**cabal.project** (master → e0e6615)](https://github.com/haskell/haddock/blob/e0e6615dd421f1b332ce2b11a98de768fa7c29a8/cabal.project) * [haskell/cabal/.../**cabal.project** (master → 5afa362)](https://github.com/haskell/cabal/blob/5afa3624f4fc6d733c76973f806b2d1fbfe1d173/cabal.project) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dh3gzm4.)^.
So it sounds like your issue is just that you want to use packages within a project that are incompatible due to the dependencies required. I don't think Stack or anything else would really help there, it's up to the maintainers to update their packages and such. If I'm understanding correctly, this isn't what's usually referred to as "Cabal hell".
Steven Roman has a series of six 1-hour long videos on Category Theory (see link below). I will say, as someone trying to learn category theory, it is very, very abstract stuff. I am finding it helpful to step back and learn some abstract algebra/group theory before I tackle it seriously. https://www.youtube.com/watch?v=If6VUXZIB-4&amp;list=PLiyVurqwtq0Y40IZhB6T1wM2fMduEVe56
The word you're looking for is &gt;**vi·al** ˈvī(ə)l/ noun a small container, typically cylindrical and made of glass, used especially for holding liquid medicines. Not &gt;**vile** vīl/ adjective extremely unpleasant. 
One counterargument is that stack does not come with ghc. If you want a non-haskell coworker to run your script for the first time, they will have to wait for stack to download and install ghc before the script is able to run.
Not my project. Maybe ask the author why he named it that :)
Weirich, Voizard, Amorim, and Eisenberg (/u/goldfirere) are clearly trying to start a flame war with their paper: https://www.seas.upenn.edu/~sweirich/papers/systemd-submission.pdf systemd in Haskell? Not on my watch, Poettering! (Tongue in cheek.) 
&gt; but being able to swap around modules You mean like swapping `import Data.Map` out with something like `import Data.HashMap`? That is indeed an interesting prospect that I remember liking the first time I heard about some years ago. But in practice I can't say I have ever needed it. I'm much more often inconvenienced by "Dang it, what was the name of that package again? And its module name? Off I go to Google again, I guess." The one time I did do something similar was indeed when I was migrating a project to HashMaps, but even then I used an aliased qualification: `import Data.HashMap as Map`. Which I think could keep the functionality that I heard from you and others as beneficial of multi-layering namespaces: aliasing.
the : restriction is like the operator version of the capital letter restriction for non-operator constructors. it only deals with constructors 
It is currently a member of a class and often actually implemented. If we retroactively generalize the type of sequence we run into the problem that the existing implementations will mostly be wrong: sequence = mapM id is the backwards compatible definition today, but if we changed the signature of sequence, then it'd fail because mapM requires Monad, not Applicative. If we change the signature of mapM too then this line of code goes through unmolested so long as the definition someone wrote wasn't written manually with do notation. We could move it out of the class to the top level after a deprecation of redefinition, if we added that functionality to ghc, THEN we could generalize it (and mapM), but if we do this (and include mapM), you'll get folks who like ndm's trick of using super-small stacks to test for stack leaks complaining, because you have some code you can write with mapM today that uses far less stack space than the equivalent code using traverse (even if the applicative form uses far less stack than the other code uses heap overall.) The best upgrade path I know would be to move sequence/sequenceA out of the class to top level definitions, after a deprecation window, and then switch sequence to have the more general signature, but it requires _some_ appetite for code breakage on the behalf of the community.
yeah, you might: import Data.MAP and both containers and ordered-containers could provide implementations for the `MAP` interface. you'd need to parameterize over the constraints (`Ord` or `Hashable`) too, so idk. 
Sticking systemd in GHC is never going to happen because GHC runs in Linux, OSX, and Windows. It's also absolutely useless to put inside a compiler, like why the hell would you even want to do that? I quite like this trend of adding dependencies everywhere (when done correctly). People are finally starting to understand that "not invented here" is totally okay. Build upon the blocks of those who have already built something; stand on the shoulders of giants. Don't reinvent the wheel for the millionth time. When done correctly, this should simplify your program, improve its correctness, and enhance its capabilities productively.
The problem is how can you trust any program written in a non-functional language? Those are riddled with pointer errors, buffer overflows, and other failure states that are completely unacceptable. We should insist on a system stack, from transistors up, that is purely functional. 
There are other more specialized solutions for specific frameworks, but I've always relied on the simple [wai-middleware-static](https://hackage.haskell.org/package/wai-middleware-static-0.8.1/docs/Network-Wai-Middleware-Static.html) package. In the middleware approach the requests get intercepted before reaching the rest of your app, which works fine in most cases.
He does mention ML in "What is Object Oriented Programming?" which also makes the influence of CLU, Ada, and Modula-2 clear. It's clear he didn't get a whole lot from ML and didn't seem to understand it very deeply, but he was at least aware that it supported some form of type parameterization.
I've just updated my gist once more to show that the approach also works if you need to use `Identity` rather than `I`.
I found out about it partway through, but the documentation is much less straightforward for gi-gtk so I decoded not to switch. The gtk3 package has enough functionality for me to complete [my project](https://github.com/george-steel/maxent-learner) with just a few extra attribute definitions.
I don't understand that. Why can't instance writers just define `*&gt;` to equal `&gt;&gt;`?
I can't believe nobody has mentioned the awesome videos from the Catsters yet! Highly recommended. https://www.youtube.com/user/TheCatsters (see also http://simonwillerton.staff.shef.ac.uk/TheCatsters/index.html for my attempt to put some order to them, though that list is probably not up to date.)
Why Zypper instead of Zipper? Am I missing something?
I hope the stack won't be lazy, or it's going to be hell to program on those computers :)
[Faster Coroutine Pipelines](http://spivey.oriel.ox.ac.uk/wiki2/images/a/a1/Copipes.pdf) looks great. I don't see any reference to releasing the code as a library though.
I avoid partial functions whenever I can. I would rather use my original apprach than (!).
I'm mapping over a function? AFAIK, I map an anonymous function over a list of values from a sum type. I store them in a map because in my web application, I need access to those values after startup.
Shameless plug, a DSL version of the same concept: https://github.com/NorfairKing/zifter (still very young)
**A** can be defined as a construction on any functor, so we avoid a limited `newtype` over lists newtype f :$ a = App (f a) instance Functor ((:$) f) where type Dom ((:$) f) = Equal type Cod ((:$) f) = Equal fmap :: Equal a a' -&gt; Equal (f :$ a) (f :$ a') fmap Refl = Refl It could also have `(-&gt;)` as a codomain, so these two cases could be defined for any functor `f` and wouldn't have to be tied to lists. **C** and **D** are good examples, it would be fun to implement **C** using Liquid Haskell as the paper [*Verified Parallel String Matching in Haskell*](http://goto.ucsd.edu/~nvazou/papers/esop17.pdf) type Morphism n m F = x:n -&gt; y:n -&gt; { F mempty = mempty &amp;&amp; F (x &lt;&gt; y) == F x &lt;&gt; F y }
I use `mapM` as a type-specialized form of `traverse`. I use it when I want to make it very clear in my code, both to human readers and the type checker, that I am talking specifically about lists and a `Monad`here, not just any `Traversable` and not just any `Applicative`. So first of all, I am opposed to all the suggestions about generalizing `mapM`. If you want general, why not just use `traverse`? So I think `mapA` would actually be quite nice - but definitely not as a synonym for `traverse`. If I would see `mapA` in code, I would understand it to mean `traverse` specialized to lists - halfway between `traverse` and `mapM`. That would be quite useful. But I also understand that there's a limit to how many names we should throw into every namespace in the core library.
&gt; is your stack up to date? Of course. The only thing unique about my setup might be that I use Nix integration to get GHC? But that really shouldn't affect Stack's caching logic in a way like that.
This is so great. The Haskell native GUI situation is 10x better than it was even a year ago.
Sounds like it should use Void instead of ()?
Underrated. We use it all over the place in the middle of the book.
Yes, I was fully aware of that. I read the op's comment the wrong way and so my comment doesn't make any sense now that I've had more sleep...
Still working my way through that. Had a question part way through because I have seen this pattern multiple times before: The direct implementation uses existential quantification with data DirectPipe ι o α = ... | ∀ β . Effect (IO β) (β → DirectPipe ι o α) Is there a reason this couldn't just be data DirectPipe ι o α = ... | Effect (IO (DirectPipe ι o α)) 
So .. what _is_ a Haskell study group?
Not sure what a Haskell study group is but it sure sounds welcoming.
They can, but don't always remember to.
Eh, you can just wait and write the program when you need to use it. It'll be fine. 
I was meaning "effects" as in state changes or value accumulation, or of course printing and such. I didn't mean like "side effects" in the sense of IO. `traverse` is only ever more powerful than `fmap` when you have some sort of "effect" between each item you traverse over. 
Spock has a function [middleware](https://hackage.haskell.org/package/Spock-0.12.0.0/docs/Web-Spock.html#v:middleware) that enables the use of WAI middleware. There is a package called [wai-middleware-static](https://hackage.haskell.org/package/wai-middleware-static) that can be used to do what you're asking. 
Happy birthday! http://taylor.fausak.me/2017/05/04/one-year-of-haskell-weekly/
But mapM is not specialised to lists as of base 4.8. Are you using an older GHC? https://hackage.haskell.org/package/base-4.8.0.0/docs/Prelude.html https://hackage.haskell.org/package/base-4.7.0.0/docs/Prelude.html
Would you call `traverse (\a -&gt; [x | ...])` an effectful function? I've never heard the term "effect" used for something a pure list function like that. I'd probably call it an algorithm.
thx
Now we just need to work on the documentation problem.
This usage of "effect" dates at least to [Wadler's papers on monads](http://homepages.inf.ed.ac.uk/wadler/topics/monads.html) from the early 90's. Here are two representative quotes from *Monads for functional programming*. On page 6: &gt;In general, a function of type *a → b* is replaced by a function of type *a → M b*. This can be read as a function that accepts an argument of type *a* and returns a result of type *b*, with a possible additional effect captured by *M*. This effect may be to act on state, generate output, raise an exception, or what have you. On page 21: &gt;If monads encapsulate effects and lists form a monad, do lists correspond to some effect? Indeed they do, and the effect they correspond to is choice. One can think of a computation of type *[a]* as offering a choice of values, one for each element of the list. The monadic equivalent of a function of type *a → b* is a function of type *a → [b]*. The key link lies in the "correspond to" qualifier in the second quote: Wadler's pitch was that monads can be used to express things that impure languages often encode as side effects (cf. the abstract of the paper, which talks about "simulating effects found in other languages"). Through metonymy, that has readily led to an additional, broader meaning of "effect". It is this sense that shows up in the title of [*Applicative Programming with Effects*](http://www.staff.city.ac.uk/~ross/papers/Applicative.html), the functional pearl by McBride and Paterson that introduced `Applicative` (and `Traversable`). 
Lux has macros and a free monad-supporting type system, so you could build a free monad-generating DSL in it
You're recursively calling `restrict p handle`, with `handle :: b -&gt; StateT s m c`, so there's a problem with your implementation since this does not use the `Restrict m n a b` constraint you require. The following implementation type checks, but may have a bit of a counter-intuitive behavior because the handle resets to the state before running the `lower` action. restrict p handle lower = StateT $ \s -&gt; restrict p (\b -&gt; runStateT (handle b) s) (runStateT lower s) It isn't possible to do this without changing the types. Since `StateT` is the outermost layer of your transformer stack, `StateT s m a`, isomorphic to `s -&gt; m (a, s)`, the state will be lost on failure. --- Now, why exactly did you run into an infinite compie-time loop? restrict p handle = StateT . (\k -&gt; restrict p handle . k) . runStateT :: StateT s n c -&gt; StateT s m c Let's deduce the expected type of `restrict`: (\k -&gt; restrict p handle . k) :: (s -&gt; n (c, s)) -&gt; (s -&gt; m (c, s)) restrict p handle :: n (c, s) -&gt; (s -&gt; m (c, s)) restrict :: APrism' b a -&gt; (b -&gt; StateT s m c) -&gt; n (c, s) -&gt; (s -&gt; m (c, s)) Note that the actual type of `restrict` is restrict :: Retrict p q a b =&gt; APrism' b a -&gt; (b -&gt; p c) -&gt; q c -&gt; p c Thus, we deduce some type equalities: `p ~ StateT s m`, `q ~ n`, `p ~ m`, and from the functional dependency `p b -&gt; q`, we get `(StateT s m) b -&gt; StateT s n` thus `q ~ StateT s n`. We get infinite types: `StateT s m ~ m`, `StateT s n ~ n`. Then for some reason (I'm not certain whether there is a good one) the constraint solver is being run for `Restrict (StateT s m) (StateT s n) a b` with infinite types `m` and `n`. The constraint context then requires `Restrict m n a b`, which is equivalent to the original constraint, and there's the loop. By default there is a bound on the constraint solver to ensure termination; did you disable it? In addition to the error about a potential infinite loop, I get errors about the two infinite types/occurs check failures, as well as a mismatch between `c` and `(c, s)` we can see above.
My complaint is that `f :$ a` and `f a` are distinct (isomorphic) types. This gets cumbersome when we deal with the same type constructor in different contexts. For example, newtype Prompt p a = P { unP :: forall b. (forall i. p i -&gt; (i -&gt; b) -&gt; b) -&gt; (a -&gt; b) -&gt; b } instance Monad (Prompt p) where return a = P $ \c k -&gt; k a m &gt;&gt;= f = P $ \c k -&gt; unP m c $ \a -&gt; unP (f a) c k instance IMonad Prompt where iskip p = P $ \c -&gt; c p iextend f m = P $ \c -&gt; unP m $ \p -&gt; unP (f p) c There's no way to define `Prompt` in `hask` so that we get both of these instances. If we want to reflect that `Prompt p` is a free monad for the type constructor `p`, then we need to have instance Functor Prompt where type Cod Prompt = Nat Equal Hask -- type constructor type Dom Prompt = Nat Hask Hask -- functor but now we can't make `Prompt` a monad, because that would require instance Functor Prompt where type Cod Prompt = Nat Equal Hask type Dom Prompt = Nat Equal Hask Since `Nat Hask Hask` is a subset of `Nat Equal Hask`, there's a general construction that will one to the other, but now we have to use newtypes all over the place. In contrast, right now we can intermingle `&gt;&gt;=` and `iextend` all we want.
&gt; `handle :: b -&gt; StateT s m c` Well that seems dumb now that you've pointed it out to me. I guess I automatically assumed it would be `handle :: b -&gt; m c` because that's the type I really wanted! The fact that the compiler looped forever meant that I didn't get the type error to tell me I was wrong... Thanks for bringing up the problem about the state being thrown away! I wonder if I can come up with a solution by changing the type of `restrict`. I didn't disable any constraint solving bounds, but I am using GHC 7.10 (mostly by accident) so maybe there's a difference in behaviour with 8.0? --- I'd still love to have `Magnified` and `Zoomed` explained to me. I fear few people other than /u/edwardkmett understand what's going on there.
Personally I'd be happy to join a study group run on these principles.
I don't know why you'd try to avoid the java dependency. Tools like closure compiler just work on any platform with basically any configuration and will probably work for another 20 years. By contrast prepack is a node package which means it'll maybe work today maybe on your system and you need node installed and npm. Regarding GHCJS, it doesn't output idiomatic JS code. It outputs a metric tonne of assignment statements and various goto like use of switches with registers. The runtime is more idiomatic JS so you might get some saving. I'd expect it to be minimal.
Yeah I think that is a fair definition, I definitely don't have anything better. I mean I guess you could say something like an applicative map, or a map lifted into an applicative, although that may have a different meaning than the one I intend.
Yes, I conceded the point about complexity earlier: &gt; &gt; I stand corrected - the complexity is optimal here And I predicted the constant factor to be terrible on the weird-lazy version.
...and if you don't?
&gt; Closure Compiler is written in Java [closure-compiler-js](https://github.com/google/closure-compiler-js)
Then probably two spock services on different ports
[removed]
Not if the monad instance has stronger constraints than the applicative instance.
You need node to build GHCJS (and TH) anyways, not seeing much of a downside if it was already a dependency.
how is that better?
thx
This is interesting, do you have information on `Prompt`? Will respond to the rest later
`(:+:)` exists in base as [`Sum`](https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Functor-Sum.html)
It's standard practice to serve static files directly from Apache or Nginx because they are designed to perform this task. They do it really well and they do it quickly. By contrast, most web server frameworks are designed to render responses, often by looking up some information in a database or cache. In other words, your webserver is a program designed to mash together some data in order to assemble a response, and your static content isn't dynamic, so why run another program to return it to users? You're (usually) already running a server and it's great at sending files back. That's why this is a standard move. 
FreeBSD and linux are two different operating systems? What was your expectation when you ran the binary?
`FATAL ERROR: CALL_AND_RETRY_LAST Allocation failed - JavaScript heap out of memory` ...on a 17MB ghcjs output file.
You're looking for languages with built-in effect system. See `Frank`, `Koka` etc.
Same for 3 purescript projects I tried it with (i.e TypeError somewhere)
There is now a JS version of the closure compiler: https://github.com/google/closure-compiler-js There isn't any need to have the JRE installed anymore!
&gt; So .. what is a Haskell study group? Think of it as a warm tortilla, into which you are wrapped with black beans.and cheese.
Have you tried compiling and running something simpler (e.g., a C file containing an empty `int main() { return 0; }`)? From what I found in the [FreeBSD handbook](https://www.freebsd.org/doc/handbook/linuxemu-lbc-install.html) it seems that you need to configure its Linux binary compatibility layer before running Linux executables.
Thanks for the feedback! I also published this piece on Medium: https://medium.com/@sjsyrek/what-a-haskell-study-group-is-not-470f4aeb9673 And I wrote a little tutorial to accompany it: https://medium.com/@sjsyrek/some-notes-on-haskell-pedagogy-de43281b1a5c The title was inspired by an article I assumed was well-known, but maybe not: https://wiki.haskell.org/What_a_Monad_is_not As for the snark, I'm pretty snarky myself, and whenever you publish anything you expose yourself to a certain amount of that. Heck, I'm actually a Shakespeare scholar, and my colleagues and I would often sit around making snarky comments about Shakespeare, whose works themselves are at least 60% snark. I do hope the material is useful. All I can say is that so far this is what has worked for me, based partly on running an actual study group and partly on having been a teacher for over a decade (which is what makes me sound like a pedagogue). It may sound disciplinarian, but it was actually pretty fun because we were all on the same page. But, as with anything, YMMV.
Amen brother
It's the length of the first incredible prefix. But that code isn't copyrighted, feel free to use a 0 in your version :P
Looked at your fix and it still seems wrong. The first incredible prefix of [3,3,4,3,5,2] has length 6, but your code returns 4. The first incredible prefix of [2,2,3,2,1] has length 4, but your code returns 2. The first incredible prefix of [2,3,2,3,2] has length 4, but your code returns 5.
I was definitely not aware of that article! Thank you for providing context and for reacting positively to the ... snarky reactions to your article :)
You'll probably need additional parameters for all the sorts you want to use mutually recursive. That also means you need a fix operator that can cope with multiple sorts. I recently googled and found a solution for that, but I'm currently on mobile. Edit: also, depending on what you want, literals in that position totally couldmake sense and have to be sorted out by the type system.
Nice! I agree that it works, has the right complexity, and can be made streaming. But I think the streaming version will have worse asymptotics in a certain sense. If there are large values at the start of the list, you'll need to build a large set, even if the eventual return value is small. Whereas the last solution from my post is always O(n log n) where n is the return value. I wonder if it's possible to keep track of occupied positions instead of unoccupied? Or keep track of occupied intervals, or something.
Yes, that's true. I started off trying to do it with occupied positions and couldn't see an obvious way to find the right position in log time with Data.Set. But it does seem like there could be a data structure that has that operation - need to have a look on Hackage.
You need indexed functors for that. [Here's](http://lpaste.net/355229) a quick example.
Should my proof of enrollment be submitted before the deadline on May 6th ?
I use ipython as my Python REPL, which is pretty good. I actually love the vim+ipython integration which allows me to edit a file and interpret bits and pieces of it. It looks error-prone, but it's actually a very nice way of working. Unfortunately I have not gotten it to work 100% across all the servers I am forced to use, so often I'm using normal ipython.
Found this, uploaded two days ago: https://hackage.haskell.org/package/frpnow-gtk3 FRPNow seems similar in power to Reflex, which has commercial users and is under active development. It even has soft documentation: https://reflex-frp.readthedocs.io/en/latest/
The short answer is yes, the long answer is maybe. It is sometimes possible to build a Haskell binary on one Linux distribution and then run it on another Linux distribution, a long as they are run sufficiently similar versions of the Linux kernel. Even then, you can have problems if systems libraries like libc are not the same. That can sometimes be worked around by statically linking everything into your binary so that the only external calls it makes are to the kernel. While FreeBSD and Linux may feel very similar at a high-level, FreeBSD has its own kernel, its on libc, etc. So building a binary on Linux and then copying it to a FreeBSD system is no different than copying it to an OS X or Windows system. It won't run because the kernel, the libraries, and everything else it expects to find are not there. So, yes, you do need to install Haskell on FreeBSD and compile it all over again. Now, that said, there is a Linux emulation layer available for FreeBSD. I'm pretty surer it was developed because certain vendors released Linux-only binaries that FreeBSD users wanted. For example, things like Macromedia Flash. Could you use that compatibility layer to run the Linux binary? Perhaps. I do not expect it will be trivial. It would depend how similar your Arch build environment is to the Linux compatibility layer that FreeBSD provides. You would also likely need to statically link the binary, to minimize the number of dependencies you need to copy over. I would expect to see better performance if you compile natively for FreeBSD. 
The JavaScript version of the Closure-Compiler is transpiled by GWT from the Java source
Hmm... Last time I used frpnow in ghcjs, the total memory used by the JS process increased every time I pressed a key. You didn't even need the profiler window open to observe this -- user interaction got visibly slower each time. Is there a new release out? This was a year ago that I tested.
I've written an implementation I believe will work in linear time using mutation. It's in javascript so you can test it in the browser if you'd like. function solve(data) { const arr = data.map(_ =&gt; 0); const n = data.length; data.forEach(p =&gt; { if (p - 1 &lt; arr.length) { arr[p - 1]++; } }); let acc = 0; for (let i = 0; i &lt; n; i++) { if (acc &gt; i) { return i; } acc = acc + arr[i]; } return n; } It relies on the observation that if you have `n` elements and you receive the information that a game is in my top `n, n + 1, ...` favourites there is no way this could cause problems for the "credibility" of the set. Am I wrong here, can you provide a failing test case?
No, don't worry about it, if we need it we will give students more than enough time to request it from their universities.
This returns 1 for [2,1,1]. Should be 2. edit: because it throws away the input order it can never give different results for different permutations - but things like [10,10,10,10,1,1] vs [1,1,10,10,10,10] have different valid prefixes. 
That probably works. Quadratic runtime isn't very good though :-(
[This](https://www.andres-loeh.de/Rec/MutualRec-2008.pdf) is the paper I was talking about.
I've updated the post with a new imperative version that tracks occupied intervals using union-find. It should be the fastest so far, almost O(n) time, though it doesn't work well with purity or streaming. No idea how to make it better yet.
What are the differences between FRPNow and Reflex?
 sudo pkg install linux_base-c7 sudo kldload linux64 ./linux-binary :) I used it for [a game](https://sites.google.com/site/broguegame/home) and [an SMT solver](http://cvc4.cs.stanford.edu/web/) (the latter I actually built natively once, but that required some fixes). And there are some pre-packaged Linux apps in ports/pkg, e.g. `linux-sublime3`, `linux-doom3`, `linux-quake4`… You can also unpack a filesystem image of whatever distro (e.g. [ubuntu cloud images](https://cloud-images.ubuntu.com/)) somewhere and run something like `jail -c path=/where/you/extracted ip4=inherit linux=new command=/bin/bash` and you're in a Linux shell :D However some binaries (e.g. everything in Alpine Linux) aren't marked as Linux binaries, you'd have to `brandelf` them.
That's interesting. I didn't realize there was much of a difference in the way that those two tools worked.
Put [Caddy](http://caddyserver.com) in front of it.
[It lazily executed, by not executing at all.](http://i2.kym-cdn.com/entries/icons/original/000/022/138/reece.JPG)
It's O(m log m) where m is the length of the longest credible prefix, and m can't be greater than k for obvious reasons.
Also `ScopedTypeVariables`, for very similar reasons.
On the PureScript project I tried it with (the `purescript-unfoldable` test suite), it worked and even ran the tests at compile time. [Here is the compiled JS output](https://gist.github.com/paf31/6ecb04958f409127bd29305013c3dcc4#file-purescript-prepack-js-L895). Maybe you were using something with a `window` dependency?
yes, but how to exactly? I've updated my question.
 app &lt;- spockAsApp . spock spock_config Warp.run port app 
For whatever it is worth, there is no need to obfuscate your code with `.` and `$` and guards and other such things here. This function can be defined in terms of pattern matching in a straightforward manner as such: tryAgain :: [Int] -&gt; [Int] tryAgain [] = [] tryAgain [x] = if x &lt; 8 then [x + 1] else [] tryAgain (x : xs) = x : tryAgain xs A benefit of defining it this way is that the function is total, i.e. it will always return a result and will not crash on things like the empty list.
That's... good, I guess, but if FRPNow is really "finished" and "proven", I'd really like to see a 1.0 release even if it is identical to the 0.18 one... or at least a statement in the Hackage description that says something along the lines of "This library is considered complete and there will be no API changes (except additions), modulo bugs". Also, the [issues list](https://github.com/atzeus/FRPNow/issues) is also somewhat interesting. It basically appears that even *if* FRPNow is complete, it's missing a maintainer who's willing to maintain it. (Don't get me wrong, I fully understand that people may not always have time to devote to stuff they've put out there, but what I absolutely cannot stand is just letting $PROJECT linger in *uncertainty* about its maintenance status. If you don't have time/interest -- just say so!)
Well, one is that Reflex is being actively worked on. (No opinion of which is better/cleaner/whatever.)
Surely Reflex *does* have some issues with space/time leaks, though? I mean this is a research-level problem unless you invent a new language (see [2] in my other post.)
Yeah, it seems like I misunderstood what you meant by k. Sorry. That said, I don't see why your estimate is correct. It seems like for each x you potentially look through many bits.
where is "runTls"?
It's in this package: https://hackage.haskell.org/package/warp-tls-3.2.3/docs/Network-Wai-Handler-WarpTLS.html So, you'd need to import `Network.Wai.Handler.WarpTLS as WarpTLS`, and use `WarpTLS.runTLS` with the appropriate tls settings and warp settings.
Yeah, it can, but it's usually not too many bits to go through. 
Done! http://hackage.haskell.org/package/duckling
haha, natural language is so ridiculously ambiguous, from parsing to semantics. 
Thank you very much for your help. I would have never figured it out myself. I started to integrate your code in my project and it compiles. I didn't manage to actually test it as I filled my hard drive and lost a few days of code trying to make some disc space. Anyway, is the undecidable superclass really necessary ? My project uses an old version of ghc and I don't really have the time to upgrade to ghc-8 at the moment. Also, why using I instead of Identity ? Identity is standard and has probably more instances than I. If it's just to sorthen its name maybe you could use pattern synonyms instead to alias Identity to I Regarding your design question I already have a Record (without any parameter) which comes from a database. I need to group similar rows and show the differences if any in web page so the user can detect eventual mistakes and fix them if needed. Rows represent basically product variations (like style + colour + price) and we need to check that all variations of the same style have the same price. I generate a `Record f` from `Record` so I probably could generate a non parametric `RecordMaybe` instead. However, the real case is a bit more complicated and the Maybe will end up being a big functor decorating each field with information relative to validation and how to display each values (I'm typing on my phone so I apologize for any mistakes or typo)
I'd recommend reading [Haskell Programming from First Principle](http://haskellbook.com/) first
Building a little bit on /u/djfletch's approach, you don't actually have to traverse the whole list from the beginning. You can keep track of the largest item seen so far: cred :: [Int] -&gt; [Int] cred xs = build $ \c n -&gt; foldr (f c n) (\_ _ -&gt; n) xs 2 (IntSet.singleton 1) where f c n e a b s | e &lt; b = maybe n (c e . a b . flip IntSet.delete s) (IntSet.lookupLE e s) | otherwise = e `c` a (e + 1) (IntSet.union s (IntSet.fromList [b..e-1])) Since we're doing a `deleteLE`-kind-of operation, which `IntSet` doesn't have, you can roll your own: data Tree = Leaf | Node {-# UNPACK #-} !Int Tree Tree cred :: [Int] -&gt; [Int] cred xs = build $ \c n -&gt; foldr (f c n) (\_ _ -&gt; n) xs 2 (Node 1 Leaf Leaf) where f c n e a b = case compare e b of GT -&gt; c e . a (e + 1) . extend EQ -&gt; c e . a (e + 1) LT -&gt; maybe n (c e . a b) . deleteLE e where extend Leaf = mk b (e - 1) extend (Node x l r) = Node x l (extend r) mk b e | b &gt; e = Leaf | otherwise = m `seq` Node m (mk b (m - 1)) (mk (m + 1) e) where m = (b + e) `div` 2 deleteLE _ Leaf = Nothing deleteLE x (Node y l r) = case compare x y of LT -&gt; flip (Node y) r &lt;$&gt; deleteLE x l EQ -&gt; Just (merge l r) GT -&gt; Just (maybe (merge l r) (Node y l) (deleteLE x r)) merge Leaf Leaf = Leaf merge l Leaf = l merge Leaf r = r merge l (Node y yl yr) = Node key l r' where (key,r') = minView y yl yr minView y Leaf r = (y, r) minView y (Node x xl xr) r = case minView x xl xr of (ny,nl) -&gt; (ny, Node y nl r) Based on some quick benchmarks, this takes about 180ms for lists of length 100000.
 &gt;Maybe you were using something with a `window` dependency? Yes. 
Ah, it turns out you were right that a linear search was not sufficient - I was testing with randomly generated data, but the malevolent case (replicate 10000 10000) kills mine, while other algorithms hold up well 
The reason is simple, actually: my spelling is crappy.
fwiw there's built-in windows (mingw-w64) cross-compilation support in nixpkgs. though I don't know if there's anything like patchelf for PE files, so using pre-compiled binaries might be tricky.
Why? Hasn't it been shown that warp performs better than nginx? Maybe haproxy to terminate TLS.
Better benchmarks (using replicate n n): benchmarking replicate 10 10 /cl'_me - 1.655 us (1.639 us .. 1.678 us) /cred_djfletch - 2.700 us (2.561 us .. 2.872 us) /cred_foBrowsing - 672.0 ns (665.8 ns .. 679.0 ns) /cred_foBrowsing' - 967.8 ns (961.9 ns .. 973.8 ns) /credibleLength3_OP - 2.878 us (2.829 us .. 2.922 us) /credibleLength4_OP - 1.544 us (1.221 us .. 1.784 us) /credibleLen_Purlox - 5.493 us (5.404 us .. 5.582 us) benchmarking replicate 1000 1000 /cl'_me - 1.078 ms (1.052 ms .. 1.108 ms) /cred_djfletch - 1.552 ms (1.520 ms .. 1.583 ms) /cred_foBrowsing - 304.2 us (298.3 us .. 312.0 us) /cred_foBrowsing' - 427.3 us (420.1 us .. 434.8 us) /credibleLength3_OP - 614.7 us (602.8 us .. 627.1 us) /credibleLength4_OP - 253.4 us (250.7 us .. 257.0 us) /credibleLen_Purlox - 36.84 ms (36.15 ms .. 37.38 ms) benchmarking replicate 100000 100000 /cl'_me - 1.254 s (-282.8 ms .. 2.416 s) /cred_djfletch - 232.3 ms (228.9 ms .. 236.0 ms) /cred_foBrowsing - 59.86 ms (58.54 ms .. 60.99 ms) /cred_foBrowsing' - 85.26 ms (84.03 ms .. 86.34 ms) /credibleLength3_OP - 246.9 ms (203.7 ms .. 286.9 ms) /credibleLength4_OP - 101.4 ms (98.12 ms .. 105.9 ms) /credibleLen_Purlox - no finish Here's my better code: cl' :: [Int] -&gt; Int cl' ls = go mempty ls where go acc [] = IntSet.size acc go acc (x:xs) = case missingLTE x acc of 0 -&gt; IntSet.size acc x' -&gt; go (IntSet.insert x' acc) xs missingLTE :: Int -&gt; IntSet -&gt; Int missingLTE n s | not isin = n | otherwise = case IntSet.splitRoot smaller of [ ] -&gt; pred n [_] | pred n `IntSet.notMember` smaller -&gt; pred n | otherwise -&gt; IntSet.findMax $ IntSet.map pred smaller IntSet.\\ smaller ss -&gt; either id id $ foldr (\a e -&gt; e &gt;&gt;= folding a) (Right $ pred n) $ mapMaybe mhole ss where (smaller, isin, _) = IntSet.splitMember n s folding :: (Either (Int,Int) (Int,Int)) -&gt; Int -&gt; Either Int Int folding (Left (thismax,missing)) x = Left $ if thismax == x then missing else x folding (Right (thismin,thismax)) x = if thismax == x then Right $ pred thismin else Left x -- returns Nothing on null, or Left (max,firstmissing) or Right (mix,max) mhole :: IntSet -&gt; Maybe (Either (Int,Int) (Int,Int)) mhole s | IntSet.null s = Nothing | otherwise = Just $ let (siz,(l,r)) = (IntSet.size s, (IntSet.findMin s, IntSet.findMax s)) in if r - l == pred siz then Right (l,r) else Left $ (,) r $ missingLTE r s
no
You could try starting with the free [Haskell wikibook](https://en.wikibooks.org/wiki/Haskell). I have a friend who is learning from that with good results. Before you can really grok Spock, you'll need to have basic familiarity how types work in Haskell and how to work with `do` notation fluently. Since your existing programming experience is with untyped, impure, imperative programming languages, you can humbly consider yourself a beginner when it comes to typed, pure, functional programming. The two worlds are quite different! Most of us had to go through a similar path, so we can sympathize with the difficulty you're having.
Have a look at HalVM. They've done most of this RTS work already. 
It's possible you don't need UndecidableSuperClasses in ghc-7. There are a few situations where the flag is necessary with ghc-8 where the code just compiles with ghc-7. Regarding I vs Identity. The arguments are given in the module comments of Generics.SOP.BasicFunctors. We might switch to the standard versions + pattern synonyms once the exhaustiveness checker is good enough. I do see the need for computing all the Maybe values. What I am trying to understand is why you need a concrete datatype. Why is `SOP Maybe (Code OriginalRecord)` not good enough. Wouldn't your library provide all the functions to work with this type? Is your code available anywhere so that I could see what you are doing? 
Good question, I am curious about that too only in Yesod
`mod` is for chumps. fizzbuzz n = zipWith coalesce fzbz $ show &lt;$&gt; [1..n] where fz = cycle $ replicate 4 "" ++ ["Fizz"] bz = cycle $ replicate 2 "" ++ ["Buzz"] fzbz = zipWith (++) fz bz coalesce a b = if null a then b else a (I'm not really a fan of this solution since it's biased to start at 1 and can't handle negatives, but the same applies to the article, so!)
I usually just run behind a reverse proxy that handles https for me, as well as virtual hosts, static files, caching, and a bunch of other things. No need to bake that into the app itself.
+1
You can build a static Linux binary with the `-optl-static` argument to GHC. You may need to build on Alpine to have access to a static `libcrt0`, I build inside an Alpine Docker container on a Mac. Copy the resulting binary to a FreeBSD machine with the `linux64` ABI enabled (`kldstat` will list loaded kernel modules). You will need to tag it as a Linux ELF with `brandelf -t Linux your-binary`. Then it _should_ just run. The only fbsd machines I have login to right now only have 32-bit Linux ABI enabled, and cross-compiling to 32-bits with GHC is a different challenge I'm not up for right now :-)
A bit simplified: fizzbuzz n = take n $ zipWith ($) (cycle fs) [1 ..] where fs = [ show, show, cFizz, show , cBuzz , cFizz, show, show, cFizz, cBuzz , show, cFizz, show, show , const "FizzBuzz" ] cFizz = const "Fizz" cBuzz = const "Buzz"
&gt; An Example. &gt; where am I supposed to use my function "app"?
Your `app` is my `appRouter`
My favorite dumb FizzBuzz: fizzes = cycle $ Just "Fizz" : replicate 2 Nothing buzzes = cycle $ Just "Buzz" : replicate 4 Nothing noNumbers = zipWith mappend fizzes buzzes fizzBuzz = take 100 $ tail $ zipWith fromMaybe (show &lt;$&gt; [0..]) noNumbers
I assume you're suggesting that referential transparency is broken. I believe this is not the case, since you can only read your `FindData` in `IO`. `IORef`s work just like this too.
Thanks :)
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [maxigit/Fames/.../**Index.hs** (items-table → 71d903e)](https://github.com/maxigit/Fames/blob/71d903e4c410e37c3aa3ada97ee4211661014468/Handler/Items/Index.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dh77ng3.)^.
It's unfortunate no one is explaining the downvotes. Perhaps we need an FAQ entry. Basically, posts advertising discord channels for FP and Haskell chat have become very common lately. Meanwhile, the vast majority of existing Haskell users prefer the existing IRC channel. Good luck with your discord, sorry about the downvotes.
Please let me know when the tutorial is ready :)
Thanks that makes sense, misunderstood the problem somewhat.
I'm not familiar with extensible, but I'll have a look. If it's just name-based lenses/accessors, you can easily define them with generics-sop. I don't think that changing the types of individual fields is directly within scope of the library as it is now, although I'm working on a library for datatype migrations on top of generics-sop which will come with some amount of support for such things.
There is definitely some overlap between extensible HList and SOP NP as well some type family machinery. I'll guess we'll have that kind of problem until some of those features becomes parts of Haskell itself (or at least are moved to base). Disclaimer : I haven't used extensible, just tried using for this same problem and it didn't work either out of the box.
&gt; IIRC reactive-banana made some changes to some combinators / semantics after the FRPNow paper came out, and that got rid of the potential for leaks there The changes were unrelated to the FRPNow paper; time leaks had already been eliminated since the first version of reactive-banana (but the API didn't reach the point where this mattered until reactive-banana-0.7 in 2012). But there was a choice between two quite different APIs for doing this, and I had come to the conclusion that the choice that reactive-banana was trying out was unwieldy in practice. [More detailed discussion of the changes][1]. More changes where introduced later, but they [just fixed an oversight on my part][2]. [2]: http://apfelmus.nfshost.com/blog/2016/01/04-frp-banana-1-1.html [1]: http://apfelmus.nfshost.com/blog/2015/08/29-frp-banana-api-redesign.html
Great work! It looks like I still have a lot to learn about Haskell. I rewrote the code without dots or dollars, but it's still completely mystifying. import Data.IntSet (singleton, delete, lookupLE, union, fromList) import GHC.Exts (build) cred xs = build (\c n -&gt; foldr (f c n) (\_ _ -&gt; n) xs 2 (singleton 1)) where f c n e a b s = if e &lt; b then maybe n (\x -&gt; c e (a b (delete x s))) (lookupLE e s) else c e (a (e + 1) (union s (fromList [b..e-1]))) Is there an easy way to see the types of f, c, n, e, a, b and s?
Looking back at it now, I think my code is probably mystifying because it's poorly written! I really should have used better variable names (or a comment or two). For the `f` function, there's a couple (probably unnecessary) tricks going on. The first is the `build` function: this is an optimisation for building up lists. It works like this: if you have a function that uses `:` and `[]` to build up a list: map' :: (a -&gt; b) -&gt; [a] -&gt; [b] map' _ [] = [] map' f (x:xs) = f x : map' f xs You can rewrite it using `build`, replacing `:` and `[]` with the first and second parameters, respectively: map' :: (a -&gt; b) -&gt; [a] -&gt; [b] map' f xs = build (\c n -&gt; mapBuild c n f xs) mapBuild c n _ [] = n mapBuild c n f (x:xs) = f x `c` mapBuild c n f xs The point of this is that, using rewrite rules, if the resulting list is consumed using `foldr`, the "combining" function can be subbed in for `c`, and the base case can be subbed in for `n`, avoiding the intermediate list. Let's say I had defined `length` like this: length = foldr (\_ a -&gt; 1 + a) 0 Applying that to the `map'` function would be rewritten (by GHC) like this: length (map' f xs) length (build (\c n -&gt; mapBuild c n f xs)) foldr (\_ a -&gt; 1 + a) 0 (build (\c n -&gt; mapBuild c n f xs)) mapBuild (\_ a -&gt; 1 + a) 0 f xs This is important here because I want the function to build up the longest credible prefix, but if only the length is needed, I want it to skip the intermediate list. That in mind, you can rewrite the function to be like this: cred :: [Int] -&gt; [Int] cred xs = foldr f (\_ _ -&gt; []) xs 2 (IntSet.singleton 1) where f e a b s | e &lt; b = maybe [] ((e:) . a b . flip IntSet.delete s) (IntSet.lookupLE e s) | otherwise = e : a (e + 1) (IntSet.union s (IntSet.fromList [b..e-1])) The next issue is that my fold is [leaning so far right it came back left](https://wiki.haskell.org/Foldl_as_foldr_alternative). This is a technique where you add an extra argument to the function passed to `foldr`, which effectively makes it act like `foldl`. The reason for that is, again, to use `foldr` to get fusion. However, you can of course rewrite it using manual recursion: cred :: [Int] -&gt; [Int] cred xs = cred' xs 2 (IntSet.singleton 1) where cred' [] _ _ = [] cred' (x:xs) b s | x &lt; b = maybe [] ((x:) . cred' xs b . flip IntSet.delete s) (IntSet.lookupLE x s) | otherwise = x : cred' xs (x+1) (IntSet.union s (IntSet.fromList [b..x-1])) Un-pointfree-ing the above will give you: cred :: [Int] -&gt; [Int] cred ys = cred' ys 2 (IntSet.singleton 1) where cred' :: [Int] -&gt; Int -&gt; IntSet.IntSet -&gt; [Int] cred' [] _ _ = [] cred' (x:xs) b s = if x &lt; b then case IntSet.lookupLE x s of Nothing -&gt; [] Just le -&gt; x : cred' xs b (IntSet.delete le s) else x : cred' xs (x+1) (IntSet.union s (IntSet.fromList [b..x-1])) The `cred'` function takes the list to check for credibility, the largest number seen so far, a set representing open positions, and returns the longest credible prefix. 
Not really. The only thing you have to cater for is that internal linls your application emits have to be https, otherwise you will be serving mixed content, which modern browsers don't like (and rightfully so). The same obviously applied if your reverse proxy does additional rewriting, e.g. mounting your app on a sub-path. Other than that, though, the whole idea of a reverse proxy is that your app just serves http, and is blissfully ignorant of the reverse proxy.
Ah, yes, thank you.
I don't think I believe that coders intentionally complicate their code, nor that legislatures intentionally complicate legislation (at least, not for the "fame" of making complex legislation). 
Why? Haskell is a general purpose programming language, and an incredibly safe one at that. Definitely a trait that is especially desirable for operating systems. Edit: not to mention an incredibly performant language.
Haskell is a wonderful language to write an operating system in. I've written kernels in both c and Haskell. Haskell by far had the more pleasant experience -- once I got the rts down, there really weren't many bugs. My work is on GitHub: https://github.com/tathougies/hos (I used JHC instead of GHC so I can't answer the OPs question).
This [article on halvm3](http://uhsure.com/halvm3.html) is very informative. 
why? (as opposed to any of the other myriad choices)
&gt; Why cross-compilation? If your on x86 then no need to cross-compile. Well if that was true, you wouldn't need something like MinGW to compile for Windows under a Unix system.
Somewhere I’ve got a wildly over dense version of FizzBuzz that I saved last time it came around. Ah, here it is: -- matching function: (~&gt;) :: (Integral a) =&gt; a -&gt; String -&gt; a -&gt; Maybe String (n ~&gt; str) i = str &lt;$ guard (i `mod` n == 0) -- chaining function fizzbuzz = 3~&gt;"Fizz" &lt;&gt; 5~&gt;"Buzz" main = putStr $ concatMap (maybe "" (++ "\n")) $ fizzbuzz &lt;$&gt; [1..100] clearly this version has too many calls to mod however.
naqsha is a Persian word borrowed into Hindi/Urdu. The q is pronounced as a soft k so it sounds more like naksha. If one uses "ksha" in the english representation of a indic word it gives a sharper sound. 
It mentions that Conduit library by /u/snoyberg uses ADT as core type, but that's not the case. Actually it uses the same [CPS'ed representation](https://hackage.haskell.org/package/conduit-1.2.10/docs/src/Data-Conduit-Internal-Conduit.html#ConduitM). Also no mention of [machines](https://hackage.haskell.org/package/machines), which have both ADT and CPS'ed representations, combining advantages of both.
I wondered about that but assumed I was missing something.
Have you looked at my geodetics library? https://hackage.haskell.org/package/geodetics 
I use `Spock`, `warp-tls`, and LetsEncrypt for my homepage https://int-index.com Here's simplified code of how I run the application: httpsPort :: Warp.Port httpsPort = 443 mainSpockM :: SpockM conn sess st () mainSpockM = -- routing main :: IO () main = do conf &lt;- defaultSpockCfg () PCNoDatabase () app &lt;- spockAsApp (spock conf mainSpockM) let certPath = "/etc/letsencrypt/live/int-index.com/" Warp.runTLS (Warp.tlsSettingsChain (certPath ++ "cert.pem") [certPath ++ "chain.pem"] (certPath ++ "privkey.pem")) (Warp.setPort httpsPort Warp.defaultSettings) app Stackage resolver `lts-8.12`.
I guess now is a good time to mention Maciej Priog's solution: FizzBuzz in Haskell by Embedding a Domain Specific Language https://themonadreader.files.wordpress.com/2014/04/fizzbuzz.pdf
Your gitup page mention fallbacks to C when necessary. What aspects of the Haskell primops were unsuitable?
If your module is "Hello", the source file must be "Hello.hs", not "hello.hs".
Oh, ha. That worked. I didn't realize the file name was case sensitive. Thanks!
What does it mean? 
The API is written this way to avoid memory leaks, keep the memory usage down and limit the performance impact. `FindData` is a wrapper around a `ForeignPtr`. The memory is allocated by `findFirstFile`. A directory listing is an operation that can return lots of data. If the type was as you suggested it would mean that each invocation of `findNextFile` has to allocate space for a new foreign pointer. Remember that foreign pointers are pinned memory and are not garbage collected. (Though you could register finalizers but those aren't free). If you look at how this API would typically be used in C, you would probably write it in a loop, where you re-use the previous structure so you don't have to keep allocating memory for the next call. Long story short, the API is as it is, to force you to use it in a way that does not incur a large heap or GC overhead when iterating through large directories. Right now the memory use is constant, changing it to return a new `FindData` each time would make it linear.
I'd also like to mention this project that's been happening on and off at my school. It's HASP (high assurance system programming). The language, habit, is based on Haskell with an emphasis on low level manipulation of stuff like bits and registers safely. Definitely worth checking out as it's very much a "write an operating system in Haskell" sort of project. http://hasp.cs.pdx.edu/
Wait so let me make sure I understand. If the module is called *Main* does it matter if I call the file *main.hs* or *Main.hs* as long as I compile using the correct capitalization? Or is SOP to put a module *Main* in a file correspondingly *Main.hs*, or does it matter?
Hey! So as some people in other places in this thread have pointed out, haskell's type system can be used to verify a number of security guarantees :) It's also just a hobby project of mine, so I'm in part just motivated by liking operating systems and haskell :)
Exactly! I already have a gcc cross compiler and a full cross binutils :)
&gt; The module Main is the only module that is allowed to exist in a .hs file with any name. This is not true, GHC doesn't care what the filename is, but it will only *look* for files with the same name as the module, if you pass the other files in on the command line, it will happily compile.
Do you have any thoughts on what you would prefer instead of this sort of announcement? I mean, they have to get the word out some how so that people can learn about it and join. Just to be clear, I sympathize with OP. They are clearly annoying people here but I don't know how else they can let people know.
 I'm not interested in what you and your blue-pilled equals believe or not believe.
Maybe update categorization if it was missed by OP.
I'm very sorry for the joke, but Haskell not longer will be unjustly pinpointed for "that language in which you need to be a mathematician to do IO" but justly pinpointed by "that language in which you need to know the foundations of Mathematics to extract a value from an array" This makes me angry because I love Haskell
Lens and IO are complicated to implement; they're not complicated for basic use. (EDIT: wording)
you definitely don't need to know any foundamentions of mathematics to extract a value from an array :) The array API is pretty clear and straighforward, `x ! 2` would get the item in position 2, etc., and is similar to `x[2]` in other languages.
Honestly, shitposts like this are 99% of the reason that people think Haskell requires a math degree.
oh, wait, now my website is avaialbe also at http://my_website.com:3000, that's not good. I should close the port 3000 via firewall, right? 
I am a Haskell fan and used it for many projects and have a CS degree, but honestly, lenses are just too damn difficult to handle. Haskell alone has already a steep learning curve, and as elegant as lenses might be in theory, I think we can't afford to make lenses part of the standard Haskell experience unless they get a lot more tutorials and become easier to handle.
There's not really a standard Haskell experience. It's a powerful platform that supports multiple programming paradigms. What you said about lenses is equally applicable to Applicatives and Monads. Where do you draw the line? Lenses, like everything else, become easy with practice. I routinely use them and very rarely do they feel difficult (more when doing exotic compositions of optics).
That IMO is syntactic sugar marginally better and sometimes worse, that fill a nostalgia for OOP syntax. It may be worse since it disregard genuine functional programming techniques like pattern matching. Moreover using heavy state may be a code smell for bad functional programming used to emulate OOP techniques or a bad design of database applications or both.
I am aware, but the community settles on a "standard" suite of the most used libraries and idioms. I am not sure where to draw the line, and Applicatives and Monads sure are part of why the learning curve of Haskell is steep. If we add too many abstractions to the "community standard", we stand even less of a chance of going mainstream. Just saying, as someone with a lot of Haskell experience and a CS degree and someone generally proficient in many programming languages, maths, and abstract thinking, but struggling with lenses: if I can't get on board easily, than most other people won't either.
&gt; if I can't get on board easily, than most other people won't either. And you didn't find monads and applicatives even harder? A friend of mine used the exact same argument about Haskell when he was learning it - but now he uses it proficiently and recommends it to others. I think it is a natural artifact of the frustration we almost all feel as we struggle to learn these hard-to-learn concepts. After the learning curve, monads and lenses aren't hard.
But very often it seems so
This is an example of something I write in 1-2 lines with lens, and would be super tedious and error-prone to write without: https://github.com/lamdu/lamdu/blob/20ce54f6555bb0ef585ba4821d3494e9034bb7d6/Lamdu/Sugar/Convert/Hole.hs#L228-L230
As I am not at the point yet where lenses completely clicked for me yet, I can't really compare. I do see that learning Monads and Applicatives is hard as a major problem as well. I am willing to accept some level of difficulty, as I feel like stock Haskell + monoid/applicative/monads is worth the learning curve as those things gives you an immensely powerful language. But for each difficult-to-learn abstraction that we add on top, it becomes that much harder to get into it, which hinders adoption a lot. Don't get me wrong, I am sure lenses are great once you get them. I am specifically talking about the learning curve and how it kills Haskell adoption.
Depends on what service you want to use to send it. I use Amazon's SES to send email and it has a got a nice Haskell library: https://www.stackage.org/haddock/lts-8.13/mime-mail-ses-0.3.2.3/Network-Mail-Mime-SES.html#v:sendMailSES
btw: Did you watch [SPJ's talk](https://skillsmatter.com/skillscasts/4251-lenses-compositional-data-access-and-manipulation) about lenses?
So the keys `image.container.name` and `docker.image` should coincide? It's just that this isn't immediately clear. (~~I also assume image is not a parent of `enable`[^[1]](https://docs.haskellstack.org/en/stable/docker_integration/)~~ comment's been fixed)
I think it doesn't quite work right on Windows - but I am not sure, I don't have Windows to test. We'd really welcome a contribution to make sure it works on Windows, though! We use GLFW-b and bindings to the freetype-gl lib, which should be portable to Windows.
IME, the most difficult part is the error messages involving type-classes like `Getting`. IIRC, there's `Getter`, `Get`, `Getting` and indexed variants of those various relationships between them that are hard to remember - so a type error involving those can be quite difficult to decipher. When using overloaded lenses like `_1`, `_2` the errors are super-verbose ones about missing instances, rather than failed unifications - which are also harder to work with. 
I agree that putting a web application behind a capable reverse proxy is a good design. However in this case, the web application should listen only to localhost and not to leave this decision to a firewall setup. It seems to me that Spock and many other web framework, network service etc. in Haskell world do not make that easy, since they do not expose how to bind to a given interface. Instead they bind to all available interface as a default. To bind to a specific interface, I believe, one should go down in the layers of web/service/network libraries. I think, a *runWhatever host port arg1 ...* format would be better for that would make it easy to cater not only for the reverse proxy case both also for multi-homed hosts. If a runWhatever function does not take host/interface parameter the default should better be localhost. Here is my quick attempt for Spock. I am not so familiar with this so Spock's author (/u/agrafix) could probably show you a better solution: import Web.Spock import Web.Spock.Config import qualified Network.Wai as Wai import qualified Network.Wai.Handler.Warp as Warp import qualified Data.Streaming.Network.Internal as NetI import Text.Printf (printf) runSpock' :: String -&gt; Warp.Port -&gt; IO Wai.Middleware -&gt; IO () runSpock' host port mw = do app &lt;- spockAsApp mw let host' = NetI.Host host warpCfg = Warp.setPort port $ Warp.setHost host' Warp.defaultSettings printf "Spock is running at http://%s:%s/\n" host (show port) Warp.runSettings warpCfg app 
&gt; IIRC, there's `Getter`, `Get`, `Getting` and indexed variants of those various relationships between them that are hard to remember Not to mention that `Getting` often really means `Fold` :) &gt;When using overloaded lenses like `_1`, `_2` the errors are super-verbose ones about missing instances Yup, one needs a bit of a trained eye to see through these -- and synonyms can't hide them. 
This is what we use, too. As an alternative, Brendon Hay's great [amazonka](https://github.com/brendanhay/amazonka) suite provides a more full-featured [SES interface](http://hackage.haskell.org/package/amazonka-ses).
Exactly, It is not monads what is difficult IMHO but the disgrace called monad tranformers. Don't conflate this with monads. They look like eternally non finished pieces of code for beginners.
But surely there is a cooler but contorted way to do it with lenses..
I feel that Lamdu is a "real-world project" -- and it's allowing me to focus on interesting problems and create rich data types without worrying about tediousness of access. 
What is wrong with {-# Language RankNTypes #-} import Data.Functor.Identity import Data.Functor.Const type Direct s t a b = s -&gt; (a, b -&gt; t) type Indirect f s t a b = (a -&gt; f b) -&gt; s -&gt; f t from :: Functor f =&gt; Direct s t a b -&gt; Indirect f s t a b from splice transform s = case splice s of (a, reassemble) -&gt; fmap reassemble (transform a) to :: (forall f . Functor f =&gt; Indirect f s t a b) -&gt; Direct s t a b to indirect s = (a, setTo) where a = getConst (indirect Const s) setTo b = runIdentity (indirect (Identity . const b) s) 
Yes. You should do that anyway - block everything by default, then punch holes as needed.
That's what I'm here for! Give me lens errors and we'll see whether I make them better. I'm not sure what those other 45 comments are doing in this thread.
I can attest to having used mime-mail with success as well.
For context: it's not so much about Haskell in industry as about talking to the cryptocurrency community about technology like FP, Haskell and formal methods that can help with the difficulty of building correct robust systems. The cryptocurrency community are waking up to these issues at the moment given the number of high profile mistakes in recent years. In particular one of our clients works in the cryptocurrency space and is taking seriously the issue of formal methods and assurance.
I was trying not to bias people towards posting code that produces errors helped or not helped by this patch, but gur ernfbavat vf gung tup hacnpxf glcr nyvnfrf vs gung nyybjf vg gb chyy n sbenyy gb gur sebag, fb V znqr jung'f yrsg nsgre lbh chyy bhg gur sbenyyf n glcr nyvnf ntnva. Jurgure gung urycf jvgu gur reebef gur choyvp vf npghnyyl naablrq ol va cenpgvpr vf jung V'z urer gb svaq bhg.
I agree that having practical understanding of monads and monad transformers and how they are used to structure programs, is the first big hurdle from the LYAH level to real world Haskell programming. I would not recommend Yesod instead, everything you need to know to use Spock you also have to know to use Yesod. It is true that Yesod has engineered solutions to a broader set of web programming related problems, but trying to use them without having a basic understanding how they work will result in even more frustration vs. learning Haskell incrementally starting with the basics and IMO a fool's errand. The only advanced bit in Spock are type level lists to encode routes, the rest is very straightforward. To get comfortable with monads and monad transformers, I had good experience with translating shell scripts to Haskell.
I successfully use Mailgun + Hailgun.
Sorry for the cross-post, folks – but I figured that some are likely to read Reddit but not haskell-cafe. I've been neglecting IHaskell for a while now, and since due to various life circumstances it seems like that's unlikely to change, I figured I'd see if anyone is interested in picking up with this project where I've left off :)
Thank you for the talk! Lots of interesting stuff, but then again I'm a Bitcoin+Haskell enthusiast, so this stuff is right up my alley. I'm curious to hear your viewpoints on inflation/deflation/GDP, as was mentioned towards the end. Sounds like you had some good points (I'm also a certified armchair economist).
&gt; mailing lists are just too archaic to me and have really limited history, while the browsing interface is just nuts (do I have to click [Thread], [Next] or [Subject] to get listed the full thread, and why the heck do I have to do anything to view all messages in the first place I understand the complaint, but it's not a problem with mailing lists being archaic, rather web-based mailing list archive interfaces being archaic.
&gt; OOP style Using `.` to access fields does not "OOP style" make.
&gt; I'm curious to hear your viewpoints on inflation/deflation/GDP, as was mentioned towards the end. Sounds like you had some good points (I'm also a certified armchair economist). Hah, I stepped into a hot topic there! My main point there is that macroeconomics is a real field of study and macroeconomists do know things. It's ok to disagree with experts from a point of knowledge, but not from a point of ignorance. My feeling is that too many cryptocurrency enthusiasts disagree with mainstream macroeconomics from a point of ignorance.
Functions are data. Nevertheless I usually also prefer a *deep embedding*. ;)
I'm sure there is, but it's definitely not required by any means. writing obfuscated code is possible in *any* language if you tried hard enough. 
I use this too. It took me a while to get the mailgun account verified (mostly due to my own inexperience with DNS settings), but hailgun has worked flawlessly the whole time.
Reddit is *for* cross-posts. No apology required.
If I have to build **this** program (the fake Linux CLI), how would you recommend me do it? I suppose a single data type `data Command = Ls String | Cat String` and couple of functions (`exec`, `parse`) without needlessly creating type classes should be the way to go. May be I should rather try and invest in using a library to create a good CLI parser for my fake CLI. That should teach me a thing or two to level up a little from n00b. Thing is, I absolutely **have** to build something of my own imagination. Otherwise I will not be able to continue, and then I'll leave it half-way, and feel bad about it for some time, and then come back to it, rinse and repeat. Story of my life. Did I bite more than I can chew? It looked like a small enough program to start with.
Thank you : )
Lens isn't easy to learn, but learning how to use getters/setters via lenses is. 
If you deliberately obfuscate your code, you have code that's hard to read. 
I already use them as getters/setters. &gt;I think we can't afford to make lenses part of the standard Haskell experience unless they get a lot more tutorials This is tangential and not directed at you specifically, but pedagogy is something to pay for! I don't doubt we need more books, but tutorials are never going to make hard things easy.
&gt; I am not sure where to draw the line, and Applicatives and Monads sure are part of why the learning curve of Haskell is steep. If we add too many abstractions to the "community standard", we stand even less of a chance of going mainstream. The more cool stuff is written in Haskell and publicized, the more Haskell will go mainstream. If lenses help you do cool stuff, people will learn. 
Because people keep shitposting without knowing anything about category theory. 
For the functor instance you can simply use the *fmap* for *Tape* twice instance Functor Universe where fmap f = Universe . (fmap . fmap) f . getUniverse For the comonad instance of *Universe,* you can use the helper functions you probably already made for the comonad instance of Stream and Tape, but let me repeat them here. A common way of looking at a comonad is as a container with a focused element. The *duplicate*, or *cobind,* function will not change the values in the container, but will only shift the focused element around. So, let's write two functions that changes the focused element of a *Tape*: left :: Tape a -&gt; Tape a left (Tape (Cons x xs) y zs) = Tape xs x (Cons y zs) right :: Tape a -&gt; Tape a right (Tape xs y (Cons z zs)) = Tape (Cons y xs) z zs And let's write a function that produces a *Stream* with repeated applications of a function *f* to some initial element *x*, i.e. [x, f x, f (f x), ...]. repeated :: (a -&gt; a) -&gt; a -&gt; Stream a repeated f x = Cons x (iterate' f (f x)) The comonad instance then follow from applying these functions at the 'right level'. Note that the result of duplicate wil be a *Universe (Universe a)*, in other words, four nested tapes. These two new outer tapes correspond to changing the focused element, the two inner tapes correspond to the container. For reference, this is an example instance: instance Comonad Universe where extract = extract . extract . getUniverse duplicate = fmap Universe . Universe . shifts . shifts . getUniverse where shifts :: Tape (Tape a) -&gt; Tape (Tape (Tape a)) shifts tape = Tape (repeated (fmap left) (left &lt;$&gt; tape)) tape (repeated (fmap right) (right &lt;$&gt; tape)) 
[This](https://page.mi.fu-berlin.de/scravy/realworldhaskell/materialien/monad-transformers-step-by-step.pdf) is a well-known and well-written monad transformers paper ("the" monad transformers paper?). A web app is a good beginner project, because they usually serve some purpose and so are inherently motivating. Otherwise, maybe re-implement something from /r/coolgithubprojects or /r/commandline. Also, both #haskell and #haskell-beginners on IRC are friendly and helpful.
I don't think there's significant advantage to `f` being a functor. I've found the polykinded-ness of `vinyl` and `union` to be quite useful. `xs` can be a type level list of `(key :: Symbol) =: (value :: k)` and `f` can be a type `k -&gt; *`. Thus, I think you just need `Monoid (f value)` for all values in `xs` (similar to your first option).
That paper looks pretty good. I'll give it a closer read. Thank you. 
I suppose that's my fault really, in more familiar languages all of that stuff is pretty much the same between languages with some small tweaks. I may have come at it with, well, too large a scope. 
It's intentional, it seems, because it describes itself as a pain in the ass.
I wrote a tutorial on writing a small CLI app that does one "feature complete" thing over here: [called `teleport`. Click for link](http://bollu.github.io/teleport) Hope you like it :)
For me the reminder was appropriate.
The problem is that there are many gollum haskellers that want haskell only as a toy for themselves, as a form of self esteem enhancement, for side projects that can program in the evening and show to their co-workers in the next morning. For these people, haskell is valuable as long as it is difficult, so that their boss and his coworkers are ever too busy to learn it, since they do have a real life. That relieves them from being bad Java programmers at work. Of course they want to find the warm of their getters and setters at home.
A brilliantly lucid explanation of functional vs imperative programming and "how we got here" with respect to industry adoption of language paradigms. And despite the constant use of "blockchain" in the hosts' questions, very little to do with cryptocurrency, and everything to do with building correct systems, as you say. This is now my go-to recommendation for friends who ask, "why are you into Haskell?" Thank you for your great work!
I've been working sporadically on this project for some time. In the last year it's stabilised, and is seeing some real use. 
I can try to help. We use jupyter extensively at work, and if I could get IHaskell to run on Windows that'd be a huge door for people to interact with our Haskell codebase. How much technical debt do you think you have, and how hard do you think it'd be to get Windows support? 
HappStack is a Web app framework for Haskell that's not too complicated
I know that you are not a fan of the Lens library and Optics functionality in general. You post keep posting in threads about how unhappy it makes you. I'm one of several researchers who developed the theory behind these modern implementations of Lenses and other Optics, and yes I use math and category theory to help develop it. One of the early things I did was notice that the three laws for getters and setters matched the laws of a co-algebra for a co-monad. From there I reasoned out the analogous laws for the Van Laarhoven representation which generalize to the laws for all optics. The theory of optics is all about characterizing different classes of functors and containers. With the Van Laarhoven representation, these classes of functors are characterized by which other classes of functors distribute through them. With the profunctor representation, these classes of functors are characterized by which profunctors lift through them. In both cases, these characterizations end up defining fundamental operations for these various classes of functors and containers. We can turn these mathematical descriptions in to real libraries and theses behave quite different from other Haskell libraries. I'm pretty excited by the resulting libraries. The other people who do research and development on Optics are excited by the resulting libraries. I remember thinking 7 years ago that this is going to change how people write functional programs, and I think it has. Please understand that creating libraries for Lens, the authors of these libraries are not forcing it upon you, or the community or anyone else. Other people use these Lens libraries because they feel it is useful and helpful for them. Perhaps, unfortunately for you, this also means they use lenses and optics in their APIs for their libraries that they make. I'm sorry that what other people do with their free time, the choices they make with what libraries they use and the APIs they develop, is so problematic for you, but it is their choice to make. However, I don't think you are going to be able to convince others to stop making Lensified API in their libraries just because you "don't get it". Instead of trying to get everyone else to change, you are either going to have to change yourself instead, or drop the matter and move on, because lenses and optics are here to stay. One day they are going to be taught in theoretical computer science courses, and next they will be taught in non-theoretical computer science courses. Eventually even Java programmers will be eschewing for loops for traversals as some already do for mapping today. You might as well get used to it sooner rather than later.
What relation do you think a project like this has with something like Google's `protoc` compiler for protobuf specifications? I don't think `proto-lens` handles custom types, but that's relatively easy to handle with an `Iso`. Your project doesn't do lens accessors at all, I note - not useful for you?
&gt; I don't doubt we need more books, but tutorials are never going to make hard things easy. I read less than a handful of books, but hundreds if not thousands of tutorials. Web-dev was hard. Now I fire up Django and by following along their excellent tutorials I can produce a working complex website (with db, task queuing, auth and much more) very quickly, even if I am knew to either Python or Django. Unfortunately, such a thing is still impossible with Haskell.
At my uni a lot of people write a compiler for their bachelor's thesis. Unfortunately, we were restricted to doing so in C. But haskell is one of the best/nicest languages to write a compiler in, and writing a compiler is insanely fun. It also lets you use some advanced features if you go that route, since you can model a lot of invariants in the type system.
Drifting back to the topic, here is something slightly different than what you asked for: a list of some Stack Overflow questions involving *lens* errors (I picked whatever seemed relevant from [this query](http://stackoverflow.com/search?tab=newest&amp;q=%5bhaskell%5d%20%5blens%5d%20error%20is%3aquestion)). I added a crude summary of the cause of each problem is next to each link. Note that the sample consists primarily of beginner mix-ups, and many of the errors are missing constraint ones ("No instance for Monoid", "No instance for Contravariant", etc) that don't actually mention the raw Van Laarhoven types. * http://stackoverflow.com/q/41005322/2751851 (`toListOf` instead of `traverse`) * http://stackoverflow.com/q/38340097/2751851 (signature for monoidally combining `Fold`s) * http://stackoverflow.com/q/36815529/2751851 (mixing `(^.)` and `(.~)`) * http://stackoverflow.com/q/35816019/2751851 (misinference due to missing signatures) * http://stackoverflow.com/q/34758746/2751851 (mixing `(^.)` and `(.~)`; missing `_Just`) * http://stackoverflow.com/q/33959274/2751851 (using `(^.)` with non-specialised type-changing lens) * http://stackoverflow.com/q/33115595/2751851 (`Lens` instead of `Traversal`, `(^?)` instead of `(.)`) * http://stackoverflow.com/q/29478398/2751851 (trying to use a traversal as a getter) * http://stackoverflow.com/q/29267759/2751851 (trying to use a traversal as a getter) * http://stackoverflow.com/q/27842805/2751851 (passing a getter to `Control.Zipper.within`) * http://stackoverflow.com/q/27623260/2751851 (trying to use a getter as a setter) * http://stackoverflow.com/q/23401372/2751851 (`(^.)` instead of `(^..)`) * http://stackoverflow.com/q/20528718/2751851 (trying to use a getter as a setter) * http://stackoverflow.com/q/17538691/2751851 (`(^.)` instead of `(^?)`) * http://stackoverflow.com/q/17518301/2751851 (`(^.)` instead of `(^?)`) 
Is Haskell often used for embedded systems?
Holy shit man. You're a hero. Keep being how you are.
No, but that's not what I was implying. I'm saying that it is weird that the only interface that bluez provides to BLE is through dbus. So even if you want to write a C program to do BLE, you still have a dbus dependency.
Würzburg?
iiuc, ReadOnlyDb is non-deterministic but non-effectful, whereas IO can both have effects and return different outputs per same input. it's a lighter constraint, but still impure, so why is MonadIO bad? 
Haskell isn't nearly as popular as go. That's a huge part of it. Packages in Haskell tend to provide extremely abstract interfaces, which makes documentation difficult to write. The lack of an IDE is a bogus criticism that has nothing to do with Haskell as a language.
Don't get me wrong, I'm using XMonad, too. I should have worded it better (see also edited post) ... while Haskell can be perfomant to a certain extend, as you said you cannot most of the time get to the performance of languages like C. But in my opinion it is important for an OS to have the best performance possible. But I see that safety is also a big thing, so while I probably would never use this OS as my daily OS, I still see that it could be very useful, when performance is not that important.
Honestly, I felt getting started with Go was pretty clunky, and their documentation was pretty bad, for the couple things I was using. For an IDE, Atom has great support for Haskell, especially with ghc-mod.
for the Editor: go with VisualCode or Atom - I cannot say much about Atom any more (dropped it in favor of Code) but for the first one I would recommend [Haskero](https://gitlab.com/vannnns/haskero/blob/master/client/doc/installation.md) - let us know if you have trouble setting things up I'm sure someone will help you in no time
please pardon me answering (as I was not asked) but I think one thing we could probably try to fix is the lack of examples. IMO Elm really nailed this and it helps out newcommers a lot to see how things are used (it might even help them figure out to read the types correctly)
Go documentation was bad when Go was new, and until almost 2 years ago. But Go only came out in 2009, whereas Haskell has been since 1990. But I get it, Haskell has a different way, implicit one, but little explicit information would not hurt at least for beginners I feel.
I get Haskell is not as popular as Go, but Rust which is arguably even less popular has better IDE support. I understand that lot of Haskellers prefer simple Editors, but for the average SE this is not very convenient, and if they need to jump through hoops and refer to ten webpages just to setup IDE/Editor/Anything that offers features like modern IDE like Eclipse/InteliiJ then they might just give up on this. And to be very honest how do you think it sounds if I try to pitch to my team in office to develop a new microService using Haskell, but then tell them a 50-step list just to setup dev environment, I don't think it is then bogus criticism.
There are many reasons why lambda-calculus is important. 1. Untyped lambda-calculus (ULC), along with Turing machines, combinatorial logic, partial recursive functions and type-0 grammars, is one of the foundational models of computation, so we know that if ULC can be translated into a programming language then that language can express any computation. 2. Compared with partial recursive functions, ULC is syntactic and easily axiomatized, so it's easy to list all the rules. You don't need a background in recursion theory or domain theory to grasp the definition. 3. Compared with the other models, LC is notationally simple. To write down a program, you just need to write out a term; you don't need to define a machine or tape symbols; you don't need a separate disembodied list of definitions; scoping is extremely clear. Compared to combinatorial logic, it's more human-readable. To transform a program or show two programs are "the same", you can use essentially the same methods that you learned in high school to manipulate algebraic expressions. You can execute a program by hand. 4. LC has both equational and rewriting models. An equational model says when two programs give the same result for the same inputs, but ignores the space/time complexity. Rewriting models are similar, except they also note the steps, so you can reason about complexity. In LC, the relationship between these two is usually pretty simple, so it's easy to start thinking about a problem in terms of correctness and then, later, once you've convinced yourself of that, think about rewrites and efficiency. This promotes separation of concerns. 5. It's fairly easy to add types to ULC, and to compare the typed and untyped versions. 6. When you add types in the most obvious way, types correspond to logical propositions and typed terms correspond to proofs of those propositions, so you get an additional way of thinking about programs, and writing total, correct programs becomes an exercise in proving theorems in constructive logic. 7. These types "coordinatize" the space of computations so we can think about it in parts (e.g., sums, products) and not just as a big ball of mud. 8. LC is pretty amenable to extension with features we see in other programming languages, such as I/O, mutation and concurrency. 9. There is a huge body of literature about lambda-calculi, so it's easy to benefit from the work of other people. LC is a lingua franca. It's conventions are well-established; it's concise; conceptually, it's robust enough to accommodate many sorts of extensions. You mentioned unnecessary jargon and complexity. Of course, I don't know specifically what you're referring to (and I haven't read the book you mention), but chances are it's probably not unnecessary. Because LC is concise, treatments of it can afford to give you the whole story. Most programming language definitions sweep a lot of things under the rug and/or punt it to a vague, assumed understanding of a von Neumann architecture. Practically none give you a complete, unambiguous list of ALL the rules which say how two programs are related. Think about the power of this as a tool. In pure ULC, you can prove that two programs do exactly the same thing on all inputs with 100% confidence, and it doesn't involve any testing or assumptions about the implementation or architecture.
Currently I am using SublimeText with SublimeHaskell, but they don't really play nice, and I need to fallback to terminal to build and exec the code. I will check out VSCode, thanks!
Installing stack was not the difficult part, it was configuring it to work with something like SublimeText, which I haven't been able till date. I mean I can build and execute using terminal doesn't mean I like to do that instead of simple keyboard shortcut or even better on document save. I get it about Haskell-way, and maybe with time I may learn to infer and understand by just looking at Type signature, but to be honest for every 10 people who may try Haskell I imagine 9 give up which is overall not really good for the health of the language. Sorry I don't agree with you on `time` package, it is God-awful. I would take Go's `time` package anyday over Haskell one. As a developer what is the point of reinventing the wheel for simple things? Libraries are supposed to make things convenient not the other way round. And Go actually packages time database with the Go itself as a fallback, which is pretty cool. FTFY &gt; centering around the upper end of the developer skill spectrum centering around developers who are familar with Haskell. It is not intelligence if you have to jump through fifty loops, repeating the same thing again and again. Before I come as biased towards Go, I am not a Go developer, and really want to get into Haskell, but I cannot help but compare how it's taking me more LoC and time (debatable) compared to when I just started out with Go. Since right now I am just reimplementing what I already have done in Go.
[removed]
It's Haskell's time API that is not well designed in this case, not really Haskell. Haskell trivially supports an API like this: utcHour :: Time t =&gt; t -&gt; Int And maybe the `time` package should have it.
Computerphile has a video on [lambdas](https://www.youtube.com/watch?v=eis11j_iGMs) and [functional programming](https://www.youtube.com/watch?v=LnX3B9oaKzw). 
Please, I understand my criticism may paint Haskell's `time` package inferior compared to Go, but Julian Calendar? There is no country on this planet that uses it. Sometimes "get shit attitude" really works in favour of Go. Completeness and Correctness for the sake of it, are actually counter-productive.
I never said generalized that Haskell is bad, I was specifically talking about the `time` package. I am so irritated with the stupidity and over-complexity of the `time` package, I just might write a new library for this.
This reminds me of [You could have invited Monads!](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html).
alright, that is reassuring. It was starting to feel like an impermeable IQ barrier to Haskell. I'll likely come back to it after i get some handle on the language itself
Doesn't that require installing all the libraries you need in your project to be installed globally as well for `ghci`? I don't want to install libraries globally.
thanks for the thorough response, I realize that they wouldn't add a section about lambda calculus to nearly every haskell tutorial if it wasn't important. I guess in order to reach your level of appreciation for ULC I've probably got to play with the language. I'll be coming back here to review your breakdown too, so thanks!
I think maybe it doesn't matter if you all you are doing academic projects. But it definitely helps when you are time constrained or code maintainability is important. How does just providing the primitives and making the devs re-invent the wheel each type they need something useful done a design philosophy? And maybe that's okay for the language itself, but why make libraries like that as well, it just violates the [DRY principle](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)
I am also working through the book, and I find the chapter a good exercise on analyzing program structures. It helped me develop an intuition about scope of closure and composition of functions. I actually wonder why it is not taught in colleges, since I find the concept of composition immensely useful in Calculus. But I digress. Composition of functions would then relate to a coding style called "point-free style", which is ironically full of points but has a reason that defeats the irony. So don't worry if you can't get it right there right then, it's pretty difficult. But keep it in mind, and know that you may reap much benefit if you reflect on it time and again and master it. 
which is one really strong indication that said chapter should probably not be the first in the book
How Rust dealt with their errors may be of useful reference: http://www.jonathanturner.org/2016/08/helping-out-with-rust-errors.html Good luck!
&gt; I think maybe it doesn't matter if you all you are doing academic projects. &gt; But it definitely helps when you are time constrained or code maintainability is important. Yeah, but what if you care about your code being correct, instead of having shipped as quickly as possible ?
Every time I see such similar libraries I would wish the authors could work together to come up with an ultimate solution. Sadly I have only seen this happen once. Maybe something like "similar questions" of Quora can be done there?
Hey! Fun question! You seem to be missing an idea. I wrote an article about it just for you. I haven't proof read it yet so it's not published officially but here's a link: https://two-wrongs.com/dynamic-dispatch-in-haskell-how-to-make-code-extendable Edit: Proof read, improved and published!
I'm not "trying" anything. I understand completely that the reasoning process can be clearer in the absense of side effects. I am just pointing out that the benefit of easier reasoning comes with practical complications, such as having to learn lambda calculus (which the OP explicitly asked about!) for a start. Having said this, I'm not sure it's actually easier to reason about functions. Rather, it's just that you can be more confident that your reasoning is correct when you're done. If it really was easier, we would all have thrown away other approaches by now. This is my opinion/observation, not an argument. 
This seems interesting. I will try this! 
Can someone explain to me how type families aren't just a gigantic hack to deal with the lack of dependent types? I feel like we should just get -XDependentHaskell working rather than advancing type families by further complicating the type system, but I don't know enough about this stuff to say this is definitely that's definitely the right course of action.
Hey. Thanks for the inspiring words :) I actually like Haskell, how terse and clear it is when written correctly. Unfortunately I am still not convinced of the design logic of the `time` package and I will write my own library for my project using whatever low level access Haskell provides for time. And if I am successful I might even release it. 
Thanks a lot for going through so much trouble. It's a great write up. I'll try this approach.
I haven't taken the time to read the post carefully but I was wondering about the Writer monad and Log example. I have read a couple of times that the Writer monad was pretty useless in production code and a bad choice for implementing a fast logger (of course it is ok for pedagogical purpose as it is the case here). If this is confirmed, I would suggest to add a warning about that fact (+ some explanations). Cheers
&gt; I wouldn't want to implement the core logic of an application in terms of protoc generated types. Why not? In Java/C++/Python etc you get the loading/saving for free by using protobuf. I see protobuf mostly as &gt; specifying language independent data models, which can potentially be used as core data types in an application's implementation
Just so you understand why you are being downvoted: Your word choice is not very constructive. My coworker and I used Go professionally for 4 years starting back in 2011. I really like the choices they made with their time library, I too think it is well designed and easy to use so I truly do get why you would be frustrated. Oddly enough we were talking about the Haskell time library last week in much the same light as yourself. I explained to him what everyone else is basically explaining to you: It is built for correctness and composibility; everything you need is there, but you do need to do a tad more work to massage it to where you want to go. Until you grasp the philosophy of Haskell programming in general this library will continue to frustrate you, and likely many others. Once the concept of "combining things" sinks in your experience will become much smoother.
The title of your post literally contains "Why is Haskell so inferior?".
"Inferior" is a value judgement and a very polarizing word, so that's not the word I would have picked. However, I do agree with you that Haskell is not newbie friendly. Why? Because Haskell is unlike every other language you've ever used. There is no other language in mainstream use today that has purity. This might not seem like a big deal, but it's huge. In general, I don't think the `time` library is a good way to judge Haskell, which is what it appears you are doing. But since you chose that example it actually does illustrate my point. Some of the things that you think are difficult about the time package are there because of purity. This isn't obvious to newcomers because they're not used to taking purity into account when thinking about API design. There are some things about `time` that could be improved. But dealing with dates and times is inherently a complex issue with lots of corner cases. Bottom line: you can't expect Haskell to come easily. It's going to take work to learn because it's so different from anything else you've used. But most of us are here because we strongly believe that the effort will pay off in the long run. So keep at it. Don't get discouraged. Just keep your expectations realistic.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [timbod7/adl/.../**types.adl** (master → 717c7ba)](https://github.com/timbod7/adl/blob/717c7ba871005e658291310d259c1aa9b44c16a9/adl/stdlib/sys/types.adl) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dha48u6.)^.
You shouldn't. I did this semi-recently: https://atcol.wordpress.com/2016/06/03/sublimetext-3-haskell-in-9-steps/ "recently" xD
Why is every version other than 6.0 deprecated? I haven't seen a package like that before. 
&gt;Sorry I don't agree with you on time package, it is God-awful. I would take Go's time package anyday over Haskell one. As a developer what is the point of reinventing the wheel for simple things? Libraries are supposed to make things convenient not the other way round. But that's just the point - these things *aren't* simple things, where one-size-fits-all.
Thanks for the reference. I'll study adtgen in more detail. And it's good to hear of your positive experience with this kind of tool. &gt; I'd highly prefer one with syntax and semantics as close to Haskell as possible ADL is used by developers unfamiliar with haskell, and hence I chose a syntax less foreign to those users. But ultimately the inspiration for this was the the ADTs of haskell, so I would expect the semantics are close enough for you. 
You could say haskell 5 years ago is not the same haskell than today. Hell, lens 0.1 upload to hackage is from 2012. Going back in time a little more, we find Text in 2009. Mtl in 2006. And this is just libraries. In [ghc extensions manual](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#syntactic-extensions) , you'll find a ton of cool stuff added since 7.0 (released in 2010). One of haskell problem is discoverability: haskell ecosystem is fast changing and old tutorials and books stick around. We are still hearing 'cabal hell' from time to time in /r/haskell even though stack and cabal mitigated that problem a long time a go. 
Well, you don't exactly need to know or understand lambda calculus to program in Haskell. It is just a general computer science subject that is good to know. For example a common thing is to show how you can implement tuples with nothing but lambdas. But you should really never do that. Just use the builtin tuples.
&gt;Julian Calendar? There is no country on this planet that uses it. Astronomy uses Julian dates heavily. 
Haskell doesn't have enough manpower to support a multitude of editors, so the experience is hit and miss, but in recent years it got better (Intero was my real step closer to an IDE experience, and i've heard around here good things about Haskero). [This chart can get be usefull to you](https://github.com/rainbyte/haskell-ide-chart) PS: Compiling in a terminal should not be a show stopper. Most of my time learning haskell was with notepad++. Some IDE functionality, like code completion, would be nice but they aren't as much of a must have like in C# or java.
Perhaps take a look at [thyme](https://hackage.haskell.org/package/thyme), which has a lens interface. It's a bit more advanced, so it might not suit you, but it's super convenient (at least IMO).
&gt; I realize that they wouldn't add a section about lambda calculus to nearly every haskell tutorial if it wasn't important. I think the importance for practical applications is overstated. LC is wonderful for thinking about computation! Lambda calculus is great if you wonder how you can express computation systems formally and it's simplicity give it a certain beauty in that regard. From a learning perspective LC is the simplest possible language to learn about functions as arguments/partial application/recursion/purity. If you understand these already LC is trivially to learn, if not it's the simplest complete example you can arrive at. So it's a great way to explain all of these concepts in one go. In terms of practical application you can think of Haskell as LC with a lot of fluff on top of it so at least the concepts will still apply even though Lambdas in practice are just anonymous functions.
Nix can cache per-file, too. 
I would say, think of lambda calculus, not as some abstruse mathematics but as a way of thinking. That way of thinking makes it easier to compartmentalize problems. A lot of the programming that we do in Haskell is really just reducing the expressions ourselves or abstracting them out based on the rules of lambda calculus. In other word we are really just removing the lambdas by substitution or wrapping things in lambdas (and creating variables) to get more abstraction in order to express different concepts in the code. I would suggest you might not understand what I mean by this unless you spend some time studying and actually doing some complex reductions yourself on paper. For example, I didn't understand monads until I desugared them and did the actual reduction of them myself on large pieces of tracing paper. Also since Haskell code has a lot of sugar that might obfuscate the core calculus, thus I'd suggest reading this http://www.inf.fu-berlin.de/lehre/WS03/alpi/lambda.pdf
I'm just speculating, but perhaps there will be tradeoffs between type families and full dependent types. For cases when you just want compile-time computation, type families might come to be seen as a conservative solution. Doesn't Servant for example depend on open type families for extensibility? 
A hundred or so years ago, some very very clever mathematicians came up with a way to reduce numbers to sets. In this scheme, zero is equivalent to the empty set, one is equivalent to a set containing nothing but zero, two is equivalent to a set containing zero and one, and so forth. Then, fifty or so years ago, educators decided that American math curricula needed reform. Noticing how set theory was (according to this school of mathematics) a foundation for arithmetic, they decided that the primary-school math curriculum ought to include set theory. This aspect of “the New Math” has been quietly dropped, because teachers noticed that even if set theory is necessary to put mathematics on rigorous logical foundation, teaching children set theory does not do much to help them add, subtract, multiply, and divide. When authors of functional-programming textbooks put “let me show you how to represent numbers and Booleans in pure lambda calculus” in their opening chapters, they are following in the footsteps of teachers who taught set theory in first grade. From the standpoint of pure CS theory, yes, everything begins with the lambda calculus. (Or everything begins with S and K combinators.) If you’re the kind of person who thinks these kinds of logical manipulations are cool, you will like this kind of presentation. However, from the point of view of “how do I make a Web site that doesn’t crash”, I don’t think this kind of presentation is helpful.
Plain old functions are mappings from some values to another values. Type families are mappings from types to types. Dependently typed functions can have both. Basically, good old functions and type families are just specific cases of _dependently typed functions_. But functions with dependent types have some problems: bad type inference and sometimes even worse compiler error messages. I can't imagine how things will be when `-XDependentTypes` lands. I tried to read paper but not carefully. Though knowing reputation of compiler error messages in Haskell I'm not optimistic... It's not enough to have `-XTypeFamilies` to emulate dependent types. You also need `-XDataKinds`. And `-XTypeOperators` to write code in more fancier manner. And I think that type families won't die. Restricting dependent functions to only mappings from types to types (if it's enough for you) can help with having better error messages and better and faster type inference. Especially taking into account that there're a lot of efforts already put in type families, like injective type families.
After checking in with the folks in the IRC channel, some of them confirmed that it is fast enough for a general purpose but may cause too much overhead in high performant computations. In case it is useful for anyone, they recommended [fast-logger](https://hackage.haskell.org/package/fast-logger) as an alternative if it is necessary. I will try to add it as soon as possible to the explanation. 
The average SE? Editor vs. IDE is an ancient holy war akin to Vim vs Emacs. But that point aside - It's a two step list. Step one: Install stack Step two: Install spacemacs If that doesn't work for you, there are some fairly painless alternatives for other editors. If the idea of installing an editor plugin is just too much for your team to deal with, I question their professional aptitude. Are they software engineers, or do they just cast chicken bones in front of eclipse until the gods have been pleased? 
I personally skipped that chapter initially as I also found it hard to understand. When I was ~half way through the book I went back to read it and it made a lot more sense the second time. The thing to take away from it though (the 'big picture' so to speak) is that that chapter is used to teach you a bunch of new vocabulary and concepts that are actually super simple, they are just wrapped up in a bunch of words you probably haven't heard before. Those words should be definable from the surrounding context though.
&gt; Sorry I don't agree with you on time package, it is God-awful. No. It's correct, and time is [complex](http://infiniteundo.com/post/25326999628/falsehoods-programmers-believe-about-time), so the complexity in the time library is essential complexity.
The lambda calculus is one of several models of computing; the others I can think of offhand are Turing machines and, I think, Markov algorithms. In a way, Turing machines won out in the real world; today's computer hardware and instruction sets are built around the notion of state and places where values can be stored, overwriting what was there. Haskell and other functional programming languages (e.g. Lisp) are built around the lambda calculus. You're right that to get started on Haskell, all you need to know is that \&lt;identifier list&gt; -&gt; &lt;expression&gt; is "the function that, if you pass it an expression for each of the identifiers in &lt;identifier list&gt;, will give back the result of evaluating &lt;expression&gt; replacing the identifiers with the corresponding expressions you passed"... but the more you know about lambda calculus, the more tools you have in your toolbox that you can use, and the more you'll know about how Haskell works.
My program (written in C) ended up parsing gatttool output because dbus is not available on the target I'm building for. It was a terrible kludge. Using the dbus API is definitely the write choice for a Haskell library.
To be honest, you can likely just skip that chapter. It will not meaningfully enhance your understanding of Haskell until much later. I have a friend learning Haskell who read part of that book and told me the lambda calculus chapter hurt more than it helped.
&gt; I never heard any serious programmer ever to say that setters, getters and container management is a real concern for them whatsoever. As a serious programmer, the first layer of get/set is never a problem. But, nesting them generates a surprising amount of almost-same code that starts obscuring things after a while. It's still not my most pressing issue, though. Dropped messages and auto-restarting infrastructure services are my bane for now.
I understand and sympathize, even empathize. I recently complemented my Emacs + Scala setup at work with the community edition of IntelliJ + the Scala plugin and was instantly more productive. IDEs bring huge advantages especially for exploring codebases and dependencies. Being able to just click around is definitely a huge time saver. I think many here recognize the value of an IDE but unfortunately it is a *huge* amount of work even to produce something minimally viable and the maintenance burden over time is vast. And on top of this GHC is changing faster than any IDE developer(s) can keep up. Languages like Rust and Go are meant to converge on a 1.0 and remain largely frozen afterwards so with funding and commitment it's quite possible to produce a good IDE. GHC, so far, hasn't made that a priority. Features (eg. linear types) are still being added at the cost of backwards compatibility. Not to discourage you from Haskell but given your IDE requirements and the fact that it will likely not be addressed here, you might enjoy the OCaml ecosystem much more. The build system is quite good, Windows support is a priority and people in the community are working on IDEs so if there isn't something out-of-the-box you can at least pitch in. 
For what it's worth, my college definitely went through set theory, logic, some lambda calculus, regular languages, Turing machines, etc. Definitely could've had more lambda calculus and it would've been cool to have had it outside of a CS math course, but I do remember it being introduced at least. 
I've also built something similar for Haskell/Elm/Purescript: https://github.com/typed-wire/typed-wire
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [jonascarpay/convoluted/.../**Static** (master → 3272787)](https://github.com/jonascarpay/convoluted/tree/32727873437df6398f151aa7d3dbe9293adbcf48/src/Static) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dhaio5i.)^.
You'd probably like this paper by Jeremy Gibbons, ["APLicative Programming with Naperian Functors"](http://www.cs.ox.ac.uk/people/jeremy.gibbons/publications/aplicative.pdf). `Hyper` is fairly close looking to your `Tensor` type. I took the liberty of typing up most of the code in this paper [here](https://gist.github.com/thoughtpolice/c408773f43972e48f93377ce3e4f441f) just a few days ago, though the symbolic transformations I never got around to. You should in theory be able to fit this onto Accelerate or something, though.
I've actually attempted doing this (type-safe generic tensor libraries with automatic differentiation) for **two** libraries, actually, heh. The first was [tensor-ops](https://github.com/mstksg/tensor-ops), and the second was [backprop-learn](https://github.com/mstksg/backprop-learn), which I'm still developing. To add some clarity, here would be some of the sensible type signatures for what you gave: two :: Tensor t =&gt; t '[] two = konst 2 mulBy2 :: Tensor t =&gt; t s -&gt; t s mulBy2 x = two `mul` x The original examples you gave don't kind check. The first one it looks like you're using the first parameter as a list of indices (so a list of nats), but in the second example it looks like you're using the first parameter as the *kind* of the indices, so it's not clear what the examples are used for. For reference, here is the implementation of a generic tensor typeclass: https://github.com/mstksg/backprop-learn/blob/master/src/Numeric/Tensor.hs#L84-L158 and here are implementations for: 1. [HMatrix wrapper](https://github.com/mstksg/backprop-learn/blob/master/src/Numeric/BLAS/HMatrix.hs) 2. [Nested vector wrapper](https://github.com/mstksg/backprop-learn/blob/master/src/Numeric/BLAS/NVector.hs) 3. [Flattened vector wrapper](https://github.com/mstksg/backprop-learn/blob/master/src/Numeric/BLAS/FVector.hs)
We can achieve 1) with type class methods of the form `Proxy a -&gt; Type`. 2) is definitely unique to type families, as it would violate parametricity in dependent types. I'm not convinced we need this though.
Benefits of this over a DSL like [HSpec](http://hspec.github.io/)? 
Why would you want to discourage use of older versions? What if the latest has a bug? I feel that unless your a version has a security vulnerability or is broken in some horrible way it shouldn't be deprecated.
With vscode you can try this plugin https://marketplace.visualstudio.com/items?itemName=Vans.haskero : its documentation and usage is dedicated to newbies :)
it's a valid criticism that has everything to do with GHC as a compiler. if the GHC API were cleaner and more featureful, it would be easier for single authors to support different editors. 
Rust has institutional support (like funding) from the Mozilla. 
It looks like someone is trying to give you some information they think might be useful to you, and you have decided to respond with open hostility. I know which behavior I'd like to see less of...
I'm confused as to how you would do type level programming without something as fundamental as pattern matching, perhaps I am missing something?
I have the same feelings about you on this but I can't square them with the observed fact that Haskell Book is a very popular and successful way to learn Haskell.
Hey mods ([randomly](https://www.random.org/lists/) pinging /u/camccann &amp; /u/godofpumpkins)! Idea: let's add this discord link to sidebar. It's a win-win: - no more ~~spam~~ discord channels ANNs - those who prefer Discord to canonical channels will go there and… be happy
&gt; For example a common thing is to show how you can implement tuples with nothing but lambdas. But you should really never do that. Unless you're a crazy hacker working on their own ULC-inspired language implementation for fun and education...
I don't think that what you're describing requires anything that compact regions offer. If this could be done on the compact heap, then it could be done on the normal GCed heap as well. Regardless, it's not really possible to do this. Let's take the example you bring up, the array-as-a-compact-list idea, and see what problems it runs into. You describe it as a "Compact-optimized layout", but what operation is it really optimized for. The operation you probably have in mind is a fold. Any kind of fold is better with an array than with a list. You don't have to follow pointers to get to the next element. There are other operations that become better as well: take, drop, uncons, split; all of these can use refer to slices of the old array. This is precisely how the `vector` library works. Notably, `take` and `split` are asymptotically better than there list equivalents since they are `O(1)` instead of `O(n)`. However, not everything is better. What about `cons`? What about things like this: let a = [1,4,8] b = 55 : a c = 44 : 32 : a d = 12 : a ... Trying to do this kind of thing with an array would be asymptotically worse. So the question is "What are you optimizing for?". Generally, when there are two data structures that can represent the same thing, one is not completely better than the other at all operations. You have to know which operations you're interested in performing in order to decide which one to use.
 A A A A A / A / A A A A A A A A A A A A A A A A A A / A / A A A A A 
I think it's very easy to square them since lots of successful things have faults in them in them or experiments that didn't quite succeed.
https://www.reddit.com/r/haskell/comments/69wcm3/haskell_programming_from_first_principles_why_do/dhav3a9/ We didn't initially. People were confused because they didn't understand the evaluation model. Added the chapter, problem went away.
We say this all over the introduction and off and on throughout the book. Where should we add this? 
That's a different RealWorld than I usually read about on here.
Servant is a _very_ thin framework, though. It really only exists to reify API constraints in Haskell's type system, anything beyond that is meant to be addressed by plugging in different libraries/sibling frameworks. I can imagine three or four different DB drivers and two different HTTP serving-frameworks that could idiomatically be used here.
Honestly, I have no idea. I got the message loud and clear about being able to skim and go back. It seems to be a fairly constant thing I've noticed among a lot of beginners, though. The bigger issue that I personally had/have is that I often have trouble making the jump from "I know how to do these things" to "I know how to think in Haskell and setup problems in Haskell" (currently am stuck on the telephone exercise from chapter 11, for reference)
Something other than SQLite/others-simple for persistence. A little safer. May I suggest Opaleye too?
sure, but as a point of comparison, if you go to the root project to view implementations, would you expect to see the full cartesian-product of all BackendxPeristsence combinations for node-javascript backends? if i recall there is one highligted one, even though are also many libs on npm for persistence, but i assume they agreed on one that is an ideal fit for this project in particular and would also be ideal for readers to understand.
&gt; Why is the community so small if it's such a loved language and has been around for a few decades? Haskell has had a very odd growth over the years. It never exploded exactly, it just grew slowly, and now is growing more dramatically. It's also small because, until recently, the tooling was unreliable. Stack changed a lot in terms of commercial adoption. 
Type signatures are one form of documentation; tutorials and examples are another. Missing examples is bad, but it's not a death sentence (depends on the library though). 
Eh, I don't like IDEs because they don't work with my workflow. Unless you're doing small projects or using a REPL, you're going to need a custom build setup anyhow. Besides, Haskell's tooling isn't as bad as people say, especially with stack. Property-driven testing is something only Scala has. 
&gt; I do prefer to focus my time on writing actual code rather than spend it configuring my favorite editor. That's not really how either vim or emacs work. 
IDEs have a lot of features that really just aren't there yet. Like semantic autocompletion. 
`ReaderM Connection` should do Edit: ... If `:&lt;|&gt;` is somewhat like `Alternative` and we had indexed variants.
&gt; each of which have 200-350 different custom commands and various outputs. So it seems each parser would have different outputs. Are the possible types of the parsed fields known in advance; could they be just put in a sum type? Does the program perform further processing that would require more type safety?
[removed]
[removed]
&gt; Are the possible types of the parsed fields known in advance; could they be just put in a sum type? There are cases where the manufacturer of a given device changes things in a software update. For example, we just had a case last month where 3 commands got consolidated into 1 and broke the existing proprietary software we use. There's no guarantee all devices on the network will be running the same version of the manufacturer's software at any given point in time either, so we'd have to consider supporting the different input/outputs for each possible software version. &gt; Does the program perform further processing that would require more type safety? I believe if template variables can map to an explicit data type and that is checked upon initialization then there is no more need for any additional type safety checks. The consumed data would simply help dictate what step to take next given the current vs. desired state.
While not a bad idea, we want to avoid handing ISP Engineers Haskell parser library documentation as a guide to configuring any of this.
BDD with Cucumber (and Gherkin) isn't meant to replace unit testing, it complements it...
Ok, so there's sort of a layer of indirection between writing the tests and writing the implementation of the tests that helps grease the skids. That makes sense. Especially in larger organizations/projects, I imagine.
Practical benefits: * having stakeholders , devs and designers collaborating on the definition of "what correct looks like" is very beneficial to successful solutions and solidifying understanding * the debate/discussion around features help reaffirm domain concepts and weed out inconsistencies and issues with definintion before coding begins * newcomers to teams can easily access requirements and see their paths through the code at various levels * if you're using the correct tools, tests can be extremely terse and one liners, especially when you've a mature codebase When it comes to time: it's a trade off, if you're doing it right. Instead of spending time writing code and getting it wrong, you can discuss &amp; collaborate with others before writing code, and having a common starting point for test/dev/business folk.
Thanks. That helps a lot. 
Documentation is mostly in the [tutorial](http://haskell-servant.readthedocs.io/en/stable/tutorial/Server.html#using-another-monad-for-your-handlers)
uh - we have a Facebook page too and we love likes and shares: [mygonimo](https://facebook.com/mygonimo)! Thank you! This would really help - it is pretty hard with no money to reach potential users!
You're assuming that the types of exported entities is all that is needed to compile a depending module. That is certainly not true for ghc. You could conceivably make such a mode, but it would produce bad code. 
You're welcome. Unfortunately the options for Cucumber &amp; Haskell are non-existent, although some initial work has been done on thinking around how it might look. I am tempted to see if I can start something but feel it's a little above my Haskell skills!
Usually, it is better to ask for something specific, instead of explaining everything line by line. That's actually a countdown program, it starts at 5, wrote "Counter is x" until it reachs 0, write "BEEP" and stop. The countdown is done in a separate thread and the synchronisation between the main thread and the countdown thread is done using a mailbox. Comments are mine, I removed the only one commented line (`--return ()`) which was useless (because code commented are not run ;) Some stuff are rough approximations... -- module declaration module Main where -- This bring into scope some functions from the Control.Concurrent module import Control.Concurrent -- The main function, first its type declaration, it's an `IO ()`, meaning -- "roughly" that it does side effect and that it returns unit. A unit is a type with only one value `()`. We use it when we don't care about any value because in Haskell it is not possible to "don't have value". main :: IO () -- Don't bother with the `do`: for now, that's just a way of opening a multi line block main = do -- this create an empty box called `done` done &lt;- newEmptyMVar -- this forks a new thread of execution by calling the `thread` function with the argument `done` and `5`. The `$` is just syntax sugar for `forkIO (thread done 5)`. forkIO $ thread done 5 -- read the value in the box `done` -- actually, this will block the program execution until the box is filled by another thread (well, perhaps the one we just forked ?) -- Here we don't really care about the value inside the MVar, it is only used as a synchronisation primitive. takeMVar done -- The thread function, as we saw earlier, it takes a box (i.e. a `MVar`), an `Int`. -- It does side effect (hence the `IO` and returns nothing (i.e. the `()`) -- Here the `MVar` type is qualified with `()`, meaning that it is a box which can be empty or contain an `()`. Please note that there is a difference between an empty box and a box filled with `()`) thread :: MVar () -&gt; Int -&gt; IO () -- This function count from 5 (as written in the `main` function) to 0. At each step, it calls the `tick` function and calls itself again with the next value. When the value reaches 0, it calls the `beep` function and put something (`()`) inside the MVar (this will unblock the main thread). thread done n = do if n == 0 then do beep putMVar done () else do tick n thread done (n-1) -- The tick function just write a line of log tick :: Int -&gt; IO () -- here, `++` is string concatenation, and the `$` is syntax for `putStrLn ("Count is " ++ show n)`. `show` converts an int to `String`. tick n = putStrLn $ "Counter is " ++ show n -- write a line of log beep :: IO () beep = putStrLn "BEEP!!"
I think the easiest way might be a parsec parser which parses the template into the actual parsec parser at runtime. templateParser :: TemplateParser (StreamParser Weather) Might be sensible to tokenize first.
Awesome, congrats for the release! There's a lot to learn from your work. Also, thank you for distributing gonimo as Free Software.
Thank you kindly.
:-) you are welcome, glad you like it!
&gt; I see your point, but I think this, to some extent, conflates two different things. We have no idea which share of these supposed "90%" give up due to completely inessential reasons (arbitrary example: having a bad first experience due to chancing upon a poorly documented corner of Hackage), and not due to any deep-lying incompatibility with the community values. We don't know *why* exactly they give up, but we do know that they *do* give up. And we also know that they are not giving up due to the Haskell community rejecting them, because frankly, the Haskell community has some of the most helpful people I've ever met, and generally speaking, if you ask questions, you'll be met with overwhelming helpfulness, even though you may not always like the answers. So what does this mean? I don't know for sure, but I believe that many of those who quit are looking for instant gratification, and Haskell is not going to give it to you, because it would require making sacrifices elsewhere - a prime example for this is PHP, which has an extremely low barrier to entry, at the expense of being problematic to work with for large scale production-quality projects. More concretely; let's say our fresh new Haskell learner does indeed run into a badly documented library on Hackage. It happens; a lot, actually, and it's kind of a necessary evil. So, our new Haskeller can react to this in many ways. One option is to flip a table and call it quits; that's fine of course, not very productive, but someone who gives up at such a minor inconvenience is unlikely to make it through the initial learning curve to the point where things start to make sense and the productivity boost kicks in. Other options include: - See if there are other alternatives. Underdocumented could mean that it's not production ready. - Figure out what knowledge you're lacking. A coproduct library won't make much sense if you don't know what a coproduct is, and adding that to the documentation is not a great solution; but if you do know coproducts, then the names and type signatures in the library should be all you need in terms of documentation. You'll also notice that many libraries, despite being scarcely documented, do link to a paper describing the underlying concepts; it's often a bit mathy-sciency, but the explanations are usually there. - Ask for help. I believe that those who quit early are those who want quick results (the "table flippers"), while those who stick with it are those who go through the other options. &gt; I must say I don't like this contraposition very much. It makes it sound like "get[ting] confused and frustrated and complain[ing] on Reddit" is a problem, or a sign that you don't belong to the "10%", as opposed to a fairly natural reaction to finding yourself in an unfamiliar environment. There's an important difference between complaining and asking for help. Asking for help is fine; complaining means you might not have the required patience, and a language that gives you quicker results might be a better fit for you.
Absolutely, yes, but that's only half the story as you then need a way of finding &amp; executing test steps on a statement-by-statemeny basis. For example, if I have this feature: Given a valid comment And a valid user account When I save to the database Then I should see a new row in the "comments" table each line has to execute some code. How ?
ghc is not very efficient at doing lots of compilation in parallel and chances are with 40-core VM you will be spending most of the time doing GC. You can get some speed ups depending on modules interdependencies by compiling them one at a time with ghc -c. In my project it showed 10% improvement (~550 modules).
As far as I can tell, this comes from [Parallel and Concurrent Programming in Haskell](http://chimera.labs.oreilly.com/books/1230000000929/ch07.html#sec_reminders), page 126. Very odd and random of you to ask about this, when the book the program comes from clearly explains it's purpose. But if you look at the order of instructions in the do block in the definition of loop, you can see that it asks for input, checks if it should exit, and, if that check fails, it sets a reminder and calls itself recursively. For future reference, you can paste code yourself by indenting with four or more spaces, like this; import Control.Concurrent import Text.Printf main :: IO () main = loop where loop = do s &lt;- getLine if s == "exit" then return () else do forkIO $ setReminder s loop setReminder :: String -&gt; IO () setReminder s = do let t = read s :: Int printf "Reminder set for %d s.\n" t threadDelay (10^6 * t) printf "%d s is up.\n" t and the code won't work unless properly indented.
I agree, however it might needs some clarifying (for developper) on how such complements can't be already written with unit testing tools ...
I didn't think you could find this program on Google. I thought this was a program my lecturer had made.
For what it's worth, it seems like you're jumping into the deep end without knowing the basics. You should have at least a few weeks of experience with Haskell, or at least a similar language, before looking into specific applications, like concurrency. I don't know what class you're taking, but I'd consider looking for a more basic one if I found myself asking the questions you are.
congrats! :-)
Thanks.
Nice! :-) With my first daughter, I ended up just running shoutcast on a raspberry pi, connecting to it with avplay. Theoretically, you can use a web browser to listen to the shoutcast stream, but browsers don't restart the stream on a connection drop, and there was more of a delay than with avplay (so I had my avplay script reconnect if it stayed silent for too long, or reboot the pi if it could connect but got no audio, and stop the laptop from going to sleep while listening). So many hacks :) gonimo seems quite polished, but can you e.g. stop the device from going to sleep while it's recording or listening?
It's realistic: I've done it. It's a selling point if you're a responsible software engineer who cares about correct solutions and accept your own limitations. Sounds like you've had bad luck with your colleagues in those 20 years. The vast majority of the technical discussions I've had have been mature and meritocratic.
Can you talk some more about the telnet lib? Is it parsing all the IACs, doing option negotiation etc? Do you plan to release it?
MVar is something from Haskell that you can find with a hoogle lookup. It can be used for threads to pass values around. As for Monitor, I'm not so sure as I can't find it on hoogle. Maybe you mean a Java monitor.
I agree that the duplication looks ugly, but I do wonder if introducing transformer stacks or whatever else to eliminate all possible duplication is Haskell's version of [ActiveSupport](https://github.com/rails/rails/blob/master/activesupport/lib/active_support/core_ext/object/with_options.rb0). For those not fluent, this is part of Rails that monkey-patches the default Object class with a method that allows you to call several methods passing the same options. It reduces duplication, but at the cost of high levels of WTF. Obviously the approaches you'd take in Haskell don't involve monkey patching classes at runtime (sigh of relief). But thinking of the complexity to a reader of the code, I am sometimes sceptical of the benefits. Duplication might look ugly, but it's very clear. Of course, it's probably a moot point in this case. Doubtless in a real application you'd have enough context to pass down to each request that you'd have an app-specific monad (which might be a transformer stack) which is well-documented and ubiquitous. I was just reminded of ActiveSupport by the use case here, and wanted to vent my reservations!
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [rails/rails/.../**with_options.rb0** (master → 943efa3)](https://github.com/rails/rails/blob/943efa30f5475f51fe90df5f85703d191dbd7d6e/activesupport/lib/active_support/core_ext/object/with_options.rb0) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dhbpygm.)^.
Paging /u/winterkoninkje.
Because your import your steps module at the beginning of your main. (the main I gave as an example is actually a custom main written by the user. It should really be 3 lines. You'll have to do a custom main for Hspec as well).
Syntax sugar for monads &gt;&gt;= and &gt;&gt;
foldl (\acc x -&gt; x conn :&lt; acc) [loginHandler, registerHandler...] ?
Hope it helps! 
`onscreen` does not work for me. I forked the repository with a temporary hack to make it work, but the dev is aware of the problem. Writing to a file works fine out of the box thou.
A few year ago, we assigned our CS242 students a lab to implement monitors using MVars in Haskell. The lab skeleton is here: https://github.com/securitylab/cs242-fall14-concurrency/blob/master/Monitor.hs You might find it helpful to figure out the differences.
I wrote a Gist for using `enter` because I always forget exactly how to do it: https://gist.github.com/tfausak/f5eca37cb937ca95e3f6281af1a7c183
&gt; This aspect of “the New Math” has been quietly dropped, because teachers noticed that even if set theory is necessary to put mathematics on rigorous logical foundation, teaching children set theory does not do much to help them add, subtract, multiply, and divide. Maybe it helps, maybe it doesn't. All we have to go by are correlations. So what happened to the arithmetic test scores after the set theory got introduced, and what happened after it was removed? 
LiquidHaskell translates measures into refined types for data constructors. For example, your `emp` measure will refine the list constructors like so [] :: {v:[a] | emp v = true} -- logical bool, not Haskell Bool (:) :: x:a -&gt; xs:[a] -&gt; {v:[a] | emp v = false} -- ditto and for the `len` measure [] :: {v:[a] | len v = 0} (:) :: x:a -&gt; xs:[a] -&gt; {v:[a] | len v = 1 + len xs} (If you have multiple measures, LH will just conjoin the refinements) Now, whenever it sees one of the list constructors it learns something about the constructed list. This works both when you **construct** a list and when you **match** on a list. So, LH is effectively unrolling the loop, but it's using the structure of your program to do so. There are many more examples and in-depth explanations in chapters 6-8 of the [LiquidHaskell Book](http://ucsd-progsys.github.io/liquidhaskell-tutorial).
Okay. If you are getting pent up about that. You might need to spend your time doing something else. I better let this go now otherwise I'll get the banhammer or some shit. There is no rule that prevents me from posting and asking for help. It might not be what **you** think is a good question, but you know, even if I did ask a good question (which is entirely opinionated) someone would have said the classic "Google it."
I'm not sure. It probably is a Java monitor coming to think of it as my lecturer did make these comparisons in his lectures. That was the context I was missing I think, thanks.
&gt; In object-oriented theory, applications tend to be extended "downward from the bottom". [...] The users of your code are expected to modify and add classes with specific interfaces on a lower level, and then your upper-level main classes automatically glue everything together through dynamic dispatch. In functional programming, we often extend applications "at the top". Your application is written as a library that consists of a set of "lower-level" modules, and then the user creates the main functions that glue your lower-level functions together. [...] With the functional approach, think of the user more as a curator of components, and less as an implementer of contracts. **Very** well put. Nice article.
I'm sure you have more answers than what you were looking for. But if you are wanting for more inspiration. Some of Ada's OS implementations may be of some assistance. * [MaRTEOS](http://marte.unican.es/) * [RTEMS](http://www.rtems.com/) * [tamp (uKernel) ](https://github.com/Lucretia/tamp) * [Army Secure Operating System \(pdf\)](http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA340370&amp;Location=U2&amp;doc=GetTRDoc.pdf)
Yeah that's it. I thought there was also a monitor in Haskell but apparently not. Thank you!
Wow. No one here is adverse to answering questions. In fact, in my opinion, the haskell community is one of the best ones at doing just that. Look at some of the other threads where people are asking questions. There are some with dozens and dozens of answers, like people are almost falling over each other trying to help. Being spoonfed doesn't help you. It leaves you with a sense of understanding which doesn't go deeper than the explanation you were given for that particular example, and breaks down completely when you are forced to actually think and come up with answers of your own. So not only are you being an entitled brat, but also working counter to your own goals: understanding haskell. I do understand how it is to be a student, because I am student, and so are many of the others that often answer questions here. I know the workload and I know how it is to feel under pressure, but I also know that I won't pass my exams unless I actually understand the subjects on my own. Running to ask someone else every time you hit a speed bump is not the way to learn anything. As for your claim that most everyone else is fine with this kind of question spam: I think the downvotes on your posts say otherwise. 
See the thing is you don't understand my situation. Not only have I had to learn Haskell, I've also had to learn Java concurrency, a book in itself that teaches Concurrent concepts and a modelling language (which is 300 pages long), plus the Haskell stuff for the exam. In fact, I don't even need to know the Haskell stuff to pass. But since I do work my ass off -- I want to know everything I can before the exam. To me this isn't being entitled, it's being a hardworker. Sometimes you need a boost to be pushed in the right direction. Essentially this is what I was implying. Y'know I'm basically spinning plates here and I don't have as good as a memory as some people in CS. Nevertheless. The book is 280 pages long (Simon's book). I can't cover all of that with the amount of other stuff that's going on. I did get help from my instructor today and he said only to cover the parts that he said. This is what I was trying to figure out -- I can't read everything in the book, I simply do not have the time. I get massive pieces of coursework on top of exams and lab classes (all in a short timeframe of 3 months for FOUR different aspects of CS). I have other Maths heavy modules that I have to focus on as well and it doesn't help that the module leader for that module has made the exam WAY harder this year as well. That is the situation here on my end. If you think I'm still being entitled... okay well that's your opinion. But I can say to you now sir (or madame), I am not a lazy bastard. I'm not going to go into the whole life story of this stuff but that is the situation here... never found programming or maths easy either. I've had to work very hard to get to this point, with some help (from patient and understanding people).
One thing I envy about the Rust community is how nice everyone is to beginners and how well problems like these are handled. As much as what you're saying is true, this is not how we should be responding to people. Answering a lack of respect with a lack of respect may be fair, but it's not going to help anyone; least of all this beginner who is now significantly less likely to become a contributing member of our community. We should have just told him "hey man it looks like you need some help with the fundamentals. Here's a few ways you can go about doing that on your time frame: ..." (fwiw, the answer is that when you don't have time, you need money: this guy needs to hire a tutor).
Yes exactly. Thank you. Someone who understands. I apologise if the threads have been coming off as a lack of respect but it seems some people have a lack of understanding on the amount of little time that I have (I've explained this so many times). If someone could have just pointed me in the right direction as you suggested that would have been fine. Anyway my instructor gave me a good answer today, one I needed. I've got to forget about this shit now because I got studying to do. Perhaps in the future Haskell community members will be a little less abrasive to beginners. Cheers for the understanding.
Uh - good point, I don't have such settings. Depending on your operating system gonimo might infact prevent that! If you could try out if it is the case for your system, that would be awesome!
If you do not have a linkedin account, you can send your resume to vagif.verdi at google email. 
Here is a bonus proof for those who managed to read this far: s -&gt; (a, b -&gt; t) = {Writer and Reader composed functor} Compose (Reader s) (Writer a) (b -&gt; t) = {Co-Yoneda} exists r. (Compose (Reader s) (Writer a) r, r -&gt; b -&gt; t) = {Writer and Reader composed functor} exists r. (s -&gt; (a, r), r -&gt; b -&gt; t) = {Swap &amp; Uncurry} exists r. (s -&gt; (r, a), (r, b) -&gt; t)
Dependant types, as I understand them, are practically orthogonal to type families. Type families are computations of types *from types*. Dependant types are computations of types **from values**. That is, right now, we can have a function `:: String -&gt; Either a b`, with dependant types, we could have a function `:: String -&gt; {a, b}` where the resultant type is one of either `a` or `b` or both, and which it is can *depend* on the `String` argument given. This would mean that we no longer need to wrap up symbol types in an existentially quantified wrapper ala `:: String -&gt; SomeSymbol`, instead it could actually produce the correct symbol type from the `String`. Most dependently typed type systems seem to also include computing types from other types, but that seems to be a feature of *the type system*, rather than particularly because it is a *dependant typed system*.
As you might imagine, `unsafePerformIO` in this case is a bad idea and totally defeats the purpose of IO. Perhaps the easies thing to do here is generate an infinite list of random values inside of our main function and then just pass this list to your placeShips function as a parameter. 
This is for an assignment, and I'm not allowed to mess with the method signature for placeShips. I wish I could just do that :/ I suppose I could probably make a new function placeShips' that receives the infinite list of randoms, and then calls placeShips ? The unsafePerformIO is the only way I managed to get any sort of random functionality working in my code. I've seen a couple of threads and tutorials that advice against using it... Why does it even exist if not to use? 
&gt; The unsafePerformIO is the only way I managed to get any sort of random functionality working in my code. Have you tried calling `randomIO` from inside an IO computation? Have you tried to use the other random functions which do not have `IO` in their name, such as `random`, `randomR`, and `randomRs`? &gt; Why does it even exist if not to use? It's not that `unsafePerformIO` should *never* be used, if that was the case, like you said, it might as well not exist. But the number of cases in which it is useful is very small, and using it correctly requires specialized knowledge. I know this sounds elitist, but: you are not ready. `unsafePerformIO` is advanced black magic, if you try to use it before you're a certified level 10 black wizard, you will just burn yourself. *edit*: Would it sound less elitist if I mentioned that even us level 10 black wizards stay away from level 20 spells like [`accursedUnutterablePerformIO`](https://www.reddit.com/r/haskell/comments/2cbgpz/flee_traveller_flee_or_you_will_be_corrupted_and/)?
Well, it is called "unsafe" and it does live in a module with "Unsafe" in the name. It should be pretty obviously that you need to know what you're doing it you're using it. There are cases where you can use it for various performance purposes and things like that but those are very much edge cases you won't be encountering to a while.
Haha, alright I deleted the unsafe stuff. Now I have `rand = randomRIO(1,10)` Is there a way to easily convert the function `placeShips` to be an IO function? 
Great article! Thanks for writing this! Two small typos: your record declarations have extra commas: data Command = Command { name :: String, , with :: String -&gt; Invocation } data Invocation = Invocation { of :: Command, , args :: String, , exec :: String } 
You don't need to do that: main = do rand &lt;- randomRIO (1, 10) grid $ placeShips rand [5,4,3,2,2] (mkBoard 10) Or something on those lines, you do not need to change `placeShips`, you actually should not change it. Well you do need to take in the random number, but it doesn't need to be `IO` or generate its own random numbers. Are you sure you can't change the signature, and are you sure you need to generate random numbers at all?
&gt; lol, a fellow classmate got his code fully working using unsafe. The prof should and almost certainly will take a lot of points off for that. Look at my other comment for a proper way to do it.
It's a reasoning tool for me. It's much easier for me to reason about functions when as many types are generic as possible, and I can clearly see what information does and doesn't matter. If you throw out parametricity, then you basically have the nightmare that is how `instanceof` gets used in Java. As types and terms become more unified, I'd rather not straddle that boundary.
There is a Python C API but it's tricky. For one thing there are no good bindings for Haskell. Another is that there's a single global Python instance for your program and when a plot goes wrong for some reason bad things can happen to it. There's nominally support for separate python environments but that's just hopelessly broken. This seems like a simpler solution. Also, give it another try and let me know if you encounter any trouble.
Wouldn't I have to make some changes to `placeShips` if I'm passing in another value? because now it's picking up an `Int` where a `Board` should be. Am I missing something here? import System.Random import System.IO main = do rand &lt;- randomRIO (1, 10) grid $ placeShips rand [5,4,3,2,2] (mkBoard 10) placeShips :: [Int] -&gt; Board -&gt; Board placeShips ships board = if (ships == []) then board else if (isShipPlaceable (head ships) rand rand (rand == 1) board) == True then placeShip (head ships) rand rand (rand == 1) board else placeShips ships board Changing the signature to: placeShips :: Int -&gt; [Int] -&gt; Board -&gt; Board placeShips rand ships board = allows me to use the function, however, since I am only passing in the 1 rand value, x and y are the same. So if rand is 4, the ship head will get placed at (4,4). I'm stuck where I was before where I am unable to "reshuffle" and get new random values from within the placeShips function. This is a bigger issue when you consider that if rand is out of the range of the board, then when I recursively call placeShips, it calls it using the same rand value as before and still unable to be placed resulting in an infinite recursive loop as new rands are not being generated from within the function. 
It's parsing and sending IACs in a naive manner and not tracking the state of these exchanges at the moment, but would be fairly trivial to make work better. We need to actually start using and testing it a little more before we're comfortable with pushing it to hackage. I came across a post of yours from a few months ago and noticed you were building something similar. How did that go?
I think the usual way is something like this: data Universe = Int | Bool | Pair Universe Universe | ... typeUniverse :: Universe -&gt; Type So you do all the computation on `Universe` and then convert to types later. This has the usual extensibility problems. For stuff like associated types, you take advantage of module signatures or dependent records. So, something like data Collection = Collection { collection :: Type , element :: Type , empty :: collection , insert :: element -&gt; collection -&gt; collection } insert :: (c :: Collection) -&gt; element c -&gt; collection c -&gt; collection c This is comparable to a multi-parameter type class (without functional dependencies), except you always have to explicitly specify the instance.
&gt; I'm not allowed to mess with the method signature for placeShips. I wish I could just do that :/ Tell your professor that the internet thinks that this is an absurd restriction :D Haskell values are evaluated and then cached. A function like `foo :: Bool -&gt; Int` is *pure*, so GHC is free to cache that result. This is what is happening. When you write `fooIO :: Bool -&gt; IO Int`, GHC knows that the `IO` value means it can't reorganize or cache that value. When you remove the `IO`, GHC thinks it can cache the value (or even inline it directly into functions)!. The proper way to fix this is to make the function either 1) accept a source of random values, or 2) operate in IO, so it can generate the random values itself. The *improper* way to do this is to put a `{-# NOINLINE getRand #-}` pragma on `getRand`.
&gt; type Num `Num` is not a type, but a type class. You should safely be able to use such a function anywhere an Int is expected. You may need an explicit type signature to reduce ambiguity. &gt; or char Not sure what you are referring to &gt; or IO IO is what you want here. &gt; I can't use any of these values with the other functions I have, that are all non-IO and must take type of Int Pure functions must depend only on their input. Functions that require random values cannot be written as a pure expression unless the dependence on the random number generator is explicitly declared. Thus you have to either write your functions as type `StdGen -&gt; (a, StdGen)` (i.e., `State StdGen`), or use the convenience functions in `IO`.
Hmm. That doesn't sound very fun to work with.
&gt; Wouldn't I have to make some changes to placeShips if I'm passing in another value? because now it's picking up an Int where a Board should be. Am I missing something here? Yeah I don't know if you saw my edit earlier but doing what you want without changing the signature at all doesn't seem practical, I think you do need to pass in an extra value. But it definitely does not need to be in `IO`. Also created more random values is fine, just initially pass in a seed instead of a value, then keep cycling the seed to generate new values and seeds with `random :: (RandomGen g, Random a) =&gt; g -&gt; (a, g)`. Can you explain the EXACT spec of `placeShips :: [Int] -&gt; Board -&gt; Board`. So what do you want to be outputted for some given input.
&gt; If you are getting pent up about that. You might need to spend your time doing something else. I think I answered courteously, even though that might have been a bit terse. On the other hand, getting time management tips from you is just perfect :) &gt; There is no rule that prevents me from posting and asking for help. There is not hard rule, but that's what /r/haskell is about: &gt; Daily news and info about all things Haskell related: practical stuff, theory, types, libraries, jobs, patches, releases, events and conferences and more... There are sites dedicated to that kind of interactions, such as stackoverflow. As I said, IRC is pretty good too, as you can get instant feedback (if someone is awake).
Sure thing! So `Board` is of type [[Int]] Board is a 10x10 grid with all values initialized to 0. `Ships` is of type [Int] and contains the lengths of the ships to place onto the board. (this is hard coded to accommodate the standard Battleship format of having ships of sizes: 5, 4, 3, 3, 2). So essentially, `Ships = [5,4,3,3,2]` So we pass `placeShips` our board with all 0's, then placeShips recursively places all the ships onto a new board, then outputs that new board. placeShips should get random values for the x-coord (1-10), y-coord(1-10), and direction (1-2). It should run those random values through a function I have called `isShipPlaceable` if true, then it calls function `placeShip` to actually write that ship to the board. placeShip returns the new board with the ship placed. If the ship is not place-able at that spot, I want it to "reshuffle" those random values and call recursively call itself again. example of desired output from `placeShips`: [0,0,0,0,0,0,0,0,0,0] [0,0,5,5,5,5,5,0,0,0] [0,0,0,0,0,0,0,0,0,0] [0,0,0,0,0,0,0,4,0,0] [0,0,0,0,0,0,0,4,0,0] [0,0,0,0,0,0,0,4,0,0] [0,0,0,0,0,0,0,4,0,0] [0,0,3,3,3,0,0,0,0,0] [2,0,0,0,0,0,0,0,0,0] [2,0,0,0,3,3,3,0,0,0] [0,0,0,0,0,0,0,0,0,0]
A nice, unveiled job description. Joy to read.
Nice article. The linked [stackoverflow answer](http://stackoverflow.com/questions/13106683/dynamic-dispatch-in-haskell/13110560#13110560) is also worth reading.
If you call the function twice with the same parameters. Like ships and board. Do you expect the same placement or different placement? Because the latter is not possible (without very unsafe stuff that should not be used). The former can be done by converting your input list of ship sizes into a seed, using whatever function you like to convert the list into an `Int`, then using `mkStdGen`. But I'm guessing you want different placements each time. 
`-XDependentHaskell` is already being worked on though, and this does not seem to be in the same direction as dependent types. And I'm not sure what you mean by the "decompose" argument. 
Sorry I used the wrong word. By decompose I meant pattern match (using a type family). I'd rather have some small part of dependent types before 2022, as with everything software related it seems unlikely to come all at once or perfect/even usable in the first revision either. 
Perhaps their professor intends for placeShips in a let binding of main? 
can you elaborate? (you wrote fltkhs, right?) 
Fair enough. I already agree wholeheartedly for functions on values after all. You haven't convinced me about types, but I understand your position. As long as typecase sticks around for type families I'm happy.
Yes I am the author. The problem I'm talking about is say you have something like: class A { int foo(int x, int y); }; class B : A { void foo(int x); } The SO answer can't support a Haskell-side API where I can just: do result &lt;- foo instanceOfA 1 2 _ &lt;- foo instanceOfB 1 ... When I was working on my bindings this came up often enough that the technique in the SO answer would have made for a really annoying API. 
As someone who has coded small stuff in a lisp, gone through the upenn spring '13 lectures and likes the theory side of things - the first chapter (which i did after) was a turn off. I understand the motivation on a "start from zero" basis, but I think it's a bit much for a newbie. It's too thorough, too hand off, and emphasises add the negative stereotypes people have going in about haskell. It's dry, very 'mathematical' and you need to be really smart to use it. While you may have reduced the # of questions from people about execution model, at what cost? How many readers did you lose, that simply put the book down, decided haskell wasn't for them and never sent an email? From an educational perspective, and important part of the start of teaching any perceived challenging material is making the user feel that this is for them. That they can do this, and their motivated to continue. In my opinion as important as it is in foundational perspective, it really works against getting the reader to finish the book. Possible suggestion - entirely move it a few chapters later. Or cut out 2/3rds of it, leaving just thr essentials there and revisit the rest a few chapters later. EDIT: Actually reading what /u/QuantumBullet said, I think splitting it makes perfect sense. Even s/he mentioned it starts off simple (and presumably useful), then gets too complex. Give just enough info in chap 1 for basic substitution concepts and to understand the model - then in a later point fill in all the details for completeness. 
yeah, because you'd need to mangle everything with the type name, like `foo_A`? I thought the dealbreaker was variance (/downcasting). like you create an instance, and then its passed between a few functions that consume or produce instances at different levels of some hierarchy; and in an OO language like Java you'd just call its method with minimal boilerplate, but in haskell you might have excessive boilerplate, or be forced to perform an expensive copy, or something. 
There is one thing that dynamic dispatch in OO can do easily that, so far as I can tell, we really, REALLY do not have an easy time wtih. Modify the execution of a compiled program at the use site. Specifically, say you want to make a game, and you want your users to be able to write mods for this game because players *love* mods. In Haskell land, congratulations, *you're shipping a compiler with your game*, and compiling the entirety of the actual game logic on the players end, and that's about the extent of your choices. As the developer, the types have been eliminated on your end, and your mod writer/users have nothing to work with, and you can know nothing about the types of anything they might want to do.
Is that really a problem? If your mods allow any sort of scripting, you're _already_ shipping a compiler or interpreter, be it Lua, Python, or otherwise. Often it's unique to the game -- i.e. a DSL, something Haskell's notably great at. Meanwhile, I've seen extensive and impressive modding done in games which only expose data structures; e.g. Dungeons of Dredmor's modding interface is XML. That's just a parser, another Haskell strong suit.
This is _exactly_ the article I needed right now. I'd identified the issues with sum types and typeclasses, and was pretty sure I _should_ be doing something with function records, but hadn't yet worked out how. Thanks for the fantastically clear tutorial!
You have a small typo: Postgers should be Postgres. Excellent resume though, I would definitely apply if I had the skillset.
&gt; You can with various GHC extensions, but not in standard Haskell this was too fast. There is nothing wrong in using extensions.
Somewhere between these sentences: "You may be contemplating skipping this chapter. You may feel tempted to skip ahead to the fun stuff when we build a project. DON’T." and "Lambda calculus is your foundation, because Haskell is built on it." To me, that pretty much said, "You must understand this chapter completely, or you will not be able to understand anything else in this book."
 type Command = String -&gt; String A command is **just** a function. Get some arguments, return some result. That's it. Nothing more, nothing less. There is no need to wrap it in layers of abstractions. sortingCommand :: Command sortingCommand = unwords (sort (words args)) In particular, a command doesn't necessarily have or need a name. If you do need to access commands by their names, use a separate `Data.Map String Command` or something of this nature. `Invocation` is not needed either. If users need a tuple (name, function, arguments, result) they can build one themselves without thinking too much. Now you may want to say that there is a difference between an executed command and a command invocation that you want to preserve. You might have a point if you have used an eager language, but Haskell is lazy, so *there's no difference*. sortedWords = sortingCommand "foo bar baz" Does this *run* the `sort` function? Not until you scrutinise `sortedWords`. Now when you want to encode objects with several tightly coupled methods that share state, you may want to do something more advanced, but you don't have to. It's just a bunch of functions. Here's the ubiquitous shapes example: data Shape = Shape { draw :: Canvas -&gt; Canvas, translate :: Vector -&gt; Shape, rotate :: Point -&gt; Angle -&gt; Shape -- etc etc } Where's the shape *data*? Inside the functions of course! type TriangleData = (Point, Point, Point) myTriangleData :: TriangleData myTriangleData = ((0,0),(0,3),(4,0)) myTriangle :: Shape myTriangle = Shape (drawTriangle myTriangleData) (translateTriangle myTriangleData) (rotateTriangle myTriangleData) -- etc Edit: typos
Seems like you're describing [Data.Map](https://hackage.haskell.org/package/containers/docs/Data-Map.html) and [Data.Set](https://hackage.haskell.org/package/containers/docs/Data-Set.html). Do they suffice?
You can export the smart construction function and the data type (not its constructors) from a module. This is referred to as Smart Constructor pattern. ([smart constructor](https://wiki.haskell.org/Smart_constructors)) module MyModule ( mkProtected , getProtected , Protected ) where data Protected = Protected Int mkProtected :: Int -&gt; Maybe Protected mkProtected n | n &gt;=0 = Just n | otherwise = Nothing getProtected :: Protected -&gt; Int getProtected (Protected n) = n 
Is this required to verify that a program is well-typed, or is it only necessary in order to produce binaries? In my experience `intero` is really slow, and I wonder whether this is because it uses GHC to do stuff that's essentially only necessary when producing an actual binary.
It was fixed two weeks ago IIRC. Some weeks after the source was available from GHC, and bindist available on an unoficial github from a dev, and just before it being oficially on ghc's download page.
Is this also the case if we ignore the production of executable binaries, and just focus on type-checking (as IDE tools like `intero` do)? 
According to [Wikipedia](https://en.wikipedia.org/wiki/Virtual_method_table) a vtable is &gt; a hidden member variable to the class which points to an array of pointers to (virtual) functions so that would be `n` function pointers plus one pointer to the vtable itself. I don't know enough about the GHC internals but [this](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects#FunctionClosures) might be some guideline for estimating what Haskell functions will look like on the machine level. In the example above a `Shape` would ideally be a vtable that points to static (top-level) function closures, with the shape data as their payload. This shape data could be pointers or non-pointers. I suppose non-pointers are primitives, like raw 64-bit integers, but I'm not sure about this.
I wish I could use darcs exclusively - it's *so* much nicer than git. But the world uses git, and I need to be part of the world. (We actually use mercurial at work, like Facebook does.) I do use darcs for my own personal stuff that I don't expect other people to need to interact with. And I appreciate the git interaction features that darcs has grown. Maybe someday it *will* make a comeback, who knows.
Is there any extension that could allow this approach? And if so, why not use it? class Command something where exec :: something -&gt; String parse :: String -&gt; String -&gt; something data Echo = Echo String instance Command Echo where exec (Echo arguments) = arguments parse "echo" x = Echo x data Sort = Sort String instance Command Sort where exec (Sort arguments) = unwords (sort (words arguments)) parse "sort" x = Sort x data Help = Help instance Command Help where exec Help = "Try typing an actual command, dummy!" parse "help" x = Help main :: IO () main = do args &lt;- getArgs x &lt;- readLn let cmd = parse x (head args) putStrLn (exec cmd) 
You are absolutely right, with this approach a vtable is held in each object rather than shared by a class of objects. This inefficiency should not prevent one from playing with OO concepts though. It can most certainly be avoided, but one baby step at a time, shall we? 
But in this particular example I don't need a list of commands. I need to parse one command and then execute it.
I use it exclusively for my "own stuff". I use whatever else is necessary when collaborating with people who don't know and don't care to know Darcs. IMHO Darcs is very nice and intuitive to work with. Does it have some warts? Sure it does, but so does every other VCS out there. :o) BTW please note that we can support https://hub.darcs.net/ to help keep the one and only Darcs online repo hosting service running.
What about implementing Idris style dependent types in Lamdu? Or you could take ideas from the SoC issue tracker: https://ghc.haskell.org/trac/summer-of-code/report/1 Or just watch through the ICFP talks until you get inspired: https://www.youtube.com/channel/UCwRL68qZFfub1Ep1EScfmBw Shoot the speaker a mail, why not? Or make HalVM get feature parity with MirageOS?
Ya, it's a solid argument (the rest of the world uses Git), but it still sucks. Our team mostly writes Ruby by day, and instead of just giving up and settling for Git + whatever CI SaaS tool is cool, we built our own on Darcs hooks + AWS spot instances. While everyone else is Mocking objects because they bought into the new Git SaaS tool and they have to dumb down their tests, we write whatever the fuck tests we want, full objects, hit the DB, all associations, A-Z. It would probably take a full day to run our test suite on one badass machine. But it only costs us $20 a day to rent ~100 cores of AWS spot instances to run our tests and they finish in under 60 seconds on every `darcs push`. My point is that Git and Github has created this CI environment where neutering your tests has become so commonplace.
Sounds good to me :)
My point I guess is that no one should be mocking objects. You shouldn't have to write tests that way. You should only have to think about performance in your production code, and your tests should cover that, without having to think about performance.
Wikipedia is wrong. Vtable is the array of pointers to virtual functions itself. The hidden member would be called simply a vtable pointer, or sometimes vptr.
Your first point is exactly why I brought this up. Back in the mid-late 2000's Git and Darcs were nearly head-to-head. Git won. No arguing there. But now that functional programming is 'en vogue', I think it's worth bringing up because I just did a quick search for 'darcs' on this Haskell subreddit and there haven't been many posts.
The lack of Darcs discussion was just surprising to me.
Then your boss tells you "ok, this typeclass stuff is really neat and stuff, now show me how you build a list of commands, that's what our customers want tomorrow". 
* [Link to gist](https://gist.github.com/fumieval/4445447). * ["An introduction to ideal monads"](https://theorylunch.wordpress.com/2012/11/08/an-introduction-to-ideal-monads/) blog post. * ["What is the correct definition of ideal monads?"](https://theorylunch.wordpress.com/2012/11/22/what-is-the-correct-definition-of-ideal-monads/) blog post. * Another post in Japanese with examples: [『究極のモナド「Idealモナド」を垣間見る(続/その0)』](http://fumieval.hatenablog.com/entry/2013/01/07/113221) ([*English translation*](https://translate.google.com/translate?sl=auto&amp;tl=en&amp;js=y&amp;prev=_t&amp;hl=en&amp;ie=UTF-8&amp;u=http%3A%2F%2Ffumieval.hatenablog.com%2Fentry%2F2013%2F01%2F07%2F113221&amp;edit-text=&amp;act=url)) * Slides ["Monads and More: Part 2"](http://www.cs.nott.ac.uk/~psztxa/monads-more-2.pdf) ^^[**PDF**] * An implementation from [*category-extras*, `Control.Monad.Ideal`](https://hackage.haskell.org/package/category-extras-0.53.3/docs/Control-Monad-Ideal.html) class Functor m =&gt; MonadIdeal m where idealize :: m (Either a (m a)) -&gt; m a and [`Control.Comonad.Coideal`](https://hackage.haskell.org/package/category-extras-0.53.3/docs/Control-Comonad-Coideal.html) class Functor w =&gt; ComonadCoideal w where coidealize :: w a -&gt; w (a, w a) 
[Pijul] (https://pijul.org) seems to be picking up speed recently. It is already self hosting, see the [pijul sources](https://nest.pijul.com/pijul_org/pijul) and they just started offering a modest hosting service [The Nest] (https://nest.pijul.com/). Also, there was an [HN discussion](https://news.ycombinator.com/item?id=13643025) about pijul a while ago. In particular, there were many comments of comparing `pijul` to `darcs` and the collaborations between the two communities.
My intent wasn't to hate on Git. I guess I just want to see people get excited about Darcs again. And I want to see the Haskell community get excited about it in particular. I want to see the Haskell community write some Darcs plugins for Atom, etc.
Not that it was written in Haskell, but one of the top Atom plugins is called git-time-machine, and I wrote a parser to make it work with Darcs called darcs-time-machine: https://github.com/whitslar/darcs-time-machine Quick-and-dirty but it makes my day-to-day life at work so much nicer
FWIW you're looking at the changelog for the stable version 2.12.x, which is why you don't see many recent changes. Plenty of work is going on on HEAD, which will make it into a new stable release 2.14.0 at some point: https://hub.darcs.net/darcs/darcs-screened/changes Also, https://hub.darcs.net is the github-equivalent.
Never use typeclasses because some day you will need a list of instances?
Can you give a reason why a git user would want to switch to Darcs? What is Darcs special relationship to functional programming?
I think it's a bit more subtle. There's a dichotomy between types that allow many representations (like Java's Set, which is an interface) and types that have binary operations requiring the same representation (like Haskell's Data.Set, which allows fast union). If you don't know up front in which direction your type will grow, using the "existential typeclass antipattern" allows choosing either direction later, so it seems more future-proof than making it either a record of functions or an abstract data type (or interface vs concrete class in Java-speak).
As someone who has never used Darcs (and will probably never use), what does Darcs give me as compared to git ? With git, I can use something like magit and use github for hosting. I don't think something like that is available for darcs. While git as a whole may be complex, the features I use most of the time are a very tiny subset of what git provides.
Could you detail a bit more your point? I don't see the link between you testing strategy and why Darcs allows you to do it. Actually, I don't understand either what you are talking about mocking the object and or full testing. And I have the feeling I missed something important.
The "plugins in Haskell vs plugins in C++" issue, if it ever exists, has nothing to do with type systems of the respective languages. A compiled program written in C++ is a bunch of low level machine instructions, exactly the same as a compiled program written in Haskell. In order to write a C++ extension for a C++ program using OO concepts, one would need a set of header files defining a bunch of base classes. There are no types in the compiled executable (C++ or Haskell). For Haskell, it's a set of .hi files instead of headers, but the concept is the same. There's no need to compile anything beyond user's code in either case. Objective difficulties that do exist with writing Haskell dynamically loaded modules have nothing to do with its type system being or not being "non-OO" or "not dynamically dispatched".
http://darcs.net/DifferencesFromGit shows the main differences between darcs and git. Some points that might interest you are: * Merging in darcs is more like rebasing in git, so you can safely change the "history" of a repo. * Every command is interactive by default, so it is more beginner friendly and you're less likely to accidentally do something. * Lazy repositories which are on-demand shallow repositories. So darcs automatically downloads more history if that is necessary. Darcs is written in haskell, so it is in that way related to functional programming.
Yea it's going to be one of those monad-applicative relationship laws. Nothing in the theoretical definition of monad implies `ap = (&lt;*&gt;)`. We include that law as a proof of having `Applicative` as a superclass. There needs to be some kind of similar proof written in the laws. Imposing a monad constraint on `Idealize` kind of defeats the purpose of giving `Lift` a monad instance, no?
I compromised by making it nice for users ( the `foo` example above works) but harder (tons of boilerplate, etc) for people adding new C++ widgets. I guess extensions like `OverloadedLabels` and `OverloadedRecordFields` will let people make different tradeoffs today.
Excellent news, thanks!
Eh. My view on `Lift` is that it serves the same purpose for functors that `Maybe` serves for monoids. `Maybe` lifts any semigroup into a Monoid. `Lift` lifts any `Apply` into an `Applicative`. It seems that it should serve a similar role for monads, but it doesn't using `Bind`. Just seems like `Idealized` is how you're supposed to draw that relationship. Point being, constraining that `f` be a monad means you're just turning monads into other monads. Not doing so lifts non-monads into monads, which is much more useful. Therefore, I see no reason to impose an unnecessary constraint, when there's likely a law that could be defined to accomplish the same thing more minimally.
Perhaps another way could be {-# language ExistentialQuantification #-} data Foo a = forall x. Foo { value :: x, get :: x -&gt; a } This lets you pattern-match, but you can't do anything interesting with the value except using `get` on it.
What does it have to do with DARCS? You can set up any kind of CI with Git and you won't even have problems with GitHub unless you are doing large number of parallel pulls.
I think you can trivially prove that `Idealized f` implies `Apply f`: f &lt;.&gt; a = f &gt;&gt;~ \f' -&gt; f' &lt;$&gt; Other a So this proves an `Apply` superclass on `Idealized`. Then the law can just be that `(&lt;.&gt;)` has to equal the above proof.
Darcs is patch based, while git is snapshot based. This means that operations like cherry picking are better supported in darcs than git. For hosting, there's https://hub.darcs.net/