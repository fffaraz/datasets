You could use [`binary`](https://hackage.haskell.org/package/binary). It can convert unboxed/boxed arrays to lazy bytestrings. ST isn't particularly relevant here, you can always just (unsafe)freeze your mutable arrays. 
&gt; http://haskell-is-easy.com By the way, on the xml side, unfortunately `taggy` (which is the first advised lib for xml mentioned there) is not quite active nowadays. See the last comment by its author [here](https://github.com/alpmestan/taggy/issues/14). I have eventually ended up using `xml-html-conduit-lens` which is a small add-ons to `xml-conduit`. 
Clicking the link for scotty takes me to &gt; http://haskelliseasy.readthedocs.org/en/latest/hackage.haskell.org/package/scotty which is obviously not okay. 
My wishlist for specs would include Gitlab, Redmine, Jenkins and Hetzner among others.
&gt; Are these 4 composable with each other in any way? Yes, in the `Control.Lens` (van Laarhoven) formulation they are automatically composable in every way possible.
Yeah I got confused by the blue headings too.
Sounds reasonable. I meant to put the suggestion on the website. The only mention of IO on the site is in the section on classy-prelude. It could stand to be one of the explicit use cases.
You mean like lens? Lens is awesome and terrifying at the same time.
I actually have ran into the same problem. I wish there was a way to localize ghc-mod to only run on the file you are currently looking at ,and it's dependencies, instead of the full project. 
&gt; The implementations of the typeclass functions are constructive proofs that you can fulfill the laws of that class for that type. I don't think this is true; even when the implementations do satisfy the laws, they needn't amount to a proof that they do. (I'd be happy to be corrected if wrong, of course!)
You should still be able to run this on any projects that is sub lts-5.1 by switching out your versions. I might have to put together a follow-up to show how to get all this working with the Cabal workflow too. The tooling described here will still work for any project. You just need to run `stack install cabal-install`. Which you really just need if you don't want to run `stack init` on an existing project. `stack init` has actually become my preferred way of working with older projects.
I am actually really surprised to see Yesod as the web framework of choice. Since scotty is recommended for smaller projects I would expect spock, snap or servant for bigger projects. They seem to be in a more similar vein than Yesod. Which is written in a very different style.
If anything, Haskell *is* the Dark Souls of programming. AFAIK Souls games are considered punishing but fair with a steep learning curve and they don't hold player's hand. Sounds like Haskell, no? I think both authors should refrain from making catchy titles that don't hold water. edit: by both authors I also mean the author of the post that /u/alan_zimm linked
I now realize that my law is equivalent to the [`MonadFree`](http://hackage.haskell.org/package/free-4.12.4/docs/Control-Monad-Free-Class.html) law: wrap (fmap f x) == wrap (fmap return x) &gt;&gt;= f That is, the `MonadFree` law could be stated like this: wrap x &gt;&gt;= f == wrap (fmap (&gt;&gt;= f) x)
I suggest having ReadP (from base) highest on the parsers list, if ease is the criteria. Parsec is not really easy at all. 
I will say that I've probably put about 20-40 hours into learning Haskell. It continues to be deeply alluring but it's inappropriate for my day job. That said, everything I've learned in Haskell has been profoundly useful in my other languages. So many problems are made trivial by a functional approach. Other languages are also borrowing heavily from functional languages like Haskell. An understanding of Haskell will inform your understanding of those features. Python in the 3.5 era is informed by lazy functional patterns like those found in Haskell. In JavaScript, ES6 and ES7 are also borrowing heavily from functional languages like Haskell. For example, you'll probably find it easier to understand how ES7's Promises compose after spending some time with Haskell's type system.
No, I don't think so. There may be some relation but if so it's tangential.
I would myself rate parsers + trifecta higher. attoparsec has a nice API, but its error messages are not suitable for beginners IMO.
Well, swagger spec won't help that much. servant's API type would be much better. I could put `swagger-servant`on my writing list. i.e. Haskell/servant code generator from swagger schema, but I don't have any service with swagger schema to consume atm,
How is the transition to pipes coming along? I've kind of shelved my usage of happstack with the plan to pick it back up until that is complete. These days I'd also be curious about if happstack can work well on top of WAI.
&gt; Are these 4 composable with each other in any way? I hear sum and product, but wonder about sums that contain products, if I'm right about that being a thing. Yes! &gt; view (traverse . _Just . _Unwrapping Sum) [Just 4, Nothing, Just 3] Sum {getSum = 7} 
OTOH, `QuickCheck`is very light dependency. I'd personally of having it in main package. `hspec`is already a bit opinionated. I'd like to use `tasty`. Actually there is some functionality that should not depend on `servant`at all. Both `ToJSON`and `ToSchema` come from `swagger2`package already.
It was just a statement of fact. The author seems a little bewildered about what things seem easy and what is surprising to his student. For as novel as Haskell seems to most programmers, most of the core concepts are the bread and butter of what one learns in an intro to mathematics course and a semester of algebra. 
no servant eh... not that i disagree, for beginners the type level infix operators would probably be super confusing.
That's a good break down. I guess my point was that the style Snap is written in is easier to grasp if your coming from Scotty. Yesod takes quite a bit of a different approach and might be a higher learning curve. 
Stuff I would add: * **Fundamental data types:** bytestring, text, vector * **Time stuff:** time, thyme * **Logging Stuff:** fast-logger, hslogger * **Encryption:** cryptonite, ...???? * **Concurrency:** Control.Concurrent, stm, async * **Parallelism:** Control.Parallel, repa, accelerate 
Ah right. It’s more like “proofs of the theorems specified by the type signatures of the class functions”, which is weaker.
&gt; Sorry, why is this being praised as such a useful resource? Because nothing like it exists at the moment, as far as I know. &gt; It seems a rather low-quality effort It was started yesterday evening. &gt; littered with links to the author's non-FLO book It's littered with four links, and semi-littered with two links to the Yesod book.
Not really a study, but a great lecture by John Hughes which has a parsers as a kind of underlying topic: "Monad and all that": https://www.youtube.com/watch?v=w_KY2I34-f8&amp;list=PLGCr8P_YncjVeZTcfHT1Cb1OfVnNahek5&amp;index=1
&gt; Please don't email me to brag about how you're a genius-wizard that had zero trouble using streaming libraries unless you're going to link a resource or explain a method that made it easy. And if the link is to the standard tutorial for the library, I will not respect you. Is this really necessary? The passage could just as well be dropped, it adds nothing and sounds arrogant.
I would contend that temporary debugging IO actions may be ok. They're not intended to be a part of the program semantics, so it makes little sense to do all the work to thread IO through your entire type system just to debug some piece. But yeah, other than that I can't think of many good uses for `unsafePerformIO`.
I doubt I'll have enough will to do that, but would happily accept it as a pull-request. For integration tests, there is already an example of how to do the request, so you can use something like a [tasty framework](http://documentup.com/feuerbach/tasty) to set up these tests (run server, do requests, check responses). Unit tests are harder since it depends on what exactly would you like to test, I usually do unit-tests only for simple stuff, preferrably via QuickCheck, but mostly just integration tests.
&gt; they don't hold player's hand. On the contrary in Haskell's case, the language is designed to hold your hand with immutability, purity and an advance static type system and type inference. My hand was never held that hard before.
&gt; that's a nice turn of phrase. Thank you Joey :)
Have to try that!
Yesod is very full featured and batteries included, but for me it proved intractable as a noob. Under deadline pressure I had to give up and do my project in python. At the time I really needed something a good bit more light weight and noob friendly, but I didn't know any better then. This would have been a handy resource!
How to make cabal work with GHC 8.0 ? The debian cabal distribution is version 0.14 which fails with GHC 8.0 and compiling cabal from sources, ./bootstrap fails also....
[removed]
If you're on jessie, try grabbing binaries for an older ABI-compatible Ubuntu LTS release such as "precise" from my [ppa](https://launchpad.net/~hvr/+archive/ubuntu/ghc), and then install or unpack the `.deb`, e.g.: wget https://launchpad.net/~hvr/+archive/ubuntu/ghc/+build/8972731/+files/cabal-install-1.24_1.23+git20160204.0.7aab356~precise_amd64.deb mkdir ~/temp/ dpkg -x cabal-install-1.24_1.23+git20160204.0.7aab356~precise_amd64.deb ~/temp/ ~/temp/opt/cabal/1.24/bin/cabal --version 
Fair enough. I've had to deploy PHP stuff on the same server as Node and Go, so Docker made that slightly sane.
Revealingly, there's nothing listed in here for exposing a GUI.
I have installed stack and atom and tried installing some packages for it previously before giving up and using a notepad on a linux vm. http://pastebin.com/7kae8V93 I added what it said to what it said as it said. (Why didn't it do that automatically?)
Idris does the same. Indeed, having black and white code is not that cool. But if in an imaginary future we had both, and we could change between them with some button press, that would be awesome.
Is there some reason why it is not beneficial to use `V.fromList` here?
I am reminded of HtDP/Racket's strategy with multiple language subsets. FP10x [uses Hugs](https://www.reddit.com/r/haskell/comments/3o8rff/fp101x_using_hugs_because_ghc_changes_too_much/), but there seems to be resistence towards that. So I guess a compromise would be if GHC supported enforcing a simpler language subset like Haskell 2010.
Hi, I am on debian jessie. Tried to use it with stack, so I prepared stack8.yaml with section: compiler: ghc-8.0.0.20160204 compiler-check: match-exact setup-info: ghc: latest-ghc: 8.0.0.20160204: url: "http://downloads.haskell.org/~ghc/8.0.1-rc2/ghc-8.0.0.20160204-x86_64-deb8-linux.tar.xz" And I initialise it with: stack setup --stack-yaml stack8.yaml --ghc-bindist http://downloads.haskell.org/~ghc/8.0.1-rc2/ghc-8.0.0.20160204-x86_64-deb8-linux.tar.xz --ghc-variant latest Now I can build my package with: stack build --stack-yaml stack8.yaml --resolver nightly --ghc-variant latest -- But there is a problems with that: `allow-newer` does not work, and a lot of packages not allow to have `base==4.9.0.0` I gave up after a couple of new sections like: packages: - '.' - location: git: https://github.com/haskell/deepseq.git commit: 40d4db0a4e81a07ecd0f1bc77b8772088e75e478 - location: git: https://github.com/haskell/filepath.git commit: 5c69dd09aa8728fe07c292a1347fe26c81c1ecff - location: git: https://github.com/haskell/containers.git commit: 27b9277f83fda3d333d3964d4810fe0ba71215ea 
That's what is done in base to create the initial TextEncoding values: http://hackage.haskell.org/package/base-4.8.2.0/docs/src/GHC.IO.Encoding.html#initLocaleEncoding
What's non-FLO?
Free/Libre/Open
I guess the obvious response to this is that at least the Yesod book is FLO (Free/Libre/Open). Maybe I'm being overly sensitive, but I don't even see a *disclaimer* on this site clarifying that the linked book is his. That seems a little unapologetic for an already (IMO) somewhat controversial elephant in the room. I understand that real effort has gone into this book, and it's only "fair" to charge money for it like any other thing that requires effort. That's fine. But specifically in the Haskell community, some high-profile programmers (Bryan O'Sullivan, Don Stewart, Simon Marlowe, Michael Snoyman) have released *free* versions of their Haskell books. So, I don't know, it seems a little presumptuous for Chris to come along and promote his $59 book everywhere you look. With all due respect, I would not consider him a high-profile Haskeller. I'd just like some more transparency and decorum around being an obvious outlier in this regard, and seeing his book promoted here in this manner is off-putting to me. That said, I can't complain about more publicity for the language we all love.
I think you might be mistaken, I was referring to the "Haskell Book", which is linked to throughout haskelliseasy.com. Nothing's wrong with that resource, I think it's fantastic and I've referenced it several times before.
Actually I was agreeing with you. I was saying why do we need haskelliseasy.com if there's already WIWIKWLH. I know you were referring to that book as non-FLO. :)
It's true. Until recently I've used vim/emacs with just syntax highlighting and indenting because the fancy ghc-mod features were always too unstable for me to bother long. I finally got ghc-mod etc. working in Atom and it's been great with minimal setup, so I'd recommend that route. Of course, it's still not as featurful / smooth as many other language's IDEs, but in my opinion, the type system`*` more than makes up for it in terms of ergonomics `*` I don't even mean the static guarentees - I'm talking about the fact that I can reason pretty strongly about what a function does from its type, I can search by type, etc. It's a huge gain in productivity to NOT NEED to look inside function definitions. `:i` in ghci is your friend 
I wonder if you might like our tutorial http://happylearnhaskelltutorial.com (only vol 1 is finished)
&gt; ffort (and you have to do it in other languages as well) Also with GHC 8 we will be able to give different data declarations equal reco Oh God. Not emacs. Too many keyboard shortcuts. I already use 3 different IDE's - the less keyboard shortcuts I have to remember the better.
I'll try Atom but if that doesn't work I'm going back to IntelliJ + Scala.
For the record, at least with `vim`, you can get jump-to-definition (hasktags, codex), autocomplete (neco-ghc), and even some fancy things like type insertion and case expansion (ghc-mod). I can't say the tooling is totally worth sinking time into configuring and figuring out, but the *language* is. Scala is just not comparable.
Don't know of any, but I *do* know there's one in the Rust community and multiple n64 implementations underway over there too, so by all means, IN THE NAME OF REPRESENTING HASKELL, GO FORTH AND GODSPEED YOU! To echo klaxion's point about Helm, I read Helm's source code back in the day (sometime last year), and it's not designed to provide a low-level foundation on which to build something big. It's pretty high-level and I don't think its author ever got it to the state he wanted it to be. Either that, or he didn't expect people to build significant games on top of it. You'll be much better off with sdl2, gl, and the good ol' FFI C bindings. It sucks, but the good news is that you'll be treading new territory that very few Haskellers have experience in, so you'll definitely have some blog/tech talk material at the end of all of it.
&gt;The distinction between mathematicians and non-mathematicians is a red herring when learning Haskell for beginners Me and my math degree took a long break from Haskell after some bullshit trying to get libraries working, but I'm not a very good programmer. Not a very good mathematician either, honestly.
Going to try flipping the header levels around to avoid this. Thank you! Update: making them smaller didn't help, but making them bigger did. I basically can't use anything smaller than h2.
Perhaps I should have made it a bit more clearer, The type is, parseNamedRecord :: NamedRecord -&gt; Parser a and type NamedRecord = HashMap ByteString ByteString
I've added a section for fast-logger and linked an example from carnival of it being used. I also mentioned it uses monad-logger for the interface. I'll add time. Added a bytes and text section. Added time and thyme. Thank you!
Good thing I don't recommend Parsec. Trifecta really is tops here.
You can change emacs to use the same (if you want) or vice versa. Aside from that you really don't have to remember much: - C-x C-f (find/open a file) - C-x C-s (save) nice to have: - C-x C-c (close emacs) - C-s (to search) - C-x (to enter a command) maybe - C-c C-l (to load something into the repl) that's it - the rest you can just (use CUA keys) change to the scheme that everyone uses in every OS I know (C-c to copy, C-v to paste, arrow-up, arrow-down, page-up, ...) of course you will soon find other useful things (that will knot your fingers on a IBM keyboard - like M-t for *everything* in GHC-mod ;) ) --- PS: C = CTRL, M(eta) = Alt on IBM (by default) ... have no clue what is what on Mac PPS: there is even a menu ^^
I like classy-prelude for the IOData typeclass and other things but the way the typeclass-heavy style fucks with inference is potentially counter-productive for new people. I'm mixed on mentioning alternative preludes in this list.
&gt;Yeah the blue subheadings are confusing but this is a much needed thing and a lot of beginners will certainly find it useful! Yeah sorry about that, I'm mucking with the heading levels to try to juke around that. Thank you!
I got Atom "Cabal &gt; Build Project" working (see description) and I can run code with Packages &gt; Script &gt; Run Script and the Haskell snippet runs, but only for the current file. I don't see a full blown "Run" button anywhere and I'm not sure how to configure it to handle multiple files. Do you know how to make this thing run?
Atom IDE doesn't seem so bad, I just don't know how to get it to run (short of re-opening the project in IntelliJ and clicking "run"). Right now I think I'm going to use IntelliJ for running and auto-complete (IntelliJ shows me the type in the auto-complete) and atom for reading and Hoogle-ing. If you know how to get it to run debug mode in Atom let me know.
Another alternative is: instance FromField a =&gt; FromNamedRecord (Funnel a) where ...
It seems like a pretty easy comparison to me. Haskell is to Scala what Ruby is to C++. A less bloated, more pure, less mainstream alternative. That being said, I'm not ready to learn vim right now, but if I can get better auto-complete and a "run" button on Atom, I would be quiet happy. Maybe a "go to Declaration" option that actually works, too.
There's an plugin for atom called Script (I think, not sure, am on mobile right now) with which you can run all kinds of files by just pressing Strg-B (note: requires GHC installed of course) Support for debugging is not the best unfortunately. You could just install a command-line plugin and then use GHC and GHCi for debugging Edit: You might have to learn how to use the command line with GHC, it's worth it
hey you don't have to - all I'm saying is that you can use emacs just like every other editor out there and you can easily get to the point where most of your shortcuts are the way you are used to. And no: emacs + Haskell is not close to IntelliJ in features or ease of use (heck the debugger support in Haskell **is** a major pain) but IMO it does not have to. I'm using VisualStudio to earn my bucks so I guess I am in a similar situation but between F#+VS (where I have to admit that the support in VS for F# is laughable, ReSharper does not care at all and the only way out is a community effort to get some functionality) and Haskell+emacs I strongly prefer the later as the type-system is more important to me than a "goto-definition" or refactor-support (some of which I can have too if I want) ---- if you don't like emacs there's always the *editor of the beast* as a fallback :D BTW: the same is true for every IDE I used so far - the older Borland and VisualC++ IDEs, VS, Eclipse, Monodevelop, even tried IntelliJ for a short time - all could change their key-mappings to something I am used to (most even have extremely similar *defaults* )
Look, I'm not going around telling people to stay away from your book or anything. I already assumed that it is taking an *absurd* amount of effort, and again, my problem is *not* that it costs money. I just found the lack of disclaimer distasteful, so I spoke up. So, thank you for adding it, and good luck on the book. And to answer your questions, I'm unemployed and don't go by any other name.
head isn't partial in classy prelude—it's using the version of head from Data.MinLen from mono-travers able, iirc
That's the most prominent one, but there are others. I'd have to run down a list, probably riffing off `safe` to figure it out.
https://www.reddit.com/r/haskell/comments/44inwo/haskell_is_not_trivial_but_its_not_unfair_like/czqgwtq?context=3
I agree so strongly with you. Which is why I use Emacs for everything. ;) 
&gt; I guess my point was that the style Snap is written in is easier to grasp if your coming from Scotty. I've used Snap in production. Pretty big app, about 25k LOC. I've worked on similarly sized projects in Scotty, Yesod, and Happstack. The recommendations are not based on a cursory poke around. If you don't believe me, ask the Snap maintainers about how I was putting them through the wringer over the Snap 1.0 update. Yesod is definitely not easier than Scotty. We use Scotty, not Yesod, for the web app examples in the book for instance. However, if you need a more fully featured web framework that can do ordinary web views &amp; HTTP APIs, I think it's the best mature option right now. Especially for beginners who'll need examples to get anything done anyway, which Yesod is tops for right now. The Yesod scaffolds in stack-templates help a lot with that as well. I don't think Yesod is perfect. I don't think that matters for the parameters I care about here.
To be fair you could say that about concatenating strings too, and then we would not have monoid. ;) 
&gt;&gt;&gt; Oh God. Not emacs. &gt;&gt; *...helpful posting explaining how to adapt emacs to your needs...* &gt; I don't like your attitude. That's ok... I don't like yours either... ;-)
touché!
That looks quite laborious. Why do you have to list commit ids for packages that are bundled with GHC anyway? Fwiw, I was able to build a few of my simpler projects which didn't depend on packages still broken with GHC8 via plain cabal with far less hassle. 
What's your cabal version?
I'd like to make one someday, I'm currently planning on tackling the gameboy, but learning how it works is quite time consuming for me and most of the guide are in C, JS or similar so it takes some effort to translate to functional style. I'd recommend using sdl2 as well. I have some boilerplate for it ([here](https://github.com/soupi/chip-8/blob/master/src/Runtime/Run.hs) and [here](https://github.com/soupi/chip-8/blob/master/src/MySDL/MySDL.hs) ) from the emulator [I linked a few days ago](https://www.reddit.com/r/haskell/comments/44fnq2/an_emulator_for_the_chip8_system_written_in/). Good luck!
How about optparse-applicative for command line args handling?
I'm sorry, but that intuition isn't quite right. `id &gt;&gt;= const` and `(+1) &gt;&gt;= (*)` are both perfectly valid Monadic code, unless you want verbs to also be adjectives. In English-ish, a Monad is a Functor where you can squash multiple nested layers together, or produce new layers from thin air, and the order you chose to squash the layers doesn't affect the outcome when reduced to a single layer, and the layers you produced for free don't do anything when you squash them. Does that make any sense at all?
&gt; you can just (use CUA keys) change to the scheme that everyone uses in every OS I know (C-c to copy, C-v to paste, ...) That's a typo, right? You meant Commad-C to copy and Command-V to paste, right? Or is it right-mouse-button and middle-mouse-button? But seriously, it's fine to redefine things in Emacs to be "more familiar" as a beginner, if you feel that will make it smoother for you. But you should be aware - to really enjoy the full power of a tool, whether it's Emacs, Vim, IntelliJ, or anything else, you'll eventually want to learn to use it the way it's designed to be used. It's up to each person to decide what the most comfortable way is to get there. For me - I'd rather work a little harder for a short time at the beginning, and get a lot more productive faster.
&gt; Haskell is definitely behind the state-of-the-art in development environment. For sake of completeness: this has been identified by the Haskell community (through a survey by FPComplete) as one of the two *biggest* pain points. The other one was build tooling, which the `stack` project was made to address. Now I feel that stack has improved build tooling beyond what exists in other communities! And it is the currently in pre-alpha [`haskell-ide-engine`](https://github.com/haskell/haskell-ide-engine) that should address this pain point. May this tool also exceed our expectations :)
I can assure you that as a *older person* with a german keyboard setup I neither want to nor use some of the emacs keybindings ;) The `M` key is in a very bad position - for example `M-b` forces me to push the pinky on the left hand way back while crossing the other finger ... the right ALT key on a german keyboard is mapped differently and you need it a lot IMO in the end it's all about what is right for me ... if emacs would force me to use it's bindings I would not bother to use it ever
Ah... Why is [this](http://hackage.haskell.org/package/composition-extra-2.0.0/docs/Data-Functor-Syntax.html#v:-60--36--36--62-) a thing?
I guess, I had to do it to force stack to use the version I wanted. I guess stack does not pick the bundled ones or it is somehow hardcoded for some specific versions.
This is great! Now we can say: "Have a burrito day!"
But concatenating strings are all basically the same monoid.
Cavet, ugly ugly duckling isn't quite the same as ugly duckling, but ugly duckling implies ugly ugly duckling. Anyway, if you want to make this rigorous, construct the category of descriptions, with the arrows being reverse entailment. (For example, there is a single arrow from "animal" to "cow".) Since " duckling" is implied by "ugly duckling" (return) and "ugly ugly duckling" is implied by "ugly ugly duckling" (join), we might say ugliness is a monad. EXERCISE: Proof the monad laws for ugliness. EXERCISE: Embed Hask into this category, &lt;s&gt;with the type "A" being mapped to "isomorphic to A".&lt;/s&gt; EXERCISE: Make this category more rigorous. 
M'nad.
It is indeed a possibility. I tried to make a version without intermediaries, since `thaw` makes a copy. I don't know much about `vector`'s internals to be sure about speed.
The passage could hint at a pattern, but it does not matter, as it is completely off topic and only concerns a very small minority of the readers, and is off putting to the rest. OP can write a reddit thread or a blog post if he wants to say something on the topic.
hehe... yeah I use the Colemak keyboard layout... all the keyboard shortcuts are often in weird spots. :) Luckily zxc are all in the same spots.
haha :)
Haha - it is really scary when you open Org mode etc... in Emacs after month, year or so, think about some action and your hands suddenly do some awkward dance on your keyboard. Other time, when you get frustrated because your compiler is rejecting your code, you have to edit a lot of things, your hands might fail you. All of sudden you don't know howto move because more you do, more errors you get from compiler so your hands are like scared puppy unable to move. So you have to carefully read error messages, move slowly, until it all comes back to you and your hands start dancing again. Anyway, swap caps lock with ctrl or use [ergo emacs](http://ergoemacs.github.io/)
Have you tried the IntelliJ `Haskforce` plugin? It's a fair bit more advanced than the official Haskell plugin and supports a lot of the core things that make IntelliJ what it is (Refactoring, jump to definition, etc). It's still not perfect and quite far behind the refactoring support for Scala and Java, but it's still better (IMO) than Emacs.
You might want to use [inline-c](http://hackage.haskell.org/package/inline-c) or [language-c-inline](http://hackage.haskell.org/package/language-c-inline) instead of FFI. It allows you to put multiple C calls into single block which is then called from Haskell and you don't have to declare any foreign function - you just include C header and write inlined C into quasi quoter.
oh I tried the usual suspect (CAPS right?) but alas the fingers enjoyed but the brain turned out to be to old to learn new tricks
But you can use `V.unsafeThaw . V.fromList` as long as you do not expose immutable vector (in case of lazy loading from file).
The arrows are reverse entailment.
Thanks. I will try it
This is quite similar to my [`interpreters`](https://github.com/ocharles/interpreters) project that I'll be announcing in the next few days. Maybe have a look at that and see if it's related to what you're talking about? I think it is.
[removed]
Exactly my experience. With happstack you can use your experience from other languages/frameworks (Tcl and C in my case) and get somewhat productive in no time. Then you can peek other libraries or build your own (this is Haskell, after all, known for easy of libraries building) and get even more productive.
I'm not exactly sure if you are joking or not, but haskell-mode is about to migrate away from regexp based font-lock because, well, it is too limiting for Haskell: https://github.com/haskell/haskell-mode/issues/1131 
I find this idea better: https://medium.com/@brianwill/making-semantic-highlighting-useful-9aeac92411df
Ok, it doesn't do that, but that was unclear from your original question. I wasn't so much proposing my library to you, but rather I thought that the approach in this might be transferable to what you're doing. Note that my project doesn't require walking up and down the stack, we abstract that out with `mtl`-like type classes and infer the necessary lifting. As I said, I hope to announce it later this week, so I'll try and clarify exactly what it does and doesn't do then :)
I'd advise you to use the same resources as everyone else: - Learn you a haskell for great good (learnyouahaskell.com) - the real world haskell book (also available online) - http://dev.stephendiehl.com/hask/ more formal: the haskell report (https://www.haskell.org/onlinereport/haskell2010/) 
I hear this is a good resource to start learning Haskell: http://learnyouahaskell.com/chapters (And you can buy the book if you prefer) As an experienced C programmer you might be able to skim read through some of the chapters but I think most of it will still be relevant since Haskell has a very different paradigm to C. I hope this helps! :) EDIT: Looks like /u/epsilonhalbe beat me to it...
Worth noting that Freer doesn't require Functor constraint, and that the Eff monad based on Freer is quite structurally different, and might not enable these same things.
If it were possible with freer, then these will be possible with `free`: data Lanny f r = forall x . Lanny (x -&gt; r) (f x) instance Functor (Lanny f) where fmap f (Lanny g fx) = Lanny (f . g) fx separate :: Free (Sum (Lanny f) (Lanny g)) r -&gt; Free (Lanny f) (Free (Lanny g) r) 
Stephen Diehl's page is probably the closest thing I've seen to a comprehensive coverage of Haskell features outside the GHC spec itself, which isn't designed for learning at all and doesn't cover any third party libraries. Probably save it until you are already familiar with the basics though. And really, be prepared for almost nothing from C to transfer over. You'll be a little more prepared for the types than someone coming from a dynamically typed language, and if you've used function pointers extensively that will help a bit too.
I agree with this. I'd start with http://haskellbook.com and use http://learnyouahaskell.com/ and http://book.realworldhaskell.org/ as additional resources for a different perspective. All three are available online. I'd also caution against experienced programmers glossing over the foundational material unless they have solid experience with another functional language.
&gt; http://haskellbook.com/ Thanks.
&gt; Adjectives, when compounded with themselves, are still what they were originally. If this stuff helps you understand, OK, but I wouldn't run out and start trying to sell people this analogy, because that's not really true, and you'll confuse people. In English, doubling an adjective would generally be understood to be an emphasis; an ugly duckling is a duckling that is ugly. An ugly ugly duckling would be a duckling that is ugly even by the standards of ugly ducklings, although most people would probably not mentally expand that and just consider it an emphasis. Further, I'd suggest that you may still be confused, because your metaphor _suggests_ to me (but does not prove) that you may be conceptualizing join as always somehow just taking the inner bit of `M (M a)` and returning it. This is not true; consider the simple case of: Prelude Control.Monad&gt; join [[1, 2], [3, 4]] [1,2,3,4] `join` did not simply unwrap one of the lists; it did "work". It is true that a lot of the simpler `join`s may appear to just be "ignoring" the layering and that `join` "simply" unwraps the nesting, but that's just because that's true of the simpler ones. I find it helpful when trying to understand monadic computation to make sure that whatever you are doing works on lists. A lot of these sorts of metaphors first break down there. (Technically I suppose it would be better to say "it ought to work for the free monad" but that's just begging the question.)
At the ends of the chapters there are "recommended readings" with links to articles and things that go in even more depth and word things differently.
&gt; Easy as http://fpchat.com/ Well, but then you have to register.
Monads are largely used to abstract control flow and add context to operations. Adverbs are probably a better fit.
&gt; I'd also caution against experienced programmers glossing over the foundational material unless they have solid experience with another functional language. And The Haskell Book is very good in explaining why you should not gloss-over-fundamental-stuff when learning Haskell. I really believe this "glossing problem" is one of the main reason why learning Haskell has a *too* steep learning curve for some.
The wiki book on Haskell is also very good: https://en.wikibooks.org/wiki/Haskell
Consider taking a detour through [Rust](https://www.rust-lang.org/) to help bridge the gap between C and Haskell. Specifically look at Rust's traits and error handling and then you'll have a much easier time absorbing Haskell.
RWS, 100%. http://book.realworldhaskell.org/
Came here to say this, I'm as happy as frustrated to not be the only one to think this :)
This is a special case of monads that doesn't really come up in Haskell, actually. In type theory, there are type modifiers called modalities. An example of a modality is "necessary", or "possible". Possibility is a kind of monad: If A is True, then A is Possible. If Possibly Possibly A, then Possibly A Necessity is a kind of comonad: If A is Necessary, A is true. If A is Necessary, then it is Necessary that A is Necessary. This was spelled out by Moggi in '91. He distinguishes between "computational monads" and "T-modal operators" which are special kinds of monads. It's towards the end of this important paper: http://www.disi.unige.it/person/MoggiE/ftp/ic91.pdf Moggi used monads to describe *semantics*: using monads to explain the meanings of side effects in a lambda calculus. Wadler demonstrated the use of monads as a design pattern in this paper: http://www.eliza.ch/doc/wadler92essence_of_FP.pdf
&gt; Btw, you make a difference between the language spec and compiler features. Yeah, the Haskell2010 standard is seriously outdated wrt current GHC, and many (even popular) libraries use additional features. Nobody is particularly proud of this, but the pain has up to this point not been strong enough to do something about it. A new standards committee has been formed semi-recently, so there is hope. Aside: It may be comforting for a C programmer to know that the Haskell standard leaves *the freakin' evaluation order* partially undefined. Wouldn't want too much certainty in your programming language. ;)
I am the author of this (wip) library. This is not so much self advertisment but rather me asking if this is interesting to you and if you'd like to help me out a bit as I am still new to Haskell. The reason for this library is that I think Haskell would be perfect for the kind of work people do with R, but there was nothing that came close to the functionality and ease of use, of lets say "RStudio". If you have any question feel free to ask!
I mean that it is easy to get help any time of the day, very active channels. The contributors are also very thorough and provide well thought out answers.
`servant-client` is definitely not beginner friendly. `parsec` has a terribly outdated API. I remember being confused a lot because of the conflicts with `Control.Alternative`!
When I learned Haskell, I found the Haskell Report helpful. https://wiki.haskell.org/Language_and_library_specification
&gt; Start with Pierce's Types and Programming Languages (TaPL) instead of something Haskell-specific. Trying to understand the reason for downvotes. Is the suggestion inappropriate (for a newbie, like me) or is it a downright wrong suggestion.
I don't think you'd want to reason about lazy IO with just the non-strict requirement, would you? Not that you should use lazy IO anyway.
Looks nice! Just to mention that the IHaskell notebook has `Chart` and `diagrams` compatibility, being a nice option for live viewing of plots.
TaPL is a great book if you want to learn about... well.. type systems and programming languages. But if you want to learn Haskell, it's not a very good resource. It focuses on the lambda calculus and System-F. The code examples are in OCaml, which is *kind of* similar to Haskell, but you'd be way better off with the other, haskell-specific suffestions from this thread.
&gt; Control.Alternative Do you mean `Control.Applicative`?
TaPL is good for more in depth look at the type systems, but have little more than nothing in common with a decent haskell tutorial. For a book, I would recommend Thinking Functionally with Haskell by R. Bird. I didn't know about http://haskellbook.com, but Bird's book seems to be more finished than this one and written by a widely renown expert. If you are lazy like me about reading books, I would highly recommend you Wadler's original papers on monads, than papers like http://www.cs.nott.ac.uk/~pszgmh/fold.pdf on some basic stuff won't hurt your mind in the beginning. Papers we love repository contains links to interesting papers, you must feel comfortable reading them after you grab the idea of recursion and monads. But don't waste much of your time on reading books and papers, better learn how to use the haskell wiki as soon as possible. Edit: I feel guilty not to mention SICP in a book recommendations thread.
&gt; All three are available online. Actually only the last two titles you mention are available online, the first one is not freely available. I'd also recommend [Simon Marlow's very valuable book](http://chimera.labs.oreilly.com/books/1230000000929/index.html), as well as Stephen Diehl's survey.
It's a bit the equivalent of telling somebody who wants to learn to play an instrument to first work through a physics book about wave mechanics...
Could you why this is related to algebraic theories? I don't see it. EDIT: I mean OP's question specifically. I know how effects and handlers are related to algebraic theories! I don't see how OP's desired functions are possible even when restricted to monads generated by algebraic theories.
I'd compare it more to a book on crafting the instrument.
Yes this works on any operating system that can run web browsers with WebSocket support.
Am I the only one thinking _Write Yourself A Scheme In 48 Hours_ doesn't really fit the pattern? It's grammatical and doesn't at all seem to be a joke.
Thanks. Could you maybe help me understand how to be sure of that? Also, the other signature that returned `Eff r (m a)`, I assume the same is true for that one?
Is that atom? Looks nice!
As an aside, 78.0% productivity is actually quite good, you can easily get below 20.0% by doing concurrent allocations :-) Simplest thing you can try is increasing the initial heap size, maybe try `./pgm +RTS -H1G` etc. (you may need to compile your program with `-rtsopts`) (NOTE: I haven't look at the code, these are just general guidelines) When I use `-H1G` the productivity goes up from 83.2% to 99.4%.
please don't see the downvotes as some kind of punishment - it's just that the other answers seem to be more appropriate and should therefore come first
Self advertisement of one's work such as yours should be the norm.
Right, you can try `Data.IntSet` if you use `Int` instead of `Integer`, it should be super fast compared to the generic set. In fact, I just tried, the runtime went down from 0.50s to 0.09s.
&gt; Which part of servant-client have you found unfriendly to beginners? The type-level magic. Yes you can cargo cult the examples until it does what you want, but understanding what is going on is clearly harder than using `wreq`.
Yes thanks!
Also a language with such concise and minimalistic syntax is great for interpreter one liners. Yes ease of use is the number one priority for me in this project.
It'd probably be a good idea to mark 0.10.0.0 as deprecated on hackage.
I disagree that functions are verbs. In FP, functions are proper values and I think nouns should be used for them as well, but hardly anyone does. I think trying to import this distinction from imperative programming is what confuses a lot of beginners. I also don't think Monads are adjectives, if you are talking about `M a`, `M` is the type constructor, so it shouldn't be an adjective unless you are ready to call all types as such.
The structure you're "join"ing is contravariant and the part you apply your function to is something you'll only have after the "need" (the argument to the function) is satisfied. However, since the whole point is to write your code in terms of something you don't yet have but will by the time the terms are evaluated, Reader is entirely about collapsing the satisfaction of hypothetical function arguments down to a single entry point without worrying about manually carrying it around and applying it yourself. Reader is kinda weird if you pause to think about it. We devoted a whole chapter to it, for this and other reasons. Gets weirder still when you realize the bind in Monad is `flip (&lt;*&gt;)` and that the Reader Applicative by itself is turing complete (SK calculus) For most people learning Haskell, Reader will be one of the first times they'll write code like this: Reader $ \r -&gt; ... Most Monad instances they would've written up to that point wouldn't have a function waiting for an argument embedded in the data constructor.
Well.. I hope people jump on the boat to get things moving quicker. But yeah I'll definitely keep working on it.
The browser is called "browser-plus" and the terminal I had ghci open "term3"
*A relevant comment in this thread was deleted. You can read it below.* ---- This trope is repeated so often there is risk that some may naively believe there is a broad basis of truth to it. This sort of stuff is the software equivalent of climate change denial. The functional paradigm _is_ impressive in how it addresses the essential complexity of software as well as its ability to minimize the accidental complexity of design. Impressive, applied here, does not entail "is the definitive silver bullet solution". Just what the word means, impressive. Are there knowledgeable software developers, who after years of repeated deployments to scattered software wars, which all too quickly degenerate into brutal hand-to-hand combat with essential and accidental software complexity, who then earnestly view the functional paradigm as an unimpressive weapon? Sure, but the preponderance view it as their gun in a knife fight. [[Continued...]](http://www.resavr.com/comment/haskell-not-trivial-but-2736595) ---- *^The ^username ^of ^the ^original ^author ^has ^been ^hidden ^for ^their ^own ^privacy. ^If ^you ^are ^the ^original ^author ^of ^this ^comment ^and ^want ^it ^removed, ^please [^[Send ^this ^PM]](http://np.reddit.com/message/compose?to=resavr_bot&amp;subject=remove&amp;message=2736595)*
Haskell doesn't have split records, so you'd return a mostly identical nested structure with a new pointer for a new copy of whatever structures you touched. A { b :: B, blah :: String } B { lol :: String} AIUI - Updating B in an A copies a new B, returns a new A with the pointer to a B of the new B, but leaves everything else in A alone.
&gt;Well... just don't mention it in a beginners book then. Just introduce the type as (a-&gt;b) -&gt; a -&gt; b. It's not like a beginner is likely to run into a situation where it actually makes a difference. That's what we'll do if it doesn't get elided, but we can't really leave anything unmentioned because then people will email us. We did similar for FTP. Mentioned it, said we'd explain it later, then concreted the types in the examples.
You're misusing the word "fake". What you really mean is "mock". You have a Mock[Rolex], which is a substitute for a Rolex. If you mock the mock Rolex, you can still treat it as a Rolex and give it to someone who is expecting a mock Rolex. Don't take the analogy too far, it's just as much an analogy like the analogy that classes are nouns and that Bicycle is a class.
Oh, okay. "You have a Mock[Rolex], which is a substitute for a Rolex. If you mock the mock Rolex, you can still treat it as a Rolex and give it to someone who is expecting a mock Rolex. Don't take the analogy too far, it's just as much an analogy like the analogy that classes are nouns and that Bicycle is a class."
I've never programmed in Rust. The distinction I was making is that Ruby is pure OOP and Haskell is pure FP. Ruby reads like English. Haskell reads like Math. Give me a break, I'm 22.
Oh god. -- If you map an ugly cheeseburger to an ugly ham and cheeseburger, it's the same as mapping an ugly hamburger to an ugly ham and cheeseburger - the order of ham and cheese do not matter. -- If you take a hamburger and uglify it and then you un-uglify it, you get back the original hamburger. -- I don't know all the Monad laws this is just an analogy. I might be mixing up the hamburger analogy though. It might be that if I have an ugly future hamburger, it's the same as a future ugly hamburger - I am expecting it later on and it is shrouded in ugliness. Like I said I just started Haskell yesterday.
Not really, "Promised ring" describes the ring, that is correct. "Promise of ring" describes the promise, not the ring. The first noun is the subject of any phrase, not the second. It becomes clear if you add the article: "The Promise of ring is returned". What was returned? The Promise. EDIT: "Promise of ring" is the same as "ring promise", not "promised ring".
Ignore the below example. So if you have a Future[Possible[Int]], you can squash that from a Future[Future[Possible[Int]]], or you can add another layer on top. Example 1: &gt; You can say have a burrito of corn on the cob. Simply wrap the corn on the cob in a burrito and it becomes a burrito of corn on the cob. Unwrap it to get back your original corn on the cob. "burrito of" is an adjectival phrase, it is equivalent to an adjective that describes the corn on the cob. A burrito-d corn on the cob is a corn on the cob that has been wrapped in a burrito just like a promised corn on the cob is a corn on the cob that has been wrapped up in a promise. A burritod corn on the cob that has been wrapped in another burrito can still be made into and used as a burritod corn on the cob. ^ This sounds a lot like compressing redundant adjectives together.
Yes. That fails too. To do what you want every monad would have to be able to commute with every other monad.
I was thinking of it like an I-owe-you. If I'm talking about the promised ring that I was hoping to get, it's as if I have a promise of ring. The grammar isn't exactly perfect, but as far as analogies go this one isn't half bad.
The duckling isn't a monad. It's "Ugly[Duckling]" like "Future[Int]". "Int" doesn't imply "Future[Int]". I don't really know what I'm talking about, but I think you misunderstood.
What is reverse entailment. Also, if I have an Ugly[Future[Cheeseburger]], is that the same as a Future[Ugly[Cheeseburger]]?
No no no. You feel burrito-d. You feel the way you would feel if you were wrapped up in a burrito.
Awesome! I'm an engineer at Plotly so it's always great to see it being used in non-enterprise situations since it was open sourced! I'll definitely be sharing this with the office - I seem to be the only one Haskell-inclined there, but I'm sure they'll appreciate it. Edit: If there's enough interest, I'd *love* to convince the higher ups to let me make a library/wrapper for use with IHaskell notebooks. I work with Javascript every day so it would be **amazing** if I could spend some time writing Haskell and getting paid for it. 
You can only talk about squash order invariance when you're only dealing with one Monad. Arbitrary Monads neither compose nor commute - something of type Future (Ugly Hamburger) is necessarily not the same as Ugly (Future Hamburger) unless Ugly is a type synonym of Future, or vice versa. Even if we are talking values, and Future and Ugly are data constructors from the same Monad, they still don't have to squash to the same thing, because they're different. Taking your cheeseburger example and tweaking it a bit, it's not about mapping. Suppose you have a value `Bacon (Lettuce (Tomato Sandwich))` of type `Toppings (Toppings (Toppings BreadProduct))`. Then it must have a 'merge' function that will let you do: join $ join $ Bacon (Lettuce (Tomato Sandwich)) = join $ BaconLettuce (Tomato Sandwich) = BaconLettuceTomoato Sandwich ... *and* ... join $ Bacon (join $ Lettuce (Tomato Sandwich)) = join $ Bacon (LettuceTomato Sandwich) = BaconLettuceTomoato Sandwich Which is of type `Toppings BreadProduct`.
This may help: https://github.com/codebje/ihaskell-docker You'd need to install the required Haskell packages to take advantage of `Chart` and `diagrams`, which you can either do by creating a customised Dockerfile, or `docker exec &lt;docker id&gt; stack install diagrams`, and you still need to sort out how you want to expose the service; if you use `docker run -p ...` be sure you either set a password or bind only to localhost. 
What kind of issues? I'd like that to know as a user of io-streams.
&gt; V.unsafeThaw . V.fromList Thank you everyone.
Perhaps you missed the joke? Or perhaps there's a joke here I'm missing.
It's available online. It's not free. It's worth the money
I was going to use elerea but my lack of Haskell knowledge left me confused. Would you say Reflex is better? And would you know of any good tutorials or source code I could look through?
may as well write my emulator in C in that case :(
I'm in the same position with my project. Great thing about Haskell is that aggressively refactoring code won't get you into a hole you can't climb out of, so you can reiterate on it gradually as you learn better techniques and libraries. At least that's what I do. I prototype things quick and dirty in IO with very simple and dump data types like Int and String, and once I've got an idea of what the solution is going to look like, I factor out the purely computational stuff into pure functions and then lock it all down with smarter types. Rinse and repeat.
I feel you bro! I mean, if only there was *some* way to construct programs without these hotkeys I could finally master this Haskell thing, but I just can't without my Ctrl-B. Plus if I can't see the code in vibrant full-spectrum technocolor, it just doesn't make sense to me buddy, so this Haskell stuff is really hard to look at. I'm also trying to do some *serious* development in Haskell, but just can't without auto-complete. It's impossible to remember those ridiculously complex function names like `null` or `head`. You're preaching to the choir my friend!
You mean [String] as in a list of String? 
Ahh, that does make more sense. The usual selling point of C++ is low-level performance, which is definitely not something I'd expect from Ruby. :) But yeah, I get what you meant now.
&gt; Now, is Stackage LTS 5.3 going to adopt this, or...? Nope. LTS Haskell has a policy of only minor version bumps for packages for a given LTS series. So LTS 5.* is pegged at aeson-0.9.*. We anticipate that LTS 6 will include aeson-0.11. It will take some time for everyone on hackage to bump their upper bounds, so I wouldn't expect to see aeson-0.11 in the nightlies for another week or two.
In general, monads don't commute.
don't take this the wrong way, i really love haskell and it's community. However, despite how people have a tendency to nod and say "yes everything is possible", not enough people have actually set out to do them. The magic combo of FRP &amp; gamedev is a particularly bleeding edge space right now. Everything I've heard says Reflex is the most performant and best designed frp framework currently available. reflex is under heavy development and there isn't a great learning path that i've come across .. anybody else feel free to chime in. but maybe this ghcjs demo could be encouraging. Towards the cochleogram thing is probably a decent example of realtime graphics. EDIT: demo - https://www.youtube.com/watch?v=MfXxuy_CJSk&amp;list=PLEDlMyo0X5hdjvlBdznEuTlhyxHHI477N
That's in now.
Hear are some small errors in your analogy: Take for example `Maybe` or `IO`. `Maybe (Maybe a)` has two modes of failure: `Nothing` and `Just Nothing`. `join` sends both `Nothing` and `Just Nothing` to `Nothing` and `Just (Just a)` to `a`. `Maybe (Maybe a)` is slightly more expressive. Likewise, `IO (IO a)` is an action that returns another action. This can be used, for example, for callbacks. `join` says to run `IO (IO a)` to get `IO a`, and then to run `IO a` immediately afterwards to get `a`. `join` is useful. If I have a bunch of layers of `Maybe`s, I want to mush all the all the failures together. If I have a bunch of layers of `IO`, I want to run the layers one after the other. Also, monads do not commute (generally). `Maybe (IO a)` means I might have an IO action, or might not. I know this before I run anything. `IO (Maybe a)` means I have an action which might return an `a`, or might not, but I'm not sure until I run it.
Argh i've just been tinkering with something similar, using web microservice and leveraging javascript's extensive plotting/datavis ecoysystem to leapfrog charting capabilities. I agree with the sentiment about R. A lot of what's there should be doable in an improved way in Haskell. By the way, regarding IHaskell and IPython notebooks, as someone who works with data a lot, notebooks suck for real data analysis. They're great for tutorials, but for real analyses you often want to experiment with different combinations of composable data transformations. The FRP concept like R/Shiny is closer to what's needed, but it's still not perfected. A better solution would be FRP whose structure is hot swappable at runtime, so you can do exploration work at the same time you the dependency handling is taken care of. One thing that could be improved here is to make plots composable somehow ala ggplot2, d3, or haskell chart/diagrams. It seems like the json data is composable, but that's been abstracted away by the API. 
That is not the purpose. In FFI, to call C library function, you have to declare its type and howto pass values between C and Haskell. That can get very tiresome even for small C libraries. TemplateHaskell can be used to avoid all that work so no FFI is necessary and you can write Haskell code that is using C library (eg. your OpenGL or SDL) to implement primitives much faster. [https://www.youtube.com/watch?v=52yvHv_Ahvg](https://www.youtube.com/watch?v=52yvHv_Ahvg)
All the up to date information I can find is at the following links (and in pages linked from these): https://ghc.haskell.org/trac/ghc/wiki/ExtensibleRecords https://prime.haskell.org/wiki/FirstClassLabels There's also [Jon Sterling's `vinyl`](http://hackage.haskell.org/package/vinyl).
Fair enough, and really what I expected. Just a shame that 4 had to be dumped for 5 so quickly because of aeson 10, just a couple of weeks ago; but I imagine it'll be a while now before 6.
That's creative, but it sounds a bit confusing to me. I could probably read the names `Maybe` and `Integer` (or even something much longer) faster than it would for me to mentally compute, based on two colors, the (possibly partial) inverse of a multilinear map associated with the color of `Maybe` as it is applied to the color of `Integer`. It would probably be frustrating to me to know both that there is a pattern and that it is a pattern that would likely be difficult to figure out. I feel like I would keep trying to figure out the patterns, maybe subconsciously, which could be distracting.
You should absolutely be able to have a Some (None) as a value, and it's Some (None). It's not a Monad if you can't have this value exist. It's saying you optionally have an option. However an optional option can sensibly be squashed into just an option - that's the Monad part.
The sidebar heading list also doesn't show the hierarchy, which is confusing. The first heading my eye fell on was "I wanna write my own!" which made no sense until I discovered it was under the binary serialization section.
I'll try to work on it. I've been looking for a Haskell project, but I also don't have a ton of free time. 
But then many people will be strongly interested in 0.11 because of the performance improvements (which was one big reason why 0.10 was released).
Ctr-B works in the HaskForce Haskell plugin, but only for functions in your project (See setup instructions here: http://stackoverflow.com/questions/35259912/how-to-get-haskell-documentation-to-appear-in-intellij?noredirect=1#comment58272877_35259912 .) IntelliJ also has a HoogleIt plugin that searches Hoogle when you press "Ctr-Shift-H". Also, the regular IntelliJ plugin for Haskell has auto-complete for standard library functions like "putStrLn", but the IntelliJ Haskell plugin with auto-complete is not compatible with the Haskforce plugin with Ctr-B. I had to pick between Ctr-B and Ctr-Space and I picked Ctr-Space.
Someone else is going to probably write up something way more concise, but just incase this is enough, I'll write something here. I think of the IO monad as a track runner. The baton gets passed from main down to the first IO effect and that effect has the baton and is free to run and do his stuff. When he is done, he passes his baton down (through either &gt;&gt;= or &gt;&gt;) and then the next IO effect can run. This is sort of how order of operations is enforced with the IO monad, by the passing of this baton, though I know its more complex than that. The reason that the "putStrLn "lala"" only prints after you type a character and hit enter is because that effect is waiting on the baton to be passed to it and the getChar is holding the baton until he gets his character, so they both wait. The key is to know that &gt;&gt;= and &gt;&gt; both guarantee that this baton is properly passed under the hood and the effects occur in order.
Okay, first, temporarily stop thinking of IO as a Monad, there are two things going on right now, and it will be easier to deal with if you take it as two parts. Haskell aims to be a pure language - no side effects of any kind. But any useful interaction with the world is a side effect. What to *do*? Well, what if the results of anything involving side effects were put in an indestructible vault, that always looked the same from the outside? That's what `getChar` is. We can't examine it, because it's not a string, its an indestructible vault, and it always looks the same no matter how we inspect it from the outside, no matter whats inside. Thus, it's pure. Monads are how we make use the thing inside the indestructible vault, without violating the goal that *anything* that results from a side-effect is packaged in a vault. Don't worry about them just yet. So, what does that mean? `getChar` is a first class *action* to get a character. `getchar &gt;&gt;= (\char -&gt; putChar char)` is a *value* that says 'Take a character from stdin and print it to sdtout, leave () in the vault'. Also, `(\char -&gt; putChar char)` is fundamentally equivalent to just `putChar`. You also don't need most of those parenthesis, function application associates more stronger than operators, but operators are not assumed to be arguments to functions. So this: main = putStrLn "lala" &gt;&gt; putStrLn "lala" &gt;&gt; getChar &gt;&gt;= (\char -&gt; putChar char) &gt;&gt; -- equivalently: getChar &gt;&gt;= putChar putStrLn "lala" ... is equivalent to what you've written. `&gt;&gt;` is takes two actions, and the result is the value of doing the side effects of the first action followed by the second action. `a &gt;&gt; b` is equivalent to `a &gt;&gt;= \_ -&gt; b` or ignoring the value in the vault of a because b is independent of it. IO actually works properly (is a Monad, can interact with the real world, and is still pure) because it's treated specially by the compiler. It's probably not best to try to really understand Monads from it because of this, there are simpler ones. In fact, if you want to understand Monads, I think the best way to start is with Free Monads, because they are Monads *and nothing more*. However, there is no beginner material covering them. For now, if you need to do IO, just use do notation and treat it imperatively. Try not to use anything except input or output for IO, and for file IO, use the ByteString or Text packages, rather than the operations in prelude, as they're not wholly intuitive.
&gt; - http://dev.stephendiehl.com/hask/ This is excellent
I'm a little bit lost with all these solutions ... You mentions in this article the case of [monad-classes](https://ro-che.info/articles/extensible-effects). Why simply not use monad-classes ? What are the pro / con of this solution ? thanks
sigh fine removed it. :P
But then, GHC8.0 will (hopefully) be out in early March (in one month!). And AFAICS there's quite good progress across Stackage to support GHC-8.0, for example `aeson-0.11` supports is GHC-8.0 compatible release!
You can also go through Standard ML. Easier transition than the straight leap. Poulsen's *ML For The Working Programmer* is good. Moscow ML is a good implementation. 
There is also https://mail.haskell.org/pipermail/ghc-devs/2016-January/010966.html &gt; introduction of the DuplicateRecordFields language extension https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields/OverloadedLabels and https://phabricator.haskell.org/D1331 &gt; Implement OverloadedLabels I think that seeing how these work and interact will be very helpful in determining how true extensible records should be implemented.
You probably heard it in the context of Rust, which uses the concept of borrowing to prove (with a "borrowing checker") that memory can be safely allocated and released at certain times because nowhere in the program will there still be a dangling pointer to said freed memory. In effect, the compiler inserts the allocation and deallocation logic into the program, meaning that there is no need for a garbage collector (or, if you want to sound fancy, you could say that the garbage collection is done at compile-time ;) ).
In general, for any given a function, if you don't case match on an argument, then that value will not be reconstructed. Haskell is passing around pointers to bits of memory, so when you don't case on something, the same pointer can be used. For instance, if you have the function pairId1 :: (a,b) -&gt; (a,b) pairId1 (x,y) = (x,y) because this doesn't inspect `x` and `y`, the exact same bits of memory are used in the output for `x` and `y` as are used in the input. However, we did inspect the pair, we reconstruct that, in a new memory location. On the other hand, we could have done this: pairId2 :: (a,b) -&gt; (a,b) pairId2 p = p and since this doesn't inspect the pair, it just gives us back the original pair. What this means more generally is that the entire path down into a piece of data that gets inspect will be reconstructed. So for instance consider the gnarly type `((a,b),(c,d)`. If we write a function that replaces the `c` element like so: swapC :: c -&gt; ((a,b),(c,d)) -&gt; ((a,b),(c,d)) swapC y (x,(_,z)) = (x,(y,z)) the entire spine down to the swapped position is being rebuild -- the outermost pair, and the right pair. But the left pair is reused, and the right element of the right pair is reused. Thus, it depends on how you define your functions! If you don't inspect things, they get reused, if you do inspect, the get rebuild!
Regarding the setup, the README on github is pretty clear. There is also blog post I wrote about installing IHaskell using Stack. It's pretty straightforward once you know the steps: https://github.com/gibiansky/IHaskell http://remusao.github.io/install-ihaskell-on-ubuntu-1404-with-stack.html
Interesting, but I think what I'm vaguely remembering is more likely some but-the-compiler-is-super-smart-honest online discussion somewhere, probably before Rust existed. I had an idea once that I thought of as type mutation - there's [an old question about it here](https://www.reddit.com/r/ProgrammingLanguages/comments/1gyqq4/type_mutation_an_odd_idea_for_imperative_oop/). An answer mentions linear types, which do seem very closely related. I thought Rust was mentioned somewhere, but I don't see it now and it's not as if there's a lot to search. Anyway, I've been meaning to look at Rust for quite a while, but I haven't got around to it yet - I've not read much about it at all. 
What does GHC 8 have to do with the perf improvements in aeson 0.10?
[Here](https://hackage.haskell.org/package/glue-core-0.4.2/docs/Glue-CircuitBreaker.html) is an example implementation.
Data.ByteString.Builder doesn't seem to implement the mixed-endian float format used on some ARMs, so the output may not be portable across different hosts. Endianness is hard.
why don't you just *print* to a `MVar [String]` and then just use this when the client request an "update"?
Why not submit a PR for this? It doesn't look too hard. 
The [injectivity annotation syntax](http://downloads.haskell.org/~ghc/master/users-guide//glasgow_exts.html#ghc-flag--XTypeFamilyDependencies) follows that used for expressing [functional dependencies](http://downloads.haskell.org/~ghc/master/users-guide//glasgow_exts.html#functional-dependencies) quite closely. My understanding is that this same syntax can be used to denote things beyond injectivity (which I believe there are plans for). Unfortunately I can't provide any specific examples at the moment.
I think your confusion has to do with output buffering, rather than anything tricky about the IO monad. &gt;&gt; just executes your IO actions one after the other, so your program is printing "lala" twice, waiting for a character, printing it, then printing "lala" one more time. The order seems garbled in your output because of buffering, most likely, though I can't quite figure out how you're seeing that exact behavior. I would have expected the output buffer to flush after each putStrLn. What compiler version and platform are you using? Do the suggestions here help? https://mail.haskell.org/pipermail/beginners/2011-October/008856.html
&gt; Backwards compatibility only means we need to remember how to read the old format, not that we need to use it by default. Unfortunately I don't think that you are right - when changing default, at the next binary upgrade your Binary instances will deserialise complete garbage into your data types without warnings. I find this extremely dangerous, dangerous enough that writing "Care: we changed the Double instance, your saved data will be serialised into garbage if you don't upgrade with care" into the Changelog is not enough. Especially because it's an instance for Double, over which you have no control, your Doubles may be nested deeply in a data-type-used-by-library-used-by-library-used-by-you. I'm all for backwards-incompatible changes if there's a real benefit (and there is a tremendous benefit here), but at compile time please. I yet don't see a way that this is possible here. The only way I see how this could be done smoothly is if we could somehow detect whether some serialised data is in the old format for Doubles or in the new format. But doing that or a blob of bits may be impossible, unless you can prove that there's a bit (pattern) present if and only if it's one of the two representations :(
This uses an `IORef`, which will probably not do what you want in a multi-threaded environment. OP suggests an `MVar`, which would be better. But I would recommend using [stm](https://hackage.haskell.org/package/stm).
We had an internal discussion about this and we decided that even a major version bump is not enough, since software will break unpredictably and with no warnings, as nh2_ points out.
Thanks, I'll keep this in mind since I predict more work on serialization.
I will include this as part of the tutorial for QuickPlot.
Oh, that makes perfect sense. I hadn't thought about it at the syntax level.
Personal I would not use cereal/binary for any persistent data. If there is a chance that the binary representation might change I would use [safecopy](http://hackage.haskell.org/package/safecopy). safecopy still uses cereal -- and cereal recently change the way floats were serialized. Because safecopy version tags the data, we were able to release a new version that can automatically migrate the old format to the new format. Obviously, you pay a penalty with safecopy - because you have to have extra version tags embedded in the values. I would only use raw binary/cereal for transient data -- such as communication between processes. That would still require some initial negotiation to ensure that all parties are using the same format. One thing that I think would be useful for binary/cereal/safecopy/acid-state would be a system to automatically generate a schema. When writing out a binary blob, I'd also love to be able to write out a little schema file that contained all the types and serialization/deserializations functions need to read that data. That way, even if the original application changes, is lost, etc, there is a self contained file that can be used to read the binary data. From there, you could easily add code to export it to a different format. Given that ability, it would also be possible to create a simple hash to tag the entire binary blob. Using that hash, you would know if your current binary/cereal code can read that data or not. That would be useful in the case of client-server communication to ensure the client is using the lastest specification. Or to allow the server and client to negotiate the use of an older spec. 
Nothing. The implication was that GHC 8 will force a new LTS to be released, which will contain `aeson-0.11`.
I'm curious how you implemented the backwards-compatibility with the cereal format change in safe-copy, did you essentially have to copy-paste the old cereal code into safe-copy?
It's usually very non-obvious. `MVar` exists exactly to provide that guarantee. Concurrency almost always turns out to be far more complex than you originally thought. STM often helps quite a lot.
Just to be clear - this Haskell library works perfectly well. You are asking for a new feature - the addition of some new endianess format for floating point, in order that output from this library can be read by some other libraries on some other platform.
Got it, thanks for the link and the explanation :)
So "&gt;&gt;" is basically just a semi-colon (or comma operator in Java/C++) for monads with side effects?
So "&gt;&gt;" is basically just a semi-colon (or comma operator in Java/C++) for monads with side effects? Also, how does order not matter - if I first print "wait until kettle is boiling" with "do cup related stuff" and then "turn on kettle", wouldn't that not be the same thing as "turn on kettle" with "wait until kettle is boiling" and then "do cup related stuff." ? It seems like logically the parenthesis should affect the order in which stuff is printed. ^ Speaking of printing, how do I make it print to the terminal BEFORE I provide input and how is that not "cheating"? I mean if I print "hello" and then "bye", I might get something different (say a buffer overrun exception) than if I first print "bye" and then "hello". If it always just concatenated everything and flushed it out at once I see how order wouldn't matter, but how do I not do that and how is that not "cheating"?
Pedantic: where the exception type is unit (if the exception type were empty you wouldn't be able to generate any exceptions).
&gt; The reason that the "putStrLn "lala"" only prints after you type a character and hit enter is because that effect is waiting on the baton to be passed to it. ^ So the baton starts with "(getChar &gt;&gt;= (\char -&gt; putChar char))" and all three the other statements are waiting for the baton to get passed to them, at which point they can actually execute??? ^ But how do I make it print to the terminal BEFORE I provide input and wouldn't that be cheating? I mean if I print "hello" (flush) and then "bye", I might get something different (say a buffer overrun exception) than if I first print "bye" (flush) and then "hello". 
On my phone so can't try stuff now, but in this situation my first suspicion is always thunkpiling. Add some more bang patterns in your loops and see if it doesn't strongly reduce the garbage? Haskell really likes to defer arithmetic. 
Oh my god. That makes so much sense. "IO() &gt;&gt; IO() = IO() &gt;&gt;= ( \_ -&gt; IO() )", where "\_" is an ignored parameter. I see how I can change all my "&gt;&gt;" to "&gt;&gt;=" by just adding "= ( \_ -&gt; k )", and it compiles, but I don't see how I can do it the other way around.
Unless you want the worst of all worlds: -- Cannot throw! :) foo :: Either Void () -- Dangit, that’s not what I—ugh. :( foo = Left (error "wahaha") 
The library works perfectly well, but the serialised output isn't readable by other libraries or platforms? Sounds like a broken serialisation library to me.
Binary needs to be sharing the format of its parse 
Simplest counter example is to consider State s along with: magic :: Eff '[State s, IO] a -&gt; State s (Eff '[IO] a) When we unwrap `State s` you get Eff ['State s, IO] a -&gt; s -&gt; (Eff '[IO] a, s) This claims you can read off the final state, but the final state might depend on the IO actions, so this is clearly impossible. You can't pull out all of the consequences on the state in one pass that depend on the IO actions you will eventually handle -- because you haven't handled them yet! The handling of these effects is entangled. Here's an attempt to convey the intuition behind why what you want is impossible: There are monads that only distribute in one direction over other monads in general, or only part way. This comes from knowing how monad transformers are built. Often they come from a distributive law. ReaderT e comes from the distributive law that lets you pull (e -&gt;) out of any functor. WriterT e comes from the distributive law that lets you push (,) e into any other functor. Similarly `EitherT e` relies on the ability to discriminate sums inside of another monad (but unlike the above two examples, it doesn't work on _any_ functor). `StateT s` comes from the ability to sandwich a monad inside of an adjunction. If F -| G and M is a monad then GMF is a monad. This is a special case of composition of adjoints. On the other hand ContT is a different special case. Each of these is a monad transformer for a different reason, but we only get a limited number of "moves". The technical issue is that in general coproducts of arbitrary monads don't always exist. You can throw a bunch of effects into a pile, but you can't always peel them off the pile one by one. One subset we _can_ do that for are the algebraic effects. Moreover, Reader/Writer/State all commute with one another as transformers, but that isn't enough either. Another subset we can do this for with a different notion of coproduct is the form introduced by Lüth and Ghani in ["Composing Monads using Coproducts"](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.8.3581) . They, however, only really give you a way to organize your bigger monad into strata, such that no two occurrences of the same effect are next to each other, but rather the neighboring occurrences get fused. This is a much weaker thing than what you are seeking here.
I agree, I don't think it would be worth it unless you were already very comfortable with the lisp/scheme version.
I'd recommend scheme, you work more directly with the concepts in the book (and scheme is a tiny language). And you won't (easily) be able to implement the Y combinator in haskell.
&gt; So "&gt;&gt;" is basically just a semi-colon (or comma operator in Java/C++) for monads with side effects? When it comes to IO, that's pretty much how you use it. The difference is that you can't do the following in C like languages because semicolons end statements. void main () { turnOnKettle(); ( waitUntilKettleIsBoiling(); doCupRelatedStuff(); ); } To evaluate the above (if the language runtime for this weird C-like language worked like Haskell's runtime) we "action" first instruction, turnOnKettle(). The next instruction expands to two instructions. To "action" it we need to look inside it and "action" inner instruction in turn. These are waitUntilKettleIsBoiling(), then the second doCupRelatedStuff(). We are then out of instructions and terminate. So what got "actioned" was turnOnKettle(), then waitUntilKettleIsBoiling(), then doCupRelatedStuff(). Now consider: void main () { ( turnOnKettle(); waitUntilKettleIsBoiling(); ); doCupRelatedStuff(); } If we apply the evaluation rules again the order that things go "actioned" is still turnOnKettle(), then waitUntilKettleIsBoiling(), then doCupRelatedStuff(). So in terms of the order things were "actioned" and so what happens to the kettle, the brackets don't matter. &gt;Speaking of printing, how do I make it print to the terminal BEFORE I provide input and how is that not "cheating"? I'm sorry, I didn't quite understand the question. To print something before getting input I would main = do putStrLn "Please enter a character" c &lt;- getChar putStr "You entered" putChar c This desugars to: main = (putStrLn "Please enter a character") &gt;&gt; getChar &gt;&gt;= (\c -&gt; (putStr "You entered") &gt;&gt; (putChar c) ) 
It isn't intended to provide that sort of portability, so no, it is not broken.
How would Yuras' code look like with STM?
for one, i would not model the dates as a list. for example: Lang Java [] is invalid. i'd use a fixed-length container type: type DayRange = (Day, Day, Day) or a custom type (more boilerplate but more clarity) data DayRange = Days Day Day Day and iiuc, why use a phantom type, and not an enum? ie data Lang = Java | PHP | DotNet data Row = Row Lang Date DayRange 
[Some guy once said that calculating is better than scheming](https://www.cs.kent.ac.uk/people/staff/dat/miranda/wadler87.pdf) One of the main points of the article is that equational reasoning is much easier in Haskell than in Scheme. 
Learn both languages, at some point, but SICP is really quite opinionated towards Lisp (to the point of being a Lisp book more than a Programming book). And while Haskell is obviously much better than Lisp ( :P ), it won't hurt you to learn some Lisp - the language itself (any dialect of it, really) is tiny, and like every good programming language, it has some insights to teach you that you won't get as clearly from anything else. Coming back later, when you've done SICP in some Lisp, and also learned a good bit of Haskell, and then revisiting SICP and see how far you get doing the examples and exercises in Haskell, would be an interesting endeavor though.
Hartel &amp; Muller's *Functional C* [http://eprints.eemcs.utwente.nl/1077/02/book.pdf](http://eprints.eemcs.utwente.nl/1077/02/book.pdf) 
In this case, it looks like the code was written to carefully use `atomicModifyIORef` in the right places. You're right that mvars and stm have stronger properties. But without having done a careful audit, it certainly _seems_ plausible that this could be all have been done in a proper way.
I studied SICP by using Lua. I wrote the Lisp interpreter introduced in the book in Lua as well. That's pretty straightforward if you understand a bit more about the code rather than pursue literal directness. Haskell would be more different. It sounds to me like a good idea to re-read SICP by using Haskell. Maybe I could get rid of some lambda boilerplate thanks to Haskell's laziness. One advantage I think is this enforce a deeper understanding of the code. It enforces you out of the literal-copying mode, which you could easily slip in even you're using a syntactically different language like Lua.
Never read the book, but watched the lectures. I think you'll have problems as there are problems solved by having lisp evaluate lisp expressions. Haskell doesn't do that for good reasons. That said, there are concepts in there which are worth exploring, but it's not the only source. 
&gt; but I have a much greater interest in Haskell than I do in learning (or even using) Lisp Given your sentiment, learn Haskell and drop SCIP/scheme. You'll get much more mileage absorbing concepts, if it's something that you want to learn obviously.
On a related note: don't use `parsec`'s built-in floating point parsing. Its [implementation](https://github.com/aslatter/parsec/blob/master/Text/Parsec/Token.hs#L541-L547) parses each digit individually, then does repeated additions and divisions by 10 to reconstruct the `Double`. For example, when parsing `0.948`, `parsec` computes `(9 + (4 + (8 / 10)) / 10) / 10 :: Double`. All this floating point arithmetic means you get back the wrong answer: &gt; import Text.Parsec &gt; import Text.Parsec.Language &gt; import Text.Parsec.Token &gt; parse (float haskell) "" "0.948" Right 0.9480000000000001 [`megaparsec`](https://hackage.haskell.org/package/megaparsec), on the other hand, [implements this correctly](https://github.com/mrkkrp/megaparsec/blob/master/Text/Megaparsec/Lexer.hs#L377-L379) by deferring to `read`: &gt; import Text.Megaparsec &gt; import Text.Megaparsec.Lexer &gt; parseTest float "0.948" 0.948 On the whole I find `megaparsec` to be more flexible and easier to work with anyway.
I almost tried reactive banana once. But the examples looked complicated and I have little attention span. I guess thats the price of generality and packing a lot of functionality. My plan is too keep it as simple (and therefore limited) as possible so that users can get going almost instantly. It makes more sense in the context of interactive data exploration where people don't care about complete applications. I'd be interested in how you implemented your server? (similar to mine?) Mine is a bit rigid due to me being used to having state in abundance.
-- This is a lot of information. So &gt;&gt; is a special case of &gt;&gt;= where "IO()&gt;&gt;IO()" = "IO()&gt;&gt;=(_ -&gt; IO())". I tried to replicate the example where you explicitly pass in "GHC.Prim.RealWorld", but when I do "import GHC.Prim" the compiler says "it is a member of hidden package ghc-prim". I'm avoiding the syntactic sugar for now because I need to actually understand what is going on. -- Anyway, guess I learned state monad. Is state monad / state typeclass where the "GHC.Prim.State#" comes from? Is there any way to actually write the example out explicitly with world0, world1, world2, etc (such that it will compile)?
Ahh... that makes sense. So I'm returning this big state Monad and then the Haskell runtime does the rest after I return it. But what if I want to print now - not wait until the main function is returned?
This doesn't print anything visible to the terminal until after I enter a character. How do I make it flush to the terminal so that I can see the output before I enter a character? p.s. I think what it's doing is composing a big object using all the instructions and then passing that big object back to main when main returns. I want it to print BEFORE main returns.
Yeah. So if you had an ugliness Monad and you did Return :: duckling -&gt; Ugly duckling you would have an ugly ducking. Then the ugliness could affect the behaviour of the duckling or enshroud the duckling or whatever, 
&gt; " I'm starting to think of some of them as adverbs. Like this IO monad (GHC.Prim.State# GHC.Prim.RealWorld) that every time you map it (chain, map, whatever) it makes a new GHC.Prim.RealWorld with the changes applied in. It's like the monad describes how the changes are being applied. It's not a World Monad. It's a Worldly Monad - a Monad that dictates the behaviour of the World. That being said, Possible[Int], Definite[Int], Indeterminate[Int] are definitely adjectives. The Int is Possible. The Int is a definite Int - we know it's an Int. The Int is Indeterminate - we cannot get the Int even though we were hoping for an Int to be returned. I added a little addendum saying that I am changing my opinion from just Adjectives to Adjectives or Adverbs (depending on context/behavior/etc)."
I was just thinking the other day that you can think of (&gt;&gt;=) as assigning a callback function e.g. -- someIOFunction :: IO T -- callbackFn :: T -&gt; IO U someIOFunction &gt;&gt;= callbackFn someIOFunction does some IO and returns some value of type T. callbackFn takes that value, does some more IO, and returns some other value of type U. Here is a more concrete example that reads a line from standard input and uses a callback that prints the entered line: -- getLine :: IO String -- putStrLn :: String -&gt; IO () getLine &gt;&gt;= putStrLn Note that the whole statement will have the type IO X where X is the result of the final callback. So here the final callback is putStrLn :: (String -&gt; IO ()) so the whole statement (getLine &gt;&gt;= putStrLn) has type (IO ()) which actually follows from the type of (&gt;&gt;=). If we wanted to ignore the input string and just print "Hello World!" then we can modify the callback respectively: getLine &gt;&gt;= (\_ -&gt; putStrLn "Hello World!") Now you can think of (&gt;&gt;) as the same thing as (&gt;&gt;=) but the result of the first function is thrown out automatically, so the above code can be written: getLine &gt;&gt; (putStrLn "Hello World!") Now by chaining callbacks in this way, you can describe a series of IO actions: getLine &gt;&gt; (getLine &gt;&gt; (getLine &gt;&gt;= putStrLn)) This reads 3 lines and prints out the 3rd one. If we wan't to read 3 lines then (only after reading all three) print out the first line, then we can modify the above as follows: getLine &gt;&gt;= (\firstLine -&gt; getLine &gt;&gt; (getLine &gt;&gt; (putStrLn firstLine))) This can be written with "do" notation which is just a pretty version of the above: do firstLine &lt;- getLine getLine getLine putStrLn firstLine The "firstLine &lt;- getLine" corresponds to "getLine &gt;&gt;= (\firstLine -&gt; ..."
Awwwww... Anyway, you built a very good resource for the community, thanks!
Try `pandoc -f commonmark`. 
Now that's a lot better! Pandoc passes 578/613 tests. Next time I'm gonna read the documentation more thoroughly.
[removed]
Why not use a phantom type? Then you have more type-level information about what you are working with.
Any particular reason why you didn't like optparse-applicative or the [turtle wrapper](https://hackage.haskell.org/package/turtle-1.2.0/docs/Turtle-Tutorial.html#g:17)?
It does when I run it. ;)
Reminds me of the O(n) quantum sorting algorithm: 1. Assume we live in a multiverse 2. Randomly permute the array based on a quantum random number generator 3. Check if the list is sorted. If it isn't, destroy the universe. The quantum generator creates a universe for each possible permutation. Checking that the array is sorted takes O(n) time. If your universe exists after the algorithm runs, the array is sorted. The implementation of 3 is left as an exercise to the reader. 
&gt; How'd you make it? I used a sophisticated text-coloring utility called [LibreOffice](https://libreoffice.org) :P
You can't do it the other way around, somewhere the core behavior of what to do with an effect needs to be defined. To define a Monad in Haskell, two functions are needed, "return :: a -&gt; m a" and "(&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b" Also, you can actually separately define (&gt;&gt;) with only an Applicative instance (weaker than Monad), by using the (*&gt;) operator.
I don't know if you got your answer about making the terminal flush, but here's how you can disable the default line buffering: import System.IO hSetBuffering stdout NoBuffering
Thanks so much for posting this! Your analogy really made things click for me, and helped me solve a problem I'd been having difficulty with for a few days.
http://hackage.haskell.org/package/ghc-prim/docs/GHC-Types.html
Thank you.
Do mixed-endian floats even come up on traditional Linux/ARM under the `armhf` ABI, e.g. when you're using VFP units? IIRC, VFP does not used mixed-endian format for floats, but I could easily be wrong. In fact, I don't even know if GHC will even *work* without the `armhf` ABI (even the batty RPi1 had an ARMv6+VFP), or if it does, we sure as hell don't test it AFAIK - so this particular gotcha might be the least of your problems, to be honest.
You can't do this without hacking `Setup.lhs`, because Cabal/`System.Info` does not track the granularity of the target system as close as you'd need for this. This is only a problem on *some* ARM machines, as far as I can see - not all of them. 
Checking if the list is sorted occurs in each universe in parallel, not one universe at a time (each universe contains a computer running the same code on different data). The program destroys whichever universe it's contained in if the list was not sorted, thus the universes which survive have sorted the array, and then confirmed it with a single pass over the array. 
Randomly permuting the array is O(n) ( see [here](https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle) ). Checking if it's sorted is O(n), destroying the universe is O(1), in that its complexity does not depend on the size of the array you give it. 
Well, keep in mind that the definition of `IO` is an implementation detail. You could also implement it using as a pure computation in a free monad (see `operational` and `prompt` for examples of this sort of library), and then the low level runtime evaluates the computation up to the next instruction which is executed imperatively. In fact, something like this is how `IOSpec` works. For example, here's the definition of `STM` from the [IOSpec library](https://hackage.haskell.org/package/IOSpec-0.3/docs/src/Test-IOSpec-STM.html#STM): -- The 'STM' data type and its instances. data STM a = STMReturn a | NewTVar Data (Loc -&gt; STM a) | ReadTVar Loc (Data -&gt; STM a) | WriteTVar Loc Data (STM a) | Retry | OrElse (STM a) (STM a) 
Extracted from EM radiation remaining from the end of previous universe - //Quantum Bogosort.cpp - &gt; Implement Quantum bogosort Algorythym // (c) Jamie Bean // Dedicated to my brillant mum, Paula
With continuations, you can go back in time, sort of: http://joeyh.name/blog/entry/7drl_2015_day_5_type_directed_spell_system_development/
Pattern matching drives evaluation. Unless you use `~`.
Yakshemash, I lieke dis
&gt;for boring production work. I'd like to see a list of boring libraries that we need, though. 
The universe that survives step 3 has only observed O(n) steps being performed. Any particular shuffle can be performed in O(n) time.
If something is a monad (Ugly) you have a guarantee that for any `a` you can get `Ugly a`. This means that if you have a `duckling`, it's implied that you have an `Ugly duckling`. I don't think I misunderstood.
https://hackage.haskell.org/package/acme-realworld &gt; the ability to save the current state of the universe and restore it later. This provides a way to "undo" certain types of side effects in the IO monad: import Acme.Missiles import Acme.RealWorld main :: IO () main = do -- Save the current state of the universe world_as_we_know_it &lt;- getWorld -- Cause serious international side effects launchMissiles -- After realizing that was a terrible, terrible mistake, restore the -- pre-war state of the universe. putWorld world_as_we_know_it
Actually, at runtime, there is no RealWorld at all. 
Normally seen that with the title of 'optional typing'. 
No. It's not broken at all. It writes and reads its own format. It provides fine-grained capabilities to render and read any pre-defined binary format you want, so it can interact with any application and any platform. It provides convenience combinators to make it even simpler to build those custom formats for many of the most popular platforms. There is one particular platform+format combination that doesn't have the exact convenience combinators you want. It would be very simple to add them - a handful of lines of code. So why not go ahead and just add them instead of complaining?
It works perfectly on every platform so no such flag is needed. It's just missing some convenience combinators you want. That's certainly no reason not to compile. Besides - the platform you are rendering for or reading from is not necessarily the one you are running on, so it wouldn't even help.
First of all - this only affects certain automatically generated instances. People who need a persistent format that will remain consistent over time shouldn't use those anyway. They should specify explicitly the format that they want, which the library completely supports. This is not a library for some ISO standard format. It should be clear to anyone who does use automatically generated instances that the default format for them may change across major version numbers. All that said, I am not advocating changing the default format at this time. We should add an easy-to-use alternative automatic format which is better. After a time, we'll see how people are using the library, and we can decide how to proceed from there.
Good point. I will change it to an `Int`. Thanks.
It could have been called: ShittyBitBeingHereJustToForceSequencingOfComputationsButDontTellItVeryLoudly instead of RealWold
(not an author of this) - I am currently using optparse-applicative and honestly I don't find it nice. What I get is a very long monoid with lots of words like 'progDesc', embedded functors etc. When I tried something more complex, I couldn't sometimes achieve it. What would be very interesting is nesting of commands, e.g. "todo add message", "todo add calendar" etc. I couldn't find an easy way with optparse-applicative. At first sight, this seems to me a very nice work.
I ran into a problem with very similar symptoms while writing [exact-real](https://hackage.haskell.org/package/exact-real). Some functions weren't being inlined when profiling was turned on leading to decreased sharing and the evaluation tree going from being linear in size to exponential. Putting a few INLINE pragmas on the critical path sorted everything out for me.
&gt; I haven't found a parser that I really like. There are so many argument parsing libraries already! But, I remember looking at all the libraries I could find and deciding, like you, to [roll my own](https://github.com/gelisam/hawk/blob/master/src/Control/Monad/Trans/OptionParser.hs). I don't remember what I didn't like about the existing libraries, do you?
&gt; In what "domains" is Haskell normally used. See Gabriel's [State of the Haskell ecosystem](https://github.com/Gabriel439/post-rfc/blob/master/sotu.md), now a standard reference on the subject.
* [xmonad](http://hackage.haskell.org/package/xmonad). * [criterion](http://hackage.haskell.org/package/criterion) can be used for benchmarking even non-Haskell functions, via the FFI. * [diagrams](http://hackage.haskell.org/package/diagrams)
GHC won't unpack recursive type so the pragma does nothing here.
`optparse-applicative` can be a bit mystifying until one reads [this tutorial](https://robots.thoughtbot.com/applicative-options-parsing-in-haskell). I was kind of hoping the [configifier](https://github.com/zerobuzz/configifier) library would get further along than it did.
This looks good and very usable, I'd be happy to see progress on this :-) (disclaimer: I'm the author of Spock)
Nope, I messed up. The link should be fixed now :-)
I'd rather use e.g. `Text`, `Int` is clearly over used in this kind of examples.
Thanks. I wasn't aware of that, but what you say makes sense. I'll update it. 
Programming-languages. Elm, Purescript, agda, idris, and even GHC itself, are all written in Haskell. Edit : Haskell is a language, not a compiler. 
I used a very similar approach back then. We had to find all paths with the same cost in the assignment and had to represent no result as an empty list which means a few changes but my approach was the same. My question was mostly concerned with the idea of transforming the original graph into a general form so that one could more easily operate on it. findCheapestPath :: node -&gt; (node -&gt; Connections) -&gt; (node -&gt; Bool) -&gt; Maybe [node] would probably need to rebuild to graph internally in order to take advantage of existing code that operates on Graphs. If we use functions of the (simplified) types: &gt; AirlineConnections -&gt; GeneralGraph GeneralGraph -&gt; Start -&gt; End -&gt; Maybe GeneralPath GeneralPath -&gt; Flights we could easily and transparently compose these together to get a function that solves our Problem. It doesn't directly give us much of an advantage. We still need a (node -&gt; Connections) Function to convert the graph at least and if we don't have a generic path finding function we still need that one as well. However we could then reuse any reasonable generic algorithm from a library and also take advantage of other functions this library may provide. Without lazieness this is a terrible idea because we would have to convert the whole graph first (Which for large graphs with simple sultions would be really terrible) but with laziness it can actually makes sense to do so! For the singular example it doesn't make much of a difference and maybe it's not the best example to explain my thoughts as well. /r/ezyang 's paper covers pretty much what I was thinking about (and a bit more) when writing this post.
It's a very well written Article that showed quite well how this can be applied. Thank you!
Newtypes have the same runtime representation as the one for the type they wrap, so a bang pattern there indeed doesn't make sense.
A graph is a list of vertices and edges. Each with or without costs. `a -&gt; [(a, c)]` is the list of edges with costs for a given node. If we apply `type GeneralGraph = a -&gt; [(a, c)]` then we can easily translate between our representations: * The first function converts whatever your original data is into a function `a -&gt; [(a, c)]`. * My `findCheapestPath` is your second function with flipped parameters and a predicate instead of a specific node. Yours is just as a good. * The last function is not required in my case, because the return value is already a convenient `[a]`. There is a lot of complex logic constructing the neighbors function specific to your data structures and the problem at hand, but that's outside of the general approach. An example could be a function that looks up the flights according to a weekly timetable + special casing of rare events. There's no need to actually construct a rigid and finite graph. Furthermore, including times in our graph nodes, makes the graph of unlimited size. This is only possible, as you've noted as well, through laziness.
Any others? That's a bit vague and sort of crazy to think there are only two "popular" projects. What is popular? Do they have to be open source or just popular within some community? Darcs, Cryptol, SAW, Agda, Frege, Bluespec... the list can get huge without any real scoping.
Yeah I meant GHC 
&gt; the list can get huge without any real scoping. If you think Haskell is popular in or the most appropriate for or has good supporting libraries for domain X, I am looking for some projects in area X. That is all. (By 'popular', I probably meant 'influential') I am a newbie. So any name that you throw at me will be useful.
There's a very small probability of a cosmic ray flipping a bit that prevents your computer from calling the destroy-the-universe function, but for sufficiently large n this will eclipse the probability of accidentally finding a sorted list. So most of the undestroyed universes are ones where your computer produced an unexpected error, possibly in addition to an unsorted list.
Since it's kinda interesting, let's take an *a priori* look at what kinda features specific domains want. * Specialized language features and optimizations for particular domains (ex. MUMPS, SQL, COBOL, Fortran, APL, R, Octave, Mathematica etc). * Well supported specialized libraries for particular domains (ex. Python (numerical and scientific), Perl (linguistics and molecular biology), C++ (3D graphics)). * Unique or rare language features that lend themselves well to a particular domain (ex. type-safe units (Fortress, F#), constraint logic programming (Oz), negation as failure (SNOBOL, Icon)) * Strong guarantees on correctness (ex. Ada, Eiffel) * Strong guarantees on performance (ex. Fortran, C) * Small implementation (ex. C, Lua, GUILE Scheme) Haskell gets the first for anything that does transformation of data sets, a property it inherits from its ML heritage, and the second. This explains things like pandoc as well as the numerous compilers and interpreters written in Haskell (including the first implementation of Perl 6). In principle Haskell is a good language to write libraries against, particularly with the existence of rewrite rules that basically allow you to extend the optimizer. In practice the good Haskell libraries tend toward the general (pipes, lens, etc) and there are only a few examples of performant libraries of interest to special domains (random-fu, for example). Unique and rare language features there are. Higher kinded types, pure functional programming, and lazy evaluation are all relatively rare outside of Haskell. These are harder to translate directly to application areas than features like COBOL's extensive support for binary coded decimal and pretty printing or F#'s typed units of measure, but lambdabot could be considered an example: it takes advantage of Haskell's purity to avoid arbitrary IO. Correctness is something Haskell does okay, and it can get picked out because of this, although you are going to find better options in things like ACL2 or Fortress. (Realtime) Performance guarantees is not something Haskell should have any problem with in principle, you could implement perfect garbage collection (it's so simple that C++ programmers do it manually, they call it RAII). This comes at a cost of general case performance; when Minecraft hangs for a minute or two to run its GC, it is doing this because over the long term this means less time and energy spent doing that housekeeping, but it is an annoying thing to have to deal with and people might ditch Haskell in the hopes of not making another Minecraft. The efforts to achieve this have taken names like Disciple and Timber and are perhaps so different that you might not think of them as Haskell anymore. As for small implementations, there were a few (nhc98, hugs and gofer), but nowadays that's going to be hard. This rules out applications like embedding Haskell as an extension language in another application. (XMonad pulls off something that looks very much like this, but it does it by recompiling and reloading a user supplied module). All in all this leaves server-side web dev as the one big thing Haskell has a strong case for being used in, in which it has a lot of company, so its strength in that domain is a bit diluted. Embedded and mission-critical systems are generally out because those need the strong guarantees on performance, same with games. Good parsing libraries are a definite boon. ~~For some unique Haskell applications, the one that immediately comes to mind is metasploit, which is a penetration testing suite.~~
It depends, what is your model trying to accomplish? FRP is highly focused on lifting operations to work on a time series. Knowing what your focus is could help a lot in determining alternatives.
I FINALLY got a functional IDE, with auto-complete, pop-up documentation (for user defined functions only), working "Refactor &gt; Rename", and working "Go to Declaration". See the very bottom of my post with the setup for EclipseFP or go to http://eclipsefp.github.io/install.html and perform the "Extra configuration steps". An IDE restart and creating a new project after the Extra Configuration Steps were done was required.
Technically the correct implementation is "0.01% of the time it does nothing" or something to that effect to allow for unsolvable problems to not destroy everything.
[removed]
My approach would be, naively, to create a structure indexed by some Nat `n`, which represents the number of darts. The list of darts itself contains no additional information, so it can be omitted. You can produce a "safe" representation of a permutation in a number of ways -- one would be a vector where each element at position `i` is bounded by `n - i`, and represents where an element lands, assuming that all the elements that prior elements of the vector have already landed at have been _removed_ from the count. You can capture this with a GADT without too much work. You can represent an involution with no fixed point in a similar way, using a vector of length `n/2` where you drop both the pointed _from_ and _two_ elements from the "hit set" on each successive element. I can't think of a nice general way to add the coherence condition for involutions as described in the wikipedia link, so I think that would have to be checked at runtime, or you would need to only have "safe" combinators for building up such maps. You might think these "clever" representations are terrible to work with. But note that with view patterns, we can recover a "view" on them that looks more like a standard list, and only "internally" do we need to store things in the more clever representations.
I suppose you could translate Wittgenstein's dictum into a Haskell class definition: "The world is everything that is the case" http://genius.com/Ludwig-wittgenstein-tractatus-logico-philosophicus-proposition-1-annotated 
Teach me how to make a living programming Haskell oh wise one :) ... Seriously id love to be able to program in Haskell every day.
Semi-related, /u/ChavXO sorry for hijacking the thread if this is not the aspect that interest you... FRP is build upon denotational semantics whereas [operational or axiomatic semantics were unsatisfaing](http://stackoverflow.com/a/1030631). Have somebody tried using other semantics? [Number 5 on this list](https://symbolicanalysis.wordpress.com/2009/12/19/maximum-expressive-power-with-minimal-construction/) namely [Action semantics](https://en.wikipedia.org/wiki/Action_semantics) even promises easier extensibility... Second question, could someone comment how is FRP related to [Control theory](https://en.wikipedia.org/wiki/Control_theory)?
Yeah, that's basically what I do. I use `throwIO` in impure code and `Maybe`/`Either` in pure code.
Hahah. But it is since a few month officially unmaintained :) the irony?
I'd like to promote the concept of functional hybrid modelling. It's a slightly forgotten way of even higher level reactivity. The only example I have is that of [Hydra](https://github.com/giorgidze/Hydra), an embedded language written in haskell and [some](http://www.cs.yale.edu/homes/external/nilsson/Publications/padl2003.pdf) [white papers](http://www.cs.nott.ac.uk/~psznhn/Publications/ifl2008.pdf) and [other material](http://www.eoolt.org/2007/pres/presentation-paper7.pdf) related to its creation. As far as I understand it it is a functional and pure implementation of Dymola and Modelica from the Java world. What I find intriguing about it is the idea of non casual modeling. That is, instead of modeling computation with a given direction of data flow, from arguments to return value, you instead declare how data structures relate to each other mathematically so that figuring out the direction of the computation becomes a separate task, typically for some kind of algebraic solver to deal with.
&gt; However, (library) developers wanted to use ($) not only for types with kind *, but also for those of kind # Ick. Unlifted types are a hack. There's nothing in the Haskell 98 or Haskell 2010 Reports about them. Why couldn't they have something like `$#` for unlifted types and leave `$` alone? In other words, why embrace the desire to use `$` for everything even if it had the wrong type, then make `$` lie about its type, and now give it a really nasty type?
complexity? easier to parse into monomorphic types, you can pattern patch on enums, show them, etc. if you need a phantom type, id just make a "richer" one: refine :: [Row] -&gt; (Row2 Java, Row2 PHP, Row2) or make Row2 a GADT if you want to preserve the order. 
Remember, Friday is "causal Friday". You can wear anything if you have a reason. 
At kind level. With explicit kind annotations, `($)'s` type is ($) :: forall (a :: *) (b :: *). (a -&gt; b) -&gt; a -&gt; b But that's not true since [5dd1cbbf](https://github.com/ghc/ghc/commit/5dd1cbbfc0a19e92d7eeff6f328abc7558992fd6), where it has been changed to ($) :: forall (a :: *) (b :: Open). (a -&gt; b) -&gt; a -&gt; b This happens entirely behind the scenes in the bowels of GHC. The kind of the type variables isn't what you would expect from a regular polymorphic function (`*`).
&gt; There's nothing in the Haskell 98 or Haskell 2010 Reports about them. I believe we left the report conforming worlds a long time ago; at least since the AMP. Keep in mind that GHC's `($)` variant still works in the same contexts as the report's `($)`. It just works for additional stuff that's not handled in the report at all.
[removed]
[removed]
Thanks for the explanations! This makes sense to me and sounds like what I would expect it to be, although I didn't know that pattern matching would force elements even though the structure isn't changed (pairId1). I guess the reusing part is consistent with /u/ninereeds314 's explanation, that you just copy one reference per unchanged field. In any case, it seems that the statement in the link in my post, that everything is copied when you modify data structures in a functional (immutable) language, isn't completely true.
I do believe it is a bit of a joke. Levity generally refers to having high spirits or lifting your spirits. Levity polymorphism is then polymorphism over lifting, and as such it's capable of working on lifted and unlifted types.
its data Liftedness = Lifted | Unlifted but shorter and less awkward. like data Parity = Even | Odd 
I think Liftedness would actually be a better option.
I think it's more that you would have had to write `runST (stuff)` for `ST`, while you could write `foo $ do ...` for everything else that wanted a monadic argument. I don't know that there was a `runST` before `Rank2Types`.
`s2bs = fmap pack` Hardly even worth binding a name for, honestly. `fmap` for Maybe is defined like so: fmap _ Nothing = Nothing fmap f (Just a) = Just (f a) Which is equivalent to: fmap f ms = case ms of Just val -&gt; Just $ f val Nothing -&gt; Nothing Which is exactly your function except parameterised over what function to apply to the contents. Since you're asking this, and functors are usually brought up fairly early in any sort of Haskell introduction (almost certainly before the difference between Strings and ByteStrings), I'd suggest reading a lot more Haskell tutorials, and work your way through [this](https://wiki.haskell.org/Typeclassopedia) as you go.
I agree with your suggestion. The only reason I'm aware of for not defining a new function with the more complicated type, while actually giving `$` the type most everyone thought it had is that some uses of `$` would break. I do not think anyone has published a quantification of this breakage.
Maybe try this: s2bs :: Maybe String -&gt; Maybe ByteString s2bs = fmap pack edit: /u/WarDaft beat me to it
The FHM approach has always interested me because it gives a good story to what happens in the presence of cycles in the continuous time portion of your program. The causal modeling of FRP bothers me a great deal. Alexey Radul's work with Gerry Sussman on propagators also includes some tools for acausal equational reasoning and a chapter on "FRP" that has a bit of an FHM feel to it, but since the ability to reverse equations is purely "local" it needs work to flesh out into a theory I'd be really happy with.
You need to recompile ghc-mod while linking against ghc-7.10.2. Not sure if spacemacs needs you to put the executable in a special place or something though. 
As well as the first implementation of Perl 6!
&gt; Beyond this the general consensus seems to be that the leaves of data structures should be strict, and the spines should be lazy so that unused sub parts do not need to be evaluated and they can be efficiently traversed. I've heard this advice multiple times, but it's not clear to me what it means. What would a standard cons list look like if it were "strict in the leaves and lazy in the spine"?
We actually started to intentionally leave conformance behind with the removal of Num's superclasses, IIRC, which was ages ago.
`Cofree` and `Free` can both be considered trees where the branching factor is determined by the functor. The big difference is whether you keep the values at the leaves (`Free`) or at the nodes (`Cofree`). You might enjoy playing with `Cofree (State s) a`.
Free is FreeT Identity, FreeT ((-&gt;) t) is parsers: https://hackage.haskell.org/package/yoctoparsec-0.1.0.0/src/src/Control/Monad/Yoctoparsec.hs
Almost exactly the same, which is why STM is awesome 
&gt; but since the ability to reverse equations is purely "local" it needs work to flesh out into a theory I'd be really happy with. Do you mean FHM or FRP? Either way could you explain what you mean by that? 
Get a job working in other languages (for me, Python) and get coworkers interested. Keep in mind, don't shit talk the language you're working in, just show interest and other people will too. We still use Python for 80% of our work, but critical code is all Haskell now and it 1) never ever crashes 2) runs on a tenth of the memory of any of our Python apps and 3) is crazy succinct. Once you break the ice people will get interested. I've also done some Rust, Clojure, and Mercury (yes Mercury) for a few projects as well. It just requires a team (inc management) with an inquisitive mindset.
Can GHC even unpack sum types?
[removed]
More bang patterns don't help much. As pointed out here, changing from Set to IntSet is the big win. In fact, change all the Integer in this program to Int for a further small improvement.
For some reason, it makes me smile. "Is that thing levitating or not?" "Levity aside, it's pretty cool."
My bad, sorry :-)
I thought of it as a Schrödinger's kind: as long as you don't inspect the actual representation, it's in a superposition between `Lifted` and `Unlifted`. Since it's neither clearly at the floor (`Unlifted`), nor held up (`Lifted`), it's levitating.
There are different fusion techniques. `stream-fusion` implements a different kind of fusion from the one that GHC's `base` library comes with. I vaguely recall hearing that stream fusion is better in principle than the GHC one, but that there was some technical issue why GHC never adopted it for lists. On the other hand, the `vector` library does use stream fusion.
In the text you quoted I believe Edward was criticising propagators (saying that the theory of propagators needs to be fleshed out) on their ability to fulfill the FHM ideal. Does that answer your question?
&gt;`Cofree` and `Free` can both be considered trees where the branching factor is determined by the functor. The big difference is whether you keep the values at the leaves (`Free`) or at the nodes (`Cofree`). Wow, thanks. This is really neat intuition for `Free`! Edit: And made a lot of the other stuff in the article easier to grasp for me too. Very nice.
I neglected to add that the path given can now be installed with -iA. Mornings are hard.
The webmaster of this page may want to look over how the code snippets are rendered on phones. For me, they are tiny and hard to read!
Damn autocorrect! Fixed it.
There's a forgetful functor Friday-&gt;OtherDays.
Fair enough.
At first glance, it seems impossible with `evalCBV`'s type signature. Think about what `a -&gt; IO b` versus `IO (a -&gt; b)` means. In the first case, any side effect of the action can potentially make use of a. In the second case, any side effect is decided *before* you pass any value to the function, and the `(a -&gt; b)` function itself is (in the absence of `unsafePerformIO`) pure. The signatures for Lam and App themselves are not problematic. One option would be to use a type family in the return type so that you are not trying to construct a value of type `IO (a -&gt; b)` but rather `a -&gt; IO b`. Something like so: type family Ret m v where Ret m (a -&gt; b) = a -&gt; m b Ret m a = m a evalCBV :: Expr a -&gt; Ret IO a This shouldn't affect any of the other three patterns, but for `Lam`, it should allow you to return a more traditional Monadic function.
I'm not sure what you mean by using`putStrLn` to "track" the evaluation process. Are you trying to debug something? First write your function as a pure function, without `IO`, in a way that expresses clearly what you are trying to do. If you are looking for a bug, you could then temporarily insert `trace` calls to produce output without using the `IO` type and without explicit use of `unsafePerformIO` (although of course that is used by `trace` internally). You will of course remove the trace calls once you have fixed the bug. If you want the evaluation process to produce a trace of what happened as a feature, not just for debugging, then write a pure function whose result type represents what you want to see. Perhaps you want a result type something like `(a, [Event])` where `Event` represents an evaluation event. You might then find the `Writer` monad (from [transformers](http://hackage.haskell.org/package/transformers) or [mtl](http://hackage.haskell.org/package/mtl)) with a `DList` (from [dlist](http://hackage.haskell.org/package/dlist)) convenient for that.
Listening to arguments like this one could get the impression all people ever do is look at the type of `$` all day long and it is a major part of every Haskell learning effort to understand it.
And the best thing is: We can implement it right now, using the list monad transformer and guard and get the performance for free, when ghc supports UniverseT.
This type family will also affect the `Lit` constructor, since you can have function literals. You would have to convert these pure functions into monadic functions. To do that you need to know the shape of the type at runtime, so you would need something like singleton types.
If what you want is the latter, then your `Expr` type is probably not correct. It doesn't sound like you want to model pure functions. You want to model functions that compute a value with the side-effect of recording the steps that were followed to compute it. Here's one way you could do that: {-# LANGUAGE GADTs #-} module EvalCBV where import Data.DList (DList) import qualified Data.DList as D import Control.Monad.Trans.Writer (runWriter, tell, Writer) import Control.Monad.Fix (mfix) data Expr e a where Lam :: (Expr e a -&gt; Expr e b) -&gt; Expr e (a -&gt; e b) App :: Expr e (a -&gt; e b) -&gt; Expr e a -&gt; Expr e b Fix :: Expr e (a -&gt; e a) -&gt; Expr e a Lit :: a -&gt; Expr e a data Event = LamE | AppE | FixE | LitE deriving Show type W = Writer (DList Event) tellDL :: a -&gt; Writer (DList a) () tellDL = tell . D.singleton evalCBV :: Expr W a -&gt; (a, [Event]) evalCBV expr = (result, events) where (result, eventsDL) = runWriter $ evalCBVW expr events = D.toList eventsDL evalCBVW :: Expr W a -&gt; W a evalCBVW (Lam f) = tellDL LamE &gt;&gt; return (evalCBVW . f . Lit) evalCBVW (App mf mx) = do tellDL AppE f &lt;- evalCBVW mf x &lt;- evalCBVW mx f x evalCBVW (Fix mf) = tellDL FixE &gt;&gt; (evalCBVW mf &gt;&gt;= mfix) evalCBVW (Lit x) = tellDL LitE &gt;&gt; return x Sample uses: EvalCBV&gt; evalCBV $ App (Lam id) (Lit 5) (5,[AppE,LamE,LitE,LitE]) EvalCBV&gt; evalCBV $ App (Lam (const $ Lit 4)) (Lit 5) (4,[AppE,LamE,LitE,LitE]) EvalCBV&gt; let protoFactorial = Lit $ \f -&gt; return (\x -&gt; if x == 1 then return 1 else fmap (x*) $ f (x-1)) EvalCBV&gt; let factorial = Fix protoFactorial EvalCBV&gt; evalCBV $ App factorial (Lit 3) (6,[AppE,FixE,LitE,LitE]) Is that what you are thinking of? EDIT: Added a sample use for `Fix`.
Like this, I think: `data List a = Nil | Cons !a (List a)`
No harm done. 
The problem is having to take a job where you primarily use a language you don't like for a long time. And if you take a job using a non-shitty, non-Haskell language, it's unlikely that you'll be able to make a strong enough case for Haskell to justify using it over the existing language. 
Yes, definitely the last patch releases should have been chosen because they often fix regressions. 
I'd say that that is a bug in the English language :) For non-native speakers, the link between the property 'being lifted or not' and the word 'levity' is far from apparent.
Could you paste your full stack.yaml configuration? Which command exactly do you use to run it?
 flags: {} extra-package-dbs: [] packages: - '.' - location: git: /home/me/code/haskell/harpack/ commit: 255f16e8fa9932073402a9091bf1924a4aad7272 extra-dep: true - location: git: /home/me/code/haskell/qhull-simple/ commit: 04d254b48a1940584b6eb8727ade3ecaf9bad496 extra-dep: true extra-deps: - exact-combinatorics-0.2.0.8 - kmeans-vector-0.3.2 - storable-record-0.0.3.1 - storable-tuple-0.0.3.1 - tuple-0.3.0.2 resolver: lts-4.2 nix: enable: true packages: [ arpack , openblas , qhull ] I build it with `stack build` after sourcing the `nix-profile`, and run it with `stack exec`
Do you normally expect to be able to guess the meaning of unfamiliar words? That might work but only if you know the language that English took the word from, probably Latin in this case. 
From [a mail referenced in that discussion](https://mail.haskell.org/pipermail/ghc-devs/2016-February/011286.html) &gt; Bottom line: We *need* an alternate Prelude. But that won't happen for 8.0. It *would* be nice to have Prelude and SimplePrelude or ExperiencedPrelude and BeginnerPrelude or whatever. Something to help ward off the `map`/`fmap` split and discussions around how scary Foldable/Traversable are, and so on with this new signature for `$`. I might be a little jealous of Rust, who, while not having `Functor`, do have [`map` as a part of the `Iterator` trait](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.map), which ought to be equivalent to our `map` being a part of `Traversable`, rather than specified to lists, and the documentation even describes it in the very un-newbish terms "Takes a closure and creates an iterator which calls that closure on each element." It's kinda funny how Haskell will expose people to "scary" names like Functor, Applicative, Monad, but then shy away from complicating default type signatures, as if *that's* so scary. Newbies still don't have to [chant "public static void main"](http://programmingisterrible.com/post/40453884799/what-language-should-i-learn-first), and I suspect people are *attracted* to Haskell because usually there is a preference for explaining stuff over hiding reasoning and theoretical underpinnings away. I guess I don't sympathize much with the view that a slightly less confusing first week is worth years of annoyance and minor frustrations. Having an incantation that lets people use a less-powerful-but-with-simpler-types BeginnerMode version of Prelude just sounds sweeter and sweeter, for every discussion about "ugly type signatures" that arises.
Actually, everyone thought so and resulted in: http://hackage.haskell.org/package/hlint-1.9.27/changelog
Oh, that is pretty cool. That does look a bit like a bug though, unfortunately I do not have GHC 8.0 to check right now. But wouldn't that have to generate two versions of `both` (`both` needs to know how wide the first field is, no? Or perhaps it just alignes everything like a pointer). 
Woohoo! Question asked which was literally solved last week in hlint-1.9.27 :). And indeed, it now has warnings (things which you probably want to fix) and suggestions (things which you might find a good idea or not).
You see "Hello World" because you (and all of us) are still inhabiting the universe where "Hello World" was printed, rather than the universe it was replaced with. But the program executes in another universe; printing "Hello World" again should not be observable to you.
See http://neilmitchell.blogspot.ru/2016/02/new-hlint-version-and-api-features.html
That definitely should _not_ typecheck, and the fact that it currently does is a bug: https://ghc.haskell.org/trac/ghc/ticket/11473 **tl;dr** Levity-polymorphic types should never appear to the left of an arrow.
Thought so, given that levity polymorphism is missing on the left hand side of `($)`.
For the record, I consider Reflex to be an FRP engine. I know that there is some ongoing discussion of what constitutes FRP, but I believe that Reflex achieves the principal goals originally set forth in Conal Elliott's and Paul Hudak's papers. There were some inherent memory leaks in the model as originally proposed, so I had to make some changes, but the basic semantic model (Behaviors as "pull"-based state containers and Events as "push"-based notification streams) is the same.
I was referring to the propagator FRP story in the "Art of the Propagator" paper. It uses the purely local equational theory described in the beginning of the paper to run equations forwards and backwards, but things like `\x -&gt; x + x` break the ability to run backwards. I talked about this issue here, but I admit it is a bit of a needle in a haystack: https://www.youtube.com/watch?v=DyPzPeOPgUE
&gt; The ugliness of the type signature seems to be a major discussion point. [#11549](https://ghc.haskell.org/trac/ghc/ticket/11549) will hide the runtime representation by default.
&gt; restore the pre-war state of the universe. Unfortunately, this implies that the decision to fire the missiles would be taken again ...
This raises an interesting question about whether the program itself is part of the universe or not. 
&gt; What should be contained in SimplePrelude? What's the correct subset? […] It's not an easy task to get this right. Absolutely, and I'm not sold on the benefits. I *suspect* if someone does make something like it, it'll enjoy a brief popularity but soon become a dead project. But at least if someone then wants to start an argument about a change in the type signature for `map` or `$` or whatever becoming "ugly" and "confusing for beginners", we can point them at SimplePrelude. &gt; I've never tried this with Haskell, and I doubt that it would work immediately. Yeah, I suspect the gap between GHC and other Haskell implementations is widening. It's great that GHC is evolving, but having something hammered out as Haskell201x would probably be for the best.
the bike shedding is really intense here
I think `stack upgrade`should also work
&gt; things like `\x -&gt; x + x` break the ability to run backwards Seems like a case for linear types? ([Also reminds me of...](https://bitbucket.org/roshanjames/theseus)) Unrelatedly (just to keep it in one comment) I also found [this paper](http://haskell.cs.yale.edu/wp-content/uploads/2011/01/icfp03.pdf) a couple years ago which combines automatic differentiation, FRP, and FHM aspects -- its author, Henrik Nilsson, seems to be the same person involved in the Hydra project linked earlier... (I haven't studied any of these in detail, just a "have you seen, also looks interesting/related".)
&gt; Seems like a case for linear types? I the absence of cycles you can typically solve constraint programs in linear or O(n log n) time depending on the nature of the constraints by using very greedy approaches. Sadly not every constraint program can be broken into one without cycles. I did read that paper though back when I was getting my head around FHM.
I installed ghc-mod via stack (doing `stack install --copy-bins ghc-mod`) and it seems to be working now. Thanks However, I'm still not sure how spacemacs (or emacs) pick up the right version of ghc (and ghc-mod) depending on the stack file.
dont apologize haha im so not a mathematician
Well, I know a bit of Latin and I suppose my English is as close to being native as it'll ever get, but still 'levity' means 'light-heartedness' to me. I do not see a way that non-native speakers ever will find a connection between 'lifted' and 'levity', except being taught. 'Levity' is a non-obvious term, whereas 'Liftedness' is - clunky as it might be. I hope 'RuntimeRepresentation' wins ;-)
Opaleye upsides: + No template haskell + Lets you use lenses, arrows and profunctors to good effect + Gives safety and great query flexibility at the same time Opaleye downsides: + No template haskell + You'll want to know lenses, arrows and profunctors + Recommends a specific way of making your data types My experience with it is somewhat superficial, but I loved what I saw so far and will use it in my next project. Hopefully someone more familiar with its competitors can chime in as well.
Well that right there is the problem, it doesn't. When you have ghc-mod linked against the wrong version of GHC things just break :/ 
Can someone help me with this? It's not compiling with Cabal on Ubuntu... http://stackoverflow.com/questions/35354153/haskell-cannot-import-ghc-srcloc
You should post your command line log so we can see what you did and what output it gave you. Also make sure versions are in the log.
It's when a joke can be interpreted in multiple different ways, all funny.
Given that this is my third day programming in Haskell, I don't know what any of these things are. But I will reference them. Thank you. 
I think they mean they wants to hide methods from code that doesn't need it, but not need multiple source files to do so. Without further elaboration on the author's part, I don't know what to say in response to it.
&gt; I have also recently lost access to my (woefully out-dated) Windows 7 box. I guess you check all versions by hand, since there's no [CI button/tag on the GitHub page](https://github.com/deech/fltkhs). If you don't have (regular) access to a windows machine, you probably want to use [appveyor](http://www.appveyor.com/) or a similar service, especially if someone comes up with the correct instructions. Unfortunately my current Windows installation has several versions of binutils in its path (and in the `stack exec` paths), so I doubt that I'm able to create reproducible build instructions at the moment.
Look at this screenshot: http://i.imgur.com/fj91l1w.png If I click on the print statement, it jumps to that file. This is how I do debugging. It only works on IntelliJ, Eclipse, and Netbeans. https://github.com/JohnReedLOL/HaskellPrintDebugger/ Also, the whole reason I don't use emacs is that I take FOREVER to learn key bindings and once I learn them I take FOREVER to unlearn them. I'm like a lab rat. 
I think it's just the version of the compiler I'm using. That being said, check this out: -- http://i.imgur.com/fj91l1w.png ^ It allows you to click on the line number and jump to the print statement.
That's so true it hurts.
As on StackOverflow (where this question might belong better), please post the whole compilable source code file.
Looks like custom prelude works with ghci if you move it into a separate package: http://stackoverflow.com/questions/35357198/how-to-correctly-define-your-company-prelude
It's pretty hard to try it without without at least an example of haskell code depending on arpack and/or (since it's what depends on gfortran). Can you paste a minimal example, e.g. with one just foreign imported function from harpack? EDIT: Actually, I suspect you're getting that because arpack and openblas both depend on fortran.
Flychecker is using `haskell-stack-ghc` so I assume is using stack, which should in theory (or in the future) use the correct version of ghc-mod , shouldn't it ? 
[This is not a good way to display content in a smaller window](http://i.imgur.com/R6bF33U.png). You should never force the user to scroll to even reach the *start* of your content.
Like [this](https://hackage.haskell.org/package/getopt-generics)? *EDIT*: Sönke also talked about applying this to HTML - not sure what the status there is.
You can reexport multiple modules from single module. module A ( module B, module C ) where import B import C
The remote monad idea is very interesting in its own right (esp. the strong version with bundled commands), but I have a general question re. sending Json over the wire for RPC: on one hand it is readable and debuggable by humans, but aren't binary messages more efficient in terms of bit size?
&gt; For example, Warp looks perfect but its Github source hasn't changed in 5 years. Wrong repository. The last update was [four weeks ago on Hackage](https://hackage.haskell.org/package/warp), and the same holds for [the WAI repository](https://github.com/yesodweb/wai). Someone with write-access should update the [wiki page](https://wiki.haskell.org/Web/Servers) though (and deprecate the GitHub project).
Where are scatter and histogram (function, Data Type respectively I guess) do you use in your demo.gif ? Thanks and congrats for this great project.
I'll update the wiki. Ty.
The typo is corrected, thank you. About your code, what's your issue with `Debug.Trace.traceStack` except the compilation with `-prof` ? There is a lot to say about your code, mainly you usage of "return" in IO code in useless and clutter the code. You can use `let` or `where` in `do` blocks, such as: debugTraceIO message = do let callStacks = getCallStack (?loc) .... when debugMode (Debug.Trace.traceIO toPrint) You can `import` specific name to avoid qualifing everything, such as `import Data.List (intercalate)` It also exists a module which contains everything you need to split path on `/` or `\` depending on your OS.
Basically ;-) 
&gt; for example "map" clashes between vector and list You should use `fmap` on list since it is `Functor`. `Vector` is usually imported qualified. 
&gt; a runtime dependency on an entire platform that your end users may not otherwise want. Uh... except that GHC can (and only most versions this is still the only option) statically link your program, so the only dependency would be your program, wouldn't it?
Right. For a new programmer, it's like the difference between learning Esperanto vs learning Spanish. Some may think that Spanish is easier because it is more ubiquitous and there are more materials, but you can't deny that the rigorous logic of Esperanto makes things quite simple! 
Not that I mind, but there's a separate sub for project suggestions, in case you didn't know about it: /r/haskell_proposals Does anybody actually use the other subs? We often get posts starting with "I know there is this other sub, but it's not as active as this one, so...". Again, it doesn't bother me that people do this, I'm just wondering what's the point of having multiple subs if people don't use them.
I wish we could just dispense with command-line arguments. They're basically the antithesis of structured, strongly typed programming. Can someone invent something better?
Yeah the typo got me :)
Accessing everything through typed interfaces is a good idea. The difficulty is getting the external world to cooperate. There's much more of it than there are of us :)
Now I get it. With a single field it makes no difference because when `StrictInt` is evaluated, it can only be to evaluate the `Int`. With multiple fields, you may only need to evaluate a subset of them. With bangs, they will be evaluated even if they aren't used. I suppose a difference in behaviour (not semantically, but performance-wise) could be when unboxing of strict fields is applied though. Either way I will add another field, because I agree that it is cleaner.
That is true. I didn't mention it because it is the default, but I guess it could be interesting.
I'm mostly just talking about how it worked for me. If Haskell is your end game, then doing this strategy is a losing move. I took a job because it sounded interesting, and my team was inquisitive enough that I was able to start a few projects in Haskell and it went from there. 
&gt; I wonder where computing will be in 10-50 years. I'm pretty sure command line arguments will still be around in 50 years :(
Ok, I'll place a link. Thanks!
See [this discussion](https://ghc.haskell.org/trac/ghc/ticket/915) about bringing the `stream-fusion` functionality into GHC. Essentially there was issues making it play nicely with the `concatMap` implementation in `base`. At the end of the discussion there is a link to a solution described in [this 2014 paper](http://ittc.ku.edu/~afarmer/concatmap-pepm14.pdf). However i'm struggling find information about the status of this being incorporated into GHC.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/ruhaskell] [Новая Haskell-вакансия в Москве](https://np.reddit.com/r/ruhaskell/comments/45fdg3/новая_haskellвакансия_в_москве/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Still a work in progress, but some of you might enjoy it already. Feel free to comment issues / ideas or open issues on github. I currently expand it while writing my game.
https://www.youtube.com/watch?v=Ps6ck1ejoAw
&gt;[**Smile, Charlie Chaplin , Modern Times, 1936 [2:08]**](http://youtu.be/Ps6ck1ejoAw) &gt;&gt;Smile! &gt; [*^petrodka*](https://www.youtube.com/channel/UCfzixh9KuwKIkE4j4obwSlQ) ^in ^Film ^&amp; ^Animation &gt;*^2,482,862 ^views ^since ^Dec ^2007* [^bot ^info](http://www.reddit.com/r/youtubefactsbot/wiki/index)
I would have been able to use that capability on buffer-builder [1]. I frequently received the question "Why FFI into C? Couldn't you get good performance staying in Haskell?" When each operation amounts to a handful of instructions, branch predictor hints rearrange the code so the hot path stays in registers and the cold path can spill, so GHC just doesn't stand a chance relative to C compilers. Adding the hints measurably improved the criterion numbers. In short, yes, predictor hints would be awesome. :) [1] https://hackage.haskell.org/package/buffer-builder
Yes, command line arguments will be around for a long time, just like this Chaplin classic.
Someone should update the page for happstack server as well, among others probably -- but its hardly unmaintained. A new version was updated to hackage just two weeks ago or so! That said, the usual "light" server recommended to people that's a bit richer than `warp` is `scotty`.
OH MY GOD. // Construct a parser that parses a number. auto number = transform( sequence( oneOrMore(charRange('0', '9')), optional(sequence( exactChar&lt;'.'&gt;(), many(charRange('0', '9'))))), [](Array&lt;char&gt; whole, Maybe&lt;Array&lt;char&gt;&gt; maybeFraction) -&gt; Number* { KJ_IF_MAYBE(fraction, maybeFraction) { return new RealNumber(whole, *fraction); } else { return new WholeNumber(whole); } });
Would be nice to have some screenshots.
The same idea - amortizing the cost of remoteness using monadic bundling - can be applied with almost any serialization mechanism. We choose JSON-RPC from this example because it was easy to understand, and already supported a form of bundling. One thing JSON-RPC does not support is *nested* calls, though. We've got a new RPC library in the works that does use compact serialization, as well as nested calls. The goal of the work is to be able to say "run this over there", in a principled way.
I was looking at this recently and it seems to me that modern CPUs mostly ignore this flag anyway. It would be interesting if these flags would be of interest to the code generator.
But with C++ you are completely free to do whatever thing you want in the code (also free to shoot yourself in the foot) while Haskell require discipline: you can not throw whatever side effect you need in a pure function and you are forced to rethink the problem and to find another solution if you need that side effect (and you have to keep in mind other things too, laziness ecc...). I understand very well when the author said you must spend more energy doing something in Haskell. But... This is not a bad thing! Why is the Haskell community obsessed with wanting to disguise the language as an easy one (even with false type signature... $ anyone)? As Kennedy said: "[...] We choose to go to the Moon in this decade and do the other things, not because they are easy, but because they are hard; [...]" I choose to learn Haskell because I wanted to learn a difficult language, if it was so easy I would have lost interest very soon. There are already a lot of easy laguanges out there, but difficult ones are very few (just C++ and Haskell of the popular ones) and why can't the things stay the same? C++ programmers go prouds of their expertise and they are used to say things like "you can not master C++", "C++ is an expert only language" etc... Why Haskell can't aspire to be the same? Why when someone say that he took years to master the concept of Monads, you must say: "oohh noo, it's so easy... Just think of... You can learn Haskell in few months. Etc..." (Implying you are stupid) And then new Monad tutorials show up on the web... Why you can't say: "Yes, Haskell is a difficult language and take time to learn, but you are doing fine, Monad is a simple but very abstract concept, just keep going... you'll grok it and you'll get a lot of benefits." Instead you are focusing on disguising the language as an easy one, and then one day the beginner will be faced with the truth and will stop learning Haskell because he won't understand why he should continue the study if it require so much brainpower even for the simplest program. Of course, you lied from the start... You have not prepared the beginner for what Haskell really is. He expected a Python, and instead he'll get something leaning more toward C++.
Rich Hickey makes a good argument that [we should distinguish simple from easy](http://www.infoq.com/presentations/Simple-Made-Easy) --- although his preference is obviously for Clojure rather than Haskell. I never say that Haskell or FP in general is "easy" or "natural" or "intuitive" because IMHO these are actually not desirable properties for programming reliable systems. 
In Scala it feels like more time is spent figuring out why implicits don't trigger or how you can massage the syntax to avoid tripping up type inference than doing useful stuff. :(
Examples 2 and 3 are interactive, but they are [black triangle](http://rampantgames.com/blog/?p=7745) demos. I think the reliance on typeclasses might be a mistake, but this is probably the simplest/cleanest engine of this kind I've seen so far for Haskell for interactive 2d. I'm pretty excited meself. [Images](http://imgur.com/a/VzrJp)
What do you suggest instead of the typeclasses?
please see my edit
The lack of a consistent philosophy is just a nightmare. I can't take it. It's why as far as FP languages go, I'll take Haskell or F# over Scala.
A record of functions. 
It's an excellent suggestion, ML and Haskell both have systemF as their underpinning. It really depends what entrypoint level op wants, but that's an excellent and very strong one.
What /u/c_wraith said. You [can](https://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/) read [more](https://www.reddit.com/r/haskell/comments/45d5sy/a_modest_haskell_critique_from_the_capn_proto/czx0i7z?context=3) in [these](http://www.haskellforall.com/2012/05/scrap-your-type-classes.html) links :) To be clear, I've no problem with typeclasses but your application of them here for some of those things is a bit suspect. I say that as an mtl-style user.
I understood the difference in the beginners definition. In my opinion they won't stop because the language is difficult, but because they see no benefits in the long run. That's why I brought up C++. Its benefits is just an huge one: speed and while you can argue is one of the most difficult to learn, yet it's also the most used in industry. Haskell won't die for the lack of new programmers as long as they see the language usefulness.
I don't think static linkage solves the problem being discussed in that paragraph. The paragraph is about trying to invoke Haskell code as a library from a Python program.
Thanks for trying it out! I notice you are using `mingw32-make` which doesn't come with `ghc` or `stack` so I assume you have your own version of `mingw` or `MSYS` installed so I did the same. I changed `cmake` to use the `MinGW Makefiles` generator and adjusted the path to use the `g++` and `gcc` binaries that come with my installed version and the C code built fine! That's farther than I got last time so that's great. Unfortunately the `preprocess` step where `c2hs` converts `.chs` files into `.hs` files ran into: c2hs.exe: C header contains errors: C:/Users/owner/AppData/Local/Programs/stack/x86_64-windows/ghc-7.10.3/mingw/lib/gcc/x86_64-x64-mingw32/5.2.0/include /bmi2intrin.h:86: &lt;column 21&gt; [ERROR] &gt;&gt;&gt; Syntax error ! The symbol `__res` does not fit here. Did you run into anything like this? Thanks again! 
Thanks for the tip, `appveyor` looks great.
I learned about /r/haskell_proposals and /r/haskellquestions from the /r/haskell sidebar. I also know about /r/haskelltil and /r/haskellgamedev, because they were advertised on /r/haskell a while ago. I think they both ought to be added to the sidebar as well. And by searching for "haskell" in the top-right search box, you can find [a lot more](https://www.reddit.com/r/haskell/search?q=haskell&amp;type=sr&amp;count=3&amp;after=t5_2qpmq)! You're right, those subs aren't technically children of /r/haskell, and I don't think reddit even supports hierarchical subreddits. I don't think they're competing with each other though, I think they complement each other. My understanding of the purpose of those subreddits is best explained by the sidebar of /r/haskellgamedev: "This is a place to find resources and discussion about game development in Haskell, without having to sift through the non-game majority of /r/haskell and the non-Haskell majority of /r/gamedev!"
Sure, but the paragraph seems to imply that the Haskell makes more dependencies, whereas it really makes one one dependency (the library itself)
So here's the problem that I personally see. Haskell is a big programming language with quite a lot of depth and breath to it. However, in and of itself, Haskell's relatively good at not forcing you to care about the language features you don't know about - a TON of code can be written just with a combination of pure, simple, monomorphic functions for describing computation and the occasional IO monadic code bit for actually running/executing that pure code. If any of you guys watched that Quakecon 2013 keynote in which John Carmack basically endorsed Haskell as a sound FP language, you might recall that in his experiment of translating the original Wolfenstein 3D into Haskell, he mentioned the following: &gt; So yeah, the way I had that all segmented is, **the little bit of monadic code that happens in the main thread is just one file, everything else is absolutely pure functions, not even any of the more fancy things** that can help you escape in some ways. ^ This is something that has been true in my own experience as well. You don't need much of Haskell's features in order to get work done, there's a pretty productive starters kit at the core of the language, but you'd never know that hanging out in the Haskell community - we barely acknowledge the straight-forward part of our language. Those of us that share/present our work with others, either in the form of a library, blog post, or tech talk, typically have good mastery of the advanced topics and often have deployed those advanced topics in a way that's interesting (hence why we're paying attention to their work), those of us that aren't comfortable with such topics typically keep quiet about our inexperience and just let the experts have the floor. This is something I would really like to see change in our community in the near future, I would like to see more collaboration and trading of notes to take place between people who aren't experts. #haskell-beginners has some synergy going on, but we could do a lot better. **EDIT:** And I should add, Haskell is a language which is routinely concerned with trying new things and advancing the state of the art, but this has an unfortunate side effect of turning the experience of using Haskell into one where I regularly feel as if I don't know as much as I *should* even if that feeling is actually false. Also, we regularly insist that beginners should learn the advanced topics *properly*. "Don't read monad tutorials, it will just confuse you." Well it's either that or I don't use monads, period. When I was a beginner, I really didn't care if I had an air-tight understanding of things like monads, I just cared if it was useful for something in a way that was new and interesting. People don't need to get it 100% the first time, they just need to progress their intuition a bit and take away something concrete and useful in the meantime.
I've had great experience with it in two projects - one where it allowed us to reduce a lot of SQL duplication (we had a non-trivial join that was used in a lot of different ways), and the other where we were able to model a subset of a complex existing database pretty easily. It's a little verbose, but that's been far outweighed by the benefits in both of the (production) uses that I have it in.
I think Haskell could be a lot better for video games if it had first class records and row polymorphism. [this is as far as I've gotten currently](https://github.com/soupi/haskell-play/tree/master/src/Play) with my experiments in gamedev in Haskell.
You're defining "dependencies" narrowly as "files". I was also worried about, say, the dependency on a second garbage collector, a second set of low-level libraries, etc., running in the same process. That said, you are probably correct that when I wrote the line I was forgetting that Haskell supports static linkage. But that only solves some of the problem.
Well, you have to declare what belongs to a build somehow in every build system. Tools like Maven or Gradle usually assume (at least by default) that you're dealing with a project with exactly one output, and everything below a certain set of directories as input. Tools like CMake or Cabal by default assume that you might want multiple outputs (although cabal only allows you to create one library per project), so you have to declare what input belongs to which output.
happstack-server is definitely not abandon. In fact, it was just added to stackage indicating an even greater commitment to ensuring it is actively maintained. 
It's far from clear, in some cases, that it does come at the expense of the language at large. In an ideal world, we'd pay attention when concern for beginners leads us to realize that Haskell is accumulating incidental complexity. Simplicity and clean abstractions don't just benefit beginners; they benefit everyone.
 data EngineState a = EngineState { hover :: PosX -&gt; PosY -&gt; a -&gt; a , click :: PosX -&gt; PosY -&gt; a -&gt; a , drag :: PosX -&gt; PosY -&gt; a -&gt; a , resize :: Width -&gt; Height -&gt; a -&gt; a , getSize :: a -&gt; (Width, Height) , getTime :: a -&gt; Millisecond , setTime :: Millisecond :: a -&gt; a , getW :: a -&gt; Width , getH :: a -&gt; Height , getTitle :: a -&gt; String , toGlInstruction :: a -&gt; RenderInstruction } Then any function of type: foo :: EngineState a =&gt; ... ... you just change the constraint to passing in the record as an ordinary function argument: foo :: EngineState a -&gt; ... This is actually how Haskell type classes work under the hood, but sometimes it's better to keep the information as a record instead of a type class instance. You can transform or manipulate the record easily within the language, but type classes instances cannot be manipulated as first class values without a lot of type system hackery.
Ah, that's good to know. Do you know in which context the CPU would care about it? Would the GHC port of it be irrelevant then?
Woohoo!
Awesome. Thanks so much Simon.
People say many things. In a lot of cases, they are wrong. Or rather, in this case, the answer is undefined, since "intuitive" and "natural" are terms that only have meaning relative to the person making the statement. Haskell is quite likely to be more intuitive to people with a strong background in mathematics. On the other hand, I spend a good part of my life now teaching computer programming to children with no previous programming experience. And I can tell you this: it is sometimes claimed that Haskell is only unintuitive to many people because they have learned imperative programming languages first. This is nonsense. People with no programming background at all are just as likely to struggle with expressing ideas using pure functions and composition. I still love Haskell for sticking to that vision, and I choose to teach it for that reason. That same skill set is critical for succeeding in mathematics and logic. But it's certainly not natural and intuitive in many ways.
If you're going to respond like that, then I don't see any point to discussion. Having said that... see this talk: http://www.infoq.com/presentations/Simple-Made-Easy (Rich Hickey, "Simple Made Easy") I'd also add: The point isn't what you can do *right now*... the point is what you can do to your "future self" (or: future maintenance programmers). 
Well, the top half is pretty similar to what you might write it Haskell, except that Haskell style tends to use shorter names or even operators (you could do that it C++, but I think it harms readability). The second half is definitely more verbose than Haskell would be, but not by a ton. The need to explicitly declare parameter and return types is a big part of the problem, but has improved in C++14 (you can now replace the parameter types with "auto"). But yeah, lambas in C++ are never going to be as pretty as in Haskell.
&gt; In C++ free functions are generally preferred to member functions. I (author of the article) would dispute this FWIW. The C++ standard library is honestly pretty terrible.
&gt; you can not throw whatever side effect you need in a pure function and you are forced to rethink the problem But I want that from compiler because adding side effect might change semantics which can lead to bugs. Type errors will just point me in right direction. By learning I mean learning to write correct code. I remember very frustrating moments in C++. Writing correct code in Haskell isn't bug free but it is easier for me. I think that most difficult task in Haskell is to estimate space complexity due to laziness (reason to use profiling). But people are very aware of that. Monads are easy once you understand them. It's like trying to understand Monoid before you multiply by one or concatenate with empty string. Saying that they are easy is just to lift your worries (it might be counterproductive but anyway). It's not said to make anyone feel stupid - you can find many haskellers that will freely admit that they are not so clever or persistent at all. Don't take me wrong. C++ is very pragmatic choice due to momentum. But I simply see big opportunity for improvement.
I (author of the article) would love to see monads added to C++. ;)
One would hope that we have higher standards than "people say". EDIT: Btw, you've already revealed an obvious bias in the title(!) of your post, so please do come back with something *really* solid, or else you're going to be dismissed out of hand.
&gt; And I should add, Haskell is a language which is routinely concerned with trying new things and advancing the state of the art, I wish more energy would go into improvements of practical use, like packaging. It seems that after Cabal was introduced, things just stalled for years in this area. Now we at least have `stack`, which is an enormous improvement but not in an interesting way. `stack` and Stackage is really just a gigantic `cabal freeze` with Soviet-style central planning to make it work. I post this comment quite hesitatingly because of course volunteers work on what they want to work on, as they should. It's great that folks are pushing the envelope on stuff. But a lot of the envelope pushing just makes me scratch my head while other much more practical problems that cause huge arguments (for instance the arguments people have had about PVP adherence) just languish for ages when brainpower and tools could help solve them.
I'm pretty sure that functions are applied from left to right (correctly) due to laziness. Take: f = take 10 g = filter even h = drop 10 x = [1..] f . g . h $ x GHC isn't going to evaluate x first, nor h, not g. Because for all it knows 'f' could be 'const 5' and it would be a waste of time to evaluate the rest of it and also not lazy. It is going to start with 'f' and because 'f' is looking for the first 10 elements of a list it is then going to defer to 'g', and 'g' is only getting even numbers so it is going to ask 'h' for the first 10 even numbers and 'h' only wants the elements after the first 10 from 'x'. So then 'x' will evaluate just enough elements to satisfy all that.
I meant in what order the functions are applied on the data. So in your example, first `h` will drop first 10 elements, then `g` discards odd elements from that, then `f` takes first 10 elements of the result.
But that data is "requested" from left to right. So it essentially goes: f -&gt; g -&gt; h -&gt; x -&gt; h -&gt; g - &gt; f with the first half being to do with what data is needed and the second half being to do with that requested data coming back out. IMO left to right to left makes sense.
&gt; If you're going to respond like that, then I don't see any point to discussion. Sorry I have trouble understanding the meaning of your statement. Do you mean you find my point trivial/stupid not worth a discussion or do you agree with my point and so the discussion can be stopped here?
Disclaimer: I, too, have never used FLTK or FLTKHS. I have a Windows 64-bit GHC installed using stack, which also comes with (but doesn't fully expose) a MSYS2 environment. I cleared out my path, blasted away and re-setup stack's MSYS2 environment, and tried to install FLTKHS following your [instructions](http://hackage.haskell.org/package/fltkhs-0.4.0.2/docs/Graphics-UI-FLTK-LowLevel-FLTKHS.html#g:4), modifying them as needed. Update stack's MSYS2 environment: stack exec pacman -- -Sy --needed bash msys2-runtime pacman pacman-mirrors stack exec pacman -- -Syu Install make into stack's MSYS2 environment: stack exec pacman -- -Sy --needed make Install CMake using its [installer](https://cmake.org/download/). Ensure that CMake is made available on the path. You might have to log out and back in for your path to update or you can set it manually with: set PATH=%PATH%;C:\Program Files (x86)\CMake\bin Download and build FLTK-1.3.3. Note that I use an in-place(!) build because I don't know how to fix FLTKHS's CMakeLists.txt to search the right paths. stack exec curl -- -O http://fltk.org/pub/fltk/1.3.3/fltk-1.3.3-source.tar.gz stack exec bsdtar -- xf fltk-1.3.3-source.tar.gz cd fltk-1.3.3 stack exec cmake -- . -G "MSYS Makefiles" stack exec make Set FLTK_HOME set FLTK_HOME=&lt;some-directory&gt;\fltk-1.3.3 Download FLTKHS-0.4.0.2 stack exec curl -- -O http://hackage.haskell.org/package/fltkhs-0.4.0.2/fltkhs-0.4.0.2.tar.gz stack exec bsdtar -- xf fltkhs-0.4.0.2.tar.gz cd fltkhs-0.4.0.2 Edit `c-src/CMakeLists.txt` and add the following (at the bottom of the file): include_directories(${FLTK_HOME}) Use my own stack.yaml because different GHC version, and then build: del stack.yaml stack init --resolver lts-5.0 stack build Unfortunately, after you have read all this, you'll find out that I ran into the same error as you. Wouldn't this be an incompatibility between c2hs and the version (5.2) of GCC that came with ~~stack's MSYS2 environment~~GHC? [100%] Built target fltkc_static Preprocessing library fltkhs-0.4.0.2... c2hs.exe: C header contains errors: &lt;...&gt;/AppData/Local/Programs/stack/x86_64-windows/ghc-7.10.3/mingw/lib/gcc/x86_64-w64-mingw32/5.2.0/include/bmi2intrin.h:86: (column 21) [ERROR] &gt;&gt;&gt; Syntax error ! The symbol `__res' does not fit here. -- While building package fltkhs-0.4.0.2 using: &lt;...&gt;\tmp\fltkhs-0.4.0.2\.stack-work\dist\2672c1f3\setup\setup --builddir=.stack-work\dist\2672c1f3 build lib:fltkhs exe:fltkhs-buttons exe:fltkhs-fluidtohs --ghc-options " -ddump-hi -ddump-to-file" Process exited with code: ExitFailure 1 
Use a Haskell search engine like [Hoogle](https://www.stackage.org/lts-5.2/hoogle?q=%3A%3C%7C%3E) or [Hayoo](http://hayoo.fh-wedel.de/?query=%3A%3C%7C%3E). These operators appear to be constructors from the [servant](https://hackage.haskell.org/package/servant) package. [:&lt;|&gt;](https://hackage.haskell.org/package/servant-0.4.4.6/docs/Servant-API-Alternative.html#t::-60--124--62-) Union of two APIs [:&gt;](https://hackage.haskell.org/package/servant-0.4.4.6/docs/Servant-API-Sub.html#t::-62-) Sub API
Do you know the entire Haskell language? Are you sure about that? Every single technicality of every single GHC extension? Every detail of every main Haskell Platform library? C++ is an old language that has evolved a lot, experimenting with a lot of ideas (like Haskell), some of which failed but can't be instantly eliminated because of existing code (increasingly like Haskell). There's a lot that's only there for backward compatibility, or for a few specialist purposes. Despite that, there's a weigh-the-standards argument that C++ is actually simpler than a lot of languages that throw around that "C++ is too complex" accusation. In both C++ and Haskell, you're supposed to spend your time using a certain subset of features that form the core of the language. In C++, you're not supposed to spend all your time obsessing about explicit strictness and unboxing, or using the FFI to manipulate raw memory directly. Oh wait - I mean in **Haskell**, you're not supposed to do those things.
Have a look at [Spock](https://hackage.haskell.org/package/Spock). It's lightweight and pretty straightforward.
Sorry if that was a little harsh. I'm trying to get my print debugger working from the Hackage repository... http://stackoverflow.com/questions/35375051/haskell-how-to-get-code-from-hackage-into-project-cabal-install-project-failed ^ Please help.
Your print debugger doesn't have a cabal file. You need one.
Scala have complicated type system and considered WOL.
This is a lame post.
Practice more. Many people don't have to massage the syntax, they are just creating software.
I said &gt; "fault" and that was probably too harsh of a word choice. I don't see how an expert being unable to understand the whole language at once (just the language, not including the standard library) could be seen as beneficial though. I also didn't mean to imply that C++ was "about being an elitist, expert-only language". That's what I was arguing against. I don't believe a language itself is elitist. The C++ **community** generally has an elitist stigma attached to it. I am trying to say that words like "hard" "difficult" "easy" are already so poorly defined in the english language, that they shouldn't be used to describe an entire language, especially without **specific** justification. Helpful programming answers don't come in the form of a single word. I have been around programming long enough that words and a few elitists don't scare me off. Not everyone has this sort of "thick skin" and if a community is to grow in a positive way I believe answers without provided justification need to be questioned. NOT DIRECTED AT ANYONE IN PARTICULAR: "RTFM" is more likely to derail a discussion than a simple link to the manual. A link provides the opportunity for a more specific follow up question. EDIT: I feel that I have helped us stray from the topic of this post and subredit. I apologize for the off topic points. Anyone please feel free to pm me (notification of a new public post is welcome also). Bear in mind that I don't frequent Reddit as much as I used too (If I don't get back to you soon, it's not intentional).
Emphasis on monads really isn't what makes Haskell special. Nor is it that it has compiler magic. Lazy by default is a pretty big part of what makes Haskell special since it allows us to write control structure as normal functions. For example: if2 True x y = x if2 False x y = y Further the type system and all the cool things that it allows us, those make Haskell way more special than an emphasis on Monads. It is, in a way, a cousin of lisp, retaining a lot of the power but then enhancing it with a pretty amazing type system.
https://hackage.haskell.org/package/print-debugger ^ Does this work?
isn't the y combinator just: fix f = let x = f x in x or alternatively: y f = f (y f)
I've certainly never even remotely suggested that it's easy. I've always focused on why it's worth it anyway, and that it would be *easy***er** if they hadn't already learned some ugly stateful poorly encapsulated language like almost all of the rest of them. I've also said things like "It's not as hard as you might think" because it probably isn't. At the same time, it's also probably far harder than most people might think because of good ol' Dunning Kruger.
Try my package: https://hackage.haskell.org/package/print-debugger ^ It lets you write print statements like this: http://i.imgur.com/KCXYHNk.png If the package doesn't work, you can copy-paste code from here: https://github.com/JohnReedLOL/HaskellPrintDebugger ^ Please tell me if it works. Please.
I disagree that Haskell is difficult for beginners. I think it is difficult for beginners *in Haskell* who are coming from imperative languages. I also don't think learning about monads is difficult, it is just that people want everything digested to them and don't want to learn the intermediate steps. The truth is that I find Haskell an easy language, because even when I had no idea about monads and more advanced concepts, I could still use it to produce working programs by myself. The only thing I find hard about Haskell is the tooling, which is poor, but improving.
What does this do that the GHCi debugger's `:trace` doesn't? Is it more complete? Faster? Is it just that you can access the traces at runtime?
Welcome to Haskell. That's a very nice little package indeed :) Thanks for contributing to the Haskell ecosystem. I just read your code and I've got few suggestions for improvement. Do take these with a grain of salt, as I'm not really much more familiar with Haskell either. 1. You need not announce your package like that. A link to hackage should be enough (e.g. http://hackage.haskell.org/package/print-debugger ). The installation instructions should better be presented with code blocks containing shell sessions, if they need to be presented at all. 2. Your code seems to be littered with `-- returns foo` comments. If you want to do that, consider making them actual type annotations so that you get them checked. e.g.: -- Without touching other itchy points let callStack = Data.List.last(callStacks) :: (String, SrcLoc) noMonadThreadId &lt;- myThreadId :: IO(ThreadID) 3. You code contains many variables that are used only once or so (e.g. `thredNumberString`). On the other hand the long Windows check is being repeated three times. You may have spent a long time getting the types to match, and it's probably time to clean it up. Consider merging variables together first. For example: EDIT: should have kept the return there callStacks &lt;- return(getCallStack (?loc)) let callStack = Data.List.last(callStacks) -- returns (String, SrcLoc) let callOrigin = snd callStack -- returns SrcLoc Could as well be let callOrigin = snd (Data.List.last (getCallStack (?loc))) And here are a few more specific suggestions 1. We can't change the `debugMode`! 2. Why limit to a single line of call stack? It seems like an unnecessary restriction. EDIT: The call stack is one-line because we do not normally use the implicit parameter 3. Consider adding an import list (`module Debug.Print.StackTraceDebug (debugTraceIO, prt) where`), so that the test is only internal. Even better, move the test to a test suite or something. IIRC cabal supports test suites. 4. The haddock comments look weird to me. The first and last "`--`" lines are not necessary, and I don't think they should be there. EDIT: it's there to help eclipseFP Oh, and for some boring small suggestions, I would recommend using hlint to check through your code. Do note that the suggestions are only things you should consider, not things you should do. (Hlint suggestions are usually much weaker than compiler warnings, if you ask me.) EDIT: formatting (4 times). I think need a previewer or something EDIT: put in some notes according to OP
First I have to say that I may have written that post a few years ago. But I think now that this kind of self admiring stuff is bad for the haskell community and for the language. It is not because it is a bit ridiculous (it is for many programmers) but because it makes the haskell world more and more away from the real world, since it increases the perception inside and outside, that Haskell is not a programming language (i.e a tool) but something like a nonsense in search of an aesthetic endeavour that has nothing to do with practical problems. 
I don't think anyone mentioned this, but I think this "easy/hard" problem does not arise that often when learning Haskell as a second language. It arises when it's the *third*. From my experience, these are some of my reactions when learned Ruby after C++ - Wow no need to declare stuffs anymore - Block structures look different, but understandable - Classes and objects everywhere, but I think I can make do with it - Hashes *rock* - Weeee, I can write a reverse-polish calculator sooo easily. These are for Haskell - Weee powerful calculator - Wat, no (re-)assignments? Not even mutable arrays? - Hey why there's no way to sequence statements? Oh there are no statements - Lazy evaluation makes heads hurt anyone? - Sounds like an unconventional but sort of well-organized system of features, but looks like a lot to learn - What's the point, guys, of the Maybe type? Why? Because Haskell is different. Most other languages are similar. IMHO, the wide spread idea that Haskell is hard (and opposing people saying Haskell is easy) comes from the fact that other languages are *way too easy* to learn. Example? https://www.reddit.com/r/haskell/comments/44gtjy &gt; So here's my concern though: All of these symbols and keywords are (for better or worse) really darn confusing (Like the &lt;- operator confuses me as well as -&gt;), so it makes it very hard for me to just jump in and code like I could with JavaScript. See? Haskell may be easy, but definitely not as easy as only requiring a "jump" into it.
There is something mentioned here: http://www.agner.org/optimize/microarchitecture.pdf It seems to me that at most it is the inital settings of the predictor and generally there is no advantage in using such code. Of course that doesn't rule out optimizations made by the compiler e.g. by different register allocation.
I'm going to second what abaquis said ([see my reply to their comment](https://www.reddit.com/r/haskell/comments/44xqlw/studying_scip_with_haskell_rather_than_lisp/czty8kp)) and suggest that you just start learning Haskell, straight-up, given that you're more interested in it. I dabbled with Lisps a few months ago (mainly Racket and Scheme), and I'm absolutely interested in spending more time with them in the future (I want to write a metacircular evaluator, sounds awesome). That said, Haskell just compelled me much more, and though a language like Clojure really seems great (Clojure being the most obvious Lisp choice for getting real work done), a Strong Type System is really hard to give up once you get a taste of it and how much peace of mind it gives. There's obviously *so* much more to Haskell than that, but types are a huge deal. Check my reply linked above for a book recommendation and a repeat of my encouraging you to join us on IRC (Freenode server) at #haskell and #haskell-beginners, if you haven't already. People there will help you with anything. Hope to see you around sometime!
It uses your `ghc-mod` that works fine^-ish with stack.
&gt; Lazy by default is a pretty big part of what makes Haskell special since it allows us to write control structure as normal functions. This is why I consider Haskell to have done more good with Smalltalk's legacy than Java... and that irony is delicious. :)
I think this is the problem, I will see if I can build each of them separately.
I worked quite hard the last couple days on this and have to slow down now because life's calling for attention again. I hope you like it!
 foo :: Processor a -&gt; [a] -&gt; Builder foo Processor{..} xs = ...
When I requested the feature, `stack` didn't exist, and I had some problems with `ghc-mod` in my other editor. But to be honest, I completely forgot that request. Only got reminded by the email that /u/tejon mentioned today. Funny enough, I don't use VSCode anymore.
So first, binaries are only built for a limited subset of the things that are available. The first cut for that is, I believe, compiler version, so using ghc7102, rather than ghc7103, means you are quite likely to have to build everything yourself. The second cut is whether it's buildable---because if it's not buildable, there can be no pre-build binaries. And indeed, when I try to build the package with ghc7103, I get: src/Zot.hs:69:11: No instance for (Foldable t0) arising from a use of ‘foldM’ The type variable ‘t0’ is ambiguous Note: there are several potential instances: instance Foldable (Either a) -- Defined in ‘Data.Foldable’ instance Foldable Data.Proxy.Proxy -- Defined in ‘Data.Foldable’ instance GHC.Arr.Ix i =&gt; Foldable (GHC.Arr.Array i) -- Defined in ‘Data.Foldable’ ...plus three others In the first argument of ‘(.)’, namely ‘foldM ($$) empty’ In the expression: foldM ($$) empty . read . filter (`elem` "01") In an equation for ‘readZot’: readZot = foldM ($$) empty . read . filter (`elem` "01") src/Zot.hs:69:32: No instance for (Read (t0 Fun)) arising from a use of ‘read’ The type variable ‘t0’ is ambiguous Note: there are several potential instances: instance (Read a, Read b) =&gt; Read (Either a b) -- Defined in ‘Data.Either’ instance Read a =&gt; Read (Data.Ord.Down a) -- Defined in ‘Data.Ord’ instance forall (k :: BOX) (s :: k). Read (Data.Proxy.Proxy s) -- Defined in ‘Data.Proxy’ ...plus 18 others In the first argument of ‘(.)’, namely ‘read’ In the second argument of ‘(.)’, namely ‘read . filter (`elem` "01")’ In the expression: foldM ($$) empty . read . filter (`elem` "01") Incidentally, a nix-focussed question like this---and "why am I not pulling from a binary cache" is definitely nix-focussed---is at least a little more likely to get an answer in /r/nix.
Yes, any recursive call truncates back to the original call.
Makes sense. Thanks for the info.
I eneded up choosing Spock. It's lovely and has a slightly less verbose syntax compared to Warp. I've also added it to the [Wiki](https://wiki.haskell.org/Web/Servers#Spock).
the only thing hard about Haskell is the utter and total lack of proper explorable click-through-able documentation, like they have in Python. otherwise, it's peaches. &gt;:-L 
You can check if a substitute (binary package) is available for a package with the `nix-env` command. nix-env -f "&lt;nixpkgs&gt;" -qaPs -A 'haskellPackages.zot' --- haskellPackages.zot zot-0.0.2 Here for `zot` you can see that no substitute (the last `-`) is available, so it means that to install it you have to build from source. To know the reason why binary substitute is not available, you should check the hydra log for that package. (I think that haskell packages are build on a separate hydra, but please someone correct me if I am wrong) As /u/mdorman pointed, building the package from source is a good way to figure why the hydra build failed, and a good opportunity to fix it :)
 "callStacks &lt;- return(getCallStack (?loc))" ^ No, this statement MUST be in a return, if you hoist it with "let" it messes up the call stack in implicit parameter "loc". I repeat, DO NOT change "callStacks &lt;- return(getCallStack (?loc))" to "let callStack = Data.List.last(callStacks) :: (String, SrcLoc)" it will break the code.
So the interesting thing here is `-fexternal-interpreter` which runs the interpreter in a separate process. I assume that ghcjs will be able to base their Template Haskell support on top of it, and it will hopefully be useful in other cross-compiler contexts such as ARM.
~~I didn't even know about this. Now I have something to peruse while I try to come up with ideas for side projects.~~ Alas, as your original post stated, traffic there is infrequent enough to nearly render that subreddit useless. Most resent post was 11 days ago, but just a few more posts back are dated 2 years ago...
Neat. Can this be used to quickly verify if an expression is properly undergoing fusion?
:) Yeah, you are certainly right on the "you don't understand" points As for the explicit call stack thing, I thought you meant to use it, but if you didn't, then your approach is fine. Don't worry, it's not like I'm changing your code or something. Relax and don't be that nervous. Perhaps you should treat my suggestions as weaker than hlint? :)
Curiosity: is there a particular reason to release the library as GPL3?
Okay. But how should I choose those bounds? I don't know which older versions break this library.
I don't the sidebar but only haskellnews.org (which stopped working 3 weeks ago, BTW). It might aggregate different subreddits related to haskell, but to be honest, I don't know.
That's the problem indeed. Normally, just stick the current version which is actually used and extend to next major version, of if you are using version of 1.2.3.4 of a package put `&gt;= 1.2.3.4 &amp; &lt; 1.3`. That's not great, but that's better than nothing.
Thanks for the nice reply. I am learning Haskell, (reading Real World Haskell now, among others, will definitely look at the Haskell Book next) but always thought that SICP was more of a "computer science" book than a "Lisp" book. Isn't the language secondary to the lesson?
Have you installed `flycheck-haskell` package as well? Are you using some sort of “starter kit” (e.g. prelude), or it's mostly vanilla Emacs? 
You're right about me being nervous. I'm afraid that any minute this thing will break. Given that it depends on everything from the compiler version to the prelude version to implicits, it can be brittle.
is there a reason not to? servers, sure. plotting, not so much. 
Great work, Simon!
 cabal freeze is a command that emits a cabal.config file with the current exact versions. also, cabal-bounds is a package that can automatically edit your cabal file to update version bounds. 
You're right. There indeed appears to be a [bug](https://github.com/haskell/c2hs/issues/157#issuecomment-183704164) with C2HS and GHC 7.10.3 on 64-bit Windows due to the version of gcc (5.2.0) bundled with 7.10.3. Thanks for narrowing this down! I suspect that once this is fixed I can totally use your instructions to install FLTKHS on Windows. I'm going to try now with 7.10.2 and Stack using your method to see if it works.
Thanks for coming to share your experience.
What's VSCode? Is it like Atom editor? Written in javascript and run on webkit?
Yes, it's based on the same core as Atom, but MS has done a lot of work to make the experience better. It has pretty solid debugging integration for TypeScript/JS, C#, ASP.NET, autocompletion, rendering comment documentation, git, etc. Python support (debugging, etc) is being added soon. I actually use it when I need to quickly edit C++ or Clojure code. Lots of features are being added.
Remember that frp in a language that you are familliar will come with a learning curve of it own.
Nice. Great to see number-theoretic stuff in Haskell. Have you seen https://github.com/cartazio/arithmoi? It is a library for number theoretic computations with excellent performance, but, for instance, it lacks support of Gaussian integers. You are welcome to contribute, if you like.
Some other posts here have useful suggestions. But to clarify: In Haskell, there is by definition no authoritative "reference" of operators, because operators are not built in (except for one, the cons operator "`:`" for lists). Operators are just functions which people have defined in libraries, like any other functions. So if you see an operator and you want to know what it does, then just like for any other functions, you need to look at the imports at the top of the module and figure out which module it was imported from, or perhaps even find a definition for it within the same module. If the author of the code did not use explicit imports, then you may have to resort to one of the Haskell-specific search engines to look for it.
Not all countries treat public domain the same way, I would recommend you choose a copyright license that conveys the terms you intend in releasing something to the public domain instead.
Basically a version of Atom that's not constantly broken or unusable. (Yes, I'm bitter about Atom.)
I'm working on this. Sort of. I'm doing the HTML part in the context of the reflex FRP framework. It will be implemented via a typeclass and all your fields need to have instances. Sort of like the Aeson classes. That way it will work with nested structures, etc. I have a version with generics but it doesn't handle sum types very well. So I'm working on a template version--it seems easier to loop over possible constructors that way. Once that works I'll go back to generics and try to figure out how Aeson makes FromJSON instances for sum types (I think with fromConstrM from Data.Data and magic?). The FRP version basically takes a value (as long as the type has an instance) and puts up an editable form with all the fields, defaulted to whatever they were in the given value. As you edit them it updates the value--Maybe a so it can be Nothing when a field is set to an unusable value. It needs to be upgraded to getting a Maybe a as an input so it can handle forms with no given default. It will all compose via m (f (Maybe a)) where m could be IO (for the command-line) or MonadWidget (for reflex-dom) or whatever, and f is a functor to handle the dynamics of FRP. But the work is sort of the same to do what you ask since it involves traversing the structure of a type and producing a "builder" of some sort that reassembles the type from the smaller instances using, if possible, the applicative structure of m,f and Maybe. So maybe what I'm doing would generalize relatively easily to the command-line case and static HTML monads as well. I'll have to see. Once I figure out template haskell :)
It is perhaps somewhat comforting that almost none of these examples actually come up in everyday Haskell programming for typical application domains. With the possible exception of the Num hierarchy problem, but even that is almost never worse than slightly annoying. On the other hand, perhaps that shouldn't be comforting. Perhaps some of those things *would* come up more often in everyday Haskell programming for typical application domains if they weren't broken.
Just to make it clear - Spock is a simple web programming framework that runs on top of WAI and warp. So you're using all three of those.
I used to think the same way. Nowadays, I am moving towards [Unlicense](http://unlicense.org/). Anyhow, it is an off-topic discussion. /u/tepf, you've done a great job and certainly I'll be using it soon! 🙂
Yeah it was my understanding that CC0 worked better for people in states with weird public domain laws.
Unary negation is also built in :)
I'd argue that in similar terms learning Haskell is itself a major lesson in computer science. Familiarity with type systems, purity, category theoretic constructs (think design patterns except with an actual mathematical basis), and lazy evaluation are all things you gain from learning Haskell. I haven't finished SICP though, so I don't really know how much I'm missing out on, and I do understand your point. In any case, I'm glad that you're learning Haskell. Do you have much programming experience outside of Haskell? I do not, and my experience with *Real World Haskell* is that after four or five chapters it stopped being helpful to me compared to other resources, as it seemed to presume comfort with working with larger programs, a comfort that has taken a while for me to develop. Clearly you won't be at a loss if you read through one of the most acclaimed books in computer science. Personally though, I'm just waiting for http://www.sicpdistilled.com/ to be finished.
Thinking back to my C++ days it seems like call stacks would be more useful if they actually showed parameters (at least those with a Show instance), is there any work being done to add this or a particular reason why parameters seem to be omitted completely from the call stacks judging by the examples in the post?
Thanks. But as you mentioned this is similar to typeclasses, and I'm actually finding it hard to see, aside from syntax, where the real difference is. `toRep`, `to String`, and `toBuilder` are all still within the namespace, but can't be called on things without `Processor`s being included. How is that different from having class Processor a where toRep :: a -&gt; Rep toString :: a -&gt; String toBuilder :: a -&gt; Builder and then writing foo :: Processor a =&gt; [a] -&gt; Builder foo xs = ... ? Is it that you can define *different* processors for the same data? Whereas `instance Processor Int` won't have stuff that works on `hex`s without making a `newtype Hex = Hex Int` and then making `instance Processor Hex`? If so, that to me only seems like a difference in style, as instead of passing an actual Processor you use the `Processor a =&gt;` constraint and marshal to and from the appropriate newtypes. Well, actually, I guess it would also mean that there would now be instances that would get exported, but I'm trying to think in terms of working in a single file. I'm not trying to critique the pattern you're showing, I'm just trying to understand if it's stylistic or if it comes with functionality that I'm not yet seeing.
For me it's that it uses features I don't really feel like I understand, and fear I won't be able to debug if something goes wrong. (Arrows, profunctors). The types seems complicated enough that if I hit an edge case not covered in the docs, I'll be lost. It looks like it uses postgres-simple connections, so there should always be the escape hatch of droping down to writing the raw sql, so I'm probably just going to give it a shot.
Okay, cool. Thank you. I wasn't sure if there was more to it. I'll have to try it out and compare the coding experience.
"rsaGenKeys :: Integral a =&gt; a -&gt; a -&gt; Either String [(Key a, Key a)]" I would never use a library made by someone who doesn't know to not roll their own crypto.
I haven't seen that yet, I'll check it out. Thanks!
Ok, this confirms my suspicions that it's 7.10.3 + Win64 + C2HS that is causing the trouble. Your trying again is very much appreciated.
Fair enough, though it's the basis of general recursion in Haskell. So it is kind of important.
Probably meant that "we know that it returns a `Maybe Int` and doesn't curse out grandma." Of course, this is false if using GHC (although curse out grandma errors are still rare.)
I wasn't trying to state it as a guarantee (which is why the corresponding property is under the 'documentation' header). What I was trying to say it's a useful thing to be able to tell what a function *likely* does based on its signature (as evidenced by the Hoogle link), and this is a very useful thing.
Note that if you require the inferred type of the implementation to match the given type then that constrains the problem space much further
Speak for yourself, it called mine a piece of skolem just yesterday
I don't consider myself a stack user and I currently use both (even sometimes on the same project!). However I tend to start new project with stack just because it doesn't have to recompile everything so it's much quicker. And also because it manages different version of ghc so I don't have to upgrade ghc manually. I'm not using it for its curated properties.
I'm surprised, stack manage to use the correct version of ghc and the correct libraries so I assume it could handle the correct version of ghc-mode. Like stack exec ghc-mode ... I'm apparently wrong.
right, because the first would be inferred as x :: a -&gt; b -&gt; Maybe c great point!
ghc-mod is a bit of a special case since it links against GHC. That means it's essentially another copy of the compiler that stack doesn't know about. It thinks it's just an executable so it doesn't do anything special to it.
Ok, just found the way: https://plot.ly/javascript/line-and-scatter/ Great!
Well, the laws for `abs` and `signum` are weak enough you could just declare `abs = id` and `signum _ = 1` for a custom Gaussian integer type -- they don't actually have a stated relationship to other parts of `Num`, but it'd still be awkwardly separated from the normal `Complex` type.
It would solve it by insisting your users depends on a global version specified by Stackage, in the same way that you can specify dependency on a set of local versions using cabal. Its not as open to say the least as specifying bounds at local level, but at least the set specified is agreed upon/vouched at collective level. Stackage and PVP are good answers to different problems, both legitimate. How to make them dialogue nicely is the challenge. Once done it will benefit both.
Is it, though? Functions are "lazy" in any language, and I'd argue recursion with functions is far more common than recursion with variables.
I'm not sure I agree that Stackage &amp; PVP are answers to *different* problems. Don't both aim to specify compatibility between package versions? To me it seems they're opposite answers to the same or at least overlapping problems. With Stackage running into scalability issues as it attempts to keep all of Hackage in lockstep, whereas the PVP is more flexible by allowing locally consistent regions of valid install-plans. I very much prefer the latter, as it allows me to benefit from recent releases without having to wait for Stackage to pick them up, and packages I don't even need nor knew they existed to be updated to be compatible with those releases.
Yes absolutely. Sometimes forcing everything to be within one particular stackage set just doesn't work. Stack allows you to specify versions manually in that case, but then you are back to working out dependencies. And I imagine that the algorithm for working out a stackage version set to begin with uses dependency bounds in some way. If it doesn't - it ought to. The valuable information inside a library developer's head about what approximate ranges of dependency versions the library might be expected to work with should be shared with the world. That is true no matter what build tools we are using. And the language we have for expressing that is dependency bounds.
I apologize for not having time to read your post in detail, but a few things that significantly speed up my dev cycle are: * using ghci and ghcid * building with -O0 during development
Using `stack build --fast --file-watch` helps me a lot. I even do `stack test --fast --file-watch` to run tests immediately too.
[This post](http://chrisdone.com/posts/making-ghci-fast) awhile back helped to greatly improve the edit-compile loop for me with some of my big projects.
* (1) You can use it with SDL/SDL2 or other, only the examples uses GLFW to create window with OpenGL context, you can choose from various libs to do that also. * (2) Sure, 2D can be done, e.g. during vertex transformation skip the 3D perspective projection step. * (3) The current system can generate pipelines for WebGL1.0/GLES2.0 and OpenGL3.3. However the lambdacube-gl package is for GL3.3, and we have a PureScript backend implemented for WebGL. Would you like to use GLES from GHC?
I use ghcid and recompiles are normally super fast, even with a large project.
Functions are not lazy in any language. The reason why recursive function calls terminate in other languages is because you have conditionals in them which provide the laziness. Otherwise a recursive function call takes forever (or until the stack blows). If you want to make a distinction between "recursion with functions" and "recursion with variables" here's a case where if2 being lazy is important: factorial x = if2 (x == 0) 1 (x * factorial (x-1)) It should be obvious that without laziness this is not going to terminate. Further this is clearly a recursive function.
The large modules point is worth repeating. If you end up with a large Haskell file, GHC usually gets unreasonably (beyond linear, I'm pretty sure) slow at compiling it.
Thanks, the only pathological thing I've ever seen is this file: https://github.com/blitzcode/ray-marching-distance-fields/blob/master/Font.hs Takes about ~35s to recompile this, I suspect the compiler chokes on the large list literal in the file. That's the one thing I can think of where filing a bug might be worth it. I tried the show-passes thing, but I'm not sure I can interpret the output, Hm ;-(
&gt; If you can provide your codebase I'd be happy to chat about specifics. Agreed. There's not much anyone can do without being able to see the code.
Odd, for me, on GHC 7.10.3 it takes only 25s to compile the entire project (with stack build) from a clean repository clone after installing dependencies. Font.hs is certainly the slowest file but only takes a few seconds total. A recompile when appending a newline to Font.hs takes 15s (including the recompiled App.hs and linking). Did you make any significant changes already since you measured your numbers?
I compiled the whole project (stack with GHC 7.10.3) and it took 36 sec. Recompiling just Font (and App and linking) took 23 sec. Recompiling just Font with `miscFixed6x12Data = []` (and App and linking) took 9 sec. So yes, something funny is happening with that list literal.
It's not completely implausible that your computer is just 3x faster than /u/SirRockALot1's!
Slower compared to what languages? Slower than C for sure, but that is not Haskell's target domain (systems programming). 
Aside from what everyone else is saying here: Yes, we are aware the compiler performance has tanked a lot over recent releases. No, I don't think it's fundamentally anything about Haskell (or GHC itself necessarily) that makes it so. I think at this point it's basically our fault and a failure of our development process. I actually had extensive discussions about this with people in New York last week. I would say it felt like almost every professional Haskell programmer cornered me and brought up compiler performance. It's gotten really bad. So bad I'm sort of annoyed to even talk about it, because the only possible changes to fix it would be so radical as to upset our current "development ethos" (more later). Ultimately, I think all the talk about modules being smaller or using tricks is pretty much a red herring, at some point. You have to bite the bullet and just call a spade a spade. It's slow. We can only "hold off" for so long with things like this. If we have to do 40 extra things to make a rebuild fast *now* and the compiler gets 15% slower *again*, what are we even doing? It's a lost cause of hopeless complexity. Second, the inability to have fast iteration times has a *real* impact on your ability to write code itself. First off, Haskell code tends to be general and deeply reusable, so the "smaller modules" thing doesn't really cut it: most of your dependency trees are very "deep" anyway due to so much reuse, especially in larger applications and libraries, so you're not ultimately getting a huge win here (just a theory). I also don't think `ghc -j` will ever really be scalable for this reason either, because most Haskell code admits too deep a dependency tree to meaningfully compile in parallel, at least at package-boundaries (we'd need module-level dependency tracking like GHC's build for any sensible level of parallelism I'd think). But more than that, you cannot compartmentalize code correctly. Let's say I have 20 libraries/apps at my company, and 10 of these are user-facing, while 1 of them is the core "business logic" library. When compile times get bad enough, you are *actively discouraged* from updating or modifying that "business logic". So instead, you modify each of your "10 applications" and add little helpers/fixings into *their* individual `Utils.hs`, and only move that stuff to `core-bizniz-logic.cabal` months later and tidy the stuff up. Because you don't want to eat a 20 minute compile time *now* for what is really a very small, sensible change. Humans are very bad at this risk calculation, so they will *always* take "20 minutes of time later" vs "frustration of any kind, now" That sounds ridiculous when Haskell prides itself on code reuse. But I had someone in this basic situation talk to me in NYC, and they'd rather *duplicate changes into each 10 projects* for a time, before eating the cost of moving some common changes into the core library! That is a real situation! That's a complete and total failure of a development tool, if it encourages that. The only thing at this point that can be done, IMO, is to just strictly never accept changes that ever make the compiler slower, in any reasonable benchmarks/tests we have, and if you want that to happen, you are always responsible for making the compiler faster in some other ways to compensate. Otherwise your change is not accepted, simple as that. No percentages or random imperceptible changes in some statistical calculation. It's an absolute number, and either it stays the same or gets smaller. There should be no exceptions for this. Ultimately I know many of the developers we have will not like this change and it will seem "overblown", but truthfully I think it's the only sensible way forward, and other teams have done this successfully. But it's not going to go over well I think, and people will consider it "nuclear" I'm guessing. It's a pretty huge change to propose, TBH. **EDIT**: Just to be super clear about this, I'm not calling anyone out here specifically. I'm just saying, developers are in general picky, and we all suffer from "don't rock the boat" syndrome. Hell, you can make an *improvement* and people will slightly grumble sometimes. The proposed change here is *huge*. Again, it's factual people will simply dislike this change in process, no matter if it makes life 100x better even in 6 months. I also chatted with someone from Microsoft who had insight into the C# compiler team, who was never allowed to ship anything if *any compiler benchmark ever got slower*. Their compiler has never gotten slower over 6-8 years, dozens of releases and improvements. The Chez scheme people modified their compiler *and added over a hundred new optimization intermediate representations* with the Nanopass framework and it never got slower. They never accepted any 'optimization' that did not at least pay for itself when bootstrapping. These examples are worth thinking about for their diligence in execution. 
Try compiling on ARM. Compile times are beyond ridiculous, not to mention the large memory requirements. 
I would be shocked if it wasn't, I measured on the ancient laptop ;-)
Do you use a REPL developments style? Doing :r reloads in 'stack ghci' is very fast for me.
Oh dear, yes, `-dshow-passes`.
Oh yes, -dynamic is HUGE. I was so happy when dynamic linking got fixed on OS X and now stack seems to build all libraries as both shared and static, so I can just switch on -dynamic and save myself 15 seconds of linking...
&gt; You have to bite the bullet and just call a spade a spade. It's slow. Thank you for that. I usually have a "Foo.Types" module in my libraries, with all the data types and TH stuff (lenses &amp; aeson mostly). The compile time of this module is usually atrocious, and all other modules depends on it, so I am indeed often reluctant to refactor it! `hdevtools` helps a bit. But to answer the OP question, it seems that you are basically saying that nothing is being done to address the performance issues, but also that it is unlikely it will ever be addressed ? What is currently slow? Tricky type inference algorithms that are hard to understand, or more mundane stuff that could be solved by an average Haskeller with a bit of looking around?
Perhaps, but that actually slowed it down in my super informal shitty test (pass in `-threaded` when linking, pass in `+RTS -N2` for my 2-core machine)
Possible but unlikely if he considers C++ compile times fast on his machine because my experience is generally the opposite (as a Gentoo user) and if OP considers buying a new machine as an option that is unlikely to help since mine is about two years old by now and was in the mid-price-range for desktop PCs at the time (certainly not high end).
Thanks for this lengthy reply! I was thinking before posting that the policy regarding performance regression you describe is what I would've considered. It's also not just the compiler getting slower, it's also how we use it. Things like lens suddenly mean a lot of modules have a TH pass in them to generate lenses. I can imagine that a lot of the features in GHC are contributed by individuals who really care about their particular feature and might be upset if their contribution can't be merged into the mainline GHC because it causes a performance regression. For many GHC is a research vehicle, but for me it is a practical compiler. Thinks like reproducible builds and stack traces have a much bigger impact on my productivity than the latest type system work. In the past I heard people suggest that it might make sense to split GHC into a research and a production branch. I really feel the impact of compile times on certain types of work. If you're doing graphics or game development fast feedback and tweaking / visual debugging is essential. I always thought that GHC developers must feel the impact of slow builds the most. It's pretty much the largest Haskell code base out there, isn't it?
I haven't been able to upload this to Hackage yet because it's not sending me the confirmation email when I try to register for an account. Any help with that would be much appreciated. EDIT: I tried a different email address and that worked. http://hackage.haskell.org/package/streaming-png-0.1.0.0
Thanks for your answers. I'm indeed looking for GLES from GHC. I'm currently looking at Kmett's gl package, as it allows me to specifically target GLES. But my project now uses the OpenGL package.
I'm perfectly happy with C++ build times for my small-ish projects. I had my complaints at work about &gt;1M LoC codebases, but it never reached the 'I just can't deal with this anymore' level of annoyance that I have with GHC. I mean, I have a C++ project that's a single 5000 line file which compiles in ~5s on my slowest machine, and I could invest a few minutes to get that down to &lt;1sec by splitting up the code and enabling parallel builds etc. If I had a 5k lines Haskell source file I'd probably be going nuts at the 30-60 seconds this file alone would take up.
&gt; But to answer the OP question, it seems that you are basically saying that nothing is being done to address the performance issues, but also that it is unlikely it will ever be addressed ? I'm not saying it won't be fixed - just that right now, I don't know what timeline that could happen on, whether the majority of people are OK with this being a refocused priority, and exactly what we have to do to support people, so we can make that happen. It will require work and retooling to support these kinds of processes. If we want to do this, we will have to sacrifice other things. Aside from that, these were all very recent conversations, but it's gotten bad enough where I think something does have to happen, soon. So yes, I would expect things will happen here. &gt; What is currently slow? Tricky type inference algorithms that are hard to understand, or more mundane stuff that could be solved by an average Haskeller with a bit of looking around? Hardly; I am almost certain that "Normal" Haskell programmers could make improvements (I use scare quotes because I strongly believe GHC is "just software", it merely operates with many different conceptual requirements than most), I just don't think we're looking for them. More importantly, it is not ingrained in our development process to win back improvements continuously. It's probably tons of small things like, "Use strictness here helps a little", "Search a 5 entry array with linear search, not binary search", "simplify this other code, oh look, we can delete some" (these aren't exact examples but I mean, I've done all this stuff before, it's very common, rote optimization nonsense.) Efficiency is a feature; performance is just some after effect of some other thing. We have to continuously program with efficiency in mind. The other problem is humans have very bad cost associations in their head about how these things work, typically, including us. Everyone wants a big boost, but those rarely happen. So if it takes someone 2 days to find a 0.5% performance win, that is "a huge waste of time". But if you zoom out, that means in just a week you can improve the compiler by a solid 1.5-2% almost at that rate. In just a month, that's nearly 6%. And in the process of doing these things, you sometimes *will* find nice things that get you a solid 1-2%, or even 5%. You have to trim the low-hanging fruit continuously, because it grows back quickly. That's an enormous win, for a relatively modest cost, in all honesty. It merely seems like a sunk cost because it has to be continuously paid, immediately, where as features tend to have a larger "up front cost", so the relative tail-end isn't so bad (*despite* the ongoing costs from continuing maintenance - we fundamentally don't think of these two things the same way, but they're similar). Furthermore that 6% is spread over every individual compile and user, so it speeds you up even more over time, especially as GHC does things like bootstrap itself. Once you begin putting these pieces into place it becomes *very obvious* that this work should be ingrained in the development process itself. So, we need to do that.
Same here - after switching to stack I abandoned ghc-mod for now - it's just not worth the constant trouble I had inside Emacs
&gt; I can imagine that a lot of the features in GHC are contributed by individuals who really care about their particular feature and might be upset if their contribution can't be merged into the mainline GHC because it causes a performance regression. For many GHC is a research vehicle, but for me it is a practical compiler. Thinks like reproducible builds and stack traces have a much bigger impact on my productivity than the latest type system work. I imagine at some level there's a matter of publication deadlines, etc. But in another sense we just need to change how we approach development, so that these costs of doing business are easier to bear, the bar you need to clear is more visibile, and the tools are available to help. If you equip people with tools to handle the problem, they can handle it, but without help it will be more frustrating. It's no wonder people let performance get worse when things like our benchmarks are arcane and old. I do not think that GHC being fast and GHC being a vehicle for exciting new stuff is mutually exclusive. Rather, we have excessively focused on one, and left the other unchecked. It's very easy to see when your feature completely breaks the compiler (mostly). It's harder right now to see when it makes it actively harder to use, due to performance. &gt; In the past I heard people suggest that it might make sense to split GHC into a research and a production branch. This would not work well, considering we have a relatively short amount of labor in supply. Everyone is a volunteer (well, *practically*, but both Ben and I have very real bandwidth constraints), so it's no wonder none of us really want to double the maintenance workflow. I think Ben would agree just maintaining the STABLE branch today can be a chore. :) &gt; I always thought that GHC developers must feel the impact of slow builds the most. It's pretty much the largest Haskell code base out there, isn't it? Our build system is bespoke (for good reason) and has far better dependency tracking and tools available to it than what other tools offer. So, in the fast case (working on `stage2`), a lot of changes can be rebuilt in the 2-3 second range.
I never use the compiler in development, neither ghci, since it is not useful for multithreaded programs. runghc will "compile" much faster and will execute the program.
&gt; Is GHC being profiled? &gt; If so, where are the performance hot spots? I don't think it would be too meaningful right now. My experience with the built-in profiler is that it gives good hints about what part of the code is taking a lot of time (or making a lot of allocation), but there are two problems: * the profiling builds don't have the same performance characteristics as the standard builds * the cost centers might be misleading in the presence of optimizations I suppose that the DWARF support that is coming will help with the first item, but I am not sure about the second ...
&gt; Nobody likes to opt into fewer warnings. `-Wall` is too much of a nanny, especially when it comes to things like shadowing, so I don't routinely use it. Other `-Wall` nanniness includes `-fwarn-unused-do-bind`. I only routinely use `-W`. If I get a bug I can't track down I can always flip `-Wall` on temporarily. As the linked email says, using `-Wall` and expecting it to be warning clean everywhere is a bad idea, especially with all the recent `base` churning. Certainly I would never use CPP just to make `-Wall` shut up. Just develop with `-Wall` and then turn it off rather than adding CPP ugliness. Overall it would seem to me that `-Wall` is supposed to warn of any potentially smelly stuff even if it isn't really smelly so I would throw `-Wcompat` in there too.
This pay-as-you-go rule is intriguing. It's like treating compile-time as a budget. Fascinating.
On the other hand I have had 1k LOC files in C++ which provide about as much functionality as a few lines of data type declaration and a few deriving statements in Haskell. The LOC are very hard to compare between any two languages. C++ usually derives its compile times from its include system which often results in 100k+ LOC compilation units after all files are included, not from doing anything particularly usefull with the code. In my experience the longest C++ compile times are when lots of template metaprogramming is used (e.g. some boost libraries) where splitting up files is not feasible. In my experience you would be hard-pressed to find a single project where the actively worked on part was more than 20k LOC or so though as Haskell has a culture of small libraries so most of the code in truly large projects is in the dependencies.
&gt; -Wall is too much of a nanny, especially when it comes to things like shadowing, so I don't routinely use it. Releasing to hackage with `-Wall` turned on is a current common practice. As you note, it isn't universal, but it is very common. The main concern is that `-Wcompat` will start complaining at you about things you can't really act upon yet without using CPP to hide it from older versions of GHC. &gt; Certainly I would never use CPP just to make -Wall shut up. `-Wcompat` today will start complaining about missing `MonadFail` instances, which means you need to pick up a dependency on the `fail` package to backport them or hide those instances behind CPP. On the other hand, we aren't going to switch the desugaring over to use `MonadFail` [until GHC 8.6](https://prime.haskell.org/wiki/Libraries/Proposals), and in 8.4 the warning would be actionable by folks who were willing to limit their support to the last 3 GHCs and will move into `-Wall`.
Rejecting any patches that degrade performance sounds good, but is really bad policy. As you said,most people working on GHC are volunteers. I can't find where I got this phrase from, but open source projects that don't gain contributors faster than they lose them die. Rejecting people's work is a great way to get them to spend their time elsewhere. Eventually you'd end up with no one around to work on performance.
&gt; I always thought that GHC developers must feel the impact of slow builds the most. Ha! GHC developers don't profile their code, so they can use `-O0`!
Can you see if you get a similar result? The gist I linked should work as a replacement for Main.hs. In some sense it stands to reason, since only `main` is exported, the compiler can get a view of what matters, but compile time for Main seems to be shorter, though I didn't test that repeatedly. 
Yeah, llvm. The compiler has made some good progress on arm the past year or so - working ghci is a breakthrough. Here's hoping for template haskell cross compiling, that would really be helpful. 
I'm not actually seeing that strange behaviour. The original source takes about the same length of time as the original source patched with your new Main (8ish seconds). EDIT: More precisely, compiling the original App and Main takes the same length of time as compiling the patched Main.
&gt; But yes, I also remember the compile time going up significantly. Indeed, I suspect that much of the problem is that we ask LLVM to do a lot of optimization, despite having already done a lot of optimization in GHC. It would be useful if someone could go through the optimization passes that we ask LLVM to perform, evaluate how they fit together with those performed by GHC, and eliminate those that are redundant. There is a [Trac ticket](https://ghc.haskell.org/trac/ghc/ticket/11295) to track this task; I'd love for someone to pick it up. &gt; Here's hoping for template haskell cross compiling Now since we have remote GHCi support I think there is hope for this.
If you alter one of the files that App.hs uses, Main.hs + App.hs just take 8-9 s? Hm. Unfortunately I have to run I will look into this later. 
The problem is, I'm not sure of any other path to fix this. And I'd rather not leave this unfixed. It might be nuclear, and there may be exceptions, but something drastic must happen. First, one of the big problems is we don't have good means of characterizing performance failures right now in a lot of ways, although it's getting better. This doesn't require a lot of compiler knowledge but it's often boring; people tend to just say "it's slower" and you can SEE it is slower, but rationalizing that into an issue is more difficult than it should be right now. That needs work. When you have tools to help you pinpoint these things and make sure they're kept track off, individuals can be far more effective at combating egregious perf problems and fixing them. This lowers the burden of your patch having to meet the performance criteria, because you're not shooting in the dark. Second, you can't simply have a single person who drives down costs for example, because you can only individually sit there for so long fixing stupid performance issues before you get tired of it. Unless someone was actively getting paid to continuously improve compiler performance, we can't simply have singular individual people do it. There were even people in NY who said they would possibly pay people to do this - it's that important. Finally, the idea that GHC will magically "go away" tomorrow is overblown. Yes, some projects can die on the vine, but GHC has lived for 20 years and is not one of them. Short of everyone getting hit by a bus, I think we could *probably* live with some people having to do some extra work. Haskell is closer to the "Threshold of Immortality" than you think it is, and GHC will surely *survive* even if people aren't merging 30 new major features into every single release every year. Short of me revoking everybody's SSH keys or getting hit by a bus, it's hard for me to imagine how this would so deeply negatively impact the current developer prospects as to be deadly in the short term. It would really suck, but not be a death blow. And it's hard for me to imagine how it would spell death in the long run either: if this was a goal of ours, *we would be optimizing to make it easier to achieve*, and over time, it would be easier. That's the difference: we're not focusing on this. If we were forced to focus on it, I can bet things would look very different. But nobody is paying right now for any of this to be done. And we're still fine, even better off than we ever have been in # of contributors. Yet fundamentally, there has to be a change in the actual development process for it to continue being sustainable. So we have to make everyone do it. But the problem is... &gt; Eventually you'd end up with no one around to work on performance. There already is nobody working on performance, and that's part of the problem too. The only way to change this is to change the incentive/cost structure around penalizing compiler performance, and that means penalizing things that make it worse. There can be exceptions, but when your compiler gets 10% to 15% slower like, every release, it's not an "exception" anymore when you let through slow code. It's the "normal". Or, we don't change anything, and just accept the fact that with time GHC will likely be the slowest compiler around, and we'll need to think up new ingenious "hacks" to make life tolerable, ad infinitum. I would rather not do that.
Though, in the case of instances, such as `Semigroup` and `MonadFail` we want popular packages to pick them up asap, so they're available for other packages to rely upon, and thereby have a better chance of extending the compatibility window. To that end, I've tried to make sure we have `MonadFail` and `Semigroup` instances in place in most if not all the packages bundled with GHC, as those are less flexible to update.
&gt; Is GHC being profiled? We have /u/nomeata's great [gipeda](https://perf.haskell.org/ghc/) interface but we badly need a larger selection of performance tests. &gt; If so, where are the performance hot spots? (my apologies if the comments below are obvious) Like most performance analysis problems in large software projects, it's very hard to tell. One of the tricky things about compiler performance is that users write programs which exercise vastly different parts in the compiler even within the same library. One module may stress the typechecker, while another may trigger some suboptimal behavior in the Core-to-Core simplifier, while yet another may hit a quadratic case in the C-- optimizer. In the best case you find an implementation of avoidably non-linear complexity in the compiler where you can simply make the program bigger until it's crystal clear where the culprit is. Many of the performance tickets we get are of this nature and they are typically straightforward to fix. This is distinct from the observations made here, however, where the compiler is just generally "slow". You can profile, but interpreting the results of a profile can be very difficult as you have no idea how the profile of an "ideal" compiler would look. The fact that previous GHC releases are faster does indeed help as it gives you something of a reference for what costs should look like, although it's still far from trivial to interpret. The best you can do in this case is to use your intuition to choose from among the hot spots which is the most likely to be "too expensive." Then try to work your way through the implementation, build a model of its costs, and compare against what you believe the costs should be. If you are lucky you will notice something suboptimal; if not, move on to the next most likely hot spot. This process doesn't necessarily require a vast amount of experience in compiler design or even GHC internals. It does, however, require a large amount of time and a fair bit of thought. &gt; Is there a wiki page on how to begin contributing to optimizing GHC? /u/thomie pretty has this covered in his response.
Public domain means "entirely free to use, share, or modify, even commercially". Where that isn't legally possible, I guess I can fall back to MIT License, but that change will happen later.
I don't have these problems, but I can only guess about why not. I have 112k lines at this point, spread across 576 modules, usual module size is around 200-300 lines. It compiles to a 27mb binary, dynamically linked. Not sure if that's big or small, relatively speaking. I use shake to build, a "do nothing" build takes 0.7s (it used to be 0.2 but I haven't gotten around to figuring out what happened). A rebuild when a few leaf files changed just took 3s when I tried it. In my experience optimization makes a large difference in build time, depending of course on how many files need to be rebuilt. One thing is that dynamic linking improved link time a lot. Also since I'm using shake it's fully parallelized, even though due to some bottlenecks it can't always get enough parallelism. I haven't bothered to try to solve those since it's been fast enough. This is on a 2013 macbook pro, 2ghz i7 with 4 cores. I don't use TH or boot modules. I've actually done quite a bit of annoying gymnastics to avoid circular dependencies, because when I tried boot modules they were even worse. I use fclabels and I "write" lenses by copy and paste, it's just one line per field and seems a small price to pay. I spend much more time updating the documentation for each field than its lens. I use ghci for instant typechecking, so I don't actually compile that often. Since I use ghci for tests most of what I'm doing is modifying some function and its test, hitting :r, and typing the test name again. Some tests don't work in ghci due to C++ dependencies, but I have a special shake rule that just builds the files for that one test, it takes a second or so. I know there's a "shake server" thing out there that I think watches files and rebuilds when they change. I wouldn't use it to avoid wasting battery, but presumably if you are plugged in it could reduce latency. I also wrote a persistent ghc server to retain .hi files in memory which in theory should make things a lot faster... I never used it myself because my builds are fast enough but I heard at least one other person is. Also ezyang is integrating shake into ghc, this might be able to get the same result in a more reliable way. I think the biggest thing is the tests, actually. Since I write tests for most things, I don't often need to run the whole app. But even when I do, a compile is usually &lt; 5s (a complete build just took 20s). And I can use that time updating the TODO list, or comments, or even going over a darcs record if I'm pretty sure this is the last change. And honestly even if I am totally blocked and have to wait until the compile completes, it's nice to have a few moments to stare. It's true though, compiling the C++ parts of the project is way faster than the haskell parts :( Now compiling Java at work is basically as painful as you describe. Even with a super sophisticated build system and a build farm. And java does very little compile time optimization, so it should be really fast right? I think it's due to out of control dependencies (multi-gb output binary) and lack of REPL and high latency to engage that giant build farm, and writing those giant jar files. To me this hints that how you set up the project has a bigger impact than compiler performance.
By comparing stack to Make you are making an apples vs. oranges comparison. Make only deals with dependencies between your files, not external dependencies. You should at the very least compare it to something like CMake or autotools. I think you are also mistaken in your assumption that C++ does more optimization than Haskell.
sup, sorhed
As I see it there are two extreme ends of the spectrum of possible users. Those who want to avoid changing existing code for as long as possible without running into warnings and those who want to write code now that will still work for as long as possible into the future without requiring changes. The former type is probably dealing with legacy code so they have been around for a while already. The latter type might include existing and new users. Wouldn't it make more sense to have the group that consists largely of existing Haskell users go the more complex way and enable -Wall -Wno-compat in their packages while the latter group gets the benefit of as much information about future Haskell versions with the easier -Wall?
Unn, I don't experience this, though most of our projects are small (micro service architecture) and I also don't do "build" in my workflow. I use ghci reload (most specifically, ghcid) to rebuild since that's faster than rebuild. 
Nice, if it wasn't for onsite work I'd apply for such a unique job.
I don't agree that it's an unfair comparison. If stack runs the equivalent of a full autotools configure on every build, that's a problem with Haskell tooling, not with the comparison. Even if we take the build system completely out of the picture and just measure the time the actual compiler &amp; linker run there is still an order of magnitude difference. I didn't say that I assume that C++ does more optimizations.
As a counterexample, Linux is draconian in accepting patches, and it's still a successful open source project. GHC may well be the Linux of typed functional programming.
The most interesting part to me is that we don't have to wait much longer: &gt; Those features are already there, if the viewers are interested in trying out this new version, which I was just talking about, where the types and kinds are combined together and that does extend the expressiveness of the type level programming. My student Richard Eisenberg has a branch of GHC on Github where it’s available, the types system extensions, they can play around with it. And he’s currently working on merging that branch back in with GHC. So it shouldn’t be much longer before all Haskell programmers can play with it. &gt; [...] so we plan it to be in GHC 8, the next major release of GHC. 
 repeat :: (a -&gt; a) -&gt; (a -&gt; Bool) -&gt; a -&gt; a repeat x y z | z == y = z | otherwise = repeat x y (x z) edit: I probably should have specified, I wasn't offering a solution, I was just reformatting it to make it easier for people to read since OP didn't format his answer properly. 
I just recently started using ghcid -- I so regret not knowing about it sooner!
I hate tests. I normally fill my code with fatal assertions (which are turned off for production mode) just to cut down on the number of tests I have to write. Fatal assertions are kind of like tests, but I don't have to run them. 
Thanks, you fixed it! The problem was that I was setting the pitch to the width of the image, instead of 4*width (it is a RGBA8 Image) as you explained.
I second everything that @bgamari said. 45s is definitely unreasonable. Is that 45s of recompiling dependent modules after a change? Relinking? or what? You didn't give much detail. I'm pretty sure we could solve this if you provide more info.
I had a quick look at the code and as far as I can see it's nice. Looks to me like using Streaming was not too hard for you. Is that true?
Opting out, in a cabal file, would require the use of an awkward if impl(ghc &gt;= 8) ghc-options: -Wno-compat -fno-warn-redundant-constraints in your cabal file today. On the other hand, leaving `-Wcompat` out of `-Wall` and doing nothing wouldn't change anything from the status quo. Note: You'd still get warnings before you have to act, at a time when you could do so in a backwards compatible manner. Opting in on the other hand eventually becomes part of `-Weverything` which can include a bunch of stuff that isn't strictly a compatibility warning. Things like warning about unused constraints and stuff like that that aren't always actionable (because the class might just be supplying a law) also fall into that bucket. (Though IIRC, that is slated for 8.2.) The `-Wall` vs. `-Weverything` distinction is fairly common in other compilers, but with the two options fused it'd be a much more awkward `-Wall; if impl(ghc &gt;= 8) ghc-options: -Wno-compat -fno-warn-redundant-constraints` vs. `-Weverything` split. Note: Given that both Simons are on the other side of the debate, I fully expect to lose this debate. I also expect that the result will be messier than it could have been.
comonad too, no?
No problem, I'm happy to answer. When I first looked at conduit and pipes I had no idea what was going on, and fairly quickly ran away from it. But I think that was mainly because I didn't have a great deal of experience with Haskell at the time, and also partly because I didn't quite understand the significance of streaming in general (as in, why it was particularly necessary). A couple of months later, I had a brief interaction with conduit while writing a webserver with Yesod. I remember it taking me *ages* to figure out how to actually read from a file with conduit. This is because that functionality is hidden away in [conduit-extra](https://hackage.haskell.org/package/conduit-extra), and not actually part of the main conduit package. The main package could definitely do with a prominent link to conduit-extra, which is basically essential for anyone writing applications (as opposed to libraries) using conduit. Fast forward another few months from then and I was confident enough with Haskell that I no longer found the streaming libraries daunting as a concept, and I used pipes in a somewhat complicated way in my [robo-monad](https://github.com/bch29/robo-monad) project: logic for each individual robot is its own pipe, and they all hook together along with the graphics pipe using pipes-async in a tight loop that I spent a long time trying to optimise. I found pipes as a concept much more challenging than conduits, to be honest, partly because of all the category theory stuff and the fact that pipes are way more general than what almost any user needs from them. Thankfully /u/Tekmo's excellent work on documentation helped me get over that hurdle without too many problems. At the time I still didn't understand [pipes-group](https://hackage.haskell.org/package/pipes-group) though, which is basically what the streaming library is -- only the latter is like the distilled essence of the idea. Compared to everything else out there, streaming's solution to the 'perfect streaming' problem is by far and away the most intuitive, at least as far as I'm concerned. If you're not aware, the 'perfect streaming' problem I'm referring to is the problem of splitting up a stream into multiple streams that are supposed to run one after the other, while never having to load anything more into memory than what resulted from the original, unbroken stream. For example, splitting a streaming `ByteString` up into lines delimited by `\n` characters. If you do that naively and just produce something isomorphic to a `Pipe Word8 ByteString m r`, you end up having to load each `ByteString` into memory as you encounter it -- which can have disastrous consequences if a malicious user gives you a file with a single line that's 50GB in size, for example. But yeah, I recently discovered streaming and streaming-bytestring, and they're super-easy to use once you have a bit of experience with the API, which doesn't take all that long. /u/michaelt_ did a fantastic job with the documentation available on the [github page](https://github.com/michaelt/streaming).
Thank you :)
Well this is exciting. I needed a better IMAP library for an experiment the other day. Thank you for creating and sharing this!
It's been my day job for more than a year, so I'm constantly practicing. :) It's usually when I attempt to do things the Haskell way that I run into snags, writing OO style is less problematic.
How? Do your projects consist of only a single file? How does this work?
Today I compiled Cocos2D-X on a dual core with 8GB. It took around 40 minutes per architecture and build type. 45s is nothing compared to this.
Thanks mdorman! That makes sense. In general, is your experience that only the latest compiler version is available from Hydra? The same thing happens not only with zot (picked that one as I don't have it installed locally, so I can copy paste), but e.g. with parsec package. I.e. I hope I don't have to upgrade all projects to use the latest ghc every time it's updated to benefit from the cache - is that really the case? I agree, could have posted on /r/nix to get more exposure, though I'm pretty happy what I got here so far :)
I am one of the people who cornered Austin last weekend in NYC and would be happy to provide further details on any issues we are observing with GHC slow compile speeds in production. One note I sent out to ghc-devs mailing list last year can be found [here](https://mail.haskell.org/pipermail/ghc-devs/2015-April/008828.html), though I must apologize for not following up with a formal ticket on trac. The one point I would like to make clear, in summary here, is that I have been personally observing GHC get slower and slower with each major release over the years. It is easy for me to spot it because I have exposure to large projects I have been compiling for years and over multiple GHC releases. The productivity loss, let alone the psychological cost, has been real: Just going from 7.6 to 7.8, we took a major hit in compile times that ended up breaking even only because we could now use multiple cores instead of a single core in compiling the application using modern beefy desktops. GHCI compilation in comparison, which is single threaded in 7.8, went through the roof - possibly doubling in time if not more. As a result, we/I have had to change our day-to-day workflow and resort to swapping -f-object-code and f-byte-code all the time to have (albeit somewhat degraded) REPL fast-typechecking during development.
While Chez is awesomely fast, that isn't quite correct about nanopass. After a long history of never accepting an optimization that didn't make the compiler faster rather than slower, Kent decided that they had made it fast enough that they could take the 50% performance hit that nanopass caused. Note that after this, a compiler bootstrap was still under 10 seconds. Sources: /u/ezyang's [notes](http://ezyang.tumblr.com/post/62423869825/icfp-a-nanopass-framework-for-commercial-compiler) and my conversations with Kent.
I've managed to get a Hackage account now, so this is now available on Hackage: http://hackage.haskell.org/package/streaming-png-0.1.0.0 . Apparently my Windows Live email address didn't like Hackage, so I tried a gmail one instead and it worked.
One single 50% performance regression over the course of 30 years seems pretty acceptable. ;)
For the bulk of the linux kernel, things are pretty shallow. It is very much the sort of thing where 'many hands make light work', since hacking on different drivers, platforms, etc. are almost all nearly embarrassingly parallelizable tasks.
How much RAM does your computer have? In my experience that can make a huge difference. My usual rule of thumb is you have to have at least 8 gigs of RAM to get decent build times with GHC on large projects. To be clear, this is an exaggeration--it's certainly possible to do ok with less RAM. This is just the rule of thumb that I use to make sure I'm buying computers with a wide safety margin in terms of GHC build times.
Provide both: a standard build box plus the ability to test performance locally. That way developers can quickly assess the impact of their changes locally, but the build box has the final word
I'm not sure. And for some reason, I'm having a hard time finding citations about the Norway thing I definitely remember reading news about a while back. It's still true that someone in a country that respects public domain can always take anything from the public domain and then publish it under BSD-3 or something (or under proprietary terms too, whatever). So, I don't think public domain is horrible, although CC0 is *probably* best. FWIW, I saw a whole bookshelf of "classics" at a book store recently with every one of the public domain works including a "©2015 All Rights Reserved" indication with *zero* idea for anyone that this Mark Twain stuff is not totally restricted. Of course, their copyright isn't on the words, it's only on the particular layout in that printing. &gt;:(
Yep. The whole plan is outlined on the Wiki: [Dependent Haskell][dep]. GHC 8.0 is going to have the features from [Phase 1][phase], most importantly Type in Type which unifies types and kinds. [dep]: https://ghc.haskell.org/trac/ghc/wiki/DependentHaskell [phase]: https://ghc.haskell.org/trac/ghc/wiki/DependentHaskell/Phase1
You could speed up the machines doing the compiling.
Lots of ram and an SSD drive... ram is cheap these days...
why? strictness? 
One of the annoying things about testing performance locally is that, naively, you generally need to do a performance build of the compiler both before and after your change. Which can definitely take a while. Perhaps there need to be scripts which automate this process.
Yeah, it's tedious, but the beauty of it is that once you begin to optimize the compiler it becomes a virtuous cycle because the more performance gains you make the faster the compiler will build itself
I think you have to write the `Eq` instance manually (or normalise `TotalMap`): &gt; fromList 'a' [] == fromList 'a' [('b', 'a')] False 
Oops, turns out it wasn't an entirely clean build, ghc's compilation avoidance skipped a lot of files. Sorry. The clean build is closer to your estimate: mk build/debug/seq 246.37s user 34.21s system 312% cpu 1:29.82 total Here's the report, though I never really figured out how to use those: http://ofb.net/~elaforge/shake/report.html?mode=summary It also includes bootstrapping the shakefile itself via runghc and then -O2, which is really slow. Source is: http://ofb.net/~elaforge/shake/Shake/
afaik you should choose `-N(n-1)` where `n` is the amount of cores you have for optimal runtime (or just use `-N`) because the garbage collector runs on one thread or something.
You are right, you can look at it as a pure description of an action with side effects, but I don't think that contradicts the quoted statement.
I've made a EDSL command line parser that is very very similar a while ago, that I finally managed to "finish" last month or so (as part of another project), and finally split apart in its separate package: [cli on hackage](http://hackage.haskell.org/package/cli)
Intel will make the next generation of chips run slightly faster per thread.
Thanks! Glad to hear this is being considered as an options.
&gt; It's also not just the compiler getting slower, it's also how we use it. Things like lens suddenly mean a lot of modules have a TH pass in them to generate lenses. Indeed: Johannes Bechberger tried to measure compiler performance and used the compiler language shootout game programs as the benchmarks. And he found that there were _no_ significant changes in compiler performance: https://uqudy.serpens.uberspace.de/blog/2016/02/08/ghc-performance-over-time/ This raises the hypothesis that at least for small, idomatic, simple Haskell98 code, the compiler did not go slower.
I strongly disagree that stack checking dependencies outside the current project is a tooling problem. On the contrary, I think C++ not doing that is what leads to your C++ project's failure to compile on my system without many manual changes while the Haskell project compiled immediately.
What are your alternatives? I would argue that the compilation alone is just part of the usual development cycle. Haskell tends to require smaller test-suites and fewer runs of the actual program, navigation to the point you need to test and actual manual testing than most other languages.
&gt; Unless someone was actively getting paid to continuously improve compiler performance, we can't simply have singular individual people do it. Where can I apply? :-)
Indeed it is here, but its current incarnation isn't quite usable. I poured a substantial amount of effort into [fixing up](https://ghc.haskell.org/trac/ghc/wiki/DWARF/80Status) a variety of issues for 8.0, but unfortunately found a rather tricky issue late in the game which I didn't have time to resolve. The trouble is that in the face of FFI calls (which are quite common in Haskell code) stack unwinding can result in segmentation faults. This is quite bad for what is supposed to be a debugging mechanism. I have a pretty good idea of what needs to happen to fix this, but it will require a bit of surgery in the C-- code generator which I've not had a good block of time to work through.
You pass pass it in a tuple `plot (data,layout)`. Tooltips are enabled by default.
Hard rules always have drawbacks, but the idea of having a tick flavored and tock flavored might make sense. Same critic for the unlimited perf degradation to 0 perf degradation..
I'm not sure what I'm doing differently but on my laptop your C++ project compiles in about 10 seconds whereas `cabal build` (which is still doing more work than `make`) takes 27 seconds. Yes, this is a factor of three, but that is a far cry from a (decimal) order of magnitude. I'm not saying this isn't a problem; I just wanted to quantify the effect we are talking about here.
Having such options better surfaced in our tools would probably go a long way practically..
y is a function here you are missing an arg no? But otherwise, such cases are always good opportunity to have a look at base libraries.
Large list literals like you have in `Font` (which alone takes 14 seconds to compile on my machine) are quite a challenge for the compiler as they build O(N) individual expressions which ultimately get floated to the top-level which the simplifier needs to examine every pass. This is because your list, after desugaring, will look something like, miscFixed6x12Data :: [Word32] miscFixed6x12Data = GHC.Base.build @ Word32 (\ (@ a_dqCj) (c_dqCk [OS=OneShot] :: Word32 -&gt; a_dqCj -&gt; a_dqCj) (n_dqCl [OS=OneShot] :: a_dqCj) -&gt; c_dqCk (fromInteger @ Word32 GHC.Word.$fNumWord32 (__integer 0)) (c_dqCk (fromInteger @ Word32 GHC.Word.$fNumWord32 (__integer 0)) (c_dqCk (fromInteger @ Word32 GHC.Word.$fNumWord32 (__integer 537395712)) ... To make matters worse, `vector`'s fusion will then attempt to go to work on this list. It is easy to see this by compiling just this module with `-dshow-passes`. You'll find that the "size" of the program that the simplifier produces is quite massive, peaking at over 40k terms (a factor of 20 larger than the program it started with). If you look at one the compiler is actually doing (`-ddump-rule-firings -ddump-inlinings`) you'll see lots of inlining of `foldr`, which suggests that the compiler is attempting to fuse away your list into its consumer. If you simply attach a `NOINLINE` to `miscFixed6x12Data` you'll find that compilation time goes from 20 seconds to 2 seconds. Alternatively, the approach I would likely take here is to either place this data in another module (e.g. `Font.Data`) or read it dynamically at runtime. In the former case compilation of `Font` drops to around 1.5 seconds. In general, to work out compile time performance issues like yours you first need to identify what the compiler is doing that takes so long. This generally will start by looking at `-dshow-passes` and see where the compiler "stalls" the longest. It's can also be helpful to look at the relative changes in program sizes over the course of compilation. Once you have a hypothesis for which phase is responsible, dump the intermediate representation that the phase begins with and what it ends up with (using the [dump flags](http://downloads.haskell.org/~ghc/master/users-guide//debugging.html#options-debugging)). Try to understand what the compiler is doing to your program and how much you suspect this should cost. Often this will reveal your answer, as it did here.
Does "Type in Type" by itself extend the scope of what can be expressed in Haskell, type safety-wise ? My understanding was that it would remove some boilerplate when doing (e.g., singleton-based) type-level programming, and is a step toward, but does not yet allow, true dependent types. Is that correct ?
It certainly doesn't. I'm questioning not if what you're saying is true, I'm questioning if it's relevant. Concurrency and Parallelization are expert topics, and getting them right isn't easy anywhere.
Also, `-O0` helps significantly for me although strangely cabal's `--disable-optimization` flag doesn't seem to have any effect (*edit* see below), $ cabal clean ; cabal configure --disable-optimization --ghc-options='-O0' $ time cabal build &gt;/dev/null real 0m6.837s user 0m6.258s sys 0m0.588s $ cabal clean ; cabal configure --disable-optimization $ time cabal build &gt;/dev/null real 0m13.595s user 0m12.940s sys 0m0.657s $ cabal clean ; cabal configure $ time cabal build &gt;/dev/null real 0m14.023s user 0m13.363s sys 0m0.658s I also noticed that `cabal build` is essentially no slower than `ghc` alone so long as the project is already configured, which is heartening. For what it's worth, GHC's builld times for your Haskell project compiled with `-O2` are approximately in line with `gcc`'s build times on your C++ project with `-O3`. *edit* I just noticed that you include `-O2` in `ghc-options`; this is why `--disable-optimization` does not work; perhaps `cabal` should pass `-O0` explicitly if `--disable-optimization` is given to ensure that any user-specified optimization flags are nullified.
Fwiw, there's [this idea](https://ghc.haskell.org/trac/ghc/ticket/10752) to have GHC annotate warnings with the flags that triggered them. This could be made more verbose on demand, to also tell which non-trivial [warning-sets](https://git.haskell.org/ghc.git/commitdiff/86897e1fe23cb26fa2278e86542b34c33301606a) a specific warning is an element of.
ok, it's the same problem, but coming from different perspective : end user or library author. version bounds are the gold standard for building stuff, but practically it has the risk of being inconsistent, and it happened quite a lot for me when casually trying out a project. Ideally those two approaches work as a tandem, relaxing bounds to allow for more reusability. this example is really a posterchild example. ok the OP did not know about PVP and stuff. but hey, he got a lib out. now he can be introduced to it. I prefer that, than having him trip and fight arcane versions issues and explain he needs to remove bounds on `base`, tinker 10 other libs before he has something basic working. Beside, having a set of libraries used by many people is extremely useful and reassuring, independently of how it is crafted. that in itself is a big win. It poses new questions, but better address that collectively than each one on its own (especially for noobs..)
absolutely agree. the (highly laudable IMO) benefits brought by stackage pining should not be detrimental to standard way to compose pieces and increase reuse. i think once we have the necessary tooling, we will see this as totally obvious and complementary. it's exciting to see that we are addressing those kind of practical issues
I see you uploaded a new version with lower bounds, however, you also need to set upper bounds to comply with the PVP and the Hackage upload guidelines.
For what it's worth, Conal has an [implementation](http://hackage.haskell.org/package/total-map-0.0.4/docs/Data-TotalMap.html) of this idea up on Hackage, although it is a bit bare-bones. You should ping him and see whether you can merge.
Really good to know. As ondrap said, it might have more to do with register allocation than actual branch prediction. In that case it's probably better left to GHC to decide. Did you document your process? I'd love to go through the commits and see the difference (ahem, if I find the time) to get a feel for it.
&gt; I imagine at some level there's a matter of publication deadlines, etc. So, compiler speed improvements are not phd worthy?
Very interesting. Does other tooling work with your project then? Like ghc-mod.
This made me think about: import Data.These import Data.Align data Aligned f a = Aligned a (f a) instance Functor f =&gt; Functor (Aligned f) where fmap f (Aligned x xs) = Aligned (f x) (fmap f xs) instance Align f =&gt; Applicative (Aligned f) where pure x = Aligned x nil Aligned f fs &lt;*&gt; Aligned x xs = Aligned (f x) (alignWith combine fs xs) where combine (These g y) = g y combine (This g) = g x combine (That y) = f y There is probably some type-class with `join` / `&gt;&gt;=` analogue which would make `Aligned` a monad. Have anyone done that exercise already? Is it any interesting? Should it be added to `these` package (and the `Aligned` data-type?)
I took a stab myself at the "perfect streaming" thing in my foldl-transduce package. The grouping is done consumer-side, which can be useful sometimes.
Replying to myself, just in case someone else is interested. The answer is essentially in that paper: http://www.seas.upenn.edu/~sweirich/papers/fckinds.pdf . The "Type-in-Type" axiom is fundamental in extending System FC with "kind equalities", instead of just "type equalities". An immediate bonus is that GADTs can now be promoted, and the paper gives some examples as to why this might be useful.
[This](https://github.com/bitemyapp/skynet/blob/master/haskell/src/Parallel.hs) is the way I'd expect a beginner Haskeller to write that, except maybe for the bang patterns. This is the first way one learns to deal with parallelism in Marlow's excellent book. And if that's not the case then we are not advocating this book as the starter guide to parallelism in Haskell enough.
Rather than guessing, actually let a beginner write the code, and then we can talk ;) Nothing is more accurate than real-world data.
Never heard of ghcid or reloaf. Thanks!
Oh okay. I will do that when I upload the next version.
I run the project interpreted. this compiles everything necessary to run main &gt; ghc --make main.hs Then if you change any source file in ./src, it will be interpreted and run by runghc. the rest of the modules will run with the compiled versions: &gt; runghc -isrc main.hs &lt;params....&gt; It is way faster
Your check has already been sent in the mail!
*findWithDefault* not good enough?
I don't understand this entire 'dependency checking' tangent. The entire C++ project compiles in 6s, I have single Haskell source files taking longer than that to build. Even if dependency checking would be infinitely fast, that changes nothing about my point.
Refactor out the pure CPU-bound segment and use par. Or if it fits into `repa`'s model, use that.
Yes, I have noticed this too. Do you have an idea of why? I haven't profiled it myself. 
So many people say they switched to ghci-ng. Is there an intention to merge ghci-ng back into ghci? 
I'm pretty sure that it's only the latest release compiler version that's going to be available from the binary cache, but that is based strictly on observation, not any sort of inside knowledge. The good news, such as it is, is that even if you have to compile those packages locally, you should only ever have to do it once---additional references should resolve to the same store location.
First of all the documentation on mew.org is completely outdated unfortunately so don't trust anything it says. - The type command does usually work pretty well so I'm not sure why you're getting an error there. You should try it on the command line with logging enabled and report an issue: `ghc-mod type -vvv Foo.hs 10 20` (10 is the line number, 20 the column. adjust as needed). - As for the "lacks an accompanying binding" thing we have a bug report and PR for that: https://github.com/DanielG/ghc-mod/issues/704, https://github.com/DanielG/ghc-mod/issues/736. If you could test that and report back that would be a huge help. - `M-t` is another one of those flaky commands, sometimes it works sometimes it doesn't. Unfortunately the Elisp portion of ghc-mod is pretty ill maintained since no one wants to touch it. Unless you want to help fix it :) Also note that if you're trying to use the case splitting and refinement stuff that might work significantly better with the current version from git since I fixed a long standing bug in the session caching (https://github.com/DanielG/ghc-mod/commit/2e4c2b52280290cd6af58d96c5368ed1ae0ae15f). Some very limited testing suggested that that was the issue that was breaking that code but I'd love to get some feedback from someone actually using that. I'm really sorry for the sorry state of ghc-mod from a user experience standpoint. Most of my time has gone into improving the underlying magic that allows ghc-mod to even work at all (unlike a year ago). Unfortunately the simple bits that actually do stuff have now bit-rotted and don't work so well anymore. We really need some help keeping this stuff going. 
Yes. This is true of every applicative.
Not that I'm aware of. Maybe long term. Eventually, ide-backend and haskell-ide-engine might be the real long term solution. There's a lot of big shake-ups in the Haskell tooling scene going on right now.
I wrote [this](https://hackage.haskell.org/package/total-maps-1.0.0.2/docs/Data-Total-Map-Sparse.html) a while ago. It has a fancy foldable instance that uses exponentiation by squaring.
How come not `memcpy`?
I believe it's better to have a competition than waste time on negotiations. For small packages like this one at least.
Single files are a bad measurement for both C++ and Haskell since both support running arbitrary turing complete programs at compile time. The compiler can only optimize the parts that do not involve Template Haskell or C++ template metaprogramming.
Wow, that comment just opened my eyes
There are warnings, not errors, doesn't the program work? How did you install GHC? /usr/local is a funny place for it to be.
Wait, so you just built it and didn't install it? That would cause the problem. You need to install it into your sandbox for other packages using the sandbox to see it...
What does "only morally true" mean in this context?
It means that you there exists a function `f` and values `x`, `y`, such that x == y = True f x == f y = False Generally, if two values are equal according to `==`, one should be able to freely substitute them in all contexts. I sometimes violate this too, but if I do, I promise myself not to observe the difference.
Woot! I can't wait to start using this! type MyMap = TotalMap Int (Maybe Value) Should do the trick! :P Seriously, nice work though.
If you change the type signature to the correct type (`(a -&gt; a) -&gt; a -&gt; a -&gt; a`) your implementation does what you say you're trying to do. However, this function does not match your example, and is also fairly useless as it either simply returns the second argument or loops forever. What I believe you actually want, is something that repeatedly applies a funcion, starting with a given value, until a predicate (which is a fancy name for a function that returns a `Bool`) returns true for the result. A function fitting this description would have the type signature you wrote and would make your example work (assuming you meant `repeat (+ 10) (&gt;= 100) 3` should return 103). To accomplish this you simply need to change the `z == y` in your example, to `y z`. As an aside, an "element" usually refers to something inside a list (or more generically, inside a collection), the thing given to a function is called an "argument", or sometimes a "parameter". Also, conventially arguments that are functions are usually named starting with "f". Using this convention, you should rename "x" to "f", "y" to "g" (I would probably use "p", for predicate, instead), and "z" to "x" (because it's not a function). This convention helps convey if a variable is function (and therfore can be applied to an argument) or not, without having to look at the types. You may also run into naming conflicts due to [repeat](http://hackage.haskell.org/package/base-4.8.2.0/docs/Prelude.html#v:repeat) being defined in the prelude (which is implicitly imported by default). To fix this, just rename your function to something else or exclude repeat from the import by including the Pelude import explicitly (`import Prelude excluding (repeat)`). I highly suggest renaming, as it will help avoid confusion regarding what function you're using. If you just want to use a function that does this rather than define one youself, [until](http://hackage.haskell.org/package/base-4.8.2.0/docs/Prelude.html#v:until) seem to do what you want except the predicate is given as the first parameter and the function to be repeated as the second. In the future, if know what you want to do but you don't where, or even if, it's defined, [a quick search on hoogle](https://www.haskell.org/hoogle/?hoogle=%28a+-%3E+a%29+-%3E+%28a+-%3E+Bool%29+-%3E+a+-%3E+a) can yeild exactly what you want. EDIT: Changed `x` in my solution to `z`. Seriously, I wrote 3 characters of code, including whitespace, and still managed to mess it up! (Although I did accidentilly provide an example for how naming functions starting with "f" helps avoid bugs)
Having a type system that catches bugs is good, but there is no substitute for fatal assertions in development mode. If I can assert something fatally and that assertion doesn't go off, then I know that for the entire rest of my application, the assertion must have been true. Given that the assertion was true in the past, I can make inferences. What would be really nice is an inference system where you can specify fatal assertions and the things that can inferred if the fatal assertion is not triggered and let the compiler use those inferences at any point after the assertion.
Doesn't allow you to have Applicative.
me too man, don't worry. 
Only if `k` is monoid (just like (`k -&gt;`) trace comonad)
? Given data TotalMap k a = TotalMap a (Data.Map k a) What's wrong with instance Comonad (TotalMap k) where extract (TotalMap a _) = a duplicate w@(TotalMap _ t) = TotalMap w $ fmap (\a -&gt; TotalMap a empty) t
Yes, unfortunately an average InfoQ reader won't get any intuition about what dependent typing is useful other than that one can encode more invariants in types. All she says only makes sense when you already know a bit about the topic but for somebody new (an "ordinary" programmer), it must sound like mambo jumbo... (indexed types?!?, gadts?,...)
I don't have any solutions to the problem, but I have some random observations. At work we have about 250kloc of Haskell code compiled with ghc, and 2.5Mloc compiled with our own Haskell compiler. So I can offer some observations both as a ghc user and a compiler writer. * ghc compile times have pretty consistently gone up with every new release of ghc. Usually 5-10%, unless there's a ghc performance bug (and they get fixed). I've done these measurement since around 2009, but sadly I never saved any results. * The performance of the compiled code has pretty consistently gone down with every ghc release. Again, I've saved no numbers. Also around 5% per release. * Our own Haskell compiler does type checking and warning generation at about 10000 l/s. That's quick enough to get fast feedback of type errors in the IDE, and to check your full application in 20-30s. I'm reasonably happy with this number. Speaking of happy, about 25% of the time is spent in the parser when running the compiler in this mode. * With minimal optimization the compiler processes about 2500 l/s. While this might seem like a nice number, it's really too slow. Our compiler is a whole program compiler, which means that an application can take 1-3 minutes to compile. * With -O2 the compiler processes about 1000 l/s. * I've just spent about 4 weeks speeding up the compiler. It's boring and unrewarding work. I'd rather implement new features. But it's something that is necessary to do now and then (4 weeks of work resulted in 40% speed up of -O2). * ghc profiling is really not very helpful for speeding things up. Turning on profiling changes the code too much. I'm eagerly awaiting working sample based profiling. (We are still of ghc 7.8.3; every ghc upgrade is a major undertaking.) End of brain dump. 
That's as expected. GHC and GHCi don't know about sandboxes, so to use them you have to interact via cabal. See here... http://chromaticleaves.com/posts/cabal-sandbox-workflow.html
I'm not understanding the question... do you mean on an average change how much stuff needs to be recompiled? I guess that varies a lot depending on the project, but surely good design will encourage mostly leaf modules with fewer core modules, which means most changes require little recompilation.
I haven't though of that. Functor is fine though.
Dunno, I don't use ghc-mod. Don't see why it wouldn't though.
I usually consider two things of type `a` equal if all functions of the form `a -&gt; ()` gives the same result on both (since `()` has two values: `()` and `_|_`). I think these two notions are the same, but I'm not sure.
I've been playing with PRNG libraries recently (including `mwc-probability` just recently, coincidentally—nice library!), and I'm reminded that [`random-fu`](https://hackage.haskell.org/package/random-fu) is implemented (indirectly, through [`rvar`](https://hackage.haskell.org/package/rvar)) terms of the [`MonadPrompt`](https://hackage.haskell.org/package/MonadPrompt) library, which is in the same space as free/operational/freer monads. I've also been meaning to learn a bit of statistics and it's just going to be funny if I end up going "oh, it's just another monad." So for example, in addition to thinking of conditional probability in the standard introductory "P(A|B)" vocabulary of sample spaces and events, this means I can look at it in the `b &gt;&gt;= f :: Prob A` vocabulary of a distribution monad, right?
AFAIK FPCo are making Docker images with pre-built Stackage snapshots. That could be a goldmine of compile performance information if gathering such information did not make the process to expensive. Kill 2 birds with 1 stone.
A number of the comments seem to be assuming that, as the submitter of the article, i'm the author of the article. (For example, writing "You ..." rather than "The article's author ...") This is not so. :-) i submitted this article because i'm interested in /r/haskell's take on it, so, thanks to those who have replied thus far!
The `TotalMap` is inspectable. For example it can be serialised (transferred over wire) or two `TotalMap`s can be compared for the equality (even if `k` is not finite).
sounds like [Sequence](http://hackage.haskell.org/package/containers-0.2.0.1/docs/Data-Sequence.html) to me. O(1) cons/snoc/length. While lookup/insert aren't O(1) they are O(log(stuff)) and afaik the constant factor is small enough that unless you have a large dataset it should be fine. Although I would be curious what Clojure's vector would look like in Haskell and if it already exists. (And if not why not)
Learning about the "Full Employment Theorem" in my compilers course kind of blew my mind. It's cool to see it brought up here!
The MOOC is quite good but there isn't much more than Graham Hutton Haskell's book. The progression is pretty smooth and well designed till almost the end of the course. From time to time you will see in the exercises use of terms that are neither in the book, nor explained in the videos, so be sure to document yourself well enough before attempting to answer. There are a few exercises which a poorly designed and more than a couple of multiple answer questions that are purely there to test your eye sight. Also, all of a sudden the labs go from counting apples and oranges to implementing poor man's concurrency on your own which I found that the course had totally not prepared the student for (it's was quite a big leap). At the end of the course (especially if you have explored Haskell before) you can expect to have a more solid understanding of the basic, but nothing more. I would not recommend this MOOC to a beginner however, I would first read "Learn you a Haskell" and then take the MOOC. Anyways Delft University and Professor Meijer have put together a good course and are to be thanked to the time and effort they have put in. 
You are supposed to be using `Functor` for `fmap` and `Foldable` for `fold`.
Great work! Overkill? No, not really. Here's your next mini project: take all this great setup work you did and wrap it up as a library that others can reuse. Bonus points if you make it possible to override some of your defaults easily with some simple module or configuration file - but if you don't have immediate time for that, don't let it delay just getting this good stuff out there.
I can't see any text on mobile :(
Yeah, I was thinking the latter example is nicer (indeed, it's the entire motivation for ApplicativeDo in the first place), especially if you're building a general purpose probabilistic programming DSL.
Close. When it comes to `b &gt;&gt;= f`, bind is taking the uncertainty captured by `b` and smearing it into `f`. This is distinct from taking some concrete value and using it to calculate `f`. In that sense it's more of a marginalizing operator than a conditioning operator. When you interpret a probability monad you produce a [predictive distribution](https://www2.stat.duke.edu/courses/Fall09/sta290/Lectures/Normal/predictive.pdf) (or a sample from a predictive distribution), rather than a conditional one.
Here it is: name: servant version: 1.0 Build-Type: Simple cabal-version: &gt;= 1.2 executable servant main-is: Main.hs hs-source-dirs: src build-depends: base, aeson, time, wai, servant-server, mtl, transformers, either, attoparsec, bytestring, string-conversions, lucid, http-media, directory, blaze-markup, blaze-html, warp, servant-lucid 
Really interesting article /u/jaredtobin ! AFAIU, `ApplicativeDo` simplifies `do` blocks to applicative style (hence parallel execution) only if there is no functional dependence between subsequent statements. What I don't understand is how to reconcile these two aspects : 1. Monad is a special case of Applicative 2. Statistical independence is a special case (RV's are dependent until proven otherwise) 
This is not our code, we just [fixed some little things](https://github.com/CIFASIS/ttasm/commit/e7dae1496e4dbcf577ceb7c67bc4d3ec55f4f17f) to make it run in modern GHC versions
I took this course 2014 and [the Ocaml course](https://www.france-universite-numerique-mooc.fr/courses/parisdiderot/56002/session01/about) last year and I would strongly recommend the later over the former Both will probably get another instance later this year --- FP101x was a big (personal) disappointment - the videos where ok (including banans - but sadly no barbed wires) but the exercises where very poor and tedious (the last two weeks where a bit better) What really *triggered me* was the way they handled the community in the forums. It was basically Erik hunting for ways to either "let me google this for you" or telling people that they violated the rules by not googling it while punishing people trying to help out others Some might like this (and learn quite a few things I guess) - for me and the way I try to help people it was a constant annoyance to the point where I literally *rage quited* the forum ;) Then last year the Hugs VS. GHC stuff ... well let's say I find this borderline-trolling. --- The Ocaml MOOC was quite the difference - the videos where great and actually explained stuff and the exercises where pure gold - they did manage to put up an online Ocaml-Repl/Environment where all their exercises where running with tests - so you could just play around online (no need to download or upload anything) and push your code against their tests. On top of this the exercises where really good - to the point where you could implement a puzzle solver (which did ouput the steps in Jupyter/IPython style inside the environment) as the final project. --- ### small update I just saw that you can play with the online system the FUN Mooc (Ocaml) used [TryOcaml](https://try.ocamlpro.com/fun-demo/tryocaml_index.html)
it's based on the same book but Eriks style changed quite a bit (no he divorced MS and GHC ^^) and of course there are actual exercises
One of the problems is that we don't have a solid, widely accepted foundation for all operations. We have no accepted typeclass that does `filter`, and a set is clearly "mappable", but it is not a functor.
The problem is that beginners are encouraged to use Haskell lists as arrays, by 1. `base`, which doesn't contain any other sequence type, 2. tutorials, which use lists that way, and 3. the language syntax, which looks like arrays in many other languages. So it's perhaps not the author that is making a mistake, but large parts of the community surrounding Haskell.
The deeper problem with `Set` is not actually the constraint, rather the fact that sets break the functor laws. `fmap` for a set needs to remove any duplicate elements after the transformation. We'd like fmap (f . g) = fmap f . fmap g but if we can find a `g` that retains the information of the original element but returns a value that compares equally to some other value (and an `f` that is the inverse of that) we'll fairly quickly see that the two operations are not equal for sets. Here's an example: -- A newtype that compares an Int according to its signedness newtype Signum = Signum Int deriving Show instance Eq Signum where (Signum i) == (Signum j) = signum i == signum j instance Ord Signum where compare (Signum i) (Signum j) = compare (signum i) (signum j) When mapping this over a set, we reduce the number of members for obvious reasons: signumified = Data.Set.map Signum (fromList [-2, -1, 0, 1, 2]) -- returns -- fromList [Signum (-2),Signum 0,Signum 2] and when mapping this back, we get what we expect: Data.Set.map (\(Signum i) -&gt; i) signumified -- returns -- fromList [-2,0,2] However, this is not at all what happens if we do map both functions over the set in one go: Data.Set.map ((\(Signum i) -&gt; i) . Signum) (fromList [-2, -1, 0, 1, 2]) -- returns -- fromList [-2,-1,0,1,2] So by setting g = Signum and f = \(Signum i) -&gt; i we have a situation where fmap f . fmap g reduces the number of elements of the set, while fmap (f . g) does not. This is a general problem with every kind of container where the number of elements need to be reduced or increased when mapping over it. (Or, for that matter, the shape of the container needs to change in an observable way.)
I see. So in that case there is a need for either accepting only functions that preserve equals `x == y =&gt; f x == f y`, which is not something you can really enforce in non-dependant Haskell. Now I understand why SubHask talks about monotonic functions too. Thanks!
Maybe it is just me, but this problem looks like another of these "software engineering" challenges that actually only arise when using ill-designed or intentionally crippled languages such as Java. What's wrong with having a Default instance for your datatype, and then updating just those fields that you care about in whatever order ? data Human = Human { name :: String , age :: Int , money :: Int } instance Default Human where def = Human "" 0 0 myHuman :: Human myHuman = def { age = 18 } If you need further processing that just storing the provided values into the target data type, just make a smart constructor and use the same trick to provide its arguments through a record type. You can use Maybe to represent empty arguments. 
Thanks for that wonderful account, `bradley_hardy` , especially the connection to pipes and pipes-group! One of the reasons why I wrote the `Streaming.Prelude` module is that I had repeatedly found people arguing that the use of `FreeT` in pipes-group / pipes-bytestring / pipes-text was somehow artificial and proved that there was some incapacity in the pipes idea. This argument would transparently empty if it were raised in connection with `Streaming.Prelude` - and of course everything written with `Streaming.Prelude` and `Data.ByteString.Streaming` can be refactored to use pipes `Producer`s with basically no thought at all. So in that respect it's a bit of pro-pipes polemic. The other reason was to provide the simplest genuinely streaming prelude possible, and one that the beginner could use to comprehend the differences between the fancier streaming libraries. Thus the various charts mapping isomorphisms and quasi isomorphisms are not just for purposes of interoperation but potential refactoring, and above all for comprehension of their purposes and the differences between them. They are revolve around the fundamental concept `Stream (Of a) m r` or `Stream (Of a) m ()` and view it in different lights.
The problem is only #2 - specifically, whatever tutorials the author read, if any. And if the author devoted at least a minimal amount of effort to think about and understand what was written in the tutorials. I disagree with #3. I started out saying that I also disagree with #1. But after writing my reasons below, I realized that I actually completely agree with what you are trying to say in #1. I just disagree about how it should be fixed. \#3: Haskell list syntax is simple, clear, elegant, and convenient. And yes, an appropriate metaphor to legacy array types, even though it is much different from them in some ways. As one of the most fundamental semantic concepts in a pure language, the list deserves its special place in the syntax. It is not hard to teach beginners what a list is. \#1: Base has nothing to do with this - base has to do with bootstrapping libraries in Haskell compilers and should be completely orthogonal to the beginners experience. In fact, the notion of base should be invisible for non-beginners, too, unless you are hacking on a compiler. But that's another story. Beginners who install the basic standard Haskell installation package should get everything they need to get started. That is the idea of the Haskell Platform "batteries included" package set. Without going off topic about all that is wrong with HP, the fixed version of HP or whatever replaces HP must do that in some convenient way.
I will, as soon as the stack fanbois stop going off topic and repeating literally "why don't you use stack" in every thread that mentions cabal.
[removed]
&gt; pasteb Hello, I am the original author of the article. Sorry I didn't realize the data structure difference between Perl and Haskell. I can try to implement using Vector when I have time and compare the performance difference. I have updated my Haskell solution on pastebin and here is the link: https://pastebin.com/BRVU53B5
Hello yitz, I am original author of the article and thanks for your comment on my blog. Sorry I didn't realize I was using improper data structure in my algorithm, I will try to implement it using Data.Vector and see how it performs. However, still I am a little skeptical about how it will improve the result, because I have a STUArray version of solution but it still cannot outperform Perl's solution. I have repost my STUArray solution on pastebin here: https://pastebin.com/BRVU53B5
Nice, I did not know this company. they are in a sweet spot
During a [previous discussion of builders in haskell](https://plus.google.com/+IvanMiljenovic/posts/Sktv1qbxTRD), I came up with this solution: import Data.Proxy import Control.Monad.Identity data FooContainer x y z = Foo { bar :: x, baz :: y, quux :: z } type FooBuilder f g h = FooContainer (f Int) (g String) (h Double) type Foo = FooContainer Int String Double unset :: FooBuilder Proxy Proxy Proxy unset = Foo Proxy Proxy Proxy setBar :: Int -&gt; FooBuilder f g h -&gt; FooBuilder Identity g h setBar int foo = foo { bar = Identity int } setBaz :: String -&gt; FooBuilder f g h -&gt; FooBuilder f Identity h setBaz str foo = foo { baz = Identity str } setQuux :: Double -&gt; FooBuilder f g h -&gt; FooBuilder f g Identity setQuux dbl foo = foo { quux = Identity dbl } eval :: FooBuilder Identity Identity Identity -&gt; Foo eval (Foo { bar = Identity int, baz = Identity str, quux = Identity dbl }) = Foo int str dbl 
One option could be to have a budget of performance enhancements per release. Some percentage of performance enhancements go to making the next release faster, some go to offsetting additional features. Patches go into a priority queue (based off of community support for a feature), and get merged in when *someone* has paid for them by making something else faster. The submitter of a performance enhancement can also apply it to a specific new feature. This would encourage members of the community to step up and tackle low hanging fruit: want a particular cool new feature? Make a tiny bit of ghc a bit faster, and it will get added!
Haskell lists have something like O(n)-with-terrible-constants snoc... far from the O(1) you speak of.
See my [`semigroupoids`](http://hackage.haskell.org/package/semigroupoids) package for an approach along the lines of what you do here, which is capable of handling the product example. My [`hask`](http://github.com/ekmett/hask) package provides a powerful encoding capable of encoding things like limits can yield products. It doesn't build on GHC 7.10, however, due to a change in the way `Any` works, depriving me of my generic `Compose` construction. With `UndecidableSuperClasses` coming in in GHC 8, I should be able to rework this to be much, much cleaner. This is currently blocked on [#11401](https://ghc.haskell.org/trac/ghc/ticket/11401) which keeps the fix to [#11523](https://ghc.haskell.org/trac/ghc/ticket/11523) from working. Sjoerd Visscher's [`data-category`](http://hackage.haskell.org/package/data-category) package provides a third point in the design space, which is more true to "real" category theory, but loses a lot of the benefits we get from parametricity.
You are right, it's a completely useless exercise that is nonsensical if you are using a functional language like `Haskell`. I think it's still an interesting exercise :)
I don't understand why everyone seems to recommend vectors when there's a nicer solution. Resort to mutable vectors only in desperate times! I posted a purely functional solution in http://stackoverflow.com/a/35439652/3970496
There are many who wish Perl were dead but alas it's immortal as defined by Simon Peyton Jones. 
Oh... Yeah... Because of leakless FRP efforts...
Thanks for the feedback! Will definitely use a more structured way of building the URLs.
For whether to use do-notation or &gt;&gt;=, simply become familiar with both, and then use whichever one feels more natural. The problem with do-notation is people often use it when they should use &gt;&gt;= because they don't know how &gt;&gt;= even works, treating it like magic.
Oh my I didn't explain this well. The point was that if we always copy the future and never share it then `always 0` and `fmap now (always numbers)` do work almost identically...
Also learn applicative and functor styles. `&lt;$&gt;` is especially useful.
Looks like it does not remove duplicates? I see FIREWORK FULL COVER listed twice in this list, https://www.youtube.com/watch?v=Oy_JlG7C-T8&amp;index=10&amp;list=PLANmAk8RPd1ZOmCjA4DfHMH2Gi1-crjlC
The problem with `(&gt;&gt;=)` is that it's generally cumbersome for anything but desugaring do notation. Prefer `(=&lt;&lt;)`. Otherwise agreed. ;)
Yeah. Learning all the tricks in the trade are important.
That's good to hear! FYI Penny appears on http://plaintextaccounting.org, announced today. I look forward to moving it up to the active list.
Why not use record notation? data Behavior next a = AndThen { now :: a , future :: next (Behavior next a) }
The same (or very similar?) issue is described in this [stack overflow question](http://stackoverflow.com/questions/6208006/any-way-to-create-the-unmemo-monad). A similar solution is sought for ("Unmemo monad"), but does not exist.
A questioner asked if Helium kept historical Criterion reports to track performance over time, which Reid said he wanted but didn't have set up. I wonder if a web service that took in Criterion reports and plotted historical performance would be useful? It seems like historical performance tracking has been something the Haskell community has been more interested in recently (though I've mostly noticed this being about GHC compilation performance).
I like that! But I do not understand the purpose of Chipped and Dungeon...
What I mean is that when you say that blaze is slow for you, how large is your project compared to the project sizes that people complain about in this thread? If your project is 10x larger, then the approach, applied to Haskell, could still "solve" the problem.
Because hackage meta-data is extremely uneditable. No "see also", no comments. Just inlining a discussion box like this: https://www.stackage.org/lts-5.3/package/aeson-better-errors-0.9.0 would solve the problem as anyone could add the info you just discovered.
Do you think that this sort of GHC hackery is the best/only way to do it?
I do generally prefer applicative style. I feel like I keep having internal debates on if this (Attoparsec parser example) pointParser :: Parser Point pointParser = do x &lt;- decimal char ',' y &lt;- decimal return (x,y) is better than this pointParser2 :: Parser Point pointParser2 = (,) &lt;$&gt; (decimal &lt;* char ',') &lt;*&gt; decimal
What annoys me is that none of the proposed solutions, no matter how clever, gives any justice to the elegance and compactness of Haskell. When Java programmers ask "how do you solve X in Haskell ?", we shouldn't come up with artificially complex, highly unreadable programs that poorly mimick Java solutions by force-encoding its constructs into the type system. Instead, we should reply: "X is actually a non issue" because, most of the time, that is the case. The community as a whole should be aware of its image, if it ever wants to expand. We have sound, pragmatic reasons to love Haskell that go beyond the inherent beauty of functional programming. And yet, we appear as dreamy theoreticians lacking any sort of practical sense, because most of the Haskell programs that imperative programmers ever see are obscure, pointless and over-engineered hacks.
Its useful to pay attention to `return`. In this case its redundant you are using (&gt;&gt;=) where the `Functor` instance would do. I would properly eta reduce some of the smaller helpers and prefer `(.)` over `($)` but thats subjective I guess. https://github.com/brsunter/RedditYoutubeBot/blob/master/src/Utility.hs#L15 -- Old ------------------------------------------------------------ formattedTodayDate :: IO Text formattedTodayDate = getCurrentTime &gt;&gt;= return . pack . formatTime defaultTimeLocale "%A, %B %e %Y" -- New ------------------------------------------------------------ formattedTodayDate :: IO Text formattedTodayDate = pack . formatTime defaultTimeLocale "%A, %B %e %Y" &lt;$&gt; getCurrentTime -- or -- (&lt;&amp;&gt;) = flip (&lt;$&gt;), I use the one in Lens for time to time formattedTodayDate = getCurrentTime &lt;&amp;&gt; pack . formatTime defaultTimeLocale "%A, %B %e %Y" -- if i went for the above I would properly write, disclaimer might need parenthesis formattedTodayDate = getCurrentTime &lt;&amp;&gt; defaultTimeLocale "%A, %B %e %Y" &gt;&gt;&gt; formatTime &gt;&gt;&gt; pack https://github.com/brsunter/RedditYoutubeBot/blob/master/src/YouTube.hs#L36 -- Old ------------------------------------------------------------ readEnvVariable :: String -&gt; IO (Maybe Text) readEnvVariable envVar = lookupEnv envVar &gt;&gt;= return . (pack &lt;$&gt;) -- New ------------------------------------------------------------ readEnvVariable :: String -&gt; IO (Maybe Text) readEnvVariable envVar = fmap pack &lt;$&gt; lookupEnv envVar https://github.com/brsunter/RedditYoutubeBot/blob/master/src/YouTube.hs#L94 -- Old ------------------------------------------------------------ addVideoToPlaylist :: S.Session -&gt; Text-&gt; Text -&gt; Text -&gt; IO (Maybe Text) addVideoToPlaylist sess token playlist video = S.postWith (googleAuthOptions token) sess googleItemAPI (PlaylistItemBody playlist video) &gt;&gt;= return . extractIdResponse -- New ------------------------------------------------------------ addVideoToPlaylist :: S.Session -&gt; Text-&gt; Text -&gt; Text -&gt; IO (Maybe Text) addVideoToPlaylist sess token playlist video = extractIdResponse &lt;$&gt; S.postWith (googleAuthOptions token) sess googleItemAPI (PlaylistItemBody playlist video)
Monads are applicatives plus the ability to make later computations depend on the results of previous ones. Statistically dependent random variables are statistically independent random variables plus the ability to make the outcomes of some of them depend on the outcomes of others. :P
Use `cabal exec ghci`.
Who is we? Is there anything we should do at the next Chicago Haskell Hack night to speed things up? Or things we should definitely not waste our time doing? 
"We" is haskell committee. It's just a project underway we haven't publicized because its just starting out. I'll try to loop you in with more details.
Does this still matter when the Applicative in question is a concrete type? As in, is it possible to get an efficiency benefit from a type being an Applicative, if that type is already also a Monad? 
Technical feedback aside, I think this is a great idea for a first Haskell project, and a really good project in general. Congratulations on getting it all up and running! How has your experience been with Haskell on Heroku? Positive?
Excellent!
I wasnt speaking about free monads specifically, but rather in the vein of things like the list monad or list applicative. Part of the utility of newtypes (i.e. ZipList) is seleting the relevant effect we are interested in. Distribution's could be sequenced in different ways depending on the implementation of bind, as the parent comment alludes to.
Thanks a ton for taking the time to type that, this is exactly the kind of feedback I was hoping for. Those originally came from me desugaring a bunch of two line do statements and I kept thinking there had to be a better way.
I was assuming you were referring to [the probability monad](https://ncatlab.org/nlab/show/Giry+monad) in particular, not to free monads in general. There's really only one interpretation of bind for this monad, though the implementation details will vary depending on how you choose to represent your measures/distributions.
You should use monad transformers in the Scrape.hs file. In particular, you are working within the IO monad, but also using Maybe a lot. Consider using MaybeT transformer
agreed; Reid is always an excellent speaker
The way you've asked the question on SO, I don't think there are many good answers. However, there are a lot of great tools for quality in Haskell! Here is an opinionated list: * [HLint](https://github.com/ndmitchell/hlint) provides suggestions, primarily for syntax. * [criterion](http://www.serpentine.com/criterion/) is a library ([on Hackage](https://hackage.haskell.org/package/criterion)) for writing benchmarks. * GHC has [support for profiling](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/profiling.html) time and space usage. * GHC supports code coverage analysis with [hpc](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/hpc.html). * [QuickCheck](https://wiki.haskell.org/Introduction_to_QuickCheck2) introduced property-based testing to the world. * [tasty](http://documentup.com/feuerbach/tasty) is a framework for writing unit and property tests.
Sorry, Edward, I'm not sure I understand what you're saying. I'm fairly new in this space. By "product" example I presume you tuples? Because the instance of (o) for SemiGroupoid does something quite different that what I'm wanting it to do: instance Semigroupoid (,) where o (_,k) (i,_) = (i,k) How do you define an instance of Semigroupoid that does this: (f, g) . (f', g') = (f . f', g . g')? Or can one of the other classes you mentioned do it?
There is [argon](https://hackage.haskell.org/package/argon), which produces a measure of code complexity. Some things are inherently complex, but it has led to me reconsidering (and then refactoring) some more complex things.
Nice, I like it. I wanted to ask about the constraints. Is (Enum a, Bounded a) the standard way to say we can list out all possible values of 'a'?
The instance of Semigroupoid for (,) isn't what you are looking for. It is the definition of a ['rectangular band'](https://en.wikipedia.org/wiki/Band_(mathematics)). The instance you are looking for is more like data Product :: (i -&gt; i -&gt; *) -&gt; (j -&gt; j -&gt; *) -&gt; (i,j) -&gt; (i,j) -&gt; * where Pair :: p a b -&gt; q c d -&gt; Product p q '(a,c) '(b,d) instance (Semigroupoid p, Semigroupoid q) =&gt; Semigroupoid (Product p q) This is supplied in [`semigroupoid-extras`](http://hackage.haskell.org/package/semigroupoid-extras-5/docs/Data-Semigroupoid-Product.html) because it relies on type system features not present in GHCs &lt; 7.6. Older versions of the package used to limit themselves to kind *, and used pairs of things in kind *, but that means you can't talk about the category/semigroupoid of constraints, etc. That gave the simpler looking data Product p q x y where Pair :: p a b -&gt; q c d -&gt; Product p q (a,c) (b,d) but could only handle products of semigroupoids where the objects lived in kind *, which makes things like the category of [`constraints`](http://hackage.haskell.org/package/constraints) either not work or too noisy to be usable.
That is simply not true at all. Both cons and snoc on lazy lists create a thunk in memory which occurs in O(1) time. When a pattern match occurs on the thunk, the head of the list is constructed in O(1) time and the tail of the list remains unevaluated because of laziness. O(1) + O(1) = O(1) time complexity, therefore cons is O(1) time. Even the expression `(lst ++ [e])` constructs a thunk in O(1) time. Then when you perform a strict `foldl` over the list, `lst` part of this expression can be evaluated before evaluating `(++ [e])` because `(lst ++ [e])` is equivalent to `(head lst : tail lst ++ [e])` so it is possible to evaluate the head before evaluating the append operation. Finally, when evaluation arrives at the end of `lst` where `lst == []` the expression `[] ++ [e]` is evaluated in O(1) time. Therefore, even if you build a list with recursive snoc operations and `foldl` over it requires O(n) time as if snoc were an O(1) time operation. Granted, this is depends on how clever your optimizing compiler is, but as far as I know GHC can do that. But I think the best way to fold over a list is to build your list using cons `(:)` and then use `foldr`. The optimizer is clever enough to recognize this as a loop and will optimize away the list data structure entirely, which is slightly faster than using strict `foldl` over a list built with recursive snoc. I have tried it before with a program that performed a sum over immutable vectors. The `foldl` over the list built using snoc, and the `foldr` over the list built using cons had similar time and memory properties, with `foldl` being slighly slower due to needing to cycle the garbage collector more often. Only indexing is O(n) but this is true for any linked-list data structure, and everyone knows that you shouldn't use linked lists if you need fast indexing. 
Does this really have anything to do with FRP in particular? The equivalent program main = do let numbers = [1::Int ..] alwaysNumbers = fmap head (repeat numbers) pairs = zip numbers alwaysNumbers mapM_ print pairs is of course a menace too, and for the same reason, or am I wrong? Don't we usually think that the above is just bad Haskell, rather than a crisis with `Data.List`? Edit: use `repeat` not `tails`
This also means that if you have two Behaviors derived from other Behaviors e.g. b0 = fmap (+1) someB b1 = fmap (+2) b0 b2 = fmap (+2) b0 That the computation (+1) will be run twice independently as well? This seems somewhat undesirable. 
Was this recorded? I'd really like to watch it if it was. 
&gt; (not wrapped in a data type). `(,)` is a data type. `Product` is a data type. You have the same amount of constructors between you and your 'stuff'. The problem is that working parametrically with the classes involved in your code is a mess. You can't just typecheck something like: (.) :: Semicategory p =&gt; p b c -&gt; p a b -&gt; p a c but instead have to deal with a rat's nest of class constraints that as you start building up larger and larger parametric expressions `foo . bar . baz . quux` just don't go away until you finally pick a concrete instance then all the constraints can collapse all at once. Worse, you'll often wind up in situations that need junk like `AllowAmbiguousTypes` and get stuck wondering why inference isn't working. If you need to do something recursive, like folding over a non-empty thrist you get stuck. There are plenty of thoughts you can't think down the direction you've chosen to encode this. The data types in the encoding I mentioned ensure that you don't get locked out of writing code that is parametric in the instance. The difference is between a construction that is completely ad hoc polymorphic that you have to reason about instance by instance and can't abstract over meaningfully using the class you have, or something where you have to use explicit data types, but they all fit into a common abstraction that you can use to, well, abstract.
This reminds me of the issue with logic programming systems, where you usually do not want sharing in order to avoid (often catastrophic) space leaks. Also tangentially related, is the first part of my post on generating subsequences of a list, ["On Powersets and Folds"](http://blog.melding-monads.com/2010/04/04/on-powersets-and-folds/).
The monoid instance for UserInput doesn't obey the left identity (or maybe right identity) law.
&gt; Well, keep in mind that the definition of IO is an implementation detail. Or your could make it `RealWorld -&gt; (RealWorld, a)`, but where `RealWorld` actually *is* the entire world (or maybe just your computer's RAM or something.)
having helpers seems indeed a major issue of the project. What could be done to entice more participation ? Identifying pain-points and opportunities for that aspect might be worth investigating
I'm sorry, I don't have any particular pointers---it's mostly just knowledge I've accreted over time. So I happened to have a freshly-installed, only-barely-configured 15.09 machine around, so I decided to experiment, picking something fairly infamous for its dependency chain: ``` -bash-4.3# nix-shell --dry-run -p 'haskell.packages.ghc7102.ghcWithPackages (p: [p.lens])' these derivations will be built: /nix/store/vlxsizza1h7kv00l4il1rc9kksv403qh-ghc-7.10.2.drv these paths will be fetched (8.87 MiB download, 143.31 MiB unpacked): /nix/store/2i49m9brq6564s095w21jy0k7zacl063-lens-4.12.3 /nix/store/3ks8r5dkg48p0g2n60jxlz7hffvk4fh7-reflection-2 /nix/store/3qclykwrcf5ish8ny4hrdwndf6jxf8qn-void-0.7.1 /nix/store/4njsy7jbwrx7nkgqd1mhpmi3gj9km64r-StateVar-1.1.0.1 /nix/store/4ws6zgisn1jnvyfcdsz73psvs45hhznp-adjunctions-4.2.2 /nix/store/4yzhfih4qmqrgx85skb9ij40i1x5l8ax-base-orphans-0.4.4 /nix/store/6ldkn2rkn82fjb73c76n8108rml46hyc-exceptions-0.8.0.2 /nix/store/6mhggl04hyd5h4vikj0vm1gzm4wwrrzj-nats-1.1 /nix/store/6p6v76kbjia4k5xlzhp0p8nhlphgy1iz-semigroups-0.16.2.2 /nix/store/7klc95lg68q3ap9r4mx60r05c4sf0hfy-free-4.12.1 /nix/store/82l846ffagnj35iydb5yia1dggycjx7n-contravariant-1.3.3 /nix/store/8a4lb2ic65xfwg5chry5shb1plb5wnld-kan-extensions-4.2.3 /nix/store/8sy4wlaj7fy50yj8zc37kvlxi1rb6n1s-bifunctors-5 /nix/store/8vwxmmlbr9hspzqmcdm44cik5wb2vjr2-primitive-0.6.1.0 /nix/store/bpq0k19f6kgzjymhv1367kh5f33jr82x-profunctors-5.1.1 /nix/store/h2anm24hghjwl9kizqnbwm6jcm07agbp-comonad-4.2.7.2 /nix/store/hhgl8cim3lmj2msrm52ias4l901f5lqp-text-1.2.1.3 /nix/store/ini90ffa0d4nn67x6vvgj7fqzwqyk52k-parallel-3.2.0.6 /nix/store/jqn4grsz2x3png6isbls86zhanv24zcr-stm-2.4.4 /nix/store/lbisn7xbvb4xmgljjx39jqr2c734xlyp-hashable-1.2.3.3 /nix/store/nl91msnvqaaakmvndy42jzbj6xr8mnkb-transformers-compat-0.4.0.4 /nix/store/q80n10wi2gaimcxl25mq9kh5r72dzrf3-semigroupoids-5.0.0.4 /nix/store/s845fak6zv045g8vw36a5d065vzylq8m-distributive-0.4.4 /nix/store/shrnynwz6bp5qkgqc7rw789pnsgigsrh-tagged-0.8.2 /nix/store/vvnr7k157h2h8lvkyjw5fvyd6f8cnzip-prelude-extras-0.4.0.2 /nix/store/w1ymjvbl0zybixl16l7aryry29q57q4g-vector-0.11.0.0 /nix/store/whjw4adbhg955g8cbsxzm97qhmqbjc9i-unordered-containers-0.2.5.1 /nix/store/wvcb2aqn7kl07spjsi0ixjzy2d1ax8vk-mtl-2.2.1 build input /nix/store/6nfniwjib1rcq5j9481c9kd1c4qshr1q-ghc-7.10.2 does not exist ``` Surprisinly anticlimactic, really. Everyone makes it sound like lens goes on *forever*. :) Anyway, out of the box, 15.09 seems pretty good for 7.10.2. So I'm wondering, what, if anything, have you changed on your system? Are you working from a release at all, or are you tracking the nixpkgs repo? Any more details you can think that might be pertinent?
As a foreigner, seeing kmett throwing typeclass faster than I can type this message was .. mesmerizing.
yep.
I changed the name of my project to Servant1, and I got this error: $ cabal install --only-dependencies Resolving dependencies... cabal: user error (Could not resolve dependencies: trying: aeson-0.11.0.0/installed-850... (user goal) trying: servant-0.4.4.6/installed-021... (user goal) trying: servant-0.4.4.6/installed-021... (user goal) next goal: servant-server (user goal) rejecting: servant-server-0.2.4/installed-cf3... (conflict: servant==0.4.4.6/installed-021..., servant-server =&gt; servant==0.2.2/installed-a1a...) rejecting: servant-server-0.4.4.6, 0.4.4.5, 0.4.4.4, 0.4.4.3 (conflict: aeson==0.11.0.0/installed-850..., servant-server =&gt; aeson&gt;=0.7 &amp;&amp; &lt;0.11) rejecting: servant-server-0.4.4.2, 0.4.4.1, 0.4.4, 0.4.3.1, 0.4.3, 0.4.2 (conflict: aeson==0.11.0.0/installed-850..., servant-server =&gt; aeson&gt;=0.7 &amp;&amp; &lt;0.10) rejecting: servant-server-0.4.1, 0.4.0 (conflict: aeson==0.11.0.0/installed-850..., servant-server =&gt; aeson&gt;=0.7 &amp;&amp; &lt;0.9) rejecting: servant-server-0.2.4 (conflict: servant==0.4.4.6/installed-021..., servant-server =&gt; servant&gt;=0.2.2 &amp;&amp; &lt;0.3) rejecting: servant-server-0.2.3, 0.2.2, 0.2.1 (conflict: servant==0.4.4.6/installed-021..., servant-server =&gt; servant&gt;=0.2 &amp;&amp; &lt;0.3) Backjump limit reached (change with --max-backjumps). Note: when using a sandbox, all packages are required to have consistent dependencies. Try reinstalling/unregistering the offending packages or recreating the sandbox.) 
Relatedly, is there a package that implements an O(n) median? Ideally also for vectors?
This one seems acceptable: https://hackage.haskell.org/package/hstats/docs/src/Math-Statistics.html#median mean :: Floating a =&gt; [a] -&gt; a mean x = fst $ foldl' (\(!m, !n) x -&gt; (m+(x-m)/(n+1),n+1)) (0,0) x median :: (Floating a, Ord a) =&gt; [a] -&gt; a median x | odd n = head $ drop (n `div` 2) y | even n = mean $ take 2 $ drop i y where i = (length y `div` 2) - 1 y = sort x n = length x
I thought you do take the middle entry if an array length is odd for median. What's wrong with it? I'm just curious.
From the samples and the description it looks like something similar to thrift or grpc but with a different format, written in Haskell, and that can generate Haskell, purescript and elm libraries.
The list needs to be sorted before taking the middle entry.
Yes, this looks even better.
Having encountered a problem to which a "Language independent type-safe communication" tool could be a solution, I think I have a good idea of what this is about. With a micro-service architecture, the different services communicate by sending messages to each other in json or some other generic format. The type-checker makes sure that the message you send is indeed a json value, and your json library makes sure that the string you send is valid json, but it's still quite easy to accidentally send the wrong json, for example by forgetting a field or putting a field in the wrong subtree. You wouldn't have that problem if you were calling a local function, because that function would take a precisely-typed value as input, not some arbitrary json structure. So in this sense, json-based communication is not type-safe. The way in which I've been attacking this problem will probably be familiar. Many Haskell programs have an outer shell which parses any string input and pretty-prints the program's precisely-typed output, so that the meat of the program can use precisely-typed values. Similarly, I've defined a precise type on both micro-services, and we serialize to and from json at the boundary. This way, we reduce the number of mistakes to those in which the two sides accidentally use different precise types, which occurs less often than mistakes in manually encoding values into json. This works fine and really isn't rocket science, but our micro-services are written in different programming languages, and so I'll soon have to write transliterations of those same precisely-typed structures, and keep them in sync with the original. That sounds like a maintainability burden, especially if we want to change those structures. So looking at typedwire's readme, I assume this is a tool which makes it easier to maintain those cross-language definitions by generating data structure definitions (and their serialization instances) from some central format, of which I could find an example [here](https://github.com/typed-wire/typed-wire/blob/master/samples/Basic.tywi).
Thanks! That makes sense and sounds kinda cool.
Now you're at a relatively sane looking error. You have a servant built against aeson-0.11 apparently, while servant-server requires an aeson less than 0.11. This probably came about because you were doing things piecemeal while sorting out your earlier issues. Now that everything sort of makes sense, you should be able to run cabal sandbox delete cabal sandbox init cabal install --dependencies-only cabal build cabal run just as was proposed earlier. This is the "recreating the sandbox" suggestion at the bottom of the paste you provided.
Really nice project, thanks for building it :)
There are a number of sorts implemented in `vector-algoritms` (some of which use in-place updates which may or may not be easy to follow for a Haskell novice), and the indexing function is called `!` rather than `!!`. The rest (e.g. /u/j0sejuan 's implementation elsewhere on this page) stays pretty much the same :)
Yes, at least informally. The problem is that statistical independence "falls out" as a simplification (e.g. the joint probability densify function of multiple random variables factors as the product of the PDFs of the individual RVs, see e.g. the [wiki page](https://en.wikipedia.org/wiki/Independence_(probability_theory)))
Hi /u/NorfairKing: I did not know about this and has made my life so much better now! I do not have to keep switching to the command line to `stack build`. Thank you! I have a `servant` / `wai` server running so would love to automatically restart the server on successful compilation. Is there a way to do this automatically similar to `yesod devel`? Right now I need to kill my server and run `stack exec my-server` again 
Hmm, the only thing that comes to mind is to have `stack build --file-watch--exec "killall &lt;your-server&gt; &amp;&amp; ./&lt;your_server&gt;"` or something.
Funny how I didn't feel the same. I recognize ocamlmooc pragmatism as very very nice. But fp101x tests were stricter and revolved around very interesting concepts (equational reasoning). You had to actually think to get things right, no fiddling. It was tedious but I valued that. Last weeks were far more inspiring even tough I couldn't make it on time to do the PMC lab. The countdown problem was nice, the monadic tree too. Ocamlmooc exercises felt like real job ones, but I found there were no FP principles taught. Mostly an OCaml primer about it's syntax and features. Except maybe the module part. The live OCaml env was a great idea too.
Thanks again /u/NorfairKing! I did not know about `exec` being able to run arbitrary commands! I just looked through the docs and it is very useful. It looks like using`killall` is a bit tricky as `--exec` seems to rely on exit status code of each command and quits when starting the server for the first time (even `||` does not seem to solve this issue but I could be do something wrong). This seems to work: # restart-server.sh #!/usr/bin/sh killall my-server stack exec my-server &amp; I then do `stack build --fast --file-watch --exec "sh restart-server.sh"`. 
I think here it was mostly very clear cut. All of the actual runtime algorithmic work is Rust, Haskell is just the framework and a few external apps that actually prepared some of the data used, offline. In general, I would want to use Haskell wherever I can use Haskell. It's the most expressive, concise, readable and maintainable language I know how to use. That being said, I always had a terrible time writing anything very performance critical in Haskell. In my tries I found sometimes that straightforward to write C code running on a single core outperforms parallel Haskell code that had days of profiling and tweaking invested in it. Worse, in order to get performance out of Haskell I generally had to abandon most things that make it so elegant, as the idiomatic tools and abstraction were too costly. In the end, I'd manually write `Storable` instances and use stuff like `pokeByteOff`, a million pragmas and strictness annotations, strip away anything generic, monad transformers etc. and basically just write a clumsy, verbose, even more error prone dialect of C. The idea of writing something like the inner loop of a software rasterizer in Haskell seems just unproductive to me. So Rust on the other hand felt very much like C/C++ performance wise. It's nearly effortless to have it run reasonably fast. You have to fight it a bit as unlike C/C++ it's default is 'safe, not fast'. So it'll take a few more decorations to convince `rustc` that, yes, I really want to write to this piece of memory without bounds checking, and yes, I want to do it from 2 threads and I don't care that you can't guarantee they won't clobber each others data. But that's probably for the better. One piece of code on the wrong side of the divide is parsing the mesh data, which is in a simple text format and not performance critical. This would've been like 10 lines of attoparsec code, but is very ugly in my Rust implementation. I just wanted to give it a try and passing that data over the FFI seemed kinda painful as well. Maybe there's a nice parser combinator library for Rust, but without things like Functor/Applicative/Monad stuff like parser state and error handling will probably just be kinda tedious. So yeah, it seems the two languages complement each other really well, they both shine at tasks where the other degenerates into nightmare territory ;-)
In Scraper.hs you use a lot of nested case statements. You could transform that into something like that: ... token &lt;- fromMaybe (error "cannot obtain access token") &lt;$&gt; getYoutubeAccessToken credentials playlist &lt;- fromMaybe (error "Cannot Make Playlist") &lt;$&gt; makeYoutubePlaylist token $ T.concat [videoMessage , today] S.withSession $ \sess -&gt; runEffect $ postStream sess "" &gt;-&gt; filterYoutube &gt;-&gt; P.take 40 &gt;-&gt; postToYoutube sess token playlist Edit: not tested This is only a small thing but improves the readability of that portion of the code significantly. This gets also rid of "one time use variables" like `at` and `pl`
which is totally ok - as I said this is probably due to my subjective dissapointment. I had really high hopes for FP101x (I was looking forward to it for half a year, took the verified course, submitted some F# labs for the course and tried to help out almost constantly) so maybe my expectation was way to high and crashed right into the way Erik managed the community and those trying to help... The exercises I indeed remember quite differently: all you had to do was copy&amp;paste it into GHCi to look for small mistakes or the types (Erik even told you to do exactly this - nothing else - I cannot remember him even mention Eq.reasoning to be honest.. all I remember is wax-on ... wax-of ...) - I usually took *educated* guesses (which was quite thrilling for the first few times because you could only submit once) - but I was really bored around the middle and even did a few just by randomly selecting answers . In the FUN (sic!) one the exercises where closer to real problems and far bigger in scope - yes at the first few weeks there where plenty of things you could answer with copy&amp;pasting and looking for the answer too - but always there was some kind of bigger project - something FP101x did not do till the last weeks. It seems to me that maybe FP101x was your first course (I had no real issue with either FP101x nor FUN - I know enough Haskell to easily pass the first one and I use F# in my day to day work so the second one was even easier) - so maybe this is why you think you did not learn as much FP there. Maybe I should revisit FP101x this year (if only to learn Hugs ... or is it Miranda this year?) - is the same stuff still around or are they just playing it back?
You can just define a record that corresponds directly to your structs: data Person = Person { age :: Int64 , name :: String , favoriteFood :: Maybe String , limbs :: [Limb] } The use of `Maybe` distinguishes nullable and nonnull fields. You may later want to encode slightly different constraints, for example that `favoriteFood` must be present in a certain part of the application. In that case, it may be convenient to parameterise the record and define concrete variants using type synonyms: data Person age name food limbs = Person { age :: age , name :: name , favoriteFood :: food , limbs :: limbs } type PersonMaybeFood = Person Int64 String (Maybe String) [Limb] type PersonFood = Person Int64 String String [Limb]
This is exactly what I need since I'm doing an Haskell/Elm project, if you could provide some examples and documentation, I'll probably use it
Oh I hadn't considered that stack depends on the `exec` command to finish. You solved it brilliantly.
The version before `12:43, 9 December 2013` _is_ the quickselect you suggest. I'm not asking for suggestions on how to implement it, only for somebody to roll back to that old version who has the capability to register, or already an account, on that site.
`sort` based implementations cannot achieve O(n). [Median-of-medians](https://en.wikipedia.org/wiki/Median_of_medians) quickselect can.
*TLDR: "Special" and "general" are dual concepts, and functional programmers sometimes tend to view computational structures from a "backwards" perspective. Flip the special-general relation, rejigger things a bit, and you'll find the correspondence you're looking for.* Monad implies applicative, but not vice versa, and independent random variable implies (general, "dependent") random variable, but not vice versa. And yet we used monads to model general random variables and applicatives to model independent random variables. Why, you're asking, do we use the special case (monads) to model the general case (r.v.s), and the general case (applicatives) to model the special case (independent r.v.s)? Let's think again about what's going on here. Two random variables are independent when the outcome of one tells you nothing about the outcome of the other; viewing random variables as functions from the sample space into the real numbers, we see that this means we can model them using a computational structure in which we can apply functions to values without needing the result of any of those function applications to depend on the result of any other. And that's exactly what applicatives provide. Monads, as a "special case" of applicatives, provide additional structure that allows for modeling dependence. When people talk about a special case, they're talking about a subset of the general case, imagining an interior circle in a Venn diagram. But there are always two dual ways of thinking about the basic objects of interest and of categorizing them into sets and subsets. Considered as algebraic structures, applicatives are the general superset and monads are the special subset, because monads have all of the algebraic structure of applicatives, plus some more. But we can categorize the objects into sets differently. Define your sets by the structure they *lack* (that is, by what you *can't* do with the objects in them, or in terms of the *restrictions* on their use) and monads become the general case, the superset, because there are fewer things you can't model with them compared with applicatives. (If you're familiar with set-theoretic duality, you'll recognize that all I'm saying is this: *A is a subset of B if and only if the complement of B is a subset of the complement of A*.) As type-mad functional programmers, we sometimes tend to use types in just this way—to *restrict* the sort of computations we can do. This way of thinking about types, in terms of what we *can't* do with them (i.e., in terms of the operations that types *don't* support) is rather backwards from the usual mathematical practice of classifying algebraic structures in terms of what we *can* do with them (i.e., in terms of the operations they *do* support). And so you get the sort of modeling inversion you noticed in this case. Monads, in relation to applicatives, are the more general computational structures in our backwards what-restrictions-do-they-offer sense—there are fewer things we can't do with them. That's what motivated the blogger's post, finding a way to encode restrictions on usage into the types. Independent random variables can be modeled in a more restricted setting, and so we prefer to model them in that restricted setting, using applicatives, to ensure that we don't screw things up and introduce unintended statistical dependence. Does that make sense?
There's many definitions of fast. A browser engine is such a flexible and enormous piece of code, you're probably doing really well if you can get it to perform within 5-10% of the theoretical peak performance of the hardware you're running on. You could never hope to optimize it like some single screen page inner loop. So for those type of applications safety features are likely a good trade-off. But then there are things that really have to be fast. Stuff that performs at 300GFlops on a machine with 350GFlops peak. Think of the most critical routines in a library like BLAS, an FFT library, core components of a video codec, the intersection and traversal code of raytracer, a super optimized N-body simulation or rasterizer etc. You'd possibly lose 2-4x performance just going from assembler / intrinsics to plain C. You'd possibly lose another 2-4x adding safety features like bounds checking. I think for 90% of the code in the world a language like Haskell is fast enough. For the next 9% Rust with all safety on is fast enough. The next .9% percent can probably be fine with unsafe Rust / plain C. And then there's a few things you basically want to write in optimized C + asm/intrinsics. So I think the answer on if you can write 'fast' code without unsafe really depends on the problem and the definition of fast.
&gt; I got really tired of defining all data types in Haskell, Elm, PureScript, ... and writing JSON parsing/serialization functions for them in all languages. This is exactly why I think Elm/Purescript/etc., although a massive improvement over straight-up javascript programming, just aren't enough. Using aeson and GHCJS, the generic to/from JSON instances are understood server- and client-side, so you don't even have to lift a finger to get exchangeable datatypes; just define them once, use them everywhere. Throw servant in the mix and you can even derive XHRs from the same types which define the server routes.
`favFood :: Maybe String` I would not do this because "not yet picked a favorite food" should just be `favFood = ""`. If "not yet picked a favorite food" is `Nothing` then what does `Just ""` mean?
[I found this blog post to be useful](http://adit.io/posts/2013-06-10-three-useful-monads.html#the-state-monad) when it came to state monads. It also covers reader and writer if you're not familiar with them OP.
And [chomp](https://crates.io/crates/chomp).
Since servo is using OpenGL, a good number of things is written in GLSL, which would be the same regardless of programming languages. However, it also makes good use of Rust's features to e.g. avoid needless copying.
Rust iterators don't do boundary checks, by the way, because you don't directly index anything.
I get broken CAPTCHA too. :(
I'm not sure if I'd use it, but now I'm quite curious to see how it's implemented :)
I've been 'cheating' with my Haskell code and just ran stuff in a compute shader. I clearly have still a lot to learn about Rust, but the core ideas are super appealing.
Mostly for interaction with web frontends (js, purescript, elm etc) 
I'm not that experienced with either, but I would say yes, always lenses and not IORef. IORef is unsafe and kind of breaks the idea of purity in Haskell, especially if you dealing with non-IO types like Person and Limb. For more discussion about the differences, see this stack overflow post I found: http://stackoverflow.com/questions/5545517/difference-between-state-st-ioref-and-mvar
Gotcha, in which case binary (protobufs) is more hassle than benefit. 
Ah, it's not just (+1) to each of them. I just used the same `intU`over and over for each `Int` because I didn't want the example to drag out. Each one could have it's own distinct updater, and combined with `down` can even use different input types. Eg: data Thingie = Thingie {getStr :: String, getInt :: Int} data Employee = Emp String Int strU :: Updater String String strU = U (++) intU :: Updater Thingie String intU = down getInt $ U (+) updateEmployee :: Updater Thingie Employee updateEmployee = magic $ down getStr strU # intU # endU % voidU runUpdater updateEmployee (Thingie "Yarr " 2) $ Employee "Har" 5 == Employee "Yarr Har" 7
Thanks for the feedback regarding the issues regarding transactions. I'm wondering if I was really aware that using begin/abort/commit incorrectly tends to result in warnings rather than errors; in any case, if I was, while I was fully aware that transactions can't be nested, I certainly wasn't overly cognizant of what would happen if they were. While the typed approach in postgresql-transactional is certainly a solution, there are other solutions. The one I have in mind is that libpq offers `transactionStatus` to determine whether or not the connection is in a transaction, and this is a cheap, purely local computation. The most conservative thing to do would be to throw an exception when someone tries to use `withTransaction` inside another transaction, but one could conceivably take another approach, such as simply performing the inner IO action, or by using savepoints to emulate a mildly limited form of nested transactions. There's certainly a number of issues to consider carefully though, with regard to these less conservative design points.
Yes, that's a good point - though I'm not sure what the alternative typeclass is. Is there a typeclass in base that describes types that have an identity but no associative binary operator? I think I've seen this in some libraries as `Default`, though it would be a bummer to have to define a new typeclass. Another option would be to have a version of `stepMany` that takes a first value and then a list as well. I'll see what works well, thanks :)
[removed]
You'll be hard pressed coming up with a law on it, so it probably shouldn't even be a type class.
Updater appears to be a profunctor, which has convinced me even further that it is equivalent to something hidden in lens. If only /u/edwardkmett were here.
I did look at profunctors prior to building this, but the way I wanted it to work seems more like a... I don't know, ProApplicative, which doesn't even really make sense as a thing. Also, it can't be an instance of `Profunctor`, because a `c -&gt; d` function is not enough to take an `Updater a c` to an `Updater a d` even if `b -&gt; a` can take care of the first parameter. You also need a `d -&gt; c` function to do the conversion.
In short: It's all about tagged unions and parameterized types. These are not supported by thrift/protos in a "natural" way (although the support has improved since proto3). Apart from that I did not like the code generated by hprotoc, but that's just my personal taste. I'll put up an in depth comparison as soon as I update the project with a proper documentation.
Why not Cap'N Proto?
My ideal would play nice with LaTeX, since about half of what I want `diagrams` for is to avoid using TikZ and PGF; unfortunately, last I checked, `diagrams` was a bit flaky on that front. My pie-in-the-sky wishlist would be if I could draw diagrams shapes/arrows anchored by parts of the latex document (e.g. red box around this word, arrow from this word to this word, etc); I had a colleague do this in TikZ.
No, I meant about one thousand lines per second. Not great, but that's what we have. 
Thanks for your answer. Actually, the SO question is not mine. Someone else asked the question. But, all of the tools, but hlint, that you listed here are not considered as "code quality checker". They definitely help with improving the quality of the application, but you could for instance write a really crappy QuickCheck tests. Code quality checking is mostly about detecting bad smells in your code and suggesting good coding practices. For instance, if you have functions or data types with lost of parameters, there is probably something wrong with your design.
Well, I was hoping to get the whole thing tidied up and write Haddock for it and perhaps even be the core of the first thing I publish to Hackage, but I kept nit-picking and didn't get anywhere yet. So the core of it is as thus: newtype Updater a b = U {runUpdater :: a -&gt; b -&gt; b} (U fb) # (U fc) = U $ \a (b,c) -&gt; (fb a b, fc a c) (U fb) % (U fc) = U $ \a b -&gt; case b of Left b -&gt; Left $ fb a b Right c -&gt; Right $ fc a c idU = U (flip const) voidU = idU :: Updater a Void endU = idU :: Updater a () trans f t (U fa) = U $ \v -&gt; f . fa v . t magic = trans fromEot toEot `#` is for tuples, `%` is for Eithers, `trans` converts between updated types, and `fromEot, toEot` from `Generics.Eot` happen to be exactly the right functions for `trans` to convert to any type that's `Generic`. Also, for some reason, GHCi can infer a working type for magic, but GHC cannot.
I slipped on the double negations :) this means that I didn't fully grasp your duality argument. Perhaps I need a concrete example. IIUC, in `m &gt;&gt;= \x -&gt; ...`, `m` is evaluated only when `x` is requested by a later expression, later in this case denoting a more nested lexical scope (the expression "doesn't factorize" into independent computation paths). On the other hand, `... &lt;$&gt; y &lt;*&gt; z `, `y` and `z` are not necessarily computed in the order in which they are stated in the expression. I think my confusion stems from the use of "restricted"; I still find it easier to think of it as "smaller set", rather than "having fewer class functions".
I haven't actually used any of the ones out there, but I know there's a couple of different implementations. 
Yes... unfortunately this is not really workable - especially if you wished to then pass the Behavior into other functions which use the Behavior independently. So it seems there are cases you want sharing and other times you don't - and choosing one or the other will always have undesirable behavior in some situations, back to square 1? 
Don't lists usually have O(n) concatenation?
sorry, I meat in this case: s1 ++ s2 ++ s3 ++ s4 ++ s5 ++ .. ++ sn. If the expression is evaluate from left to right, then for every concatenation, we'll need to start from the very beginning. then n*O(n) = &gt; O(n^2)
I'm asking this because I'm concerned when using Writer String a And worried whether or not tell xxx could lead to poor performance.
`(++)` is *right* associative, exactly for this reason, so the expression above will do only linear work. In some other situations, a recursion might left-associate concatenations, resulting in quadratic time. That situation requires a deeper transformation, say using an accumulation parameter.
I'm against both. I think the "not yet picked a favourite food" should be something like `String -&gt; Person` or even: data Person' food = Person' { age :: Int , name :: String , favouriteFood :: food , limbs :: [Limb] } type Person = Person' String type NoFavFoodPerson = Person' () So that if your function need not require a favourite food, it will work with both types, and if you need the food name, it will have to be `Person` not `NoFavFoodPerson`. 
Thanks - the code you pasted turned into 1-line, I assume it looked like this: -bash-4.3# nix-shell --dry-run -p 'haskell.packages.ghc7102.ghcWithPackages (p: [p.lens])' these derivations will be built: /nix/store/vlxsizza1h7kv00l4il1rc9kksv403qh-ghc-7.10.2.drv these paths will be fetched (8.87 MiB download, 143.31 MiB unpacked): /nix/store/2i49m9brq6564s095w21jy0k7zacl063-lens-4.12.3 ... /nix/store/wvcb2aqn7kl07spjsi0ixjzy2d1ax8vk-mtl-2.2.1 build input /nix/store/6nfniwjib1rcq5j9481c9kd1c4qshr1q-ghc-7.10.2 does not exist When I run it on my machine (nix on Fedora 22 as you say), I get this: $ nix-shell --dry-run -p 'haskell.packages.ghc7102.ghcWithPackages (p: [p.lens])' these derivations will be built: /nix/store/83f2lff2pv2c130rvwcs6p5m4jwznxyh-xml-1.3.14.tar.gz.drv ... /nix/store/98bbs753dmihjylq3arn0iwljb859jnd-ghc-7.10.2.drv build input /nix/store/3ma8hf4mj7z4f94mw105jjrwq0r56f36-ghc-7.10.2 does not exist In other words - and if I understand correctly - all will be built and nothing will get downloaded from binary cache, as I don't have the line that says "these paths will be fetched". Am I right? Now I'm a Nix newbie and I'm not sure what if anything I did to mess it up, I'll try to google for some pointers on how to troubleshoot this...
It's not a `Profunctor`, but it is an [`Invariant` functor](http://hackage.haskell.org/package/invariant-0.3.1/docs/Data-Functor-Invariant.html): instance Invariant (Updater a) where invmap f g (U u) = U $ \a -&gt; f . u a . g instance Invariant2 Updater where invmap2 _ f' g g' (U u) = U $ \a -&gt; g . u (f' a) . g'
Well, a few days later and I agree that TH is...challenging. Anyway, I have something very simple working, sort of, and test cases would be helpful. A quick summary: The user of this library needs to specify a Monad to build in (IO for your command line or some HTML monad for the form), an applicative-like thing--which can be set to Identity in the non-FRP case-- a function for dealing with sum types (those are options in the command line case but, maybe, a dropdown in the HTML case? For a static form you could likely only have enums, not sum types with fields, otherwise you would need to change the form when the selected value changes. FRP can handle the dynamic form...) and the base case parsers (which I call "builders" since they build values) for simple types, which are instances of a class: type MFM m f a = m (f (Maybe a)) class Builder m f a where build::(Monad m, ApplicativeLike f,Summable m f)=&gt;Maybe a-&gt;MFM m f a That can just be something like readMaybe for the IO case or some basic input fields in the HTML case. Then you use the TH to generate instances of Builder for more complex types. If your only sum types are enums, I think you can do it all pretty easily with generics. It's sum types with fields that get kinda hairy. But they got hairy in TH as well... For command line parsing I think I can manage a skeleton and see if it works. For now, it will require input in the order things are laid out in the data structure used to generate the parser. I'll see what I can do to fix that and use field names as flags as in your example. On the HTML side, do you have a particular library in mind? My only experience is reflex-dom which adds the complication of FRP. But you are interested in a static form, right? Adam 
This looks a bit like [Divisible and Decidable](https://hackage.haskell.org/package/contravariant-1.4/docs/Data-Functor-Contravariant-Divisible.html). I've been wondering what I can use for something Applicative / Lensey, although I keep working around the lack of it using my own liftL2 / liftL3 / etc... combinators. It occurred to me on the way into work this morning that Divisible and Decidable might be the things I need (if I can massage them into shape). I've been making mine work with `Prism`s from `lens`, which has been pretty handy so far. I should probably try to make it a bit more generic in the kinds of lens-like thing it can handle. I've already got a profunctor / applicative / alternative in the mix, so I guess the next steps are proapplicative and proalternative...
What's maxos? Or is that a typo?
I had an idea a couple months ago when my boss said "we only use regex parsers here." The idea was to parse regex expressions into an ADT. Then transform the ADT into an ADT that is more efficient or uses less parsers. Then turn the ADT into an attoparsec parser. It's been burning in the back of my mind for an interesting Haskell project. 
Yes, Parsec is great because its really easy to work with and debug etc. Does it meet your speed requirements? Only you know what they are. My suggestion is to test it and see if you're happy with the performance. If not there are way of speeding up some Parsec parsers, but that depends very much on the application domain. If it can't be sped up, you can try using another parser combinator library or Happy/Alex and then benchmark it against your original Parsec implementation. 
Ok so this is what confused me &gt; With minimal optimization the compiler processes about 2500 l/s But I think I see -- you mean that with the compiler itself doing minimal optimization you get the higher rate, not with the compiler itself compiled _at_ minimal optimization you get the higher rate. Co/contravariance strikes again :-)
Parsing into ADT is really cool. My use case is much more simpler, I just need to parse ``.``, or ``*``. so the parsing can be easiler converted to a context free parser. will update the main thread.
The problem is here: https://leetcode.com/problems/regular-expression-matching/ And my solution based on parsec: https://gist.github.com/wangbj/0a08c0b0bc17b4ddaa67 I ran crition on my regex parser as well as ``=~`` from ``Text.Regex.TDFA``, so far I didn't find the regex parser is slower. Code can also be found below: {-# LANGUAGE FlexibleContexts #-} import Text.Parsec import Control.Monad.Identity import Data.Either dot = do _ &lt;- anyChar return () regular c = do _ &lt;- char c return () star c = do let p = if c == '.' then anyChar else char c _ &lt;- many p return () compileHelper = sequence_ . reverse . go . reverse where go [] = [] go (x:xs) | x == '.' = dot : go xs | x == '*' = star (head xs) : go (tail xs) | otherwise = regular x : go xs compile s = compileHelper s &gt;&gt; eof match :: String -&gt; String -&gt; Bool match s p = isRight . runIdentity . runParserT (compile p) 0 "&lt;stdin&gt;" $ s 
I created a very simple test case with criterion, looks like the generated regex parser isn't slower than Text.Regex.TDFA, as the 2nd test case will be killed. (not sure if `` commitBuffer: invalid argument '' means anything). $ stack exec re1-exe -- --output re1.html benchmarking match/1 time 54.22 ms (49.80 ms .. 57.79 ms) 0.985 Rre1-exe: &lt;stdout&gt;: commitBuffer: invalid argument (invalid character) $ stack exec re1-exe -- --output re1.html benchmarking match/2 Killed https://gist.github.com/wangbj/32cab9dc145ccf4b1646
Ah, didn't see you over there. You are surely about 9 hours earlier than me. Thank you https://www.reddit.com/r/haskell/comments/469n8s/best_practices_for_modifiable_state_in_haskell/d041nmj
IORef (and maybe STRef?) is an optimisation, and maybe not the first tool to reach for. There is a lot to be said for just manually pushing out the new instance instead of the old one too. Lenses (or more generally optics) are great though! Running person &amp; limbs . ix 2 . length %~ (+5) will return a new person object identical to the old one except the third limb has grown by five. This new person object can then be returned and recombined into the person list. If you want to work with a `State Person ()` you can instead run limbs . ix 2 . length %= (+5) which will update the person in the state to the new person with a better limb length. Maybe they start walking on all fours? No problem! limbs . each . isLeg .= True This is one of the great things about optics -- they can change multiple values at once.
&gt; person &amp; limbs . at 2 . length %~ (+5) Oh, I see. "&amp;" accesses something inside person, 2 is the index, and +5 is the curried function that you are applying to the length variable. Pretty cool. I love how intuitive Haskell syntax is. That is pretty cool.
Yes, indeed. Ghc always gets -O2, Sorry about the confusion.
If I use Arrows instead of Monad, then we get, let me see ... Something like this, I guess: myPipeline' = proc context -&gt; do size &lt;- contextSize -&lt; context texScence &lt;- R.renderedTexture &lt;&lt;&lt; drawMainScence -&lt; size texMask &lt;- R.renderedTexture &lt;&lt;&lt; drawSomeMask -&lt; size texBlended &lt;- R.renderedTexture &lt;&lt;&lt; drawBlended -&lt; (size, texScence, texMask) drawTexturedQuad -&lt; (size, texBlended) &gt; you can extract information about the steps without having to provide a value of type a and executing the steps, like you would have to do with a monad. So we can forbid the user from the `ArrowChoice` and `ArrowLoop` part, then the user can't have something that behaves like do (x, y) &lt;- getContextSize if y &gt; 100 then replicate x drawFoo else drawBar (I'm not sure enough that people would never have such requirements.)
So regex is part of your company culture. A perl shop by chance?
Wouldn't you have to wait for your last call to tell and build a thunk until then to make use of the optimization you have in mind in this case?
The back-end lib/framework `servant` is picking up a lot of momentum lately, and it also comes with "client side API wrapper code gen". Currently a Haskell client and a JS client (which can support jQuery, Angular and plain JS). Do you foresee that `servant` could at some point use `typed-wire` to implement part of it's code generation?
As a real noob. Did you just use the standard FFI or you have to wrap Rust functions?
that page asks for O(n) algorithm, specifically rejecting sort-based solutions. 
That's not *quite* what `(&amp;)` does, though the fact that it looks that way is intentional. :) The following is exactly equivalent: (limbs . at 2 . length %~ (+5)) person What you're actually doing here is composing a custom accessor function which *takes* a `Person` as its argument, and returns a modified copy with the second limb lengthened by 5. All `(&amp;)` really does is this: x &amp; f = f x
Standard FFI. It's pretty easy to [export rust functions for use with a C FFI](https://doc.rust-lang.org/stable/book/rust-inside-other-languages.html)
Keep in mind this is not Haskell syntax! The compiler knows nothing of this. Those operators are user-defined in a library. :) You could also write the same thing as over (limbs . ix 2 . length) (+5) person without the operators, and which you prefer is a personal choice. I'm a fan of the operators myself. They follow a very easy to learn pattern: * `thing &amp; accessor .~ value` returns a new `thing` where the `accessor` part inside is set to `value`. * `thing &amp; accessor %~ function` returns a new `thing` where the `function` is applied to the `accessor` part inside. * `thing &amp; accessor +~ number` returns a new `thing` where the `accessor` part inside is increased by `number`. These also exist in "state-enabled" versions, where instead of specify which "`thing`" you want to operate on, it just assumes the state is the thing you want to operate on: * `accessor .= value` modifies the state thing so the `accessor` part inside is set to value. * `accessor %= function` modifies the state thing by applying `function` to the `accessor` part inside. * `accessor += number` modifies the state thing by increasing the `accessor` part inside it by `number`. What's really cool about these is that in traditional object-oriented languages, `accessor` can only be one field. If you want to add interest to all accounts in a bank, you have to do for account in bank.accounts: account.amount *= 1.02 With these powerful optics, you just do (with the stateful version) accounts . each . amount *= 1.02 and it applies to all of them. The `accessor` part can target multiple fields. It's an obvious extension of regular object-oriented accessors, and one you can't live without once you've had the taste of it. Anyway, the operators for getting values out of objects (we've only talked about setting them so far...) is similar: * `thing ^. accessor` returns the value under `accessor` from inside the `thing`. If `accessor` points to many fields, you can get a list of all of their values. * `thing ^.. accessor` returns a list of all values pointed to by `accessor`. 
&gt; `newtype Updater a b = U {runUpdater :: a -&gt; b -&gt; b}` This is (isomorphic to) a `Setter () a b b`, and I suspect you'll be best off if you just do all this stuff in terms of lens machinery. https://hackage.haskell.org/package/lens-4.13.2/docs/Control-Lens-Setter.html
Generally, you cannot naively encode regexp into parsec parser. Take for a example a bit contrived regular expression: `((a)*)*`. Trivially encoding would be `many (many (char 'a'))`. And that will throw a runtime exception: you cannot `many` of parser which accepts empty string. Also regular expressions like: `(.*)\.extension` are tricky for parsec as well. Unfortunately I don't know if there is comparison of `regex-applicative` with `parsec`, but either way, writing directly in Haskell feels much easier then the generation step. If you really want to generate, you could consider using `alex` to generate token stream lexer and then parse that with e.g. `parsec`. *EDIT* there is also https://hackage.haskell.org/package/lexer-applicative
If you change `tails` to `repeat` and compile with `-O0` (the default) this program [uses more and more space](http://pasteboard.co/1E31YKDu.svg+xml).
You'll be able to reuse a whole bunch of existing code and whatever you do will be reusable by people who already use lens.
Notice that in the interface you guessed, textures appear on the left of `&lt;-` and on the right of `-&lt;`. That means that the choice of which texture to use can change from frame to frame, like this for example: drawTexturedQuad -&lt; (size, if fst size == 1024 then texScene else texBlended) So now you can examine the steps and determine how many calls to `R.renderedTexture`, `drawMainScence` etc. occur in the pipeline and in which order, but you can't determine which texture is used by each part, so that's not at all what I suggested. Now that I think about it though, maybe you don't need to know which part uses which texture after all! You only need to know how many textures are needed in total, right? So if `R.renderedTexture` is the command which conceptually allocates a texture but in practice will try to reuse an existing texture for performance reasons, then being able to count the number of calls to `R.renderedTexture` in your pipeline might be enough for your needs. &gt; So we can forbid the user from the `ArrowChoice` and `ArrowLoop` part I guess you could... but you don't have to! You don't know in advance whether the `then` or the `else` branch will be taken, but you do know which calls are used in each. So you could, for example, count how many textures are needed in each branch and allocate the max of those two counts. Now you have two ways to change the behaviour of the pipeline: either via an `if` inside the pipeline, or by changing the entire pipeline. If you tell your users to use `if` for short term changes like making a sprite blink, the texture for the sprite will be kept around instead of being deallocated and reallocated each time it blinks, which is probably what you want. And if you tell your users to change the pipeline for long term changes, like when entering a bonus level, the textures which aren't used in the bonus level will be deallocated when you enter it, as needed. Regarding `ArrowLoop`, you can again inspect in advance which steps are used inside the loop, so if there are no textures allocated inside the loop, you know that the total will be zero regardless of how many times the loop is executed. Hmm, but if it turns out that textures are allocated, you'll have to count infinity times the number of textures allocated during each iteration... yeah, maybe forbid `ArrowLoop` :)
Sure! On my website there's a list of all the references I found helpful, be sure to check that out. Also, maybe go back a few revs in the repository, the code of course gets increasingly tricky as optimizations are made ;-)
In terms of data structure, there is a mechanism for that called difference list. the various builders are making use of this. It's a specific case of an embedding of a monoid into the endomorphism monoid called the cayley representation. another example is the codensity monad. Rewrite rule is more straightforward ! but theory is nice too
Your use case is simpler but an ADT would probably still be good for future upgrades. In your specific use case you could use traverseable where you traverse the ADT with maybe and return your allowed parsers wrapped in a just. An ADT is very flexible. 
The closest analogue that comes to mind is Ahman and Uustalu's notion of an "update lens": http://homepages.inf.ed.ac.uk/s1225336/papers/types14.pdf Right now, in `lens`, we always fix the update language of a lens to be the state type rather than some monoid that acts on that portion of the state, but that need not be the case. Upgraded to an "update traversal" form, you'd get something that could walk the targets and change multiple things, but get access to the old value at each step. Downgraded from that to an "update setter" you'd get something closer to what you have here. There are composition issues with them, however.
To be clear, I didn't intend any negative feedback against postgresql-simple. We love and use _tons_ of postgresql-simple!
&gt; and yes, I want to do it from 2 threads and I don't care that you can't guarantee they won't clobber each others data. Were you using a slice here? With slices you can use [`split_at_mut`](https://doc.rust-lang.org/std/primitive.slice.html#method.split_at_mut) to obtain two distinct mutable slices from one slice, then hand over each slice to a distinct thread while keeping rustc happy. Underneath the covers it's obviously unsafe, but it's been guaranteed safe so you don't have to take risks. &gt; I really want to write to this piece of memory without bounds checking The Rust community is always interested in looking at code samples in which bounds checking is not elided by the optimizer; the last time it led to a pull-request to LLVM. Do you have some specific examples you could share which confused the optimizer?
And [peruse](https://github.com/DanSimon/peruse). And [parsell](https://github.com/asajeffrey/parsell). There's also a bunch of parser generators and PEG libraries like [lalrpop](https://github.com/nikomatsakis/lalrpop), [glr-parser](https://github.com/qdwang/glr-parser-rs), [rust-peg](https://github.com/kevinmehall/rust-peg), and [oak](https://github.com/ptal/oak).
The Parser uses Happy, which sloccount thinks is yacc due to syntax similarities. If I had to do it over again, I'd give it a try with the [Earley](https://github.com/ollef/Earley) parsing library. Edit: forgot to mention the early versions of Hython used Parsec, but then I learned why maximal munch was needed the hard way :(
Use [dlist](https://hackage.haskell.org/package/dlist) when you want to use a list type with a Writer monad.
A different direction is the dependent type approach: data ProtoPerson food = Person { age :: Int , name :: String , favFood :: food , limbs :: [Limb] } type FoodedPerson = ProtoPerson String type NonFoodedPerson = ProtoPerson () This allows you ensure at compile time that you won't use a person for whom the favorite food has not yet been specified in a context where it is required. It also frees you from the obligation to thread Maybe checks throughout your program even in entire sections where you already know whether or not the favorite food has been specified. You can get fancier with type tricks, like limiting the type parameter to be either `String` or `()`. But that's the basics.
I have just tried again and it worked. Thank you all for helping.
This is so awesome. Thanks for doing it! I've never seen Happy in the wild outside of GHC itself. You say that if you had to do it over you'd choose Earley? Any performance reasons or merely a nicer API? Also could I use this to take the source for a pure Python 3 library, interpret it with Hython and just use it in a Haskell project without invoking the Python interpreter?
&gt; lenses and IORef and STRef these are all *totally* different things for totally different purposes! # lenses Lenses are a principled way of talking about indexing into deeply nested data structures. They work *very* nicely with stateful computations for many of the reasons that /u/kqr talked about. Generally, the `s` in a `State s a` is going to be somewhat complex, so having a convenient way to reference and update it is fantastic. # STRef STRef is used in the `ST` monad. ST is a clever trick that allows you to run certain kinds of impure code in a pure context. If we have a pure function that's just a little too slow using immutable values, we can wrap it all up in `ST` and get access to local mutation, and the compiler ensures that none of the mutation can escape into the outside world. Consider `quickSort :: Ord a =&gt; Vector a -&gt; Vector a`: the immutable version is vastly slower than the mutable version, and uses much more memory. So we can lean on `ST` to optimize our code where mutation makes more sense. # IORef An IORef is a reference, just like you're used to in Java or C++ or whatever. They are about as annoying and problematic to deal with in Haskell as they are in every other language, and that's why you don't see them too often in idiomatic Haskell. Generally, if you're working with performance sensitive code in `IO`, it's easier to just go right to `TVar`s and enjoy Haskell's excellent concurrency and safety from STM.
So FoodedPerson has a favFood of type String and NonFoodedPerson has a favourite food of type ().
I don't mean to dismiss your concern, and for `Writer String`, I think the pathologically bad (quadratic) performance is a real issue.
I often think of that trick to, for example be able to change the type of containers (Maybe, List, or plain) or transform type so you can "remove" fields to make them a monoid. The problem with this approach is you can't constraints the variable type to a subset of type, so I can have `ProtoPerson (ProtoPerson Int)`.
I recently noticed a trick with CString from [Text.Show](https://hackage.haskell.org/package/base-4.8.2.0/docs/Text-Show.html). &gt; The shows functions return a function that prepends the output String to an existing String. This allows constant-time concatenation of results using function composition.
Nice, I've been looking for an Earley parser! I will definitely try it.
So your parsec parser took negative nineteen times the time of the old parser and was complete before it started? A pedant points out 2000% faster is not the inverse of 2000% slower.... Sorry couldn't resist
I mean that update lenses don't compose with other update lenses in a nesting sense. Consider s -&gt; (a, m -&gt; s) where m is just some Monoid `m` that describes how you change `a` with an action `act :: m -&gt; a -&gt; a`, where `act mempty = id`, `act (m ``mappend`` n) = act m . act n`. `act` is a monoid homomorphism from `m` to `Endo a` This doesn't let you compose two of these recursively, because such a lens takes a modifier for the internal 'a', but gives back something that does the the change, not a 'delta' that in turn acts on `s`, but a whole new `s`. We could modify this scheme, however, to have it be something like `s -&gt; (a, m -&gt; n)` where m acts on `a` and the function m -&gt; n should be a monoid homomorphism that gives back `n` that acts on `s` given a change in `a`, but then packaging this would need a typeclass that inferred "the" monoid that acted on `s` (or `a`) given just `s` or `a`. Choosing such a thing canonically is a mess. But even just the `a -&gt; b -&gt; b` scheme doesn't give enough to be able to recursively nest these structures, there is no (.) for sticking together such updaters. This is why a `Setter` in `lens` is the`Functor`-like thing rather than something like this, which naively seems closer to the common intuition for what a `Setter` would be. 
Awesome, I can't wait to play with `Earley` in another project. Was there anything you missed moving from `happy`?
I tried to use it for the [these project](https://github.com/isomorphism/these) but `stack exec -- sensei test/Tests.hs` failed due to some CPP problems. Looking for a test suite without CPP I tried [`pipes`](https://github.com/Gabriel439/Haskell-Pipes-Library). Running `stack exec -- sensei tests/Main.hs` and changing one of the library files in src results in Ok, modules loaded: Main. --&gt; /home/simon/src/Haskell-Pipes-Library/src/Pipes.hs but it doesn't seem to reload the changed file. I believe I'm supplying the wrong arguments to `sensei` here? `sensei --help` doesn't do any good either.
Sure. https://github.com/adamConnerSax/dataBuilder But...some caveats: 1. The example (an interactive IO builder) is entirely contrived. 2. I need to think harder about the types before I settle into a design. I think the I need to allow more user control at the monad level and that might change things some. 3. I think this can all be done via generics (I'm looking at the generics-Eot library which is based on generics-sop) and I may try to figure that out soon. And that would be simpler, I think. 4. Generating an applicative parser is interesting and makes me wonder if I can drop the outer monad entirely in some cases. My FRP version needs the m (f (Maybe a)) structure but maybe that's an exception. 5. I'm new to github so I'm not quite sure that project will compile as is. But it does for me (with stack). Adam 
&gt; What do you actually mean by that then? Consider the lens notion of `_2._2`. There is no analogous nestable construction here.
Woah, this was really interesting! Thanks for sharing!! :)
Same here, I was using `Emacs-24.5.1` on Ubuntu and OS X with `spacemacs-0.105.9`. The strange thing is that I couldn't reproduce this problem using a bare `.emacs` with only `evil` and `haskell-mode` installed..
Thanks for suggesting it. I just took a look though, and it seems to suffer from the same problems I listed above. I can write purescript just for the sake of writing purescript, but I'd be more productive and run into no interop problems just using javascript instead. I would prefer a backend language that improves my toolbelt rather than a compile-to-js type language that attempts to replace the one language I'm pretty good at. 
For webdev you could consider [GHCJS](https://github.com/ghcjs/ghcjs), which is a very cool project, but you're more likely to enjoy [haste](http://haste-lang.org/) or [elm](http://elm-lang.org/) as they are smaller and more geared towards webdev than just generating js. Haskell is a good language for making DSLs and so I could imagine making a DSL for your data science needs, but it would be a big upfront investment. Finally, if you really do want to become a better programmer it's good to step out of your domain and stretch your muscles. It's healthy to make a point of regularly working on something different than your norm. You could reserve haskell for this type of programming activity. Set aside some time each week or month to just do haskell. Maybe do haskell 3 hours / week for a year? I bet you'll learn a ton.
But the point of writing it in a FP language would be the greater safety and correctness it gives you. It's not a matter of whether you can write it in another language. You can make this argument for almost anything. The idea is that once you're really good at FP, you'll both be more productive and more correct with it.
so when you copy&amp;paste from and to the same buffer your indentation grows? Can you add your `.emacs` (or whatever other configurations you are using)?
IMO it's not strictness or memory safety that gives Rust an advantage in graphics programming - it's linear ownership. GPU objects like buffers, textures, contexts, etc., all make sense as linear entities. When you write in Haskell, you're writing in a logic which does not really model the domain you're working in. There really isn't any way to say you can't write `nonsense :: GLContext -&gt; (GLContext , GLContext)` in Haskell, but it's trivial in Rust.
I guess most people who befriended Haskell have been in this position. There is nothing you can do about it. Haskell can do most things, even well. But the fact that it is not widely used makes it less appealing if you just want focus on your work and not write 10 custom libraries along the way. My conclusion is: Just use the tool that doesn't distract you from your actual work. That might be Haskell (in case of spam prevention in Facebook) or another language. It's more worth to know about the domain your working in rather than the tools your using.
Huh… is that a troll or… ? :) Where did I mention space leaks and things hanging around?
My rough understanding is that there are a few problems: * The really old &amp; widely used ones are roughly "ready" to be part of the Haskell language itself, but no one is championing them. * Many of the old &amp; widely used ones are still not specified enough to be added to the language. * Newer extensions are still controversial and it will take more time to understand if they are a good idea or not. * Some extensions don't really make sense without enabling other extensions (I'm looking at you, `MultiParamTypeClasses`), and hence there is a problem of untangling exactly what should go in the spec and what stays out.
I guess one thing to note, if you expose a pure interface I can start using it with FRP (e.g. reflex) without having to add another interpretation layer (though for various reasons I might anyway).
Low level suck. We are humans, not compilers
Knowlessman comment: I suppose there will be a performance cost associated with each extension. 
&gt; a lot of code one sees is littered with `{-# LANGUAGE ... #-}` at the beginning of every source file I like 'm. I've been programming in languages that did not have 'm and it feels like these languages are very limited in their evolution. This because only a few can try/test new language features and new features are either "on" for your version of the language or "off". I also very much hope a new Haskell spec is made. Discussions on this [have been seen on this subreddit](https://www.reddit.com/r/haskell/comments/2tu3yn/is_haskell_haskell_prime_defunct/), but I do not know what the progress is. Not having a new spec does not hurt enough I guess...
Haskell is really good at parsing, expressing in-variants in your code and solving a problem using functional methods. So anything that involves any kind of parsing (i.e. DSLs, Text,and Languages) is game for it. And if you want to verify your specifications and programs, you have the correct tool for it. So, I would look for opportunities where you can add value by interpreting or cleaning the data and writing the correct programs (safety critical systems). You work at a higher abstraction so your code is concise and has fewer bugs.
&gt;Some extensions don't really make sense without enabling other extensions (I'm looking at you, `MultiParamTypeClasses`) I assume you are talking about `FunctionalDependencies`? Possibly a naive question but in such cases, is there any specific reason that the two language extensions couldn't be combined into one? 
I dont think that would be a great idea since Undecidable-Instances for example makes the type system suck...
What's wrong with "normal" record syntax as in Rust, F#, and even C?
I think if you namespace record fields per record type rather than globally, you've already solved at least 50% of Haskell's issues. Then build basic lens-like functionality into the core language (in a nutshell, declaring a record behaves as if you had done a `makeLenses` on it), and you're pretty much there, which should solve almost all the other issues.
I mean some of them will break your code by making things work differently or more strict, others might make things less strict and allow some things through that you would normally expect a type error for. So enabling them all is really dangerous.
More importantly some of them literally break existing code, such as by changing the way things work or by making the language more strict.
Sorry went to bed so I haven't had the chance to respond to any of these. That's not what I meant. I know Haskell is a great language. It's that if I chose to do Haskell, I'd be handicapped. This comment captures my sentiments pretty well: &gt; A crappy language with a library that fits the task at hand often is a much better tool than a great language with impedance mismatch. Javascript is much more widely supported as well. 
&gt; I often find myself in a pub, without pen or paper, trying to persuade someone to try Haskell. So do I. Thanks for sharing your tips!
Here is a long document explaining where Haskell is and is not an appropriate solution to a problem: https://github.com/Gabriel439/post-rfc/blob/master/sotu.md
To expand on this a bit, the perceived problem with not having a spec for every extension that satisfies everyone is that Haskell used to be a standards-driven language so that there could exist multiple compatible implementations. We could say that a given extension is defined by what GHC-7.10.3 does, but this would be a break with written history. There's more than a little aspect of a farce to the whole thing seeing as how we get things like type roles and levity polymorphism affecting everything while bickering endlessly over what deserves a chance to live behind a language extension.
Why is that a bad thing? What better way of validating an extension then having it widely adopted in the wild? No amount of lab experiments can achieve that. To me, once an extension becomes a "de facto" standard, it's a strong signal that it should become official part of the language. I think the huge and rapid growth of Haskell is only possible because of the extensions mechanism. The new and experimental ones should simply be marked as such and perhaps even a compiler could issue a warning (or hlint).
For me it is mostly the compiler. I really like the fact that I can be reasonably certain that if it compiles it will run and do what it's supposed to do and in most cases not encounter any runtime errors. Beyond that the easy composability of functions and concise syntax make it very pleasing to work with. Neither scala nor java make it as easy as haskell to work with monads and neither of them has the awesomely powerful destructuring capabilities as Haskell does. The cherry on top is that it is pure and you'll get pretty nice control over your side effects. I also like that when using external libraries you can get a pretty good idea if what a function does by just looking at it's type signature. As for library support, sure there are many languages out there with a lot more libraries for any job, but more doesn't always mean better. JavaScripts ecosystem in particular is very short lived. Libraries are born and die faster than in any other language. Even though Scala claims to be compatible it is painful working with Java libraries, because the stdlib collections like List and Map need to be converted all the time. Plus good Java libraries are quite often proprietary, Haskell ecosystem is largely open source (this is a subjective opinion I have no evidence). Oh and `type`. `type` is so awesome, makes it much easier to keep signatures readable while not requiring you to write a bunch of proxy functions. It also makes it much easier to change a type later on without having to go through all the code and changing every occurrence.
Why be hasty 
Adoption is great. But people assume stability where they should not. That makes it hard/impossible to remove or redesign an extension. As a result extensions are frozen in some random state. And yes, marking extensions as stable/experimental will be a huge improvement. 
Maybe this is a dumb question, but why are type families better than FDs?
Haha I googled it right after I wrote that, and came across the same thing.
Aww, the author skipped `makeClassy`, which is the only form of `makeLenses` I find I ever use. (It also generates more interesting splices to look at.)
I'm pretty sure language extensions are a wildly successful idea. I know I love having them available if I want, and standard Haskell 2010 is right there otherwise. I wish other languages had a similar mechanisn
Hm, interesting – I've seen `makeLenses` and `makeFields` very often, but not `makeClassy`. When I asked on #haskell-lens, I ended up getting a reply in the form of “here's what `makeClassy` does, in comparision `makeFields` is completely ad-hoc, but I never used `makeClassy`”, which further convinced me that `makeClassy` is rare. On the other hand, it just occurred to me to make a Github search, and it turns out that `makeClassy` and `makeFields` are used equally often. Research failure on my part, admittedly. Could you give an example of how `makeClassy` works and what it's good for? (I've seen the one in lens docs, but it doesn't really answer the “what it's good for” question.)