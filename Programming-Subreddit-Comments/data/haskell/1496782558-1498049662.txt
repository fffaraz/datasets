What are the two unrelated changes? My understanding is that there never were negative literals in the first place, so the extension is not changing the interpretation of the negative literals. `-123` was not parsed as `negate blablabla` because the report asked negative literals to be parsed and then desugared to that, rather, the report only asked positive literals to be parsed, and so `-123` was not parsed as a literal, it was parsed as a unary negation applied to a positive literal.
Or https://hackage.haskell.org/package/classy-prelude
Well…you could probably hack apart the closure with `unsafeCoerce` if you’re intimately familiar with data representations in the RTS, but the point stands that it’s way harder to use incorrectly.
Huh ... I wonder ...
Exactly, just the binary :D
Then I recommend reverting this decision! Interpreting *every* string as html will make it harder to examine the true contents of a string. I'd recommend an Html type and a ToHtml instance which would allow, say, values of type Diagram or Png to be rendered automatically.
Actually, this wasn't a decision but rather a side effect of the rendering engine we are using. In any case, if you dont want to render it just use `show coolStuff` or `putStrLn coolStuff`
Indeed, it used to rely on PATH on the past, we changed it to pointing to the /usr/local/bin directory, but this will be changed in the near future.
I need to change system name from ios to darwin to run configure. autoconf version 2.69.
&gt; I'm a student doing a dissertation on functional languages and am fortunate enough to be working with one of the creators of Haskell. Nice! Are you allowed to tell us more?
You might also consider these threads: [Papers Every Haskeller Should Read](https://www.reddit.com/r/haskell/comments/2blsqa/papers_every_haskeller_should_read/) [Papers for an Intermediate Haskeller](https://www.reddit.com/r/haskell/comments/4bpd6v/papers_for_an_intermediate_haskell_programmer/) [Request: Your Favorite Research Papers](https://www.reddit.com/r/haskell/comments/4xr4q8/request_your_favorite_research_papers/)
I love [Data Types a la Carte](http://www.cs.ru.nl/~W.Swierstra/Publications/DataTypesALaCarte.pdf). It's a magnificent example of a thorny real-world problem (the expression problem), and how Haskell's typeclass machinery can give us a solution that's as convenient as an imperative/OO language with all the safety and beauty (and no vtable lookups!) of a functional language.
At least it isn't hard-coded to something like `/home/foo/tmp/test/20170607/Copy\ of\ The\ Project\ (2)/build/output/bin/My_Utility.sh`.
I wrote a paper introducing Haskell EDSLs for the ACM Q that you might find useful. "Domain-specific Languages and Code Synthesis Using Haskell" https://queue.acm.org/detail.cfm?id=2617811
How does this compare to [IHaskell](https://github.com/gibiansky/IHaskell) and [HyperHaskell](https://github.com/HeinrichApfelmus/hyper-haskell)?
Be the change you want to see in the world.
Processors are operational; Haskell is denotational. I like programming functional, and so do you. *(Nah I can't poetry)* To be fair C was definitely designed with the processor in mind, so it's going to be easier for the processor...
Have a look at the example here https://github.com/hedgehogqa/haskell-hedgehog/blob/master/hedgehog-example/test/Test/Example/STLC.hs It's generating well typed and ill typed terms for a simply typed lambda calculus. 
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [hedgehogqa/haskell-hedgehog/.../**STLC.hs** (master → 10c0589)](https://github.com/hedgehogqa/haskell-hedgehog/blob/10c0589d79513a3e231ea6112020dd2328709f30/hedgehog-example/test/Test/Example/STLC.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dikffec.)^.
Well if you read the zipper paper, then you should also read the scrap your zippers paper. Otherwise one of my favorites is the Data Types a la Carte that was mentioned.
The main benefits of using an EDSL for SQL queries: - EDSLs can be type-checked. In a well-designed SQL EDSL, well-typed and well-formed align, that is, the EDSL will typecheck if and only if the resulting SQL would be well-formed. - SQL injection safety is no longer an honor system. With a string DSL plus parametrized queries, it is possible to avoid dangerous code, but you can still write it. An EDSL can be designed such that SQL injections become entirely impossible to write. - An EDSL can be designed to support multiple SQL dialects, making your code DBMS-agnostic. And the downsides: - The EDSL is yet another minilanguage to learn, while most developers are already familiar with SQL. - Type safety is of limited use, because the SQL dialect guaranteed by the EDSL's type constraints cannot be guaranteed to match the actual database you're running against in production. - There is a performance overhead to generating those queries at runtime rather than reading them from hard-coded strings compiled into the binary. - In practice, being able to run in a DBMS-agnostic way is somewhat limited, because the EDSL designer has to make choices as to how you want to handle non-overlapping features: you can limit the EDSL to only support the common subset between the supported backends (thus limiting the EDSL's power), or you can choose to error out on backends that do not support the requested features (thus putting the burden of working around the differences on the user after all, and rendering the abstraction somewhat moot), or you can emulate missing features, making the EDSL implementation more complicated, and introducing a potential source of unexpected problems (moving resource load from the DB server to the client for emulated features, and possibly even breaking transactional guarantees). So let me relativize the quote from that answer a little: &gt; People also try embedding more complex languages like SQL and GLSL as strings. Not only is this awkward (do you want to write your whole function inside string literals?), it also opens the door for horrible bugs. SQL injection vulnerabilities are probably the single most common security issue online and exist entirely because treating SQL code as a string is a bad idea™. Writing SQL as SQL isn't without merits (see above), but you need to address three concerns: 1. Using string operations to construct SQL queries is bad, unless heavily guarded by a restrictive abstraction layer (i.e., an EDSL). So either use an EDSL, or hard-code all your queries (using SQL parameters to insert dynamic values). 2. Writing SQL queries as string literals makes for ugly, unreadable code. One possible solution is to externalize the queries and embed them at compile time (the `file-embed` package is very useful for this kind of thing); then you can write stuff like `resultSet &lt;- quickQuery' $(embedStringFile "queries/getUser.sql") [toSql username]`). 3. While many DBMSes are typed, the SQL language itself isn't, and neither is `HDBC` (in the sense that parameters and result sets use the `SqlValue` unitype). In order to avoid run-time type errors, you need to wrap your raw `HDBC` calls in typed functions, and manually match those functions' types to the types that the SQL uses implicitly (e.g., if you're fetching an `INTEGER NULL` value from the database, you need to manually declare the wrapper to return `Maybe Integer`, because the SQL itself doesn't contain that type information). So there is no clear-cut answer either way; both SQL EDSLs and writing SQL as SQL have their merits, and you need to decide for yourself what your priorities are. Personally, I prefer writing SQL as SQL, and I have written [a little library](http://hackage.haskell.org/package/yeshql) that attempts to smooth out the three concerns mentioned above, and I have used it with good success.
More hand waving: Functor: change something about your current state before you enter the future. (If you're going on a date, you might want to fmap niceClothes.) Applicative: combine two or more futures into a single future, using the state from both. (If Peyton-Jones is at a restaurant in one future, and you and your date is at the same restaurant, you might want to ap those futures and get both in parallel instead of being satisfied with just one.) Monad: split a future into multiple possible branching futures, where only one is picked, based on the state. (If you don't know whether you'll take your date or Peyton-Jones to drinks after the meal (depends on how the date goes) you may want to bind the outcome of the meal.) 
You are a Windows user? :)
No, everything is compiled with regular GHC using stack :)
Yeah, I was gonna say that it's kind of related, maybe in other more parallel problems the right tuning is that other option that Simon added: https://simonmar.github.io/posts/2016-12-08-Haskell-in-the-datacentre.html "What we’d like to do is to have a larger set of mutator threads, but only use a subset of those when it’s time to GC. That’s exactly what this new flag does"
What do you think about something like this: https://hackage.haskell.org/package/postgresql-typed-0.5.1/docs/Database-PostgreSQL-Typed.html
There is no builtin support, but here is a solution using my Variant type (an anonymous sum type): #!/usr/bin/env stack -- stack --install-ghc --resolver lts-8.15 runghc --package haskus-utils-0.6.0.0 {-# LANGUAGE DataKinds #-} import Haskus.Utils.Variant import Haskus.Utils.ContFlow data N = N data M = M -- let P be the sum type M|N type P = Variant '[M,N] combineDisjointFunctions :: (M -&gt; a, N -&gt; a) -&gt; P -&gt; a combineDisjointFunctions (m,n) p = variantToCont p &gt;::&gt; (m,n) main :: IO () main = do let --p = setVariant N p = setVariant M r = combineDisjointFunctions (const "was M",const "was N") p print r This approach is mainly useful if you want more than 2 types in your sums, otherwise you'd better use Either. See http://hsyl20.fr/home/posts/2016-12-12-control-flow-in-haskell-part-2.html
 But that's no different from any other string-based DSL, is it?
You actually get functions like `addArticle` (except for the `ReaderT` part), provided that you have suitable.`SqlRow` instances in place. You annotate your SQL with types and function names, and yeshql generates the functions for you.
But you shouldn't, at least not without severe caution and a safety harnass.
Yeah, after I got my new laptop with OEM Windows 10 on it about two months ago. It's not too bad :)
I don't understand this comment. What has `-N` to do with FFI? The `-threaded` runtime ensures that safe FFI calls execute in their own thread and don't block Haskell code, but this is independent of `-N`.
A default 30% performance penalty in common use cases sounds like a UX bug to me.
``` (QuerySegment "SELECT username, password FROM users" []) &lt;&gt; (QuerySegment "WHERE username = ?" [username]) &lt;&gt; (QuerySegment "AND role = ?" [role_id]) ``` Can you spot the bug? 
I'm also just learning right now, but I've gotten the impression so far that making an executable program, even hello world, is too advanced for a beginner. This comes from the fact that you can't just do IO like you can in C or Java. Thankfully GHCI means that we can learn the basics and syntax just using functions and don't need to learn advanced topics like IO right away. 
That's awesome, will try it again! :D For us, the good thing about haskell.do is the fact that we start with a blank canvas. We can add features to the GUI that are not possible with IHaskell. And other features could be added too, of course. For example, code completion (not intellisense) using `djinn`, better hoogle support, maybe snippets, refactoring, etc... It's not about making a clone of IHaskell, it's about making a brand new tool to work with Haskell interactively, and make the experience different from other languages. If you show IHaskell to someone who works in Python, the reaction will be "Alright, I can do the same stuff with Python". The key here is to show a tool that leverages Haskell's power so it is an attraction to the language itself, not the tool :)
Boilerplate for beginners who want to write executables: If you only want output. main :: IO () main = putStrLn result result :: String result = &lt;&lt;your thing here&gt;&gt; If you want input and output. main :: IO () main = interact program program :: String -&gt; String program = &lt;&lt;your thing here&gt;&gt; This is enough to write turn-based games (connect 4, chess, toy text adventures), or programs that you can give a file on standard input and have them do something with it (word counters, histogram displayers, sudoku solvers). Woth knowing here are the `lines` and `unlines` functions - a common form for `program` is `unlines . f . lines` with `f :: [String] -&gt; [String]`.
The "ghc developers hate me" joke is that Austin is himself a ghc developer, so don't worry, they're aware of the issue :)
I don't know what are you exact goals, but I would consider Nix. That would give the possibility to manage not only Haskell dependencies, but external ones too. Since this is data science related, you will probably need lots of external packages, and it can be very useful.
Looks promising - I'll take a look. Thanks!
There are a number of ways to ensure a global order in Postgres, but autoinc keys isn't one. Of course, whether those solutions are desirable or even necessary in your application is another question. The technique I use for situations that need the utmost reliability, is to have a trigger insert a row into a side table for each (receiver,event) pair. This side table has a partial index on unprocessed events. Each receiver consults this table to find events it has not seen, and then marks the (receiver,event) pairs as processed as appropriate. This has a number of tradeoffs relative to local stateful tracking of the position in the event stream. I am interested in other techniques applicable to postgres. Watching the WAL is a particularly interesting idea to me, though that would probably take quite a bit of work.
Definitely! Thanks for pointing this out, its some very valuable feedback! :)
Someone needs to come up with an empirical analysis that is similarly sloppy and demonstrates the opposite result. It shouldn't be hard to do.
I don’t know of any good resources offhand, but I do have one bit of advice from experience designing programming languages and APIs. Start with examples of the code you *wish* you could write, then figure out the implementation that will let you make those examples work—with the correct semantics, reasonable efficiency, and so on. Often you’ll find that it’s necessary to tweak the examples slightly, and that’s fine; it’s still better to start by implementing things that are *necessary* to make practical examples work than to conjure up abstractions in the hope that they’ll be useful. 
So you execute it in the server and send the result back? What about creating server applications from snippets, similar to the ones that School of Haskell had?
Thanks!
You know, funnily enough, the first thing I ended up trying was `sequence`, and I got an error about () not matching [()], and gave up that route. I've used `sequence_` a million times in other contexts because of the exact same scenario, but for some weird reason it just didn't occur to me that it'd work here too. I also came up with a hack-solution of doing: `runConduitRes $ (D.yieldMany $ paths) | awaitForever D.sourceFile ` But that really just didn't sit right with me at all. `sequence_` is certainly the superior solution, thank you! 
&gt; As for the &lt;|&gt; and +++ I'm honestly not sure. They both seem to function as or I think? So why are you sometimes using one and sometimes the other?
[`Semigroup`](https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Semigroup.html) is in *base* now, `Data.Semigroup` since GHC 8
Cool, thank you! I learned a lot from the typeclassopedia when starting Haskell. Bifunctor, Bifoldable and Bitraversable could be added. I would also like to see Profunctor and the relation to Arrow in the typeclassopedia.
[I've explained Arrows purely in terms of profunctors before](http://elvishjerricco.github.io/2017/03/10/profunctors-arrows-and-static-analysis.html). The relationship is really strong *if you choose to interpret arrows as such.* But there are several ways to interpret arrows. Only the profunctor explanations come out very nice IMO though. Everything else makes arrows seem fairly disappointing.
Could you help me understand the Functor instance? Specifically, what types do `k` and `f` have in `fmap`?
The reality is that people slap on `-threaded -N` to their build systems and rarely think about it, because it seems like a marginal cost to say "Spawn threads if you need", all things considered. But the parallel GC is often a hidden cost that people don't expect or account for. In this particular case we could have removed `-N` as well, although that isn't always possible. The example is a real one, but only meant to characterize the general idea of what's going wrong.
Processors *pretend* to be operational, but that abstraction leaks. Things like caches, branch prediction and out-of-order execution are under your level of abstraction but, like any real abstraction, can have a *massive* effect on real-world performance.
Oh, I definitely wasn't saying that the bug was in the UX code. Much to the contrary, this sounds like a tuning problem through and through.
I just added `MonadFail`, but it could surely use some more info (e.g., links to documentation, Haskell Prime discussion, etc.).
[Stream Fusion](http://code.haskell.org/~dons/papers/icfp088-coutts.pdf) is my favorite.
`f :: a -&gt; b` `k :: b -&gt; r` `runCont c :: (a -&gt; r) -&gt; r` So `k . f :: a -&gt; r`
You can also choose to interpret arrow notation as a notation for function-application-like DSLs. This happens to be a profunctor because of `arr`. But the interpretation itself doesn't necessitate anything about profunctors, so they aren't core to that definition. In fact, `arr` is seen as a mistake for this interpretation.
Not a terribly common one, but whenever I see complaints about "rigid, skolem" variables "escaping scope" I still cringe a little. cf. https://stackoverflow.com/questions/12719435/what-are-skolems
[This solution for number 7 of the 99 haskell problems](https://pastebin.com/DTeRH1QS) produces [this](https://pastebin.com/MLQ8fpNh) error. Basically, ambiguous type errors are far too verbose. Just show me an ascii arrow and a code snippit. The only thing that really needs to be said is "ambiguous type; add annotation". Please don't print the bajillion instances, ghc, I don't care about them; I already know which one I want to use and probably didn't know half of the others existed. Parsing errors in general are also either far too verbose or just absolutely useless.
Oh! Is "Scrap Your ___" a common motif for papers? I've read "Scrap Your Boilerplate" but no others. Thanks for the suggestions!
Hmm the only other ones I've seen besides SYB is [Scrap Your Type Classes](http://www.haskellforall.com/2012/05/scrap-your-type-classes.html) and [Scrap Your Constructors](http://programmable.computer/posts/church_encoding.html). 
Ooh neat! Those sound pretty interesting. Thanks for the links!
Thank you! 
Are you all collaborating with any of the various other Haskell code editor/IDE projects? Are you aware of Visual Studio code which only requires you to create a server and handles all the GUI for you?
Thank you!
That makes me think of the classic *Counterexamples in Topology* by Steen and Seebach.
thank you!
Thank you for the great work! Surely this deserves a mention on the top of the page, no? Something like 'up-to-date June 2017/GHC 8.0.2' ? 
&gt; The relationship is really strong Heh
Sometimes I find myself refactoring a function used in multiple contexts, by adding a continuation parameter. I don't have a clear instinct for when this is a good choice to make.
When I used to use Scheme, I found multirember&amp;co example in ch. 8 of Little Schemer instructive for building up an intuition for a simple use of CPS style. Seasoned Schemer has some more examples of using continuations directly.
Thanks!
Oh, good idea, thanks!
I did not know you could hide typeclasses. Presumably any instances are also hidden? Thanks!
Let me try to summarize: C# offers `do`-notation-like syntax for types with `fmap`- and `bind`-like functions, but it is not possible to write functions polymorphic in the monad type constructor, because C# has no bounded polymorphism for higher kinded types.
The SQL-like syntax of LINQ screams "this is just for collection-like things". To use the LINQ syntax to work with other monads is possible, and if you're used to "seeing past" the syntax it won't be an issue *for you*. But your coworkers will think you're crazy. 
&gt; […] but I have never ever encountered the real need to implement a generic method working for any monad. Not sure how useful they are in C#, but in Haskell I often use the functions from [`Control.Monad`](http://hackage.haskell.org/package/base-4.9.1.0/docs/Control-Monad.html#v:mapM), especially `mapM`, `(&gt;=&gt;)`, and `when`. 
I don't see anything broken. It's, at most, not powerful enough. But "broken" is an overstatement.
Thanks for the feedback! You're right that a lot of the content was on Reddit throughout the week. I think this issue has 3 links that I didn't see on Reddit. Featuring 30% unseen content is pretty good in my eyes :) I've thought about featuring old content before (see [issue 51](https://github.com/haskellweekly/haskellweekly.github.io/issues/51)) but ultimately decided against it. I want to keep Haskell Weekly about news. At the end of each calendar year I'll do a roundup of the top links of the year, but otherwise I'll stay focused on the current week or so. 
The instances for `Prelude.Monad` will be imported, but they won't conflict with your instances for `Main.Monad`.
And consider `RebindableSyntax` for `do` notation.
&gt;Either cannot exist in C#, because it lacks the union types. You can have sum types like this: public abstract class Either&lt;L, R&gt; { private Either() { } public sealed class Left : Either&lt;L, R&gt; { public L Value; public Left(L value) { Value = value; } } public sealed class Right : Either&lt;L, R&gt; { public R Value; public Right(R value) { Value = value; } } }
The operator overloads have to be static methods, which cannot be added to interfaces or base classes, and as such, cannot be used through a generic constraint.
&gt; for all generic classes F&lt;???&gt; implementing Select and SelectMany, given an instance of F containing As, another instance of F containing Bs, and a Func&lt;A,B,C, return an F containing Cs Does this satisfy: using System; interface L1&lt;A&gt; { L1&lt;B&gt; Select&lt;B&gt;(Func&lt;A,B&gt; f); L1&lt;B&gt; SelectMany&lt;B&gt;(Func&lt;A,L1&lt;B&gt;&gt; f); } static class Foo { static L1&lt;C&gt; CombineWith&lt;A,B,C&gt;(L1&lt;A&gt; ops1, L1&lt;B&gt; ops2, Func &lt;A,B,C&gt; f) { return ops1.SelectMany(op1 =&gt; ops2.Select(op2 =&gt; f(op1,op2))); } } Only one thing is missing from C# - you cannot make class implement an interface with extension methods, if it was not declared as implementing it at creation. So instead for an existing class there should be a wrapper which implements the interface. In Haskell it is possible to declare any type implementing any class, but note that if those typeclass and type are not yours then this would be orphaned instance. Which is considered as bad thing, and recommendation is to make a `newtype` wrapper which implements the typeclass. So basically it comes to same thing as in C#
Ah, didn't think of the operator overloads. Thanks.
&gt; Being a C# programmer who's also a fan of Haskell, I do understand this issue, but I have never ever encountered the real need to implement a generic method working for any monad. The lack of higher-kinded types is mostly felt when attempting a finally tagless abstraction. These would typically be much more lightweight than a full LINQ implementation.
You cannot do this: T Add&lt;T&gt;(T a, T b) =&gt; a + b; It doesn't know which version of + to use, and if the data type has the + operator at all. Basically, no Num type class.
What the author is describing is something more [like this](http://higherlogics.blogspot.ca/2009/10/abstracting-over-type-constructors.html). Higher kinded types require casting on the CLR, unfortunately.
I wouldn't mind a EDSL, and I'd probably use it when I could. But, I would like to have `(ParameterizedStatement, [Value])` as a fallback.
Higher kinded types. I never knew I needed them until I wrote Haskell and then went back to Java/C++/C#.
Line up your "where" clauses along the same column. Haskell is sensitive to whitespaces.
Thanks! I didn't know you could do the newline after where, which is extremely helpful. 
I half-indent the `where`, require a newline if the where body is more than one line, and full-indent the body of the `where`. Example: function x y = z w where z = y (w x) w = y (z x)
"Xyz is broken" has become a tedious euphemism for "Xyz doesn't do what I want it to, consequently I'm throwing my toys out of the pram and will forthwith loudly denounce it to everyone as unfit for purpose". "Optional is broken", "Britain is broken" etc.
Pandoc is amazing. I used it long before I knew what an exciting language it is written in.
I'm excited about [Project: M36](https://github.com/agentm/project-m36) which seeks to take RDBMS back to it's pure relational algebra roots. It avoids all sorts of complexity by [not having NULL](https://github.com/agentm/project-m36/blob/master/docs/on_null.markdown) (but adding ADTs like `Maybe`) and not having column defaults. It has promising features like [long running transactions](https://github.com/agentm/project-m36/blob/master/docs/transaction_graph_operators.markdown) (think version control for transactions), time travel, relation-valued attributes (tables inside of rows), custom data types and database functions written as native Haskell code, and lots more.
I see. Yes, such type computation seems not available in C# I actually managed to abuse generics to produce something which looks like what you described. But the result is pretty ugly: using System; interface S&lt;A&gt; { } interface Mapper&lt;AS, BS, A, B&gt; where AS : S&lt;A&gt; where BS : S&lt;B&gt; { BS Map(AS input, Func&lt;A, B&gt; f); } interface Joiner&lt;AS, BS, A&gt; where AS : S&lt;A&gt; { BS Join(AS input, Func&lt;A, BS&gt; f); } static class Foo { static CS CombineWithG&lt;AS, BS, CS, A, B, C, M, J&gt; (M mapper, J joiner, AS input1, BS input2, Func&lt;A, B, C&gt; f) where AS : S&lt;A&gt; where BS : S&lt;B&gt; where CS : S&lt;C&gt; where M : Mapper&lt;BS, CS, B, C&gt; where J : Joiner&lt;AS, CS, A&gt; { return joiner.Join(input1, op1 =&gt; mapper.Map(input2, op2 =&gt; f(op1, op2))); } }
... And in an ecosystem like C\#'s, you can get pretty far with what LINQ provides. Of course, writing libraries is a hassle, since you have to rewrite `sequence_`, etc. for every new type, but that's a one time rewrite for every *library*, not each *client*. Not saying it's great, but it's bearable as long as you're the consumer and not C\#'s Ed Kmett (Jon Skeet).
Doesn't C++ have higher kinded types via template template parameters? template &lt;template&lt;class&gt; class F, class A&gt; void f(F&lt;A&gt; x) { } But they are rarely used as people often instead go with template &lt;class FA&gt; void f(FA x) { } and require that `FA` provides associate typedefs `FA::F` and `FA::A`. I guess this has to do with the lack of partially applied type level functions: the second approach also allows `FA`'s which are really `F&lt;B,A&gt;`.
servant
C++ is a pretty useful language and is becoming more open to ideas similar to Haskell's. Python, Java and JS are also useful for getting things done and getting jobs. Clojure and Rust are getting there but aren't really mainstream yet. What do you want to build? 
Wow, Ducking looks amazing! 
Fortunately I'm quite satisfied in my current job, so the job market isn't a concern for my language choice. Thanks!
C, rust, python, Javascript, java. I would pick c (or rust, if it has really easy c interop) or python, since you can use purescript or elm or ghcjs to do javascript tasks. Everything in the world seems to either have a python or c API out of the box Clojure is also a nice pick as it gives you java access and exercises meta programming with macros. Elixir to get you thinking about actors EDIT: If there is some recent practical descendant of Mercury, Mozart, or Prolog (possibly the Kanren based systems), that might be an interesting new direction as well?
I'm a big Servant fan as well. I love all the possibilities for interpreting API descriptions, projects like servant-quickcheck, for instance, I find really cool. I also like the idea of auto-generating clients. 
I would pick C. Is a (relatively) small language and in many ways the opposite of Haskell. It's still the language of choice when you need good control of your hardware.
In that case JavaScript and python sound perfect for your needs. JavaScript particularly as it is extremely web oriented. I'd recommend looking for node.js tutorials. Python is a bit more developed to do complex querying and processing as a bunch of libraries already exist for it. Good luck. Sounds interesting. 
This question and the answer pleasantly surprised me considering how I ended up in a functional programming language. Formally, i.e. as part of a course, I learned only Fortran 77 (man what a mess) learned C in my first year Undergraduate vacations, wrote almost all the code in C during my UG and then tried learning C++ and gave up in disgust. It was during PhD that I picked up FP starting with ML and then Haskell. I always used to think that the ideal experience should have been the reverse of what I suffered through and used to think that that is an Utopian dream. Looks like Haskell has gained so much brand value that this is now becoming a reality.
If you want something practical, then Python is the right choice for you. It has some of the best library support of any language out there, and has beautiful clean syntax that makes it by far the easiest language to code in. If you want to get things done, Python is a good choice. The other language I'd give serious thought to is C because if you don't understand C you don't really understand how your computer works. But if you want something practical, C is not the right choice. Honorary mention goes to Rust because it is a beautiful language that combines many of the best elements from many other languages in an effective an interesting way, but Rust only *really* shines in situations where you need high level performance and safety. Since for the large majority of tasks nowadays performance just isn't a big deal, it's much less practically useful than Python.
&gt; Either cannot exist in C#, because it lacks the union types. Except it does have true union types, https://msdn.microsoft.com/en-us/library/system.runtime.interopservices.layoutkind(v=vs.110).aspx &gt; The lack of Maybe used to be somewhat annoying. They recently introduced the ?. syntax into C# You never needed this for an optional value. The same could have been simulated using a simple extension methods taking a selector function. I've used this for years before `?.` was available with little issue.
I'm looking forward to faster compilations but nothing will beat the repl as a playground. There are a few usability things I'd like to see improved such as retaining state with code reloading (still need to give https://hackage.haskell.org/package/rapid a try)
F# actually started out as a dialect of OCaml for .NET, as I recall. I believe it has diverged a little, but it's still basically OCaml.
what's going on with ghcjs? Seems like Luite's work on it has slowed down. EDIT: still in development, commits are in the ghc-8.0 branch
How about a "paper of the week"? (or a talk or a blog post or even a language extension?) This could be decided by a poll of some sort from the previous week. There are, after all, plenty of papers that an intermediate Haskeller can read and profit from: the pattern synonyms paper or the roles paper come to mind.
Some Haskellers go on to learn Coq. This is also practical, in a rather different sense. I've been meaning to give *Software Foundations* a go.
Happy to oblige! Ahrefs works with data at essentially Google scale, three years behind. In fact, their crawler is [more active](https://www.incapsula.com/blog/most-active-good-bots.html) than any search engine except Google (another loss for Bing)! I have a lot of admiration for their work.
Oh that's interesting! I heard of them when I looked through ICFP sponsors for this year, but I didn't really know what they did and they didn't have applications for interns that I could find, so I forgot about them haha. Thanks for the info! :)
Mm I don't doubt it! I just don't know or any offhand, and JS is the go-to example as far as I've seen.
Well, I certainly hope I haven't started with Haskell only to descend! :)
Typescript is simply a layer on top of vanilla javascript, it does only add type checking to your code. To learn typescript, in the end, you have to learn javascript, nothing more. If you want to use in clients simply compile it and use `browserify` to concatenate the whole thing in a single js file. So you can drop babel and webpack from your learning list at first. To start you can simply use the typescript compiler and npm for external dependencies.
I recommend R. 
At any point of time one always can go higher. In your case you can go for something like Idris or some of the proof assistants like Coq. But I believe it is great to start with a sane language like ML or Haskell and then learn C. I still like C for its simplicity and down to earth design. As a system PL or as a portable assembly language, C is still great. I will be surprised if you do not know C but if you do not maybe it is a sane option to learn particularly if you want to write some FFI code. But it is also important to steer clear of those bad "enterprisey" languages. The two bad boys here are C++ for its insane complexity and Java for its pointless verbosity. Yes it is another matter if ones "food on the plate" depends on it but otherwise learning any of this is completely point less IMO. Some people like Java for the JVM. If one is forced on to JVM find other alternatives like Clojure or scala if you want to remain sane.
Couldn't agree more. There's nothing you would learn from Go, whereas picking up Python (or just building something substantial in Haskell) would bring many paradigms into your toolbox
I would like to add to what /u/FunctorYogi said Just look at the parallel between Go's goal of being "easy to learn" and Java's "simpler C++" philosophy. This philosophy might give Go its success (like Java) but it will end up like the same ship wreck that Java is. I would say Go is the Java of 2010's which in turn in many ways is the COBOL of 1990's. Remember COBOL was designed to be so "simple" that even your pointy head manager can write code. 
Shameless self-plug: I'm building [`simplexhc`](https://github.com/bollu/simplexhc), which is a custom backend that takes `STG` and lowers it directly to `LLVM`, without going through `C--`. The reason I'd like to do this is twofold: 1. Going from `STG` -&gt; `C--` -&gt; `LLVM` loses a *lot* of information that you could have otherwise supplied to the LLVM optimiser. So, `LLVM` does a much poorer job of optimising `STG` than what it could have done. 2. There was a very interesting talk at EuroLLVM 2017 about `JIT`ing JVM bytecode using `LLVM`. They describe a process of "gradual lowering", where they first convert `JVM` instructions into LLVM function calls which do the same thing. Then, they implement JVM specific optimisations *as LLVM passes*. So, for example, something that de-virtualises a method call is now an LLVM pass. That way, LLVM's own passes line inlining and dead code elimination can work hand-in-hand with your higher-level domain specific passes. 3. This is a long-shot, but I've been considering trying to extend [ideas from polyhedral compilation](http://polyhedral.info/) to work for things like mutually recursive function calls, since if you look at them as a graph, they're roughly the same structure. This is pure speculation, and much more "out there" so I don't consider this actual motivation. [Link to the EuroLLVM talk for all those interested](https://www.youtube.com/watch?v=sKIRIilZDnE) 
SECTION | CONTENT :--|:-- Title | 2017 EuroLLVM Developers’ Meeting: A. Pilipenko “Expressing high level optimizations within LLVM” Description | http://www.LLVM.org/devmtg/2017-03/ — Expressing high level optimizations within LLVM - Artur Pilipenko, Azul Systems Slides: http://llvm.org/devmtg/2017-03//assets/slides/expressing_high_level_optimizations_within_llvm.pdf — At Azul we are building a production quality, state of the art LLVM based JIT compiler for Java. Originally targeted for C and C++, the LLVM IR is a rather low-level representation, which makes it challenging to represent and utilize high level Java semantics in the optimi... Length | 0:30:12 **** ^(I am a bot, this is an auto-generated reply | )^[Info](https://www.reddit.com/u/video_descriptionbot) ^| ^[Feedback](https://www.reddit.com/message/compose/?to=video_descriptionbot&amp;subject=Feedback) ^| ^(Reply STOP to opt out permanently)
I too would second Coq, although I know nothing about it. Apparently, it is all about depending typing. This lets you write your program using types alone. If you can imagine Haskell where you start by writing the type of your function, then you let the computer fill in the function's code automatically based entirely on the type. While this does force you to write a lot more types just to tease out the specific behavior of your function, it results a much more rigorously type-checked program. No "bottom", no undefined code, no unmatched patterns. The only way you can incorporate a bug into such a program would be to completely misunderstand the algorithm you are implementing. Coq, Agda, and Idris have all been on my list of languages to learn.
So if you know enough of Haskell in 9 months then you can learn ALL the other languages in two months, as well as many other things. So why pick one single language? In my case only investigating the servant-* saga or reviewing the posts about the Maybe monad could let me busy for many reincarnations. I only can recommend something for your spare time: these lectures on [General Relativity](https://www.youtube.com/watch?v=JRZgW1YjCKk)
One problem with OCaml is that the syntax has quite a lot of unpleasant gotchas accumulated over time. Facebook recently wrote a front-end to the compiler called Reason (search ReasonML) so you can use a syntax which is a good bit saner, but gives you access to all the power of the language. You can also compile down to Javascript via Bloomberg's Bucklescript compiler, so it gives you access to that ecosystem without having to actually write JS. It is a new language so the tooling isn't quite there yet, but as a novel take on modern programming, it's a promising start.
If you need do do any work in the Java/JVM ecosystem I like Kotlin. Really great interoperability with Java libraries. Elixir is another great language for building servers. The Phoenix web framework and Ecto database library are very easy to get up and running. The package manager and build tool is also very easy to use. Next on my list to learn is Elm, bringing type safety to client side web development. 
For Hoare, you can use the ancient but dependable B method. I've written a blog post ages ago that should give you a small taste of it: http://unsafePerform.IO/blog/2010-02-16-the_b_method_for_programmers_(part_1)/ and http://unsafePerform.IO/blog/2010-02-22-the_b_method_for_programmers_(part_2)/
Iirc the issue is that ghc allocates terrabytes of ram on startup without committing. Linux doesn't do any extra work because of this. The linux subsystem on windows has linear startup time on allocated memory. So the repl only has to start once but that still takes a silly amount of time for now. 
Languages like ML and Haskell encourages to think more about the problem than the implementation. That is why I think they should be the starting point. But if one is interested in pursuing some of the Core CS subjects like Compilers, Operating Systems and Architecture a fairly good understanding of C and if possible an assembly language is good. That is what I meant by reverse, of course there is no point in learning Fortran 77 after Haskell.
`configure` complains about a broken c compiler. Can you try compiling a Hello World (in C of course) and see if it works ?
For minimal but positive amounts of boilerplate, you can take the same approach as `Traversable` and eg `Foldable`: see [foldMapDefault](https://www.stackage.org/haddock/lts-8.17/base-4.9.1.0/Data-Traversable.html#v:foldMapDefault).
Think Python is a great book. They aim to introduce general computer science concepts, and use Python as an example language. [Think Python is available online for free.](http://greenteapress.com/thinkpython/thinkpython.pdf)
C++ is way too much headache to be worth it. Learn it only if you need it for a job (and that one better be well paying to make it worth it).
Try Python's Hypothesis. It's a QuickCheck port, and will give a surprisingly long way towards making your Python coding feel (and be) better.
I can see you have Clojure in your list, I'd recommend you to take a look at its big brother Common Lisp to get to know how simple powerful metaprogramming looks like.
We use it at work (as many others do), it's super convenient ^_^
&gt; Do either of you have advice for a thorough, from the foundations on upward, book or resource that would be good to start with? First, I also vote for Python. About learning resources, check [the wiki](https://www.reddit.com/r/learnpython/wiki/index) of /r/learnpython. One of the most recommended books for (complete) beginners is [Automate The Boring Stuff with Python](https://automatetheboringstuff.com/), but this might be too basic for you. Please DON'T start with Learn Python the Hard Way (I won't even link it; you can find reasons for this if you search Python subreddits). And to spare you from countless discussions about Python 2 or Python 3 - do yourself a favour and learn Python 3.
**Here's a sneak peek of [/r/learnpython](https://np.reddit.com/r/learnpython) using the [top posts](https://np.reddit.com/r/learnpython/top/?sort=top&amp;t=year) of the year!** \#1: [Python 201 Book is Free for 48 hours](https://np.reddit.com/r/learnpython/comments/5814lw/python_201_book_is_free_for_48_hours/) \#2: [Python 101 Book FREE for 48 hours!](https://np.reddit.com/r/learnpython/comments/5bmaz0/python_101_book_free_for_48_hours/) \#3: [90% Python in 90 minutes](https://np.reddit.com/r/learnpython/comments/661o5a/90_python_in_90_minutes/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/5lveo6/blacklist/)
What's WSL?
That is one known technique to *reduce* the boilerplate, not remove it completely. With that, you still need to write 2 lines for each type and type-class handing it this default function as an implementation.
The Windows Subsystem for Linux. Currently programs compiled with GHC8+ are un-runnable on it due to a memory management bug/hack.
Yes, I can compile the Hello World source code.
Optimistic concurrency control (such as STM) assumes, optimistically, that there will be no conflict, and pays a performance penalty if that assumption is wrong. Pessimistic concurrency control (such as MVar) assumes, pessimistically, that there will be a conflict, and pays a performance penalty if that assumption is wrong. If you happen to know whether your particular application will have a lot of conflicts, and you care about throughput, then pick the right tool for the job. If you want to make your program correct first, then fast second, then STM is a much better choice because its API is more expressive, more composable, and less prone to deadlocks. Also note that the amount of conflicts depends on the granularity at which your resources are tracked. For example, if you store all of your state in a giant Map, and you make updates to your state using `Map.insert` to overwrite the value for a previous key, then every single write will conflict with every other write. But if the leaves of the Map are mutable, you can make updates to those individual references without having to modify the Map, so updates to different leaves won’t conflict anymore. Doing this with MVars can easily lead to a deadlock if you need to lock more than one leaf at a time, whereas with TVars, orderings which would have led to a deadlock will cause a retry instead.
Me too, but ironically you end up writing your "business logic" in stored procedures which are of course, not type checked. Anyone want to invent a way to write Postgres stored procedures in Haskell :) ? 
Whenever someone learns Java, I'd recommend her to learn C\# instead. Learn C\# from 2017 and you know every Java version to be released until 2030, assuming they keep their current pace. Or instead of Java, learn Kotlin. That's Java minus the boilerplate. Still has amazing Java interop.
Somehow never learned about constraint kinds. That is way nicer, thanks!
I agree with you. I didn't intend to imply that OP would be writing in an FP style in Python, just that Haskell would probably guide him/her to write solid code. The original post seemed to be seeking something different and that's why I think Python is a solid choice.
I am interested in Cabal, as I am curious to see what decisions were made in how the solver was implemented. I also want to read Smooth, the simple Haskell smt solver. I am curious to look under the hood of Unison and Idris. I will do a sweep later for any existing libraries that implement n^3 algorithms like network flow or approximation algorithms, as well as if there are any implementations of recent research in random sources. 
I'm not going to learn a new language for a while. But if I did, I'd choose Racket (or even [Hackett](https://github.com/lexi-lambda/hackett)).
AFAIK, it still doesn't support functors (higher-order modules, in OCaml parlance), which are probably OCaml's nicest feature.
Right, I know Facebook uses Haskell, since they have SPJ. And it's only a smallish team focused on spam avoidance or something, right? I didn't know Bloomberg and Docker were using FP. Is it the majority of their respective codebases, or only specific teams, or what? I'm just curious for more info haha.
Nice, really happy about the `IsString` change!
On an unrelated note, I appreciate your use of diaeresis in "reëxports".
Idris? Great introductory book has been released recently, and the language is Pac-man complete.
This is why I imagined there were no such thing as pessimistic STM. But your point about `Arrow` is precisely what I suspected. There are abstractions which give you *something* like this, though not fully monadic.
Take a look at: https://hackage.haskell.org/package/blocking-transactions
TBH, I see no reason (haha) not to use plain OCaml. I find the syntax fine, although not as terse as Haskell's.
Videos linked by /u/bss03: Title|Channel|Published|Duration|Likes|Total Views :----------:|:----------:|:----------:|:----------:|:----------:|:----------: [Erlang The Movie II: The Sequel](https://youtube.com/watch?v=rRbY3TMUcgQ)|gar1t|2013-03-22|0:09:55|1,062+ (97%)|70,309 [Erlang: The Movie](https://youtube.com/watch?v=xrIjfIjssLE))|uncertainyesterday|2012-05-03|0:11:32|760+ (98%)|78,224 --- [^Info](https://np.reddit.com/r/youtubot/wiki/index) ^| [^/u/bss03 ^can ^delete](https://np.reddit.com/message/compose/?to=_youtubot_&amp;subject=delete\%20comment&amp;message=diojvqv\%0A\%0AReason\%3A\%20\%2A\%2Aplease+help+us+improve\%2A\%2A) ^| ^v1.1.1b
&gt; I absolutely love it, that name is fucking gangsta
Yes, the ecosystem can be overwhelming. It's one of the most frequent criticisms I've seen. There is loads of tooling around compiling, packaging, optimisation, etc... Even more confusingly, some of it applies to server-side JS (Node), some of it to client-side, and some to both. The nice thing is that you don't need any of that stuff to start out. You can start with an empty HTML document and begin playing with the language. Or even better, visit https://jsfiddle.net and get playing right in your browser. TypedScript is just "a typed superset of JavaScript that compiles to plain JavaScript", and lets you start off with plain JS, and migrate it to fully-typed TypeScript as necessary, or as you come up to speed with the features. For that you'll probably want to download an editor like (free) VSCode, which is probably the best IDE for TypeScript development. However, JSFiddle support TypeScript, too. I haven't really used any resources beyond the official docs, and some blog posts, because I'm coming from a background of knowing JavaScript. But one of the nice things about the JS and TypeScript ecosystems is there is a massive amount of information out there. Mozilla has a nice-looking, up-to-date tutorial at https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide
Whaaaat?! Does this really work? Like, you see Core or C-- or ..?
That was the inspiration :)
This seems very similar to [A Simple Proof Technique for Certain Parametricty Results](https://www.cs.cmu.edu/~crary/papers/1998/param/) (Crary, ICFP 1998). The idea there is that given a type like `f : forall a. a -&gt; a` since `a` can be instantiated with anything, you can in particular instantiate it with a singleton type `S(v1)` of just a single value. And (for this particular example) you can immediately see that the function must be (extensionally equal to) the identity since `f [S(v1)] v1 : S(v1)` and the only value of type `S(v1)` is precisely `v1`. The paper shows how to use singletons and union types to prove more complex results.
Probably jonesforth, there's a mirror here: https://github.com/AlexandreAbreu/jonesforth See jonesforth.{S,f}
(author here) Thanks, I didn't know of the reference, and this looks interesting. I would not say that the two techniques are "very similar" though, in fact they seem quite different to me -- and this is interesting. Crary's technique is very extensional, you reason on the type of an expression, not its code -- and the trick is to extend the type system to be precise enough to tell things about specific possible resulting values. By contrast, my technique (which I believe is folklore) is intentional, relies on analyzing all possible (normal) terms at the type (without extending the type system). Crary claims that it is easy to extend his technique to languages with computational effects, I think that this is not so much the case here, as you need an understanding of the canonical forms in presence of effects. (Of course with Crary's technique what you get for free is guarantees on the "output value", but not on what effects may happen.) Another difference is that there is a rather direct correspondence between Crary's technique and the usual free theorem bring-your-own-relation arguments: whenever people will pick singleton relations, he chooses a singleton type, if their relation relates two elements he uses a union of singletons, etc. In a sense it can be described as using type theory instead of set theory for the same arguments (the underlying way the arguments work are different, but the things the human prover has to come up with are similar) -- this clearly explains why the choice of singleton types and union types. On the other hand, with a proof by canonical proof search, it is *possible* to look at the relation between the canonical proof derivation and the relations picked in a usual parametricity proof, but the relation is less clear to me; on a case-by-case basis, it can be done, but I don't have strong intuition in general. One thing that is interesting about Crary's work is that he uses types (singleton and union types) that are known to be tricky to get right in a type system and metatheory, but are much simpler in his setting because he does not use the elimination forms. (The elimination form for union types typically breaks subject reduction if you are not careful.). This makes those extra type constructors much less scary than it sounds at first. The reason why you don't need the elimination forms, I think, is that the program couldn't rely on them anyway, as only type variables get instantiated with those constructors. I don't understand yet if this is just a mildly interesting observation or a deep idea. Instead of elimination forms, a canonicity argument (in a closed context) is used on positive occurrences of the type in the result -- this is a relation with my style of "syntactic parametricity", of course. Historically, in general LF people / CMU are well aware of canonicity arguments, as they are key to justify adequacy of encoding of theories in the Logical Framework.
I'd like to nominate [async](http://hackage.haskell.org/package/async) :)
I find this article interesting, but I am very much a beginner when it comes to parametricty. Is there a good tutorial to get me up to speed? 
Reminds me of the tools used in a talk about [how zero-cost are C++ abstractions](https://www.youtube.com/watch?v=zBkNBP00wJE). It could be wonderfully awesome to see the effect of different approaches on assembly size (and possibly speed). For example, the example code I was given had a 24-instruction inner loop, and a 18-instruction overhead. Using a strict accumulator, I got that down to a 13-instruction innner loop, and a 19-instruction overhead. :)
BTW, be sure to change your compiler options to -O2.
Whoever made this work: Thank you! 
Feel free to enter in the room of transient-axiom: https://gitter.im/Transient-Transient-Universe-HPlay/Lobby
Heya! First of all, thank you for your work with extra documentation for Opaleye. I built a data processing tool with Servant and Opaleye earlier this year and relied on some of the information you guys put together. I should have mentioned this in my post above -- I still plan to use Haskell as my primary language. I've invested at least an hour a day into the language every day for 9 months and I'm seeing the returns in the practical work I can do. I don't know if I could call the projects I'm working on "substantial" but they do stress my ability. &gt; You'll learn a lot about how to use a type-system and FP to solve problems effectively. Where typed-FP helps, and where it hinders. This is actually one motivation for exploring other languages. So far, I _only_ know how to use typed FP; without context, I'm not sure where it helps or hinders. It's all I know at the moment. So I am curious to learn from other paradigms -- close to the metal with Rust or C, another abstract approach with a Lisp, object oriented with Python...and see what I can take from that and bring over to my work in Haskell. Thanks for the feedback!
I certainly haven't saturated my Haskell learning, and I'll continue to use it as my primary language. I don't do much with monad transformers; I don't understand much of what goes on beneath the surface in IO; I know only vaguely of of GADTs, arrows, lenses, profunctors...there's still a lot to learn! However, just as people find value in coming Haskell to learn another paradigm (even if they won't be using Haskell at work), I'd be curious what I can learn from other languages that would benefit me in Haskell. Right now, my only context is Haskell and pure functional programming.
&gt; Readable, straightforward, and maintainable Python uses good _naming_ first and foremost, and copious documentation (and as of recently, type annotations). Given that a major segment of my code uses names like `n` or `m'`, perhaps I'll find the idioms of Python useful to at least consider!
If my first priority was learning something new and interesting, and my second priority was practicality, would that change your honorary mention? I first want to learn something interesting and different from what I have experienced so far in Haskell. The practicality part isn't that I want the _most_ practical language, but that once I've learned it, I at least _can_ build practical things with it.
What do you think of learning C vs. Rust? My understanding is that Rust is also quite close to the hardware and might be a more pleasant experience.
I've put this on my list of things to at least research and try. Javascript is everywhere, too, so it's to my benefit to understand it at least superficially. Thank you for taking the time to share this info!
Awesome! Thanks for sharing this :3
Does it run?
They do exist. Matveev and Shavit's paper ["Towards a fully pessimistic STM model"](http://transact2012.cse.lehigh.edu/papers/matveev.pdf) gives an algorithm for one. As you might expect, there's tradeoffs involved, but it does have some nice advantages over optimistic STM.
It’s a shame, really. I didn’t understand arrows very well until recently, but they’re pretty expressive. They’re basically a verbose concatenative language where `&amp;&amp;&amp;` corresponds to `dup` or `bi`/`both` combinators, `&gt;&gt;&gt;` is sequential composition (horizontal concatenation of dataflow), `***` is parallel composition (vertical concatenation), and arrow notation is pointful sugar for them.
Consider [D](http://dlang.org) or [Nim](http://nim-lang.org). The metaprogramming story there is far ahead of anything in Haskell or Rust. In the same sense that Rust bets on type level resource management for systems programming, D (and Nim to a lesser degree) go all in on compile time reflection which I like better. You can get a lot of the benefits of HKTs without trait contortions. They don't look much like functional programming langs. because the syntax is designed to appeal to C++ and Python devs respectively but unlike Rust which *looks* like an ML, they support TCO which IMO makes FP easier along that axis; you can even emulate [CT](http://www.infognition.com/blog/2014/recursive_algebraic_types_in_d.html) but it's pretty ugly. OTOH Rust is immutable by default while D and Nim require annotations so ¯\_(ツ)_/¯.
I'd say that Rust is the better language, but learn C anyway. C is useful because it's low level, simple, and available on every platform. C is much more different experience from Haskell than Rust. 
I like how pluggable it is. Authentication, in-code documentation (swagger), HTML responses with lucid, generated client bindings for all kinds of languages/libs (purescript, elm, js, ..., haskell, [ghcjs-react-flux](https://bitbucket.org/wuzzeb/react-flux-servant)). The cool thing is that it leverages Haskell's strong types, such that less-strongly-typed languages then Haskell can not have a tool like this. 
Amazing! This is why I never regret asking what I think is a dumb question on /r/haskell!
A bit late to the party, but may as well toss something in: I think, in your specific situation, F# might be the best choice. It's in the ML syntax family like Haskell, but it runs on .Net and so by necessity is strict, allows impurity, and supports an object model. Strictness is mandatory, but impurity and objects aren't, which means you can learn those concepts at your own pace -- and they're essential skills if you ever want to interact with other peoples' code. You'll also be learning the .Net ecosystem, which is one of the big ones; F# can interact directly with C# and VB. A similar argument could be made for PureScript or maybe Elm, with respect to JS architecture and ecosystem instead of .Net. If you see web development in your future, that might be the better option.
I'll try it out tomorrow unless someone beats me to it. 
I fully agree with /u/3w4v that &gt; we should put at least some trust in the ability of a student to eventually, with experience, form their own informed perspectives. Good luck!
You might be in the interested in the "refinement calculus", where you specify in one domain and implement in another, with refinement step proofs to show the connection between the two. This gives you a way to prove things that will be true about the implementation, without needing to do the full proof in terms of that implementation. And the refinement proofs can be broken down to several management steps.
&gt; since they have SPJ SPJ is at Microsoft. They have another primary GHC dev, Simon Marlow at Facebook working on [spam protection](https://code.facebook.com/posts/745068642270222/fighting-spam-with-haskell/)
Oh shoot, I confused my Simons! My bad haha.
Hi Ed, the monadic API is exactly as expressive as the arrow one isn't it? Two monadic layers being isomorphic to an Arrow sounds like the kind of thing you'd have spent some time thinking about. The first question that comes to mind is, "Can I losslessly decompose every Arrow to two monadic layers?". Have you spent any time pondering over this issue? Or is it obvious to you that the venue is a dead end?
I use XMonad as my window manager and I like it a lot. I also use darcs for version control on most of my projects. I want to learn more Idris in the short term - it looks potentially quite useful for e.g. compilers. 
Could someone tell me the benefit of using postgrest over plain postgres communication? The idea of REST API is to add an abstraction layer over DB (i.e., hide the details) so one can change the internals without changing the API, while postgrest technically reveals all the details of the DB. Or am I not understanding it correctly?
The Learning Python OReilly book is one of the few programming books I read cover-to-cover, it really is that good. I'm hesitant to recommend Python nowadays for anything larger than prototypes, because I've seen so many projects written in it which fell apart because of the difficulty of reasoning about the code. But if you want to try it, that book is a great introduction.
Oops, the High Sierra beta is "coming soon". I can't test it yet. 
Try using MSYS instead of Cygwin.
LSP does sound good to me. It is not here to replace any of the Elisp stuff. There is a high probablity that the LSP for language xyz is implemented in xyz. Hardware, I don't know about that (I assume you use AMD64 then it will work). OS, I don't know. Software wise, yes language servers might have additional dependencies... but you get to use them on all text editors. The thing is currently VS Code supports it well out of the box. Emacs has an active project [lsp-mode](https://github.com/emacs-lsp/lsp-mode). Even RMS wants it to be a part of Emacs.
Am currently on High Sierra and haven't experienced any issues. I'll try to uninstall and install to see if I can get any weird behaviour out of it - will update in a bit! EDIT: Okay uninstalling/installing stack might not make much sense since it's usually prebuilt. It works for building my projects though (which are mostly Yesod sites). If there are anything you would like me to try and build or any commands to run, then please let me know! :)
Oh, duh. I don't know why I was looking at the public beta rather than the developer beta. I've been away from macOS for too long! I'm downloading the High Sierra beta now. It's good to hear that you haven't had any problems. I'll report back when I get it set up. 
I've yet to see hie being used anywhere. I'm still using haskell-mode, and it interacts with ghc via ghci and gives me all the feedback I need. Errors, warnings, types, code navigation etc. Works perfectly with stack too. So I'm not sure what's the point of intero as well, especially considering intero also requires haskell-mode. 
Is this worth adding, do you have other uses?
it is being used in production and still in development... see edit. Would be more comfortable if there were more contributors though.
How about {-# language TypeApplications #-} sig :: Functor f =&gt; f a -&gt; f a sig = fmap id foo = sig @Maybe $ do Nothing Nothing Or {-# language PartialTypeSignatures #-} foo = do Just 'a' :: Maybe _ Nothing
Yeah and assisting type inference
We can avoid traversing the `f` by writing with :: forall f a. f a -&gt; f a with fa = fa Along with [`ArgumentDo`](https://ghc.haskell.org/trac/ghc/wiki/ArgumentDo) it looks very readable foo = with @Maybe do Nothing Nothing Such a simple trick without mucking with GHC, I just have to remember to use it! Thanks
I don't like workarounds where obvious refactoring/code cleanup breaks the purpose. I.e., someone looking over the code would just remove the useless lines. Tools like HLint might even suggest it.
I’m slightly in favor of this but it’s worth noting that this breaks with `do` being a purely syntactic construct: Currently something like `do True` is valid but your annotations require a type of kind `* -&gt; *`. I never really liked the fact that `do True` is valid so maybe this would be a good opportunity to fix that. Allowing the type annotations *and* still having `do True` be valid Haskell seems too confusing imho.
There's no difference. Let's understand what what you wrote means. `doubleAll xs = map (multiply 2) xs`: there's an `xs` on both sides: whatever the value of `xs` (provided it has the right type, of course), you *define* `doubleAll xs` to be equal to `map (multiply 2) xs`. Now are we clear with the fact `map (multiply 2) xs` actually means `(map (multiply 2)) xs`? In other words, `map` takes the function `multiply 2` as an argument and gives back another function, `map (multiply 2)` that you then apply to `xs` to give a list of numbers, or some other value of a non-weird type. So what you're doing is just saying "Hey, whatever the value of `xs`, `doubleAll` applied to it is the same as `map (multiply 2)` applied to it. So you have two functions that are the same on all inputs. That means the functions themselves are equal, right? That's how we define equality of two functions\*. Well then why not write just that, that both functions are equal? That's how you would end up writing `doubleAll = map (multiply 2)`. This way of writing things without arguments is called "point-free style". It can probably be a good exercise to try converting, err "point-full" definitions to point-free ones, but both are correct. Sometimes point-free is the way to go, because it's shorter, more elegant, but sometimes it would just make everything more obscure. It's all equivalent in the end :) \* There's a subtility here since there's another notion of equality, namely `==`, that's not exactly the same, but it's not the one we care about here.
You can ask ghci to tell you the types of things to get a sense of what's going on: GHCi, version 8.2.0.20170507: http://www.haskell.org/ghc/ :? for help Prelude&gt; let multiply = (*) Prelude&gt; :t multiply multiply :: Num a =&gt; a -&gt; a -&gt; a Prelude&gt; :t multiply 2 multiply 2 :: Num a =&gt; a -&gt; a Prelude&gt; :t multiply 2 3 multiply 2 3 :: Num a =&gt; a Prelude&gt; :t (multiply) (multiply) :: Num a =&gt; a -&gt; a -&gt; a Prelude&gt; :t ((multiply) 2) ((multiply) 2) :: Num a =&gt; a -&gt; a Prelude&gt; :t (((multiply) 2) 3) (((multiply) 2) 3) :: Num a =&gt; a Prelude&gt; (((multiply) 2) 3) 6 Prelude&gt; :t map map :: (a -&gt; b) -&gt; [a] -&gt; [b] Prelude&gt; :t map (multiply 2) map (multiply 2) :: Num b =&gt; [b] -&gt; [b] But yes, "currying" and "partial application" are the keywords for this sort of thing but there's nothing deep or particularly interesting going on. Some people like to say that every haskell function takes a single argument and possibly returns a function. So you could write the type of multiply this way (with redundant parentheses; the `-&gt;` is right-associative) if you wanted to emphasize that: multiply :: Int -&gt; (Int -&gt; Int) This can be read as "a function taking an Int and returning a function from Int to Int". It's not always easy to find what you're looking for, but be sure to utilize stackoverflow search; there have been lots of good expositions of this and just about every other question beginners have had over the years on there.
Just use Type Application on the pure / return / last polymorphic binding.
Overall, works great! I was able to install Stack with [one minor speed bump](https://github.com/commercialhaskell/stack/issues/3208). Installing GHC 8.0.2 (via LTS-8.17) didn't hit any problems. Building `yesod` and `amazonka-s3` finished cleanly. Upgrading Stack with `stack upgrade --git` went off without a hitch. I was even able to install and use [GHC 8.2.1-rc2](https://gist.github.com/tfausak/534b2b7a5f2b412ad688b627f841529f); I had to install `xz` first because apparently it's not included anymore, but that was it. 
No. Functions can be defined on *type classes*, which is the best way to return different results for different types. You might also be looking for monad transformers, which enable you to work inside two monads at once. 
That's...good news but also a bit mysterious. Thank you!
I am kind of wary of this extension because of you have to know the order of type variables. What's the advantage of `TypeApplications` in this case over just adding a top level type signature? If you didn't want to bother with type sigs. for the other arguments you could use `PartialTypeSignatures`.
For me it has nothing to do with how cool the code looks, whatever that means, but rather how well it communicates its intent. The first version reads like this to me: "to double all of something, map the 'multiply by 2' function over it." The second version reads more like this to me: "to double all of something, which we will call xs, map the 'multiply by 2' function over xs". The second one has additional unnecessary details that just slow down comprehension of the code. On a single isolated and trivial case like this it doesn't make much difference but those choices add up and multiply together over time.
I'm kind of amazed how interpretable the output looks. Is it possible to import libraries somehow?
Yes. In fact, the data structures from Haskell are ideal for creating complex (circular) types that Rust can actually reference count. "Purely functional" is a great proxy for the issues that arise; either problem demands that one cleanly cut graphs into trees.
I actually use it quite often (in my personal code only) do let a = 1 let b = 2 a + b
Sure, I can imagine it being better for beginners, and that's a valid concern, but once you've become comfortable with the concepts and the syntax it will slow you way down. And again, it's not about the effect it has on one statement in isolation, it's about the cumulative affect that it has if you don't keep your code as simple and terse as possible in general. It gets out of hand quickly. Edit: As a simple example: Every time you type that extra `xs` (which has to be at least 2 now) is a time that you can make a typo or just have a brain fart and write a different name. Since you're committed to the extra verbosity there will be a lot of nearby names for your brainfart to choose from. In a lot of cases the compiler will catch this because the wrong name will be out of scope or of a different type, but if your brainfart happens to pick one that is the same type and in scope, now you have a bug the compiler can't catch. If you leave the names out, you can't get them wrong. Every time a name is written in your code you can make a previously possible set of bugs impossible by removing the name.
Ok my edit was actually an example of a different benefit -- prevention of errors -- rather than of clearly communicating intent. But the intent is clearer by the same argument -- there are less nearby names to get confused with, less similar names (`x` vs `xx` and`xs`) to get confused between and have to slow down and read more carefully, etc.
Cool paper, but the code doesn't run with 4.02.1+modular-implicits-ber so I'm not sure how to try it out.
Haskell mode (haskell-interactive-mode) does have these.
I'm beginning to believe that you neither understand OOP programming nor functional programming. Maybe you should read a book or two? 
What are the benefits over this? let a = 1 b = 2 in a + b
Sometimes I don't feel like lets
Interestingly, this is not the case. The senior engineers have essentially commandeered LINQ query syntax to gain similar, but not identical, style to do notation. My understanding is that the "from...in..." syntax calls different SelectMany( function parameter) functions for the type after the "in", and this is what we are using as bind. Kind of clever and kind of horrible IMO. The senior engineers seem to have implemented multiple binds for the things they are calling monads that are dependent on the type of date contained within the monad. So for example, there are multiple definitions of SelectMany(IEnumerable&lt;whatever&gt; variable). So, we might have bind :: m a -&gt; (a -&gt; m b) -&gt; m b and bind :: m c -&gt; (c -&gt; m b) -&gt; m b meaning that the particular bind operation is selected based on its inner value. Additionally, any particular bind sometimes contains extra logic aside from mapping one inner value to another. SelectMany has its roots within the LINQ library and IEnumerables to begin with, so I'm not yet sure whether it's abuse of the concept of monadic bind or a lack of distinction between which SelectManys are binds and which are overloads of SelectMany for IEnumerable. But then, lists can be regarded as monads as well... and indeed SelectMany works similarly to join for lists within the wikibook article on category theory for Haskell... Your explanations were insightful. I have a lot of questions I need to ask the senior folk on Monday. 
Adding type application to `do` is *fundamentally different* syntactically. Unlike most things in Haskell, `do` is not a function. It's built-in syntax. That makes this particular syntactic proposal very undesirable IMO. However, I really *do* like the idea of annotating what monad you're in because that can be confusing without a nice editor at hand where you can hover over it and get the type. I like the proposed alternatives that do not touch GHC. 
some conferences have spoiled me on quality ...watching something like this is like going back to cassette tape :(
Is that the msys which is included in the Haskell for Windows?
Recently we have a rudimentary liquid haskell integration to the raaz cryptographic library. https://github.com/raaz-crypto/raaz/issues/227 Currently very little actual specs has be written but hopefully as the library matures, we will have a lot more which will give us more assurance. Here is why I am excited about it 1. The "burden of proof" is very little. Just refine the already existing types and you are done. This makes is pretty simple to integrate. 2. Pretty good support for pointer access which is particularly suited for the low level pointer gymnastics that raaz has to do. Some of these things are difficult to QuickCheck on. Overall raaz seems to be the kind of application where liquid haskell can be of substantial help. That said, its integration with cabal can make it much more potent. Also may be there should be a way for upstream libraries to expose its liquid specs to down stream users which can make it truly powerful. (I am a beginner w.r.t liquid Haskell and I thank alanz @freenode for many interesting discussion on irc). 
Those sound like very interesting features. Have you considered contributing them back to upstream GHCi so everyone rather than only Stack users can benefit?
Why is a DSL necessary? Can't MySQL queries be modeled with higher order collection methods like filter and select? I'm pretty sure that this is what [Slick](http://slick.lightbend.com/) was able to prove, though I haven't looked through its API extensively enough to say for sure.
&gt; ∗ You might ask why we need a separate map function. Why not just do away with the current list-only map function, and rename fmap to map instead? Well, that’s a good question. The usual argument is that someone just learning Haskell, when using map incorrectly, would much rather see an error about lists than about Functors. What a terrible argument. The fact that two functions are used to do the same exact thing yet are named different is way more confusing, and it makes the beginner wonder how many other ways they've been lied to to "help" and "make things easier". Definitely *not* a good first impression for the language to make.
They've already been merged into GHC 8.
No idea, I don't usually worry about that sort of thing.
Alright, thanks for your response anyway.
&gt; You could probably implement something like you are describing in Haskell by abusing Typeable and switching on types at runtime. Nah, your implementation of `(&gt;&gt;=)` can't ask for a Typeable instance, for the same reason a Set can't be a Monad because its `(&gt;&gt;=)` can't ask for an Ord instance.
Much ado about Nothing
This is super neat. How's the performance compare to `lens`?
Highly recommended, was very good last year. Bonus: Budapest is a wonderful city!
Do you have a link around about what's wrong with WSL and why 8.2 is significant to this? I'm quite curious.
But those are static! If only inlining kicks in really well (I'm not optimistic on this...)
[removed]
GHC.Generics is literally more modern -- here's the paper introducing it in 2010: http://dreixel.net/research/pdf/gdmh.pdf SYB was introduced in 2003: https://www.microsoft.com/en-us/research/wp-content/uploads/2003/01/hmap.pdf I don't have any good performance numbers offhand, but have seen plenty. The key issue is that the SYB approach involves wrapping, unwrapping and testing at _runtime_ while there is much less overhead when such things are performed statically and inlined as much as possible at compiletime. Here is one paper that compares a variety of approaches under a variety of issues: https://ai2-s2-pdfs.s3.amazonaws.com/51eb/0e70ed65f59141a17d29b3790aab08b12e58.pdf
&gt; I first tried with Generics-SOP and found it too complex. Try generics-eot, it's a lot simpler.
Yes - finally discovered the package through helpful people at IRC. It should somehow be linked to from GHC.Generic and Generics-SOP, both.
Possible to give an example of how a `[(Text, Text)]` can be converted to any record-type having all Text fields?
Nothing against generics-eot which is a nice library, but I have heard it being said several times now that generics-eot is somehow a simpler version of generics-sop, and while not technically wrong, I still find it a misleading statement. The two libraries have a very different feel. generics-sop uses n-ary sums and products and encourages the (re)use of higher-order functions. generics-eot users binary sums and products and encourages defining generic functions by writing several class instances for the representation types. In that, generics-eot is really much closer to GHC.Generics itself, and can easily be seen as a simpler version of that. If you are looking for something that feels more like generics-sop but is less heavy in the type system machinery it uses, I would recommend taking a look at one-liner.
The approach being used by GHC.Generics is actually older than SYB. But the implementation in its current form is certainly much more recent. Older ghcs supported "Derivable type classes" which were unfortunately somewhat limited and never gained widespread use. While it is true that syb has a reputation for being somewhat slow, among other things due to a lot of runtime type checking, there are also other libraries that take an syb-like approach yet do not suffer from the same problems. uniplate is somewhat more limited yet still widely applicable, extremely elegant and reasonably fast. There are also geniplate and tyb that try to use TH to statically specialise syb-like code, making it really fast. Unfortunately, I do not know how well maintained these (geniplate and tyb) currently are. 
Sounds like a nice idea, although `:help` isn't especially useful to anyone who would type `help` into ghci. Suggesting they read `some-useful-haskell-web-page.com` might be a better idea.
Isn't this pretty typical for REPLs?
The info line `GHCi, version 7.10.3: http://www.haskell.org/ghc/ :? for help` looks sufficient to me, well it could be extended to `:? or :help for help`.
I guess that's why I never managed to learn Node, Ruby, Lua or Swift.
Next step is enable autocorrection. So that if user mistypes `help` as `hlp` the help is still provided
No, it's because numbers are numbers. *Numbers* themselves do not have a mathematical ordering. Only certain sets of numbers do. The set of integers, the fractional numbers, etc., are total ordered; but the complex numbers are not. If something can be compared, it implies that they have a total ordering; that's not the case for all numbers, only numbers that most people use. less20 :: Int -&gt; Bool less20 x = x &lt; 20 works just fine. (You can also write it as `less20 = (&lt; 20)` which will make sense eventually). This is one of those things about Haskell that makes obvious sense from a mathematical perspective but is a little weird at first.
Right, so, haskell deals with imaginary numbers Why doesn't sqrt return imaginary numbers then? example: Prelude&gt; sqrt (-1) NaN Why define Nums s.t. they observe complex numbers when your other functions don't? This is where I got especially confused. The only thing I can think of is compliance with other languanges, sqrt usually returns 'NaN'. But that seems silly because haskell is so much different in any other respect and keeping this tradition doesn't help port code or anything.
Fields are numbers?
Why can you do comparisons on Reals then? example: less20 :: (Real a) =&gt; a -&gt; Bool less20 x = x &lt; 20 main = do print (show (less20 19))
Unfortunately updates are effectively linear in the entire generic data type's size, even the parts you aren't focusing on with the lens. ~~GHC.Generics made a rather unfortunate selection to generically handle the full recursive data type declaration, rather than peel one level at a time.~~
 Prelude&gt; import Data.Complex Prelude Data.Complex&gt; sqrt (-1) :: Complex Double (-0.0) :+ 1.0
&gt; Why doesn't sqrt return imaginary numbers then? It does, when its type allows: Prelude Data.Complex&gt; sqrt (-1) :: Complex Double 0.0 :+ 1.0
This is still on the front page from its posting [2 days ago](https://www.reddit.com/r/haskell/comments/6g9s0r/view_ghcs_assembly_online_with_compiler_explorer/) =P
It seems sufficient to you, but here we have a counterexample that demonstrate it's not. I'm not arguing for the custom Prelude option, just pointing out that it's not far to try to say there's no problem.
True, but a single data point isn't really enough to say there's a problem either. Considering we've gotten by for decades without frequent complaints from newcomers, I'm willing to assume it's not a problem until someone proves otherwise. EDIT: Also it doesn't even look like it was a real problem for OP anyway, judging by their other comment. So not even one data point.
Integers and rationals are not well ordered as, well, there is no least element for the entire set. 
&gt; My understanding is that the "from...in..." syntax calls different SelectMany( function parameter) functions for the type after the "in", and this is what we are using as bind. Kind of clever and kind of horrible IMO. That's far from horrible: LINQ was actually designed as a way to sneak monads into .NET! See Erik Meijer's paper "[Confessions of a Used Programming Language Salesman - Getting the Masses Hooked on Haskell](https://pdfs.semanticscholar.org/0198/d88e6af02ba4de1960d71c34f57ff9a4d2ba.pdf)", it's an easy read and contains one of my favourite quotes: "functional programming has finally reached the masses, except that it is called Visual Basic 9 instead of Haskell 98". The relevant part for this conversation is section 5.1: &gt; query comprehensions are a generalization of Haskell’s monad (or list) comprehensions to include support for SQL-style operations such as joins, grouping and aggregation, and sorting. and section 5.2: &gt; [...] the standard query operator pattern contains the well-know higher-order monadic and list processing functions such as filter, renamed to `Where`; `map`, renamed to `Select`; and of course `&gt;&gt;=` (bind), renamed to `SelectMany` So if your senior engineers are using SelectMany as bind, it seems like they know what they're doing! &gt; So for example, there are multiple definitions of `SelectMany(IEnumerable&lt;whatever&gt; variable)`. &gt; &gt; So, we might have `bind :: m a -&gt; (a -&gt; m b) -&gt; m b` and `bind :: m c -&gt; (c -&gt; m b) -&gt; m b` Note that in Haskell, those two signatures are equivalent, because in both cases, the two type variables can be instantiated to any type. `bind :: m Int -&gt; (Int -&gt; m String) -&gt; m String` and `bind :: m Float -&gt; (Float -&gt; m Double) -&gt; m Double` are different from each other though. &gt; Additionally, any particular bind sometimes contains extra logic aside from mapping one inner value to another. That makes sense: bind does not simply *map* the inner value(s) to another type (that's what the fmap function does), it also flattens the structures. For example, here's the fmap for lists: fmap :: (a -&gt; b) -&gt; [a] -&gt; [b] fmap f [] = [] fmap f (x:xs) = f x : (fmap f xs) and here's its bind: (&gt;&gt;=) :: [a] -&gt; (a -&gt; [b]) -&gt; [b] [] &gt;&gt;= f = [] (x:xs) &gt;&gt;= f = f x ++ (xs &gt;&gt;= f) That is, in addition to mapping the inner values, bind also concatenates all the lists returned by `f`. Since your senior engineers seem to know what they're doing, I have another hypothesis regarding what they might be trying to do. Do they really write overloads for a concrete inner value type, like `IEnumerable&lt;S&gt; SelectMany&lt;T&gt;(IEnumerable&lt;Int&gt; xs, Func&lt;Int, IEnumerable&lt;T&gt;&gt; f)`, or for a concrete container type parameterized by a polymorphic inner value type, like `IEnumerable&lt;Option&lt;T&gt;&gt; SelectMany&lt;S,T&gt;(IEnumerable&lt;Option&lt;S&gt;&gt; xs, Func&lt;S, IEnumerable&lt;Option&lt;T&gt;&gt;&gt; f)`? If it's the later, they might be trying a more lightweight (but slightly broken) encoding of monad transformers.
Numbers are fields. Well, rational numbers and real numbers are fields, with their appropriate operations. Integers, and especially _n_-bit sized integers, are not really fields. They're the petty criminals of the `Num` typeclass - not quite lawful, but not egregiously bad like, say, using `Num` to do C++-style operator overloading to let you "add" things that strictly should be a monoid (or group), because you like the `+` operator better. Oh, and IEEE floating point numbers are only approximately real numbers, of course, so they're approximately a field. Squint a bit, it'll be fine. Which brings us back to "fields aren't always comparable."
Hint: it's a list comprehension.
Is it just me or does this shorthand causeore confusion that it solves (vs just using map)
Looks awesome. I am all about drive by contributions, go Neil! The Diehl talk looks fascinating as well.
&gt; what numbers aren't comparable? complex numbers, quaternions, function spaces, expressions that build up a data type for a dsl, Z mod p is somewhat hinky, some kinds of field extensions, ...
I know that wasn't the best explanation. I guess I see things as follows... Formal specification and verification are hard and slow, and can become large undertakings. Most projects people develop aren't mission critical, but could benefit from some time allocated to formal verification, along with usual testing and development. If you have limited resources and you have to divide time between developing features, formal verification, and testing, you can't aim for verifying every detailed line of code. But you might be able to verify the design for desired properties at a higher level of description. For example, the containers library might take too long to verify in detail, but you might be able to verify parts of a more abstract Haskell version of it in a reasonable amount of time. Do the authors of Liquid Haskell or Liquid Types have this use case in mind? Or is it intended mostly as a tool to verify code at the most detailed implementation level, similar to unit tests or quickcheck properties? What are the use cases where Liquid Haskell's envision these techniques (https://ucsd-progsys.github.io/liquidhaskell-blog/2016/09/18/refinement-reflection.lhs/) being used on a large Haskell project ? Would they ever use it instead of Coq or Isabelle? 
Is EOT slower than SOP? Are there any other downsides of EOT vis-a-vis SOP? I struggled a lot with SOP but was simply unable to get an intuition for what was going on, and was basically reduced to copy-pasting code, with no ability to tweak it to the problem at hand. 
I don't know why this is downvoted, I thought it was a pretty funny post :)
Nice slides! I didn't know about [weeder](http://hackage.haskell.org/package/weeder), and found an unused dependency in one of my libraries.
Hmm. I think it would be entirely appropriate to use fun = zipWith (+) Which is clearer than the list comprehension version. 
None of the functions I specified perform the same operation as zipWith (+), hence their utility. The OP's function outputs 31 for the input of [1, 2, 3, 4, 5]. My list comprehension and do-block example evaluates to [2, 3, 3, 4] for inputs of [1, 2] [1, 2]. zipWith is definitely the best way to get (a -&gt; b -&gt; c) -&gt; [a] -&gt; [b] -&gt; [c] Equivalent list comprehension syntax would be let fun f xs ys = [f x y | (x, y) &lt;- zip xs ys] There might be an even more concise way to put it as well. I'm still mostly a beginner too :)
At least GHC.Generics and generics-sop have that now. There is certainly no deep reason why other libraries could not add that too, but it adds a bit of complexity. 
I have done no benchmarking of generics-sop, and little active experience of generics-eot. (If anyone wants to help with benchmarking generics-sop, that would be great.) I would expect to g-eot to be about the same as GHC.Generics and g-sop to currently be a bit slower. But all that is mostly speculation. On the other hand, many potential uses of generic programming are not performance critical. I am using generics-sop for many things and it never was a problem for me for my use cases. SYB may also be perfectly ok for you performance-wise, depending on what you are doing. 
Well, now I hope you appreciate why it seems so hard to use generics (either GHC.Generics or SOP Generics)!
I find the reasoning about pointers to be pretty useful. While such things can in principle be done in Coq, there is a large amount of effort one would need for this. In particular, one would have to start with a model for pointers and indexing and the some tactics to despose of some of the constraints that the indexing poses. Liquid haskell is much more light weight, almost no work at all. I am almost sure that is the biggest killer feature that the authors would be selling. (At least I have bought it for that)
I generally don't use parentheses for `&lt;$&gt;`, `&lt;$`, and `&lt;*&gt;`, but I start needing them every time I use `*&gt;` or `&lt;*`. For example: `f &lt;$&gt; (a *&gt; b &lt;* c) &lt;*&gt; (d *&gt; e &lt;* f)` is a pretty common pattern in my code.
Well that looks awesome. Thanks once again.
For monads that are also `Alternative`s: you can often reduce work duplication by using the applicative interface instead of the monad one; but as far as I know parsec is an exception to that rule.
Btw, I'm struggling with EOT as well. I'm getting stuck with deserialising using **every single library**. [Posted on Stackoverflow](https://stackoverflow.com/questions/44494286/how-to-write-a-generic-function-that-can-serialise-deserialize-any-record-from-a)
What you uncovered is the [type defaulting mechanism](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ghci.html#type-defaulting-in-ghci) in GHCi. Try this (`:t` will print out the type of the expression instead of trying to evaluate it): &gt; :t sqrt (-1) sqrt (-1) :: Floating a =&gt; a You see, `sqrt (-1)` is a lot more general than it looks! What is the result of evaluating `sqrt (-1)`? Without knowing what specific `Floating` `a` we are dealing with, we can't tell. Some `Floating` instance might have imaginary parts, others have not. So the right thing for GHCi would be to complain about it. GHCi however applies said defaulting rules for convenience. Otherwise expressions like `3 + 5` (which has type `Num a =&gt; a`) would not evaluate further for the same reasons. Same here: GHCi will default `Floating a =&gt; a` to `Double`, so that `sin 5` evaluates at all. This however means that `sqrt (-1)` will also be evaluated as a `Double`, unless you tell it not to with a type signature.
Thanks for the summary! All talks were recorded and should be available on youtube later this week.
You're going to have to provide more details for anyone to be able to help you.
I don't understand your question, I've just said that all "Real" types have an ordering, the ordering of the real numbers.
Ah. Sorry. Thought they were lists. That is different
Thanks a lot for the recap! Every ZuriHac seems to be better than the previous :).
Correctness and "fit for purpose" are two different things -- in a language that supports "equational reasoning" and proof we can/should statically establish correctness -- but meeting the requirements for a piece of software is another thing entirely -- so don't we need to analyse certain classes of bugs? I don't think anyone ever claimed to eliminate all bugs -- just a certain class of them, and indeed as FAICT the claims stop short unless we have total functions, proof and static checkers of such; types as propositions not withstanding? 
Like most metaphors given by scientists ;-) I'm thinking "selfish gene", "spooky action at a distance" and "monads are Burritos" are in the same boat here... only faintly comprehensible to those that already understand what the deal is; and frankly plain misleading for everyone else... popularisations have a lot to answer for. I nominate those three for the top bad metaphors of the age... 
It looks like this grammar doesn't need the power of monadic parsing, applicatives are sufficient. So it would be much easier to develop applicative-style parser straight from grammar, without translating C++ code.
I would love to provide more details, but I am not sure what specific details can help in this situation.
Abstractions are leaky, most of APIs are superfluous, REST is overrated.
Rust could be a good option if you're interested in low-level. A lisp, such as Racket, LFE or Clojure would be quite different and expose you to new concepts.
Then we have no choice but to give you ultra-generic advice: isolate the faulty component, and reduce the complexity until the cause is clear. First, I'd determine whether the problem is selenium itself, the haskell wrapper, or the browser. If it's the haskell wrapper, I'd simplify the code until you have a minimal example which exhibits the problem.
I like how you organized these slides, including that category on the corner. That's a good idea.
&gt; To what extent do Haskell abilities translate into being a worthy programmer of Scala or some other functional language more popular in business? In my experience (a single data point), a lot. My previous job was at a Scala startup, and the way I obtained it was by mingling at some startup event. They seemed interested in hiring me, I specifically told them that my experience was with Haskell, not Scala, and that it would make more sense for them to hire a person with Scala experience to work on their Scala codebase (I guess I'm not good at selling myself...), and it didn't diminish their interest one bit.
I would indulge in whatever you find most interesting in Computer Science before leaving school, because you will have plenty of time to program and master languages soon after. You don't often get to use a variety of topics from Computer Science in a given industry position.
Ah, maybe I've missed a part of my Haskell education. What difference is there between lists and ziplists. I'm only familiar with [a] lists.
I like the Lua one. I mean, why didn't they just say "NEIN!!!"
I reject your justification. I think an identifier exported by *any* module, internal or not, deserves documentation. When functions can't be given proper documentation, that's usually because they don't mean enough to deserve to cross the module boundary. And sometimes it means they should really be caged in a `where` clause.
comparing https://godbolt.org/g/ZVSidX to the rust version https://godbolt.org/g/25xEKB there is no `imul`... only .quad,movq,jmp... I've found out that in cases like this, it's jumping into the haskell RTS—we're not really seeing all the asm when it jumps into the RTS.
Not the case here, but sometimes you need `TypeApplications`, because some type variables do not appear in the actual type (`-XAmbiguousTypes`). Fox example in the `constraints` package you find some type signatures like [this](https://hackage.haskell.org/package/constraints-0.9.1/docs/Data-Constraint-Nat.html#v:plusCommutes): plusCommutes :: forall n m. Dict ((m + n) ~ (n + m))
I started working on a constructor prism: https://github.com/benweitzman/generic-lens/blob/master/src/Data/Generics/Sum/HasConstructor.hs
It's more normal once you realize that main = do let prompt q = putStrLn q &gt;&gt; getLine name &lt;- prompt "What is your name?" quest &lt;- prompt "What is your quest?" color &lt;- prompt "What is your favorite color?" putStrLn ("Good luck " ++ name ++ " on your quest to " ++ quest ++ ". May it be very " ++ color) Is simply syntatical sugar for main = let prompt q = putStrLn q &gt;&gt; getLine in prompt "What is your name?" &gt;&gt;= \name -&gt; prompt "What is your quest?" &gt;&gt;= \quest -&gt; prompt "What is your favorite color?" &gt;&gt;= \color -&gt; putStrLn ("Good luck " ++ name ++ " on your quest to " ++ quest ++ ". May it be very " ++ color) Single-use anonymous functions are omnipresent in haskell. As a matter of style, if I have a function that takes another function as an argument, when the latter is quite long, it's often convenient if its is the *last* argument -- compare foo a b c $ \d e f -&gt; something with d something with e -- ... something with f -- and foo' a (\d e f -&gt; something with d something with e -- ... something with f ) b c This is the reason that `Control.Monad` exports `forM` and `mapM`, which differ only on the order of arguments. 
Good info, although I'm in the "likes `ExceptT`" camp. &gt; some of the downsides of StateT and WriterT apply to ExceptT as well. For example, how do you handle concurrency in an ExceptT? With runtime exceptions, the behavior is clear: when using concurrently, if any child thread throws an exception, the other thread is killed and the exception rethrown in the parent. What behavior do you want with ExceptT? A coherent behaviour would be: kill the other thread and return the error value as the result of the whole computation. I have implemented such a variant of `Concurrently` in my [conceit](http://hackage.haskell.org/package/conceit) package. My motivation was to avoid having to create new exceptions in order to return non-IO errors from concurrent computations. (There is a monad-control version of `Concurrently` that can be used with `ExceptT`, but IIRC it always waits for both actions to finish even if one of the `ExceptT` fails. Ditto for `Compose Concurrently Either`.)
wat
Oh yes, sorry, I got the terms mixed up
1. Seems like `ImplicitParams` would also work here. 2. Wow, the `Has*` pattern to statically dispatch based whether a complex datastructure contains some behavior is basically an ad-hoc version of D's `static-if`. In D that's a core design technique and supported really nicely by the language semantics. I'm just tickled that a Haskeller would discover something similar.
It's not really like `static-if`, is it? It doesn't do different things depending on the presence or absence of a capability. Something like the [IfCtx trick](https://github.com/mikeizbicki/ifcxt) is more similar, methinks.
So basically when you are dealing with lists in a list comprehension or in do notation or in any applicative / monad context. What you are doing is looping through every combination of the two lists, just like a double nested for loop or list comprehension in any other language. When dealing with ziplists in do notation or in any applicative / monad context (they aren't monads themselves, but sometimes things that look like monadic contexts are actually applicative, and depending on extensions they work then), or even in list comprehensions if you enable the monad comprehensions extension, or the zipWith / zip function. What you are doing is zipping the lists together index by index. So: liftA2 (,) [1, 2] [3, 4] (,) &lt;$&gt; [1, 2] &lt;*&gt; [3, 4] [ (x, y) | x &lt;- [1, 2], y &lt;- [3, 4] ] do x &lt;- [1, 2] y &lt;- [3, 4] pure (x, y) All return: [(1, 3), (1, 4), (2, 3), (2, 4)] And: zip [1, 2] [3, 4] zipWith (,) [1, 2] [3, 4] liftA2 (,) (ZipList [1, 2]) (ZipList [3, 4]) (,) &lt;$&gt; ZipList [1, 2] &lt;*&gt; ZipList [3, 4] [ (x, y) | x &lt;- ZipList [1, 2], y &lt;- ZipList [3, 4] ] do x &lt;- ZipList [1, 2] y &lt;- ZipList [3, 4] pure (x, y) All return: [(1, 3), (2, 4)] Well technically all except the top two return: ZipList [(1, 2), (3, 4)] But you can always call `toList` to get back to a [a]. 
Thank you, got it! Lift is really powerful, it hasn't been something I've done a lot with, off to learn more Haskell :)
This is amazingly good stuff.
"once you've bought into ReaderT, you can just throw it away entirely, and manually pass your Env around" camp here. It doesnt feel bothersome, using lens and data-default. One advantage is associations between functions and configuration are explicit. A global Env usually splits into multiple Conf's and leads the way towards natural refactors. So I read the article as how to bolt IO on to pure code, and it feels very safe. Because you're explicitly starting with pure code and wiring in some IO, exceptions are easily handled and isolated via a ResourceT or ManagedT.
Ah, sorry, I should've been able to figure that out but I couldn't make it parse right in my brain. Thanks.
Basically, I'm referring to situations where we've got things like fun1 a (lambda b: logic a b $ (lambda c: logic2 (lambda d: d + 1) 4)) (Pardon my bad pseudo-code) But, like, 10 levels deep. 
This almost seems like an improvement in tone from the past - more content, less sarcasm. I am always curious to read alternative opinions. Feel free to link to alternatives you find interesting here or in the future.
Yes; the real numbers are one example.
Ah this great! I was just writing some haddock last night and every time I have try to track down how do what I want. This is very helpful!
I was just looking for one of these the other day!
Note that `for` and `traverse` should be preferred over `forM` and `mapM`. Seeing as they are strictly more general, with no penalty to type inference or anything like that. IMO `forM` and `mapM` should be deprecated in the long run.
me too! I trudged through manual, but it'll be nice to have it all on one sheet :)
This is a fairly common pattern called 'continuation passing style' or 'callback hell'. Is it 10 levels *deep*? Can you understand it as 10 calls *long*? Have you checked [this comment](https://www.reddit.com/r/haskell/comments/6gtyhm/how_big_can_a_lambda_be_before_it_should_be/dit3cyj/)?
just find some JD: [personal flight](https://jobs.lever.co/kittyhawk.aero/89a6630b-4210-4a4a-89d8-936d458f6c8a) [haskell data scientist](https://jobs.target.com/job/-/-/1118/2012182?src=JB-10182)
[removed]
Overall, topics such as purity and immutable data structures begin appearing more and more often in C++ community. See [C++ now videos](https://www.youtube.com/playlist?list=PL_AKIMJc4roXJldxjJGtH8PJb4dY6nN1D). There are now lambdas, auto, and various approaches to safety and memory management using shared pointers and guidelines that border on garbage collection. 
The code is: import Test.WebDriver myConfig :: WDConfig myConfig = defaultConfig main :: IO () main = do runSession myConfig $ do openPage "https://www.google.com" - Then I built this code in a sandbox. - Run the selenium-server-standalone-2.53.1.jar - Run the built application, and it does not open Google page.
[removed]
Hmm. Odd that it doesn't actually render but okay
What doesn't actually render? Both the pdf and svg render fine for me.
There is the type of nonempty lists, data NonEmpty a = a :| [a] which has two valid implementations for `(&gt;&gt;=)`: (x :| xs) &gt;&gt;= f = case f x of y :| ys -&gt; y :| (ys ++ xs &gt;&gt;= f') where f' a = case f a of (b :| bs) -&gt; b:bs and (x :| xs) &gt;&gt;= f = case f x of y :| ys -&gt; y :| (fmap f' xs ++ ys) where f' a = case f a of (b :| _) -&gt; b which is used in [this type](https://mail.haskell.org/pipermail/haskell-cafe/2017-February/126199.html) for simplfying `shrink` in `QuickCheck`.
Indeed; my impression is that modern C++ is surprisingly good and tasteful about picking up useful things from current language research and adapting them to the language in idiomatic ways. I guess part of the reason is because C++ is no longer the dominant "enterprise" language, and has retreated into niches that benefits more from these things, and I think the language has gotten better for it.
Oh... I see. I don't know. I've never tried to view a github rendered document on mobile. Maybe open an issue with github.
Exactly. &gt; How do you define a new type that supports an existing typeclass? With an `instance` declaration. Here is an example for the `Show` type class and a simple custom data type just wrapping `Int`: data MyInt = MyInt Int instance Show MyInt where show (MyInt i) = show i Note that type class instances generally^* either belong in the same module as the type classes definition or in the module defining the data type for which to implement them. That's why I created a new data type. \* otherwise you define orphan (as in defined in a module together with neither parent) instances, which can lead to incoherent (read: bad) situations, unless you know what you're doing.
I do something like that with the `labels` package [1]: foo :: (Has "store" Store env, MonadReader env m) =&gt; ... foo = do store &lt;- asks (get #store) -- ... foo That way it's really simple to test as I only need to construct a `Label` with the actually needed fields w/o implementing "mock/testing/..."-type classes [1]
+1
This should be easily available as the first search result for haddock markup. And/or posted in a prominent public place (which will lead to the first result).
But fair and very reasonable... this is what I love about Haskell and it's community... you ask a wee question about how a function behaves and you get an education in number theory with a hint of algebra for free... beautiful! 
Yup, fair question. Typically this pops up for me when some library has a function that takes a callback that lives in `IO` to perform some action, and we want to do some logging inside there. If we have the logging function as a field inside a reader, we can just `ask` for it and pass that function in within `IO` and everything matches up. However, with `MonadLogger` as I designed it, that's impossible. Your choices are: * Use `MonadLoggerIO`, which was added specifically to work around this wart * Use `monad-control` (or `monad-unlift`) to capture the monadic state, which can be painful. And generally, given the topics I raised in this blog post, it can be _wrong_. Consider this example, where we have a pretend `upload` function, presumably provided by a library, that hard-codes a `LogFunc` argument that lives in `IO`. We then have our own `sinkUpload` function which wants to remain generic, and therefore is defined in terms of mtl-style typeclasses. If we concretely stated the full transformer stack with `LoggingT IO` at the bottom, we would be able to extract an `IO` function. But in terms of `MonadLogger` as a constraint, we have no way of knowing that the underlying logging function is in `IO`. Therefore, we need to use `MonadLoggerIO` instead. #!/usr/bin/env stack -- stack --resolver lts-8.12 script {-# LANGUAGE OverloadedStrings #-} import Control.Monad.Logger.CallStack import Conduit import Data.Monoid import Data.Text (Text) -- Should be exported from monad-logger... type LogFunc = Loc -&gt; LogSource -&gt; LogLevel -&gt; LogStr -&gt; IO () upload :: LogFunc -&gt; Text -- ^ contents -&gt; IO () upload logFunc contents = flip runLoggingT logFunc $ logInfo $ "This is a fake function. Contents: " &lt;&gt; contents sinkUpload :: MonadLoggerIO m =&gt; ConduitM Text o m () sinkUpload = do log' &lt;- askLoggerIO mapM_C (liftIO . upload log') main :: IO () main = runStdoutLoggingT $ runConduitRes $ sourceFile "/usr/share/dict/words" .| decodeUtf8C .| linesUnboundedC .| takeC 20 .| sinkUpload
You see this on the big mainstream OS vendors (Apple, Google, Microsoft) where C++ is now mostly used on the lowest layer close to the hardware of their architecture cakes, with everything else taken by other languages on their SDKs.
At least, in OOP languages, state management is not a problem. As the article mentions, pure state is not useful in real programming .. unless backtracking is used. But excluding parsing, mainstream Haskellers have demonstrated a huge lack of ingenuity in making use of pure state and backtracking. And backtracking is the key for composability and in general for unleashing the potential of a pure and lazy language. What remains of Haskell without pure state and backtracking? a language with an excellent type system and an excellent platform for producing CT papers that is bad at everything else. Typed programming is not functional programming. This voluntary impoverishment leaves mutable state as the only alternative, in which OOP is the king. OOP was made to manage, encapsulate, modularize, reuse mutable state. Fat state convert a program in a monolith. It is impossible to make his parts composable in the strong functional, law abiding sense. It may be reusable by some tinkering using OOP techniques. OOP gives at least a some chances of it. "Has" classes, like lenses, is one more OOPization technique. But remember that this is not composability in the functional sense. It is reusability. Frankly, Why you use Haskell to express your OOP mindset? Go for the original! If Haskell were invented today, this generation would not have invented Parsec or it would be marginal, out of the hey-look-at-me circles. This generation of haskellers is too busy trying to imitate the notation and techniques of their native languages and frameworks: Ruby, javascript, C#, python... all that c..p. The stagnation of the mainstream haskell community is only comparable to his unjustified self pride. That is astonishing. I have seen better use of functional techniques in languages like fsharp or Scala than in Haskell.
That's pretty advanced talk, I actually learned some new stuff (deterministic parallelism). I wonder how it was received by the audience.
The guys at CppCast liked it. They briefly mentioned the D, Rust and Haskell keynotes on the last podcast.
Well, since `upload` takes an `IO ()` and returns an `IO ()`, seems like `liftBaseDiscard` is our only option. In that case you at least get to use your current context inside of your logging function, even if you don't get to retrieve the modified context after the logging is done. This is strictly more than you can do if your logging function lives in IO. That being said, it seems like the ideal solution here would be if the library only provided mtl-style functions instead of IO ones. So `upload :: LogFunc m -&gt; Text -&gt; m ()`. But I get that this isn't something you can always have control over.
FWIW I was able to get this working on Mint. I used `selenium-server-standalone-3.4.0.jar`, Java 1.8.0_131, [geckodriver](https://github.com/mozilla/geckodriver/releases/tag/v0.17.0) and invoked with: java -Dwebdriver.gecko.driver=./geckodriver -jar selenium-server-standalone-3.4.0.jar and it pulled up Google just fine. I uploaded my [test repo](https://github.com/deech/selenium-test) so you can play with it.
IMO, the most striking flaw is that while you cannot use a function that uses an implicit parameter in a context that doesn't bind it, you can use it in *any* context that *does* bind it, no matter at what nesting level the binding happens, and no matter where or how often it has been rebound. The "implicit" part, and concerns about it, are not about the type (where implicit params aren't actually implicit), but about the term level (where they are). Implicit parameters are harder to trace through a call graph than explicit ones, so using them makes code more context dependent and thus harder to reason about.
While I may dislike Yaml, I prefer it to invented file formats. You might be able to do "better", but it's only better if you ignore all the compatibility/conventions etc. It doesn't require manually tracking modules because it does a directory listing to find them. That has upsides and downsides.
SECTION | CONTENT :--|:-- Title | C++Now 2017: Ryan Newton "Haskell taketh away: limiting side effects for parallel programming" Description | http://cppnow.org — Presentation Slides, PDFs, Source Code and other presenter materials are available at: https://github.com/boostcon/cppnow_presentations_2017 — In designing parallel programming abstractions, taking away user capabilities is as important as granting them. In this talk, I'll explain the role of this idea in several different parallel programming libraries for Haskell, C++, and other languages--spanning from shared memory to big data. The Haskell language is an experiment in m... Length | 1:29:53 **** ^(I am a bot, this is an auto-generated reply | )^[Info](https://www.reddit.com/u/video_descriptionbot) ^| ^[Feedback](https://www.reddit.com/message/compose/?to=video_descriptionbot&amp;subject=Feedback) ^| ^(Reply STOP to opt out permanently)
I've wondered about this too, and in fact I have used ImplicitParams for some specific purposes a few times, happily.
How does this differ to Reader? It seems that you can apply the same criticisms to it. The observation that it makes things context dependent aka implicit seems a bit like stating the obvious. Perhaps there's a code example of where it differs to Reader?
Yeah, that works now. The selenium-server-standalone-2.53.1.jar does not work, but the selenium-server-standalone-3.4.0.jar and geckodriver work. Thank you very much.
Here's a relevant [comment](https://www.reddit.com/r/haskell/comments/5xqozf/implicit_parameters_vs_reflection/dek9eqg/) from another thread. The follow-up discussion is helpful too. Basically, typeclasses are normally **coherent** in haskell, meaning that every time you see a constraint, there is at most one typeclass dictionary that will fill it. For example, when you see a function whose type signature has the form `Ord a =&gt; ...`, one of two things will happen when someone calls this function: - GHC figures out that there isn't an `Ord` instance for the type. - GHC finds the unique `Ord` instance for the type. If I call this function in two different places and make `a` be `Int` both times, I'm sure that the same dictionary will be used in both places. With `IncoherentInstances`, you can subvert this guarantee, but normally coherence is a nice thing that helps us make sure that code does what we think it will. The way `ImplicitParams` works is that it relies on incoherence. Normally, you don't have to worry about multiple typeclass dictionaries being in scope because instances are unique, but you do have to think about this with `ImplicitParams`. Edward talks about this in the thread I linked to earlier.
Having a case of the *imperativeitus* that day, don't we, Mr Patient? :)
I think /u/int_index gave an example of funky interaction with existential types in a GADT in the GHC proposals repo.
Interesting, I wonder why? Let's try: -- | -- &gt;&gt;&gt; let ?acc = [] in withSig 4 -- [1,2,3,4] withSig :: (?acc :: [Int]) =&gt; Int -&gt; [Int] withSig 0 = ?acc withSig n = let ?acc = n : ?acc -- wait, why isn't this defining an infinite list? in withSig (n-1) -- | -- &gt;&gt;&gt; let ?acc = [] in withoutSig 4 -- [] withoutSig 0 = ?acc withoutSig n = let ?acc = n : ?acc in withoutSig (n-1) Okay, the behaviour is indeed very different. The inferred signature is `forall a. (?acc::[a], Num a, Eq a) =&gt; a -&gt; [a]`, which is different than the one for `withSig`, but if I add that signature to `withoutSig`, it behaves the same as `withSig`, so it's really putting a type signature or not which makes a difference, not the fact that the inferred signature is different from the manually-written signature. The only explanation I can think of which would give this result is that GHC somehow rewrites `withoutSig` to -- | -- &gt;&gt;&gt; let ?acc = [] in withoutSig2 4 -- [] withoutSig2 :: (?acc :: [Int]) =&gt; Int -&gt; [Int] withoutSig2 = go where go :: Int -&gt; [Int] go 0 = ?acc go n = let ?acc = n : ?acc -- ignored, since go doesn't take an implicit ?acc in go (n-1) But I'm not sure why it would do that. Maybe something about recursive functions never being inlined, so they're rewritten in a slightly less recursive form so that the wrapper can be specialized?
Is there a code example of "bringing into scope two parameters from different sources"? **EDIT**: referring to this: &gt; There are also corner cases involving ImplicitParameters that are entirely implementation defined. If you bring into scope two (?foo :: Int)'s from two different sources, "which one wins" is very much up in the air and up to GHC. GHC has some hacks in the handling of instance resolution to say the 'most recent one wins' -- good luck figuring out what that means. Only replacing the current implicit in scope with a let ?foo = ... in .. is rigorously defined.
You're right that YAML is kind of gross, but I still prefer it to the Cabal file format. The *only* way to parse a `package.cabal` file is to use Haskell (and in particular the `Cabal` library). I can parse YAML in pretty much any language. That makes it easy to write scripts that do something with `package.yaml` files. 
By that argument, we should rather use XML or JSON which have an even better chance of having support in your language of choice (I actually know languages where you'd be out of luck if you wanted to parse YAML with those; but which would otoh support XML natively, and possibly also JSON). We could generalise your argument to the programming language, and claim that Haskell's syntax can only be parsed with Haskell, and we should rather use some simpler grammar, let's say Lisp, which you can parse in pretty much any other language. :-) That being said, what kind of scripts would you want to write to do something on `.cabal` files?
It reduces repetition, and therefore manual errors, which is especially helpful in large multi-section cabal files that get updated regularly.
I have a general rule in life not to complain about something I'm not willing to work towards fixing. Your posts are pretty frequent here and I often can't tell if you are on to something or trolling. If you really have a problem, why not write about. Make a book, write some blog posts, provide some examples. I feel like you keep talking about this promised functional land, but you never really reveal what it is.
The advantages seem pretty negligible. Using Hpack can really screw with less common tooling like Nix though (can't simply fetch from github).
You totally could. It wouldn't be hard to write a helper function to make it just: haskellPackages.callCabal2nix "package" (doHpack (fetchFromGitHub { owner = "ElvishJerricco"; repo = "package"; rev = "..."; sha256 = "..."; })) {} It's just that such a function does not currently exist in Nixpkgs (easy pull request), and it's a little nontrivial to figure out why the package didn't build in the first place. And Nix is lucky enough to make this solution possible. I'm sure there's a lot of cabal-centric tooling out there that won't be nearly as forgiving.
Good news then, as [(typed) common definition stanzas](https://github.com/haskell/cabal/issues/2832) will help with that (they're currently blocked by the `.cabal`-parsec transition not being completed yet).
Does it make sense to tell you about any solution if you don't even admit the problem?
I'd say not, except perhaps if your lambdas are more like "blocks" (e.g. `with` blocks in other languages are just functions taking a lambda in Haskell). When doing lots of case analysis, you can easily end up with many layers of nested scopes, although it may not use "lambdas" per se. If, however, what you're seeing is CPS (continuation passing style), then Haskell has an excellent solution to that which eliminates the nested callbacks entirely. The solution is to use a different monad (`Cont` monad or some sort of streaming/async monad).
&gt; a format btw, for which we don't even have a recommendable proper pure Haskell library on Hackage! Indeed, I recently discovered that the [yaml](http://hackage.haskell.org/package/yaml) package is a wrapper around a C library, not a pure Haskell implementation. Yes, I know, it says so in the second line of the readme, I was distracted :) The reason I discovered this is because we thought there was a bug in that package, it was outputting multiline strings in a way which the javascript yaml library couldn't read. So I dived into the code to fix it... and discovered that we were wrapping the C library, so the bug is in one of those two other yaml libraries, not Haskell's. I ended up writing a pure Haskell pretty-printer, but of course it's the parser which is the tricky part.
Being able to do the identity refactoring is nice. Personally I would pretty-print my YAML, run the refactoring, and pretty-print again. 
That's something you can do for your own personal uses, but for the use-cases I have in mind, we need to retain as much as possible from the original formatting of `.cabal` files in order to be `diff`-friendly.
I'm new to the Haskell scene, still trying to learn it. If those in the know aren't willing to teach then the only place to get info is from all the sources you are claiming are doing it wrong. I'm not in a position of knowing enough to "admit the problem," but if you want to have a real discussion I'd be happy to try and learn.
For example, a state with a map (pure) or a hashtable (mutable) of polimorphic data. Each data can be indexed by his type. Any developer of any part of the application can add and remove his state data with a simple interface (set get delete) at any moment at development time without disturbing the rest of the modules neither adding monad transformers neither needing long substructures neither using OOP techniques like Has classes or getters. This has the fastest access times compatible with the flexibility required. It is comparable in performance to extensible records and stacked monad transformers but more convenient and flexible. It can not be done better IMO.
I think your first example by itself is fine, because name shadowing is the same (simply replace `?a` with `a` in your example to get regular Haskell 98 code). I think it's really the *difference* in behavior between the first and the second that gives people the creeps. I think that hits the nail on the head. Thanks for your example. Without both I wouldn't have seen the problem. Condensing this down, I get: &gt; let ?a = "first scope" in let b = ?a in let ?a = "second scope" in b "first scope" &gt; :set -XNoMonomorphismRestriction &gt; let ?a = "first scope" in let b = ?a in let ?a = "second scope" in b "second scope" Enabling `NoMonomorphismRestriction` allows `b`'s type to be general enough to have the constraint, allowing the second scope to override the first scope for `b`. (This is documented [here](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#implicit-parameters-and-monomorphism) in the GHC manual.) So the "horror" of this situation is that forgetting to add or remove a constraint basically determines whether your update of the parameter gets paid attention to or ignored.
&gt; cabal-exactprint &gt; &gt; cabal-refact Nice! That will be useful for my [cabal-rangefinder](https://github.com/gelisam/cabal-rangefinder) project, in which I wrote an ad-hoc parser which extracts the version ranges and keeps everything else the same, precisely to avoid changing the styling of the cabal file.
No. There's (usually) a difference between *programming* languages and *configuration* languages -- for good reasons. You'll note that almost nobody chooses to program in XSLT. Also: I'd personally be fine with XML for the .cabal file. At least it'd give us instant support for *basic* syntax highlighting... and with an XSD you could go a bit farther and get completion for various attributes, etc. (JSON is slightly worse IMO, but that's mainly just because of minor syntactic quibbles and the lack of comments. Plus the lack of basic validation support, reinvented poorly as JSON Schema.)
Are you being deliberately obtuse?
I'm pretty sure there should be an arrow from Alternative to MonadPlus.
wonderful, someone invites us into their house and then we advertise it as an "invasion". Way to stay competitive and classy, guys.
Sure, that's why I said "personally". Let's say that hpack chose a format that can be losslessly round-tripped, like JSON. (Or, equivalently, let's say that we have a YAML library that can losslessly round-trip documents.) Assuming that's the case, would you still prefer to use the Cabal file format? If so, why? 
7\. A lambda should be no more than 7 lines big. [[Miller, 1956](https://en.wikipedia.org/wiki/The_Magical_Number_Seven,_Plus_or_Minus_Two)] Here's a rough refactoring of your example below, which I realize you're just roughing out also: lamD d = d + 4 logic2 = id lamC = logic2 (lamD 4) logic a b c = lamC lamB b = logic a b lamC fun1 a f = f m = fun1 a lamB a = 0 maiN = m 0 
##The Magical Number Seven, Plus or Minus Two "The Magical Number Seven, Plus or Minus Two: Some Limits on Our Capacity for Processing Information" is one of the most highly cited papers in psychology. It was published in 1956 by the cognitive psychologist George A. Miller of Princeton University's Department of Psychology in Psychological Review. It is often interpreted to argue that the number of objects an average human can hold in working memory is 7 ± 2. This is frequently referred to as Miller's Law. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^] ^Downvote ^to ^remove ^| ^v0.2
From what I can tell, some people using hpack commit their cabal files.
I'll admit I'm not very good at coming up with catchy/interesting titles myself
Be warned that this can be annoying. For example, PureScript does this and one of the maintainers doesn't think it's a net benefit. https://twitter.com/hdgarrood/status/874313385369903104
Since Haskell is a pure language, the following rule always holds when `f` is a function: f == \x -&gt; f x For example, if `f` were to be `sum`: sum [1, 2, 3] == (\x -&gt; sum x) [1, 2, 3] You can do this an arbitrary number of times: \x -&gt; sum x == \x -&gt; (\x -&gt; sum x) x == ... The process of going from `f` to `\x -&gt; f x` is called an eta expansion. The second definition you gave is just the eta expansion of the first: map (multiply 2) == \xs -&gt; map (multiply2) xs Hope that helps!
That's a good point about name shadowing. One practical difference though is that GHC can warn you about name shadowing, and if you compile with `-Werror`, it can forbid it entirely. With `ImplicitParams`, the compiler doesn't help you. Also, the example you gave with involving the monomorphism restriction is truly terrifying. I think it illustrates the problem better than either of the examples I gave.
Huh, so foo :: Whatever foo = exp ?a uses the `?a` in scope where `foo` was defined, while foo (?a :: A) =&gt; Whatever foo = exp ?a uses the `?a` in scope where `foo` is used (lexical and dynamic scoping respectively, I guess). Scary!
I had problems with hpack for the conditional compilation of executables, which is not supported. There are still reasons for using cabal.
[What did you call me?!](https://www.youtube.com/watch?v=dakxwoVV7yM)
Yes, but that part's not scary IMHO, it's exactly the same thing as with ordinary parameters: foo :: Whatever foo = exp a uses the `a` in scope where `foo` is defined, while foo :: A -&gt; Whatever foo a = exp a uses the `a` passed explicitly as an argument where `foo` is used. The only difference is that with implicit params, the argument is passed implicitly, so it's indeed the `a` in scope where `foo` is used. The scary part, I think, is that without a type signature, it's not clear which of those two is going to get picked, and unless `foo` is at the top-level, GHC isn't going to warn me about the missing type signature.
SECTION | CONTENT :--|:-- Title | Obtuse Length | 0:00:11 **** ^(I am a bot, this is an auto-generated reply | )^[Info](https://www.reddit.com/u/video_descriptionbot) ^| ^[Feedback](https://www.reddit.com/message/compose/?to=video_descriptionbot&amp;subject=Feedback) ^| ^(Reply STOP to opt out permanently)
S-expressions: Well, I guess it depends on the particular language embedded in those S-expressions, doesn't it? (S-expressions are just syntax. Whether something is a programming language or configuration language doesn't really depend on the particular syntax chosen.) Dhall is an interesting case, but given Gabriel's choice to call it a configuration language, I'm going to go with "configuration" on that one. I'm not sure what your point is.
It's a bit of a stretch to say "it's exactly the same thing as with ordinary parameters" when one of the cases doesn't even have a parameter!
okay, "similar to the corresponding situation with ordinary parameters". I found another scary situation: if I refactor `foo` so that it now accepts a `?b` instead of an `?a`, and there is a use site at which I still do `let ?a = ... in foo`, I won't get a warning if `?b` is already in scope at that use site.
I personally don't care anymore about appealing to C++ programmers. I am sure this is a good talk. I just feel bitter from years of being cast as impractical for mentioning FP languages around various Java and C++ programmers, then all of a sudden being embraced now. The only thing that has changed is that more time has passed and industry hype has needed a new "hot" thing to discuss.
&gt; okay, "similar to the corresponding situation with ordinary parameters". Yeah, but my point is that with ordinary parameters you *know* you're taking the parameter at the call site because you're explicitly passing it in at the call site!
Yes, and basically for the remaining reasons I already pointed out in that email thread; if you choose a generic format that doesn't adequately capture `.cabal`'s expressivity and data-model, you end up becoming a [Genius Tailor](http://www.wealthculture.cn/infoShow.asp?nid=880&amp;sort=Article%20List). `.cabal` files are supposed to be written and edited by humans; I consider `.cabal` files to be declarative specifications of the package (components and their modules and compiler flags), with informational meta-data (synopsis, description, author, licence, etc), and most importantly an accurate specification of its dependencies including version constraints, and on top of that control structures such as conditionals and definable package flags. Having a custom syntax allows us to have a more concise language to express those specifications which can get fairly complex. It's true that there's currently some avoidable boilerplate (the redundancies you point out elsewhere), but that's no reason to throw out the baby with the bathwater and YAML all the things, rather than improving and extending the `.cabal` syntax to address the shortcomings. But I don't consider not-being-{XML,JSON,YAML} a shortcoming, on the contrary, in my metric it's a big advantage for the goal of being a domain-specific language intended to be manually edited. To come back to the genius tailor; we want to be able to customize the suit, rather than contorting our body to fit into the maladjusted suit.
Ah, I see! I was happy to accept either lexical or dynamic scoping as long as which one it was is written somewhere, in this case in the type signature, whereas you would like that fact to be clarified at each call site as well, got it.
I'm not sure what you mean. Are you talking about the `buildable` flag? I think you can do that with hpack via [conditionals](https://github.com/sol/hpack/blob/ce514cee60d0ae8b8b17e372ce5033c0996fa692/README.md#conditionals). flags: build-the-executable: manual: true default: true executables: the-executable: when: - condition: flag(build-the-executable) then: { buildable: true } else: { buildable: false } 
Well then we fundamentally disagree. I don't see anything about the package specification that requires a bespoke format. I don't feel like I have to contort myself to describe my packages with YAML. In fact, I feel like Cabal is the syntax I have to contort myself around. Why do some keys have colons (`name:`) but others don't (`test-suite`)? Why do some lists have commas (`build-depends`) but others don't (`default-extensions`)? Why do some sections take arguments (`executable`) but others don't (`library`)? How are you supposed to format text in the `description` field? Elsewhere in the thread you complain that the YAML spec is complicated, but at least it *has* a spec. As far as I can tell the Cabal file format is specified by its implementation. 
Generic code like `[1..10] :: Num t, Enum t =&gt; [t]` is compiled down to explicit dictionary passing for each class instance. So this code at the core level accepts two extra arguments for the instance dictionary, like `\num enum -&gt; enumFromTo enum (fromInteger num 1) (fromInteger num 10)`. So it's not really a "thunk" that can be "cached" in the way that `[1..10]` is. It's an actual function that's called with arguments every time. Function calls aren't memoized like that. So that's why `length` on your generic list doesn't force the whole list and update the thunk. It makes a new list every time. Does that make sense? --- **EDIT**: now I'm back at a computer, I can demo if you enable `:set -ddump-simpl` to show the Core: Prelude&gt; :set -ddump-simpl Prelude&gt; x = [1..10] ==================== Tidy Core ==================== Result size of Tidy Core = {terms: 22, types: 20, coercions: 0} -- RHS size: {terms: 11, types: 9, coercions: 0} x :: forall t_a17V. (Num t_a17V, Enum t_a17V) =&gt; [t_a17V] [GblId, Arity=2, Caf=NoCafRefs, Str=DmdType] x = \ (@ t_a17V) ($dNum_a1ha :: Num t_a17V) ($dEnum_a1hb :: Enum t_a17V) -&gt; enumFromTo @ t_a17V $dEnum_a1hb (fromInteger @ t_a17V $dNum_a1ha 1) (fromInteger @ t_a17V $dNum_a1ha 10) vs Prelude&gt; y = [1..10 :: Int] ==================== Tidy Core ==================== Result size of Tidy Core = {terms: 17, types: 6, coercions: 0} -- RHS size: {terms: 6, types: 1, coercions: 0} y :: [Int] [GblId, Str=DmdType] y = enumFromTo @ Int GHC.Enum.$fEnumInt (GHC.Types.I# 1#) (GHC.Types.I# 10#)
&gt; Why do some keys have colons (name:) but others don't (test-suite)? Because they are structurally different. `test-suite` is a control-structure defining a new entity and opening a block, whereas `name:` denotes a specific key/value pair (where each key can have a different monoid associated - the list-ish ones are concatenative). &gt; Why do some lists have commas (build-depends) but others don't (default-extensions)? The question should rather be: why do some lists allow to omit the commas; and the answer is that commas have been made optional in some contexts where it can be done without ambiguity. &gt; Why do some sections take arguments (executable) but others don't (library)? Who says `library` doesn't take an optional argument? ;-) You shouldn't consider `executable` or `library` as merely a data-ish section, but rather as a kind of control structure keyword, defining a new entity and opening a new block, which can lateron be referred to inside the same `.cabal` file by its label. Similiar to how `flag` declares a new flag entity which can lateron be referred to in conditionals. You appear to be thinking in terms of the YAML model which doesn't accurately capture the semantics and intent of `.cabal` files. It's one of the points I'm trying to make that seeing a `.cabal` file as merely flat data can be misleading. &gt; How are you supposed to format text in the description field? Haddock markup. &gt; Elsewhere in the thread you complain that the YAML spec is complicated, but at least it has a spec. As far as I can tell the Cabal file format is specified by its implementation. That's a fair point and I think we should fix that! The *current* cabal format doesn't have a written down spec (I say *current* because the original CABAL 1.0 spec was actually written down! Back then, Cabal files looked a lot more like `.deb`'s `control` files -- another very popular custom format). The conversion to the parsec-style parser has actually brought us a bit closer to getting a new specification for the `.cabal` format written down.
Hi, I have a question about this, especially about the RTS output. From what I have read I was under the impression that having a significant difference between the "Mutator" user time and its elapsed time (user time &gt; elapsed time) signals that more than one CPU (core) was in use, which means that the program has used parallelism. But in this blog post, just like in my experiments, +RTS -qg causes a significant performance boost (about 50%ish) but it also decreases the MUT user time dramatically. In my experiment it does this such that the user time is in fact much lower than the elapsed time. In case this is unclear, I am referring to these lines from the blog post: MUT time 785.992s (430.742s elapsed) &lt;- without -qg, low productivity MUT time 132.592s (133.190s elapsed) &lt;- with -qg, high productivity And in my case the numbers look like this: MUT time 27.224s ( 21.460s elapsed) &lt;- without -qg, low productivity MUT time 9.740s ( 18.529s elapsed) &lt;- with -qg, high productivity Could somebody enlighten me here? I probably got something wrong here. Thanks! Moritz 
&gt; Because they are structurally different. That doesn't really explain why it can't be `name foo` instead of `name: foo` or `test-suite foo: ...` instead of `test-suite foo ...`. &gt; commas have been made optional in some contexts Even more specifically: Commas are required for dependencies to avoid checking is a token is a package name or a version constraint. &gt; You appear to be thinking in terms of the YAML model which doesn't accurately capture the semantics and intent of `.cabal` files. I don't see how `flag foo ...` more accurately represents intent than `flags: { foo: ... }`. &gt; Haddock markup. Except that it's not. For example, you have to separate paragraphs with periods for some reason. description: This is the first paragraph. . This is the second paragraph. But my larger point is that I only have to ask these questions because Cabal uses a custom file format. I already know all the answers in YAML. I see what you're saying that Cabal lets you have control structures that define new entities, but I disagree that YAML (or whatever) doesn't let you do the same thing. `{ "flags": { "foo": { ... } } }` is exactly as much of a control structure as `flag foo ...` is. 
Notice that `:sprint a` prints `_`, not `[_,_,_,_,_,_,_,_,_,_]` as you would expect if `length` was forcing the spine but not the elements in order to compute the length. What is happening is that `length a` first specializes `a :: (Num t, Enum t) =&gt; [t]` to `a' :: [Integer]` (my ghci displays a big warning telling me that it did this, "warning: [-Wtype-defaults] Defaulting the following constraints to type Integer"), and then the length of that `a'` is computed. This forces the spine of `a'`, but not the spine of `a`, which is why it is still unevaluated when you `:sprint` it. See my blog post "[Will it memoize?](http://gelisam.blogspot.com/2015/06/will-it-memoize.html)" for a lot more circumstances under which the evaluations you perform in one computation will or will not be reflected in a later computation. Second, the reason your second `:sprint` displays `[1,2,3,4,5,6,7,8,9,10]` and not `[_,_,_,_,_,_,_,_,_,_]` is because the [implementation](http://hackage.haskell.org/package/base-4.9.1.0/docs/src/GHC.Enum.html#line-788) of `enumFromTo` for Integer uses `seq` to make sure that each element is forced as you traverse the list.
I've never managed to use (and understand) the reflection package. Could you provide a side by side comparison or example ?
You are right, local works indeed. I'm more thinking about using typeclass to solve implicit parameters problems.
Aha, yes that is indeed a problem.
Good stuff. It would be really interesting to see how other languages would perform here. Optimised version in Java, C#, F#... all presumably slower but by how much?
There are bindings for Spark RDDs: sparkle https://github.com/tweag/sparkle bindings for Spark data frames https://hackage.haskell.org/package/krapsh There is an implementation of map-reduce using cloud-haskell. https://github.com/yogeshsajanikar/hspark The project transient implement map-reduce over RDDs completely in Haskell: https://hackage.haskell.org/package/transient-universe-0.4.5/docs/Transient-MapReduce.html Resilience and dataframes are not implemented Since the speaker argues that genomics needs to develop special kinds of maps and reductions as well as other distributed algorithms, implementing these new algorithms in transient is arguably easier since it is functional from the ground up. On the contrary Spark is not internally functional but more object orented and exposes a thin functional layer. In the other side, this implementation has been tested in small distributed settings. it has not been tested with huge quantities of data, so it is not production ready yet for big data analysis.
I enjoyed reading this! It's fun to read the code and scroll down to see the comparison. Congrats on the migration.
&gt; I already know all the answers in YAML. I don't; and I frequently trip up over the weird escaping and conversion rules of YAML, and other surprising non-intuitive "features". Maybe if I used it exclusively I'd contort myself enough and/or learn all its warts so that my muscle memory would help me avoid them... :-) However, we're getting to the point where this becomes more about which school of thought you subscribe to. Do you rather prefer to reuse a generic completely uniform syntax which doesn't have nuances to signal differences, or do you lean towards a bespoke domain specific language tailored to what you want to express. I've already agreed that there's warts and infelicities in `.cabal`'s syntax and that we should try to fix/improve them. But beyond that I don't know what else I can tell you without repeating myself. Maybe we just have to agree to disagree. At the end of the day, we have already made that choice long time ago. We've built an ecosystem around that design choice which is still evolving, and which isn't going away. Fighting this choice by trying to establish a second competing format which merely has a different philosophy and exhibits somewhat different trade-offs seems a little bit like another attempt at splitting the ecosystem albeit at a lower contact point. We already have to explain why some projects use a `cabal.project` file (another of cabal's files which share that common format) while others have one or more `stack.yaml` files; now we're starting to also need to explain why there's two different competing package meta-data files, either `$pkgname.cabal` or `package.yaml` in Git repos. What's worse, the confusion to newcomers about this peculiar split personality of our community, or the initial inconvenience of learning an unfamiliar Haskell syntax as well as a custom package specification language with a syntax evolved from Debian's `control` files by adding Haskell-ish syntax extensions?
&gt; parses YAML into Haskell values and then does additional validation Yes, but then you shift validation at a later point, which comes with its own problems. Moreover, if the argument was that YAML enables tooling in non-Haskell languages, you'd end up lacking validation features unless you reimplemented the same validation rules everywhere. The point being, you sacrifice the benefits of a domain specific language for the superficial gain of tooling being able to recognizing a known coarse structure (but not its fine structure). This is rehashing old arguments from the email thread.
XML, CSV, JSON, YAML - all these are general interchange formats. Cabal files are custom syntax for Cabal packages only - that's my objection. Note that OCaml now uses S-Expressions for much of their metadata - again in the general interchange format pile.
You can use include files so multiple projects share common fields/dependencies/extensions. That's huge.
It's not at all obvious, but the Data.Text haddocks at least mention that it's O(n). 
Boxing. Haskell is uniquely good at unboxed data for being a GC'ed lang.
If by uniquely you mean Java is bad at it. .NET has a pretty solid story around value types.
/u/kosmikus referenced this old GHC feature in the Generics thread. It's interesting to look at the origins of the idea.
I think this is the best one so far.
GHC.Generics derived lenses like these benefit from using the lens package's [fusing](http://hackage.haskell.org/package/lens-4.15.3/docs/Control-Lens-Lens.html#v:fusing) function to allow the generic representation to fuse away in the case that it doesn't inline.
&gt; you assert that Simon Hengel (/u/solirc) ... Calm down, you're barking up the wrong tree; I did nothing of the kind.
agreed. Most jobs are all pretty much the same thing anyway. At least in client-facing fields. Make something awesome and interesting to you. Employers will focus on passion and willingness to learn slightly more than on your current knowledge.
[Image](https://imgs.xkcd.com/comics/standards.png) [Mobile](https://m.xkcd.com/927/) **Title:** Standards **Title-text:** Fortunately, the charging one has been solved now that we've all standardized on mini\-USB\. Or is it micro\-USB? Shit\. [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/927#Explanation) **Stats:** This comic has been referenced 4586 times, representing 2.8580% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_divrhux)
I actually agree with both sides as you picture them: 1. YAML is terrible :-) 2. Reuse an existing spec when it makes sense (which I argue it doesn't in `.cabal`'s case - and if that isn't strong enough, we already have a format which is (re)used in various config files in Cabal, we don't need to complicate matters by introducing a 2nd competing one). 3. And if you don't reuse a spec, make sure to write one down (and keep it updated)!
Yes, good point. But it's much more common for MR to cause confusion in GHCi than for lack of it to cause confusion. That's why it was correctly disabled in GHCi.
&gt; Our standard vector libraries do automatic Array of Struct (AOS) to Struct of Array (SOA) representation transformations That is very interesting! Source? Why isn't this advertised?
*Monad and Functor got fixed* in Haskell as well, there are [`MonadFail`](https://hackage.haskell.org/package/base-4.9.1.0/docs/Control-Monad-Fail.html) proceeding, and `return` is optional already, defined as `return = pure`. 
The default value from a failed `cast` is annoying me. Otherwise, I found interesting that some of the point can be easily "fixed" in Haskell with a custom prelude: better `String` type, `Cast` typeclass, "fixed" `Monad` instances, better `Num` hierarchy.
I agree that alternative preludes are a Good Thing. Including even classy prelude, even though it's not my personal favorite. But for avoiding `head`, no alternative prelude or special library is needed. There are plenty of great tools and techniques out of the box for idiomatic Haskell programming that avoids partial functions like `head`. The main tool is the `Maybe` type, together with its massive array of useful combinators and instances, including: * Combinators: `maybe`, `fromMaybe`, `listToMaybe`, `maybeToList`, `mapMaybe`, `catMaybes` * Instances: `Functor`, `Alternative`, `Applicative`, `Monad`, `Traversable`, `Foldable`
Although these days I often write type signatures before code: sometimes I'll temporarily add `{-# LANGUAGE NoMonomorphismRestriction #-}` at the top of a file, comment out a type signature, and find out whether or not the type was unnecessarily monomorphic.
I'd was about to mention, that if you want to improve on JSON, please consider [edn](https://github.com/edn-format/edn). I don't know the current state of `hedn`, but I hope it's easy enough to improve.
Just to point out (well hidden in comments :), that there are *very poor* design decisions in Cabal format too, for example you **can** write: ``` other-extensions: DeriveFunctor , DeriveFoldable DeriveTraversable ``` Preserving the location of an completely optional comma: *duh*.
More solid than Haskell, given that you can unbox polymorphic fields (which is really one of the most important kind of unboxing, as it's what makes your generic data types fast).
I've said it many times and I'll repeat it here: in a language with dependent types, you *can* express your exact preconditions quite easily, say `(xs : List A ** sort xs = xs)` to express that you expect your input to be sorted, so you are likely to do so. But this imposes a burden to all your callers, who must now prove to the compiler that this precondition is satisfied. This is great if you need to be 100% sure of your code's correctness, but not so great if you want to get some work done. Java-style types aren't expressive enough, dependent types are too expressive, Haskell's type system is in between, at a sweet spot where you can express many but not all preconditions, but the preconditions which can be expressed are those which can be checked easily by the compiler at the call site. *edit*: of course, with a more powerful type system, it's possible to choose not to use those more advanced, too-precise-for-your-own-good types, and thereby get the same experience as in Haskell. But it's not as simple as "don't use dependent types", since dependent types can both be used to create easy-to-check and hard-to-check APIs. People sometimes complain that Haskell has a bunch of advanced features like type families and type equality constraints which could all be replaced by a single, more powerful feature, namely dependent types. But I like how those different features allow you to push towards different parts of dependent types, while remaining easy-to-check. It would be great if there was a way to unify all of those into a single, *equally* powerful feature, but before we can get there I think we have to keep pushing towards-dependent-types-but-not-completely-there along different angles, like the automated proofs in LiquidHaskell and maybe the TypeInType stuff, in order to find the boundaries between hard-to-check and easy-to-check dependent-type-like features.
In my opinion there is no such thing as "unnecessarily monomorphic". For application code at least, gratuitous polymorphism is a code smell. Semantically precise types make code more readable and understandable, both for humans and for the compiler.
I actually consider the opt-in instead of opt-out laziness one of Idris' biggest downsides when compared to Haskell. Laziness is simply the better default. Sure there are problems with it too, but the benefits simply outweigh the drawbacks for me.
&gt; Should choice of language features be driven by what's easiest of rate compiler to check, or what's most useful for the programmer to deliver value to their users? I don't know what "rate" means in this context, but I think I can address the part of the question about delivering value to users. "Delivering value" is agile-speak for "working on the features which the users want". Your compiler is not going to help you determine whether you're working on the right feature, you still have to interact with your users in order to do that. That means involving them early, and showing them prototypes of incomplete features before spending too much time working on a feature which makes sense for you but might not be quite what they have in mind. I don't think precise types help much here; precise types help with correctness, but in the prototype version the bugs don't matter, they prevent the tool from being used in anger but it doesn't prevent it from being used in a demo to communicate to the user what the feature is about. Precise types do help with _completing_ those features, by eliminating enough bugs to make the feature usable. More precise types catch more bugs, but more complex types are harder to work with, slowing down the pace of development, so it's a tradeoff. Like I said, I think Haskell makes a better tradeoff there than dependently-typed languages.
&gt; in order to find the boundaries between hard-to-check and easy-to-check dependent-type-like features. Is there really such a thing? It's not a choice between *hard-to-check* and *easy-to-check*. It's a choice between *can express complex proofs* and *cannot express complex proof*. So basically you're advocating for dumbing down dependent types in order to prevent a user from shooting herself in the foot. It's a stance that is taken by a lot of programming languages, but it doesn't seem to be in the spirit of Haskell, *at all*.
&gt; it doesn't seem to be in the spirit of Haskell, *at all*. If you move from Java-style types to Haskell-style types, then "the spirit of Haskell" will look like "using more powerful types to make illegal states unrepresentable", for sure, and dependent types certainly do move even further in that direction. But I moved from Java-style types straight to Agda's dependent types and only then to Haskell, and so I view "the spirit of Haskell" as an excellent compromise between those two extremes.
Check out [RustBelt](http://plv.mpi-sws.org/rustbelt/). There's a lot of research going in to formalising Rust.
I meant compromising features to avoid users from messing up. EDIT: I think your point is more about how they are used than what they can do. As you won't do something that you can't do, it makes Haskell naturally fit the middle ground you're looking for. That doesn't mean such a middle ground is unattainable in a more powerful language. Furthermore, you mentioned having used Agda first. Well in Agda they ~~go *crazy* with~~ really focus on dependent types (that's the point of it after all). EDIT: Reworded the *go crazy* as it's too connoted for what I meant to say.
Is there much of a difference between strict and lazy when you also have totality enforced? I've barely used `Lazy` and `total` in the same program before, so I'm not experienced with laziness mixed with totality.
&gt; Callers can always pass a trustme proof if they don't want to. Nope! That only works for proofs which contain no computational content, e.g., definitional equality proofs or unit. But proofs often contain computational information: for example, the proof of `{a b : N} -&gt; a &lt; b -&gt; ...` will contain computational information which can be used later. If you drop in a `postulate` where a proof entailed computational content which is then used later, your program will crash or hang at runtime.
In my humble opinion, the JVM world is too far ahead. Some frameworks might be a bit ugly to work with, but the industry spent decades writing libraries and large frameworks for pretty much everything. Yes, Java is horrible and bloated, but it has network effect. When you start building a new system, you don't start from scratch and you can build on top of other existing layers. This has a good side (on productivity) and a bad one (on complexity). Don't get me wrong, I love the functional community, it's amazing, but it's simply its focus is on something different. While here we spend time discussing about abstract concepts and many things regarding elegance, the Java community has been spending its time on getting things done and publishing new code. Also, I can say the same of the Go community, which it's main focus is on simplicity and pragmatism. But they are still getting there on distributed systems. So, while most people was making fun of Java for being too enterprisey, there were busy working on stuff running in production. So, not everything they did is great but they made relevant progress on the type of systems that were developed. As a side note, I'm not jugging if Java is a good or bad language, I'm just talking about its community and the product of their effort producing working projects.
[Author here.] More details about the compilation problems would be appreciated! I can think of two possible issues (both noted in the paper, I think). First, OCaml doesn't yet support `module type rec`, and so you have to use recursive modules instead, writing ``` module rec M : sig module type T = ... end = M ``` in place of ``` module type rec T = ... ``` There's [an example](https://github.com/yallop/metaocaml-syb/blob/a5370649bf9c90c1bb323718049d257b10152e64/lib/syb_classes.ml#L22-L47) in the code for a previous version of the paper. Second, I've omitted some type annotations for readability, especially where the types are listed in the signatures. OCaml's type propagation isn't especially aggressive, and so sometimes types need to be repeated, which can clutter up code, especially where the code itself is very short. (A way of separating signatures from definitions, Haskell style, could really help here.) I'm planning to put the full code for the paper online, I hope before ICFP.
To be clear, I'm not arguing in favor of YAML specifically. I never knew that Cabal files were based off Debian's control files; that's good to know! Layering Cabal on top of [Debian.Control](https://www.stackage.org/haddock/lts-6.35/debian-3.89/Debian-Control.html) might work, but that format isn't generic enough to support everything Cabal needs (as you note). It's easy to pick apart an appeal to the people. If that's the way we chose everything, we'd all be writing JavaScript. But again, I'm not talking about YAML *in particular*. Any other common document format has the same benefits. Case in point, how would you feel about using XML with a schema (XSD)? With regards to mapping Haskell's syntax to YAML, I see what you're saying. Embedding a programming language in a document format is a bad idea. (Just ask XSLT.) Which languages are appropriate to embed in other formats? I don't know how to answer that in general, but my argument is that Haskell package descriptions fit into generic document formats pretty well. I think starting (or using) another standard is a perfectly reasonable way to alleviate pain points. Using the PVP as an example, addressing my concerns with it would turn it into SemVer, so I use SemVer. Similarly, addressing my concerns with Cabal would turn it into hpack, so I use hpack. 
Isn't it exactly the spirit of Haskell? I could use Python if I trusted myself to not shoot myself in the foot. Good languages reduce the likelihood of users shooting themselves in the foot.
Oh does Idris go less crazy with dependent types? I assumed it was Agda "but pacman-complete", is there more to it?
&gt; As soon as proofs aren't irrelevant to the implementation anymore It's a bit tricky: sometimes the proofs are not relevant per se but pattern-matching directly on them does allow you to only consider the valid cases, which is nice as it makes the code lighter. For instance, you can write minus like so: minus :: (a, b :: Nat) -&gt; a &gt;= b -&gt; Nat minus a Z _ = a minus (S a) (S b) prf = minus a b (gePred prf) minus _ _ _ = 0 but most people would probably write: minus :: (a, b :: Nat) -&gt; a &gt;= b -&gt; Nat minus a _ geZ = a minus (S a) (S b) (geS prf) = minus a b prf The second implementation gets stuck on non-canonical proofs but you could argue that it ought to have the same computational behaviour (it is possible to prove that any two proofs of `a &gt;= b` are equal!). Now imagine a datatype with a hundred constructors and an inductive predicate ensuring only two of them are used to construct a term. If you write a function by pattern-matching, in one case you have a 100 lines function (most of which is just saying "this case is impossible") which doesn't get stuck on a proof while in the other one you only have a 2 lines function but does get stuck. But the property itself does not contain any "interesting" content from the computational point of view.
&gt; I'm not sure what you mean. I meant that as soon as we're doing control flow by pattern matching on a proof (value), it isn't irrelevant anymore and would have to be modeled as a value argument after erasure to e.g. Haskell 98 anyway. Those kind of arguments naturally can't be handled by a simple `postulate` (proof search still might), but neither could you omit them in a plain Haskell implementation. Then again, the idiomatic Haskell implementation might look different than the idiomatic Idris definition...
&gt; a flag like `fno-error-proof-search` I like that idea! This way we would encode our full requirements precisely in the types, and yet it wouldn't over-burden the callers unless they decide it's worth the effort. Of course, as /u/dnkndnts explained, this would only apply to irrelevant proofs, so we'd still be going towards-dependent-types-but-not-completely-there like I suggest, but irrelevant proofs are quite a large portion of all dependent types, so it might also satisfy those who disagree with my assesment. *edit*: I should also address the second part of your comment. First, I think the burden of writing proofs is much bigger than the burden of annotating types. Second, I agree that proofs which can be inferred impose no burden on the callers, but at the moment we don't have a language-level way of distinguishing between arguments which are easy to infer and arguments which aren't, so in a dependent language, we need a lot of care and experience in order to design a good API, whereas in Haskell, I feel like the language is pushing me towards a good API. Again, I think we need to continue experimenting with pushing towards-dependent-types-but-not-completely-there, so we can find more classes of proofs which are guaranteed to be inferable.
Enjoy the comments on HN: https://news.ycombinator.com/item?id=14550606
 mystery package = how_long_ive_known_about package / how_well_i_understand package head (sortBy (flip (comparing mystery)) packages) == "reflection" 
&gt; At ZuriHac Edward had an insightful comment about the *Overloading without Type Classes*, when you (= the human) see the overloaded `+`. in the code, do you know which one you use? In Idris, we solve this with tooling. In Emacs, hovering over the plus will give you a tooltip showing which `+` was chosen along with its type signature and a summary of its docstring. Right-clicking it lets you do things like see the definition, read the full docs, and browse the namespace in which it is defined. This does mean that it's harder to use text editors with poor Idris support to understand the code, however.
Big argument on this post! Personally I think the most important thing is that Cabal-the-library have 1. A precisely specified type `CabalFile`, say, which represents *exactly* the possible Cabal files there can be -- no more, no less. 2. Functions `toCabalFile :: CabalFile -&gt; String` and `fromCabalFile :: String -&gt; Maybe CabalFile` (or s/`String`/`Text`/g, or whatever) satisfying `fromCabalFile . toCabalFile == Just`. That way anyone else can unilaterally write their "cabal-file-like-thing" however they want and know it's going to be compatible with Cabal-the-library.
You're conflating correct code with complex features. Haskell protects users from their mistake by giving them the tools to check that their program is correct. It doesn't protect the users by limiting the available set of features in order to avoid complicated code.
If your type is precise enough, the compiler can write the implementation for you (or at the very least help you toward a correct one). Take a look at this: https://twitter.com/beala/status/739698465240801280
&gt; By that argument, we should rather use XML or JSON which have an even better chance of having support in your language of choice Having Cabal config in XML or JSON would be very nice. My own preference would be s-expressions &gt; JSON &gt; XML &gt; YAML. One of the "killer features" of using an established, generic syntax is that you can avoid using that syntax. Tools already exist to convert between different formats, and it's not hard to invent a new "frontend" language using these existing libraries. There are also CLIs, TUIs, GUIs, Web UIs, etc. to view and edit such formats indirectly. By choosing a custom format, we don't have any of that, so everyone's forced to actually interact with that format. &gt; We could generalise your argument to the programming language, and claim that Haskell's syntax can only be parsed with Haskell, and we should rather use some simpler grammar, let's say Lisp, which you can parse in pretty much any other language. :-) If only! I've been banging this drum for a while, and have accumulated years' worth of hacks to work around Haskell's hostility to anything other than GHC's frontend. Your inclusion of `:-)` makes this proposal sound like a joke, but consider how much faffing around we could avoid if Haskell had a nice, general syntax like s-expressions. TemplateHaskell would be less necessary; things which TemplateHaskell can't handle (e.g. generating import lists) could be achieved with preprocessors; we wouldn't have to use the *CPP*! Of course, we could still all program in the existing syntax; a killer feature of an s-expression language is that it's trivial to compile down to it from any surface syntax you might want.
I'm actually in the middle of debugging a problematic code on Flink (another stream processing framework somewhat comparable to Spark) and it's annoying the dig through all the layers of complexity. But even if I'd like it to be more simple to understand quicker what is happening inside, I have to admit that it's complex because it offers a lot of functionality off the shelf. Just because the framework isn't written in one of my favorite programming languages is not a excuse good enough. Happy to hear production ready alternatives that are able to tolerate failure and cope with heavy workloads in production. Unfortunately, I'm afraid I won't see many right now. I know pretty well some companies are using Haskell or something equivalent for complex stuff. For large scale data processing, not so much as far as I know. I'd like to know for example what kind of stuff Jane Street does related to this matter apart from trading code.
You got me all excited that it was an example of how to use it. Then I stared for about 40 seconds. Then I laughed. Well done.
It's a bit sad that people are still having order-of-magnitude performance problems with Haskell in this day and age. I wish there was a big collection of slower-than-Python Haskell code I could look through so I could write a blog post about how to automatically fix the three biggest problems. Unfortunately, I just don't know what problems people are running into, besides building up a big chain of thunks in recursive calls, and materialising lists when they didn't expect to. EDIT: This was supposed to be a reply to https://www.reddit.com/r/haskell/comments/6h6h8b/rust_as_a_gateway_drug_to_haskell/diw2lus/. The HN thread is full of people reporting poor experiences with Haskell performance.
I'm aware of the existence of attempts at [Lisp-flavoured Haskells](https://wiki.haskell.org/Haskell_Lisp#Haskell_with_Lisp_Syntax). But you appear to suggest S-Expressions to be used as an intermediate syntax representation, while keeping the more convenient surface syntax we like about Haskell. But such a construction doesn't seem very convenient to me if you want to implement `ghc-exactprint` with the goal of refactoring the surface language; even more so if you wanted to support multiple surface languages mapping to the s-expression IR.
Take this with a block of salt to stay sane lol.
This gotcha is so real it hurts.
Nice to see I'm not alone. I also use my own complex typeclass that has similar functionality. I work in numerical linear algebra and my own motivation is that most matrix algorithms can have the same implementation for real matrices and complex matrices if you interpret transposition as conjugate transposition and add on the occasional scalar conjugation, where conjugation does nothing on real types. People give implementations of algorithms that work for both real and complex matrices in Matlab all the time, but the type of `conjugate` from `Data.Complex` prevents this in Haskell unless you go with your own complex typeclass like your `ComplexLike` that includes conjugation. It sort of puts you back to what Fortran does, where you end up writing two versions of every algorithm, one for real and one for complex. It's kind of embarrassing to do that in a language with such a wonderfully flexible type system. I don't know how attached you are to having the constant `i` in `ComplexLike`, but if you drop it, then real instances also make sense: instance ComplexLike Double where conjugate = id real = id imaginary _ = 0 That's similar to what I do. But then, I have pretty specific needs and don't usually need to represent `i` as a constant except at the end when I construct the matrices on which my program runs with a concrete numeric type of `Double` or `Complex Double`. 
Although it doesn't use s-expressions, [Core](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/CoreSynType) is quite a bit smaller than GHC Haskell. In terms of Lisp-like Haskells, I think [Hackett](https://github.com/lexi-lambda/hackett) is the current best choice. ([Shen](http://shenlanguage.org/) might be the best Haskell-like Lisp.) 
I think that map-reduce is not the final world in big data analysis. That model is a general way to distribute computation explicitly at the cost of having to reformulate the problem in terms of that model. a map-reduce formula is about how to partition the problem, compute each part and join the results. This is not particularly elegant. That formula is not done in terms of the problem itself, but in terms of an abstract cluster of nodes: the hardware. That reformulation of the problem is the harder part and the speaker in the video tells about the difficulties of that process. It happens also in tensorflow. This hard step should be done by the software itself. The programmer should simply introduce the pure maps and folds and tensor operations in terms of the particular problem as they appear in the books. I think that this is more close to be achievable in Haskell. 
None of the examples AFAICT are consequences of implicits. Point 2 (TDNR) is a separate overloading mechanism, which I do sometimes wish for in Haskell, in addition to type classes.
* Pervasive laziness * Globally unique instances * Open type families
Thank you, but I can't get your snippets to type check (in my head at least) against the types I see in `Data.Reflection`. For example, `Reifies` takes two types as arguments, and you've provided one. Also, you're passing a function as the first argument to `reify`, whereas I _think_ you meant that as the second argument. Would it be possible to share a self-contained example? __EDIT__ To try and motivate it a bit, here is an example where I'd use `MonadReader`, and how I implemented it with reflection (both via `Reifies` and `Given`). Perhaps an argument could be made that `Given` is more elegant, but in this case `Reifies` is clearly worse IMO, since explicitly passing in the argument would be less work. #!/usr/bin/env stack -- stack --resolver lts-8.12 script {-# LANGUAGE FlexibleContexts #-} import Data.Reflection import Control.Monad.Reader data Config = Config { configLog :: String -&gt; IO () } doSomething1 :: Reifies s Config =&gt; proxy s -&gt; IO () doSomething1 p = configLog (reflect p) "doSomething1" doSomething2 :: Given Config =&gt; IO () doSomething2 = configLog given "doSomething2" doSomething3 :: (MonadReader Config m, MonadIO m) =&gt; m () doSomething3 = do c &lt;- ask liftIO $ configLog c "doSomething3" main :: IO () main = do reify config doSomething1 give config doSomething2 runReaderT doSomething3 config where config = Config { configLog = putStrLn } __EDIT2__ /u/chrisdoner shared an example from [a blog post by John Wiegley](http://newartisans.com/2017/02/a-case-of-reflection/) that demonstrates a case where reflection works and other approaches won't, namely sneaking an extra value into a typeclass method. That makes me think that reflection is intended for a much narrower use case than `ReaderT`. {-# LANGUAGE ScopedTypeVariables #-} {-# LANGUAGE UndecidableInstances #-} module Main where import Data.Proxy import Data.Reflection import Test.QuickCheck.Arbitrary import Test.QuickCheck.Gen import System.Environment data Foo s = Foo [Int] [String] deriving Show instance Reifies s Int =&gt; Arbitrary (Foo s) where arbitrary = do xs &lt;- listOf chooseAny len &lt;- choose (1, reflect (Proxy :: Proxy s)) ys &lt;- vectorOf len (shuffle "Hello, world") return $ Foo xs ys main :: IO () main = do [len] &lt;- getArgs reify (read len :: Int) $ \(Proxy :: Proxy s) -&gt; print =&lt;&lt; generate (arbitrary :: Gen (Foo s)) 
Yeah, `given` seems to be a lot nicer than `Reifies` here, and plays better with my (bad) understanding of the library.
&gt; The Haskell analogue is, in turn, based on typeclasses, though the specifics can be a little bit trickier: -- Typeclass definition class Draw a where draw :: a -&gt; IO () -- Polymorphic wrapper type data Draw' = forall a. Draw a =&gt; Draw' a instance Draw Draw' where draw (Draw' d) = draw d -- Data types instantiating ("implementing") the typeclass data Circle = Circle () instance Draw Circle where draw = undefined -- omitted data Square = Square () instance Draw Square where draw = undefined -- omitted &gt; Here, the generic function can use typeclass constraints directly ((Draw a) =&gt; ...), but creating a container of different object types requires a polymorphic wrapper. That's some terrible Haskell code the author is presenting here. Why not `type Draw = a -&gt; IO ()` or an ADT? Where do people learn to write code like this?
Also correct me if I'm wrong. But aren't functions like. `traverse`, `replicateM`, `filterM`, `^`, and things like monad transformers and lenses, just straight up impossible to define. As they are polymorphic functions that internally call other polymorphic functions, and therefore you won't be able to resolve at compile time which instance of the internal overloaded function to use, without something like typeclasses to help out. By impossible to define I mean you can't define them generically. You have to define them once for every type you use them on. Or for things like `traverse` you have to define it once for every pair of types you use it on, which just sounds brutal. And once you decide you want typeclasses, then adding in overloads on top of typeclasses gives minimal benefit for substantial cost. Since you can't define anything new that you couldn't have already defined with type families / multi parameter type classes, and you lose local reasoning and type inference becomes extremely hard, probably even undecidable.
I don't really know. Maybe people view laziness as so stereotypically bad for performance they just bail at the first difficulty.
I disagree. Laziness by default leads to bugs and space leaks that can be very difficult to track back to the source. Strictness by default surfaces errors at the actual call site. I see it as a similar benefit to static typing: errors are caught sooner instead of being propagated deeply into your program before blowing up. Opt in laziness means you can reap any benefits of laziness that you want, without the misery of hunting down the points in your code that are accidentally too lazy.
But that assumes you care about proofs and should make sure you provide them. I was referring to the argument that Haskell is as usable while complex as it can get, because Proofs complicate the type level and the exposed API, making it harder for callers to just toy around with a function. In these cases it's perfectly fine to even have partial functions, so we should be able to tell the compiler that we are OK.
&gt; Where do people learn to write code like this? Very good question. I think we should ask.
Extensible records are an attempt to solve the problem of monad transformers . But as you say it does not solve all the problems of fat state with complicated structures and ad hoc syntax which probably would be lensified to fill everithing with strange operators. All of that pain for using some counter in a local calculation? It does not worth the pain. In that case a local state transformer is less painful. Instead, a map or a hashTable indexed by type is simple: usesAny :: forall m e. Typeable e =&gt; StateT (Data.Map TypeRep (MyDyn e)) m (Maybe Result) I can put and get anything typeable in the map. Rather than use long typed guarantees, simply make sure that the computation receive an initial value when there is nothing of that type in the map. Then you don't need the types to assure that you get a value. Since getx makes a lookup it can return Nothing (empty). Use alternative to assign an initial value: putx x ... x &lt;- getx &lt;|&gt; return initx ... del x This is as casual and lightweight that can be used for any purpose. This is drawn from the package "transient". Yes it is not rocket science. it does not need sexy operators and it doesn't worth a paper It does not advance the cause of dependent types neither new extensions. Coding this may be 10 lines of standard Haskell. Hard time for cool kids
Do you really need lazy lists for this though? I'd think most use cases would be covered via an iteration type class (which likely already exists, but I don't know of it): class Iterable t where iterate :: (Int -&gt; a -&gt; b -&gt; a) -&gt; a -&gt; t b -&gt; a instance Iterable [] where iterate = iterate' 0 where iterate' _ _ z [] = z iterate' i f z (x : xs) = iterate' (i + 1) f (f i z x) xs -- [(0,'h'),(1,'e'),(2,'l'),(3,'l'),(4,'o')] main = putStrLn $ show $ reverse $ iterate (\i z x -&gt; (i, x) : z) [] "hello"
I'm so much happier with even the worst approaches in Haskell than I am with the Best Practices of Ruby/PHP/JavaScript/etc. It probably just matters less that we use the Best Practices since our waterline for correctness/refactorability is so high 
You would probably enjoy my [`hash-rekt`](https://github.com/parsonsmatt/hash-rekt/blob/master/src/Data/HashRecord/Internal.hs) library, which provides an extensible-records implementation that's backed by a `HashMap String Dynamic`. `lookup @"foo" somRec` is a fully type safe operation which, given some `someRec :: HashRecord ["foo" =: Int]`, returns the `Int` contained. `insert @"bar" 'a' someRec` provides you `HashRecord ["bar" =: Char, "foo" =: Int]`. You usually don't want `Map TypeRep Dynamic` as you'll likely want a) names and b) disparate things of the same type.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [parsonsmatt/hash-rekt/.../**Internal.hs** (master → 6bdc6dd)](https://github.com/parsonsmatt/hash-rekt/blob/6bdc6ddcbd3aa12b63c2090eb595a535da7f352f/src/Data/HashRecord/Internal.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply diwf345.)^.
&gt; Where do people learn to write code like this? I doubt anybody is actually teaching people to do this; rather, my guess is that people see the `class` keyword, assume it does something similar to classes in OO languages, and then that initial miscomprehension taints all their future encounters with documentation, examples, etc., until they encounter an article telling them not to do that.
A monad with alternative instance and initial values shallow the Maybe. Using types for lookup makes getting and setting values trivial and parameterless, just like `set` and `get` on the state monad, but in this case, set and get are polymorphic. For different data, different newtypes.
Something I would like to point out is that you can admit certain properties without really giving any formal proofs for it. So, if you just want to verify some high level details, that is certainly doable.
After having used spark for a few years, I'm not sure it's such a big deal. Basically the AWS setup with S3 + Spark at least is a huge waste of time: - S3 throughput is abysomal - For most problems, just hire one of the huge machines that are now available on AWS. It's *much* *much* faster. - Keeping clusters either using EMR or your own cluster is just too much busywork. KISS is better.
From the definition, this looks like half a solution, not what `hpack` does. Where do I declare my common definition stanzas across all of my cabal files?
We're forced to deal with lists when using parsers, say: ``` many $ string "foo" ```
&gt; negate, abs or signup methods I think your spell checker got the better of you here. The `Num` method is `signum`, not signup.
That wouldn't really enforce them, but just make it more likely that a non-law-biding instance would show surprising runtime behavior.
TDD with Idris uses both `total` and `Lazy` when discussing recursive view types and covering functions. It's not uncommon.
I agree with you, but I like that Idris actually differentiates between least and greatest fixed-points, something that's difficult to do in Haskell.
&gt; Agda feels much more mathematically inclined and proof-oriented than Idris The feeling I get from the Idris community is less about doing math, and more about writing useful programs, but using dependent types to guarantee various properties (sometimes correctness, sometimes other things). That's not to say there's isn't some interesting CompSci research done using Idris, but even that feels less abstract than some of the Agda code I'm seeing.
Umm... I wonder why this comment was downvoted? I took it as cheerful sarcasm.
https://github.com/haskell/vector/commits/master/Data/Vector/Unboxed/Base.hs probably because it's done in 2010 :)
It might sound minor, but the sound quality is making the listening experience unnecessary annoying. Sounds like a recored skype call, e.g. horrible.
You can rename IPs, right? `let ?config = ?configA in ...` or some such. (I remember that not working as expected in GHC vOLD and John Hughes pointing it out in a paper.) I suspect that'd be sufficient for many (most? enough? project dependent) compositions to be manageable. As long as the IPs have *un*unifiable types, which seems easy/natural to guarantee in practice.
I've yet to see slower than Python haskell code. Unless of course someone is trying to read large text files into String. 
I would like to hear more about these ‘really interesting questions’ asked by students learning Dependent Haskell who didn't already know Haskell. I'm sure there's a lot of insight there
How are those 20 modules related to each other? How does the dependency graph look like? Do those 20 libraries/executables logically constitute a partitioned big package, like e.g. the `amazonka-*` libraries? 
&gt; 1) Idris Strings are not lists I agree that having `[Char]` be the default string is bad. IMO `OverloadedStrings` enabled by default is the way to go. Works just fine with numbers, and we can maybe add some type defaulting rules around it for now, I do wish there was a more principled form of type defaulting available. Honestly I would prefer `OverloadedStrings` over any one implementation of strings, `[Char]` is pretty nice when messing around or when performance isn't that crucial, as linked lists in general are really fun to work with in functional languages, due to pattern matching and all the functions people have already defined that work on them. &gt; 2) Overloading without Type Classes I prefer principal types (seriously, principal types are fantastic) and only one form of overloading personally. I like that `&gt;&gt;` does not mean both bitshift and piping stuff to output. Plus in situations where you really do want one thing to mean a few fairly different things, you can use type families and multiparamtypeclasses to make it work. Plus correct me if I'm wrong, but type inference / deciding the overload to use in the presence of both overloading and typeclasses sounds like it will fail most of the time if at least one of the options involves typeclasses in some way. It seems like its only really going to work ok on monomorphic-ish functions like `Set.fromList`, `Data.List.sort` etc. and records. &gt; 3) Records fields are namespaced Namespacing and modules in Haskell right now are not great, being able to define namespaces / modules within a file, and modules being more first class in general, would be wonderful. For example when you think you need a global mutable (`IORef` or similar), instead of using `unsafePerformIO`, you make it a parameter of the modules that use it, and have `main` create it and import all the modules within the main function. You could also basically implement backpack (or a lot of it, I'm not a backpack expert, but at least some of the examples I have been given) in standard Haskell, by making the methods and types and such needed be module parameters. Then you just have to deal with dependency-resolution side of backpack. &gt; 4) Records update and access syntax Kinda cool I guess. IMO it would be cool if Haskell just did away with record syntax altogether, and then made defining data with records create lenses. I also have a potential solution to the issue of colliding record names involving this. If we made string literals work in a type level context, `-XDataKinds` style. Then we can have the function `record :: forall (r :: String). Has r s =&gt; Lens s s (Record r s) (Record r s)`, and to get a lens on "foo" we can just call `record @"foo"`. With a bit more thought this can be extended to a lens that is allowed to change the type of the parent, in case you have something like `Foo a ... { ... foo :: a ... }` where `foo` is the only thing of type `a`. &gt; 5) Monad and Functor got fixed Yeah, I do wish Haskell would hurry up and finish the job on that one, I don't like `fail` and `return`. And I do wish `fmap` was called `map`, particularly since we already went ahead and did that generalization for all the `Foldable` functions. On a side note, `intercalate` should have type `(Monoid a, Foldable t) =&gt; a -&gt; t a -&gt; a`. &gt; 6) Laziness is opt-in, not opt-out I think opt-out laziness is better, I have argued this in depth plenty of times before, and there are lots of articles and such on the topic. So I won't rehash it too much here. But basically, maintaining lazyness in a generally strict environment is almost impossible, but getting the necessary strictness to avoid a space leak in a lazy environment is much much easier. And then laziness is much more composable (you don't need to manually fuse functions very often, with strictness you almost always do) and without it many algorithms NEED to be asymptotically slower by a log factor in an immutable language. Also strictness is usually death by a thousand cuts, but laziness is just one space leak every once in a while (they really aren't common, at least not in my experience). &gt; 7) A smaller Num class Please Haskell, break everything up into ring and group-like structures, and also do the same stuff for ordering, lattices and such and bringing Bounded and Enum into the ordering hierarchy (enum should imply a total ordering dammit, and bounded should imply at least a partial ordering). [Here](https://www.reddit.com/r/haskell/comments/6h64ue/10_pleasant_surprises_of_idris_that_improve_the/diwix1e/) is my much longer comment on this. &gt; 8) The Cast type class The default stuff is stupid, but otherwise seems neat. Could be added to Haskell pretty easily. Not sure what laws and such you would have around that, (since many of these conversions will be lossy), maybe something like idempotency of casting round trips, so converting `a -&gt; b -&gt; a` is the same as converting `a -&gt; b -&gt; a -&gt; b -&gt; a`. &gt; 9) Clear separation of evaluation and execution Eh, so so on that one. Would prefer something like `:inspect` that can explicitly create ASTs. I pretty much always want to just execute the action. Although you have a fair point that there is an asymmetry between calling `show` on values and executing IO actions, and both are invoked the same way, but there is also an asymmetry between calling `show` on values and printing the AST of IO actions. So maybe when you do `print "foo"` it should just say "no instance of Show ...", and then add in `:inspect` and `:exec`, as well as appropriate shorthands. You could also just make this kind of thing highly configurable, I could see people wanting automatic `show`ing, AST inspection, execution, as well as evaluating to NF and showing the true internal structure instead of calling show, so you can view what a function has been evaluated down to. &gt; 10) The Do notation without monads `-XRebindableSyntax` solves that one, but yeah generally I would like it if as much of Haskell's syntax as possible was rebindable. Even stuff like using your own `Bool` type in guards would be IMO kind of cool, even if it would almost never actually be used.
Also, one should be careful that the streaming method isn't placing pressure on other database clients, due to potentially holding connections open longer if there is a complex computation involving the data in the stream.
[@snoyberg's latest tweet](http://i.imgur.com/jwYehTH.jpg) [@snoyberg on Twitter](https://twitter.com/snoyberg) - ^i ^am ^a ^bot ^| ^[feedback](https://www.reddit.com/message/compose/?to=twinkiac)
&gt; That's some terrible Haskell code the author is presenting here. &gt; &gt; Why not type Draw = a -&gt; IO () or an ADT? Where do people learn to write code like this? Wow. That's harsh. Yes, it's an illustration of a well-known anti-pattern, but there are legitimate reasons to structure your code this way. Specifically, if there is only logical one way to draw a particular type, a type class will enforce this invariant. I'd still personally use a higher-order function as you suggest but there are friendlier ways of getting the point across. Here's a blog post that may interest you, OP: https://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/
&gt; Globally unique instances Haskell doesn't have this. &gt; Open type families Depending on what you are going for, you can emulate this in Idris. You don't get the nice Haskell syntax. Since types are first-class in Idris, this is effectively just function with a `Maybe Type` result (or more likely `Maybe (t : Type ** p t)`).
Hey there! Just posting a link to the first blog post about the HSOC project I took on. I'll be posting updates (somewhat) regularly to the blog; if there's interest, I'll cross-post them here as well. Feel free to AMA in general or about the project.
I'm a bit confused: * If you have lots of data you should probably use the DB to perform the computation. * Why would you want to print out 100.000 (7m) values? That's not really reflecting the real problem. 
The problem only arises because he used a let binding and gave it a name, right? 
1) Your type assures only that the global data has a HOLE for your data. It does not guarantee that the data has been initialized. 2) A map/hashtable with a initial value in an alternative expression assures that you have booth: The hole for your data and an the initialization. It is semantically similar to a state transformer, which needs and initialization. 3) once used, with the second aternative you can get rid of it and make the state sleek and fast, only with the payload necessary.
Problem I see there is that some casts are guaranteed to succeed while others aren't. Eg String-&gt;Int vs Int-&gt;String. Forcing the second to use maybe would very quickly annoy me. Maybe some kind of class hierarchy for casts. 
Could we get more moderation on /r/haskell? I'm guessing a couple thousand people read this subreddit -- it's a waste of time for them to all see spam submissions.
Thanks for the feedback. I tried a new editing process for this one and thought that it had given a better result, but it's hard to be objective after listening to the audio repeatedly during editing.
Caveat-ridden SO answers about simulating OO, I'd guess.
Type classes do enforce a given type signature, and suggest some semantics (and enforce some semantics via parametricity). It's not perfect, but it does provide a lot more information than ad-hoc overloading.
Thanks!
Which is just static duck typing
&gt; If you have lots of data you should probably use the DB to perform the computation. You might need to perform correlation that's either hard or really expensive to do in base. I think there are many reasons for handling a lot of data out of the database.
Sort of, but more precisely the problem only arises because it has been given a let binding and *reused*.
But why does this list need to be lazy?
&gt; By this definition example1 definitely contains a space leak. I think this is predicated on the expectation that lists can behave as a range abstraction, which they fundamentally do not. I do not think this is unexpected. &gt; There should be no such thing as a "flawed question"! That's fair; calling it flawed may have been a little flawed =P. But I stand by the claim that "is not needed" is very different from "is not better." So the better statement would be that I don't see that much of a point in the question "is it needed?" &gt; So far I am the only commenter in this thread to provide an example of a Haskell list being used in a a way that's neither ephemeral nor spine-and-value strict, which suggests to me that the question is actually really interesting. `take 10 . sort` having an optimized asymptotic time would be a classic example. Basically, I think there are two main things laziness is useful for: Value level recursion (think `fix`, where `a` is not a function type), and short circuiting evaluation. I find little instances of these with lists somewhat often.
But that is not guaranteed by the type. It is guaranteed by the monad. And you enter in the initial problems: big fat state, initialized in a centrar location, impossible to compose, or a lot of ugly monad transformer boilerplate everywhere. If you have to initialize it anyway, do it locally where it is relevant, get rid of the type, that assures nothing, and do it in the less verbose and boilerplatey way possible. There are more interesting things to do... I think that Haskell is dominated by a cargo-cult mentality that throw every paraphernalia possible to the problems trying to make something big enough to produce a paper or a package or a project instead of looking for a solution. Everyone is looking for the next big thing, the next "monad". That would not be bad if the problem were worth the effort, but amazingly, the effort is concentrated in trivial things like how to write pretty getters and setters and loops, how to store and retrieve. That is insane and deleterious.
&gt; &gt; By this definition example1 definitely contains a space leak. &gt; I think this is predicated on the expectation that lists can behave as a range abstraction, which they fundamentally do not. I do not think this is unexpected. In this thread you will find /u/chrisdoner, a rather experienced and senior Haskeller, admitting to using `zip [0 ..]` as a means of enumeration, so the expectation that lists behave as a range abstraction is pretty common in the Haskell community. [Here's an example claiming that "Haskell uses lists as a control flow structure"](https://www.reddit.com/r/haskell/comments/1q0jsj/why_lists/cd7xzmu/). That is a pretty common refrain. I think it's your expectation that is not aligned with the larger Haskell community. &gt; `take 10 . sort` having an optimized asymptotic time would be a classic example. OK, good example, thanks. &gt; I find little instances of these with lists somewhat often. More examples would be welcome then! &gt; Value level recursion (think fix, where a is not a function type), and short circuiting evaluation. I think it's worth pointing out that short circuiting evaluation is a result of lazy function application, not of lazy datastructures.
I consider the export syntax to be one of the best things. You don't repeat all exported identifiers at the top of a module, instead you write 'export' before them. 
I'm not the author, but I feel like at some point I've had the kind of confusion that might've lead to that kind of code, so I'll take a stab at it: - People coming from OOP often don't know what to do where they'd usually use inheritance. This tends to come up early on, before they're comfortable with the language, like the first time they think they need to do something to a list of differently-typed items. Maybe they reach for typeclasses, or maybe they reach for HList. - Being used to strict languages with no sum types, they prematurely rule out "just apply a function/constructor" - It's not just that typeclasses look like an answer, some things at first glance appear to reinforce that notion that they're the right answer. Let's say I'm writing a library function, and I want the caller to be able to give me a list of shapes, of which I'll pick one to draw. In OOP, I'd have them pass a list of objects that implement a `Drawable` interface. To channel my younger self, I'm tempted to reach for typeclasses because: - What if I want my users to be able to define new shapes? I clearly need an open sum/interface. Isn't that what a typeclass is? - Why would I want to force someone to explicitly cast their `Circle` to `Drawable` to call `draw`? That's not a very user-friendly interface. How do I let users just call `draw` on something of type `Circle` directly? - Why would I want the caller to pre-emptively cast the whole list? It doesn't make sense to do all that work if I'll only be drawing one item. - What if I want to do introspection and call a class-specific method? If they get casted before being passed, they must be _losing_ information, including the original type information and access to the type-specific methods. Sometimes I just need to be able to break encapsulation, just like I need unconstrained mutation. - It's a "draw" method - that seems to be a direct analogy to `Show`. How does `Show` work? - I've heard you can pick a different instance by using `newtype`, kind of like overriding methods of a superclass - Typeclass laws aren't enforced - adherance to them seems to be just a convention/light suggestion I feel like I can respond to those points, but I'm still not sure I could bring someone from that point of confusion to a place where they'd feel _confident_ in their understanding of when to use or not use typeclasses, or how to best design an interface for a library, so I'm curious to hear your thoughts (particularly since I'll likely need to bring a few strong OOP programmers with no functional background up to speed on Haskell very soon).
I can't speak for /u/chrisdoner, but I think you're making rather a broad assumption about "the larger Haskell community" based on only few examples of people deliberately talking about only a subset of the usage of lists. Lists are very clearly a data structure, and expecting them not to materialize when you use them twice is clearly naive. You definitely *can* use lists as generators/processors/whatever, but this is neither the main purpose of `[]` nor the main usage. &gt; I think it's worth pointing out that short circuiting evaluation is a result of lazy function application, not of lazy datastructures. `take k` is a good example of short circuiting evaluation via lazy data rather than lazy functions.
Really? That's a bummer. I hoped that it might. Is that the same for the PostgreSQL and SQLite drivers, or are they streaming more memory-efficiently? And even if those don't stream either, could it be possible to construct a lazy stream of concatenated sub-queries like the following? ... &lt;&gt; f "... LIMIT 100 OFFSET 200" &lt;&gt; f "... LIMIT 100 OFFSET 300" &lt;&gt; ...
When you need to "try" things and return the first thing working which is usually done like this in imperative language if a: return a' if b: return b' if c: return c' Can be done with lazy list. For example here is my code to try to parse date using different format newtype AllFormatsDay = AllFormatsDay {allFormatsDay :: Day} deriving Show instance Csv.FromField AllFormatsDay where parseField bs = do str &lt;- Csv.parseField bs case concat [parseTimeM True defaultTimeLocale f str | f &lt;- formats] of [day] -&gt; return $ AllFormatsDay day (d:ds) -&gt; error (show (d:ds)) _ -&gt; mzero where -- formats to try. Normally there shouldn't be any overlap bettween the different format. -- The 0 in %0Y is important. It guarantes that only 4 digits are accepted. -- without 11/01/01 will be parsed by %Y/%m/%d and %d/%m/%y formats = [ "%0Y/%m/%d" , "%d/%m/%0Y" , "%d/%m/%y" , "%0Y-%m-%d" , "%d %b %0Y" , "%d-%b-%0Y" , "%d %b %y" , "%d-%b-%y" , "%0Y %b %d" , "%0Y-%b-%d" , "%a %d %b %0Y" ] (Ok it might not be the best example, as the error message uses the full list, but you get the idea). Things like that could probably be converted to a case expression, but in this particular case (no pun intended) having a list has many advantage. The list of the different formats could be reused if needed to be displayed to the user (in case of error) for example. It also can comes from a configuration file. 
&gt; You might need to perform correlation that's either hard or really expensive to do in base. Could you give an example that is more complex than zipping the current value with the previous one? *By the way:* Even that is possible efficiently if the database is aware of a certain order, i.e. a join of *row numbers* is implemented without actually joining. &gt; I think there are many reasons for handling a lot of data out of the database. Could you name some?
Well, I have a "bad" example, where log-like data is stored in a relational database. Each row describes a state change to a particular item, and I need to get the state of collection of items at a particular time. It is possible to write that in PS/SQL, but it is much less error-prone and much more pleasant in Haskell.
Right. Unfortunately it doesn't scale well, for two reasons. * You need to manually write `Unbox`instances for types not already covered. A smaller nuisance perhaps. * The approach completely breaks down for containers with two type parameters (e.g. `Map k v`) because the programmer needs to write O(n^2 ) instances. That's hundreds of instances just for the basic scalar types in base, not to mention pairs, records, and the like. The only approach I've seen that works well here is what e.g. C++ does: have the compiler generate a specialization automatically when it sees a use site. We already kinda do this for functions with `INLINABLE` (although not in an entirely reliable manner), but we have nothing for data types yet.
I'm not forcing anyone to use a specific set of tools. But you made a claim and I want to understand your reasons. If you can't give a single one then I won't be convinced. In that case your *opinion* is really not worth much (no offense).
&gt; There's just a bunch of stuff you can do lazily that you can't do strictly nearly as nicely So I've already called out lazy cons lists and infinite tries. What else do you have in mind?
&gt; You are merely saying doing it any other way is wrong, I never said that. That's probably what you heard. &gt; and put the burden of convincing you otherwise on the whole world. *In epistemology, the burden of proof [..] is the obligation on a party in a dispute to provide sufficient warrant for their position.*
&gt; I don't get to choose how the data is stored. If your answer is "then your architecture is stupid", you are not helping. I don't like when people put words in my mouth. &gt; Also, there are many item, but a limited amount of state changes per item, so this will work with tons of data. I guess we should define things before we start arguing about details (size, requests per second, relational model).
I think you missed my point: list is meant to be produced and consumed lazily. In another word, it's meant to be processed in constant space no matter fused or not: code like `mapM XXX . fliter YYY . [AAA...BBB]` is meant to be running at constant space, and strict list will simply blow the heap. In case of sharing things get complicated. It's up to programmer to decide what is cheap and what is expensive, compiler often make mistake here and there. I merely want to show some tools to help. And i don't see how these questions of time vs space and sharing vs duplicated work will disappear in strict settings. It became even more complicated in case of IO involved: we meant to extend lazily/constant space behavior to IO operations, but doing this will challenge resource/exception safety. So we choose chunked IO with effect label on the result overtime. But this dosen't mean lazy list is useless: it just doesn't fit in cache IO input. In fact since we are already inside IO, we can just use vector/array to cache input. From there on, list could do a good job on processing.
I think it is a bit of a hack to coarsely categorise a type parameter as *nominal*, as a `Map X` should be coercable to `Map Y` if `Y` inherits the same `Ord` semantics from `X` (i.e. by deriving it by newtype rules). But surely /u/goldfirere considered (and rejected) this in their paper.
True, but if you have `cast :: a -&gt; Maybe b` then it's trivial to define `forceCast :: a -&gt; b` in terms of it, since no information is lost. The same cannot be said if failure is encoded as a default value.
Maybe I did miss your point. Now I think you're talking about the same thing Lennart Augustsson wrote about, which I linked in [my other reply](https://www.reddit.com/r/haskell/comments/6h84vg/when_do_we_really_need_lazy_lists/dixkt23/).
As a point of terminology, doesn't "laziness" always imply memoization, and therefore there's no such thing as "call-by-name laziness"?
Hi Jared, I'm really glad you are taking that on! Just a thought: are you sure "git notes" are the good way to go? For sure test numbers needs to be associated with each commits, but IMO it would be cleaner to put them in a separate repo.
The big drawback of this approach, IMO, is that you have to infect your data definitions with these `HasRTTI` constraints.
Could be implemented as extensions of `Fn`, ala [Trees That Grow](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/trees-that-grow.pdf)
A dynamic map has only or can have only the data that is necessary at each moment in the computation. &gt;You're saying that the best way to program in Haskell is to use Python instead. Exactly: That is what real programmers have decided after 20 years of lost opportunities: to use Python, javascript, clojure, scala, fsharp. Despite having the best language, the haskell community instead of creating an easier and welcoming ecosystem has been working hardly in finding contrived problems for trivial solutions like state management, that is a solution, not a problem, because it is trivial in every language. &gt;Thanks for the discussion. Back to the Ivory tower? Be careful with the stairs ;) I knew that from the beginning. The cool kids would never ever accept anything less than a "solution" with five extensions, 10 papers and 100 blog posts for managing an Integer state. 
I'm just not seeing your point. I'm not disagreeing that we sometimes use lists for streaming. In fact, I'm saying this is a nice feature of lists. I'm just saying that you can't *always* expect lists to behave as a streaming abstraction, since it is memorized, which I think everyone understands.
**Lazy evaluation** In programming language theory, lazy evaluation, or call-by-need is an evaluation strategy which delays the evaluation of an expression until its value is needed (non-strict evaluation) and which also avoids repeated evaluations (sharing). The sharing can reduce the running time of certain functions by an exponential factor over other non-strict evaluation strategies, such as call-by-name. The benefits of lazy evaluation include: The ability to define control flow (structures) as abstractions instead of primitives. The ability to define potentially infinite data structures. This allows for more straightforward implementation of some algorithms. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^] ^Downvote ^to ^remove ^| ^v0.21
Believe me I know =P But I've had this conversation with Scala people enough to know that a lot of people see things differently.
With `Typeable`, you cannot express the constraint that in `Val a`, the parameter `a` will only be instantiated with `Int` or `Double`. 
To be more concrete: the Typeable solution is import Data.Typeable instance Typeable a =&gt; Binary (Val a) where put (VI i) = put i put (VD d) = put d get = do case eqT @a @Int of Just Refl -&gt; VI &lt;$&gt; get Nothing -&gt; case eqT @a @Double of Just Refl -&gt; VD &lt;$&gt; get Nothing -&gt; error "uninhabited" Meaning we can call `get :: Get (Val String)` and get a runtime error, whereas with instance HasRTTI Val a =&gt; Binary (Val a) where put = putVal get = getVal rtti we'll get a compile time error because the `HasRTTI Val String` constraint is not satisfied.
&gt; I wonder if we have the same idea of what a “large”, or “important” team is. Or lots of money, for that matter. From the rest of your comment I can confidently say we do not have the same idea. &gt; What I'd like to hear about is the costs and benefits of applying haskell to person-century sized projects ... Standard Chartered might just sneak through your requirements barrier. Facebook might get there in a few years. Otherwise I think there is no example of Haskell being used at the scale you are requesting. &gt; Meanwhile, the people who are hanging around here—and similar venues—could be a bit more…cognizant of how small their world can look. Sure, they could be. But it's their backyard not yours. If you want something from someone high up you've got to pay the entry fee. May I ask what person-century sized projects you're responsible for? Do you think that such projects constitute the bulk of software development performed commercially (by any metric you choose)?
I'm pretty sure that languages like Coq can prove things like this.
The [Hume](http://www.hume-lang.org/) family of languages has these 5 layers: Full Hume * Full recursion PR−Hume * Primitive Recursive functions * Inductive data structures Template−Hume * Predefined higher−order functions * Inductive data structures FSM−Hume * Non−recursive first−order functions * Non−recursive data structures HW−Hume * No functions * Non−recursive data structures 
&gt; And if the language supports infinite data structures, it's possible to define a "Delay Monad" within which non-terminating recursion is allowed. Even without infinite structures, you can still *represent* programs using general recursion by using the following datatype: data GenRec (A : Set) (B : A -&gt; Set) (X : Set) : Set where pure : X -&gt; GenRec A B X ask : (a : A) -&gt; (B a -&gt; GenRec A B X) -&gt; GenRec A B X Pi : (A : Set) (B : A -&gt; Set) → Set Pi A B = (a : A) → GenRec A B (B a) infix 2 _⁇ _⁇ : {A : Set} {B : A → Set} → Pi A B a ⁇ = ask a pure infixr 1 _&gt;&gt;=_ _&gt;&gt;=_ : {A : Set} {B : A → Set} {X Y : Set} → GenRec A B X → (X → GenRec A B Y) → GenRec A B Y pure x &gt;&gt;= f = f x ask a prg &gt;&gt;= f = ask a (λ b → prg b &gt;&gt;= f) You can then write the code of your program. E.g.: first : (P : ℕ → Bool) → Pi ℕ (\ n -&gt; ∃ λ m → n ≤ m × P m ≡ true) first P n with P n | inspect P n ... | true | [ eq ] = pure (n , ≤-refl , eq) ... | false | _ = suc n ⁇ &gt;&gt;= λ res → pure $ map id (map (≤-trans (n≤1+n n)) id) res Because you have an initial encoding, you can then run them in various ways and get a fuel-based function with a `Maybe` result, one in the delay monad, etc. For more, cf. [Turing-Completeness Totally Free](https://link.springer.com/chapter/10.1007/978-3-319-19797-5_13)
Another way to enforce them would simply be to have people file bug reports whenever they see an instance that doesn't obey the laws, perhaps using quickCheck to help out or even somewhat automate that process.
Any type with non-structural equality (e.g. you want to store IDs and compare by IDs, without comparing the rest of the data structure) breaks `Eq`. HAXL breaks `ap` = `(&lt;*&gt;)` (admittedly barely). Lens uses law-violating `Traversable` instances internally. `Functor` laws are impossible to break without e.g. `unsafePerformIO`.
I would agree that what matters more is what is easy and useful for the programmers. But there is a pretty strong correlation between the two. Generally something that is hard for the compiler to do, also requires a lot more effort from the user to follow along with, such as interacting with theorem provers or just generally reasoning in ways more akin to what you'd expect from a Math grad student. As opposed to things like type checking which is feasible for the compiler, and usually not too hard for the user to follow along. In an ideal world (where Godel didn't ruin all our hopes and dreams of building the perfect compiler), the compiler would let you assert what ever you want, and it would reason globally and in impressive ways about your code to see if your assertions hold, and if they don't it would point out the line of code that violated them. But in reality we are going to struggle to get anything like that except with things like type checking (where we currently basically do get that, which is just fantastic).
What do you mean? The constraints don't show up in the data definitions, it's something you define on the side, like a Show instance.
&gt; you can't pattern match on types Yeah, this breaks parametricity, among other things. Here's an example of a partial function with domain `Type`: Example : Nat -&gt; (Type -&gt; a) -&gt; (a -&gt; Type) -&gt; Type -&gt; Maybe Type Example Z _ _ t = Just t Example (S Z) lower lift t = Nothing Example (S (S n)) lower lift t = Example n lower lift (lift (lower t))
&gt; &gt; Globally unique instances &gt; Haskell doesn't have this. I mean any time you don't have this you have a compile error, so in all programs that compile you do have this guarantee. Except of course if you mix overlapping instances and orphans, which I do think Haskell should address by explicitly forbidding such a mix (if it doesn't already, haven't tried). But I mean if you allow mixing of a controversial type extension and a despised practice to be a counterexample of the claim that Haskell doesn't have globally unique instances. Then you have to also allow `unsafePerformIO` or `unsafeCoerce` to be a counterexample of the claim that Haskell is type safe.
What about Just (List.foldl' (\i (c, d) -&gt; if c == d then i+1 else i) 0 (Text.zip a b)) ? I ran a couple of benchmarks yesterday and that simple fold+zip was incredibly performant. Just looking at it, I would think a completely fused fold-and-zip would be a tighter loop than what you have, which appears to construct an entire new Text object in memory! Edit: What the hell? I re-ran the exact same benchmarks just now, and now the zip+fold version is one of the slowest ones...
[Here](https://personal.cis.strath.ac.uk/conor.mcbride/TotallyFree.pdf) is a direct link to the PDF of the paper.
Yeah but now that I think about it, you're kind of stuck if you have an existential either way. The constructor is gonna have to know about serialization.
Hey /u/jared--w, how set are you on putting the performance information into git notes? I ask because the discoverability of these notes is extremely low. So low that while I consider myself proficient in version control systems, I had to look up the feature and I have never heard of any project using them. Some questions I have: Do any editors surface these notes in their interface? And what tool(s) exist to pull data from git notes? Will this incur costs on people down the road wanting to process this data in other languages or scripts? e.g.: what would it take to produce something like http://arewefastyet.com/ in say, PHP with the perf data stored in git notes?
You don't need overlapping. Just orphans are enough, IIRC. I'd have to review the example from /u/ezyang again. I don't have any love for unsafePerformIO or any other "escape hatch", but they are a lot harder to insert accidentally compared to using orphans badly.
I write however the hell it comes to me and restructure later :) 
I need to start using typed holes over `undefined` in these situations. They're both easier and more meaningful!
Honestly ~half of these feel like bad ideas.
foldl' does not fuse well I think. 
Worst case execution time is a common question in embedded and hard real time contexts. It's often done on imperative (or synchrone) languages, not functional ones. See [the wikipedia article](https://en.wikipedia.org/wiki/Worst-case_execution_time). It's a static verification technique, not really a typing one. From a typing point on view, it's possible to have type systems which limits the complexity of programs, especially if you consider linear logic (for example "if your program type, it's in P"). I don't remember the exact technical term, but I don't think any actual programming languages uses that.
**Worst-case execution time** The worst-case execution time (WCET) of a computational task is the maximum length of time the task could take to execute on a specific hardware platform. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^] ^Downvote ^to ^remove ^| ^v0.21
Yeah, I'm at an awkward stage right now where I can often compose an expression by building it up piece by piece which I then can't easily read when I come back to it. Then I have to slow way down to understand my own code again even just a short time later.
You might like to look into [Resource Aware ML](http://www.raml.co/).
[Clearly not!](http://i.xkqr.org/scrot_20170615223112.png) Manually fusing with tail recursion and strict accumulators does pretty well, though.
Why isn't `stringLiteralChar` of type `Parser Char`?
Flipping and partially applying composition aren’t the best ways to arrive at readable point-free code. Arrow combinators or the function applicative tend to be nicer: curry $ uncurry (&amp;&amp;) . (all isUpper *** even) curry $ liftA2 (&amp;&amp;) (all isUpper . fst) (even . snd) curry $ all isUpper . fst .&amp;&amp;. even . snd (.&amp;&amp;.) = liftA2 (&amp;&amp;) infixr 3 .&amp;&amp;. 
I kept trying and changing the code, there is no actual reason for it to be: `Parser (Maybe Char)` At some point, I thought the parser should consume the apostrophe and check if there is a following apostrophe. If it was anything else the parser would return `Nothing`, it actually did this, but the side-effect is: the string `"'abc'def'"` parses to: `"abcdef"`, which is wrong (should throw an error). That's probably the wrong path to follow...
 quoted :: Parser String quoted = between (char '\'') (char '\'') (many (try escaped &lt;|&gt; normalChar)) where escaped = '\'' &lt;$ string "''" normalChar = satisfy (/='\'') 
**Implicit computational complexity** Implicit computational complexity (ICC) is a subfield of computational complexity theory that characterizes algorithms by constraints on the way in which they are constructed, without reference to a specific underlying machine model or to explicit bounds on computational resources unlike conventional complexity theory. ICC was developed in the 1990s and employs the techniques of proof theory, substructural logic, model theory and recursion theory to prove bounds on the expressive power of high-level formal languages. ICC is also concerned with the practical realization of functional programming languages, language tools and type theory that can control the resource usage of programs in a formally verifiable sense. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^] ^Downvote ^to ^remove ^| ^v0.21
Got It, Thanks!
A string literal char is either an unquoted character other than apostrophe, or a quoted apostrophe. You need to backtrack (`try`) on apostrophes such that the quoted-apostrophe parser doesn't consume the end-of-string delimiter.
I see no difference in the result unless you're dealing with a type with an unusual Num instance.
In the wild, more code uses orphans than uses unsafePerformIO, independent of compiler settings.
Outermost. I pick a good name, assign it a type, then proceed to write the definition, mostly depth-first. There is, of course, refactoring after I'm finished, or when I've completed a leaf.
Different types, in GHC. `bar :: (Eq a, Num a) =&gt; a -&gt; a -&gt; a`. `bar' :: Num a =&gt; a -&gt; a -&gt; a`. Operationally, `bar` is probably going to do more work, and may be a bit more strict. Though, either of these differences could be subverted depending on the specific `Num`/`Eq` instances.
bar will typically be a little bit *less* strict, though one of the guards is unnecessary and doesn't actually help this. Prelude&gt; bar x y | x == 0 = 0 | y == 0 = 0 | otherwise = x * y Prelude&gt; bar 0 undefined 0 Prelude&gt; bar' x y = x * y Prelude&gt; bar' 0 undefined *** Exception: Prelude.undefined It doesn't matter what the Eq instance is like -- in the case where x == 0 holds, y isn't mentioned and won't be pattern matched on. However, the strictness in the case of bar' is down to the multiplication in the Num instance for Integer being strict. However, of course it doesn't work when you swap the arguments: if x is undefined, then the comparison x == 0 will already fail and the test for y == 0 will never occur.
As of megaparsec 4, you don't need to `try` a `string` parser - it backtracks automatically. It doesn't hurt, of course.
ironically, doesn't Arbitrary bend some monad law?
you can google for Standard Chartered's or Facebook's experiences with Haskell. or 
The *point* of TDNR is that you deduce which name to use from the context. Why `user.userInfo.userInfoName` when you can infer it from `user.info.name`? This is *not* going to be generic reasoning.
`cabal-bounds` does that. if the .cabal format parser preserved formatting (like a "cabal-exact-print" package), it would be safer to write the scripts in haskell. but it doesn't, so overwriting nukes your indentation, and (iirc) isn't consistent with the default emacs haskell-mode indentation anyways, so you're thrashing between three styles (the one printed by the library, the ide's, and your own). also, the cabal ast isn't well documented and doesn't have any utilities for manipulation. or education obviously, lenses aren't there for portability, but some utilities that modified the most commonly needed fields. for example, i made a lens for some locations, like _exposedModules :: Lens CabalConfig [FilePath] with a utility that checks if the listed modules are present on the filesystem. but it took a while and i wasn't robust. for example, there's no function that "evaluates" a .cabal by removing all guards. like: evalCabal :: CabalVariables -&gt; ConditionalCabal -&gt; EvaluatedCabal where data CabalVariables = CabalVariables { cOperatintSystem :: String, cUserFlags :: Set String, ... } i had a project with a windows-source osx-source and it wasn't clear how to reliably evaluate away the platform specific conditionals from: if os(darwin) source-dir: osx-source ... 
Ooooh, I love this!
This seems like it's crying out for a GHC Derive extension.
&gt; What does HAXL do exactly to break it? `(&lt;*&gt;)` uses parallelism and batching; this provides a major speed improvement and was one of the biggest motivations for `ApplicativeDo`. The defense is that it satisfies the laws up to some quotienting operation. &gt; As long as its just internally and there are comments indicating such, that is fine with me. Your claim was that the laws are "pretty much universally obeyed"; I mentioned counterexamples.
&gt; Yeah, this breaks parametricity, So I keep hearing. I'm not convinced this is a bad trade-off on the expressiveness/reasoning scale, so long as it is limited in scope and/or still possible to enforce parametricity sometimes. &gt; Here's an example of a partial function with domain `Type` I was interested in "open", not "partial"; nevertheless, that's pretty cool. Still of limited utility to me, though, since I can't think of a useful implementation of `lower`.
&gt; A parser for a thing, &gt; Is a function from a string, &gt; To a list of pairs &gt; of strings and things. A parser for a thing, Is a function from a string, To a list of pairs of strings and things. =) 
The [exinst](https://hackage.haskell.org/package/exinst-0.4/docs/Exinst.html) package is intended to cover the singletons case mentioned at the end of the article. 
`Eq` probably will never be fixed in Prelude. I'm pretty comfortable defining `Equivalence` and `Equality` on top of `Eq` to add the laws of symmetry and substitution, respectively, but it's really just a name without a corresponding proof obligation anyway.
Source on that one? I see a fair amount of unsafePerformIO in the libraries I have been using recently. I know react-hs uses zero orphans but a whole lot of unsafePerformIO. 
[removed]
I'm a relatively new Haskell programmer, and those are all exactly the thoughts and issues I've had with my Haskell programs. The only issue is I've never quite found an answer on how to avoid them. I was wondering if you would be able to enlighten me?
The discoverability isn't that low, a simple `git log` would show them as git shows notes by default. It's also no harder to processes than the current situation. Which uses a set of custom scripts to process a custom format to extract the information. you get all this machinery for free using `git notes` e.g. (`git log --pretty=format:"%H %N" --show-notes &lt;branch&gt;` will just list all the notes over time. or you can use a `git grep --notes` to selectively filter them. or just list performance over time for a single test if for instance you put the individual test results in their own namespace. All the performance data would also be available offline which is a huge win. It's also just a ref in git, so you can also just read the information out using libgit2 or just some simple text parsing. Just because the command isn't well known (and git has a lot of these not well known commands) doesn't make it extremely useful. As an example, any project using Gerrit for reviews uses Notes as Gerrit stores review information in notes. https://www.gerritcodereview.com/ Let's keep this simple :)
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [BP-HUG/presentations/.../**2016-04-07_Let_a_thousand_lambdas_bloom.md#csaba-hruska-data-definition-haskell-edsl-for-cross-language-data-exchange-supporting-javacchaskellpurescript** (master → 9f7f599)](https://github.com/BP-HUG/presentations/blob/9f7f59976631ee9199fe3aa6995982ace10db090/2016_april/2016-04-07_Let_a_thousand_lambdas_bloom.md#csaba-hruska-data-definition-haskell-edsl-for-cross-language-data-exchange-supporting-javacchaskellpurescript) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply diz4tj0.)^.
Others here could definitely provide better advice, so hopefully someone will jump in if I'm way off somewhere. - (Only because I mentioned it) You most likely don't need HList, which is more like a more granularly defined version of tuples, for things like describing database columns. - You likely don't _actually_ need a list of things of different types. You probably don't need type classes. - Keep in mind, you _can_ just convert all your Circles and Squares to Shapes - or to functions, or directly to IO actions. Remember that it's lazy - casting a list of Circles to Shapes doesn't mean you're actually doing that work - you can think of it as just a different view on the data. - If you're used to GoF style OOP, it's easy to forget that passing around [a list of] functions is a normal thing to do - Java pretty much didn't let you do it before Java 8, and even in Javascript, you can do it, but it makes for terrible errors/stack traces - wrapping everything in an object is the best way to compensate. - It's tempting to think of casting between data types as lossy, but it doesn't need to be. If you need some information, just shove it in the type you're transforming to. The interface of the type you're transforming to should describe the information you need to do what you need to do. - If you really need the original data, refined to the original type, you can probably just wrap it in a sum type. `Shape = CircleShape Circle | SquareShape Square`. - If you need "open", just use functions - if someone makes a new shape, they can write a new "toShape" function - [This](http://www.haskellforall.com/2012/05/scrap-your-type-classes.html) is worth a read. - Mentioned further up in the thread, [this](https://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/) article has some good insight. - More conversation [here](https://www.reddit.com/r/haskell/comments/6cn5mi/type_class_patterns_and_antipatterns/) So when _do_ you use typeclasses? - The laws are important - The most compelling place to use typeclasses is when the parts of the interface have a relationship to each other. Look at Monoid - the Monoid constraint is saying that the operation is associative, and applying the mempty value with the operation is idempotent - that's an assertion that you can use to refactor or optimize code. Type families let you extend this to include relationships between types. A notable point is that the laws make it refuteable that something might be an instance. - If you look at the [Show](https://hackage.haskell.org/package/base-4.9.1.0/docs/Text-Show.html) typeclass, there are actually other functions there (with default implementations), and some degree of a law. - Typeclasses also do get used for more basic ad hoc polymorphism, like `fromRational`, mostly where it would be really tedious to keep track of all the different implementations, or change the type of the value it's applied to, and it makes sense to let the type system pick it for you. They also get used to create type constraints, like with [lens](https://artyom.me/lens-over-tea-6) creating typeclasses for has&lt;field&gt;. These always struck me as a little less compelling, but sometimes they're what you need, since coding in the "scrap your typeclasses" style isn't always practical. I realize that't not all the most concrete advice. Hopefully someone with more practical experience writing libraries can hop in and add some corrections or clarity, but I hope that's at least helpful.
In Haskell you can write class HasInfo a where info :: Lens' a Info using `Lens` gives you accessor laws, so it's not lawless as `info :: a -&gt; Info` would be. And that gives you already a tool to reason, that x ^. info :: Info In traditional OOP language, if you see an expression x.info you can only *wish* that the accessor actually returns `Info`, in other words it requires more programmer *discipline*.
 when :: Bool -&gt; IO () -&gt; IO () when condition action world | condition = action world | otherwise = ((), world) Is equivalent to when :: Bool -&gt; IO () -&gt; IO () when condition action | condition = \world -&gt; action world | otherwise = \world -&gt; ((), world) (Some optimizations like inlining fire differently but the behavior is the same) 
Your type signature is neglecting to specify the third argument 'world' to the function 'when'. You're overlooking partial-application. The Bool -&gt; IO () -&gt; IO () signature appears to take two arguments, but because IO () is itself a function type, it's actually three arguments.
Can you give an example of a list comprehension that cannot be achieved using a combination of folds?
Great! Thx joe462. I just figured it out: type RealWorld = String type EA a = RealWorld -&gt; (a, RealWorld) when :: Bool -&gt; EA () -&gt; EA () when condition action world = if condition then action world else ((), world) 
I know, but for didactic reasons I like the article anyhow.
From the test cases the simplest examples are ascii art. One could imagine that something like [needle](http://scrambledeggsontoast.github.io/2014/09/28/needle-announce/) could be written with it.
I do use a list comprehension when it is more readable. However, there's rarely a moment when I found a large nested list comprehension to be more readable. I guess my reasoning would be that I'm so used to reading function composition that it feels more natural to me and hence I can read code with function composition much faster.
That's fair enough, but I think function composition quickly becomes unreadable when there are multiple lists involved, which is much more readable in monadic style.
Thanks /u/HuwCampbell !
Generally, any data where alignment across lines is semantically significant. The first use cases I thought of were things like ASCII-art tables, diagrams, and math notation with positional formatting: +----------–-+-------+ | Date | Count | +------------+-------+ | 2017-01-01 | 8 | +------------+-------+ | 2017-01-02 | 12 | +------------+-------+ +------+ +----------+ +-------+ +---------+ | Read |--&gt;| Tokenize |--&gt;| Parse |--&gt;| Codegen | +------+ +----------+ +-------+ +---------+ ________ / 2 -b ± √ b - 4ac ----------------- 2a So if you were writing a document converter, you could use Toody to extract textual diagrams to SVG/TikZ, textual tables to HTML tables, textual math typesetting to (La)TeX/MathJax, and so on. It could be used to provide progressive enhancement for ASCII diagrams in Haddock comments: readable in the text-only source, but prettier in the rendered output. If you were writing a programming language*, you could use it to handle complex layout-sensitive syntax like matrices and decision tables, which are easy to parse for a sighted human but aren’t easily handled with 1D stream-based parsing libraries. A mundane use case is an old-fashioned programming language where column position is significant. Plus myriad use cases that I’m sure I haven’t thought of. \* There are (esoteric) 2D programming languages, but most of them aren’t great examples, because they generally don’t require parsing, typically using one command per cell. [Funciton](https://esolangs.org/wiki/Funciton) is a notable (and beautiful) exception. 
`sequence` is strictly less powerful because it has the exact same semantics as `sequenceA`, but it can be applied to fewer things.
fwiw, make sure it's not easier to just use `traverse`. Basically any time you see a `sequence $ map f _`, that should have been a `traverse f _`.
Hadn't seen the [Haskell on Android and iOS](http://keera.co.uk/blog/2017/06/01/haskell-android-ios/) one before. It seems closer to something like react native than the haskell as c-style backend other recent posts were about? Doesn't seem clear whether this will be opened up at some point or whether it is their product, though.
Do you have a preferred name?
haskell-jp would be nice!
Yeah, it seems like the “consuming” aspect of traditional parsers is a side effect of the fact that a stream is half of a zipper: the only direction you can move is onward in the stream. But when you lift that restriction, consuming input doesn’t make sense anymore. And the nondeterminism is not just a list, but the entire branching context—i.e. a comonadic “duplicate”.
Thank you!
&gt; And the nondeterminism is not just a list, but the entire branching context—i.e. a comonadic “duplicate”. Interesting, how does that work? Let's see, if I have a comonadic action which parses an Int starting at the current location, and I'm in a zipped list comonad, I can `extend` this to a function which parses an Int starting at any location. That makes sense to implement searches like your `everywhere` combinator, but not if I want to combine this Int parser with a String parser, i.e. `(MyInt &lt;$&gt; parseInt) &lt;|&gt; (MyString &lt;$&gt; parseString)`. For that, I really do need prioritized choice, that is, a monad like Maybe or List, no?
I think a `Parser` will be something like `direction -&gt; space -&gt; Either error (result, space)`. So you replace "consuming input" with "moving in a space". The Either takes care of prioritised choice with no motion. The "moving in a space" naturally leads to comonads, where `space` is some sort of comonadic "2D universe" zipper similar to what you use for comonadic cellular automata.
Thanks for the recommendation, sadly having trouble finding that on google or melpa
&gt; Until now, I could summarise what I knew about cryptography into two points: &gt; 1. It works because factoring prime numbers is hard &gt; 2. I don’t know enough about it to implement it reliably, I should use proven, off-the-shelf components instead While I understand this is a comment on the author's prior ignorance of the subject, I'm amused at just _how_ wrong the first point is. &gt; It works because factoring **prime** numbers is hard &gt; factoring **prime** numbers I have an O(1) algorithm to factor prime numbers for ya. Further, while "factoring numbers is hard" is a pretty good description of asymmetric cryptography, especially if you're willing to be very loose with the definitions of "factoring" and "numbers", symmetric cryptography is vastly more important, and a whole different ball game; symmetric cryptography works because some crypto geniuses commune with spirits/deities to deliver us primitives like AES, and then mere mortals figure out how to use them without ruining their perfection.
A person-century isn't that much. It's worth doing the arithmetic to make it concrete. There are about 260 working days in a calendar year, of which I would expect a programmer—one of mine, anyway—to be productive for about 210. So about 21,000 productive days in a person-century. A project staff of 20 therefore will burn that in about a thousand project days. Less than 5 years elapsed time. I've managed, or been a principal consultant or similar into, projects of that order to for example rebuild the toolchain—all bespoke applications—that a broadcaster uses to predict audiences, plan schedules, market and sell advertising, queue content for broadcast. For another, a project to use CFD and other simulation techniques to produce complete engineering drawings for a steam turbine given the desired thermodynamic performance. Another: to build a new suite of firmware and userland apps for a new mobile phone. Another: to build a new retail app for a global bank, _and_ a new security framework for it, _and_ new microservices middleware to talk to the mainframe, _and_ a new build pipe and devops environment for all of that. Another: a multi-tenanted web app that provides a self-service portal for wireless data device management for more than one world-wide mobile network. Do I think that such projects constitute the bulk of software development performed commercially? I don't know. I think there's a lot of them, mostly in-house, mostly not discussed very widely. There are much, much bigger projects, and there is almost certainly a long tail of many more much smaller projects—indeed, many of mine have been smaller. I think of person-century projects as the solid middle-ground of commercial development. And the most interesting. Big enough to have a noticeable impact on a blue-chip client, not so big that the work product rumbles off into obsolescence before it's even finished. These projects do not stand or fall on how well the implementation of a certain fine-grained algorithm written against one data structure can be transparently re-applied to another. In my view, this is the kind of project that a “new” technology needs to help notably with if it's going to get people to pay attention. Java went from zero to being widely applied on this class of project _very_ quickly. And Java (by design) isn't even that big of a move away from C++, which was the default choice for that kind of project at the time. And I've seen a lot of major technical improvements fail to gain any traction. Standard Chartered is probably the closest to what I have in mind, but—and I say this without any special knowledge of what they are doing, and also without casting any aspersions on their work, and also without making any predictions of its success—I have seen banks banks throw away vast piles of cash on whimsical nonsense that goes nowhere. They have, literally, all the money in the world and if a Exec VP decides to indulge some whim, they can do that. And when it all blows up, they shrug and move on to the next thing. So I tend to be a bit underwhelmed by the news that FirstUnionCityBank of Elbonia has a team using xyz technology. 
I don't mean the user of the compiler, I mean the user of the software built with it.
Ignoring bottom that is, since `bar 0 undefined == 0`, the same may or may not be true for `bar'` depending on type, but generally not true. And yeah I do hope that no one writes a num instance where `0 * x /= 0` or `x * 0 /= 0`. I wish we had a proper ring hierarchy with laws such as the above.
There are many hidden, confounding variables that your claimed correlation does not take into account.
I think a very simple argument in favor of lazy lists over strict lists isn't that lazy lists are amazing, but that strict lists suck. fmap f (x : xs) = f x : fmap f xs fmap _ [] = [] Is perfectly performant on a lazy list, but sucks on a strict list, (`O(n)` stack space and significant constant factor slower even ignoring memory usage). And it's not just `fmap`, literally any time you build a list you are going to have performance issues, be it `toList`, `some`, `many` or hand rolled recursion. Regardless of whether or not that list is fully materialized eventually or not. Guarded recursion / tail recursion modulo cons is a wonderful thing. Throwing it away would be silly.
Well until you make a compiler that makes it actually pleasant and easy to work with proofs such as the ordering of a list, then there is really very little against my claim that things that are easier for the compiler like typechecking are the way to go.
You don't need such an example though. Since the benefit of lazy lists is not that they might not always be fully evaluated, the benefit is that functions like `fmap` and basically all functions that build lists forward don't suck, like they would with a strict list. Lazy lists are awesome because they can be built bidirectionally with good performance, either through guarded recursion (forwards), or tail recursion (backwards).
Ah, but what if someone else uses a note? Is there a standard format or metadata that will be used to extract *just* these perf items? At issue for me is that (a.) discoverability is still low, as cloning the repo won't show me perf data and I have to use command line tools to manipulate these pieces of important text data, and (b.) we are polluting a general purpose alternate stream of text data with meaningful data. What's the benefit to this over just storing the perf data in a standard format as files (e.g.: json)? Notes seem to be permanent, do not get version tracked (they are just... attached to commits?) and thus we lose diffing, we lose tracking changes easily, and we will have to rely on magic strings or some sort of parser to extract *just* the right text data from the notes, lest another committer attach a note.
I have not. But, I also haven't had problems related to `unsafePerformIO` (or performance issues due to lacking it).
Haskell's function composition makes 'outside-in' a practical strategy (as opposed to the strategy employed by the famous middle-out compression algorithm). You start with both the arguments and the desired result (and their types). You compose functions to the left of the arguments, and compose functions to the right of the result until the types meet in the middle. This way you start with the interface you want to build, and figure you the logic along the way to connect your arguments to results. f :: a -&gt; b f argument = result . ... $ argument 
&gt; We could, for instance, transform every run-time-independent expression to weak-head-normal-form before byte-code-generation Sometimes the whole point of compiling a program is to perform the reduction of a pure expression efficiently at runtime. WHNF also does not force much for many types, typically lists. IMO it's not a good target to optimize towards.
Supercompilation. I think this is the name for what you are thinking about. Pure values with finite lazy representations, but infinite (or just too darn large) WHNF. `fibs = fix $ (0:) . scanl (+) (1 :: Integer)` Use of `unsafePerformIO`, with an external, verified safety proof. It's safe, but the compiler doesn't know it is.
ah, it has a different equivalence, that's fair
That's an argument in favour of streams, not lazy lists.
That's an argument in favour of streams, not lazy lists.
That's an argument in favour of streams, not lazy lists.
And I should also point out, we already use things like `submodules` which also don't get clones automatically without a `--recursive` or a `git submodule init`. Making full use of the features of a version control is not a sin.
So you are arguing for strictness (at least in lists)? Which I already gave my counter argument for, namely crap performance for a large portion functions that operate on them. So I actually already acknowledged in my comment that you might be talking about eager / strict evaluation, and not about totality checking. Like I get your point, I don't really think it's a bad one. But I don't get your solution, if you even are trying to suggest one.
&gt; And yeah I do hope that no one writes a num instance where 0 * x /= 0 or x * 0 /= 0 Too late. Prelude&gt; 0 * (1/0) /= 0 True Prelude&gt; 0 * (1/0) NaN
You also run the risk of infinite loops at the compile step. Sure, the code would break regardless, but knowing that compiler won't infinite loop is sort of nice.
I'm not sure how much of a math and CS background you have. But basically it comes down to an incredibly famous and well known proof that the halting problem, or in other words calculating (in finite time) whether an arbitrary program will halt is undecidable. Now you can write a checker that returns either False, True or Maybe. I mean that is obviously possible since you can just return True for `print "foo"`, False for `main = main` and return Maybe otherwise. That isn't a useful one and you can obviously do better, but there will always be infinitely many Maybe results. So eventually if you want a yes/no answer you need to decide if you want false positives or false negatives. Having the compiler / runtime system (when in debug mode) try it's best but admit false positives where it falsely thinks the program will terminate would be IMO a nice improvement, as right now the compiler doesn't really try at all and the runtime only works in very limited cases. Now the issue with requiring explicit rec for recursion is that basically all built in functions use recursion under the cover. And a ton of them can be made to not terminate without explicit recursion. So it would sort of be false security, unless you also needed to use `rec` to use already defined recursive functions. Which would be a huge pain as basically all of them do, also it wouldn't mix well with typeclasses where sometimes it is recursive and sometimes it isn't. 
I guess. At least you have a developer on hand to address the issue. Unlike when it happens on your server farm or your customer's mobile device. I'd almost rather allow full turning-completeness in the types, but limit terms to known total / productive forms.
Oh, sorry for assuming wrongly. I'm not sure yet what's happening then.
This is really cool :D
That is true! In fact that line of non-recursive looping combinators, such as the omega combinator. Is a great way to make the compiler panic. I think the russels paradox compiler panic also internally reduces to a form of the omega combinator, which is good, without doing something like that we would have proved false!
Ah ok, yeah then your specific question should not reduce to the halting problem. Well as long as you are ok with: foo x | x == True = bar | x == False = baz | otherwise = foo (not x) And similar functions being flagged as recursive, even though technically they will never actually recurse. Then yeah what you are asking is absolutely computable. Now since a massive amount of functions involve recursion at some point internally, I do think you will be somewhat limited in how often this saves you from nontermination. But if you truly think it will pay off then yeah there is nothing fundamentally wrong with such an extension. I do think you should seriously consider my suggestion to have a runtime debug mode that does a bunch of analysis on the code as it runs looking for infinite loops (and also giving really thorough stack traces and values of things when you crash or an infinite loop is detected). I think it would probably be able to detect and give you good error messages for the vast majority of the infinite loops you have detected.
Oh, it could be added via extension sure. The default for everything to be mutually recursive is probably pretty well baked into the existing GHC frontend, but it's not tied into STG and maybe not even Core.
Yeah. I see the same behavior (pass with `-j 1` passed to `tasty`, fails with `-j 4`)
It's not at all obvious how the Halting Problem relates to loop detection at compile time, but there's [Rice's theorem](https://en.wikipedia.org/wiki/Rice%27s_theorem) which connects the two. Basically, every non-trivial semantic detail you want to find out leads you to undecidable edge cases. They are rare (that's why totality checkers and automated theorem provers work so well these days), but it's something to keep in mind. It's also why most optimizing compilers only *approximate* runtime semantics in different ways in their analyses: They cut away slack until they aren't willing to pay any more compiler performance for it, because of diminishing returns.
Core differentiates between a single [`NonRec`](https://downloads.haskell.org/~ghc/8.0.1/docs/html/libraries/ghc-8.0.1/CoreSyn.html#v:NonRec) binding at a time vs. a [`Rec`](https://downloads.haskell.org/~ghc/8.0.1/docs/html/libraries/ghc-8.0.1/CoreSyn.html#v:Rec) binding group. The part of GHC which does the conversion from a single, recursive group to these let nestings is called the Occurence Analyzer. It's basically a strongly connected component analysis, once you have figured out all edges.
&gt; a syntactic property (for instance, does the program contain an if-then-else statement) I think what op wants to test *is* a syntactic property though. I believe they just want to ask "Is there a mutually recursive binding that does not have my new keyword requesting mutual recursion in the program?"
It looks like that language hasn't been updated in a while. The most recent entry on the website is from 2014 and the git repository hasn't had any updates to the `master` branch in a few years as well. I doubt there would be any confusion, especially given the different domains.
&gt; I am arguing to have finite data and potentially infinite codata. Haskell only has the latter. I am asking one very simple thing. Would the built in list type, like for finite lists, be lazy or strict. Because IF IT IS STRICT, `fmap`, `traverse` and and all forward list builders will be very difficult to make fast, whereas if it is lazy they are fast for free with guarded recursion. &gt; Do you always discuss things this way? Your tone is pretty awful. Pretty tired of people trying to shoehorn strictness by default into Haskell, despite the massive advantages that they get every single day from where clauses working nicely by default to almost never needing to hand fuze algorithms to being able to easily define functions and data that involve control flow without massive amounts of effort maintaining laziness. Figured you were another one of those people, who just don't appreciate valuable features because their benefits aren't visible unless you think about them for a while.
C was not updated between 1998 and 2011, and yet naming another language C would have been awkward as hell -- even a music description language. Mezzo is a research/experimental language, and it is normal in this domain to be dormant for years.
This is lovely. Perhaps a [markdeep](https://casual-effects.com/markdeep/) integration for `pandoc` would be a use-case?
How is this a "bad habit" ? Say you have two datatypes: one `Person` and one `Landmark`. They both have a `name` field. In Haskell you can't do this conveniently. If you don't want to specify at the call site which name is which you need to define a typeclass. But there is no real meaning associated with it. In Idris the naming overlap is silently resolved. The compiler will require you to be explicit only if it's ambiguous. To me this seems to be a quality of life improvement. It says nothing about your abstractions, as names are irrelevant to those. I fail to see in which bad habit you would indulge. 
Yes, and the problem of detecting infinite loops in finite time is known as the Halting Problem. Google it.
haskell.jp started few months back and they setup a slack to communicate. They have like 214 members in the slack group now... I recommended them to have a reddit as a main place for community engagement...
Sure that would be great!
Cool idea. I agree with /u/gelisam that the notion of consumption makes no sense. Not to optimize too early, but your parser should also get more efficient if it can't modify the grid by consuming its contents. On a more general note, jumping straight to the notion of a 2D parser seems like skipping a conceptual step or two. Think about grammars first. Chomsky-style generative grammars have properties like: L (G1 | G2) = L(G1) U L(G2) L (G1 &lt;&gt; G2) = L(G1) &lt;&gt; L(G2) A 2D grammar should have the same properties, except instead of simple string and grammar concatenation you'd need four different directions. Look at [diagrams](http://projects.haskell.org/diagrams/) for some combinator inspiration. 
&gt; There's no shortage of advanced talks but truly introductory talks are rare. Funny, I have the opposite impression! I feel like there are tons of books, blog posts and videos introducing the basics, and not enough advanced material. Which makes sense, since there are a lot more people who don't yet understand Haskell than there are people who do, so the audience for introduction material must be bigger :) &gt; Those talks are still too advanced for me. Oh, really? I purposefully posted those two because I felt like they introduced Haskell approaches quite clearly without assuming too much prior knowledge. If you don't know Haskell, of course you won't be able to read all the code in the slides, but while a video which does nothing but introduce the syntax would be a good first video for someone who's trying to learn Haskell, it wouldn't be a very exciting video for convincing someone that Haskell is worth learning. Maybe the conclusion is that most material is in fact aimed at an intermediate level, so it simultaneously feels too basic to me and too advanced for you?
One subset of the language I think would benefit from this is string literals, especially with `OverloadedStrings` (for cheap template Haskell) and/or `unlines`. It would be nice if every Overloaded String was converted at compile time. 
Yeah, and it is annoying - for instance, if you try to `coerce` a Map you know ought to `coerce`, you can't. However, hopefully they (containers maintainers) can add a rewrite rules for `Map.mapKeysMonotonic coerce m = coerce m` which would fire if I write `Map.mapKeysMonotonic unMyType`. I think that's a reasonable negotiation. 
&gt; your parser should also get more efficient if it can't modify the grid by consuming its contents I think the parser will still modify the grid, not by consuming its content, but by changing the focus of the 2D zipper the grid is represented by.
What are the advantages of avoiding the state monad here?
Very interesting. Is there a paper associated with this project, or recommended reading material? I have plenty of experience with dependent types, but not as it relates to music.
Very interesting. Is there a paper associated with this project, or recommended reading material? I have plenty of experience with dependent types, but not as it relates to music.
Very interesting. Is there a paper associated with this project, or recommended reading material? I have plenty of experience with dependent types, but not as it relates to music.
Very interesting. Is there a paper associated with this project, or recommended reading material? I have plenty of experience with dependent types, but not as it relates to music.
Very interesting. Is there a paper associated with this project, or recommended reading material? I have plenty of experience with dependent types, but not as it relates to music.
Very interesting. Is there a paper associated with this project, or recommended reading material? I have plenty of experience with dependent types, but not as it relates to music.
Very interesting. Is there a paper associated with this project, or recommended reading material? I have plenty of experience with dependent types, but not as it relates to music.
Very interesting. Is there a paper associated with this project, or recommended reading material? I have plenty of experience with dependent types, but not as it relates to music.
Very interesting. Is there a paper associated with this project, or recommended reading material? I have plenty of experience with dependent types, but not as it relates to music.
Very interesting. Is there a paper associated with this project, or recommended reading material? I have plenty of experience with dependent types, but not as it relates to music.
Very interesting. Is there a paper associated with this project, or recommended reading material? I have plenty of experience with dependent types, but not as it relates to music.
Very interesting. Is there a paper associated with this project, or recommended reading material? I have plenty of experience with dependent types, but not as it relates to music.
Very interesting. Is there a paper associated with this project, or recommended reading material? I have plenty of experience with dependent types, but not as it relates to music.
Nice write up. I think that lpaste links are perishable, so you may want to host glguy's solution elsewhere.
Free monads are great when you're willing to pay a reasonable performance cost for the monadic structure. What this means is that if your bottlenecks are pure functions or IO, then the cost of the free monad is unimportant. But as an example where this isn't true, consider Reflex. Everything you do runs in a monadic stack, and the DOM-driven work is a small percentage of the work Reflex-DOM has to do. So if Reflex were built on a free monad approach, the performance would likely be demolished. Not to mention, `mtl`-style is a strictly more capable approach than free monads, with almost always better performance. I say "strictly more capable" because there are monads you can encode with a transformer that can't be encoded with a free monad, but these aren't hugely useful monads, so this point doesn't matter much. The important thing is that `mtl` style is *vastly* more optimizable for GHC. I've seen a few benchmarks claim that free monads match `mtl` in performance, but these either use older versions of `transformers` that didn't properly inline things, or they don't turn on GHC optimizations. In general, I've found `mtl` to be [at least 3-4x faster than the best extensible effect implementations](https://www.reddit.com/r/haskell/comments/65kvei/name_for_the_style_of_programming_in/dgblmfg/), with that gap widening significantly as you increase the number of effects. And if you fully specialize the entire transformer stack so there's no runtime dictionary passing, transformers tend to be 10x, sometimes even 30x faster (when you're only using blazing fast transformers like `ReaderT`). Getting free monads to perform at these speeds would require super compilation, which is a known hard problem, and probably won't be solved any time soon. In addition, if you stick to `mtl` style, and avoid using concrete transformers in your app, I've found that `mtl` is only marginally more difficult to test and mock; and that difficulty is mostly in boilerplate. Basically, if you would ever write an effect like this: data DBAccess a where GetUsername :: UserId -&gt; DBAccess Username GetFriends :: UserId -&gt; DBAccess (Set UserId) You can always translate it to an `mtl` style class like this: class Monad m =&gt; DBAccess m where getUsername :: UserId -&gt; m Username getFriends :: UserId -&gt; m (Set UserId) Whatever effect interpreter you would write for that effect type, you can implement a transformer that does the same thing for this class. If you ever need to mock this API, you can write another transformer that's designed to do the same thing that the mock interpreter would do for the free monad version. There's more boilerplate involved, and you have to do a little bit more thinking about how you're going to jam that effect in a transformer, but it's usually easy. More often than not, you end up just writing a newtype around `ReaderT SomeEnv m a`, using `GeneralizedNewtypeDeriving` to get most of the boilerplate in one line, and figuring out how to use that `SomeEnv` type to enable the work you need to do. Since `ReaderT` is a blazingly fast transformer, this is basically free in terms of performance (modulo GHC failing to specialize, which happens a lot, unfortunately). --- This isn't to say free monads are all bad. If performance isn't your #1 concern, they're definitely more convenient. You don't have to put hardly any thought into how you're going to get an effect into your system. Just add an obvious effect type, and write a simple interpreter, and you're done. It's marginally less complex. Plus you don't have to futz around with writing newtypes with different instances for testing's sake. Free monads definitely get points for convenience, and making your effect implementations nicer. EDIT: Oh man I almost forgot the biggest advantage to free monads. The n^2 instances problem! With `mtl` style, if you want to treat a class as an effect, and a transformer as an interpreter, then you end up needing n^2 type class instances for `n` effects, if you want to have total coverage. That is, if we have effects `Foo`, `Bar`, and `Baz`, and we have transformers `FooT`, `BarT`, and `BazT`, then if we want to be able to use each effect from any possible stack of those transformers without using `lift`, then you need 3^2 = 9 instance. instance Monad m =&gt; Foo (FooT m) where foo = ... instance Foo m =&gt; Foo (BarT m) where foo = lift foo instance Foo m =&gt; Foo (BazT m) where foo = lift foo instance Monad m =&gt; Bar (BarT m) where bar = ... instance Bar m =&gt; Bar (FooT m) where bar = lift bar instance Bar m =&gt; Bar (BazT m) where bar = lift bar instance Monad m =&gt; Baz (BazT m) where baz = ... instance Baz m =&gt; Baz (FooT m) where baz = lift baz instance Baz m =&gt; Baz (BarT m) where baz = lift baz Usually, this boilerplate is really easy to minimize. Between `GeneralizedNewtypeDeriving` and `DefaultSignatures`, you can usually get this boilerplate down to an entry in the `deriving` line, or in a one-line instance with no body per pair of transformer and effect. But overlappable instances is also an option, if you're willing to step a bit into the dark side =P. You can write an instance that implements the class on any transformer so that no other transformer has to implement the class. Then you just mark it as overlappable so the main transformer can overlap it with the correct implementation. But the reason I called this "the dark side" is that in rare circumstances (and I do mean rare; like, orphan instances rare), it can lead to instance incoherence. For this reason, I think it's totally cool in applications, but I would never put an overlappable / overlapping instance in a library that I expect other people to use. Anyway, with extensible effects, this problem doesn't even exist. You can arbitrarily use any effect with any other regardless of whether any of them have taken each other into account. --- TL;DR: Since there are no effects you can encode in free monads that you can't encode with `mtl` style, I've found that it's just a tradeoff between a marginal amount of convenience and a substantial amount of performance (in some class of projects).
Are you basically saying that free monads are not worth it for most real-life applications? One advantage that I *thought* existed for free monads, was the ability to generate a sequence of actions via QC, for supercharged randomised testing. Is there any sane way of doing that with mtl style monads? quickcheck-state-machine has just been released, but its types seem too complex! And just to be clear, are you referring to any particular library when you're referring to extensible effects? 
The `puppet-language` interpreter runs entirely in a free monad. It is between 20/50 times faster than the actual (ruby) `puppet` was when I wrote it, so it was fast enough for my purpose. And yes, it made testing much easier! I also am maintaining a webapp with servant where all handlers are written as a free monad, and it is clearly fast enough. I like the free-monad approach as I find it faster to get started with and to refactor than a custom datatype + typeclass based approach. The only "large" application where speed was a critical factor is `language-puppet`, but it was fast enough so as to make testing a faster implementation unattractive. edit: I actually use things like "operational", which are even easier to use.
&gt; At what "level" is the free monad's DSL usually implemented? That is entirely up to you! I have a webapp where the free monad is basically at what you would call "low" level, but I also have a free-applicative that encodes authorization/authentication on top of it. For [a seven wonders clone over XMPP](https://hbtvl.banquise.net/series/7%20Startups.html), the free monad actions are [very high level](https://github.com/bartavelle/7startups/blob/00490a41ae1695ac97282de80b4fc60bfa3d3915/Startups/GameTypes.hs#L83-L90). 
I haven't looked at your code, but would it help if you loaded a batch of files into memory. I had a similar problem (reading-&gt;writing continuously) and it helped to have part of the pipeline load 100 or so files before dumping some data onto the disk. If so, [conduitVector](https://hackage.haskell.org/package/conduit-combinators-1.1.1/docs/Conduit.html#v:conduitVector) is perfect for breaking up streams.
Another way of asking the same question is to look for examples of people using ASTs to represent structures at run-time, for the purposes of analysis and multiple evaluation. The "free monad" is just a nice abstraction for easily building such things out of a functor you have on hand. What I mean by saying this is that your question is about more than just using a free monad to build those constructions. Also, depending on what you're doing, the different free monad representations have different runtime characteristics (free vs. final free vs. freer vs. van Laarhoven free, etc).
But I think that specifically the comonad may be the right abstraction. It was suggested here and there by /u/boothead https://www.reddit.com/r/haskell/comments/3jiqcf/haskells_map_fold_zip_and_their_deepneuralnetwork/cuqbhik/
With Postgres, there are two approaches that come to mind: * keep the hostname/port no/credentials constant and create a new database for each test * spin up a docker container for Postgres on a different port for each test The former has much less overhead but less isolation, and which is more appropriate will depend on what you're doing in those tests. The latter could make sense if your test data is large / takes a while to load though (since the Docker image will be faster to start than a SQL dump to import), and could even allow you to specify a different container per test. 
[Don't Fear the Monad - Brian Beckman](https://youtu.be/ZhuHCtR3xq8) A really great introduction to monads and monoids, in an hour, assuming no functional programming background, and only basic-ish programming skills. 
ASN.1 is the most complicated and convoluted standard out there. Please avoid it like plague. Please do not go by the years of standardisation you will regret it.
Nice work!
I've always found creating DSLs with `ContT` requires fewer gymnastics than doing the same with free monads, and the resulting DSLs have slightly better performance. However: if the DSL has multiple interpretations then free monads are often more suitable.
Perhaps a topic for another thread, but why did pusher move away from Haskell?
Thank you! It's available on [Hackage](https://hackage.haskell.org/package/mezzo) if you're interested!
Well that escalated quickly :D
Thank you! That is a surprising coincidence :) Yes I have Fux's book on my bookshelf too, I didn't follow it very explicitly (the rules implemented come from a bunch of different books, guides and tutorials), but I don't think it would be difficult to add them to Mezzo. A GUI would be nice of course, but I'm not sure how that would interact with the typechecker? Type-level web apps hmmmm.... I'm planning to look into Idris this summer and see how it compares to dependent Haskell!
Thanks a lot!
Learn Swedish &amp; Haskell https://www.youtube.com/watch?v=RqvCNb7fKsg
Not to mention the progress in C compilers in that time.
I always liked the Chalmers recordings: http://www.cse.chalmers.se/edu/year/2016/course/course/TDA452/FPLectures/Vid/
I just read the description of your project again. Actually, mine wasn't exactly the same. I never intended to make incorrect compositions ill-typed, dependent types were just nice to have, especially when dealing with intervals. So I would not have had to ask myself that question :p Idris actually compiles to javascript, so "type-level web apps" doesn't sound too weird (depending on what you mean by "type-level"). Regarding rules, is there a way to dynamically change them, if you don't mind me asking? I guess any choice of composition rules has to be arbitrary, so it would probably be nice to be able to change them without recompiling the whole thing...
Ah ok. In that case I guess one piece of evidence in favor of lazy lists is that even when fully evaluated they are better and faster to generate and map over than strict lists. I will reply to your other comment with benchmarked examples at some point. So basically one absolutely massive advantage of lazy lists is simply guarded recursion. 
Do you mean `zipMatch _ _`? It's the catch-all clause that matches whenever both arguments have different constructors.
 {-# LANGUAGE DeriveFoldable #-} import Data.List (foldl') data List a = Nil | a :- List a deriving (Foldable, Show) data List' a = Nil' | a := !(List' a) deriving (Foldable, Show) instance Functor List where fmap f (x :- xs) = f x :- fmap f xs fmap _ Nil = Nil instance Functor List' where fmap f (x := xs) = f x := fmap f xs fmap _ Nil' = Nil' build :: Int -&gt; List Int build 0 = Nil build n = n :- build (n - 1) build' :: Int -&gt; List' Int build' 0 = Nil' build' n = n := build' (n - 1) main :: IO () main = print . foldl' (+) 0 . fmap (+ 1) . build $ 10 ^ 8 So the above code runs in `3s` on my machine, but when I change `build` to `build'` it runs in `41s`. With `10 ^ 7` elements instead it runs in `0.3s` with the lazy list and `3s` with the strict list. Then with `10 ^ 6` its `0.036s` vs `0.35s`. Both the `foldl' (+) 0` calls should take pretty similar time, so you are paying a massive amount for both `build` and `fmap` for the strict list. Since you just completely lost any kind of tail or guarded recursion.
I just responded to you with a code snippet [here](https://www.reddit.com/r/haskell/comments/6h84vg/when_do_we_really_need_lazy_lists/dj13bn0/). And alright cool, if that is the purpose, then this code snippet should be a massive piece of evidence in favor of lists being lazy.
すごい
it's not too bad isn't it
That one doesn't use vinyl in the examples does it? If so, I feel like it'd have to be at least an intermediate video.
Thank you for your answer :) &gt; Since pretty much all computation (i.e. rule-checking) happens statically, you can't really "avoid" recompilation. So definitely not what I would call dynamic. It makes sense to me though, it's just further from my idea than what I thought first. I don't know how innovative the rule set idea is, but I really like it :) &gt; although some understanding of the internals might be required. Isn't it just a matter of selecting the rules you would like to see enforced?
http://okmij.org/ftp/tagless-final/index.html 
As far as monadic effects go, that seems like exactly `mtl` style, no?
Maybe it could be seen as a kind of `mtl` style. I'm not familiar enough to say.
General recursion is what prevents compile-time loop detection. However, there are a lot of restricted flavors of recursion are completely safe and can be proved to halt. The most reliable way to prevent loops is just to ban recursion language (i.e. no recursive types and no recursive terms). You actually don't lose too much in the process, since a lot of common recursive idioms can be transformed to non-recursive forms. This is, for example, how my Dhall language guarantees the absence of loops.
This is probably what you meant, but in most situations you could probably just create ephemeral schemas rather than ephemeral databases.
Tagless final is precisely `mtl`-style.
I WANT to buy the haskellbook. I can afford it and I'm waiting for it, but I want the hardcopy not the ebook version. I like dead trees :) . Seriously the ebook format does not work for me. I've reached out to the authors a few times but haven't gotten any information on availability of the hardcopy so its Real World Haskell next after LYAH. 
So it doesn't yet support music with time signatures outside the normal 2, 3, or 4, and doesn't have support for varying time signatures?
I know not doing exercises is not the best way to go but at the moment I am concentrating on understanding the philosophy of the language and will often stay put on one paragraph for days, weeks and even months till I get past a knotty concept. The implementation of the State Monad bind for example was hard to wrap my mind round. I'm pretty much past that hump but it took about 3 months. The other reason why I'm not writing much Haskell code ATM is that between my day job (as a C++ programmer) and my gigantic side project that consumes all my free time the only time I get to devote to the study of Haskell is during my commute. But I'm steadfast and I've sunk my teeth into Haskell and will not leg go till I understand it. I plan to write one of the three components of my side project in Haskell but that can only happen after I complete phase 1. Anyway the point I'm trying to make is that I have a compelling reason to learn Haskell. I found Erik Meijer's talks on Haskell to be really good and very helpful, especially the early ones, for example where he talks about the level of syntactic noise, or lack thereof, in Haskell. Can I just say that if Haskell indeed has a very steep learning curve the community ought to give more thought to helping newbies understand the basics (I know the Haskell book is taking a step in that direction), but more beginner-friendly screencasts would be great. I've heard someone say that the problem really isn't that Haskell is difficult. Its just that "we" (the community) haven't yet figured out how best to teach it.
Honestly, the simplest answer would be something like this Church-encoded free monad: newtype Rand a = Rand { unRand :: (a -&gt; r) -&gt; ((Int -&gt; r) -&gt; r) -&gt; r } instance Functor Rand where fmap f (Rand m) = Rand $ \c -&gt; m $ c . f instance Applicative Rand where pure a = Rand $ \c _ -&gt; c a Rand mf &lt;*&gt; Rand ma = Rand $ \c fc -&gt; mf (\f -&gt; ma (c . f) fc) fc Rand ma *&gt; Rand mb = Rand $ \c fc -&gt; ma (const $ mb c fc) fc instance Monad Rand where return = pure (&gt;&gt;) = (*&gt;) Rand m &gt;&gt;= f = Rand $ \c fc -&gt; m (\a -&gt; unRand (f a) c fc) fc randOracle :: Rand Int randOracle = Rand $ \c fc -&gt; fc c runRand :: RandomGen g =&gt; Rand a -&gt; g -&gt; (a, g) runRand (Rand m) = m (,) (\f -&gt; uncurry f . random) This keeps everything right-associated, ensures you don't have to pass around a `Monad` dictionary, and also means you don't have to produce an additional `State` `Monad` to pass into the transformer.
That's an interesting idea. Have another language that compiles, along with interpreters as input, to fast code that combines those two ahead of time. I think this is the sort of thing that Conal Elliott's new [concat](https://github.com/conal/concat) library is meant for. But this *definitely* leaves the realm of ordinary monad talk =P Now we're talking about actual sublanguages.
That's a very interesting pattern. I wonder how it could be extended to more exotic variants of unification, for example [Algebraic Subtyping](https://www.cl.cam.ac.uk/~sd601/thesis.pdf)'s biunification.
Well, the time signatures only really apply to the way the chords in progressions are generated (which need some more work anyway), so a 3/4 progression would have 3 chords in a measure, a 4/4 progression would have 4 etc. If progressions are not used, the time signature is irrelevant, as the duration of notes is always explicitly given (technically the time signature is always included in the MIDI metadata, but this has no effect on how the piece sounds). Varying time signatures can be implemented by breaking up the piece into different sections and setting their time signatures independently.
Huge fan of your blog dude. Loved the Seven Startups series
The concept of "side effects" is always *with respect to* something. In Haskell, "effects" usually do *not* include low-level system operations like allocation, context switching, etc. Other systems do consider these effects and let you track them in the type system. These tend to be good for real-time systems with low-level requirements. But, as you might imagine, they mostly get in the way when you don't care and want to operate at a higher level.
[Functional programming design patterns by Scott Wlaschin](https://www.youtube.com/watch?v=E8I19uA-wGY)
Forgive my confusion, but, isn't Fraxl based on a free monad?
Function have side effects means you can tell the difference when it was executed 0, 1 or n times. Whatever that is - character on the screen, file, beep or launched missiles. From computation point of view thunk containing (1 + 2) is indistinguishable from computed value 3.
&gt; I am concentrating on understanding the philosophy of the language and will often stay put on one paragraph for days, weeks and even months till I get past a knotty concept. Let me be really really honest with you -- you're completely wasting your time. LYAH is not a good learning book, at all. The two books that /u/gabe4k cited are good. LYAH can, at best, get you excited about Haskell and give a whirlwind tour of the syntax and things it can do. If you want to understand the philosophy of the language, *actually use the language*. Haskell is, first and foremost, a language for experimenting with *practical* *applications* of theory -- it was designed so people could *do*, *experiment*, and *play* with concepts and ideas in a lazy purely functional manner. I would strongly recommend *not* sticking on a point until you fully grasp it. Instead, read something, and try to understand it pretty well for an hour or so. If you don't get it, move on and come back to it later. I didn't get applicative functors at all the first time I learned about them and they stumped me really hard. Then I moved on, and came back to them again with another approach. I still didn't really get it, but I was comfortable *using* them. The third time I approached the concept, I finally understood it, and it's only because I had used them so frequenttly.
So, easy to add new rules, harder to create new ones. Makes sense. Thank you for your explanations :)
I refuse to! . . . Not because I'm being unhelpful, but because I'd rather send you to Oleg's material, which is much better than any explanation I could ever offer. http://okmij.org/ftp/tagless-final/index.html I found the lecture notes to be most helpful.
i'm a "pay for correctness with boilerplate" programmer. but, using fail for Maybe to match types for equality-like or comparison-like functions feels hacky. what if we derived a type that's "maybe-both"? i.e. data T = A x | B y z data TS = AS (Both x) | BS (Both (y,z)) | Z where type Both a = (a,a) 
I've personally found Spacemacs to be a terrible experience - it's completely non-composable, in a manner that is typically reserved for JavaScript frameworks. I just use Emacs + Evil + Intero.
I'm new to spacemacs, can you explain in which way it it non-composable? Isn't it basically Emacs + Evil?
Yep. I'm currently working on something that will achieve the same goal as Fraxl without the free Monad.
You might be confusing parallelism with concurrency. Creating a new thread is indeed considered a side effect. 
The two main differences between `putStr` and `par` that I see are: 1) The compiler can freely replace something like a `par` a `par` b = a `par` b However, you probably want the computer to be somewhat more conservative about something like: putStr a &gt;&gt;= \x -&gt; putStr a &gt;&gt;= \y -&gt; return () 2) Exceptions. Idk if you can close `stdout`, so maybe it'd be hard to make `putStr a` return different results at different run points in your program, but `hPutStr` definitely will change if you run it multiple times. 
`haskell` and [`liquid-types`](https://github.com/ucsd-progsys/liquid-types-spacemacs).
I don't use Spacemacs, but [dante](https://github.com/jyp/dante) should work there just fine, modulo keybindings. I like it because it's the most configurable (in that all it cares about is being given a GHCi session; you can choose how to get it one) and it doesn't rely on any external tools.
Essentially because (a) there's an implementation which produces the same values more slowly that is obviously pure so that (b) these two implementations are the same "as far as we observe". The last bit is in quotes because you can break tradition and say that you observe two computations as "different" if one uses more parallelism than the other. This is just a choice and ignoring parallelism is both convenient and traditional.
[Haskell Amuse-Bouche](https://www.youtube.com/watch?v=b9FagOVqxmI), a Google Tech Talk.
Ah that makes a ton of sense now thank you for the help!
&gt;real-life large application Well, all applications are real-life applications. Anyhow, my company uses free monads in our codebase. It simplifies a lot of things but I don't think we've ever benchmarked it. 
So the problem here is that we ideally want a set of performance numbers. For example: Test A Windows: 1005 Linux: 985 OSX: 1055 We want these performance numbers to be attached to every single test. This is actually almost *exactly* the reason git notes were created in the first place. In this situation, we have preexisting tests that we want to attach information to (without messing with git history). A separate repo might theoretically be cleaner, but it still ends up ruining the git history; worse yet, if somehow the format is changed later on, the same choice must be made again: git notes or new repo (or some other history destructive method)? With git notes, it becomes much easier to incrementally improve upon the solutions should we want to. But, the main desires behind git notes are: being able to add meta-data without ruining git commit history, and the amount of tooling available to help automate and take advantage of git notes.
Well, the way it's implemented, sparking the same thunk multiple times may lead to it being evaluated by two cores at the same time for a short while. So sparking it twice is distinguishable from doing it once (sometimes)
How is that distinguishable? The denotational meaning of the program does not change, even if that happens.
I am a bit late but I found the way typeclasses are implemented in ghc (and most other haskell implementation) quite enlightening. Class definition: class Fooable a where foo :: a -&gt; Int data FooableDict a = FooableDict { foo :: a -&gt; Int } Instance definition: instance Fooable Int where foo = id fooableInt = FooableDict { foo = id } Class constraint: fooer :: Fooable a =&gt; a -&gt; String fooer a = show (foo a + 1) fooer :: FooableDict a -&gt; a -&gt; String fooer dict a = show (foo dict a + 1) Calling: fooer 3 fooer fooableInt 3 So type classes don't give us anything we couldn't write by hand. There are still advantages to type classes, though. Some frequent ones: - We want exactly one implementation per type. `Functor a` is an example of this, there is only one valid implementation per type so passing it by hand is unnecessary busywork. If we passed an Ord instance by hand it could change between insertions and lookups from a Map, creating hard to find bugs - More generally type classes make it easier to document rules implementations should follow - We want haskell to assemble function dictionaries from parts, like `Ord a =&gt; Ord [a]`. Haskell takes the dictionaries from `a` and `forall a. [a]` and mashes them together for us. With explicit dictionaries we would have to do the assembly by hand
That isn't really Spacemacs' problem. *Nothing* in emacs is composable. It's all a bunch of plugins that modify some global state, and any code you run within emacs can modify anything and make your life very difficult. I chose spacemacs for this very reason. At the very least, spacemacs is a configuration that has been thoroughly tested, where you can be mostly sure that all the layers that are offered won't mess with other layers and so on. 
Very nice work! Apparently the first example in the Readme doesn't work, it probably should be something like main = renderScore "comp.mid" "comp" comp
(note that you need to add four spaces in front of your code snippets or reddit will mess up your formatting)
Ah thanks! That's what you get when you make last-minute breaking changes :) Fixed now!
Can you show an example of a DSL that uses ContT?
There's a great talk by SPJ ([here](https://www.youtube.com/watch?v=hlyQjK1qjw8)) that describe the design space of parallel programming which I really recommend watching.
&gt; parMapChunk What library does that come from? I didn't see it in `Control.Parallel.Strategies` nor did Hoogle find it.
Not so. `BaseType` holds the base type, not unification variables.
What benefits do you see to putting checks in the type system? I ask because I've been working on a DSL for describing rhythmic scores. In lots of cases I couldn't think of an easy way to enforce things in the type, so I just resorted to 'error'. Normally I never do that, but for the scores there's no real separation between compile time and runtime. I edit a score, reload in ghci, run the function to realize the score, and get all the errors (with source position, courtesy of the ghc stack extension). Since there isn't a big difference between compile and run (basically compile once, run once) and there isn't much complicated conditional branching or recursion, it's easy to get a single run to have 100% coverage. So I throw "realize all scores" in the tests and so far it hasn't been a problem. Still, all the partial functions and calls to 'error' bother me a bit, and I've thought of trying to do a version in idris, mostly as an exercise. Also, type checking gives you a binary pass/fail answer, but music theory rules are more like guidelines, or degrees of ok-ness, which can vary depending on circumstances, style, and era. Also they would have a notion of "almost right", e.g. change one accidental and you'll be "righter", but type errors don't really carry closeness to right along with suggestions of how to get closer... though it would be interesting if they did! Even in my rhythm oriented notation, I've found it useful to demote errors to warnings, e.g. if beats don't add up, I drop a Left in the output but keep going, so I can show the realized score with a marker where things got off, which in practice is more useful than "error at X" and no further output.
Thanks for the reply Tekmo. Don't take too much time looking at my problem, I'm being lazy and asking you instead of diving into Dhall for myself! Your Dhall example is exactly how you would write it in my interpreter, but without the ins and lets. Many of my end users will be biologists and I've been warned that any nontrivial syntax could scare them off. In an ideal world, I'd reduce the syntax of Dhall to interpret a list of "&lt;variable&gt; = &lt;expression&gt;" and "&lt;expression&gt;", I'd have Dhall interpret the expressions using typeWith as you said, and I might disallow assignments that aren't of a CustomString type. That's a lot to ask. Maybe I'll just write a preprocessor to translate the syntax and just not mention the other capabilities to users.
Absolutely. That is the signature I would have chosen for library-quality code rather than something I threw together in a few minutes to test parallelism. 
I had an Erlang injection in a product I took over because of that attitude. Only a term injection, technically, not a code injection, but still. These things bite you in the real world.
What does the `pseq` do? Eg, why not: parMap f (x:xs) = let e = f x in e `par` e : parMap f xs
I believe the `pseq` serves as a hint to the compiler to attempt to complete the computations of both `f e` and `go es` to WHNF before combining them with `(:)`.
&gt; In order to be a win, then, specialisation has to deliver considerable benefit. The reality is that sometimes specialisation delivers enough benefit to be worthwhile, and sometimes it does not. Thus 'let's evaluate everything that we can at compile time' is not a good idea as has been known since the early days of compilers. Honestly, I am skeptical of what you are saying. Obviously, taken to its limit you are correct, but if we look at functional languages today - any functional language in fact, then it would be a pretty safe bet that they are doing too little inlining especially where higher order functions are concerned. Inlining guarantees in the face of abstraction would be wonderful to have - one use for them would be the ability to use first class functions in GPU kernels and other domains where heap allocation is not an option. It would be also the first and necessary step towards ending the reign of C and other imperative languages.
I cannot find anywhere what "liquid-types" is
If your test is flaky and you are sure the thing you are testing must be true, then you have a bug in your code. If you are running your test again just to avoid the failure then you aren't really using QuickCheck to its full effectiveness. If QuickCheck is taking a ton of time, you might want to re-evaluate your `Arbitrary` instances. If they are recursive, check out the [`resize`](https://hackage.haskell.org/package/QuickCheck-2.10.0.1/docs/Test-QuickCheck.html#v:resize) function. Here is an example `Arbitrary` instance for a aeson `Value`: instance Arbitrary Value where arbitrary = sized sizedArbitraryValue sizedArbitraryValue :: Int -&gt; Gen Value sizedArbitraryValue n | n &lt;= 0 = oneof [pure Null, bool, number, string] | otherwise = resize n' $ oneof [pure Null, bool, number, string, array, object'] where n' = n `div` 2 bool = Bool &lt;$&gt; arbitrary number = Number &lt;$&gt; arbitrary string = String &lt;$&gt; arbitrary array = Array &lt;$&gt; arbitrary object' = Object &lt;$&gt; arbitrary Before I added `resize n'`, the performance of the tests that used this was wildly unpredictable because sometimes I would generate deeply nested JSON values. My suggestion is to embrace the randomness of QuickCheck. I've had enough legitimate bugs discovered from QuickCheck that if I can hook up QuickCheck to test a piece of my code and the tests pass, then I'm way more confident that code is correct.
Quickcheck is not a replacement for more "classic" tests with fixed values. If you know that a value is special and important in any way, you can still use a test with fixed value with HUnit. If you want predictability, you can fix the seed used by quickcheck with the `replay` parameter, see http://hackage.haskell.org/package/QuickCheck-2.6/docs/Test-QuickCheck.html#t:Args but, well, you don't want that, because you will miss the interesting property of quickcheck which is that, with some time (well, an infinite number of time), you will check all the input space of your properties. If you want to test ALL possibles combinations, use https://hackage.haskell.org/package/smallcheck but this may take times. On the other hand, Quickcheck (and similar libraries, have a look to HedgeHog) will randomly check all the possibilities, but the tests are spread among many test runs. Keep in mind that tests can prove the presence of bug, but not their absence, so this is still better than nothing. Designing smart random data generators (i.e. `Arbitrary` instance or `forAll`) is something difficult. My rule of thumb, go to something simple AND which is fast / small enough. Don't hesitate to have many generators (for example, small list, small list of minimum 2 items, small list of minimum 4 items, ...). If you found a regression, do not try to change your generator to create a smarter one which target this regression, just add a new test and a new generator. 
You can use both QuickCheck and HUnit, they are complementary approaches, and one is not meant to replace the other. Unit tests are useful for corner cases that are difficult to generate and that need to be consistently checked, random testing generates lots of tests that you may not have thought about.
I found Tom Stuart's talk on it a great intro. I was able to introduce my non-Haskell coworkers at the time to the idea with ease. At the time, I was just learning Haskell and it cleared up a lot for me too.
I don't have one on hand that uses `ContT` itself, but the conduit library and `Text.ParserCombinators.ReadP` both implement monads with identical structure to `ContT`.
I'm really impressed with how clean the code is for [`haskell-lang`](https://github.com/haskell-lang/haskell-lang). 
Looks beautiful! Did you enjoy working with brick? I've been thinking about migrating a big project to brick from ncurse..
Wow, sounds great, I'll definitely give brick a try then. I cloned and built tetris with no problem. Very fun! One tiny, tiny suggestion: if you add nix: packages: [gcc, ncurses] to the end of your stack.yaml file, you'll make it easier for people using nix to build.
Looks alright to me, can you also update the doc to make it clear that you don't need to be root. Using curl as root isn't the best plan.
Hey, I never knew goldfire(re) wrote Glambda. :O
&gt; If you are running your test again just to avoid the failure then you aren't really using QuickCheck to its full effectiveness. But what if you're running it again because you're worried about a spurious success?
So, I have a related question - has anyone proposed a system which automatically adds new unit tests each time a quickcheck test fails? This seems like something that would be incredibly handy.
I'm not that familiar with EDE, but the main differences that I can think of after skimming it quickly: * The Dhall language is backend-agnostic. EDE, on the other hand, is highly specialized to templating text. This is the reason why Dhall supports more integrations. For example, you can [marshal Dhall expressions into Haskell datatypes](https://hackage.haskell.org/package/dhall/docs/Dhall-Tutorial.html), compile to Dhall to Nix (including functions), or generate JSON/YAML/Bash from Dhall code * Dhall is more general: it's lambda calculus + some builtins + an import system. In Dhall, templating text is just a special case of applying a user-defined function (i.e. the template) to a user-defined value (i.e. the payload), where either the function or value may be optionally imported. Similarly, validating a schema in Dhall is just adding a type annotation. * Dhall supports user-defined functions within the Dhall language * All of Dhall's integrations include a command-line interface (such as the `dhall-to-text` executable for templating text). This lets you template from the command line without writing any Haskell code. More generally, there is nothing Haskell-specific about Dhall * Dhall has a more sophisticated import system, meaning that you can import values from files, URLs (with custom headers), and environment variables. Imported expressions can have transitive imports of their own * Dhall has a [Prelude of utilities](https://ipfs.io/ipfs/QmRHdo2Jg59EZUT8Toq7MCZFN6e7wNbBtvaF7HCTrDFPxG/Prelude) hosted online and you can use any of these Dhall functions by their URL * Everything is an expression in Dhall and you can import or infer the type of any expression. For example, since a template in Dhall is just a function, and a function is just an expression, that means that you can query the type of the template. Contrast that with EDE where you can't query the type of a template in order to discover what sort of payload you must provide. Types are also expressions in Dhall, and a schema validation is just a type annotation, so you can validate an expression with an imported schema (type). There are lots of other differences, but those are the big ones However, I think the best way to compare the two is to use both. I recommend going through the [Dhall tutorial](https://hackage.haskell.org/package/dhall-1.4.2/docs/Dhall-Tutorial.html) if you haven't already. Once you use Dhall you'll see that it's a much more general tool that just happens to support templating text.
So, for example, the [shakespeare](http://hackage.haskell.org/package/shakespeare) library is a type-safe templating system. Any interpolated data into the template is at Haskell types you define in your application, and your template will not compile if your interpolated data does not type check. Obviously that itself does not guarantee that there are no injections. But the fact that you're using templates adds zero additional risk. And it's not hard to write a web application which verifies at compile time that none of the common kinds of injections exist. I haven't tried it yet, but with Dhall, dependent types could make it even easier to provide compile-time guarantees - in this case actual proofs - that the common injections do not exist in your web app. Although it also might be a bit more awkward to integrate the Dhall types with the Haskell types in the web app, compared to shakespeare where the types **are** the Haskell types in the web app. EDIT: To be more clear - I am claiming that typed templating systems not only do not add risk for injections, they can significantly *reduce* the risk, providing strong compile-time guarantees that injections do not exist.
&gt; This seems like misappropriating a tool that's used for non-content data. This is content. These are used for tests, even if not every developer should use them because of machine-to-machine variability, this would mean moving a ton of useful data out of the file tree and into this esoteric system that adds yet one more thing that developers have to learn to work with GHC. You seem to be under incorrect assumption that the current situation where you have to manually update performance data is a feature rather than a short coming of the current performance test-suite. The whole point of this change, as far as I understand. Is to make it so you don't HAVE to edit any files to update any numbers, because you, won't have re-portable numbers. Unless you have the same exact hardware as everyone else? &gt; As a newcomer currently hacking on and working on patches to GHC, trust me when I say: we don't need to add more tooling and more complications here, and we don't need to make it harder for even very eager folks like myself to discover where data exists in the repository. Why do you need to know how and why read-only data is stored? All you as a developer need to know is what difference in performance your patch makes. That's it. You don't need to know anything else. Just like the test-suite reports numbers for perf test right now, it will continue to do so, without the need to read a json file. &gt; this would mean moving a ton of useful data out of the file tree and into this esoteric system that adds yet one more thing that developers have to learn to work with GHC. Again, large misconception. You don't need to know it to work on GHC. Infact you shouldn't know it. The fact that you do is a short coming, not a feature. And the data is only useful if it's accurate, currently it's more of a ball park number, some tests have variances of +-20%. &gt; Again, why is this better than just storing a T7023-perf.json object? Because you're polluting the tree with test output that change often and are not features. If the system is adopted that a CI tests each commit, such as perf-haskell (which is what we generally use to validate perf numbers anyway) then logically the CI should update the file with the latest test data. Which means, for each commit, you are introducing another commit to update test files. You can't amend the commits as this, aside from changing the hash will require a force push, and we don't allow force pushes for obvious reasons. Currently perf just stores the numbers in a database and the values in the tree are not corrected. Keeping the files in tree actively prevents us from having a centralized server perform and update perf data for EACH commit and report it back as part of the tree. So someone doesn't have to go in later and manually update the data. &gt; Is there a compelling argument for not storing the data in-tree so that existing tooling will continue to work for developers that do want to work with the perf data? e.g.: text editors, arc and Phabricator, visibility on the GitHub mirror and git.haskell.org, diff tools that assume changes exist in files and not "notes", etc.? You're trying to find a problem with a solution instead of a solution for a problem. But fine. 1) arc nor Phabricator care about perf numbers. Your commits don't get blocked because of bad perf data, and never will. Arc diff can be updated to push notes as well, this is not a problem. each diff is backed by a git repo. 2) Very few people use the stats file updates on a differential to determine the performance change. Most look at https://perf.haskell.org/ which again, has re-portable/stable numbers. Looking at stats files in Phabricator has very little point, nothing has verified those numbers! 3) github same, the numbers in the git repo are rarely looked at directly. Of course they're there to get updated so perf.haskell.org can verify them. hopefully you don't even need to update them anymore, perf can just add a note with the numbers it observed. I don't think the proposal is to just store the data somewhere else and call it a day. As far as I understand the proposal is to make perf data more reliable. This is not compatible with "every developer manually updates perf numbers".
The form requires a Google sign in. I can make up a fake account for the purposes of this form but if you can just remove that requirement it would be better.
How much work is actually involved? The options for "How much time would you be able to spend" are a bit intimidating :-)
The easy answer to that question is "that was the original plan" – I wanted a dependently typed music composition library, not a music composition library which I later decided to implement with dependent types. I do however think that there are benefits to making the checks static which would even outweigh the long-ish compilation times. First, you get the usual advantages of having static types: you can ensure that all arguments to functions are "correct" and all outputs from functions must also be "correct". As an example, see the [`voices`](https://github.com/DimaSamoz/mezzo/blob/master/src/Mezzo/Compose/Combine.hs#L74) function: the number of voices of two sequentially composed pieces is equal to the number of voices in one of the pieces, as the GADT for `(:|:)` statically enforces that the number of voices must be equal. Second, I wanted to use the Haskore music algebra as the "composition interface" (because I found it very intuitive), but I found no good way of implementing musical rules in it because of the recursive, tree-like structure of pieces. Therefore it was necessary to either compute or maintain a more structured, abstract representation of the music, so doing dynamic checks would not have made the implementation significantly simpler. The third, and probably the most practical benefit is the errors: with compiler errors (and suitable editor), you not only get notified of mistakes when you save the file (i.e. no need to load or execute anything), but get very clear indication of the location of the error, usually as some red squiggles or highlighting right in the source code. However, I do agree that compile-time errors have their limitations and doing runtime analysis would give more context and even suggestions for correcting the scores. This binary pass/fail rule-checking is what rule sets attempt to solve (originally the rules were completely baked into the system, but I realised that that would be way too restrictive). In practice, what I ended up doing most of the time is composing a piece, seeing what errors it produces and then turning off rule-checking to "force" compilation. This made sense with transcribing existing pieces (as you can't expect every piano composition to follow the rules of strict counterpoint), but of course if your aim was to avoid all errors (e.g. if you were doing some composition assignment), you would turn on the strictest rule set and leave it turned on. Musical rules are definitely not binary, but there is not much flexibility with compiler errors :) I do want to try reimplementing Mezzo with normal runtime rule-checking and see how it compares though!
The libraries you use, or want to use. It never hurts to be able to debug a layer (or two) down from your application code.
&gt; It already is. Has been for a while. Actually, that's not what I meant. What I meant here was: right now most of the offerings aren't even complete. Compare trying to host on Azure vs. Amazon for .Net development (or Java for that matter), for example. At some point you'll be able to expect all cloud vendors and all SaaS to be able to provide the same services and price will be all that's left to compare. Today, for a lot of things you need to do it doesn't matter so much if one vendor is 10% more expensive because the other vendors can't actually support your needs. &gt;That's only true if getting as far as the proof, and maintaining the proof in the face of changing requirements, is cheaper (which includes recruitment and training) than doing that with the tests. Actually it's ok if getting the proof is X times more expensive than the standard level of unit testing (for some size of X, not any possible X). The maintenance phase is much longer and more expensive than the development phase so the costs should eventually pay back, so long as maintenance is actually cheaper. &gt;I do not believe that this has been convincingly demonstrated for an economically meaningful class of problem. As far as I know, this has never been done. This is simply a belief of mine that I'm interested in perusing. The concept of "worse is better" would tell us that I have it exactly backwards, that crappy incomplete tests will win the day, but what keeps me interested in software is the hope that one day the correct way to do things will also be the most successful way. If terrible engineering practices will always win forever then I welcome our robot (or whatever) overlords taking software engineering away from humans.
Thanks! Yeah I might write a short tutorial on it, but the user guide is already really good to start off with. If I do write something up I'll be sure to post it here.
Related thread from haskell-cafe - https://mail.haskell.org/pipermail/haskell-cafe/2017-January/126152.html
I turned off everything related to google in the form, but I think google forms just require a google sign-in, I'll double check in a bit to make sure I didn't miss something. EDIT: I had 'limit to one response' turned on, which apparently requires a sign-in. I turned that off, give it a shot now.
As little or as much as you'd like, I probably spend somewhere in the vicinity of an hour/day across the various subreddits I moderate. The metric is really more about understanding how much of the week would be covered by your contribution if added as a mod.
&gt; The maintenance phase is much longer and more expensive than the development phase so the costs should eventually pay back, so long as maintenance is actually cheaper. Yes, that's what I mean by “cheaper”. Although, increasingly a product or system doesn't have _development_ followed by _maintainance_, it's all maintainance, including enhancement to deal with new, changed, or merely newly discovered requirements, from a point quite early in the lifecycle. So, getting to an initial proof of an initial implementation needs to arrive quickly and cheaply, and changing and enhancing and reporting the system needs to _continue_ to be quick and easy. It's not clear to me that this can be done. One nice thing about large suites of automated tests is that your tend to get moving hot-spots of activity but most of the time most of them don't change. Do proof-based approaches lend themselves to that? &gt; If terrible engineering practices… Here's a thing: good engineering is about tradeoffs. It's about doing a thing only just well enough, in order to be fast and cheap enough to be profitable enough. This seems to be fundamentally misunderstood amongst Computer Scientists at large and the FP community in particular. There's “good engineering” and there's “overbuilt”. Good engineering is the _very difficult_ and _extremely intellectually challenging_ discipline of making a product only just well enough—not making it as well as we possibly can make it, doing that is not engineering. 
My understanding is that using `INLINE` ensures the function will *always* be inlined. Without any pragma, only *some* small functions will be treated as `INLINABLE`, meaning their unoptimized code will be available in the `hi` file for GHC to decide on whether or not to inline later (though it usually will for these functions). `INLINABLE` functions are added to the `hi` for sure, but whether that code becomes inlined or not is still up to GHC.
This is the `DeriveAnyClass` language extension.
I learned a lot reading (much of) the code in `containers`. Most of the code in `Data.Tree`, `Data.Map` and `Data.Set` is pretty clean, though I suggest you steer clear of `alterF` on your first skim. The same goes for `Data.Sequence` (but don't hit `&lt;*&gt;`, `cycleTaking`, `zipWith`, or `chunksOf` first). `Data.IntMap` and `Data.IntSet` are definitely the most intimidating, and I still haven't studied them properly. Another nice package is `machines`. It's kind of experimental, and not an ideal end-state, but pretty much everything is really simple. Elsewhere in the stream space, I've been meaning to take a look at how the `streaming` package works. Yet another: `attoparsec`. It may not be the best library for parsing programming languages, but its relative simplicity makes it more approachable than most others. Edit: If you decide to read (or even use!) `aeson`, please don't be influenced by the less fortunate aspects of its design. The `ToJSON` and `FromJSON` instances for `HashMap` make me sad, and some of its parsing terminology is a bit strange.
&gt; If your test is flaky and you are sure the thing you are testing must be true, then you have a bug in your code. If you are running your test again just to avoid the failure then you aren't really using QuickCheck to its full effectiveness. &gt; Then make the default number of runs rather high instead of running 100 tests over and over. For example, the `unordered-containers` package uses a default of 30000 tests.
That's fine. I'm not suggesting you never run tests more than once. In fact I'm suggesting allowing your tests to run often so you have a higher chance of seeing a failure. QuickCheck is like the null hypothesis tester of tests. Your null hypothesis is that the test will fail, and your are trying to gather tons of evidence that the null hypothesis is false. However, one failing test is enough to prove the null hypothesis true.
One place where the heuristics don't work is inlining all the typeclass instances required for `mtl` style of coding.
I've added some Rust code for comparison, see the link in my other comment on this page (but the Rust code uses UTF-8 instead of UTF-16).
&gt; Well, most of what's actually happening in the software world is not this. Certainly not, I don't claim that it is. But I also refute the easy soundbite that writing proof-y code in a strongly statically typed pure functional language would be “good engineering” all by itself, while writing code in anything can't be. I don't ascribe that view to you, but there's a lot of it about. &gt; I don't have time to write the amount of tests I would need in a language with poorer typing. Have you considered that you might not be writing the right tests? If you are writing test that duplicate the checks that the type cheer would check if you had one then, yes, that's not a good use of time. But those are not the only kind of test, nor are they the kind that you should write most of, nor are they the kind that you should be writing even if you do have a checker. When I read a lot of FP material, and especially Haskell material, I reminded of what some leading Lisp programmers (oh the irony!) used to say about Scheme: the goals of the Scheme community are so far removed from the goals of the average working programmer (remember, it was big name Lisp folks saying this!) that Scheme is unlikely ever to have much success in industry. Here's an interesting, and true, observation: &gt;The whole "correctness proof" line is silly because no one writes correctness proofs. Real programs - programs that solve real problems - are generally large and complex enough that even if you're using a language like Haskell, writing a correctness proof still isn't practical. Sure, you can easily write a correctness proof for an implementation of a binary search tree in Haskell. You can write a correctness proof for that in, say, Java. But the Haskell proof will be easier and cleaner, and you'll be able to trust the connection between the code and the proof much more than you could in Java. But really, a good set of tests - which you should be writing, whether you're programming in Java, C, or Haskell - does as good a job of verifying the correctness of something simple like that. And for anything significantly more complicated than that, people just don't write proofs. I've certainly never seen a correctness proof for a Haskell program, except in papers arguing about how provable correctness is in Haskell. (Does GHC have correctness proofs? I doubt it. It's certainly had quite its share of bugs, which means that either it's correctness isn't proven, or the proof is wrong!) —[girlloveshaskell](http://girlloveshaskell.com/index.html)
Is there a place on github where this spacemacs dante layer lives and is organized?
I know I'm totally side-stepping the main thrust of your question, but I implemented a slightly different algorithm that doesn't require a hashmap using the vector package. The idea is to: 1. Create a vector of `Bool`s for all 2 million numbers, each set to `False` to indicate that it's prime. (Yes, `True` would be more logical, this was an optimization.) 2. Iterate through all the odd numbers from 3 less than 2 million 3. Look up the value in the vector at that index. If it's `True`, then the number is not prime, and iterate. 4. If it's `False`, then loop through all multiples of that number and set the value in the index to `True`. 5. For each prime number, update a `total` value by adding the current index. The total value starts at 2 for the first prime number (which we don't look at directly). Here's my code: #!/usr/bin/env stack -- stack --resolver lts-8.12 script --optimize {-# LANGUAGE BangPatterns #-} import Control.Monad.ST import qualified Data.Vector.Unboxed.Mutable as V n :: Int n = 2000000 main :: IO () main = print $ runST $ do v &lt;- V.new (n + 1) let loop !idx !total | idx &gt; n = return total | otherwise = do notPrime &lt;- V.unsafeRead v idx if notPrime then loop (idx + 2) total else let loop2 !idx2 | idx2 &gt; n = loop (idx + 2) (total + idx) | otherwise = do V.unsafeWrite v idx2 True loop2 (idx2 + idx) in loop2 (idx + idx) loop 3 (2 :: Int) On my machine, this runs in: real 0m0.024s user 0m0.012s sys 0m0.008s By contrast, the C++ code runs in: real 0m0.072s user 0m0.065s sys 0m0.003s Of course, this is an apples-to-oranges comparison, since they're different algorithms. __EDIT__ I realized there's another constant factor improvement, by incrementing by `idx * 2` instead of just `idx` in `loop2` to avoid dealing with the even numbers.
&gt; Have you considered that you might not be writing the right tests? I think I'm pretty good at writing tests but the fact is, if you write proper unit tests you will end up with more tests than code because you need to hit every branch (the critical branches at a minimum). I don't think I've ever written type checking tests. &gt;I reminded of what some leading Lisp programmers (oh the irony!) used to say about Scheme: the goals of the Scheme community are so far removed from the goals of the average working programmer (remember, it was big name Lisp folks saying this!) that Scheme is unlikely ever to have much success in industry. Here I wonder if you might be conflating what haskellers tend to talk about (i.e. things like type theory) with their actual goals. I'm not sure people are using type theory for bizarre, far out-there goals but rather as a means of time saving by coming up with very general solutions. For example, the `lens` library has some pretty advanced features but what it brought us was not only a nice solution for nested record access (finally!) it gives a nice solution for nested access in general. It you compare what you had to do before to what you can do now it's not only more "correct" (for some definition... etc.) it's actually less effort than similar tasks were in the past. As for the observation, there's some truth to it but the knock on GHC is a little unfair since GHC predates so much work that has happened since its creation. I would also say that I would expect the goals with proofs to be like the goals of composition in general: make small, provably correct pieces and build your programs with them. The observation also has a bit of a "blub" feel to me: of course "no one writes correctness proofs" as it's traditionally been so difficult to do, where writing tests has been easy for a long while. It will be interesting to see what the landscape looks like in, say, 10 years with the advancement of things like Idris.
Out of curiosity, how well would this Haskell code of mine compete? It uses mutable state and gets the sum to (2*10^9) in 0.3 seconds on my workstation. primeSum :: Integer -&gt; Integer primeSum n = (primeLucy id (\m -&gt; div (m*m+m) 2) n) n primeLucy :: (Integer -&gt; Integer) -&gt; (Integer -&gt; Integer) -&gt; Integer -&gt; (Integer-&gt;Integer) primeLucy f sf n = g where r = fromIntegral $ integerSquareRoot n ni = fromIntegral n loop from to c = let go i = ControlMonad.when (to&lt;=i) (c i &gt;&gt; go (i-1)) in go from k = ArrayST.runSTArray $ do k &lt;- ArrayST.newListArray (-r,r) $ force $ [sf (div n (toInteger i)) - sf 1|i&lt;-[r,r-1..1]] ++ [0] ++ [sf (toInteger i) - sf 1|i&lt;-[1..r]] ControlMonad.forM_ (takeWhile (&lt;=r) primes) $ \p -&gt; do l &lt;- ArrayST.readArray k (p-1) let q = force $ f (toInteger p) let adjust = \i j -&gt; do { v &lt;- ArrayBase.unsafeRead k (i+r); w &lt;- ArrayBase.unsafeRead k (j+r); ArrayBase.unsafeWrite k (i+r) $!! v+q*(l-w) } loop (-1) (-div r p) $ \i -&gt; adjust i (i*p) loop (-div r p-1) (-min r (div ni (p*p))) $ \i -&gt; adjust i (div (-ni) (i*p)) loop r (p*p) $ \i -&gt; adjust i (div i p) return k g :: Integer -&gt; Integer g m | m &gt;= 1 &amp;&amp; m &lt;= integerSquareRoot n = k Array.! (fromIntegral m) | m &gt;= integerSquareRoot n &amp;&amp; m &lt;= n &amp;&amp; div n (div n m)==m = k Array.! (fromIntegral (negate (div n m))) | otherwise = error $ "Function not precalculated for value " ++ show m 
Is there something to automate this process?
In case you are curious, the same algorithm gives the sum to (10^12) or 18435588552550705911377 in 19.6s (if given enough memory). The time taken to get the sum to 2 million is within measurement error of 0. 
I added those imports to get it to compile import qualified Control.Monad as ControlMonad import qualified Data.Array.Base as ArrayBase import qualified Data.Array.ST as ArrayST import qualified Data.Array as Array but got that error instead stack ghc -- -O2 primediv.hs [1 of 1] Compiling Main ( primediv.hs, primediv.o ) ghc: panic! (the 'impossible' happened) (GHC version 8.0.2 for x86_64-unknown-linux): initTc: unsolved constraints WC {wc_insol = [W] integerSquareRoot_a441 :: t_a440[tau:1] (CHoleCan: integerSquareRoot) [W] integerSquareRoot_a44n :: t_a44m[tau:1] (CHoleCan: integerSquareRoot)} Please report this as a GHC bug: http://www.haskell.org/ghc/reportabug 
You just implemented naive version of solution for this problem. It's known as eratosthene sieve. It works good for small `n` like 2 * 10^6, but task from question runs this on `n = 2 * 10^9`. You can't possibly store billion elements array. Of course, there's some technique how to make sieve work for large `n`. But algorithm from answer has faster performance and better asymptotic because it uses even cleverer approach with dynamic programming.
I worked for Standard Chartered once, not anymore.
Sure, but now we've got tests that take a very long time, and running those in line in a typical CI setup is painful - builds already take too long. I suspect that we should move our generative testing out of line - why not have it run continuously?
All true. It was the constant factor optimizations that I want completely certain of in this case.
Hi, I'm the author of Brick - this is awesome! I'm glad you had a good experience using the library. :)
Sorry for leaving out the imports and for running across that integerSquareRoot bug. For the relevant magnitudes, you should be able to drop in this code as a replacement without any impact on performance: integerSquareRoot = floor . sqrt . fromIntegral
Don't go to Utrecht. They don't make up for what they lack in mathematical logic (w.r.t. U of A) in any way. For instance, there are some courses on program analysis, but the logical foundation is quite shallow, and there's *not a single mention* of practical applications, methodology, didactics of software engineering, or even history.
Despite its influence, it's important to keep in mind that category theory doesn't *really* hold up well for Haskell. For example, `()` is not a terminal object, since there are at least two distinct "morphisms" from `Int` to `()`: foo, bar :: Int -&gt; () foo = const () bar = undefined
Can you post the entire source file that caused this panic? If it's present in GHC 8.2 then one of us should file a bug 
This is true in general, if all you are doing is exact replay. Once we're playing canned test cases, though, we might want to do more work to ensure a good set of cases (maybe look at coverage, or even break out the SMT solver to find edge cases, or...?). In which case it wouldn't apply.
What do you mean by saying _run sieve for an sqrt(n)_? It's enough to filter primes but you still need to distinguish primes from non-primes. Thus you need to store sieved result somewhere. So you can't have that enough memory (and time).
Yeah, but that's not fast enough for this problem. It's a solution but not so optimal.
https://www.reddit.com/r/haskell/comments/6i1utt/quickcheck_randomly_generates_data_im_wondering/dj3jl24?context=2
Well, for 2*10¹⁰ it doesn't work ;) While optimized algorithm from question can work up to 10¹⁶ because it requires sqrt(n) memory. But that's not the main point. Sure, using bits instead of bytes can help with memory problems. But you still have time problems. You still at least need to test every bit up to `n`. And you just possible can't wait up to 2*10⁹.
How does this compare to what's in the `categories` package?
[Fast and Loose Reasoning is Morally Correct](http://www.cse.chalmers.se/~nad/publications/danielsson-et-al-popl2006.html)
Some solutions that weren't posted yet: * https://github.com/massysett/anonymous-sums * https://hackage.haskell.org/package/compdata-0.10/docs/Data-Comp-Sum.html These were already posted: * https://nikita-volkov.github.io/first-class-sums-and-products/ * http://hsyl20.fr/home/posts/2016-12-12-control-flow-in-haskell-part-2.html Also, this package does not provide anonymous sums but may be solving the problem you want to solve with them: * https://github.com/jdreaver/sum-type-boilerplate This one is so far beyond my comprehension that I don't even know whether it provides anonymous sums. But I think it does: * http://hackage.haskell.org/package/yoko And here is a technique to make using plain old (nested) `Either` less painful: * https://mail.haskell.org/pipermail/haskell-cafe/2011-June/093364.html
Because uniqueness is required for universal maps. In a total variant of Haskell, `()` would indeed be terminal, as the only *total* map from any type `a` into `()` is the constant map. However, since Haskell allows partiality, uniqueness can no longer be guaranteed, as Eat_More_Robots's example shows. This does not mean that we cannot use categorical notions to make sense of structures in Haskell. It just means that we need to be a little more careful about it. Using restriction categories, or working in the Kleisli category of a suitable classifying monad, seems a better idea to account for partiality.
I discuss [the `categories` package somewhat in the article](https://github.com/rampion/kinder-functor#a-more-categorical-definition). The `categories` `Functor` is strictly more powerful, as you can define its `Contravariant` in terms of its `Functor`. The `categories` package's Functor isn't currently poly-kinded, but there's no reason it couldn't be (no `TypeInType` required). You can see [my test code](https://github.com/rampion/kinder-functor/blob/master/Kmett.hs) for an example.
Would it be better to just store the data in a second branch or another repo, though? And store the test suite *target* data, for users who wish to invoke the *optional* performance tests, in-tree so that they can be viewed in editors as well? My #1 issue with the way GHC is developed right now is visibility. Until very recently, hacking on GHC was an enormous pain and it seemed like the core contributors put in as many ways to disincentivize developers as possible. The ways this was done was primarily by adopting non-standard tooling and non-standard approaches to software development. Look: git notes is a cool feature, but there's almost zero real world adoption and the tooling doesn't exist to provide great visibility into it. If I wanted to write a simple website that tracked perf data over time, and - hypothetically - say I didn't want to host the repository myself or clone it periodically and instead just have it generate the data via a `cron` job that pulls data from git.haskell.org or github.com/ghc/ghc, is that possible? GitHub deprecated git notes functionality in 2014. I'm not sure if it's even possible using their API to view git notes. I can't find any evidence that it's possible using the GitWeb cgi script that powers `git.haskell.org`. Git notes do not support merge tool use, rebasing, and so on. This is with the real git command line tool. There seems to be almost no real world use of git notes other than Gerrit, which I'm sure spent significant developer resources on ensuring their web interface worked well, but their use case doesn't ever involve other people being able to work with the data stored in notes. Can you point to a project other than Gerrit using notes effectively? Are there any case studies on using notes? And can you point to *any* existing tooling for working with notes in an automated way, either displaying them as part of a web-based frontend to Git (e.g.: GitHub, GitLab, BitBucket, GitWeb, etc.), or in a text editor (Atom, VS Code, Sublime, etc.) and so on? I think you cannot, and for that reason, I think going down the road of using git notes is committing GHC to *yet another* obscure use of tooling to contribute, *yet another* set of commands and tools I have to memorize to be proficient as a contributor, and so on.
&gt; `int`imidating
Full disclosure: I'm a lecturer in Utrecht. Both are good options. Like you say, it's a matter of what you would like to learn during your MSc. The Master in Logic is a much better choice if you're interested in Philosophy or Mathematical Logic. Given that you're posting this to the Haskell Reddit, I'm guessing you're interested in programming in Haskell. If so, Utrecht is probably a better choice. We offer several courses using Haskell (including Advanced Functional Programming, Software Technology for Teaching and Learning, Compiler Construction, and a few others). We have been teaching Haskell for at least 20 years and it is pervasive throughout our MSc degree. I don't believe that this holds for UvA's MSc program. Don't hesitate to get in touch if you have any specific questions. 
For instance, what is the dual of "rekt"?
Then the options you listed are definitely misleading, since they make it appear that dedicating one hour a day to moderate just /r/haskell would be on the low end of the scale.
This is interesting, do you think this method can be applied to the boilerplate related to database interaction? Very often we need a type for regular use, one for the representation in the database. The approach in Opaleye is to use full polymorphism for each parameter, but I always asked myself if there was not another option.
Can someone with more experience using these kinds of TypeFamily based approaches speak about them in practice? - How bad is the compilation penalty? - Does this extend well / play well with common libraries? - Does this make development unnecessarily hard? I love seeing all these cool techniques in demo / toy languages (and projects) but I always wonder how they scale up. 
You can get a similar default mechanism with hask, type Funct i j = i -&gt; j instance Functor (Maybe :: Funct Type Type) where type Dom Maybe = (-&gt;) type Cod Maybe = (-&gt;) fmap :: (a -&gt; b) -&gt; (Maybe a -&gt; Maybe b) fmap f = \case Nothing -&gt; Nothing Just a -&gt; Just (f a) by defining type family DOM (f :: Funct i j) :: Cat i where DOM (f :: Funct i (i -&gt; Type)) = f -- silly DOM (f :: Funct i _) = DOM' i type family DOM' (i :: Type) = (res :: Cat i) | res -&gt; i type instance DOM' Type = (-&gt;) type family COD (j :: Type) = (res :: Cat j) | res -&gt; j type instance COD Type = (-&gt;) class (Category (Dom f), Category (Cod f)) =&gt; Functor (f :: Funct i j) where type Dom f :: Cat i type Dom f = DOM f type Cod f :: Cat j type Cod (f :: Funct i j) = COD j letting us omit the associated instances instance Functor Maybe where fmap :: (a -&gt; b) -&gt; (Maybe a -&gt; Maybe b) fmap f = \case Nothing -&gt; Nothing Just a -&gt; Just (f a) and also things like `Dom Coercion = Coercion`, `Dom (:~:) = (:~:)` and type instance DOM' Constraint = (:-) lets us write a `Functor Dict` instance, without specifying either `Dom` or `Cod` instance Functor (Dict :: Funct Constraint Type) where type Dom Dict = (:-) type Cod Dict = (-&gt;) fmap :: (a :- b) -&gt; (Dict a -&gt; Dict b) fmap p Dict = case p of Sub q -&gt; q
* do you mean performance of the compiled program or compilation speed? The equalities have no performance penalty, and the indexed types are similar to polymorphic types. I haven't noticed compilation slowdowns * the equalities (like gadts, which are implemented with equalities) do not play well with deriving and generics, which is why I usually pack them up in a smaller type for which I define instances manually * no, it's very ergonomic Note that I use this stuff at my day job and Ben uses it for the disciple compiler (I think), so it's definitely a trick which is useful in practice.
I have not thought about that, somebody should try!
Yes sorry, I meant the compilation speed penalty, I know that most advanced type level features carry a compile speed cost. Is it required to use GADTs? Or Can this work using normal ADTs as long as you don't have existential values / equalities? 
Link to paper: [Trees That Grow](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/trees-that-grow.pdf)
True, but I can still use the type family to represent annotations evolving over stages (as well as things like names). It's an improvement over having many normal type params, especially since only specific combinations of them are desired. 
Agreed.
I still believe that lenses are the simpler solution to this problem, as I explain in this post: * [Explicit is better than implicit](http://www.haskellforall.com/2015/10/explicit-is-better-than-implicit.html) The "over" function from lens generalizes fmap so you can do "over _Left" or "over _1"
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [ekmett/hask/.../**Category.hs#L88** (master → cd4d30e)](https://github.com/ekmett/hask/blob/cd4d30e7911dd7cc2da78383fd833272b1ff9303/src/Hask/Category.hs#L88) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dj4ndnr.)^.
I usually just say "for a hypothetical total and wart-free subset of Haskell" to work around these issues
That's prone to stack-overflow depending on your desired solution size (and this version does so in MSVC). `std::bitset` would be a "more C++" solution, and less prone to that problem (`std::vector&lt;bool&gt;` needed for much bigger cases): #include &lt;bitset&gt; #include &lt;iostream&gt; int main() { std::bitset&lt;2'000'001&gt; is_composite; std::uint64_t total = 2; for (size_t i = 3; i &lt; is_composite.size(); i += 2) { if (!is_composite[i]) { total += i; for (size_t j = 2*i; j &lt; is_composite.size(); j += i) { is_composite[j] = true; } } } std::cout &lt;&lt; total &lt;&lt; std::endl; } 
&gt; Fixing this requires that, among other things I want this to move forward but I'm not sure what is needed from your examples, are we missing language features or can it be encoded using clever engineering
&gt; Now the serious magic begins Oh dear... 
If I may hijack this discussion to ask about [subhask](https://github.com/mikeizbicki/subhask), do you know what is the latest status on that effort to re-write/re-imagine the Prelude? I'm very interested in the idea of Subhask and the direction it takes (as I would like to see better/cleaner numerical computing capabilities in Haskell). The latest commit to that seems to be a year old now. Any new changes that you are aware of? Do (semi)dependent haskell features (like TypeInType) make a difference?
Didn't get it. What's specialization vs inlining? I was investigating a significant slowdown in using lucid"s transformer stack and missing INLINE pragma's was why it eventually boiled down to. 
This is indeed nicely lightweight :)
Specialization is GHC's ability to rewrite a function for a given instance of a type class. When you write a function that is polymorphic on a `Num` type, for example, specialization can recompile that function for `Int` specifically, in order to take advantage of optimizations available to that type and it's instance. Specialization often occurs when a function is inlined because the compiler is able to see exactly what types are in use. &gt; missing INLINE pragma's was why it eventually boiled down to. I'd imagine that many of those `INLINE` pragmas could be replaced with `INLINABLE`, which should help both compile time and runtime performance. When a function is `INLINABLE`, GHC can specialize it anywhere, however it wants. When it is marked `INLINE`, it will always inline it (which in many cases is actually a performance *loss*), but likely won't specialize it any more often than `INLINABLE`. Without either, GHC is completely unable to specialize it, except for within the module it was defined in. You can also add `{-# SPECIALIZE foo :: Int -&gt; Int #-}` to make a specialized (and optimized) version available to all other modules.
I've been working on something vaguely similar: https://github.com/mrkgnao/noether/
For me, the key difference between "continuously" running the generative tests (as I was using the term) and current practice is what happens when there *aren't* new commits. When resources provisioned for testing would otherwise be sitting idle, they could be running generative tests. As to your question... When a new commit does come in, we have choices. I don't see a clear winner, all things considered - most points in this space sound like they might be reasonable. We could treat things as very static, finishing a complete run before going on to run again against the same commit or another one. If multiple commits have come in we're faced with the choice of whether that "other one" is the next or the newest. Alternatively, we could be substantially more dynamic - at any point running against the newest code the tests that are most likely to be useful, based on when each test ran last and what has been modified since.
``` // evil floating point bit level hacking ```
Sure. The idea with a classifying monad is that partiality is an effect, and so should be handled within the framework of effects – monads. The idea is to produce a monad `m` and a bijective correspondence between maps of type `a -&gt; b` and those of type `a -&gt; m b`, such that the total maps `a -&gt; b` are precisely the pure maps `a -&gt; m b`, while partial maps `a -&gt; b` are the corresponding impure maps . Such a monad could be the `Maybe` monad (though this requires partiality to be decidable, which is not the case for computable functions in general) or the `Delay` monad (which makes no such assumptions), i.e., data Delay a = Now a | Later (Delay a) Restriction categories go a somewhat different route by axiomatizing partiality in a certain way. This is done by assigning to each map `f :: a -&gt; b` a restriction idempotent `f̅ :: a -&gt; a` satisfying equations such as `f o f̅ = f` (and others). Intuitively, the restriction idempotent `f̅` can be thought of as a partial identity function defined precisely where `f` is defined. The whole point of this exercise is to snuff out structure, as partiality seems to mess up a lot of universal properties unless we take steps to handle it explicitly. As discussed, `()` fails to be terminal. However, if we have a gauge of "how partial" a given map is (which is precisely what restriction idempotents give us), we can define analogues to these properties (in a sensible way) that make sense for categories of partial maps. For example, for terminality, the appropriate notion when partiality is involved turns out to be (essentially) that for each object `a`, there is a unique *total* map `a -&gt; ()`.
(Shameless self-plug:) Another option might be: https://github.com/BardurArantsson/pg-harness It has the advantage of running independently of the code being tested and so will always do proper cleanup. (A disadvantage is that you don't get stderr/stdout, but the drivers should give you most of the interesting output anyway :) ) E: Whoops, meant that to be a top-level reply, sorry about that.
...and also the infamous Vfefe ←→ Covfefe duality
I use vim with Tabular and a few other customizations.
Once you have a more general notion of functor like described here you can use the lens formulation in far more exotic situations. Lenses into the category of constraints, lenses into data types indexed by a type parameter so you can talk about traversals of well-typed EDSLs... in many ways this is a stepping stone to _being_ more explicit in exactly the manner you prescribe. ;)
The code in categories was defined before polykinds existed. More recent approaches for this that take polykinds into account are in branches or in the hask repository on my github account. Unfortunately if it was just polykinded and kept in its current incarnation it runs into the same limitations you do here.
Very cool. I have limited experience with Haskell, but could definitely follow what was going on. Looks good!
Hrmm. I should fix those.
could you please share with us your haskell experience with vim?
Perhaps you could map a constructor annotation to Void. But I guess it would be more cumbersome when pattern-matching.
thank you for the great and awesome answer. I was confused between using `ghci` or `intero` also I have no idea what is the difference between both
Small typo: desugarIf :: (If ix ~ 'True, If ix' ~ False) =&gt; Expr ix -&gt; Expr ix' Should be `'False`.
Oh boy this reminds me how much I dislike Vimscript.
Without prior Emacs experience (and with prior Vi(m) experience), I'd advise go for Spacemacs. If you already used Emacs and have a config of you own, only now you want to use it to hack on Haskell code, then Spacemacs may not be for you. I'd advice to try Spacemacs for a few hours, see if it works for you. Then decide.
[removed]
Intero works with stack. If all your projects use stack then `'intero` is good. I'm not sure if it works properly with projects that don't use stack. If it doesn't you may want to use `'ghc-mod`. I think ghc-mod was a bit of a memory hog which is why I switched, but it was a while ago. If neither works properly (which happened to me once on a project with exotic and buggy dependencies, use `'ghci`.
I have used spacemacs but with `org-mode` and using `evil-mode` to create `.doc` , I did not use Emacs, thats why I was asking which is best suited for haskell development
Semantics but can we not pretend these are two different editors? Spacemacs is generally a framework to configure sets of emacs plug ins not a distinct editor. Its like saying I've driven a Civic but not a Honda which is better? 
Start with emacs and add the packages that you want.
What is the meaning of the "!" in the type definitions? eg &gt;bPrevHash :: ! BHash
Thanks i get it now :)
+1 for more mods. This is getting to be a pain.
+1 to more mods, even low-activity ones just to reduce the mean time till deletion of spam. It's getting quite annoying.
Both vim and emacs GUIs allow you to mouse click to move the cursor, and pretty much every terminal emulator I used in the past few years will forward clicks as well, which again both editors will handle. Personally I just prefer not having to move my hand to reach the mouse, I'm that lazy.
haskeleton and myprojectname in stack new example should be swapped.
[Caught 54 minutes in](https://www.reddit.com/r/haskell/comments/6idlny/obrien_and_omahony_will_scare_all_blacks_haskell/dj5dixs/).
[Caught 52 minutes in](https://www.reddit.com/r/haskell/comments/6idlny/obrien_and_omahony_will_scare_all_blacks_haskell/dj5dixs/).
This looks like GHC issue #13805, likely triggered by a newer XCode or OS X version: https://ghc.haskell.org/trac/ghc/ticket/13805
I never "move down 6 lines and right 20 columns". If I can see a word somewhere on the screen, and it's far from where I am, I'll typically use avy ([see demo-gif](http://pragmaticemacs.com/emacs/super-efficient-movement-using-avy/)), or if I know it's somewhere in the file, but I'm not sure where, I'll more often use `/` (regex search) or [swiper](https://github.com/abo-abo/swiper/#screenshots) (which is nice because it can fall-back to grep if the file is extremely large, but you don't even notice that it does that). It's *much* faster than reaching for the mouse and fiddling (though Emacs of course lets you do that too). Hitting `M-.` I go to the definition of the function, although when my cursor is on the function name the signature shows up at the bottom of the screen anyway. So no, I don't miss having to let go of my keyboard just to find a function definition :-) ----- Btw, here's a much better explanation of why "6 down and 20 right" is the wrong way to look at text editing: https://stackoverflow.com/questions/1218390/what-is-your-most-productive-shortcut-with-vim/1220118#1220118
Vim
nvim
Unfortunately I can't help you there, I have no idea how to upgrade llvm/clang.
It looks like the situation has improved *considerably* since I last looked at it (before version 1.0.0.0)! It may even be satisfactory now.
Technically you are right, but the user experiences between both system is totally different. I personally use Spacemacs and dont' know anything about emacs. Being a vim user it took about 2 mn to get up and running with spacemacs. This is not the case with Emacs. So it's more like saying I've driven a Civic but not a toolbox and a bunch of screws ...
I'm all ears for potential questions! (:
In what sense was `Any` distinguishable? I think if you could ever get GHC to conclude that a type was not equal to `Any`, then that was surely a bug.
I understood it as "How many hours of the day are you nearby Reddit to help with a moderation request", rather than "How many hours a day will you spend purely dedicated to moderation requests"!
Ah, I see.
 class Foo (t :: Bool) instance Foo 'False instance Foo 'True instance Foo Any instance Foo (Any Int) instance Foo (Any Monad) Any punched an infinite number of holes into the assumption that a kind only had a fixed set of members for a closed kind. Now it is just a type family, so you can't hang instances off it.
I agree.
Your instincts are correct. After using Emacs and Vim-flavored Emacs for a number of years I'm pretty sure the mouse is indeed faster for getting to random spots. I no longer believe the keyboard-for-everything hype. I suggest learning the keyboard shortcuts but just using the mouse when it feels right. Unfortunately with Emacs at least somehow the mouse targeting (catching?) isn't as good. I find it much easier to select a region of text with a mouse in IntelliJ vs. Emacs.
I used it a bit recently and the main drawback is it can become quickly really messy. In this [example](https://github.com/maxigit/Fames/blob/master/Handler/WH/Stocktake.hs#L500), I'm trying to parse a complex csv representing a stocktake. (A stocktake is basically , a box, it's dimension, it's content (style + quantity), who's done it and when). Some fields can be skipped and some combination of missing fields have different meaning. Also, some field which are blank can be filled with the previous value. Finally , I want to be able to display the parsed result displaying error if needed. So if a field is of type `a`, The result of the parsing should be `Either InvalidField a` where `InvalidField` is basically the initial value and an error message. When displaying the result of the csv validation, I need to show if a value as been actually provided by the user or guessed from the previous row. The type for a validate row would then be `ValidField a`. Finally, if a field is not needed for certain configuration I should be of type `()`. In practice I use a `Null` functor. An example of such configuration, is an item is lost (`QuickTake`). We can't a find a box, therefore it's quantity should be 0 but the box barcode and it's dimension should be left empty. Another one, is when doing the stocktake of a box which is sealed already has a barcode. The csv should only have the barcode, the operator, the date and leave everything blank. So depending on the `stage` a field can be of type `Either InvalidField a`, `ValidField a`, `a` or even `Null a`. This gives me the following type data TakeRow s = TakeRow -- Basic QuickTake BarcodeLookup { rowStyle :: FieldTF s Text Identity Identity , rowColour :: FieldTF s Text Identity Null , rowQuantity :: FieldTF s (Known Int) Identity Null , rowLocation :: FieldTF s Location' Null Identity , rowBarcode :: FieldTF s Text Null Identity , rowLength :: FieldTF s Double Null Null , rowWidth :: FieldTF s Double Null Null , rowHeight :: FieldTF s Double Null Null , rowDate :: FieldTF s Day Identity Identity , rowOperator :: FieldTF s Operator' Identity Identity , rowComment :: FieldTF s (Maybe Text) Identity Identity } Depending on the stage `FieldTF` will use the main type or apply one of the given functor : type family FieldTF (s :: TakeRowType) a z lk where FieldTF 'RawT a z lk = FieldForRaw a FieldTF 'PartialT a z lk = FieldForPartial a FieldTF 'FullT a z lk = FieldForValid a FieldTF 'QuickT a z lk = FieldForValid (UnIdentity (z a)) FieldTF 'FinalQuickT a z lk = UnIdentity (z a) FieldTF 'BarcodeLookupT a z lk = FieldForValid (UnIdentity (lk a)) FieldTF 'FinalT a z lk = a type FieldForRaw a = Either InvalidField (Maybe (ValidField (UnMaybe a))) type FieldForPartial a = (Maybe (ValidField (UnMaybe a))) The final result is quite complicated, as the problem is. I didn't designed that way, it just grew ... To be honest, given a field a stage I can't tell from the top of my head which type the field should be, even though I know the business logic. Had I to redo it, I probably try using a more conventional approach. 
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [maxigit/Fames/.../**Stocktake.hs#L500** (master → 359682e)](https://github.com/maxigit/Fames/blob/359682e5f21124b53336f50a95dcebb3c361808b/Handler/WH/Stocktake.hs#L500) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dj5qk5z.)^.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [m0ar/safe-streaming/.../**LMonad.hs** (master → f746cfd)](https://github.com/m0ar/safe-streaming/blob/f746cfdb8df35ac6d7dfbc50a9b2d45b71e704d4/src/Control/Monad/LMonad.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dj5qozi.)^.
&gt; Also, I think that neovim served as a kick-in-the-pants that vim development neede I agree :-). I switched to Spacemacs just before vim 8 and I'm really happy with it.
I'm assuming by more conventional you mean having a set of 'reified' types for each stage and mapping between them? Do you think this an issue with the approach in general or with it's usage in this situation? 
LMonad currently has `&gt;&gt;` as: (&gt;&gt;) :: m () ⊸ m b ⊸ m b I think it would be better to write this as: (&gt;&gt;) :: m a ⊸ m b ⊸ m b This makes it consistent with the way that `&gt;&gt;` and `*&gt;` are defined. This occasionally has performance implications.
The subtle tyranny of low expectations. 
That wouldn't be possible since `&gt;&gt;` ignores its argument, which is not very linear. Therefore `m ()` is the only instance that makes sense to blind, since it does not pass anything on!
Did you test tail recursion without strictness? I have a feeling the bang patterns aren't necessary.
What happens if an async exception arrives between the fork and the register?
ah brilliant, thanks!
If you are going to have very many processes pulling off the same queue, you should look into ways to reduce contention. Here's two good posts on that subject by a postgres person: [Queues, queues, they all fall down](https://johtopg.blogspot.com/2015/01/queues-queues-they-all-fall-down.html) [Queues in SQL](https://johtopg.blogspot.com/2010/12/queues-in-sql.html)
Hello! I asked a similar question here a month ago: https://www.reddit.com/r/haskell/comments/6aj4zb/what_cities_in_europe_have_the_most_thriving/
yah, inartful wording on my part. I don't think I can change the text w/o screwing up the sheet now, but it's definitely intended to be "How much time do you spend nearish enough to reddit to do moderation", and _not_ "How much time will you spend staring at the queue waiting for spam" 
Haha, actually right now it *only* has support for that glyph, but `-o` is the plan of course :) 
You are conflating evil with spacemacs. It takes 30 seconds to install.
Thanks for the resources. The queue was really just a motivating example for the iteration methodology. At some point in the future I want show how to benchmark the queue with criterion. I'll take closer look if/when that happens. 
I wouldn't say that `&gt;&gt;` ignores its first argument. The value of type `m a` is used, even though the `a` component is not. Consider the following type from section 2.5 of Retrofitting Linear Types: data Unrestricted a where Unrestricted :: a → Unrestricted a Notice the use of the non-linear arrow. Linear consumption of a value of type `Unrestricted a` only requires that we pattern match on `Unrestricted`, not that we consume the value inside. So, we could write: instance LMonad Unrestricted where (&gt;&gt;) :: Unrestricted a ⊸ Unrestricted b ⊸ Unrestricted b Unrestricted _ &gt;&gt; Unrestricted b = Unrestricted b ... I believe this will typecheck, although I don't have a build of the ghc linear types branch to confirm this.
Now ask us to explain why f = length ([f,f],[f,f]) also binds `f` to 1… :-)
I believe queue_classic reduces contention by trying to lock a job among the N first jobs at random. So they are not processed exactly in a FIFO manner but shouldn't be a problem in most applications.
This was the most important part of the article for me: &gt; Calling out to a C function from Haskell code carries some overhead. The primary cost is that the Haskell compiler can't inline and optimise across the language boundary; when the Haskell compiler sees a function written in C, it simply treats that as a black box. It sets up according to the correct calling convention, and then dispatches control entirely over to the code compiled from C. &gt; The C function will be optimised – but only in isolation, not in conjunction with the code surrounding the call. This means that when a Haskell and a C function have equivalent performance under no opimisations, once you apply optimisations the Haskell code will be faster, because it can be inlined and optimised with regards to its surroundings. I hate this, but it's true. It means it isn't obvious when it's beneficial to call an isolated C function, given that GHC can't optimize it within the larger context of your code.
London. Oh wait...
Well, how nice can you really expect x86 assembly code to be? :)
From the "cup-is-half-full" side, this is a really great problem to have for a high level language! I can't imagine saying that for Python, which farms out to C as much as it can.
This is roughly what I settled on in https://github.com/uber/queryparser - it seemed to work out well.
Interesting that assembly generation uses prettyprint. I would think that would be slow? At least it uses FastString instead of String, but I'd think all the fancy vertical line-up stuff wouldn't be needed, maybe just a simple line-based keep track of indent level thing.
I don't think it's such a big deal. You'd normally only start optimizing once you've determined some part of your code is a problem performance wise, and in that case you'll have a haskell baseline you can compare your C version too to check if a FFI call makes sense there anyway. You need to measure actual performance anyway after all.
I suggest you to watch [this John Hughes keynote](https://www.youtube.com/watch?v=Z35Tt87pIpg). At the end of the talk, he speaks about FP and hardware.
Lisp Machine? And I remember some "scheme machine" mentioned in SICP
I believe a related concept is the "graph reduction machine", intro video [here](https://www.youtube.com/watch?v=GawiQQCn3bk). Also: [SECD machine](https://en.wikipedia.org/wiki/SECD_machine). The Book of Shen ([contents](http://www.shenlanguage.org/learn-shen/TBoS/contents.pdf)) has a wonderful chapter 21 on how to implement the SECD.
Cont also has a dual-like relationship with free monads.
https://en.m.wikipedia.org/wiki/Lisp_machine This was a big deal in the late 70s to early 80s. I've actually been in a wikihole on this subject all day on account of Google Now randomly throwing me a white paper about the SKIM mk II S K I combinator reduction machine that I've been unable to find again. Fascinating stuff, I'd be super interested to learn more about how much of this research made it's way into modern chip design
Non-Mobile link: https://en.wikipedia.org/wiki/Lisp_machine *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^82279
For individual functions I usually start with the type signature then write the function definition. I don't use `concat` at all so it's moot asking whether I'd add it first or later. In the case of that particular function I'd write specialPermsUltra' [] _ = [[]] specialPermsUltra' xs i = do (pref, (x: stuff)) &lt;- zip (inits xs) (tails xs) guard $ x `mod` i == 0 return $ do rest &lt;- specialPermsUltra' (pref ++ stuff) (i - 1) return (x : rest) And I'd write it from start to finish in the order the characters appear in the file. For full programs I usually start by defining some types, then progressively more complex functions until I hit `main`, often going back and adding more types and simple functions on the way.
That's only true in cases where failure is the only reason a value can be absent.
Vim generally has superior navigation capabilities than emacs. It does indeed have several options that replace avy (and probably some that are somewhat superior)
If you find the paper again, let me know! It sounds really interesting.
You should link those options for the curious
Isn't it true in the case of a parser that "failure to meet the requirements of this parser" is the only reason the parser can't produce one or more values?
one thing i've experienced, if you _really_ know the behavior of your C function, unsafe FFI is much much faster than the default marshalling operations. This is one reason I ended up using hs-gsl-random over other options when I had to iterate over the FFI operation a lot. It is, on the other hand, unsafe...
1. [Highlights optimal characters for use with normal f/F t/T line-wise movement](https://github.com/unblevable/quick-scope). I find it interesting and just kinda cool. 2. [Easy-motion](https://github.com/easymotion/vim-easymotion) is the vim equivalent of avy. Supports fuzzy narrowing. 1. See easyoperator-\* plugins as well for some interesting ones you might want to use. 2. *tons* of features if you want to dig down the rabbit hole and customize the plugin. 3. [Easymotion Segments](https://github.com/aykamko/vim-easymotion-segments) allows for camelCase, snake_case, sensitive jumping. 3. [Sneak](https://github.com/justinmk/vim-sneak), my favorite "f/F t/T on steroids" plugin. 4. [Targets.vim](https://github.com/wellle/targets.vim) one of my personal favorites. This adds tons of additional targets to operators, so you can operate over pairs, quotes, separator text (comma, colon, etc), or "arguments". Has support for the concept of "next/previous" object, and automatic seeking as well. So if there's a line `[] function(arg1)` where [] is your cursor, typing `ci(` will jump inside the () and leave you in insert mode with the resulting string ` function([])` where [] is your cursor. 5. [Jumpline](https://github.com/jocap/jumpinline.vim) drops you at a certain % of the way through a line. So with a 1,000 column line, you can jump to column 500 by entering a submode and then 1-9 as relative percentages; 1 is 10%, 5 is 50%, 9 is 90%, etc. (A submode is equivalent to spacemac's transient states) Only easy-motion is a direct "replacement" for avy, but a lot of the plugins here tend to end up doing in 1 operation what you would've jumped and then done using avy.
There this little thing in the UK they're calling "Brexit".
I would hope `seq` wouldn't have a lollipop type. That would allow you to "use" a file handle without closing it.
Not at all. For example: think more carefully about the difference between `return [1,2]` and `return 1 &lt;|&gt; return 2`. In the first case: the parser is returning a list of 2 numbers, which were presumably both in the input. In the second case the parser is unsure whether it parsed a 1 or a 2. A grammar may allow an element to occur 0 or 1 times. When parsing such grammars: failing in the 0 case is incorrect.
This kind of approach actually goes all the way back to the enumerator days. One of the advantages of a left fold enumerator was that it could allocate resources. But the result was that the iteratee _couldn't_ allocate. This had to do with only half of the equation being coroutine based. The big change I introduced in conduit was giving up entirely on either side having control, and making both sides coroutine based. Since we needed something like `ResourceT` for at least one side, I figured may as well use it on both.
The standard way to get guaranteed cleanup in Haskell is with the bracket pattern, which essentially translates to the guaranteed cleanup you get in languages like C++ and Rust via RAII. The difference is that bracket is an explicit function call, whereas RAII is built into function scoping. In both Haskell and C++/Rust, we have to introduce extra concepts when we have non-static lifetimes of objects. `ResourceT` is such an approach in Haskell. In C++ we may use a smart pointer, and in Rust an `Rc` or `Arc`. In other words, even if we added the features that other languages have, we'd probably still end up with something like this.
Unsafe to interrupt, that is. `safe` just means the C code can participate in pre-emptive scheduling like normal Haskell code.
The approach in general adds a level of indirection in the type declaration : the type family. The more complex is the type family, the harder is to "see" the real type, but probably the more useful it is.
This looked interesting (never seen the indexed monads before) and I'd like to play around with them to find out, but can't get the `category-extras` package to install because of a ton of conflicts. :c Anyhow, when just replacing the arrow in the definition of `IxStateT` I think the compiler would only ensure you that `runIxStateT` uses `i` once; it would not care if you pass the same `i` to two `runIxStateT` since the linearity responsibility is never on the caller, only the implementation of the linear function. This means that every linear function can be passed non-linear arguments (which is what you do in your examples), what you need is the index variables themselves to be linear in the do-blocks and how to solve that I have no idea! Returning a linear continuation of sorts maybe, or some kind of linear sequence, but this is way over my head. Much of this is speculation from my side, interesting question!
The little assembly guy inside me reads that as "branch to exit" every time I see brexit.
Do you think they’re not necessary because GHC’s strictness analyzer kicks in or because strictness is not necessary here? The latter is wrong for `distance` since the value is never forced so you’ll be accumulating `(distance + 1) … + 1`. For `a` and `b` you’re right since `Text.null` and `Text.head` should force evaluation (there is a slight difference for `b` if `Text.null a` is `True` but you shouldn’t gain anything by forcing `b` in that case).
I hardly ever notice spam. And even if there is, it's easy to ignore. Not so much for violations of "jfredett's patented rule". However, as a prerequisite for moderation to act on misconduct you'd first have to come up with a [Haskell code of conduct (HCoC)](https://www.reddit.com/r/haskell/comments/5m678s/suggestion_code_of_conduct_for_haskell_community/) to clearly delineate what is considered misconduct. As [shown by past discussions](https://www.reddit.com/r/haskell/comments/631brq/mailing_list_post_civility_notes/dfqmgcr/) it won't be easy to find wide consensus. 
Is there a chance that by using linear types we can also prevent accidental sharing and thereby the spaceleaks described in [https://www.well-typed.com/blog/2016/09/sharing-conduit/](https://www.well-typed.com/blog/2016/09/sharing-conduit/)? It seems like sharing makes no sense if you can only use it once so it might help here.
In some cases whole-program optimization can do miracles. Profiling is not a substitute, since the whole-program optimizer may rearrange the order in which code is executed, in which case slow sections of code may not be the bottleneck. I should note that I am talking about whole-program optimization in general, and not a specific pass in GHC.
Berlin and London, but Berlin is a much more livable city.
Have you tried learning Elm? Maybe you can start with Elm and once you get the hang of that, you can come back to Haskell. 
My problem with bracket style functions is that they really don't handle overlapping regions very well. Iirc operationally rust inserts flags when a struct with destructor is only destroyed in some branches and checks them before returning. Sorry for my janky rust: use std::io::{BufReader, ErrorKind, Error, Lines}; use std::io::prelude::*; use std::fs::File; fn main() { let lines = get_file().unwrap(); for line in lines { println!("{}", line.unwrap()); } } fn get_file() -&gt; std::io::Result&lt;Lines&lt;BufReader&lt;File&gt;&gt;&gt; { let mut paths = File::open("paths.txt").map(BufReader::new)?.lines(); let new_path = paths.next() .unwrap_or(Err(Error::new(ErrorKind::Other, "Empty File")))?; match File::open(new_path) { Ok(new_file) =&gt; Ok(BufReader::new(new_file).lines()), Err(_) =&gt;Ok(paths) } } Silly example but `paths` lives either until the end of get_file or main depending on whether its first line could be openend as a file. As far as I know rc/arc are only needed if you need multiple overlapping references to some memory location, which of course also wouldn't work with linear types.
- [Haskell Programming From First Principles](http://haskellbook.com) is a book recommended by many. It is very thorough and is pretty long (1000+ pages). Try the [sample](http://haskellbook.com/assets/img/sample.pdf) (95 pages) to see if this resource is for you. - [Haskell wikibook](https://en.wikibooks.org/wiki/Haskell) - The wikibook could also work but I'm not entirely sure how good of a resource it is. For example you should definitely install [stack](https://haskell-lang.org/get-started) to get started, but there are plenty of good chapters there. - [IRC](https://haskell-lang.org/irc) - You can ask questions at the very lively #haskell irc channel on Freenode Self plug: I've written a list with a few more resources one can use to learn Haskell [here](http://gilmi.xyz/post/2015/02/25/after-lyah).
How is your general experience + knowledge in other programming languages? I'd recommend grasping the general concepts first if this is not already the case for you. 
&gt; Beware of conditional compilation (e.g. CPP and the Cabal flag mechanism), as these may mean that something is currently a weed, but in different configurations it is not. Hmm, that sounds like a very big current limitation. Will detecting that ever be supported? 
The first link is about category theory, not really Haskell, so I am not sure it is a great idea to recommend that.
&gt; very thorough I would also recommend it, and add that: * the order that was choosen to introduce concepts seems good to me (even though I didn't experience the book as a beginner) * it has many exercises!
Do you maybe have an example of something you have a hard time understanding?
If you want more freeform exercises you might try [exercism.io](http://exercism.io). They start easier, you have a lot of other solutions to ~~steal~~ learn from and it is built on code reviews so you can get some feedback. To actually learn haskell I can echo the [Haskell Programming From First Principles](http://haskellbook.com/) recommendation.
I remember reading Learn you Haskell and [Reald World Haskell](http://book.realworldhaskell.org/). I had similar experience like yours. Take a break, do something else, think about your stumbling blocks and when you feel ready try again from scratch. It takes time before you can write something useful in Haskell - it is not like jumping from C to Java; Haskell is different.
`seq` is inherently non-linear (`deepSeq` might be possible to make linear though), and from of the types of `&gt;&gt;` this would not compile since the linear `a` is used in an unconstrained context. 
Hi. I have a resource that may be of some use. I'm attempting to cast my perspective on the problem of learning to use Haskell from first principles in the form of a collection of notes I'm writing, that you can check out [here](http://locallycompact.gitlab.io/ANLGTH/). This is still very much a work in progress but I've had a few people report some success in getting over the initial hurdle of how to get started problem solving in Haskell, maybe you'd like to read some of the introduction and tell me your thoughts on it or if I'm making the same mistakes as the other authors you're reading. If you have any specific questions on particular techniques in Haskell I'd be happy to help.
&gt; 2 + 4 - 2 should optimize down to a constant rather than doing the math, but if you called out to C, it couldn't Do you mean when calling `int foo(int a, int b, int c) { return a + b - c; }` as `foo(2, 4, 2)` from Haskell? If so, then that's exactly something link-time optimization can do, if I understand correctly (and also eliminate `foo` if there are no remaining calls). Though of course GHC wouldn't be able to use it to optimize further. &gt; I wouldn't think that Link time optimizations are going to get you a ton of performance gains if you're not already taking advantage of the prior GHC stages. But why wouldn't you? I don't think the parent comment suggests not optimizing Haskell (and C) separately first.
I had a similar problem. I tried starting with LYAH but I kinda failed. After that I tried the wikibook, which helped me to learn the basics, but I had some problems understanding the more complex stuff. Then I decided to buy "Haskell Programming from first principles" and I REALLY recommend it. The explanations are great and it has lots of exercises.
&gt; modern Intel processors already perform some form of parallel graph reduction The GHC runtime _is_ essentially a parallel graph reduction machine. But rather than a dedicated piece of hardware, it's implemented as software on Intel/AMD/.. processors. The GHC RTS's sparkpools support the `par` primitive to split reducible expressions into multiple potentially parallel reducible expressions spread over different conventional processor cores.
I found OCaml much easier to learn as a first typed functional programming language. I learned how to transform data to solve problems - using map, filter, fold, unfold as well as algebraic data types and pattern matching, lambdas, currying etc. Almost all of that comes across wholesale to haskell. Haskell adds monads, which take time to understand to use in a practical way, and may add unnecessary confusion at first.
You're totally right. I thought about this some more, and the semantics I gave for `&gt;&gt;` are inconsistent with how `&gt;&gt;=` is defined for `LMonad`. I think that `&gt;&gt;` should not be included in the typeclass though because it isn't ever possible to provide anything more performant than the default implementation. Interestingly, I think there is another formulation of `LMonad` with different semantics, one where the monadic context must be used linearly but the values do not have to be. Here's the one from m0ar's repo: return :: a ⊸ m a (&gt;&gt;=) :: m a ⊸ (a ⊸ m b) ⊸ m b fmap :: (a ⊸ b) ⊸ m a ⊸ m b (&lt;*&gt;) :: m (a ⊸ b) ⊸ m a ⊸ m b An alternative definition would be: return :: a -&gt; m a (&gt;&gt;=) :: m a ⊸ (a -&gt; m b) ⊸ m b fmap :: (a -&gt; b) ⊸ m a ⊸ m b (&lt;*&gt;) :: m (a -&gt; b) ⊸ m a ⊸ m b (*&gt;) :: m a ⊸ m b ⊸ m b This seems like it would less often be what people actually want, but it could be useful for something like `State` that's only linear in the state but not in the values. This would mean that you couldn't use `put`, but you could use `modify`.
Potentially, if the output data type supported a `intersect` operation, then the user could run `weed` on all possible configurations and intersect the results.