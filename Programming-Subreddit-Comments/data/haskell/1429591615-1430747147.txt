Hmm, I just tried and didn't have any trouble. Never installed spock before, either. I threw together a quick github repo with my steps. Most of it's on the command line (documented in the README), but I've added my sample SpockTest.hs and temp-spock.cabal files to the repo. https://github.com/stormont/temp-spock Let me know if this helps, or if any of your existing configuration is any different (e.g., version of OS, Haskell Platform, GHC, cabal, unicorns...).
Yes. I was asked why I consider darcs to be easier to use, and I answered. Because it requires fewer operations to do the things I want to do. That surely can't be controversial.
Thanks :) [This](https://github.com/elm-lang/core/issues/218) looks like the same thing? The issue might be that your PATH is not pointing to the new 0.15 executables.
Presumably it would allow arbitrary chaining too like the current ones: Prelude&gt; let x@y@z = 1 in (x,y,z) (1,1,1) 
&gt; I like it, although it doesn't cover the "or later" variation requested by the OP. Actually it does with SPDX 2.0's license expression syntax.... 
More specifically the [delayed representation](https://hackage.haskell.org/package/repa-3.3.1.2/docs/Data-Array-Repa-Repr-Delayed.html) in Repa allows you to convert any lookup function to a repa array. Repa arrays in turn provide map, zipWith, append, traverse and other useful functions.
If someone takes on MediaWiki maintenance tasks, that's great. But really we need a technology upgrade now, just like we did for the last wiki migration. The wiki should be a modern markdown-based wiki. I'd hate to entrust even more of our community IP to GitHub Inc., but a github wiki is the kind of thing we need. We found out last time that migrating the wiki is a rather daunting task. But I don't think it would be more than a year or two's worth of legacy MediaWiki maintenance effort if we *don't* migrate.
No. They should be.
Can't we scrape all the community wiki to a markdown format and put them under a git folder ? We can then slowly start fixing all the markdown issues. And mean while the new wiki can reside under something like new-wiki.haskell.org.
Why does Elm deviate from Haskell in little things (eg, import syntax), when there is no real reason?
Why does Haskell deviate from ML in little things (eg, `:` vs `::`), when there is no real reason? But no, there is real reason to deviate from Haskell in the import syntax. With the Haskell import syntax, the default is to dump everything into the current namespace unqualified. That is not a great idea, in my opinion.
Yes, something like that. But that is already work. Does pandoc already support MediaWiki input? If not, we need to write that backend. And like any content conversion, the automated tool output will never be perfect. It will need lots of tweaking. We need to look into the content licensing issue. Last time, that was perhaps the most difficult issue with the migration. If the license is changing, we would need to contact every single author and get permission, which is a very difficult task.
Yes, Haskell should have used `:` instead of `::`, but Haskell wasn't trying to look like ML, it was trying to look like Miranda (for better and worse). I agree that dumping everything into the namespace isn't the best idea, but since Elm has that option anyway, I think it could have picked the Haskell syntax. I have a feeling it's more about fixing little things that could be better in Haskell. While I agree with this sentiment, there's a lot to be said for being Haskell compatible when it's possible.
On the [café thread](https://mail.haskell.org/pipermail/haskell-cafe/2015-April/119219.html), Henning suggested an alternative of a gitit/pandoc site. That has pluses and minuses compared to a github wiki. See those messages for details.
So then please slightly modify the example code to make its own background lighter. :)
For the same reason Elm isn't a Haskell library. Part of the goal is to do things the optimal way, with a disregard for backwards compatibility that an established language like Haskell can't afford. 
With view patterns you can define @, so with view patterns you can define your own version of @ that works the way you like. both : a -&gt; (a,a) both x = (x,x) pattern (:@) p1 p2 &lt;- (both -&gt; (p1, p2)) Now you can use `p1:@p2` as a pattern. 
&gt; Does that mean there is already some related work is going there ? No, there is no decision to do this. Gershom's original proposal was to leave the wiki on MediaWiki and find a volunteer to be the MediaWiki admin. I proposed this direction as an alternative. &gt; Pandoc supports MediaWiki markup according to it's documentation. Excellent. So we just need to check if any tweaking is needed for this particular use case. &gt; For licensing, it seems we need to get only for those authors who have contributed before 2006. Also good news. But I do recommend that someone who is good at licenses think through the whole issue very carefully, both for a migration and for the future. And the pre-2006 issue could end up being a bigger thorn in our side than you think.
How do you define "should be expected"? When you say that a hash "agrees" - agrees with what? What two versions of hashes does a user compare?
Nice post. In idiomatic haskell we usually try to avoid explicit recursion on lists in favor of higher order functions like map, foldr, and foldl. Using these is less error prone.
It seems like the new `ssh` package (version 0.3) has been uploaded to hackage earlier today, but the changelog doesn't mention fixing this vulnerability. Imho this information definitely belongs in there to prevent others from allowing their packages to build against earlier versions.
`total` is only adding syntax that vaguely resembles a case statement. There's nothing more to it than that.
A hash given to me with a PGP signature, for example. If I have any type of hash that I trust, I need to be able to ensure that the source code I have actually is what I'm told I should have.
I disagree; I can't find anything that is clearly "or later" in the specification. You could define your own license-ref and include it as part of a disjoint license set in order to indicate "or later", but defining your own license-ref is roughly equivalent to using "Other". There's no standard decorator / variant for taking an existing short name and allowing future versions with the same base short name. Could you point me to where you are seeing a standard "or later" variation in the specification? I'm going off the 1.2 specification, since the 2.0 spec isn't finalized, yet.
&gt; Commit what you want. Then darcs amend. That's fine for adding changes to an existing commit. However, I often also *split* commits, or only commit *part* of my working directory changes via `git add -p` or similar. The staging area is quite valuable to me. There are other ways to do it, I'm sure, and I'm open to that, but I do use and want to configure using something *like* `git checkout -p` and `git add -p` so I can select only parts of the changes to a file, and even (on occasion) edit the patch directly.
&gt; The size of git [...] stackoverflow questions says otherwise. And that couldn't be influenced by the relative popularity, at all. /s
it's always interesting to read these articles because I'm one of the small subset of people who learned Haskell as their first programming language. So I had a sort of mirror image of these thoughts/issues when trying to learn imperative and/or dynamically-typed languages for the first time. Your part about monads might rub some people the wrong way, see http://dev.stephendiehl.com/hask/#monadic-myths 
It's still O(M*N) work no matter how you divvy it up.
Size and complexity of the documentation — couldn't.
`curry` untuples. `uncurry` tuples.
Is there a reason why this type signature for the factorial function factorial :: (Integral a) =&gt; a -&gt; a is better than (or more useful than) factorial :: Int -&gt; Int (or Integer -&gt; Integer) ?
Ok, we have a temporary fix in place where the code only displays on when the browser window is wide enough.
You are welcome! I figured it out a bit later, and yeah, what was happening is that I had an old install of elm that I'd forgotten about that was earlier in my path (in my .cabal folder). That'll teach me to not use sandboxes.
... which isn't strictly better. Imagine using "base factorial" representation of numbers. So, 5! = 10000_fact (and 10 = 120_fact and 100 = 4020_fact). Then, factorial is just a "bitshift", but the parametric version implements it through repeated multiplication. More polymorphism generally restricts the implementation, allowing us to better predict it's behavior. It's a trade-off, like most programming decisions.
Last time this issue came up (in 2012, [here](https://groups.google.com/d/msg/gitit-discuss/GV64xqX-XuU/pdXUeuvaoVIJ) and [here](http://www.reddit.com/r/haskell/comments/txn1p/what_if_haskellwiki_were_run_by_a_haskell_wiki/)), I wrote a [script to scrape the wiki's content and convert it all to a Markdown-based gitit wiki](https://github.com/jgm/hw2gitit). As a demo, here is the 2012 edition of haskell wiki running on gitit: &lt;http://haskellwiki.gitit.net&gt;. Note that [the wiki's history](http://haskellwiki.gitit.net/_history/Applications%20and%20libraries/Database%20interfaces), and not just the content, has been imported. Nothing came of this. There were legitimate concerns about whether gitit was sufficiently battle-tested and security hardened to run Haskell wiki. If I had time, I'd write a new gitit-like wiki engine, using [gitlib](https://hackage.haskell.org/package/gitlib) instead of filestore to handle the backend, and maybe [cmark](https://hackage.haskell.org/package/cmark) instead of pandoc for markdown processing. 
In the current (rc3) version of the [draft 2.0 specification](https://spdx.org/SPDX-specifications/spdx-version-2.0) [PDF link at bottom], the "license expression syntax" is in Appendix IV and does include an "or later" decorator that can be applied to any versioned short name.
darcs amend --unrecord
Not PM_ME but there are a few resources out there with a lot of value. Often instead of OOP you see people talking about ADTs instead (which are similar). Jeremy Gibbons' [*Unfolding Abstract Data Types*](http://www.cs.ox.ac.uk/jeremy.gibbons/publications/adt.pdf) is one of my favorites. PM_ME's suggestion to read Jacobs is pretty great as well. I also recommend, if you don't mind a funny little math book, Barwise and Moss' [*Vicious Circles*](http://www.amazon.com/Vicious-Circles-Center-Language-Information/dp/1575860082). I think I also recommend it just due to the (creepy) cover art. No, I'm kidding, it's a good book.
Damn! I wrote a big reply to you from my other laptop thanking you for all the feedback and criticism. It's disappeared. :/
Your analogy doesn't hold up in most languages that include goto. The benefit of using a while loop is that you prevent jumping into invalid contexts. That's a very strong guarantee. In contrast, `foldr` only prevents you from using general recursion. And with Haskell's general stance on partial functions, this is considered a non-issue in the language.
Quite the opposite: I am building a language where universal folds are the only way to consume things (for reasons...), but I don't fool myself into thinking it will be readable.
Haha, I left off the /s ;)
Firstly, thanks so much for all the feedback! I really hope to reach a point where I can write Haskell instinctively. Like you mentioned, the point where I can 'think less and type more'. At the moment, I have to carefully plan how I'm going to write my code and the abstractions I can add and so on. But I guess that comes with writing more code. Its very much possible I may have got some points wrong or may have some concepts misconstrued in my head. There is bound to be a point where I realise mistakes and correct them. Thats a part of learning and its good to be wrong sometimes. I'm glad I got a chance to play with Haskell and use it in a project at Pusher. Having some smart Haskell engineers around me helped me as well! And thank you so much for viewing this post from the point of a newcomer; I hope other readers do so too. I'll make sure I give your links a read.
serious answer, it's a general purpose language that is good for almost all applications. 
Yes. It does everything(tm)
Nope, for anything else you need to upgrade to PHP.
Factorial!
You can write a microservice. One that computes the Fibonacci sequence.
&gt; There were legitimate concerns about whether gitit was sufficiently battle-tested and security hardened to run Haskell wiki. Well it's never going to be if nobody uses it.
Ironically, implementing a real quicksort in Haskell is nigh impossible. You can, however, do merge sort.
For the places where it actually matters, you can chose polymorphism that allows each instance to select a custom implementation. Factorial is not one of these places, since it grows too quickly to be meaningful for most uses.
People say *quicksort* is a sort on a mutable array. Being a sort based on *divide and conquer* too, the variant for lists is also called *filtersort*. 
I find "a map then a fold then an application" (ie "h . foldl g . fmap f") easier to read than some Python loop or general recursive haskell function. (often, not always :) a step-by-step pipeline. because I can easily see each "part" of the recursion, rather than having some mess of code that I need to mentally translate into "aggregating" and "transforming" sooner or later.
Is your background physics by any chance?
That's sort of the point of purity as applied to programming: a guarantee of the (weak) equivalence of call-by-name, call-by-value and call-by-need evaluation strategies.
I agree. We've had the same experience with our own in-house language. Whatever benefits come from "improvements" in the small are lost by the fact that, in the large, code is no longer as easily translatable as one might like, and furthermore there is a useless mental friction-cost from switching between two close-but-different syntaxes, etc.
And oHm uses pipes-concurrency and Mvc to give you a message passing front end library :-)
Yes, roughly analogous to `*`. The collection of arrows for a category is a family of types, indexed by objects; for each pair of objects, there is a *type* of arrows (whose elements are the arrows themselves).
*([original comment](http://www.reddit.com/r/haskelltil/comments/337g7t/endo_the_monoid_of_endomorphisms_under/cqihh1l) by /u/bheklilr)* The documentation isn't trying to be scary, it's trying to be precise. Anything with "endo" means "back to the same thing", while "morphism" means "shape changer". So an "endomorphism" changes somethings shape back to itself. In Haskell data has a shape, we call it the type, so an endomorphism in haskell is something that takes data and returns new data with the same type, otherwise known as `a -&gt; a`. Calling it `Endo` keeps the name short and usable. Personally I can't think of a single word to use for this newtype that would describe its purpose so clearly as `Endo`.
*([original comment](http://www.reddit.com/r/haskelltil/comments/337g7t/endo_the_monoid_of_endomorphisms_under/cqitbkg) by /u/jlimperg)* To be honest, while the documentation is precise, it also doesn't try at all to cater to anyone who doesn't happen to have the required mathematical background. If you don't, tough luck, go read either the implementation or a maths textbook. While the former happens to be quite manageable in this instance, I can't really blame people for finding the latter a bit of a drastic requirement.
*([original comment](http://www.reddit.com/r/haskelltil/comments/337g7t/endo_the_monoid_of_endomorphisms_under/cqivzoi) by /u/tejon)* The problem is, it's not precise; and in my case there's not even a vocabulary issue. I know enough Greek to figure out "endomorphism" on my own, I've learned what a monoid is, and "composition" is gradeschool English. Even with all that, the sentence is ambiguous for two reasons. 1. 'Under' is not the preposition I would pick here. Perhaps it's standard in mathematical discourse, and perhaps if I had that background I wouldn't find it ambiguous, but on its own it doesn't really imply how the concept of composition relates to the concept of a monoid of endofunctors. And that very disjunction informs the second issue... 2. My initial parse separated the clauses as *"the monoid of endofunctors"* and *"under composition."* In fact, they are *"the monoid of"* and *"endofunctors under composition."* Until I realized this, which required reading the code (easy in this case, but not always), I couldn't make heads or tails of the description, and not for lack of thinking. Once again, this comes down to picking a preposition, 'of', that doesn't have the expected connotation. And again, I can see how it fits in hindsight (prepositions are mostly arbitrary to begin with) but my expectations from common usage don't match the usage here, whether or not it's standard in mathematics. The combination of 1 and 2 had me reeling trying to figure out what concepts I was missing, and if it weren't for the dead-simple constructor just below, I would very likely have decided this was still over my head. And it's all down to grammatical imprecision. If the documentation had stated that Endo was: &gt; The monoid for composition of endomorphisms. I never would have made this post, because each clause is clear in its relation to others and all the potentially scary concepts are clearly wrapped up in single words, not ambiguous conjunctions. --- With all that said, you jumped straight to "endomorphisms" as the scary thing, and it could certainly qualify for anyone who isn't a hardened etymology geek. Yes, *this* word is used for precision, because it's somewhat shorter than "functions whose return type is the same as their argument type." But how much would a footnote or parenthetical explaining that definition really cost? This is almost certainly the only place you'd need it, unless there's some other place where endomorphisms are special-cased with their own type. In fact, we probably can assume that anyone reading this documentation understands Haskell type signatures, so it can probably get away with being a very short addendum: &gt; The monoid for composition of endomorphisms (:: a -&gt; a). Unambiguous, gives a hint about the scariest word, and clocks in only ten characters longer than the current documentation -- QED scarier than it needs to be! --- As /u/jlimperg points out, *this* example is trivial to work out from the implementation. Many are significantly less so, but how many of them are *scarier than they need to be* in the same way? Bottom line, there's an ivory tower at work; it's very daunting and feels downright exclusionary. This isn't anyone's intention -- mostly, it's just [the curse of knowledge](http://en.wikipedia.org/wiki/Curse_of_knowledge) -- but now that the general sentiment of Haskell culture is drifting away from "avoid success" in favor of pursuing wider adoption, this is a real problem hindering that goal and "it's really not that bad" is not a solution. It can't just not try to be scary. It has to try to be not scary.
Your data declaration for `Number` is really strange and doesn't compile for me: `int` and `double` are type variables (because they are lower case) which occur on the right hand side but not the left. Could it be that you changed something when reducing the example for this post?
Your type signature isn't strong enough to capture what you want. Further, GHC cannot infer the right one for you. That said, you *can* specify the one you want using `RankNTypes`. Essentially, you want more polymorphism than you're asking for. Currently, GHC is noting that it can find explicit types for `f` in two places: (1) at `Int -&gt; Int -&gt; Int` and then again at `Double -&gt; Double -&gt; Double`. It then complains because it can't unify those two types. You actually want to be handed an `f` which is able to be specialized to either one *at the time it is called*. The type you want is therefore: {-# LANGUAGE RankNTypes #-} coercion :: (forall a . Num a =&gt; a -&gt; a -&gt; a) -&gt; Number -&gt; Number -&gt; Number Note that `RankNTypes` lets me use `forall` inside of arguments and then note that my `forall` is scoped to be "inside" of the first argument. This means that functions passed to `coercion` have to be completely generic over the argument type `a` excepting that `a` instantiates `Num`. It also means that in the implementation of `coercion` you have a value `f :: forall a . Num a =&gt; a -&gt; a -&gt; a` which you can instantiate at *different types* each time you call it. To be super explicit here: coercion f a b = case (a, b) of (INT a, INT b) -&gt; INT (f_int a b) (DOUBLE a, DOUBLE b) -&gt; DOUBLE (f_double a b) ... where -- explicit specialization: this isn't necessary but it drives the idea home f_int :: Int -&gt; Int -&gt; Int f_int = f f_double :: Double -&gt; Double -&gt; Double f_double = f
Or parametricity!
I'm trying to remember the history. It goes back about ten years. [This mailing list posting](http://permalink.gmane.org/gmane.comp.lang.haskell.libraries/5313) puts an upper bound on it, but it's clear that the deal was already done: I'm failing to find earlier archive copy, but I know it must exist. I was certainly one of those who provoked the change from instance Monoid (a -&gt; a) with identity and composition to instance Monoid m =&gt; Monoid (a -&gt; m) with pointwise lifting, and also the compensatory newtype wrapping giving the instance Monoid (Endo a) in question. My fingerprints will not be found on the "official" version, or on this documentation, but I can't claim to be entirely blameless. The context of that comment, however, is that historical breaking change: it's directed at explaining more what's different to people who already have established expectations than what's going on at all to people who are just arriving. In its day, what was being said was "*this* is the one of the two things we have been arguing about on the mailing list that is ending up getting the newtype treatment". Clearly, we're not in that context any more. Thank goodness. It probably would help to revisit the documentation.
Thanks! This fixed the issue. Is it better to try and avoid using RankNTypes when possible?
I'm sorry! They should have been Int and Double. Accidentally made them lower case when I posted
Straight to /r/haskelltil with you...
I had the same problem about a year ago. I worked around it by passing in multiple functions to my implementation: data Value = Integer Integer | Double Double | Rational Rational deriving Show -- I tried implementing this with a single op, and the type checker complained -- that op had multiple types on multiple lines of this function. So, now we -- have 3 ops, one for each type, and we just pass in the same version for all -- of them later on, and it's happy turning the Nums into Integers or Doubles or -- whatever differently for the different instances of ops. combine iop dop rop (Integer x) (Integer y) = Integer (iop x y) combine iop dop rop (Integer x) (Double y) = Double (dop (fromInteger x) y) combine iop dop rop (Integer x) (Rational y) = Rational (rop (fromInteger x) y) combine iop dop rop (Double x) (Integer y) = Double (dop x (fromInteger y)) combine iop dop rop (Double x) (Double y) = Double (dop x y) combine iop dop rop (Double x) (Rational y) = Double (dop x (fromRational y)) combine iop dop rop (Rational x) (Integer y) = Rational (rop x (fromInteger y)) combine iop dop rop (Rational x) (Double y) = Double (dop (fromRational x) y) combine iop dop rop (Rational x) (Rational y) = Rational (rop x y) instance Num Value where (+) = combine (+) (+) (+) (-) = combine (-) (-) (-) (*) = combine (*) (*) (*) abs (Integer x) = Integer (abs x) abs (Double x) = Double (abs x) abs (Rational x) = Rational (abs x) signum (Integer x) = Integer (signum x) signum (Double x) = Double (signum x) signum (Rational x) = Rational (signum x) fromInteger x = Integer x It's not pretty, but it worked and let me move on to the more interesting problems.
So yeah, how do I get started using it? :-) 
&gt; A monoid to a mathematician is a set with a binary operation which together pass certain criteria. In Haskell it's seen more of an operator (&lt;&gt;) that works on a lot of types. I'm not sure I see the difference here :) Surely a mathematician sees a monoid as (&lt;&gt;). They just spell it differently, writing ab instead of a &lt;&gt; b. &gt; What I've seen repeatedly is, the mystique of the great and powerful monad leads people to assume that there's something subtle and very difficult that they've overlooked, when in fact the definition really is as straightforward as it looks. I would definitely agree here. I see this same reaction from the 4th graders I work with. I will slip up and say "divide by 2" and the kid will look at me terrified and come out with "we haven't learned division yet!" I explain all I mean is "halve the number" and they go "ooooooh!" and go on their merry way. It would be nice if we could do this with monads. It's "just callbacks", as far as you care. `readLine &gt;&gt;= \x -&gt; putStr x` just means "execute `readLine` and whatever the result, pass it to `putStr`.
I was trying to make a funny ;_;
You probably want to use "darcs amend --unrecord". This removes changes from the selected patch, so they appear again as unrecorded changes that you can save as another patch.
To me, problems solved with RankNTypes are pretty clear cut. I think if you get to a point where you know you need them, it's a good case for using them. There might be cases where you don't need RankNTypes but you accidentally use them unnecessarily... but I can't really think of any off the top of my head now :) 
So, Coq, right? ;)
For the two instances I gave, reflexivity and transitivity, the overlap is real: you could either use reflexivity directly, or use transitivity to combine two reflexivity instances, or use transitivity to combine two of those combined instances, and so on. But what about combinations of rules where there is no overlap? Well, this transitivity rule seems very powerful to me, so finding solutions involving it is probably equivalent to the halting problem or at least some NP-complete problem. Let's figure out which! Let's see, can I implement a turing machine using instances? Let's use a finite number of types `Halt`, `State0`, `State1`... to represent the machine's state, and a finite number of types `Blank`, `Symbol0`, `Symbol1`... to represent the symbols on the tape. Finally, to represent the tape itself, let's use a type-level zipper `(left, head, right)`, using `()` to represent the untouched, all blank part of the tape, and `(_, (_, (_, (..., ()))))` to represent the explored part on each side. Then each rule can be represented using two instances. To represent the rule which, from state `State` when the head is on symbol `Symbol`, writes symbol `Symbol'`, moves the head to the left, and switches to state `State'`, write the following instances: instance Compatible (State, ((), Symbol, right)) (State, ((), Blank, (Symbol', right))) where ... instance Compatible (State, ((left,new_head), Symbol, right)) (State, (left, new_head, (Symbol', right))) where ... Finally, add a catch-all rule to turn the `Halt` state into a simpler type which isn't encumbered by the tape's state. instance Compatible (Halt, tape) Halt where ... To solve the halting problem, encode the machine as instances and ask the type system if there exists an instance from the starting state to `Halt`. QED! That being said, with the appropriate GHC extensions, Haskell's type system is [already turing complete](http://www.reddit.com/r/haskell/comments/1jwp0s/typo_a_programming_language_that_runs_in_haskells/), so I guess it's not unreasonable to imagine an extension which would lift the limitation at the cost of sometimes getting caught into a loop during typechecking.
I think they are trying to suggest that mathematicians conceive of a monoid as a set first, which is additionally equipped with an operation satisfying some laws, rather than as an operation (which of course has some domain). That is why we say "the monoid of endomorphisms under composition"; the set is first, the operation is second. Or we just elide the operation completely and just say "the monoid of endomorphisms".
Beautiful! Thank you!
How do you know that you aren't processing more things in your program which take up more memory vs the compiler being at fault? Do you have a minimal test case that you can demonstrate this behavior on so that if we compile it on 7.8 it uses x memory and when we compile it on 7.10, it uses y memory? where x &lt;&lt; y Without more info, this is a blind shot into the subreddit seeing if you can find the needle in the haystack.
I agree strongly with this. `Kleisli` categories are another good example of an extremely simple idea made to seem much more complicated by naming it after a mathematician. Just seeing the name makes one think, "oh crap, I had better learn category theory or I'll never understand what the heck that is used for." If the instantiation is only one line of code, just copy that line into the comments. Endofunctor is a perfect example of how something simple can be made more clear just by showing the code. Learning a whole new jargon doesn't bother me at all, but often times, reading the mathematical descriptions of an idea, like *Endofunctors* for example, on Wikipedia or on someone's blog, does not tell me anything about how the the idea is supposed to help me write code. And I admit this listing the code in the documentation will not always work. I can recall trying to learn about the `StateT` monad, looking through the code that defined it, and being completely lost as to how stateful data was supposed to be represented that way. But `Identity` is straight forward, `Endo` and `Dual` are straight forward, `Category` and `Arrow` over functions and `Kleisli` categories are very straight forward.
On the gripping hand, for API documentation, I get a lot of value from example code. (For data structure and algorithm documentation, I do prefer prose.)
Haskell isn't keen on uncontrolled mutation, but mutation is fine : http://stackoverflow.com/a/7833043/285471
I'm getting a compile error with the example code that uses renderRoute: src/Main.hs:20:29: Couldn't match expected type `Integer -&gt; Text' with actual type `Text' The function `renderRoute' is applied to three arguments, but its type `Path ((':) * Int ((':) * Int ('[] *))) -&gt; reroute-0.2.2.1:Data.HVect.HVect ((':) * Int ((':) * Int ('[] *))) -&gt; Text' has only two In the first argument of `(&lt;&gt;)', namely `renderRoute addR 313 3' In the second argument of `(&lt;&gt;)', namely `renderRoute addR 313 3 &lt;&gt; "'&gt;Calculate 313 + 3&lt;/a&gt;"'
By persuading someone to implement it, of course.
There was a [thread](http://permalink.gmane.org/gmane.comp.lang.haskell.glasgow.user/25533) on the ghc-devs mailing list about this.
Unfortunately, you never know until you get a reproducible use-case. p.s.: my servers started going OOM after an upgrade to 7.6 from 7.4 also, but I never figured out if that was lib-updates or ghc update, since adding a bit of RAM was so much easier, but I promised myself that I'll take a closer look next time :)
It would be fascinating to read about your experiences learning imperative languages after having started with Haskell!
Prime Minister of Singapore that is. Apparently it was proposed to him by his children both of whom graduated from MIT.
No :) some of Coq's recent soundness bugs existed precisely because that isn't the case.
Singapore is unusual in that most members of the parliament are engineers. Not lawyers.
I happened to meet someone from SG last week and he mentioned how SG politics is a meritocracy and how they have a lot of very able politicians. As an Indian, I was mind blown. 
&gt; Also the 0, 20, 40, 60, 80, 100 numbers seem to be meaningless, they aren't necessary, the thickness of the colours already represents a ratio, and they're misleading implying order matters. Presenting data meaningfully is hard, let's go shopping. I don't think this is wrong. It's presenting percentages, which is a perfectly fine way to present fractions of a total. The numbers also give you some sense of proportion, and allow you to compare different blocks to each other (e.g. military in China vs economics in Brazil). I agree that the colors are chosen poorly, though.
The amount of RAM ghc uses to build large files is a big problem for commercial users. Perhaps not so much the big players that can afford to throw money at the problem, but for startups that need to get rolling quickly. These folk can't afford to spend time on infrastructure while their competition uses nodejs/etc out of the box on Heroku (with 512MB RAM for build servers). Someone on the list also mentions Travis CI: &gt; And while I can build the tests on 7.10 in around 5 minutes, travis times out building them after around half an hour. I find, even on my laptop with GHC 7.8 and 4GB of RAM, compiling certain bits of code (in particular, [persistent entities](https://ghc.haskell.org/trac/ghc/ticket/9669) and large records) can grind things to a halt, so this makes me wary of upgrading to 7.10. That is to say, please take this as a plea to take special care when looking at build performance. It factors into my personal efficiency more than new language features do.
In fact, by comparison, I think that highly debated changes like [FTP](http://www.reddit.com/r/haskell/comments/2vfczx/ghc_710_prelude_we_need_your_opinion/) will barely have any effect on me as a commercial user (I see it as a welcome opportunity to refactor if needed). On the other hand, build times have a running cost to the entire team.
What are they?
String is not greater than IO.
Well the OpenGL code was already out of date 7 years ago in the sense that Modern OpenGL doesn't use a fixed function pipeline. The current http://hackage.haskell.org/package/gl-0.7.5 haskell bindings, finally support everything up to OpenGL 4.5, but are only a direct wrapper of the C-Api. So together with the https://hackage.haskell.org/package/sdl2 bindings to SDL 2, you would be able to port a C-Program that used SDL2 and OpenGL fairly easily. Unfortunately to achieve optimal performance and no driver overhead (might not be your priority), you need to use glBufferStorage and essentially manually and destructively update a ring-buffer, use glMultiDrawIndirect, which again comes down to manually managing memory, and you also need to implement some virtual memory system to manage texture memory (see http://www.slideshare.net/CassEveritt/approaching-zero-driver-overhead). Ultimately you want many threads (one per game entity for example) to be able to concurrently destructively update parts of those buffers, at least that is how some modern game engines are structured, there is no time for copying or immutable data structures on the CPU, as performance hinges on the fact that while one part of the buffers are updated, another is uploaded to the GPU and the third is used to render the current frame. 
Yep yep.
Can't they both be the most important feature? Although, whenever I come to play around with dependent types, I feel this way. You almost *need* a computer to perform the various reductions for you. But usually what's missing is mundane stuff, like how to spell a certain lemma or constructor or how to plumb an object between isomorphic types. The one thing I still can't wrap my head around in Haskell is how anyone keeps track of typeclass resolution at the term level. It's great that GHC can do Prolog-fu and work out a whole-program constraint problem to resolve instances. I would find that just as hard to do on a bus as the most fancy of type voodoo.
&gt; But in all seriousness, Wikipedia's math and programming content is quite good. The math articles might be good as references. But IMO not for learning.
It is just a simple question. It can be helpful to know if other people are experiencing the same problem or if it is just you in order to know where to start looking for the problem. Given that GHC gets slower and uses more memory with every release, this isn't an unreasonable question.
*imagines Mel Brooks as Haskell programmer* I would buy such a T-shirt.
Cool stuff, thanks! I was worried that such an extension could make it possible to cheat the type system or something, but Turing completeness is not that big a deal.
So this is going to be a bit of a boost for Haskell.SG right?
 merchandising :: () -&gt; Money merchandising () = unsafePerformIO haskellTheFlamethrower
Don't get me wrong, I have no regrets using Haskell. It would be worth it even if it were just for the general sense of self-improvement and mental well-being it brings to my life :). It's just not the same in other languages, you don't learn anything new...
https://wiki.haskell.org/Thompson-Wheeler_logo
Right but does `-fno-call-arity` help you? 
The example compiles fine with me. What Spock version are you using, and which GHC? You'll need Spock 0.7.9.0 and any GHC &gt; 7.6 should work.
What? My post was about the misconception of "you can't run out of memory just have more swap". It has nothing to do with any specific bug.
It wasn't much of a choice, I just realized I hadn't touched it in a few years so I didn't bother setting it back up when I moved servers. Even before that the only thing I used it for was signing up for crap.
It is really sad when we've gotten to the point where we think go compile times are "nearly instant". Go is nearly as slow as C++ and java to compile: C++ 774 Java 812 Go 596 Ocaml 125
Sorry, I thought that you were OP.
&gt; It is really sad when we've gotten to the point where we think go compile times are "nearly instant". Go is nearly as slow as C++ and java to compile: &gt; what are these numbers? Super fast compile times are a design goal of Go, and in my limited experience I've found it live up to that.
We always wait to change our production systems until the .2 or .3 release of any ghc version. Usually by then any performance regressions have been uncovered and fixed.
https://wiki.haskell.org/Merchandise Updating this seems to be the way to go.
Compile times for a small piece of code. It doesn't appear that go has lived up to any of its design goals. It was touted as a systems language and can't be used as such. It was a replacement for C++ that is replacing python. It was supposed to have fast compile times, but it is nearly as slow as C++. Yeah, if you compare go to Haskell or rust its compile times will seem great. But compared to C or ocaml it is awful.
Could you elaborate? Links or otherwise? Those are very interesting numbers, I must say.
That may be the first programming book ever plugged by a politician!
Actually, `lens-family` and `lens-family-core` both provide zoom and state monad operators. They are in `Lens.Family.State.{Strict|Lazy}` `Prism`s are the only things that I miss, but sometimes I will hand-roll prism support using the `profunctors` library
There's http://www.haskellers.com/bling but I'm not sure about where the money goes.
Still better than a pie chart, I guess. And the labels do give length-to-numer mapping, that is somewhat helpful.
Yeah, I just use `hoist` if I want to target lower layers. THis works because `zoom l` is a monad morphism if `l` obeys the relevant lens laws I only use the state operators when showing off lenses to other people. For my private code I just use `zoom` in conjunction with `get` and `put`.
that's a good one :) I can see it arising from naively expanding type synonyms. i was thinking of lens in particular when I was writing my first post.... it made me pause and hesitate a bit, but I eventually posted without thinking if this situation. thanks! 
Can we have a Prelude that fixes `Num` hierarchy, by either (a) turning it into group/ring/field based hierarchy of classes or (b) behaving more like the C type heirarchy it seems to be emulating (by, e.g., letting `(/)` work with any arithmetic type and `(.&amp;.)` on any integer type -- automatic casts in lossless cases would also be nice.)? I prefer the former, but I can see reason for the later, particularly since the C types (vs. types from some other langauge) are somewhat privileged in Haskell due to the Foreign.C package hierarchy. In theory both could live side by side, but we'd have to determine which one has to use alternative operator glyphs.
Yes, please! Why is nobody working on that? That would be beautiful. Is there any library (not necessarily for Haskell) that does this? Anything in the world I can peek at?
No, I appreciate the derailing! :) Yeah, the purpose of `mvc` is *not* to simplify programming. It's only if you want to transform impure and concurrent code into pure single-threaded code, for property-based testing and equational reasoning. If you don't need that then you don't need `mvc`.
It is a fair ways down in this post: http://hookrace.net/blog/what-is-special-about-nim/
&gt; Yeah, the purpose of mvc is not to simplify programming. It's only if you want to transform impure and concurrent code into pure single-threaded code, for property-based testing and equational reasoning. If you don't need that then you don't need mvc. Why not have both? I mean, I see some symmetry between functions of type MonadPlus m =&gt; a -&gt; m b that can be instantiated by (a -&gt; ListT (State _) b) used by MVC, and a "handle" type, composed of a View and a Controller. Both can be combined laterally to form complex models or view/controller pairs out of simpler ones. But this seems to need type-level black magic to be usable (a'la your `total` but without the need to enumerate all possibilites in type parameters) 
I disagree that the documentation is enough. It doesn't answer the "why". Why would I use `Endo` over a simple `.` When does it come in handy? If I'm new to category theory, what should I look for to grok the Endo name? etc. I agree that the documentation in this case can contain the literal implementation, but it should also have text to help new and intermediate programmers out. It does no harm to advanced users to have it there.
Apparently you need more reputation to comment than to answer. So, have an extra answer. :D
Yeah, I don't know of a way to retain totality without the type parameters, though.
Well, I tried to, using with the code I linked to. It seems to me that set-like operations on lists of types, to manage "malleable" sum types, combined with using "malleable" product types (that are already provided by `vinyl`) parametrised by functions, should be enough to check for totality. Unfortunately, I admit I gave up in the end (the "polymorph" code I've linked to is more than half a year old, and it's tailored to `vinyl` 0.4; I've been using `hsqml` and direct monadic code for GUI since then) but I still think this approach has potential.
Aha. This was discussed here: http://www.reddit.com/r/haskell/comments/31i8o3/installing_ghc_710_from_scratch_on_windows/ Snippet: So far any package that depends on `data-default-instances-dlist-0.0.1` or depends on `data-default-instances-old-locale-0.0.1` will not compile. I managed to fix this for a particular sandbox like so: 1. Run the Global Cabal Packages below (use `rm -R ~/AppData/Roaming/ghc ~/AppData/Roaming/cabal` if you need to start again) 2. Set up the sandbox 3. In the bash run `cabal install -j` so that it attempts to build the package 4. Execute the following `cabal install data-default -j1 -force-reinstall` 5. Then run `cabal install -j` one more time Package should compile.
Wow, Rust is so much slower than I'd imagined. Still looks attractive, though (compared to, say, D, which is a total shock with those numbers). Rust will be focusing on optimizing the compilation process itself once they have Rust 1.0 final out, hopefully. Haskell feels like the eccentric outsider coming with the incomprehensible words and concepts and doing the job mysteriously but still suffering a bit for working in the unfamiliar lands of the other languages. Or at least it feels that way, with the huge difference between Haskell's and OCaml's numbers :/ .
&gt; I admit that using several versions of the verb "raise" in one piece of code is a design smell to me Eh, the same ambiguity affects both mathematical discourse and English language prose in this particular case. Maybe it is a smell, but it's a smell I'm quite used to.
Probably you are aware that this function is part of Applicative and Monad. I'm not sure what you mean by "wouldn't be able to ...", but let me just say this— You should probably think of "values" like `"welp"` and `Just "welp"` as *morphisms* in categories. So in a more general setting, where we cannot rely on the objects of a category to necessarily be sets or types (for instance, consider the category formed by a preorder on a set), if you wanted to talk about a "lift" operation, you might say that such a functor `F : C -&gt; D` must additionally map each morphism `C[1C,X]` to a morphism `D[1D, F(X)]` (where `1C` and `1D` denote initial objects in `C` and `D`). Anyway, my category theory is not so good, so I don't know if that helps or not.
`Just` is the structure map in the `Maybe` coalgebra. Hope that helps.
I've been wanting to give a talk on Haskell where I wait until I'm giving the talk to start thinking about and writing down what I want to say. I think I'll just start with Q&amp;A, and base the talk on what people ask. It depends on the crowd, though. If they already have experience in Haskell, we might not get around to me doing anything until people start asking them on Twitter how my talk went.
nope :( i did see the page when i initially ran into this but the issue they were having seemed different cabal install data-default -j1 -force-reinstall Resolving dependencies... Configuring data-default-instances-containers-0.0.1... Building data-default-instances-containers-0.0.1... Preprocessing library data-default-instances-containers-0.0.1... [1 of 1] Compiling Data.Default.Instances.Containers ( Data\Default\Instances\Containers.hs, dist\dist-sandbox-5cd49764\build\Data\Default\Instances\Containers.o ) C:\Program Files\MinGHC-7.10.1\ghc-7.10.1\mingw\bin\ar.exe: dist/dist-sandbox-5cd49764\build\libHSdata-default-instances-containers-0.0.1-GVMvZTIbSoDCiC0TxHw1BG.a-19136\libHSdata-default-instances-containers-0.0.1-GVMvZTIbSoDCiC0TxHw1BG.a: No such file or directory Failed to install data-default-instances-containers-0.0.1 Configuring data-default-instances-old-locale-0.0.1... Building data-default-instances-old-locale-0.0.1... Preprocessing library data-default-instances-old-locale-0.0.1... [1 of 1] Compiling Data.Default.Instances.OldLocale ( Data\Default\Instances\OldLocale.hs, dist\dist-sandbox-5cd49764\build\Data\Default\Instances\OldLocale.o ) C:\Program Files\MinGHC-7.10.1\ghc-7.10.1\mingw\bin\ar.exe: dist/dist-sandbox-5cd49764\build\libHSdata-default-instances-old-locale-0.0.1-6jcjjaR25tK4x3nJhHHjFM.a-19136\libHSdata-default-instances-old-locale-0.0.1-6jcjjaR25tK4x3nJhHHjFM.a: No such file or directory Failed to install data-default-instances-old-locale-0.0.1 cabal.exe: Error: some packages failed to install: data-default-0.5.3 depends on data-default-instances-old-locale-0.0.1 which failed to install. data-default-instances-containers-0.0.1 failed during the building phase. The exception was: ExitFailure 1 data-default-instances-old-locale-0.0.1 failed during the building phase. The exception was: ExitFailure 1 
He was the senior Wrangler[1] at Cambridge. So hopefully he won't have too much trouble picking up what a Monad is. [1] https://en.wikipedia.org/wiki/Senior_Wrangler_%28University_of_Cambridge%29
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Senior Wrangler (University of Cambridge)**](https://en.wikipedia.org/wiki/Senior%20Wrangler%20%28University%20of%20Cambridge%29): [](#sfw) --- &gt; &gt;The __Senior Wrangler__ is the top [mathematics](https://en.wikipedia.org/wiki/Mathematics) [undergraduate](https://en.wikipedia.org/wiki/Undergraduate) at [Cambridge University](https://en.wikipedia.org/wiki/University_of_Cambridge) in [England](https://en.wikipedia.org/wiki/England), a position once regarded as "the greatest intellectual achievement attainable in Britain." &gt;Specifically, it is the person who achieves the highest overall mark among the [Wranglers](https://en.wikipedia.org/wiki/Wrangler_(University_of_Cambridge\)) – the [students](https://en.wikipedia.org/wiki/Student) at Cambridge who gain [first-class](https://en.wikipedia.org/wiki/British_undergraduate_degree_classification) [degrees](https://en.wikipedia.org/wiki/Academic_degree) in mathematics. The Cambridge undergraduate mathematics course, or [Mathematical Tripos](https://en.wikipedia.org/wiki/Cambridge_Mathematical_Tripos), is famously difficult. &gt;Many Senior Wranglers have become world-leading figures in mathematics, [physics](https://en.wikipedia.org/wiki/Physics), and other fields. They include [George Airy](https://en.wikipedia.org/wiki/George_Biddell_Airy), [John Herschel](https://en.wikipedia.org/wiki/John_Herschel), [Arthur Cayley](https://en.wikipedia.org/wiki/Arthur_Cayley), [James Inman](https://en.wikipedia.org/wiki/James_Inman), [George Stokes](https://en.wikipedia.org/wiki/George_Gabriel_Stokes), [Isaac Todhunter](https://en.wikipedia.org/wiki/Isaac_Todhunter), [Morris Pell](https://en.wikipedia.org/wiki/Morris_Birkbeck_Pell), [Lord Rayleigh](https://en.wikipedia.org/wiki/Lord_Rayleigh), [Arthur Eddington](https://en.wikipedia.org/wiki/Arthur_Stanley_Eddington), [J. E. Littlewood](https://en.wikipedia.org/wiki/John_Edensor_Littlewood), [Frank Ramsey](https://en.wikipedia.org/wiki/Frank_P._Ramsey), [Donald Coxeter](https://en.wikipedia.org/wiki/Donald_Coxeter), [Jacob Bronowski](https://en.wikipedia.org/wiki/Jacob_Bronowski), [Kevin Buzzard](https://en.wikipedia.org/wiki/Kevin_Buzzard) and [Ben Green](https://en.wikipedia.org/wiki/Ben_J._Green), and even [Lee Hsien Loong](https://en.wikipedia.org/wiki/Lee_Hsien_Loong), Prime Minister of [Singapore](https://en.wikipedia.org/wiki/Singapore). &gt;==== &gt;[**Image**](https://i.imgur.com/Snt8HWQ.jpg) [^(i)](https://commons.wikimedia.org/wiki/File:Senior_Wrangler_1842.jpg) - *The Senior Wrangler, achiever of "academic supremacy", is admitted to his degree, 1842* --- ^Interesting: [^Richard ^Watson ^\(bishop ^of ^Llandaff)](https://en.wikipedia.org/wiki/Richard_Watson_\(bishop_of_Llandaff\)) ^| [^Lists ^of ^mathematicians](https://en.wikipedia.org/wiki/Lists_of_mathematicians) ^| [^List ^of ^University ^of ^Cambridge ^people](https://en.wikipedia.org/wiki/List_of_University_of_Cambridge_people) ^| [^Wrangler ^\(University ^of ^Cambridge)](https://en.wikipedia.org/wiki/Wrangler_\(University_of_Cambridge\)) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cqlhylu) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cqlhylu)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Humor detectors aren't working that good here. ;) For now, Haskell is also the best language to stay with. 
Why was his name cut off in the title of this post?
In a way, lens was edwardk's response to ClassyPrelude.
its Cherlene.
Could be that poster thought Loong is his last name, but actually Lee is.
Yes , well I didn't mean to *only* use the code as the documentation, I meant that if one would just copy the single line of code into the documentation (along with the other comments) it would be very helpful without making the documentation too verbose. 
Neat! Note that Oleg gave a general approach to encoding (finite) problems of this sort in vanilla Haskell: http://okmij.org/ftp/Algorithms.html#dyn-epistemology
Sorry, I'm just catching up. Glad you worked it out! It sounds like some of the libraries aren't yet fully compatible with GHC 7.10, out-of-the-box. 7.10 is only a month old, so hopefully this gets better. That issue link also makes an interesting suggestion about length of names. This actually might be a real problem; even modern Windows versions have issues with path sizes (I see this a lot in my day job with our automation system). That might make sense for why the cabal get and cabal install operations work better when separated, if the problem lies with the length of shell commands.
We have a number of good bindings for mysql and postgres. The ODBC drivers are also adequate, though the bindings are of an older sort and perhaps not as modern in style, last I checked. There also are a sea of bindings to the various NoSQL dbs around if that's your style -- cassandra, mongo, you name it. Essentially, I think there's good bindings for nearly anything out there, and there are a variety of preferences. Furthermore, many common layers over databases, like persistent, hdbc, and haskelldb all offer multiple backends. If anything, there's way _too much_ choice to quickly make an informed decision these days!
hmm, still, why would the last name be cut off? Its like saying "Minister Michelle considers leaning Haskell" rather that "Minister Michelle Johnson considers learning Haskell"
Well, because that's what was linked? Also, I don't work with projects too big (mostly C++, with several biggish cpp and h files, on VC++ and an i3 laptop, and I still get lower times than the post's C++ times, so... \*shrug\* ).
I've been getting the impression that a lot of these libraries aren't used much though. The download numbers are relatively lower than I'd expect and there doesn't seem to be any libraries that stand out as stable enough for use in a production system. What do you think?
I am not an experienced Haskeller, but I managed to use Redis in my Haskell-based server monitoring application (with the [hedis API](https://hackage.haskell.org/package/hedis)).
Mostly postgresql and sqlite with the "simple" bindings
How come?
As someone who uses both python and haskell, I'd like to say that the items that you've picked out are pretty poor examples. Only items one, three, and four having anything at all to do with python's type system and item number three is almost purely a matter of style and not a deficiency of dynamic typing. And, as you say, four is a tough comparison, what with haskell not having the concept of subclasses. The second item has more to do with the fact that python allows you to rebind variables. A valid issue and one that haskell doesn't have, but static typing has nothing to do with it. I'm not sure what you even mean with item five. Python has list comprehensions too, but this isn't about list comprehensions it's about dictionary comprehensions. Arguably, haskell fares worse on this item than python since it doesn't have the "solution". You can iterate over a pair of lists perfectly fine using indices in Haskell. Item six is relevant to both languages.
I'm not convinced that's true. If I write the following two sets on a blackboard... { 1, 3, 5 } { 5, 3, 1 } I think you'd have no difficulty considering these two sets equal, irrespective of the fact that they're represented differently. "But they're sets, not lists!" I hear you say. To which I say that actually the they're chalk marks which we understand to represent sets. Why can't we just as easily understand particular Haskell lists to represent sets? Sure the type system then doesn't know everything and can't protect us from certain kinds of mistakes, but short of making the type system prove everything we do correct (which is not a world I want to live in) that's a fact of life anyway. 
for traditional sql -- hdbc, mysql-simple, postgresql-simple, persistent and groundhog are all fairly commonly used and have all been used in _lots_ of production systems over the years. there are more type-safe layers that are also very flexible, like haskelldb and opaleye, depending on how comfortable you are with some tricky types. my recommendation is pick a database first, then look at bindings for it and choose among the common ones based on the sort of api you're looking for, how much type-safety you want vs. how much flexibility, how much control vs. how much abstraction, etc.
The problem is we have different overloaded meanings for the `Eq` class and just one class, so no way to distinguish these meanings. I don't think its a bug either in `HashSet` or in our `Eq` class -- rather its an indication we ideally would have a few classes with different properties attached to each.
I'd consider an equality definition that violates the property to be flawed and at least unexpected.
&gt; Why is nobody working on that? Except for [numeric-prelude](http://hackage.haskell.org/package/numeric-prelude), which has been actively developed since 2001 by a group of prominent Haskellers led by Henning Thielemann.
&gt; Arguably, haskell fares worse on this item than python since it doesn't have the "solution". You might be able to concoct something with a state monad and the MonadComprehensions pragma, but I'm not sure. I think the most elegant "solution" in Haskell is to do a list comprehension that generates tuples and then run Map.fromList on it. Which you could do in Python as well, but it's slower (?) and not as clean syntactically.
I liked your encoding very much. I took it and rewrote it in a language with native support for logical constraints: http://dtai.cs.kuleuven.be/krr/idp-ide/?src=8f3fd1909a4962f8cd37
We use Postgres. It's well supported, there are several different bindings, from low level bindings with libpq to high-level type-safe libraries like Opaleye (which we also use).
/u/siblip: &gt;&gt; Does that mean there is already some related work is going there ? Me: &gt; No, there is no decision to do this. And it turns out I was wrong. John MacFarlane, the author of pandoc, [already worked on it](https://mail.haskell.org/pipermail/haskell-cafe/2015-April/119253.html) in 2012, even before pandoc supported MediaWiki as an input format. EDIT: See also [John's post](http://www.reddit.com/r/haskell/comments/339qxm/haskellcafe_help_wanted_with_wikihaskellorg/cqjuaue) right here in this reddit thread.
The most boring part of the current Prelude is that it doesn’t export the right abstractions. For instance, `(.)`, `id`, `mapM`, `mapM_` and others are wrong. I’d love to see them defaulted to their respective abstractions – that is, `Category`, `Traversable` and `Foldable`. That wouldn’t even break code.
PostgreSQL with Persistent as the interface. Persistent works for me because I never have to interface with existing databases.
Why not toList definition to be flawed?
This confusion is the result of misuse of the `Eq` and `Ord` classes in the APIs of container libraries. Those libraries require only a weaker notion of comparison, but their APIs traditionally force the use of `Eq` and `Ord`. So we end up writing invalid instances in order to be able to use the containers. In the case of `HashSet`, we could have had valid `Eq` and `Ord` instances and a separate comparison function to capture the notion of "only with respect to its elements". But that would make life difficult, because then you couldn't nest a `HashSet` in a set, or use it as the key for a `Map`. Since that is a much more common use in practice, we got the invalid instances instead.
I think the "principle of least surprise" should apply. I would expect two Sets with the same contents to be considered equal, regardless of the underlying structure. I would even expect two different implementations meeting the same interface to be considered equal Such that HashSet {1,2,3} == BTreeSet {3,1,2}. (Mind you I don't believe this example is possible in Haskell to begin with, but that's another debate). I would also expect that a Set, when being converted to a List, does not have a guaranteed ordering unless I specify one, which I rarely would since it wastes CPU cycles. From the perspective of an engineer (not a mathematician) my expectation would be that a Set behaves as described above, such that f(a) == f(b) does not imply a == b I can definitely see a case for a more strict version of equality where this law does hold true, but in practical terms I don't see it providing any benefit to the types of code I personally write (YMMV)
Hmm ... nobody forced me ... I'll treat this talk as a lazy list of slides and switch forth only when asked to :)
Because `f(x)=f(y) where x=y` is a strong and widely applying intuition which you can do equational reasoning based on. It *shouldn't matter what `f` does* if the property x=y holds. We can consider on a case-by-case basis that `f(x)=f(y) where x=y` and `g(x)=g(y) where x=y`, etc. but then you're only proving things about special functions whereas I'd prefer the much more general proof that my equality is reliable across all functions. Here's a practical example from the [system-filepath](http://hackage.haskell.org/package/system-filepath) package. Consider this: λ&gt; "/foo/bar" &lt;&gt; "/" == "/foo/bar/" True Okay, if `x` and `y` are equal then `f x == f y`, right? λ&gt; parent ("/foo/bar" &lt;&gt; "/") == parent "/foo/bar/" False Nope. Same for `collapse`: λ&gt; "/foo//bar/" == "/foo/" &lt;&gt; "/" &lt;&gt; "bar/" True λ&gt; collapse ("/foo//bar/") == collapse ("/foo/" &lt;&gt; "/" &lt;&gt; "bar/") False Is this a flaw with `parent` and `collapse`? Or `&lt;&gt;`? If so, what other functions are flawed? That's a wild goose chase. I think the best way to nip it in the bud is to deal with the equality properly from the start and then all functions will satisfy this property.
The problem is that there really isn't a unique function f :: HashSet Int -&gt; [Int] in mathematical terms such a function is a section, that is you have a guarantee that fromList . f = id but there isn't a unique way to impose an order on a set, which is why the implementation of the HashSet as some kind of tree leaks, when converting it back to a List.
&gt; Take alpha-equivalence on lambda-terms with explicit names, for example, where two terms t and u often are alpha-equivalent but not syntactically-identical. I think if you want to treat alpha-equivalent terms as equal then you shouldn't expose actual names of terms (`t` or `u`) once consumed but rather normalize everything to e.g. \1.1 and \1.1 or w/e. Mixing and matching "equal sometimes except when not" just leads to JavaScript-style bugs eventually. &gt; but you *could* write functions that perform branches on particular choices of bound name, violating the property above. The intuition that equality means EQUAL is very strong, it'll make you (and your coworkers) discard evidence when debugging, assuming that things are equal, when actually functions are using sneaky bits of data. I've had the urge plenty of times to define equality instances on just parts of a data structure but in the end it's not a good road to go down. Although obviously, if you can prove the rest of the data isn't accessible or is always the same for the given "key" part that you're comparing, it's okay. 
I see your point here. Two things concern me about it: 1. I find this property on sets rather intuitive (`{}` means empty set and `.insert` is a method to insert element): {}.insert(a).insert(b) == {}.insert(b).insert(a) With a proposition that `==` is wrong in OPs case, it would imply that new one should not state these equal (while non-importance of element-insertion into a set seems like an obvious thing to me). 2. I think that there is no "natural" toList operation for a set, so every toList operation that you implement must be "internal" to set's implementation, e.g. look behind the abstraction. So, overall I would rather say that the problem lies inside `toList` operation, not in `==` one, and [Data.HashSet from unordered-containers](http://hackage.haskell.org/package/unordered-containers-0.2.5.1/docs/Data-HashSet.html) does hold this property (probably by having strict ordering rules for `toList` implementation). Do you think this makes sense? p.s.: `system-filepath` "parent" example is little different. I don't really understand what would it mean to concatenate `/foo/bar` and `/`, I would make a library to only allow concatenation where second argument is not an absolute path, since it doesn't seem as a valid operation in terms of path-manipulation. p.p.s.: same for "collapse" example. Why is `/foo//bar/` a valid path? I know that different software translates this to different paths (some to `/foo/bar`, some to `/bar`). Should be illegal, I think.
Why is persistent not type-safe in your view?
This is what I was thinking, use list comprehensions and 'Data.Map.fromList' or 'Data.Array.IArray.array' -- of course the array is an array and not a dictionary, but it list comprehension works just as well.
The examples I picked were things that you can but should not do in Python (because they are anti-patterns), but are also things that you **cannot** do in Haskell because the program wouldn't compile if you did. This is what I mean when I say Haskell solves these Python anti-patterns.
Yeah, I agree. `toList` breaks the abstraction. My feeling on the matter is that just one operation in the API that exposes different internal data—for the same external data—than is made available in your equality compromises that equality's reliability. Whether `toList (fromList [5,2,2,1]))` outputs `[1,2,2,5]` or `[5,2,2,1]` doesn't matter, but `toList (x &lt;&gt; y) = toList (y &lt;&gt; x)` *should* hold. P.S. I don't think much in system-filepath makes much sense, either. I wouldn't design it like how it has been designed.
I've found things to be the opposite... it's impossible for me to imagine using python on any large project where maintainability and safety and flexibility are important. large projects is where haskell shines. on the other hand I often resort to scripting languahes like python or ruby for small ad-hoc scripts. 
I use mysql with mysql-simple which is an awesome library. I just miss the possibility to call stored procedures. ( I thought about it for a while after talking to /u/bos who pointed me in the right direction, but I still wasn't able to find a way to do it ). 
&gt; Here's a practical example from the system-filepath package. Consider this: &gt; &gt; λ&gt; "/foo/bar" &lt;&gt; "/" == "/foo/bar/" &gt; True &gt; &gt; Okay, if x and y are equal then f x == f y, right? &gt; &gt; λ&gt; parent ("/foo/bar" &lt;&gt; "/") == parent "/foo/bar/" &gt; False &gt; &gt; Nope. Are you sure you're using `Filesystem.Path.FilePath` in your first example, and not `String`? &gt; ("/foo/bar" ++ "/") == ("/foo/bar/" :: Filesystem.Path.FilePath) False
I suppose it depends. In category theory, equal is meaning &gt; *p* is true for **A** iff it is true for **B** So let the two sets be represented by **A = {1, 2, 3}** and **B = {1, 3, 2}**, you could handwave something about notation and memory layout being unimportant. But the to list function is basically a function that take the traverse the set and return a list - in other words the above definition is no longer true, for this particular implementation. And the devil is in the details to be honest, because a set in Haskell is not really a set but a particular representation of a set, and my reasoning is that you should not have to worry about the implementation to know if equal works or not. If they both represent the same memory layout or everything you could possibly do short of reading and comparing bit for bit, will result in the same output - then they're equal.
I totally agree that `system-filepath` seems like a poor design, although this is the first time I've heard about that package, and I haven't looked at the particular problem carefully. I don't agree with your assessment regarding `toList` though; I do think it's undesirable, but I still think it's the best choice. &gt; Whether toList (fromList [5,2,2,1])) outputs [1,2,2,5] or [5,2,2,1] doesn't matter I agree, but this also isn't really relevant. What's relevant from your perspective is whether or not `toList (fromList [1,2,2,5]) == toList (fromList [5,2,2,1])`. &gt; `toList (x &lt;&gt; y) = toList (y &lt;&gt; x)` _should_ hold. That would be nice, I agree. =) But you need to be able to iterate over the contents of a finite set. Sometimes you need to break the abstraction to some degree or another, and this particular case (unlike your system-filepath examples) seems like a pretty mild breakage that should be obvious to anybody who understands basic data structures and where there isn't a obviously superior alternative. I guess that I've played around with various number-theoretic algorithms enough that this doesn't seem like a big deal to me. These algorithms also involve coarser notions of equality that are important to reasoning about the code, and I haven't experienced anything that reasonably captures the sorts of things you want to capture in this particular domain.
I think next time i have haskell questions I'll ask instead of being a question vampire and quitting though :) and maybe not try out the latest and greatest until i'm more experienced in the ecosystem.. good thought on the path size i may try to look at it in another sandbox more 
I think this is highly dependent on who you are, and what your background is. I for one, would expect a general equality function to mean that everything that is true about either Set is true about the other, and then be able to give an equals function a parameter that define what kind of equality I'm looking for. It would be extra code, but for mathematically thinking people it would immediately clear what I mean.
I think your example is a type error. But point taken.
Examples include a [parser of expressions with (Agda-style) mixfix identifiers](https://github.com/ollef/Earley/blob/master/examples/Mixfix.hs) which is rather easy to do in this setting where left-recursion is handled gracefully, in contrast to most combinator libraries where it's a little bit trickier.
`(.)` and `id` will take a while, but I thought FTP gave `mapM` and `mapM_` the right types. *checks* Ah, almost, `mapM` and `mapM_` are basically just `traverse` and `traverse_` with specialized type signatures in the latest base from GHC.
This just seems like a place where practicality of use has won out over strict adherence to principle. Also == is not =. The fact that you treat then as the sane is only because you're already a programmer. My wife asked me what == was for when I was introducing her to programming. The idea that it was true mathematical equality wasnt even brought up. It's just a function.
Looks cool! How does your algorithm compare to [marpa's](http://savage.net.au/Marpa.html)?
99.9999% of the time you don't need NoSQL. Your project is probably not a special snowflake that can't be better covered by RDBMSes
Wait, are you arguing that (==) should sometimes return False for two sets with the same elements?
For the benefit of those of us watching from home: Marpa is written in Perl, not Haskell. 
Ah, thanks! It was a while since I last worked with it.
We do the same, plus [esqueleto](http://hackage.haskell.org/package/esqueleto) for more complex queries.
I'm going to register my stark disagreement here. In math, we often consider a set of structures "up to such-and-such equations". For example, we might take the free commutative monoid as a halfway decent example of this: one fine way to define it is to start from the free monoid and identify "a &lt;&gt; b = b &lt;&gt; a" everywhere -- which is to say take the obvious congruence relation implied by this equation and quotient the free monoid by it. To talk about equality well, I think it makes perfect sense to first identify what observations you intend to make on the objects in question. Then to say that `x = y` means `f x = f y` for all possible observations `f` is perfectly cromulent. So I don't think "absolutely identical" is the only meaningful way to talk about equality. Abstraction boundaries are a thing in both math *and* programming. I agree with OP (as I told him on IRC): the problem is that, since hash sets "quotient" out by the collision ordering, they should do so consistently, and in particular `toList` should also quotient out by this, e.g. by requiring elements to have an `Ord` instance and sorting collision buckets before returning the list.
&gt; I'd consider an equality definition that violates the property to be flawed and at least unexpected. But the equality here does not violate the property, under the condition that the definition of the datatype is encapsulated and functions like `toList` are not exported. The only way `toList` should be able to work is to add a `Ord a` constraint and provide a sorted list. This shouldn't be a problem because real code (ie, not used for viewing the data in ghci) would not call this function and thus the extra cost of sorting shouldn't be an issue. (Of course, the fact that you might be using a hashset for non-ordered data would suck, though)
I agree that's a problem. But in this case it seems fairly easy to fix things without reworking the entire language design by simply having `toList` sort each collision bucket.
That makes sense, thanks for explaining!
This question is amusing to me as an enterprise developer. It is completely oblivious of the fact that 99.99% developers do not chose a database (and definitely not because of a language they write the code with). They work with an existing one. And unfortunately haskell ecosystem still has practically no support for big commercial databases (MS Sql, Oracle, DB2). 
I've wanted this for ages!
PostgreSQL with postgresql-simple and groundhog
Your comment is definitely true for the field in which you work. However, in the non-enterprise world (of which I am a part), I frequently choose my datastore. I agree that the liberties I have are uncommon, but I think that they're more common than your statistic indicates.
In particular, if you are at the beginning of a project, a relational DB certainly makes more sense. At least wait until you figure out how your data should be used before trading flexibility for speed.
Not really. I do use hdbc-odbc to work with Ms Sql Server. And current library has showstopper bugs that i myself cannot fix. For example large text fields are cutoff on 4096 mark. I made an ugly hack to the existing code by allocating a whopping 2 Mb of Ram for text fields and hoping that my specific use case will never encounter a situation where the text is larger than 2 Mb. I simply do not know how to properly fix it because low level odbc C API is arcane and i cannot figure it out, especially on linux. Not only that but error handling is also broken. For example if you execute several statements in one go, it will not report error if the first statement succeeded but the statements after that failed. This currently forces me to execute statements one by one. I can assure you current haskell odbc driver is not production ready. 
I would argue that your view is affected by a self-selection bias. Since haskell is practically not used at all in enterprise environment, there are no enterprise developers visiting this sub. This creates an illusion that there are much more people like you that there really is :) 
I would be most interested in a mousepad with that logo
In 2015, I shake my head at the notion of having to save. Lose my work by default, unless I press save, wat?
Yup, Postgres with [postgresql-simple](https://hackage.haskell.org/package/postgresql-simple). We considered esqueleto but ultimately decided that raw SQL is superior (though we have had a number of bugs at the Haskell/SQL barrier). 
`(.)` and `id` from `Category` break type inference for a number of more complicated use cases. I don't have an example at the moment (issues with higher-ranked types and lens compositions iirc) but I stopped using `BasePrelude` because of this. 
Yes I think you are right, but I'm not 100% certain.
Good point about quotients and equivalence classes, because I find they come up all over the place in programming because they's always the tension between "what these bits mean" vs "what these bits actually are." There are probably many pragmatic uses of `toList`. I would say it should have a note of warning in its docs but it's otherwise fine.
I hope thi answers your question. I believe your error can be fixed in the in the `(n+1)` constructor you chose to define. According to the DList class, there can only be elements mapped to a singular monad b, thus `fromList :: [a] -&gt; List b` So in oder to have a sequential sequence I might try using: `cons :: DList n a -&gt; DList n (a -&gt; _ + 1 -&gt; b ) -&gt; DList n _ b ` This would describe the transformation and would allow you to then use: `zip' :: Dlist n a -&gt; DList n b -&gt; DList n (a b) zip' (Cons x xs) (Cons y ys) = Cons (x,y) $ zip' xs ys` Thanks
How about [reporting it](https://github.com/haskell/haddock/issues)? Sounds like it has something to do with module re-exports. It may be related to [#296](https://github.com/haskell/haddock/issues/296).
I don't think Haskell is different. You need both the type and instance. 
Hey, thank you so much for this. I have three chapters left of LYAH and was a bit worried about how I would continue studying the language. The bitemyapp guide looks like the perfect next step.
Agreed that putting Ord constraint on set elements would allow one to write toList in a way that doesn't violate the property (as I hinted at in the original question). What if you want a HashSet of elements that don't have a total order? Would you then say toList should be removed?
Hey, it got us UNIX (including all of the BSDs) and Linux (over Hurd). Thing is, you can always introduce enough Sum types or even Dyn uses to get the same things done in Haskell. Haskell makes the "wrong way" harder, but refactoring really easy, which I *think* is an advantage.
The first version allows the result of fix to be shared, while the second must recompute the result on each iteration. That means not just more CPU time, but more memory allocation as well. Consider this GHCi session: &gt;&gt;&gt; let fix f = let x = f x in x &gt;&gt;&gt; let fix' f = f (fix' f) &gt;&gt;&gt; :set +s &gt;&gt;&gt; length . take 10000000 $ fix (0:) 10000000 (0.11 secs, 567547472 bytes) &gt;&gt;&gt; length . take 10000000 $ fix' (0:) 10000000 (2.75 secs, 1280162880 bytes) 
I think the main advantage that RDBMSes could have over NoSQL is ACID, but very few RDBMSes are truly ACID and even when they *are* we often don't to transactions that are the correct size to take *advantage* of ACID properties. The relational algebra, in particular the SQL syntax is also a pretty good tool even if everyone writes it like they are a fortran/cobol programmer and their employer didn't spring for the lowercase upgrade to their terminal. (Did you know SQL keywords are *case-insensitive*?) But, there's nothing about the SQL syntax that enforces traditional RDBMS storage or ACID behavior -- I believe there are several NoSQL data stores that allow SQL or SQL-like queries. That said, I still reach for PostgreSQL because I'm already familiar with it. I should definitely force myself to explore some other things, next time I get to choose my data store.
Hi, author here. This work orginally existed as a [patch against GHC](https://gist.github.com/christiaanb/8396614). I was however very fortunate that Adam Gundry and Iavor Diatchki teamed up to create type-checker plugins for GHC 7.10. At the moment, I use the plugin for my [Haskell to hardware](http://www.clash-lang.org) compiler, and only intended to expose the users of that compiler to the plugin. Especially as both Adam and Iavor have upcoming publications about their own plugins and I didn't want to steal their thunder. Anyhow, I think the [testsuite](https://github.com/christiaanb/ghc-typelits-natnormalise/blob/master/tests/Tests.hs) gives a feel what is possible to write with the plugin. I intend to write a blog post on both the plugin itself, and how to write your own plugin soonish.
but it's not just a function. in Haskell, it's a typeclass method with laws. otherwise, don't use ==, use 'equals'.
Can't wait! I'm sorry to say I will at least be using it for now without using clash :-) Thanks for the great work.
Very nice! I wonder whether the same technic (or even parts of your library) could be used to make parsing library for [PEGs](http://en.wikipedia.org/wiki/Parsing_expression_grammar).
why should a HashSet ever equal a BTreeSet? can you give me an example of a situation where you want this? if they have the same exact interface, then can't you just use mutual conversion functions? maybe using a new "duckEqual" would be better.
Parts of these were *very* accurate observations. To the degree where I, having used Python and Haskell for many years, still hadn't found the words you did to describe what we both have experienced. Good write! But one small question. It says to read chapter 8 of LYAH the first thing you do. Is that really what you meant to say?
Whoops, I meant reading chapters 1-8, not just 8! I've updated the site with that correction. And thank you!
Slightly off-topic, but when did the "browse" link start laying out files in a tree? Super useful.
Am I correct in saying that the "problem" here is that the runtime/compiler/whatever doesn't know that (fix f) = (fix f)?
It would have to know that the result of fix' is infinite enough so that when you take 10000000 elements you'll really be measuring the length of the infinite list. Here fix' function is simple enough, but what if function definition isn't that clear? Also, take 5 xs won't always return a list of length 5. If you have a somewhat clear idea how one would generalize this so it's useful for other cases, I'm sure it would be a lovely thing to implement.
In general GHC doesn't do a lot of common sub expression elimination / cycle creation as it can induce space leaks that don't exist in the source.
You're welcome to write a compiler plugin to reason about this state and perform this optimization. It doesn't do so out of the box though, no.
I will credit you! Everyone who helped me, everything I read to solve stuff will get credit! I'm sorry I haven't responded to your help. I wrote my question right before a lecture of the Leuven Haskell User Group and only got to work on the exercise the following day. And then my girlfriend came over and programming is a non-factor then.. I managed to solve the exercise and gain insight in class instances for functions!
How can cycle creation introduce a space leak?
Cool, good luck! Very excited to see what you come up with.
That's actually an accurate description of what I'm doing. I prove the program correct first and then translate the equations to Haskell code, which is easy because Haskell code is basically not that different from the original equations! This was how I wrote the `pipes` library, for example. I actually implemented the entire library on pen and paper first, proved the relevant laws by hand, and then translated my notes to Haskell. Same thing for `mvc`, too, which was conceived entirely on pen and paper.
Really great, constructive article that mirrors many of my experiences switching from Python to Haskell. Thanks for posting!
Go to Hackage page for this project and click "browse". All the files are shown and not just the top level directory.
Here is an example where CSE (common subexpression elimination) can introduce a space leak. ([1..]!!1000000, [1..]!!999999) The above expression evaluates in constant space, however evaluating let l = [1..] in (l!!1000000, l!!999999) will consume space proportional to the value 1000000. The trade off is that it takes less time. The same principle can happen when you introduce any sort of sharing, including the sharing done by the let expression in `fix f = let x = f x in x`. Having the compiler introduce that let could mean that the heap holds onto a lot of partially evaluated values in a first pass that never get used in a second pass over the data structure. Without the let expression, the partially evaluated values in the first pass can be tossed away as you go along, and would be recomputed, if necessary, in the second pass. Trading space for time.
I really liked your comment about anchoring words to something more relatable. I personally like "sequenceable" for Monads because using using the sequence function was the first time Monads clicked for me. I'll disagree about not worrying about IO at first. I think it's important to learn to separate out pure code from IO code early on. E.g. Reading from the file system or user input and passing that to a series of functions that are pure, and then returning that to the main function. E.g. In your sudoku example it might be better if someone doing it returned a string that they could print to the console, rather than learning ways to "get around" IO at the beginning. On another note - Python programmers often like how terse their language is without all the semicolons and braces. Now that you've done Haskell for a while do you find writing in Python to be verbose? I know there are type signatures in Haskell that add what some might see as clutter, but I find Python still has its excesses. I wonder how J programmers must feel looking at everyone else?
It was just a point about naming really. I totally agree that in Haskell you need both type and instance, it was more just that you would call `&lt;&gt;` the monoid _operation_ in mathematics, rather than calling it the monoid. Not really a disagreement, just trying to clarify the terminology on the mathematics side.
&gt; I was however very fortunate that Adam Gundry and Iavor Diatchki teamed up to create type-checker plugins for GHC 7.10. So does this mean GHC 7.10 supports this kind of Nat constraint solving out of the box?
I'm curious about this now. How did you start using this method of working? Was it something you explicitly decided to start doing (or perhaps to try doing) or did you fall into it naturally? I'm curious because I'm a mathematician by inclination and training, and programming has always been a side interest, and I'd never have thought of doing things this way, even though it's absolutely the only way I can imagine trying to formally prove a conjecture. I suppose I'm also interested in trying to understand if I can leverage my background in theorem proving to find a different way of programming or at least thinking about it. I can see the big picture of how you do it, but not the details. Do you know of or have you written anything anywhere as an example of this kind of thing?
The "Browse" link on [the top-level Hackage page for this package](http://hackage.haskell.org/package/Earley) links me to [a list of all packages](http://hackage.haskell.org/packages/), organized by category. Maybe you could put a link directly to what you're seeing?
Ehm, no. I means that now everybody can use a plugin to do this kind of Nat constraint solving out of the box, instead of having to patch GHC.
Nice article. The only thing I miss it the REPL. When it comes to debugging typing things into the REPL give a whole new workflow. I liked this in Python, but in Haskell it's somehow more pronounced.
He means "browse" under the "Downloads" subhead which lets you inspect the tarball, like so: http://hackage.haskell.org/package/Earley-0.6.0/src/
Sometimes you simply don't care about the underlying data structure. You care that you have a Set - the underlying details are an unimportant black box. For example I might expose an API call that takes a Set as an argument, and compares it against another Set internally. My API does not want to enforce that the user uses a particular version of Set - they might have a very good reason for using a different implementation to mine - nor should it need to care which implementation they use. &gt; maybe using a new "duckEqual" would be better. I think it's probably a more complex discussion about what the definition of equality is, but you're right - this might be better off as a separate type of equality check with its own name and laws.
It was a fairly subjective comment - I'm sure everyone's way of comprehending it is different. For me I see a ton of operations of sequencing Monads (`&gt;&gt;=`, `&gt;&gt;`, `sequence`,`mapM`, etc.). I can't see it as joining because what does `join :: IO (IO a) -&gt; IO a` join together? Does it merge two `RealWorld`s together? To me it sequences two IO actions, so that the outer one happens first followed by the inner one. And `join :: [[a]] -&gt; [a]` is just `concat` but I still see it as creating a single list sequence from multiple lists. You see it as joining, I see it as sequencing. So in that sense my understanding is subjective. If we want to be precise we might as well just say that "a monad is monoid in the category of endofunctors" and be done with it.
If you want to use rethinkdb, also take a look at the alternative haskell driver: https://github.com/wereHamster/rethinkdb-client-driver.
I can imagine `join` being used with `IO`. Example: main :: IO a main = join getCommand getCommand :: IO (IO a) getCommand = getLine &gt;&gt;= parseCommand parseCommand :: (String -&gt; IO (IO a)) Of course, `join` is just `(&gt;&gt;= id)`, so we can rewrite as: main = getLine &gt;&gt;= parseCommand &gt;&gt;= id
If you go with the "Box" analogy for IO that's fair enough. But the [implementation for bind in IO](http://hackage.haskell.org/package/base-4.8.0.0/docs/src/GHC-Base.html#line-1064) is: bindIO :: IO a -&gt; (a -&gt; IO b) -&gt; IO b bindIO (IO m) k = IO $ \ s -&gt; case m s of (# new_s, a #) -&gt; unIO (k a) new_s unIO :: IO a -&gt; (State# RealWorld -&gt; (# State# RealWorld, a #)) unIO (IO a) = a By passing around a `State# RealWorld` the compiler is able to determine what sequence a set of IO actions happen in. Consider `(IO a) &gt;&gt;= (a -&gt; IO b)` because `IO b` relies on the `RealWorld` value that's threaded to it from the previous IO action, you can't get the value of `b` without running the side effects of `IO a` first. This is different to say, `let {x = 1 + 2; y = 3 + 4} in x + y` where the compiler can choose to evaluate `x` and `y` in any order. But with the IO example, in order to get to `b` we must process `unIO :: IO b -&gt; (State# RealWorld -&gt; (# State# RealWorld, b #))` first. This requires passing in a `State# RealWorld` from the `IO a` action first. While you could say that you are joining the two actions together, the order of that join is significant. You can't get the `RealWorld` from the inner action and apply it to the outer action, you must take the outer `RealWorld` and thread it through. So the sequence in which `backupSystem &gt;&gt; deleteAllOfTheThings` happens matters.
Where are the laws specified? I've never seen any for Eq.
Point taken. =)
Thanks! I found Hoogle earlier in my research, but I didn't fully understand how to navigate it or understand what it was trying to tell me. Looking at it now, I feel idiotic since lines pretty much is word for word what I was looking for.
For the last 3 years I've had really long bus commutes to and from work, and I didn't feel comfortable programming on the bus. However, I didn't mind using pen and paper or just thinking (if I had to stand) so I had to learn how to program either mentally or in pen and paper. Fortunately, mathematical reasoning lends itself really well to this style, especially if you use abstract algebra or category theory to structure code/proofs. Those two branches of math really excel at proof reuse and proof compression, so I organize my code along categories/functors/monoids/applicatives/semirings so that I can understand and manipulate the proof at a high level without getting lost in the weeds. Here's a concrete example of an equational reasoning session I just did yesterday. I have a syntax tree for a programming language with updatable sub-expressions and I want to compose those individual updates into updates for the whole tree. The syntax tree looks something like this: data Expr a = Lam ... | Var ... | ... | Import a ... where the "a" will be a path to some external updatable resource (like a file that I am monitoring). The solution to my problem is a bunch of very concise "abstract nonsense": I can make Express a monad (it's easy because it is a free monad for a specific functor) and make it Traversable (using GHC's DeriveTraversable extension) and I have an Applicative in my MVC-updates library for Applicative "Updatable" values. Then the solution becomes: update :: (FilePath -&gt; Updatable (Expr Void)) -- Listener for updates -&gt; Expr FilePath -- Expression to continuously update -&gt; Updatable (Expr Void) -- Stream of updates update listen expr = fmap join (traverse listen Expr) In other words, I apply the listener to each Import sub-expression, then use traverse to combine them into one big listener for the whole tree, and use join to fix the types. Without writing any code or consulting any type checker I know it's "obviously correct" because I know each component is correct in isolation and the "update" function that glues them together is small enough to fit in my head and reason about. Once I know things like "Updatable is an Applicative", "Expr is a free monad", I can reason about them at a high level, abstracting over the specifics of their implementation, even if the implementation is really complicated. This is what I mean when I say that category theory and abstract algebra lead to large proof compression. The proofs become so short that I can fit them in my head, and then reuse the result for downstream proofs without any leaky abstractions. Sorry if that was a bit terse, but I'm typing all this out on my phone, otherwise I would elaborate more. The closest thing I've written on this subject is my blog post on [equational reasoning](http://www.haskellforall.com/2013/12/equational-reasoning.html) and my follow-up post on [scaling equational reasoning](http://www.haskellforall.com/2014/07/equational-reasoning-at-scale.html).
It wasn't meant as a characterization of the current state of classy-prelude, just the original release of classy-prelude. There's no doubt that classy-prelude has gotten much better since then; my point was rather that `lens` appears to have been born out of a desire to make the use of van laarhoven lenses a suitable replacement for (the original release of) classy-prelude. June 23, 2012. roconnor publishes [Polymorphic Update with van Laarhoven Lenses]( http://r6.ca/blog/20120623T104901Z.html) June 24, 2012. edwardk publishes [Mirrored lenses](http://comonad.com/reader/2012/mirrored-lenses/) July 12, 2012. snoyberg publishes [classy-prelude-0.1.0.0]( http://hackage.haskell.org/package/classy-prelude-0.1.0.0) July 25, 2012. edwardk publishes [lens-0.1](http://hackage.haskell.org/package/lens-0.1) 
One extra thing. What would your suggestion be to sort a list of first and last names by last names? Would using splitOn " " and sortBy (compare `on` snd) (this is assuming I'm getting the syntax correct) be the way to go?
&gt; **nikita-volkov** commented 5 hours ago &gt; &gt; You cannot have a Monad instance, which executes things concurrently, because due to the nature of `Monad` a further action depends on the result of a preceding one. Here's a simple example: &gt; &gt; do &gt; decision &lt;- decide &gt; doSomethingBasedOnADecision decision &gt; &gt; Executing `doSomethingBasedOnADecision` concurrently with `decide` simply doesn't make sense. How valid is this argument, really? To what extent is it based on assumptions specific to this library, vs. the nature of `Monad` itself? As I understand it it, the `Monad` laws ~~require~~ allow the *effects* of `doSomethingBasedOnADecision` to depend on the *result* of `decide`, but allow for the *effects* of both actions to be ordered either way (see e.g. the reverse state monad), or even to interleave if this is meaningful in the model. The requirement is fundamentally that effects be combined in an associative fashion. So if the result of `decide` could be known sooner that all of its effects happen, couldn't `doSomethingBasedOnADecision`'s start happening from that point. And now that I say this, I see this comment which is kind of the converse of my remark: &gt; **rwbarton** commented 2 hours ago &gt; &gt; Why does it simply not make sense? `doSomethingBasedOnADecision` might do some things before needing to consult the value of `decision`, and those can be executed concurrently with `decide`. In the extreme case, such as when implementing `ap`, the `f` in `m &gt;&gt;= f` could perform all of its `IO` effects without ever forcing the result produced by `m`.
`splitOn` works on "Text" not on "String", so you would need to first `pack` your string, then `splitOn`, and finally `unpack` it. For example: {-# LANGUAGE OverloadedStrings #-} import qualified Data.Text as T splitOnSpace :: String -&gt; [String] splitOnSpace = map T.unpack . T.splitOn " " . T.pack Alternatively, you can use `words`: &gt; words "Hello World" ["Hello", "World"]
Excellent article! The part that was the high point for me was this: &gt; The bugs I do encounter are generally more meaningful and lead me to understanding the problem more. I think this statement is a better summary than the popular "If it compiles it works!" statement that gets thrown around so much. (Yes, "If it compiles it works!" is jocular, but other folks can't tell.)
Acid State is no doubt the most fun DB to use with Haskell.. It kinda bugs me that its all in memory though. 
Is GHCi not as good as Python's REPL? It's been a long time since I used the Python one. 
What about having something like toList' which would always return lists in the same order?
To be fair it depends on how many entities you want to put on the screen. A simpler engine could be done using Haskell. However I would agree that the common sentiment to gate your Haskell code to not handle the actual graphics work is the best.
I've tried words, but it takes the list and doesn't keep each of the names grouped together. If the list of names is: first1 last1 first2 last2 What I end up getting is: first1 last1 first2 last2 I can't seem to figure out how to keep them grouped together since there is nothing to compare the two names other than the fact that they're next to each other.
It is an easy fix, but it also means removing the `Foldable HashSet` instance. `foldMap` doesn't have an `Ord` constaint, and allows you to create `toList`.
toAscList and toDescList already exist, I guess that's why the discussion is about whether to eliminate toList, or mark it unsafe. I suppose that whenever you consider using toList you should ask why whatever you want to do with that list can't be done with a Set.
I consider it an unavoidable wart (and therefore excusable) rather than a bug. Implementing data types in such a way that the implementations are truly isomorphic to their intended denotations is an ideal that many Haskell programmers shoot for, but can't always be achieved. One of the difficulties here is that representing commutative data in Haskell is very tricky. Suppose I want to implement an *unordered pair* in Haskell: data Unordered a = Unordered a a instance Eq a =&gt; Eq (Unordered a) where Unordered a b == Unordered c d = (a == c &amp;&amp; b == d) || (a == d &amp;&amp; b == c) Problem #1: If my module exports the `Unordered` data constructor, clients can observe the order of its components. Problem #2: Even if I hide the data constructor, I will likely still export some functions that allow clients to tell `Unordered 1 2` apart from `Unordered 2 1`. For example, either of these will expose the order: toPair :: Unordered a -&gt; (a, a) toPair (Unordered a a') = (a, a') runUnordered :: Unordered a -&gt; a -&gt; a -&gt; r runUnordered (Unordered a a') f = f a a' This is a toy example, but I suspect that a big number of the violations that you have in mind boil down to some elaborate variant of this example. [Gelisam's blog entry on the `Commutative` monad is well worth reading.](http://gelisam.blogspot.com/2013/07/the-commutative-monad.html)
This would require an additional type constraint (presumably Ord) and might drastically increase the time complexity compared to an arbitrarily-ordered toList.
Oh, that *is* a shame! But perhaps, like `Set` not being a `Monad` instance (for basically analogous reasons), this pain can be swallowed, and more advanced techniques like indexed foldables or the mono-traversable solution rolled out when the pain is too great.
Aha! I, too, did not know about this feature. Seems nice, thanks to both you and /u/deech for pointing it out.
There are a few cassandra libraries. cql-io seemed more up to date than cassandra-cql. cassandra drivers are tricky though since they need to do more complex things than traditional drivers, which all deal with the cluster rather than a single database instance, and it's not helped that cassandra is somewhat a moving target as well. I think it's awesome that folks are putting time into libraries like cql-io and hope it offers real distributed database access for the haskell crowd. 
If your input data looks like: last2 first1 last1 first2 Then, you can intially `readFile` it into `"last2 first1\nlast1 first2"`. Then, you can use `lines` to get `["last2 first1", "last1 first2"]`. Then, you can use `map words` to get `[["last2", "first1"],["last1","first2"]]`. Now, you can `sortBy (comparing head)` to get `[["last1", "first2"],["last2","first1"]]`. Finally `mapM_ (putStrLn . unwords)` to print out in (roughly) the same format you read. `first1` ends up being associated with `last2` because they are in the same "word list", and *that list* is an element of the "lines list" that actually get sorted.
&gt; I was trying to append a single element to an existing list. Then you want `:` not `++`, `&lt;&gt;`, or `mappend`. The first is of type `a -&gt; [a] -&gt; [a]`. The later three are all (generalizations of) `[a] -&gt; [a] -&gt; [a]`. EDIT: I'm an idiot, ignore me.
I'm a python programmer by day and a Haskell amateur by night. In Python I use Ipython, which is the most amazing REPL I've ever encountered: tab completion, source inspection, on-the-fly temporary file editing, rerunning and editing commands, and the list goes on and on. Ipython is an absolute must at this point for me to write Python. I find that the biggest thing I struggle with in Haskell is that I regularly and utterly fail to accomplish much exploratory programming in GHCi and I often wonder if I'm just trained to use the REPL too much and maybe that's not how other people do it?
&gt; How valid is this argument, really? It's not. The reverse-state monad has "effects" traveling the other way. The tardis monad has effects traveling in both directions. The reader monad doesn't have any effects. Statements in a monadic do block have an order in the same way that a string of composed functions have an order. However, `EitherT e` (with no constraints on `e`) basically only has the "throw" interpretation, which many programmers expect to enforce a top-to-bottom "effect" ordering, that conflicts with `Concurrently`'s "effect" ordering of "all at once" / non-deterministic order. Neither the `Monad` instance of `Concurrently` nor the `MonadTrans` instance of `EitherT e` is *wrong*, but the latter is incompatible with the former. Over on the either issue tracker, glguy mentions a transformer that *is* compatible with `Concurrently` and is also implemented *in terms of* `Either`: the validation transformer (not sure if it's already in a package) where `Left` values trump `Right` values and multiple `Left` values are folded together via a `SemiGroup`. EDIT: Fixed credits; thanks /u/rpglover64.
The bind function has signature: (&gt;&gt;=) :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b The `id` function has signature: id :: a -&gt; a The only way that `id` can be the second operand of `&gt;&gt;=` is if it specialized to the inner monad: id :: Monad m =&gt; m b -&gt; m b So, it's `id` that drops the outer state, not bind.
But is there a way to associate documentation to a particular instance? Sometimes just the fact "ThisDataType is an instance of ThatTypeClass" isn't enough to deduce *how* it is so.
Well, the outer state is not dropped or lost: the final outer state is the initial inner state. Similarly for nested IO, join does not drop the RealWorld of the outer IO: it threads it into the inner IO.
You should check out [IHaskell](https://github.com/gibiansky/IHaskell)! 
Interesting, thanks for the details.
Sorry for the lack of context in my original question :) The main high-level library is in Perl, but the core marpa algorithm is actually written in C. To my knowledge there isn't a Haskell wrapper of it.
I will. Thanks for the recommendation!
I think I echo the thoughts of anyone who's met Prof. Hudak when I say that we are losing a truly great human being. Every interaction I was lucky enough to have with Prof. Hudak left me better off than I was before. Prof. Hudak is everything academia should be. He will be missed but not forgotten. My heart goes out to those close to him and I hope they can find peace.
I'm very interested in seeing what the client code looks like with a good rendering library. I conceptually am a big fan of this approach.
You can get rid of all the `Text.pack ","` with `OverloadedStrings`, which is really handy. On a related note, `Text.pack ""` is `mempty`, as you use `&lt;&gt;` in the following line. You use `fromJust` sometimes, and `case` with `error` other times to extract `Maybe` that must be `Just`. When doing something like that : case x of Just x -&gt; Just (foo x) Nothing -&gt; Nothing you are actually duplicating `fmap`. So you could have : parentColor = fmap colorOfFocus . parent In `parent`, `Just focus &gt;&gt;= up` is "just" `up focus`, so `parent` is the same function as `up`, and `grandParent` could be `up &gt;=&gt; up` (from Control.Monad). Instead of writing functions such as `rbGetColor`, you can just name the fields of your record. This is just linting, it's still too early to comment the general code ;)
I prefer ghci to the python repl, mostly because you don't have to continually qualify with the module name. :t is nice too. My workflow is simple, :L is bound to "load the module open in vim". ",t" in vim switches between the test and tested modules. Then I write the function and the test, and rerun the test with :r, test_whatever. If I want a persistent value, I just assign it to something in the editor, t0 = whatever. Then I can just indent one level and it's in the test function. Python wants me to put the test into a Unittest subclass, and run it via some special test framework thingy, and even then I have to type "import" the first time and "reload" after that and it won't properly reload any other modules if you modified one, and it's just all around more awkward than ghci.
&gt; using the sequence function was the first time Monads clicked for me Careful! `sequence` works on mere Applicatives :)
&gt; `sequence`, `mapM` These aren't `Monad` operations. They're `Applicative` operations in disguise.
The fact that our type system can carry round a custom notion of equality that _differs_ from the definition that you'd get if you derived `Eq` is one of the reasons Haskell is a powerful programming language. It means we can represent lists by trees and get O(1) append. It means we can represent sets. This is just about the existence of an abstraction-breaking `toList` and how you respond to that. Don't throw out a fundamental benefit of typeclasses; abstraction.
Referential transparency is about =, not `==`. `toList` exposes facts about the representation, and thus breaks the abstraction.
I (intentionally) didn't say it would break referential transparency, although I wouldn't be shocked to discover a clever case where it does. Since equational reasoning takes place on paper, it's probably good enough to have `toList` warn you in the documentation that it breaks abstraction; but I do think that warning is crucial. As an example, suppose someone naive and foolish is trying to make their favorite Set type an instance of [Hashable](http://hackage.haskell.org/package/hashable-1.2.3.2/docs/Data-Hashable.html#g:2). They see that there is already an instance `Hashable a =&gt; Hashable [a]`, and so they write hashWithSalt salt set = hashWithSalt salt (toList set) This will end badly, and lacking a **big honking warning** in the docs, the unsuspecting beginner will have a lot of trouble figuring out where the (intermittent, possibly hard to trace) bugs are coming from.
`vs &lt;&gt; [v]`
My up vote is, of course, for Paul and not the contents of the post. He was a good friend. 
His contributions to the knowledge of mankind remain with us. https://www.haskell.org/tutorial/
I’m pretty sure the `Category` instance for `-&gt;` uses `Prelude.(.)`, and if not, it could. And even though the functions are not the “same” for you, they’re isomorphic. To me, “simple and clear as possible” implies beauty and simplicity. A `Category` is elegant and simple. Furthermore, even though you only use it with `-&gt;`, you have the choice to use it with something else. I use `netwire` a lot by these times, and I found uglier (and more complex) to `import Prelude hiding ( (.), id )` in almost every modules I write. About errors reporting, see this [little snippet I just wrote](http://lpaste.net/7281721333556183040). As you can see, using the `Category` or the `Prelude` makes GHC behave the same way. It infers the types. Then, tell me how that’s “far more difficult for a human reader” to read that.
This is great! Haskell with pluggable dependent types? Yes please.
&gt; parentColor = fmap colorOfFocus . parent Or you could use applicative style: parentColor = colorOfFocus &lt;$&gt; parent The similarity with `$` is very intentional btw.
&gt; Sorry if that was a bit terse, but I'm typing all this out on my phone Every time you end another huge comment (with source-code) with these words someone's cup falls out of a hand.
That presentation seems like something I'd enjoy. Gotta remember to watch that some time this weekend!
This isn't bad, actually. It highlights the tree-style nature of combined IO operations.
That looks amazing. 
I initially wanted to disagree with you, but on reflection, I came around to the same viewpoint.
Unless you enable `-XOverloadedLists`, there's no difference of `vs ++ [v]` relative to `vs &lt;&gt; [v]` whatsoever...
And makes the code slower to compile, as GHC has to walk through all the instances finding the concrete types again.
I though the name Applicative came from ap, when people realize `ap` could be implemented by things *weaker* than Monad. Therefore `&lt;*&gt; == ap` is the definition of `&lt;*&gt;` (and it's call the *ap* operator. Also since the BBP making Applicative a superclass of Monad, I think it make sense to have `&lt;*&gt; == ap`. You might technically have them different, but I don't think it's a good idea.
If all typeclasses are "-able", then it becomes a kind-of Hungarian notation. Not that there is anything wrong with that.
I wholeheartedly agree with this post, it closely resembles my own experience with Haskell. Note the refactoring being a breeze part, it could be emphasized even more.
That's true. I guess the "-able" suffix necessarily comes from the fact that type classes define functions that apply to instances. I wonder if the semantics of code would be better reflected if we make the following changes: class Good where ===&gt; property Good: ... ... instance Good X where ===&gt; X is Good: ... ...
It seems to me that one enormously important difference for this library is that it reports all possible parses. The *parsec libraries, polyparse, uu-parsinglib, and Happy will only give you a single successful parse. When your grammar is good, that's typically what you want. Problems arise when your grammar actually leads to more than one successful parse. Since Happy does global grammar analysis it will warn you. The *parsec and polyparse libraries will simply descend down a particular branch of the grammar, so you don't even get a warning that the grammar is ambiguous. uu-parsinglib will simply bomb out with an error message (though unlike the *parsecs it is much more flexible when it comes to grammars with partial overlap.) This Earley library will give you all the parses, which at least will help me debug the parser if I don't want ambiguity, or allow me to use the ambiguous results if that's what I wanted. I like the simple API of this library, which is a lot easier to take in than the kitchen sink approaches of the *parsecs and of uu-parsinglib. What I would really like is a parser combinator library that could do global grammar analysis and warn me of ambiguities. Or, perhaps what I really need is just a grammar analysis tool that could examine the grammar and spot ambiguities. I haven't got the expertise to write either of these things though. This looks like a great new parsing library and I'm eager to experiment with it a bit.
By the way, I like your website design -- minimalistic + elegant.
:-( I have a book of his. It really helped me on my Haskell journey.
Right, but then there should be no function with a set domain that can distinguish "equal" sets
The (not so) gentle introduction to Haskell was how I learned.
Ugh, these examples are stressing me out. I *really* like Python when I do some NumPy stuff, but then something like that hits and it's like stepping in something while walking through some familiar streets. "Just, ew, really? C'mon guys." At least that's better than the lambda restriction which gets defended in the name of syntax.
Oh, it gets better. Much, much better. *AWESOMELY* better. :D
Yeah, I've heard IPython deals with reloads much better than the built-in REPL.
What's so bad about the last example? Sure, the lists are mutable, but they don't get mutated. You could sprinkle `tuple()` everywhere and they become mostly immutable. Yes, that's part of how I survive in Python. Know your `tuple`s, `frozenset`s and `namedtuple`s. They are decent read-only versions of `list`, `set` and `dict`.
My usual way to use HLint is to treat it like a drunken undergraduate doing a code review for me. I look at everything it raises, address the 1-2 salient points, and squelch the rest either by burying it in #ifndef HLINT blocks, adding an explicit HLint.hs, or by adding module annotations just to get it to shut up. If the code uses too many language features that HLint doesn't support and the costs start to exceed the benefits then I shut off HLint entirely. This here is an example of HLint reaching for the Ballmer peak. ;)
&gt; mming in GHCi and I often wonder if I'm just tr I've found setting `-fdefer-type-errors` in my `.ghci` file really helpful, and gives a more python like workflow.
Yup. I agree with all that. (Except I think you'll need `unsafePerformIO` or `Typeable` or foreign imports to break referential transparency. Vanilla pure code can't do it.) Haskell knows they're different, even if `(==)` intentionally doesn't. The problem with `toList` is that it leaks implementation details that ought to be kept quiet in principal and avoided in practice. **Big honking warnings** would make a lot of sense, certainly. In the abstraction, a deterministic function of type `HashSet a -&gt; [a]` does not exist.
By "GHC gets slower" do you mean compilation times increase or the resulting binaries are optimized less effectively?
Haskell is turing complete and there *are* examples of Haskell being used on embedded devices. Or at least to implement a DSL to compile into code suitable for use on embedded devices. Close enough, IMO.
I mean that the existence of `toList :: HashSet a -&gt; [a]` with no `Ord` constraint prevents _WhatThisCodeRepresents_ from being a functor from `Hask` to semantics. I mean that `toList` leaks implementation details that aren't part of the semantics of `HaskSet`. I agree it's dangerous from a program correctness point of view to allow `toList` to be exported/used, but it doesn't break equational reasoning (or referential transparency) in that we can use equational reasoning to expose the problem that we can have `x == y` without `toList x == toList y`. It stops our code from using equational reasoning to derive correct results, but it doesn't stop us.
Great use of equivalence class terminology. A function respects your abstraction if it gives rise to a well-defined function from the equivalence classes. &gt; you need a differently-spelled equivalence-testing function I think that something like that would be a big mistake. For the sake of argument, let's call the existing custom instance function `==` and the currently-not-provided, built-in representation equality `====`. The fact that `==` is programmer-definable, and is the _only_ equality game in town means that you can (a) use Haskell to represent things like sets and graphs and other objects and (b) trust libraries/programmers to not break your abstraction by peeking in the representation by using `====` instead of `==`. I agree that `toList` creates a problem in this particular case, but the existence of implementation-detail-revealing `====` would make that problem universal! `====` would break any abstraction where `==` was not defined to be `====`.
&gt; Cache oblivious algorithms admit some caching system exists, but it doesn't assume anything about it's behavior, so it strives for memory locality (e.g.) in general rather than optimizing for a specific micro-architecture. Ah, I think may have been the bit that I missed. I've been reading a few papers throughout the years, but somehow I didn't latch onto that particular aspect of it. (I mean the graphs all showed results for particular sizes of caches, &amp;c., so what was I to conclude?) I still think that calling them "cache aware" would be better pedagogy-wise, because they actually *adapt* (in some sense) to the caching structure of the system. :) EDIT: Thank you, btw. I always wondered about this, but your explanation makes sense, even though I don't necessarily agree with it :). I will try to work my way through the PDF eventually.
/u/mechanical-elephant, kraml wants to send you a Bitcoin tip for 4,319 bits ($1.00). Follow me to **[collect it](https://www.changetip.com/collect/739734).** -- [^^what ^^is ^^ChangeTip?](https://www.reddit.com/r/changetip/wiki/tipping-on-reddit)
I'm guessing you mean "succinct" instead of concise?
This is one of the most thoughtful, helpful and illuminating posts in this thread.
Not knowing the language isn't an argument for the language being bad.
Would you insist on only reflexivity, transitivity and symmetry, or would you also require extensionality at the module level in the sense that for all exported functions `f`, `x==y` implies `f x == f y`?
I will be disappointed if that first line doesn't make it into the Haskell Weekly News quotes section
minor typo, under Overall Layout: should be "work with" not "worth with". also, the first code example shows "Key not found" in one of the declarations.
cool, never though of that. when you say "slower", is it noticeable? like (obviously it depends on context) 100us per method? also is "walk through" linear? I've found (I think) that template haskell adds a lot to my compile times. so if I have to use it in a project (like one using lens) I just deal with longer build times. of course, that's for a personal project. it might make a huge difference on a large codebase.
Good point that Haskell knows more than `==`. Are there any compiler optimizations that take advantage of typeclass contracts, e.g. by assuming that `==` is reflexive? 
thanks! I think I just totally forgot to put this in haha. I've added it now :) 
A surprising number of real projects can use acid-state. We had two client projects where it turned out to be the right choice for them. You have to consider the tradeoffs carefully of course.
GHC, not binaries compiled using GHC.
What I have done after LYAH was, I bought Real World Haskell. It was a tough book at first. And after a while I realised was trying to comprehend everything, on every page, right off the bat. So then I decided to read it in a different way: I decided to put myself into "gullible" mode, meaning that I just kept reading and decided to accept everything at face value. You could probably almost hear me think "uhm... okay!" when turning a page. :-) This helped a lot. The second time I read about concepts they were no longer new to me and I felt much more confident when making my own code.
I (leadstage.com) use sqlite pretty heavily, and ElasticSearch (if that counts).
&gt; We field about a question a month on reddit (and more on stackoverflow) that involes (/) not working for Int/Integer or mod not working on Double. I would be tempted to keep the distinction, plenty of code breaks because people assume integer division works the same as floating point division. &gt; Being explicit with lossless conversion doesn't buy any safety It depends on the lossless conversion. Int32 -&gt; Double isn't a good thing since it transforms the meaning of the number significantly. Also implicit conversions are tricky with automatic type deduction and can lead to bugs in complex code. However if we had explicit rings for numbers and a more lightweight way to lift lossless conversion that would be great. (Accidentally going from Int16 to Int32 is bad, but typing up the necessary conversion is painful).
Because `(a,i) = getchar 1` is *two* steps— first, it gets the tuple (the whole thing), then it destructures it. Because `a` and `i` are being created *at the same time*, and `b` depends on the value of `i`, `a` is evaluated, always, before `b`. 
Yeah, there definitely needs to be something more going on. Here's one way data Tup a b = Tup !a !b getchar :: Int -&gt; Tup Char Int
Yeah, unsafePreformIO breaks all of these rules apart. Really, unsafe *anything* breaks all of these rules to pieces, hence the unsafe. If getchar always returned the same char in the tuple, this also falls apart, but this doesn't seem to be the point of this introduction to how monads work. There are, admittedly, a few places where this doesn't work, but it's a tutorial about monads and it implies that getchar is deterministic on its argument. 
The point is to simulate `IO` by manual state token threading, so we assume that `MyOh a ~ Int -&gt; (a, Int)` and are allowed to use unsafe things inside of that type so long as we control their effect. You have to force the "evaluation thread" to match the "state token thread" in order to ensure safety this way. Otherwise, it's very easy to reorder effects. With getchar :: MyOh Char getchar i = (unsafePerformIO getChar, i) we can write a version of getting two chars where the effects run in the opposite order as the state tokens getTwo :: MyOh [Char] getTwo i0 = let (c1, i1) = getchar i0 (c2, i2) = getchar i1 in c2 `seq` [c2, c1] 
..or by assuming that `fmap f . fmap g` is `fmap (f.g)`. There's no reason that there couldn't be, as you can define your own rewrite rules, but I don't know whether there are any rewrite rules for `==`. A lot of that goes on in stream fusion (eliminating intermediate lists), much of which is done by rewrite rules that respect the semantics of the code.
An even better select is the one that gives you all possible zippers; e.g. zippers :: [a] -&gt; [([a], a, [a])] zippers = go [] where go b [] = [] go b (x:xs) = (b,x,xs) : go (x:b) xs It is "better" in the sense that it actually gives you more information about which element was chosen (which is useful in many situations) and has more sharing (roughly speaking two extra copies of the list, rather than as many copies as there are elements). The two functions in the article can be recovered as select xs = [(v, reverse b ++ e) | (b, v, e) &lt;- zippers xs] select' xs = [(v, b ++ e) | (b, v, e) &lt;- zippers xs] which nicely illustrates why the latter is more efficient than the former (though `zippers` is more efficient than either).
thanks!
Cool! Are you the author? I'd love to see some example queries and writes. 
 "better" &gt;&gt;= replicate 2
I was confused by what you meant at first. Then I saw that you created [another thread](http://www.reddit.com/r/haskell/comments/33s1kh/acidstate_using_ixset_to_keep_only_keys_in_memory/) that clarifies your question. (For what it's worth, I have never heard of doing this before.)
LOL, did I inadvertently write an analogy-based monad tutorial? Uh-oh..
What does NixOS have to do with Haskell?
A new subreddit has been settled up for this kind of questions. Try [/r/haskellquestions](/r/haskellquestions) next time.
If you are into "fancy" solutions HalVM might get you rid of the underlying OS. No idea if it compatible with EC2 though.
The difference in speed from running the send-more-money example compiled or not is indeed huge. It's ~24 seconds from source and ~0.2 seconds compiled on my machine.
Looks nice! I'd request: 1. optparse-applicative 2. generics-sop
I wasn't proposing a representational equality operator. I was proposing a function along the lines of... representsSameSet :: Eq a =&gt; [a] -&gt; [a] -&gt; Bool That's not some big deal - it's just another function. And I wasn't even proposing it as a library feature. After all, all you really need is... (f a) == (f b) where f = nub . sort My sense of "spelling" didn't even really mean you need a new name. Only that you don't the standard `==`. That said, by arguing against it, you've raised a good point - what's wrong with having a standard spelling for representational equality? Sure it's a leak in the abstraction but, as you say, only for abstractions that are leaking anyway. You say it's a larger leak but actually no - it's only leaking an aspect of the implementation that was leaking anyway. It's more like a standard way to detect the leak. It's even still an abstraction in itself. After all, does representational equality mean all the way down to bits and bytes in memory? No, it doesn't necessarily even mean all details of the data constructors etc. The goal is to know when some aspect of the implementation might be observable via the results of functions on that type, so we can define representational equality in those terms. [**EDIT** And even then, ignoring issues with bottom, which we always "play fast and loose" with.] Did you know I can easily buy a gadget to detect gas leaks? That if I have pipes that leak gas, this gadget will create much more visible leaks in that gas-pipe abstraction by indicating where my implementation of that gas-pipe abstraction is leaking that gas? The shop didn't claim that I should bury my head in the sand and pretend my gas pipes implement the gas-pipe abstraction perfectly, and neither did they assume (or even ask) if my intent was to go around finding gas leaks so I can set fires. They even seemed to consider these gas leak detectors to be safety equipment. Shocking, eh! Similarly, I was taught about scissors when I was a toddler!!! Not that they're evil and their existence intolerable - I was taught how to use them and, over a period, trusted more and more to use them without supervision. I could have killed myself!!! I could have killed other children!!! I could have killed animals!!! I could have killed my parents in their sleep!!! My God, **ANYTHING COULD HAVE HAPPENED**. Safety is a balance, not an absolute. Risk is something that you have to manage, not eliminate. The real question here isn't whether a standard spelling for a representational equality test is dangerous, it's whether there's a genuine need. I suspect there is precisely *because* referential transparency is a big deal and appears to be violated - I think it could be useful in GHCi to help diagnose related problems. Sure, you can ask for a particular alternative representation such as a list and compare that, but that's an unreliable check - some variations in the Set representation might only leak via some other computation, perhaps only indirectly. And you could provide a public read-only view of the representation, but that really is a much larger (and unnecessary) abstraction leak. 
Sorry if I misunderstood you. How does Haskell have the unsafe low-level one?
Came here to say the same. HalVM is pretty interesting, though not for every kind of application (at least not yet). I never used it, but I guess if it fits your problem you are deploying your Haskell apps as-clean-as-possible. &gt; No idea if it compatible with EC2 though. I thought it was, [quite some things show up on a simple ggl search](https://www.google.nl/search?q=halvm+ec2).
You're welcome. Nice project is Auto, I like the "interface agnostic" place it takes. Some for Virtual-DOM: I think that binding with React.js from Haskell (GHCJS) is too high-up in the stack. Some of the benefits React.js has for a JS programmer, have much better approaches in a language like Haskell. While having a Virtual-DOM seems indispensable to any serious JS app nowadays.
How does it "limit" interesting applicatives ? It seemt to only limit Applicatives which are also Monads. In that case, this implies that there are two applicative instances for the *interesting* type (a *dull* one compatible with the Monad laws, and a *interesting* one which is only applicative). In that case, the usual solution is `newtype`.
Perhaps we differ on what counts as a broken abstraction. As far as I'm concerned, I think a closed abstraction can have non-deterministic representation with no canonical choice within each equivalence class, as long as the abstraction doesn't leak in the sense that you can get differing results from two items in the same equivalence class. The existence of ==== would mean that the only closed abstractions would be ones where all functions that return an element of that type normalise the data, which in my view is unnecessary and precludes some solutions that have certain desirable performance characteristics. ==== lets you check for leaky abstractions in much the same way that putting a loaded handgun in every teacher's desk allows you to keep school children safe. Assignment, side-effects and pointer arithmetic are very handy. By disallowing them we can do things in Haskell that we can't do in C, like reason about our code or write sandboxed operations simply. I guess my point is that there's always a tradeoff with freedom. Freedom to inspect limits freedom to encapsulate. Freedom to build walls limits freedom to roam. 
If you consider mere representational equality to be an intollerable leak then I can only assume you think that providing address-based equality would be like having nuclear weapons in every teachers desk. The thing is [we already have that](https://downloads.haskell.org/~ghc/latest/docs/html/libraries/ghc-prim-0.4.0.0/GHC-Prim.html#g:21) and the sky hasn't fallen. &gt; Freedom to inspect limits freedom to encapsulate. A representational equality operator doesn't allow you to inspect anything more than you can already inspect - whether or not two supposedly-equal values have the same representation. Which you can already inspect via more indirect means. As my metaphor was meant to point out, the gas pipe may already have a hole in it, a representational equality test just makes it easier to check whether that's really the problem. 
This is known as the [subset sum problem](http://en.wikipedia.org/wiki/Subset_sum_problem) and is NP-complete.
If you're looking into Cloud Haskell on AWS, I've been working on https://github.com/IanConnolly/distributed-process-aws and https://github.com/IanConnolly/aws-service-api as part of my undergrad project. Not quite ready for prime-time yet (hence not up on Hackage), but happy to walk you through what I've learned if you'd like.
Could we stop whitewashing [an oppressive regime](http://www.theguardian.com/commentisfree/2015/apr/23/you-can-tell-a-lot-about-the-west-by-the-way-it-celebrates-autocrats-deaths) just because its PM declared he wanted to learn Haskell when he retires? I mean, I know that some people are super keen to celebrate [Singapore being a clean city with trains arriving on time](http://www.reddit.com/r/haskell/comments/2nkjdq/an_opportunity_in_singapore/) on this sub but... come on...
diagrams
Always weird to see people using my own un-released libraries :)
Just curious, have you actually lived in Singapore?
Cloud Haskell is orthogonal to deployment strategy - you simply build an executable program using the Cloud Haskell libraries and then you deploy it using any of the many strategies and tools available for deploying programs to EC2 and managing VMs. You do have to decide how to get all the nodes talking to each other; if you have all your nodes in a single VPC then you can use the [simple local net back-end](https://hackage.haskell.org/package/distributed-process-simplelocalnet-0.2.2.0/docs/Control-Distributed-Process-Backend-SimpleLocalnet.html) for discovery.
NP-Complete simply means that longer lists of numbers will require more time to find an answer (and not just more time, but much more time - a list twice as long might take 10 times as long). Have a look a Hutton's Countdown problem. http://www.cs.nott.ac.uk/~gmh/countdown.pdf 
Well, he's boned
I only need it for N *small* (&lt;10) and I'm happy to a full night if needed ;-)
I think you have to @remember it on IRC to get it to the top of HWS.
Yeah, for small sets, there's a couple of algorithms on the Wikipedia page that will work in a reasonable amount of time.
&gt; fmap definition is incomplete OP only provides the type signature for `fmap`, not a definition. And his type signature IS correct because it occurs as a member of the `Functor` typeclass (so `f` is already restricted to being a `Functor`). Sorry if I didn't explain that clearly. You can see the difference by doing `:i fmap` in ghci and doing `:t fmap`.
Oh, I did not see the `class functor where` above that. Lol
In that case, there's less than 3^10 possibilities to try. That will just take a fraction of a second. This will do the trick: filter ((== 0) . sum) . mapM (\x -&gt; [-x, 0, x]) For example, Prelude&gt; filter ((== 0) . sum) . mapM (\x -&gt; [-x, 0, x]) $ [1130,2713,1398,1511,2726,1897,3561,1454,1433,4100] [[-1130,2713,1398,1511,2726,1897,-3561,-1454,0,-4100],[0,0,0,0,0,0,0,0,0,0],[1130,-2713,-1398,-1511,-2726,-1897,3561,1454,0,4100]] I was a bit surprised that I needed to enter 4-digit numbers to get it down to 1 (unique, non-trivial) solution :)
The "naive" algorithm (O(2^n * n) as the wiki states) is pretty much the specification of the problem: -- Assume [a] to be a set of a. This is a simplification, but all right for our purposes. type Set a = [a] -- "all subsets of a set" means that every element of a set is non-deterministically included and non included (hence filterM with two values). powerset :: Set a-&gt; [Set a] powerset = filterM (const [False, True]) -- You'll have to define a Monoid instance for Int with mempty = 0 and mappend = (+). -- Then 'mconcat' will be 'sum'. sumsToZero :: (Eq a, Monoid a) =&gt; [a] -&gt; Bool sumsToZero = (mempty==) . mconcat -- generate all subsets, then filter for those whose sum is 0. zeroSubsets :: Set a -&gt; [Set a] zeroSubsets = filter sumsToZero . powerset However, in your example, you multiplied the 10 by (-1) in your solution. Do you just need to select a subset {s_i1,...,s_im} from a set {s_1,...,s_n} s.t. s_i1+...+s_im = 0, or do also have to select integer coefficients c_i1,...,c_im s.t. (c_i1)*(s_i1)+...+(c_im)*(s_im)? If it's the former, you can just sum up the numbers, but if it's the latter, you'll have to solve a linear equation that will, in general, have a vector of solutions. In a real-world setting, I'd use a dynamic programming or CSP package, btw.
VCache seems interesting. Have you run into any problems with it? It's listed as experimental on Hackage..
Off the top of my head: * `DefaultSignatures` are where it usually dies first in my code. * The parser has some bugs in its `MagicHas`h implementation. It assumes _everything_ that starts with `(#` is the start of an unboxed tuple, this means definitions of `(#)` or prefixed uses of that combinator need to be spaced out `( # )` to trick the parser. It doesn't backtrack to catch the other case or try out the section parse first. * I'm pretty religious about ensuring I have proper type role annotations. Those don't parse at last check. * Whenever I use a rank-n type on a function and I get the eta-reduce tip I roll my eyes. It might be worth looking at the type associated with the function and hunting for a rank-2 type on the top level function, or even just explicit foralls there and disabling that warning. * `ConstraintKinds` don't work, things like this rejected as p being an invalid context. data Dict p where Dict :: p =&gt; Dict p * These last two are what got it shut off in `hask` and my `constraints` package, IIRC. * Bang annotations in GADT signatures trip it up: #ifndef HLINT data Coil t i a where Coil :: Coil Top Int a Snoc :: Ord i =&gt; !(Coil h j s) -&gt; AnIndexedTraversal' i s a -&gt; Int -&gt; !Int -&gt; !(Path j s) -&gt; j -&gt; (Jacket i a -&gt; s) -&gt; Coil (Zipper h j s) i a #endif * Exporting infix type constructors. profunctors/src/Data/Profunctor.hs:1160:#ifndef HLINT profunctors/src/Data/Profunctor.hs-1174- , (:-&gt;) profunctors/src/Data/Profunctor.hs-1184-#endif * Pattern synonyms. * There are some corner cases in the export lists used by GHC that aren't supported. `pattern` exports and explicit exports with the `type` keyword, which are sometimes needed. * I've had a number of cases where one of the unused language pragma checks was wrong, but I'm not coming up with a compelling example at the moment. There are probably a half dozen or so other cases that bite me regularly, but they escape me at the moment.
&gt; I'd like to use it for web developpement, little interpreter/compilater etc. Haskell is definitely capable. But trust me on this, you don't want Haskell.
I prefer less repetitive content here. Questions tend to repeat as different users cross the same inflection points.
Why not? 
IIRC, It is more about how you deal with "nonsense" / abstraction; the better you are about following arbitrary rules without connecting them to a concrete meaning, the easier you'll find both type theory and other things Haskell borrows from maths. I'm not sure that's "discipline", but maybe so.
A police state isn't defined by the arbitrary exercise of power by the police - in that case, a country like Uzbekistan would be a police state too -, but by the ubiquity of the police and the wide-ranging regulation of citizen's lives. If the police in a country were to behave lawfully at all times, but you had cops with military equipment at every corner and people being whisked away in the depth of night on charges of (duly outlawed) sedition, that would still be a police state. Of course, what people do and do not find tolerable depends on their temperament. I'm not saying this pejoratively, but Westerners have stronger traditions of liberty and greater distrust of government power than the people in what, to me, seem to be societies influenced by Confucianism. While the more sanguine types will advocate the torture of criminals in a moment of passion, Europeans and Americans are generally horrified at the notion of violating people's bodily integrity by methods such as caning. It's one thing to put someone in prison, but in our moral universe, it's very much different to shred a criminal's flesh, or to cut off his limbs, or to draw and quarter him (all of which we've done in the past). I don't know too much about the Singaporean mind, but to my understanding, it seems to regard the forceful punishment of criminals as necessary to the stability of society. The Western mind, on the other hand, immediately thinks: "next they'll be coming for me!" The same thing was on my mind when I read the job posting, by the way: "What if they catch me chewing gum? What if someone plants drugs on me? What if they single me out because I'm a foreigner? What if I'm fired and they confiscate my passport? What if, what if, what if?" For me, the issue isn't freedom of expression, but physical safety in the domain of a government that seems a bit to comfortable with violence.
Perhaps this example gives you an intuition (assuming you know the list monad). mapM (\x -&gt; [-x, 0, x]) [2, 3, 5, 10] is equivalent to do a &lt;- [-2, 0, 2] b &lt;- [-3, 0, 3] c &lt;- [-5, 0, 5] d &lt;- [-10, 0, 10] return [a, b, c, d] For each number in our list, it will try all of [negate it, remove it, keep it], and then it returns the list containing all possibilities!
It's sorta works. The devs of HalVM are happy to take patch to get it running properly on EC2.
First, make sure you understand the [] and StateT monad instances. I'll write them below, but before peeking, write them out yourself. instance Monad [] where return = \x -&gt; [x] [] &gt;&gt;= _ = [] (x:xs) &gt;&gt;= f = f x ++ (xs &gt;&gt;= f) instance Monad m =&gt; Monad (StateT s m) where return = \x -&gt; StateT (\s -&gt; return (s, x)) ma &gt;&gt;= mf = StateT $ \s -&gt; do (s', a) &lt;- runStateT ma s runStateT (mf a) s' 
Sure it is, but you'd have to manually pass around the (_selected element_, _remaining elements_) tuples yourself, rather than relying on StateT's `&gt;&gt;=`
Trying to avoid analogies is like putting too much air in a balloon.
The Monad instance for List is magic the first few times you see it!
I see. Thanks
Categorically, a functor is a mapping that acts on both the objects (sets/types) and morphisms (functions) in a category. The type constructor `f` in Haskell morally plays the role of the mapping that sends types to types. This can be seen in its kind: `* -&gt; *`. Then `fmap` is the mapping that acts on functions.
&gt; Every author of a library motivated to properly document and explain their code needs to write both blog articles and haddock explaining the same thing. I very, very, very much agree. (Disclaimer: I'm the author of the lens post on your list.)
It's new, but the API is solid. Main problem is the need to reinvent collections types to use VRefs. So far there is only vcache-trie. 
Why don't mods delete duplicates? It seems that our mods are as lazy as our language.^sorry^bad^joke
Thanks, fixed.
Thanks for your answer! I'm already a programmer, I use python since 3 years, I developped a few websites with Flask (a python microframework), I learned Java at school, but I still like a beginner since I had never do big project and I dont stick on any language. Thats why I'd like to learn Haskell as I found it very elegant and powerfull, but I dont want to learn it and then realise that I cant do anything with it. 
Is it really above to the code sample above? When I run that code, it checks all possible combinations rather than what you posted. 
But surely that checks something like whether one has been copied from the other, not whether they're the same.
Let's go all the way to Godwin's Law because you're obviously trying to derail the conversation here: I have never lived in nazi Germany. Does that make me unable to condemn the oppressive regime nazi Germany was? Can we stop talking about me and focus on the matter at hand?
Now that I thought about that, there is a way to get context-sensitivity. I'm going to show how to write a partial isomorphism which unpacks compound s-expression and checks whether the first element of a list contains length of a tail of this list. In an opposite direction, it would take a list of s-expressions and pack it to compound one, prepending an atom which would contain length of source list. Couple of helper definitions (these assume all definitions from the post are available): int :: Iso (String, r) (Int, r) int = printable 0 &gt;&gt;&gt; lift (flip empty) checkSatisfy :: (a -&gt; Bool) -&gt; Iso (a, r) (a, r) checkSatisfy p = Iso id' id' where id' (x, r) | p x = Just (x, r) | otherwise = Nothing We already have a way to sequence partial isomorphisms, and thus we want to somehow "lift" a function of type `a -&gt; Iso b c` to some isomorphism. In the forward direction, we obviously would be able to produce `c` out of `(a, b)`, but in backward direction we would need a way to extract `a` out of `c`. There isn't an obvious way how to do that, but we can just require user to pass that extraction function in: bindLike :: (a -&gt; Iso b c) -&gt; (c -&gt; a) -&gt; Iso (a, b) c bindLike f back = Iso input output where input (a, b) = runForward (f a) b output c = let a = back c in (,) a &lt;$&gt; runBackward (f a) c Now we can use `bindLike` to define a partial isomorphism desribed above: listOfLength :: Iso (Sexp, r) ([Sexp], r) listOfLength = flip list &gt;&gt;&gt; flip cons &gt;&gt;&gt; flip atom &gt;&gt;&gt; int &gt;&gt;&gt; bindLike (\len -&gt; checkSatisfy $ (== len) . length) (length . fst) And a couple of tests to make sure it works: test4 :: Sexp test4 = List [Atom "3", Atom "add", Atom "3", List [Atom "mul", Atom "3", Atom "4"]] test5 :: Sexp test5 = List [Atom "2", Atom "add", Atom "3", List [Atom "mul", Atom "3", Atom "4"]] λ&gt; runForward listOfLength (test4, ()) Just ([Atom "add",Atom "3",List [Atom "mul",Atom "3",Atom "4"]],()) λ&gt; runForward listOfLength (test5, ()) Nothing λ&gt; runBackward listOfLength ([Atom "1", Atom "test"], ()) Just (List [Atom "2",Atom "1",Atom "test"],()) 
The phrasing "oppressive regime" is what I have a beef with. It's like every time someone gets hit on the street, to the media it's a "brutal beating". Also, how are we whitewashing anything here?
It's easy to have an opinion. 
I actually modified a script I found on [someone else's blog](https://github.com/blaenk/blaenk.github.io/blob/source/src/deploy.sh), so there are at least three of us!
And some feedback: - The popup window (when your module doesn't compile) doesn't automatically close. Not sure if this is intentional, because you probably do want the window to stay open when you're working with hole information. - Some type errors don't seem to elicit an error window, for example, `foo = True :: Int` (but `foo = a :: Int` shows `Not in scope: 'a'`. - GHC extensions aren't respected and throw compiler errors. Overall the typechecking aspect is a vastly inferior experience to, say, `syntastic` + `hdevtools`. It's unfortunate that both plugins have to essentially duplicate each others' work (or do they?). - With: - foo :: Int -&gt; Bool foo n = (&lt; 5) _ I see Goal: _ :: a_a366 Aside from the fact that the type variable is oddly named (why not `a1`?), shouldn't it just be `Int`? Does the type signature of the function not affect type inference? Anyways, I would LOVE to use this plugin as part of my workflow. Great work so far!
If you called `(.)` a map, that's probably because it *is* one: it's `fmap` for `((-&gt;) e)`.
I'm really uncomfortable with the `Maybe` decorators in something called `Iso`.
hmatrix, cloudhaskell, concurrent, FFI and how to generate prime numbers.
Thanks so much for the feedback. &gt; The popup window (when your module doesn't compile) doesn't automatically close. I'm not exactly sure what you mean, since typically you want the window to stay there (with error messages) when your module doesn't compile. Are you referring to the fact that the window with error message remains after your module successfully compiles? This is definitely a problem and I'll fix it at some point soon. &gt; GHC extensions aren't respected and throw compiler errors. Overall the typechecking aspect is a vastly inferior experience to, say, syntastic + hdevtools. It's unfortunate that both plugins have to essentially duplicate each others' work (or do they?). I'm aware of this issue. It's actually only certain extensions I think. I'm honestly not sure what the cause is but fixing it is high on my to-do list. &gt; Aside from the fact that the type variable is oddly named (why not a1?), shouldn't it just be Int? Does the type signature of the function not affect type inference? Well, `(&lt; 5)` has type `(Ord a, Num a) =&gt; a -&gt; Bool` and there's no reason that `a` should unify with `Int` in this example.
For the last one, there is no reference to a value of type Int anywhere in the right hand side. It could be independent of n, for all we know, so the hole still has type `(Num a, Ord a) =&gt; a`. Start using n in the expression (or e.g. `(3 :: Int)`) and I'm sure the plugin would propose a monotype.
No problem. &gt; there's no reason that a should unify with Int in this example. What about the fact that foo was given an explicit type signature? Not trying to be snarky, I've just never used the GHC API and have no idea how it works.
Right. Duh. Thanks, haha
Just merged a change that fixes the bug with extensions throwing errors. Let me know if it works for you.
To be honest I am not sure whether putting your solutions and thought process online actually makes it easier for others to learn, or whether it just makes it harder (https://byorgey.wordpress.com/2009/01/12/abstraction-intuition-and-the-monad-tutorial-fallacy/). I used to ask people not to post their solutions publically, but at this point I think that ship has sailed. =)
&gt; crystallized meaning What, other than reciting the definition or using an example (nothing concrete, now) does it *mean* to be a monad? A category? A proof? I consider myself fairly passionate about mathematics, but some of our rules are pretty arbitrary, and it is somewhat coincidental that they apply to anything concrete at all.
Yeah one of the disadvantages would be that, depending on how sophisticated it is, you will get false negatives. I'm not sure that you can fundamentally disentangle this, if you want to define two distinguishable things as equal, you are going to be able to write functions that violate equals for equals. So either you need a notion of equality that is very strict in what it allows as being equal (such as only declaring two things equal if they point to exactly the same memory location) or an API that does not allow you to distinguish values different represented when they're semantically the same. Or you just give up on the equals for equals property altogether.
Incidentally, Haskell Curry seems to have been more of a formalist: "Curry was the most formalistic person I've ever met; he didn't really think in terms of models." https://www.youtube.com/watch?v=7cPtCpyBPNI&amp;t=1185
How does this call GHC? Does it keep a ghci slave process running? Does it/can it use ``cabal exec``?
That's a nice blog post, and you're right. Showing one's own "magical" insights won't help people learn things. I'm also reconsidering posting all those readme's. I worked like at least 2 hours to write the thoughtprocess of the "validating creditcard numbers" and I feel it's just too much work to do everything.
As someone who's been burned on that same thing, I agree with this point. Cabal is terrible crap, undeniably and unequivocally. It's a sad state of affairs. Another painful wart of Cabal is that with default settings, it does not install the profiling versions of anything. And if you've installed 50 packages and then one day decided to profile some code that depends on 40 of them, then - hooray for Cabal! - you have to reinstall all those 40 dependencies with profiling enabled. Talk about an insane default.
Hello! Would you like me to include this in the NLP Haskell Communities and Activities Report section? We have a small [mailing list](http://projects.haskell.org/cgi-bin/mailman/listinfo/nlp) for what it's worth. I had a naive attempt at a [generic chart parser library](http://code.haskell.org/nlp/chartparser/) a few years back, but I never really got around to fleshing it out. Was always hoping somebody would just step in and turn it into something useful.
That's a trade-off between doubling install times for everyone against the significant extra pain when you do actually need profiling. I also find it annoying occasionally but overall I'm happy with the trade-off.
This seems rather silly. Sure, it's a significant annoyance, but there's now an easy workaround of using sandboxes which the article mentions and dismisses. So if using cabal causes a screaming rage, that seems entirely self-inflicted to me.
`ghc-pkg unregister` is often what you want - [wiki](https://wiki.haskell.org/Ghc-pkg) Edit: you = op. *I* use sandboxes and almost never need it.
Let's face it, most users of Haskell are developers, either as a hobby or professionally. The vast majority of developers need to profile their or others' code at some point or another. So if like 90% of users will need to pull in the profiling versions for most packages eventually, is that really a good trade-off? As an even better solution, wouldn't it be better to display an explicit notice in bold letters right after a cabal installation? "Hey, you know, if you're planning on ever profiling anything, you should change this line in this config file right now". Cause otherwise this news may hit the user in a totally different manner, in a "every package you've ever installed was wrong, you have to do it all again" manner at a very inappropriate moment.
`cabal sandbox hc-pkg unregister {name}` works well for me. You can also backup sandboxes before you mutate them. `cp -r .cabal-sandbox/ .cabal-sandbox-bkup`, but if you really want to solve the problem there is this thing called `nix-pkgs`. Maybe you're using too many unnecessary dependencies. The more deps. you use the higher probability of cabal hell (someone didn't abide by the pvp). You can use `cabal freeze` to save the working state of your dependencies too. Here's a helpful link, http://softwaresimply.blogspot.com/2014/07/haskell-best-practices-for-avoiding.html.
I am pretty competent with haskell and I believe cabal fiasco a hindrance. Sure I can handle cabal hell but when I distribute my opensource project no one is going to waste the time going through that just to evaluate my project. Sandboxes really don't help in every case. What concerns me is **I feel** (*could be wrong*) that the current state of affairs is poor software engineering masquerading as brilliance and no one is winning in this fail.
Like all cabal woes, the answers are probably "cabal is not a package manager" and "try nix", the purely functional package manager. I haven't tried nix yet (sandboxes have reduced the pain enough that I don't feel the need to look for a better solution), but I can see in its documentation that [nix supports uninstalling packages](https://nixos.org/wiki/Nix_Installing_Packages#How_to_uninstall_software_packages.3F).
That's almost never what I want. Because I am very careful never to install anything in my global package databases except what came with the Haskell Platform. When things go wrong in my sandbox - which is not very often nowadays, with modern cabal - I most often solve it with: cabal sandbox delete cabal sandbox init Occasionally I try `cabal sandbox hc-pkg unregister`, and even less often that works. Rebuilding a sandbox is not that big a deal, so when something goes sour in a sandbox, it's not worth the time to start mucking around, just rebuild it.
s/Haskell/GHC
Annoyingly, I think it turned out that compiling Haskell with GHC for profiling can actually make some corner cases worse asymptotically. So there is overhead if you recompile everything to work with the profiler, and if you do, the profiler might actually give inaccurate information, compared to wrapping calls to code that does not have the profiling hooks.
[Celebrate good times come on!](http://www.youtube.com/watch?v=oQvzvu2TZsY)
I've never seen anyone promote cabal as "brilliant". The problems are known, but ~~aren't getting fixed (for whatever reasons)~~ Edit: sandbox may be a partial solution but it sure helped me. So the problems are getting fixed but not as fast as you would like.
&gt; The op is refusing to work with sandboxes The OP is even refusing to work with Haskell. We are trying to make it clear that there are reasonable workflows, and make them more well-known so that OP's negative experience won't happen to others. I wouldn't consider a non-sandbox workflow reasonable.
If you see the diamond dependency graph in the accepted answer, you will see that if B and C are only built when D (your project) is built, they have to be built against the same version of A, so this particular dependency issue doesn't exist.
In my humble opinion Stackage solves most, if not all, Cabal-induced headache. I almost forgot how bad the situation was without it, until I switched to 7.10 and found out there are no Stackage snapshots available yet ;)
1. You're still making "think of the children" arguments. Just where are all these 5-year-old Haskell developers? 2. Those GHC primitives are only "out of childrens reach" to the extent that children aren't programming Haskell anyway. `import GHC.Prim` doesn't have a lock and isn't hidden away somewhere. 3. The point about positional equality was that sharp tools exist, not that representational equality should be handled the same. What I was thinking of was a typeclass rather similar to Eq, but with a slightly different intent. After all... 4. There are certainly types for which representational equality makes no sense - functions, for example, don't have a referentially transparent representation/implementation at all. Optimizations depend on context, and the same function can be implemented differently for different calls due to inlining decisions. 5. At the same time, given that for many abstractions it's impractical or impossible to provide canonical representations, what's so terrible about providing a "representation of set" abstraction as opposed to a "set" abstraction? 6. The "failure of encapsulation" here is microscopic. Sure, you can see if two supposedly-equal values are represented differently - which you can see in more indirect ways anyway. But you can't see what the representation *is*. It's hardly encouraging abuse. 7. And if someone actually finds a way to abuse that, it's their problem and their responsibility when the underlying representation changes and representational equality yields different results. That's the thing about Haskell developers not actually being 5-year-olds - they're expected to take some responsibility for what they do. 8. Even when those Haskell developers are learning, even if they *were* toddlers, I think helping them learn to cross the road safely makes a lot more sense than pretending the road is perfectly safe when it isn't. And considering we don't even lock the "dangerous primitives" module... 
You can also take a look at IRC/Freenode: * Python ~ 1600 * Haskell ~ 1400 * Javascript ~ 1100 * C++ ~ 800 * Ruby ~ 800 * PHP ~ 600 * Java ~ 400 * C# ~ 400 
Ok, but at the end of the day, cabal installs packages and their dependencies, is that not what a package manager does ? Or is that because there is no uninstall option, cabal is only a 'package installer' and doesn't deserve the full title of manager ? Cabal might be a build tool, but how does use it for building ? When I need to build something (as long as all the dependencies are sorted ), I just need to do do `ghc something` (or maybe `ghc --make` (which I have never needed)) and it build everything needed. However, having the package installer/manager built in the build tool is what makes cabal great (I like cabal, even though there are lots of weird stuff about cabal, and it's probably #15 why I write Haskell). 
The whole nightmare of managing packages with haskell is also my biggest problem with the platform. The gold standard of package management for me is how ruby does it through bundler and rvm. Bundler lets you have a list of gems, and the can create a Gemfile.lock file which locks in the exact dependencies. Rvm then lets you specify the ruby version you are running against. This has the following benefits over cabal: * No sandboxes to manage. Just have a Gemfile in the project root listing dependencies. * Must faster install, since multiple projects can share the same library versions. For example, if you install rails-4.0.1 in one project, it won't need to be re-downloaded and installed for another project * You can easily upgrade, downgrade and remove packages. * You can also specify the ruby version you are running in a .ruby-version file. Once set, once you enter the directory your tools will change to use the new version. Compare that to switching between Haskell versions. * The Gemfile.lock allows you to recover to the exact same set of gems used previously. This is amazing when working with archived projects. Along with rvm specifying the ruby version, you can often start developing again straight away even after years of not being used. I doubt the same would be true with haskell, as not only can't you specify the ghc version, but the library changes may break your product as well if you didn't have a perfect set of version guards in the cabal file. [EDIT: `cabal freeze` covers half of this issue, allowing you to freeze library versions] I'm not sure why fixing this in Haskell is such a harder problem that it is for ruby, but the day cabal gets close to the functionality of bundler+rvm, the biggest problem with the Haskell ecosystem will disappear.
cabal has improved greatly in recent versions. I absolutely love sandboxes, cabal exec and cabal repl. Kudos to the developers. &gt; I also understand that cabal sandboxes are a thing. I don’t care about that either. Ok. 
Neither would I, in fact I somehow seem to manage without problems almost all the time (of course, I use sandboxes). Specifically for the issue raised by the op, which I admit is not my own problem almost always, unregister is a reasonable solution.
[Cabal: the simple guide](http://katychuang.com/cabal-guide/) has sandboxes at step 2. A good resource for newbies.
2015 = year of Haskell? :)
Sure, they may well not be obvious to everyone, but the author of the article explicitly mentions them, so he at least certainly knows about them.
Yay! Avoid success at all costs ... wait ...
I'm on nixos with nix as package manager. I don't care about cabal (though I'm just starting with Haskell) - Nix does everything I want!
this is great! I'd love to help but I don't know latex :P definitely looking forward for the Async one :D 
I was told there would be cake.
I would consider a package manager a tool which could install, upgrade and remove packages. Cabal can't do the last two, as far as I know. `ghc something` doesn't work well when you're inside a sandbox, does it? (when dependencies have been installed inside the sandbox) That's why you have `cabal build`, `cabal run` and `cabal repl` as substitutes. As I've mentioned in my previous comment, things like sandboxes don't work well with applications, as is. Other systems that have package managers can install applications just fine, however try to install xmonad/yi inside a sandbox and then update their configuration file (which is somewhere in your ~ folder and is a Haskell program). Unlikely to compile since they run ghc to rebuild themselves, which doesn't take in account the sandbox. The point is: cabal isn't a package manager; maybe we should stop pretending it is :)
You mean linkbait? 
Looks like a lot of stuff was fixed in the last 6 months in haskell-src-exts. The way I get hlint and haskell-src-exts now is to just cabal install on travis, and that is fresh every time, but still exhibits these issues, so I'll have to see what the script is doing that is causing us to not see these updates.
I've used profiling twice. It would be an absolutely massive waste of my time if profiling were enabled by default. I have a very hard time believing that most Haskell developers need to profile every single project they work on, which would be the only scenario that makes enabling profiling by default sensible.
&gt; at the end of the day, cabal installs packages and their dependencies, is that not what a package manager does ? Here is a famous blog post which explains in detail why cabal is not a package manager: [Repeat after me: "Cabal is not a Package Manager"](https://ivanmiljenovic.wordpress.com/2010/03/15/repeat-after-me-cabal-is-not-a-package-manager/)
&gt;Sometimes rebuilding can take several long minutes Well over an hour in fact.
nix is a huge investment in learning and poking around and i honestly don't see why anyone would use it unless they are actively writing nix expressions and finding the abstraction to be useful in their work when that's what you want though, by gods, it's amazing
Why not "(((((avoid) success) at) all) costs)" :P
Hey, turn that :-/ to a :-) If it were easy you would probably have chosen something else. It's about the challenge, right?
I understand that "cabal is not a package manager", but is there any technical reason that cabal does not provide this functionality or would it be easy to implement?
Because that's another functional language family.
I think it mostly measures how community-oriented the language is. Languages which are lower on the list aren't necessarily worse or less popular, they just have communities of nine-to-fivers rather than hobbyists and enthusiasts. I love C#, but I've never set foot on that IRC channel because I don't know what we would talk about. I'm a return customer for the Haskell one, where I can shoot the shit about math and design patterns and what not.
My two cents - Yesod is not forgiving to newcomers. Have you looked into the Spock and Scotty microframeworks? I've also heard good things about Snap and whatever that other framework is. Yesod is sick, but I feel like it requires a lot of investment, too much for hobbyist usage.
Since the post is about how community is growing, allow me to bring up a related topic. I'd like the Haskell community to eschew tribalism as much as possible. It's already giving us a bad name. People are already pattern-matching Haskell proselytism into a Vim vs. Emacs kind of thing. Haskell isn't the only language with that issue. Javascript, Ruby and Scala also get very personal. C#, F#, Ocaml and many others manage to have amazing communities without having to engage in ~~identity politics~~ sectarianism and what not.
it means we've gotten a mindshare in one of the most inaccessible mediums that is embedded in the heartland of the hacker ethos this is something that haskell needs to pay attention to as the community grows. freenode and reddit are the centers of our community, and while we have certainly carved out a relatively welcoming atmosphere in these to places, we must not forget how that sets a tone for people. far too often reddit and freenode are not welcoming places for exactly the kinds of people you don't see showing up to haskell events in meetspace
What's wrong with NixOS?
[Duplicate](http://www.reddit.com/r/haskell/comments/33sc98/haskell_cheat_sheets_for_common_library/) and still on the front page of /r/haskell.
And thank you for writing!
Guess you're right... haven't noticed Python got the first place (blind me) which definitely isn't more complex than Haskell. 
Yesod is a huge and complex system that has provisions for building server side html, css and javascript (templates), server side processing of forms with data validation (formlets). Modern web development went a different direction (Single page applications) where the UI is built and manipulated on the client side (reactive javascript frameworks like reactjs, angular ember) and html forms are not used anymore for communication with the server. If you want to build web applications in that way you may find that a lighter web frameworks that provide bare necessities (session handling and routing) are easer to learn than full yesod. I would advise you to look at [spock](http://www.spock.li/) Also, i wish yesod developers would realize that a light version of yesod that does not pull 75% of hackage with it is something that a lot of us would find useful. By that i mean yesod without templates, formlets, persistent. Just session handling and routes. 
Sure, but that seems orthogonal to the complaint. "Other systems have this problem" is not a statement that the problem is intractable or even terribly difficult to manage. It is just a declaration that the problem isn't unique. I *do* understand that the problem *is* difficult to manage. It is still a serious problem. 
Can you clarify what you mean? I don't know what this: &gt;People are already pattern-matching Haskell proselytism into a Vim vs. Emacs kind of thing. means. And what does identity politics have to do with anything? Or is that a separate concern?
I see that the build error in protocol-buffers is due to a conflict between the newly-introduced [uncons](http://hackage.haskell.org/package/base-4.8.0.0/docs/Data-List.html#v:uncons) function in Data.List and the existing [uncons](https://hackage.haskell.org/package/protocol-buffers-fork-2.0.16/docs/Text-ProtocolBuffers-Identifiers.html#v:uncons) method in one of protocol-buffer's type classes. This kind of conflict is trivial to fix, so I have sent a patch to the maintainer for you; however given the existence of a package named [protocol-buffers-fork](https://hackage.haskell.org/package/protocol-buffers-fork), I suspect that the maintainer might not be very responsive. In the mean time, you can apply the following patch yourself. $ cabal get protocol-buffers $ cd protocol-buffers-2.0.17 $ cat ~/fix-protocol-buffers.patch --- Text/ProtocolBuffers/Identifiers.hs.orig 2015-04-26 12:18:24.000000000 -0400 +++ Text/ProtocolBuffers/Identifiers.hs 2015-04-26 12:18:20.000000000 -0400 @@ -30,7 +30,7 @@ import qualified Data.ByteString.Lazy.Char8 as LC import qualified Data.ByteString.Lazy.UTF8 as U import Data.Char -import Data.List +import Data.List hiding (uncons) import Data.Monoid import Data.Generics(Data) import Data.Typeable(Typeable) $ patch -p0 &lt; ~/fix-protocol-buffers.patch patching file Text/ProtocolBuffers/Identifiers.hs 
I definitely agree it's a problem. I was just addressing your "unreasonable" comment. If it works similarly in other environments, maybe it's not unreasonable, just plain bad.
Vim vs. Emacs was notorious as a feud in which people were *way* too involved given what was at stake, at least from a technical point of view. Things got kind of personal; it was 90% "I am red team, therefore blue team sucks and is inferior". AKA, identity politics. No one changed their minds, instead they started tuning out the other side. I personally think that's what risks happening with Haskell and functional programming vs. object-oriented programming. 
There aren't many Haskell jobs. There are a growing number of Scala, Clojure, etc. jobs. If you're strictly interested in job prospects, those might be better choices.
&gt; I'm not sure why fixing this in Haskell is such a harder problem that it is for ruby Two reasons really * Ruby delays breakage from wrong dependencies to runtime, so you only ever have a chance to notice it if it happens within the subset of a package's API you actually use * Haskell does very wide-ranging inlining optimizations which means an API of a package can include functions prepared for inlining where the body of those functions contains inlined code of dependencies, this causes the API to change if the dependencies change 
Upvote for spock.
In that case, for now I would install `cabalparse` and `mote` by cloning the git repo and using cabal install and then move the `vim` folder in the `mote` repo to wherever you put your bundles. I'll move the vim plugin to its own repo when I have the chance to make this easier.
&gt; How does this call GHC? It uses the [GHC API](https://downloads.haskell.org/~ghc/7.8.3/docs/html/libraries/ghc/index.html). The vim plugin starts a mote process running in the background. Could you clarify what you mean by &gt; Does it/can it use `cabal exec`? I don't really use this feature of `cabal` so I'm not sure what kind of functionality you'd like to see.
&gt; nomadic Did you mean "monadic"? Or, do your parsers not settle into a permanent residence?
Having been at least as frustrated as the post's author has been with cabal, it's good to hear sandboxes work well. I can't think of a world where I *don't* want the profiled versions of packages. If I'm not measuring what I wrote I don't really understand it well, and that seems double true for languages like Haskell with non-strict evaluation.
Same deal as sandboxes with Cabal: dependency conflicts between projects.
Use fp compleat. Never use cabal directly again. 
That is often what happens. 
I'm sure, but at the moment all my attempt to install Nix on my Mac have failed. The best I got was NixOs on virtualbox which is a pain for different reason.
Usually you just clone git-repository (or run `cabal get` to get sources), fix issue and install fixed package to do fixing for 7.10.
really? as a newcomer to Haskell, I was impressed by how much experience everyone has with other languages, and how quick they are to point out Haskell's flaws, when applicable.
&gt;Identity politics is something else Maybe you're right, but then I don't have a term for the kind of dog-whistle tribalism that I'm talking about. &gt;Ocaml and F# communities are just as much into FP as the Haskell community is. I'm not complaining that Haskell is into FP, I'm complaining that we have a big bunch of truisms and canned replies that eventually fit together into a pattern of sectarianism.
maybe they are asking if it works in a sandbox?
fwiw, when I had installed profiled libraries, I couldn't build ghcjs. also, I think doctest + template haskell + profiling didn't work? or two of the three, I dunno. 
I already [posted a solution](https://gist.github.com/jorin-vogel/2e43ffa981a97bc17259#comment-1440951) as close to the original Ruby solution as possible by using maps instead of records.
I was thinking about creating a solution with wreq and lens-aeson (essentially I imagine it should be possible to do with just a long composition of optics) but I'm not enough of a wizard to do that quickly.
&gt; I'd like the Haskell community to eschew tribalism I don't really see that. What I see on the other hand, and maybe it feeds into the perception, are the large amounts of articles written by new (to the language) programmers. For whatever reason, such articles leak into larger groups (like /r/programming), but not reddit limited, which give the impression that everything is wonderful in Haskellandia. I understand that, since around the time I was learning Haskell, it was the only language I used to talk about in the office. This sense of wonder doesn't really translate well into blogs of mass consumption. Equational reasoning, no side effects, purity, immutable, monads, etc. have become some sort of joke outside of /r/haskell. Also there seems to be a view that haskell (reddit/irc) is becoming less friendly to new users. I've seen some discussion taking place here about just that, that new users questions getting downvoted. I have personally never felt that, not here, not on IRC where I go most of the time when I have a Haskell related question. Should it be a problem the fact that questions that could be solved by a trivial Google are downvoted? In honesty, is there a programming community that you would recommend for their behaviour; and detail in what ways the Haskell community is different than it. What could be learned, and improved? 
Join the [Haskell community on Twitter](https://twitter.com/search?q=%23Haskell)!
or just insert . uncurry Map
&gt;Equational reasoning, no side effects, purity, immutable, monads, etc. have become some sort of joke outside of /r/haskell. That's EXACTLY what I was trying to communicate. Thanks for putting it so succinctly. &gt;In honesty, is there a programming community that you would recommend for their behaviour; and detail in what ways the Haskell community is different than it. I'm partial to C#, F#, Ocaml because they're down to earth and get shit done. In general, any language whose users seem to be self-aware and conservative about their claims. I'm starting to believe that this can only happen around "second systems" (Ocaml descends from SML, F# from Ocaml, C# from Java), otherwise the hype takes over. "First systems" have more rough corners because they're trailblazers, but because they're innovative people tend to downplay design issues.
For me, it is nix and nixos that has made Haskell development a joy again. cabal sandboxes, unfortunately, I've had many issues with, for example build times and ghci use in emacs. 
My advice is focus on learning Haskell. You'll be able to pick up any other functional language quickly if you know Haskell well (because you are [looking down the language spectrum](http://www.paulgraham.com/avg.html)). For example, when I applied to Twitter I learned Scala two weeks before interviewing just by going to Hacker Rank and answering as many problems in Scala as possible and that was enough so that I could answer all my interview questions in Scala. Also, it's true that there are more jobs in other functional languages than Haskell, but you only need one Haskell job, not ten.
&gt;"Haskell is useless because of laziness" Huh?
&gt; Therefore, the article is indeed about cabal-install Ok, the title is misleading then ;-) 
&gt; No sandboxes to manage. Just have a Gemfile in the project root listing dependencies. Isn't that the exact same thing ?
Given [instance for functor](https://github.com/aslatter/parsec/blob/b5a5c73afb47be12eb4a022f083e978deabb2a80/Text/Parsec/Prim.hs#L174) uses `parsecMap`, I would say no, unless you're needing to be explicit about the types without providing a signature. For example, in a where clause without using [ScopedTypeVariables](https://downloads.haskell.org/~ghc/7.10.1/docs/html/users_guide/other-type-extensions.html#scoped-type-variables) Note, I'm only guessing here. Someone more informed should definitely confirm/correct my suspicions.
As ekmett pointed out in the comments `ghc-pkg unregister` has existed even before `cabal`. The article is exceedingly silly.
I guess it's a matter of taste, but I never do. It doesn't make much sense to me when other do, either.
If only there was a way for anybody to edit it....
If you need to ask how much it costs, you can't afford it. 
Wonderful. Thanks ! 
If only I had the time, energy and chutzpah to edit large swathes of someone else's article ...
Sandboxes are a pain, as everytime you have to setup a new project, all the packages need to be downloaded and installed again. Instead, bundler dynamically chooses which gems are used from a single local gem repository that is shared by all projects running the same ruby version. This works as multiple gem versions can be installed at the same time. It isn't a huge benefit, it is just a pain having to wait to reinstall packages for every new project when the same task with ruby will only be done once per library version.
&gt; It depends on the amount of paranoia you need for the apps you are hosting. For most ruby apps (which would likely be small to medium sized rails websites), running rvm and bundler on the server is just fine. I sure had gripes about the whole security of this, but I can't blame Ruby about this, as it is just about the same with other languages (RVM's install procedure is particularly bad though). It's more about the inconvenience of having specialized setups on servers running Ruby apps, compared to languages where you can just ship a binary. Also reducing reducing the amount of dependencies was not my call ...
[stackage-cli blog post](https://www.fpcomplete.com/blog/2015/04/announcing-stackage-cli). That does look interesting. It basically relies on the fact that a single stackage lts haskell version will always use the same cabal versions, so the is no problem of sharing packages like before. I'm not sure how it works if you use packages outside of stackage though. 
Unfortunately, it doesn't build a tree and uninstall it for things that "unregistering package xxx would also break yyy zzz ...", currently I do that manually (in bash-loop). Created issue for cabal-uninstall, maybe I or someone else could implement that functionality.
I wasn't casting aspersions on Haskell, I was casting aspersions on OP.
Should be relatively easy for someone familiar with the source code, but none of the regular contributors is actively looking into it. It's also not a priority for the industrial users FWICT.
If it could at least do the expensive, correct thing when you said "cabal install -p foo". Instead, it just fails with a bad error message, and you have to either nuke your package repo, or manually list all packages that are (transitively) needed.
I do, because it's easier to document methods individually. although Haddock's recent "jump to source of instance" helps. also, unless the definition is a one liner, I don't like the extra indentation level. 
developers versus admins/testers?
Not only that, but there's also `cab outdated` to see which packages need to be uninstalled. There's no way I'd be able to keep dependencies straight without `cab`.
&gt; Laziness is (was?) a sacred cow Laziness is the *reason* Haskell **exists**. Purity was introduced as a way to manage laziness. Monads were introduced as a way of dealing with "the akward squad" while maintaining purity. I think every language has fan-boys (and -girls). I'm pretty sure I *was* a Haskell fan-boy at one point. But most languages[1] do have respected members of their community admitting mistakes and trying to address them. Often, these people do have various level of experience with other languages. I don't think this is really an advantage of Haskell, but it would be a decided *dis*advantage if we weren't honest with out community about our flaws. Personally, while I can understand the hate for laziness, I think the biggest mistake was (a) asynchronous exceptions and (b) using them as the default way to communicate to other threads. In particular Java showed that (b) is bad mojo and deprecated their way of doing that long ago. [1] PHP being a notable exception. ;)
Well, we really only have this _one_ language trying it out in -- a language that was explicitly created to explore non-strict semantics. You have every other language to see how the alternative works.
I'm not sure it's a problem anyway. Don't let it bother you?
It has the best packaging story for Haskell of any linux distro, and is built on top of a similarly purely functional lazy language
I don't. I feel like it just clutters up an API. Also, I would highly encourage users to use fmap and various typeclass methods because they come with laws, and open up your ability to do equational reasoning and tells the user how they can manipulate and refactor and how they work together. also your intent is much clearer when people unfamiliar with the library read your code. You can document instances already in haddock, so I don't think documentation is an issue.
After seeing many such pieces of criticism being dismissed, I think that it's important to view this kind of feedback from two angles. The technical angle: This is the most obvious way to look at madjar's complaint. Constructivists prove that the deficiencies in the Haskell ecosystem don't exist because of the existence of cabal-sandbox, ghc-pkg unregister, etc. It's important to note that part of the complaint is that many paths through the ecosystem to a solution are flakey, not just that there are no paths. The perception angle: In some ways it does not matter that the frustration is based on false premisses. If Haskell is to be beginner-friendly, then the ecosystem should be discoverable, and we should strive to avoid shallow paths into peril. That being said, it would be nice if people raising concerns were less negative in their tone, but let's view it as an opportunity to view perception without it being sugar-coated. Now it may be that the over-all prioritisation of work on things such as perception is not high enough to warrant a response to criticism resolving these issues, but if this is the case then it would be good to be able to point to a central response rather than simply dismissing the criticism.
wow! I never heard of it before. Just installed and it's great! It should be featured on [haskell.org](http://haskell.org), for sure :-) EDIT: I guess, I spoke too soon. It's nice for working with global and user package repos but doesn't work for sandboxes.
If Cabal is not a package manager, what is Haskell's package manager?
What is Haskell's package manager?
5 hours for me... but I'm preparing to switch to Linux.
Nix is cool, I really like it. I never use it though because it's complicated and hard to use. 
The haddocks I most commonly find myself looking at are `containers` and `unordered-containers`. I imagine Vector, Map, and Set are ubiquitous in many Haskellers' code, but I can never remember all the fancy variations of the operations supported! Although perhaps one might have those APIs committed to memory if you're spending more time wielding Haskell in anger...
That factoid about Haskell being among the first user created subreddits is in itself mildly impressive though...
I imagine anything is possible with a sufficiently long composition of optics.
I have to admit that, as a perpetual haskell neophyte, the endless round of cabal breakages and searches for documentation that doesn't end in a paper that requires me to read 100s of other papers wears me down. It usually takes me a few weeks to start writing acceptably decent code in most languages, from assembly to Prolog, even if the finer details of fluent expression escape me. Most of my haskell experience has been endless hunts. Things improved when I started using eclipsefp, since it seems to handle the sandboxing without tears and is generally helpful. But just today I started looking at snap as a web framework for reworking a project that I did ages ago and which deserves dragging into the current decade. eclipsefp needs to install snap via cabal. OK. Trying to install snap via cabal breaks. Fine. I've got better things to do and other options. One comment on the technical side of things. There's already a comment about unregister with the reply &gt;If by “done” you mean “Cool, you’ve put your system into a new and excitingly different inconsistent state”, then yes. :-p The technical side of things bleeds over into the perception side of things. As you point out, the technical solutions can look more like additional problems than solutions, due to flakiness. I suspect that the author's unwillingness to use sandboxes stems from earlier bad experiences.
Soon(tm) Maybe We can get /u/snoyberg to put together a prerelease or something.
As someone who works at FP complete, I still use cabal. The online IDE fits particular use cases very well. It's great for demonstration, learning, and as a scratch pad when you don't have easy access to your dev machine. However, it isn't suitable for any of my day to day programming tasks. And at this point, we aren't trying to make the online IDE suitable for our day to day programming tasks. Instead, we're working on an improved Haskell tool chain. Some of our new tools work with cabal, particularly, cabal sandboxes. See: https://www.fpcomplete.com/blog/2015/04/future-of-soh-fphc
Once I read the challenge I immediately began thinking in the Bash scripting language. This was a reflex, Haskell was not the language that first occurred to me, the solution I immediately saw without consciously thinking was a Bash script. Bash is still unbeatable when it comes to this kind of thing, it even beats out Python, Perl, and Ruby. I think Haskell could do better. On thing that I noticed with the various Haskell solutions is that they all required about 20 lines of `import` statements. Someone ought to release a `Prelude_for_shell_scripters` version of `Prelude` that contains everything you need to write in Haskell whatever it is you could write in Bash.
Although I haven't worked on it in a while, this is also the approach that I took in this project.... https://github.com/jamshidh/translato. I ended up creating a parser language that was suitable for both parsing and unparsing text (geared towards language parsing), you can see some of the examples in translato/parser/resources/specs. Although I used Haskell to write the tools, the parsed output is actually in XML (I had planned to generate Haskell types also, but never implemented that). 
I worked on stackage-cli, and I'm glad to hear people are using it! You still have to deal with recompilation, but hopefully the shared sandbox paradigm helps to cut down the number of times you have to reinstall the same package. We intend future iterations of stackage-cli to be even better at sharing build artifacts across sandboxes or even across machines. Recompiling the same thing over and over is wasteful, so we are working on a development workflow that cuts that down as much as possible. The current iteration of stackage-cli is just one step in that direction. Feedback is extremely welcome.
This is one of the coolest things I have ever seen in a long time. You have inspired me to define my parsers and pretty printers as an isomorphism data type in my own library.
If you need monads in your Isomorphism, use the `Kleisli` constructor in the `Control.Arrow` module. That is, instead of using `a -&gt; Maybe b` like what was done in the article, use `Kleisli m a b`, and then yes, you will have all the power of monads.
If you want stateful monads, you can just use the `Kleisli` category in your `Iso` type. So instead of `data Iso a b = Iso (a -&gt; Maybe b) (b -&gt; Maybe a)` you can have `data Iso m a b = Iso (Kleisli m a b) (Kleisli m b a)`. But lets say someone were to make an `Iso` of type: newtype ParsePretty a b = ParsePretty (Iso (StateT PPState) a b) (Iso (StateT PPState) b a) This means the `PPState` would need to have the state for both the parser and the pretty printer combined. Perhaps you could pair the Parser's state and the Pretty Printer's state into a tuple and use that as the `PPState` type. Or you could just make each monad separate: newtype MonadIso m1 m2 a b = MonadIso (a -&gt; m1 b) (b -&gt; m2 a) 
This may have been an accident in the transition from Parsec 2.0 which didn't export them. As I remember, Parsec 3.0 had quite a difficult genesis due to performance degradation and changes to the Parser type. Maybe they simply got exposed by mistake, then left as is.
This is just beautiful.
I would have expected http://hackage.haskell.org/package/logict to be mentioned at some point...
[Duplicate](http://www.reddit.com/r/haskell/comments/33846k/ann_diagrams_13/)-ish. It was on page 3.
Fwiw, http://hackage.haskell.org/package/cab does support recursive removal
You must be using `cabal` different than me then... With `cabal sandbox` and `cab {uninstall -r,outdated}` I pretty much can avoid the headaches I had (except for some package authors seemingly having a disdain for proper version bounds at the expense of their users... but luckily you can avoid their packages most of the time)
You might be interested in [Turtle](https://github.com/Gabriel439/Haskell-Turtle-Library). &gt; Turtle is a reimplementation of the Unix command line environment in Haskell so that you can use Haskell as a scripting language or a shell. Think of turtle as coreutils embedded within the Haskell language.
I would highly *discourage* users to use overly polymorphic versions of functions when type-specific versions are available, except in specific cases where there is a clear need to emphasize an analogy with other contexts. We find that well-named functions - appropriate for the specific case - are an important factor in making code more readable and reducing the cost of maintaining software.
I agree that this isn't a black/white issue. I would still use `(:[])` over `pure`/`return` unless you're in an Applicative or Monadic context, for example. Or `Just` instead of `pure`/`return`. `Nothing` instead of `empty` when not dealing with `Alternative`. But I feel like...if i saw someone use `parserReturn`, `parserMap`, `parserFail`...I'd have to go up and look in the documentation to know what they really mean, or why they are saying. I'd have to look up what `parserMap` was, what guaruntees/laws it had...I'd have to look up `parserFail` and read that it is the identity of `&lt;|&gt;` or whatever choice operator they had... But if someone just wrote `fmap`, `(&gt;&gt;=)`, `empty`/`mzero`...I'd immediately know what they were talking about and immediately know what they were even trying to do. As an extra bonus, I'd also immediately know how I can refactor the code, immediately know what sort of laws and overall structure are at play here. It's not black and white...but I think it's good enough where I wouldn't *discourage* using `fmap`, per se...
I think you forgot to append ".csv" to the file name. Also - the current version of the time library no longer requires importing `defaultTimeLocale` from old-locale. You can get it directly from `Data.Time`.
Sigh, I looked exactly at page 1 and 2. Sorry.
I mentioned in the first paragraph that the post was a direct response/reply to this one referenced by OP :)
Spock and Scotty are micro web frameworks, check them out.
One thing you could do, which isn't necessarily very pretty in the general case, is to parametrize your datatype with type variables for all the fields. So a `User` becomes a specific version of a more general user representation datatype: data UserRep c n = User { credentials :: c , name :: n } deriving Show deriveJSON ''UserRep type User = UserRep Credentials Name type Err = Maybe String validateUser :: User -&gt; UserRep Err Err validateUser (User c n) = User (validateCredentials c) (validateName n) ---- edit: Note that there are ways to derive even more abstract representations for your datatype using techniques generally named *generic programming*, but those techniques are pretty advanced and require you to very creatively reasons about types in a very generic way. Definitely worth looking into, but I wouldn't apply it here in this simple use case.
From what I can see, the number of job openings in Haskell is growing rapidly, it's just hard to find hard evidence because a good portion of them is (still?) filled through personal contacts. This would be my reason to recommend that you focus on Haskell and make personal contacts, and only spread out iff you feel the urge. If you end up applying for non-Haskell jobs, the good ones tend to be the ones where even if it is not directly used, Haskell experience is valued more than the fact that they don't have to teach you another few layers of cruft (every language has them). what is your github handle? (-:
Why not just document the typeclass instance? [Example.](https://hackage.haskell.org/package/rainbow-0.26.0.0/docs/Rainbow.html#g:1)
You can most likely improve this quite a bit, but something in this shape might work: import Data.Either.Validation -- Just a nicety, you could just use Either data CredentialsValidation = CredentialsValidation { eusername : Either String String , epassword : Either String String } instance FromJSON CredentialsValidation where parseJSON = withObject "CredentialsValidation" $ \o -&gt; CredentialsValidation &lt;$&gt; (validateUsername &lt;$&gt; o .: "username") &lt;*&gt; (validatePassword &lt;$&gt; o .: "password") fromValidation :: CredentialsValidation -&gt; Validation [String] Credentials fromValidation = {- ... -} 
Cabal is indeed brilliant. It is tackling a problem which is very, very non-trivial in many ways. It includes some significant new algorithms and approaches to some of those problems. During the first few years after use of cabal became widespread, a few more very non-trivial problems arose that weren't originally envisioned. These became collectively known as "cabal hell". Some of those problems weren't really problems with cabal; they were problems with the community infrastructure for publishing packages. Hackage 2 with its metadata-editing feature, Stackage, and Backpack are important examples of new facilities that mitigate some of those kinds of problems. Other aspects of "cabal hell" really were problems with cabal itself; either bugs, or important missing features. A major amount of effort has been invested by a number of very talented people on the cabal team to fix those during the past two or three years. All in all, "cabal hell" is now pretty much a thing of the past. Except that the documentation is lagging far behind, which I think is the main lesson we learned from this post.
Cabal sandboxes have always supported shared sandboxes.
stackage-cli is a great tool. But shared sandboxes have always been supported in cabal sandboxes.
It's fun to notice that the author of Turtle is at the top of this thread as also being the author of my favorite Haskell solution to this challenge. But he didn't use Turtle in his solution.
The article explains that you should use your operating system's package manager. Disclaimer: that's what the article says, not what I personally believe. I haven't given the subject much thought beyond reading the article. In particular, I do notice the obvious flaw that your OS maintainers are unlikely to have packaged all the obscure Haskell packages you might be interested in. I personally use cabal to install the Haskell side of things, and when it complains about a missing external dependency, I use brew to install the non-Haskell side of things, and I don't mind that I don't have a single tool which know how to install both.
Thanks! Actually I was already aware of "Turtle" but I haven't tried it yet. I don't know if it is good enough to replace Bash though, but it certainly seems it has the potential to become something like Bash.
[I love Houdini](http://3dprojectsteam.com/2011/12/houdini-and-3ds-max.html)! I'd be very eager to join a Haskell project which was going in that direction. 
This is one of the nicer solutions I've seen to this problem. I especially like that the entire algorithm is built using nothing more than the list monad, I thought that was especially elegant.
 do let f = foldl1 (\a -&gt; (a * 10 +)) [s,e,n,d,m,o,r,y,_,_] &lt;- permutations [0..9] let send = f [s,e,n,d] more = f [m,o,r,e] money = f [m,o,n,e,y] guard (s /= 0 &amp;&amp; m /= 0 &amp;&amp; send + more == money) return (send, more, money) [(9567,1085,10652),(9567,1085,10652)] 
If I were you, I'd definitely give ["Thinking Functionally with Haskell"](http://www.cambridge.org/us/academic/subjects/computer-science/programming-languages-and-applied-logic/thinking-functionally-haskell) (Richard Bird) a try. Your background in math will help. Otherwise, no significant prior knowledge of programming languages is assumed. The book's extensive collection of programming assignments (with solutions) can only be called a treasure trove. An ideal companion for self-study. 
That's a long laundry list of features, but that's not what I was asking for. My apologies for not being clear about what I was curious about. Let me try again: Have you heard of Shake? It's a Haskell DSL version of Make. The main idea of Shake is: monadic dependencies, meaning that a first set of dependencies can generate a set of files, and that more dependencies can be obtained by parsing those files. That's the Haskell-inspired feature which makes Shake intellectually interesting. You were talking about a build system which would not use any state. Like a pure function, presumably? That sounds like a Haskell-inspired feature worth thinking about, to see what the implications would be, and see which other features come for free from that core decision. Anybody can write down a wish list of features. That's not where the value is, and nobody is going to implement your tool for you. But if you can rally people behind an idea - for this sub, preferably a Haskell-inspired idea like monads or immutability or pure functions - and show how many benefits fall from this central idea, that's how great projects begin.
You may be interested in using or at least learning from Vinyl. The approach of that library is to parametrize all fields with a functor, allowing you to create records with different computational characteristics but the same structure. So by setting the functor to Identity you get the ordinary record and by setting it to Either you get one suitable for representing a validated record. There are a few reasonably accessible videos about them. If you don't mind some mathematical details look for the ones by the author, Jon Sterling (https://vimeo.com/95694918, https://vimeo.com/102785458), if you do then check out "framing the discussion on EDSLs" https://www.youtube.com/watch?v=_KioQRICpmo .
Someone out there (thanks!) fixed these doc bugs in protobuf. I just rebuild the haddocks and uploaded them to Hackage.
That's just a regular cabal sandbox! You can get it with "cabal sandbox init".
http://hackage.haskell.org/package/flamingra-0.1.1.1
Isn't the reasonable thing to do to disable the install command in cabal then? 
Very interesting, thank you.
Yea, it is, and yes, you can! Stackage sandbox is just a simple wrapper around cabal sandbox that encourages a stackage-based paradigm of sharing sandboxes, so that you don't have to install the same things over and over for each project. By using stackage sandbox upgrade instead of just cabal sandbox init, you get two very simple things: * A stackage cabal.config to go with it * Easy management of shared stackage sandboxes But that's the whole extent of the magic.
I like this idea. With this method, would I only have to write the JSON instances once? Or once for each concrete type?
File extensions? I guess those can be useful sometimes... are you also routing based on the Accept header appropriately?
It seems like you make use of the choice type particulary when choose to handleParse of the /d as either a function of baz or bar. When it comes to finding the right header `responseFile :: Request -&gt; Response responseFile _ foo = baz -&gt; pathInfo : _ [baz &lt;= Maybe responseFile] do baz = | foo =&gt; ConnectSMTPPort : URL [IO -&gt; connectStream int] | readLines _ &lt;- parseBody _ bdy | bdy -&gt; nilFile return NilConnection | bdy -&gt; responseFile | sendCommand conn int _ =&gt; parseReponse _ responseFile | closeSMTP conn msg &lt;- responseToSource msg _ &lt;= IO responseFile
I am not! Thank you, I'll get that working.
I am not sure what you mean by this.
What do you mean by "lazy reactive semantics"? I know about lazy evaluation and FRP, but I don't see how either concept relates to Houdini. Since Houdini has a lot of features and you mentioned animations, which I don't consider to be such an important part of Houdini, I should mention which parts of Houdini I'm the most enthusiastic about. Most important ones first: 1. Procedural modelling: Instead of destructively modifying the model, we destructively modify a graph of transformations which completely specifies the resulting model. 1. Abstraction (aka "Digital assets"): A subset of the graph can be given a name and parameters, so that we can build gradually more powerful abstractions and express more with a graph containing the same number of nodes. Using python/HScript expressions to specify implementation parameters in terms of abstraction parameters in an essential part of this. 1. Different systems for different needs: the modelling graphs, the image composition graphs and the computation graphs are not just using different sets of nodes, the nodes are also shaped differently and plugging one into the other means something slightly different in each case. This reminds me of how in Haskell we use different monads for our different needs (but I'd say Houdini is using different categories, not different monads). 1. GUI version of the above: it's much easier to create a model by linking nodes, tweaking values and manipulating handles than by writing code. Especially impressive is that they even have a GUI for abstraction: the parameter editor, using which you can create a GUI interface for your own abstractions. I think it's also possible to expose your own handles, but I forgot how. 1. Data extensibility: I'm not limited to X, Y and Z, I can add my own attributes and groups, and the nodes will preserve them. This would not be possible if the system was just a GUI over arbitrary functions allowing us to feed arbitrary data into other nodes expecting the same data; the system is intentionally limited to manipulating a very specific kind of data, namely points and faces (in the geometry context at least), and this allows all the primitive transformations to maintain the identity of those points and faces. Each context has their own data carriers; in the case of image composition for example you can create your own data planes, so it's the pixel positions which maintain their identities and can be associated with extra information.
How can I quantify based on the flame graph? Given a flame graph, let's say I start to refactor parts of the code; how do I compare two flame graph and how do I know one performs better than the other? Since the X axis is not time, I should couple this with another tool to measure timing/memory?
You might want to look at [quine](https://github.com/ekmett/quine). I don't really know how it looks like under the hood, but it does have the coolest screens for a graphical haskell project. :D
&gt; How can I quantify based on the flame graph? Quantify what? I guess the main thing that you can quantify is individual time for a stack frame as a percentage of the total running time -- you can hover over a stack frame to get that number. &gt; Given a flame graph, let's say I start to refactor parts of the code; how do I compare two flame graph and how do I know one performs better than the other? Well, you'll see the code path you're optimizing shrink in the flamegraph. If you're talking about measuring overall performance improvements, then getting the total running time might be a better way. &gt; Since the X axis is not time, I should couple this with another tool to measure timing/memory? I'm not sure what you mean. For what concerns "timing", it does measure time, but not in a sequential fashion -- it just splits the total time between the stack frames. For what concerns "allocation", I've thought of providing a flag that will make the tool spit out the numbers for allocation that the profiling gives you, instead of the timing ones.
Edsko has written his own little Haskell tool for the tree pictures (which in turn uses the excellent diagrams package). As far as I know, he is planning to eventually publish the code. 
So here is a thing we can do easily, and I'm sure cabal would accept as a patch: If a user types "cabal uninstall" then provide a message pointing the user to ghc-pkg unregister :-)
I am not sure I know what karma decay is
I ended up getting this working like so: data CredentialsRep uType pType = Credentials { username :: uType , password :: pType } deriving (Show) data NameRep fType lType = Name { first :: fType , last :: lType } deriving (Show) data UserRep cType nType = User { credentials :: cType , name :: nType} deriving (Show) type Err = Maybe String type Credentials = CredentialsRep String String type CredentialsValidation = CredentialsRep Err Err type Name = NameRep String String type NameValidation = NameRep Err Err type User = UserRep Credentials Name type UserValidation = UserRep CredentialsValidation NameValidation instance (ToJSON a, ToJSON b) =&gt; ToJSON (CredentialsRep a b) where toJSON (Credentials username password) = object [ "username" .= username, "password" .= password ] instance (FromJSON a, FromJSON b) =&gt; FromJSON (CredentialsRep a b) where parseJSON (Object v) = Credentials &lt;$&gt; v .: "username" &lt;*&gt; v .: "password" parseJSON _ = mzero instance (ToJSON a, ToJSON b) =&gt; ToJSON (NameRep a b) where toJSON (Name first last) = object [ "first" .= first, "last" .= last ] instance (FromJSON a, FromJSON b) =&gt; FromJSON (NameRep a b) where parseJSON (Object v) = Name &lt;$&gt; v .: "first" &lt;*&gt; v .: "last" parseJSON _ = mzero instance (ToJSON a, ToJSON b) =&gt; ToJSON (UserRep a b) where toJSON (User credentials name) = object [ "credentials" .= credentials , "name" .= name ] instance (FromJSON a, FromJSON b) =&gt; FromJSON (UserRep a b) where parseJSON (Object v) = do creds &lt;- parseJSON =&lt;&lt; (v .: "credentials") name &lt;- parseJSON =&lt;&lt; (v .: "name") return $ User creds name parseJSON _ = mzero Why do I not need to write the `toJSON ({User,Credentials,Name}Validation) ...` functions? It just works as long as I write `toJSON ({User,Credentials,Name}) ...` and leave out the `Validation` types.
Subreddit: /r/KarmaDecay Website: http://karmadecay.com/ How to [check for repost](http://www.reddit.com/r/howto/comments/1s1634/how_to_checking_for_a_repost/).
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Run–D.M.C**](https://en.wikipedia.org/wiki/Run%E2%80%93D.M.C): [](#sfw) --- &gt; &gt;See https://en.wikipedia.org/w/api.php for API usage &gt; --- ^Interesting: [^Ultimate ^Run–D.M.C.](https://en.wikipedia.org/wiki/Ultimate_Run%E2%80%93D.M.C.) ^| [^Run–D.M.C.](https://en.wikipedia.org/wiki/Run%E2%80%93D.M.C.) ^| [^Run–D.M.C. ^\(album)](https://en.wikipedia.org/wiki/Run%E2%80%93D.M.C._\(album\)) ^| [^Greatest ^Hits ^\(Run–D.M.C. ^album)](https://en.wikipedia.org/wiki/Greatest_Hits_\(Run%E2%80%93D.M.C._album\)) ^| [^Pause ^\(Run–D.M.C. ^song)](https://en.wikipedia.org/wiki/Pause_\(Run%E2%80%93D.M.C._song\)) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cqqmosf) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cqqmosf)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
A pattern synonym would make this library even nicer to use: pattern Refine x &lt;- (unrefine -&gt; x) where Refine x = refine x 
I was surprised this joke hadn't been made before. I asked my artist friend to make this up. 
Does anyone know why one would use unsafeCoerce to implement unrefine instead of simply unpacking the (newtype) constructor? Shouldn't the generated code be the very same?
[**@ertesx**](https://twitter.com/ertesx/) &gt; [2014-12-21 03:55 UTC](https://twitter.com/ertesx/status/546514293735567360) &gt; Existentials via RankNTypes. Existential fields make pattern-matching universal: &gt; http://lpaste.net/116939 ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://www.np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
This package seems to require the program to be run with `-P` , whereas ghc-prof-flamegraph requires `-p`. Can someone compare the two options? Is one slower than the other? Also, would ghc-prof-flamegraph work with `-P` too?
How does this compare to [Liquid Haskell](http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/about/)?
The main thing I need for the O(1) cons is a 'root' for the tree, for that the "+1" relationship is very convenient. For query efficiency, anything with a calculated skew would do.
It compiles and that's the way it is! Ugh
I'm excited to see the outcome of the Liquid Haskell one. I would absolutely love first class (or at least an extension) for Liquid Haskell.
There's a bit more information on [this ticket](https://github.com/ucsd-progsys/liquidhaskell/issues/340).
I usually try to get away from any particular hardware optimization and get into a cache-oblivious model when the problems get big, and use bad transdichomotous, homogeneous RAM model assumptions only when the problem is small.
I think calling this an implementation of refinement types is a bit of a stretch. The killer feature of refinement types (IMO) is *subtyping via implication*, i.e. if I have a function div :: Int -&gt; Refined NotZero Int -&gt; Int and a value x :: Refined Positive Int then the expression 5 `div` x should be well-typed, as x &gt; 0 ==&gt; x != 0 I suspect that this library would throw a type error on my example as `NotZero` won't unify with `Positive`.
Do want!
Not necessarily. I've been studying Haskell for over 3 years, and while I admire some aspects of it (purely functional with static typing was what attracted me to it in the first place), it never really "got better" for me. I still find the language pretty much unusable for a variety of reasons.
Oh hi, that's me! I'm very excited to be working on this, both for the opportunity and for how this'll make LiquidHaskell better. The ticket that /u/Lossy linked gives a good idea of the goal, though the final design might look a little different from the code samples there. `:` and `|` of course aren't valid operators, and we might end up with a quasi-quoter layer to translate between the current and native type signature forms. Further down the road, with this Haskell↔LiquidHaskell type infrastructure in place, I hope that we can replace some bits of this encoding with new Haskell features. For example, it would be nice to support whatever syntax [DependentHaskell](https://ghc.haskell.org/trac/ghc/wiki/DependentHaskell) produces for binders (the `x` and `y` in `x:Int -&gt; y:Int -&gt; ...`), assuming that ultimately makes it in.
 \_ -&gt; "Ignorance"
Please turn it into a t-shirt!
This looks interesting, should we just email Levent at his personal address or is there an intel page you'd like us to visit?
I like the concept of teaching by example a lot. In fact I actually found [Roseta Code](http://rosettacode.org/wiki/Rosetta_Code) to be very useful when I was starting out. My main feedback is that having everything on one page might be a bit much. I will add that I don't think the [pipeline operator section](https://github.com/caiorss/Functional-Programming#pipelining-operator) is very idiomatic. Maybe other people like reversing the direction of operators, but I'm use to reading a Haskell pipeline like a pipeline of function composition: `f ∘ g ∘ h (x) ~ f . g . h $ x` So I would do your examples like this: &gt; take 3 (reverse (filter even [1..10])) [10,8,6] &gt; [1..10] |&gt; filter even |&gt; reverse |&gt; take 3 [10,8,6] &gt; take 3 . reverse $ filter even [1..10] &gt; [1..10] |&gt;&gt; (^2) |&gt;&gt; (/10) |&gt;&gt; (+100) [100.1,100.4,100.9,101.6,102.5,103.6,104.9,106.4,108.1,110.0] &gt; (+100) &lt;$&gt; (/10) &lt;$&gt; (^2) &lt;$&gt; [1..10] [100.1,100.4,100.9,101.6,102.5,103.6,104.9,106.4,108.1,110.0] There isn't one for `filter` but you could keep it consistent like so: &gt; let p &lt;?&gt; xs = filter p xs &gt; even &lt;?&gt; [1..10] [2,4,6,8,10] But that brings its own concerns because &lt;$&gt;, &lt;*&gt; operate on Functors and Applicatives. What does &lt;?&gt; operate on? But additionally I am under the impression that the compiler is really good at stream fusion, so it can optimise a `map f . filter g` into a single function rather than running over the list twice. If you introduce your own operator that's not part of base I have no idea what the compiler would do with regards to fusion. I obviously haven't read all of it but from what I saw it seemed like a really good reference guide. EDIT: I must have skipped past the section just above it on [Misc. operators](https://github.com/caiorss/Functional-Programming#misc-operators) so your reasoning for putting it in was probably just to explicitly compare to F#.
I want this as a shirt or hat. PLEASEE
This would honestly make a big difference in usability for newcomers. I'm down.
It should. Hence it doesn't make much difference. Although, probably, for the sake of source code readability, I indeed should just go with unpacking.
I suspect, it's quite far from Liquid Haskell in terms of features. However, it does get its job done well for a certain area of problems, it is a library, it is Haskell, and we can already use it in our projects. As for a comprehensive comparison, I'm afraid, I'm far from an expert on Liquid Haskell to do one.
yes, the information effect does break things :) But, you still have a weaker amortization argument--given a uniform distribution of binary numbers the expected value of the increment operation is O(1). EDIT: that talk of yours looks rather interesting. Thanks.
Yes, but you can also go and build a custom number system like skew binary (or this Leonardo thing) where succ/cons is _really_ O(1) worst-case, which isn't vulnerable to adversarial work loads and timing attacks and all sorts of things we don't want to deal with so the point is moot.
Nix
Could we get O(log n) reverse for subintervals?
You can do that in a regular binary tree though, too.
I really like the solution arrived at down ocharles' branch here, but another option would be to shift the interface to: newtype Offset = Offset (Refined Positive Int) newtype Length = Length (Refined Positive Int) substring :: Offset -&gt; Length -&gt; String -&gt; String
Or is it 3.7 vs 3... Fast enough anyway.
Of course, but it seems to me that this would be done with splitting the tree into three parts - reversing the middle tree and consing back, so if cons is as quick as it seems, this would be an advantage.
Oddly enough, there was [an email to cabal-devel an hour ago](https://mail.haskell.org/pipermail/cabal-devel/2015-April/010124.html) about adding HTTPS support to cabal-install.
You'd want a type-level lambda: instance SomeClass (/\ x . SomeType a b x c d) where... which does not currently exist, but happens to be the [subject of my MSc thesis](https://xnyhps.nl/~thijs/share/paper.pdf).
Does/could it also run sdist automatically? https://github.com/haskell/cabal/issues/272
It doesn't right now, but I have no objection to adding that feature. Design points here: * As @23Skidoo brought up, should it prompt users? * Should it run `cabal check` first? * Getting crazy: should it run a full `cabal test` on the resulting tarball? __EDIT__ One possibility for behavior is that `stackage-upload` would take either files or directories as arguments and, if given an argument, would do the `cabal sdist` for you. Then you could get the desired behavior with `stackage upload .`.
Personally I don't need any prompts. When I run `cabal upload`, it is *always* to upload the current codebase. In other words, it always follows this pattern of `cabal sdist &amp;&amp; cabal upload "$result.tar.gz"`. I don't know if it's common for other people to upload a tarball that was generated from a non-current state of the code. I don't mind having to specify a dot or `--no-prompt`.
&gt;I agree that this isn't a black/white issue. I would still use (:[]) over pure/return unless you're in an Applicative or Monadic context, for example. Or Just instead of pure/return. Nothing instead of empty when not dealing with Alternative. Yes yes yes this absolutely. If your code is explicitly dealing with `Maybe` then use `Just` and `Nothing`. If your code is explicitly dealing with lists then using `liftM` is just weird. But if you're working with `(Monad m) =&gt; m` then clearly yes use `return` and `liftM`. 
Nah. you don't have the power to reshape the tree given in this post, like that. If you can split things up easily, you don't have the structure I mentioned in this post. It offers O(log n) drop, but not O(log n) take. On the other hand, you can find the 'seams' in either this or a more traditional tree that you want to reverse within in log n time, and flip the bits at the top of each 'cut' you make.
I find it amusing that the offer is (only) for "a recent graduate (BS/MS/PhD) of a US institution". It may be due to legal barriers for hiring foreign workers, but that is the first job offer I've seen phrased this way.
&gt; x = (Cons 1 3 a (Cons 3 5 b (Cons 5 9 c d)) shoudn't be reachable from a sequence of `cons`es. I apparently omitted a property in that short description. You'll only ever have at most two adjacent Leonardo numbers in the entire spine and they must be your smallest. This is similar to how a skew binary random access list only allows two trees of the same size, and they're the smallest size currently present. * 17 elements should arrange into 1,1,15 as 3,5,9 violates the non-initial neighbors policy. * 18 elements should arrange into 3,15 as 9,9 shows why having neighbors would eventually lead to violating the expected structure when performing a `cons`. Similarly at smaller scale, this forces the 5 element list to be a single tree of size 5, not 1,1,3. Dijkstra uses this in the smoothsort representation behind the scenes to just track the pair of Leonardo numbers needed to start counting up from his least signicant active Leonardo number and a bit vector of what elements are present starting from there. He only has to care about if the bit vector of active Leonardo numbers is congruent to 3 mod 8 or to 1 mod 4 for his case analysis. (He really wanted smoothsort to have a constant amount of space overhead.) [Edit: I fixed the wording in that section to include that other unstated invariant, and provide an example sequence.]
What about `Data.Coerce`? Would that work here, or does that have the problem of requiring that your library users have the constructor in scope?
A [similar tool](http://www.dcs.warwick.ac.uk/people/academic/Stephen.Jarvis/profiler/) was developed by Stephen Jarvis for his thesis in 1996. However, I'm not sure how easy it is to run that technology nowadays.
&gt; No rights reserved on this image It is derivative work, though
**shameless selfplug** Turns out, I've been hacking on a similar tool (but tailored more to [Trustees](https://wiki.haskell.org/Hackage_trustees) needs): [`hackage-cli`](https://github.com/hvr/hackage-cli): hackage-cli - CLI tool for Hackage Usage: hackage-cli [--version] [--verbose] [--hostname HOSTNAME] COMMAND Available options: -h,--help Show this help text --version output version information and exit --verbose enable verbose output --hostname HOSTNAME Hackage hostname (default: "hackage.haskell.org") Available commands: pull-cabal download .cabal files for a package push-cabal upload revised .cabal files push-candidate upload package candidate(s) list-versions list versions for a package Each command has a sub-`--help` text. Hackage credentials are expected to be stored in an `${HOME}/.netrc`-entry for the respective Hackage hostname. E.g. "machine hackage.haskell.org login MyUserName password TrustNo1". 
`length` has that same property.
No. It works fine as well. Just tested it.
FYI, `|&gt;` is `&amp;` as defined in `Data.Function` as of GHC 7.10 / base-4.8 -- | '&amp;' is a reverse application operator. This provides notational -- convenience. Its precedence is one higher than that of the forward -- application operator '$', which allows '&amp;' to be nested in '$'. -- -- @since 4.8.0.0 (&amp;) :: a -&gt; (a -&gt; b) -&gt; b x &amp; f = f x
Or add a similar plugin system to cabal, to avoid the impression of being stackage-specific
Anti-quote all the things?
With my Trustee hat on, I'm a bit worried about promoting the workflow of uploading packages straight from the current source-repo w/o any `--non-candidate` flag failsafe to make it a conscious choice. This means more work for Trustees to manually edit `.cabal` files for packages accidentally uploaded broken that didn't pass basic QA...
Almost. `length` is of the form `recursive_calls + 1`, but the base case is 0, not 1.
Nice solution but quite inefficient; the search space can be pruned much faster if you do the arithmetic by digits from the least significant to the most significant. Here's such a solution: http://codepad.org/09iEupRX
Pipeline operators are not idiomatic Haskell, but I would like them to be. [My post about Flow](http://www.reddit.com/r/haskell/comments/324415/write_more_understandable_haskell_with_flow/) generated a lot of comments, both positive and negative. Many other functional languages (Elixir, Elm, F#, OCaml) have pipelines. It is a natural way to think about transforming data. 
I'm sure this will get me downvoted, but my experience of stackage so far is simply that it doesn't work. (Yes, I accept it must work for some people, but it's not a foolproof solution, where fool=me). I just get errors about some global constraint sadly, and it won't choose any packages. Sandbox builds work for me; stackage sandbox ones don't. (I'm using GHC 7.10.1 from the hvr PPA).
Well, a capable student is made super productive by the mentors. :-)
&gt; These have got to be the most capable/productive summer interns in the world... Also, mentors are above-average, that should make any hardworking and capable individual accomplish given goals.
We've never published details on unaccepted proposals, so I think you are misremembering. We've posted results at the end of the summer in different formats summarizing what passed and failed, however. The former wouldn't be fair to the student. A student who submitted a decent proposal but not quite high enough to clear whatever bar we had that year would effectively be penalized by having their failure to get into the summer of code publicized. This would create a barrier of fear for people applying, and so Google very heavily frowns on the idea of saying much of anything about particular rejected proposals unless we're asked by the student themselves.
[Here is a version in Agda with dependent types](https://gist.github.com/twanvl/97451e4137f0852f0173). I didn't include the `(!)` function yet. I think you can actually enforce the invariants in standard Haskell2010 using irregular data types. But it is probably hard to write functions for these types. data Tree' a b c = Here b | More (Tree' a c (a,b,c)) type Tree a = Tree' a a a The annoying thing about Leonardo sized trees is that there are two ways to encode a tree of size 1. And you actually have to convert between the two if you do things properly. In the original post Edward can get away with saying &gt; I cheat a bit, and use the (1,1) digit twice to save a little work, rather than start with (1,-1). 
A great list with concise yet informative reviews. Nice!
Nice library, interesting approach. &gt; So, what we need is a positive integer. Sadly Haskell does not define any such type. Well, there is `Numeric.Natural`, originally part of [Edward](/u/edwardkmett)'s [semigroups](http://hackage.haskell.org/package/semigroups) package, later moved to [its own package](http://hackage.haskell.org/package/nats), and now in base. But that allows zero. And having a much more general yet still simple way to make those, without having to invoke one of Haskell's currently very awkward dependent types mechanisms, is really useful.
Wow! 2 out of 18 are for purescript, while nothing for ghcjs. 
I think I have an idea on solving this implicitly. Using type-families and the `Bool` kind: type family Implies a b :: Bool type instance Implies Positive (Not (EqualTo 0)) = True type instance Implies Negative (Not (EqualTo 0)) = True safeDiv :: (Integral a, Implies p (Not (EqualTo 0)) ~ True) =&gt; a -&gt; Refined p a -&gt; a safeDiv a b = div a (unrefine b) Alternatively we can do using empty typeclass: class Implies a b instance Implies Positive (Not (EqualTo 0)) instance Implies Negative (Not (EqualTo 0)) safeDiv :: (Integral a, Implies p (Not (EqualTo 0))) =&gt; a -&gt; Refined p a -&gt; a safeDiv a b = div a (unrefine b) My concern is that with either of the approaches the types might get scary soon. OTOH this does seem to provide what /u/gridaphobe was looking for. 
Okay, it turns out that you can indeed write a completely safe variant in standard haskell, [see here](https://gist.github.com/twanvl/642c5ab6aabc69f968d6). Example: &gt; mapM_ print $ scanr cons nil [0..9] NS (NCons 0 (NMore (NCons (1,(2,3,4),(5,6,(7,8,9))) Nil))) NS (NMore (NMore (NMore (NCons (1,(2,3,4),(5,6,(7,8,9))) Nil)))) AS (AMore (AMore (ACons (2,3,4) (5,6,(7,8,9)) Nil))) AS (ACons 3 4 (NCons (5,6,(7,8,9)) Nil)) NS (NCons 4 (NCons (5,6,(7,8,9)) Nil)) NS (NMore (NMore (NCons (5,6,(7,8,9)) Nil))) AS (AMore (ACons 6 (7,8,9) Nil)) NS (NMore (NCons (7,8,9) Nil)) AS (ACons 8 9 Nil) NS (NCons 9 Nil) NS Nil It involves nice types like ACons :: b -&gt; c -&gt; NS a (a,c,(a,b,c)) (a,(a,b,c),(a,c,(a,b,c))) -&gt; AS a b c I.e. abusing tuples to encode Leonardo trees at the type level.
Why can't things like indexed monads, or even finally tagless DSLs with linear types, be used to recover the amortized guarantees? 
Cool stuff! How common is it for hardware to be designed in (straight) C? I'd have thought people would be using something more high level these days.
The main issue with cabal as I understand it is that we want it to be installable easily _without already having cabal_ -- so there is a tight focus on minimizing dependencies.
The obvious solution in this particular case is that we don't immediately send something to the processor -- rather we move the cart to an "in process" state and have another thread manage connections to the payment processor and the update of the cart state on successful completion. This effectively is a form of "convert the request to nonblocking" -- it falls out naturally from a web architecture because we want to be able to poll for progress updates on the clientside anyway...
n.b. *some* stackage tools are stackage specific. But in general, it's meant to be a tool suite for improving your dev workflow, and so actually using stackage is just one part of that. (And of course you are free to pick and choose which tools work for you. You can ignore the stackage specific ones if they don't appeal to you.)
Something we should have made clearer is that stackage-cli is intended for ghc-7.8 right now. We haven't published any 7.10 snapshots yet, but we're working on it. So if you don't mind downgrading to 7.8 for a little while, you might find that stackage sandboxes do usually work. Feel free to submit an issue to the bug tracker if you still have problems with ghc-7.8. https://github.com/fpco/stackage-cli/issues
We gave ghcjs a lot of love last year, but this year nobody applied with a ghcjs project.
I like the fact that it's colorblind friendly (even though I'm not or dont have any friend colorblind). The book I learn haskell with, [A Gentle introduction to Haskell](https://www.haskell.org/tutorial/) doesn't appear to be in the list. It's my favorite, and the one I recommend to every body who want to learn Haskell. It doesn't fuck about and starts straight away with what make Haskell interesting : types , polymorphism (template the right way) and Algebric data types. This book is great . 
Right, though eschewing promiscuous contact with the rest of the system seems to be the point. (I'm not suggesting your method isn't in this case better than finding ways to allow for amortized analysis via linear types - it's of course better to be able to hold on to, and use, references from anywhere along the construction. I just don't quite see Okasaki's point about amortized analysis in a functional setting being an insurmountable problem in Haskell.) Could you give a pointer to enforcing linear use of one resource in non-indexed monads?
What needs to be done on ghcjs? How can we equip next year's students to successfully apply to work on it?
I'm excited to see that I'm not the only one missing [ggplot2](http://ggplot2.org/) (so I could ditch definetly R ;-) as well as the `nix-like` cabal features.
Chen and Hudak, [Rolling Your Own Mutable ADT](http://haskell.cs.yale.edu/?post_type=publication&amp;p=311). Effectively what you're asking the user to do is to give up this nice language they find themselves in and restrict themselves to an EDSL in which they can express a problem involving a mutable object. Of course then the fastest implementation of that EDSL can straight up use `ST s`, it is linear after all, and there is no real need to appeal to functional techniques at all. On the other hand, Okasaki's point is that you can build many data structures we want within the broader language without shackling yourself _at all_ and still have access to all the things that make functional programming great. e.g. If you need non-determinism you can just work in the list monad, and your sets and maps now have multiple future versions of themselves automatically sharing structure with the old version. Your linear resources are just that, linear. You have to rebuild the whole thing to introduce non-determinism, backtracking, etc. What Okasaki shows is that we can hit many of the same asymptotics that you get while wearing the imperative straight-jacket without putting it on.
I agree with you. If I am writing a function, I typically start at the top with the types and work down to the definitions. That means I think in terms of the result. However when I am working in a REPL, I inevitably think in terms of a pipeline. For instance: let xs = [1 .. 4] xs -- [1, 2, 3, 4] xs |&gt; filter even -- [2, 4] xs |&gt; filter even |&gt; map (^ 2) -- [4, 16]
Good luck. I earwormed /u/godofpumpkins with a similar problem involving efficient skew binary random access lists in agda, and a few months later he fled the company: I'm pretty sure it was in part to escape me and that problem. ;)
It is very common. C is still the most widely used language to program Embedded Systems like micro controllers, DSP - Digital Signal Processors, Car Engine Control Unit (ECU) and low level stuff like Linux Kernel modules, Windows Drivers and so on. Microcontroller is a self contained computer on a chip. It is inside car motors, computers boards, toys, printers, aircrafts, coffee machines ... There is 8, 16, 32 and 64 bit micro controllers. Embedded system has low memory resources and many of them cannot run a full operating system like Windows, Linux or BSD. 
Yeah. I think that taking it over would be a good experience for me. I've never published anything to hackage before, but making a few small changes to an older package seems like a good way to learn. I don't know how you "hand over a package" to someone else. Let me look into where I need to create an account.
Please add the [Haskell Wikibook](https://en.wikibooks.org/wiki/Haskell). Not only is it excellent, it's the only really good intro that assume *no* programming experience, whereas LYAH isn't as great for total beginners.
Yes, the Wikibook is excellent. I am not the author of the page.
 data CarType = Null | Car ... :(
Could you please stop blowing my mind, already? It's blown enough as it is... EDIT: I'm only not catching up on the Fritz Henglein talks, and now you post this. It's hard to keep up with a guy who obviously has a time machine to make his 24h stretch into 48h. :D
That's OK. I'll happily admit that there are a lot of infrastructure problems plaguing the Haskell[1] ecosystem at the moment, but I have every confidence that it'll get better in a few years. I'll happily grant that it's not as easy as getting a JVM server application running, but that's primarily because it hasn't had billions of dollars poured into it. About the present: If you *can* control your deployment ecosystem to some extent (e.g. via containers or something similar), you'll get a lot more out of Haskell[1], and I dare say it'll be about as easy as most other languages. [1] Well, GHC, really. Currently, it's the only game in town. PS: I should ask: What *are* your reasons, specifically?
From git commits: Merge pull request #1000 from kazu-yamamoto/wai-3.0-ghc-7.8 I found this: https://github.com/kazu-yamamoto/FrameworkBenchmarks/tree/master/wai It uses Warp as the handler.
Um, #3 on a large EC2. But, on dedicated hardware it doesn't show up until #13. Wai itself also doesn't do much; it wasn't even tested in most of the scenarios. It's odd to me that snap handily beats yesod on EC2, but on heavy-duty hardware the results are reversed.
I don't see anything Haskell related in the top results of the most recent round. What are you referring to in the headline?
Yes, I was going to suggest that too. Also, what's your thesis about? Should that be on the list? I don't think I noticed it
&gt; But, on dedicated hardware it doesn't show up until #13 Yeah, the reddit title is a little bit too optimistic. However, it's better than last years #31.
It's WAI running on top of Warp.
Me: &gt; Um, #3 on a large EC2. You: &gt; JSON serialization on EC2 has Wai at #3 Did you repeat me (and the headline) for any purpose? My post at least claimed the "#3" was sensationalist; it isn't true on any of the other tests or any of the other system configurations.
My mistake, meant to reply to Taladar
Happy with these results! Even the bare-metal results are quite solid. I'm a bit disappointed to not see warp in the other benchmarks, though. Why is that?
`wai` ~ `warp`
also, the results shown by default are for an instance with a 256 concurrency level. the results are different again. JSON serialization, Dell R720xd dual-Xeon E5 v2 + 10 GbE: Concurrency Level: WAI Rank 8: #39 16: #32 32: #22 64: #22 128: #14 256: #13 Only wish we had results for wai/warp on the other 5 tests, instead of just the json serialization test. *edit- updated answer to refer to concurrency level instead of GB RAM (which i was mistaken about)
that's already included in the list that was linked, so it's not a constructive suggestion
Can anyone offer any insight into the wild performance differences in both absolute and relative (between the frameworks) performance between the two hardware platforms? In what way is wai performance not scaling up as well as others on the dual-Xeon, or, alternatively, why does wai do so well, comparatively, on the EC2 instance?
yes, the title is intentionally like that :-)
This structure looks to be a special case of Qifeng Chen's [size-balanced tree](http://wenku.baidu.com/view/33dfd9f2f90f76c661371a0e.html) of 2006. Chen is now a PhD student at Stanford, though it seems he was a high-school student in China at the time he invented this structure. Chen clearly describes the functions to build and maintain the tree, and benchmarks it favourably against other types of tree. I came across this size-balance tree through a paper in the journal "Geomorphology" entitled "An efficient and comprehensive method for drainage network evaluation from DEM with billions of pixels using a size-balanced binary search tree" where the tree is used for a least-cost path analysis. 
http://www.dohaskell.com :)
That's kind of terrible news! I thought the IO manager scaled that high these days.
Update: It's also possible to make a more advanced version with a bit of type-level magic: https://gist.github.com/chpatrick/758b2533c7c14cec1ecb This supports multiple constructors and you don't have to provide the one you're using to `codec`, but the types are quite a bit noisier and it uses a lot of extensions. This version ensures that the `Field`s you compose all operate on the same actual constructor, and lets you combine `Codec`s for different constructors, but I think it's at the expense of some elegance. Which one do you prefer?
ah, i see it's basically the exact same solution too haha
I've posted my proposal as well: [Native Haskell Type Encoding for LiquidHaskell](https://gist.github.com/spinda/b261167303515cc8a1d9)
Not *quite* the same, but the next best thing!
I confess I don't see the connection. =/ * We're not dealing with a key-value store like a binary search tree. * The size balanced tree tries to maintain a balanced tree; this one very deliberately skews each tree. You'll never see a balanced tree of size bigger than 3 here. * Chen's tree performs rotations AVL style; based on size, but this never does, because nothing ever inserts into the tree, you merely provide a left most node. * Chen's tree is the usual sort of binary tree with the nodes in order, whereas here we're working preorder. * Here we have several trees of ascending size maintained in a spine; there you have a single structure. * The goal here is to provide O(1) cons and random access lookup, modeling a finite list, but with faster lookup. The goal in Chen's tree is to provide basically Data.Map operations with some extra select-by-rank operations added on. The one area where they are directly comparable is that both offer `O(log n)` time `Select(t,k)`. A much, much closer and more interesting comparison to Chen's tree (for me at least) would be to compare it against the ["trees of bounded balance"](http://dl.acm.org/citation.cfm?id=804906) that make up Data.Map, which are more directly based on Stephen Adams ["Implementing Sets Efficiently In a Functional Language"](http://groups.csail.mit.edu/mac/users/adams/BB/), but which date back to ~1972 through the Nievergelt and Reingold paper. I'd be curious to see how the 'deeper' balancing act it pulls off compares from a benchmark perspective. I'd mostly be worried that peering at your grandkids would require more pointer dereferences in the common balanced case. Chen's nodes constantly asks their grandkids for their sizes, while those you are constructing in "bounded balance trees" approach only have to look one level down, potentially trading worse balance for better locality of reference. There we're comparing apples to apples and the size imbalance guarantees are comparable if differently enforced. In the trees of bounced balance you compare sizes directly for your left and right children and rebalance when they get too far out of whack, while in Chen's construction you provide guarantees against the size of your sibling's children instead.
Ur/Web is actually *very* highly rated throughout the benchmarks and has a really low latency.
My partner is an expert Haskell programmer, and she has successfully gotten jobs ranging from Kernel hacking using C to data analytics in a combination of Haskell and Scala. I, knowing only Haskell and ML and a handful of research languages, have gotten tons of job offers for Scala or Clojure or F# based functional jobs. Haskell is a good skill to have, no matter where you focus your attention in your career. Where do you live? Presumably the northern hemisphere? Perhaps there are some people here who are looking for summer interns.
edit: nevermind. Thanks /u/mstksg 
Nice. But why is no mention made of Tom Hawkins' pioneering work at Eaton? They use the [atom](http://hackage.haskell.org/package/atom) Haskell EDSL to create safe highly concurrent automotive controller code in C. Atom is quite mature. It has been used in real vehicles for years. Tom also created the [Confluence](http://www.eetimes.com/document.asp?doc_id=1217201) DSL that compiles to VHDL or Verilog in 2003. But the Confluence compiler is written C, not Haskell, and it is listed under "Retired projects" on [Tom's site](http://tomahawkins.org).
Thanks. I added it to the list.
That's a good example of win-win communication campaign: - Singapore's prime minister is happy to project and reinforce his image as a tech-educated politician piloting a technocratic government. - Haskell people are happy to see their language mentioned in a mainstream context, and will relay the message without much reflection about its political instrumentalization. &gt; It’s pretty impressive that Prime Minister Lee plans to learn Haskell after retirement, since it’s known to be a particularly difficult language to learn. Of course, C++ also has the reputation of being complex, and PM Lee already has the skills to code an app in that. It looks like there’s truly no man better to take the lead in Singapore’s ideal of a Smart Nation. Yay, good things said about Haskell, let's all reblog this!
A Java programmer can write Java in any language, I suppose.
Can you expand on what you mean ?
You might look at what [reflex](http://hackage.haskell.org/package/reflex) does with it's [qDyn quasiquoter](http://hackage.haskell.org/package/reflex-0.1.1/docs/Reflex-Dynamic-TH.html).
Very nice and enjoyable post. I just wish that people would stop maligning Haskell's list type. It's not "bad". It happens to be *exactly* the simplest and most useful way to express iteration in a lazy language with general recursion. As such it's one of the most beautiful features of Haskell. I'm sorry if it can't play exactly the same role in Haskell that the ML list type does in ML, but that's to be expected; it really is something very different. EDIT: Fixed stupid abbreviation "GR". Thanks to the person who pointed that out and then immediately deleted the reply. Luckily, I happened to notice it before it was deleted. :)
It's a great start. But it looks like it hasn't been updated in over four years. :(
Update update: I've made an even more advanced version: https://gist.github.com/chpatrick/c0079dd636f4e44edc13 Here the types are starting to look quite grotty but it supports field names being shared across constructors and even across types, so you could use it as an overloaded record fields implementation.
Would be nice to have [Software foundations](http://www.cis.upenn.edu/~bcpierce/sf/current/index.html) equivalent of Agda.
Yes, but also good performance of the new IO manager, I suppose.
I think that what Dan says about induction is *precisely* what people mean when they say you cannot reason by induction on Haskell “data” types. That is to say, you cannot reason using the natural, intended, naïve induction principles: you have got to reason using these frankenstein induction principles that have little to do with the intent of your program. “In Haskell, you cannot reason by induction” is shorthand for “In Haskell, you don't get any of the right induction principles.” It kind of ticks me off when I get #WellActually'd by people who will say things like, “of course you can reason on Nat by induction, it's just the induction principle of the vertical natural numbers” (or something like that). Anyway, thanks for posting this! It's nice to see the idea fleshed out.
Have you considered open sourcing it and accepting pull requests?
&gt; little to do with the intent of your program this is a place to be a bit careful with language. maybe little to do with _your_ intent -- the whole issue with infinitraversals and comes up because lots of people do intend to reason in this expanded domain. this is not to say that the people doing that can all give an account of generic fibrational induction, or whatever, but they certainly do want to be thinking with a model that is more permissive than in a strict setting. so all these words here, "natural, intended, naive" -- only the last is i think maybe ok in this context -- the other two, if they are to be used in a precise way, need a lot more care. i.e. natural to who? intended to who?
I tend to think the names of types betray the *intent*. So until Haskell folks stop using names like `Nat` and `List` and `Tree`, I am going to assume that the intent behind them is the standard interpretation of these concepts. Not to mention, the whole takeaway from "Fast and loose reasoning is morally correct" is that you should indeed just pretend that these things hold, and that if you do, things will probably work out.
Lists are pretty awesome if what you want is O(1) cons and potential infinite recursion on the right. All this post is trying to argue is that in the presence of the various infinite cases we get in Haskell, it isn't "free". For tons of operations it is just the right structure to use and isn't going anywhere.
I think it's an alternative or applicative if you hide the internal representation. Otherwise, you would break things like associativity because you could tell the difference between: Or e1 (Or e2 e3) /= Or (Or e1 e2) e3 You should also check out how `optparse-applicative` works, which is somewhat similar (it's a free applicative and alternative, to).
&gt; You should also check out how optparse-applicative works, which is somewhat similar (it's a free applicative and alternative, to). It's intended to subsume that. &gt; I think it's an alternative or applicative if you hide the internal representation. Otherwise, you would break things like associativity because you could tell the difference between: &gt; &gt; Or e1 (Or e2 e3) /= Or (Or e1 e2) e3 Hiding the representation is a problem because it being exposed is part of the advantage of the library; you use the generated tree. However, perhaps Or construction in Alternative could be normalizing so that it's always x-associative? So that `x &lt;|&gt; (y &lt;|&gt; x) = (x &lt;|&gt; y) &lt;|&gt; x` because `&lt;|&gt;` could normalize to e.g. left nesting.
You could refactor the AST so that it enforces the laws by construction. The class example of this trick is transforming this: data List a = Mempty | Singleton a | Mappend (List a) (List a) ... into this: data List a = Cons a (List a) | Nil The former requires enforcing the monoid laws via "quotients" and the latter is "correct by construction" (the syntax tree itself cannot encode violations to the monoid laws). I think Edward calls this "right-sizing" the abstraction (making it exactly as large as it needs to be and no larger). I don't know if it's possible to do this for something that is both a free applicative and a free alternative, but if you can do it then it would greatly simplify a lot of things.
Agreed, endofunctions on a finite set form a semigroup. They are in essence the foundation of functional programming.
I might have said that about "fast and loose" the first few times through it, but honestly it's a really rich paper and I keep getting more out of it. The key point is right in the abstract -- it isn't about strict vs. nonstrict languages. It is about total vs. partial languages. That is to say, it addresses issues that occur with-or-without strictness in giving a denotational account to partial languages. The key result is that in the total subset of such languages, total reasoning holds. So the takeaway for me is a much more precise claim: If you use equational reasoning that ignores partiality, then your equational reasoning will hold _up to partiality_. One consequence of this is if you use any other sort of reasoning, like induction principles, that also ignores partiality, then that reasoning will also hold _up to partiality_. An analogous sort of claim is "If you add and multiply only the lowest six digits of integers, then your result will be correct _up to the lowest six digits of integers_." So supposing you are only interested in the result of a calculation as it affects the low order bits, then you need only care about such bits. However, if you care about the higher order bits, then you can expend more effort and reason about those too!
&gt; Next, on my local programming IRC channel, I shared a little trick I’d seen on Twitter for replacing a div with a shift: (i+1)%3 == (1&lt;&lt;i)&amp;3 for i in [0, 2]...And then we got to "you have to run a profiler to know it matters." I contend it’s possible to use your eyes and brain to see that a div is on the critical path in an inner loop without fancy tools. You just have to have a rough sense of operation cost and how out-of-order CPUs work. &gt; Over the years I have built a mental framework for thinking about performance beyond the oft-recommended "get the program working, then profile and optimize the hot spots". This is a very dangerous habit to get into - and completely unnecessary. It takes literally seconds to just run `perf record -ag -- sleep 10 &amp;&amp; perf report` and get the _definitive_ answer as to: * What functions/paths are _actually_ hot in your code. * What the compiler actually did to your code. * _Why_ your code is slow (branch mispredicts, context switches, cache misses) &gt; A profiler is not needed to achieve the desired performance characteristics. An understanding of the problem, an understanding of the constraints, and careful attention to the generated code is all you need. Or, you know, just run a profiler. 
In haskell in particular it's possible to have an understanding of your problem and still just mess up writing the code. Especially for this specific application, profiling will help catch places you're accidentally creating intermediate values that could be avoided. Not profiling is the performance equivalent of submitting a program that compiles without checking that it actually works.
What I used in a little toy based on Dana Xu's work on extended static checking / static contract checking were predicates like: insert :: Ord a =&gt; a -&gt; { as :: [a] | sorted as } -&gt; { bs :: [a] | sorted bs } or insert :: Ord a =&gt; a -&gt; { as :: [a] } -&gt; { bs :: [a] | sorted as ==&gt; sorted bs } And then you can give signatures for things like sort: sort :: Ord a =&gt; [a] -&gt; { as :: [a] | sorted as } or sort :: Ord a =&gt; {as :: [a] } -&gt; { bs :: [a] | sorted bs &amp;&amp; length as == length bs } or go further and claim it is a permutation of the original list, etc. That relied on the fact that it was for ESC style code where we could just write the predicates in regular Haskell, but it might give an idea. Better ideas probably exist, however, and I've fallen way behind on whatever Richard is doing on DependentHaskell so there may be overriding concerns.
That is really cool! I've been wondering how it for such great performance. Thank you.
My comment is a bit late - but I am not sure the new HWS maintainers use the same system, as I seem to recall most of the quotes have reddit links lately. Edit: Also Twitter.
Is there a mechanical way to derive the corresponding free type from the type class without invoking quotients?
I would like to add that in some cases passing std::string by value instead of const std::string&amp; will be actually faster in C++11. This is because of rvalue references. The author could read, for instance: "Item 17. Consider pass by value for cheap-to-move parameters that are always copied" from Scott Meyers book "Effective Modern C++".
Interesting. C++ never fails to be have one more unintuitive weird rule. It is quite ridiculous.
I live on the East Coast, relatively close to New York. That's where I'm planning to go after graduation. I feel like I'm not good enough for anything serious though, which is why I'm trying to bring out my Haskell chops this summer with projects. Some context, I had a phase where I just didn't program for a while and was sick of the cs major, so I'm really rusty on a lot of stuff. I don't feel qualified for anything at this point.
Haha. I'm gonna upvote this.
There's nothing like real data on the runtime characteristics of your program that a profiler will give you. It's empirical data that can be easily obtained. Why not use it. Optimization can be an empirical process if you want it to be. However, it's your call, obviously.
Once I've polished up stackage-setup, you won't need cabal to install the stackage suite of tools. In fact, just the reverse: you can use stackage-setup to get cabal! (And cabal is currently required by stackage-install, to compute the build plan, so for now you do still need it.) https://www.stackage.org/download Be gentle, it's in prerelease. Also, depending on what you mean by "binary download stackage", you can add it to the "planned by stackage folk" category. Spoilers...
The first half is an exercise driven introduction to Coq, and the second half applies it to PLT (while teaching more of Coq along the way). The first half makes learning Coq really approachable and fun - that's the "trick" that I'd love to see translated :) With that said, I haven't tried Learn you an Agda yet, so maybe that's what I'm looking for.
"... that are always copied?" Why are they always copied?
When Chad and I wrote BufferBuilder, we actually started by deciding on the assembly instructions we wanted GHC to produce. We worked backwards to find Haskell code that would produce those instructions. Measurement was still a big part of the process, but we didn't spend much time with a profiler. We basically just wrote benchmarks, read assembly, and tweaked things until we got the assembly (and the performance) we wanted.
Reworded it means "Consider pass by value for cheap-to-move parameters *if* they are always copied" it is a condition. Basically it boils down to passing by value costing you an extra move compared to the alternatives.
Is this a reference to the Scala that comes out of Haskell programmers?
Why does passing by value *cost* an extra move? Do you mean passing by reference? Edit: Or maybe not. I'm confused now about what it is we're supposed to be gaining and how.
&gt; we actually started by deciding on the assembly instructions we wanted GHC to produce. We worked backwards to find Haskell code that would produce those instructions. Why even write the Haskell at that point?
There is a non-zero overhead of calling from Haskell out to C/C++ FFI bindings. If you can get the same performance in Haskell, you avoid that overhead. Also, with an FFI binding you have to choose between a somewhat low overhead (10s of nanoseconds) using `unsafe` vs thread safety and not blocking the Haskell runtime (using `safe`).
Yes, that is what the other commenter said as well.
Syntax simplicity. You can overload rvalue/lvalue references, use a universal reference, or pass by value. For complex reasons it might make sense to pass by value since it just costs you a move and is simplest syntactically. Hard to explain without a lot of context unfortunately. 
Didn't see it then ;)
* ePUB? * Could someone upload agda-mode to [MELPA](http://melpa.org/) for easier setup?
But the github version compiles on Ubuntu 14.04, with ghc-7.10.1 installed by ppa.
Corysama's argument is based on a vague hypothetical: "After you write your code, it might be too late"--or it may not. How's does one determine which side of "may" one is going to be on. And if some performance measure is a requirement of your program, obviously it won't be too late. As retort to corysama's vague and ill-defined argument (if it can be called that): http://www.flounder.com/optimization.htm. 
I've been (theoretically) learning [Auto](http://blog.jle.im/entries/series/+all-about-auto) and this looks like it would be a really good use-case for that, as an alternative to at least 2/3 of your monad stack (Reader might stay). Of course, "completely reimplement" is a bit heavier than "refactor." :)
Got a link? I've never heard of Auto and google is being less than helpful.
My bad. I had somehow thought that `optional` was part of the repo, not `std` (with the file being named `optional.hpp` and what not). So what I quoted is actually Haskell's `maybe :: a -&gt; (b -&gt; a) -&gt; Maybe b -&gt; a`?
Don't tell anyone, but there's a link in the previous comment. ;) That goes to an introductory/tutorial blog series, but the first episode has package and documentation links too. It's a fairly new library, just happened to come out right as I was starting to look into FRP. It's technically *not* FRP by a strict definition, in the same way that Elerea isn't: it's built around discrete events rather than continuous time. But that's the right tool for many jobs!
I had the same error. I run Ubuntu, so I just installed the prebuilt package with apt-get install agda-mode Adga instillation instructions [here](http://wiki.portal.chalmers.se/agda/pmwiki.php?n=Main.Download)
I've seen it happen once or twice in the past week. When I've hit refresh it's resolved. Can you try that as well? If we have a reliable repro (which I haven't found yet) we can try to fix it.
Disclaimer: I'm wrong with a lot of stuff. Please put down the beating stick. Slides: Theory: https://github.com/athanclark/dt-haskell-intro/blob/master/dtt-4-1.pdf Haskell Stuff: https://github.com/athanclark/dt-haskell-intro/blob/master/dt-haskell-4-1.pdf **EDIT**: Oh wow, it looks like the video skips around a bit... use your imagination I guess? :D
The problem is that if you don’t write your code with performance in mind from the beginning, you’ll end up with an evenly distributed performance loss after attacking the hotspots, and at that point you can’t really do anything about it. Since nowadays cache misses account for most of this wasted time, thinking about your memory layout and access patterns from the beginning is a must if you need good performance beyond microbenchmarks.
Could someone enlighten me what is "mathematical interpretation of a list". I googled this and no useful results emerged. Does this have something to do with that haskell list is potentially infinite?
Might not be. int fact(int n) { if (n &lt;= 1) { return n; } else { return n * fact(n-1); } } This version of factorial will be faster than an equivalent one in Haskell. Meaning that you can put n = 1000000000 and it'll be done in around a second.
Returning an utterly wrong result fast is a waste of time.
I'm just pointing out an example where gcc will optimize this into a vastly different and superior function compared to the final result of GHC. Even the let fib n = if n &lt;= 1 then n else fib (n-1) + fib (n-2) would be much more superior (not really exponential) in C++. I'm just offering examples of C++ where recursion might not be such a bad thing.
You'll get the wrong answer, though -- int will typically be 32 or 64 bits, and one billion factorial will definitely overflow that. If you switch to GMP bigints, then this will overflow the stack since GCC/clang will no longer be able to do tail-call optimization on the recursive call (since it doesn't know that GMP's multiplication is associative and commutative). 
For those interested in this kind of thing, Bjarne Stroustrup himself is in the game with a pattern matching library (Mach7) with some pretty impressive performance specs. https://parasol.tamu.edu/mach7/ https://github.com/solodon4/Mach7 I've been meaning to try it but I've been using up all my research time learning haskell...
Why is Haskell function correct? I'm obviously comparing equivalent functions. fib 100 :: Int
Yes, there's a lot going on. Looks like the Core will become dependently typed. ​https://phabricator.haskell.org/D808 After that, the surface syntax will have to be extended. I recently read the [extended system FC paper](http://www.seas.upenn.edu/~eir/papers/2013/fckinds/fckinds-extended.pdf), which explains what's coming. 
Stephanie Weirich talks a bit about it in the latest type theory podcast.
No examples, sorry. Data.Configurator can read your config files, you can install a signal handler to catch sighup, and use an atomic Int to notify there main process that it should reload. Or let it's auto-reload do it.
I am unreasonably excited for this. 
While I also want them in `base`, everytime I've found myself building something on a `Semigroup` I ultimately find a better way to build on top of a `Monoid`. Often the ability to throw in an identity into the equation means that someone far away has the ability to completely "discard themselves" from the final product, which I usually end up needing. Very similar to how every time I think I want to work with just `Apply` it turns out there is a nice notion of a `pure` and I'm back to working with `Applicative`. That said, for plain old reduction, it can be a useful abstraction.
[For the lazy](http://typetheorypodcast.com/)
Writing the same function for Int in Haskell wouldn't really make Haskell overflow, it would make Haskell stack overflow.
You'll get the wrong answer in Haskell too, or you won't get an answer at all due to stack overflow. Your interesting observation is definitely the fact that this is tailored towards ints. Doing the same with floats would make C++ behave in similar way as Haskell (due to floating operations lacking laws of associativity and commutativity).
Let's say I've written it for Int. I'm completely aware that the function I've written above is defaulting to Integer, but I was obviously trying to point out an example where recursion - that could cause a stackoverflow and does it in Haskell - isn't that harmful in C++. I wasn't really talking about computing fibonacci or factorials correctly.
I would be remiss if I did not point out that /u/chadaustin and /u/implicitcast spent a considerable amount of time profiling before they came to realize the source of their slowdown. What chad is talking about is design. Not investigation. 
Nothing specific must be done for systemd, just create your application (no forking/daemonization required) and print output to stdout to get it in the journal. Edit: same goes for upstart, since they are intended to daemonize programs and output to log management. Edit 2: I didn't read you entire question. Configuration reload, HUP handler, etc must be dealt by your application obviously. But it's not a requirement for a daemon
Richard has stated that he is isn't going to graduate until he gets DependentHaskell into GHC mainline. Recently (as in, two and a half weeks ago) Richard and Simon worked out a major simplification on how kinds are going to work implementation-wise.
&gt; what my OMSCS from GT is covering What does that mean? 
For those unaware who he was, he was known as the creator of Haskell. You can see the official post here: https://messages.yale.edu/messages/University/univmsgs/detail/121669
You can check out [Angel](https://github.com/MichaelXavier/Angel/tree/85e5ae38a16552adc56dba6585feb01b669b50d5) which is a daemon that daemonizes other programs and is written in Haskell.
RIP
Something like this: Say I want to make a recursive function for a data type indexed by some proof - as we construct the data type with it's GADT constructors, it's type changes - and becomes heterogeneous. By using type classes, we can still make a pseudo-recursive value-level function that hinges on hypotheses as instance heads. [Here's an example](https://github.com/athanclark/nested-routes/blob/master/src/Web/Routes/Nested/Types.hs#L40) in [nested-routes](http://hackage.haskell.org/package/nested-routes). Notice, line `43` is the base case for the induction, and `46` is our step. The instance heads in `46` _and_ the functional dependencies are what force the induction to work without type variables being ambiguous: If I have a `Singleton` from `a` to `trie0`, and an `Extend` from `trie0` to `trie1`, then I have a `Singleton` from `a` to `trie1`. It's just like how you would write a recursive function, but this time the types diverge. We use typeclasses to make sure the divergence is _just_ right.
I do not :\ I will be writing a guide to dependent typing / type-level programming soon, though, which will be open to refinement.
One of the computers I'm using it on hasn't actually been switched to systemd and while I think that systemd is overall an excellent thing, I'm wary of it outgrowing the unix philosophy. Another program that acheives the same thing is [Angel](https://github.com/MichaelXavier/Angel), but I wanted something command line driven rather configuration file driven.
Just FYI: Haskore has been superseded by Euterpea (also from Yale) and just had a new release several weeks ago. [github repo here](https://github.com/Euterpea/Euterpea)
RIP
I'm not OP, but `systemd` is an pile of growing complexity.
&gt;modern init systems like systemd/upstart Nothing is modern about `systemd` or `upstart`. In fact they're the sort of old-style mega-monoliths you might have seen on a computer running Windows 95. 
A great talk about euterpea that he gave at Haskell NYC last year: https://vimeo.com/96744621
Install an older version of cpphs? You need to download the cpphs version just before 1.19, compile it in a sandbox, build it, make sure the directory the build binary ends up in is first on your `$PATH`, then try rebuilding agda.
`cabal install` does something most people don't want, namely to install in the global space (which is deprecated in favor of sandboxes), and it does not have an `cabal uninstall` command (breaking the "least surprise" rule). A new `cabal` user will most likely use the tool incorrectly. Thus the tool needs to change, not the user. The correct invocation of this command should be something like `cabal --experimental --force global-modify`
Luite was also busy this year with other stuff
I don't think that is such a domain-specific optimization. I have wanted this in memory subsystems for years, and it has been a reasonable thing to implement since we started writing request/response based servers. In ghc, compare what Ur/Web does with being able to ask for a large nursery, and then have an interface to GC where a single value is exported from the nursery before it is killed off. The single value must be traced, just like what the GC does during collection, but the difference is that the user can explicitly ask for nursery collection at the end of a request, when there is basically no state to collect. Another option would be to design light weight threads that can only communicate with other threads through message passing. Then that thread can process a request with no garbage collection. I don't think it is unreasonable to try to improve GC to deal with this situations in a manner that is more efficient than what we can do in C.
I can share more details in private conversation. Yes, you are right. But sometimes (maybe rarely) not own idea could also be very interesting.
Purescript is quite similar to Haskell - arguably moreso than Fay :-)
I think there are multiple reasons why the functor representation is a good one. One is that simply the types are nicer than the types in your example. Another is that it easily allows a form of sub-typing between lenses and traversals given by the relation ship between functor and applicative. This allows us to use any lens as a traversal. Also I don't think your example works as written as view (_1. _1) ((1,"a"),'a') gives a type error while using lens it works correctly. 
Yes, splitOn and sortBy sounds good. Note however that splitOn is a function from an additional package you'll have to install through cabal. 
Yeah, that seems like pretty much exactly what I'm looking for. Except that where you used `WriterT` in your example, I'd probably want some composition of `WriterT` and `EitherT`. Thanks for the link to your example code. That makes the whole library a lot more approachable.
Thank you!
I'm not sure how closely related this is to what you are trying to do, but you may want to look at the [workflow](https://hackage.haskell.org/package/Workflow-0.8.2/docs/Control-Workflow.html) library on hackage. At the least, I find the whole "partial monad transformer" thing to be an interesting approach to the problem.
One of the creators. Haskell is a committee language.
Isn't shared information enough to decide if this could be *potentially* interesting, to request more details? Application in Haskell should generate implementations of data structures.
It's for 10 hours, so that might as well be per day. Good money for a student, even in Western Europe. I would have taken it!
That's a great idea. I've added it to the issue tracker [here](https://github.com/ollef/Earley/issues/2). Stay tuned!
I've spent some time exploring "coindexed" prisms which would let you give back a detailed problem description when things go wrong like this. Sadly, I don't know how to fit them into the existing lens framework in such a way that inference still works, however. They can be engineered to compose with normal lenses/prisms, but when you compose them with an indexed lens/prism the result fails inference. There are other more subtle issues, but that one is the showstopper that has kept us with proceeding with them at all.
Highlights: &gt; Piskac added that Hudak will be missed far beyond campus, a sentiment echoed by many others in the department. &gt; “He is the most complete person I have ever seen, embodying qualities that you don’t believe can coexist … extremely kind, patient, gentle yet also super sharp, smart, and creative, and also highly eloquent, and totally cool,” CS professor Zhong Shao said. “He has made enormous impact on so many people in different walks of his life, but most of all, he, by himself, is the best role model which all of us all strive to become. He has certainly influenced me in a very profound way, not just career wise but also how to be a great person.” And this part is pretty cool: &gt; During his tenure as department chair from 1999 to 2005, Hudak made an important effort to expand the CS’s diversity, hiring four female senior female professors to a department that had existed for nearly three decades without a single tenured woman, according to CS professor Julie Dorsey.
That's true; Paul, though, was instrumental in organizing the committee, and organizing a committee of a bunch of PL nerds is no small feat. (I'm too young to have been there; am just summarizing what I've read.) A quote from the "History of Haskell" paper (which Paul was the primary author of): &gt; The first edition of the Haskell Report was published on April 1, 1990. It was mostly an accident that it appeared on April Fool’s Day—a date had to be chosen, and the release was close enough to April 1 to justify using that date. Of course Haskell was no joke, but the release did lead to a number of subsequent April Fool’s jokes. &gt; What got it all started was a rather frantic year of Haskell development in which Hudak’s role as editor of the Report was especially stressful. On April 1 a year or two later, he sent an email message to the Haskell Committee saying that it was all too much for him, and that he was not only resigning from the committee, he was also quitting Yale to pursue a career in music. Many members of the committee bought into the story, and David Wise immediately phoned Hudak to plead with him to reconsider his decision. http://haskell.cs.yale.edu/wp-content/uploads/2011/02/history.pdf
I realize that pay varies a lot by geographical location, but you should be aware that you are not going to hire a Haskell programmer for $15/hour if that person is based in the US, Canada, or Western Europe. I have worked for that rate once in my adult life, and it was doing desktop tech support for what was basically a nonprofit organization. Not even undergrad students. An undergrad who knows Haskell can get a research assistantship that pays significantly more than that, at least at many schools. I think it's great that you're stating the pay rate up front -- it's more fair that way -- but you should also know that people are going to see this as an exploitive pay rate given the level of knowledge required for the job. In the US or Western Europe, the only people who are going to bite are people who undervalue themselves severely.
When you work on your own project for free, the compensation you're receiving is the pleasure of following your own dream. When you employ somebody else, though, they do not get that compensation -- due to the intrinsic nature of the employee/employer relationship -- and that's why employers need to pay employees fairly for their work. I'm bothering to say this at all because I want to encourage anybody who's reading this, even if they feel they are young and/or relatively inexperienced, to think twice about taking such an opportunity.
I think you might get a better response if you describe this as "looking for part-time undergraduate research assistant." This description better suits the pay level and education level you are seeking. Also, a first-year student is unlikely to have the skills you are seeking.
&gt; An undergrad who knows Haskell can get a research assistantship that pays significantly more than that, at least at many schools. I seriously doubt this. In my experience, it's very difficult to find *any* part-time role for a student. Not talking about remote. Not talking about being academic enough, to be joined with obligatory course work. Not talking about being in Haskell. In Haskell, Carl!
How do any co-founded projects start? Most of the time, the initial idea, anyway, belong to someone alone.
You said you were looking for an employee, not a co-founder.
That's the range of money I was making for 10 hrs/week as an undergrad. I had a summer programming job that paid $16/hr and that was a sweet deal for us students.
Difference? As I said I'm ready to share 50% of potential revenue. But this is not a startup, this is a project.
Pinging /u/chrisdoner, this sounds like something [the descriptive package](http://hackage.haskell.org/package/descriptive) was designed to handle.
"Exploitive rate" is unfair for something which only difference from voluntary participation the project, is that it paid.
At a past job I was able to set up a (proprietary) application based on Happstack to run under upstart. Sorry I don't have code to show, but if you just want to try something simple as a test case, try making a simple Web app under Happstack and experiment by first getting upstart to be able to kill/restart it, then try with reloading config files (it could just be an app that prints out a string concatenated with some other string from the environment)?
I think halcyon does this as well.
($) is called (&lt;|) and (&amp;) is (|&gt;) in F#, perhaps right-pointing and left-pointing triangles?
I'm partial to `(·)` as application when playing around with small languages (and composition otherwise).
I like these operators a lot, though they don't seem to be very Haskell'ish.
&gt; There are those who look at things the way they are, and ask why... I dream of things that never were, and ask why not? ~ Robert Kennedy
I love acme-cofunctor. I use something (alpha-)equivalent in most of my modules.
I'm actually scared to `cabal install` this..
Cute, but I can definitely see that getting confusing.
Ascribing the meaning of "function application" to the symbol `$` was arcane in the first place.
&gt; Does it directly deliver to the destination? Yes, so it's not going to be very reliable.
I got sidetracked by reading the update monad article you linked at the top, wrote my own didactical implementation of it, came back... and noticed that it's basically the same as yours, except that I found that this little function goes a long way: -- | Choose an action based on the state, and emit as result the -- state that obtains from taking that action. update :: RightAction s p =&gt; (s -&gt; p) -&gt; Update s p s update f = Update $ \s -&gt; let p = f s in (p, s `act` p) 
A Profunctor looks like: class Profunctor p where dimap :: (a -&gt; b) -&gt; (c -&gt; d) -&gt; p b c -&gt; p a d with laws like `fmap`. It means you can map 'forward' on the second argument, and 'backwards' on the first. In slightly more category theoretic terms it is a bifunctor that is contravariant in the first argument. As for "positive and negative position" that is a bit trickier. In general we start with terms in 'positive' position. `a` is a single positive occurrence of 'a'. When we go under type constructors this starts to change. `(a,b)` is a type with both `a` and `b` in positive position. In general any `Functor` with a capital `f` preserves the 'sign'. we're working with. On the other hand (-&gt;) is a bit special, the first argument flips the sign. * `a -&gt; b` has `a` in negative position, and `b` in positive position. * `a -&gt; b -&gt; c = a -&gt; (b -&gt; c)` -- so `a` is in negative position and `b` is in negative position, `c` is positive. * `(a -&gt; b) -&gt; c` flips around `a` twice, so `a` is positive, `b` is negative, `c` is positive. You can have a variable in both positive and negative position, usually we'd say such an argument is invariant. Given an argument `a` that only occurs in positive position in a structure and a function `(a -&gt; b)` you can define a way to map it to the transformed structure with all the a's replaced by b's. Given an argument `b` that only occurs in _negative_ position in a structure and a function `(a -&gt; b)` you can define a way to map it to the transformed structure with all of the b's replaced by a's. If the argument occurs in both positive and negative position, then neither is sufficient. You'd need an isomorphism between a and b, a pair of functions (a -&gt; b) and (b -&gt; a) that are inverses to transform the structure. This pretty much precludes "losing any information", but it also rules out almost any interesting functions. If the argument occurs in neither positive or negative position, you're basically talking about a phantom type argument. Concerns about positive/negative positions shows up in logic. We have [tonoids](http://consequently.org/papers/dggl.pdf) for example. But they also show up in type theory. In many.. more pedantic.. languages that try to stay total, like Agda and Coq, you are limited to data types that recurse in a _strictly positive_ manner. Strict positivity is a stronger claim than positive. Basically, you can't double negate to get back to strict positivity. This rules out `Cont r a = (a -&gt; r) -&gt; r` for instance. `a` is in positive position, but is not used "strictly positively." If you don't rule these out you get terms with no normal form and constructions like data P : Set where MakeP : (P -&gt; Set) -&gt; P can be used to derive a contradiction. To rule out these contradictions they commonly insist that if you have data PLike : Set where MakePLike : F X -&gt; X that F must be strictly positive in its argument. **tl;dr** Having the variable occur in solely positive or negative position is a sign we might be able to provide an operation like `fmap` or `contramap` for it.
Yep, that is the Update monad version of `modify`.
Call me conservative, but my 2¢ suggestion: keep $ as it is. It's not like you can condense it - in that sense it is not similar to &gt;&gt;=.
It's perhaps worth mentioning that [/u/tikhonjelvis](http://www.reddit.com/user/tikhonjelvis) is working on an emacs version of the [mote](https://github.com/imeckler/mote) plugin (which has a nice holes interface and comprehensive case expansion). 
Not a whole lot. My code might look a little better is all. It's not something I would subject anyone else to. I've been wanting to get into the habit of writing all my function compositions in reverse style (like a unix pipe), so some concealing may look better than x &amp; process &gt;&gt;&gt; process &gt;&gt;&gt; process
Nice and minimalist. And reminiscent of &lt;*&gt;
It could have been just a contravariant functor...
You could just use `id` for that. fmap (`id` x) [f, g, h] Now *that's* arcane. XD
Manipulative?! Come on.
My favorite is [acme-realworld](https://hackage.haskell.org/package/acme-realworld): "Primitives for manipulating the state of the universe"
`State s a` is `s -&gt; (a, s)`. The first `s` appears on the left of an arrow, which is called negative position. The second `s` appears on the right, which is called positive. (It's a tiny bit more complicated than this when you have nested arrows.)
So, only slightly fewer dependencies than `lens`?
The problem is basically a number of folks colloquially refer to contravariant functors as "cofunctors", but when you go through the actual dualization, you get right back to functor. functor = cofunctor, contravariant functors are not covariant cofunctors. This is why co-monads are covariant functors, not contravariant ones. Fixing the vocabulary is an important step towards deobfuscating why that is the case.
First, concealing symbol as couple of drawbacks : It messes indentation, and having "custom" symbols doesn't help you to read haskell outside of you editor (on reddit, stackoverflow, haddock etc ...). If you really want to use your own symbol, as `$` can be seen as parenthesis which doesn't need to be closed, maybe you coul use `(` (one different from the real one). Having said that, to break the symetry I just usually stick the `$` to the function making it look like a new function. Example, for return $ Just 1 I'll write return$ Just 1 
These conflict with Data.Sequence resp. Data.Sequence.Unicode.
Currently, there is `($)` and `(&amp;)` for forward and backward function application. By using visually symmetric symbols, you could gain better pattern matching by your brain, as it detects that they are both the same but one is flipped, which sort of gives the right intuition for their semantics. For example, consider the inequalities `1 &lt; 2` and `2 &gt; 1`, which can be visually observed to be equivalent, whereas I find myself having a harder time observing that `f $ x` and `x &amp; f` are equivalent. Something like `f ◀ x` and `x ▶ f` would give me the intuition of feeding `x` into `f`. If I'm used to one, then there is a much lesser barrier for me to also adapt the other, whereas currently I'm used to `($)`, but not to `(&amp;)`, because they are asymmetrical.
As monocrom said, we lack a "Acme.Safe" that includes things like: safeFromJust :: Maybe a -&gt; Maybe a
HESH WANTS SOME SEX
How does it compare to [Turtle](https://github.com/Gabriel439/Haskell-Turtle-Library)?
God uses Haskell, apparently.
Sort of awkwardly, Haskell definitely eschews a "container" type. I think once you get used to it there are advantages since "containerness" is sort of difficult to really nail down as a property despite everyone having a great physical intuition for it. The relevant classes are the ones you've already named * `Applicative` indicates containers which can be weakly "matched" or "zipped" together * `Monad` indicates containers which can be "flattened", `c (c a) -&gt; c a` * `Monoid` indicates containers which can be merged `c a -&gt; c a -&gt; c a` * `Foldable` indicates containers which can be thought of as having an ordered list of elements * `Traversable` indicates containers which are foldable and have a knowable shape There are also others as well, but these are common enough. There exists a class in (where else) `lens` which much more directly achieves what you want. It's a bit complicated, but it eventually provides an operation cons :: Cons' s a =&gt; a -&gt; s -&gt; s where `Cons'` is a bit of a simplified lie (the real class is called [`Cons`](https://hackage.haskell.org/package/lens-4.9.1/docs/Control-Lens-Cons.html)) and this signature will look nicer if you replace `s` by `f a`, but `cons` is a bit more general than that. I think that, tbh, this class is a bit more of a practical one than what you usually get in Haskell.
That's actually Kennedy paraphrasing George Bernard Shaw: http://en.wikiquote.org/wiki/Robert_F._Kennedy#Misattributed , http://www.bartleby.com/73/465.html
I had to think about it for a few seconds.... Sealab 2021. "Do you want the stash on, or off?" "Off, please." ".......Too bad."
is there any reason you couldn't use an array of arrays / vector of vectors?
That sounds nice and simple.
Thanks. I forgot that other people actually distribute software to end-users! I'm stuck in the web-service world and don't have to think about that. I got the impression that Nix could be something of an alternative to Docker in the Haskell world. [Apparently NixOS also provides its own containers solution ...](https://nixos.org/releases/nixos/14.12/nixos-14.12.374.61adf9e/manual/ch-containers.html)
I completely agree. It's just that I personally hate PDF as a reading format. You can't please everyone. Especially if there's fussy people like me in the crowd.
Hi! I'm responsible for building and deploying Haskell applications at $WORK, so I suppose I have some insight here... &gt; I guess everyone uses Cabal? Yep, and various home-grown scripts and tools of varying complexity that support (or sometimes subvert!) cabal. You might want to look at [Halcyon](https://halcyon.sh) though. It looks promising. &gt; Do you deploy to bare metal, or VMs, or some "container" solution, or ..? AWS EC2 instances by creating AMIs that run the binaries on Ubuntu using upstart, launched with CloudFormation. I am looking at AWS's new ECS (a container running service) though. &gt; Do you use externally-hosted machines, e.g. AWS or Heroku? AWS &gt; Do you use Nix? Do you use cabal2nix? Do you use NixOS? Do you use NixOps? Do you use Disnix? No, but they're on my list of things to experiment with. I have used NixOS on my own machine and found it as powerful as it is hard to use. I think it's both a documentation problem and a UI/UX problem. &gt; Do you provision machines with e.g. Puppet? We use ansible for various provisioning and automation tasks, but we treat our servers as immutable and create them from AMIs. We do use ansible to provision and bake the AMIs. &gt; Are you able to reproduce your production environment locally? What do you use for that? We use docker containers that replicate the AMI configuration. &gt; Do you have a CI server? What compiles the binaries you put into production? Do you build them locally? We are moving from Jenkins to GitlabCI. We use Docker containers to run tests and build binaries. &gt; Do you build big statically linked binaries? Does this work? It is nearly impossible to build a truly static binary on linux with GHC, so we don't try. We include the necessary libs in the Docker containers and AMIs. It isn't really a hardship. &gt; Are you able to build and deploy with git push or similar? Is this desirable? It isn't a suitable deployment method for us, but it can work well for other use cases. &gt; Do you use framework-specific deployment tools like Keter (for Yesod)? Nope.
Well written Generic code does not do heavy lifting at runtime, instead it's done at compile time by the inliner and other compiler optimizations.
There are advantages, as you say, to Haskell's way of separating different aspects of "containerness" into different type classes. But to use them in combination one might need to know how they are related. 
Ha! I know right :\ I did a bad job and I should feel bad. Sure I guess I would like some feedback.
That sounds an awful lot like a 'sufficiently smart compiler' argument. Compared to TH which always runs completely at compile time Generic code certainly does run completely at runtime subject to all the same optimization that any code would benefit from.
We use stackage and compile binaries. We use mesosphere and ansible on dedicated servers. I tried to convert our sysadmin to Nix without any success. Yet. Actually we didn't integrated a CI for our Haskell components yet. It is just a matter of using the right prepared VM. We have an external library dependency which make compiling a bit harder than just a cabal install. 
Cabal, Stackage, bare metal &amp; virtual, plan to migrate to docker-based infrastructure, moving to AWS. Deploy via rsync &amp; supervisord reread/update. Jenkins has the job to compile on build-server and then deploy. Big static binaries, but without static files compiled in, they're separately-living and rsynced also. Quite straight-forward stuff really.
On the other hand you would have to know both styles anyway since Haskell code you read will not always be in your editor.
IIRC, Turtle is just a library; Hesh seems to do some preprocessing. Also, the Hesh manual says that an interactive shell is an eventual goal for Hesh; I don't believe this is the case for Turtle.
Sure you can! Use pandoc to generate PDF from HTML!
&gt; I think it's both a documentation problem and a UI/UX problem. For me the problem is more like an apparent lack of manpower. There are a load of packages that I need to be up-to-date, and Arch is doing a good job on that.. but I will move to NixOS some day (and probably update the packages I need myself).
I didn't bother looking. This being haskell, and the PDF just *reeking* of latex, I figured someone loved his math and science documents. Thanks :)
I think with the introduction of [ngHaskell in Nix](http://lists.science.uu.nl/pipermail/nix-dev/2015-January/015591.html) mostly everything is automated. And IMO Nix does a good job with Haskell packages, the only problem I found is that the documentation still needs to be improved. But that being said, their IRC is extremely friendly (Thanks /u/lethalman ! ).
Thanks, that was quite illuminating! 
Thanks for the extensive answer. I had not seen the concept of positivity and negativity in relation to haskell or type theory before and having seen it now I think this should be promoted more to intermediates and beginners in the language. It gives another way to reason about types and I feel after getting used to the concept I'll have a better understanding of some of the more exotic types. In particular its making your lens library make a lot more sense already! 
Cabal sandboxes for local dev (although I'm in the process of migrating to nix-pkgs locally), CircleCI for continuous integration and deployment to the dev environment (EC2), Docker for applications in QA and Prod (the company I work for is big on docker). Whenever a new git branch is pushed to Github CircleCI automatically attempts to build it, and run the tests. When the feature branch is merged into the dev branch, it is built, tested and deployed to an ec2 instance (the dev environment) via scp (tar it first though, ghc binaries compress incredibly well) and monitored w/ ubuntu upstart. If merged into the qa branch or master, circle does the build/test dance again but this time will build a docker image (via a Dockerfile based on this image: https://github.com/freebroccolo/docker-haskell) and push the new image to a private hub for dev ops to take over. CircleCI seems pretty progressive when it comes to supporting haskell dev (7.10 will be available to all containers soon). If you don't like any of their default commands you can override them easily. The circle.yml file is really just bash commands, so anything is possible, but it also gives you services for free like redis, postgresql, etc, to run integration tests. It lets you clone private repos easily w/o fiddling with ssh keys (only if you register w/ your Github account). CircleCI also lets you cache sandboxes, which is huge, and is free on private repos! You only pay if you want to have multiple containers build the same project. One downside is that containers are single-threaded, so this slows down the install/build process, but that's about it. The GUI is nice for watching your build, it also emails you the build result (pretty standard). As this project grows I'd like to move towards using shake, and potentially bake (if we outgrow circles single-threaded container limit). On circle my project takes 30 minutes to fully install/build from scratch, and ~4 mins when the sandbox is cached. It's a lot faster on my local 8 core mac though. NixOps looks pretty cool.
[This has been on reddit already](http://www.reddit.com/r/haskell/comments/3425pk/refinement_types_as_a_library/).
I am a Haskell newb / most of my work has been in languages like Python and PHP. What is the alternative to statically linked binaries? What exactly does statically linked mean and result in in this context? Does this just allow for portability? (i.e. affords the ability to copy over the executable to virtually any machine with a similar CPU and execute it without issues?)
&gt; I guess everyone uses Cabal? For now. &gt; Do you deploy to bare metal, or VMs, or some "container" solution, or ..? Yes. &gt; Do you use externally-hosted machines, e.g. AWS or Heroku? Sometimes. &gt; Do you use Nix? Do you use cabal2nix? Do you use NixOS? Do you use NixOps? Do you use Disnix? No. &gt; Do you provision machines with e.g. Puppet? Rarely. &gt; Are you able to reproduce your production environment locally? What do you use for that? Yes. It varies. Used vargrant for a while but usually wget ghc, build cabal, then run along. &gt; Do you have a CI server? What compiles the binaries you put into production? Do you build them locally? &gt; Do you build big statically linked binaries? Does this work? Nope. &gt; Are you able to build and deploy with git push or similar? Is this desirable? I don't, others might. &gt; Do you use framework-specific deployment tools like Keter (for Yesod)? A home-cooked deployment framework in one case, but don't assume every programmer even has deployment concerns.
I would think repa or yarr (http://hackage.haskell.org/package/yarr) would do e.g I used both to simulate planetary motion: https://idontgetoutmuch.wordpress.com/2013/08/06/planetary-simulation-with-excursions-in-symplectic-manifolds-6/ BTW you can tensor lots of things but linear algebra suggests vector spaces. The idea is to create the free vector space of two vector spaces and then take the quotient using relations you expect to hold e.g. a(x,y) = (ax,y) = (x,ay) where x and y are vectors from the spaces you want to tensor.
But with a couple new instances this little awkward misunderstanding can be solved, right?
I am assuming you don't have any actual data stored on the systems?
Hmm, that's interesting. I wonder how much of an actual difference there is? Apparently for Aeson the difference was enough for them to spend a lot of time making the TH interface and explicitly warn against the Generic one, but like you said, that may not be as true for Binary.
Statically linked in the Haskell world usually means that the executable program is self-contained: it doesn't need any particular libraries installed to run, because the libraries are bundled with the executable file. It would be a chore to have to install a bunch of Haskel libraries to run the executable. It does not mean you can copy the executable to a different system and expect it to run, though. Different operating systems use different executable formats.
Heh, no. Actually, I wouldn't even normally offer somebody feedback like this, but you have a good voice and already are doing a good job at varying your pitch and inflection (speaking in a monotone seems to be a mistake a lot of computer science speakers make, but you're already not doing that), so I think you have the potential to give really good talks! My thoughts were just: * the lighting in the room isn't great, which makes it hard to see you. I realize this probably wasn't something that was on your mind, because somebody else was taping! Just something to think about for next time. * you're sitting down the whole time (well ok, I didn't watch the entire video, but it seems that way), which -- especially for a long talk -- makes it less engaging for the viewer. If you haven't seen it, Simon PJ has a great [video on how to give a good research talk](http://research.microsoft.com/en-us/um/people/simonpj/papers/giving-a-talk/giving-a-talk.htm). The great thing about that video is that he's jumping around and providing a clear demonstration of some of the principles for public speaking that he's talking about :) * the slides are text-heavy and have a lot of text per slide. Personally I prefer more slides with less text on each one, and -- when possible -- images. (I know it's often hard to illustrate this kind of theoretical material with pictures, but it does help when it can be done.) As an example, the "System F_omega" slide (around 25 minutes in) has way too many typing rules for one slide. This is going to overwhelm even people with a Ph.D. It's good to put one rule per slide if you can (this can also entail leaving out rules that aren't too different or important and referring people to a paper for them). Hope that helps. Another thing that Simon PJ says in that video is to never apologize for your own presentation. Teaching (and that's what's going on in any talk) is a gift that you are giving to people. It may be imperfect, but that doesn't mean you have to apologize for that... the flaws make it real :) From your tone in your subject line and comments, it sounds like you're expecting to be criticized harshly, maybe due to personal experience or maybe just due to watching it happen to other people. This is something I deal with myself and it's a hard thing to come to terms with. I don't know what to recommend about that except to just keep putting yourself out there, and wait for time to do its magic :)
What's an example of "well written Generic code" that uses ghc RULES to leverage the inliner in the way you describe? I understand that conceptually it's possible but I've never seen any code like that before.
I've not yet seen Generics code that *doesn't* totally disappear once compiled with optimizations, not that I'm saying it doesn't exist. Special rewrite rules aren't even usually required.
Can I copy from one computer to another if they both have the same operating system?
&gt; I guess everyone uses Cabal? With (recently shared) sandboxed stackage builds. &gt; Do you deploy to bare metal, or VMs, or some "container" solution, or ..? Base hosts usually virtualized with a generic ubuntu/centos layouts. &gt; Do you use externally-hosted machines, e.g. AWS or Heroku? AWS and DO, sometimes. &gt; Do you use Nix? Do you use cabal2nix? Do you use NixOS? Do you use NixOps? Do you use Disnix? Nope. LTS sandboxes are just enough. &gt; Do you provision machines with e.g. Puppet? Our hardware pool is not that large (&lt;10 hw hosts). &gt; Do you have a CI server? What compiles the binaries you put into production? Do you build them locally? Developer-built (since our dev machines match production) keter bundles. &gt; Do you use framework-specific deployment tools like Keter (for Yesod)? We use keter with one user/instance per project. There are few shared keter hosts but they exhibit some non-obvious startup issues and it is preferable not to take down unrelated services while restarting one that stuck.
They made the TH interface long before the Generic one, Generic is a pretty new feature.
People keep making a big deal about the statically linked binary thing. I think it may have been a big deal in the past, but many of the folks I know in the Java world do the same thing with standalone jars. The size of your binary is minimal compared to e.g. size of data, storage available, benefit to be gained by putting effort into static linking. It basically doesn't matter except in certain embedded cases.
Author here. This package did indeed start as a joke on `lens`' dependencies, and how a very innocent package can take a humongous amount of time to compile if it has the right dependencies - imagine one depending on "only" `base`, `lens`, and `yesod`. The current version is a very naive implementation (via premium shell scripting awfulness). As you can see on the Github issue list, the key feature is still missing: minimizing the number of explicit dependencies, so that it still installs everything, but as much as it can as transitive dependencies. Edit: Now that I think of it, the initial joke was when a colleague pulled in `either` as a dependency because he liked the `leftToMaybe :: Either a b -&gt; Maybe a` function.
`cabal sdist` made my 4 gig RAM go well into happy swapping mode, I'm not sure you'd like the dependency checking (if it worked).
You have *no idea* how much joy this gives me.
Of course. That's what the platform is for. Ignore that fool about the same CPU garbage. We don't live in the 80s. As long as the other CPU supports the same instruction sets, ie. it's an Intel/AMD of the last 15 years (and it was compiled for 32-bit, otherwise you need an x64 CPU), then you can run it provided you have enough RAM and the same operating system.
Could you elaborate a little on how would a "coindexed" prism behave? Would it return the "missing index" on a match failure, or something like that?
Nope, it's just the same ol' optimizations GHC uses for everything else. It's just very amenable to them.
Aeson does not recommend against using Generics, the documentation even says "Please switch to GHC generics or Data.Aeson.TH instead.". It's the module Data.Aeson.Generic that's deprecated, probably since it uses Data, I'm guessing it does not use aeson instances for inner types. 
Shake'n'Bake! * We use Shake for building. While Cabal works fine for Haskell, a lot of "real" projects are a lot more than just Haskell. As soon as you get generated code, cross-language interop, large chunks of C++ etc you need something more than just Cabal. * Bake is a suitable CI server. Most CI approaches let anyone check in code to the main branch, then test it, and find out after-the-fact that you got it wrong. That approach either slows down developers (they have to test more thoroughly before pushing) or results in annoyed developers (other people break their code). Generally, it tends to result in slowing down the good developers who care, but not the bad ones who break stuff anyway. With Bake, code never gets into the master branch until it compiles and passes the tests.
I looked a bit through the files. It uses macros to make it look reasonable. I basically found `#define Match(s)`, `#define Case(...)`, `define Otherwise`, `#define EndMatch` (in file match.hpp). The actual code hidden behind the macros is plenty ugly and long, making me worry about the error messages it will produce.
Why use either for that though? Just use lens! `preview _Left :: Either a b -&gt; Maybe a`
&gt; I guess everyone uses Cabal? I do &gt; Do you deploy to bare metal, or VMs, or some "container" solution, or ..? All of the above &gt; Do you use externally-hosted machines, e.g. AWS or Heroku? AWS yes (Elastic Beanstalk + Docker), Heroku no (other stuff yes, tried a third-party Haskell buildpack a while back but didn't have much luck) &gt; Do you use Nix? Do you use cabal2nix? Do you use NixOS? Do you use NixOps? Do you use Disnix? No, but Nix+friends are on the list of things to look at &gt; Do you provision machines with e.g. Puppet? Yes (masterless Puppet) &gt; Are you able to reproduce your production environment locally? What do you use for that? Yes; Vagrant for the "bare metal" / VM deployments (+ Puppet), Docker for the AWS Elastic Beanstalk deployments &gt; Do you have a CI server? What compiles the binaries you put into production? Do you build them locally? Yes / my CI server (Jenkins, CircleCI, Travis but not for Haskell) / Local builds only for personal testing &gt; Do you build big statically linked binaries? Does this work? Only what comes out of a fairly vanilla `cabal build` &gt; Are you able to build and deploy with git push or similar? Is this desirable? Pretty much (in the sense that when a GitHub PR is merged into `master`, the CI server builds and deploys. This is highly desirable &gt; Do you use framework-specific deployment tools like Keter (for Yesod)? No So here's the thing - virtually none of the above is Haskell-specific for me. I have a lot of different things in production, in multiple environments. To be frank, Haskell doesn't make up much of my production code, but at the same time, none of my environments really _care_ - I build a deployment artifact, I ship it to my servers, it runs. Things like [12factor](http://12factor.net/) help that happen.
Interesting. One remaining doubt: when you compose two coindexed prisms, what is the type of the "failure" associated to the composition? Some kind of sum type perhaps?
When you compose two indexed lenses you get the second index. We have special combinators (&lt;.) (.&gt;) and (&lt;.&gt;) that let you pick the first, second (same as (.)) or the both. This works out because when you compose them you get `p a (f b) -&gt; s -&gt; f t` composed with something else of the same shape, forcing `p = (-&gt;)`, which indicates you aren't using the outermost index. For a coindexed prism notion, if we could work through all the other issues, we'd need similar combinators to merge multiple error types.
Spread the news. Someone also made me aware of it and I'm forever thankful for it. :) Don't miss [the top](http://www.reddit.com/r/lolphp/top/), of this [twitter stream](https://twitter.com/php_ceo) either. 
Not convenient (can't use V3 as indexes for example).
I've looked into this a couple of times. Basically, it's hard to get GHC to reliably optimise all the generics representations away, but it often does a pretty good job: José Pedro Magalhães. Optimisation of Generic Programs through Inlining. In 24th Symposium on Implementation and Application of Functional Languages (IFL'12), 2013. http://dreixel.net/research/pdf/ogpi.pdf
Oh wow, I definitely misinterpreted that as "Do not use Generic".
 (source'', args) &lt;- if useStdin || (args' == []) then (\x -&gt; (x, args')) `fmap` TIO.getContents else (\x -&gt; (x, tail args')) `fmap` TIO.readFile (head args') **This is everything that is wrong with Haskell**. I have a bigger complaint with Hesh. It's a cool thing that it takes fully-qualified identifiers and imports the module for you. A bit less cool is that it takes the next logical step and writes a Cabal file naming dependencies. What's creepy and psycho is that Hesh then `cabal install`s those dependencies.
I have a moderately sized (10k lines of haskell) Yesod based webapp in production. It's an internal web site in a large US based company along with a couple of helper applications. &gt; I guess everyone uses Cabal? Use cabal to build, but generate Debian packages for deployment. Just recently moved to using cabal snadboxes because this project started before cabal's sandboxing capabilities were released. &gt; Do you deploy to bare metal, or VMs, or some "container" solution, or ..? VMs and hardware running Ubuntu. &gt; Do you use externally-hosted machines, e.g. AWS or Heroku? Internal machines. &gt; Do you use Nix? Do you use cabal2nix? Do you use NixOS? Do you use NixOps? Do you use Disnix? Nope. &gt; Do you provision machines with e.g. Puppet? Chef. &gt; Are you able to reproduce your production environment locally? What do you use for that? Have CI, testing, staging and production environments. &gt; Do you have a CI server? What compiles the binaries you put into production? Do you build them locally? CI builds Debian packages to run on the CI and test instances. Use the same build script to build a Debian package we test on staging, and then move to production. &gt; Do you build big statically linked binaries? Does this work? All the haskell code is statically linked, all the underlying C libraries are dynamically linked. These run time C libraries requirements are encoded in the Debian package. &gt; Are you able to build and deploy with git push or similar? Is this desirable? We do that for CI and testing. For staging (a practice run for production) and production we like to have human eyes on it as well. &gt; Do you use framework-specific deployment tools like Keter (for Yesod)? I've never used Keter. 
Broken? All of the code blocks are the same. Looks like: instance Category Parser where id = Parser $ \ case a:_ -&gt; [(1,a)] _ -&gt; [] ... 
What do you mean? That is the content of the last code block on the page. That fragment there uses `LambdaCase` and I didn't supply the (.) instance instead opting to showcase what is wrong with it in the comment below. Maybe try refreshing? It could be something went goofy with the javascript on the page.
Anyone else have display issues with this website? [HERE](http://i.imgur.com/eBBqR91.png)'s what I see in Chromium on Linux. If I try Firefox, even the `import` lines disappear and the codebox is totally empty.
What a great thread. I just deployed my first Haskell binary to production Friday, at a large investment bank. Bash script for test/build (Cabal) and tar. Manual scp to hosts (same h/w), bash script to start/stop. Still need CI -- very interested in some new discoveries on this thread like Gitlab CI. Would love to have something more stable than just Cabal for building, so interested in Shake. How are people hosting their source for Cabal? Currently doing manual imports from adjacent cabal dirs ... equivalent I guess to a git submodule. Would rather have a company hackage. I will say this: deploy binary to server that just freaking runs and never crashes (OK haven't been in prod very long but over a month in prod-like UAT), uses an order of magnitude less RAM than comparable Java services and starts up way quicker. VERY happy with the stability and performance.
Yep, they're all stateless.
&gt; There exists a class in (where else) lens which much more directly achieves what you want. It's a bit complicated, but it eventually provides an operation &gt; &gt; &gt; &gt; cons :: Cons' s a =&gt; a -&gt; s -&gt; s &gt; &gt; &gt; &gt; where Cons' is a bit of a simplified lie (the real class is called Cons) and this signature will look nicer if you replace s by f a, but cons is a bit more general than that. I think that, tbh, this class is a bit more of a practical one than what you usually get in Haskell. Every time I think I've thought of something new, it turns out Kmett has already implemented it and then hidden it somewhere I would never look.
Nice! The existence of this workshop makes ICFP way more appealing to me, as someone who has never been to one. I'm really glad to see stuff like this.
Maybe most CI approaches still work that way, but the trendy, modern ones (Travis CI, and most Jenkins and BuildBot configurations I've seen) build and test the result of a code-merge before the code gets to master. I'm only piping up to make sure random people reading this know that most any CI server can do this.
I disagree - it's routinely used in division, which is non-commutative. The association exists, and furthermore there's a backslash to reinforce the idea that it's a directed operator.
In your version you need to supply the limit when creating a non-empty list. How then would you implement the pure method for an applicative based on these lists? And in general, every time when you create lists in your program, you somehow need to come up with this limit to put into the list. So having the limit in the type makes things *simpler*: * No need to supply the limit when creating lists * No need to initialize the limit with 0 (which isn't correct) * When appending, you know by construction that two lists have the same limit, so no need to take the maximum And other than that, the type-level limit doesn't add more complexity than, say, generalizing `String` to `e`.
I like `safePerformIO = Just . don't`
Repa is quite good and the types become intuitive pretty quickly IMO. It's what I usually turn to.
The Jewish liturgical poem [el adon](http://en.m.wikipedia.org/wiki/El_Adon) says so: &gt; יצרם בדעת בבינה ובהשכל Which describes how the creator made the universe: &gt; Made with understanding, wisdom, and Haskell 
Non-mobile: [el adon](http://en.wikipedia.org/wiki/El_Adon) ^That's ^why ^I'm ^here, ^I ^don't ^judge ^you. ^PM ^/u/xl0 ^if ^I'm ^causing ^any ^trouble. [^WUT?](https://github.com/xl0/LittleHelperRobot/wiki/What's-this-all-about%3F)
&gt;&gt; Do you use Nix? Do you use cabal2nix? Do you use NixOS? Do you use NixOps? Do you use Disnix? &gt;No, but they're on my list of things to experiment with. I have used NixOS on my own machine and found it as powerful as it is hard to use. I think it's both a documentation problem and a UI/UX problem. Hi! I'm a NixOS guy. Which parts of the documentation are lacking information? I want to contribute to NixOS as much as I can, so every pointervwherevI can help is highly apprechiated! Atm I'm only submitting packages and discussion, but I'd greatfully submit documentation as well, at least where I'm able to improve the existing one ...
&gt; you want failure on first error (e.g. in production) We may be talking about different kinds of errors. I am talking not about programmer errors (exceptions), but about user errors. I want to give the user a summary of their errors without overwhelming them. I think that's what you mean by «summarize them if there are lots of them»; except summarizing errors intelligently is a very hard problem, and a simple way to do that is just to give the first n errors.
All that and much, much more is provided by the [Edison API](http://hackage.haskell.org/package/EdisonAPI). It was created by Chris Okasaki at around the same time that he wrote his seminal text Purely Functional Data Structures ([thesis PDF](http://www.cs.cmu.edu/~rwh/theses/okasaki.pdf), or [real book](http://www.amazon.com/Purely-Functional-Structures-Chris-Okasaki/dp/0521663504)) in the mid 1990's. I think the reason it didn't catch on was that the API is in some ways hard-wired to Chris' own set of container implementations.
&gt; Your `logError` function has the wrong type: it should be `e -&gt; SmartValidate e a`. Hmm, but if `logError` claims to return any `a`, doesn't that force it to abort the computation the first time it is called? Oh, I see! That's why your article is focussing on Applicative instead of Monad: the result of one Applicative computation is not used to determine the next, so it's possible to execute several steps of the computation even if none of them return a value. Okay, let me try again: I'll use an extra Maybe in order to account for the fact that some steps may not return a value, but I won't incorporate it into my monad stack because I don't want those steps to abort my entire computation. Only a few more changes are required: {-# LANGUAGE DeriveFunctor #-} import Control.Applicative import Control.Monad.Trans.Class import Control.Monad.Trans.State data SmartValidate e a = SmartValidate { unSmartValidate :: StateT (Int, [e]) (Either [e]) (Maybe a) } -- 'a' was changed to 'Maybe a' deriving Functor logError :: e -&gt; SmartValidate e a -- this type has changed logError e = SmartValidate $ do (remainingErrors, es) &lt;- get let remainingErrors' = remainingErrors - 1 -- oops, my previous implementation used '+' let es' = es ++ [e] if remainingErrors' == 0 then lift $ Left es' else put (remainingErrors', es') return Nothing -- this line has changed instance Applicative (SmartValidate e) where pure = SmartValidate . pure . Just -- 'Just' was added SmartValidate mmf &lt;*&gt; SmartValidate mmx = SmartValidate mmy where mmy = liftA2 (&lt;*&gt;) mmf mmx -- this line is new runSmartValidate :: Int -&gt; SmartValidate e a -&gt; Either [e] a runSmartValidate maxErrors mx = case finalResult of Right (Just x, _) -&gt; Right x -- this line has changed Right (_, (_, es)) -&gt; Left es Left es -&gt; Left es where finalResult = runStateT (unSmartValidate mx) (maxErrors, []) &gt; The point of the `SmartValidation` type from my article is that it allows you both to collect errors (which your implementation does, too) and to abort without returning a value regardless of whether the limit is reached (which yours falls short of). I did notice that you implemented fatal errors, but since you did not mention them in the surrounding text, I assumed they were of secondary importance. In any case, since my monad stack incorporates Either, they are trivial to add. fatalError :: e -&gt; SmartValidate e a fatalError e = SmartValidate $ do (_, es) &lt;- get lift $ Left (e:es) Speaking of that method, I don't understand how your `fatal` achieves the goal of aborting the computation early. Its type says instead that it allows you to embed a SmartValidation computation inside an Either computation, so a closer match would be my `runSmartValidate`, not my `fatalError`.
That's confusing. And cuts out a lot of potential future features by stealing away the possibility of regular TH slices.
Oh that's awful - it actually cabal installs without asking me? I am very very careful never to cabal install anything not in a sandbox. It would destroy my work environment. Not hard to fix - just reinstall HP - but unless there is some other way to use this tool, it's definitely not for me.
Perhaps it's possible to pre-compile scripts and upload the executable. Or even create an interactive environment which does that in real time automatically.
&gt; Okay, let me try again: I'll use an extra Maybe in order to account for the fact that some steps may not return a value, but I won't incorporate it into my monad stack because I don't want those steps to abort my entire computation. Right, this should work. &gt; I did notice that you implemented fatal errors No, that's not what I meant. `fatal` is what it is: a way to convert from `SmartValidation` to `Either`. It's called so because Either's semantics wrt lefts is "fatal", compared to the "non-fatal" semantics of SmartValidation.
&gt; This is why co-monads are covariant functors, not contravariant ones. Fixing the vocabulary is an important step towards ***co***obfuscating why that is the case. FTFY
thanks for the kind words :)
Sorry, I'm a bit late to the party. We use [Halcyon](https://halcyon.sh/) to deploy [CircuitHub](https://circuithub.com) to Heroku. &gt; I guess everyone uses Cabal? For now yes, I'd like to look into using stackage-install for security reasons though. &gt; Are you able to reproduce your production environment locally? What do you use for that? We replicate our production environment to some extent by using Halcyon to install ghc + cabal locally and will probably look at using it to speed up full sandbox builds locally in the future. We're also looking into doing our builds on Digitial Ocean instead (still using Halcyon) since there seems to be more beefy options than Heroku's [PX dyno](https://devcenter.heroku.com/articles/dyno-size#available-dyno-sizes). &gt; Are you able to build and deploy with git push or similar? Is this desirable? Yes, usually `git push` + `heroku build -s PX`, although the later step will hopefully go away with [this enhancement](https://github.com/mietek/haskell-on-heroku/issues/49). In general, Halcyon works very well for us: * The author, /u/myetech, is very [responsive](https://github.com/mietek/halcyon/issues) * Lots of documentation for [Haskell on Heroku](https://haskellonheroku.com/tutorial/) as well as [Halcyon](https://halcyon.sh/tutorial/) * Workarounds for a couple of cabal-install [issues](https://github.com/haskell/cabal/issues?utf8=%E2%9C%93&amp;q=is%3Aissue+author%3Amietek+) * Useful for producing binaries quickly for lots of platforms (using Digital Ocean) / environments (e.g. CI). * Shared, [cached](https://halcyon.sh/#fast) sandboxes (e.g. perhaps you're running lots of microservices and want to share a large number of sandbox dependencies...) * Support for [private sandbox sources](https://halcyon.sh/reference/#halcyon_sandbox_sources). * Tested with [lots of web frameworks](https://halcyon.sh/shootout/#results) and [lots of real sites](https://halcyon.sh/examples/). 
Funny!
I found this blog post Google'ing for an update on Cabal-1.22 and Stackage, found [my own question "Why is Stackage stuck on Cabal-1.18?"](http://stackoverflow.com/questions/26544853/why-is-stackage-currently-stuck-on-cabal-1-18); understood that GHC 7.10 will probably no longer have the Cabal-1.18 dependency, and then figured it was probably time to move over to 7.10. Trying to confirm my assumption I found the article hereby posted to Reddit (could not remember seeing it). Now I'm left with these questions: * Will the upcoming Stackage nightly builds with GHC 7.10 ship with Cabal-1.22? * What prevents GHCJS from going into Stackage? (Wild guess: too experimental) * Can anyone share their experiences with 7.10, GHCJS and/or Yesod?
[**@mmachenry**](https://twitter.com/mmachenry/) &gt; [2015-04-21 18:12 UTC](https://twitter.com/mmachenry/status/590578833201434625) &gt; It's tricky to rock a rhyme at compile time. \#rundmc \#haskell [[Attached pic]](http://pbs.twimg.com/media/CDIoUaLWMAAUXmR.png) [[Imgur rehost]](http://i.imgur.com/CFSp3ke.png) ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://www.np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
AMP and BBP were expected to cause quite a bit of breakage, I assume.
&gt; excel &amp;#3232;_&amp;#3232;
There's [lookup](https://hackage.haskell.org/package/base-4.8.0.0/docs/Data-List.html#v:lookup) and possibly other useful things in Data.List.
That seems oddly familiar. ~~Although I can't find it, wasn't there a recent post about pretty much the same scenario, but~~ as a student job for ~100$/week? (I remembered the old post wrong, sorry for the inconvenience)
It was a quite different scenario.
many a pull request was sent :P
If you want to google more information about this representation, it's called an "association list". The term "hashmap" is specifically for maps which are implemented by hashing the keys into integers so that fast array indexing can be used as part of the implementation.
How are folks handling library docs on GHC 7.10? Because of [this bug](https://ghc.haskell.org/trac/ghc/ticket/10206) my docs on GHC 7.10 are useless. Even the [online docs](https://downloads.haskell.org/~ghc/latest/docs/html/libraries/) lack links to the source. As a result I have been sticking with 7.8.
and you can't steal backticks (well, you could). still, confused me too.
Sorry, it was just very recent.
I have updated the README to reflect you input.. I hope it helps you, thanks for the feedback! 
btw, do you know what keeps ghcjs from being on hackage like haste? does it use its own ghc or something?
I haven't had to use rewrite rules. The datatypes I use aren't gigantic though, nor are the functions.
I interpreted "I think it may have been a big deal in the past" as "it *used to be* a recommended practice, but now isn't", which seems to contradict the text that follows: "but many of the folks I know in the Java world do the same thing with standalone jars". It would seem one of two is likely: * You've mixed up dynamic vs static linking, or * My (possibly "our") interpretation of the first half of your sentence is wrong. By "I think it may have been a big deal in the past" do you, perhaps, mean that it used to be difficult, but now it's straightforward... or something else?
Oh don't get me wrong, I've been learning for longer than an hour, I was just trying to picture an image of how far I actually am when it comes to learning Haskell, which is close to halfway through LYAH. Don't I need to use Floats bec I'm using sqrt?
I see what you did there
Alright so I changed it to the following where numberList = numbersToCheck (floor (sqrt (fromIntegral x))) It compiles but I'm getting the following warning: http://i.imgur.com/eSR48Ls.jpg Is there something I can do about it or is that just a generic warning?
Instead of comparing to the square root... you can also compare the square of the number to the limit.
Nope. You can convert between types. See fromIntegral and round, floor, ceiling, toInteger. 
&gt; that fool be nice
I did more or less the same thing and chronicled it in a blog post: http://taylor.fausak.me/2014/10/21/building-a-json-rest-api-in-haskell/. More examples are always better, though! 
Thanks for organising this workshop. This is a great complement to the technical material! It is also quite clear that the FP community is not really shining with respect to diversity; so, we surely have much to learn.
Oh nice! Funnily enough I already planned on pinging you somewhere this week. I've had surgery a while ago, recovery etc. and did not have the mental bandwidth for much Haskell, hehe. But i'm finally starting to feel a bit better. Do you hang around on IRC somewhere, btw?
I was really enjoying the video before it got too out of sync. Thanks! I'll be waiting for the reupload!
The package has generalized recursion schemes, so you can handle more complex things that just foldr / scanl. I've never tried doing quicksort with it, but I think with the right choice of intermediate functor you could do a hylomorphism / metamorphism to get a "true" quicksort. Though, IIRC, modified mergesort is faster on lists. I think the readability would largely hinge on familiarity with generalized recursion schemes. I particular, fold-then-unfold is more immediately familiar than hylomorphism. Also, all of them are higher-order functions, and some people (myself included) don't immediately go there when thinking though a problem.
On Linux, glibc is dynamically linked into the executable, so you need a compatible version of glibc. This can be a pain in the a*** for deployment sometimes.
(I may be missing the point (if so, sorry), but... a jar runs on the JVM, so it's not really a binary, it's bytecode, since it needs to be compiled on the fly if it's not interpreted. Hence Java is designed to avoid many cross-platform compatability issues.)
I know you are doing this as an exercise, and that's great. But after your own efforts, you may want to see more information and advanced techniques for generating prime numbers in Haskell on the [wiki](https://wiki.haskell.org/Prime_numbers).
On the other hand, the squaring happens for every element between 1 and sqrt(n), whereas taking the square root happens only once (for a given call to numbersToCheck), so I suspect taking the square root is faster in the long run.
Oh. People don't say to link haskell programs statically for release because of binary size. Static linking makes binaries significantly larger, in fact. But it also makes them not require installing every haskell library they depend upon as a shared library on the target system. It's about simplifying release by bundling everything together. Size is irrelevant, as you say.