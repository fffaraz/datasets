I thought the solution would be similarly succinct as [this](https://wiki.haskell.org/Euler_problems/11_to_20#Problem_11) one.
Sure! The problem is that lenses (or van Laarhoven lenses anyway) are just ordinary functions, which means that the IsLabel instance in generic-lens-labels is defined for the “a -&gt; b” type, instead of some bespoke newtype. This is of course an orphan instance. So when is this a problem? There is at least one other use for IsLabel instances for “a -&gt; b”, which is the field selector of some extensible record libraries. Long story short, generic-lens-labels can’t be used together with certain extensible record libraries, because their selector instances clash. Using it in libraries is thus a bad idea (just like using extensible record libraries, for the same reason), but it should be OK in applications.
This looks weird, HasField should take 5 arguments - can you tell me which version are you using?
Visual Studio Code is great for Haskell. If you're using Stack, [Haskero](https://marketplace.visualstudio.com/items?itemName=Vans.haskero) is probably your best bet: It's stable, it uses Intero so it's more likely to work than some of the alternatives, and I found it to be more polished and more respectful of the usual VS Code ways of doing things than Haskelly. There are newer options that promise more than Haskero, but I haven't found them to be particularly reliable or easy to set up just yet. For graphical debugging, use [phoityne-vscode](https://marketplace.visualstudio.com/items?itemName=phoityne.phoityne-vscode). It works surprisingly well. For linting, [the hlint plugin that uses hlint &gt;= 2.0](https://marketplace.visualstudio.com/items?itemName=lunaryorn.hlint) is great. Finally, for formatting, use whatever you prefer. The [stylish-haskell](https://marketplace.visualstudio.com/items?itemName=vigoo.stylish-haskell) plugin always works well for me.
Yes, they are mutually exclusive.
where should I start, from the bottom up?
People love haskellbook.com , but it is very long and builds from basics.
You can check out the links given under the section of books, courses and tutorials on the [documentation page](https://www.haskell.org/documentation) and use stack overflow when ever you get stuck. Start writing programs :) 
I cannot recommend the haskell book highly enough, even as an experienced programmer. It is funny and incredibly well written, and crucially forces you to do short and well thought out exercises that ensures you understand what is going on every single step of the way before moving you on to more advanced topics!
Is it worth it for a beginner ?
This isn't the most helpful comment. This thread is almost a week old and this reply contains the same quote as [my sibling comment](https://np.reddit.com/r/haskell/comments/7yfdei/haskell_ecosystem_requests/duhmskh/?context=1), so the person you're responding to is already aware of it. What are you trying to communicate here? 
I am by no means an expert when it comes to `optparse-applicative`, but I think you might want to use `eitherReader` as described in [the "Option readers" section](https://github.com/pcapriotti/optparse-applicative/blob/0.14.1/README.md#option-readers) of the documentation. That way you can parse the string into a number, then validate that the number is within the range that you expect. 
ShellCheck is a wonderful tool! One thing I love about the project (that doesn't have anything to do with it being written in Haskell) is the excellent descriptions of the warnings and suggestions on [the wiki](https://github.com/koalaman/shellcheck/wiki/Home/8628a986887c97eb97a98668418d322647569fbd). 
thanks
Nice post! This is a good run-down of some of the base type classes and their methods. In a couple places, you (I'm assuming the OP is also the author) compare type signatures. For example, `fmap` and `traverse`. One way to make their commonality clearer is to align the interesting bits, like this: fmap :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b traverse :: (Traversable t, Applicative f) =&gt; (a -&gt; f b) -&gt; t a -&gt; f (t b) For a lot more of these aligned type signatures, check out Chris Martin's [Lined-up Haskell types](https://chris-martin.github.io/haskell-aligned/). 
I've asked similar questions on SO recently. You might find the answer useful: * https://stackoverflow.com/questions/48833162/programming-pattern-or-library-i-e-idiomatic-way-to-handle-cli-arguments-sema
&gt; `Grid` can actually be implemented as the composition of two comonads: `Env` and `Store`, which gives us the correct comonadic behaviour for free. For extra comonad-transforming goodness, such a 2D grid can also be implemented as the composition of two 1D comonad transformers. The 1D comonad transformer can in turn be implemented by composing `Env` and `Store`, but that's not the approach I chose in my [Game of Life implementation](https://github.com/gelisam/conway#readme).
Are summer internship positions available?
The most interesting application of comonads I saw is a two-dimensional comonadic parser-combinators: * https://www.reddit.com/r/haskell/comments/6hjsvf/requesting_feedback_on_2d_parser_combinator/
I have been playing around with different tree traversal algorithms for the last two days, in both Haskell and C. When you do the "classical" imperative pointer-chasing implementations, you muddle up the traversal strategy with the logic. On the other hand, Haskell teaches you to factor out the recursion. Here it is being done via `recursion-schemes`, but of course you can write your own traverse[Pre/In/Post/Lvl]Order :: Applicative f =&gt; (a -&gt; f b) -&gt; BTree a -&gt; f (BTree b) and create default implementations of the `Foldable` and `Functor` methods using `Const/Endo` and `Identity`. However, as the function signature suggests, some valuable information about the current position in the tree is lost, making it hard to, say, find the minimum element by only traversing the left branches, or build a list of all root-to-leaf paths.. You might come up with some ad-hoc "indexed traversal" or a Zipper instead, which is what I am reviewing currently. But I think I read that `cata` is strictly more powerful than `foldr`, and it seems the solution-space is even larger. Is there some unifying bird's eye view on "What does X buy us over Y", taking into account `Functor/Foldable/Traversable`, `recursion-schemes/Fix/Free`, Zippers and/or Lenses?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ChrisPenner/conway/.../**Conway.hs** (master → e83c7bf)](https://github.com/ChrisPenner/conway/blob/e83c7bff436cc5e14789a44f0deabb6412f1b6ce/src/Conway.hs) ---- 
Didn't know about this one, very useful. Of course, it shouldn't matter that it is written in Haskell... ... is what I would say if I was a little more mature. However, judging by how I have been laughing for two minutes about some web framework mentioned in the other thread, I am not. Yay Haskell.
In this case, `a` is a variable. Therefore, `Num a =&gt; a` must be read as "element of type `a`, such that `a` implements the typeclass `Num`". The reason for this is that numerals in Haskell are overloaded so that you can have `1 :: Int` and `1 :: Double`.
I use it for my dot files, it taught me a lot about Bash
Small typo: " binding between GTK3 and WxHaskell" you probably meant reactive-banana and WxHaskell.
Fixed, thanks.
"As for the choice of of the letter a exactly, this is decided by some compiler heuristic, it is not really important." that*s exactly my questions, dont seems heuristic, everty time you use :t, the result ist the same, seems defined, not heuristic.
My error, is ghc, not prelude. I understand why to pick a letter and all the meaning, but why THAT letter, where is defined in ghc. to return that result.
If it's explained at all it's probably only explained in the GHC source or something.
My guess is that `a`, `b`, etc is used for function application, `t` for "naked" values. I may be wrong, but I think that your second example, `(-4)` is **not** the constant -4 but the [unary minus](https://wiki.haskell.org/Unary_minus) function with 4 as its argument. As to where it's defined, I think you'll have to look in the GHCI code :)
If you play a little with `lens` you will get the stranger type variables you can imagine...
IIRC, it tries to use the same type variable names as the ones in the original type declaration. For number literals it probably gets the type variable name from the name used in `fromInteger` in the `Num` class.
`p` seems to be the default type variable name: &gt; :t \a -&gt; a \a -&gt; a :: p -&gt; p &gt; :t \a b -&gt; () \a b -&gt; () :: p1 -&gt; p2 -&gt; () &gt; :t \a b c -&gt; () \a b c -&gt; () :: p1 -&gt; p2 -&gt; p3 -&gt; () But if an operation which has a type signature is involved, it overwrites the default: &gt; :{ | myId :: myTypeVariable -&gt; myTypeVariable | myId x = x | :} &gt; :t \x -&gt; myId x \x -&gt; myId x :: myTypeVariable -&gt; myTypeVariable In your case, `(-4)` desugars to `negate 4`, `negate` has a type signature which uses `a`, so that's the letter which is used. &gt; :t negate 4 negate 4 :: Num a =&gt; a Interestingly, `4` is supposed to desugar to `fromInteger 4`, and `fromInteger` has a type signature which uses `a`: &gt; :t fromInteger 4 fromInteger 4 :: Num a =&gt; a And yet `:t 4` does _not_ use `fromInteger`'s type variable name! I suspect that different desugarings occur at different times, and that the type variable name is picked after `negate` gets desugared but before literals get desugared. Interestingly, not all literals seem to be desugared at the same time either: &gt; :set -XOverloadedStrings &gt; :t "foo" "foo" :: IsString p =&gt; p &gt; :set -XOverloadedLists &gt; :t [] [] :: GHC.Exts.IsList l =&gt; l
The letter chosen is completely unimportant and has no meaning whatsoever. Any symbol beginning with any lower case letter could have been used. Whichever one is used, it has exactly the same meaning: "any type". To understand the output of `:t` you don't need to pay any attention to which one GHC happens to have picked. I think that's what /u/guaraqe meant by "some compiler heuristc".
There is reactive-banana-gi-gtk bindings package.
I've been working along with this in GHCi and had some trouble with the Hbifunctor instance for Day, particularly in hbimap. I couldn't get the written definition to work, and it doesn't particularly make much sense taking everything as it is written, but it does if you simply remove the application of g to gxa in the definition of hbimap, and just leave it as f fx :&lt;**&gt;: gxa. 
Maintainer here. `ReadM` is a monad, so assuming you've written a function `validPort` so you can do something like port :: ReadM Port port = do i &lt;- auto case validPort i of True -&gt; return $ Port i False -&gt; readerError "Port isn't in the range 1025 - 65535" Alternatively, use `eitherReader` and parse and validate with something like Parsec.
You're right! Interesting. Prelude&gt; data T = T deriving Show Prelude&gt; instance Num T where negate = error "negate" Prelude&gt; -4 :: T *** Exception: negate 
There are some past threads on Reddit with related details. I would tell someone to build locally on their laptop Linux and copy the executable to a server with the same Linux, old school ssh/scp and shell scripts (no ansible, no docker). Then, when you want to get fancier, use nixpkgs on your preferred linux and build on the server. Then, get more elaborate from there.
Wait for a day or two and I will have an article up explaining how to approach this with Heroku.
Some day you are going to have to link back to this Reddit post's discussion from the following weeks Haskell weekly. I think if you link to the current week's Reddit post from Haskell weekly, you could tie the knot!
I’ve been covering the basics of operational semantics (Pierces TAPL book) and they make a lot of mention of axiomatic semantics. I know Monadic treatments of effects can make use of laws to derive effect implementations (see Gibbons and Hinze, just do it) and I’m interested in looking into the algebra of programming with algebraic effects but the literature is a bit impenetrable upon a first look. Does anyone have easier tutorials/links to papers the introduce deriving programs from axioms using algebraic effects? 
&gt; as IIRC negative number literals are actually just `negate positiveLiteral` unless `-XNegativeLiterals` is on, which it probably should be
I don't know TH well, but can't you ask for a `Q Pat` and work with patterns directly?
`stack image container` + anything that can do Docker
When using applicatives, is there any reason why people write `(pure f) &lt;*&gt; x &lt;*&gt; y &lt;*&gt; z` and not `(fmap f x) &lt;*&gt; y &lt;*&gt; z`?
People do write the first way quite a lot but they usually use infix fmap, `&lt;$&gt;`, instead of splat (`&lt;*&gt;`). Thus you frequently see `f &lt;$&gt; x &lt;*&gt; y &lt;*&gt; z`.
Yes, and that would be a much better API! Unfortunately you can't nest a `[p|...|]` inside a `[||...||]` (nor a `[|...|]`), so I can't simply write `$$(surjective [||\covers -&gt; ... covers [p|Just True|] (Just True)||]`. Perhaps I could write `$$(surjective $ \covers -&gt; [||... $(covers [p|Just True|]) (Just True)||]` thought?
For stack users, one way to install this RC is as follows: $ mkdir scratch &amp;&amp; cd scratch $ wget https://gist.githubusercontent.com/DanBurton/f33f67c23ef3846ca48c0abbbddff726/raw/bb72841368f3e14e3bd24dc1db21fa531a770d27/stack.yaml $ stack setup $ stack repl Here's the [stack.yaml gist](https://gist.github.com/DanBurton/f33f67c23ef3846ca48c0abbbddff726), and the [script that generated it](https://gist.github.com/DanBurton/9d5655f64ab5d5f2a588e6fb809481fc). Once installed this way, you can use `compiler: ghc-8.4.0.20180224` to try it out in any of your stackified projects. (Suggested: also use `compiler-check: exact`.)
Thanks! I'm happy to see that generating `stack.yaml`s for GHC release candidates has been automated :) 
I'd suggest dockerizing the application so you bake in all of your system dependencies at build time. This makes deployment quick and immune to external dependency providers downtime. AWS, GCP and Azure offer services to deploy docker apps rather painlessly. From personal experience, AWS Beanstalk provides some out the box devops goodies like monitoring, blue-green deployment, load balancing + auto scaling, alerting, config management, log collection. Once you outgrow beanstalk, you can start looking at AWS ECS + Fargate for more fine tuning but still retain the ability to _just_ focus on development and leaving the rest to AWS. See [post](https://www.reddit.com/r/haskell/comments/7yntqp/walkthrough_building_an_ultra_light_docker_image/) that goes more into how to setup stack+docker for a Haskell app.
Plus, Haskell is so different from most languages that honestly, even if you know how to program, you should probably still use a better book like the Haskell book (as opposed to something like the lyah book)
If you're using `takeWhile` and `dropWhile` with the same arguments then it's more efficient to use `break` and do both at once. split _ [] = [] split d s0 = let s1 = dropWhile (== ' ') s0 (a,(_:r)) = break (== d) s1 in a : split d r
 split :: Char -&gt; String -&gt; [String] split char string = undefined split ',' "one, two, three, four" ==["one","two","three","four"] 
That implementation does not work. Try calling `split ',' "foo"`.
If this is a practical question — as opposed to pedagogical — then checkout [`Data.List.Split.splitOn`](https://hackage.haskell.org/package/split-0.2.3.3/docs/Data-List-Split.html#v:splitOn)
Good catch! Second attempt: split _ [] = [] split d s0 = let s1 = dropWhile (== ' ') s0 in case break (== d) s1 of (a,(_:r)) -&gt; a : split d r (a,[]) -&gt; [a]
Not sure exactly what the OP wants with regards to spaces. Such as whether or not to drop them before the separator / after the separator / at the end / at the beginning. But yeah that looks good for the "drop spaces at the beginning and after each separator but not before each separator or at the end" spec.
It's like when you learn haskell, you lose the ability to communicate to the rest of is. So I think this is a great idea, and a good start, but I find it hard to follow. Last year, I taught myself FSharp. There is a wonderful website - https://fsharpforfunandprofit.com/. It took me about a week to learn FSharp well enough to write a stupid little shmup game using SDL2. I installed ghc about a week ago, and I'm still looking for a readable tutorial. I've been a dev for almost 40 yrs, and I don't remember seeing anything as confusing as the learning documentation being written for haskell. Then there is the Learn You a Haskell.. or Happy Learn Haskell - don't even get me started on those, they make me want to throw my laptop at the wall :)
Learn you a Haskell is a great book! I managed to convince my co-workers to learn Haskell, and began a "book-club" where we are reading http://book.realworldhaskell.org/. Also a great book. Both are freely available online ;)
Thanks for making them manually for so long!
I use Gitlab + Gitlab CI to Dockerize my application. On ther server side, I do not have automatic updates yet, but I would just do a `docker pull` and then start the new container. Eventually I want to have automatic updates as well, but not sure what to use for that.
The most basic concepts of set theory as used in day-to-day mathematics are needed for reading something like TAPL, but not much beyond that. A bit more "set theory" is useful if you want to pursue more advanced type theory research, but is easy enough to learn along the way. Studying a book on set theory should not be necessary for you at this point.
no just continue and look up definitions of things you are not familiar with (you don't really need much set theory to understand the material IMO) you will shave a lot of yaks (read books till you are grey) if you try to have a strong fundament on set-theory etc. (the rapid hole goes really deep)
&gt; rapid hole Not sure if this was just a typo, but the expression is "rabbit hole", and refers to Alice in Wonderland where Alice follows the white rabbit down a hole and arrives in an increasingly crazy world.
yeah it's a typo ... sorry
I always recommend How to Prove it by Velleman as a prerequisite for TAPL. It'll teach you more or less everything you need, and it should be approachable for motivated high school students and beyond. 
https://downloads.haskell.org/~ghc/master/users-guide/8.4.1-notes.html
Simplest - just use [keter](https://github.com/snoyberg/keter). It takes literally a few minutes to install keter on the server. After that, to deploy you type `yesod keter`, and your app deploys. Currently connected site users are automatically hot-swapped. If you use PostgeSQL, DBs are managed automatically. Pausing and restarting is just moving a single file from one folder to another on the server, and then back again. Multi-app tenancy is automatically supported. Launching a copy on another server is just `rsync` of a single file. If you're planning to stay pretty much on the same platform (e.g. Ubuntu), solutions like docker are way overkill. But if you do decide to go to docker later, it's easy to migrate.
Me.
Since nobody seems to like my `hackage-everything`, here’s a more useful module from the Everything™ universe. If you’re using this on a fresh system, I’d be interested in how long it takes to download a recent LTS release from scratch!
"alas" as in Balmer was good for programming languages? In what way?
Looking forward to it
Not native english speaker. You just found one of those words, I do not know their meaning, but just go with inferred one. Thank you.
You can also have a look at these lectures: http://www.cis.upenn.edu/~cis194/spring13/ . I found the assignments really helpful. In week 7 right now and its going pretty well so far. Also, join the Slack channel for Functional Programming. Developers are more than ready to clear doubts over there.
Okay. I'll be prepared the next time I have to travel with the Deutsche Bahn.
[Submit a ticket as usual](https://ghc.haskell.org/trac/ghc/newticket?type=bug) and select "Documentation bug" in the "Type of failure" field.
I may actually do this before my next long flight. Approximately how long does it take on your machine?
I hate slack, but I will join gladly! Which channel?
This draws a bit from the [Foldable](https://en.wikibooks.org/wiki/Haskell/Foldable) and [Traversable](https://en.wikibooks.org/wiki/Haskell/Traversable) chapters of the Wikibook, so it would be nice if it linked back to them. In the section about `toList`, it says: &gt; Since we know folding with `Foldable` operations will cause in some loss of information if the data structure is complex, using `toList` to implement folds make it possible to reconstruct the original structure. That's not really possible. In fact, `foldMap f = foldMap f . toList`, so `toList` is no less lossy than `foldMap`, `foldr`, etc. Consider what happens, for instance, when you to implement `Foldable` for a binary tree (see also: the exercise in the corresponding section of the Wikibook).
Did you still get paid in BTC or in USD? Obviously the former is worth 10x more now :)
I just timed it on a semi-clean system; expect a couple of minutes (2:20 in my case) and less than 100 MiB traffic.
#haskell-beginners and #haskell
Yea considering `toList` can be implemented in terms of `foldMap` or `foldr` (and in fact these are all isomorphic to each other), I'm not sure what the OP is trying to claim here really
Topology can be based on set theory, but it also can be based on something else or even taken as viable basis (cf. Homotopy type theory). There are a lot of mathematicians interested in alternatives to set theory. I won't detail things here. Check out "criticisms" under ZFC in Wikipedia. 
I've read both LYAH and RWH and didn't like either of them, I found I also didn't get much understanding out of them either. www.haskellbook.com was the first thing I read where everything started to click.
Oh - it only downloads them! Even better. Thought it would spend 100 hours compiling them all.
Thanks! Created: * https://ghc.haskell.org/trac/ghc/ticket/14857#ticket
Yeah that’s the main point! I should probably make that stand out more in the readme then
I have felt indeed set theory is implicitly used as the basis of some branches of mathematics. I have read A Book of Abstract Algebra and in the first few introductory chapters set theory is used to describe what a group is. I understand your point and would just start on TAPL now. Thank you!
One tidbit: there are packages like [pipes-concurrency](http://hackage.haskell.org/package/pipes-concurrency) and [stm-conduit](http://hackage.haskell.org/package/stm-conduit) that bridge streaming libraries and STM channels. The [async](http://hackage.haskell.org/package/async) library is very useful for concurrency.
Thanks for the link! The first few chapters of this book seem very relevant to type theory. 
The holistic picture is very instructive! Would you mind giving an introductory link to probabilistic logic? 
Probably you will be arrested in the check-in since the dangerous amount of energy consumed in the compilation could suppose a security threat for the flight
Happy to help! Also look at the MGS notes and defo practice with eg Agda/Coq 
I think I've seen the first way in the documentation for Applicative
I recommend checking out my `foldl` library, which let's you define composable and efficient aggregations. Check out this talk: https://youtu.be/6a5Ti0r8Q2s .. and the matching slides: https://github.com/Gabriel439/slides/blob/master/munihac/foldmap.md
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Gabriel439/slides/.../**foldmap.md** (master → 34dea09)](https://github.com/Gabriel439/slides/blob/34dea099ba6df0784594ae82ad8e9f0e44c710e5/munihac/foldmap.md) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply duv6tqr.)
It doesn't mention intuitionistic logic, but does you just enough classical logic and set theory to study any book that mentions “doesn't have any prerequisites, but requires mathematical maturity” in the preface. It also makes you fearless at proving theorems. (I personally loved [*Book of Proof*](http://www.people.vcu.edu/~rhammack/BookOfProof/), but Velleman's *How To Prove It* is easily the best book of the genre).
I wonder how fast the download would be on a 300 baud acoustic coupler? How things have changed since 1974! :-)
Do you have any more details on the work environment? What is it like working for SC?
Yeah, from the inside, it's interesting that once you have `pure` and `&lt;*&gt;`, suddenly `fmap` is redundant. This leads to a nice normal form for applicative expressions, where one can basically pull out the expression structure into a pure function in the front, and then supply the arguments linearly after that. It's an important concept when it comes to understanding the expressiveness of Applicative. Things like this sometimes distort the view from a library maintainer's perspective, so that one doesn't see how saying everything in this way is actually pretty bad in terms of communication.
I'd go a bit further, and say that non-trivial Haskell programming involves thinking *more* mathematically than programming in general. This goes pretty deep. * Haskell is remarkably close to standard mathematical notation. Up to a few minor differences (like disambiguating the meaning of juxtaposition, single vs double colons in function signatures, etc.) you can write a surprising amount of math directly and trust it to be good Haskell syntax. * The semantics of Haskell are similar to mathematics. This is just the well-known point that the meanings of "function" and "variable" are all wrong in imperative languages. And from a semantics standpoint, impure functional languages may as well be imperative. * Most importantly, Haskell just has a knack for letting you work in some of the same idioms and tricks you can in mathematics. You can work directly with infinite sequences. You can treat functions as objects in their own right. You can apply equational reasoning. You can define new operators. You can start your definitions with a structure and set of axioms over values (and it's common to do so in type classes). And these things aren't done artificially to teach you those specific ideas; instead, they come up *naturally* in the course of solving problems. I'd go as far as to say that learning Haskell well teaches you a kind of mathematics that math departments do NOT teach well. Rarely do you see a math department take seriously the subject of *communicating* *formally*. This is something students are expected to pick up on their own, as a side effect of learning basic analysis and algebra and just being asked to do it. It's a rough transition, and many students don't make it through. Programming, though, is less about finding the clever techniques or deep connections, and more about just learning to communicate everyday ideas in formal notation. I believe it's a perfect lead-in to learning more serious mathematics.
I'm honestly likely to use this because I'm in Australia more than anything to do with planes. Thank you!
[This package](https://i.imgur.com/9gaSC.gif)
[removed]
Thank you Tekmo, engaging talk!
[removed]
!RemindMe 
**Defaulted to one day.** I will be messaging you on [**2018-02-27 21:18:28 UTC**](http://www.wolframalpha.com/input/?i=2018-02-27 21:18:28 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/haskell/comments/807qix/yesod_best_path_to_deploy_web_app_for_novices/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/haskell/comments/807qix/yesod_best_path_to_deploy_web_app_for_novices/]%0A%0ARemindMe! ) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
Users of chocolatey can get this using &gt; choco install ghc --version 8.4.1-rc1 --pre or upgrade from the alphas using &gt; choco upgrade ghc --version 8.4.1-rc1 --pre
I still find it mildly irritating that `stack new` requires internet access.
Not to pick up on this discussion again, but I was sorta waiting for the full clip to surface before commenting on this. The one you linked is HEAVILY edited. You can check out an extended one https://www.youtube.com/watch?v=T5ahmzySUB8. I already suspected this, because Peterson is excellent at discussing while keeping calm, and those replies in your clip were way to quick and not thought out. I'd recommend checking out the full clip, because his views on women are not wrong, unless you are deliberately trying to twist his words.
Oh, it's no fun unless you're trying to download it on the plane about to treasure off tethered to your phone. 
I found https://chocolatey.org and this looks very interesting. Is there some tutorial for setting up a Haskell development environment with chocolatey? Can Stack be installed via chocolatey?
Great work! I feel like this is such a comment question/problem. It'll be nice to have a quality video like this to point to as an answer. 
Brief summary: I managed to find a way to write a function holes :: Traversable t =&gt; t a -&gt; t (a, a -&gt; t a) similar to `lens`'s [`holesOf`](http://hackage.haskell.org/package/lens-4.16/docs/Control-Lens-Traversal.html#v:holesOf), but that produces a container of contexts instead of a list of them. Unlike `holesOf`, it works just fine with containers that extend infinitely to the left and is as efficient for left-leaning containers as for right-leaning ones.
I mean he literally claims that the purpose of makeup is to be sexually provocative. And that it is "self evident" that this is the case. That's false on the face of it -- no amount of editing alters that. The longer video is just as wrong, just at greater and more exhausting length. (edit: he's wearing a tie in the interview. is that a sexual display? his collar is buttoned down and neat. is that a sexual display? his hair is combed. is that a sexual display? the claim that a woman _in particular_ doing something socially normative with regards to a presentable appearance is innately sexual is in itself deeply sexist.)
Right. Most of the time it’s no problem but it would be great if it could fall back to a cached one.
&gt; a trip through the German countryside by train I actually quite enjoy that. But not so much as chewing on a Haskell problem, so count me in.
I don't know when exactly it changed, but at least Stack 1.6.5 supports local templates with `stack new`. This is an excerpt from the help text: &gt; TEMPLATE_NAME: Name of a template or a local template in a file or a URL. For example: `foo` or `foo.hsfiles` or `~/foo` or `https://example.com/foo.hsfiles`.
I’m new to programming and Haskell. I recently picked up the book Haskell Programming written by Christopher Allen &amp; Julie Moronuki. And I’m absolutely stuck on Beta Reduction..... is there anyone that could *dumb it down* for me? I’m really interested in learning. And unfortunately, Google is of no use. 
[removed]
There is this one about suffix tree implementation with pure functional languages. It used Haskell. Don't quite remember the title nor the authors but if you search for the keywords you'll find it. It's the only one relating suffix tree and Haskell. Some set theory base is required I believe. I used it as a reference when implementing my own basic suffix tree library but since it used a really old version of Haskell I used a less efficient method for the tree construction
Here's a way to use category theory and datatypes to do linear time sorting: http://chrispenner.ca/posts/representable-discrimination
May be off center from what you’re requesting but you find this interesting: Chris Okasaki’s [Purely Functional Data Structure](https://www.cs.cmu.edu/~rwh/theses/okasaki.pdf)
Expanding the points doesn't make them more correct. Serious social scientists don't take Peterson seriously. He's a well-spoken right-wing troll, and while I don't want to get into an argument here, there's plenty of research that shows that a certain degree of makeup is considered part of professional attire, and that people who do not play along face consequences. (And certainly, people widely believe this: https://jezebel.com/5972605/when-is-wearing-makeup-a-choice). In this respect it is also akin to a uniform. Peterson, in the guise of "facts," perpetuates double-standards towards the detriment of women. I was hoping that what seems to me a blatantly clear example of this would sway you. But since you're digging in defending it, I'll leave it at that.
Maybe something along these lines? [Lazy Depth-First Search and Linear Graph Algorithms in Haskell](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.3876)
This seems really interesting! I'll have to wrap my head around Representable and Distributive stuff first to really get it, it seems.
Is this it? http://www.scs.stanford.edu/14sp-cs240h/projects/isaacs_geiduscheck.pdf
I stumbled upon this paper earlier today, actually. Looks like there's a lot of interesting stuff there.
There's a also a book published by Cambridge University Press, same author and title.
This might help a little with that! http://chrispenner.ca/posts/representable-cofree-zippers
Guy Blelloch and Bob Harper (both of CMU) have co-written some really nice papers in this area. Here are a couple I've enjoyed in particular: http://www.cs.cmu.edu/~rwh/papers/farray/popl17.pdf http://www.cs.cmu.edu/~rwh/papers/iolambda-cacm/cacm.pdf
I'm rather curious whether this is already in a library somewhere, and if not, whether it should be added to one. `lens` seems likely to be the right place, but I don't know what sort of type it should be massaged to in order to fit into that package.
It would be nice if it would automatically cache things like that on the computer and then attempt to use those; ideally, after I've "built up" a cache of packages, I shouldn't ever really need internet access to program; needing it to download a 50kB template file every time seems silly.
What a garbage clickbaty title.
Neat. Any uses of this?
[removed]
Thanks! Yeah, I hope I can provide more of those videos. :)
Due to garbage collection, Haskell's run time system cannot guarantee any fixed response time. As such, it is unfortunately not directly suitable for (hard) real-time programming. This is not to say Haskell cannot be useful as tooling for developing real-time systems: using Haskell to compile DSLs of restricted languages that don't need GC can be a very effective path to meeting real-time goals.
[removed]
Update: Rampion has come up with a [much better implementation](https://stackoverflow.com/a/49001904/1477667) based on the same idea, but with a beautifully tied knot. It appears to get a tremendous amount of sharing among the results.
[removed]
We the haskellers are too busy playing with abstract lists. No time for doing something useful
Yes, that makes sense for Agda, where containers are inherently finite and fully defined and runtime performance (usually) doesn't matter. In Haskell, however, the situation is completely different, so the representation in terms of vectors is unsatisfactory.
[removed]
Another angle that might be useful is the pitfalls of lazy functional design. [The Genuine Sieve of Eratosthenes](https://www.cs.hmc.edu/~oneill/papers/Sieve-JFP.pdf) shows how what was once thought of as an implementation of the sieve is actually trial devision in disguise.
[Pearls of functional algorithm design](http://www.cambridge.org/gb/academic/subjects/computer-science/programming-languages-and-applied-logic/pearls-functional-algorithm-design?format=HB&amp;isbn=9780521513388#JXQLWK1RD4yz4QU3.97) is pretty nice. It derives a more complex, but better performing solution from a simple, ineffecient solution, by using equational reasoning. 
Don't ask me! I just saw the type of `holesOf` and thought it looked wrong.
fmap is from Functor so maybe Applicative docs try to stick to Applicative interface?
Both `holes` and `holesList` are implemented in [fixplate](http://hackage.haskell.org/package/fixplate-0.1.7/docs/Data-Generics-Fixplate-Open.html), with exactly the same names and type signatures. I would guess they won't work for containers that extend infinitely to the left though.
I wouldn't agree with that. Applicative is a subclass of Functor so everything that is part of Functor is certainly part of Applicative 
I would really like to donate on patreon, but as a student I can't really afford it. Great work and keep it up!
Totally understandable! :) You can always help out by spreading the message, sharing it with others that might be interested.
I wish people didn't call it linear. To be more precise, there's no known way to sort N machine words in O(N) word operations, with the constant factor independent of word size. Category theory doesn't help. Sometime ago I tried to find the right bound for discrimination-based sorting (which works for sorting integers, strings, etc) in terms of word operations, and it came out superlinear w.r.t. input size in words. It would be cool if someone could rederive it on their own and we'd check.
The team is now large enough that there's no single description of the environment. The NYC role in particular is very front-office facing, so it's based on the trading floor, having continuous interaction with traders, quickly diagnosing problems, coming up with specifications, and developing solutions.
This is one of those slightly irritating things in Haskell where the types kind of get in the way and the functions to deal with them are verbose. The problem is that the `length` function returns an `Int` and to do division you want something like a `Float` or a `Double`. The function to convert from an `Int` into one of those is `fromIntegral`. So what you want to do is something like `fromIntegral (length likes) / fromIntegral (length dislikes + length likes)`. There's also the `genericLength` function that gives a more general result so replacing the `length` with that will probably also work.
Or just avoid `Double` and instead use exact ratios from [`Data.Ratio`](https://www.stackage.org/haddock/lts-10.7/base-4.10.1.0/Data-Ratio.html)
Sort of. But there is something to it in that many of the rewrite rules that should be permitted from category theory can't used in Haskell precisely because they don't actually hold. That being said, when I write Haskell, I just don't do anything silly, use categorical structures as if their laws held, and everything tends to work pretty well. Unfortunately, some of the silliness has made its way into learning materials, like deliberately conflating inductive data with coinductive data, so for example, `length :: Foldable t =&gt; f a -&gt; Int` isn't actually defined on stuff like `[1..]` despite `[1..] :: List Int` and `List a` being `Foldable`.
I've started a Haskell course and I've this assignment where they ask me to: "Define a data type which takes and Integer between 0 and 255". As I understood I can define "types" like this: `data MyType = Num Int` The problem is that I don't understand how to "limit" the number between 0 and 255. Any tips?
Ha! Funny that we made up the same names. Guess I'd better ask them if they want a new (non-Report-Haskell) implementation of `holes`.
I believe it is essentially the same content for a much higher price.
"Them" is me. By "non-Report-Haskell" you mean that your implementation (haven't had time to check it out) requires language extensions? I'm afraid the rest of that library doesn't play well with such funny containers either, so I'm not sure it's worth it. What I should definitely do is to understand and describe the exact limitations better.
Would e nice to use the `Validation` applicative instead of the `Either` applicative. Then you can find all the validation errors in one go :)
I'm gonna need some learning before I fully understand how this one works :)
If you want to learn how functional programming really works and how functional programmers think in the most pure sense of the word functional, you can't do better than this book. However, it's definitely not something you study through hoping to actually learn specific and concrete things you can use to improve your code. It's the journey you're after here, not the answer.
&gt; But Haskell follows Unix philosophy - do each little thing and do it well. The rest can be composed. Quite correct :) another way to think about it is that since data and functions are separate in functional programming (as opposed to OOP), the notion of manipulating data can be separated out into libraries that work on very generic data which you then massage into a type that represents your problem domain. It leads to very powerful and abstract single purpose libraries that glue together well. If you're feeling up to it, I'd encourage you to get nerd sniped by looking at a library such as streamly. That one (and one other whose name escapes me at the moment) expose the raw Stream type and operate solely on that. Rather than being ugly, I find it truer and more elegant than most other streaming approaches. It also feels much more functional and Haskell to me than large and invasive libraries like conduit/pipes (although I confess I don't have much real world experience to back this up). However, learning that library would give you a very solid understanding of streams themselves which would help quite a bit with any other streaming library. Data streaming as a programming model is also very enlightening to learn.
As I noted in an SO comment, /u/rampion's implementation is also much better even for `Data.Map`. But yes, the implementation uses a GADT, and I don't see an obvious way to avoid that.
Yes, agreed, and especially if those errors would end up in a user interface. :+1:
What does it mean for a type to "take an Integer between 0 and 255"? Does it mean the type can only represent integers in this range? Does it mean the only types you can construct are in this range? If you want to only represent types in that range then consider using `Word8` from `Data.Word`. Since it is unsigned and 8 bits then that type is already exactly what you need - it can represent integrals between 0 and 255. If you want a wrapper around the `Integer` type, such as `data Foo = FooC Integer` but only allow construction of values in the correct range then you are looking for what is called "smart constructors". By making a module with your data type (`Foo` above) and NOT exporting the constructor (`FooC`) then you can control what values become instantiated by only exporting a function that will enforce invariants such as `buildFoo int = if inRange int then Just (FooC int) else Nothing`.
So I kept playing. `Mag a b` is not a law-abiding functor, but can be made so by adding a parameter to `One`: ```haskell data Mag a b c where One :: (b -&gt; c) -&gt; a -&gt; Mag a b c Pure :: c -&gt; Mag a b c Ap :: Mag a b (c -&gt; d) -&gt; Mag a b c -&gt; Mag a b d instance Functor (Mag a b) where fmap f (One g a) = One (f . g) a fmap f (Pure c) = Pure (f c) fmap f (Ap mh mx) = Ap (fmap (f .) mh) mx instance Applicative (Mag a b) where pure = Pure Pure f &lt;*&gt; mx = fmap f mx mf &lt;*&gt; Pure x = fmap ($x) mf mf &lt;*&gt; mx = Ap mf mx holes :: forall t a. Traversable t =&gt; t a -&gt; t (a, a -&gt; t a) holes = \t -&gt; let m :: Mag a b (t b) m = traverse (One id) t in fst $ go id m m where go :: (x -&gt; y) -&gt; Mag a (a, a -&gt; y) z -&gt; Mag a a x -&gt; (z, x) go f (One g a) (One h _) = (g (a, f . h), h a) go _ (Pure z) (Pure x) = (z, x) go f (Ap mg mi) (Ap mh mj) = let ~(g, h) = go (f . ($j)) mg mh ~(i, j) = go (f . h ) mi mj in (g i, h j) go _ _ _ = error "only called with same value twice, constructors must match" ``` Going further along this path, it seems possible to define a non-recursive `Mag a b c` ```haskell data Mag a b c where One :: (Tree shape b -&gt; c) -&gt; Tree shape a -&gt; Mag a b c Pure :: c -&gt; Mag a b c data Tree shape a where Leaf :: a -&gt; Tree Identity a Branch :: Tree i a -&gt; Tree j a -&gt; Tree (Product i j) a instance Functor (Mag a b) where fmap f (One g a) = One (f . g) a fmap f (Pure c) = Pure (f c) instance Applicative (Mag a b) where pure = Pure Pure f &lt;*&gt; mx = fmap f mx mf &lt;*&gt; Pure x = fmap ($x) mf One g ia &lt;*&gt; One h ja = One (\(Branch ib jb) -&gt; g ib (h jb)) (Branch ia ja) ``` and we can annotate a tree easily enough with its own context: ``` context :: (Tree shape a -&gt; b) -&gt; Tree shape a -&gt; Tree shape (a, a -&gt; b) context f (Leaf a) = Leaf (a, f . Leaf) context f (Branch x y) = Branch (context (f . flip Branch y) x) (context (f . Branch x) y) ``` But I'm stuck trying to figure out how to get a `forall b. Tree i b -&gt; t b` function out of a `One` constructor: ``` mag :: a -&gt; Mag a b b mag = One (\(Leaf b) -&gt; b) . Leaf holes :: forall t a. Traversable t =&gt; t a -&gt; t (a, a -&gt; t a) holes = \ta -&gt; go (traverse mag ta) where go :: (forall b. Mag a b (t b)) -&gt; t (a, a -&gt; t a) go (Pure z) = z go (One g ta) = g (context g ta) -- this doesn't work ```
You should also check out Roman's solution. I haven't looked into its performance yet, but it looks very clean.
One day, continuations won't tie my brain in knots. One day...
It can avoid a `GADTs`, but not `ExistentialQuantification`, which I suspect is sufficiently non-Report.
Amazing, thank you.
Several years after that, I may attain the same level of enlightenment. By the way, how could I entice you to come to the Haskell DC meetup? I'm not sure if I'm going to be in town for the next one, but I think you'd make a great contribution.
Roman's solution doesn't seem nearly as efficient as yours, unfortunately. I tested each with &gt; import qualified Data.Map.Strict as M &gt; (M.map (($ 100) . snd) $ holes $ M.fromAscList (zip [1..10000] [1,10..])) `seq` () Your implementation succeeds in 0.17s, allocating 50MB. Roman's falls on its face with that (it allocates 83MB for a map a tenth that size). I suspect Roman's also has a space leak resulting from the use of `fmap` to separate a `t a` out from `t (a, a -&gt; t a)`. These problems might be inherent to avoiding the ugly parallel pattern matching of our solutions.
Why can't templates be compiled into the executable itself? It's just a few files, hardly more than a few kbytes.
The law-abiding `Mag` isn't lazy enough. Consider data Never a = Never (Never a) instance Traversable Never where traverse f (Never n) = Never &lt;$&gt; traverse f n We want `holes (fix Never)` to be `fix Never`, but with the law-abiding `Mag`, we'd end up with bottom.
Ah yes, you can avoid GADTs by adding a function to `One`. What was I thinking?
Something in the related to compilers arena - https://en.m.wikipedia.org/wiki/Graph_rewriting ? 
It's weird that `length` returns an `Int`. What's wrong with: class Foldable t where length :: Num b =&gt; t a -&gt; b instance Foldable [] where length = foldl' (const . (+ 1)) 0
High frequency trading is doable in a GC'd language like Haskell (I've worked on such client projects), but it depends on which level of trading. If milliseconds matter then Haskell can handle it (GC behaves consistently if your workload is known, and usually it is known within a range e.g. 10 million messages per second for example). If nanoseconds matter then Haskell is at the wrong level and you're better off with DSLs or other languages.
Hey. i came across a few more problems. Im trying to define a function that takes genererEchantillon :: Etat -&gt; Double -&gt; [Double] genererEchantillon (LesEtats note octave volume duree) tempo = (map( (volume/10)*sin(laFrequence*360)*)([0, (60*duree / ((1 / 44100)*tempo))..((1/ 44100)*tempo / 60)]) ) where laFrequence = genererFrequence (LesEtats note octave volume duree) 
in case you want to reward the author for good work :)
In that case what's wrong with: class Foldable t where ... length :: Num b =&gt; t a -&gt; b length = fromIntegral . foldl' (const . (+ 1)) (0 :: Int) And then types where overflow isn't completely impossible can override the above to my original suggestion. Also I assume my original suggestion would perform fine if and when you do use `Int`. So you could also just define `fastLength = fromIntegral . length @_ @Int`, and tell people to use that whenever they have a container of size less than 2^29 and want to get the `length` in a non-`Int` type but currently `length` is too slow.
map needs toctake a function Double -&gt; [Double]. The function genererOnde needs one argument (etat) to turn in such a function. So, (genererOnde etat) is the first argument for map. You, instead, did map genererOnde (etat tempo) which actually, because the way application works is: (map genererOnde) (etat tempo) which is not what you want. Give the correct parenthesis and you should be good.
I really like Parallel and Concurrent Haskell: https://www.youtube.com/playlist?list=PLbgaMIhjbmEm_51-HWv9BQUXcmHYtl4sw By Bartosz Milewski
I dont use windows, just wanted to say that I love brick! Thanks for your effort! Also, I'm looking to draw terminal plots (with different colours). My plots are vertical boxplots that are combindes horizontally (like a stock chart). I tried building Vty Images but they don't seem to perform well (I built the plot vertically and then horizontally, because it makes sense from a logical kind of view). Is there a library to draw colourful images, canvases or charts?
&gt; I dont use windows, just wanted to say that I love brick! Thanks for your effort! You're welcome. :) As for what you're trying to do, are you building Vty images *directly* and then using e.g. Brick's `raw` function to embed them in the UI? Have you tried using Brick's rendering cache to avoid rebuilding such images when the underlying state hasn't changed? If you want to take this up in more detail, feel free to contact me directly. I'm not sure if there is a library to do what you want, but a couple of other projects do come to mind: * https://github.com/weeezes/plot * https://github.com/jimenezrick/termplot
To answer this question lets first look at what the type signature of map is: map :: (a -&gt; b) -&gt; [a] -&gt; [b] From this we can intuitively guess that map takes a function and applies that function to each element of a list producing a new list with values that are the return type of the input function in this case b. Now that we have a high level idea of what map does lets look at how it defined: map :: (a -&gt; b) -&gt; [a] -&gt; [b] map f [] = [] map f (x:xs) = f x : map f xs As you can see it is a fairly straightforward definition, the biggest thing to notice is that f is applied to each element of the list and that result is combined (using (:) which can be pronounced as cons) with the recursive call on the rest of the list. This is what we would expect given the previous intuitions we gained from the type signature. Now in your case we need to specialize the type signature and replace the as and bs with what they represent in your use of map. As you said you are trying to map over a list that looks like [0, (60duree / ((1 / 44100)*tempo))..((1/ 44100)*tempo / 60)]. I assume from your type signatures that it is a list of doubles so a = [Double]. Thus we have a specialized map so far that looks like this: map :: (Double -&gt; b) -&gt; [Double] -&gt; b where I simply replaced a with Double. You mention the function sinus I am not sure what type it is but as long as it follows the structure (Double -&gt; b) it will work when given to map. In the example code you gave there are a few problems the first being that last (*) in ( (volume/10)*sin(laFrequence*360)*) is only being applied to one argument this leads me to believe that you want its other argument to be the elements in the list?? If so an easier way to define that is not point free (not naming variables) is to write: (\x -&gt; ((volume/10) * sin (laFrequence * 360)) * x) As you can see this is a function of one argument, a Double, that returns a Double so it fits the mold (Double -&gt; b). This should then work provided that the list you're mapping over is in fact Doubles and that you want to return that list transformed by the the above function. One last note is that every element in a list must be the same type so make sure that each element in the list has the type you think it does as that could be causing errors. Let me know if this makes sense and I would encourage you to play around with map before trying to rewrite your function so you get a feeling for how it works!
So i've come up with a different solution, here it is: genererOnde :: [Etat] -&gt; Int -&gt;[[Double]] genererOnde [] _ = [] genererOnde (LesEtats note octave volume duree : xs) tempo = (map ((volume/10)*) [sin(laFrequence*360*0),sin(laFrequence* 360* (60*duree / ((1 / 44100)*tempo)))..sin(laFrequence* 360*((1/ 44100)*tempo / 60))]) : genererOnde xs tempo where laFrequence = genererFrequence (LesEtats note octave volume duree) 
Here's another gist where I've developed the idea further. For polymorphic recursion and other weird types it may be possible to get away with a few well chosen instances. This incurs some overhead but still avoids having to explicitly pattern-match on the whole type. https://gist.github.com/Lysxia/6405198e85adb6b32c4b40a9c335b6d0#file-functor-hs-L157
Minor nitpick: using `foldMap (Sum 1)` is probably better than `foldl' (const . (+1)) 0` for tree-like structures.
Ok, so what we want to do, is basically a takeWhile, the rest of the stuff that wasn't in the takeWhile result, and somehow call that on itself a bunch, right? `span f` is basically a tuple with `takeWhile f` as the first element, and the rest of the list as the second. That sounds like a really solid place to start. Using `fmap (drop 1)` on the result tuple from `span` drops the separator so that it doesn't show up in the list of results. Combing the two gives us the basic operation we want to keep repeating, which I'll call `split'.` `split' n = fmap (drop 1) . span (/= n)` Now you need to figure out how to build a list using those results. The input is `[Char]`, and we want `[[Char]]` as our output, and we need a `Char` to know what to split on, so, that gives us our type signature. split :: Char -&gt; String -&gt; [String] The return of our `split'` function, specialized to `[Char]`, is `([Char],[Char])`. We'll want both 'ends' of that to use in our function, because we need the first half to build our result list, and the second half to recurse on. So we can use de-structuring assignment... split x ys = let (l,r) = split' x ys in (???) OK, Now we've got all the ingredients. The first thing we should probably establish is how to tell when we're done, and what we should do in that case. Putting that up front helps make sure we don't accidentally get ourselves in an infinite loop. So, we'll know we are done when there is no more string left for us to look at - So, when `r` is empty. We can use `null` to check for that case. This gives us: split x ys = let (l,r) = split' x ys in if null r split x ys = let (l,r) = split' x ys in if null r then (???) else (???) Well, if `r` is empty, we still know that `l` is a string, and we want a list of strings as our output. Ok, that's easy - split :: Char -&gt; String -&gt; [String] split x ys = let (l,r) = split' x ys in if null r then [l] else (???) OK, now we need to figure out what we're really doing here. We know we need to slap `l` together with some result of an operation on `r`, and we know we need a `[String]` as a result. If we have a `String`, and we want a `[String]`, that's probably going to involve `:` in some fashion, so, that solves the first part of this problem for us... split :: Char -&gt; String -&gt; [String] split x ys = let (l,r) = split' x ys in if null r then [l] else l : (???) Ok, well, now we have a `String` that needs to get prepended to a `[String]`. We know that the `[String]` needs to get built from our `r` variable, because that's the rest of what we're processing, and also it's the only other thing in scope for us to work with anyway. So we need a function of `String -&gt; [String]`... That looks a lot like the function we're making right now, right? Didn't we just define the operation we wanted to loop? Well, heck, lets just do that! We can just cheat and call ourselves again... split :: Char -&gt; String -&gt; [String] split x ys = let (l,r) = split' x ys in if null r then [l] else l : split x r And there we go, that's a functional implementation of `split`, along with basically how we try to think about structuring recursive algorithms, and how the type system can help us answer questions while we work.
Nope. Here is an example: https://en.wikipedia.org/wiki/Intuitionistic_type_theory 
On reflection this reminds me of a special case of a comonadic fix. I wonder if kwf’s “quick fix” machinery can’t handle this...
I was researching information related to some unexpected performance problems when using `TVars` and stumbled on this. It appears to be part of a doctoral candidate's work and has a related paper: https://www.cs.rochester.edu/u/ryates/files/HASKELL2016.pdf. I haven't been able to find any indication why this wasn't included into GHC, especially if they implemented the patches for the paper anyways! 
Let me know if you think of something.
You can remove all "dead ends" from the map. Firstly remove all vertices that don't have any edges. Secondly find any vertex that only has one edge, remove that vertex and then remove the other side of that edge from the graph. Keep doing this until you have either an empty map (which means the graph is acyclic) or all vertices have more than one edge (the graph is cyclic). This is one way of doing it. 
TStruct sounds weirdly similar to compact regions, except compact regions are inherently immutable.
Yep :-). "having a bit of fun and trying things" basically summarises why do *anything* at all! Haskellers do however have a good aptitude for applying some of these academic-seeming adventures in Real World applications. e.g. `lens`, `recursion-schemes`, finger trees, CPS encodings, etc.
Works for me with: % ghci GHCi, version 8.2.2: http://www.haskell.org/ghc/ :? for help Loaded GHCi configuration from /home/ogre/.ghci λ&gt; :set -XKindSignatures -XRankNTypes -XTypeOperators λ&gt; type f ~&gt; g = forall a. f a -&gt; g a λ&gt; :{ Prelude| class HBifunctor (h :: (* -&gt; *) -&gt; (* -&gt; *) -&gt; (* -&gt; *)) where Prelude| bfmap :: Functor g =&gt; (a -&gt; b) -&gt; h f g a -&gt; h f g b Prelude| hbimap :: (f ~&gt; f') -&gt; (g ~&gt; g') -&gt; (h f g ~&gt; h f' g') Prelude| :} λ&gt; :i HBifunctor class HBifunctor (h :: (* -&gt; *) -&gt; (* -&gt; *) -&gt; * -&gt; *) where bfmap :: Functor g =&gt; (a -&gt; b) -&gt; h f g a -&gt; h f g b hbimap :: (f ~&gt; f') -&gt; (g ~&gt; g') -&gt; h f g ~&gt; h f' g' {-# MINIMAL bfmap, hbimap #-} -- Defined at &lt;interactive&gt;:4:1 λ&gt; :set -XGADTs λ&gt; data Day f g a where (:&lt;**&gt;:) :: f x -&gt; g (x -&gt; a) -&gt; Day f g a λ&gt; :{ Prelude| instance Functor g =&gt; Functor (Day f g) where Prelude| fmap h (fx :&lt;**&gt;: gxa) = fx :&lt;**&gt;: fmap (h .) gxa Prelude| :} λ&gt; :{ Prelude| instance HBifunctor Day where Prelude| bfmap = fmap Prelude| hbimap f g (fx :&lt;**&gt;: gxa) = f fx :&lt;**&gt;: g gxa Prelude| :} λ&gt; :i Day type role Day representational representational nominal data Day (f :: * -&gt; *) (g :: * -&gt; *) a where (:&lt;**&gt;:) :: (f x) -&gt; (g (x -&gt; a)) -&gt; Day f g a -- Defined at &lt;interactive&gt;:10:1 instance [safe] HBifunctor Day -- Defined at &lt;interactive&gt;:16:10 instance [safe] Functor g =&gt; Functor (Day f g) -- Defined at &lt;interactive&gt;:12:10 What error do you get?
There is some history with Bazel actually supporting Haskell and being used internally at Google. I don't want to incorrectly paraphrase the reason it was removed, maybe Judah Jacobson or one of the other original contributors could chime in.
Not from point of view of a person that write documentation for Applicative. Even if functor capabilities are equivalent, using Applicative names helps introduce it to "student".
BTW what performance problems did you notice?
Wouldn’t that fail for graphs in shape of `8`? I guess that depends on your definition of cycle. `8` just isn’t a simple cycle.
Exactly. Not that the vertices all form a cycle, which seemed to be what OP was after, maybe I misunderstood.
Op's example with three vertices is ambiguous. Could be either. If you're correct then finding a hamiltonian path is NP compete. OP should use brute force as that would have the lowest overhead.
Interesting, what number of TVars do you create, roughly? I seem to remember we did some testing a few years back with tens of millions (?) of TVars and performance degradation was noticeable, but I might be misremembering.
Very cool! &gt; Having both FE and BE in Haskell is awesome. There have been many occasions where I have moved the code (by copy-paste) between the three projects. Are you duplicating code between the three projects, or moving the code to the common package? &gt; So haskey is a new library and definitely not yet ready for production. Are there other production ready Haskell DB libraries, preferably integrated with persistent? Sometimes I don't need/want to rely on an external DB. 
&gt; Are you duplicating code between the three projects, or moving the code to the common package? I can't really speak for him, but hopefully that just means moving it to the common package. :)
Moving the code and reusing it.. I should have written cut-paste instead of copy-paste...
In that case, the first algorithm works. Slice your subset from the original graph, then remove all dead end vertices until there are no more dead ends. If the result is empty it is acyclic, if it is non empty, it is cyclic. Also keep in mind that if your graph subset has edges equal to or greater than the number of vertices, it automatically contains a cycle, and you don't have to check any further.
&gt; When opening the app, about 10% of the time there are some strange errors / assertion in reflex runtime like “Causality loop found”. On doing a refresh it mostly works fine. If you're getting this message, it is almost always an indication of a bug where you have an Event (could be the updated part of a Dynamic) whose firing depends in a circular-definition sort of way on its own firing. These can be tricky to track down, particularly when they only happen conditionally. It sounds like there's some sort of race condition on the startup of your application which sometimes triggers this to occur. I'd have to be more familiar with your application's code to really say where to look, but watch out for the 'prompt' versions of things like `switchPromptlyDyn` and `tagPromptlyDyn`, and question whether you really need them, or if you can get away with things which only depend on the Behavior part of the Dynamic like `switch . current` or `tag . current` etc. By only relying on the value of the Dynamic at times previous to the current moment, operations which only use the Behavior part tend to break cycles like that. 
This is [issue 3850](https://github.com/commercialhaskell/stack/issues/3850). It's tagged 'awaiting PR' and 'newcomer friendly'. What better opportunity ;-)
What happens is is that when I use `mapConcurrently` in combination with `resource-pool` (built on top of TVars), I notice a major performance degradation as I increase the number of capabilities. Basically, I'm trying to [execute a bunch of mysql queries](github.com/xldenis/mittens) simultaneously through a connection pool. It _seems_ like the problem is related to the number of threads waiting on a TVar
We looked at multiple existing systems (pants, Buck, nix, …) including Shake. Neil's conclusion is pretty much exactly it: the buy-in would be a bit too much. Existing rules for multiple languages was a major reason why bazel was chosen.
4 PM EST happens when this comment is 5 hours and 9 minutes old. You can find the live countdown here: https://countle.com/148161ok3K --- I'm a bot, if you want to send feedback, please comment below or send a PM.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/amaaggregator] [At 4 PM EST today, two of the main contributors to the Haskell web3 library will be livestreaming an AMA. Join to ask them anything.](https://www.reddit.com/r/AMAAggregator/comments/80x147/at_4_pm_est_today_two_of_the_main_contributors_to/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
4 PM EST happens when this comment is 5 hours and 4 minutes old. You can find the live countdown here: https://countle.com/dqy9148172 --- I'm a bot, if you want to send feedback, please comment below or send a PM.
&gt; I'm a bot, if you want to send feedback, please comment below or send a PM. You've already commented on this story. No need for a second comment. Thanks!
Given that Nix is already popular in the Haskell ecosystem and it's also polyglot, why not build the monorepo with that instead of Bazel? 
I just realized that it's possible for two unrelated bots to trigger each other and cause a loop. Hah.
Haha yeah, pretty interesting. It happened in a couple other of the crossposts for this.
Assuming your edges are directed, this isn’t right. For example, take the graph on {1,2,...,N} with an edge from i to j whenever i &lt; j.
Someone (you?) asked this question with the exact same wording on [Hacker News](https://news.ycombinator.com/item?id=16483187). The answer by /u/bennofs is pretty much spot on I think: &gt; I am a Nix user, but one advantage that Bazel has is that it can do more fine-grained incremental rebuilding (nix as commonly used can only do per-project rebuilds). Also while there is some overlap, Nix usually relies on language-specific package managers to perform the actual building, whereas bazel handles that itself. Nix is more of a packaging tool, whereas bazel is a build tool. Only thing I'd add is that Nix's evaluation model is less suited for a build system, where aggressively avoiding recompiling anything that doesn't need to is key to good performance. In Nix, the store path changes any time any of the inputs change, so any downstream dependencies also need to be rebuilt completely. Whereas in Bazel, changes to source files that didn't lead to a change in the interface files don't trigger a rebuild of everything downstream. That said, Nix is great to provision upstream published dependencies that seldom change. As the blog post says at the end - at the moment we use Nix to pull in published versions of packages (system libraries, Hackage packages etc) and Bazel to build the monorepo code.
Thanks! I also estimated that haskell and GTK wouldn't scale if an application got larger and for that reason looked at FRP. Didn't manage to figure out how to do it though, so I'm very interested in your approach. Unfortunately too far into my current program to move to FRP by now 😢 but maybe in a future project!
That's interesting. About 18 months ago I hypothesised about the existence of such a think. I concluded that you'd very quickly invent monads to handle sequencing IO effects in a comprehensible manner. http://h2.jaguarpaw.co.uk/posts/impure-lazy-language/
another awesome project from tweag.io
The answers to this question listed here seem to presume some knowledge of the Haskell ecosystem. I'm going to do my best to answer it assuming you have almost no familiarity with the language. Haskell can do state, it can do performance, and it can do IO. Achieving (reasonable) performance or IO in Haskell is rarely clumsy enough to be a problem for the intermediate -&gt; advanced Haskeller. State is very frequently a problem that can just be architected right out by thinking of the issue a different way. In the cases where actual state is necessary, or desirable, we have tools in language for dealing with adjusting logic statefully, and using them isn't generally terribly clumsy. When we talk about 'purity' or a lack of 'side effects' what we mean is that our type system draws a clear line telling us what our code is 'allowed' to do effectfully. That's different than '100% referential transparency forever' - What it means in practice is that we have clear, hard, easily visible lines between code that isn't referentially transparent, and code that is. On top of that, we have powerful tools to 'glue' pure code to impure code. That means that, if we have problems of the 'weird state' or 'IO failure' variety, we can pinpoint what code is causing those problems really quickly, because it's been essentially quarantined. We have two reasons, generally, for interfacing with another language or ecosystem - Compatibility with an existing solution for a complex problem domain that we don't feel the desire/need to re-implement - OS APIs, GUI frameworks, legacy codebases, etc. Extreme performance issues - Strong need for realtime (in the order of nanoseconds) processing - Really, really intense number crunching. Those things don't really come up in app code for all problem domains, so it's more common to see a portion of the app 'hook into' another language in some domains than others, but in general it's vastly more common to hook into a Haskell library that has an interface to another language's solution to a problem than it is to directly 'hook into' imperative code from your core business logic for a particular application.
Technically it's possible for one bot to post and trigger itself too. :)
Is there some surgical procedure that members of the Haskell community use that just lets you take Edward Kmett's brain and plop it into your skull for a few days? Like, seriously, how does anyone learn this? I am beyond mystified that anyone is able to pick apart any of these libraries and actually use them to accomplish something.
That's interesting. About 18 months ago I hypothesised about the existence of such a thing. I concluded that you'd very quickly invent monads to handle sequencing IO effects in a comprehensible manner. http://h2.jaguarpaw.co.uk/posts/impure-lazy-language/
I like seeing people actively working on this problem. I still hope it eventually gets solved by a richer nix integration with each language specific tool, and some standard ways to for language specific tools to coordinate with nix. Whatever comes of tools like bazel and buck will help move things forward regardless.
I'm usually pretty happy with the paradigm of `nix-shell --run 'incrementalBuildTool'`, where `nix-shell` provides the dev environment and hopefully all of your external dependencies . We've been running into the need for incremental builds in deployments a little bit at work, so we've been considering using something akin to `nix-shell --run 'cabal build'` as our main deployment build, just to make it incremental. Seems like it'll work pretty well. I'm not at all optimistic about ever making Nix itself support incremental builds.
For (2), [cassava](http://hackage.haskell.org/package/cassava) is probably your best bet, although a simple parser with one of the parser packages might also work.
HTML Parsing: https://www.stackage.org/haddock/lts-10.7/lucid-2.9.9/Lucid.html#v:toHtml CSV Handling: https://www.stackage.org/lts-10.7/package/cassava-0.5.1.0
Ok cool. Done. 
As a general note that might be useful: there doesn't seem to exist the equivalent of "pythonic" in the Haskell world. For example, there usually are quite a few different ways to implement something. I've had plenty of analysis paralysis trying to implement various things haskelically (haskelishly?), wouldn't recommend. --- For some review: [25](https://github.com/JonathanReeve/corpus-db/blob/master/src/Main.hs#L25): avoid stringly typing - consider `data Env = Prod | Dev` and `mkEnv :: String -&gt; Maybe Env`. [37](https://github.com/JonathanReeve/corpus-db/blob/master/src/Main.hs#L37): dunno about the SQL libs, but AFAIK there's no need to "require" instances for concrete types. As I understand, the typeclass constraints are used for parametric polymorphism ("generics"), so `IConnection conn` should be enough. Although this approach might be somehow useful with the dangerous `IncoherentInstances` or such. [64](https://github.com/JonathanReeve/corpus-db/blob/master/src/Main.hs#L64): try [`fmap`](http://hackage.haskell.org/package/base-4.10.1.0/docs/Prelude.html#v:fmap) [76](https://github.com/JonathanReeve/corpus-db/blob/master/src/Main.hs#L76): consider [`fromMaybe`](http://hackage.haskell.org/package/base-4.10.1.0/docs/Data-Maybe.html#v:fromMaybe) with `fmap` You might like [`LambdaCase`](https://www.schoolofhaskell.com/school/to-infinity-and-beyond/pick-of-the-week/guide-to-ghc-extensions/basic-syntax-extensions#lambdacase) for the hard to refactor `f x = case x of {...}` (turning into `f = \case {...}`).
Typing this on my phone here so sorry if I'm a bit short :) A few points: * Nice job so far! The first rule is always to get it working, so don't worry too much about being non-idiomatic until after you understand everything pretty well. * In Haskell, any expression can be pulled out and treated as a value. What I mean by that is if you have any code where you say the same thing twice, you can pull that out into it's own function and replace the duplicate code with that. Honestly, sometimes I almost think of it in terms of text substitution. So for your getX functions... Any time you see yourself writing multiple functions with similar names, try writing a very general version of that function and then making the others special cases of it. Example: ``` getOddsInList [] = [] getOddsInList (x:xs) = if odd x then x : getOddsInList xs else getOddsinList xs getEvensInList [] = [] getEvensInList (x:xs) = if even x then x : getEvensInList xs else getEvensList xs (Imagine more functions of the form getX in list) ``` Hmm... Looks like you can basically copy and paste that and then change just one thing. In Haskell, what we'd just do is write something like ``` getXInList _ [] = [] getXInList p (x:xs) = if p x then x : getXInList xs else getXInlist xs ``` And then p can be something that tells you whether or not you want that element. Turns out, that's the filter function in Haskell. (And you can go one more level. The whole idea of traversing a list and applying something to every single element is another pattern we can abstract out) In your code, the getXbyY functions can definitely be abstracted out like this. Your SQL to text, and your filter out by fields functions are also secretly the same function waiting to be abstracted out :) As far as other idiomatic things: * People tend to use newtypes a lot for things and try to avoid string as much as possible. Newtypes over text will be good to look into using. * Qualified imports can make some of your code cleaner and shorter * There are more terse ways to handle maybe, depending on how you want to do it. * The environment is almost always handled in Haskell by using the reader pattern, iirc * Instead of making your own toJson functions, see if you can use some of the to/from json typeclasses. I have to go or I'd elaborate more on those points, but hopefully someone else will comment on them :)
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [JonathanReeve/corpus-db/.../**Main.hs#L37** (master → 46155ce)](https://github.com/JonathanReeve/corpus-db/blob/46155ce9d71d76de22e044fc867412a585f2e5b3/src/Main.hs#L37) * [JonathanReeve/corpus-db/.../**Main.hs#L64** (master → 46155ce)](https://github.com/JonathanReeve/corpus-db/blob/46155ce9d71d76de22e044fc867412a585f2e5b3/src/Main.hs#L64) * [JonathanReeve/corpus-db/.../**Main.hs#L25** (master → 46155ce)](https://github.com/JonathanReeve/corpus-db/blob/46155ce9d71d76de22e044fc867412a585f2e5b3/src/Main.hs#L25) * [JonathanReeve/corpus-db/.../**Main.hs#L76** (master → 46155ce)](https://github.com/JonathanReeve/corpus-db/blob/46155ce9d71d76de22e044fc867412a585f2e5b3/src/Main.hs#L76) ---- 
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://github.com/JonathanReeve/corpus-db/blob/master/src/Main.hs#L25) - Previous text "25" [Here is link number 2](https://github.com/JonathanReeve/corpus-db/blob/master/src/Main.hs#L37) - Previous text "37" [Here is link number 3](https://github.com/JonathanReeve/corpus-db/blob/master/src/Main.hs#L64) - Previous text "64" [Here is link number 4](https://github.com/JonathanReeve/corpus-db/blob/master/src/Main.hs#L76) - Previous text "76" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20duzb4uv) 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [JonathanReeve/corpus-db/.../**Main.hs#L37** (master → 46155ce)](https://github.com/JonathanReeve/corpus-db/blob/46155ce9d71d76de22e044fc867412a585f2e5b3/src/Main.hs#L37) * [JonathanReeve/corpus-db/.../**Main.hs#L64** (master → 46155ce)](https://github.com/JonathanReeve/corpus-db/blob/46155ce9d71d76de22e044fc867412a585f2e5b3/src/Main.hs#L64) * [JonathanReeve/corpus-db/.../**Main.hs#L25** (master → 46155ce)](https://github.com/JonathanReeve/corpus-db/blob/46155ce9d71d76de22e044fc867412a585f2e5b3/src/Main.hs#L25) * [JonathanReeve/corpus-db/.../**Main.hs#L76** (master → 46155ce)](https://github.com/JonathanReeve/corpus-db/blob/46155ce9d71d76de22e044fc867412a585f2e5b3/src/Main.hs#L76) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply duzb50i.)
Haha, I remember the only question I got wrong on my CS 61A[1] final was about what happened when you evaluated a lazy Scheme program with effects. On the one hand, it showed I didn't quite have an operational grasp of lazy programs; on the other, I was annoyed because I thought *effects and laziness were just something you would never mix*. I was already a Haskeller at heart :). [1]: Our first intro CS class, based on *The Structure and Interpretation of Computer Programs*
This is war!
Yes! I have some web app episodes in my backlog, one with validation would be nice. Thanks!
The assumption was that they were undirected, but in that case you could use dfs. I just choose the above algorithm because you don't need to keep track of which vertices you have visited. I came up with a slow but working solution in only a few lines of code.
[As promised.](http://fosskers.ca/blog/deploying-haskell-en.html) (English version)
Thank you
My bad, looks like the images don't render in that version. If you view it from [the main page](http://fosskers.ca), they do.
I recently used [scalpel](https://hackage.haskell.org/package/scalpel) for a project, and I would recommend it. It basically works like Parsec but on HTML, and it can also fetch the html from a url, no fuss.
&gt; 76: consider fromMaybe with fmap fromMaybe z (fmap f x) == maybe z f x `maybe` is a nice function :)
I’m sorta a noob too ... haven’t looked at your code but maybe run hlint against it to see what it says? Don’t have to do everything it says but it gives ideas at least.
Perhaps. The hermiticity and reproducibility goes a long way to be used in a remote execution system like https://github.com/bazelbuild/bazel-buildfarm
*smacks head* Dangit, I copied down the definition of hbimap wrong. Thank you for this, even though my mistake was so simple. 
I would imagine its inefficiencies in the io manager or elsewhere and not the tvars that are at fault here...
Nice, given you are so new. Definitely don't worry about idiomatic, as others have noted. Consider focusing on one library, like only HDBC + HDBC.Sqlite3 or only Scotty, then work up to using them in combination. You will have a lot of Haskell concepts to keep getting used to, so having a smaller surface area of libraries to have to understand at the same time might make the experience less painful. 
You can do this with nix as well.
Ah, I somehow missed that in the OP. Right you are! For content then, I'll point out that you *only* need to track vertex and edge counts, plus a connected component count, to determine if an undirected graph is loop-free.
For all of the getXInList examples, why not just use filter? That is, why bother writing getEvensInList when you could write filter (\x -&gt; x % 2 == 0) xs instead? I would find making use of Prelude functions more idiomatic.
The point was to demonstrate recognizing patterns and abstracting them out, not so much to specifically rewrite a list function into filter :)
Right, and filter has already abstracted that pattern out even further.
 filter even
Are you referring to package dependencies or do you mean on a per-module level?
Other people have mentioned cassava, if you're dealing with a lot of data you'll also want cassava-conduit.
By the way, I just found a much nicer implementation of `HolesList`. It seems to need `RankNTypes`, so I don't know if you're interested, but if you are, feel free to use it: https://gist.github.com/treeowl/789d43a641eff65083f724fc56d28234
In fact, definitely use filter on the guts, but I like to use specialised functions named semantically to make really readable / self-documenting code. 
On a package level. There’s no way to distribute builds on a module level with any tooling afaik, and I suspect the payoff wouldn’t be worth it, typically.
Yes, but recognizing that pattern and abstracting over it is more important than saying "Oh, there's a library function for that." Also, if they didn't see the pattern in the first place, they wouldn't have thought to look for that library function anyway.
Kinda off topic, but a new build tool by a prominent Scala dev looks pretty interesting. It probably won't be the next big thing, but he has one of the best blogs I've ever read about what a build tool should achieve and how it should achieve it. http://www.lihaoyi.com/post/BuildToolsasPureFunctionalPrograms.html
Nix doesn't do caching and deduplication based on hashes of outputs. It's a showstopper for an incremental build system. The Nix's intensional model would solve it, but it was never finalized.
This one looks nice, thanks!
Lucid does no parsing, quite the contrary.
I also recommend cassava - and if you are dealing with any messy CSV data (which is inevitable) I highly recommend using [Data.Csv.Incremental](https://hackage.haskell.org/package/cassava-0.5.1.0/docs/Data-Csv-Incremental.html) With a little tweaking of the example in the docs you can skip over the lines that contain the annoying title lines and data definitions that people insist in putting in CSVs.
Weird, I've had `toHtml` "do the right thing" on `Text` values before.
I'm confused. What /u/Tysonzero wrote *is* `length` generalised to `Num`. https://hackage.haskell.org/package/base-4.10.1.0/docs/src/Data.Foldable.html#length How can it be less efficient than length in the presence of specialisation?
Nice one, thanks!
I find it more idiomatic to write functions like `sqlToText`, `filterOutFields` and `textToJson` with the case expression outside the function body via guards. sqlToText Nothing = Nothing sqlToText (Just sqlPairList) = ... filterOutFields Nothing = Nothing filterOutFields (Just sqlPairList) = ... textToJson Nothing = "" textToJson (Just pairList) = ... And instead of erroring out, I would have those checks return an Either type. Since those errors can only happen at startup it isn't a big deal in this case.
Just a minor refactoring. You can in general replace functions of this form (in your case sqlToText and filterOutFields) ``` func :: Maybe a -&gt; Maybe b func mayA = case mayA of Nothing -&gt; Nothing Just a -&gt; Just $ someOtherFunctionFromAToB a -- Usage u = func v ``` with a non-Maybe version and use fmap at the use site: ``` func :: a -&gt; b func a = someOtherFunctionFromAToB a -- Usage u = fmap func v -- OR use (&lt;$&gt;) = fmap u' = func &lt;$&gt; v ```
hmm - I downloaded your file and tried this: module Main where main :: IO () main = do raw &lt;- readFile "input09.txt" let [x, n] = words $ head $ lines raw print $ (read x :: Integer, read n :: Integer) and it works fine here - can you try this? My best guess is that you run into some buffer problems when pipe it in - so you play with `hSetBuffering stdin NoBuffering` too - see https://hackage.haskell.org/package/base-4.10.1.0/docs/System-IO.html#v:hSetBuffering
Can you elaborate more on **"by providing financing and governance primitives at the core in addition to the code and collaboration tools provided by GitHub or Gitlab"** This could mean a lot of things.
I think that solution is related to callCC: mkJump :: MonadCont m =&gt; a -&gt; m (a, a -&gt; m b) mkJump a = callCC $ \curCont -&gt; let jmp a' = curCont (a', jmp) in return (a, jmp) forLoop :: Monad m =&gt; t -&gt; (t -&gt; t) -&gt; (t -&gt; Bool) -&gt; (t -&gt; m a) -&gt; m () forLoop i0 step pred body = flip runContT return $ do (i, jmp) &lt;- mkJump i0 lift (body i) if pred i then jmp (step i) else return () main :: IO () main = forLoop 0 (+1) (&lt;10) print Though I think the callCC version is uglier: -- without the `map fst $ unCont` bit we end up with `t (a, a -&gt; Cont r b` captureCont a = callCC $ \curCont -&gt; return (a, fix $ \f a' -&gt; map fst $ unCont $ curCont (a', f)) holes = unCont . traverse captureCont unCont = flip runCont id
It will be a major paradigm shift. The tooling will also require a bit of orientation (though not as much of a shift as it would coming from, say, .net). The learning curve on the simpler aspects of the language will be easier (i.e., no advanced pragmas; just glossing over "functor" and "monad" until the rest of the code makes sense. As far as an opportunity goes, with your background, a sabbatical in Haskell will with out a doubt make you a better programmer in php and js. So even if you do it for six or nine months lightly, there isn't much of a downside beyond the opportunity cost. 
Just do not give up! It takes some time.
Just as a note, sum can cause a stackoverflow if your list is large. Using foldl' (+) 0 will sum in constant space. See: https://gist.github.com/TravisWhitaker/25d9e39c5b3fe938e243116b88589bcc Also Integer is an arbitrarily large integer and is not as speedy as Int. 
I have 2 questions: **1.** I have a datatype with many fields where all the fields are monoids: data Foo = Foo [Text] (Map Text Int).... I can initiate a empty value manually: emptyFoo :: Foo emptyFoo = Foo mempty mempty... However this gets a bit boring since every time I add or remove a field I also have to change `emptyFoo`. Is there a way to automatically create `emptyFoo` based on the knowledge that all the fields can be initiated with `mempty`? **2.** I have a record where the type variable has kind `(* -&gt; *) -&gt; *`: data Bar f = Bar { bar1 :: f String, bar2 :: f Int } I can derive instances with standalone deriving when I specify `f`: deriving instance Show (Bar Identity) deriving instance Show (Bar Maybe) However I would like to derive show for any `f` where `Show (f a)` holds. I tried the following, but neither works: deriving instance forall a. Show (f a) =&gt; Show (Bar f) deriving instance Show1 f =&gt; Show (Bar f) Is this possible?
 &gt;&gt;&gt; :browse GHC.Prim data GHC.Prim.Word# GHC.Prim.plusWord# :: Word# -&gt; Word# -&gt; Word# GHC.Prim.govermentPlusWord# :: Word# -&gt; Word# -&gt; Word# -- note: somewhat more bureaucratic than plusWord#
Seldom does a typo sound cooler than the intended word, but I think you've managed to do it.
It's not bad, albeit, i am casually learning at my own pace. With what I've learned so far over the past few months and the small personal projects I've done, i can honestly say that i like Haskell better than C and Python. The biggest mindfuck will be how simple some things are in comparison, once you get into monads, you will see the real potential of it. Are you using a UNIX/UNIX-like operating system?
I will bravely shoulder that burden for you. Just tell me where they give Haskell jobs to PHP devs ;) (/joke but I would take such job offer myself) If know procedural PHP/JS, you will like super powers of Haskell switch. It not only work on ANY type of data, but it can also do destructing on the fly, including such perls as checking weather array have at least n, elements in it. If you like OOP PHP/JS, you will love small cost of abstraction and precision in Haskell. Have to deal with lot's of ID's of type integer? No longer. As in Haskell with just single line you can "duplicate" each logical type, and Haskell will keep track of who's who on the fly. If you murdered at least one core PHP dev for implementing type coercion, you will love Haskell for keeping it's hands far away from your data ;) As for learning curve. It's definitively there. But if you used immutable data structures, functional programming libraries (ramda), map/filter/reduce, etc. Then you already know basics, and some of JS libs are inspired by Haskell anyway. PS. If you will work alone, I would suggest Haskell from first principles as very good introduction, that do not assume any prior experience with FP. 
That's a solid advice, I'd just like to note that this is called pattern matching, not guards :) Also if we're already talking about `sqlToText`, I think the [`maybe`](https://hackage.haskell.org/package/base-4.10.1.0/docs/Data-Maybe.html#v:maybe) function can come in handy here
Very nicely done! Plus the documentation and the simple code seems to be a good resource to study for beginners like myself! :-)
&gt; Proof-of-stake consensus like cardano sl
From time to time, it will feel frustrating because you will know how to solve the problem in JS but not in Haskell. You will miss the awesome community of webdev, not only for its size but also for its warmth. That said, you should have fun and get better at programming. Basic tips- Every function in Haskell is like a js function that returns immediately. You can't do any steps between call and return (but the expression you return can be as complicated as needed). Don't read too much into the formalities of monads- the fact of the matter is that the monadic properties aren't important to you in most cases. The useful bit is that they are variant types- a union type with an alias. Read the reason documentation on variants and look up implementations of monads in JS . Examples are better presented there than in any Haskell resource I've seen. Gl, hf! 
 The typo adds a syllable is surely microwavable and mostly oscillatable the crunchy documentatino ^*available ^in ^the ^freezer ^section
It's a nice article, that would be easier to read with better formatting. - In LaTeX, long identifiers like "time" are better rendered as `\textnormal{time}` or `\textrm{time}`. There are also commands for common operators like `\max` or `\lfloor` and `\rfloor`. - Avoid long lines of code, use shorter identifiers (maybe with the full names in comments). - Have code blocks indented, that separates them more visibly from prose. Use `&lt;code&gt;` blocks instead of `&lt;em&gt;`.
Thanks for the compliment and the tips! When I get home I'll certainly apply many of these changes! With respect to identifiers, I tend to prefer the long ones; I find the short ones are often cryptic. I wonder if there is some general preference on this matter. What do you think?
I tried tackling a problem similar to this with OAuth2 once. I didn't end up completing anything, but it looked like the best solution was using a lot of type-level programming (e.g. (associated) type/data families) to have different data structures for different identity providers. The problem was figuring out how to unify the different flows that weren't all exactly spec-compliant such that there was still some uniform interface across IdPs.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [simonmar/ghc-proposals/.../**0000-mutable-fields.rst** (mutable-fields → 20c1ee2)](https://github.com/simonmar/ghc-proposals/blob/20c1ee2963fd34169ff4ab8348904bcea3e2dd24/proposals/0000-mutable-fields.rst) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dv0pwk3.)
&gt; that this is called pattern matching, not guards Whoops, you're right. I just call any expression that comes before equal as a guard.
This is awesome, thanks for sharing this.
1. Yes, you can do this with generics (`GHC.Generics`, for example). I can try to write up how to do it later, but I don't know if I'll have time. 2. See `Data.Functor.Classes`. You probably want to give `f` a `Show1` constraint.
1. `emptyFoo = gmemptydefault`from Generic.Deriving.Monoid solved it. Thanks for the pointer. :) 2. That is not a good solution for me since I've got multiple records with many fields. Then I would rather just write a deriving line per `f`. Tbh its not that important since Im only using `Identity` and `Maybe` so 2 lines per instance is not too bad. It just seem to me like it should be possible but I cant figure out how. 
In particular you get an `O(n^2)` for reads where `n` is the number of distinct `TVar`s read in a transaction. There are lots of things we could do to improve that, but potentially at the cost of slowing down transactions with a small `n`. Our `TStruct` does improve this in some situations at the cost of maybe a little overhead and a potential increase in false conflicts. These are all trade-offs we need to explore more.
It can't cause a stackoverflow because Haskell does not have a bounded stack.
Depends on the definition of "reasonable". If you define it in terms of "things a haskell programmer apply" then you are correct. If you define it as "a concept that is as understandable to a beginner as possible" then you would be wrong. Why? Because the use of a library function is yet a new thing to learn (however trivial it might seem to you now). First, OP should get used to the basic concept of abstracting functions. When that is second-nature, he can write more readable code by using library functions.
Hmm. I sent an email to libaries@haskell.org like the link said.
neat!
Yea, I link to the project above. You can run it _fairly_ easily (it's not totally automated yet) 1. `docker-compose up -d` 2. `./setup_bench.sh` 3. `stack exec mittens -- --source-host 0.0.0.0 --source-user root --dest-host 0.0.0.0 --source-port 5000 --dest-user root --dest-port 3000 --source-database=sbtest --dest-database=sbtest` 4. `./bench.sh` When you do that you can easily see that `-N1` is faster than `-N` or `-NX`
My primary language prior to Haskell, and still my 'day job' language, was/is JS. It's a very different way of doing things, but as long as you're careful not fall down rabbit holes, it's not as hard to learn as it's made out to be. Biggest hurdles I had to cross conceptually: First one: When someone says 'is a monad' or 'is a functor' what they mean is 'is a concrete type that implements X typeclass interface'. Implementers of an interface, or to use the Haskell nomenclature, 'instances of a class' don't have to share code behaviors, they share code concepts. There are concepts in other languages that are like this too, but in Haskell, this concept is absolutely 100% ubiquitous, and you cannot get real work done without some intuition for it. Abstraction in Haskell is about the common 'form' of functions, which is represented by the type signature. This takes a lot of squinting cross-eyed at code before it starts to connect in a way that matters. Second one: Some of this abstract math stuff is utterly fascinating, but generally you don't need to actually understand any of it to get work done. What you're doing in Haskell when trying to wire concepts together is more like connecting AV equipment converters. You're playing 'type tetris' and trying to get input and output to line up. 95% of the time, if you have some half of an idea what you're trying to accomplish, and the types line up, it just works. The other 5% of the time, only push it if you really want to learn a specific concept. Otherwise, just try to find a simpler way to get where you're going. Simpler is often faster and easier for other people to understand anyway. Third one: Recursion, recursion, recursion everywhere, all the time. This was the easiest hurdle to cross, but it wasn't insignificant, and it took a while to really understand how to reframe problems this way. My advice: Re-implement `foldl`, `foldr`, `map`, `filter`, and as many other basic list handling functions as you can, until it sinks. Look at GHC source (No, seriously, the definitions to this stuff are really readable) when you get stuck. Fourth, and final: All documentation sucks. Really, if you're used to the more travelled bits of the JS ecosystem, the documentation situation on even the best of Haskell packages is utterly terrible. Get used to asking questions on reddit, SO, or IRC. It will probably take you at least a couple of weeks of tooling around with the language before the average Haskell package documentation is even useful to you at all. The exception to this is the `containers` package, which you should learn as soon as you get a basic grasp of syntax. This package is relatively well documented, and also happens to be utterly indispensable to getting work done. Good luck!
I have found the Haskell community to be warm in general. Yes, there are exceptions, as always. And yes, there are particular "holy wars" that tend to rile people up. But there are lots of folks who just want to help each other out, solve nifty puzzles, and make awesome programs and libraries.
Not as easy as you hope, not as bad as you fear. I think http://haskellbook.com is generally accepted as the best way to learn.
Maybe you need to be a subscriber.
&gt; After specialization to Int they're equivalent, but if you're not trying to get an Int length then you'll pay a performance cost, if I understand correctly. Oh I see. Yes, that makes sense.
This is roughly how Haxl does it. Yes, it is law breaking in a strict sense. [But I did something similar for the free monad, and think it can be *fine*](https://elvishjerricco.github.io/2016/04/13/more-on-applicative-effects-in-free-monads.html).
Pretty cool! /r/haskellgamedev/
Haskell is not any more difficult to learn than anything other language. It does not take an incredibly intelligent person to learn Haskell, any more than it takes an incredibly intelligent person to learn to program. Like everything else, it takes time, resourcefulness, practice, and intent. If you keep that in mind and make progress each day, you will pick it up no problem. My advice would be to not read any tutorials by anyone claiming you need to understand '&lt;X&gt; mathematical concept' to understand Haskell, and instead follow tutorials and manuals where you are actually building something. Algebraic understanding will come with time, practical understanding comes with practice. To program in Haskell, the latter is necessary and the former a nice add-on.
Thanks! I can reproduce the behavior here. I will have to dig in to it when I have time.
The solution with reading the data from a file works but, unfortunately, I can't use it in HR. I tried changing the stdin buffering but got the same errors with BlockBuffering (I made sure the block size I set was large enough) and LineBuffering. NoBuffering caused my program to simply halt when I ran it from my terminal and HR just gave me a runtime error.
I think the problem is `withResource` is waking every thread from `mapConcurrently_` only to have nearly all of them to block again right away. STM doesn't do a great job here, but it doesn't have much choice. If you assigned specific resources to specific threads and only waited for that resource to be free then only a subset of the threads would act, but that would still be a lot of threads. STM has no way to know that the free resource will only be available for one thread to take. You can structure your program with that knowledge (explicit work queues perhaps with stealing) and probably get the performance you want. I think you could also limit the number of threads that you make to match your resources. I'll try to write a `mapConcurrentlyN_` that will only spawn up to `n` threads at a time and as work finishes, spawn more threads. That might not exactly match your intended use case.
strange - maybe you can try `readLine` instead: main = do let [x, n] &lt;- words &lt;$&gt; readLine or else switch to Text or ByteString ...
My general opinion is that being unsure of whether you can improve B (or even being sure you can't) is never a very good reason not to improve A.
Yes, I plan to add a build caching layer to Shake. Hermetic builds are useful but kind of orthogonal - I imagine they require a lot of OS specific know-how. Shame you lost that branch, I'd love to have seen it!!!
&gt; But I'm not at all optimistic about ever making Nix itself support incremental builds. Why?
This isn't quite `mapConcurrentlyN_` (doesn't cancel) but gets the basic idea: mapConcurrentlyN_ :: Int -&gt; (a -&gt; IO b) -&gt; [a] -&gt; IO () mapConcurrentlyN_ n act xs = do let (initial, rest) = splitAt n (map (async . act) xs) as &lt;- sequence initial waitAndContinue as rest where waitAndContinue as [] = mapM_ wait as waitAndContinue as (next:rest) = do (a, _) &lt;- waitAny as n &lt;- next waitAndContinue (n : delete a as) rest Running this gives the expected same performance with `-N1` and `-N`. (Full code)[https://gist.github.com/fryguybob/e414bd03ea130d64979e2c4ee21724e1]. 
what are the benefits of this vs just using GHC?
Thanks. I'd always been imagining a more "guided" approach where the package maintainer chooses the points in which to snapshot. But of course, this also requires hashing the outputs of the snapshots, at least inside each derivation.
Yea, that's what I ended up implementing. I was just left perplexed by the fairly large regression when increasing capabilities.
How do you find GC pauses behave in respect to your game?
The architecture is quite nice—I think people coming from other languages would find this quite a compelling example of good Haskell code. :) Small observation: I’ve never seen the apostrophe used for prefixing names, like `Scene'Title`, `KeyStatus'Pressed`, &amp;c.—it’s odd but I like it. (Lately I’ve just been using `qualified` imports for everything, though, like `Scene.Title`.)
Simple. I didn't look for them.
&gt; apostrophe Thanks. I'm not sure where I picked that up. I've been falling into that style the past several months in an effort to move more away from qualifying imports. I forget about it until I read -- you know -- other people's code.
Use strict data from day zero and save a world of pain.
Liquid Haskell offers refinement types, which are more specific than types. A type signature in GHC cannot specify that an Int is greater than 3, or that a Maybe is Just Something.
Thanks. Interesting suggestion. I think I can see how a style of message passing can keep the code manageable as it gets bigger. Correct me if I'm wrong, but I believe I did some variation of that 'pattern' a while back on a server (https://github.com/jxv/t3/blob/master/t3-server/src/T3/Server/Main.hs#L39). It manages many multi-player sessions of tic-tac-toe games which originate from a lobby, so it needs multi-thread communication. The approach was born out of necessity. It felt a bit messy, so parts of the game session were later extracted to be more generic (https://github.com/jxv/turn-loop). Also, I hadn't heard of `MonadBaseControl` back then, so that it could be worth revisiting for that alone. I'd like to see code examples of this message passing style of communication with transformers. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [jxv/t3/.../**Main.hs#L39** (master → 9ab7462)](https://github.com/jxv/t3/blob/9ab74627417015a154f06d11178a7a5ce4abf853/t3-server/src/T3/Server/Main.hs#L39) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dv1m19r.)
This is true, and I errored in suggesting some of those without refreshing my memory on what the actual defs look like. I feel the point stands though, generally, it's pretty easy to figure out when you've gotten into the weeds, and there is a lot of very simple fundamental stuff in base source that is worthwhile to peek at. As a beginner I was really hesitant to go poking around in source code to learn fundamentals, and it took me entirely too long to get over that and really take the advice and go digging around. 
Has anybody solved the question about using `impossible` in the definition of `take`? (Spoiler alert: I provide a solution below, one that doesn't use `impossible`.) In the second section (the one on lists), the first exercise offers this, which does not typecheck: {-@ take :: i : Nat -&gt; List a -&gt; { ys : List a | length ys == i } @-} take :: Int -&gt; List a -&gt; List a take _ Nil = Nil take 0 _ = Nil take i (Cons x xs) = x `Cons` take (i - 1) xs It will typecheck if you change the == to &lt;=. As the function is written, I believe that's the best you can do, because you can't actually know that the result won't have a length less than i. But then they ask, "Could you use `impossible` in the definition of `take`?" I would like to say that it's impossible to take more than 0 from `Nil`, but I don't see how. This doesn't work: {-@ take :: i : Nat -&gt; List a -&gt; { ys : List a | length ys &lt;= i } @-} take :: Int -&gt; List a -&gt; List a take 0 Nil = Nil take 0 _ = Nil take i (Cons x xs) = x `Cons` take (i - 1) xs take i Nil = impossible "taking something from Nothing"
for #1 - class Default a where def :: a data Foo = Foo { field1 :: SomeMonoidA field2 :: SomeMonoidB field3 :: SomeMonoidC (...)} instance Default Foo where def = Foo mempty mempty mempty mempty (...) interestingFoo :: Foo interestingFoo = def {fieldICareAbout = someActualValue} And other variations on theme. You can also combine with RecordWildCards and friends to abuse the 'slot filling' powers of your `Default` instance to summon a bunch of mempties (the official plural for `mempty`, it's a thing, don't question it). 
I'm coming from exactly the same place as you. It has taken me since 2014 to learn it to an intermediate level, I still don't understand a lot of the things discussed in this subreddit. I've been learning very part time though, just doing a little every day. My advice is to pick up The Haskell Book and spend a month or two immersed in Haskell, you'll get to where I am much faster than I did. I tend to improve much faster when I can work with Haskell for a solid block of time. Also, you will end up hating PHP and JS, just a warning. Most Haskell web frameworks use advanced features of the language, which are inappropriate for a beginner, my advice is to avoid them (or at least: expect failure). Simple CLI binaries are much easier to write. Good luck.
For hermetic builds, my (uninformed) impression is you just sprinkle magic nixie dust on it and it solves itself, right? What I mean is, build in a nix environment. Then you group with the nix expression describing that environment, and as long as everyone agrees on the same expression, then their build outputs should be compatible? I'm pretty interested in the caching stuff, let me know if you need a tester someday :) I'll probably make another go at the shake packages stuff someday. It's pretty much a requirement if I want to break my personal projects out of their little mono-repo and be buildable elsewhere. Unfortunately it gets into reinventing bits of cabal territory, since part of the idea is different targets have different deps, and you only need the deps to build what you asked for. I'd be happy if cabal could be split into a version solver + download and install dependencies, and delegate the actual build to a more extensible system, like shake.
GLFW-b tends to do this. I’ve seen it fairly regularly.
Nice. Does HIE work with your reflex project skeleton ?
What do you mean "opportunity"? If it is a job offer you will be way in over your head. 
Played until I died. % stack exec -- dino-rush +RTS -sstderr 385,511,696 bytes allocated in the heap 6,756,432 bytes copied during GC 544,240 bytes maximum residency (3 sample(s)) 74,256 bytes maximum slop 4 MB total memory in use (0 MB lost due to fragmentation) Tot time (elapsed) Avg pause Max pause Gen 0 734 colls, 734 par 0.254s 0.067s 0.0001s 0.0016s Gen 1 3 colls, 2 par 0.007s 0.003s 0.0010s 0.0018s ... INIT time 0.000s ( 0.001s elapsed) MUT time 4.044s ( 65.410s elapsed) GC time 0.261s ( 0.070s elapsed) EXIT time 0.000s ( 0.000s elapsed) Total time 4.365s ( 65.481s elapsed) Alloc rate 95,320,627 bytes per MUT second Productivity 94.0% of total user, 99.9% of total elapsed Max GC pause `1.8ms`, so totally unproblematic.
Not exactly. I had to use Stack as the CLI frontend instead of `cabal new-build`. It's kinda ugly and bug prone, but it works. # stack.yaml resolver: ghc-8.0.2 # Tells stack not to do any package management nix: enable: true shell-file: stack.nix packages: - ./backend - ./frontend - ./common # stack.nix (import ./. {}).shells.ghc Stack won't be able to handle GHCJS, so `cabal new-build` is still necessary for that portion of it. But it will be able to do incremental builds of the GHC build, and HIE will be able to use it for editor integration. It's buggy, and HIE needs restarting for changes in `common` to be visible to `frontend`/`backend`, but it works. You also have to make sure HIE gets run from within the nix-shell, or else it won't get the right environment variables. Really, HIE just needs more proper support for `new-build`, and maybe for Nix.
It wasn't necessary, but it seemed easier. At that time, things were speaking (IIRC) a modified Thrift, and there were existing python libraries for that and also for speaking to Uber's logging/metrics infrastructure, and things were constantly in flux. I thought it better to let the Python handle that, updated straightforwardly as the ecosystem changed, and control the conversation between the Python and the Haskell.
I suppose I should ask the obligatory internship question™... Should I follow it up with the remote internship question? :)
I do not believe Haskell is particularly more difficult to learn than other languages. However, it does require more overhead in terms of study before you can actually begin to do anything. With most languages, I can pick up a basic level of competence within hours or days. I couldn't do that with Haskell. I am a mediocre programmer, at least in this early point of my career (&lt; 1 year experience). But I understand Haskell's fundamental concepts. There are lots of better programmers than myself who struggle with concepts like Monads and typeclasses and currying, as well as FP fundamentals like higher order functions and lambdas. If I can do it, so can you.
Maybe? The site was pretty unclear, it also seemed to imply that was a mailing list about a specific language extension?
If I remember correctly, the initial support was put on notice, because it was in Java and hard to maintain and kind of hacky. But a few years back they were transitioning to skylark, and they rewrote the haskell support in that. It seemed pretty functional to me, so I'm surprised it didn't make it to open source. I wasn't directly involved though, so I might be muddling it up.
If you don't need threading not using `-threaded` help with GC. Just tried Dino Rush. Max pause with `-threaded` 0.0101s, max pause without `-threaded` 0.0001s. 
This is a sort of a "dual" way of increasing type-safety to the one when you create newtypes in all sorts of places just to add some safety guarantees, you can instead keep the code simpler, but add rules for your invariants in Liquid Haskell comments.
Why do you want to do it?
``` problem x n = superDigit $ superDigit x * n ``` can we memory intensive, as `n` can we large. Optimal/expected solution is to find the catch that at first step, instead of calulating digit_sum of (n*k), calculate digit_sum of n and multiply it by k. [Editorial](https://www.hackerrank.com/challenges/super-digit/editorial)
I think you can use the System-F encoding: data Exists f = Exists (forall y . (forall x . f x -&gt; y) -&gt; y)
I don't know if you can do it with a type alias like that, but for me GADT syntax works fine: data FirstKnown a where FirstKnown :: (a , x) -&gt; FirstKnown a example :: FirstKnown Int example = FirstKnown (3 , True) 
Woah. First time I see the other encoding. That's pretty neat.
Regarding #2 I think the upcoming [`QuantifiedConstraints`](https://ghc.haskell.org/trac/ghc/wiki/QuantifiedConstraints) language extension will allow you to write: ``` instance (forall a . (Show a) =&gt; Show (f a)) =&gt; Show (Bar f) where show = ... ``` I don't know how this will work with deriving.
Would this work? type family First x where First (a,b) = a type family Second x where Second (a,b) = b test :: (First x ~ Int, Second x ~ Int) =&gt; x :~: (Int,Int) You probably still need some GADTs to discover that x is indeed a tuple type.
I didn't get it quite as low, with non-threaded, 90 seconds playing time: Tot time (elapsed) Avg pause Max pause Gen 0 982 colls, 0 par 0.058s 0.067s 0.0001s 0.0046s Gen 1 3 colls, 0 par 0.002s 0.002s 0.0006s 0.0014s 
So you get longer max pause in gen 0 with non-threaded? This doesn't seem right, as there are synchronization overheads in the threaded runtime that just don't exist in non-threaded.
Try using [type witnesses](https://wiki.haskell.org/Type_witness): {-# LANGUAGE GADTs, TypeOperators #-} import Data.Type.Equality data FirstKnown a1 a where FirstKnown :: FirstKnown a1 (a1, x) data SecondKnown a2 a where SecondKnown :: SecondKnown a2 (x, a2) test :: FirstKnown Int a -&gt; SecondKnown Int a -&gt; a :~: (Int, Int) test FirstKnown SecondKnown = Refl 
I think the library is great. I wrote a three line shell script to deal with the issues you mention when I was using it. As for ideal packaging of hs-webdriver, I don't think shipping selenium server inside of hs-webdriver is better. At most, I would add instructions to readme if they aren't already there.
Awesome. Is there a getting started with compiling for various platforms with nix somewhere? Sorry, I am a noob to Haskell's tooling.
Well, most of the cross compilation support in nix is due to /u/sonarpulse I believe. /u/shlevy is also doing some cross compilation to riscv right now. For GHC, it pretty much depends if the target is supported properly in GHC, and if the toolchain in nix plays nice with ghc out of the box. If you look at the `hs-hello.nix` expression, you will find `crossSystem = (import &lt;nixpkgs/lib&gt;).systems.examples.mingwW64;`, which is the crucial part to put nix into cross compilation mode. Other predefined systems can be found in https://github.com/NixOS/nixpkgs/blob/master/lib/systems/examples.nix. Thus with nix, if the cross compilation has matured enough for each system you are interested in, getting the result for the specific system would be as simple as swapping out the `crossSystem`.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [NixOS/nixpkgs/.../**examples.nix** (master → 11d6ada)](https://github.com/NixOS/nixpkgs/blob/11d6ada7552cb3463d34501209eb5c26a2629322/lib/systems/examples.nix) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dv2i7o6.)
This is great! Cross compiling to Windows is really freakin' exciting. Another tool to shore up Nix's lack of Windows support, alongside WSL :) Do you have an example of using `cabal` with a cross compiler? Last time I tried it (long time ago), I had to do more than just `--with-ghc`, and it still failed. Very happy to hear this is improving.
It’s just an encoding of the identity `∃x. P(x) ↔ ¬∀x. ¬P(x)`
I've tried to derive it but got stuck on `(forall x. f x -&gt; FALSE) -&gt; FALSE` You can encode `FALSE` as `Void` and this is enough to construct the `Exists` value but you can't destruct it. You can also encode it as `forall a. a` but then I get stuck on `forall z. (forall x. (forall y. f x -&gt; y)) -&gt; z`
I assume you mean, outside of nix (as nix relies on `Setup.hs`). And yes. I'm pretty confident that even `new-build` works these days (cabal HEAD checkout). You might also want to use the wrapper (https://github.com/zw3rk/toolchain-wrapper/blob/master/wrapper#L25-L46). As that will make passing the flags to cabal a bit easier. Lastly, you also want to use a *very* recent `hsc2hs`, which supports the `--via-asm` flag for constant computation when working with windows haskell sources that use `.hsc`.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [zw3rk/toolchain-wrapper/.../**wrapper#L25-L46** (master → 9277e2d)](https://github.com/zw3rk/toolchain-wrapper/blob/9277e2d052f648987841ff5ed452fd27a124523a/wrapper#L25-L46) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dv2iomz.)
Hint: think of the `y` in `(forall y . (forall x . f x -&gt; y) -&gt; y)` as FALSE
I don't know how to justify that though. Shouldn't using 'Void' instead of it be equivalent?
no, does not work
`CTRL-F stack` Phrase not found nty
If `(forall x . f x -&gt; y) -&gt; y` is true for _all_ `y`s, in particular it is true for `y ~ Void`. The converse is also true, but a bit less obvious.
I want to do this on the type-level
Yea Stack really needs to support cross compilation. No clue how it reasonably could though, since cross C toolchains are so weird. This is something that worries me slightly about WebGHC. I've been modeling it as a cross compilation system because that's the right way to do it. But Stack doesn't support that model and Im not sure how I would even give it the C toolchain other than Nix.
Sidenote, stack size in Hackerrank's environment is pretty bad. I was trying to do a competition the other day in Haskell, but test cases kept failing due to timeout. Literally translated each line of Haskell into equivalent C++ and the program passed with flying colors :\
A simple technique that might help: remove bits of code, or replace with a simpler/alternate implementation, until the leak stops.
This is very encouraging for a newcomer like myself! Go Haskell go! (pun not intended)
Can you use the same idea to define forall in terms of exists (assuming it was primitive)? I've been idly curious if ImpredicativeTypes with exists as a primitive could be any easier than with forall, or if they're of equal difficulty.
Compile with `--profile` and run with `+RTS -p`; this will dump call stacks and allocations within them ([documentation](http://downloads.haskell.org/~ghc/latest/docs/html/users_guide/profiling.html)). You can inspect the resulting profile data by eye, and it'll often show you where the leak is coming from.
 absurd :: forall a. Void -&gt; a absurd v = case v of {} can convert one way, and you can specialize with `y ~ Void` the other way.
 (forall a. p a) &lt;-&gt; Not (exists a. Not (p a)) So yes.
[w3 webdriver protocol spec](https://w3c.github.io/webdriver/webdriver-spec.html) There you go, if you want to get hacking.
For clarity- A server implementation is necessary, the greatest degree of compatibility is presented by using selenium's driver instance, (per the library docs) but the standalone bins / .exe's used by node-selenium (used by purescript) is also an option if you just don't want to use java. Probably you don't really want to do that as the library author offering up the java selenium server as the recommended option most likely speaks to a future design intent of matching the reference implementation for the webdriver spec (which was derived from the selenium project, not vice versa, FWIW). So my suggestion there is that if you want another implementation whose stated design goal is stick to the spec as offered, or use a separate driver instance as it's default, that's where you'd get started.
 data Void type Some f = forall y. (forall x. f x -&gt; y) -&gt; y type Some' f = (forall x. f x -&gt; Void) -&gt; Void absurd :: forall a. Void -&gt; a absurd v = case v of {} toSome :: Some' f -&gt; Some f toSome f = \g -&gt; absurd (f g) This is as far as I got. The problem is that `y` becomes fixed inside of the `\g -&gt; ...` scope so `g` can't be a parameter to `f` anymore.
One option is to move `= sum` to the next line (and everything below by one more line) as that leads to less noisy commits when you rename the function. This also works nicer when you have long function names; you have more space to work with below compared to moving everything to the right.
You can probably get some out of my write up on using the cross compilation tools for iOS at [codetalk: Mobile Haskell (iOS)](https://codetalk.io/posts/2018-02-07-Mobile-Haskell.html), specifically the [section on the Makefile](https://codetalk.io/posts/2018-02-07-Mobile-Haskell.html#hs-srcmakefile).
Thanks so much for this point. I've been looking into cross-compiling for Rust and Haskell for some work projects. The Rust side is pretty well-documented and ergonomic to use (even with nightly), but finding a similar path to cross-compiling with GHC has been a struggle.
I just highlight sections and/or buffers in emacs and then run them through hfmt or brittany to auto format them. with evil-mode it becomes something like '&lt;,'&gt;!hfmt to format a selection note these tools don't actually try to line up operators, but i think the style they use of consistent indentation is better than trying to line up operators in most cases.
thanks! it does inspire me in additions to https://github.com/dmjio/stripe I built something similar but the next question would be any chance to eliminate the boilerplate code at the main application. https://github.com/freizl/hoauth2/blob/hw-demo-server-refactor-3/example2/App.hs
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [freizl/hoauth2/.../**App.hs** (hw-demo-server-refactor-3 → 3d005e9)](https://github.com/freizl/hoauth2/blob/3d005e9f2225e32e4e2a3a58f999c99689c312e1/example2/App.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dv31985.)
You can't have arbitrary indentation in Haskell - see this [wiki](https://en.wikibooks.org/wiki/Haskell/Indentation).
Whoops, nevermind. You seem to be correct: `Some'` can be constructed but can't really be destructed, while `Some` can do more.
While there are some restrictions, the type of indention OP is wanting would be possible.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [huseyinyilmaz/spock-memmory-usage-test/.../**Main.hs** (master → 4815240)](https://github.com/huseyinyilmaz/spock-memmory-usage-test/blob/4815240b9dc1609ebf3cae19c2e22480108c2b88/spockmemtest/app/Main.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dv34nqq.)
I use visual studio code for my coda project and at work, but it hasn't yet become my editor of choice overall. I still have a bit too much muscle memory for vim to fully make the move. That said, it works pretty well for C++ and I kind of love the language server protocol.
Thank you. That's really nice
You can probably use `M-x align-regexp` for this 
Web GHC is GHC compiling to web assembly. The web browser is not a computer and it can't run it's own compiler, really. So ideally the best way to compile to web assembly is to cross compile and treat the browser like the target instead of the host. Unfortunately stack doesn't support this really and cross compiling is pretty painful in general. It'll be an interesting engineering challenge to solve after we get WebGHC working.
A cross compiler is just any compiler that targets a different system than the one it runs on. As an example, I would consider GHCJS a cross compiler. But most often, the term refers to things like Clang, which can cross compile C to another platform like ARM. GHC already supports cross compilation for targets with usable C toolchains, especially for LLVM targets. GHCJS isn't integrated with GHC because JS wasn't a usable C target at the time (either that or Luite just didn't want to use emscripten because it's garbage :P). WebGHC aims to use GHC's existing cross compilation infrastructure, rather than reinventing it, to target wasm with Clang/LLVM. Cross compilation brings with it a bunch of tooling problems. Most tools have cross compilation as an afterthought, with rare exceptions like CMake. Stack doesn't consider it at all; Nix only recently converted it's stapled-on support into something really robust; and of course Cabal's support is mentioned in the OP. Cross compilation really *ought* to be a first class citizen in all tooling environments, since it encourages people to really think about the platform-specific design decisions they're making. But of course we can't be so luck ;)
It's great to see IOHK giving back to the Haskell community. I hope Cardano enjoys great success and drives much more Haskell investment.
I started to suspect that this is a wai (or warp) problem. I wrote simplest wai application possible here: https://github.com/huseyinyilmaz/spock-memmory-usage-test/blob/1ea194c8dbd469ed43c246501a2cc882e92c4237/waitest/library/Example.hs I built and run it with this: stack build &amp;&amp; stack install &amp;&amp; waitest Than I started to send requests to the server with this: wrk http://localhost:3000 -d 5m When I check memory usage with htop, I see that memory usage is increasing in time. https://cl.ly/1i28272h062M 
(yeah, that's how I write most functions, which are a few compositions, in emacs)
&gt; but there again it’s not a very popular language.. which is kind of too bad since this is one of my top interests in programming. Haskellers of the world, unite! You have nothing to lose than your boilerplate! 
10 milliseconds drops to a fraction of a millisecond? Woah.
And common.
This is almost right, but technically what is being done in the encoding above is a bit different from double negation. type Not x = x -&gt; Void = x -&gt; forall a. a. There is an embedding of `x -&gt; Not (Not x)`, but in without classical logic there isn't a way back! The identity you gave doesn't hold here. You can construct `Not x &lt;-&gt; Not (Not (Not x))`, though. When you build up the double negation used there you'd have two separate quantifiers! But what else can we do? Another thing we could do is that if y doesn't mention x then we _can_ always interchange `(forall x. f x -&gt; y) ~ (exists x. f x) -&gt; y`. This is a far more useful identity as it works even when limited to intuitionistic logic. There are some fancy ways you can justify slopping foralls and exists around in signatures like this. * In category theory terms, you can show that `(, e) -| (-&gt;) e`, and right adjoints preserve limits, this preservation of limits lets you do things like turn `a -&gt; forall x. b` into `forall x. a -&gt; b` and vice versa when a doesn't mention x. and then you can show that the hom functor takes colimits to limits in the first argument. * You can also show that the Hom functor `(-&gt;)` takes colimits to limits in its first argument. When you read limit as forall and colimit as exists this turns into the above identity. Reading this identity backwards so long as the existential only occurs in negative position we can turn it into a universal. The key to working with existentials in Haskell is to observe that we can always shift the existential to negative position like that by CPSing any function that would "return" an existential. We do have another identity x ~ forall r. (x -&gt; r) -&gt; r but it doesn't come from logical double negation, but rather from the Yoneda lemma. It identifies each x with the set of all things you can observe about an x. This can be viewed as a CPS transformation, followed by eliminating the dependency on the type of the continuation by quantifying over it. This is analogous to newtype Yoneda f a = Yoneda (forall r. (a -&gt; r) -&gt; f r) which you can show is isomorphic to `f a`. Here we're picking `f = Identity`. The System F existential encoding mentioned above can be justified by using the Yoneda lemma and the preservation of colimits as limits. If you try to get there with just embedding into double negation it doesn't actually work out.
This. Bind it to C-c C-s
Unfortunately, this identity only holds in a classical setting. You can, however, use forall a. x -&gt; y ~ (exists a. x) -&gt; y when y doesn't mention `a`. This plus the Yoneda lemma is sufficient to justify an encoding of exists that works intuitionistically.
This is not the case. There is a difference between the single quantifier and double quantifier version. The double negation embedding is "lossy". On the other hand the single quantifier isn't. Let's fix f data Is a b where Is :: a -&gt; Is a a Now given forall y. (forall x. Is a x -&gt; y) -&gt; y We can pick `y = a` and extract the actual value! In general, I can't extract an `x` from `Not (Not x)`. There is an embedding but not a way back, but I can extract `x` from `(forall y. (x -&gt; y) -&gt; y)` by picking y = x. It is less obvious because it isn't true. =)
Just remember that `$` and `.` are special characters in Emacs regexps, so you will need to escape them (`\$` and `\.`).
I am a relative Haskell beginner, and I find certain theoretical aspects fascinating. I knew that nonterminating languages are not logically sound, but the explanation given went over my head. However, I was recently thinking about how the Curry-Howard Isomorphism is used to prove things, and I think that I now understand the need for termination. I realized that I could inhabit an untrue type, such as Void, with recursion. I was reminded of the termination requirement of proof languages, and also of the "bottom" value that inhabited every Haskell type, which I had previously knew was related to the inconsistency, but didn't understand. Is the following understanding of nontermination correct? 1. Recursion may be used to inhabit possibly untrue propositions. x :: Void x = x -- "Void is true because of Void!" 2. Recursion is required to write inductive proofs. 3. Valid recursive proofs must always reach a base case. 4. The termination checker prevents proofs of false propositions by preventing theorems from being proved with circular reasoning. 5. Bottom represents the result of a nonterminating expression and inhabits all types, including false ones. Thanks!
LOOOL what 
https://hackage.haskell.org/package/hssqlppp - did you all could consider forking that? I am also curious about what you studied for prior art in Haskell before implementing the new library.
Historical accident. You could very well add join as a method to Monadic as an alternative to &gt;&gt;=. In practice, it's often more natural to define join in terms of &gt;&gt;= than the opposite. For example, IO: to join an IO (IO a), you first run the outer IO, and then run the continuation of the IO value it returns
You can: https://en.wikipedia.org/wiki/Monad_(functional_programming)#fmap_and_join
I believe the GHC devs do want to get `join` into the monad class. The reason that isn't the case currently is they need `-XQauntifiedConstraints` to avoid breaking `-XgeneralizedNewtypeDeriving` for the `Monad` class.
You don't even need return now that all Monads are already Applicative. You have `pure`, so really it's just an Applicative with join. 
Subscribed! Thanks for sharing :)
The `purescript-webdriver` seems to have the same deal -- it piggybacks on [`selenium-webdriver` NPM package](https://www.npmjs.com/package/selenium-webdriver), which, per the docs: &gt; You will need to download additional components to work with each of the major browsers. The drivers for Chrome, Firefox, and Microsoft's IE and Edge web browsers are all standalone executables that should be placed on your system PATH. I haven't read the source, but I would be somewhat surprised if the `selenium-webdriver` NPM package actually started the server in a forked process behind the scenes. The `build` API that `purescript-webdriver` uses to say `build $ browser Chrome` is essentially the same as `chromeConfig`, and further options would be specified using the other `Builder` functions. in Haskell you just write the settings directly, as a record update.
If doing this, it's also worth looking into [pcre2el](https://github.com/joddie/pcre2el) for its `pcre-mode`, which lets you use Perl-style regex in places that normally expect an emacs regex. It's not perfect, but still beats writing emacs regexes in my opinion. 
That said, PCRE still has the same issue: `.` and `$` are special characters.
I'd say both definitions are equally natural, it's just that join is more useful when you're working with monad algebras (which you generally don't in Haskell) and bind is more useful when working with Kleisli arrows a-&gt;m b, which is pretty much the raison d'etre of monads in Haskell.
You're right, and it's probably good that you clarified that. I was suggesting pcre-mode as a general tip rather than implying it was a solution to `$` and `.` being treated specially. I mention it whenever emacs regex crops up because emacs regex syntax is...special, to put it nicely, and I've noticed they can be pretty off-putting even for people familiar with standard or PCRE style regex. 
This was one of my first questions when learning about monads: https://stackoverflow.com/questions/31552064/why-isnt-join-part-of-the-monad-class. This was an explicit tradeoff around generalized newtype deriving. The problem was around roles. The role of the type variable of any law abiding functor _should_ be representational or phantom - but that isn't something that GHC can know (you can have non-law-abiding functors defined). That means that given `newtype M' a = M a`, GHC wouldn't be able to coerce its way through `join :: M (M a) -&gt; M a` to `join :: M' (M' a) -&gt; a` (but it can coerce `(&gt;&gt;=)` and `return`) and therefore you couldn't GND your way to monad instances. I agree it sucks, but deriving mtl-style classes is pretty slick.
Comments and criticism :) &gt; It also depends on the Safe library, just because I like the Maybe variant of the head function While I'm all for not duplicating code, is a dependency really worth not having to type `headMay = if null xs then Nothing else Just $ head xs` once in a file somewhere? &gt; I avoided using do notation so that it’s really clear what’s going on with the monadic binding: Just as a personal aesthetic, I've never found using raw binds to be more readable than do notation with the exception of occasional well placed oneliners using `&gt;=&gt;`. I've seen some one-line functions that used a pointfree application of `=&lt;&lt;` and thought to myself "well, that was fairly elegant but that'll take me way too long to figure out how it works right now". But, 99% of the time, I find do notation vastly more readable. (Also, `pure` over `return` seems like something to strive for, although I don't confess to have a very strong argument for that). &gt; mandelbrot takes 5 integers, and returns the texel value. `mandelbrot :: Int -&gt; Int -&gt; Int -&gt; Int -&gt; Int -&gt; PixelRGB8` Haskell's type system is a thing of beauty. I really do think this would be far more readable and usable if it was more strongly typed. newtype Height = Height Int newtype Width = Width Int newtype X = X Int newtype Y = Y Int mandelbrot :: Int -&gt; Width -&gt; Height -&gt; X -&gt; Y -&gt; PixelRGB8 Bonus points if you have smart constructors to guarantee that your values are always in the correct range by the time they're passed into `mandlebrot` :) I also would've written your let more like: mandlebrot maxIter w h x y = let p = min (genMandelbrot 0 maxIter 0.0 0.0 ((fromIntegral (x - w `div` 2)) / (fromIntegral (max w h)) * 3.0) ((fromIntegral (y - h `div` 2)) / (fromIntegral (max w h)) * 3.0)) 255 in PixelRGB8 0 0 (fromIntegral p) but even more preferably: mandlebrot maxIter w h x y = -- val1 and val2 are placeholders for semantically useful names. let val1 a b = fromIntegral $ a - b `div` 2 val2 = fromIntegral $ max w h -- I think this is the right semantic name for these two? normalizeWidth = val1 x w / val2 * 3.0 normalizeHeight = val1 y h / val2 * 3.0 p = min (genMandelbrot 0 maxIter 0.0 0.0 normalizeWidth normalizeHeight) 255 in PixelRGB8 0 0 (fromIntegral p) Of course, the proliferation of 'fromIntegral' all over the code is an indication, to me, that there's something to be modified in the types. But numerical code is generally slightly funky in Haskell anyway, so it's not that bad. `genMandelbrot :: Int -&gt; Int -&gt; Float -&gt; Float -&gt; Float -&gt; Float -&gt; Int` You're killing me with those types, though :) Just personal preference, but I really like guards for functions like this: genMandelbrot iter maxIter x y px py | diverges x y || iter &gt; maxIter = iter | otherwise = genMandelbrot (iter + 1) maxIter thing1 thing2 px py where diverges = x^2 + y^2 &gt; 4.0 -- These look like they should have very semantic names thing1 = x^2 - y^2 + px thing2 = 2.0 * x * y + py The more self documenting the code, the clearer the ideas come across in my experience. Especially when trying to introduce foreign things to people. `head (firstItem : restOfList) = firstItem` is *way* clearer than `head (x:_) = x` if you're introducing people to the concept of head. It's useless noise to people who know what it does, though; finding that balance can be sort of tricky but I like to keep the quote "Things should be as simple as they can be, but no simpler" in mind when writing pedagogical code. --- I did like the article! I'm looking forward to seeing more and I think your approach to "Haskell for the Imperative" is quite interesting and I hope it goes well for you; also, nice blog name :)
You might be interested in - https://www.reddit.com/r/haskell/comments/81e0gp/what_is_new_in_cross_compiling_haskell/ - https://medium.com/@zw3rk/what-is-new-in-cross-compiling-haskell-42ba93555c69
I find that bind is easier to teach newbies because of its similarity to fmap and ap: fmap :: (a -&gt; b) -&gt; f a -&gt; f b ap :: f (a -&gt; b) -&gt; f a -&gt; f b bind :: (a -&gt; f b) -&gt; f a -&gt; f b It's a case of inserting an `f` and considering what we get.
 FYI: Figured out a decent solution by using type class https://github.com/freizl/hoauth2/commit/c2d816b3e98dd35373aaaec6437e2e1f6312f30f searched for 'expression problem' and found this interesting vidoe http://www.uni-koblenz.de/~laemmel/paradigms1011/resources/xproblem.html 
Thanks, that is much easier to read than the textual representation. Only somewhat related: what is the status of statistical profiling? I know perf mostly works know but iirc dwarf only supports a single source origin? 
You're welcome! :) I hope it helps you get started.
Thanks for the feedback! I'm still learning Haskell myself at the moment, so this is very useful. Part of the reason that the code isn't super clear is that this isn't part of my Haskell tutorial series, just a little demo I coded up one evening. I thought it would be interesting to write a short post about it. You're right though, it's a good chance to show off some of Haskell's type system.
Also `headMay` is just `listToMaybe` from Data.Maybe.
What is the relation to the C2 Wiki? E.g. http://wiki.c2.com/?PurelyFunctionalDataStructures
This is great! Hopefully this will lead to better tooling (and I'll probably take a stab at writing some of it myself). I never had much luck trying to analyse the old format, and the data files quickly became far too large to do it by hand.
Excellent, there's even very recent activity!
The main question is how much tutoring you'll get. In my team we onboarded two ppl with no prior Haskell experience and in both case we could bring them up to speed pretty fast considering the amount of things to teach (setting up the tooling, the specific libraries and app structure we curated, our repository and code-review flow). For "minor commits" like moving imports, adding logging, you can expect to be able to do that in a day or two. I'd say reading and following the rough logic of a module takes a week, and passing the "productive" threshold in around three. Unrelated: always make small edits, you can write awesome Haskell by applying a succession of trivial patches.
Nice blog! Can you by any chance provide an rss feed? :) I prefer old school feeds over WordPress.com-accounts. 
No relation.
&gt;So yes, I do prefer importing Safe over using if statements. I think the point is that the proposed function headMay = if null xs then Nothing else Just $ head xs is literally the same as the Safe library's headMay = liftMay null head where liftMay func test val = if test val then Nothing else Just $ func val [headMay source](https://hackage.haskell.org/package/safe-0.3.16/docs/src/Safe.html#headMay) [liftMay source](https://hackage.haskell.org/package/safe-0.3.3/docs/src/Safe.html#liftMay) :)
Oh I like this. I've used xml-conduit and xml-lens previously but they're a bit clumsy.
wordpress have built-in rss https://whatthefunctional.wordpress.com/feed/
Thanks for the link! I could not find any link on mobile :) 
Cabal (HEAD) with ghc (HEAD with hsc2hs HEAD) should (mostly?) just work. If not, let me know ;-)
SPJ [has a branch for `-XQuantifiedConstraints`](https://www.reddit.com/r/haskell/comments/7toutl/now_there_is_a_branch_to_play_with/) (https://github.com/ghc/ghc/tree/wip/T2893) With the latest addition we have a path to GNDing a class with a `join` method by using the implication constraint `forall a b. Coercible a b =&gt; Coercible (m a) (m b)`. [Reddit thread for paper](https://www.reddit.com/r/haskell/comments/6me3sv/quantified_class_constraints_pdf/). ---- $ git clone -b wip/T2893 --recursive https://github.com/ghc/ghc.git $ cd ghc $ cp mk/build.mk.sample mk/build.mk $ sed -i 's/#BuildFlavour = devel2/BuildFlavour = devel2/g' mk/build.mk $ ./boot $ ./configure $ make
Ok, I’ll play with it – after the ICFP deadline :-)
Most of the time wordpress doesn't give a link but you just need add /feed
Here's one way to derive `Show` for higher-kinded types like `Bar`. http://lpaste.net/363089 The idea is to first wrap `f` in a type with an actual `Show` instance, to get GHC to derive one instance, but with the wrong head. And then coerce that instance to the final one. There is some setup at the bottom that needs to be done once. Then it's two lines of code for each type: deriving instance {-# INCOHERENT #-} Show1 f =&gt; Show (Bar (Showing f)) instance Show1 f =&gt; Show (Bar f) where showsPrec = showingPrec
What is the current status of Haskell on Hadoop, YARN and maybe Spark?
I am submitting this here because Planet Haskell seems temperamental as to which one of my recent posts to actually publish. Undoubtedly I set up the feed wrong. I am looking mostly for experience reports from people who have tried compact regions for any use-case.
Thanks for explaining that Xelif.
Thanks for letting us know about the RSS feature. :)
Hadn't been reminded of the existence of Planet Haskell in a long time! 
Your link to yesterday's post is broken
This fits my experience too although I still make a point of explaining that the "essence" of it is `join` by showing them how using `fmap` with a monadic function `(a -&gt; m b)` necessitates the `m (m a) -&gt; m a`.
When I fix up the example (`module Main...` and `{-# LANGUAGE OverloadedStrings #-}`) then compile with `ghc -O2` using the latest libraries on hackage and ghc version 8.2.2 I see the memory use grows at about 4k a request from 1MB up to 4MB resident then it holds steady. In other words, I can not reproduce your issue and suspect the difference is due to how we are each are compiling or running it (i.e. your setup might be running it interpreted, or without optimization, using different libraries or compiler version, etc). Tested via `for i in $(seq 1 100000) ; do wget localhost:3000; done`.
Thank you for the contribution but it would have been nice to patch `ghc-prof-flamegraph` rather than release `ghc-prof-aeson-flamegraph`. &gt; This one is similar to ghc-prof-flamegraph except that it doesn’t bundle FlameGraph Bundling `FlameGraph` seems like a good idea to me. It's one perl script, and kind of tedious to manually install it yourself. Then you have to pipe the output of `ghc-prof-aeson-flamegraph` every time. &gt; I didn’t have to try to impose additional dependencies on the existing package I'm sure the maintainers would not be opposed to adding this dependency. It's at least wort asking. &gt; It was also not completely straight-forward to integrate with it. Fair enough, I guess my complaints stop here. If the code was not easy to patch, then it was not easy to patch. Still, it's a shame two roughly identical tools (and ideally one will be deprecated in favor of the other at some point).
This seems very cool. It might be nice for those not familiar (like me) to include an explanation of how to read a flamegraph (or e.g. link to http://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html#Description). A point mentioned in the link above: it seems as though flamegraphs were designed to visualize profile data derived from randomly sampling the stack, and so make no attempt (since it would be impossible) to distinguish between _slow_ functions and _frequently called_ functions. GHC gives us exact call counts as well as timing information right? Is there a variation to the graph that could visualize both (maybe color to represent the calls/time ratio?) ...actually, backing up, what actually do the widths represent, and what meaning do the colors have?
GHC does give us the exact numbers, yes. The widths are whatever you ask them to be. There are 3 pieces of informations that we have: allocations (in bytes), entries (number of times called) and ticks (time). The default metric that's output is ticks. Absolute values are given to flamegraph. In this case widths are "relative time spent". A bar stretching all the way across means 100% time spent. For Haskell this is the `main` function. If you have a bar on top of another, it's part of the stack and the width also matters. For example consider a bar `MAIN` that's 100% width of the graph with a single bar on top of it `MAIN.go` that's 80% of the graph and no other bars. This tells us 80% of the program time was spent in `MAIN.go` and 20% in `MAIN` itself. IT also tells us that 80% of the `MAIN` time was spent in `MAIN.go`: this is obvious here but sometimes you have wide bars with many bars on top. Using this, you can often quickly find where your problems are: you're looking for a wide bar "sticking out", i.e. one where we spend the time. Basically the wider a bar is, the more time we spent in it then we see what's on top to figure out if we spent the time in the function itself or something that was called from it. The very same logic applies to entries and allocations. You have 100% of total entries and you divide it by width of graph. 3 bars side by side with a third of the width each means each had the same number of entries (third of total). Same for memory. IIRC the colours are arbitrary and FlameGraph just picks them semi-randomly from a palette. I might be wrong there but I'm pretty sure that's the case. I believe you can configure and tweak these to make them mean actual things. I merely copied behaviour of `ghc-prof-flamegraph` because I could not find any description of the actual format flamegraph expects anywhere.
Great explanation, thanks! I'd be really curious to see what combining all the dimensions might look like... maybe entries indicated with horizontal length, ticks/time by _area_ of the box(?), and allocations/entry by color or something. 
You get much more refined types. Instead of saying dayOfMonth is an Int, you can say dayOfMonth is an Int between 1 and 31, and your program will fail to compile if your arithmetic ever tries to write something outside this range. Basically the compiler catches yet more bugs, and code is yet more self-describing. 
Studying IO mechanisms of Haskell, tutorial says the type `IO` has following type definition: ``` type IO a = RealWorld -&gt; (a, RealWorld) ``` I wonder what `RealWorld` type is. What does the value of the type `RealWorld` contain? Does it really contain the information of filesystem, etc? (I believe this isn't true; Kinda silly question, but just in case..) Is it just special(unique) value which can not be copied and used more than once?
Performance is currently very quick for us, so a slight hit on that probably wouldn't hurt much. False retries could be a problem though - we already have to work hard to avoid deadlocks.
Thanks for looking into this. I think we are very close to finding out the problem. I compile same code with stack on an ec2 instance and got the same result. I think you cannot replicate the problem because you are compiling with different versions of a dependencies. Here is dependencies I am using to compile: https://gist.github.com/huseyinyilmaz/50f99c4f5d44cf2fd61a90840e22b240 Can you send the dependency list you are using to compile? If I can switch my dependencies to your versions one by one, I bet I can find the culprit. 
The name `listToMaybe` actually describes *exactly* what it does - turn a list into a `Maybe` in the most obvious way. It doesn't make it clear how useful that is, though. Still, I personally always liked that name better than things like `headMay`.
Glad someone followed up on this now that compact regions are available. Seems pretty promising. Would love to see some more explorations of this feature.
Ah this is exactly what I was looking for a while ago, this means you can derive newtype A a = A (Tree a) deriving (Functor, Foldable, Traversable) via PostOrder newtype B a = B (Tree a) deriving (Functor, Foldable, Traversable) via PreOrder newtype C a = C (Tree a) deriving (Functor, Foldable, Traversable) via InOrder newtype D a = D (Tree a) deriving (Functor, Foldable, Traversable) via DepthOrder The `prepared` field sort of reminds me of the exponent of [`Day`](https://hackage.haskell.org/package/kan-extensions-5.1/docs/Data-Functor-Day.html) ([`newtype ExpDay f g a = ED (forall xx. f xx -&gt; g (a, xx))`](https://lirias.kuleuven.be/bitstream/123456789/499951/1/main.pdf)) but it's a tenuous connection data Task s req res f a = forall t. Traversable t =&gt; Task { prepared :: f (t res -&gt; (a, s res)) , required :: s req -&gt; t req }
Awesome work. Maybe this will also be a useful stepping stone towards inline-rust? I see that inline-java uses language-java.. I can hope :)
Coooooool. Looking forward to it working with Hoogle. Does this mean I can download all of say stackage and use this?
`digits` is quadratic. Rewrite it using `(:)` insread of `(++)`.
That's the next package I mentioned: https://github.com/harpocrates/inline-rust The main blocker at this point is the GHC patch being merged in... 
So there are these, that I've found very useful in various contexts: class Variant f where isomap :: (a -&gt; b) -&gt; (b -&gt; a) -&gt; f a -&gt; f b class Variant f =&gt; Productish f where punit :: f () (&lt;**&gt;) :: f a -&gt; f b -&gt; f (a,b) class Variant f =&gt; Summish f where svoid :: f Void (&lt;++&gt;) :: f a -&gt; f b -&gt; f (Either a b) These are actually [used internally](https://hackage.haskell.org/package/time-1.9.1/docs/src/Data-Format.html) in the time library for the `Format` type, which is used for [ISO 8601 formats](https://hackage.haskell.org/package/time-1.9.1/docs/Data-Time-Format-ISO8601.html). Basically ISO 8601 specifies a number of microformats for various time types, such as `YYYY-MM-DD` for dates, or `±hh:mm` for time-zones. `Format t` is the type of a formatter for `t`, that can both show and parse, i.e., convert both ways between `t` and `String`. Keeping the two kinds of functionality together like that simplifies the code and reduces the chance of defect. As a result, `Format` is neither covariant nor contravariant in its argument, but it is both "productish" and "summish". In particular, `&lt;**&gt;` does concatentation, while `&lt;++&gt;` does alternatives.
There's "Witherable/Filterable/FunctorMaybe": class Functor f =&gt; Filterable f where mapMaybe :: (a -&gt; Maybe b) -&gt; f a -&gt; f b it's simple but clean. It represents structures that can be empty, which provides the ability to shrink it with any predicate. For example, `[]` is Filterable, `((-&gt;) e)` isn't (nor `NonEmpty`). https://hackage.haskell.org/package/witherable-0.2/docs/Data-Witherable.html
I think [monad modules](http://coalg.org/calco15/papers/p18-Pir%C3%B3g.pdf) are kind of interesting. Right modules especially. class (Functor f, Monad m) =&gt; RightModule m f where -- | Laws: -- absorb (fmap return a) = a -- absorb (absorb a) = absorb (fmap join a) absorb :: f (m a) -&gt; f a With monadic data structures, you can make some interesting modules. With `m ~ Maybe`, you get a type class for `filter` filterMap :: RightModule Maybe f =&gt; (a -&gt; Maybe b) -&gt; f a -&gt; f b filterMap f a = absorb $ fmap f a instance RightModule Maybe [] where absorb = catMaybes instance Ord k =&gt; RightModule Maybe (Map k) where absorb = Map.mapMaybe id I'd be really interested to see an interesting instance with `m ~ Tree`.
Er, those feel more like "Control" classes to me rather than "Data" ones, if that makes sense. They're interesting, though!
heroic! my two favorite languages these days :D
I haven't looked into the details yet, but if there are pre-built hoogle databases available to download and free to use, I will choose to use that. Stackage documentation, like [this one](https://s3.amazonaws.com/haddock.stackage.org/lts-10.4/bundle.tar.xz), will definitely be considered. :)
:D
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [AshleyYakeley/Truth/.../**Filterable.hs** (master → cf2c62a)](https://github.com/AshleyYakeley/Truth/blob/cf2c62abe39c006eacf6ca8b12744c61ddc0d4ca/shapes/src/Data/Filterable.hs) ---- 
A lot of the standard typeclasses have [rank 2 equivalents](https://hackage.haskell.org/package/rank2classes-1.0.1).
Tagging /u/mgattozzi as he was also working on Haskell-Rust interop.
There are trivial instances for all `f` satisfying `Alternative f`. I'll see if I can think of anything else. I think in the case of `ReadP` there can be slight differences in performance when the same list of parsers is `&lt;|&gt;`ed together with different precedences; but the values are still semantically the same.
Wow, sounds very promising. Some questions: 1. Is this using haddock docs? If not, then what? 2. When I install a package from hackage, either directly, or indirectly as a dependency, do I automatically get docs for it in doc-browser? If not, how do I get the docs?
There was a recent post about "zip" operation which fails if the structures of two containers do not match. instance (Functor t, Eq1 t) =&gt; Zippy t where zippy :: t a -&gt; t b -&gt; Maybe (t (a,b)) with law 1. If there exists `tab` such that `ta = fst &lt;$&gt; tab` and `tb = snd &lt;$&gt; tab`, `zippy ta tb = Just tab`. 2. If there are no such value `tab`, then `zippy ta tb = Nothing` Example: Implemet pattern matching in your programming language. import Data.Foldable import Control.Monad.Free import Data.Functor.Foldable data DataCell x = Literal Int | ConsPair x x deriving (Functor, Foldable, Traversable) instance Zippy DataCell where ... type Value = Fix DataCell type Pattern = Free DataCell VarName type VarName = String zippyWith :: (Traversable t, Zippy t) =&gt; (a -&gt; b -&gt; Maybe c) -&gt; t a -&gt; t b -&gt; Maybe (t c) zippyWith f ta tb = zippy ta tb &gt;&gt;= traverse (uncurry f) match :: Pattern -&gt; Value -&gt; Maybe [(VarName, Value)] match (Pure x) value = Just [(x, value)] match (Free fmx) (Fix fv) = fold &lt;$&gt; zipped where zipped :: Maybe (DataCell [(VarName, Value)) zipped = zippyWith match fmx fv
&gt;If-then-else isn't super idiomatic, I think in idiomatic Haskell it's mostly used in one-liners like liftMay above. But there's nothing inherently wrong with it, if it fits the situation then that's absolutely fine - but in real world code usually you're dealing with more complex conditions that are better expressed with a case block. Simple true/false conditions are better expressed at the type level! I think one problem with the if-then-else is that boolean variables don't carry information beyond their value and to interpret such a value you need to know about its purpose and origin. (Is it the result of `isNull` or `not . isNull`?) In this case, you also lose the exhaustiveness check for pattern matching (-Wincomplete-patterns), which would be available for this specific problem. My one-off definition to avoid an import would simply be headMay [] = Nothing headMay (x:_) = Just x As a small bonus, I also get to use the wild card pattern `_` which keeps my hands off from irrelevant data.
&gt; Note that even though the order of the side-effects changes, the computed value is the same in both cases. What guarantees this?
I've been playing with something like those recently, trying to join together the monoidal form of `Applicative` / `Alternative` with `Divisible` / `Decidable` from `contravariant`. I think there are some wins to be had with something like `generics-eot` but with left-associated tuples and Eithers so that `&lt;**&gt;` and `&lt;++&gt;` can be left associated and behave like you would expect.
You might want to read [my comment](https://www.reddit.com/r/haskell/comments/81m1bq/im_a_haskell_noob_and_i_started_a_haskell_blog/dv5tlso/?context=3&amp;st=jecq3pq7&amp;sh=5bc0b7ed) as well.
Ah, I should qualify. (f &lt;$&gt; request x &lt;*&gt; prepare y) `evalPlan` traverse g will compute the same value as flip f &lt;$&gt; y &lt;*&gt; g x Depending on how the particular applicative works, this may or may not compute the same value as f &lt;$&gt; g x &lt;*&gt; y
 listToMaybe [x] = Just x listToMaybe _ = Nothing This also seems relatively reasonable to me.
You surely are aware of `Align`, `Unalign`, `Crosswalk` and `Bicrosswalk` from [`these`](http://hackage.haskell.org/package/these-0.7.4/docs/Data-Align.html)?
A possible candidate is the Semilattice typeclass. They form the basis of CRDTs, LVars and other distributed(and concurrent) system goodies. It and its variants(Join,Meet, Bounded) obey associativity, commutative and idempotence laws. For reference: https://hackage.haskell.org/package/lattices-1.7.1/docs/Algebra-Lattice.html
&gt; with law &gt; &gt; 1. If there exists `tab` such that `ta = fst &lt;$&gt; tab` and `tb = snd &lt;$&gt; tab`, `zippy ta tb = Just tab`. &gt; &gt; 2. If there are no such value `tab`, then `zippy ta tb = Nothing` Can you sketch out a proof that this is equivalent to the laws I proposed in my post?
&gt; There are trivial instances for all `f` satisfying `Alternative f`. I'll see if I can think of anything else. Are there? Isn't that only with left modules?
Not sure if this all is derivable from some place else, but w/ dependent types, having class Shrinkable (f :: Nat -&gt; * -&gt; *) where shrink :: (Monoid m) =&gt; { m &gt;= n } -&gt; f m a -&gt; f n a so something. Idk how -XDependentTypes would deal with proofs like m &gt;= n in the typesig, so I just wrote it implicitly a la Idris. Doing so lets you write concat for Vecs, with type Vec n a -&gt; Vec n a, just mconcatting as necessary Since you can show that Vec n (Vec n a) &lt;=&gt; Vec n^2 a, and n^2 &gt;= n forall n. 
`Foldable` actually has laws. They are even specified in in documentation: * https://hackage.haskell.org/package/base-4.10.1.0/docs/Prelude.html#t:Foldable
I apologize that I didn't reference your original post! I misremembered that with the law I also have been cooking in mine head. They seem almost equivalent. My version of the law is stricter than yours since your version allows a case of `zippy a (fmap f a) = Nothing` but my version doesn't. If you have additionally (Naturality) zippy (fmap f a) (fmap g b) = fmap (fmap (f *** g)) (zippy a b) Then they are equivalent. I didn't check rigorously, but this equation seems 'theorems for free'-type theorem which holds always in Haskell. Thus these are almost equivalent(?).
In my original post, I explicitly relied on parametricity, otherwise I would've required naturality. 
Personally, I have a feeling that type classes for data structures and containers are not explored well enough. 1. `Foldable1` for non empty foldables * https://ghc.haskell.org/trac/ghc/ticket/13573 2. `Foldable` can be implemented differently with the help of type families to allow `length`, `foldr` and other functions to work on `Text` and `Int`. 3. You can see some type classes for containers like `Set, HashMap` in `mono-traversable` package: * https://hackage.haskell.org/package/mono-traversable-1.0.8.1/docs/Data-Containers.html 4. Another approach to single type class for containers * https://stackoverflow.com/questions/48848571/generalised-newtype-deriving-on-class-functions-with-functors class C t where alterF :: Functor f =&gt; (Maybe (Value t) -&gt; f (Maybe (Value t))) -&gt; Key t -&gt; t -&gt; f t 5. There's good old `IsList` type class in `base` for `toList/fromList` pair of functions. Sometimes useful. But probably not used very often. 6. Well, there're also `Bifoldable` and `Bitraversable` (if you mentioned `Foldable` and `Traversable`). For example, if [`universum`](https://github.com/serokell/universum) package we came up with `Container` type class which is like `Foldable` but also for monomorphic containers and with some extra tricks to make functions like `elem` fast for `Set` and `HashSet` (`elem` from `Foldable` works in `O(n)` time for `Set`).
Or you can just use `Named` library: * https://hackage.haskell.org/package/named
&gt; I've been playing with something like those recently, trying to join together the monoidal form of `Applicative` / `Alternative` with `Divisible` / `Decidable` from `contravariant`. Nice, what does that look like? `contravariant` is a nice concept, but its API is pretty bare-bones. Last time I played with it, I made a wrapper for it which looks like this: -- only accepts the pair ("foo", "bar") fooAndBar :: Predicate (String, String) fooAndBar = matchProduct [ Case _1 $ Predicate (== "foo") , Case _2 $ Predicate (== "bar") ] -- only accepts the values (Left "foo") and (Right "bar") fooOrBar :: Predicate (Either String String) fooOrBar = matchSum [ Case _Left $ Predicate (== "foo") , Case _Right $ Predicate (== "bar") ] Those examples use `(,)` and `Either`, which is precisely the two types supported by `contravariant`'s bare bones API, but the advantage of this API is that `matchProduct` also works with any `Divisible` product type as long as you have lenses to designate the fields, and `matchSum` also work with any `Decidable` sum type as long as you have prisms to designate the constructors.
I missed again, my bad! But then these laws were equivalent. I'll dump my proof here. viercc ==&gt; tailcalled Diagonal: &gt; ta = fmap fst (fmap dup ta) &gt; ta = fmap snd (fmap dup ta) Holds. By applying viercc.1, &gt; zippy ta ta = Just (fmap dup ta) Preservation: Can assume following. &gt; zippy ta tb = Just tc If both &gt; fmap fst tc = ta &gt; fmap snd tc = tb don't hold, it contradicts viercc.1. tailcalled ==&gt; viercc viercc.1: &gt; ta = fmap fst tab &gt; tb = fmap snd tab holds. Using (Naturality), &gt; zippy ta tb = zippy (fmap fst tab) (fmap snd tab) = fmap (fmap (fst *** snd)) (zippy tab tab) Using Diagonal, and (fst *** snd) = id, &gt; zippy ta tb = fmap (fmap id) (Just tab) = Just tab viercc.2: There are no 'tab' such that &gt; ta = fmap fst tab &gt; tb = fmap snd tab holds. If &gt; zippy ta tb = Just tc holds, from Preservation, &gt; fmap fst tc = ta &gt; fmap snd tc = tb thus tab=tc is the counterexample to the assumption. 
People have so many different ideas and preferences for alignment (my personal favourite perversion is using a variable-width typeface). Wouldn't it be better to add support for some kind of custom presentation in the editor instead of attempting to embed it into the source code?
In the above example, add takes three arguments and returns a result. * (int -&gt; int -&gt; int) is the first argument- a function that takes two ints and returns an int. * the second argument is a result * the third argument is a result Here's a simple example of a useless function that applies a function given as an argument: doOperation :: (Int -&gt; Int -&gt; Int) -&gt; Int -&gt; Int -&gt; Int doOperation func arg1 arg2 = func arg1 arg2 doOperation add 1 5 &gt; 6 doOperation subtract 10 5 &gt; 5 See more info at this [stackoverflow answer](https://stackoverflow.com/questions/34136852/what-is-the-meaning-of-parentheses-in-haskell-type-signatures)
I looked through Hackage at the time, I don't remember seeing that package in particular, although the dates indicate it was there.
One other note, I'm sure OP meant `Int`, but as the types are written in lowercase, GHC will see them as polymorphic types. `add :: (int -&gt; int -&gt; int) -&gt; result-&gt; result -&gt; result` is the same as `add :: (a-&gt; a -&gt; a) -&gt; b -&gt; b -&gt; b`. Bottom line is `int != Int`. Don't know if that was something causing OP extra confusion.
Yes, quite possibly and this is what I was expecting to happen really. Sadly the original code was only able to measure GC pauses as opposed to actual useful latency which is why it's not transparent. This is why in the end in my conclusion I say that it's not really good for data that keeps changing like the case study. I should have mentioned this explicitly however. This is still not necessarily all that bad however because you have much more control over when pauses happen. If your use-case is a bit more accommodating than "single process under pressure all the time" then you can do things like load-balance traffic to another process while you run a copy on this one. So there is a difference. A big difference: calling `performGC` actually buys you nothing because once you've done that (and paid for a pause), GHC is going to run GC soon after again anyway (and you'll pay again). With compact regions you pay it exactly once every `n` messages.
This is basically a functor, from the category of `(&gt;=)` type Cat ob = ob -&gt; ob -&gt; Type data (&gt;=) :: Cat N where Base :: (n &gt;= O) Step :: (n &gt;= m) -&gt; (S n &gt;= S m) to the functor category `(~&gt;)` -- (~&gt;) :: Cat (k -&gt; Type) type f ~&gt; g = forall xx. f xx -&gt; g xx ---- map :: (n &gt;= m) -&gt; (Vec n ~&gt; Vec m) map Base _ = VNil map (Step n) (VCons x xs) = VCons x (vmap n xs) haven't checked the laws
https://github.com/harpocrates/inline-rust 
Man I love `these`
Were the 20 minutes spent compiling just Hakyll or is that including all of its dependencies?
Thanks StudentRadical, this is good information. :)
Why not try it and post the results? It would be interesting to know.
Just checked out `these` - `Align` would have come in handy in the part. I'm also having a hard time determining what `Crosswalk` is for.
That's good to hear! You can read a bit more about the topic in [this](https://existentialtype.wordpress.com/2011/03/15/boolean-blindness/) blog post, if you're curious.
These kind of tools would benefit all Haskell database libraries, especially if one of them becomes a shared resource. It would be awesome for "I am converting my application with lots of SQL from language X into Haskell", where library authors provided potential users with import tools to convert SQL into their library's abstractions.
I ran into a bug in `llvm-hs` the other day that was fixed by switching to the lazy state monad. I wrote a blogpost on a simplified version if you’re interested https://purelyfunctional.org/posts/2018-03-04-monadfix-lazy-strict-state.html
Do you know how I can get "ghc-boot-th-8.4.0"?
You might be interested in [this library](https://www.google.ca/search?client=ms-unknown&amp;ei=MWCcWvyHJoTgjwTtnJSACg&amp;q=parsing+with+prisms&amp;oq=parsing+with+prisms&amp;gs_l=mobile-gws-serp.3...3813.5903..6724...0....170.648.0j4..........1..mobile-gws-wiz-serp.......0i67j0i7i30.RN38idVAjNM%3D), which relaxes the functor type to `Prism' a b -&gt; f a -&gt; f b`. I was also playing around with a variant functor type to use with catamorphisms, since they have their type in both positive and negative position Though I'm still not sure how useful the idea is.
Don't worry; I expect we'll have the patch merged by 8.6.
That feels almost like more trouble than it's worth, honestly, not to mention a huge waste of space if your arguments are named anything less than superLongNameArg
Does #2 come worth any of the usual downsides I've seen mentioned before with shoehorning text into being an instance of the foldable typeclass?
 ... ++ ")" ++ eval e ++ "\nend" ... `eval e` is not a `String`, it is of type `IO ()`. As a rule of thumb, try to write as much of your code without IO (pure) and only introduce IO as a final "wrapper" around your pure code. For example, try writing your code in the following style: -- this is pure code eval :: Expr -&gt; String eval = ... -- code with side effects printExpr :: Expr -&gt; IO String printExpr e = putStrLn (eval e)
I rarely use very long names for my variables, but I do use long names for my constructors, namely the name of the type plus some suffix. So in situations where the types are long enough for the signature to be split into multiple lines, the argument patterns are likely to be even longer!
I think your suggestion is what I’d really like to have, especially one allowing graphical display and editing (e.g., data flow diagrams).
Same with `result`. It is not a concrete type, but completely polymorphic type variable. If written like this, there are really only a few such polymorphic functions (3?): add :: (a -&gt; a -&gt; a) -&gt; b -&gt; b -&gt; b -- one such possible function add _ x _ = x -- another such function add _ _ y = y -- yeah and then this abomination add _ _ _ = undefined
Ooh interesting, thanks.
This looks like a cool solution to the Foldable laws problem https://ifl2014.github.io/submissions/ifl2014_submission_12.pdf
This is included with the GHC installation. $ stack exec -- ghc-pkg list ghc-boot-th /Users/dan/.stack/programs/x86_64-osx/ghc-8.4.0.20180224/lib/ghc-8.4.0.20180224/package.conf.d ghc-boot-th-8.4.0.20180224 /Users/dan/.stack/snapshots/x86_64-osx/ghc-8.4.0.20180224/8.4.0.20180224/pkgdb (no packages) /Users/dan/scratch/ghc-boot-test/.stack-work/install/x86_64-osx/ghc-8.4.0.20180224/8.4.0.20180224/pkgdb (no packages) So try using it in your stack project, and it should just work. $ stack repl ghc-boot-th Prelude&gt; :browse GHC.Lexeme GHC.Lexeme.isVarSymChar :: Char -&gt; Bool GHC.Lexeme.okSymChar :: Char -&gt; Bool GHC.Lexeme.startsConId :: Char -&gt; Bool GHC.Lexeme.startsConSym :: Char -&gt; Bool GHC.Lexeme.startsVarId :: Char -&gt; Bool GHC.Lexeme.startsVarSym :: Char -&gt; Bool GHC.Lexeme.startsVarSymASCII :: Char -&gt; Bool 
the thing is...it doesn't 😀 Only ghc-boot-th 8.2 is working
Thank you. That makes sense and i made it work! 
&gt; if it didn't we'd all be using a..z, aa..az, ba..bz, etc. In Haskell, it often feels like people have a tendency to do precisely this.
The [Clowns to the Left of me, Jokers to the Right](http://strictlypositive.org/CJ.pdf) paper defines a type class for dissectible functors which could be a subclass of `Traversable`. I've used `Dissectible` to [implement safe zipper-like traversals](http://blog.functorial.com/posts/2017-06-18-Stack-Safe-Traversals-via-Dissection.html) in PureScript, and I'd be interested to see it used more in Haskell too.
My `cabal.project.freeze` is: constraints: any.appar ==0.1.4, any.array ==0.5.2.0, any.async ==2.2.1, async -bench, any.attoparsec ==0.13.2.2, attoparsec -developer, any.auto-update ==0.1.4, any.base ==4.10.1.0, any.binary ==0.8.5.1, any.blaze-builder ==0.4.0.2, any.byteorder ==1.0.4, any.bytestring ==0.10.8.2, any.bytestring-builder ==0.10.8.1.0, bytestring-builder +bytestring_has_builder, any.case-insensitive ==1.2.0.10, any.containers ==0.5.10.2, any.deepseq ==1.4.3.0, any.directory ==1.3.0.2, any.filepath ==1.4.1.2, any.ghc-prim ==0.5.1.1, any.hashable ==1.2.6.1, hashable -examples +integer-gmp +sse2 -sse41, any.http-date ==0.0.6.1, any.http-types ==0.12.1, any.http2 ==1.6.3, http2 -devel, any.integer-gmp ==1.0.1.0, any.integer-logarithms ==1.0.2, integer-logarithms -check-bounds +integer-gmp, any.iproute ==1.7.2, any.network ==2.6.3.3, any.primitive ==0.6.3.0, any.process ==1.6.1.0, any.psqueues ==0.2.5.0, any.random ==1.1, any.rts ==1.0, any.scientific ==0.3.5.2, scientific -bytestring-builder -integer-simple, any.semigroups ==0.18.4, semigroups +binary +bytestring -bytestring-builder +containers +deepseq +hashable +tagged +text +transformers +unordered-containers, any.simple-sendfile ==0.2.27, simple-sendfile +allow-bsd, any.stm ==2.4.5.0, any.streaming-commons ==0.1.19, streaming-commons -use-bytestring-builder, any.text ==1.2.3.0, text -bytestring-builder -developer -integer-simple, any.time ==1.8.0.2, any.transformers ==0.5.2.0, any.unix ==2.7.2.2, any.unix-compat ==0.5.0.1, unix-compat -old-time, any.unordered-containers ==0.2.9.0, unordered-containers -debug, any.vault ==0.3.1.0, vault +useghc, any.wai ==3.2.1.1, any.warp ==3.2.17, warp +allow-sendfilefd -network-bytestring -warp-debug, any.word8 ==0.1.3, any.zlib ==0.6.1.2, zlib -non-blocking-ffi
Was there anything in the answers to your previous question a few hours ago that was confusing? There were a few examples of doing that in that thread
Is this written in Haskell?
I am going :)
So my real function to my assignment is to create a interpreter in haskell, I wanted to see how to do it with a simpler function and how it works before I attempt to work it out on my homework. https://i.imgur.com/eOhBv0R.png 
[This](https://gist.github.com/dalaing/4ff035b1e355f66eedf873e6168c0f20) is what I have at the moment (after scratching around a little [here](https://gist.github.com/dalaing/c27dbe3eb209d399197b2a43deb839ff) on Friday). The punchline at the end of that first link is: data Identifier = StringId String | IntId Int deriving (Eq, Ord, Show, Generic) smIdentifier :: StringMe Identifier smIdentifier = eotSum $ smString &gt;*&lt; munit () &gt;|&lt; smInt &gt;*&lt; munit () &gt;|&lt; dunit data Blob = Blob Int Identifier Bool deriving (Eq, Ord, Show, Generic) smBlob :: StringMe Blob smBlob = eotProduct $ smInt &gt;* spaces &gt;*&lt; smIdentifier &gt;* spaces &gt;*&lt; smBool &gt;*&lt; munit () -- &gt; let s = runSerializer (contra smBlob) (Blob 2 (IntId 4) False) -- &gt; s -- "2 4 False" -- &gt; runParser (co smBlob) s -- Just ("", Blob 2 (IntId 4) False) 
Thanks!
You have two random functions and two random values.
You could be right
&gt; https://hackage.haskell.org/package/named That's an awesome library. Why haven't it talked about before?
I'm curious, why do you say it's not like the math you're familiar with? I learned Haskell from a mostly CS (and undergrad math) background. The concepts that were really aha moments for me were mostly the higher level math concepts. Maybe they don't seem as satisfying to you because they're already familiar? Either way, I'd definitely recommend the youtube series 'Category Theory for Programmers' by Bartosz Milewski. He teaches category theory and how it relates to Haskell. And if you don't have time for the entire series, the first lecture alone is very 'aha moment' conducive. He ties together math, computer science, philosophy, anthropology and even physics in a very interesting way. https://www.youtube.com/watch?v=I8LbkfSSR58
There are `Foldable`s that can't be `Buildable`s, for example `(,) x`.
oh looks like a XY-problem (based on your now deleted comment - linking to this https://i.imgur.com/L4LSZaq.png) see: https://en.wikipedia.org/wiki/XY_problem ;) anyway if I get you right you have some difficulties with the `liftOp` there right? Now this one is not really very similar to the problem you discussed here - IMO the main problem is how to deal with the `Result` type. There are very general solutions but based on what you got I don't think you are at this point in your course yet but I guess you have probably seen examples of how to deal with `Maybe a` values? This one is very similar - try to pattern match on the two results you are given: liftOp op (Ok a) (Ok b) = undefined liftOp op (Ok a) (Error errB) = undefined liftOp op (Error errA) (Ok b) = undefined liftOp op (Error errA) (Error errB) = undefined and try to figure out what the right sides should be (basically the hint is right there so I hope I don't spoiled anything)
**XY problem** The XY problem is a communication problem often found in help desk or similar situations, the cause of a problem is masked because the person asking for help has presented incomplete information as to the source of their problem. This ambiguity in the real source of a problem leads to wrong, inaccurate, or unhelpful solutions being offered. The issue arises when the person with the problem thinks that they themselves have a partial solution to their problem, and only ask for the parts they think they are "stuck" on. On the other side, the people offering to help lack information as to the root problem, and thus cannot provide ultimately useful information. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
The biggest aha moment for me, with regard to the mathematics of Haskell, was actually reading a description of the category Hask (the objects are types and the arrows are functions, if you haven’t seen it already). I had read Mac Lane and I had been programming in Haskell, but I just couldn’t see the connection between the Haskell typeclasses and the mathematical definition until then. Haskell will probably continue to feel quite different from pure math, though. Haskell borrows a lot from category theory and there are deep connections between typed functional languages and formal logic, but the goal is ultimately to write code. All of this math exists in Haskell so that programmers can be more confident that their code works correctly. Haskell thus tends to “cherry-pick” the parts of category theory that correspond to common programming tasks, and it’s not unusual for an obscure or very specific topic in category theory (strong lax monoidal functor = Applicative) to be common while major topics like Adjunctions are not so common. 
I am almost certain that this is because of warp version. I am using 3.2.12 and you tested it with 3.2.17 I think problem was fixed between those versions. To test it, I used stackage nightly build instead of LTS which includes warp version 3.2.17. As a result of that change memory usage went up to 98MB but it it not go more than that which is a reasonable memory usage in my opinion. 
your second version seems a bit strange I guess you wanted something like addResult f = ((+ 5) .) . f (here with `5` instead of `number` - in case you want to try) not so sure if I like this (or the even worse `addResult f = (((+ 5) .) .)` though
Looks like F#.
Agreed, and forgot to add that to my comment. The aha moments are mostly on the math side. Haskell is really just a tool to implement them and so it seems maybe less satisfying learning it. Unless you're really gung-ho about needing to write some software.
I did. This idea floating around that "Haskell is like maths and category theory" is quite a strange one and not particularly helpful to our community, I don't think.
I studied pure math in uni, although I don't have a PhD or anything. I worked as a developer for a couple years before I discovered Haskell, but when I discovered LYAH, it was just 2-3 days before I was like "wow, this is where I belong." It wasn't any one thing in particular, it was the just holistic approach of doing everything (well...) in a sensible way - using math functions (as opposed to subroutines) to lift ourselves into a world we can actually reason about, focusing on notions of composition which preserve the properties of the original pieces being composed, an algebraic respect of neutral elements (not conflating 0/""/false/null/NaN/etc) and a formal statement of how they respect composition (`neutral &lt;&gt; x = x`), a general deference to mathematically "good" behavior like `fromX . toX` should equal `id`, etc. In contrast, what I'd heard before was "This is the Strategy Pattern (TM) and we take our Animal class and our Dog class and... *blah blah*" Anyway, most of our fancier structures are just Variations of a Theme of Composition. The part that I think confuses people is that sometimes the language internally respects our categorical semantics (modulo bottoms and nonsense) with stuff like `f . id = f`, whereas sometimes the categorical structure we're referring to is not "understood" by the Haskell itself, e.g., `Maybe :.: Identity = Maybe`. Obviously we're talking about the same abstract notion in both of these cases, but in the first case (function composition) you can actually use the right side anywhere you can use the left side, but in the latter case (functor composition), Haskell actually will not allow you to use `Maybe` if it is expecting `Maybe Identity`. So there is often some impedance between what we're saying mathematically and what the language actually accepts.
**Log-structured merge-tree** In computer science, the log-structured merge-tree (or LSM tree) is a data structure with performance characteristics that make it attractive for providing indexed access to files with high insert volume, such as transactional log data. LSM trees, like other search trees, maintain key-value pairs. LSM trees maintain data in two or more separate structures, each of which is optimized for its respective underlying storage medium; data is synchronized between the two structures efficiently, in batches. One simple version of the LSM tree is a two-level LSM tree. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
I have been on a bit of a roll, and so have much more coming. I might make a post of my own once it all comes together.
I suppose, in the end, people have different experiences about doing mathematics, as well as about programming in Haskell. I've found that the experience of learning Haskell was comparable to working on mathematics, though obviously not in every way. You certainly can't be too literal about the comparison. They are different activities, of course, but share some of the same flavor.
Reason why it's aligning to the first character of an expression is that `=` is just assignment, with left value, and right value. Aligning parts of right value with `=` would break that at least visually.
Well, it appeared only recently. About week or two ago. Maybe author wants to polish library better before announcing it on Reddit.
I didn't see discussed downsides before. The only downsides I can tell is the one I've experienced personally so far. Specifically: 1. Type signatures are scarier. Some type signatures are merely impossible to write. Or they require some constraint synonyms. 2. Some people don't like polymorphic functions. Though, it's only about `Text`. It's also about `ByteString`, both lazy and strict plus `IntSet`.
You can also do the periodic copying in a separate thread and use multiple cores, to avoid affecting latency. So even though you're doing the same GC work that GHC would normally be doing, compaction can be done concurrently with the mutator, whereas normal GC currently cannot.
Or /r/haskellbook for questions about the Haskell book
So me, u/int_index and u/effectfully have been running a Haskell consultancy for several months by now, and we've asked for permission to publish some of our answers for the benefit of ~~people who google a random set of obscure things~~ the community. Here they are (with more coming; subscribe to our RSS feed if you want to).
That's a very good point. I wish I thought of it at the time of writing. I'll keep it in mind for the future.
You're right: Haskell doesn't feel like math. There are three big reasons for this. 1. First, Haskell's use of quantifiers is weird. That is, when you see a universal or existential quantifier in a typical mathematical definition, it will usually range over individual objects. For example, the epsilon-delta definition of a limit is something like: &gt; L is the limit as f approaches c when for every ε &gt; 0, there is a δ such that for all x, if |x - c| &lt; δ then |f(x) - L| &lt; ε Note that every quantifier in that expression talks about *individual* real numbers. In contrast, in Haskell it is impossible (without turning on a bunch of extensions) to ever quantify over individuals. Instead, Haskell has pure second (and higher-) order quantification. While mathematicians do use second order quantification, it's a much rarer thing to do than in Haskell programming. Basically, Haskell's type system makes it easier to talk about categorical limits than limits of real-valued functions, and this is just a really bizarre choice for a normal mathematician. 2. Haskell is *way* more typed than category theory. You can motivate category theory to a regular mathematician by explaining that it is a structural approach to mathematics, where you make and enforce type distinctions to ensure that doing proofs naturally have the degree of hygiene that marks good mathematical style. However, category theory is *much* less typed than Haskell. In category theory, you are free to make categories out of anything you like -- for example, given a category C you can form the arrow category from the morphisms of C and pairs of morphisms creating commuting squares. In Haskell, you can't do this, because the type/term distinction is ironclad and can never be breached. Since there's a lot of typeclass machinery built on the idea that types are objects and terms are morphisms, this means that a lot of ordinary category theory is out of reach in Haskell. 3. Haskell's design is heavily influenced by type theory, and type theory is logic, not math. On top of that, type theory is a branch of structural proof theory, which is a minority view even amongst logicians. This is a sociological, rather than a technical, distinction, but it is nonetheless a real one. Most mathematicians never learn any substantial amount of logic: I've even seen a Fields medalist get confused by (for example) the distinction between a proof by contradiction and a proof of a negation. Obviously, he understood instantly once it was explained, but the important point was that he had never needed the distinction in a very distinguished career! As a result, many of the guiding stars of the design of Haskell (eg, parametricity) are not well-known ideas amongst normal mathematicians. 
I think "Dirt Cheap Haskell Consultancy" is a great idea. Good luck with it! Since I was mentioned in the "arrows" question I will make some comments on it. &gt; when you design an API, your type might turn out to be an arrow, and then you get proc-notation, but other than that I wouldn't bother Absolutely. If your type `arr` is an arrow then `arr ()` is very likely to be a monad and your life will be much easier if you just define `m = arr ()` and forget that arrows exist. Monads are much easier to understand than arrows, much easier to use, and and much better supported in the ecosystem. &gt; the author of opaleye mentioned once that if he had to rewrite it from scratch, he wouldn't use arrows I don't think I quite said that. There's no getting away from the fact that, in the denotation Opaleye uses, SQL queries are arrows and not monads (in the absence of lateral join, and with other caveats). If I write the tutorial again I might deemphasize arrows. Since Opaleye was written GHC has gained `ApplicativeDo` which allows one to write a lot of queries without arrows at all. &gt; to understand arrows you must first understand profunctors, then profunctor composition, and then strong profunctors. I would definitely not suggest following this route to understand arrows. `proc`/`do` notation allows one to get an intuitive grasp of what one can do with arrows without understanding any technicalities. I'd suggest starting there and then perhaps never learning anything more about arrows (or profunctors). &gt; Ed. Kmett refers to them as a “historical accident” In that case a very happy little accident!
These are some fantastic answers.
I'd like to recommend reading about lambda calculus (and other formal / rewriting systems) and combinatory logic since the research done on them greatly influenced functional programming languages (especially at the early stages). Type systems, closures, reduction strategies, parameter passing strategies, graph reduction, etc. Since you're a mathematician you probably didn't have to study functional calculi and type systems until this moment, but they are simple, interesting, very important for functional languages but also having really beautiful philosophical meanings (especially when thinking about computation, Turing Machines, etc.). It's nice to see how you can do everything with so little, and you can easily implement a (typed / untyped) lambda calculus interpreter / compiler in your free time if you want.
Wow, these are cool, thanks a lot for sharing them!
Thanks for your comments, Tom! &gt; I don't think I quite said that. I do have a memory of you saying something along these lines, but apparently my memory is faulty, as I've been unable to find the specific comment — sorry for misrepresenting your position. But then again, wouldn't you agree that the `Arrow` class isn't essential to the design of Opaleye? It's only used to get the `proc` notation, not as a core abstraction. The internals mostly use `ProductProfunctor` instead?
With `ApplicativeDo`, do you have example queries that still need arrows?
It's funny that you're answering questions from Elliot Cameron who works for ~~FPComplete~~ Obsidian Systems, also a consultancy. You are the consultants' consultants! :)
What would "`Coercible` at the value level" look like?
Wouldn't that distinction anyway become moot soon, when Dependent Haskell lands?
Excellent post, clear and complete explanation of these things. Thanks Ryan!
A parser _and_ a pretty-printer, just like `invertible-syntax`? Neat! Just to make sure, though: I only brought up that library in order to explain where I took the idea of an `(&lt;*&gt;)` with type `f a -&gt; f b -&gt; f (a, b)` instead of `f (a -&gt; b) -&gt; f a -&gt; f b`, I don't think invertible parsers are a good way to show off `contravariant` APIs. Quite the opposite: a good API for invertible parsers will require something like an `Iso` (or `Invariant` in your case), not a `(b -&gt; a)` function, so the API won't be usable with datatypes which only support `Contravariant` but not `Invariant`. In any case, I really like how you use `Pair` to make your bidirectional `(&gt;*&lt;)` and `(&gt;|&lt;)` operators work with any pair of `Applicative`/`Decidable` functors!
Fractional cascading built as a little succinct rank structure is much more amenable to that treatment. No pointers.
This is awesome. Thanks to you and your clients!
Here's my understanding of this, which may have a good share of handwaving and mistakes since I'm also not a logician. 1. Yes, your example is a good demonstration. 2. Induction principles can be seen as encapsulating recursion so that you can write proof terms in a language without other recursive constructs. Then you only need to check that the induction principles you use are sound. 3. The keyword here seems to be "well-founded". Recursive calls must be made on "smaller arguments", for a well-founded ordering relation, which implies that a base case (possibly many) must be reached. 4. I'm not sure "circular reasoning" is a well-defined term. The point of termination is that it is an easy way to ensure that a proof system is consistent: there is a class of proofs ("normal forms") for which we can easily check that there is no proof of contradiction (i.e., there is no normal form of type `Void`); termination means that expressions reduce to normal forms and thus cannot lead to a contradiction, and it can be checked without having to actually reduce the expressions (which may be an expensive computation). 5. "Bottom" is part of a semantics, a way to assign expressions ("syntax") to the mathematical objects they represent. A semantics is proper to a specific language. For languages where expressions are guaranteed to terminate, there is no need for bottom to exist in their semantics, so there is no meaning in saying that "bottom inhabits all types".
&gt; ghc has to throw away some info because dwarf can't handle multiple source code origins? Correct. The fact that GHC performs so much simplification makes profiling quite tricky and pushes the DWARF specification to its limit. Getting truly useful DWARF-based profiling for Haskell will likely require some special tooling which can take of the richer debug information which GHC can produce.
&gt; But this is a serious problem. m (m a) is only ever nominally equal to m (T m a) when m a is nominally equal to T m a, and that can never happen! Is the first use of the word "nominally" in the sentence supposed to be "representationally"? What the sentence says is true, but it doesn't make sense in context.
Right now we [might could](https://en.wiktionary.org/wiki/might_could) create `Shrinkable` and it joins a multitude of others like `Functor`, `Contravariant`, `Profunctor`, `Bifunctor`, .. With [`hask`](https://gist.github.com/ekmett/b26363fc0f38777a637d) all of these become instances of a single `Functor` pattern instance Functor (Vec :: N -&gt; (Type -&gt; Type)) where type Dom Vec = (&gt;=) type Cod Vec = (~&gt;) -- type (~&gt;) = Nat (-&gt;) (-&gt;) fmap :: (n &gt;= m) -&gt; (Vec n ~&gt; Vec m) fmap = ...
This is an excellent exposition of both the problem and the solution. I never know what the Gl in RyanGlScott stands for, but I like to imagine that it should be read as Ryan Generics-Lord Scott.
I suppose so, but then gnd wouldn't work until maybe 2019 or so for monad transformers and, given the insane amount of transformers people love to write, that seems unfortunate :)
Thanks! That makes a lot sense, I thought arbitrary and coarbitrary could be passed around as the values themselves. I'll go to /r/haskellquestions next time!
Another discovery - `Traversable`-like behavior for `Map` via `Crosswalk`: &gt; sequenceL $ [M.fromList [('a', 1)], M.fromList [('b', 4), ('a', 9)], M.fromList [('c', 7)]] fromList [('a',[1,9]),('b',[4]),('c',[7])] Since `sequenceA` is not possible on `Map`. Notice the "auto-alignment" too, the collection of values of matching keys via the outer type. 
Thanks for clarifying. I was wondering if that would look strange in the public sphere.
I had been calling it `levelorder` for a few commits, and then changed for no good reason. Good reason to change it back. And the postorder is an honest mistake - comes from me skimming [this article on tree traversals](https://www.geeksforgeeks.org/tree-traversals-inorder-preorder-and-postorder/) and not giving postorder its due. I'll fix it. Thanks!
Great work and a really nice post, thanks!
First time I’ve really gotten a grip on type roles—great article!
Oops! Yes, that should indeed be "representationally". This should be fixed now.
Isn't part of the reason for calling the garbage collector to ensure that there isn't a major GC happening in the middle of a benchmark run? What if the first benchmark generates garbage that ends up in an old generation, but not enough to trigger a GC; and then during the next benchmark some more garbage pushes it over the edge and has to be collected.
[`Data.Coerce.coerce`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Coerce.html) is a great way to learn about them, we define 3 empty types with a single argument {-# Language KindSignatures, RoleAnnotations #-} import Data.Kind data Phantom :: Type -&gt; Type data Representational :: Type -&gt; Type data Nominal :: Type -&gt; Type where the argument of each type has a different role type role Phantom phantom type role Representational representational type role Nominal nominal Let's now ask GHC how to coerce from those types to themselves &gt;&gt; import Data.Coerce &gt;&gt; :set -XPartialTypeSignatures -Wno-partial-type-signatures A phantom argument is maximally permissive, it isn't used anywhere so we can coerce any `Phantom a` to any other `Phantom b` &gt;&gt; :t coerce :: Phantom _ -&gt; Phantom _ .. :: Phantom a -&gt; Phantom b A representational argument requires `Coercible a b` &gt;&gt; :t coerce :: Representational _ -&gt; Representational _ .. :: Coercible a b =&gt; Representational a -&gt; Representational b And a nominal argument can not be coerced to a different type (it requires `a ~ b`) &gt;&gt; :t coerce :: Nominal _ -&gt; Nominal _ .. :: Nominal a -&gt; Nominal a
I would be interested in such an extension, and I think others would too. This [trac ticket](https://ghc.haskell.org/trac/ghc/ticket/7495) you referred to has several interesting ideas about how to implement this. The main questions are: - Should this be compatible with the existing `OverloadedLists` machinery? (I'm not sure that it can be). - Should this provide sugar for pattern matching or for enumeration syntax? (I think not). In my mind, the simplest approach is just to have a separate `OverloadedHeterogeneousLists` extension that is completely incompatible with `OverloadedLists`. Anyone is free to [make a proposal](https://github.com/ghc-proposals/ghc-proposals) detailing the how such an extension would work, and the GHC steering committee will then make the call. In my experience, this proposal process (which has only been in place for a year and a half) has generated really good feedback and has provided clear decisions on what is accepted and what is rejected.
What do you mean by “document”? Just copy some answers to StackOverflow? &gt; Are these questions and answers CC-licensed? At the moment – no. This would have to be negotiated additionally with our clients, I think. In addition, at least some of us don't want the answers to be changed without permission, which (AFAIK) automatically means that CC licenses can't be applied.
I think it's just waiting for someone to work on it and make a proposal. It doesn't seem too difficult to build something on top of `RebindableSyntax` to allow rebinding of `(:)` and `[]`, although there are certainly many details to think about, such as whether to rename `(:)` and `[]`, whether that should work at the type-level too, how it should interact with `OverloadedLists`... Interest is probably low because custom infix constructors are really not that bad an approximation of list syntax.
Ryan's post is now out * Blog post: [How QuantifiedConstraints can let us put join back in Monad](https://ryanglscott.github.io/2018/03/04/how-quantifiedconstraints-can-let-us-put-join-back-in-monad/) * [Reddit discussion](https://www.reddit.com/r/haskell/comments/8257mz/how_quantifiedconstraints_can_let_us_put_join/)
I took a graduate level Category Theory course at university, and it never really clicked for me. When I started learning Haskell, I had a new set of examples to motivate things, and I found it much easier to make progress. Every CT construction could be interpreted with the aid of the new examples (limits, colimits), and new things I was learning in Haskell (monads, comonads) made it easier to understand the underlying theory. I also like the think of categories as "generalized programming languages" sometimes (things with types and composition) and see if that lends any useful intuitions to problems. Incidentally, I had seen Haskell a few times before, but the thing that really made me sit up and pay attention was when a friend showed me Wadler's "Recursive types for free!". It gave me a brand new application of some of the category theory I knew already. From then, it took me a while to stop tinkering around with math in Haskell and really use it for something "real world"
Its doable, but making things have the right ease of inference vs extensibility is tricky! https://github.com/cartazio/HetList/blob/master/HetList.hs was one foray i did towards this (I believe i voluntold myself at HIW ~2-3 years ago to add this to ghc) what I got stymied by was how to provide a desugaring that works on both pattern matching and construction side and has composable type inference. Just doing overloaded names seems unsatisfactory (at least to me)
Thanks, I'll look at it later!
I think this should be distinct, as `-XOverloadedTuples`. It leverages the intuition that tuples of different sizes have different types, unlike OverloadedLists, where adding or removing elements (and just reordering them) doesn't change the type. i.e. `(x,y,...)` expressions can be heterogenous, while `[x0,x1,,...]` Expressions must be homogeneous.
And when `f ~ m`, `absorb` must be `join`, right? Like concat. (Since monads are themselves functors.)
I dunno if that's a "must" or not. Certainly you can prove that when `f ~ m`, an instance *can* exist with `absorb = join`. But I dunno if that means it *must*
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rust] [X-Post from r\/haskell: \[ANN\] language-rust](https://www.reddit.com/r/rust/comments/828sxu/xpost_from_rhaskell_ann_languagerust/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Does it differ from? type ContT f m a = Codensity (Compose m f) a
What's the salary range?
Heh, I actually thought fast-tags was less accurate (or at least I heard so; never tried it myself). Are there any drawbacks, compared to hasktags?
Probably not. The instances seemed like the only possible ones. What does this mean?
I should add that I certainly didn't intend to make you or Obsidian Systems look bad with my comment.
With the above definition, `lift` has type `Compose m f a -&gt; ContT f m a`, which isn't quite what you want. This can be fixed with a newtype wrapper though
`UndecidableInstances` isn't an evil pragma at all :) https://dirtcheaphaskell.io/#2017-12-18-undecidableinstances
You could build a list of messages that need to get added after the copy operation is done. Meanwhile you can still insert messages into the old map until the other thread is done with the copying and working through the list.
It could be made compatible I'm pretty sure: [1, 2, 3] Could desugar to: fromHList $ 1 :* 2 :* 3 :* Nil Where: class FromHetList (xs :: [*]) a where fromHetList :: HList xs -&gt; a instance FromHList xs [a] =&gt; FromHList (a ': xs) [a] where ... instance FromHList '[] [a] where ... instance FromHList xs [a] =&gt; FromHList (a ': xs) (NonEmpty a) where ... instance (Ord k, FromHList xs (Map k v)) =&gt; FromHList ((k, v) ': xs) (Map k v) where ... And so on:
Hi all, finally got the ol' site up. Purescript frontend, `servant` backend. Note that the RSS feed is language-specific - if you're in English mode, the feed will only give you English posts. Right now all posts are available in both languages, but they'll probably diverge over time. Question for the r/haskell mods: is it poor etiquette to post links to new blog posts here?
Very interesting. I'm also hoping that `QuantifiedConstraints` will allow us to make `Profunctor` and `Bifunctor` subclasses of `Functor`.
Thanks for the link, these look great, I'll check them out!
`Arrow` *is* essential to the particular denotational semantics of queries in Opaleye. You can weaken it to `Monad` if you're willing to accept lateral joins or some type unsafety. The `ProductProfunctor` bits are more or less orthogonal to the `Arrow`bits.
I believe so. At least, the following typechecks using the [`QuantifiedConstraints` branch](http://git.haskell.org/ghc.git/shortlog/refs/heads/wip/T2893) of GHC: {-# LANGUAGE QuantifiedConstraints #-} class (forall a. Functor (p a)) =&gt; Bifunctor p where bimap :: (a -&gt; b) -&gt; (c -&gt; d) -&gt; p a b -&gt; p c d class (forall a. Functor (p a)) =&gt; Profunctor p where dimap :: (a -&gt; b) -&gt; (c -&gt; d) -&gt; p b c -&gt; p a d
I assume the following will also typecheck just fine? foo :: Profunctor p =&gt; (a -&gt; b) -&gt; p c a -&gt; p c b foo = fmap
Interesting link which reassures me a bit, however, I still don't understand why it's needed and would prefer a work around (just by curiousity)
I didn't mean to compare `Arrow` and `Monad`, rather `Arrow p` and `(Category p, Strong p)`. It's clear that a `Monad` instance would imply different semantics, but the observation that it'd correspond to lateral joins is insightful — thanks. I'll have to think it through.
I have a mathematics background but never dived into the theory behind Haskell. I find the category theory feature of a Haskell more like a "PR trick" when explaining why Haskell is a good programming language. The link between the mathematical theory of Haskell and writing programs in practice has never been clear, for me. The "click" for me would be when I discovered [Typeclassopedia](https://wiki.haskell.org/Typeclassopedia). It didn't learn me the underlying theory, why category theory is so great for Haskell, but it explained concepts that Haskell uses.
It's needed because your instance head is `HasEmployee (Shift k) e`. In order to not require `UndecideableInstances`, every argument in the instance head must have a concrete type constructor as the top-level entry. That requirement is part of the Haskell rules for instance declarations, even without MPTCs. `UndecideableInstances` removes that requirement. In practice, this is no bigger of a deal than the `FlexibleInstances` extension. It's not worth putting effort into avoiding. 
lol yeah this has been pretty much been my experience too, guess i just wanted to give the benefit of the doubt though.
Having said that, I agree that "heterogeneity versus homogeneity" isn't necessarily the best distinction. "Inductive versus some family of data types" is probably more fundamental. i.e. if RebindableSyntax could replace list literals with whatever `cons` and `nil` were in scope: vinyl `Rec`s still work, but unboxed vectors of finite sizes (e.g. those optimized for registers or whatever). With that distinction, inductive records like `Rec` are `OverloadedLists`, while array-backed records like `ARec`s aren't exactly; you suffer the inefficiency of creating 63 extra arrays to create a record with 64 fields, though promoting the total list length to the type level (i.e. `fromListN`) might work. https://github.com/VinylRecords/Vinyl/blob/master/Data/Vinyl/ARec.hs https://hackage.haskell.org/package/base-4.10.1.0/docs/GHC-Exts.html#v:fromListN https://ghc.haskell.org/trac/ghc/ticket/7495 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [VinylRecords/Vinyl/.../**ARec.hs** (master → 603ead3)](https://github.com/VinylRecords/Vinyl/blob/603ead3469986d9134b3eefd98e2442f9e236874/Data/Vinyl/ARec.hs) ---- 
&gt; don't want the answers to be changed without permission That's the no derivatives clause in CC
&gt; OverloadedLists 
That was also my first intuition how this could be desugared! I think we definitely want Enums and all this stuff. In practice, I view heterogenous more of a special case of homogenous, you don't use them as much. I think what /u/andrewthad meant was whether this should be a new flag or subsume `OverloadedLists`. I think it's different enough that it should be a new flag.
Stack Overflow uses CC-BY-SA, though. ND wouldn't be a good fit there, as questions and answers are meant to be editable.
I [deploy my static blog to GitHub Pages](https://github.com/vaibhavsagar/website/blob/104528016ecb1f109e452f881be69a20bdd7f6b6/.travis.yml) with Nix, but it sounds like you mean the whole server.
Wow!
I [gave a presentation](https://www.youtube.com/watch?v=RsSNEkBGmj0) on deploying a simple Haskell web app with Nix, and I would expect the approach I used to scale up to arbitrary Haskell web apps.
I'm not sure if it's even possible to resize an image without loading it. As for image processing library with low memory requirement I'm aware of [libvips](https://github.com/jcupitt/libvips); Unfortunately it doesn't have Haskell binding yet.
Possibly redundant with Planet Haskell ? Don't forget to add your feed there..
I think Enums should probably stay homogenous, since heterogenous enums don't really make sense, and fixed length ones will require dependent types or similar. I think another flag that implies `OverloadedLists` for enums and such makes sense. It would also be nice to have pattern matching for heterogenous lists as well, using machinary similar to the above.
I've got a nice library that I'll release next week, which makes Heterogeneous collections slightly easier to use. It allows you to write hList (10, "dog", True) hMap (ⵆ @'["number", "notNumber"]) (20, "i") Might make this a little less painful
Submission sent.
No way to get a `Functor` constraint on the first argument of `Bifunctor` (or a `Contravariant` constraint on the first argument of `Profunctor`) yet, right?
&gt; I prefer the partial IsList instance for NonEmpty over complicating OverloadedLists. IMO the partial `IsList` instance is pretty gross. I would strongly prefer anything else over it. &gt; FixedLength and HList should use only OverloadedTuples (as well as the concrete tuples themselves). Not sure I agree, I can think of plenty of cases where static length lists are conceptually closer to regular lists (perhaps with the only reason for static lengths being converting certain errors into compile time errors). Also what about length 1 fixed length lists and length 1 hlists? &gt; Maybe, and especially Identity, should have neither, since these aren't required for completeness, like the Foldable instance of (e,), their syntactic conveniences. This is all about syntactic convenience, none of it is strictly necessary. &gt; ExistList should take a tuple because while its output type is the same, the input types aren't, so we can refine "homogeneous" to mean that too (the inputs should all have the same type). Again not sure I agree, plenty of use cases for such lists would be morally homogenous, just with an open underlying data type. E.g. data Animal = Cat | Dog [Dog, Cat, Dog] :: [Animal] class Animal where ... data Dog = Dog data Cat = Cat instance Animal Dog where ... instance Animal Cat where ... [Dog, Cat] :: ExistList Animal &gt; None of these seems controversial to me, nor do the distinctions feel that arbitrary either; when "the inputs all share the same type", each item in the list constrains the type of every other item, which improves inference. There is going to be zero difference in type inference. All the following compile just fine with `default ()`: fromHList $ 1 :* 2 :* 3 :* Nil :: [Int] fromHList $ (1 :: Int) :* 2 :* 3 :* Nil :: [_] For completeness the above depends on the following: data HList (xs :: [*]) where Nil :: HList '[] (:*) :: x -&gt; HList xs -&gt; HList (x : xs) class FromHList (xs :: [*]) a where fromHList :: HList xs -&gt; a instance FromHList '[] [a] where fromHList Nil = [] instance (x ~ a, FromHList xs [a]) =&gt; FromHList (x : xs) [a] where fromHList (x :* xs) = x : fromHList xs
&gt; With that distinction, inductive records like Rec are OverloadedLists, while array-backed records like ARecs aren't exactly; you suffer the inefficiency of creating 63 extra arrays to create a record with 64 fields, though promoting the total list length to the type level (i.e. fromListN) might work. I mean the length of the list is known at compile time since it's in the type, so no real extra work is needed to support efficient creation of unboxed vectors. We can use the same class: class FromHList (xs :: [*]) a where fromHList :: HList (xs :: [*]) -&gt; a
even when an expression successfully gets inferred with some constrained type, "compile just fine" is not the same thing as the ideal "compiling successfully whenever possible, with the most narrow relevant type, and with helpful error messages otherwise". Because: (1) classes that are too loose make the types thay get inferred over-broad; and (2) broader types often have worse error messages (c.f. the `Foldable` methods, or anything from `lens`). For example, `[0::Int, 1, 3]` no longer implies `:: [Int]`. And `["a", "b", "c"]` would have a type like `(IsString a, IsString b, IsString c, IsList t '[a,b,c]`) =&gt; t` rather than `(IsString (Item t), IsList t) =&gt; t`. I don't see how this won't hurt existing `OverloadedLists` usages, especially with other abstract literals from `Num` and `OverloadedStrings`. Even when the usage is gated buy an extension, I want to be able to use both records and other containers in the same module, with narrower inferred types for the containers (especially if they are themselves used as record fields). 
&gt; For example, `[0::Int, 1, 3]` no longer implies `:: [Int]`. And Yeah it never did: :t [0::Int, 1, 3] (Num (Item l), IsList l) =&gt; l And `[0::Int, 1, 3] :: [_]` implies `:: [Int]` regardless of implementation choice. &gt; (1) classes that are too loose make the types thay get inferred over-broad That's not particularly important here, seeing as either way you end up with `[1, 2, 3]` having type equal to a type variable constrained by some classes (`Foo a =&gt; a`). &gt; (2) broader types often have worse error messages I don't find error messages a particularly compelling argument either: [1] ++ fromHList (1 :* "foo" :* Nil) No instance for (Num [Char]) arising from the literal ‘1’ Whenever you use `fromHList` to make a homogenous type the error messages will be just as good.
I have quite a few more different pairs now :) I also have some other interesting functionality - I can do traversals with these invariant things, and there's something I'm trying to push around that lets you do these things with a pairing between a Monad and... some other thing... on the other side. So far it has been enough to parse / print a length-prefixed string, I'll try to push it further before too long :) The work-in-progress code is [here](https://github.com/qfpl/invariant-extras/tree/master/invariant-extras), with an example applied to `binary` [here](https://github.com/qfpl/invariant-extras/blob/master/examples/binary/src/Examples/Binary.hs). I should probably do some either-of-tuples magic to make working with the sum types nicer. I'll get to that after I do a few more examples.
Nice! See you there!
heheh Poznan, center of the universe!
Ok thanks.
Sorry I meant as in getting a fixed length vector out of an enum requires dependent types. Fixed length vector literals themselves are of course fine. `[1 .. 5]` would work regardless of extension yeah, `OverloadedLists` would probably do the same thing that it does now to enums, and `OverloadedHeterogenousLists` would either do nothing, or just imply `OverloadedLists`. Pattern matching should definitely be possible yeah, with something like: class ToCons a where type Head a :: * toCons :: a -&gt; Maybe (Head a, a) class ToNil a where toNil :: a -&gt; Maybe () instance ToCons [a] where type Head [a] = a toCons = uncons instance ToNil [a] where toNil xs = if null xs then Just () else Nothing
It definitely would. It's really annoying that type application is not currently supported on literals, particularly since literals give me ambiguity errors more often than anything else.
but for with, for example, fixed sized vectors pattern matching is a bit more complicated.
Hey, did you know that Lions, hyenas, leopards and African wild dogs all prey on aardvarks, but not without a fight u/LeanderKu ? Type a on any subreddit for your own aardvark fact I am currently a work in progress and am learning more about aardvarks everyday
that's a bummer 😕 But I am optimistic that this will get fixed!
What a coincidence. I was just reading https://ghc.haskell.org/trac/ghc/ticket/9123 the other day, after thinking about the deficiency of the role system and the proposed "higher-kinded roles."
I mean the key difference is only going to be whether or not the compiler complains about exhaustiveness, right? Other than that I think everything should work the same. Can't test it out right at this moment but hopefully with pattern synonyms we can handle that ok. 
What are some non-Maybe examples? This looks cool but I'm curious what it offers over Filterable and Witherable type classes.
We can go further and define `second` and `rmap` to be `fmap` class (forall aa. Functor (p aa)) =&gt; Bifunctor p where first :: (a -&gt; a') -&gt; p a b -&gt; p a' b class (forall aa. Functor (p aa)) =&gt; Profunctor p where lmap :: (a' -&gt; a) -&gt; (p a b -&gt; p a' b) second :: Bifunctor p =&gt; (b -&gt; b') -&gt; (p a b -&gt; p a b') second = fmap rmap :: Profunctor p =&gt; (b -&gt; b') -&gt; (p a b -&gt; p a b') rmap = fmap
This exists as [`Data.Type.Coercion.Coercion`](https://hackage.haskell.org/package/base-4.10.1.0/docs/Data-Type-Coercion.html#t:Coercion) data Coercion a b where Coercion :: Coercible a b =&gt; Coercion a b
It doesn't actually carry info about filesystem etc. around, no. I found [this](https://wiki.haskell.org/IO_inside#What_is_a_monad.3F) enlightening - the section I linked to + the next one.
`HasEmployee x e` has a functional dependency `x -&gt; e`, meaning that the type `x` should determine the type `e`. In `HasEmployee (Shift k) e`, `k` does not determine `e`. You would need to look at the `HasEmployee k e` instance in the context, but that is not allowed by default, and that's what requires `UndecidableInstances`. I'm not sure why that is so, perhaps the problem is that ensuring that fundeps are satisfied with arbitrary instances is not a decidable problem. The requirement (a different one) about the shape of instance heads is removed by `FlexibleInstances`.
Well, if you don't want any graphics at all, then you start by making a data type: data World = World { character :: ... } The character needs, as a minimum, a position. If he's just moving on a plane, you can get away with 2D coordinates, but since you want to add more complex geography later, you may as well plan for it and use 3D coordinates. That could be a `(Double, Double, Double)`, or you could use a library for vector types, or make a newtype. As for the character moving around, you have to figure out what exactly you want. Do you want a step-by-step simulation? Where you can go forward and print out data after each step? Then you'll need to make a function `step :: World -&gt; World`. Who decides how the character moves? You say you want to test AI strategies, so you need your AI to get hold of (some of) the World data, and then spit out information about the moves it wants to make. The `step` function should handle passing data to the AI, and executing its commands.
Yeah, but I don't really understand it. https://www.cs.indiana.edu/~sabry/papers/rational.pdf
Out of interest, any particular resources which helped you understand multiplication/addition?
thank you for the reply. This is exactly the kind of tips im looking for. How to use Haskell types to model the game on a high level. Any example code from other projects I can draw inspiration from?
() = 1 Bool = 2 Int = 2^64 — or similar Maybe a = a + 1 — Maybe adds one more value to any type, Nothing Void = 0 — Void has zero values Either a b = a + b — Either is the canonical sum type, it has as many values as a and b combined Either Bool Int = 2 + 2^64 possible values (a,b) = a * b — Tupels have as many values as all combinations of a and b [a] = 1 + a + a^2 + a^3 + ... — lists have as many values as there are possible combinations of any number of a’s, or an infinite amount! This might not seem *useful* though, but it really is - if you take the derivative of a structure using the equations above you find that you end up with something called a *zipper*, which is like a cursor into a structure which focuses on just one part of it. There’re plenty of posts on this topic, and having a good intuition for it can tell you whether something *smells* right when looking at code. I wrote something recently which I was forced to use a Maybe Bool for, but the only valid values were Nothing or Just True, so I have a type with three possible values (1 + Bool = 1 + 2) but only two valid ones, so I know I need to wrap things to just either use Bool or a type which is isomorphic to Bool which only has two values. This is a simple example but it does scale. 
Not that I know of - there was a blog post recently posted in this subreddit, I think, with a guy who made a small game and talked about how he structured it, but it was a very different game, and it was with graphics and user interaction and such. Might still be worth looking at, though: https://www.reddit.com/r/haskell/comments/813yik/a_game_in_haskell_dino_rush/
Types are similar to sets, which is maybe a more familiar term. Sets have cardinalities (number of elements), which are nonnegative integers (if finite, but let's consider only finite sets now). You can take disjoint union of sets, and their cardinalities add together; so that's addition. You can form cartesian product of sets (meaning the set of pairs, one element from one set and one element from another set), or more generally tuples, and their cardinalities are multiplied: If you have 3 elements in the first set and 4 in the seconds, then you have 3*4 = 12 pairs; so that's multiplication. You can consider functions from one set to another set; if they are finite sets, the number of functions is the cardinality of the second set to the power of the cardinality of the first set, because for each element of the first set, you can choose an arbitrary element of the second set. So for example if the first set has 5 elements, then you can choose 5 elements of the second set, so if the latter have N elements, then you have N*N*N*N*N = N^5 choices. So that's exponentiation. Basically all the operations familiar from school are defined not on numbers, but on sets or types or other "higher dimensional" structures, and numbers are just "shadows" of those (you forget information; you only keep the cardinality). But they don't tell you that in primary school for some reason.
One way to think about it is to think of a couple of types with few inhabitants. Suppose `A` has 3 inhabitants, and `B`has only 2. `Either a b` encodes a sum type, so data Foo = Either A B data Bar = A | B are equivalent. What they say is that a value of type `Foo` or `Bar` can be any of the inhabitants of either of `A` or `B`. That means that the total number of inhabitants must be the sum of the inhabitants of the component types. Both then have 5 inhabitants (plus bottom). `(,) a b` encodes a product type, so data Baz = (A, B) data Quux = Quux A B are equivalent. What they say is that a value of `Baz` or `Quux` comprises an inhabitant of `A` with one of `B`. Using the multiplication principle, we know that if we have n ways to choose X and m ways to choose Y, then we have mn ways to choose one of each. It follows that the total number of inhabitants must be the product of the inhabitants of the component types (plus bottom). Both then have 6 inhabitants (plus bottom).
[Here](https://github.com/gelisam/ludum-dare-31)'s one I made, but I don't think that's what you need. The appeal of gloss is that it makes it easy to draw graphics and gather inputs in a purely-functional way, but you specifically said you didn't need graphics, and since it's an AI choosing actions, not a human, you don't need to gather inputs either. [Here](https://github.com/gelisam/tic-tac-top/blob/master/src/AI.hs) is a small AI I wrote for a collection of 2 player, turn-based games. The games are all variants of tic-tac-toe, so their state space is small enough that it can be completely explored. So my AI uses a naive implementation of [minmax](https://en.wikipedia.org/wiki/Minimax) which explores the entire tree. Your game sounds much bigger than that, but I think it still makes sense to present it to the AI in terms of a game state and a set of moves the AI can choose from. Or maybe you only want to show the AI a portion of the game state, so they can't cheat and look beyond the fog-of-war. It sounds like you want to first model the game, starting with a plane and making it more and more complete, and only then implementing an AI for it. I'd suggest integrating the AI much earlier. I would create an abstract representation of a game, much like [mine](https://github.com/gelisam/tic-tac-top/blob/master/src/Game.hs). An abstract type `a` would represent the game state, some derived type `Visible a` would represent the portion of the game state which is visible to the player, some other derived type `GameMove a` would represent the moves which the player can perform from this game state, and a few abstract functions would compute these for each `a`, and a way to evaluate how well the AI did, like some scoring function or just whether this is a winning position or not. Then you can write a silly AI which e.g. always picks the first move from the list. Having this framework in place will guide you while you implement the plane etc., because now you know exactly what you need: * a type representing the game state (where the characters are in that plane) * a type representing what is visible to the player (only the characters which are within a certain radius of the player character? only those within a pie slice in the direction the player character is facing?) * a type representing the possible moves the player can make (the 8 cardinal directions? An integer number between 0 and 360? A destination position to which the character will automatically walk?) * functions between those types * a scoring function (how far from the origin the player character has walked? how many other characters it has touched? later, once you add extra moves for shooting at enemies, how many other characters it has killed?)
**Minimax** Minimax (sometimes MinMax or MM) is a decision rule used in decision theory, game theory, statistics and philosophy for minimizing the possible loss for a worst case (maximum loss) scenario. When dealing with gains, it is referred to as "maximin"—to maximize the minimum gain. Originally formulated for two-player zero-sum game theory, covering both the cases where players take alternate moves and those where they make simultaneous moves, it has also been extended to more complex games and to general decision-making in the presence of uncertainty. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [gelisam/tic-tac-top/.../**AI.hs** (master → dfbf0ed)](https://github.com/gelisam/tic-tac-top/blob/dfbf0ed95af7815d1a8f0e070ae8fe72ac751fe9/src/AI.hs) * [gelisam/tic-tac-top/.../**Game.hs** (master → dfbf0ed)](https://github.com/gelisam/tic-tac-top/blob/dfbf0ed95af7815d1a8f0e070ae8fe72ac751fe9/src/Game.hs) ---- 
What is 1? * What is 2? ** What is 2 + 1? It's 3. *** What is 2 + 2? It's 4. **** What's 2 * 1? * * What's 3x2? ** ** ** What is `()`? () What is `Bool`? True False What is `Either Bool ()`? Left True Left False Right () What is `Either Bool Bool`? Left True Left False Right True Right False What is `(Bool, ())`? (True, ()) (False, ()) What is `(Either Bool (), Bool)`? (Left True, True) (Left True, False) (Left False, True) (Left False, False) (Right (), True) (Right (), False) Does that make it any clearer? 
Yes. I have endpoints that serve WebSockets. These are not natively supported in Servant. But Servant comes with the `Raw` combinator that allows you to easily fall back to raw `wai` functions when functionality isn't supported by servant: ``` type API = MyApi :&lt;|&gt; "notifications" :&gt; Raw websocket :: Wai.Application myApp :: Server API myApp = myApi :&lt;|&gt; websocket ``` 
All in one because I can't help myself data PRN :: Type -&gt; Type -&gt; Type -&gt; Type type role PRN phantom representational nominal # `PRN :: Coercible r r' =&gt; PRN p r n -&gt; PRN p' r' n` &gt;&gt; :t coerce :: PRN _ _ _ -&gt; PRN _ _ _ .. :: coerce :: Coercible r r' =&gt; PRN p r n -&gt; PRN p' r' n 
I love boulder. What’s the salary range?
Quotient types seem related to higher inductive types. At least, non-definitional equality and univalence is one way to construct quotent types. Setters (from lens) always stuck me as something of a subtraction type. They capture the idea of a hole in a value, even for the case where the type of the hole affects the type of the value.
No drawbacks as far as I know. If you find one, file a bug!
Have you seen [https://www.reddit.com/r/haskell/comments/6zzvgh/tutorial_building_a_simple_rts_game_with_apecs/](https://www.reddit.com/r/haskell/comments/6zzvgh/tutorial_building_a_simple_rts_game_with_apecs/)?
Well, only the building part, but in that sense every build system "reinvents cabal" including make. Coming up with a set of versions and downloading them is necessarily Haskell-specific... though I guess even that doesn't have to be. One difference with bazel is that it expects static BUILD files, so it's not really designed to discover dependencies as you might do with shake. That could also be an advantage because then it can enforce module visibility... though I guess shake could do the same, just have the dependency parser throw a fit if you import out of bounds. A shake-based system could pretty easily parse .cabal files to discover package deps, while with bazel I think you'd have to have a script convert them to BUILD files first. Of course that might not be a big deal, because presumably you want to import cabal packages manually anyway to enforce a one version policy, or at least a small set of known versions. My guess is that at the time cabal was developed, make was the main option for building. And make has never really gotten into the "library" concept, and is probably hard to make cross platform.
It means you can derive most of those instances (with `-XDerivingVia`, I am writing the GHC proposal now), deriving the rest is a fun challenge newtype ContT f m a = ContT (forall xx. (a -&gt; m (f xx)) -&gt; m (f xx)) deriving (Functor, Applicative, Monad) via (Codensity (Compose m f)) There may well be a deeper meaning, `Codensity` has an intimate relationship with functor composition Codensity f = Ran f f and and if we ever have Compose A B ~&gt; C we can uncurry it to A ~&gt; Ran B C
glguy made a networked gloss game called (Ninjas)[https://github.com/glguy/Ninjas/]. It is a lot of fun with the right group.
&gt; `HasEmployee x e` has a functional dependency `x -&gt; e`, meaning that the type x should determine the type e. In `HasEmployee (Shift k) e`, `k` does not determine `e`. That's what I thought too, and was looking for a way of forcing it. Does the constraint `HasEmploye k e` not just says, that `k` determine `e` ?
Any idea if something can be done about the unsafe coercion?
Yes indeed. The problem is only that whatever checks that GHC implements can't recognize that. I don't have an explanation why. The [GHC user guide](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#functional-dependencies) is quite terse on this topic, they basically say to read the paper ([PDF](http://web.cecs.pdx.edu/~mpj/pubs/fundeps-esop2000.pdf)).
This shouldn't be too difficult to do via a polyvariadic function, then there would be even less clutter than with list syntax.
If you say light, are you sure you need a CMS? Maybe you are better served with a static site generator. In this case, Hakyll is really great.
But for AI training you might want to think of more computational effective representations than the immediate / naive one as suggested by OC. 
I assume you do this as an AI exercise? Can I join in? I could dedicate 5-10hrs a week in March and the first half of April, as I'm on a sabbatical. 
Is Cloud Haskell dead? No updates (at least visible on the website) since Feb' 2016?
I wouldn't worry about graphics and performance at this stage. That's why I plan to use libraries such as Brick just to show the game status
There's http://hackage.haskell.org/package/gitit and http://hackage.haskell.org/package/clckwrks 
Did you try turning it off and on again?
Is this `makeClassy` from Control.Lens.TH? The docs specifically say &gt; Make lenses and traversals for a type, **and create a class when the type has no arguments.** http://hackage.haskell.org/package/lens-4.16/docs/Control-Lens-TH.html#v:makeClassy So I'm a little surprised that it is happily creating a class for you here, when the `Employee` type has an argument. Are the docs for lens simply outdated, or is this unsupported usage of `makeClassy`?
You want nix: https://github.com/Gabriel439/haskell-nix (btw, nix integrates transparently with stack: https://nixos.org/nixpkgs/manual/#how-to-build-a-haskell-project-using-stack)
Which tools are you looking for? `cabal new-build` works just peachy for building, both on windows and linux. For cross-compilation the story is very much in progress for ghc in general, and it will depend on which platform. These posts sketch some of the work and possibilities in this area: https://medium.com/@zw3rk
Pardon the obvious: [cabal](https://www.haskell.org/cabal/)?
Quotient types are not exactly the same as just "division on types" despite the name. You quotient by a _relation_ typically, while one would imagine division would just be dividing by _another arbitrary type_.
Related: I had a light-bulb moment about why dependently typed languages call their dependent pairs "Sigma" types, and their dependent function types "Pi" types. The dependent pair type `(x : A, B[x])` takes each point x in A, and chooses a B type for it. It then sums all the different B types together into a single large pool of values. A sum over a varying series B, i.e: [a sigma](https://en.wikipedia.org/wiki/Summation#Capital-sigma_notation). The dependent function type `(x : A -&gt; B[x])` is similar. For each point x in A, it chooses a B type for it. It then multiplies all the B types together (to form a large table of outputs for each of these input points). A product over a varying series B, i.e: [a pi](https://en.wikipedia.org/wiki/Multiplication#Capital_Pi_notation).
Run two passes -- first one that uses "position" to scrape the location of the "hdr" rows, and the second to use that position to parse into an appropriate section?
Actually it is the `makeClassy` from Lens.Micro.TH but I assume it is the same as the one from Control.Lens.TH. It is probably unsupported even though I don't see why it shouldn't work.
typically one only speaks of "mapping" a function over a container, not an arbitrary functor -- the terminology comes from scheme, not from category theory. the "fmap" of a function is often called a "lift" which may be more familiar. &gt; And for a specific example, "monads are monoids in the category of endofunctors" while sounding very "mathsy" is actually not something very intuitive in terms of "category theory for the working mathemetician" This is actually not something haskell people tend to say seriously. It is a joke that is actually drawn from an actual quote my Mac Lane in _Categories for the Working Mathematician_ where he explains monads in _precisely_ this way -- the "joke" is just that this is not useful to most programmers :-) Homomorphisms abound -- just not the terminology. They're just endofunctions which respect some specified property. Derived functors don't feel very "common" to me tbh, and the sort of tech they're used with doesn't arise very directly in Haskell -- i.e. it is hard to speak about structures on terms with that precise sort of higher quality. Limits and colimits are common, though not under that name -- we see instances of them as sums and products, and we also have fairly "universal" versions of them in the form of existential and universal quantification.
I agree that `cabal new-*` is very nice these days, and I use it exclusively – even in projects where the others are using `slack`, the tooling choices are not exclusive.
There are two main considerations for us. One is that Stack is rather complicated and fast-moving, which unfortunately also seems to make it rather unreliable. I don’t want to be unpleasant, so suffice it to say that we have encountered a lot of tool-related problems using Stack over a period of years now, and for our needs it seems the costs very clearly outweigh the benefits. The other is that Stack doesn’t seem to support our need for cross-compilation very well yet. Our ideal set of tools would be simple, reliable, and transparent about what’s going on under the covers — even if that means just going back to installing a single recent version of GHC that we can build for both native and cross-compiling on all of our platforms and a known-compatible set of libraries/packages to go with it. But with all that’s been going on in the Haskell community and us a little out of the loop on recent developments, we’re wondering if there are better options that we aren’t aware of.
OP states they want to build on Windows as one of their requirements. WSL is not 'on windows.' Nix does not fit this usecase. 
We just need a basic compiler toolchain for a recent GHC, basic compatible packages, and the ability to work on and target multiple platforms (exactly the kind of work that @zw3rk has been doing; those what’s new posts are required reading around here).
Thanks for clarifying this. The mention of cross-compilation is crucial. At first, I read the original post, and I couldn't figure out why stack wouldn't work for you. But yes, for cross-compilation, you don't want stack. It handles a particular but fairly common build scenario really well, but if that's not your scenario, you'll either need to use `cabal` or just use `ghc` directly.
How do you recommend using Stack with Nix? Cabal2Nix on the generated cabal file? Is there a guide on usage that you recommend? I'm kind of a casual NixOS user (if there is such a thing), and I've just been using the Nix integration built into Stack.
&gt; At first, I read the original post, and I couldn't figure out why stack wouldn't work for you. Neither could we, which is why we would now prefer to avoid it entirely.
No, seriously, I think this could make sense. I have a strong programming background, and learned a lot about haskell, but de facto no hands on experience. Very similar about machine learning. Which ML technique / algoruthm did you think about? -- though note that the Haskell ecosystem does not seem to have a advanced libraries, as say python for machine learning. 
&gt; How do you recommend using Stack with Nix? I don't :P I recommend the workflow with Cabal [outlined in /u/bsima's link above](https://github.com/Gabriel439/haskell-nix/blob/master/project0/README.md#building-with-cabal), but with one change. Rather than having to manually use `cabal2nix` to keep `default.nix` updated, you can just replace `haskellPackages.callPackage ./default.nix {}` with `haskellPackages.callCabal2nix "mypackagename" ./. {}`. This will run `cabal2nix` for you automatically as a part of Nix evaluation. But if you must use Stack, you can use a `stack.yaml` like this: resolver: ghc-8.0.2 nix: enable: true shell-file: stack.nix --- # stack.nix (import ./myNixFile.nix {}).env This will tell stack not to do any management of haskell dependencies, and it will use the ones that come with the GHC provided by `stack.nix`. This does work, but in my experience it is kind of buggy and I wouldn't generally recommend it.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Gabriel439/haskell-nix/.../**README.md#building-with-cabal** (master → f6c1138)](https://github.com/Gabriel439/haskell-nix/blob/f6c11383dd289c59a2a98460155c290923ed3bb4/project0/README.md#building-with-cabal) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dva5vzq.)
Yeah, these aren't the same at all. Algebraically speaking, subtraction and division are just the inverses of addition and multiplication, respectively. That implies some pretty funky stuff. For example, taking `1` to be the unit type and `0` to be void, these equations would have to hold: a - a = 0 a * 1/a = 1 along with the other [Field Axioms](https://en.wikipedia.org/wiki/Field_\(mathematics\)#Classic_definition). This means that you could essentially generate data from the unit type, which is *kind* of a problem. It could work, but it would essentially mean that all functions would have inverses, which in turn would imply that all computation would be reversible.
Thanks for the ideas. I think Windows probably is a deal-breaker for using Nix, but I also think we’d avoid that one anyway. It seems that many of the same concerns that now deter us from using Stack would also apply there. At this point, we just want to get back to something simpler and more transparent, particularly given the inherent complications of cross-compiling. If there were a simple way to just install the same specific versions of `ghc` and `cabal` tools on each system we use with nothing else beyond the compatible standard library, and if the `new-*` options for `cabal` would then let us install additional compatible packages with deterministic dependency resolution and versioning, and if the method of doing those things would continue to work exactly the same way for the foreseeable future so we could set up exactly the same thing on other systems as needs dictated, we’d be absolutely thrilled.
Note that with a few extensions, you can actually have Haskell count these for you :) class Finite a where type Cardinality :: Nat instance Finite Bool where type Cardinality = 2 instance (Finite a, Finite b) =&gt; Finite (Either a b) where type Cardinality = Cardinality a + Cardinality b
This was sort of the thing that the original Haskell Platform was designed for. It provides a small pool of packages that are known to work together and a fixed compiler version. One option would be to just tie your development to a given Haskell Platform release as a base to ensure consistent GHC versions and the like. You can fiddle with `cabal new-freeze` as needed to lock down the dependency versions to ensure they are consistent across developers. It does mean that every year or so when you decide you're going to migrate to the next platform there is a manual platform install step for your developers, but if `stack` is a deal-breaker for you, its probably not too high a cost.
&gt; Windows development is always a strange beast filled with its own horrible tooling. Sometimes, yes, but not so many years ago you just downloaded and installed something like Haskell Platform, which gave you reasonably up-to-date and compatible GHC and Cabal, and that seemed to work just fine for everyday projects. I can see the arguments for something like Stack if you’ve got people working on a lot of different Haskell-based projects with their own tooling requirements. In that environment, you’re probably also updating your Stack-related tools regularly to keep on the leading edge, and you’re probably keeping aware of ongoing changes in the wider Stack ecosystem. However, it is far from clear to me that the added complexity of layering things like Stack on top is a net win for the kinds of project I’m dealing with. We tend to work on something Haskell-based for a while, then be doing other things, then come back to the same or another Haskell-based project perhaps several months later. After the first few times you do that and find that even the most basic Stack commands in your scripted build process no longer work, and you do the whole uninstall, nuke working directories, and reinstall dance just to get back to being able to build a project, it gets old. :-)
Thanks!
&gt; This was sort of the thing that the original Haskell Platform was designed for. It provides a small pool of packages that are known to work together and a fixed compiler version. That was indeed how we did it before we moved to Stack. However, from the Haskell Platform site, it appears that it too now has some sort of Stack integration, which is why I posted the question here this evening. &gt; It does mean that every year or so when you decide you're going to migrate to the next platform there is a manual platform install step for your developers, but if stack is a deal-breaker for you, its probably not too high a cost. Honestly, that isn’t a problem at all. As long as it’s clear which version of our tools we currently have installed, how to get the same versions of any dependencies so everyone is building the same thing, and how to cleanly update to a new version if and when we need to, we would be happy to operate on that basis. Our average time to uninstalling Stack because it broke again, nuking all related working/cache directories, and reinstalling the latest version has surely been under a year for some time anyway... :-)
This part of your experience with stack does not resonate with me: &gt; even the most basic Stack commands in your scripted build process no longer work There aren't that many stack commands that I regularly use. Maybe something like `build`, `test`, `bench`, `exec`, `ghc`, and `new`. But these have all been there since the beginning, and I seldom have to delete `.stack-work/`. I don't feel like I have to do it any more or less than I do with `cabal`.
This is really cool. Does anyone have a link to the final version of this paper?
Note that this is inconsistent. `Void` is the identity element for type addition, so if there are negative types, `a + (-a)` should be isomorphic to `void`. Also, type addition is associative, so `a + (b + c)` is isomorphic to `(a + b) + c`. That means we can do this: void = void + void = void + ((-a) + a) = (void + (-a)) + a The problem is that `(void + (-a)) + a` is inhabited by `Right(a)`, and it is also isomorphic to `void`, which would imply that void is inhabited, which is obviously not true. 
Thanks
Just wanted to chime in that I've never had such problems with stack either, and it almost sounds like a different tool when I read the problems from OP. It's quite unfortunate, because the requirements they have sound literally like what people are using stack for? (Also, personally I'm not too happy with Nix on macOS, it seems like a second citizen sometimes...).
https://github.com/haskell-mafia/mafia Mafia is an opinionated wrapper around Cabal which handles things like git submodules, new-build/nix style caching, and other niceties. It was developed at Ambiata so we could handle about 150 haskell projects more easily, and does a really good job. Windows isn't one of our constraints though. It's pretty lightweight, and once it sets up robust sandboxes, normal cabal commands continue to work correctly. This means that though mafia doesn't specifically wrap sdist for instance, cabal sdist will still work fine. 
&gt; It's quite unfortunate, because the requirements they have sound literally like what people are using stack for? Indeed, that is exactly why we moved to it a few years ago. The goal of making consistent, reproducible builds the norm is laudable. Unfortunately, while I can believe that our experience has been unusually bad, that has still been our experience. Lacking any well-understood explanation for why we’ve run into so many problems during that time, we need another way forward that we can trust. FWIW, the obviously broken Stack interactions have typically happened when returning to some Haskell development after a significant break, say a few months, and working on a Windows machine. Given that a complete clear-out and reinstall of the Stack tools and working data has typically fixed these problems, with nothing else changing in our source code or scripts, it’s hard not to conclude that the tooling itself was broken in some way. My working theory is that something about the Stack infrastructure tends to break on Windows, such that incrementally keeping up-to-date has continued to work but trying to catch up from further behind the curve has not. Unfortunately, even if that theory is correct, lacking either a reproducible test case or a better understanding of the root cause of the problems, we don’t even have anything intelligent to report as a bug.
A very promising starting point! Thanks
Please refer to the other comments I’ve made this evening for more details, but the bottom line is that it hasn’t been just “a problem or two”, but because we don’t fully understand what the tools were trying to do behind the scenes when things have gone wrong, it’s difficult to do anything constructive about filing a bug report even though the pattern is clear. Obviously many others have had a better experience than ours; I’m happy for them and didn’t come here to start any sort of negative discussion or criticism. I’m simply looking for a better (and in particular simpler and more transparent) way that does work for us as well.
&gt; with a thin layer of Buck to speed up builds and storing of artifacts. Is there a blog post about this somewhere? We use Nix on my team at Target, but we don't have anything to speed it up so builds can get pretty slow...
&gt; If there were a simple way to just install the same specific versions of ghc and cabal tools on each system we use with nothing else beyond the compatible standard library for this, I recommend either `chocolatey` and ppas, or minimal platform installers, or just scripting your own (which is not that many lines -- honest!). `and if the new-* options for cabal would then let us install additional compatible packages with deterministic dependency resolution and versioning` They do this, with the exception that new package uploads to hackage can change plans. However, this is fixable by using either `new-freeze` (https://www.haskell.org/cabal/users-guide/nix-local-build.html#cabal-new-freeze) or `index-state` (https://www.haskell.org/cabal/users-guide/nix-local-build.html#cfg-field-index-state) or both together. &gt; if the method of doing those things would continue to work exactly the same way for the foreseeable future Yes, I don't imagine either of the above to change anytime soon.
Thank you! That may be the “incredibly obvious fact of the day” that we had missed, and may in fact solve all of our problems.
Copying the output when things go wrong is sufficient. You do not need to understand what's going on behind the scenes to file a bug report. Though, it is certainly appreciated when the issue creator does some investigation. I appreciate that your intention is to constructively find a better alternative. However, based on your description of the deficiencies, I don't think the tool you are looking for exists. So, it would be most constructive to address the specific issues you are having with stack. So far the main concrete issue is cross compilation. That's a tricky one! Seems to me like other than resource constrained platforms this tends to be more of a convenience than strict necessity.
Actually, the more I think about it, `0 = a + (-a)` is equivalent to stating "The law of excluded middle is false", which is true under Intuitionistic logic. The problem with taking `-a` to mean "not a", you have to have to be very careful with double negation. Specifically, `a -&gt; -(-a)`, but `-(-a) -&gt; a` cannot be valid, as this means that our logic is no longer Intuitionistic.
Sweet, when?
&gt; However, based on your description of the deficiencies, I don't think the tool you are looking for exists. Perhaps you’re right, although apparently I had misunderstood some of the recent developments with both Cabal and the Haskell Platform. Based on what I’ve learned this evening, I suspect that simply installing HP and adopting the new-style Cabal system would now be significantly better for our needs than what we’ve been doing so far. There is still the cross-compilation issue, but Stack doesn’t yet offer much help there, and it seems more difficult to try the ideas that people like Moritz have been working on that way. &gt; Seems to me like other than resource constrained platforms this tends to be more of a convenience than strict necessity. We are indeed compiling to resource-constrained platforms in the cases where cross-compilation is currently of interest. In general, the Haskell programs I work on tend to involve relatively complicated data structures and processing logic and to run in environments where high reliability is essential. In this particular case, one such program needs to run as part of some embedded firmware on a variety of target platforms, but the development workstations and build servers are pretty standard Windows or Linux boxes.
I agree that it’s difficult to report problems in build tools since they can be impossible to reproduce. Also, for clarity, are you actually cross compiling, or are you just building a windows binary or windows and a Linux binary on Linux?
Looks like it's already partly working: https://medium.com/@zw3rk/what-is-new-in-cross-compiling-haskell-42ba93555c69
Do you have any suggestions on how to improve the wording (I assume you looked at https://www.haskell.org/platform/?) that would have avoided this misconception?
Ah, yes, I misstated it, (reading + as and instead of or, whoops). The correct statement would be "forall a., the type `a or not-a` is uninhabited." And actually, that doesn't make much sense, since Unit or not Unit is not uninhabited (it is inhabited by a witness to Unit!). There _is_ a way to do a correct translation with negation meaning "implies false," and it involves continuations, but its not the way I did it. Sorry for the confusion. (By the way, "godel-gentzen" is the keyword to google to see more on the 'right way'). 
Perhaps so - HP and new-style cabal come with their own slew of problems. Cross compilation support there is very new there and no doubt rough around the edges. It looks like you'd better build the latest cabal-install version from source if you want to try that - https://github.com/haskell/cabal/issues/4939 - "cabal `new-build` is utterly unusable for cross-compilation "
&gt; but the development workstations and build servers are pretty standard Windows or Linux boxes Wait, so you don't actually have to target windows at all? If you don't need to ship native windows programs with access to windows APIs, the WSL can handle Nix just fine, and you can cross compile to other platforms from there.
Hah fair enough. If Win10 were any option I guess WSL might serve as a reasonable target, but I suppose that's a hard goal to reach. I have literally zero experience with Windows from the past 5 years, so I have no idea how painful Win10 is :P
Perhaps the word “optional” somewhere around where Stack is mentioned could prevent someone else from making the same mistake, but really, it’s my own fault. I read “batteries included” and “Stack tool”, saw the note about stack evolving rapidly and running stack upgrade to get the latest version, and inferred incorrectly that this meant Haskell Platform now also depended on Stack to install and update things.
&gt; it puts a bunch of stuff in the global DB that really ought to not be there The current recommended HP distro (minimal) doesn't add any packages to the global db that aren't already shipped with ghc. This is the case whether or not you use `new` commands. As for cross-compilation, indeed it is a work in progress, on all fronts.
ah didn't realise where the quote was from :) well maybe it's time for me to read that book ... just saw that monads come before monoidal/abelian categories which is surprising (since these are actually things i've used). What is the example you're thinking of in scheme theory? I've only seen "lift" used in the opposite sense, e.g. you don't "lift a function into sheaf cohomology" or "lift a function to its reduction mod-p"; you may or may not be able to do the opposite (depending on obstructions). 
Insightful; thanks for expanding on both points!
Bravo! That was a very enjoyable read, thank you!
anything difficult with good error-handling is going to impress. likewise anything popular with lots of users. since it's Haskell, I would say difficult is more likely to happen than popular, but then again, Elm basically exists because one person in your situation built it for their senior project. so never say never. (obviously Elm is not Haskell, but it's written in Haskell, and it's in the same basic language family.) contributing to open source always looks good. unfortunately Elm seems pretty averse to accepting contributions, but the cryptocurrency Cardano is written in Haskell too, so that might work. plus there have got to be a bunch of Haskell projects on GitHub with issues open. you could do worse than to just find a bunch of those and start solving problems for people. I've heard there are some great linguistics projects in Haskell too, but unfortunately I don't know much about that.
Oh, sorry, not "scheme" as in "scheme theory" :-) "Scheme as in "The programming language scheme, which originated as a dialect of the programming language lisp." The name `map` comes from there, and has no connection to category theory. "lift" i think comes more from a topological intuition -- i.e. you can lift a function to the tangent space or the like. Anyway, this is all a result of a _lot_ of mathematical concepts, and only so many words to go around :-)
"Stack integration" is just that it ships both the cabal installer and stack tools. You don't have to use it if you don't want to.
I don't think we have anything written up about it. It was put in place by Simon Meier before I came onboard and I've learned a lot from just figuring my way around using it. That said, there isn't much to tell. https://github.com/facebook/buck ships with a rule for building haskell libraries https://buckbuild.com/rule/haskell_library.html In practice using buck and nix means we can get a new development machine up and running from scratch to able to produce commits in maybe an hour or so. One slight downside of it is that it doesn't multi-thread as thoroughly inside a package as building directly with cabal. One big upside of it is that we're able to build all of the haskell bits and all the java and scala bits with the same toolchain and cache all of those artifacts using the same machinery. This lets multiple developers share build times.
 Personally what i find more interesting, its exactly what you are asking for. I have found that people who have a really good understanding of haskell ecosystem, ends up doing the most fascinating and simple projects (instead of trying to force something to fit in) Sorry for my english xD
Given how much embedded stuff they do, I'm gonna take a wild guess and say that the massive amounts of telemetry and "phoning home" that windows 10 does is not super pleasant... Even the Enterprise versions of it screw pretty badly with provisioning boxes and keeping them setup how you want them. It's pretty bad compared to windows 7. I think it's a huge step forward even from 8.1 in userland but Microsoft has been bleeding Good will on the corporation side ever since 8.0 and W10 certainly didn't help :)
&gt; &gt; Isn't there a way to combine stackable (not stack) and cabal? I'm fairly certain I've read that somewhere, and that's probably the middle ground I'd suggest, if adamant about not using stack itself. You mean Stacka__ge__, right? Stackage still publishes a `cabal.config` file for each snapshot such as [this one](https://www.stackage.org/lts-10.8/cabal.config).
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/darcs] [How compatible is darcs with \`git send-email\`?](https://www.reddit.com/r/darcs/comments/82k72k/how_compatible_is_darcs_with_git_sendemail/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
 If you're not interested in writing code and just want a quick blog, try my static site generator (http://hackage.haskell.org/package/stagen / https://github.com/jxv/stagen). It's intentionally light weight and written in Haskell.
But how does Buck integrate with Nix stuff? Is Nix building your Haskell dependencies or is Buck doing that?
[removed]
We have nix install all the external fancy unix tools, that would otherwise be piecemeal installed, along with things like ghc. We have buck building all our haskell libraries using the tools earlier installed by nix. Buck builds all of our in-house libraries, but I'm not sure about if it builds our third-party dependencies -- it's abstracted enough that that bit is invisible to me on a day-to-day basis and I just never bothered to look.
It's not just nix. With ghc (HEAD), and cabal (HEAD), you can do: ``` cabal new-build \ --with-ghc=x86_64-w64-mingw32-ghc \ --with-ghc-pkg=x86_64-w64-mingw32-ghc-pkg \ --with-gcc=x86_64-w64-mingw32-gcc \ --with-ld=x86_64-w64-mingw32-ld \ --with-hsc2hs=x86_64-w64-mingw32-hsc2hs \ --hsc2hs-options="--cross-compile --via-asm" \ --configure-option=--host=x86_64-w64-mingw32 \ --allow-newer \ --disable-shared ``` This will work as long as all the libraries work (hint: most don't work with ghc HEAD yet), and you do not run into `build-type: Custom` (which can most of the time just be changed to `build-type: Simple` though -- loosing haddock and doctest). 
You usually can't go wrong with a web app. Something that is deployed and live, even if it is just a toy, is easy for potential employers to look at. If you are interested in frontend development in particular, then try your hand at a ghcjs project. (Or pursecript, or elm, or a few other Haskell-like langs that compile to JS)
&gt; "cabal new-build is utterly unusable for cross-compilation" As the author of that ticket, I'm happy to report that this is no longer the case with `new-build`, and the necessary changes are all in now. The title for the ticket stems from the fact that this was a regression over `cabal build`. As I mentioned in another post, you can use `cabal new-build` with your tool chain quite well these days. It's still a bit more verbose than I'd like it to be but it's possible: cabal new-build \ --with-ghc=$target-ghc \ --with-ghc-pkg=$target-ghc-pkg \ --with-gcc=$target-gcc \ --with-ld=$target-ld \ --with-hsc2hs=$target-hsc2hs \ --hsc2hs-options="--cross-compile --via-asm" \ --configure-option=--host=$target \ --allow-newer \ --disable-shared where `$target` is the triple of the host you target. E.g. `aarch64-apple-ios`.
&gt; proof of a negation e.g, a counterexample?
I used darcs for years and had a wrenching time moving to git (because many free software projects use it) and mercurial (for the repos of a former employer). In the past I've found that the darcs workflow is cleaner and easier -- it was certainly the first dvcs I used and its mental model clicked right away for me, while I still need to constantly need to google things up whenever I conduct a git workflow slightly wrong. Sorry I don't have any good answers on the path compatibility question :-/
My understanding is that [pijul](https://pijul.org/) reimplements darcs with a similar patch theory, but in some cases asymptotically more efficient (see http://darcs.net/FAQ/Performance#is-the-exponential-merge-problem-fixed-yet) I haven't used either.
Something that someone important within your employer cares about. More often than not, it has something to do with reporting or automating a process. Ideally it's something that won't burn a lot of your time but is large enough of a problem for someone to potentially consider Haskell seriously over other options.
Well I wonder if WSL will ever have a way to access windows APIs. If that ever happens, then WSL might just become a highly preferred method of building software.
I've been trying this by hand, but I don't think you can define an `Iso` with that type at all. The `Iso (Maybe m) (Maybe n) -&gt; Iso m n` side breaks down for me. We're required to define a function `m -&gt; n`, and we have a `Maybe m -&gt; Maybe n`, and a `Maybe n -&gt; Maybe m`. We can turn the `m` into a `Maybe m` by either using `const Nothing` or `Just`, but then we're stuck with a `Maybe n` that we can't just extract the `n` from. And it is possible to write lawful `Iso (Maybe a) (Maybe b)`s that do not turn `Nothing` into `Nothing`: i :: Iso (Maybe Bool) (Maybe Bool) i = Iso from to where from Nothing = Just False from (Just False) = Just True from (Just True) = Nothing to Nothing = Just True to (Just False) = Nothing to (Just True) = Just False I haven't looked at the longer definitions that Djinn generated, but I wouldn't put it past it to have figured out this impossibility.
Admittedly, I do not know much about development flow with darcs (it looks promising), but one thing I can say that using darcs is (at least, right now) an acceptance that the majority of developers will never look at/contribute to your code.
You can get binary versions of GHC from their website and just unpack it. (You can also script this). You can also use `cabal new-build` and cousins with stacks package freeze if you want to get a starting package freeze. I have done this in the travis build for raaz. https://github.com/raaz-crypto/raaz/blob/master/scripts/stack-freeze.sh If you want something that automatically takes care of all these there is [jenga](https://hackage.haskell.org/package/jenga) which makes use of cabal under the hood I guess. (I have not used it though because, I found that directly scripting is very much possible and easy).
Hopefully Haskell.org will remain a part of GSoC for all the coming years Hopefully in a year or two I'll get a chance to develop something for them when I can legally take part in GSoC
&gt; Most of the development if not all will be done by mailing patches. Are you sure? That seems like a very antiquated method of collaboration. Why not use github, bitbucket, or whatever's the darcs equivalent? &gt; Question is how compatible this is with git. I'd like to have commands that generate/apply patches the exact same way as `git format-patch` and `git am` It's not the same format, no. `git format-patch` adds some metadata to the top of a `.patch` file, which is the format used by `git diff` and the command-line tools `diff -u` and `patch`. In [this](https://github.com/agda/agda/issues/147#issuecomment-129021944) thread, on a darcs repo which has since moved to git, I sent the same patch as a git `.patch` file and then as a darcs `.dpatch` file, you can download them and see the differences. As you can see, they both use `+` and `-` indicators to mark lines which have been added and removed, but that's the only similarity. The file delimiters are different. New files are indicated differently. The `.darcs` file ends with a large number of "Context" pieces which take more space than the actual diff. The top of the files are different too, but you can't see that because I produced my `.patch` file with `git diff`, not `git format-patch`. Here is what the top would look like if it was: Use non-mixfix form in the "Don't know how to parse" error message. --- doc/release-notes/2-3-2-2.txt | 2 ++ test/fail/Issue147a.agda | 11 +++++++++++ test/fail/Issue147a.err | 5 +++++ test/fail/Issue147b.agda | 11 +++++++++++ test/fail/Issue147b.err | 6 ++++++ 5 files changed, 35 insertions(+) create mode 100644 test/fail/Issue147a.agda create mode 100644 test/fail/Issue147a.err create mode 100644 test/fail/Issue147b.agda create mode 100644 test/fail/Issue147b.err &gt; How is it for someone who has never seen version control before? Easier than git? Oh my. Don't do this to yourself. It is technically possible: you'll have to install both version control systems on your machine, you'll have to install extra conversion tools, and those tools will add extra meta-data to your commit messages. I have done it, but it's usually a fragile setup, so you'll need to master both version control systems in order to diagnose the issues you will inevitably encounter. Git has a lot of quirks, so if this is your first time using it, you'll already have a lot on your plate. If you want to use darcs to interface with a remote git repo, you'll have to deal with both git's quirks _and_ darcs', so I really don't recommend it.
/u/peargreen Can you still reproduce the compilation performance difference for 1 splice vs. many in TH with GHCJS? I just tried this transformation on a project, and even for a module with 70 `deriveJSON`/`makeLenses` I couldn't measure a difference. There's less `Linking Template Haskell` invocations in the output but the compile time seems the same for me with ghcjs 8.0.2 in nix.
`UndecidableInstances` is both safe here and is (currently) necessary. Why? The usual check for safety is that _every_ argument is being inductively shrunken in size. However, once you have functional dependencies, this check is needlessly restrictive. When you have something like class Monad m =&gt; MonadState s m | m -&gt; s then shrinking the `m` alone is sufficient to ensure safety. The check that is used out of the box isn't smart enough to check this condition however. class HasEmployee k e | k -&gt; e has this same structure. instance HasEmployee k e =&gt; HasEmployee (Shift k) e only recursively shrinks the "head" of the functional dependency. It doesn't pass the naive structural induction check, as 'e' wasn't shrunk as well, but any infinite recursion that could arise here would have to spin on the first parameter, not the second. It'd be nice to have a name for this slightly smarter check and a well-written summary of safe rules. (It can get at least a little complicated once you have things like `| a -&gt; b, b -&gt; c` so there is some normalization to be done that would also have to be included in the check and the documentation of how the check would proceed. As a straw man for how it could proceed: You could compute a partial ordering on MPTC variables by taking the pre-order given by the functional dependencies and completing it. Then for each instance you can separately compute which parameters are doing structural induction, and induction on any variable that is *strictly* greater than another in this order variable suffices is sufficient to replace the old structural induction requirement: you don't need to do structural induction on _all_ of the variables independently. GHC doesn't have this sort of smarter induction check encoded in it today. *tl;dr* The check used to ensure the lack of cycles out of the box works fine for normal typeclasses, but once your `MultiParamTypeClasses` have `FunctionalDependencies` it is too conservative.
Thanks! We've tracked the need for streaming support [here](https://github.com/qfpl/sv/issues/10)
I wrote a blog post in 2016, [Thoughts on using fractional types to model mutable borrowing of substructures](http://evincarofautumn.blogspot.com/2016/01/thoughts-on-using-fractional-types-to.html) which might give you some ideas. It seems like they *should* be useful for something, but no one has yet come up with a model that clearly works. I also linked this from the [Negative and Fractional Types in Haskell?](https://www.reddit.com/r/haskell/comments/6e7s58/negative_and_fractional_types_in_haskell/) thread.
I use darcs for most of my stuff. It works quite well. Simple to grok and use. Well, as simple as version control systems can get. They are all a bit painful, IMHO, but darcs is definitely simpler to use than git. I also use git, which is a lot faster than darcs, but way more complicated. I'll freely admit to frequently messing up my git repos, in ways I hardly ever understand. This is not the case with darcs, where I've yet to mess up. I don't know anything about compatibility between darcs and git, but I suspect it's not a great story. I do know that you can export darcs repos to git without too much hassle. Usually I reach for darcs for personal and non-public (company) projects and git for things that are public and where sharing matters. Like it or not, git/github has more or less won the VCS "war". If you care about sharing and contribution from other developers, git is the way to go. 
&gt; There is an isomorphism between isomorphisms of types and isomorphisms of Maybes of those types. Is there? I can easily construct vacuous :: Iso (Maybe a) (Maybe b) vacuous = Iso (const Nothing) (const Nothing) Note that this works for any `a` and any `b`. Now, since you believe this is isomorphic to `Iso a b`, you should be able to easily use `vacuous` to derive an `Iso a b`, right? But of course you can't: this would amount to proving that any two arbitrary types `a` and `b` are isomorphic. The difference is that in one case I can produce Nothing values to avoid having to know anything about `a` or `b`, but in the other case I can't.
I'm excited to try this out (last time I tried CSV parsing in Haskell, it was fine but a bit annoying, and I felt we could do better), so please don't take my comment the wrong way, but... * I found `Divisible` and `Decidable` somewhere between a pain and a nightmare to use (`hasql`, IIRC). Great that they exist; questionable to design an API around them. Have you ever wanted to represent a sum type only using binary sums? Well with `divide`, you have to. * Just today I was writing `quickcheck` code but couldn't make an `Arbitrary` instance, so I had to make differently named functions for each type... It felt like writing OCaml. It wasn't unusable, but it was less pleasant than I was used to in Haskell. That said, I do look forward to trying out this package, and maybe the ergonomics of the interface are much better than I imagine.
(Disclaimer: I sit next to the author of `sv`) I've found that Divisible and Decidable are much easier to use when paired with `generics-eot`, which I played with a little [here](https://gist.github.com/dalaing/c27dbe3eb209d399197b2a43deb839ff#file-scratch-hs-L53,L64). I'm playing around with some other riffs on that theme elsewhere, and it's looking OK so far. I've been a fan of `QuickCheck` for ages, but I've been having a pretty good time with `hedgehog`. I was already using `Gen`s more than `Arbitrary`s so that I could generate many different kinds of values for a given type without having to use newtypes to get there. Getting the shrinks created for free and being kept in sync tipped me over the edge, and I'm pretty much sold now. I can come up with my own abstractions and helper functions that would be cumbersome with the typeclass solution, etc... Different folks will have different preferences, so it may not be for everyone, but I think it's really worth giving a solid go.
Cool! But I think `lens` is kind of a heavy dependency for this sort of applications. Are you considering to use something lighter like `microlens` as well?
Is it ok to have a mixed feeling about the things you do? (providing the service to drug dealers) :)
Our [encoding example](https://github.com/qfpl/sv/blob/master/examples/src/Data/Sv/Example/Encoding.hs) shows a different way to go about encoding down the bottom. You don't have to use `divide` or `conquer` directly if you don't want to.
Thanks for the detailed answer. I didn't realize that UndecidableInstance was a benign extension so I'm happy to use it. When I realized I needed a `k -&gt; e` constraint , I wondered - out of curiousity - if it was possible to do, by using for example som external type family or any tricks I am not aware of. I realize know that, such probably won't get rid of the "shrinking" problem meaning, therefore the UndecidabeInstance is needed and no tricks is necessary. 
Since sv uses trifecta to parse (by default at least; you can plug other parsers in), we depend on lens transitively anyway. Dropping the lens dependency completely would involve making sv's trifecta support a second-class citizen over in its own package. Microlens wouldn't be enough for our usage so we'd also have to duplicate a bunch of stuff from lens to provide the optics we do (Lens, Prism, Traversal, Iso, Getting, lens, prism, _1, _2, toListOf, view, review, preview, beside) So it's possible to get there from here, but as things stand I don't think it's likely we'll go that way.
How does it compares to `cassava` ? I understand that `sv` is not based on type class, but what practical benefit does it bring ? Defining a newtype and wrapping/unwrapping when necessary is only a few line. Not a big deal practicaly. Disclaimer, I've used cassava quite a bit a learn to like it (even though I didn't at the beginning). I realized that much of the issue I had could be actually solved rather elegantly ... Just a documentation problem. Cassava probably needs a cookbook. Having said that, the features I need from a csv parser - 1 ability to provide different column names for a given column (or better a `String|Text -&gt; Bool`) - 2 ability to skip comments - 3 ability to map column names (smiliar to 1) at loading time (so that you can reuse generic instances but using different column names than field name) Most of all I would like see some sort of unification between the related record &lt;-&gt; pair (field, value) transformation problem. What I mean is :conversion between csv, database table, json, etc .. are the same. It would be nice to be able to convert (via typeclass or decoders) to a "tabular" something which then convert to csv, table etc ... 
I'm going to give pijul a try. Looks straight forward so far.
I use darcs for synchronizing my personal repositories with web pages, lectures etc across laptops; at the time I started using it it seemed to be easier to setup on a user account (no need for root access to setup a server). I've also been involved in the SWERC ACM programming contest in 2014-2016 at the University of Porto and we used a darcs repository with for setting up problem sets, solutions in different programming languages, test cases, etc. I suggested darcs because its patch model seemed to fit the workflow well: each problem had 2-3 people actively working on it, so patches for distinct problems commute with each other, allowing pushes to go through. We used a machine with SSH access that acted as "server" were everyone pushed/pulled patches from. My understanding is that this workflow should minimize the exponential merge issues, but nonetheless we did experience some situations where darcs occasionally consumed several Gb of memory when doing pushes for no "obvious" reason. 
* A negation is a formula of the form "not P". You prove it by assuming P and showing that it leads to a contradiction. This proof strategy works because "not P" is equivalent to "P implies false". * Proof by contradiction works for any formula Q. You assume not-Q and show that it leads to a contradiction. Proofs of negative formulas are constructively valid, but proof by contradiction isn't. 
Very interesting. Thank you for sharing this.
Thanks. That was a significant misunderstanding I had before.
That's right, the Applicative dictionary would hold the methods for the Applicative type class and the dictionaries of all of it's super classes, in this case Functor.
[promises](https://hackage.haskell.org/package/promises) is useful for writing streaming imperative algorithms like this, if you can't find a pre-existing library.
Parser combinators are usually used in Haskell over regex, [here's a tutorial](https://two-wrongs.com/parser-combinators-parsing-for-haskell-beginners.html). It looks like your code is just matching on the first whitespace, whereas the regex101 example is doing a global substition. The [pcre-heavy](https://hackage.haskell.org/package/pcre-heavy-1.0.0.2) package has a function for doing global substition, `gsub`, which should do it: ``` {-# LANGUAGE QuasiQuotes #-} module Regex where import Text.Regex.PCRE.Heavy yourRegex :: String -&gt; String yourRegex s = gsub [re|(?&lt;=\d) +(?=\d+(?:\s|$))|] "" s ```
I still use darcs for personal things. It's just *so* much nicer than git. But the world uses git. And my workplace uses mercurial. When I nee d to be social, I use those.
Darcs natively supports export to git fast-import format.
&gt; it it seemed to be easier to setup on a user account FYI, you've never needed root access to set up a Git server.
I don't know exactly what GHC does, but this is how they're usually described in papers. The dictionaries have field containing the dictionaries of the super classes. If you would want bring the methods in scope of the super classes, you would need to recursively pattern match on those dictionaries.
Any numbers of comparative benchmarks against cassava and csv-conduit? Perhaps also worth benchmarking against `csv` if you're using trifecta.
&gt; Can you still reproduce the compilation performance difference for 1 splice vs. many in TH with GHCJS? I tried with ghcjs-0.2.0.9006020_ghc-7.10.3 and indeed it doesn't replicate. Vlad says that it definitely worked at some point, but now it's outdated advice and I've updated the page to mention that.
Note that type-classes vs combinators, and explicit vs free shrink are orthogonal issues. I'd write Arbitrary type-class (maybe there is already a lib?) for hedgehog if I'll want to use it for free shrinks.
The unfortunate point is that pijul is unstable. For example, about two weeks ago they [broke their own repository](https://discourse.pijul.org/t/current-state-of-pijul-whither-0-9/168). Another example: pijul 0.8.3 is *(at least in one case I have at hand)* incompatible with pijul 0.8.0. I am still to rescue the pijul repositories I made a few months ago.
When you define a class with a superclass, that just gets put into the dictionary: data OrdDict a = OrdDict { compare :: a -&gt; a -&gt; Ordering, eqDict :: EqDict a } This way, if you have this code, it works out trivially: foo :: Ord a =&gt; a -&gt; a -&gt; Int foo a b = bar a b bar :: Eq a =&gt; a -&gt; a -&gt; Int bar a b = ... `foo` can just pass in the `EqDict` to `bar` trivially. If you inlined the fields, you would have to reconstruct the dictionary. Rather, it's easier to implement it like this and then have a separate compiler pass which does inlining, like getting rid of dictionary passing altogether.
GHC-8.4.1 will come with parsec
Thank you for the details! I am not used to the GHC compiling phases, is STG the best suited phase to look for these operational steps, instead of Core for example?
I'd be really interested to know what you find nicer about Darcs than Git. As explained [in my comment](https://www.reddit.com/r/haskell/comments/82k516/does_anyone_here_use_darcs/dvbgko1/) I used to be in the same position as you. I'm planning to write a blog post about how I now use Git and would be happy to take your experience into consideration.
What do you mean kept in sync?
Our team used TTG for a simple c compiler at university. Some parts became more bothersome, but in general it is great. E.g. we separated the "syntactic phase" (everything is just annotated with a SourcePos) to a "semantic phase" where e.g. expressions where annotated with types and declarators with types and name. Currently I am working on something that needs a C compiler that is a little more complete than our submission. I'm using `language-c`. But I really miss having an AST that has type annotations. 
`divided` is messy and `encodeOf` requires an optic. It's much neater to do it thus (for products) https://github.com/tomjaguarpaw/sv/blob/03acea4fad67bd667ad8c919a67f764bb622ce93/examples/src/Data/Sv/Example/Encoding.hs#L93-L100 (I don't know how to make the sum examples similarly nice but `encodeOf` is a nice (partial) approach.) 
I found this blog post on line recently and found it very helpful. I still have occasional problems with stack and cabal new-* commands, and using something that was already on my system was nice. The only thing that gave me trouble was that ReadP saves all the parses so you can end up with some pretty long lists of acceptable parses if you combine a `many` with something. But ReadP seemed to provide a nice starting point that allowed me to learn a bit about parsing in Haskell, utilize a mixture of applicative and monadic styles, and produce something that helped me filter and parse a 460000 line file of eye tracking data that was not consistently formatted (sometimes spaces; sometimes tabs, weird control characters) into the 500 lines of things I actually wanted. I am not disagreeing with any of the other comments. I am just sharing that I found this a nice tutorial for beginners with some familiarity with Haskell syntax and general principles, but without parsing experience. That means mostly people who have come to Haskell via a non-CS route. 
I definitely disagree on this. - It’s far more than a few seconds saved if you have eg a large record from a database. - You have to keep non-generated code in sync with changes to your record - I’m more likely to make a mistake in my JSON parsing code than the generic instance 
I think the answer is: "No, unless it happens to work". There is a nice discussion in the paper "Seven trees in one" by Andreas Blass. Here the type of trees is defined as T = 1 + T^2. And by interpreting this relation over the complex numbers (i.e. using subtraction and other operations), it can be shown that T^7 = T and indeed, the author constructs an isomorphism between 7-tuples of trees and trees. However, T^6 = 1 also holds for the complex numbers, but is clearly not true for types.
A related question - under what circumstances (if any) is the dictionary guaranteed to be inlined at compile time?
&gt; Getting shrinks for free Given that I spent most of yesterday basically just writing `shrink`s, that does sound nice. OTOH, I found that I wanted to use the same `shrink` for two different `Gen`s and that I wanted a shrink that doesn't exactly follow the generation, so *shrug*. I have been wanting to try `hedgehog` to see how it feels, but for this project, I needed to go with the devil I knew.
I also disagree. We have at least dozens of completely trivial instances, and maintaining them if they were not automatically derived would be massively inconvenient.
One disadvantage to this approach is that you lose the exhaustiveness checking of "real" case statements. A second disadvantage is that you rely on the order of arguments to be the same between constructor and handler function. The type system will catch errors of such nature, unless you have two pieces of data in the record with the same type. It would be better if we could pass the record with its fields named, so that order doesn't matter.
Nice! Was this inspired by [this](https://mail.haskell.org/pipermail/haskell-cafe/2018-February/128569.html) recent Haskell-cafe thread, or does it just coincidentally address the issue? Does it keep a completeness check? Does it work with GADTs?
if i see this used in my clients' code (and everything that's on hackage seems to eventually end up in their code), i'm going to flip my desk
/u/casecorp Can you share your Go and Rust programs? It should be very possible to make the Rust program run faster than the Go program.
The "Haskell book" (number 2) is very good and recommended.
Story time? Bad client?
No bad clients, it's just the natural entropy of code if programmers are allowed to pull in whatever library they want.
That would be pretty awesome, and would make me sing a much different tune. I have no idea how feasible that is or if it's a project goal for WSL, but I sure hope that's a world I get to live in someday.
I don't see information about `parsec` here: * https://github.com/ghc/ghc/blob/ghc-8.4/libraries/base/changelog.md#41100-tba Maybe I'm missing some context?.. Also, I thought that `megaparsec` is way better, why bring `parsec` in base then?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ghc/ghc/.../**changelog.md#41100-tba** (ghc-8.4 → 0a3e2f3)](https://github.com/ghc/ghc/blob/0a3e2f324dbd525d626ebd3d97e8ffa1cf2f0ffb/libraries/base/changelog.md#41100-tba) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dvbs1s9.)
I've read both of them. I prefer Manning's 'Get programming' as it brings you to practical programming with Haskell very quickly. Explanations are clear and concise, you don't have to read pages and pages to get something. Every unit ends with a capstone project—they are fun to follow and can be easily extended if you want. Exercises are very helpful to get deeper understanding. Traditionally hard topics like monads etc. are clearly motivated and well explained. This book goes as deep as JSON parsing, HTTP requests, database access and even mutating arrays in ST Monad. There is a small problem in the Unit 1 where the author is too close to his Lisp background and as a result some examples seem strange a little bit for Haskellers. Aside from that the bool is very useful. My main complaint against HaskellBook is its length. I cannot imagine learning through so long material with thorough and perfectly correct but extremely boring explanations and absolutely impractical superficial examples. Nevertheless I've read tons of good reviews about it. Maybe too many for the book on Haskell.
Neat theorem. I'm guessing that the elements of A and B that are mapped to C by A+C ≅ B+C are mapped to eachother by A ≅ B? That's of course still true for my `i`. Since `Just True` is mapped to `Nothing`, and `Nothing` is mapped to `Just False`, the induced isomorphism would just map `Just True` to `Just False`. OP wanted to construct an iso between isos, but I'm pretty sure that there are in general more isos (fsvo 'more') between `A+1` and `B+1` than between `A` and `B`.
I'm going to suggest a third alternative for you to consider: Graham Hutton's Programming in Haskell is really great. http://www.cs.nott.ac.uk/~pszgmh/pih.html The first edition at least had the best explanation of monads (without ever using the word "monad") that I've ever read, and helped the concept finally click for me. Highly recommended.
Ah, that makes more sense, thanks for clarifying. Could you also share how this example would look with `lens` `Prism`s? 
&gt; One disadvantage to this approach is that you lose the exhaustiveness checking of "real" case statements. This caters to non-exhaustive case analyses. It happens that sometimes you just want to match one-to-two cases of a 10 constructor AST, for example. In this case exhaustiveness checking is irrelevant. &gt; A second disadvantage is that you rely on the order of arguments to be the same between constructor and handler function. That's a problem with programming in record-less Haskell in general, yes. A case statement has the same problem. The solution to create another type to hold the slots of your sum type's constructor works for a case or for this. --- An exhaustive TH function could be made, e.g. `$(caseApply [| \case X -&gt; f; Y -&gt; g; _ -&gt; def |])` which would demand all constructors be present.
Use a case statement for god's sake.
It's not change to `base` library. It's change to the libraries bundled with GHC. See https://downloads.haskell.org/~ghc/8.4.1-alpha3/docs/html/users_guide/8.4.1-notes.html#included-libraries (though I think it will be parsec-3.1.13.0).
&gt; he has blog post with recent comparison of all Haskell books available at the moment. Share a link, please.
&gt; An exhaustive TH function could be made, e.g. $(caseApply [| \case X -&gt; f; Y -&gt; g; _ -&gt; def |]) which would demand all constructors be present. Aha! That's exactly what was proposed here: https://mail.haskell.org/pipermail/haskell-cafe/2018-February/128569.html
You could start with `over _Human handleHuman . over _Machine handleMachine` but I don't see a straightforward way to do the `def`.
There is a lens combinator that is somewhat more general: https://hackage.haskell.org/package/lens-4.16/docs/Control-Lens-Prism.html#v:outside The type is an atrocity, but the usage isn't too bad but suffers from the same limitations as this construct. This is an example: ``` Prelude&gt; import Control.Lens Prelude Control.Lens&gt; let x = (\_ -&gt; 3) &amp; outside _Just .~ \_ -&gt; 4 Prelude Control.Lens&gt; x Nothing 3 Prelude Control.Lens&gt; x (Just 3) 4 Prelude Control.Lens&gt; ```
First thanks for a great answer! Do you know of a simple way to make darcs apply a git patch? Or is there some information needed about dependencies. So I don't want to use both git and darcs locally. Darcs should be the main repo. The thing is that this is a commercial project, though for a very small project. Sometimes consultants will do some work. Today this work flow is this: (brace yourself) • Code is mailed as a zip/tar to the consultant. • Consultant makes changes • Consultant mails zip/tar back Now this is obviously not a very good work flow. I'm looking to improve this (I'm not in charge of the project). So I'd like to introduce better version control. For this I'd like to use simple tools. (There will be very limited need for branching.) I also don't want to impose too much software on these consultants that are doing one time things. What I'd like is for them to instead of mailing back a tar, mail back a patch. If they can use darcs for that. That's great. If they want to use git to make patches, I'd like to let them. Do you think this sounds insane?
Terminology: you want to turn your typechecker (which just answers the question "is the given program well-typed?") into an elaborator (which also returns a representation of the program embellished with the inferred types). Bad news: there is not, really, a single universal technique to representing families of related ASTs in a functional programming language. It's been an open research question for a while. The most recent attempt at doing this for GHC was ["Trees that Grow"](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/trees-that-grow.pdf). We [discussed it on /r/haskell](https://www.reddit.com/r/haskell/comments/7zo9mb/any_application_of_trees_that_grow/) quite recently. That said, the `TypedExpression` approach won't be as much of a detour as you fear. Mostly because as your compiler progresses, the intermediate representations will diverge more and more from the source language and from earlier stages, so it may be quite natural for the data types for later stages to look very different from the initial language. One thing you could do, though, that's midway between Trees that Grow and just making an entire copy of the datatype is to parametrize the expression type `Expression a` and attach an `a` to every expression constructor. So your elaborator will have type `Expression () -&gt; Env -&gt; Either TypeError (Env, Expression ProgramType)`. (I wouldn't necessarily stick `Env` into the elaborated expression - later passes will probably want to track pass-specific information about the variables in the environment so it will be natural to recompute the environment again). 
&gt; Git tracks content and uses persistent data structures. Yeah it's actually not a good comparison I made. Git is pretty functional in style, although I don't think it was on purpose. What makes it functional style is its decentralized nature. By having it be decentralized, you copy the state (tracking files) to your local and make changes there instead of directly mutating the master branch. It still in theory is using mutations, however, and patches take a different approach and try to describe version control *starting* with composition of changes. This makes patches very close to many pure functional data structures like sum types, products, functors, etc. It's too bad that it suffers from performance though.
Thanks for clarifications!
Here it is: * https://medium.com/@_bravit/%D0%BA%D0%BD%D0%B8%D0%B3%D0%B8-%D0%BF%D0%BE-%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8E-%D0%BD%D0%B0-haskell-%D0%B2%D1%8B%D0%B2%D0%BE%D0%B4%D1%8B-712c1f5b7749 But you can also read /u/bravit comment here as well.
The reason the Ast seems to be duplicated between the different stages is because you don't encode the invariants of the different Asts. As such you only need different annotations (a partial solution for this exact problem is given in the Trees that Grow paper). You can take advantage of singletons + GADTs in order to encode invariants in each stage's Ast. This way each stage's ast contains proofs that the invariants are preserved and the Asts can vary greatly since many invariants require changing most if not all constructors to express. For instance the renamer may produce an Ast which contains proofs that all referenced identifiers are in scope. The typechecker may produce an Ast with proofs that the types are correct according to a specification. The two Asts will have very different representations and the duplication won't feel as much of a problem.
Huh, so without flags it seems to work mostly on the standard inlining rules. The reason I ask is because afaik in Rust, one of their selling points is that polymorphism is zero-cost, i.e., overloaded functions are always inlined. I think at least in general for Haskell, this is impossible because of RankN polymorphism. But I was curious if at least for Rank 1 we could provide the same guarantee as Rust, but it seems we don't. In fact, [I guess this wouldn't be possible if we did](http://okmij.org/ftp/Haskell/tr-15-04.pdf).
I used it for a bunch of years, before happily switching everything to git. This assessment is from when I switched (~2013 I think), so perhaps things have changed, but my understanding is that nothing is moving super quick in darcs land, so: Darcs had a better UI, for sure, but Magit in Emacs is as good or better than darcs (and git has gained many of the killer features of darcs, like git add -p), so I don't think this is a compelling reason to switch anymore. While thinking in terms of patches is really good for _operations_ (like, rebasing, which in common cases simply is a non-operation in darcs -- you simply grab the patches you need), I think the actual _data representation_ that git has is better. For example, it's really helpful to be able to have a short identifier for a particular version, which git can give you with a sha. In darcs, that doesn't exist (as a version is simply a sequence of patches, so to get anything equivalent you would need much much more data). Since git allows you to manipulate "patches" in various ways, operationally you can get the behavior that darcs has, even if the underlying data model doesn't represent it in that way at all. Branches were also missing, though I think that was being worked on, or people had hacks for it. Not to mention the elephant in the room being collaboration with others who use git...
I don't have advice, but I was *literally* planning to do this in the next week (well, start it). How has it been for you? Is doing it in Haskell a significant overhead from the intended ML (I'd been doing it to fix gaps in my compilers knowledge)?
It applies to any domain with sum types. I use them a lot. The pattern you describe above happens to me all the time, at some point you get sick of writing the same boilerplate and want to just write: filter $(isCaseOf 'Human) xs mapMaybe $(maybeCaseOf 'Human) xs mapM ($(caseOf 'Human) handleHuman mzero) xs I write compilers and handle ASTs a lot, so this possibly happens more. For once I have a TH macro that isn't heavy weight and captures a syntactic pattern I'd like to abstract.
But "I disagree with this sentiment" isn't remotely the same thing as thinking you're being rude *or* dismissive.
I see! Mailing patches does sound like an improvement over mailing zip files. If you can choose the version control system they will use them I think Subversion might be a better idea, it's simpler since it doesn't have to deal with the complexities of distributed version control, and it's easy to find help in the internet. It doesn't support patches though.
Don't feed the troll. This account asked that very same question in the rust subreddit. For rust ofc.
&gt; The only thing I see immediately is to define another type, analagous to Expression - TypedExpression - that contains both the type information, the environment, and the original Expression, and return that. My impression is that that is not a good idea because it's essentially duplicating the structure of Expression (and would duplicate it for every distinct step); however, I don't know if that's actually bad. This is one of those things that's much better in ML. You can parameterize your AST-defining module by the set of types of things that are added to the AST at each stage and then instantiate your AST functor (in the ML meaning of the term) appropriately. In Haskell, you either need to add lots of type parameters to your AST representation or just duplicate the AST for each step. (Using GADTs to awkwardly encode invariants may be another option, but I've tried this in the past and regretted it for various reasons.) Both options are pretty poor—so poor, in fact, that I believe GHC itself defines a tree with information that may or may not be present and then simply shoves in `undefined` for the earlier stages. 
Presume less. There's another reason why some people talk funny.
You know? You're right. I was very grumpy this morning and I apologize for that.
&gt; Not to mention the elephant in the room being collaboration with others who use git... This is really what I'm after. In the same way people needed git to be compatible with svn in the beginning; which you can get with git svn; I really need darcs to be compatible with git, even if it's just in really basic ways.
Well, I get that the patches in theory have some of the characteristics that functional stuff has. But If you talk data structures, git is way more functional. You never mutate anything in git. You make a new file with the changes you want. Just as you would when updating an array in Haskell. So no, there's no mutation going on in git.
Your criticism of HaskellBook is valid, but no other introduction covers the same ground. Other books seem to end at Monads, but Monads are only the half way point in HaskellBook.
Nice, but the efficiency really isn't a problem for me. &gt; ## Is Pijul interoperable with other systems? &gt; &gt; Not yet, although the darcs team is working on it. Pijul’s patches do not store exactly the same information as in other systems. However, since Pijul generalizes both git/mercurial/bazaar/svn and darcs, it should not be too hard to convert our patches to these tools. &gt; &gt; If you’re interested in the task, please contact us. This is though. :/
Conceptually, darcs is a vastly better way of thinking about code development. Darcs makes the bold statement that code development should not be thought of as a sequence of states of the full code in time, but rather as a collection of local changes woven together, and that the weaving is based on commutativity and not time sequence. For me, after years of branch-based version control, this observation was liberating and enlightening. Suddenly, version control became a kind of virtual reality that helped me to understand code development at a new level, rather than an unpleasant chore. I know that in today's world I have to use git. But after experiencing darcs, it's hard for me to believe that I will ever be able to see git as anything more than a a large and powerful set of tools to avoid much of the pain that you must ultimately suffer when you do version control the wrong way.
Interesting point. Considering the people that will work on this project, simple is a great pro though. Is there anything you'd rather use darcs for today?
Yes! I demand to know how this story ends.
&gt;My main complaint against HaskellBook is its length. I cannot imagine learning through so long material with thorough and perfectly correct but extremely boring explanations and absolutely impractical superficial examples. Nevertheless I've read tons of good reviews about it. Maybe too many for the book on Haskell. I got a lot out of the book since working through it was what made things click for me, but honestly I'd agree with all of these. I think the authors took criticism of previous learning material of Haskell too much at a face value and overcorrected. I don't think the usual complaints of mathiness, terseness, inherent brain overload of learning about monads and an excess of rigidity - whether monadic IO, type discipline, purity, functional paradigm - are the sources of real trouble for Haskell learners, but rather the most available explanations which then get repeated.
Seems to fit a very similar purpose to Prisms.
These are all standard problems. If you don't know what Haskell is then you need to address that. I think you likely missed some classes, I doubt those were the first assignments. In the meanwhile a way to start is: http://tryhaskell.org/ 
&gt; That's a problem with programming in record-less Haskell in general, yes. A case statement has the same problem. I'm not sure I follow. &gt; data Foo = Foo { bar :: String, baz = String } &gt; foo = Foo {bar="bar", baz="baz"} &gt; :set -XNamedFieldPuns &gt; case foo of Foo{baz, bar} -&gt; baz &lt;&gt; " " &lt;&gt; bar An explicit case statement allows you to access fields by name, rather than by the order they appear in the constructor. (Though the syntax is tedious if you don't use NamedFieldPuns.)
It's worth noting that the flags are enabled by default, though. Not specializing everything across module boundaries is a tradeoff. I think you can get those guarantees (where possible) with -fexpose-all-unfoldings + -fspecialise-aggressively but that, to quote the user manual, massively increase code size.
Thanks for doing this sort of pioneering work! Pushing GHC to its limit and using cutting edge features in anger helps us all.
I see. I was thinking of this in terms of reusing the existing type's constructor, though in that case you do have to manually plug all of the arguments into the function, which is sort of the whole thing you are trying to avoid.
Thanks for the response! That does help me understand the specific pattern you're abstracting and some of it's uses. 
&gt; Might I ask, why not use a Prism for this sort of data manipulation? Overkill to pull all of lens in? For me, the `lens` approach (aside from being a big dependency) has these downsides: a sophisticated types framework and a requirement to derive prisms for your types, which enter the namespace. The downside for `caseof` is the TH requirement, but it is dead simple. You can read the README in 20 seconds. Regarding your example, that corresponds to `mapCaseOf` which I added locally and have now pushed: &gt; $(mapCaseOf 'Left) succ (Left 3) Left 4 `caseOf` is more like `view`-with-a-default-value, which I'm sure exists somewhere in `lens`. 
&gt; capstone Thank you for the links. Read both of his reviews. LOLed a lot on Haskell Book review :)) 
Quite right.
Do they cover this in that paper? Seems like a glaring flaw for a published paper.
&gt; caseOf is more like view-with-a-default-value, which I'm sure exists somewhere in lens. As I have learned from another comment tree, this would be `set` + `outside`. The type and doc are inscrutable, but the usage is &gt; set (outisde _Left) succ (\_ -&gt; 0) (Left 3) 4 -- pulled the 3 "outside" Left and applied succ &gt; set (outisde _Left) succ (\_ -&gt; 0) (Right 3) 0 -- applied the "default" function to (Right 3) 
For those who haven't heard of Data61's QFPL, they are the ones responsible for the Data61 (and formerly NICTA) FP course.
The final results presented (in which compact regions cause a decrease in performance) are difficult to swallow. Various theories of mine: - Cache is occassionally reloaded (unlikely since post author seems to have a good understanding of what they're doing). - Formerly pinned `ByteArray`s (any `ByteArray` over 3KB is automatically pinned in GHC) became unpinned when compacted. Meaning that `isByteArrayPinned#` returns false. Maybe there is some FFI stuff that checks to see if a `ByteArray` is pinned and changes it's behavior if it's not pinned.
We also have the new: [Applied FP Course!](https://github.com/qfpl/applied-fp-course/) This is designed to be a follow-on or next step from the introductory course. It is designed for people that have some experience with FP concepts and would like to try their hand at building an actual application. This course is designed to be a bit more amenable to self-study than the introductory course. It walks you through building a small REST application. Including a bit of type driven design, database integration, tests, configuration, and some monad transformers. There will be another stream added to this course for building a command line application. Web applications are a nice common ground to start with, but we can do better. :)
Also, I just opened [#14900](http://ghc.haskell.org/trac/ghc/ticket/14900) because this has been bothering me so much.
Hey! I'm one of the engineers that worked on this. The cache is loaded only once on startup, shoved into the `yesod` `App`, and then basically gets relegated to `ReaderT`, so the cache getting reloaded isn't the problem. The data from this cache is read quite often, and sometimes huge chunks of it are transformed, filtered, mapped, etc. I wonder if there is some overhead pulling data out of `Compact` with `getCompact` that overshadows any sort of CPU savings from less garbage collection. Needless to say, we are also puzzled why this didn't work; we thought it would be an easy win!
The overhead should be really small. After all, `getCompact` is implemented as: getCompact :: Compact a -&gt; a getCompact (Compact _ obj _) = obj So, you're just following a pointer. That aside, it doesn't really make sense to store the `Compact Cache` in `App`. You can just call `getCompact` once right after you're done building the compact region and then store the `Cache` in `App` like you were doing before. The only benefit to keeping it wrapped in `Compact` is if you need to add more stuff to it later, which I don't think you are doing.
I found this in GitHub and thought it was cool
Great to see this kind of thing being created in Queensland, I look forward to setting some time aside to go through it.
It's a pretty sweet course, you definitely feels like you had some great help polishing it! =)
Yay! Hit us up on #qfpl or email if you have any questions. PRs and Issues are thoroughly encouraged as well, should you have suggestions or encounter problems.
The thing that made it click for me was the connection to logical quantifiers—the dependent pair type Σx:A. B[x] is related to the existentially quantified type ∃x:A. B[x], which operationally can be implemented as a pair (e.g. a closure and a function pointer, a vtable and an object’s fields, a typeclass dictionary and a value); and the dependent function type Πx:A. B[x] is related to the generic type ∀x:A. B[x]—see also Λx:A. B[x] in System F.
Interestingly, while I was initially really attracted to this world-view, in the end, I decided it just didn't make sense. Because full code has meaning -- it has tests that run, it can be type-checked, etc. Whereas a patch doesn't actually have meaning on it's own (it can't be run, it can't be type-checked, etc). It can only be applied to something with meaning to get something else with meaning. There's a (compelling) argument that, for example, while `git add -p` is really neat (and is exactly how darcs works by default), you shouldn't necessarily do it, because you will end up with commits that records states that were never actually observed (and tested, etc). And I think that conceptualizing the state of code as a sequence of changes is actually somewhat dangerous, because the meaning can become totally screwy depending on how they get composed together: i.e., you might have one set of patches that implements one feature and another set that implements another, but they interact in a subtle way so that when you put them together, it seems that it works but actually putting them together does not work. Now this can certainly happen with git as well, but you have to explicitly do it, by merging, rebasing, etc, and that always creates _new_ commits (which I think is important).
I wish you could get a better notation for this than TH. One thing that comes to mind is that a top-level `deriveCase 'Wiggle` generating a set of functions `isWoo`, `maybeWoo`, `caseWoo`, `isWibble`, &amp;c. would cut down on noise at call sites. This is like how pattern matching works in a concatenative language, where it’s clearer that `case` branches are in fact the inverses of constructors. There you also have the convenience of “eta-reducing” multiple variables via the stack—`-&gt; x, y, z; x y z foo bar baz` is equivalent to `foo bar baz` regardless of how many values they consume &amp; produce. In a language I work on, Kitten, `$(caseOf 'Woo) continuation` would be spelled `match case woo { continuation }`, where `woo` is a constructor with the type: Int32, Char -&gt; Wiggle (An abbreviation of `&lt;S...&gt; (S..., Int32, Char -&gt; S..., Wiggle)`, which you’d write as `forall s. ((s, Int), Char) -&gt; (s, Wiggle)` in Haskell-like notation.) If the continuation is empty (the identity function) then of course the `match` expression has type: Wiggle -&gt; Int32, Char This simplifies a lot of everyday code. For example, Kitten’s `flip` which swaps the components of a pair: type Pair&lt;A, B&gt;: case pair (A, B) define swap&lt;A, B&gt; (A, B -&gt; B, A): -&gt; x, y; y x define unpair&lt;A, B&gt; (Pair&lt;A, B&gt; -&gt; A, B): match case pair {} define flip&lt;A, B&gt; (Pair&lt;A, B&gt; -&gt; Pair&lt;B, A&gt;): unpair swap pair 
&gt; This involves ... Amazing problem set - sucks that there's no remote.
Jej, make the granite functional programming shine!
My experience is very limited, I know very basic C programming - aka I can just about create Tic-tac-toe/minesweeper using C. No I've never compiled a haskell program, but I've seen it done. Thanks you for the advice and sorry for the slow reply, I was asleep right after posting it and didn't think any responses would be that quick. 
The reason for this is the funding form the QFPL comes form the Queensland government, and the funding for positions required the jobs be in QLD. It's a bit odd, but hey, a government is directly funding FP, so that's pretty cool.
What's wrong with parameterizing ASTs? I think something like this: data AST typedness analysis_result = ... type NA = () type Stage1AST = AST NA NA -- before type checking type Stage2AST = AST Type NA -- after type checking but before some other analysis type Stage3AST = AST Type AnalysisResult -- AST ready to be optimized type Stage4AST = AST NA AnalysisResult -- AST after optimization, we lost type info would work nicely, althought I haven't tried it myself yet.
I will organise the aircraft for our scenic flight :)
You should be able to test this by having a two-level cache. One small L1 live cache, and one L2 compact cache.
&gt; Is there anything you'd rather use darcs for today? No, I'd never use Darcs for a new project.
Well the sub is a pretty international environment, so if someone misuses formality, it's probably because this isn't their first language, especially since English formality rules are so nonsensical due to the lack of [T/V distinction](https://en.wikipedia.org/wiki/T%E2%80%93V_distinction).
**T–V distinction** In sociolinguistics, a T–V distinction (from the Latin pronouns tu and vos) is a contrast, within one language, between various forms of addressing one's conversation partner or partners that are specialized for varying levels of politeness, social distance, courtesy, familiarity, age or insult toward the addressee. Many languages lack this type of distinction, instead relying on more explicit wording to convey these meanings. The morphosyntactic T–V distinction, though, is found in a variety of languages around the world. Modern English technically has the T–V distinction, manifested in the pronouns thou and you, though the familiar thou is no longer used in most contemporary dialects. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
I'd love to do something about `Biapplicative` traversals and such with Noah Easterly sometime soon, but I'll be out of the country then.
This depends on the application. If you have a type like "a large record from a database", and by definition you want the JSON to be derived from it, or you don't care about what the JSON looks like, then that's fine. If you want well-designed JSON, and if the structure and naming in the type differ from what the JSON should look like, then this is a poor approach. 
[I’ve tried the same approach](https://github.com/yairchu/peakachu/blob/master/src/Data/ADT/Getters.hs) before learning about lenses and prisms. Have been using lens since and am very satisfied with it. Had to add [‘makePrisms’](https://github.com/ekmett/lens/pull/198) for it but that enabled this feature along with so many more..
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [yairchu/peakachu/.../**Getters.hs** (master → 6461d16)](https://github.com/yairchu/peakachu/blob/6461d160bcd06a60f9ecefc318ee4d30966a15ea/src/Data/ADT/Getters.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dvd8dg2.)
In that situation it can make sense. As always, it depends on the details of the use case, and on various trade-offs. I find that most often explicit instances make the most sense.
What makes parts of code not consistent with others is not the VCS, it's developer error. There is no difference on that point between branch-based and patch-theory VC. On the contrary, by better semantic representation of the local-global aspects of your code, darcs helps you to have *less* such problems. But neither VCS writes your code for you or prevents you from shooting yourself in the foot in that way. I used darcs extensively, and I can't remember it ever happening that darcs caused inconsistencies. As for `git add -p` being "exactly how darcs works by default", I don't think so. The underlying representations of git and darcs are totally different conceptually. It might be possible to implement darcs on top of the git store, I have thought about that. But it would be a lot different than `git add -p`.
I agree with the original analogue of /u/joeblessyou. Git's representation is like a series of "commands" that build your code in a fixed sequence, with branches, just like imperative code. Wherease darcs describes code development by its structure, with theorder-dependant parts kept separate.
Where are you located? Is there much interest in running it in your location?
That can work so long as you do not need to track too many things. It's still a nuisance though.
I have several complaints regarding `Default` type class: 1. Type class has no laws. Basically, you have no clue from seeing `Default` constraint in type regarding what this value contain. Why `0` is default for`Int` and not `-1`? Both seems reasonable to me. 2. If you use `Default` a lot then in code you see something like this: x &lt;- foo def def def def This code is hard to read... You need to go to the `foo` type signature to see types of arguments. And then you need to go to `Default` instances for these types to see values. Just to much work to understand code. Why you need type class at first place if you can just introduce top-level values and write something like this: x &lt;- foo defPort defSlot defConfig defMessage This looks much cleaner to me. And, honestly, this is not really longer than version with `def`.
Yeah, should be. Brisbane
You can employ recursion schemes to make adding or removing annotations to expressions easier. In order to learn more I'd recommend following slides https://github.com/willtim/recursion-schemes/raw/master/slides-final.pdf (or, shameless self-promotion, https://github.com/sergv/kievfprog-2017-november/blob/master/Talk.pdf). But really there's already a lot of information about these out there so you can pick anything you like. In order to illustrate how this is going to look like and work, I have created a sample gist at https://gist.github.com/sergv/58ea87d730963d9cf59de54312331a9e. The gist of the approach (pun intended) is to separate the recursive part out. You'll be left with a type like ExprF, which you can plug into other types like Fix or Cofree in order to recover recursive type or to annotate each level of your tree with custom data, respectively. As a sneak preview, you can even add new constructors in a similar way by plugging ExprF into Free, I have not used that but I think it may be a fun thing to figure out on your own. Please do read any slides as margins of this comment are too short to fit lengthy explanation in.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [sergv/kievfprog-2017-november/.../**Talk.pdf** (master → 85155ae)](https://github.com/sergv/kievfprog-2017-november/blob/85155aee458ffb77767e8072b0af25129c7b13bb/Talk.pdf) ---- 
Please drop us an email and register your interest. This should helping with planning for future courses. contact@qfpl.io
Any ETA on when Stackage nightlies switch to 8.4.1?
This technically works, but it can be verbose. Real ASTs consist of a great number of types and adding parameters to all of them is irritating. You also tend to have a large number of functions that all work on different parts of the AST, so having a single set of type synonyms with one synonym for each stage isn't enough. It practice, the approach you describe is rather messy.
[Release Notes](https://downloads.haskell.org/~ghc/master/users-guide/8.4.1-notes.html)
NNNNice idea to use unicode Braille characters to increase resolution of the graph 8-fold!
stack.yaml file setup-info: ghc: windows32: 8.4.1: url: https://downloads.haskell.org/~ghc/8.4.1/ghc-8.4.1-i386-unknown-mingw32.tar.xz sha256: c543330f9c89f670682541e0ed24e5ec38c53ddffda48367c6e1364367045b0d linux64-nopie: 8.4.1: url: https://downloads.haskell.org/~ghc/8.4.1/ghc-8.4.1-x86_64-deb8-linux.tar.xz sha256: 427c77a934b30c3f1de992c38c072afb4323fe6fb30dbac919ca8cb6ae98fbd9 linux32-nopie: 8.4.1: url: https://downloads.haskell.org/~ghc/8.4.1/ghc-8.4.1-i386-deb8-linux.tar.xz sha256: c56c589c76c7ddcb77cdbef885a811761e669d3e76868b723d5be56dedcd4f69 linux64-tinfo: 8.4.1: url: https://downloads.haskell.org/~ghc/8.4.1/ghc-8.4.1-x86_64-fedora27-linux.tar.xz sha256: 89328a013e64b9b56825a9071fea5616ddd623d37fd41e8fb913dfebc609e7ea linux64-tinfo-nopie: 8.4.1: url: https://downloads.haskell.org/~ghc/8.4.1/ghc-8.4.1-x86_64-fedora27-linux.tar.xz sha256: 89328a013e64b9b56825a9071fea5616ddd623d37fd41e8fb913dfebc609e7ea windows64: 8.4.1: url: https://downloads.haskell.org/~ghc/8.4.1/ghc-8.4.1-x86_64-unknown-mingw32.tar.xz sha256: 328b013fc651d34e075019107e58bb6c8a578f0155cf3ad4557e6f2661b03131 macosx: 8.4.1: url: https://downloads.haskell.org/~ghc/8.4.1/ghc-8.4.1-x86_64-apple-darwin.tar.xz sha256: d774e39f3a0105843efd06709b214ee332c30203e6c5902dd6ed45e36285f9b7 resolver: ghc-8.4.1 compiler: ghc-8.4.1 compiler-check: match-exact packages: []
PIE: position-independent executable
Official link: https://downloads.haskell.org/~ghc/8.4.1/docs/html/users_guide/8.4.1-notes.html
I'm a bit curious here, can you elaborate some more on the difficulties you've had with Flycheck? The git repo looks fairly active and buzzing as well... \*two minutes later\* Nerd-sniped myself: [I found this](https://github.com/flycheck/flycheck/pull/972). That does look like a frustrating experience. Perhaps it might be worth poking the Rust community a bit and see if they're also frustrated by flycheck and whether or not they might be willing to help out? Rust loves throwing manpower at making nice programming tooling :)
Not the OP, but I agree that that’s worth considering. That is an unfortunate default behaviors of the GHC runtime.
Tinfo is short for terminfo, which is a low-level library component of ncurses
It seems that starting from Emacs 26, Flycheck functionality is moving back to flymake and into Emacs core. Read [here](https://www.reddit.com/r/emacs/comments/7x1mgf/starting_in_emacs_26_you_might_not_need_flycheck/) for example.
This sounds like an interesting avenue. There are a number of repeated artifacts in the cache which are packages in different ways. A longer compaction would not really be a problem since it would only run once. Thanks for all your interesting ideas!
I don't believe we have this issue. I was actually under the impression that pinned data would cause compaction failure.
on mac I put set .stack/global-project/stack.yaml to the above and get this: No information found for ghc-8.4.1. Supported versions for OS key 'macosx': GhcVersion 7.8.4, GhcVersion 7.10.1, GhcVersion 7.10.2, GhcVersion 7.10.3, GhcVersion 8.0.1, GhcVersion 8.0.2, GhcVersion 8.2.1, GhcVersion 8.2.2
Any detailed resources on learning about the `ST` monad ? It seems like its somewhat primitive (than, say other _normal_ monads we can define in our code), and I would like to know more about it. 
Am I missing something? It seems like the PR just breaks other use cases. How could they expect that to be merged? I understand it sucks that the other guy wouldn't just fix it since it'd be a lot easier for them, but surely *forking a central component of the Emacs ecosystem* would be several orders of magnitude more effort than simply fixing that PR up? This seems insane to me.
I said not a word about destructive changes. You yourself say: &gt; and references to the previous state So conceptually, it's a linear sequence of changes, analagous to a set of imperative commands. As opposed to a declarative semantic description of the code as it evolved.
Ballerinas are some of the most hardcore people I can think of, so yes, most definitely.
It's been fun so far, albeit kind of slow. I wanted to practice Haskell and learn compilers at the same time - trying to invest in one language, rather than learn a different one for every class I took. I don't know if ML is a better choice outside the (somewhat obvious) fact that the code samples in the book and on the website are all ML, so they can be used directly. I don't know ML either. I'd love to collaborate in some what if you'd like.
Thanks for the help, everyone! Given that this my first compiler and the most significant Haskell project I've attempted, I'm going to take solace in /u/lambdageek and /u/opensorceress and create a separate type for each step of the compiler. The other solutions are really interesting; I'll come back to them for the next compiler. Thanks again!
For Python.
My understanding of linear types is likely very limited, and I've only skimmed the article so far, but a question occurred to me at the start: &gt; In our case, a function with the type `[a] -&gt;. [a]` is necessarily a permutation. I understand why there is this permutation guarantee, since no single `a` can be dropped or duplicated. What I don't understand is why this is not exactly type-equal to `idL :: a -&gt;. a`. Why is the input list's spine allowed to be dropped and a new one created? Surely the old list's spine gets garbage collected, doesn't it? Isn't one proposed advantage of linear types a possibility to bypass the GC? Or am I off the mark here?
I don't think you can use type families like this, but checkout out "Trees That Grow" for a technique that incorporates type families.
It's pretty simple: * Contributing to flycheck is a time and energy sink, not just for me, but demonstrably for others too. Feel free to get this in and prove me wrong. * My 2-year old fork works fine for me and will continue to do so. * However, other Intero users are sitting there with half-working support because Flycheck doesn't support errors from other buffers (and based on the maintainer's feedback, has no intention of adding it). A simple thing to do is just switch Intero over to my fork. Takes no energy or time for me, it's flipping a switch. So I'm asking the userbase how they use flycheck.
I'll have to test that out. It might be a smoother way forward, to switch to flymake.
It doesn't appear yet on the [GHC home page](https://www.haskell.org/ghc/) or [GHC download page](https://www.haskell.org/ghc/download.html).
As I explain in the ticket, there are two notions of whether or not something is pinned. Only one of them causes a compaction failure. But I agree that it's unlikely that this is the issue. There is almost no code out there that uses `isByteArrayPinned#` to change its behavior.
Available for [Ubuntu](https://launchpad.net/~hvr/+archive/ubuntu/ghc/+index?batch=75&amp;memo=75&amp;start=75) and [Ubuntu WSL](https://launchpad.net/~hvr/+archive/ubuntu/ghc-wsl) vi /u/hvr_'s PPAs.
I'm sad that `QuantifiedConstraints` didn't quite make the cut, but the new faster release process means this isn't so drastic as it has been in years past. We can still get it in 6 months! =)
Totally there if there's pizza
I'd much rather offer another option completely: [Thinking Functionally with Haskell](https://www.amazon.com/Thinking-Functionally-Haskell-Richard-Bird/dp/1107452643) by Richard Bird. He has a remarkable ability to teach people to use equational reasoning.
I ended up sticking with Rust and using crossbeam to really eek out the performance. The linear time guarantee of the regex library is amazing, thanks for all your work.
That I understand completely.
I *think* that if you use an unsafe function inside a linear function that the whole thing overall still behaves linearly. After all, we can squint at the compare code and say "yeah, that's safe to pretend it's linear" but the compiler can't. However, we can tell the compiler to go with it and treat the entire sorting function as linearly typed for similar reasons that we can get away with unsafePerformIO in lower level library code. If my thinking is correct, then since the outer function is linear, any list it's called on shouldn't need to be tracked by the GC, just like unsafePerformIO returns a pure value. (I expect to be at least a little wrong here; linear types are not my strong suit)
see the edit above mine :-)
Hi There! I've ported part of the book, it is kind of a template since it is part of a Compilers Course. I didn't use nothing strange since it is an undergraduate course. Here is the code: https://git.dcc.fceia.unr.edu.ar/Compiladores/TigerHaskell/tree/master/HaskTiger There are some comments in Spanish, but you can ignore them. Another more Generic approach that I've worked out is in this work https://bitbucket.org/fbergero/modelica-hc . There I tried to write an extremely modular compiler. For example, you can write AST using simple notation as type Ast = Statement + CallEq CallArguments + ForEq + IfWhen + EString + CallExpression + ArrayCons CallArguments + Expression + Subscript + FunExp Here we use GADTs to specify where in the compilation process we were, building ASTs from smaller modular ones. Following the ideas mainly in 'Data Types à la Carte' and 'A Generic Abstract Syntax Model for Embedded Languages'.
I dipped my toe in the water of compiling Intero for GHC 8.4's API. There are two major changes (the AST has changed to a nice staging-based family approach) and some miscellaneous module graph APIs. All in all I think I can update it, while incidentally maintaining backwards compatibility back to GHC 7.8.4. Probably a couple hour's work tops.
Counting sort builds a histogram and unfolds it back to a list. A histogram is basically a representation of a multiset, so we can summarize the algorithm as: `Data.MultiSet.toAscList . Data.MultiSet.fromList :: Ord a =&gt; [a] -&gt; [a]` ([multiset](https://hackage.haskell.org/package/multiset) library). To use a less fancy structure, we can build something on top of `Map` (that corresponds to one usage of dictionaries in python for example) ([containers](https://hackage.haskell.org/package/containers-0.5.11.0/docs/Data-Map-Strict.html) library). A histogram is a map from values to integers, and we can add elements to it by incrementing the corresponding counter. Here is the type and its basic operations: import Data.List (foldl') import Data.Map (Map) import qualified Data.Map.Strict as Map newtype Histogram a = Histogram (Map a Integer) emptyH :: Histogram a emptyH = Histogram Map.empty insertH :: Ord a =&gt; Histogram a -&gt; a -&gt; Histogram a insertH (Histogram h) a = Histogram (Map.insertWith (+) a 1 h) Now we can recursively turn a list into a histogram. buildH :: Ord a =&gt; [a] -&gt; Histogram a buildH as0 = build' as0 emptyH where build' [] h = h build' (a : as) h = build' as (insertH h a) -- Or, to make it short... buildH' :: Ord a =&gt; [a] -&gt; Histogram a buildH' = foldl' insertH emptyH we can convert a map to a list of pairs with `Map.toList :: Ord a =&gt; Map a Int -&gt; [(a, Int)]`, and use `concatMap` to generate list segments from each pair and combine them into the final sorted list. histogramToList :: Histogram a -&gt; [a] histogramToList = concatMap (\(a, n) -&gt; replicate n a) . Map.toList So counting sort is: countingSort :: Ord a =&gt; [a] -&gt; [a] countingSort = histogramToList . buildH
Don't forget jokes about muddy chalk.
[The review](https://medium.com/@_bravit/christopher-allen-and-julie-moronuki-haskell-programming-from-first-principles-2015-8df5d7fda9c1), I lol'd as well :)
In this case, it has nothing to do with garbage collection avoidance (remember, by the way, that Linear Haskell only change type checking). The definition of linear functions (as summarised in the proposal) is: &gt; We say that a function f is linear when f u is consumed exactly once implies that u is consumed exactly once (defined as follows). &gt; &gt; - Consuming a value of a data type exactly once means evaluating it to head normal form exactly once, then consuming its fields exactly once &gt; - Consuming a function exactly once means applying it and consuming its result exactly once In particular, pattern-matching on a list is linear, as long as you use the head and the tail of the list linearly. (and yes, the list will then typically be garbage collected). For a bit more detail, let me invite you to visit the wiki page, where some of your questions may already have been answers: https://ghc.haskell.org/trac/ghc/wiki/LinearTypes (if you have more questions after this, I'll be happy to answer them, and complete the wiki page)
Incidentially, just yesterday I verified that `Data.List.sort`, i.e. that it returns a permutation of the input that is sorted and does not reorder equivalent elements: https://github.com/antalsz/hs-to-coq/blob/master/examples/containers/theories/SortSorted.v I guess linear types would have saved me from doing the first of these three proofs.
I second that. It's a good book.
Is this the actual release? I'm still hitting https://ghc.haskell.org/trac/ghc/ticket/14891. Also, I haven't received Ben's usual release email.
And I'm still hitting https://ghc.haskell.org/trac/ghc/ticket/14891. And I haven't seen Ben's usual release email.
Interesting technique, I would like to know *when* it has been useful. Perhaps when implementing some kind of plugin system for an application? It seems to me that, instead of hinding the state "behind functions", so to speak, we could also use an existential type, something like: data Counter = forall st. Counter { st :: st tick :: st -&gt; Counter , tock :: st -&gt; Counter , display :: st -&gt; String } What would be the advantages/disadvantages w.r.t. the approach described in the post?
available /= released ! The release manager has the last word.
I put in conscious effort to not run any 3rd party sources (JavaScript, CSS, Images...) on my website and I avoid using any JavaScript whatsoever on almost all pages. The solution I would prefer is just for people to contact me directly or on reddit/twitter, and then I'd put a footnote near the problematic section and write "vasiliy_san mentioned that this approach does not work well with blah blah...". That way it is accessible to all clients (mobile, weird text-only browsers, things like Safari reader mode...).
Hiding the state in the manner that you describe _also_ hides it from "subclasses"; try to define the `mkCounter`/`ticktock` constructors and you'll see what I mean. As for applications, that'll be a whole other blog post, which is in the works :)
I have received the same message on Ubuntu.
well, there's already a [PGP signed `ghc-8.4.1--release` Git tag](http://git.haskell.org/ghc.git/tag/f9817b5d8995bd5425e6d115962f6365c15880b2), which we usually only do when we're confident enough that the given commit is going to be the one for the final release. So while the binary distribution haven't been blessed officially yet, the GHC 8.4.1 release has been effectively cut at the source level already.
&gt; Hiding the state in the manner that you describe also hides it from "subclasses" Ah, I see. Existentials would only work for functions that don't need to know the type of the old state. The redefined `tock` is like that, but the redefined `display` isn't.
I just sent the release announcement. I'm still working on putting together the downloads page. In the future I would appreciate it if people would hold off on announcing releases until the official announcement is sent. It takes time to put together these releases and it makes logistics significantly easier if I can upload the binaries to `downloads.haskell.org` before everything is finalized without fearing that this will complicate a possible respin.
Are you certain?
Sorry if this is a dumb question, I'm not entirely familiar with linear types yet. Wouldn't it be possible to use all of the values in the input list in comparisons while storing only the largest of each comparison in the resulting list, though? If so, the result would be a subset of the input list, right? Thanks for the great article, keep up the good work!
[[ANNOUNCE]](https://mail.haskell.org/pipermail/ghc-devs/2018-March/015452.html)
The comparison function used in this article returns its two arguments, which must be consumed linearly again. So you really have to put all the elements in the returned list (exactly once).
Thank you for the update and the great work you all do on GHC :)
The docs say that `base 2.1 Core library`. The version is incorrect, right?
What's the salary range?
Isn't that type isomorphic to the one Edsko gives, and hence pretty much equivalent (in Haskell)? You can't do anything with the state other than apply the methods to it, so in a lazy language you might as well do that straight away...
Hmm.. I'll double check. I more than likely missed something.
No, just Haskell (intero / Dante)
Trees that grow is basically this + pattern synonyms
1 is `add -p` right? What's 2? 
If that's pre or pos YOW, I'd love to be in the flight too :)
It says Phase 2 of the Semigroup/Monoid proposal went through, which means Monoid should now be `class Semigroup a =&gt; Monoid a where`.
So I was looking a little into the code and for me it seems that the data types in `SemRep` are a separate data structure that does not allow me anymore to prett-print it back to a C file. Anyway, I'm currently conducting some experiments with rewriting `Language.C.Syntax.AST` in the TTG style. If that goes well, I might think about rewriting the AnalyzeAst too. But again, this is mostly specific for my use case and would be a major change especiallly if consistently applied. I'll write a little more as soon as I know more.
To be fair, the proposal hasn't even been accepted by the steering committee yet. As excited as I am about `QuantifiedContexts`, I must admit that my faith in the ghc proposals process would be a little shaken if people could introduce a change like this without approval. But yeah, just 6 more months!
Do y'all have decent strings yet?
We could translate pattern matching as deallocation and data constructors as allocation. Then we can change the permutation without violating memory safety. In praxis linear types don't change how memory is managed. We can wrap a low level api in linear type to guarantee memory safety without gc, though. 
Bob Atkey has a nice proof in Agda that a function `[a] -o [a]` necessarily computes a permutation: [Typed DSLs for sorting](https://github.com/bobatkey/sorting-types)
My sadness is tempered by an understanding of reality. Approval here is a bit nebulous, in that it is an extension that is being actively worked on by at least two members of the committee, the very folks that wrote all the surrounding code. The language is a vehicle for research. `QuantifiedConstraints` has a pretty strong underlying theory that motivates allowing the handling of quantifications and the addition of entailment to constraints. The math says yes, the papers say yes. Hell, I've implemented them myself in a toy project. There's also a nice laundry list of examples that are currently impossible to state in Haskell without it. Any concerns are more window dressing about how to allow them than if they can be done. GHC has a history of shipping "previews" of upcoming extensions before they are fully battened down to allow the community to get a sense of what is coming. Shipping `QuantifiedConstraints` in this sort of capacity wouldn't have shocked me. The changes to the compiler are quite minimal and easily gated by an extension. I'm also not unduly pained by the fact that this hasn't happened yet. That said, by contrast, _I_ would be far more shaken about the health of our process if we couldn't get to done over the next few months, when the implementation is basically feature complete today and its being worked on "inside the tent" rather than by outside folks pushing work over onto the committee, and where there isn't a huge bikeshed worth of surface area to paint. It would have been nice for me to be able to use it in my current project which leans heavily on the bleeding-edge, but I can stop work for 6 months or, worst case, work off of HEAD releases once it merges. I'm looking forward to being able to throw out large swathes of old code.
https://hackage.haskell.org/package/text
There are references in functional languages too, or what do you mean? These are immutable references, just like in Haskell. It's not a linear sequence of changes. It's snapshots of the repository and how it looked before those snapshots. Git doesn't track changes at all. It computes changes retroactively when you ask for it. It's very declarative in nature. Both in terms of data representation and in terms of use. This is no critique of darcs. But git is one of the most functional systems used. Even though it's written in an imperative language.
&gt; My sadness is tempered by an understanding of reality. Lucky you. Reality has never tempered any of my emotions.
The paper ["State In Haskell" by Launchbury and Peyton Jones](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/07/state-lasc.pdf) describes it pretty well. In particular section 2.5 explains the problem with a naive `runST` and the eventual solution (in 2.5.2).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [yesodweb/yesod/.../**Routes.hs** (master → ff5618b)](https://github.com/yesodweb/yesod/blob/ff5618bd1543e87af1624efd1f70d8ac7e562969/yesod-auth/Yesod/Auth/Routes.hs) ---- 
I'm not 100% sure , but one possible difference is that the existential version would let you "augment" a `Counter` after it has been created. If I understood the post correctly, you can't do that with the other approach.
You can work around the binary sums issue with some notation: type a :- as = (a, as) pattern (:-) :: a -&gt; as -&gt; a :- as pattern a :- as = (a,as) infixr 4 :- (&lt;:&gt;) :: Divisible f =&gt; f a -&gt; f as -&gt; f (a :- as) (&lt;:&gt;) = divide id infixr 4 &lt;:&gt; (&lt;@&gt;) :: Contravariant f =&gt; f b -&gt; (a -&gt; b) -&gt; f a (&lt;@&gt;) = flip contramap infixr 5 &lt;@&gt; which you could use like &gt;&gt;&gt; :t integer Printer Integer &gt;&gt;&gt; :t char Printer Char &gt;&gt;&gt; :t integer &lt;:&gt; char &lt;:&gt; integer &lt;:&gt; char &lt;@&gt; \Foo n a m b -&gt; n :- a :- m :- b Printer Foo 
Cool ! Nice to see * changelog mentions a lot of things made lazier. This typically implies perf wins in edge cases * removal of ‘error’-throwing code
80-100K
can't a preview be included in subsequent released (for example 8.4.2 😉)?
I've [updated the upstream setup-info](https://github.com/fpco/stackage-content/commit/5d91c17984cd7a594d7b5ad85c93fdf3ab680292#diff-287430ec2804b844873653582ccdc025) with the new bindists, so you shouldn't need the `setup-info` section anymore.
I'm asking for clarification at the moment, but I know that isn't the range.
I use it for Python and JavaScript, although I write much less of those than Haskell. 
Newer versions of most distros use a GCC that enables PIE (position-independent-executable, as heisenbug already mentioned) by default. This is incompatible with GHC when linking to system libraries, so GHC needs to pass `--no-pie` to GCC on these systems or it will fail in these cases. Since GHC 8.0.2, it will do so automatically, but older versions of GHC need a settings patch. In the past, Stack would detect these systems and download the `-nopie` variant of the GHC bindist that includes a patch. However, Stack has changed its approach and now provides a patched `configure` for these older GHC versions that will auto-detect this case, which means the `-nopie` and regular variants are now identical. This means the `-nopie` variants are no longer necessary but Stack still tries to download them on these systems so we have to keep them in the setup-info for now. The next major Stack release will remove this behaviour and at that point we'll stop includeing the `-nopie` variant for newer GHC versions (we'll keep them for older versions for some time to maintain compatibility with older Stack versions). As for `tinfo`: GHC is dynamically linked with libtinfo or libncurses in order to pretty-print and display "smart" quotes and the like. Newer versions of most distributions include version 6 of one or both of these libraries, while older distro versions include version 5. That means there need to be GHC bindists available linked to both versions of both shared libraries. Stack detects the library and version available and downloads the appropriate variant. At this point, most distros have settled on including `ncurses.so.6` (rather than only `tinfo.so.6` as some did for a while), and this is the first GHC release where there are officially released bindists that link with it (the `fedora27` version). For GHC 8.4.1, I actually made the `-tinfo6` variants in `stack-setup-2.yaml` point to the `ncurses6` bindists because I think that should work for just about everyone (at least, it did on all the Linux distros I tried) and will save some work. The current version of Stack will only try a single variant (whichever is most "precise"), but the next major release can try more than one until it finds one that matches the system, so the need for the duplicated `-tinfo6` variant metadata will also be removed.