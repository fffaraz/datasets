Thanks.
Yes. What if you want to render a list of things?
Wrong number, sorry. I meant 3 monadic effects.
Still yes: rendering values from a database where the number to retrieve is specified in the template, for instance.
Does anyone have more advice on how to do this? My current plan is to manually check each dep I add here: http://packdeps.haskellers.com/licenses Doing this manually though makes me really nervous.
Any reason we need an alias for `[Char]` at all? I don't see anything wrong with writing that.
Laziness—or, rather, non-strictness—is what makes Haskell feel so high-level and declarative. I think about it by analogy to garbage collection: both abstract over aspects of the language (evaluation order and memory management respectively), both enable more modular abstractions and both have similar performance tradeoffs. When I write code in C, I always have to be thinking about what declares memory and where it can be freed. Sometimes this makes sense; usually it's just a distraction. When I write in OCaml, I always have to be thinking about the order my code will run in—same idea. It's less expressive and less modular. (I named OCaml because it's similar to Haskell and I've used it professionally; laziness is a big thing I missed from Haskell and part of why OCaml felt relatively clunky and inexpressive.) Garbage collection is a contentious feature itself: I've heard from lifelong C and C++ programmers about how it's the worst feature of Java. But at this point, I think it's fair to conclude that it's a net win in expressiveness and in safety for all but the most performance-intensive tasks. I think the same of non-strictness—abstracting over evaluation order—although the tooling around it isn't nearly as mature as modern GCs. But even if it's only on par with GCs a decade or two ago, I figure it's the right choice until it isn't. (Perhaps I am just the sort of person who would have used Lisp back in the day when GC was still rare...)
Laziness is what makes functions in Haskell composable. Without it Haskell would be no where near as nice to program in. The very occasional space leak is absolutely worth it IMO.
Why do you need to decide that from the template, though?
1. smaller images show the same problem. 2. I separated three channels as indepdendent arrays and summed up them but no improvement. Though I cannot assure that bandwidth competition was eliminated.
I usually see this issue discussed in terms of tooling (automating imports) or Prelude issues (what should be included), but I want to present it as an issue of package organization best practices instead: Too many imports. It's too frequent I have things like: import Data.Map (Map) import qualified Data.Map as Map import Data.Set (Set) import qualified Data.Set as Set import Data.Text (Text) import qualified Data.Text as Text import Data.Text.Encoding (decodeUtf8With) import Data.Text.Encoding.Error (lenientDecode) import Control.Monad.State (MonadState(..), StateT, runStateT) import Control.Monad.Reader (MonadReader(..), ReaderT, runReaderT) ... A single datatype can require way too many imports to get the functionality you need to effectively use it. And that's just for really common programming language functionality; any specific packages you need can mean a lot more. Data.Text is a particularly egregious example: `decodeUtf8With` will almost always require importing two other modules before you can use the function: Data.Text if you want to do anything with the resulting Text value it returns, and Data.Text.Encoding.Error if you want to call the function in the first place. It making writing stuff in Haskell feels like a death-by-a-thousand-papercuts personally. One, it's hard on beginners. This was an issue for me when learning the common typeclasses. Base has *a lot* of modules, and knowing what was worth focussing on was very unclear. So I had to haphazardly learn the occasionally bizarre module structure in base (`Control.Monad` vs `Data.Functor`), which I would frequently get backwards, which was frustrating. I think this is less of an issue post-BBP, but if I could have just done `import qualified Base.PopularStuff as B` to get the things that BBP included there would have been a much weaker motivation to actually change the Prelude in the first place. Now you say "just write your own package that re-exports things in a little more organized fashion", but that just causes fragmentation/discoverability issues. I think the fact that BBP ended up happening anyway shows that trying to organize things in a meta-package just isn't realistic in practice. Second, it's just needlessly repetitive and tedious. I think "don't repeat yourself" is a pretty big theme of programming, and even if you can't cause runtime errors from mixing up an import it's still a waste of time and an annoying interruption to have to keep looking things up in hackage to find out which module exports the functionality I'm looking for. Having core functionality split across several dozen imports is just too fine grained to easily commit to memory. So I think packages need to have more modules that simply re-export some subset of the package, without providing functionality themselves. A lot of packages already do this to some extent: * `Control.Monad.State` really just re-exports `Control.Monad.State.Lazy`, which just re-exports stuff from `Control.Monad.Trans.State`,`Control.Monad.State.MonadState`, etc, but a module that exports a larger subset of `mtl` would still be handy. At the extreme end you've got things like `MTLPrelude`, but there's no reason that `mtl` shouldn't provide the functionality itself. * `Lens` also has `Control.Lens`, `Control.Lens.Type`, `Control.Lens.Combinators`, and `Control.Lens.Operators`, all of which offer different portions of the lens package. A module that also offered a more concise subset of lens (say a best-guess at what would serve 80% of users' needs) would also be handy. I just wish a bit more effort was put separately into whats-the-best-interface-to-this-package separate from whats-the-best-way-to-organize-this-package-internally. I think this is a bigger issue in Haskell than other languages since the type system makes it more practical to have so many small bits of functionality implemented separately from each other.
That was in regard to `stack build`, not your key combo. Instantaneous error checking that allows me to see error directly in code as I type is what I want. `recompile` works fine, but it can be better. I use hdevtools which is fast enough to show errors as I type. I learned about Intero here, so I will try it too.
I use either Chan or TChan for pretty much everything.
Yeah, the first import lets you refer to the type Text as without writing T.Text. That's the only thing from Data.Text that I ever import unqualified. All the other stuff conflicts with functions in Data.List or Data.ByteString, so I always bring it in qualified.
A related limitation is that it's impossible to re-export qualified. I think that if that were possible, it would enable the meta-packages or alternative preludes to satisfy everyone.
The state of the Haskell platform. It is just too minimal as it is. RStudio gives a standard worth aspiring to, coming with many useful libraries out of the box. Leksah is a decent editor, and getting an installer that sets up Leksah along with a broad suite of default libraries would be great. QtQuick could also be included along with hsqml to get a GUI designer going. This would first require a proper project manager that doesn't choke with a large global state of installed default libs, and we have that in stack, but stack hasn't really been integrated into the other tools yet. So I would go for deployment as a major weak point. If not of Haskell programs themselves, then at least for a Haskell development environment. There is no good all-in-one solution right now. This will also ease the path to teaching Haskell as an introductory language, rather than things like Java or Visual Basic. RAD platforms appeal in technical and introductory programming courses because of omnibus default libraries and plenty of tooling (including generation of import lines and project files). You can get your little calculator application up quickly and get VB Express at home to play around yourself. Haskell just makes it too hard to hit the ground running.
actually the reason I need to `stack build` so often is because Flycheck doesnt typecheck across files. So, the two problems are linked. But, thanks about the `projectile` solution. I'll try using that. Will I be able to visit all the error/warning sites from within Emacs easily?
It's just a list of opinions that were stated on the mailing list.
Building things is still a pain. The platform is too heavyweight. Things never "just work". Everything breaks, every time. Stack helps, but also increases the already high bureaucracy. Deploying is nonexistent. You're not writing web/mobile apps on Haskell. Period. Or, if you do, get ready to have a really slow, unresponsive and bloated app. Small community, not enough libs. Want to write a game? Good luck building a game engine from scratch. Machine learning? I hope you literally cracked the brain code... otherwise, most of what humanity knows is in other languages. Performance is extremely bipolar. If you have a very good understanding of laziness, thunks, is good at analyzing the complexity of functional algorithms, and is able to quickly invoke all the mutable monad mumbo jumbo when you need without getting stuck, then you can get C-level performance out of NASA-level complex programs without much effort, and you are almost a god at this point. But if you're anything less than that, you might write faster programs on interpreted Python. Generic programming could be much better. There are many ways to do derive functions for a datatype. Most are awful and require bizarre extensions. There are "automatic" derivations (case of, setters and getters), there is derive, there are generics, and in the end you use Template Haskell. And most things that could be derived in theory aren't. Want to derive a zipWith for your arbitrary datatype? Good luck with that. Haskell's polymorphism is pure genius in theory, but not always in practice. Try writing an unboxed vector of a datatype of your own. Not a mutable one; a simple, pure, unboxed vector. Something so important, even C++ has no trouble with that. On Haskell... good luck deriving the Unbox instance. Or even finding its documentation. (And you need that because it is 100x faster than the easier to use IntMap. Ref the bipolar performance rant.) Did I say documentation is almost always terrible? Which is aggravated by the fact the language itself is quite small, so everything you need is an obscure, undocumented, unmaintained lib that breaks your entire build system when you try to install it. The default syntax-based field accessors/setters are simply awful. You need lens to do anything useful. Yet, when you do... Type errors can be completely cryptic. When you find a bizarre type error that you can't solve, and nobody else can solve, what do you do? A single hard error could cost you a whole day of productivity. And in the end, turns out it was a compiler bug. And now you need to rethink your whole solution from scratch because that particular thing can't be done. This isn't a situation you find in other languages. Trivial things need an absurd amount of confusing type level formalization. Skolems for ST are the worst thing. Monad transformers, monad stacks, lifts and whatnot... help! The monomorphism restriction is worse than any weird JavaScript coercion rule. Advanced type-level programming sucks. Many things can't be done, and, those that can, are hacky and inelegant. The sheer amount of unconventional extensions required, all the special cases, the terribly ugly syntax. While Haskell has many power extensions, they rarely mix well with the base language and make it look like the C++ of functional languages. The numeric typeclasses are a mess. String types are a mess. Also this is a very unpopular opinion, but typeclasses themselves are awful, for two reasons. First, explicit is better than implicit. Do we really *need* to save those 8 keystrokes for manually passing a dictionary (or just picking the proper function for the type directly)? Second, you should only have one obvious way to do things. Typeclasses add a whole new way to do every single thing, without bringing any additional power. The Occam's Razor is completely disregarded. As a result, the whole ecosystem modeled itself after them. It isn't just a simple functional language with data (adts) and algorithms (functions) anymore. It is a category theory fest where everything is a typeclass. A whole new paradigm. And I personally hate it. Perhaps because I'm not smart enough to get it, I admit. But that's how I feel, for now. I think Haskell would be much better if people could just keep it simple... pure functions, datatypes, a minimal language, a fast compiler with multiple targets... and that's it! Why do we need so much else?
IORef doesn't block, so it can't deadlock. It's easier to debug a freeze if you know it can't be MVar misuse. If you don't mind missing commands, you don't need a channel for that. But I assumed something like "rewind" means rewind, not possibly rewind possibly not.
You, by far, have written out the longest enumeration of personal grievances against Haskell I've yet seen in any thread (though admittedly I haven't been here all that long). Which leads me inevitably to the question: Why do you even stay with Haskell at all? (And I do ask out of genuine curiosity) 
I think that's because the `traverse` function `f` is still using random access indexing. You should try and avoid that. Perhaps use the `lookup` function supplied instead? Or, perhaps just using `^+` and `map` like I suggested would work better.
I can't believe I'll use Java as a positive example of something, but here we go... In Java you usually end up importing a huge amount of classes, in fact, I believe if you compared Haskell to Java in the number of import statements versus SLOC, Java's result would be a few times higher. Yet, I've never heard the import statements as a complaint about Java (believe me I've heard many complaints about Java, many of which come from myself.). I think the difference here is that in Java you usually use an editor that writes the import statements for you, and the import statements are usually wrapped by default, so you don't even see them while editing the code. You only see and touch them in the very rare occasion that your editor imports a class with the same name but from a different package than the one you intended, so you have to delete that erroneous import manually. In conclusion, this might be an issue to be solved by tooling?
Sorry about that, remember this is just my personal opinion. In general, the functional paradigm is much better than the imperative paradigm, I'd make a much bigger list about pretty much any other language... and all the mess aside, GHC is still be best functional compiler the world has.
That's not even close to equivalent though.
&gt; What are you doing to deal with this? I never need deal with that.
I find hidden modules sometimes annoying as well. Often when I want to make new function which is almost as something provided by the library (e.g. recently make `http-client` to work with unix sockets), but the helpers aren't exported (even in `.Internal` modules), so I have to copy them. In a sense it's a similar issue as OP's. He might want to add `traceShowId` somewhere in the pipeline, but only the whole pipe (or large parts) are provided, not also the building blocks. I must admit, that I didn't send a PR to `http-client` to export needed helper. I definitely should.
No good workflow for debugging or in general working with packages with hidden modules. No flag to disable them for example.
The `Unbox` and type class cases really have the same solution. Use explicit dictionary passing (higher order functions) where appropriate, and have some tricks around that happen to be very general. Then, rather than defining your own data types, use type aliases so that instances generalize to reach what you are working on. Typeclasses simply shouldn't be in a state where you actually have to use newtypes to choose a different dictionary. Fixing the numeric typeclasses should help a bit. Structural typing obviates all of this; there is no need for type classes to be a special construction any more, functions are implicitly generalized to work like methods in type classes. If people want to name their types, they should explicitly tag them with symbols and ask for nominal typing. Unfortunately Haskell is not structurally typed.
&gt; There are many packages on hackage or stackage that deal with the real world, not things like lens or some monad library. This is just trolling, I do write "real world" Haskell programs that mostly "deal with IO", and I definitely use `lens` all the time. And probably some monad libraries too. &gt; I think most of the stack workflow is great, but this part is really terrible Not sure how stack is related to this mess. IMO it's easier with stack because you can just add the cloned repos as targets in the `stack.yaml` file ... Other than that I agree with you, even though my gripe has more to do with missing exports than with lack of tracing. Perhaps your problem has more to do with the terrible debugging story that Haskell has? With other languages, I don't have tracing enabled where it would be useful, but I can usually single-step and inspect other people's code ...
STM is always a good idea, because it is a higher-level more semantic approach to concurrency. Unless there is some specific reason you need to do manual low-level locking.
There is https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ghci.html#the-ghci-debugger Disclaimer: I never tried. And you have to interpret the module, so you'd need to grab a source anyway.
&gt; In its defense, contrary to the languages you mentioned Haskell compiles to native code (you can do that with LISP too, but then would you retain that capability ?). Not a good defense unfortunately. SBCL (standard common lisp) compiles to native code and still does this. It's also a not-supported-by-$1bn-company project. &gt; &gt; Having a Java-like escape hatch for accessing hidden stuff would be really nice too. 
&gt; When dealing with the real world (IO and friends), I almost always have a need to trace, inspect, print or similar.... I have to clone the package and manually add tracing to the various functions, and then change the build instructions to point to these cloned packages. As someone who writes primarily in Haskell for my day job, this simply does not mesh with my experience at all. I've only had to clone a package and manually add it to my project one time in over a year of production Haskell use, and that was only to widen dependency bounds. And we use persistent sockets, http client and server, OAuth, push notifications for both Android and iOS, email, and more. The only time I can ever remember having to debug with `Debug.Trace` was when I had a nasty bug where I accidentally made a non-diminishing recursive call on a small let-expression defined function, which caused the server to infinite loop on that branch for that request. That was a pain in the ass to find, and I did resort to print-tracing to do it. There are other problems with Haskell (like getting modules to build nicely together in the first place), but as far as actually developing code goes, my real-world experience has been quite close to what FP propaganda says it should be. Stuff really does mostly just work exactly as I expect it to.
Unqualified imports. It makes it so much harder to read other people's code. I think it's particularly bad in Haskell compared to other languages because of the amount of infix operators and that people redefine them for different uses in different libraries. If people only use the function in a monad or nested monads, it becomes even harder to figure out what it does and where it's from.
Thanks kamatsu, I should've known you'd have the relevant literature at hand =) I'll take a look, thanks for the link.
You should put this information --including the additions in the comments-- in a blog post, or maybe even better on the Haskell wiki (so more can help keeping it up to date). I also really like the 5 points you compare the templating solutions on.
I had to expose a `.Internal` module in the `Earley` package. It was quick, since the author was quick to respond and accept it. You're not always lucky, and even when you are, it still takes longer than a hypothetical `import unsafe`. Chart parsers must observe sharing. The "frontend", `Gram`, used HOAS / `Rank2Types` to make type-safe and guaranteed-unqiue references between `Prod`. My "frontend" needed human-readable productions for debuggability, avoided HOAS for modularity, and avoided `Rank2Types` for better(/fewer) error messages. It used `TemplateHaskell`, and was incompatible. Not a bug. `Gram` was shown to be a safe way to build a graph of `Prod`s. I made my own safe way to build them. There are other ways, in varying degrees of safety/convenience. The library author can't be expected to know and implement them all. Which is one of the many benefits of open source. 
I'm still hoping for the day that laziness is a runtime optimization, rather than a primary developer concern.
What packages are you needing to do this for? The only time I've ever had to do something similar was when I was debugging a `Thread blocked indefinitely on MVar` problem, but that was a bugfix on the library, not in my understanding of the code. I do agree that hidden modules are more annoying than not -- I *want* the option to shoot myself in the foot, and importing `Data.SomeType.Internal.Unsafe` should keep me careful enough.
I agree, if my job wasn't mostly web perhaps Ocaml could be an interesting choice. Although I'm really repelled by the "O" on the name, all the impure stuff and the weird syntax sometimes (`rec`?). It also feels quite like a lone island, with nowhere near as many active people talking about it and answering functional programming related questions.
Ah ok, in that case yeah, you'll have a lot of difficulties. For Haskell to "just work" you need to have a decent ecosystem built around its ideology, and that ecosystem definitely does not exist for iOS/Android as far as I know.
You can just use `hint` to reload your .hs file containing a lucid template at runtime. I'll make a demonstration project if anyone's interested. **EDIT**: here's an [example video](https://www.youtube.com/watch?v=oHYASkrdNq0) I just recorded and the [code to achieve it](https://github.com/haskell-lang/haskell-lang/commit/490034bcd7c9b205787df58e896231f952fca878).
In the example you provided, the real issue that you're having with heist is that it doesn't support attribute minimization. I've opened up a [github issue about this](https://github.com/snapframework/heist/issues/82).
Having slept on it and thought a bit more, I'm going to see how much I can improve my current Heist templates before giving up on it and moving to something else.
&gt; Normal-order evaluation gives an answer if and only if one exists. Applicative order evaluation does not: A counter-point to this: the issue is not evaluation order, but totality. Well-typed *total* programs don't go wrong.
Conversely, I've found multiple bugs in warp and in wai tickled by my app. I've needed to get https traces out of multiple libraries, some of which have a way to enable traces and some not. And that's only the http stuff. Bugs like &lt;https://github.com/yesodweb/wai/issues/146&gt; for example are difficult to reproduce and after adding debugging code you have to rebuild the rest of the web stack to use it. I don't know if hidden modules really make this harder however.
Perhaps I'm overstating its importance :) Look at it this way; I want to install "websockets" - I don't really care what version (yet) - the latest one you have! What I had to do was: 1. Figure out what file to edit - there are two config files - why? 2. Figure out an alien format - granted, it's not hard, but it's a surprise 3. Guess about versions: a. Are they required? b. How do I specify them? Contrast this to other systems wherein I say I want something, and the system works out what I most likely mean, installs it, and makes sure this is repeatable by sticking it in the correct config file (which incidentally I'd prefer not to care about right now because I'm trying to build a websocket app, not think about how the ideal build system might work). It's not a huge deal, but it's a surprise, and when coming from other ecosystems that do a good job of it, the experience is poor.
This is more of a "GHC: The Bad Parts", but my main complaint as a commercial user of the language is that compile times are agonizingly slow. For my part, nothing in the language or libraries approaches this rather well known, pedestrian complaint that I have. :) (I'm typing this as I'm waiting on IHaskell to compile and run each of the cells in the Jupyter notebook I'm working in)
`Double`, `Float`, `Int`, and `Byte` are common, architecture independent types shared across (almost?) all modern programming languages. They are ubiquitous because they are an exact representation of hardware primitives on (almost?) all commercial machines. These types are highly optimized for in hardware design. Haskell chooses to prudently not discard this exact representation. 
Not that typing `let rec` itself is so problematic, just gives me an impression recursion, functions and FP are a secondary aspect of the language. Yep I do think there are great type based solutions to effects, IO, etc, just that monad stacks in particular suck. I like that you mention Ocaml, though, I might certainly consider learning more about it. Those languages were built by a bunch of guys on their free times and are competing with half century old, billion dollar research funded languages.
In an immutable language, arrays aren't nearly as good as they grow in size beyond what the CPU can allocate with one clock cycle. Also, with the kind of optimizations GHC does, lists are often just a way of specifying an algorithm, rather than something that the program actually allocates - in these cases the actual data structure is largely irrelevant.
I don't even use a debugger on my own code - I write it so that testing it on the repl is enough - so that instinct/desire doesn't crop up for me.
&gt; Look at it this way; I want to install "websockets" - I don't really care what version (yet) - the latest one you have! What I had to do was: &gt; &gt; Figure out what file to edit - there are two config files - why? It tells you right there in the error message where and what you'd have to add: It is a member of the hidden package ‘bytestring-0.10.6.0@bytes_6VWy06pWzJq9evDvK2d4w6’. Perhaps you need to add ‘bytestring’ to the build-depends in your .cabal file. &gt; Figure out an alien format - granted, it's not hard, but it's a surprise In the cabal file build-depends looks like this: build-depends: base , simple It's a comma separated list, not exactly an alien way to specify something. I guess no harder to understand than to read the man page of `npm` to discover the `--save` switch? &gt; Guess about versions: a. Are they required? b. How do I specify them? How is that different with `npm`? Unless you're already familiar with the tool, how would you know that you don't have to specify / worry about it there? How's that any different than just not specifying versions in the .cabal file? &gt; Contrast this to other systems wherein I say I want something, and the system works out what I most likely mean, installs it, and makes sure this is repeatable by sticking it in the correct config file (which incidentally I'd prefer not to care about right now because I'm trying to build a websocket app, not think about how the ideal build system might work). You make it sound like `npm` just magically figured out what you want after you spoke your wishes into the laptop screen ;-) You had to type `npm install --save foo`, how did you know how to do that? How is that any better than just adding `foo` to build-depends after the build system instructed you how to? I think here you label things as `amateurish` or `good job` based solely on whether they 100% match your prior experience from other systems. I understand that there's a learning curve involved every time to switch to something new, and Haskell has its fair share of issues there, but I really struggle to see a problem here.
Try the examples from Simon Marlow's book: https://github.com/simonmar/parconc-examples/blob/master/server.hs But if you want to make a RESTful API, you want to use one of the higher level libraries, not work at the socket level. If you want something simple, maybe take a look at Scotty [https://hackage.haskell.org/package/scotty]. If you want to learn how these things work, take a look at their sourcecode. 
On a hypothetical processor that understands Haskell natively, it's entirely possible that the language implementation would be exactly as you describe. Since we don't have processors like that, we have to implement the language on the processors we have. And if Haskell ignored processor-native data types like multibyte integers, etc. to do some operations, it would be the slowest runtime language out there, and completely impractical for regular use. Plus, strictly speaking, types like Int32 or Double aren't arrays. You don't subscript integers or floating point numbers. With floating point, because things don't always line up on byte boundaries, getting the Nth element of a Double wouldn't even make sense generally. It sounds like you've got something like sour grapes over Haskell's treatment of arrays. Care to share? 
Global [coherence](http://blog.ezyang.com/2014/07/type-classes-confluence-coherence-global-uniqueness/) of type class instances is anti-modular -- most global properties are. That said, you can have all the same syntax as Haskell type classes and as long as you don't ask for coherence, instance visibility being restricted by module boundaries is completely okay. You can also play around with named instances and a syntax for explicitly passing a named instances. You get something that looks a lot like Scala implicits, really. But, without coherence, the current implementations of things like (Hash)Map and (Hash)Set become subtly incorrect. The structure of the values reflects the type class instance(s) that were previously used, and will only be correctly navigated if the same type class instance is used in the future. In this past, using incorrect instances of Typeable this could be used to write `unsafeCoerce` and violate memory safety of the language.
Arrays are part of the language spec, and work on all Haskell implementations I've ever heard about. They often perform worse than lists in a pure (immutable), persistent context, because where lists maximize sharing, arrays (even lazy arrays) minimize it. Also, the API specified in the language specification isn't exactly optimized for speed -- you need the `ST` monad for that, and I don't think that monad made it into the language spec., yet.
Ok, I see. I didn't mean "making Haskell modules first-class". I imagined first-class modules more like a record of functions with some syntactic sugar to define all the signatures and to instantiate the different implementations. Of course you wouldn't define class instances in such a "module" (maybe I am using the wrong term).
I did a mini-rant on this topic a couple of years back: https://www.youtube.com/watch?v=yFXzuCFeRGM&amp;feature=youtu.be&amp;t=1h36m55s
Yes. The basic issue is that type declarations are generative. 1. Each datatype or newtype declaration has an effect -- namely, it creates a fresh type constant, different from all others 2. The typeclass mechanism in Haskell makes this effect visible. newtype Foo = Foo Int newtype Bar = Bar Int instance Show Foo where show _ = "hi" instance Show Bar where show _ = "bye" Note that you can't just substitute `Foo` for `Bar` because they can have different `Show` instances. 3. As a result, if you permit type declarations in expression context, then evaluation order becomes observable. This would make Haskell impure (i.e., `(\x -&gt; e) e'` would not equal `[e'/x]e`). 4. Because of this, type declarations are only permitted at toplevel scope, and not in expression contexts. 5. A module is basically the same thing as a top-level namespace. 6. First-class modules let you stick a module into any place an expression could appear. 7. This reintroduces the problem that the toplevel/expression distinction was invented to solve. 
Here's another bit of nasty Heist I have: &lt;p class="intro"&gt; &lt;span class="subject"&gt;&lt;post-subject/&gt;&lt;/span&gt; &lt;span class="name"&gt; &lt;bind tag="poster"&gt; &lt;post-name/&gt; &lt;if-not-equal a="${post-capcode}" b=""&gt; &lt;span class="capcode"&gt;## &lt;post-capcode/&gt;&lt;/span&gt; &lt;/if-not-equal&gt; &lt;if-not-equal a="${post-tripcode}" b=""&gt; &lt;span class="tripcode"&gt;&lt;post-tripcode/&gt;&lt;/span&gt; &lt;/if-not-equal&gt; &lt;/bind&gt; &lt;if-equal a="${post-email}" b=""&gt; &lt;poster/&gt; &lt;/if-equal&gt; &lt;if-not-equal a="${post-email}" b=""&gt; &lt;a href="mailto:${post-email}" class="email"&gt; &lt;poster/&gt; &lt;/a&gt; &lt;/if-not-equal&gt; &lt;/span&gt; &lt;if-equal a="${poster-ids}" b="enabled"&gt; &lt;span class="poster-id"&gt; (id: &lt;post-poster-id/&gt;) &lt;/span&gt; &lt;/if-equal&gt; &lt;!-- TODO: show IP to logged in users (also add a capability for that; hash otherwise) --&gt; &lt;time datetime="${post-iso8601date}"&gt; &lt;post-rfc822date/&gt; &lt;/time&gt; &lt;a class="post_no" id="p${post-id}" href="${post-link}"&gt; No. &lt;post-id/&gt; &lt;/a&gt; &lt;!-- TODO: Sticky icon --&gt; &lt;!-- TODO: Bumplock icon --&gt; &lt;/p&gt; How would you handle all this if/if-not soup? The tripcode/capcode spans could conceivably be there all the time, with some sort of CSS magic to prepend the "##" to the capcode if it's nonempty. But the `&lt;a&gt;` is more tricky.
Love it, every time you start out positive it turns into a diatribe.. :-) But informative, upvoted again.
&gt; I think perhaps you're taking my comments to heart a little too much; they're not meant to be offensive. Don't worry, I personally complain A LOT about Haskell on here, people generally are not quick to take offense ;-) For a lot of the thing's you complained about I totally get where you're coming from. Like the mess you described you had to wade through to setup Haskell. We have a good solution now, but I agree 100% that there is so much conflicting / dated information that you'll almost inevitably get it wrong at first. &gt; Perhaps it's simply that npm has more in common with apt-get than stack does, and that threw me off. Not being able to install a dependency and have the build system track that in one command seems weird to me. Why wouldn't you want the build system install the dependencies? It needs to do that anyway if you check out the repository on another machine. For instance Rust does it the same way. You simply declare your dependencies and Rust builds them automatically: [dependencies] lazy_static = "&gt;= 0.1.15" rand = "&gt;= 0.3" That doesn't seem terribly weird. &gt; Saying that stack build told me what I'd need to add to the config file isn't correct, anyway: on the basis of my experience elsewhere I guessed that I'd need to specify the name and (possibly) version, where the build system gave me something like bytestring-0.10.6.0@bytes_6VWy06pWzJq9evDvK2d4w6 - there's little chance that a complete beginner would guess which parts of that string to add to the .cabal file, even if they guessed it's a comma-separated list (which it wasn't, because only base was in there). It says `Perhaps you need to add ‘bytestring’ to the build-depends in your .cabal file.`, that's about as explicit as it gets. &gt; Im providing a blow-by-blow listing of my thought process from last night, as a beginner who hasn't used these tools or config files before. &gt; &gt; I think it's unfair to surmise that I am basing my opinions/labels on a 100% match with previous systems. These are not problems I had going into NodeJS development - perhaps the introductory texts are simpler or aimed directly at newcomers. &gt; &gt; Whether I'm right or not is certainly up for debate. Maybe if I'd paid more attention I'd have shaved off a couple of minutes. To be honest I didn't pay a huge amount of attention to the build messages as I assumed they were coming from cabal and also assumed that stack had wrapped that for me and was managing the whole environment for me; learning that I needed to alter a .cabal file to get stack working was a surprise in itself. &gt; &gt; I understand that you are struggling to see the problems I have, but that's because you know how the pieces fit together and why their responsibilities are split as they are. The stack/cabal/cabal-install split is confusing and probably not super well explained in a single place. I can certainly see that maybe other languages / builds system have a better tutorial. The issue I have is that you have these very strong opinions on the differences between the two systems, calling one 'amateurish', the other 'good', having very strong reactions like 'seriously? Wow. Ok...'. Given those, I'd expect you have some actual, strong reasons why you think one system is so neat and the other so bad. But when I ask why it was so obvious for you to figure out that you needed to type `npm install --save foo` but following `Perhaps you need to add ‘foo’ to the build-depends in your .cabal file.` is some kind of total insufferable PITA, I don't really get anything back. Similar, why is there this huge confusion about version numbers with one system and not the other? You mention how npm did the sensible thing, and just picked a current version. That's exactly what stack and even plain cabal do? I mean I totally believe you if you say that one ecosystem has documentation that clicked a lot better with you, I just simply don't see any argument here why there's something intrinsically bad and confusing about just specifying your dependencies in Makefile-analog like so many languages do. It seems like a fairly sensible choice.
Perhaps amateurish is an overly-strong word, it has connotations I hadn't really considered. Maybe if I express my preference for a command line tool both installing libraries and updating the local config in one go in terms of the validation it provides before the dependency is enshrined in the config file, that may explain my perspective in a more favourable light. I think, upon reflection, that I see stuff in the config file as "stuff I installed successfully" rather than "stuff I'd like you to install". This is clearly something I've been trained to do, so maybe it should be taken with a pinch of salt. The comfort it gives me is provided in the form of a file that I feel relatively safe committing to source control, rather than one that may not (given my inexperience) be well-formed or explicit enough to avoid it downloading a new major version with breaking changes.
Usually, you access the database before rendering the template (in the equivalent of the controller if any). It might be hard indeed if the person which is writting the template doesn't have access to the control. 
Without totality checking, which drastically changes the language, how do you avoid it? Say your list is not empty, but proving it in Haskell's limited type system is too much work or even impossible -- do you propose propagating the `Maybe` all the way up even though it is impossible? That is in many ways *less* safe than a pure exception. I get an obscure bug rather than a visible one. Do you want division by zero to be defined to some nonsensical value, or does division return a Maybe?
&gt; I am not advocating implicits over type classes. I didn't think you were. Scala implicits and Idris named instances just jump to mind when I think about non-coherent type classes.
Another nice example is [witty](https://github.com/kazu-yamamoto/witty)
A company I know hired full time devs in Sydney, without telecommuting. They did it by forming contacts with the local functional programming group and with UNSW, that has a pretty active functional programming group and several FP classes. The results are very good, but they also were prepared to pay quite well. I know another company that did not pay so well, they ended up hiring a lot of hobbyist Haskell programmers and didn't manage to get much done, TBH. Some banks and financial companies here also hired functional programmers, and it seemed to me like they had trouble keeping them around (although this is only my 3rd party impression). I think it was more due to their management practices than the programmers, though.
&gt; And if Haskell ignored processor-native data types like multibyte integers, etc. to do some operations, it would be the slowest runtime language out there, and completely impractical for regular use *cough*agda*cough*
Wow that's really cool. Does that defer compiler errors in the loaded Haskell file to runtime? Or does it also get checked at compile time? It's certainly possible for that `Hint.as` to fail
Caveat, this does seem to work with (#).
Meh, I tried to explain and failed. I'll leave it at that.
There's some [data](http://download.fpcomplete.com/haskell-survey-2015-05.zip) from [FP Complete's survey](https://www.fpcomplete.com/blog/2015/05/thousand-user-haskell-survey) that touches on hiring; see page 78 and beyond of the second PDF, and if you search through some of the free-form commentary elsewhere you'll find some perspectives on hiring.
Thank you! 
&gt; Flycheck doesnt typecheck across files Also, flycheck *does* typecheck across files, it literally calls out to GHC. What it doesn't do is typecheck changes in one buffer against changes in another buffer. Instead, it uses the version on disk. (It takes the current buffer, and saves that to a temporary file, and then calls GHC which will use the version on disk -- not the emacs buffer -- for all *other* files). So all you have to do to get the current buffer typechecked against another modified buffer is to save the latter to disk. 
I'll add one more, though not directly language related--a killer IDE. For a language with that much rich type information, you could have the smartest, most helpful IDE ever. A cockpit that let's you just lens-ify or point-free something or make sweeping refractors in a couple clicks could do a lot for the language's popularity.
If you do any front-end web development you'd probably like Elm. It's a much more simplified pure functional language.
&gt; Is there any case where `fmap` is sensibly definable but not `&lt;*&gt;`? Yes. An example from [this StackOverflow answer](http://stackoverflow.com/a/36440023): data EitherExp i j a = ToTheI (i -&gt; a) | ToTheJ (j -&gt; a) which has a `Functor` instance: instance Functor (EitherExp i j) where fmap f (ToTheI g) = ToTheI (f . g) fmap f (ToTheJ g) = ToTheJ (f . g) but does not admit a law-abiding definition for `&lt;*&gt;`.
 data Foo f g a = F (f a) | G (g a) Which constructor "wins" for `(&lt;*&gt;)`? Maybe there is a class constraint you should use to combine the information from `f` into the `g` or maybe G's always trump F's, or maybe it is the other way around.
Thanks! This is exactly what I was looking for.
[Counterexamples of Type Classes](http://blog.functorial.com/posts/2015-12-06-Counterexamples.html) might be an interesting read.
To increase compilation speed you can pass the following flags to `cabal configure` (I haven't used stack so far, but I'm sure it has equivalent options): * `--ghc-options="-j +RTS -A128m -n2m -RTS"` * `--disable-library-vanilla` * `--enable-executable-dynamic` * `--disable-optimization` If you are interested in the rationale, look [here](http://rybczak.net/2016/03/26/how-to-reduce-compilation-times-of-haskell-projects/).
&gt; Disallow incomplete patterns. This is something on the language level &gt; If my list is not empty, I can use Data.List.NonEmpty or pass (x, xs) through, ensuring at least 1 item exists. It might be ugly sometimes, but it may be worth it. You're assuming that you can always represent your invariants in the type-level. Maybe with dependent types (and a lot of techniques!). But in the world of Haskell, there will always be some invariants that you do not express in the types. Your list will be non-empty because of things you cannot express as types. What then? &gt; Internally it could be implemented with exceptions to retain performance. Well, you just kept the exception, and if the user knows that the division will never be by zero, you've made their exposed API's less precisely typed.
This is perfect. I've never used PureScript -- is their hierarchy significantly different than Haskell's? Does it *feel* significantly different when writing code? Which would you recommend drawing more inspiration from?
&gt; f and g are Functor instances? Or they are any * -&gt; *? Typically that's something you pick when defining instances, not when defining types (IIRC, it's not even possible without DatatypeContexts, which is being deprecated).
Well I ask because I'm having trouble seeing the `Functor` instance for `Foo` unless `f` and `g` are also `Functor`.
&gt; if you were open to telecommuters Yes, within the same country as us. (We could not afford to be registering in other countries.) &gt; what you ended up with in the end I've hired two people, both working remotely, both available for physical meetings upon request. &gt; if you were hiring full-time devs or consultants One full-time, one short-term but full-time. &gt; if you had to make adjustments to your original plan No. &gt; how it went during the process We got a billion or so international applicants whom we sadly had to flat out reject. The two people I did hire were done after quite a bit of research. I looked into their previous work (one of them I actually knew personally from ages ago), and met them in person a few times with my coworker to make sure we all got along well. I asked them lots of questions that would establish whether they were cultural fits. I also asked about what they wanted from the job, what they thought they could contribute, and so on. But I didn't ask any questions regarding their qualifications, as that was something I evaluated on my own. I'm not a believer in "interviews" or whatever. I think it's quite easy to tell if someone's good or not (when you don't have to evaluate twenty people per week, or whatever, but actually get to spend time on and with someone). &gt; how it turned out in the end Hired two wonderful people, who did extraordinary work. None of us are in the company any longer, but I cherish our time together, and consider them great personal friends. If you have any more questions -- please do ask.
Hiya neelk. I've been thinking about this and related issues a bit. I'm curious if you have a list of the proper bits of literature / citations that state any aspects of this result (which I consider semi-folkloric) in a more formal fashion. (Or which would perhaps demonstrate to me that this isn't a folklore result and that there's a proper citation if I'm to discuss it).
Yeah, I don't think there is one.
Ok so Functor f and g implies Functor Foo, but is still not enough to give us `&lt;*&gt;`. That makes sense. I was confused at first because Foo in general wasn't a Functor on a.
Note: With things like data Foo f g a = Foo (f (g a)) could have different ways to get `Functor`. instance (Functor f, Functor g) =&gt; Functor (Foo f g) instance (Contravariant f, Contravariant g) =&gt; Functor (Foo f g) so even in the `Functor` case you cant always rely on it using `Functor` "all the way down".
That's really interesting. How much of this does `Generic` "know" about? It would be no surprise that the first instance would be found with `derive (Functor)`, but does it find the second as well? I never really gave any thought to this before -- I just sort of blissfully assumed it found all the Functor instances, but now I'm not even sure that's a decidable problem.
Ok, change M.Map to M.adjustWithKey "a". "fromText" and "toText" were left as 'exercises for the reader'. I'm assuming they're one-liners using Aeson. import Data.Map as M import Prelude as P fromText :: Text -&gt; Map String [Int] fromText = undefined -- assuming one-line implementation toText :: Map String [Int] -&gt; Text toText = undefined -- assuming one-line implementation toText (M.adjustWithKey "a" (P.map (+ 42)) (fromText ("{\"a\":[45,5,8]}" :: Text))) 
There are arrays in Haskell, if you need them. Popular libraries like *bytestring* or *text* are actually based on arrays. Primary role of *list* in Haskell is to represent iteration - code as data. So use of loops in imperative language should be proportional to use of list (and recursion) in Haskell. To answer why array does not feel like first class citizen, you must understand implications of immutability/referential transparency and lazy evaluation.
Sometimes I strongly miss fully automatic source code formatter like ClangFormat or gofmt. Such tool would be invaluable for maintaining consistent code style in large code bases and at the same time it will relieve some pain with maintaining code style at the writing time. This is a tough problem for many languages but seeing it solved for other not-so-easy languages like C++ but not for Haskell is upsetting.
At [factis research](http://www.factisresearch.com) we have hired five on-site full-time Haskell developers to work on our product [Checkpad](http://www.cpmed.de). We weren't open to telecommuters. Additionally we require the German language. Each time we were hiring we could go forward with our original plan: We're getting around 5-10 applications per hiring cycle and most of them pass our phone screening during which they have to show us their Haskell coding skills. This is very different to our experience when looking for other developers: In that case we have to do a lot more phone screening interviews but most of them don't pass our coding tests. All of our on-site interviews have been a pleasure and it's difficult to decide. We've hired great developers in every aspect and are very happy with the team we could build. :-)
You might have meant `&lt;*&gt;`
&gt; ``undefined -- assuming one-line implementation`` You can't demonstrate readability like this.
&gt; deriving Generic1 uses fmap to "tunnel underneath" nested data structures. Yes, that's what I was asking, thanks! &gt; Are you wondering if deriving Generic1 would ever spit out multiple instances? Well yes and no: instances of what exactly? This is similar to what was causing my confusion in the top-level example -- what exactly constitutes a Functor instance of `Foo`? `Foo` by itself doesn't have a Functor instance: it's only in the presence of additional constraints that we can make a Functor from it at all. Am I not allowed use different sets of constraints to derive many possible Functor instances of Foo under those constraints? I assumed GHC would only complain if I actually tried to use `&lt;$&gt;` and it found conflicting dictionaries.
In your case, it's built-in to the _Array and _Number functions. In my case, it's built in to those two... you hide your complexity behind an import statement, I hide it behind this comment.
Those are lenses that call out to Aeson. So they're exactly the thing you're saying (but not demonstrating) can be replaced by more readable code.
Could you clarify what you mean by your second bullet point?
I didn't write the lens code snippet. Anyway, I personally find your half-implemented example utterly unconvincing, but so it goes.
Not in Haskell, unfortunately. Once I get to a Haskell compiler I will look at the Aeson docs and implement this.
Problem is, so do I. I'm trying to demonstrate that the three step approach is easier to follow. (1) Convert text to a simple data structure. (2) Manipulate simple data structure. (3) Converting back to text. Advantage: the types involved are less complex. There's no way I could write that lens snippet correctly the first try; and the types that ghci spits back at me are very difficult to grok. So I actually agree that it's more concise, and if you understand lenses, it's perfectly easy to read. But it's harder to maintain because the types are complex. The more I think about it though, this argument is (almost) analogous to "regular expressions vs. manual search". Lenses are another tool in the toolbox, and an immensely useful tool. They make debugging harder, but allow more succinct code. I'm just wary of having too many tools, and being too succinct. There's a certain limit where once you go too high level and too succinct, you lose understandability. For me, lenses crosses this line because there's so many operators and I can't keep track of them all. I may as well be reading APL. But yes, if you're experienced with lenses and can actually grok all of that correctly on the first read, then it's appropriate for you. I think many programmers (myself included) aren't at that level. 
Lots of nice selections there! Too bad I can't find a AUTOBAHN preprint yet...
Yeah, so not exporting types that are part of your API is bad. People should never do that
For those cases I would just use IO
So this one is a bit out there, but I wanted to try to write a version that isn't a straight up translation of the C++. Disclaimer: I consider myself an intermediate (barely) Haskeller. (Haskellista? Haskellite? Tikka Masala?) Anyways, for the isPrime function I I use a list comprehension to generate a list of values from 2 to the integer sqrt of x (the parameter) and generate x `mod` n. Technically there are some invariants that this function doesn't take care (x = -1, -2, etc.) but then neither did the C++ version. I then check if any of the modded values are equal to 0 with the 'any' list function. 'not' works pretty much how you'd expect it to. The xyPairs function just makes a list of pairs of x,y values, it basically does the job of the body of your 'for' statement up to the 'if'. I'm going to skip printTrues for now since it makes more sense if you understand Main. One of the coolest parts of Haskell IMO is that though technically everything is evaluated rather than computed. What I mean is that values are reduced from definitions to a value in normal form (generally, see these [articles](https://hackhands.com/guide-lazy-evaluation-haskell/) for more explanation on reduction) but we can also model computation using evaluation. This is important because in IO I show how we can represent a computation as a value. Look at the definition of promptNum. Essentially I'm taking a string to print out at the prompt and generating an IO action (computation) which prints it, then retrieves a line. In the end the part which says ( putStrLn s &gt;&gt; getLine) will evaluate to a value of IO String. The 'read' function is a member of the Read typeclass (sort of like a Java interface) and it converts string representations of values into actual values. The '&lt;$&gt;' is an infix version of fmap, which we gain from the Functor Typeclass (obligatory [typeclassopedia](https://wiki.haskell.org/Typeclassopedia) link). What it does in this case is take a function of type String -&gt; Int (technically any type that's a member of the Integral typeclass, but I've simplified it a bit here) and turns it into a function of type IO String -&gt; IO Int. This is called 'lifting', which will come up again. To get the min/max input I lift the xyPairs function into the IO type using liftA2 which is provided by Control.Applicative. This function is sort of like fmap, but it works for functions with multiple inputs, in this case it has a type like this: (Int -&gt; Int -&gt; [(Int, Int)] ) -&gt; IO Int -&gt; IO Int -&gt; IO [(Int, Int)]. In this case I also simplified things a bit by writing them as Ints. I then use the &gt;&gt;= provided by the infamous Monad class, to extract the [(Int, Int)] value and feed it into printTrues, which is another function which takes a value and returns an IO Action. In this case, it actually goes through the pairs and checks if each is prime. If they are, it prints them. If not I use return () to return an empty IO action. If you don't understand this, don't worry. I wrote it more to hopefully give you some vocabulary and resources you can use to learn a bit more about the concepts at work in this solution. To the more experienced Haskellers, I'd appreciate critique of the write-up and code as a pedagogical tool for OP (and myself) module Main where import Control.Applicative import Control.Monad isPrime :: Integral a =&gt; a -&gt; Bool isPrime x | x &lt;= 2 = True | otherwise = not $ any (== 0) mods where mods = [ x `mod` n | n &lt;- [ 2..upper ] ] upper = floor . sqrt . fromIntegral $ x xyPairs :: Integral a =&gt; a -&gt; a -&gt; [(a, a)] xyPairs min max = [(xFn n, yFn n) | n &lt;- [min..max] ] where xFn i = i * 30 - 19 yFn i = i * 30 - 17 printTrues :: (Integral a , Show a) =&gt; [(a, a)] -&gt; IO () printTrues = mapM_ $ \(x, y) -&gt; case (isPrime x, isPrime y) of (True, True) -&gt; putStrLn $ (show x) ++ " " ++ (show y) _ -&gt; return () main :: IO () main = liftA2 xyPairs (promptNum "Enter min: ") (promptNum "enter max") &gt;&gt;= printTrues where promptNum :: (Integral a, Read a) =&gt; String -&gt; IO a promptNum s = read &lt;$&gt; (putStrLn s &gt;&gt; getLine) 
Yes. Probably easier if you use a custom-prelude and overloaded strings. (So that doing the right thing is the path of least resistance.)
Been hiring full time Haskellers for a couple of years now at [Front Row Education](https://frontrow.workable.com/), both local and distributed. You probably have seen our hiring ads on every Haskell-related thing on the Internet, sorry :) Worked with both FTEs and contractors, no consultants though. Didn't have to adjust our plan much, we always hired for fairly niche ecosystems (Clojure in 2013-2014, Haskell in 2014-current) so we've never known much better. From what I can tell, based on talking to other companies of our size in the Bay Area, the situation isn't that vastly different from hiring great people for any other language. You're (probably) ultimately trying to hire great software engineers, not great Haskellers. A few solid, productive software engineers out there who are a joy to interview and to work with. Plenty of not so great ones. Vast majority are hobbyists in Haskell, but that's to be expected and shouldn't actually stop anybody from hiring them. Many come from other FP ecosystems. Majority of the programmer population has never touched Haskell, that's fine and you have to accept it, a good software developer should be able to ramp up quickly given enough support and time. Onboarding newbies can be slow if your codebase is advanced, it will take people more time to understand all of the extensions and quirks. Need to be very aware of how much pro-grade wizardry you're stuffing into the codebase, it's inversely proportional to ramp up time, but possibly a good deal for super long term maintenance costs. Noticed a couple of quirks though, that might be unique to Haskell hiring: * people who haven't done much Haskell at all, but really want to commit to working full time in it, and it's a big selling point to them - that's a bit strange to me, why not try more before you "buy"? * people who like Haskell and are passionate about PLT and the more "academic" side of programming, but who are not good software engineers, at all * need to be aware that if you're hiring Haskellers, you'll end up with 80%+ backend development applicants, hard to find people who are actually great and passionate about the full stack * plenty people who are super sold on the vision of the language, are sick of other ecosystems and want the chance to work in Haskell. * occasional "everything but haskell sucks" zealots, but surprisingly fewer than you'd expect. That's a pretty good indicator of community health to me. * expect to have to train almost everybody up from fairly rudimentary language proficiency. This is not Java/Rails/etc, most developers will not come with batteries included. At Front Row we managed to filter for engineers who were already pretty advanced with the language (a couple with 5-10 years of Haskell experience, often professional), but that's absolutely the exception, and depends on whether your organization will be able to attract that kind of talent. There just aren't that many people with production experience in this stuff, so optimize for training.
`derive Functor` assumes that the only source of contravariance is something that is explicitly the first argument to (-&gt;). This is a reasonably safe assumption and usually "does what you mean" but it isn't perfect.
Typically I don't find the need to emit instrumentation or logging information in pure functions. I still have a lot of pure functions, mind, but the instrumentation and logging is always done in the monadic functions that call the pure functions.
Not exactly, having `mult :: (f a, f b) -&gt; f (a,b)` and `pure :: a -&gt; f a`, isn't enough to define `fmap :: (a -&gt; b) -&gt; f a -&gt; f b`. You need all three to make an `Applicative`. Where only `pure` and `&lt;*&gt;` is enough, as `fmap f x = pure f &lt;*&gt; x`. A bit similar as you need `join` *and* `fmap` to make `&gt;&gt;=`, `join` and `return` cannot make `fmap`, `&gt;&gt;=` and `return` make `fmap`; and `&gt;&gt;=` alone gives `join`. -- | one possible way class Functor f =&gt; Applicative f where mult :: f a -&gt; f b -&gt; f (a, b) mult x y = (,) &lt;$&gt; x &lt;*&gt; y -- | ($) is present in a cartesian closed category, -- which is stronger than (lax) monoidal category. (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b f &lt;*&gt; b = uncurry ($) &lt;$&gt; mult f b pure :: a -&gt; f a 
Can I ask how many people your team is?
If you are talking about implementation detail then absolutely everything is just a dumb bunch of bytes. But I don't see why a high level language like Haskell needs to expose this. Indeed in most languages not everything is built on top of raw bytes. You mentioned C family languages; can you tell me how double or float is builtin on top of bytes? They aren't. And conversion between them is undefined behaviour even in C. 
That's correct, I just assumed the `Functor` constraint for applicatives, sorry it was not precise.
&gt; Haskell is one of the few languages that enforces this global coherence property, and /u/edwardkmett strongly advocates for it in his talk Type Classes vs. the World. I was semi-aware of this before -- I knew you couldn't, for example, define two Monoid instances on Nats for `+` and `*`. What I wasn't aware of is that this referred specifically to the right side of `=&gt;` in the instance declaration and that the left constraints were not considered "part of the instance". &gt; Doing so would prevent global coherence of instance resolution. That is, the semantics of your program might change depending on what order GHC tries searching through the in-scope instances to find dictionaries. Well that needn't be the case -- when you have ambiguity at the resolution site, you simply give a compile error. This is what Agda does. It's true that type classes aren't globally coherent there, but it's not true that instance resolution is ambiguous.
&gt; need to be aware that if you're hiring Haskellers, you'll end up with 80%+ backend development applicants, hard to find people who are actually great and passionate about the full stack I haven't heard others mention this before, but I have also observed similar findings. 
I wonder if Idris would be better served targeting GHC Core or LLVM. There are various projects to compile either to JS, so that shouldn't be *too* big a problem. It would certainly be a massive undertaking, but the performance gains could be massive.
Yes, the library code is usually buggy or behaving in an unspecified or unexpected manner in some aspect then the library is interfacing with the real world.
FWIW, I also ran into those two issues the last time I worked on a production project in Haskell (~2,000 lines). I was also bothered by how much thrashing of the code base was required to fix my performance issues by adding memoizing (adding the caches to my data structure, then wrapping everything in a monad). I also felt like I was losing the benefits of Haskell's type system. The correctness of my code depended more and more on the consistency of my caches than on my types fitting together. In the beginning, I trusted my code because it consisted of tiny pure functions that served as the ground truth for whatever value they represented. In the end, I couldn't trust much of my code at all because of all the hidden dependencies, like the success of one lookup depending on the lookup before it correctly updating the cache. I'm used to other languages where it's trivial to add production log statements to monitor a product's health. But I guess in Haskell you need to embed every module in a logging monad, which seems heavy-handed. So I punted on that issue entirely. I still needed logging for debugging, which is why I am now familiar with the handful of syntax tricks you need to know to embed `trace` in the various types of Haskell expressions.
I wouldn't have to clone the library, only the function. Consider: a = ... .. .. exported = do a b c I want to see what happens in `exported`, I could, in javascript write the equivalent of: exported' = do a liftIO $ putStr ... b c I can't do that when `a`, `b`, and `c` are not exported. You can hide access in Javascript as well, but the default is open. For example in node, although you export symbols, there is no difference between a package-internal or package-external consumer of those symbols, so only functions very local to a module would potentially be out of scope for reuse.
The problem, I guess, is that side effects are fundamentally incompatible with pure code. There are *no* guarantees your pure code is even run, as long as the result is correct. If you want to guarantee an order of operations and that all code is executed, you *have to* run it in some sort of side-effectful semi-strict context.
I sympathize. It's very disappointing and frustrating to have to convert a definition from genuinely pure to monadic for some "external" reason. Monads are used to embed impure effects into a pure language. That bears much fruit, as we've seen, but --- implicit or explicit --- they're still *effects*, and I'd prefer to avoid them. The nub of your problem description seems to be that various needs for monadic effects "here and there" requires converting pretty much the whole module to monadic form. Some ideas come to mind: * The "monadification" transform might be a good fit, if it were robustly toolified? * I think monadic style is often overkill for code where it's just a Writer effect you want. It can be simpler to just manually collect the extra monoidal result. (This usually provides some helpful instrumentation.) The "simplicity" is because monadic syntax requires a total commitment to explicit evaluation order, whereas that's typically pretty unimportant for Writer. ... Hrrrm, it seems like an implicit effect could fit here that is sort of the opposite of Implicit Parameters? * I'm always impressed with/inspired by the reach of laziness. Maybe you can leverage it? Examples: the Partiality monad (not Maybe) and "splittable" supplies (see Iavor's `value-supply`). * `ST` usually fits well with graph algorithms. And `runST` provides a "lid" on the state effects that can tamp down the "leaking". * Maybe "monoidifying" parts of your program would be more attractive that "monadifying" them? (I'm really grasping, at this point.) * The "bundles" of Generalized Stream Fusion of Mainland et al struck me as particularly innovative: Your DSL/API "always builds and maintains every form of the value that you could possibly want", and then you ultimately choose one with a final projection, and the DSL musters the best version of that product component it can at that point. * On the DSL front: maybe you can reformulate portions of your code in a bespoke DSL with various interpretations: "simple/golden", "performant", "instrumented", etc. I mentioned the vector bundles because they were an interesting way to do that. And /u/phadej's "type classes" suggestion is akin to Oleg's Finally Tagless DSLs. I hope that mental purge was at least a bit helpful/encouraging for you. Good luck. Edit: rabble rabble phone rabble tired rabble Edit: I always forget about the GHC RTS `eventlog`. CTRL-F "eventlog" on the `Debug.Trace` [Haddock page](https://downloads.haskell.org/~ghc/latest/docs/html/libraries/base-4.9.0.0/Debug-Trace.html). Might be useful for instrumentation.
But it's typically in violation of the laws of the country where the contractor lives. Unless the contractor is self employed. Note that most of the payments going through Upwork and similar are violating local laws, but these companies are not in the business of hiring out labor, only facilitating payment, thus you are the one with the liabilities.
Out of interest - does anyone know what guarantees (or lack thereof) something like [ur/web](http://impredicative.com/ur/) provides in the presence of both ML first-class modules and type-classes?
Is there any chance we could see your code? If so, I suspect you will get much more useful suggestions.
Out of interest, is it possible to install Stack without Haskell? I mean, is GHC a dependency of Stack? I ask because last night I set up a Windows machine for Haskell dev by: 1. Downloading the Haskell Platform 2. Installing the platform along with Stack 3. Running `stack setup` And it looked like `stack setup` downloaded everything again? There may well be differences in the two that passed me by.
Stack will use the `ghc` in your PATH, if it happens to be the right version (and you can bypass that check via `--skip-ghc-check`). But your mistake was to ask Stack to install a new one explicitly by using `stack setup`.
Oh! I only ran `stack setup` because `stack build` told me to. Also, you have an interesting username ;)
&gt; Constructor User does not have required strict fields: userCreatedAt and userUpdatedAt Note that persistent generates strict record fields by default, but it's possible to turn them into lazy fields using ~: https://github.com/yesodweb/persistent/wiki/Persistent-entity-syntax#laziness. I believe this should solve the problem that you're having with your third try. --- That said, may I suggest that it might not be so horrible to do this the manual way while waiting on https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields to make it into GHC: {-# LANGUAGE NamedFieldPuns #-} assignTimestamps :: (UTCTime -&gt; UTCTime -&gt; a) -&gt; IO a assignTimestamps f = do time &lt;- getCurrentTime return (f time time) createUser :: String -&gt; Maybe String -&gt; OAuthAccessToken -&gt; OAuthRefreshToken -&gt; SqlPersisT IO User createUser userEmail userName userAccessToken userRefreshToken = do user &lt;- assignTimestamps (\userCreatedAt userUpdatedAt -&gt; User {..}) insertEntity user --- Also, notice that createUser is not so different from the User constructor function that Haskell already generated for you. I'd suggest that what you're really reaching for are insert / update functions that perform the house keeping on your behalf. E.g. If createdAt and updatedAt are the last fields in the record you may be able to elide the createUser function entirely. insertWithTimestamps :: _ =&gt; (UTCTime -&gt; UTCTime -&gt; a) -&gt; SqlPersistT IO (Entity a) insertWithTimestamps f = do now &lt;- liftIO getCurrentTime insertEntity (f now now) insertWithTimestamps (User email name accessToken refreshToken) --- Alternatively, it may be possible to generate a TimeStampedEntity or something similar, e.g. data TimeStampedEntity record = TimeStampedEntity UTCTime UTCTime (Entity record) ..and then quickly code up all the persistent instances / ToJSON / FromJSON etc around it. I feel it might (?) not necessarily be the most productive use of time though, unless you really have a pretty large number of entities. 
...it's also the 2nd ["Biggest Loser"](https://stackoverflow.com/research/developer-survey-2016#technology-trending-tech-on-stack-overflow) in "Trending Tech" with a staggering -39.6% growth, second only to *Windows Phone* (-65.2%) in net adoption loss (measured by relevant activity on Stack Overflow). **What is going on here?** Is this a fluke? Have Haskellers just stopped raising questions on Stack Overflow, or is this an accurate proxy for a wider and more significant adoption trend? I'm at a loss to explain this, can anyone help? 
Not saying it's the case but in theory, an excellent intuitive language with no bugs and excellent documentation should have fewer questions than an equally popular language with poor documentation and lots of unexpected behaviour.
&gt;an excellent intuitive language with no bugs and excellent documentation The only parts of this sentence that apply to Haskell are the words "excellent" and "language."
Let me disagree ;) We can say that Lambda calculus, type and category théories are well documented ;) Seriously, haskell is vert close to its theoritical foundations which are very well covered in lots of materials: books, courses, blogs, research papers, ... Compared with languages with lots of arbitrary choices or implementation détails making semantics unclear. Haskell has a clean and well known semantics.
https://stackoverflow.com/research/developer-survey-2016#developer-profile-education 70% self-taught developers. Statistics are just a tool to troll the world nowadays.
Supermonads? I am intrigued. I was worried Haskell was becoming too accessible these days. /s
Ha, that would be a fantastic correlation, yes.
It's absurd to think that it is intuitive to programmers, which on the whole don't have a category theory background. It is more reasonable to say that CS students with a good grasp of foundations will find Haskell familiar in spirit. I'm also very skeptical that knowledge of the expected lambda calculus and type theory (as would be taught in an undergrad curriculum) would really make Haskell intuitive. There are a ton of corners people won't have seen: extrapolating multiparameter type classes, monads, GADTs, etc... from knowing the lambda calculus is a huge stretch. And what's more, to really *use* a lot of the parts of the language, you do need to have at least a basic understanding of some of these features. 
I think logging is something that is OK to break purity for. The side effect a logging operation performs will not change the result of the function. That log will not be used as an input anywhere else in the program, and so it's essentially effect-less. Adding a logging operation has not made the pure function impure in any practical sense. On the other hand lifting non-monadic code into a writer monad will force everything logged to be evaluated, and enforce a serial order of evaluation on everything. Logging should have minimal impact on how a program runs. Pulling code into a monad is not minimal. 
Looking at the text under the graph it appear to be %of votes out of all votes cast on SO, so i't does not show growth but 'market share'. The most likely explanation then is that the SO vote 'market' as a whole has grown more quickly than haskells share of that 'market'
Unfortunately, currently it is a work-around, and a bad one at that. Persistent offers many automation features that provide type safety, a very simple human-readable schema DSL, and automatic database creation and migration. It's not worth throwing all that out the window just for the created/revised timestamps. There are better solutions. If someone implements an automatic create/revised timestamp feature for Persistent, then it might be able to work that way under the hood for some backends.
But then again, tooling and ecosystem. What would it take to backport the Purescript category hierarchy to Haskell? Does having named instances make that big of a difference?
The array is immutable. So you have to allocate a new array when you want to update any part of it, no matter how small. There are operations where you can optimise away the intermediate arrays, but that doesn't really depend on them being arrays. 
[removed]
&gt; The problem, I guess, is that side effects are fundamentally incompatible with pure code. There are no guarantees your pure code is even run, as long as the result is correct. Indeed, but this is why the two cases I’m talking about are interesting: their effects don’t matter to the overall structure and behaviour of the program, only in their own little worlds of maintaining the cache or writing the log. Put another way, I don’t want a lot of guarantees about whether these effects happen or in what order. Ideally, I want them to be completely transparent to the rest of the program. In design terms, I would like to reason about and compose functions that happen to be memoized or to include some logging output in the same ways that I could reason about and compose their truly pure equivalents. The challenge I have found in Haskell is that there’s no way to differentiate between the locality of different effects like this. Functions and modules can’t manage private state and effects transparently and still appear as pure functions to the rest of the program. Of course, since the behaviours I’m describing aren’t really pure under the hood, it’s perfectly reasonable for Haskell to object to the kind of thing I’d like to do and force me to explicitly thread the necessary resources through the code so they’re available where they are needed and the effects are properly co-ordinated. It’s just that unlike Haskell, I know those effects should have no bearing on the rest of the program outside of very specific areas. It seems unfortunate that huge swathes of reasonably tidy and already working pure code need wrapping in monadic infrastructure to accommodate these practical programming tools that, by their nature, are often most useful at the lowest levels of the code. I was wondering if anyone had come up with a better way, but sadly it looks like this particular limitation is inescapable in Haskell’s programming model.
Say, what? Where can I read more about this?
Sorry, that’s not practical in this case. The code is proprietary and contains a client’s trade secrets.
That’s essentially what I’ve been looking at. However, as the program gets larger, I find all those monadic annotations get unwieldy. It’s the classic problem that now if a new low-level function needs access to that memoized function, everything that can ever call it suddenly needs the monadic wrapper, lifting, and so on, all the way down the call stack. Throw in combining multiple variations because similar issues arise with logging, and my once clean and elegant algorithms are starting to look a lot more like boilerplate and a lot less clean and elegant. :-(
It sounds like for this problem you're living right on the cusp of the pure vs monadic world. Before I found Haskell I did a lot of game programming (two player perfect information games like chess, loa, etc). So when I started learning Haskell the first thing I wanted to do was write a chess engine. Now, after six and a half years of full-time professional Haskell development I still haven't written a chess engine in Haskell. This is mostly because my interest has shifted to other things, but I think it is also partly due to this problem you describe. The elegant examples you see here and there are pure, but robust real world solutions require monadic code. For example, check out these two examples that I found from a quick Google search: http://stackoverflow.com/questions/29617817/alpha-beta-prune-for-chess-always-returning-the-first-move-in-the-list https://wiki.haskell.org/Principal_variation_search Real world chess engines cannot be written like this because they have to do time management. Every so many nodes they must check how much time has elapsed and determine whether the search must be stopped or not. It took me awhile to realize that a real world competitive chess program in Haskell would require a much different design than the trivial pure stuff that you see in most of the freely available examples. I think that is what you are experiencing here. If you want to do logging inside your graph computations and you want the logging to include timestamps, then you have to build your infrastructure on top of IO. If you don't need timestamps, then you could potentially do something like accumulate the log messages purely and then actually log them later. Another possibility that I'm surprised hasn't been mentioned in this thread is to use a free monad. With that approach you essentially define the primitive operations ("instructions" as it were) of your problem. Then you write the algorithm in terms of these operations. Ultimately this will be executed by an interpreter that you write. You can have multiple interpreters. You might have a "pure" one for debugging and tests and a fully fledged production one that runs in IO and does all the caching and instrumentation that you need in production.
The Causal Commutative Arrows Revisited paper is amazing. Wow.
Thanks. These seem like workable approaches. Where can I read more about the lazy option of Persistent?
Thanks for the detailed reply. The kinds of thing you’re suggesting are indeed what I’ve been looking at. I’m just not very satisfied with the results of either “monadifying” or “monoidifying” so far. For example, some of those graph processing algorithms can run to hundreds of lines and a whole family of mutually recursive functions. Given the inherent complexity of the requirements, I’m reasonably happy with the design here. The degree of mutual recursion isn’t ideal, but at least the functions have clear logic and they are combined using recognisable folding patterns. However, that means I’m already threading accumulators through mutually recursive functions, because that accumulated state is inherently necessary to the underlying algorithm. A lot of these functions already have signatures with at least four types in them, and in many cases those types themselves include further structure like Maps or lists. In short, these functions are quite complicated enough just meeting their essential requirements. I’m not keen to start adding yet more parameters to pass generic state around, or bolting monads on top of everything else, just in case some low level helper function called within one of these algorithms happens to need memoization to improve performance or I want to add some logging in those low-level functions to investigate some unexpected behaviour. &gt; The nub of your problem description seems to be that various needs for monadic effects "here and there" requires converting pretty much the whole module to monadic form. Yes. More specifically, *causing* these effects is pervasive: many low-level functions might want to access these caches or log diagnostic messages. However, *depending on* the effects is extremely localised: nothing outside of the memoized function itself or a small set of logging functions even needs to know the underlying mutable resources exist. Ideally, a function using memoization or logging internally could still appear just like its pure equivalent to any other part of the code. However, I don’t see any way to achieve that with Haskell’s programming model. There are no mechanisms, as far as I know, to manage hidden internal resources in such an arrangement. That’s why I’ve been wondering how others with growing projects handle these kinds of issues.
That’s broadly the approach I’ve been experimenting with so far. I just find it rather unsatisfying as the size of the program scales up. You can certainly compose functions that need these effects, but if something low down on the call stack needs access to a cache-supported algorithm, you still have to monadify everything down the entire stack to get there. For something that would be literally a couple of extra lines of code in many other languages, all that boilerplate seems like a sledgehammer to crack a nut. We are forced to tightly control everything because we’re dealing with effects, but for these effects we know the rest of the program won’t actually care about the result. It’s hidden away in a cache or a log file that nothing outside the memoized function or the logging module even needs to know exists.
and "bugs"
I think you're being a little opinionated here: yes, it is the Haskell paradigm to use "dumb data" and leave constraints to functions on that data, and that does work only because of global coherency, yes. But that's not the Agda paradigm: in Agda you would paramaterize your sorted set on both the type and its ordering. Insertion and lookup via different orderings is not an issue. &gt; They also don't really do much of anything for instance search only looking for a first order instance in the surrounding context I'm not quite sure what you mean here, but I haven't really had problems with it. I've been able to emulate Haskell typeclasses, even for arbitrary universe polymorphism, including verification of laws, and the resulting user-level code looks quite Haskell-like. For example, I have `Eq` and `DecEq a`, where you can define your equality (intensional or otherwise, so long as it's an equivalence relation). It works on both non-polymorphic types like `String` and `Nat`, and on polymorphic types like `List t` where `t` has an `Eq` instance. `List t` where `t` has an `Eq` instance will yield an equality on `List t` which respects the equality of the parameter. And from the user experience perspective, it works without clutter exactly as one would expect -- you just use your `==` operator and it works. So I'm not sure exactly what instance resolution power is missing, but the power that's there is sufficient for what I've tried to do with it so far.
I use it, installed from the Ubuntu repositories.
Great article!
Thank you for the explanation. I wonder if there could be a concentrated effort for one GHC release to focus primarily on the (native) codegen. I'm thinking fast and small code (more aggressive dead code elimination than split-objects).
Just a minor inconsistent type signature for `mult` in the last code block: mult :: (f a, f b) -&gt; f (a, b) mult (x, y) = (,) &lt;$&gt; x &lt;*&gt; y
Super duper monads to be presented next year 
[This blog post from haskellforall](http://www.haskellforall.com/2012/07/purify-code-using-free-monads.html) helped me figure out how a free monad works. 
It's a new feature. The Yesod book isn't updated about it yet, and so far I haven't seen any blog posts about it. If anyone knows of any, please let us know. But it's pretty clear how to use the new feature - in general - from the [Haddocks for 2.5](https://www.stackage.org/haddock/nightly-2016-07-14/persistent-2.5/Database-Persist-Sql.html). Basically, instead of the old `SqlPersistT` monad, you can use two monads, `SqlReadT` and `SqlWriteT`, depending on whether your operation requires write access or not. In your case, for example, if you only use a limited number of operations that modify the DB, you could expose only `SqlReadT` directly. Then wrap `SqlWriteT` in your own monad which provides versions of your modify operations which correctly update the timestamps.
Have you tested if caching improves your code performance? If you did, can you share the results? In theory GHC would do all the caching you may need, but I imagine in practice it doesn't work that way. I know it does some amount of caching, but never checked how much, and now you made me curious. About logging, if you are trying to see what your program is doing for performance tuning, I think the only way you can do that is with unsafePerfomIO. Anything else will change your code behavior and make any measurement useless. But for functional logging, yes, the way to do it is with some writer monad. You can use laziness to make your code not accumulate messages on memory, but GHC will change the executable so it runs all the logging statements you ask for the number of times you call them. The standard recommendation is that you only log on a higher layer, above your complex functional code. You already said you can not do that, but did you try restructuring that upper layer so it sees the complex code as data? It may or may not be worth it, it's hard to say without looking at the code, and it might imply in creating yet anther monad.
Right after React :D
In total functions, it does not. 
&gt;Do you know of a practical AOP framework for Haskell? I don't know of any. However, I will look for the research you mention.
&gt; I think you're being a little opinionated here Yes, I am. &gt; in Agda you would paramaterize your sorted set on both the type and its ordering. Insertion and lookup via different orderings is not an issue. In the talk I mention how this starts to break down once you need more classes. Consider `Compose f g`. We can use one "Compose" in haskell and its bifunctorial. you can map over either f or g with an arbitrary natural transformation. As you do so the properties that f and g satisfy change. If they are Foldable, the result is Foldable, etc. If you live in a world where you tag the composition then you lose this ability to use common operations to switch out f or g, because they don't switch out the 'tag' at the same time. Everything devolves into one-off operations that are peculiar to the data type that you are working with. This is highly antithetical to code reuse in my opinionated experience. ;) You have to know the properties of `f` to replace `g` and put in an appropriate tag that 'varies in power' based on the intersection of functionality of f and g. Module based approaches run into issues of decidable equality when you want applicative modules and they run into issues of being unable to share values returned by structures or getting way too overly parameterized if you start working with generative modules. e.g. Coq gives me "typeclasses" but then requires me to bless minimum spanning tree through a graph where in Haskell every path in the graph commutes. I have code that I'm actively prevented from writing as a result, where I need to know that the result of weakening "Monoid" to get "Semigroup" then to get "Magma" or weakening it to a "Unital Magma" and then to get "Magma" commutes. Look, every single dependently typed language has taken the path you are advocating here. I'm clearly expressing a minority opinion or at least one that is antithetical to the current cultures of these languages, but it is this thing that keeps me from falling in love with Coq or Agda. I'm being highly opinionated here because I feel that there is something of value being lost in the current path we're taking in the dependently typed setting, and I feel that I would be highly remiss if I didn't at least try to articulate it.
&gt; I think logging is something that is OK to break purity for. I think this is true for _tracing_ (for debugging), but not logging. &gt; The side effect a logging operation performs will not change the result of the function. That log will not be used as an input anywhere else in the program, and so it's essentially effect-less. Adding a logging operation has not made the pure function impure in any practical sense. True, except insofar as the fact that logging in a complex system is often part of the specification of the function's behavior. It's also the unfortunate case that using impure logging for complexly mutually recursive functions risks introducing deadlocks.
Ooh, that seems really promising! I'll add that and link to your comment :D
How does you record construction compare to the service handles in https://www.schoolofhaskell.com/user/meiersi/the-service-pattern? It seems as if they give you most of your solution with less complicated types. They also allow you to easily use multiple implementations of the same service in the same function. For example, if you'd want to two http connection pools, one for general web-crawling and one for establishing important internal connections.
I've finally reflected on this enough that the pessimist in me now worries that you and I have grown greedy and spoiled gorging ourselves on the luxuries that Haskell *does* provide. In particular: &gt; There are no mechanisms, as far as I know, to manage hidden internal resources in such an arrangement. I agreed at first. But, there is one "mechanism" exactly for this: the `unsafePerformIO`escape hatch. However, it pulls *no punches whatsoever*. Maybe there's room for a family of `lessUnsafePerformIO` functions --- time will tell. As far as I know, though, anything like that currently amounts to "careful library design around an `unsafePerformIO` function", at the moment (e.g. `ST`). Edit: formatting
On the note of split objects, GHC 8 features [split sections](https://ghc.haskell.org/trac/ghc/ticket/8405) now. This basically puts every function into its own text segment, rather than an entirely new object file, and instead the linker just has to garbage collect unused sections at the end of the program. Which is nice. Unfortunately, GNU ld still has some pathological edge cases with extremely high numbers of sections, just like high numbers of object files, so this still can't be enabled by default just yet. Honestly, if we want to truly explore the DCE route, I think a profitable route would be global intra-package DCE, without relying on binutils. This wouldn't need to work at the native codegen level. At some point in compilation this would turn GHC into a 'whole program' or link-time compiler in the same vein as MLton (purely whole program, global program is available at all points in the compilation), or OCaml with flambda (which, IIRC, does global DCE instead in a link-time manner, by throwing out unused code when you invoke the compiler to put your program together). But, since you mentioned it: the native codegen could use some work too, and I strongly suspect it could be improved in a few areas without exploding the complexity budget too much.
Thanks for sharing, I hadn't seen that post! The design in this post is rooted in `IO`. Every part of the stack is in `IO`, and there's no avoiding it. Code in `IO` is essentially impossible to reason about, and you get no guarantees on the effects. This implementation does allow you to provide different implementations in a pretty seamless way without the fancier types. However, since the code is all in `IO`, there's nothing stopping a function from launching missiles, etc. Abstracting the effects into a type class allows you to *restrict* what each function does. This is the primary benefit. By restricting the effects, you lock down what each bit of your app can do, which makes the application overall easier to reason about. The post you linked describes the issue with type classes well, and I think that my approach solves this issue entirely. By carrying a record of interpreters that are parametric, you can easily swap out different implementations, and you can even ensure on a finer-grained level how your effects work. Taking the two HTTP connection pools as an example, you could have: data Services eff = Services { runCrawling :: eff `InterpreterFor` WebCrawlingHttp , runInterlConn :: eff `InterpreterFor` InternalHttp } and now you're guaranteed not to run requests on the wrong pool.
6 on engineering atm. Don't have experience hiring Haskell for large teams.
There is at least a bit of a blog post: http://www.yesodweb.com/blog/2016/04/split-db
Really interesting. Although I'm not sure how the free monad was important here? It seems you could have left that out and been left with the same insights. The technique you described is not made any easier by using the free monad
Free monads have interesting properties, and this lets you use them pretty transparently where appropriate without having to tie your whole architecture to them.
Thank you for the work updating this stuff, although I'm not sure what this has to do with the DependentHaskell programme.
With this, you'd be constructing your record of functions in `main`, or very close. How would you push it out more?
I've found the article here: http://eprints.nottingham.ac.uk/32826/ but it's behind a paywall so I wasn't able to get access :(
&gt; For example, I’m manipulating some large graph-like data structures, and need to perform significantly expensive computations on various node and edge labels while walking the graph. In an imperative, stateful style, I would typically cache the results to avoid unnecessary repetition for the same inputs later. In a pure functional style, a direct equivalent isn’t possible. In a lazy language like Haskell, bind the expensive computation to a name. The computation won't actually be done until that name is forced, and the result of the computation will be transparently used to update the binding.
Does your original issue refer to a multi-package project while your EDIT2 refers to a single-package project? I'd like to take a look at the project in any case!
Spotted a small typo here: "Plus, we have the assurance that we’re [*not*] going to be accessing the database or printing any output in our MonadHttp functions." Edit: Some thoughts so this isn't an empty comment. A very nice post. I've been having issues messing around with monad transformers, typeclasses, IO, etc for a while now and it's been hindering my projects. I really like the interface for working with the various effects. Passing your function to an interpreter looks very clean and seems super flexible (a lot more so than when I tried to do something similar with transformer stacks).
I wonder if this can form a category of some sort? Given a natural transformation from rank n types under some constraint `a` to types under some constraint `b`, is this a morphism in a category? That is, can this type implement `Control.Category`? newtype X a b = X (forall m x. b m =&gt; (forall n. a n =&gt; n x) -&gt; m x) I've been tinkering with this for a few minutes. I'm inclined to believe it can, but I'm having trouble getting some of the types to line up... Anyway, this would be awesome because we could chain modular interpreters this way. runHttp :: X MonadHttp MonadIO runHttp = X ... runRestApi :: X MonadRestApi MonadHttp runRestApi = X ... runApplication :: X MonadRestApi MonadIO runApplication = runHttp . runRestApi -- ^ -- | -- Control.Category.(.) EDIT: To exemplify why this would be cool, we can leave `runRestApi` the same, and compose it with a *different* `runHttp` to get a new interpreter, without having to change `runRestApi`. mockHttp :: X MonadHttp (MonadState MockState) mockHttp = ... mockApplication :: X MonadRestApi (MonadState MockState) mockApplication = mockHttp . runRestApi
I use it with debian/ubuntu, and I install it from either the package manager or cabal in my global environment, and then I use stack for everything else. I tried using xmonad with stack, but I couldn't find an easy way to make the recompilation work.
Is this code in use somewhere? Can we see that?
 type Application = ReaderT Services IO foobar :: Application Int foobar = do service &lt;- ask runHttp service $ do page &lt;- get "http://wwww.google.com/" post "http://secret-data" (collectData page) pure (length page) In this example your functions are bound to `Application` which is in turn bound to a concrete `Services` representation. If you could utilize a type class for row polymorphism then a function that encompasses multiple services would only need declare which services it is concerned with. Though I might be missing something.
I think he's referring to e.g. the type level naturals that some of the libraries had implemented themselves back then, which have (clashing) versions in base now.
&gt; I've been working on a large codebase (about 12k LOC) using what I call the "Haskell is my favorite imperative language" style of programming: the spine of your program is in the IO monad and calls out to lots of small pure functions. That was my initial style as well, but unfortunately it relies on anything effectful that you want to do being close enough to the high level, monadic code to keep the rest pure. In general with the kind of effects I’m asking about here, that won’t necessarily be the case.
&gt; Maybe there's room for a family of `lessUnsafePerformIO` functions --- time will tell. As far as I know, though, anything like that currently amounts to "careful library design around an `unsafePerformIO` function", at the moment (e.g. `ST`). I confess: I was half-hoping that someone would know of libraries that implement either or both of the aspects I’m asking about in some clever way with something like `unsafePerformIO` hidden inside.
Ok, I just watched your Typeclasses vs the World talk again (I had watched it a long time ago, but I was very new to FP at the time). Fantastic talk! Regarding the commutative plumbing: in Agda, it's true you're not guaranteed that the diagram commutes, but if it does, there's no issue, and if it doesn't, Agda will complain and demand your divine intervention. Second, and you mention this in your talk, some of the most important cases only have one proof anyway. It's not like `List t` has a bunch of different monad instances to choose from. Now it's true that many do have more than one possible construction; but I will contend that even in this case, we can in practice do pretty well. Despite the fact that I can define some silly equality on String by comparing the first character or something, there is in practice only one sensible choice to have in global scope. Whether one instance is constructible or one instance is constructed, we get the same user experience -- and unlike Haskell, defining a strange instance in some tiny module somewhere doesn't pollute our whole project! In cases where there truly is more than one sensible construction, I actually like the instance approach: I can explicitly bless an instance via application, and then reason in that scope normally without corrupting the global namespace for the rest of my project. No need for Proxy hacks :) That being said, doing typeclasses in Agda does have some major difficulties: for me, the hardest one by far has been dealing with universe polymorphism. For example, say we want to have a verified Functor. fmap takes a function from `a -&gt; b`, but `a` and `b` might be in separate universes, so we need to throw in two universe params. But the functor law is composition, so we now have three type variables for that field, which means... we need three type universes. So ok, we'll just add a third universe param to our `Functor` typeclass! Except... now how do we resolve the universes for `fmap` when the user tries to use it? There's now a third universe param required to access our `Functor` module, and obviously you can't (well, more accurately Agda won't) resolve it implicitly by using a function which only takes two universe parameters. And that's not all. I said we need three universes, but that's three universes just to state the laws; we actually want to make sure the laws *hold* for any three universes, not just three hardcoded ones. Well that's easy enough, right? Just throw a forall on the universe levels for the laws? Nope! Set omega! Your `Functor` is now no longer containable in any given universe, so Agda rejects it. My solution to resolving mismatching numbers of universe parameters was to put items with differing numbers of necessary universes into different modules. This way, our operation will never ask for something that uses fewer universes than its containing record, so we can always infer them from the context. My solution to the second problem is equally as inelegant: since universal foralls are not allowed inside a record, just move them outside the record to the parameters of a different one! [Here is the full declaration for my universe-polymorphic Functor](http://pastebin.com/jQqRrx5R). Ugly as hell, and a pain in the ass to write. To me, this is the real problem with dependently typed programming, be it typeclasses or not: saying what you actually want to say in a mathematically formal way is just really hard. But that being said, from the user's perspective, that Functor does "just work" -- user level code looks pretty much exactly as it would in Haskell. `_+_ 5 &lt;$&gt; just 3`!
Its funny, coming from many years of coding too, the article resonates for me.
It's in use in a proprietary app at work. I'll try to get a complete/working example running, but it'll take me a bit to get to that
If you enable split-objects in ~/.cabal/config, the resulting binaries are considerably smaller, so it works quite well in GHC. I cannot say the same of Go. I tried to get smaller binaries via GHC's llvm backend, but that didn't make a difference, though Rust's LLVM-generated binaries are pretty slim if you ignore the bundled jemalloc. In summary, GHC's binaries are smaller than what Go generates, but there's probably still room for improvement if we consider very simple programs that just happen to use a few libraries.
I got it! import Prelude hiding (id, (.)) import qualified Prelude as P newtype Interpret c d = Interpret (forall n a. d n =&gt; (forall m. c m =&gt; m a) -&gt; n a) instance Category Interpret where id = Interpret P.id Interpret f . Interpret g = Interpret $ \h -&gt; f (g h) --- Putting it all together, we get our nice composition. {-# LANGUAGE ConstraintKinds #-} {-# LANGUAGE DeriveFunctor #-} {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE GeneralizedNewtypeDeriving #-} {-# LANGUAGE MultiParamTypeClasses #-} {-# LANGUAGE PolyKinds #-} {-# LANGUAGE RankNTypes #-} {-# LANGUAGE UndecidableInstances #-} module Lib where import Control.Category import Control.Monad.Free import Control.Monad.IO.Class import Control.Monad.Reader import Prelude hiding (id, (.)) import qualified Prelude as P newtype Interpret c d = Interpret (forall n a. d n =&gt; (forall m. c m =&gt; m a) -&gt; n a) instance Category Interpret where id = Interpret P.id Interpret f . Interpret g = Interpret $ \h -&gt; f (g h) class Monad m =&gt; MonadHttp m where httpGet :: String -&gt; m String newtype HttpApp a = HttpApp { runHttpApp :: IO a } deriving (Functor, Applicative, Monad, MonadIO) instance MonadHttp HttpApp where httpGet _ = return "[]" -- Should do actual IO runIO :: Interpret MonadHttp MonadIO runIO = Interpret $ \x -&gt; liftIO $ runHttpApp x newtype MockHttp m a = MockHttp { runMockHttp :: m a } deriving (Functor, Applicative, Monad) instance MonadReader r m =&gt; MonadReader r (MockHttp m) where ask = MockHttp ask local f (MockHttp m) = MockHttp $ local f m instance MonadReader String m =&gt; MonadHttp (MockHttp m) where httpGet _ = ask runMock :: Interpret MonadHttp (MonadReader String) runMock = Interpret runMockHttp class Monad m =&gt; MonadRestApi m where getUserIds :: m [Int] data RestApi a = GetUsers ([Int] -&gt; a) deriving Functor instance MonadRestApi (Free RestApi) where getUserIds = liftF $ GetUsers id runRestApi :: Interpret MonadRestApi MonadHttp runRestApi = Interpret $ iterA go where go (GetUsers f) = do response &lt;- httpGet "url" f $ read response runApplication :: Interpret MonadRestApi MonadIO runApplication = runIO . runRestApi mockApplication :: Interpret MonadRestApi (MonadReader String) mockApplication = runMock . runRestApi --- EDIT: The problem I was having was that `Prelude.(.)` does not behave well with RankNTypes that use constraints. Took me forever to realize that all my compiler errors came down to using that operator, when manually writing the composition worked just fine.
&gt; I'd say that you are describing a program whose behaviour *does* depend on the sequencing of operations. To some extent, but the distinctive thing about the cases I’m asking about here is that any co-ordination that is required is only relevant within the functions/modules that have the “hidden” functionality. Ideally, nothing outside those functions ever needs to know about it, and thus there is no need for those implementation details to leak through the interface. For example, nothing that calls a memoized function should even know that it’s memoized or have to make any special allowances. It could just be a beneficial implementation that is transparent to the rest of the program. Likewise, the externally observable result of calling a function that has some logging for diagnostic purposes should be identical to calling the same function with no logging, and preferably the presence or absence of logging shouldn't make any other difference to evaluation strategies or call orders either. The catch with a pure functional implementation is you have no way to allocate and manage those private resources and any effects that mutate them as you can in various other programming styles. Given that logging and performance improvement aren’t exactly unusual requirements, I was wondering whether others had found better strategies for coping with them than just passing everything around explicitly.
Not sure if it fits your bill, but [Write Yourself A Scheme in 48 Hours](http://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours) aims to cover the basics of implementing a simple programming language in idiomatic Haskell. It does so for the purpose of actually teaching Haskell, but it's a good read anyway even if you've learned the basics already.
I'm of the opinion that control structures just don't work in templates. There are 2 reasons for this: [they usually end up looking like a rat's nest](http://www.clearsilver.net/docs/template_basics.hdf) and they end up giving you just enough control to make it annoying when you need just a little bit more and it's not there. If there are things in the template that need to be hidden because the related optional data is missing, I write a splice specifically for that, and the tag it is bound to has semantic meaning. Here's a template I have that does exactly that: ``` &lt;nav&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/rosters/${slug}/"&gt;View&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/rosters/${slug}/history/"&gt;History&lt;/a&gt;&lt;/li&gt; &lt;canEdit&gt; &lt;li&gt;&lt;a href="/rosters/${slug}/edit"&gt;Edit&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/rosters/${slug}/rename"&gt;Rename&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/rosters/${slug}/delete"&gt;Delete&lt;/a&gt;&lt;/li&gt; &lt;/canEdit&gt; &lt;/ul&gt; &lt;/nav&gt; ``` If the person viewing the page is the owner of the page's content, they're allowed to see the administrative links (edit/rename/delete). If they don't own it, it's hidden. You might be thinking to yourself that it's not really that much different than what you're currently doing. You might even argue that your way is better because it is more generalized: you can just bind that one splice up front rather than on a per page basis like I'm doing. However, by hard coding information in your templates, you're setting yourself up for a maintenance problem when the rules change. I could wake up tomorrow and decide that I wanted to make it so that site administrators can modify any user's content. It's a lot simpler to change the splice than it is to change multiple templates that display administrative links. If you're a web designer like myself, this should sound very familiar to you. We use the same arguments for using an external stylesheet rather than using inline styles right in your markup! The result is the same, but the maintenance costs are dramatically higher with the inline method.
Sorry, I should have been clearer. What I was targeting was this: &gt; a back door like `Debug.Trace`, but more general and with enough guarantees that it’s safe and reliable for use in production code. Which I interpreted as "I want my logging messages to make sense from a strict, imperative standpoint." If you don't care when, how and what gets logged, then that's exactly when Debug.Trace/unsafePerformIO is good, no?
&gt; Clearly, you haven't seen to logging statements written by bad programmers. I've seen too many relatives of the heisenbug that turned out to be side-effects in logging statements. I've seen such things, but not in Haskell. I'm sure you could do it if you tried hard enough, but my feeling is you'd have to try a lot harder. &gt; You might still have problem where changing the logging level changes the order expressions are reduced to WHNF. It's not just changing the log level. By introducing the monad you're using to keep the log, you've introduced a concrete sequence of evaluation. A sequence dictated by the log, and not by the work being done. The job is never to write a log, it's to do some work and possibly log it. To have the log dictate structure seems wrong to me. 
Unless it's a top level binding, GHC wild have forgot about it the second time the containing function is called.
It's a long rant founded on very little basis. In the fizzbuzz, section, he basically says *parameterize for anticipated changes*. He takes 3+ pages to say the four words. In the line breaking example, he identifies a bug and takes pages and pages to harp on it with no lesson learned. The lego and line fitting sections are just laughable. The thesis, *anticipate probable changes*, is a noble goal. Unfortunately, it's not worth the time to read his discourse on it. 
1) I have cabal and ghc installed from the distro, and xmonad through cabal. This is the only haskell code I ever build outside of a sandbox, so it's fine, and this is easier than figuring out how to hook up xmonad's self-recompilation thing with stack. 2) Yes, everything else is done through stack. Easiest way to isolate haskell development environments between projects without creating massive amounts of redundancy. 3) Debian, currently on jessie, but going to switch to testing as soon as the age of the provided packages starts annoying me.
If you are certain that your effects are sufficiently benign you can always try to use ``unsafeIO`` and wish for the best. ``Debug.trace`` works this way, up to the point where it doesn't and produces letter soup. I think your problem isn't Haskell, but purely functional programming in general. Whenever you disentangle a function from its effects you'll get something more valuable from it. And it would be surprising if it wouldn't cost you something. I mean, impure functional programming is always there with ``IO`` and OCaml, but we can try to aim higher than that. Or maybe you are, like me, simply unhappy with the ``MonadEverything m`` pattern, where every little thing has to be shoehorned into a monad typeclass of some kind before it's diluted in abstractions, so we can write ``MonadReader Env m =&gt; m a`` instead of ``Env -&gt; m a``. Because you don't always need monads to keep your code pure. In a path tracer I wrote I had a function that calculates a uniformly distributed random point on a hemisphere. Instead of uniformHemisphere :: MonadRandom m =&gt; m (V3 Double) I realized that the randomizing part can be factored out, since the result actually depends on just two random numbers. uniformHemisphere :: Double -&gt; Double -&gt; V3 Double I don't know if this helps, but this method has been very useful to me. And not just in Haskell.
I just found it in the wiki page I linked above, but I think it's reasonably simple to use: User name String Maybe email String accessToken OAuthAccessToken refreshToken OAuthRefreshToken ~createdAt UTCTime ~updatedAt UTCTime deriving Show should make those two fields lazy. --- Haskell has something called "strictness annotations" that you can read about here: * https://wiki.haskell.org/Performance/Data_types#Strict_fields * https://stackoverflow.com/questions/993112/what-does-the-exclamation-mark-mean-in-a-haskell-declaration ~ is the reverse of the ! strictness annotation (and is also available in regular Haskell via https://ghc.haskell.org/trac/ghc/wiki/StrictPragma extensions). --- Aside: In general strict is a good default for record fields because lazy records is an unsafe (mis-)feature of the language. I.e. The following will crash at run time: data X = X { a :: Int, b :: Int } b (X { a = 1 }) *** Exception: &lt;interactive&gt;:4:4-14: Missing field in record construction b 
&gt; The thesis, anticipate probable changes, is a noble goal. There is a tradeoff between a dumb static interface and a smart generic future proof one which is ten times more complex and will never be used ;) It is too tempting to over engineer a problem when a simpler but less flexible solution exists.
In simple cases, yes, you certainly can use techniques like that and they work fine. Unfortunately, I’ve been finding with my current project that the same neat tricks often don’t scale to more challenging applications. To put this in perspective, the technique you mentioned works nicely with a single list of simple integers, where we have handy `zipWith` and `tail` concepts to set up the recursion while remembering enough previous values to avoid the usual inefficiency with naive recursive Fibonacci. However, what would be the equivalent if we had an algorithm that walked a more general directed acyclic graph, the graph’s nodes and edges were labelled with more complicated structured data involving things like maps and lists, decisions about how to navigate and/or modify the graph were based on significantly expensive computations involving those label values, and those computations were the functions we’d really like to memoize? That is actually a typical scenario from the current project that prompted me to start this discussion. The code involved in one of the more complicated algorithms might run to 500 lines, divided into a family of perhaps a dozen functions with various mutual recursion to propagate through the graph. Now, there is already a fair bit of folding and accumulation going on in some of these functions, and consequently various accumulated values being passed around explicitly in the recursive functions. I certainly could add an extra cache to be accumulated similarly, which is probably the closest analogy to reusing the earlier elements of the list in your `fibs` example. Unfortunately, I’ve been finding that the code often doesn’t work out so neatly at this scale. There would be extra parameters to almost every function in the module of course, but also changes in their implementations like turning maps into map-accumulates. It’s not an insignificant amount of code to change, all to pass around a cache that might only be used in a handful of different places and that is completely irrelevant to all that other code that now explicitly depends on it.
Couldn't find anything on it. Got a link?
Before you get the pitchforks out, this was posted tongue-in-cheek. Good motivation, touches on the themes of an earlier thread about large-scale architecture in Haskell, but crappy write-up. What the heck the Physics Of Software anyway?
Is something like this what you are looking for? It generates multiple solutions for your [3,3,3,3] problem since there is so much symmetry. import Data.List import Control.Monad import Text.Printf dec k a na | na &gt; k = [ (a,na-k) ] | otherwise = [] graphs :: [ (Char,Int) ] -&gt; [(Char,Char)] -&gt; [ [String] ] graphs [x] avoids = [] graphs [(a,na),(b,nb)] avoids | elem (a,b) avoids = [] | na == nb = let (a',b') = if a &lt; b then (a,b) else (b,a) in [ replicate na (a':b':[]) ] | otherwise = [] graphs ((a,na):rest) avoids = case break (\(x,nx) -&gt; notElem (a,x) avoids) rest of (_,[]) -&gt; [] (pre,(b,nb):post) -&gt; do let (a',b') = if a &lt; b then (a,b) else (b,a) k &lt;- [0.. min na nb] let avoids' = [(a,b),(b,a)] ++ avoids g &lt;- graphs (dec k a na ++ dec k b nb ++ pre ++ post) avoids' return $ (replicate k (a':b':[])) ++ g check :: [(Char,Int)] -&gt; [ String ] -&gt; Bool check degs pairs = all (\xy -&gt; length xy == 2) pairs &amp;&amp; all (\(a,na) -&gt; length [ x | xy &lt;- pairs, x &lt;- xy, x == a ] == na ) degs checkSolutions degs = all (check degs) (graphs degs []) degs1 = [('a',2),('b',1),('c',1)] degs2 = [('a',2), ('b',2), ('c',2)] degs3 = [('a',4), ('b',3), ('c',3)] degs4 = [('a',3), ('b',3), ('c',3), ('d',3)] degs5 = [('a',5), ('b',3), ('c',4), ('d',2)] test1 = graphs degs1 [] test2 = graphs degs2 [] test3 = graphs degs3 [] test4 = graphs degs4 [] test5 = graphs degs5 [] showdups xs = forM_ (group (map sort xs)) $ \g -&gt; do if length g &gt; 1 then putStrLn $ printf "%3d" (length g) ++ " " ++ show (head g) else return () 
[Paper source](https://github.com/koengit/KeyMonad/blob/master/paper.lhs) and [implementation](https://github.com/koengit/KeyMonad/blob/master/KeyM.hs): data TreePath = Start | Leftc TreePath | Rightc TreePath deriving (Eq) type Name = TreePath type NameSupply = TreePath newNameSupply :: NameSupply newNameSupply = Start split :: NameSupply -&gt; (NameSupply, NameSupply) split s = (Leftc s, Rightc s) supplyName :: NameSupply -&gt; Name supplyName = id instance TestEquality (Key s) where testEquality :: Key s a -&gt; Key s b -&gt; Maybe (a :~: b) testEquality (Key i) (Key j) | i == j = Just (unsafeCoerce Refl) | otherwise = Nothing data KeyM s a = KeyM { getKeyM :: NameSupply -&gt; a } data Key s a = Key Name newKey :: KeyM s (Key s a) newKey = KeyM $ \s -&gt; Key (supplyName s) instance Functor (KeyM s) where fmap = liftM instance Applicative (KeyM s) where pure = return (&lt;*&gt;) = ap instance Monad (KeyM s) where return x = KeyM $ \_ -&gt; x m &gt;&gt;= f = KeyM $ \s -&gt; let (sl,sr) = split s in getKeyM (f (getKeyM m sl)) sr runKeyM :: (forall s. KeyM s a) -&gt; a runKeyM (KeyM f) = f newNameSupply
I would totally used this! 
The species of all simple hypergraphs (that is, where there is at most one hyperedge for a given subset of vertices) can be described as Subset [] Subset+, where [] is the functorial composition operator, Subset = E*E is the species of subsets, and Subset+ = E+ * E is the species of nonempty subsets. However, you seem to be working with multihypergraphs, where any number of copies of each hyperedge are allowed. Unfortunately this is not a species, since there are infinitely many possible structures of a given size (assuming size is measured by number of vertices). There are other ways you could measure the size of a hypergraph, and you could probably write down species descriptions for them, but in any case this whole line of reasoning is not really going to help you, since you want to enumerate a very restricted subset of the possible hypergraphs. Enumerating all hypergraphs and then only keeping the ones that satisfy the constraints is likely to be astronomically inefficient. The problem is that you want to enumerate something subject to certain constraints. Constrained enumeration is in general a hard problem and the theory of species does not really help much. Sometimes in combinatorics one can show that structure-X-with-constraints-Y is in bijection with some other structure Z, and then X-with-constraints can be enumerated by enumerating Z. But this is a special situation and requires some combinatorial insight. Doing some kind of backtracking search, like /u/mn-haskell-guy below, is probably your best bet.
No, everything was done in a single-package project. 
Yes, you have to ensure the binding last as long as you need it to. If you have a non-functor data type, turning it into a functor by adding a polymorphic field is an okay way to attach these caches or other decorations.
&gt; The free monad over the identity functor is effectively a nesting of Identity an arbitrary number of times, eventually resulting in an a. You are right I totally misspoke. I actually meant the free `Monad`, a data structure that knows it is a `Monad`, but isn't simplified according to the monad laws, since the monad laws aren't in the definition of a `Monad`. Then, the final structure not only reflects the `return` calls, but also the `&gt;&gt;=` calls (and calls to other methods of `Monad`, `Applicative`, and `Functor`). (I think maybe this is also referred to as the Operational monad, maybe?) You can simplify (e.g. unify `pure` and `return`) but you don't have to. Depending on what you end up with it may or may not be law abiding (or observationally law-abiding).
&gt; If fn :: Monad m =&gt; _ -&gt; m _ knows nothing about m, it cannot create an instance of the M constructor. Yes, it can. By calling your `(&gt;&gt;=)` implementation that does so. The `Monad m =&gt; a -&gt; m b` can't extract any information from the `m`, but how it uses the `m` can embed information in the result.
&gt; The monad law does not apply to all `Monad` instances.
Not by necessity. But just because something implements `Monad` doesn't mean its a monad. You should never give something a `Monad` instance without actually being a monad. For one thing, the `M` I described above does in fact obey the monad laws. The `Operational` monad (IIRC) obeys the laws because it doesn't export any constructors or means to break the laws; so while it is feasible if you have unrestricted access to internals, it is not possible using *just* the `Monad` instance, or any of the powers available to your program. Point being that `Monad`s that break the laws are not relevant. They should simply never be in the discussion. Even those that break the laws tend to ensure that you will never break them, and that the `Monad` instance alone never will.
No it can't. If `fn` is given no prior instances of `m`, it can only create them with `return`, thus `Pure`. Since `Pure a &gt;&gt;= k` is defined as `k a`, and since `k` falls prey to the same limitations as `fn`, it is *guaranteed* that `(&gt;&gt;=)` will return `Pure` from a lifted pure function. If `fn` knew about the concrete `M`, it could start building `M` trees and getting "useful" information. But it just doesn't have this power.
Honestly I'd really like it if Haskell had good dtrace integration options, both for the runtime and for user code.
If a monad instance is broken, then so will be any function that uses that monad instance. The results of broken functions are not representative of the intended results, and thus are insignificant.
&gt; But doing this makes both fn and the monad completely useless! I disagree. I still get nice syntax. Also, since the function I'm analyzing is `Monad m =&gt; a -&gt; m b`, I can always swap in a law-abiding instance when I'm done with my analysis.
[oden](https://oden-lang.org/blog/compiler/2016/01/18/the-haskell-rewrite.html) share their rewrite experience in Haskell
&gt; I don't believe your suggestion of separating a pure alpha-beta component out from an impure iterative deepening component is the best solution. This is because you really don't know how long the next iteration will take. Why not use [`waitEither`](https://hackage.haskell.org/package/async-2.1.0/docs/Control-Concurrent-Async.html#v:waitEither) on one thread containing your calculation, and another that's just [a timer](https://hackage.haskell.org/package/base-4.9.0.0/docs/Control-Concurrent.html#v:threadDelay)? As each iteration completes, spawn another while keeping the same timer running. When the timer expires, take the last completed iteration.
If neither your project nor any dependencies make heavy use of `Integer`, use `integer-simple` instead of the (default) `integer-gmp`. The advantage is licensing: GMP is under the LGPL, while `integer-simple` is under the same BSD license that GHC is under. That means that: - you don't need to distribute the GMP source code and the LGPL with your project. - you don't need to link with GMP dynamically, which can be a pain. Windows: the C runtime is a pain. Rust's solution (don't use the CRT at all, except for a few trivial and stable functions) is ideal, but GHC hasn't gone that route. If you don't need to link to anything that needs a later version, your best bet is dynamically linking to `msvcrt.dll` (which every Windows system has) because it is standalone.
You can also do this on any other Linux system by statically linking to musl libc.
I'm not sure if you've seen https://hackage.haskell.org/package/haskell-awk which seems similar.
Please tell me the name is provisional.
Functor has the benefit of being ["property-like structure"](https://ncatlab.org/nlab/show/stuff,+structure,+property) in that if you can define it correctly it is unique. Sadly, most typeclasses aren't property-like. &gt; defining a strange instance in some tiny module somewhere doesn't pollute our whole project! In an idealized world we'd live without orphan instances. I don't really consider them part of the "real" story of typeclasses, but rather just an implementation detail of the existing implementation. There is more here about missing functionality in the language that affects packaging of code, but that is another talk for another day. =)
Yes "hs" for Haskell Shell would be better. &lt;/s&gt; Seriously, in this kind of application there's a real trade-off between brevity and distinctiveness...
Some libraries have associated [package-data](https://www.haskell.org/cabal/users-guide/developing-packages.html#accessing-data-files-from-package-code). How do you handle that?
It's already taken anyway: http://hackage.haskell.org/package/H
&gt; I'm not sure how the free monad was important here? It seems you could have left that out and been left with the same insights. The technique you described is not made any easier by using the free monad =&gt; &gt; Free monads have interesting properties, and this lets you use them pretty transparently where appropriate without having to tie your whole architecture to them. I don't think you answered his question so much as said that you like free monads. 
I've actually found it pretty satisfying when I'm forced to think about where exactly this effect is coming from and where it's ultimately handled. It would probably be nice if there was an automated way of adding additional constraints though.
See http://hackage.haskell.org/package/accelerate-cuda. Pinging /u/Ramin_HAL9001
&gt; There are only 26 single-letter package names Package names are case-sensitive. So I'd think there are at least 52 single-letter package names. Also, there doesn't seem to be a rule that says that package names can only use ASCII letters. That means, we really have enough single-letter package names for everyone! :)
What do you have against gardening ?
It's a good idea, but... $ ls -l | time h "T.intersperse ' '" h "T.intersperse ' '" 0.37s user 0.10s system 99% cpu 0.471 total It's not the same function, but just to illustrate: $ ls -l | time awk -F" " '{print $1}' awk -F" " '{print $1}' 0.00s user 0.00s system 43% cpu 0.003 total Unfortunately `h` adds a huge amount of latency to shell command invocation. So it cannot be a drop-in replacement. Perhaps it can use a daemon? Is there a way to cache the load times somehow?
Ok, so I deliberately dodged your Compose example because it was late and I was too tired to think about it. But now I'm refreshed and good to go! [Check this out](http://pastebin.com/XQRnXGgr)! EDIT: [screenshot of the above with nice colors](http://i.imgur.com/FAJiOkK.png)
Avoid it, then.
How would you combine two effects in the same function? For example in addition to your `MonadHttp` you would have: class (Monad m) =&gt; MonadPersist m where persist :: FileName -&gt; ByteString -&gt; m () And now you want to write a function: getAndPersist :: (MonadHttp m, MonadPersist m) =&gt; m () getAndPersist = do page &lt;- get "example.com" persist "example.txt" page How would you use this function in your application?
We're helpful with that too, for people who are willing to listen to us. But sometimes "no instant gratification, you need to use your brain for this, but we'll be happy to help" is a difficult message to accept.
Ooh, that sounds like a nice approach! I guess I missed that possibility because my thinking on these topics is probably predisposed to be more imperative and single-threaded due to having developed my ideas long ago when computers where much less parallel and I much less experienced. Now you've got me curious about whether a game engine written in Haskell using this approach could be competitive with strong state-of-the-art engines written in C! Also, even though you may have hit upon something that makes my example of game programs not a good illustration of this kind of program that really should be formulated monadically even though there seem to be elegant but simplistic pure solutions...it seems to me that maybe my comments above still might apply to the OP's domain? What do you think?
This is cool. I often do some pretty heavy command-line shell hacking using grep/sed/tr. I already stopped using awk/perl long ago. When it gets complicated enough to require that (and I'm pretty good at grep/sed/tr so it has to be *quite* complicated), I open ghci (it used to be the python shell), do stuff there, save it in files, then go back to the shell. This may help me avoid the slightly awkward ghci phase in some cases.
[hoe?](https://www.youtube.com/watch?v=2p-l8v9JV_k)
Where is this on Hackage? The name "h" makes it hard to find.
Yeah, nor would stack. My comment above was a half-joke really. The cabal user guide currently says: &gt; A package is identified by a globally-unique package name, which consists of one or more alphanumeric words separated by hyphens. To avoid ambiguity, each of these words should contain at least one letter. Personally, I think it _should_ restrict package names to ASCII.
Singletons :) and type level naturals.
The package author is a native speaker of Japanese. I doubt he was aware of that somewhat obscure cultural context.
I obviously disagree with this very misguided post. But I upvoted, because it is interesting to know that it exists and that it was linked in /r/programming. Please remember, up/down votes are about whether you think other people in the channel would be interested in seeing this, not about whether you agree.
WYAS is good, but you only build an interpreter.
Those are basically the same in any language; I doubt there'd be a benefit in using Haskell over grep or sed in that case. 
&gt; ..., but I assume anything awk, sed, cut, and tail can do, Haskell can do better! You might be in for a surprise. 
I agree that it should restrict package names; just wanted to check whether it was actually the case. I should have known to check the user guide.
[removed]
That was said with half an /s in mind
q.v. https://www.youtube.com/watch?v=4K76HbbxsBs
I had not! Thanks!
Hm, how do you mean? Can you give an example?
[removed]
I haven't encountered these, so I don't know.
Type annotations are supported, and yes - I suppose I could make some sane defaulting rules rather than using Text only for a "not [Char]" string type, since both list manipulation and string manipulation are common command line tasks.
&gt; hawk: &gt; &gt; * Function or constant evaluation modes selected by a command-line parameter, and then your expression is type-checked to match. We plan to use the type of the expression to determine the action, like the other two. It's been a TODO since 2013, should come out any day now :) &gt; * Any `Show` instance is allowed as the output type (I think String, Text, and ByteString are special cases to avoid always outputting quotes) Hmm, now that you mention it, we don't do it for Text. I'll add it to the list. &gt; somewhat confusingly called prelude.hs We call it the user prelude because we thought the analogy with the Prelude would make it immediately clear what the file does, but if it makes things more confusing instead of simpler, we'll gladly rename it. How would you call it?
&gt; Huh, I wonder how this is accomplished. In poking around the GHC API I could only figure out how to compile monomorphic functions. Oh, I just thought of how - simply take their string expression `s` and try compiling `"(show ." ++ s ++ ") :: String -&gt; String"` :)
see `hawk` https://github.com/gelisam/hawk/blob/master/doc/README.md https://hackage.haskell.org/package/haskell-awk
A hidden module is a module listed in `other-modules` in the cabal file. GHC will not let me import its exported symbols.
That's good! It doesn't solve the hidden modules issue, but it's great that it solves the hidden packages issue.
`Hawk.hs`
For one liners, it's also possible to use `ghc -e`: &gt; seq 10 | ghc -e 'interact (unlines . take 3 . lines)' 1 2 3 The advantage of tools like `h` and `hawk` is that we automate some of the parsing and IO interaction: &gt; seq 10 | hawk -a 'take 3' 1 2 3 Hawk also supports tabular formats, and storing helper functions in a per-project file.
Thank you so much for this. I have been looking for something like it for awhile now.
I've posted an example Stack project that uses Alex and Happy, see [happy-alex-example](https://github.com/da-x/happy-alex-example).
Which can be used with stack easily as well: `stack eval`
Maybe a simple example would be writing a command-line sum function for numeric input.
&gt; since this problem doesn't require any random access, there's little reason to use arrays instead of lists. Even if you don't need random access, an unboxed array might end up faster just due to cache locality and fewer indirections. 
Being "intiutive" is very subjective. It heavily depends on everyone's background. You do not need to understand monads, GADTs or type classes to code efficiently in Haskell. Those are advanced concepts. And yes the first line of my comment was a joke, as the smileys hinted. What makes Haskell non intuitive for most programmers used to strict semantics and mutations is non-strictness and purity. But it makes also Haskell easy to learn! Haskell, by design, has less corner cases than most programming languages. Questions like: what kind of values this function accepts and returns? Does it raise exceptions? On which input? Is this code thread safe? How does it change the state of my program? These questions are part of what is called being experienced in a language. All of them are generally simpler to answer in Haskell because often the code is pure, thread safe and does not raise exception. Most of the time in Haskell, the type of a function and a few words about its purpose gives all the necessary documentation. That's not true for lots of languages!
I use a system-installed GHC exclusively for xmonad, then tell Stack to ignore the system-ghc for my projects. https://github.com/bitemyapp/dotfiles/blob/master/.install.sh
is Interpret (forall a m n. (c m, d n) =&gt; m a -&gt; n a)) enough? or is Interpret (forall a. (forall m. c m =&gt; m a) -&gt; (forall n. d n =&gt; n a)) needed? 
Thank you! If you have any questions or run into trouble don't hesitate to ask me anything. 
Hmm OK I'll have to provide a customised version as it's proprietary.
Don't use apachebench, it's garbage. https://github.com/giltene/wrk2 is more sound. Also I think you might be asking a bit of a 512mb EC2 instance. EC2 has among the slowest VMs you can get and the 512mb one is pretty weak. 50 concurrent requests (I don't know if ab has proper metering or not - doubt it) is not small for that VM. Reiterating what /u/garethrowlands said. We need code. Put together an MWE that replicates the perf problem.
[removed]
Thank you for this, it's now a stack project that builds
No I was not, this looks very interesting. I'll have to look at this some more.
It allows some cool things like safely embedding [arrow notation](https://github.com/koengit/KeyMonad/blob/master/Arrow.hs) in pure Haskell: addA :: Arrow a =&gt; a b Int -&gt; a b Int -&gt; a b Int addA f g = proc $ \z -&gt; do x &lt;- f -&lt; z y &lt;- g -&lt; z return $ (+) &lt;$&gt; x &lt;*&gt; y instead of addA :: Arrow a =&gt; a b Int -&gt; a b Int -&gt; a b Int addA f g = proc z -&gt; do x &lt;- f -&lt; z y &lt;- g -&lt; z returnA -&lt; x + y
cool!
You mean like [Scratch](https://scratch.mit.edu/)? The problem is indeed to represent monad, types, type constructor ... 
Thanks, will have a look! One question from me, what's the advantage of using Alex/happy instead of parsec? Is it just that it's much easier? In what situation would I use Alex/happy vs parsec?
He's answering to your strawman. If you didn't want to debate the reality of cabal hell, don't question it. To add to his points: cabal stopped supporting upgrade a while ago, yet cabal hell still lurks if you use cabal.
But ext* had to write data to spinning disks, so it's unsurprising you can't measure a slowdown from endianness swapping. (I'm not sure how often you serialize without some slow IO, but I imagine there are some use cases, possibly including SSD and high-performance networking).
I think this "snap the blocks" notation is not that interesting. It amounts to something like pure implicational linear logic, where substitution, or 'plugging' is the only thing that matters. For anything more interesting, you need wires - and even then, representing a reasonably complete programming language is quite non-trivial. The best representation for (first-class) functions IMHO is one that simply represents inputs as negative patterns or 'receptacles' and outputs as positive 'plugs'. So a -&gt; b -&gt; c is simply: +---a---+ | | +-^b--c-+ But again, this seems to require some wire-like element to be useful. Pattern matching gets even more complicated, since it is more like control flow, and the graphical language for that is different again.
Thanks for your replies, I actually changed the `nextModel` function where I was allocating that array to: Vector sn nResultElements $ nxtModelArr // [(1, (modelArr ! 1) - learningRate * difference)] where nxtModelArr = listArray (1,nResultElements) $ zipWith (-) (elems modelArr) $ (zipWith (+) `on` elems) (amap (* difference) featureArr) (amap (* lambda) modelArr) and it is a little slower that doing the explicit allocation. (I'm still using Array, though) (EDIT: BC, it still allocates the array) I used the `Int` in the `Vector` to avoid the problems you mention with `SNat`. Thanks for mentioning criterion and vector. I'll look into that next.
https://www.reddit.com/r/haskell/comments/4qmduf/blocks_for_codeworld/ was interesting
If you give toRad 77 degrees and 55 minutes, do you get the right answer? 
This sounds vaguely similar to a project I am working on to visualize Haskell: [Glance](https://github.com/rgleichman/glance). It sounds like the blocks are mainly a way to visualize function application. More generally though, you can use blocks to represent any tree like structure. For example, you could also represent patterns with blocks. The Glance getting started guide ([the big image in the readme](https://github.com/rgleichman/glance/blob/master/README.md)) discusses these ideas. Here's an except (although it makes more sense in context): &gt; &gt; Lets go back to the function apply icon. If you are used to other graphical &gt; visual programming languages, you may be thinking that this does not look very &gt; graphical. Here graphical is referring to a graph topology, &gt; and no it does not look graphical. The core idea is that nested function &gt; application has a tree topology, not a graph topology. The result of each &gt; sub-expression is only used once. Glance takes advantage of this tree topology &gt; to make its drawings more compact. &gt; &gt; As soon as an expression is used more than once, the tree topology is lost, &gt; and Glance extracts the sub-expression into a seperate (non-nested) icon. &gt; &gt; There are many different ways that function application trees can be represented. &gt; The linear layout Glance currently uses is just the simplest. Large expressions &gt; (just like long lines of code) become hard to read with the linear layout. &gt; Other tree layouts can make these large expressions much more readable. &amp;nbsp; Using the shape of the port to represent its type is interesting. You could probably also vary other visual aspects other than shape (size, color, texture, shading, etc.). I think this could work for very common types (Int, List, String, etc), but I don't see how this could work for all customs types. ("I forget... What does the blue squiggly triangle represent again?") Perhaps each type could be given a unique pictogram. The IO type could be a missile launch for instance.
&gt; All of them are generally simpler to answer in Haskell because often the code is pure, thread safe and does not raise exception The fact that you can answer these questions easier as an experienced Haskell programmer is not sufficient evidence to establish that it is an objectively more intuitive mental model for beginning programmers than (say) Python. I totally agree with all of the pros you mention. I also agree that Haskell has a more *economical* mental model, being based on a relatively smaller set of primitive concepts. But it is not at all obvious to me that it is clearly more intuitive generally. Having taught a mix of programmers both styles of languages, I would say that anecdotally they are both intuitive but to different people. Some people find mutation of heaps intuitive. Some people don't find higher order functions intuitive. I am not rejecting that it is intuitive for many people: but promulgating the false narrative that Haskell will be more intuitive for everyone does no good service to the community.
I love that book. The feature I added to my Core (that I wish was in Haskell!) was partial orders on user-defined binary operators. So you could specify, for example: infixl + infixl - equal + infixl * above + infixr / equal * infixl application above * infix == below application infixl . above application with these parses: a+b*c // a+(b*c) a*b/c // parse error, ambiguous infixl * and infixr / of equal precedence a+b-c+d // ((a+b)-c)+d a == b == c // parse error, ambiguous infix == and infix == of equal precedence a == b*c // parse error, no defined precedence between * and == infix == below + // clarify precedence of == a == b*c // a == (b*c) since == below + below * a == b c // a == (b c) obj.function x == (y+z) // ((obj . function) x) == (y+z) 
Well, you could in theory, but there are cultural reasons why you can't really change Haskell's category hierarchy without a lot of effort. For example a lot of people think `Num` is bad and that `Monad`s should have `fail`. In purescript such things are much easier to change.
You could just look for exit code 127, the standard shell exit code for command not found. You did say you wanted a runtime error, right? That's runtime.
Looks to not be a monad, just a Monad. (&gt;=&gt;) isn't associative. I think has long as the Key definition isn't exposed, it can be observationally law-abiding. /u/elvishjerricco [thinks](https://www.reddit.com/r/haskell/comments/4srjcc/architecture_patterns_for_larger_haskell_programs/d5cyapv) we should avoid non-law-abinding instances "at all costs". In actuality, this could replace some of the uses of `ST` / `IO` for observable sharing, I think.
Some visual functional programming projects: * [Viskell](https://github.com/viskell/viskell) - Visual functional programming environment * [Lamdu](https://github.com/lamdu/lamdu) - Structured code editor * [CodeWorld Blocks](https://code.world/blocks) - Educational visual functional programming environment * [Luna](http://www.luna-lang.org/) - Visual and textual based functional language. Not Haskell based * [Sifflet](http://mypage.iu.edu/~gdweber/software/sifflet/home.html) - Visual functional language There are also a few papers on this: * [Higher order functional composition in visual form](http://asg.unige.ch/site/papers/Dami96a.pdf) * [Data flow, visual and functional programming](http://ptolemy.eecs.berkeley.edu/~johnr/papers/pdf/thesis.pdf) * [Hazelnut](https://tfp2016.org/papers/TFP_2016_paper_17.pdf) - Structured editor I'm sure there are more if you look around. I'm working on the CodeWorld blocks project, which is educationally focused but will cover functions, list comprehensions and so on to a certain degree. I don't think we will be doing monads though. The project uses Blockly and blocks snap together. We have different connector shapes representing different types and blocks only snap together if these types match. 
Thanks! Not actually sure if that was the fix as I also removed hardening-wrapper at the same time as just changing the linker gave me another error though..
Food for thought ---- From *The Key Monad* paper &gt; Note that the Key monad laws from Figure ... only hold for this implementation *up to observation*. If we have access to the definition of Keys, we can discriminate between, for example, `m &gt;&gt; n` and `n`. However, in the interface `Key` is an abstract type, and thus users can be blissfully ignorant of this. ---- `QuickCheck`'s [`Gen` monad](https://hackage.haskell.org/package/QuickCheck-2.8.2/docs/Test-QuickCheck.html#t:Gen) doesn't satisfy (see [**6.4 On Randomness**](https://www.eecs.northwestern.edu/~robby/courses/395-495-2009-fall/quick.pdf)) return x &gt;&gt;= f = f x but morally it does. ---- From [*There is no Fork*](http://community.haskell.org/~simonmar/papers/haxl-icfp14.pdf), **5.4 Semantics of `Fetch`**: &gt; Arguably we broke the rules: while the `Applicative` laws do hold for `Fetch`, the documentation for `Applicative` also states that if a type is also a `Monad`, then its `Applicative` instance should satisfy `pure = return` and `&lt;*&gt; = ap`. This is clearly not the case for our `Applicative` instance. But in some sense, our intentions are pure: the goal is for code written using `Applicative` to execute more efficiently, not for it to give a different answer than when written using `Monad`. The `Applicative` and `Monad` instances are equal.. up to the number of steps it takes to execute.
I'm really hopeful about visual programming languages, because when I think about an algorithm, it's usually visual, exploring a space in my head. Unfortunately I haven't yet seen a visual representation that I've found matches my intuition; most obscure rather than "reveal" the behaviour. It really needs this property, because at the moment everything I see is way more verbose than traditional syntax.
If I'm not mistaken, happy takes care of this by using precidences and left, right, and no associativity
It's very interesting, but how does this take care that the declared dependencies are sufficient? Does it delegate to `apt-get` for this?
I find this a bit suspect: where deg = fromIntegral $ round x min = x - deg What happens if you replace [`round`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Prelude.html#v:round) with [`floor`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Prelude.html#v:floor)? 
I'm guessing cos/acos compiles to a single x86 instruction, so there's not much of an implementation to look for. E: Actually, there is no acos instruction, so the implementation of this one likely differs between various compilers/libraries.
I am just a little bit disappointed that Haskell doesn't provide single interface for all digit-indexable data types. I hoped that I will finally find a language, where you can switch between list, array, tree, etc. and their strictness - laziness just by replacing the factory method, without breaking your application. Once you learn the API of list, you know API of everything else.
Hi! Awesome work!!! I am implementing that compiler too, it is an awesome book!! I have taken a different approach, for example the parser (I hate parsers :P) was implemented with parsec but not fully there are still some things to do there :P. I placed the code here https://bitbucket.org/martinceresa/tiger-compiler inside a folder named FuncComp, it is not completed yet, next step is to implement Scheme B. Cheers!! 
Have you looked at https://hackage.haskell.org/package/memoize? It seems like it would present a fairly lightweight introduction of memoization via laziness. Might be worth a try.
I think there's an error in the specification. It says "10.53" is supposed to be interpreted as 10 degrees and 53 minutes, or `pi2 * (10 + 53/60.0)/180.0` radians. But using their example code will result in it being 11 degrees and -47 minutes, or `pi2 * (11 - 47/60.0)/180.0` radians. The example code uses `nint` which works like haskell's `round`, but the logic here implies that they want to use a round-towards-zero function like `truncate` (not `floor`, since coordinates can be negative). truncateDouble :: Double -&gt; Double truncateDouble x = let xint = (truncate x) :: Int in fromIntegral xint toRad :: Double -&gt; Double toRad x = pi2 * (deg + 5.0 * min / 3.0) / 180 where deg = truncateDouble x min = x - deg 
What if an application deliberately returns 127 for some other reason? $ python -c 'exit(127)' $ echo $? 127 
&gt; their interfaces changed, it becomes very ugly. You can cache the intermediate result it in the monad state. if the state is a Map Nodeid Result
I've been working on something like this for quite a while. I'm trying to get a proof of concept done.
It's also possible to use parsec with alex. AFAICS parsing infix operators with user defined precedences/associativity is impossible with happy without postprocessing (see other comment in this thread). With parsec (or other parsing combinator library) it can be done quite elegantly.
What's wrong with a little friendly competition? ;) (`hawk` looks nice by the way! Custom prelude is a great idea)
Ah, yes it can! $ seq 10 | h sum 55 It's not really a (streaming) fold in that it brings the whole list into memory at once.
&gt; And please don't try to speak for me I quoted you.
Heh, I doubt someone asking this question would know how to navigate the `record` package without a little guidance.
&gt;&gt; No constant type supported directly, but easy to work around using const Some of the others allow you to write Haskell code whose type is not a function. It just provides a String value to put on stdout, ignoring stdin. I don't think it's very important, and your flexible approach allows it anyway, so I'd say don't worry about this. &gt; Feel free to submit a PR bringing in additional useful modules, I just threw those together Since you allow arbitrary imports, your API is already the most flexible here. I think what you have is fine. &gt; Can you give an example where Text is painful and String is preferred? For one liners it is sometimes useful to have the full `Data.List` available, plus the list monad, etc. Other times, specific text-focused utilities from `Data.Text` and `Data.Text.Encoding` are what you need. So I don't mind if you prefer Text as the default, I would just like it to be a bit easier to use String. &gt; Huh, I wonder how this is accomplished. Yeah, I don't know, pretty cool. Maybe look at the source code over there. But I don't think this is a critical feature. 
&gt; We call it the user prelude because we thought the analogy with the Prelude would make it immediately clear what the file does That makes sense. The problem is that if you start supporting imports, this is going to cause trouble. By the way, really nice utility, and thanks for jumping in on this thread.
&gt; Since you allow arbitrary imports, your API is already the most flexible here. I think what you have is fine. Do you mean passing import statements to `h` itself? That won't work, I think you'll have to modify the source to bring in extra modules at the moment. &gt; I would just like it to be a bit easier to use String After playing around a bit with what I had up yesterday, I found there were just too many functions that were ambiguous, and I couldn't come up with sensible defaults. So, I now require lines to be treated as `Text`, _not_ `String`. However, `pack`, `unpack`, and `-XViewPatterns` are all in there by default, so it shouldn't be _too_ painful to do e.g. `h '\(unpack -&gt; s) -&gt; ...` The alternative here is a command line flag that says "do this to each line" or "do this to the whole input". I initially wanted to avoid command line flags altogether and have the simplest possible application, but I already found a use for at least one command line flag today (`--keep`), so I'm honestly not sure anymore! &gt; Maybe look at the source code over there. But I don't think this is a critical feature. Got it working already =)
Yes it was. Awk was *not* originally designed for tabulated data. It was originally designed for *exactly* the same kind of use cases as sed, except for when more power is needed. That is what I am pointing out. But soon after it was introduced awk quickly began to be used for many other things, including tabulated data, which is possible since it is a Turing complete language and sed is not.
&gt; Do you mean passing import statements to h itself? That won't work, I think you'll have to modify the source to bring in extra modules at the moment. Oh, I misunderstood. Too bad. OK, I'll modify my comparison comment.
Yup, that did it. Thanks! It's blatantly obvious, but I already spent a great deal of time wondering what that is all about, and it would probably have taken forever for it to occur to me to look at that again. I think I ultimately came up with the idea that the minutes were meant to always be north/east, so, like, you'd write -4.59 to symbolize 3.01. Anyway, I replaced round with truncate and got the 55209km exactly as it should be. Thanks a lot! Now how do I mark this post solved?
I refuse to feel stupid over a mistake that THEY made in the specification =.=
My user prelude does support imports: they are automatically visible in the user expression, using the same qualifier if applicable. What kind of trouble should I look out for?
Yes, I would expect it to be the case.
I had an odd linking error when playing around with HaskellR, using gld fixed the problem: https://www.reddit.com/r/haskell/comments/3qv1rl/a_link_error_in_haskellr/
I don't see why semigroups should be considered interesting in any sense. Any semigroup worth anyone's time is also a monoid. We don't care about composition for composition's sake. The real object of interest is some kind of configuration space and its dynamics. We want to know, if the system is in state X and I do T, what state Y do I end up with? I guess I would like to see a good example of semigroups which don't have a natural monoid structure. There are clearly trivial ones, like x o y = y for all x, y in some set S. I believe if you have a two-sided identity, then it is unique. Similarly, if you have both a left and right identity, then they are equal and so two-sided, thus unique. And I believe a free construction will quotient together any one-sided identities a semigroup already has and generate a monoid with essentially the same elements. But if anyone has any thoughts on the matter, I'd like to hear them.
The GHC in Fedora is there mainly to support the build and run-time of other Fedora packages that are written in Haskell (such as xmonad). It's the same for RPM packages that parallel packages from Hackage. So, it's not there for the benefit of the Haskell community, thus not much interest in the latest bleeding edge compiler. It makes sense, because the target audience of Fedora are users that are not necessarily developers in any language. It should not cater much to the Haskell developer community, who already have their own tools (e.g. Stack) to deal with latest packaging and compiler. However, I have a [low priority attempt](https://copr.fedorainfracloud.org/coprs/alonid/stackage-dist-lts-6.1/builds/) at creating a 1:1 between Stackage and RPMs for pre-compilation and distribution. It leans on [an idea I proposed](https://github.com/commercialhaskell/stack/issues/2205) (stack system root), but due to its low priority I cannot guarantee its completion any time soon. (disclaimer: this is my own opinion, I don't represent Fedora)
&gt; Your comment seemed to serve the sole purpose of showing people how wrong you think I was, in a completely different thread My purpose was to provide you (yet another) a concrete example of a `Monad` that isn't a monad that serve a very practical purpose.
`Either` and `Nonempty` are the only instances of `Semigroup` in `base-4.9` that aren't instances of `Monoid`. But the difference between the two classes is important in my opinion. No reason to require a monoid if all you need is a semigroup.
&gt; I don't think I understood your point about custom monads. Actually a monad isn't exactly what you want. Since you want to be able to pull static information out of a dynamic expression, an applicative or arrow would suit you better. You want something to where `a -&gt; M b` is isomorphic to ([Dependency], a -&gt; IO b). Unfortunately, without restricting the domain of `a`, we can't get that in Haskell. I think this is particularly suited to an arrow. `arr` would provide an empty list of dependencies, `first` and `second` would pass through the dependencies, and `(***)` and `(&amp;&amp;&amp;)` would combine the dependencies. `(&lt;+&gt;)` would combine dependencies, `left` and `right`would pass through, etc. You'd provide a "primitive" that produced an value documenting a single dependency, and ran it. If you can figure out a value for `app`, I guess we do have a monad. Alternatively, you could just decorate the `M a` values so that they were isomorphic to `([Dependency], IO a)`. The problem there is that bind can't "look forward" through the function argument without doing some execution, albeit execution you've already checked the dependencies for. So, each bind sort of stops the dependencies from "bubbling up". You could still check dependencies before executing the result of the bind, but that may not be as early as you'd like. (The more applicative style you use, the more you can bubble up the information; it's a definite use case for applicative do.) Basically, you've roped in the compiler and type class machinery for something that's not that difficult to do at the value level.
Nonempty lists are semigroups with no monoid instance. If you extend the idea to functors, you can have the `Applicative f =&gt; Alternative f` class with `empty :: f a` and `(&lt;|&gt;) :: f a -&gt; f a -&gt; f a` (aka monoidal functor) but really, the `&lt;|&gt;` is perfectly useful on it's own. If you have an `Either` instance for `Alternative`, then you either require a monoid on `e` or the current `Exception` constraint. If you restricted it to a semigroup (aka just `&lt;|&gt;`) then you can have a useful instance for `Either`.
&gt; For example, in our preceding match function, somewhere inside there’s probably a conditional that looks like this: &gt; let (pattern', target') = case cse of &gt; CaseInsensitive -&gt; (toUpperCase pattern, toUpperCase target) &gt; CaseSensitive -&gt; (pattern, target) Um, no. Case-folding both parts is definitely the wrong way to go about this, at least in Unicode. Especially since you're probably going to want to change that `Boolean` return value into a list of captures. But, yes, [Boolean bindness](https://existentialtype.wordpress.com/2011/03/15/boolean-blindness/) is a horrible thing, and pattern matching is just slightly better. (Not only can you get a little bit of type safety, but you make it slightly easier to add a 3rd case and refactor in the future.) First class functions (or in lesser languages objects with run-time dispatch methods) are often a much more modular approach.
If you have refined types like Liquid Haskell then you can make `NonEmpty` into a special case of a refined list type which does have an associative operation with an identity. Just define: {-@ type MinList n a = { xs : [a] | n &lt;= len xs } @-} ... and then the type of `(++)` becomes: {-@ (++) :: MinList m a -&gt; MinList n a -&gt; MinList (m + n) a @-} ... and it's identity is the empty list: {-@ [] :: MinList 0 a @-} `Nonempty` is just the special case where `n = 1`: {-@ type NonEmpty = MinList 1 a @-} This is why I feel that searching for an identity element usually improves the design. If you just used the non-refined `NonEmpty`type then the type is actually less precise than it could be. For example, if you concatenate two `NonEmpty` lists without using refinement types, you know that the result has at least two elements, but you lose that information because the result is still just `NonEmpty`.
And if you don't have liquid haskell?
As far as the article goes, it simply aims to build intuition for the associativity rule, which doesn't require identity. Similarly my post on [Identity elements](http://philipnilsson.github.io/Badness10k/posts/2016-06-29-functional-patterns-identity-element.html) tries to build intuition for those. From a didactic perspective I believe it's easier to understand these concepts separately. Now for the standard library, I personally support including Semigroup as a standard typeclass, mostly for the reason that the cost is really low, and it has some small benefits.
That's just a bool by another name and not what the article is suggesting which is replacing the bool with a lambda.
Every time the word "natural" appears in such discussions it is a sign of preference, unless there's some sort of study documenting its "natural" advantages..
I should have read the article more thoroughly, I thought it was merely suggesting replacing Bool with new-Bools. If it’s suggesting lambdas instead of Bools then that’s probably not such a good idea. Sorry about that.
Semigroup is a very important abstraction that comes up quite commonly. My favorite example: a bounding box on a canvas. Bounding boxes can be combined as a semigroup, but there is no natural "empty bounding box" because an empty bounding box has no coordinates. A semigroup that is not a monoid in any natural way can only be made into a monoid by artificially augmenting the type with an "empty" value that will never occur and makes no sense. If you do that, your code becomes littered with unneeded function equations, pattern matches, and guards, and code like `'error "This code cannot be reached"'`.
The use of "natural" in narrow contexts like category theory (natural transformation) or proof theory (natural deductions) have precise meanings. In Haskell, however, I'm not sure what a "natural case" is. 
&gt; I believe that a type and all the functions that return that type should be connected in the docs the same way a class is connected to the instances. Isn't it already the case? If it's an algebraic data type, Haddock displays its constructors, and if it has smart constructors instead, every library I've encountered puts the smart constructors immediately after the type definition so you just have to scroll down a bit too see how to construct it. And even if they don't, you should be able to find them easily via hoogle. *Side note*: Speaking of this kind of hoogle search, isn't it weird that searching for `a -&gt; IO FilePath` gives me all the functions which return an `IO FilePath`, with concrete types for `a`? I mean, it's convenient, but since `[Double] -&gt; Double` correctly gives me `head :: [a] -&gt; a`, I would have expected hoogle to find types which are the same or more general than my requested type, that is, to find values I can actually use in a hole with the type I requested. But of course, if hoogle did that, there would be no way for me to find the actions which produce a FilePath. It seems to me like the more principled way to search for types would be to use two kinds of variables: `a` when I'm looking for more general types, and `_a` (the syntax for partial type signatures!) when I'm looking for more specific types.
 createDirectoryIfMissing :: Bool -&gt; FilePath -&gt; IO () where the `Bool` if `True` means create the parent directories too, if needed. How would this `Bool` be replaced with a lambda?
&gt; overgenerality and confusion makes simple things really hard, and hard things impossible Reworking your above statement to this: &gt; formal generality make simple things easy and hard things possible to do correctly to me is obviously true. However, the operative word here is "formal." We all have different coding styles and sensibilities. Your choice to return to C is valid as anyone's choice not to. Although, I didn't find this blog particularly interesting or insightful, I do enjoy using and seeing modules that employ some mathematical technique. I understand I may be in a minority but reading papers on some FP concept and oftentimes being able to play with a Haskell implementation of it is pretty cool to me for a programming language and community.
[removed]
This was indeed the problem. I changed the 'cClientID' in the mqtt-hs `defaultConfig`, and now I can have two connections at the same time. Thanks!
Not "natural case", natural "case" - "our natural case" says that our singular language feature called case is "natural".
This is tangential, but what would the consequences be of using (0,0) as the coordinates for an "empty" bounding box?
False is identity, true is a function that calls the createIfMissing function on the direct parent, including the recursive call to itself. (I'm not endorsing this approach, just pointing out that it's possible) 
Then I guess you have a type for lists that have at least 0 elements, a type for lists that have at least 1 element, and not a type for lists that have at least 2 elements because we just decided to stop there.
A lambda that is run before the main action of the function, presumably. It could check for the parent directories and create them if needed. You could supply such a lambda with your library for this common case. Of course there should also be a lambda run afterwards, to check file permissions or whatever. Don't mix them up...
&gt; In reality, I end up with a development process where everything is cloned. This is not composable, because the cloned package would have to be modified to use another cloned component etc. I cannot help but point out that when using the Nix system, it is actually easy to clone packages this way. Well, maybe not easy for someone who hasn't used Nix before, but certainly doable. The Nixpkgs manual's section on [How to build [Haskell] projects that depend on each other](https://nixos.org/nixpkgs/manual/#how-to-build-projects-that-depend-on-each-other) gives the general idea how to do this. You make a local copy of whatever package that you want to modify, and then you take the default `haskellPackages` expression of all haskell packages and create a new expression that overrides the package you want with your local copy. This will place your modified package into the middle of the modified `haskellPackages` dependency tree meaning when you use `nix` to build using your modified `haskellPackages` expression, everything you build will depend on your local modifications (and when you use `nix` to build using the default `haskellPackages` expression, you will still get the default packages for everything). This process isn't even Haskell specific, though the Haskell packages on Nix currently have the state-of-the-art ability to do this sort of meddling. 
This is probably the most convincing example I've seen. This situation reminds me of the peculiarity in basic set theory when you want to take intersections and unions of sets. There is an issue when you start taking intersections of *families* of sets, and you consider the intersection of the empty famliy. When considering subsets of a fixed universal set, it just gives you the universal set. But when not constrained, the definition says it should give the set of all sets... which undefined. Your bounding boxes example is exactly analogous, since bounding boxes form a lattice with no bottom element. Thinking about your example, though I think you can model the situation just as well with a monoid action on bounding boxes. Lists of bounding boxes become your monoid and the action is simply the semigroup operation your program speaks of. The issue (as I'm framing it) seems to be that people conflate the state space and the dynamics. Obviously the "empty" bounding box isn't a valid state in your scenario. But monoids are really to be *transformations of states*, rather than states themselves. So any time you see a semigroup, what you are really looking at (in this perspective) is a monoid action on some set. In your bounding box example, the monoid is actually lists of bounding boxes. Any list of bounding boxes can then be lifted to an endomorphism of bounding boxes. There is no need for errors or anything of the sort. Similarly, with nonempty lists, we have a natural action of the monoid of lists (empty or not) on the nonempty lists. In fact, the semigroup operation is in a sense weaker, since *both* lists are required to be nonempty by the type system, when in fact one nonempty list of the two will suffice.
Have you looked at Idris? * http://www.idris-lang.org/ * https://www.youtube.com/watch?v=fVBck2Zngjo * https://www.youtube.com/watch?v=4i7KrG1Afbk
Ah gotcha. Makes sense.
Working in the monoid of lists of bounding boxes would eventually force you into the semigroup of non-empty lists of bounding boxes - which effectively brings you right back to where you started.
I mean, at some level, just practice. Read about new ideas. Learn. Try new things. Reliability should always be evaluated in terms of explicitly discussing the cost of engineering for a target level of assurance vs the cost of errors for all the project stakeholders. I often informally call this chat the "ask the business folks what the most terrible buggy thing they would be happy to have next week" conversation. If you want to poke at different tools / approaches to software verification, look at stuff like Isabelle/hol and coq. Both have great learning materials and have been used to do verified code for all sorts of cool things like sel4 (a verified os) and compcert (a verified c compiler for 32bit pointer architectures ) As for light weight formal methods: •Use parametric polymorphism / universal quantifiers like it's your job whenever it lets you simplify code, parametricity is a very powerful tool! • use quickcheck and small check like it's your job • unless / until you have a good reason, always prefer defining newtype / data declarations over Type synonyms. It's cheap and easy to have lots of data types and stuff. Use em! • the hpc tool that ghc comes with with pretty aweosme for code path coverage and recent cabal-install versions have great support • get comfortable with all the different sorts of space time profiling tooling • read the source of libraries you use to understand what they do • don't define a type class unless there's some nice rule or pattern of how the operations are guaranteed to behave so that you can treat type classes as a sort of constrained universal quantification. Also just write code and ask for design feedback. Light weight formal methods and making sure you don't mix stuff up can get you pretty far. Happy hacking!
If I'm not mistaken the consensus is that Haskell is currently not really up to stating and proving high-level properties about your programs. You kind of hit a sore spot there, because the constraints you mention are exactly of the sort that would make true specification driven development possible, a dream of many minds. I can think of three approaches that can give more correctness (that is, let you state and enforce specification) today. * Pushing the type system to its limits. You mentioned type safety. Types are the primary tool to give Haskell programs specification and structure. Their power can be gradually expanded. Beyond the standard extensions (Existentials, GADTs, RankN, ..), TypeFamilies (essentially functions at the type level), DataKinds (promote values to types and types to kinds), the singletons library (promotes functions to type families for you) and TypeInType (collapses types and kinds) give notably more power and convenience. Still I don't believe anyone thinks this is at the level of usability necessary for wider adoption. On the expressiveness side, I believe the missing pieces to the full fledged equivalent of a higher-order logic are families of types (not the same as type families), that is functions from *some type* to *the types* (`(T : Type) -&gt; Type`, if you will). Those would be necessary for dependent quantifiers. But this is shaky ground for me and if this isn't correct I would appreciate if someone sets me straight. * Refinement types. As implemented by [LiquidHaskell](http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/about/). Modern SMT solvers are powerful beasts. LiquidHaskell unleashes them on your program. I didn't came around to use it yet, but from the distance this looks extremely promising. * TemplateHaskell. Essentially you can execute whatever you like at compile time (say smart constructors). This doesn't help you if you depend on data provided at run time, but often you don't. Outside of Haskell the already mentioned Idris is probably what you are looking for.
The main problem that I have with the author's assertion is that it's not uncommon for it to be the case that I can guarantee correctness of the algorithm when given one of a fixed set of values, but not when given an arbitrary function. Also, you're going to be at some point interfacing with external OS code that requires booleans or other flags. I mean, what lambda can you pass to openFile that replaces FileMode?
Correctness can be enforced through the types. the title draws a false dichotomy? 
I just read the article again. The first time around I just didn't agree for technical reasons. In this read through my disagreement is more simple; you see, the author uses an example from Purescript source: publish :: Bool -&gt; IO () publish isDryRun = if isDryRun then do _ &lt;- unsafePreparePackage dryRunOptions putStrLn "Dry run completed, no errors." else do pkg &lt;- unsafePreparePackage defaultPublishOptions putStrLn (A.encode pkg) Then he refactors to the following after making a few type aliases and stuffing a few things into an ADT that he then encodes as functions. publish :: PublishOptions -&gt; IO () publish options = unsafePreparePackage options &gt;&gt;= announcer options I am afraid that I find the original code much more clear and easy to modify. Is such a trivial piece of code worthy of all the abstraction and trickery? Some code really benefits from such techniques yet I would be willing to argue that there is plenty of code out there that is just fine using the straight forward implementation. In short, I agree with @po8 entirely. Also, there is nothing wrong with the original code. I don't think too many people are going to suffer hours wasted time debugging code like the above due to `boolean blindness` or any other intellectually over exaggerated issue. I'd like to see the experimental proof that this is actually more than lipstick on a style-nazi issue of a pig.
Yes, but fancier types can at least enforce the caller to do the validation (for example.something better than Int for positive number). On the other hand - I am often wondering what would be practical difference between being forced by powerful types to perform the validation (and provide proofs) and just passing the data and then handling the failure encoded in some form of Maybe/Either (think division - you can allow passing 0 and return (let's say) Maybe, or you can use dependent type and require passing proof that a number you've given is not 0. When you take user input into consideration it just seems like only difference is that you validate either before, or after the call).
Check out a bunch of the Galois Inc code for examples of Haskell use from areas where correctness is critical. 
Fair enough. I was originally responding to "Destroy All Ifs", though it is somewhat atypical. Seriously, read *A Little Java, A Few Patterns* if you haven't already. I've always believed that it was intended pretty tongue-in-cheek, but taken seriously it Destroys All Ifs, replacing them with subclassing. (TBH, though, it's been many years since I read it, so maybe I'm misremembering a bit?)
I ended up Making a myxmonad.cabal file and stack.yaml that generates a myxmonad executable from $HOME/.xmonad.hs. Then I have a link from $HOME/bin/xmonad to ../.local/bin/myxmonad. Then I added a key binding to restart it -- Restart xmonad , ((modm , xK_q ), spawn "cd /home/onmach/.xmonad &amp;&amp; stack install &amp;&amp; xmonad --restart") And it works great. It'd be great if it ever gets stack support of its own.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/xmonad] [Best way to install xmonad along with haskell](https://np.reddit.com/r/xmonad/comments/4tcdp3/best_way_to_install_xmonad_along_with_haskell/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
do you have examples of these "fancier types" by any chance? Is it basically about parameterising the type with the result of a validation? For example `FormInput` vs `String` in most web stacks? so in my example it would be `validateEntry :: RawEntry -&gt; ValudatedEntry` and almost all other functions work only with `ValidatedEntry`
One problem I had with hawk I keep meaning to send an issue for is there is no safe index mechanism. Things like tail just give back identity in the case of an error whereas !! crashes.
I'd love to see Idris matured. Is there any reason it doesn't target GHC Core or LLVM? I feel like that would help its performance issues quite a bit
The double entry accounting one for sure. While this is a super simplification, consider: data Accounts (balance :: Nat) where Accounts :: Credits balance -&gt; Debits balance -&gt; Accounts balance This would represent a change to account that must be balanced. `Debits a` and `Credits a` would be their own relevant GADTs. That it is possible doesn't mean it would be pleasant to work with, however. A reservation system would be harder, I think, but doable. Tieing in a billing service, probably isn't possible to enforce right now, not sure.
I think you're getting at the idea behind [this article](http://www.sandimetz.com/blog/2016/1/20/the-wrong-abstraction) - that sometimes, it's better to have a little bit of duplication than to prematurely refactor code to have the wrong abstraction. In this case it's arguable whether the abstraction is right or wrong. It could have also been written more nicely with `do` notation: publish options = do pkg &lt;- unsafePreparePackage options announcer options pkg
Not in Haskell. I'm curious about those things that you are asking for too, so far it seems it's not that easy and many examples of dependent types focus on very basics. What comes to my mind when it comes to Idris: https://gist.github.com/edwinb/0047a2aff46a0f49c881 (example validation and passing "validated" data to core function, but it won't get you that far, as "So" type is only for trivial stuff) and probably stuff from Type Driven Development book: https://www.manning.com/books/type-driven-development-with-idris
As I understand it, GHC cannot currently directly compile Core that is generated by something other than GHC (or a GHC Core-to-Core plugin). That'd be cool if they added that feature at some point though!
I don't think replacing booleans by their church encoding is what he had in mind, though.
There is now also a freenode channel called `#munihac`.
You can also have a look at Isabelle. It's a theorem prover - it enables you to prove properties of functions and it can generate haskell code for these functions. However, I think that it's quite difficult to learn.
Thank you for all the tips :) Regarding the crt stuff, I had the problem with ghc 7.8 and some native library. After a bit of research, I discovered that it was related to this [ticket](https://bugs.launchpad.net/ubuntu/+source/gcc-4.4/+bug/640734). If you compile with llvm I suppose this bug does not exist or if the bug is still present with ghc 8. I will give it a shoot when I have some spare time. By the way, the --split-obj feature that comes with ghc 8 does wonders on the size of the binary
I use Gentoo, and I just install from portage, unisolated 
Yeah but most distros don't like to package static library (Ex: archlinux), so it's a bit painful to try to build something statically on those. With alpine linux, the default option is static linking so everything go smoother.
That is a much more complicated way of looking at things. Semigroups are simpler than monoids. This is naturally a semigroup, not a monoid. You can force it to be a monoid artificially in several different ways, but that will always add unneeded complexity.
You can do better than that, but it requires an additional constructors (although you might be able to do some trickery with RankNTypes) data Nat = Z | S Nat data MinList (n :: Nat) :: * where Nil :: MinList Z Cons :: MinList n -&gt; MinList (S n) Cons' :: MinList n -&gt; MinList n 
IMO, an alternative approach would be a very similar article entitled "How to read Haskell for non-Haskell programmers". Would have saved me a *lot* of time learning.
Poking around the library, I found [Data.Monoid.Action](https://hackage.haskell.org/package/monoid-extras-0.4.2/docs/Data-Monoid-Action.html) in monoid-extras. This is essentially the tool I would prefer to use in this kind of situation. In your example, you would have: instance Action [BoundingBox] BoundingBox where [] `act` b0 = b0 (b:bs) `act` b0 = combineBoundingBoxes b b0 Other than being perhaps less familiar, I don't see that this would add any complexity.
You can't enforce this directly in the type system without dependent types. What you can do is have something like: mkTransaction :: [Payment] -&gt; Maybe Transaction This will return a Transaction object if and only if the sum of Payments is zero. Then you can rely on the invariant everywhere else. Of course you could have something similar in any language. However in Haskell values are immutable, so you can be sure that once you have created a Transaction the invariant cannot be broken by some other piece of code monkeying with a Payment after the fact.
&gt; Sometimes a Boolean is just a Boolean. Which times?
Of course it's complexity. Because all I need is a semigroup, and then just combine them directly in the intuitive way with `&lt;&gt;`. Why in the world would I want to add that `Action` and then have to wrap and unwrap lists all the time? Why not do it the simple way?
Plus, Core is also a moving target.
The phd's I've known have been awful programmers. There have been exceptions, but it was more often the case than not. Getting a PHD is displaying high level of certain traits, but programming ability isn't necessarily one of them.
They didn't "do Unix right", that was a large part of the point of "Worse is better" to begin with. Unix may be the best operating system in current use, but if so that's because it killed off superior solutions by being "good enough" quicker. The good is the enemy of the great, etc.
I hope not, there are still a lot of things that need to be fixed first (e.g. Num).
One problem problem here is your `GenValidity` instance does not guarantee that each arbitrary value will pass `isValid`. And why should it? At some point, you just have to write the correct code. Also, why would I use `GenValidity` over `Arbitrary`? Certainly the `Arbitrary` instance of `GreaterThanOne` would not be `fmap GreaterThanOne arbitrary`, so what's the point of a new typeclass?
&gt; Of course you could have something similar in any language. It is easy in many OO languages to create a system where the only legal way to have a `Transaction` is by passing through some code that validates the legality of the transaction. However, in most of those languages, it becomes increasingly difficult to provide an API that allows you to further compose those objects in a way that guarantees their invariants. There isn't a sharp line where something is possible in Haskell but impossible in those languages, there's just a fuzzy line where it gets harder and harder in the OO language until people stop trying it. For instance, it's easier in idiomatic Haskell to ensure that the one and only way to commit a finalized set of transactions is for it to be processed via the full validation suite; idiomatic OO would produce situations where it would be very easy to serialize just _this_ transaction but not _that_ one to the DB, because you've got the objects in hand in a context where you can serialize them (i.e. you're always "in IO"). You _could_ construct an OO solution that has the same property, but it's going to be grossly non-idiomatic.
Well, it would no longer be a sort of "traversal" because you lose all notion of "getting" (`Const` is not a monad). Hm, not sure what else to say - this higher-rank formulation of lens typically looks useless until someone comes along and shows you a clever `f` to pick. I can't see any. &gt; bazaar Heh. Lens humor.
True, it's not a `Fold` by the `lens` definition, but it is a `Setter`. Moreover, it does admit an alternative definition of folding using Writer rather Const: foldMapOfMTraversal :: Monoid r =&gt; MTraversal' s a -&gt; (a -&gt; r) -&gt; s -&gt; r foldMapOfMTraversal tr f s = execWriter (tr (\ a -&gt; writer (a, f a)) s) I guess my question boils down to this: what do I lose if I work with `MTraversal` rather than `Traversal`? Obviously one thing is lens-style getters/folds, but are there any nice properties that it violates?
But then one could argue that a perfect language is impossible. And some of Haskell problems are impossible to solve without a python 3 type of situation
From the [docs](https://hackage.haskell.org/package/Chart-1.1/docs/Control-Lens-Traversal.html) &gt;A `Traversal s t a b` is a generalization of traverse from `Traversable`. It allows you to traverse over a structure and change out its contents with monadic or Applicative side-effects. It says, it allows for traversal through monadic structures as well (is it because `Applicative` is a superclass of `monad`?, I am not sure). Come to think of it, I don't know how would you even traverse through a monad and more importantly what would that mean? Somebody with a better understanding, please chime in.
&gt; So, for example, if I'm writing a double-entry accounting system, and the business demands that it should be impossible to create an entry where total debits and total credits are not equal, how do I get the Haskell type-system to ensure this? You might be able to do something with [type-level naturals](https://ghc.haskell.org/trac/ghc/wiki/TypeNats) (or [this link](https://wiki.haskell.org/Type_arithmetic)), but it might get pretty complicated. Most of your functions would be defined on an Account type that's parametric over its balance represented by a type-level nat. Your function to add debits/credits would need to accept a fixed set of Accounts and a fixed set of debits/credits whose type-level nats are equal, and the modified Accounts will have their adjusted balances represented in the types and the arithmetic checked by the type checker. This will probably be pretty onerous though, but it might be worth an experiment to see if you can devise a small enough kernel with verified behaviour to make this worthwhile (along the lines of [lightweight static capabilities](http://okmij.org/ftp/papers/lightweight-static-capabilities.pdf)).
Thank you for your bug report, although I must admit I don't quite understand what you're asking for. `tail` has a very narrow API, basically a single integer, and so there is very little room for this "program" to fail with an error. And yet if you do make a mistake by giving something which is not an integer, `tail` doesn't fall back to the identity function (why would you want that?), it tells you that you made a mistake: $ tail -n foo tail: illegal offset -- foo `awk` and `hawk` allow you to specify a more complicated expression, so now there is more room for mistakes, and again both programs tell you when you make one: $ echo "1 2 3" | awk '{print $(-1)}' awk: trying to access out of range field -1 $ echo "1 2 3" | hawk -m '(!! (-1))' hawk: Prelude.!!: negative index Since you mention a case in which `tail` behaves like the identity instead of failing, I can only assume you refer to the fact that `tail -n 10` behaves like `cat` when given an input which has less than 10 lines. But so does hawk: $ seq 3 | hawk -a 'reverse . take 10 . reverse' 1 2 3 This is, of course, not really a feature of `hawk`, but a feature of `take`. Similarly, the fact that an exception is thrown when you index outside of the bounds of the input is not a bug/feature of `hawk`, but a bug/feature of `(!!)`. I know that `awk` returns the empty string when you index beyond the input: $ echo "1 2 3" | awk '{print $1, $20, $3}' 1 3 And if that's the behavior you want, it is easy to implement an alternate indexing operator: $ echo 'xs ! i = (xs ++ repeat "") !! i' &gt;&gt; ~/.hawk/prelude. hs $ echo "1 2 3" | hawk -m '\xs -&gt; (xs ! 0, xs ! 10, xs ! 2)' 1 3 But since the goal of `hawk` is to allow you to manipulate text using Haskell expressions, not to manipulate text using a DSL which emulates `awk` expressions, I don't think it would be appropriate for `hawk` to ship such an indexing function in its default user prelude. If you want such a DSL, then by any means, please implement one and put it up on hackage, so that other `hawk` users can simply import it from their user prelude if that's also what they want. But for the default behavior, I intend to stick with Haskell's semantics.
As long as the language hasn't reach a critical mass of users, I don't see why any of the biggest warts can't be fixed. We got AMP, why not the others? Is there some specific problem you're thinking of that would fundamentally change everything about the language? I'm just strictly thinking of library issues that could actually fixed today for anyone willing to forgo commitment to the Haskell standard libraries.
From [theoretical results](http://r6.ca/blog/20140210T181244Z.html), forall m. Monad m =&gt; (i -&gt; m j) -&gt; m a is isomorphic to FreeMonad (PStore i j) a Therefore your `MTraversal s t a b` is isomorphic to `s -&gt; FreeMonad (PStore a b) t` or `Kleisli (FreeMonad (PStore a b)) s t`, which is why you have a Kleisli composition. As an aside (warning, see [correction below](https://www.reddit.com/r/haskell/comments/4tfao3/monadic_traversals/d5h3wmi?context=3)), I believe it is known that forall a b s t. forall m. Monad m =&gt; (a -&gt; m b) -&gt; s -&gt; m t and forall a b s t. forall f. Applicative f =&gt; (a -&gt; f b) -&gt; s -&gt; f t are equivalent in power (note the universally quantified `a`, `b`, `s`, and `t`) and I think one can prove that the two types are isomorphic, but I don't know how to do that off hand.
I like this language because it benefits from the standard curriculum comprehension of natural number algebra. The absence of division in the presence of a zero is already understood, and the notation is compact. Connections to the abstract notion of semigroup can be made without running over general language. 
&gt; The quote says that you can traverse the data structure described by the Traversal (for example the Strings of a [String], or the two Ints of a (Int,Int)) using side-effects which are monadic (for example IO, Writer, etc), it does not say that you can traverse IO and Writer etc. Oh Ok. I suspected as much. &gt; When traversing the elements of a structure, the effects depend on the elements themselves, not on the values which have been computed via previous effects. So that's why traversals only require the effect type to have an Applicative instance. I understand this part. So now the question is, what does it mean to traverse through a monadic structure? 
&gt; what does it mean to traverse through a monadic structure? ...the same thing as when we traverse a non-monadic structure? Take list, for example: it's both traversable and monadic. The meaning of "traversing a list" is the same whether or not list has a Monad instance.
Summary, to my understanding: you're arguing for a way to designate top-level functions as only relevant for TH, which would permit GHC to only load those specific functions when compiling splices, not the entire program. E.g. something like module M where import X import Y {-# CompileTime #-} makeDataTypes names = ... $( makeDataTypes ["A", "B", "C"] ) A CompileTime-macro (or whatever), would allow one to only load those parts of a program that are thus designated, instead of every module imported by M (X and Y). Also, it would permit one to use functions that are defined in M itself, which is currently also impossible (if one wants to use top-level functions in slices, they have to be defined in X or Y). I think this is a good idea, though implementing it would probably entail significant effort, and the resources of the GHC team are finite... and I'd really prefer to get that mythical SMT solver that will let me prove a+b = b+a on the type level.
You also need a pragma for compile-time only imports. Actually I had previously made this suggestion: https://ghc.haskell.org/trac/ghc/ticket/11377 I don't think implementing this would be that difficult. But we would need to hammer out how BC works, and work out the full proposal (Racket phases are more complicated than you might think.) Maybe a good thing to take the new proposal process out for a spin on.
I think it's worthwhile to learn. I have no formal education whatsoever (Unless finishing kindergarten counts!) and I came from imperative languages like C, Python, etc. There are definitely beneficial effects to learning functional programming, even if you aren't using Haskell for your development. It influences how you think about programming in general. However, I think it's important to have realistic expectations. You can go from a language like Python to Ruby and write reasonably complex Ruby programs with a bit of documentation referencing even if you started out knowing no Ruby at all. Haskell and functional programming is a completely different paradigm and takes a lot more time and effort to become proficient. It took me several years of doing Haskell stuff on the side before I felt comfortable enough to deploy complicated Haskell programs at my job. Even now after around 8 years, I learn a lot with each new project. 
I guess it makes sense, but I still have trouble wrapping my mind around that concept. The 'elements' of monad that we wish to traverse through, depend on the type of computation that we perform right (in other words, they are effectful), so is 'traversal' meaningful in this context? And, in a related question, does the monad container preserve enough structure to traverse? I thought, we lose some information in a monad context. I might be wrong and may be getting things mixed up here. I don't know if these are dumb questions, but thank you for trying to explain.
Are there any inhabitants of your asides?! Or did you mean something else?
Oops, I went a little crazy with my quantifiers. forall a b. forall m. Monad m =&gt; (a -&gt; m b) -&gt; t a -&gt; m (t b) forall a b. forall f. Applicative f =&gt; (a -&gt; f b) -&gt; t a -&gt; f (t b) The above two are equivalent, and presumably isomorphic, for any `t`. Maybe `t` needs to be assumed to be a `Functor`, or maybe not as `fmap` is definable from either function, but the laws for `fmap` might be needed. edit: This result (which I haven't proven) is the reason for [Edward's remarks](https://www.reddit.com/r/haskell/comments/4tfao3/monadic_traversals/d5hbhry) that moving to the Monad constraint doesn't add any power. Though I haven't really explained it very well.
&gt; for example the Strings of a [String], or the two Ints of a (Int,Int) I'm still procrastinating on learning lenses in full, so it's possible that I'm just wrong here... but [the docs](https://hackage.haskell.org/package/Chart-1.1/docs/Control-Lens-Traversal.html) seem to suggest that Traversal, like other Functor subclasses, can only operate on the final member of a tuple; i.e., it knows only the `a` of `(,) b a`. **Edit:** Wrong I am. Neat!
Ah nevermind, I think I understand it now. `m a` where `m` is a `monad`, is traversable if `a` is traversable, am I going in the right direction? 
There's the free `Functional Programming in Haskell` course from the university of Glasgow starting the 19th of September, it seems to be an introductory course: https://www.futurelearn.com/courses/functional-programming-haskell
BC?
Ah, that's where the confusion comes from! No, it is `m` which is traversable, not `a`. List, for example, has both a Monad instance and a Traversable instance. This means you can both 1. use `[]`'s Monad instance to chain together computations which each produce a list of values, resulting in a computation which tries all the execution paths and returns the list of all possible values, and 1. use `[]`'s Traversable instance to apply the same effect (an IO effect for example) to every element of a list. For example, here is a computation which uses both: plusOrMinusOne :: Int -&gt; [Int] plusOrMinusOne x = [x-1, x+1] multiValuedComputation :: [Int] multiValuedComputation = do aboutTwo &lt;- plusOrMinusOne 2 aboutFive &lt;- plusOrMinusOne 5 return (aboutTwo * aboutFive) -- | -- &gt;&gt;&gt; main -- 4 -- 6 -- 12 -- 18 -- [(),(),(),()] main :: IO () main = do r &lt;- traverse print multiValuedComputation print r `plusOrMinusOne` has the side-effect of returning multiple values (sometimes called the "non-determinism" effect), I compose two `plusOrMinusOne` calls into one `multiValuedComputation`, it goes through all 4 possibilities of multiplying 1 or 3 by 4 or 6, and it returns the 4 possible results [4,6,12,18]. Then I use `traverse` to execute the IO action `print` on every element of the list, which has the side effect of printing 4, 6, 12, and 18, and the final result of the traversal is `[(),(),(),()]`, a list of unit values, because each call to `print :: ... -&gt; IO ()` produces a unit value.
The Traversable type class, like Functor and most other type classes, is indeed only modifying the second element of a pair. The Traversal optic, on the other hand, is more flexible than Traverse, as it is able to focus on both sides: type Traversal s t a b = forall f. Applicative f =&gt; (a -&gt; f b) -&gt; s -&gt; f t -- | -- &gt;&gt;&gt; traverse print (3,4) -- 4 -- (3,()) -- &gt;&gt;&gt; pair print (3,4) -- 3 -- 4 -- ((),()) pair :: Traversal (a,a) (b,b) a b pair f (x1,x2) = (,) &lt;$&gt; f x1 &lt;*&gt; f x2 
&gt; BC? "Backwards compatibility", I assume.
&gt; The books I read try to teach you about OOP as a more natural way to think, with analogies with the physical constitution of objects, with metaphors, etc. but they never cash those metaphors and analogies in something that is not an analogy or a metaphor That's because these books get it wrong, they approach OOP with an overly naive mindset. At its core, OOP is about two things: one, bundling microstates (fields / properties) with related behavior (methods), and two, encapsulation, that is, narrowing the interfaces through which these bundles (objects) communicate. The theory exists, and it's not actually mind-blowing, because frankly, there's not a lot to it; the most interesting question on the matter is at what level you make your bundles, where you put up bundle boundaries, and how you design the interfaces. There is some merit to the general idea, and in a way, the pattern keeps surfacing at the architectural level all the time - Unix processes can be thought of as objects, and pipes as the interfaces; Erlang's actor model is probably closer to the original vision of OOP than modern Java or C++; microservice architecture follows the same idea with services as objects and message-passing as the communication interfaces; etc. &gt; I stopped wrestling with that when I realized that was more productive to think about classes and objects as a way to organize code and not to model some part of reality or providing some general concepts to think about non-computational entities. That's pretty much how it works, yes. The "model some part of reality" thing is really just kind of a red herring that makes it easy to sell OOP designs to management, and it works like a charm. But as far as teaching is concerned, it's positively harmful and often leads to horribly overengineered code. &gt; As with most things with Python, the book it’s very practical (‘do x and you’ll get y’) but gives little theory or general understanding of what is going on. Most assertions are backuped by anecdotes or opinions. Correct. Python is a very pragmatic language, for better or worse; building code or the language itself upon a solid theoretical foundation is not generally considered a priority, the community seems to put a lot more value on being "intuitive", by whichever standard. Given this, it makes little sense to provide theory - the language aims to match a layman's intuition, not some theoretical framework. The downside of this is that Python's theoretical foundation is brittle; the upside is that the low end of the learning curve is extremely smooth, and the barrier of entry is only marginally higher than general literacy, elementary school calculus, and basic English language skills. Psychology is probably a more helpful background than Math or CS when it comes to understanding Python code. &gt; not only the language but more importantly the discussions around it seems a lot more substantial than the anecdotes of some random guy about how he wrestled with some library. Also correct; the Haskell community in general values correctness and soundness a lot, to the point of (occasionally) being rendered completely paralyzed or lost in accidental complexity (which, fortunately, is a fairly rare exception). One of the biggest compliments I ever received as a Haskell programmer was the word "tasteful", and I believe many Haskellers share that sentiment. &gt; I wonder if it makes sense to learn Haskell even if you haven’t a CS education, not just because to learn a new programming language but as a way to acquire a more scientific view about software development. By all means give it a spin. Compared to Python and other imperative languages, you'll have a lot to unlearn, and your brain may hurt a bit during your journey, but it seems like you're looking for the right things, and Haskell has them. Worst case, you'll learn something that challenges and expands your mindset, and then you conclude that it's not practical for your goals; but you'll still be a better programmer for it (as with any language or technology that challenges the way you think).
&gt; The books I read try to teach you about OOP as a more natural way to think, with analogies with the physical constitution of objects, with metaphors, etc. but they never cash those metaphors and analogies in something that is not an analogy or a metaphor That's because these books get it wrong, they approach OOP with an overly naive mindset. At its core, OOP is about two things: one, bundling microstates (fields / properties) with related behavior (methods), and two, encapsulation, that is, narrowing the interfaces through which these bundles (objects) communicate. The theory exists, and it's not actually mind-blowing, because frankly, there's not a lot to it; the most interesting question on the matter is at what level you make your bundles, where you put up bundle boundaries, and how you design the interfaces. There is some merit to the general idea, and in a way, the pattern keeps surfacing at the architectural level all the time - Unix processes can be thought of as objects, and pipes as the interfaces; Erlang's actor model is probably closer to the original vision of OOP than modern Java or C++; microservice architecture follows the same idea with services as objects and message-passing as the communication interfaces; etc. &gt; I stopped wrestling with that when I realized that was more productive to think about classes and objects as a way to organize code and not to model some part of reality or providing some general concepts to think about non-computational entities. That's pretty much how it works, yes. The "model some part of reality" thing is really just kind of a red herring that makes it easy to sell OOP designs to management, and it works like a charm. But as far as teaching is concerned, it's positively harmful and often leads to horribly overengineered code. &gt; As with most things with Python, the book it’s very practical (‘do x and you’ll get y’) but gives little theory or general understanding of what is going on. Most assertions are backuped by anecdotes or opinions. Correct. Python is a very pragmatic language, for better or worse; building code or the language itself upon a solid theoretical foundation is not generally considered a priority, the community seems to put a lot more value on being "intuitive", by whichever standard. Given this, it makes little sense to provide theory - the language aims to match a layman's intuition, not some theoretical framework. The downside of this is that Python's theoretical foundation is brittle; the upside is that the low end of the learning curve is extremely smooth, and the barrier of entry is only marginally higher than general literacy, elementary school calculus, and basic English language skills. Psychology is probably a more helpful background than Math or CS when it comes to understanding Python code. &gt; not only the language but more importantly the discussions around it seems a lot more substantial than the anecdotes of some random guy about how he wrestled with some library. Also correct; the Haskell community in general values correctness and soundness a lot, to the point of (occasionally) being rendered completely paralyzed or lost in accidental complexity (which, fortunately, is a fairly rare exception). One of the biggest compliments I ever received as a Haskell programmer was the word "tasteful", and I believe many Haskellers share that sentiment. &gt; I wonder if it makes sense to learn Haskell even if you haven’t a CS education, not just because to learn a new programming language but as a way to acquire a more scientific view about software development. By all means give it a spin. Compared to Python and other imperative languages, you'll have a lot to unlearn, and your brain may hurt a bit during your journey, but it seems like you're looking for the right things, and Haskell has them. Worst case, you'll learn something that challenges and expands your mindset, and then you conclude that it's not practical for your goals; but you'll still be a better programmer for it (as with any language or technology that challenges the way you think).
I'm not sure I see the upsides fully. In particular, we still need to do everything we're doing, we just have the option to not use some stuff in "phase 2" that we need in "phase 1" and vice versa. And the additional CompileTime annotations just effectively means we _still_ have two modules, it just so happens that they're "woven" together and the compiler is given instructions on how to unweave them. Forgive my point-missing, but what's precisely the win here? (edit: and here's a strawman counterproposal. we use the toposort we already need for typechecking to *automatically* separate the components of a module that don't require any TH to be run before they can be compiled, and "magically" compile that partial module and link it in to do the codegen. what's the obstacle to this?)
 For your double accounting system you can do it with plain haskell. Basically you need at least 2 accounts but only n-1 amounts. You can model it that way (or any permutation of it) data Transaction = Transaction (Account, Amount) [(Account, Amount)] Account a normal transaction A 100 | B 100 =&gt; `Transaction (A, 100) [] B` a triple one A 100 , B 50 | C1 50 =&gt; `Transaction (A, 100) [(B,50)] C 50` etc ... You can add a smart constructor which takes all the value and checks they are balanced. transaction :: [(Account, Amount)] -&gt; Either Amount Transaction. 
QuickCheck's [Gen a](http://hackage.haskell.org/package/QuickCheck-2.9.1/docs/Test-QuickCheck-Gen.html#t:Gen) and SmallCheck's [Series a](https://hackage.haskell.org/package/smallcheck-1.1.1/docs/Test-SmallCheck-Series.html#t:Series) are two examples I bump into pretty often. More generally, any Applicative that's not also an Alternative can be trivially be made a Semigroup, but not a Monoid. 
First of all, code locality is nothing to scoff at. Second of all, IIUC, the phases aren't 1 and 2, but _n_ and _n+1_: you get arbitrary staging almost for free. See [Composable and compilable macros: you want it when?](https://dl.acm.org/citation.cfm?id=581486) ([pdf](https://www.cs.utah.edu/plt/publications/macromod.pdf), [presentation](https://harryrschwartz.com/2015/02/08/composable-and-compilable-macros.html)).
IIRC, `hasql` is one of the fastest SQL libraries and works with a lot of databases, including Postgres. I like it a lot
My degree is in Biology. I started with PERL, then Python, and now Haskell and Elm. Learning Haskell was immensely rewarding and led me to a greater understanding of all my code. I highly recommend it. You might want to check out Elm as an entry point if Haskell seems too daunting. What you learn in Elm will largely be directly translatable into Haskell as the syntax and concepts are so similar. [Here's where you'd start](http://guide.elm-lang.org/) 
Total languages enable another design choice: do all pure computation at compile time
They did do Unix right. Unix isn't "good enough", it's objectively superior to all these other "superior" solutions that are actually just overly complicated ways of doing exactly the same thing.
I'm a TH noob but I've written a fair number of Lisp macros so take it for what it's worth but one of the biggest limitations of TH (and why I remain a noob) is not being able to generate modules. Every macro system needs a way of gen'ing up a namespace/sandbox of some sort and I hit this all the time and have to resort to something else. Hopefully OverloadedRecordFields will address this issue. IMO, this is as big an issue as functions being available at compile vs. runtime.
&gt; Information cannot flow from run-time to compile-time Ha! Conor restricted the information flow in the opposite direction in [Worldly Type Systems](https://m.reddit.com/r/dependent_types/comments/4hz77u/conor_mcbride_worldly_type_systems). Maybe information can flow both ways after all?
But why do I want arbitrary staging?
Yeah that looks a bit glibc ish, so I've not found any shared library in musl that has s text segment so I think that's mostly unnecessary. And split objs is nice, but all the runtimes add up in size no matter what :/.
It sounds to me like you'd be interested in learning Smalltalk, which is where OOP comes from. Ruby today carries on a lot of the legacy of Smalltalk, especially when taught by ex-Smalltalkers, such as [Sandi Metz](https://www.youtube.com/watch?v=OMPfEXIlTVE). 
I'd say Haskell is quite good for web-dev! Thinks like [Yesod](http://www.yesodweb.com) and [Servant](http://haskell-servant.readthedocs.io/en/stable/) have been soooo much more pleasant to work with than any other framework in other languages I have worked with (e.g. Flask and Django in Python, Rails in Ruby and a ton of PHP ones...). I sorely miss the compile-time feedback from Haskell in my job atm :(
I think `hasql` is only for postgres, but the author originally intended more backends. It's performance is second to none though
Okasaki had a classic paper that answered your question: http://dl.acm.org/citation.cfm?id=969615 I'm curious if you have a similar case for arbitrary staging :-) (and, erm, don't we sort of have arbitrary staging as is, with the caveat that we need to striate it across module boundaries carefully?)
I don't have any formal CS education, but that did not pose any hurdles to learning Haskell. So you should certainly go ahead. You may also enjoy reading some programming language theory. Pierce's *Types and Programming Languages* and *Advanced Topics in Types and Programming Languages* are very good and the first doesn't assume any prior knowledge, though you have to be comfortable reading formal arguments and proofs. Pierce and others also have an online book [*Software Foundations*](https://www.cis.upenn.edu/~bcpierce/sf/current/index.html) which discusses the use of formal methods and automatic proof assistants for designing reliable software. However, as far as I have been able to figure out, there is no ‘scientific’ knowledge about how to build software, by which I mean the problems of formalizing real-world requirements into code, managing the resulting logical complexity and managing large teams. To go beyond anecdotes you would have to be able to run controlled experiments but I know of no such experiments on large-scale software projects.
Now that you mention it, I completely agree! In every case I've had to fall back to raw codegen rather than TH, its been because I want to generate modules and not just bindings _within_ a given module.
Oh my, you're right, I have failed to properly distinguish the distinctions! His setup allows information to flow from the terms to the types, but of course, it's only under Milner's aligned distinctions that this would mean the same thing as from run-time to compile-time. Thanks for pointing out my mistake!
Huh... well that answers my question. Thanks!
Does all of that not just mean that GADTs are closed data families? In fact, the syntax for open type families &lt;=&gt; the syntax for closed type families ≅ the syntax for data families &lt;=&gt; the syntax for GADTs.
Close. You still can't get the same functionality of classes with GADTs as with data families. With data families, each individual instance can choose to implement (or not implement) a class in its own custom way. With GADTs, you can't quite do this.
You got me interested, do we have some benchmarks comparing Haskell SQL libraries?
&gt; people who haven't done much Haskell at all, but really want to commit to working full time in it, and it's a big selling point to them - that's a bit strange to me, why not try more before you "buy"? &gt; How much Haskell would you consider as "much at all"? I ask because I suspect that I might be someone you'd lump into this category - so I am curious. I first learned Haskell by reading Hutton's book when it came out. I read (and worked out code from) large portions of RWH. I have also read many of the initial papers (Wadler etc), some of them multiple times. I am proficient enough to be able to read basic blog articles in the language and read many of the Functional Pearls papers. Yet, I haven't gone ahead and written any project of significant size in Haskell - nothing beyond following the first half of jekor's redo implementation videos. Yet, Haskell use is a big selling point for me, as you noted. I did consider applying at Front Row Education. Will I be a good fit? 
It's very fascinating when two such seemingly disparate areas get to exchange ideas.
I know we're sort of playing into your point with all these comments saying Haskell is good for web stuff (if you ask /r/Haskell if Haskell is good for web, they're gonna say yes). But I have to say I really do believe it is. Servant has been such a fantastic way to build servers. The type level API is useful for way more than it seems.
GADTs: If you match on the constructors, you can find out the type. Data families: If you know the type, you can match on the constructors.
I think that waitForEvent is wrong. It only succeed if the desired message is the first. So it waits until some process insert it. Don't use that library.
Yes, what you're doing works. In fact, I believe `lens` detects duplicate record names and automatically does the `Multiclass` + `FunctionalDependancies` variation of what you're doing.
I think the point you've overlooked is that by making the phases explicit, one can avoid the *need* for having multiple modules. Exports would still be achievable, via annotations, but they wouldn't be necessary. That's definitely a win for me. I hate having to create a [small dummy module](https://git.snowdrift.coop/sd/snowdrift/blob/master/website/src/Handler/TH.hs) just because I want to extend a Q Exp by embedding it in another Q Exp. /u/rpglover64 says it more pithily with, "Code locality is nothing to scoff at."
For the binary size i use [upx](http://upx.sourceforge.net/) to reduce it further. With split-obj and upx, I end up with an executable &lt; 2MB instead of the original 10-15MB P.S: upx i really easy to use, just upx -9 yourBinary
Sorry for hijacking the thread, but does anyone know of solution to the following (related) problem: module A where data User = User {_name::String} $(makeLenses ''User) module B where data Animal = Animal {_name::String} $(makeLenses ''Animal) module C where import A import B -- cannot use 'name' without namespace conflicts now 
Basically my assumption was that Haskell's type-inference would be able to work out which version of the function to use based on the surrounding context. 
I'm trying to understand how is the HList library implemented. At first I didn't realize why they are using data families nowadays, instead of GADTs to define HLists. What I've found is that in general for recursive functions the GADT definition allows you to define them using pattern matching, which is intuitive. The data family version requires type class definitions, which is at first somewhat uncomfortable. BUT data family definition allows you to use a lot of preconditions as restrictions when you define the functions.
But neither do you with closed type families either.
All about finding the balance! It's much easier to read and understand code that isn't abstracted away, but if something is expected to change a lot, maybe it's worth it. Definitely err on the side of getting it to work in a _specific_ case first, though, after a couple of years in the business. Things rarely change so drastically in practice that initial solutions must be generalized.
Might it be better to use Stack instead and have IntelliJ delegate to that? See: https://github.com/carymrobbins/intellij-haskforce/wiki/Quick-Start-Guide#importing-an-existing-stack-project
Thanks for the response. But I think I should stick with cabal at this moment: Note: Run configurations are not supported for stack at the moment. This is for cabal projects only. 
This is hugely useful. Thanks!
It's not unsafe at all to duplicate r, but if you aren't careful, it would be. Yes, if you could change the API to sequence you could probably do it in _O(1)_ stack easily enough, but this was about trying to match the existing API.
:'(
My bet is that Haskell has just too many libraries for which splitting the Num or changing String to something saner would just explode everything. If we want a language people can trust to use in production we should limit break-universe changes, even if it means living with some warts.
&gt; needs a way of gen'ing up a namespace/sandbox of some sort Is the `Q` monad not filling your need here? Typically what you do is use `newName` which follows some rules regarding what they call "name capture" but it does what you need it to do if you are looking to generate unique names.
That is what I do, set up an IntelliJ project to run external tool during project build process. Nice setup.
I mean that is exactly what it does with the code I put in the original post! So it definitely is possible with TypeFamilies. It's just that arbitrary function overloading breaks type inference pretty hard, so you have to do it in some disciplined way.
I took a look at that module and I'm still confused. What does that module do that requires TH rather than a plain function?
&gt; So you wind up with something that can be used in fewer situations with fewer combinators, backed up with laws that keep you from using the extra power. I haven't worked it out, but I don't think it is the laws that keep you from using the extra power. Even after forgetting about the optical laws, if you make a traversal of type forall a b. forall m. Monad m =&gt; (a -&gt; m b) -&gt; t a -&gt; m (t b) It is the parametricity of `b` that prevents you from using bind in any meaningful way. The extra power of bind is supposed to let you use the output value of one monadic action to determine which monadic action to follow it by. But here, since `b` is parametric, then Renyolds's abstraction theorem says you aren't allowed to use the values of types `b` to determine anything. To help see this, we can rewrite the type as something isomorphic to show the free monad: t a -&gt; FreeMonad (PStore a b) (t b) This is a function that takes a `t a` and produce a tree with nodes holding values of type `a` and branches labeled with values of type `b`, and leaves holding `(t b)`. By the abstraction theorem, the shape of different branches in the free monad cannot depend on the values of type `b`. Therefore, every branch must hold identically shaped subtrees, with only the (contents of the) values held in the leaves able to depend on which branch they live in. So even though we have this tree structure, the shapes of every subtree at every level are identical. So we really have a full `b`-branching tree of some depth *n*. All the `a` values held in nodes at level *i* must be identical, since they cannot depend on the values of `b` either. So we really are holding an *n*-tuple of `a`'s. Plus we have leaves that hold values of `t b` (whose contents) can depend on the values of the `b`'s. So we have a function from an `n`-tuple of `b`'s to `t b`. And that is all the data we are allowed to have, which turns out to be exactly the information already held in a `FreeApplicative` structure.
&gt; what is wrong with the current namespace situation? A file per namespace doesn't work because of circular dependencies. Also, http://blog.haskell-exists.com/yuras/posts/namespaces-modules-qualified-imports-and-a-constant-pain.html
That seems to salvage most of the differences, but without the laws you can do things like reuse "effects" given by calling (a -&gt; m b), like Gibbons feared in the original essence of the iterator pattern paper. So, without those laws this does give you some "extra" power, as you can write traversals that use access to (&gt;&gt;=) to do "horizontal" composition of two different lenses that access different parts of a structure, but with no proof of disjointness needed because, well, no laws.
My claim is that you cannot do anything more with a lawless "monadic-traversal" than you can already do with a lawless traversal. You simply cannot use `(&gt;&gt;=)` to do anything you couldn't do already with `(&lt;*&gt;)` and friends (given `b` is parametric).
If this feature lands, using packages that use it could be annoying. For instance, if `mylib` has optional `Text` instances, they have to be in their own module. That means actually using those instances might look like this: import MyLib import MyLib.Text () That gets more annoying as the number of optional instances go up. I assume there is some way to abuse Template Haskell or the C preprocessor to avoid these problems, but I don't know how to do that myself. 
If we relax the orphan instance restriction to be per package, can we get a per-package uniqueness check?
I think one possible solution is to indeed have optional dependencies. As in, have certain functionalities of the package force dependencies on the client on a need-to-know basis. If your package has modules A, B and C, maybe module A depends on package T1, module B depends on module T2 and module C doesn't depend on any. If I am a user of this package, if I just want to use module C I shouldn't need to incur the dependencies T1 nor T2. If I just want to use B, I would indeed need dependency T2, but why should I depend on T1? Some of the problems with orphan instances could be solved this way. Expose different modules that target different dependencies and have those define the specific instances of your typeclass (as well as other functions related to it). Then the user can use any of those if he wants, he'd just need to reference the specific module and specific dependency, but not any of the others.
Thanks! I figured there was something like that. You'd also have to wrap the module export with the conditional. Or alias all the imported modules as something (like `import A.Text as Export`) and export that instead. 
That one is pretty subjective I suppose, hard to draw a concrete line as far as how much is too much, so I wouldn't be able to answer that. Regarding your fit question, the answer is "yes" if you think you would enjoy writing applications in FP / Haskell and we're all happy working with each other by the end of the various interview stages.
One significant benefit of his halfway signature match :: String -&gt; Case -&gt; Predicate -&gt; String -&gt; Bool over his final Church-encoded match :: String -&gt; (String -&gt; Bool) -&gt; String -&gt; Bool is that the latter is more general: it accepts any (String -&gt; Bool) function, and there are many more than two possible. That may occasionally be beneficial, but it also condemns the `match` function to terrible performance. Any performant string-matching function needs to special-case the two common CaseSensitive/CaseInsensitive uses. Once you abstract these two cases into a function argument that's closed to introspection, you've given up on any future optimization.
Fair point about String but I don't think there are enough libraries where e.g. Num would be so devastating. Also I think Haskell has a lot of tools to locate the breaks and offer patches before any actual change takes place. I personally think it's important to remove as many warts as possible before getting to a mainstream point that large amounts of people expect production readiness. And it feels like in this community there are some who feel like just their company having done something and not wanting to touch it anymore means the rest of the echo system now has to be fixed in place.
Great post. `mapM` for IO has long been a mystery to me because using `mapM` for IO was the cause for one of the worst space leaks I've ever had in my professional Haskell code. At that point I realized that `IO [a]` isn't what it seemed to be. Without tricks a la `unsafeInterleaveIO` or the presented state token duplication it's an IO effect that has to finish and return before it delivers a (thunk to a) lazy list. In case of reading files, if I understand this correctly, after finishing the IO effect the entire file content has to be in memory already for that thunk to work on. You might as well use strict IO and save yourself the headache. I think `IO [a]` is really an absurd type and rarely what you want. It only makes sense if your use case is either simple or very special. For streaming IO the proper way to do this seems to be with types of the form `IO (a, IO (a, IO (a, ...)))` where the type properly reflects the interleaving of effects with the element stream. This is basically what `pipes`, `conduit`, and `streaming` do where this would be something like `Stream (Of a) IO ()` or `Producer a IO ()`. By the way, I think the space leak was fixed by using ``mapM_``.
If the constructor is exposed, then yes. If the constructor is exposed through a module named "Internal", then no. If the constructor isn't exposed, then no, but in that case you can't pattern-match on it anyway.
I don't think returning the proper `r` buys you anything in practice - but it would be certainly cleaner. What code were you imagining? I don't see how to do the continuation transformation.
I agree. I'm more interested in the general traversal setting here we lose the parametricity guarantee. There when you have two traversals that each traverse part of some container `s` that return the same type `a`, you can make a "horizontal" composition operator if you knew that they were accessing disjoint parts of `s` that traversed both, e.g. something that combined `_1` and `_3`. Without laws you can start to smash these two things together but in the obvious encoding you get two layers of `m` that you need to join away. The `Traversable`-like form you give there isn't vulnerable to this sort of thing, due to a quirk of parametricity, but the general traversal case is.
Something like this, maybe. sequenceIO :: [IO a] -&gt; IO [a] sequenceIO xs = IO $ \r -&gt; apply (\(# r', ys #) -&gt; (# r', ys #)) r xs where apply :: ((# State# RealWorld, [a] #) -&gt; (# State# RealWorld, [a] #)) -&gt; State# RealWorld -&gt; [IO a] -&gt; (# State# RealWorld, [a] #) apply f r [] = f (# r, [] #) apply f r (IO x:xs') = case x r of (# r', y #) -&gt; apply (\(# r'', ys #) -&gt; f (# r'', y:ys #)) r' xs' I'm only guessing here, but I assumed that evaluating the proper `r` forces the correct order of IO, eliminating the need for `demand`. But again, I don't know what I'm talking about =P
This is a stylistic preference issue... but wow do I disagree with that. My rule is avoid case whenever possible. It complicates code and makes it harder to read. If case really is the only reasonable way to do it - it is a complex set of pattern matches, refactoring as a set of function equations or guards would make the code more complicated, there is no reasonably simple way to write it using simpler constructs such as the `maybe` function, various class instances of `Maybe`, and yes, `if..then..else` - then use case. But otherwise, if it's even close, stay away from case.
I disagree. I found purity to be much helpful in large scale as well. All the goodies, such as equational reasoning hold for large code bases even with a stronger sense. One major benefit of pure code in a multi-developer setting is that one developer cannot screw up the whole logic by altering the global state. Bugs could be really isolated. 
Oh I don't think cabal actually provides these macros yet. I just put forward a potential solution. Right now, cabal tries to install all packages listed in `build-depends`. This extension introduces best effort dependencies, and thus could potentially define these macros.
Touché! :) (Not that I have anything to do with the project, I just agree that it's been... **oversold**, I guess would be the word?)
"Any language" may be too strong but what it can do out of the box, provided it works as advertised obviously, is rather impressive.
I'm not sure I see the problem?
If you _really_ want this, you can use `makeFields`. The first time it encounters a field name, it'll generate a class and instance for HasName the second time it'll make an instance. One of module A or B will have to import the other for them to share a class though. It'll have to generate the type of the field as a fundep or class associated type (I forget which.) Note: You have to prefix the field name with the name of the object it is in as well. `_animalName`, `_userName`, etc. I personally prefer to make a class for this on an ad hoc basis rather than through `makeFields` when I need a commonly used field name with common semantics: class HasName t where name :: Lens' t String data User = User { _userName :: String} makeLenses ''User instance HasName User where name = userName data Animal = Animal { _animalName :: String} makeLenses ''Animal instance HasName Animal where name = animalName Now `name` can be used with reasonable semantics, and animalName and userName are also available. But you need to pick some place to put your HasName class. The boilerplate is pretty manageable and you can be explicit about overloading.
Hello guys, **TL;DR:** General visual programming comment. I think it is not as cool as it seems. I am not familiar with Haskell, just browsing FP subreddits and stumbled upon this thread. It so happens that I am forced to use a "visual" language at work, so I thought I would share my experience and thoughts. The language we have to use is a mixture of Java, XML/XSLT and MVEL (Expression language embedded in Java). Part of the language is a set of blocks that you can connect to perform certain actions and another part are Java/MVEL expressions and XSLT transforms that are represented as blocks too and can be used in the program. Initially I thought it's interesting. I thought the visual part will help me grasp the general structure of the program while the heavy lifting can be done using expressions. Well, I have to say that after 2 years of doing this I really, really hate it. I think it is an overkill and the visual part blurs the purpose of the program instead of helping to understand it. Visual part tends to grow very large even for moderately complex programs. I can't imagine doing anything serious with this hybrid language/tool. I also used some domain specific visual languages: Reaktor for building synthesizers and LabView for lab measurements. While they were certainly better than the niche tool I am using at work, they suffered from the same issues: the promise of simplicity was quickly gone as program became moderately complex. I would probably like my work if we just used a general purpose, well designed, traditional (as in: text based) language. That being said, I can see how it could be useful to have code visualization tools that could help you see your project from "bird's view".
The test suite driver seems to be written in Python. Python is a good high-level scripting language. It's like asking 'why does GHC use Make instead of haskell'? Probably because make is better at running shell programs with external dependency resolution built-in. The tests themselves seem to be written in Haskell, verifying certain properties of the compiler and catching regressions. If they fail, it looks like the python driver is informed, and then would report the error to the user.
Sorry, I reread your post and the original reddit thread and that's exactly what you are talking about. Though it would be immensely useful not just to provide typeclass instances but for any other "optional" functionality your package may provide. For instance, maybe your package can provide functionality for 2 different versions of a different package X (X.1.0 and X.2.0 let's call them). If a user depends on X.1.0 then it can use a subset of your package, but if he depends on X.2.0 he can use another subset while still being able to have the most recent version of your package (and not be locked to a previous version which may have bugs and security issues that were patched in more recent versions).
He's saying that the encoding of the solution is not necessarily the best encoding. Here's a more obvious example: a program that adds up five numbers by doing each addition successively is going to be far more efficient than one that adds up a number then 10,000,000 zeroes and then the next.
&gt; In your example, isn't `plusOrMinusOne` deterministic? Only if you execute it on the REPL in your head to determine how many values it produces. The type signature doesn't say there's always two values, it says given an int, the function will produce zero or more ints.
Adding to /u/Lossy's comment, *you* can also create *your own* pattern: -- pattern Entity key val &lt;- (entityKey &amp;&amp;&amp; entityVal -&gt; (key, val)) getKeyVal :: Entity -&gt; (Key, Val) getKeyVal entity = (entityKey entity, entityVal entity) pattern Entity :: Key -&gt; Val -&gt; Entity pattern Entity key val &lt;- (getKeyVal -&gt; (key, val)) If you have a function `makeEntity :: Key -&gt; Val -&gt; Entity` you can make it (*explicitly-*)bidirectional pattern Entity :: Key -&gt; Val -&gt; Entity pattern Entity key val &lt;- (getKeyVal -&gt; (key, val)) where Entity key val = makeEntity key val Now it acts not just as a pattern, but as a data constructor (*function*) `Entity :: Key -&gt; Val -&gt; Entity` and you have control over the design.
I tried to make it clear in the blog post that this isn't restricted to orphan instances with that example about `lens`, but, to be abundantly clear, it's not just for instances. Your example of multiple versions is, however, *not* supported. Practically speaking, the proposed mechanism degrades to all optional exposures becoming actual exposures as with today's Cabal, which wouldn't allow depending on multiple versions of a dependency. 
From what you've said, you've got the mind and personality of someone who wants things to be grounded deeply. Regardless of how practical it is, I think you will naturally find yourself drifting away from "get 'er done" tools like Python/Ruby/Go/Java to ones that put a strong emphasis on theory and quality of thought. I think Haskell is extremely practical, but even if it isn't you sound like the kind of person who, like many of us here, simply can't be satisfied with "banging something out" as is the norm in many (if not most) programming circles. Instead of offering advice, I want to attempt a prediction: you won't be satisfied with programming until/unless you use Haskell or something similar.
This is very similar in spirit to the [K Framework](http://www.kframework.org/index.php/Main_Page), which allows you to specify the syntax and semantics of a programming language and automatically generate an interpreter from it. You can also do some neat things with your specification, though I don't remember the details (something to do with model checking). The K language is quite nice for specifying programming languages: you basically just write down a BNF for the syntax and rewrite rules for the semantics. So far [JavaScript](https://github.com/kframework/javascript-semantics) and [C](https://github.com/kframework/c-semantics) have complete specifications, as far as I know. There seems to also be some progress on specifying [Agda](http://www.itspy.cz/wp-content/uploads/2015/11/acmspy2015__32.pdf), [LLVM](https://github.com/kframework/llvm-semantics), [WebAssembly](https://github.com/kframework/wasm-semantics), [JVM bytecode](https://github.com/kframework/jvm-semantics), [Java](https://github.com/kframework/java-semantics), [OCaml](https://github.com/kframework/ocaml-semantics), and [Python](https://github.com/kframework/python-semantics), but I don't know how complete those are. K is based on matching logic, which is pretty straightforward to learn IMO (strictness is handled via a mechanism that is really just a veiled state monad). The syntax can be difficult to parse, but I suspect that is fixable. That said, the current implementation of K is pretty bad. Despite being written by PL researchers, it is written in Java, and there are some _majorly_ crufty things in that codebase. A K compiler written in Haskell would greatly fix many of the issues with the language. Full disclosure: I was paid to work on K last summer
Not even all open source.
GHC has an LLVM backend. You can compile Haskell to LLVM today. In some cases, it's notably faster than GHC's typical compilation process. Once you compile to LLVM, you use Graal &amp; Truffle on the LLVM. This may not be more efficient than just compiling LLVM to machine code, but according to their claims, it's not much worse.
You need to link your program with the threaded runtime (`-threaded`) and add `-N` to the RTS options (`-with-rtsopts=-N`) . Relevant GHC documentation [here](https://downloads.haskell.org/~ghc/master/users-guide/using-concurrent.html#using-smp-parallelism)
Data.Map and Data.Set use binary search trees internally. 
Hi. Servant developer here. We piggyback on the warp web server which uses the Haskell runtime system to schedule a lightweight thread per client. The runtime system uses an event loop behind the scenes (epoll or kqueue syscalls) similar to NodeJS so we are able to handle thousands of requests on a single core. This is different from say Apache web server where a single thread is assigned to a request and there are only as many threads as there are CPU cores.( Also read: http://www.kegel.com/c10k.html) However. You can still use all cores by telling the Haskell runtime system to do so. Compile your code with the -threaded flag and then run your programme with the following options: ` +RTS -N4` to run servant on 4 cores. Hope this helps.
I've seen some other programs where the number of "main threads" or "cores used" (not sure exactly which of these it is), is specified by a command line flag. In Haskell land we seem to have to compile it into the binary. Now I do not have much problem with recompiling, and I guess most of us do not, but still I'm curious why it is like this and if it is possible to create efficient binaries with Haskell that accept this configuration option as a CLI flag...
There is actually a CLI flag! The RTS can be instructed to use more cores through the CLI of your generated binary. `+RTS -N4`. You're not setting that flag at compile time but runtime! 
Pithy replies like this -- which amount to nothing more than hair splitting over some verbiage -- convey the kind of attitude that make people think Haskell programmers are smug. Your statement comes off more as a fashion display, rather than any meaningful statement about the topic. It's a medium dot com blog post; it's embellished. A thinkpiece. It's not like you're writing an editorial for the NYT. Read between the lines, people. Otherwise you will make probably write internet comments based on certain assumptions and if people intuit those assumptions and disagree, you might not be very convincing. Despite a condescending tone and lack of goalposts of what constitutes 'dull' and 'dreary' - Graal is extremely important and truly unique work. I'm not aware of any other production quality, multi-language partial evaluator. It is not the legendary UNCOL, but it is not really claiming to be either, if you actually follow it. So I find it strange to sit around saying "_Any_ language?" like it was claiming that in the first place. I suppose you can just take the author's word if they literally say it, but I find that a rather lazy excuse when much has been written about Graal elsewhere. I'm interested to know why you think the languages listed are so 'dull' and 'dreary'. Are individual languages the only metric to judge Graal by, or should we judge it by the fact it can implement C all the way to Smalltalk? Is Smalltalk really that dreary? It's quite a lot more advanced than most other dynamic languages, and has a much richer history of efficient implementations and research. A easily-created, fast Smalltalk implementation is quite impressive on its own. And do you think Graal couldn't work for say, Scheme, which has its own rich history of fast interpreters, compilers, and even partial evaulators? Or OCaml? Ports to the JVM have exited for a while already. These seem quite within reach of the design of Graal, in fact. Languages like Prolog or Mercury are more difficult; but we hardly judge any other compiler technology by whether I can write the world's fastest Prolog, either. For these particular cases - most people build bespoke systems for that language. Just like dynamic languages, in fact, since most reusable (static compiler) technologies weren't applicable to them. Only recently have systems like Graal and RPython breathed life into more-reusable dynamic compiler technology, IMO. Perhaps logic programming languages will have life breathed into them as well on this front. It would be interesting to think about why it would or would not work. But it's annoying to contribute to this subreddit when half of the people want to act smug (including me, of course) like this, and cannot reasonably take time to make a judgement about competing technology without knee-jerking first. If you're hung up on the fact you can't write a fast Prolog or Haskell, just say that.
How about PyPy? RPython + Meta tracing JIT!
I wanted to play around with this, but I didn't really feel like setting up a virtual machine or trying to get the stuff to build from source, and the Oracle website provides binaries only for Linux and OS X. Which struck me as a bit strange because this is all supposed to be on top of Java!
Where is this discussion even happening? The fact that this is topic is being posted shows wherever it is, it's on a platform most people don't use.
It's on the ghc-devs mailing list. I don't really like mailing lists (didn't grow up using them), but they do seem to be a way that a lot of people are comfortable communicating.
Indeed I should have made this more clear. You can either leave your feedback on the `ghc-devs` [thread](https://mail.haskell.org/pipermail/ghc-devs/2016-July/012468.html) or the [Pull Request](https://github.com/ghc-proposals/ghc-proposals/pull/1).
To be clear, you can leave your feedback either on the [Pull Request](https://github.com/ghc-proposals/ghc-proposals/pull/1) or the `ghc-devs` [thread](https://mail.haskell.org/pipermail/ghc-devs/2016-July/012468.html).
Comparing Rust and Python processes I believe that Rust's works better even when it's often more painful. The discussion about including a green runtime e.g. raged on forever but the result is great. As much as I love a lot of Python 3 changes there's also a lot of BDFL questionable stuff going on that doesn't have full community support (such as making asyncio part of core Python e.g. when greenlets already filled that need fairly well)
I'm having a hard time finding a way to do that; in hindsight I should have submitted a link directly to the pull request.
The fact is most programming language, to be popular, try hard to pretend they are "easy" by hiding as much theory as they can. That's even true for most software engineering books! So yes, it's very likely that you'll be disappointed by most of what you'll see in languages, books, etc. &gt; I kept thinking to myself there has to be a better way to build software Indeed there is. Today's software engineering is more like alchemy than science but the theory and the tools for a scientific approach do exists! I recommend you to have a look into [Software Foundations](https://www.cis.upenn.edu/~bcpierce/sf/current/index.html). Don't worry, a philosophy education may be a better fit than a CS one for learning Haskell. It is so different to any other programming language (non strictness, purity, rich type system, etc) that being used to program in another lang does not give any advantage. But learning Haskell is just the first step. What really make the difference are advanced concepts such as category theory (to reason about structures), logic/type-theory (to encode/enforce logical properties in your programs), higher-order programming (for composability), etc.
I use `persistent` a lot, so I'm familiar with the `Entity` data constructor in your example. In this particular case, I would definitely choose to pattern match on the data constructor rather than use the accessors. The reason is that the `Entity` data constructor is pretty much fixed. If it changed, `persistent` itself would need some huge changes. A lot of things would break. There are two main situations in which you don't want to pattern match on a value: 1. There are invariants not captured by the type system. Notice that the `Map` type from `Data.Map.Strict` doesn't have a data constructor exposed. If it did, you could build a bad `Map`. The interface is the functions (much like in OOP, the internal representation is hidden). 2. The data type is a big ol' bag of settings. Look at [ReaderOptions](http://hackage.haskell.org/package/pandoc-1.17.2/docs/Text-Pandoc-Options.html#t:ReaderOptions) from pandoc (or even better, further down on the page is `WriterOptions`). Don't pattern match on that, because it's plausible that the author may add more fields to it. Just my two cents.
Yes this is correct. Handling http requests and sending responses is all done by warp in most haskell web frameworks. But routing etc is not handled by warp. That is done differently per framework and will matter in performance somewhat. Also, each framework has their preferred way to talking to a database which might also impact performance. Recently we wrote a benchmark for servant at least comparing it to snap and spock. https://turingjump.com/posts/tech-empower.html 
It's spread out over many issues / PRs so probably not the best example. Also it was more acrimonious than I remembered, so maybe not the best example for that reason, too. * https://github.com/rust-lang/rfcs/pull/219 * https://github.com/rust-lang/rust/pull/18967 * https://github.com/rust-lang/rust/issues/18000 It looks like "thestinger" managed to get rust to do the sane thing (remove runtime) but stopped being a contributor during the process because he felt treated unfairly.
Great example! Helped me grasp succinctly what that article was about more than I had in previous readings.
Ah, I see. So this is a result of writing that `widget` macro using quotes I think. If it was written in a "direct" style (manipulating the AST explicitly) then it would read uglier certainly, but could live in the same module. Since I tend to write things in the "direct" style, then that probably accounts for one reason I haven't felt the need for this stuff as much as others may have :-)
just figured out the error with help from sdx23 from #haskell irc,, https://gist.github.com/gregnwosu/ef6fefa42bf2862d57df301aead022fb/revisions?diff=unified
&gt; In your example, isn't plusOrMinusOne deterministic? Why do you say it has side-effects (does returning multiple values count as side-effects)? Don't let yourself be fooled by the name, here "non-deterministic" has nothing to do with randomness, it's just a name which some people use for the effect of returning zero or more values. And even then, you might wonder why that is considered an "effect". Let's ignore IO and ST for now. Every other monadic (and applicative) effect is simulated using pure code. With the state monad, for example, each part of the stateful computation is implemented via a pure function with an extra input and an extra output. In this extra input, the function receives an immutable value representing the state before that part of the stateful computation, and in the extra output, the function returns a different immutable value representing the state after that part of the stateful computation. There is no underlying piece of memory whose contents gets replaced, only pure functions and immutable values whose contents never changes. And yet, using `do` notation, it really does looks like there is a piece of anonymous state which is being accessed and modified by `get` and `put`: pop :: State [a] (Maybe a) pop = do xs &lt;- get case xs of [] -&gt; return Nothing (x:xs) -&gt; do put xs return (Just x) I find it much easier to reason about State code by thinking in terms of those stateful effects than by thinking about the functions to which the code expands. It's the same for every other monadic effect, including list's. "Non-determinism" is implemented by deterministic functions returning multiple values and by backtracking, but when using `do` notation: pythagorean_triples :: Int -&gt; [(Int,Int,Int)] pythagorean_triples limit = do x &lt;- [0..limit] y &lt;- [y..limit] z &lt;- [z..limit] guard (x**x + y**y == z**z) return (x,y,z) When I think about the code above, I think "pick 3 non-decreasing values `x`, `y`, and `z` such that the condition holds", not "try `x = 0`, `y = 0`, `z = 0`, then if the condition doesn't hold, try `z = 1`, then...". Sometimes I imagine that there is an oracle which magically knows which `x` it needs to choose in order for the rest of the code to pass all the guards. It's non-deterministic in the sense of a [non-deterministic finite automaton](https://en.wikipedia.org/wiki/Nondeterministic_finite_automaton), which accepts if there *exists* a sequence of valid transitions from start to finish, even though the automaton does not give a procedure for deterministically selecting a transition for each input symbol. Of course I know it's not how the code actually behaves at runtime, I know that State uses pure functions and that list uses backtracking. But it's useful to temporarily forget about that and to code as if there were effects. It's easier this way, so it's a useful abstraction. This is what monadic (and applicative) effects really are: an abstraction which allows you to write code as if there were effects. How the effects are implemented has little importance. IO and ST are implemented using real memory modifications, State isn't, but the beauty of it is that the code looks the same in both cases, so you don't need to care about those details.
Because Oracle will leverage any advantage they can. Sun Microsystems approved of Google using Java in Android, and yet Oracle has dumped untold amounts of money into trying to squeeze money out of Google for using Java. They literally bought Sun Microsystems to sue Google, as far as I have ever been able to tell, based on the negligence they have poured into the Sun projects Oracle acquired.
[Easier to read version](https://github.com/ghc-proposals/ghc-proposals/blob/proposal-process/proposals/0000-proposal-process.rst) (rendered RST instead of Github diff view)
I can't speak for SPJ, but I don't think he _wants_ to have to think through all the details of such proposals. And even if he (or the two Simons together) did end up having final say (which, I think, in practice, almost everyone would yield him/them) there would still need to be a process by which they could delegate and organize getting things into a state where they _can_ make such calls without too much work on their part. Like, even though Python has a BDFL, it also has a PIP process.
For me the state of the codebase and choice of language was especially disheartening, as I was working on a Standard ML code generation backend (based on the [nascent OCaml backend](https://github.com/dwightguth/k/tree/ocaml7)). This codegen was based on the new (at the time) intermediate language, KORE. The "hard" part of rewriting K in Haskell would be the stuff before KORE, everything after that would be a couple weeks of work, tops. Particularly difficult is parsing K, but I think it's soluble. The real issue is toolchain-based; the people who currently put the most time into K are primarily Java programmers, AFAIK, and are probably not willing to radically change the way they work on K. On the other hand, if they did decide to switch to Haskell, I'm sure there would be no end to the number of cheap but efficient interns they could hire to work on it.
It is part of the standard argument of the streaming libraries that somewhere or other their types encompass or specialize to ListT done right see e.g. https://hackage.haskell.org/package/pipes-4.2.0/docs/Pipes-Tutorial.html or the first row of the table [here](https://hackage.haskell.org/package/streaming-0.1.4.3/docs/Streaming-Prelude.html) which gives some ListT equivalents
Thanks quchen!
&gt;We piggyback on the warp web server which uses the Haskell runtime system to schedule a lightweight thread per client. Might it be worth mentioning the prefork behavior Warp uses on top of this?
I mean the TypeFamily approach I demonstrated in the original post would work fine for your examples (failing on the first, but not on the second). The only thing it would not be able to do is type inference based on the return type as type families are not injective, but that downside also gets rid of the problematic part that you acknowledged about your suggestion.
I think you get `-N` with just `-threaded`, whereas `-rtsopts` unlocks additional options that are not as common as `-threaded`.
Is it possible to have a link to the rendered version in the pull request? Reading diffs is pretty awkward.
Bickering with the GHC committee (and getting his owns proposals accepted) will cost him more time than making a dictatorial decision on some syntax extension once or twice per year. The main focus of those working on GHC is and should be on bug fixing (there are &gt;1000 open bugs!), performance improvements, documentation, mentoring others, code review, system administration etc. This focus on features and what process to use is just a massive distraction away from those more important tasks.
It literally says "Graal &amp; Truffle are a product of Oracle Labs", just before the download link in the article. Did you just not read that part? It's hardly burying the fact, but it's not particularly notable beyond that point. Should it say "This is an Oracle product" every 6 sentences? I'm about to give up on this place - the people posting in this subreddit are utterly worthless and incapable of reading it seems, at least for the most part.
FYI, your OS might have a setting that determines the max number of threads that a single app can simultaneously support. You might need to bump it up. 
/u/ezyang you mention that the result of the type generativity conincides in all phases. How does this relate to cases where host-dependent types like `Int` are differently implemented in the compile-time RTS and the runtime RTS? What about CPP depending on machine-dependent macros?
&gt; Next time, we'll look at futumorphisms and histomorphisms Can't wait!
I believe that the dual of `(,,)` would be an `Either3` type that takes three arguments, and so on and so forth for tuples of greater arity. 
I'd guess `(a,(b,c))` is the dual of `Either a (Either b c)`. And `(a,(b,c))` is isomorphic to `(a,b,c)`.
This sounds delusional. It is an article aimed at a more general audience than programming language and compiler aficionados. What you quote is simply some persons choice of writing style (who by the way isn't associated in any way to the project, why should he have access to an Oracle blog?). I can't find anything factual incorrect in it. He explicitly says that Graal and Truffle are products of Oracle Labs. Why should he say that more than once when it is not even really relevant to the article other than providing some context? By the time one reaches high-school it is usually known and accepted that redundancy is not desirable in writing. And as if it were even possible to hide Oracles involvement. The first related result for "Graal" on Google is the OpenJDK page. Now, you have of course the right to be put off by whatever you choose. But your attempt to style this as anything else than personal bias (and quite dogmatic at that, TBH) is rather futile. Generally speaking, Oracle is certainly not well-loved (and rightly so, IMO). But Graal and Truffle are (L)GPL licensed (no one can ever take that code away from you) and even if it were not, what is your loss? As long as even one insight is made, it is a net win for the public. But that point was already made and your answer missed it completely, so I have little hope it will ever get through to you.
edit: Brainfarted and forgot about `modify`. See /u/mstksg's [post](https://www.reddit.com/r/haskell/comments/4tthow/whats_the_easiest_way_to_use_and_sort_arrays/d5k63tl). So the issue here is that V.sort works on mutable vectors, not regular vectors, and you need to sort it within some monad to capture the mutability. Try something like this (using the ST monad): import qualified Data.Vector as V import qualified Data.Vector.Algorithms.Merge as V import Control.Monad.ST as ST main = print . V.toList . sortVec . V.fromList $ [3,1,4,1,5,9] sortVec :: Ord a =&gt; V.Vector a -&gt; V.Vector a sortVec vec = ST.runST $ do mvec &lt;- V.thaw vec V.sort mvec V.freeze mvec 
You can use `modify` to run an algorithm on mutable vectors immutable vectors. myVec = V.modify sort oldVec Its probably better to avoid directly working with ST and mutable vectors if all you want to do is run an algorithm on your vector :) If you want to run an arbitrary mutable vector algorithm, you can use modify as well: myVec = flip V.modify oldVec $ \mvec -&gt; do -- stuff to do with the mutable vector (slicing, assignments, etc.) 
The idea of running a haskell behind an onion is interesting. You can assume that the easiest way to find who is running an onion is to exploit the web service since that is all you really have access to. By forcing good practices (eg. string sanitation) with types I'm guessing you'd end up with a more secure service. 
I'm no expert in LH (I played around with it for a while before getting too frustrated by its immaturity as a project and my lack of knowledge in the domain), but I think I can answer part 2: Like many things in Haskell, the answer is "newtype"s. You might be able to use [abstract refinements](http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/blog/2013/06/03/abstracting-over-refinements.lhs/), but you shouldn't need to; you make sure functions like indexing are lifted to be accessible in LH annotations (IIRC the term is "measure", but I'm very much not sure), and you write the invariant on the constructor like anything else. I'm sure /u/ranjitjhala can say more.
Thank you! Sorry for new text before search about it :(
Yea, so if you're targeting exclusively GHC 7.10+, you can leave out the `return = pure` lines. But if not, you'll need to keep doing it.
As an aside, something that's kinda nice in a blog post series is if you can deduce the URLs of the other posts in the series from one of them. Not a serious issue, obviously. 
I don't see that totality solves the problem here. Here's the thing: totality means that all terms could be reduced to normal form at compilation time, but *macro systems don't just work on terms*—a good macro system should allow you to generate things that are not terms, like a set of `data` declarations.
Hmm, I don't know! But the only way data can be transferred from the host language to the target language is by the macro expansion. So even though they "refer" to the same thing at the end of the day the output is just syntax.
See [D2375](https://phabricator.haskell.org/D2375) :-) You might need: [D2362 ](https://phabricator.haskell.org/D2362) as well. If you want to build the `x86_64` simulator, you will likely also need, [D2378 ](https://phabricator.haskell.org/D2378) though there are some issues will LLVM and the target triple for ARM and the ghc calling convention. I'm trying to hammer those out. So if you end seeing llvm breaking for some architecture. Try to selectively revert the data layout + triple changes from D2378 You might also want to try my [ghc-ios-scripts fork](https://github.com/angerman/ghc-ios-scripts) 
As a Yesod developer, I'm trying to figure out what you mean by "even Yesod". Warp was originally designed for use with Yesod and continues to be developed and maintained in tandem with Yesod. But it was intentionally given a more general API, called WAI, which allows it to be shared easily with other web frameworks as well.
Wow, that's amazing, thanks for the work and the info!
Indeed; that's because shake does it better! These days, Haskell does a pretty darn good job of handling test suites, but that may not have been the case way back in the day. @eacameron's point about build dependencies may be spot on. Another possibility is that it's just a lot of trouble to rewrite the test suite to use Haskell and the potential benefit is relatively tiny.
In the case of memoization, you can **prove** that the memoization has no observable effect (other than time complexity). In fact, the memoized function is actually pure. That means that you can – correctly and safely – use `unsafePerformIO`.
Yeah. Now I know what cata/ana/para/apomorphisms are. You explained it better (for me at least) than anyone else.
Oracle Labs is the continuation of Sun's incredibly impressive PL research efforts. I suspect it's thriving *despite* Oracle, not because of it.
Do you guys have a chatroom now? I mean not IRC. We may need to paste images, paste code, paste links during discussion. I only see Gitter and Slack can do this.
I agree. Everyone is on slack nowadays. As much as I hate to say it, it is useful because of the features alone. Clojure and Elm guys have a slack team as well. I guess gitter could also work, and it might even be a better idea due to a better free offering.
I'm in Clojure's Slack team, and that's where the inspirations is from. Chatroom of Gitter are connected to orgs and repositories. Since I'm only in the `haskell-china` org, I don't have a good repo to start a Gitter chatroom. So I chose Slack.
Is there any relation between Recursion Schemes and Free/Cofree? For example RCoalgebra looks similar to Free where Pure can also be used to separate the flow of computation.
Needs to cover Zygohistomorphic prepromorphisms ;)
Links work fine in IRC. For code we have [lpaste](http://lpaste.net/). I'm not sure why images would be useful. #haskell on freenode also has the advantage of there being a lot of people in it.
You're right, it works - essentially by building a chain of applies in memory which application then traverses. The downside is it is much slower (~ 60% in rough benchmarks).
Now it's simple. Check this picture: ![](https://pbs.twimg.com/media/Cn4enKHUMAE1eV4.png)
I find the use of "pattern" and "anti-pattern" very imprecise. Besides given that it's all math why resort to such imprecision?
I have not seen this before. The [only other discussion I could find](https://np.reddit.com/r/haskell/comments/atkei/haskell_antipattern_existential_typeclass/) was from 6 years ago. 
That's awesome. Congrats!
Well it's a great post and the discussion is usually interesting. It's perfectly OK with me if you want to discuss it here.
I believe R. Harper and P. Wadler, and many others, would strongly disagree. The beauty of FP is that programs and math can be the same. (edit: mispelling)
&gt; The beauty of FP is that programs and math can be the same. Maybe. But designing programs and doing maths are not the same. This post is about designing programs.
&gt; Scala proves that Java/C#/many others' vision of OOP is just the beginning of the introduction of what can be done. In a way, so does Erlang ;)
Your 3 articles were joy to read. Btw would the final discussed morphism be zygohistomorphic prepromorphism?
Thank you. One of my underlying questions was whether the approach in `MemoUgly` really was safe in this sort of situation. Edit: For anyone wondering why I was concerned, the many [caveats for using `unsafePerformIO`](http://hackage.haskell.org/package/base-4.9.0.0/docs/System-IO-Unsafe.html#v:unsafePerformIO) make me nervous, given this is a system where reliability, testability and ideally provable correctness are important.
Pardon the name dropping, but Conal Elliot would disagree: look up his name and "denotational design."
So are you saying that the use of those terms is an anti-pattern?
Yes: futumorphisms and histomorphisms, which I will cover in a subsequent installment, operate on `Free` and `Cofree` rather than `Term`.
Really, have you asked him? I've read Conal's denotational design work - it's great (but not the final word on design). I have no problem with using maths in programming, or I probably wouldn't frequent /r/haskell. And I'm all for binning ad hoc patterns where there's a more precise mathematical concept too. But surely Luke Palmer should be allowed to use the words "pattern" and "anti-pattern" in his blog post. He is not wrong. If you don't think he should have said that, what do you think he should have said?
Do you know about `-XLambdaCase`? checkEntity :: Entity MyRecord -&gt; SqlPersistT IO (Entity MyRecord) checkEntity (Entity entityId _) = do e@(Entity _ entity) &lt;- updateAndRefetch entityId forM (entity ^. parentId) Yesod.get &gt;&gt;= \case -&gt; Just parent -&gt; checkEntity parent Nothing -&gt; return e `forM` or `mapM` from `Data.Traversable` will also help. If `entity ^. parentId :: Maybe x` and `Yesod.get :: x -&gt; m y`, then `forM (entity ^. parentId) Yesod.get :: m (Maybe y)`.
This may interest you: let's say you want to double every third number in a list. We make an infinite list of functions `[(2 *), id, id, (2 *), id, id, (2 *), id, ...` with [`cycle :: [a] -&gt; [a]`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-List.html#v:cycle). Here we have given a name to “double every third”: doubleEveryThird :: [Int -&gt; Int] doubleEveryThird = cycle [(2 *), id, id] using [`zipWith :: (a -&gt; b -&gt; c) -&gt; ([a] -&gt; [b] -&gt; [c])`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-List.html#v:zipWith) to apply a function to its argument element-wise: &gt;&gt;&gt; zipWith (\fun x -&gt; fun x) doubleEveryThird [1,11,111,1111,11111,111111,1111111] [2,11,111,2222,11111,111111,2222222] This turns it into [(2 *) 1, id 11, id 111, (2 *) 1111, id 11111, id 111111, (2 *) 1111111] = [2, 11, 111, 2222, 11111, 111111, 2222222] Let's simplify `(\fun x -&gt; fun x)` \fun x -&gt; fun x = \fun x -&gt; fun $ x = \fun x -&gt; ($) fun x = \fun -&gt; ($) fun = ($) zipWith ($) doubleEveryThird :: [Int] -&gt; [Int] but recall that `($) = id` \fun x -&gt; fun x = \fun -&gt; fun = id zipWith id doubleEveryThird :: [Int] -&gt; [Int] `id`s everywhere! ---- Unrelated, list comprehension &gt;&gt;&gt; [ fun x | (fun, x) &lt;- zip doubleEveryThird [1,11,111,1111,11111,111111,1111111] ] [2,11,111,2222,11111,111111,2222222] Using [`ParallelListComp`](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#parallel-list-comprehensions) ([24 Days of GHC Extensions](https://ocharles.org.uk/blog/guest-posts/2014-12-07-list-comprehensions.html)) &gt;&gt;&gt; [ fun x | fun &lt;- doubleEveryThird | x &lt;- [1,11,111,1111,11111,111111,1111111] ] [2,11,111,2222,11111,111111,2222222] ---- Unrelated, lenses: divides :: Int -&gt; Int -&gt; Bool d `divides` n = n `mod` d == 0 Before we ‘complected’ the notion of *doubling* and *every third*, we can factor out the concept of “every third” into a(n indexed) traversal everyThird :: IndexedTraversal' Int [a] a everyThird = traversed.indices (3 `divides`) Now we can access every third element &gt;&gt;&gt; toListOf everyThird [1,11,111,1111,11111,111111,1111111] [1,1111,1111111] and double every third element &gt;&gt;&gt; over everyThird (2 *) [1,11,111,1111,11111,111111,1111111] [2,11,111,2222,11111,111111,2222222]
That's also landing this week. The hard part is now done, other features will be using unboxed sums implemented in this patch under the hood.
I think the problem is that the API surface area is huge, not the docs being too verbose.
I think this is pretty fabulous. Any ideas when UnliftedDataTypes might arrive? Or am I just being greedy? 
In my experience, the more advanced the type system feature, the more poorly it interacts with other advanced type system features. This is not a drawback of Haskell, but rather just a result of the fact that mixing type-level proofs, so to speak, is hard. So because I _do_ want to use those advanced features when necessary, I find that _avoiding them_ whenever possible is a better course of action. That way, I haven't precluded using them later, due to unfortunate interactions elsewhere :-) Stated another way -- the easiest things to do type level proofs over are simple terms. So the simpler I can keep my terms, the more possibilities I have to do fancy proofs over them later.
No, I haven't asked him but I've read a paper by him on it. And, yes, denotation design is obviously not the final word on design. It's a retort to your statement: &gt; But designing programs and doing maths are not the same They obviously can be the same. 
Nope, not at all. I hold no normative position on what words one uses or doesn't use.
Designing programs and _designing proofs_ are more the same. It just so happens that not all of math, surprisingly, is designing proofs.
I'd argue that there are patterns and anti-patterns in math too, and you can tell because there's a notion of a "good" and "bad" proof of the same thing even if both are correct.
That's interesting, and could be quite impactful for Haskell I think.
As anything that is an artifice, there's a always a sense of aesthetics. So, "good" and "bad" in that sense. But most mathematicians spend their time encoding their insights into proofs rather than writing about "patterns" and "anti-patterns." When programmers use "pattern" or "anti-pattern," they take on normative positions. And most of the time at least in my experience, it's merely an opinion. Hence the imprecision. 
Sorry if I'm misunderstanding. That's already unboxed and that's the only anonymous sum we have. We don't have boxed variants. Boxed variant is not special in any way, but unboxed variant needed special support in the code generator. That part is done now.
This only applies to strict constructors, right? So we'd have to use a strict Maybe and Either to see a benefit. And then possibly only if it can figure out the exact type and inline the various functions to specialize them? So basically the same caveats if you want to get unboxed integers. Though that reminds me, Integer itself is a strict sum type, so maybe Integer-heavy code could see a boost? Also, speaking of new optimization techniques, the preceding commit is compact regions: http://git.haskell.org/ghc.git/commit/cf989ffe490c146be4ed0fd7e0c00d3ff8fe1453 It looks like it could let you load or generate a lot of static data, and then turn off the GC for it. It would be nice to be able to do this for large CAFs.
&gt; It looks like it could let you load or generate a lot of static data, and then turn off the GC for it. It would be nice to be able to do this for large CAFs. Indeed, I am very excited about this feature.
No, lots of mathematicians also write "soft" texts on what makes proofs good or "beautiful" or not. And they certainly lecture on it! Polya, Ziegler, Erdos, and Rota all come to mind immediately in this regard.
To get an invite - visit: http://fpchat.com/
Just imagine all the puns exploiting the similarity to S**t**ack!
[Arrays](https://hackage.haskell.org/package/array) are actually not as nice the the stuff from Data.Vector. But, both model several values stored in contiguous memory.
Do either [`evalListN`](https://hackage.haskell.org/package/parallel-3.2.1.0/docs/Control-Parallel-Strategies.html#v:evalListN) or [`parListN`](https://hackage.haskell.org/package/parallel-3.2.1.0/docs/Control-Parallel-Strategies.html#v:parListN) do what you want? 
This is true. The pattern matching on `(#|||#)` is extremely ugly.
Probably https://github.com/commercialhaskell/intero?
You actually got what I was getting at. I think this will be a blog series where in the end I'm going to have something like pfold for cocoa. I think that this is actually achievable in swift relatively easily. 
&gt; And like even if you are not using events directly, you are still running on a heavily event based system, so technically if the system is reactive, your program, being a subset of the system, is also reactive (lol transitive property?). Technically, your program is executed by manipulating electrons in specific ways. Thus, programs are really mechanisms for manipulating electrons. I don't find this sort of reductionism very compelling. Your line of reasoning is interesting enough; grandiose, over-reaching, unsupported claims do not enhance its presentation.
Misuse of types must be given an alliterative appellation, in the spirit of "Boolean Blindness". How about "Integer Infatuation" or perhaps "Signed single-mindedness"?
You are taking it to the absurd though. No one programs electrons. But events and event loops are the way of life in systems. In fact there's this quote that all that systems are is event loops and caches. I guess my view might be a bit too systems oriented, but fundamentally I still stand behind that statement. If one model subsumes another model, the subsumer is more general right? The reactive model subsumes the non-reactive model.
&gt; You are taking it to the absurd though. Yes, it is reductio ad absurdum. &gt; I guess my view might be a bit too systems oriented, but fundamentally I still stand behind that statement. I'm not asking you to abandon the statement, I'm asking you to support it. &gt; The reactive model subsumes the non-reactive model. Maybe, but saying it doesn't make it so. Convince your audience. Argue your points, don't just assert them.
I guess my question boils down to what's your background? My argument really depends on what you are familiar with. Are you familiar with Elm?
There are two possible improvements: do automatic runtime bounds checking (like Ada), or use dependent types to prove that it's a valid reference at compile time. Haskell currently has neither. Until it does, it seems like using unsigned ints would actually make matters worse: you can't detect overflow/underflow. 
Unfortunately the primitive integers in Haskell can overflow silently, which means when you do arithmetic on unsigned integers it is very easy to underflow when doing arithmetic operations, making them rather tricky to use properly. Signed integers suffer from the same problem but problems are not as likely to arise under normal circumstances.
https://github.com/agocorona/transient-universe/blob/master/.travis.yml 
&gt; Because it reifies a behavior of the system What behavior? How is this behavior "reified"? What do you mean by "reified" in this context? Can you rephrase this without using jargon? &gt; that's important to get things like pfold Why is pfold important? Why is this unspecified behavior important for "getting" pfold? &gt; and good architecture possible. "X is good" is lazy writing. Which architecture is "good"? What is "good" about it? In what specific ways is it better than other architectures? Why is this "good architecture" only possible if "a behavior" is "reified"? &gt; These operations are very useful. Which operations? Why and in what way are they "useful"? Again, be specific. Even if I stipulate all of these things, I still fail to see how they lead to your conclusion that the reactive model "subsumes" the non-reactive model. I'm also not sure I know what you mean by "subsumes" in this context. If I can offer a suggestion without seeming rude, I think some material on technical writing (like [this slide deck from MIT](http://web.mit.edu/me-ugoffice/communication/technical-writing.pdf)) might help you better organize your thoughts and better support your arguments.
There's gcore on Linux to get a core dump. You want give this a go on your space leak: http://neilmitchell.blogspot.com/2015/09/detecting-space-leaks.html 
There was a presentation of this at Haskell Symposium 2015: * https://dl.acm.org/citation.cfm?id=2804308&amp;CFID=646497563&amp;CFTOKEN=91071371 * https://www.reddit.com/r/haskell/comments/3m5y3h/improving_implicit_parallelism_icfp_2015/
I'm not familiar with it, but isn't it possible to use Liquid Haskell for checking this kind of bounds?
I believe /u/ReinH did that already. There isn't much I or anyone else could add to that until you address his concerns.
&gt; Reification is a CS term, not jargon CS terms *are* jargon. I know what "reify" means, but I don't know what *you* mean when you use it in this context, since you are being incredibly vague about everything. &gt; what unspecified behavior? That's what I'm asking you. You said "a behavior". You did not specify. &gt; like what do you consider good architecture? Again, that's what I'm asking *you*. &gt; Do you want me to explain all of these too? There's no need to be snide. You asked for feedback and I am making a good faith effort to give you useful feedback. &gt; I don't mean to be rude either but you are yet to make a single counter argument, I am not trying to make a counter argument. I am trying to understand your argument. I can't counter an argument I don't understand, and I may not have a counter argument: if I understood your argument, I might agree with it. &gt; I'm not sure what exactly it is that you don't understand. I feel like I have been pretty clear. &gt; You just keep asking for clarification for things that should be obvious. When you say things like "because it reifies a behavior of the system that's important to get things like pfold and good architecture possible", which is a grammatically incorrect sentence full of dangling modifiers, your meaning isn't obvious to anyone other than you. &gt; You asked what reification is. No, I asked what *you* meant by reification. Before a meaningful discussion can occur, we have to at least agree on the vocabulary being used, and I am not sure that we do. Rather than making unwarranted assumptions that can lead to further misunderstanding, I am doing the usual thing and asking you to define your terms. I am trying to help you because you asked for feedback and your writing has room for improvement (an opinion which is supported by the votes on this reddit submission). If that feedback is unwelcome, I will stop wasting my time.
You have the patience of a saint.
&gt; CS terms are jargon. I know what "reify" means, but I don't know what you mean when you use it in this context, since you are being incredibly vague about everything. What is FRP supposed to reify? If you don't know the answer, I'm not sure you understand FRP. That being said reactive isn't FRP since it doesn't reify the same thing, it reifies events. &gt; There's no need to be snide. You asked for feedback and I am making a good faith effort to give you useful feedback. Your feedback isn't very useful though. Read this blog post https://medium.com/design-x-code/elmification-of-swift-af14b7f92b30#.xs2hzg98r When you see the use of scan function, think about what role it's playing in the codebase. https://github.com/momentumworks/swift-elm/blob/2e48a0d8b8615580c6227121484411c440730a61/swift-elm/AppDelegate.swift#L32 &gt; good architecture possible I skipped a "make". &gt; No, I asked what you meant by reification. That is an important distinction. There's only one definition. Reifying X means making X part of the data model. 
&gt; What is FRP supposed to reify? If you don't know the answer, I'm not sure you understand FRP. That being said reactive isn't FRP since it doesn't reify the same thing. So you are talking about FRP, and I am supposed to know this even though you never said so. Except you are *not* talking about FRP, since you are talking about reactive and reactive isn't FRP. Make up your mind. Or perhaps just actually say what the thing is that is being reified, rather than making us continue to guess. If it's obvious then it should be easy to explain. &gt; Your feedback isn't very useful though. It certainly doesn't seem to be very effective, in any case. Good luck with your blog posts. Perhaps try to make them relevant to Haskell if you want them to be well received in r/haskell. 
You said you understood FRP. I'm checking that you understand FRP. What is reified in FRP? And no, I'm not talking about FRP, but FRP is related so it's worth talking about it. &gt; It certainly doesn't seem to be very effective, in any case. Good luck with your blog posts. Perhaps try to make them relevant to Haskell if you want them to be well received in r/haskell. It is relevant to haskell as what I'm talking about is fundamentally at the core of building systems in Haskell. Read the blog post I linked above. /u/heisenbug mentioned that this is similar to pfold in elm. I think that he understood what I was getting at. 
Perhaps you can ascertain my understanding of FRP from [my interview with its creator](http://www.haskellcast.com/episode/009-conal-elliott-on-frp-and-denotational-design). In any event, I'm not interested in continuing our conversation. Thanks.
Ooh, maybe we can start hoping for UNPACKing polymorphic fields at some point then. I'm guessing that's a lot more work though.
One nice thing this work allows is to finally have an efficient but type safe version of null in Haskell. For a long time using `Maybe` had a penalty over using null in other languages, as `Maybe` added extra indirection and allocation. Now, we're close to being able to use `Maybe` without the overhead. A `Maybe a` can be represented as a pointer directly to the `a`, plus an additional flag stored next to it. A further improvement would be to merge the flag into the pointer, but this already gets us lots of the benefit by removing one layer of indirection.
heisenbug seems to have understood it just fine.
Isn't scan just a specialized histomorphism? Or am i mistaken?
Sidenote, that interview was awesome.
If you're going to take that position, you obviously don't want feedback.
Haskell for Mac does this. It also works really well.
There's a strong signed-integer bias in a lot of languages - even in C, despite the fact that in that language signed integer underflow and overflow result in undefined behaviour, whereas unsigned integer underflow and overflow give fully defined results (modulo 2^n where n is the bit-width, which is platform-defined for types like `unsigned int`). One of the dubious claimed advantages is that negative numbers are convenient special-case values, e.g. `-1` for failure, with of course similar issues to null pointers. In C++, where `std::size_t` and all those member type aliases are such a big thing, you hear plenty of protests about that. Personally, I just wish all those aliases were independent types, so using the wrong one would give an error. Though the fear then is typecast torture. 
The term "pattern" meant something very precise to Christopher Alexander, who started the whole "pattern language" thing. https://en.wikipedia.org/wiki/The_Timeless_Way_of_Building Basically, a "pattern language" is a collection of patterns that all relate to the same thing, and a pattern is just a) an observation that some problem keeps showing up over and over again and b) there is some solution to that problem that appears to be optimal. Practitioners of some art can argue among themselves what problems are really problems and what solutions are the best, and can give names to those solutions to make communication easier, and a collection of those is a pattern language. I don't know the origin of the term anti-pattern, but I can't remember coming across it in any of Alexander's books. It seems to be a shorthand for "I don't like such-and-such, so I'll call it an anti-pattern in hopes that others will repeat the sentiment". The whole idea of an anti-pattern seems unnecessary. If existential typeclasses are in some sense a bad solution to the problem of having heterogeneous things in containers, then it's quite alright to suggest a better pattern. However, sometimes the reason a particular pattern was adopted is not immediately apparent to bystanders. So, the problem with Luke's post from a pattern language point of view is that he didn't sufficiently explain why existential typeclasses are in some way worse than records of closures (a good argument he might have made is that existential typeclasses are, iirc, a GHC extension and not officially part of the language), and he missed the reason why I had used them in GlomeTrace in the first place, which was (in theory at least) more compact memory usage. He means well, I'm sure, but this gets re-posted from time to time and I sort of feel obligated each time to explain why I wrote GlomeTrace the way I did.
https://www.reddit.com/r/haskell/comments/1nh2xe/the_socalled_existential_antipattern_vs_record_of/ https://www.reddit.com/r/haskell/comments/18rcc6/easy_in_oop_harder_in_fp_list_of_animals/c8h9vof https://www.reddit.com/r/haskell/comments/31zagw/why_are_we_naming_types_instead_of_instances_when/cq86ff1 https://www.reddit.com/r/haskell/comments/3am0qa/existentials_and_the_heterogenous_list_fallacy/ https://www.reddit.com/r/haskell/comments/32wjoz/lawful_foldable/cqfbtd6 https://www.reddit.com/r/haskell/comments/2xpmoe/a_route_to_learning_the_haskell_type_system/ https://www.reddit.com/r/haskell/comments/24k2wz/thinking_in_types/ch7xq62 http://lmgtfy.com/?q=site%3Awww.reddit.com%2Fr%2Fhaskell+existential+typeclass+antipattern
&gt; Personally, I just wish all those aliases were independent types, so using the wrong one would give an error. Though the fear then is typecast torture. In C, the right thing to do (IME) is almost always to wrap primitive types in a single-element struct. That gets you nominal typing at no runtime cost, and a cast out of it is just .v or whatever.
All functions are strict in their return values, as you wouldn't evaluate a function if you weren't going to look at the return values.
Pattern synonyms?
Because mixed signedness is a mess. Using different types of integers would only litter the code with type conversions. Let's see: &gt;&gt;&gt; :m +Data.Word &gt;&gt;&gt; let a = -3 :: Int &gt;&gt;&gt; let b = 1 :: Word &gt;&gt;&gt; a + b &lt;interactive&gt;:5:5: Couldn't match expected type `Int' with actual type `Word' &gt;&gt;&gt; a + fromIntegral b -2 &gt;&gt;&gt; fromIntegral a + b 18446744073709551614 Wrong `fromIntegral a + b` is no harder to write than correct `a + fromIntegral b`, nor is it obviously wrong. Now, let's try to use result of `a+b`: &gt;&gt;&gt; takesNonNegative = id :: Word -&gt; Word &gt;&gt;&gt; takesNonNegative (a + fromIntegral b) Couldn't match expected type `Word' with actual type `Int' &gt;&gt;&gt; takesNonNegative (fromIntegral a + b) 18446744073709551614 I am not convinced that passing `18446744073709551614` is any better than passing `-1` number here, despite of being a positive number. I hope this example demonstrates how easy it is to break assumptions that unsigned integers provide. Furthermore, only knowing that the number is non-negative is not enough most of time. Usually you need an upper bound as well. Non-negative types alone are not that useful. As /u/guaraqe suggests, Liquid Haskell can check bounds. Including `&gt;=0`. In this case there's no need for unsigned integers at all. C does no better here. It has implicit conversions between signed and unsigned types, which is an endless source of hard to find bugs. Once I got fed up with this and turned on warnings for implicit sign conversion. It didn't turn out well - explicit type casts become useless when they are all over place. I think using the same integer type everywhere is great. Coming from C++ it feels so ..unburdensome. Of course, unique identifiers (like file descriptors) and other types where integer arithmetic makes no sense is entirely different story.
Which is exactly the problem with C++. There was even a Cppcon panel where Stroutstoup and other members of the committee were apologizing for that. 
Ah, good insight, thanks!
Aw that's disappointing. I can't say I'm surprised. I imagine the creation of one lambda per element strains the memory.
I would wait a little bit till the documentation is actually out of beta. There are many questions that aren't yet cleared, e.g. what about cross-language topics. So please join the discussion on meta SO.
https://ghc.haskell.org/trac/ghc/ticket/8761 
What I am trying to say is you have an intuition but you lack 5-10 years of hard study of a hard subject to convey it. Persevere
 I think your problem is at the `return (rnf sim')`: askUser sim = do sim' &lt;- execSimulationSteps (5*460) sim return (rnf sim') x &lt;- putStr "Press ..." &gt;&gt; hFlush stdout &gt;&gt; getLine if x /= "q" then askUser sim' else return sim' The `rnf ...` call won't be evaluated. Instead, try: sim' &lt;- fmap force $ execSimulationSteps (5*460) sim 
Is that all you got out of it?
I probably missing something here. When do you know the 'n'? Or it is implicit somehow in the list, as in dependencies between the element of the list? Technically 'pseq' does that. It computes the whnf form of its first argument and then continues with the next... Something like '(a pseq (par b b))' will do what you want, but the thing here is that if b is in whnf it will be sparked it anyway. 
&gt; But the problem is that in general almost every automatic method does "to much parallelism", i. e., they execute in parallel very small computations so the overhead is bigger than the computation itself. Ironically, parallelizing 'very small computations' is exactly how you can extract performance from the code in some architectures like VLIW, and GPU compute units. ISTM that the job of serializing/coalescing computations when this is effective and appropriate should be left to the compiler itself, since it knows about the target architecture and can make the best choices. Launching an OS thread or even a 'spark' for each parallelizable computation is clearly not the way to go, but sizable chunks of work with little data-sharing going on across them should still be assigned to different threads.
Very nice and simple introduction! Loved it!
That's a good point, and I liked your syntax. Note that the wiki page originally had some similar ideas about the syntax (see "design questions" sectio nof this version https://ghc.haskell.org/trac/ghc/wiki/UnpackedSumTypes?version=32), but I removed those to reflect the current implementation and plans. We have plenty of time until 8.2 to improve the syntax. If no one else does it before me, I'll send an email to the ghc-devs and start a discussion about the syntax.
Might want to pop a bang in there too. `askUser !sim = ...` or `!sim' &lt;- ..`
From the outside, it looked like too much upfront planning and ambition rather than just getting out a bit that works and iterating. The normal reason projects fail. 
Thanks guys, this has been very informative! In the case that inspired my question, I need exactly such a horizontal composition operator for general traversals, but only where they are known to be disjoint. I suppose this isn't generally useful because one could typically write the traversal directly (but doing so isn't possible in my case for boring reasons).
Well I for one wish you luck getting your momentum back. I would love to see Haskell refactoring everywhere.
Does anyone have a good resource for describing the heap representation of haskell objects (CAFs, lambdas, closures, thunks, etc)?
Isn't focusing and polarity only relevant to classical languages?
What's the type of `(# case 0 | i #)`? Does it inject `i` into a binary sum, a ternary sum, ...? As you say, you'd need `(# case 0 in 4 | i #)`. 
`(0 of 4 | x)` seems fine :-) I haven't studied the parser, so I don't know what's easy to build on existing grammatical productions. My guess is `(|||_)` comes from TupleSections (`(,,,_)` expressions/patterns). 
What's polarity and focusing?
I take it you can tell that it isn't just that the (extremely complicated) `Simulation` state is getting bigger? Certainly the output makes it seem like something is growing. If I run it once I see e.g. Queue state: Current Status: QueueIsIdle Statistics: Idle: 2300 of 2300 (100.00%) Processing: 0 of 2300 (0.00%) Repaired: 0 of 2300 (0.00%) but if I run it 10 times, I see Queue state: Current Status: QueueIsIdle Statistics: Idle: 25300 of 25300 (100.00%) Processing: 0 of 25300 (0.00%) Repaired: 0 of 25300 (0.00%) Other statistics that are printed are similarly increased. I'm not sure what they mean though - is it just past history being recorded? The use of the progressive in `Processing: 0 of 25300 (0.00%)` suggests not. If they mean what they seem to say, this would lead one to expect memory use to increase in proportion to the number of times you press return - which is what seems to be happening. None of the obvious strictness inducing devices people are mentioning seem, including deepseq-ing the state after each major step, seems to alter the basic facts about the program. 
Once in a while I fondly remember Modula-2, which has separate INTEGER and CARDINAL types, the latter being non-negative. It may seem like the same design as int and unsigned int in C, but psychologically it wasn't, for me at least. Having a completely different name somehow seemed to prevent me from thinking of CARDINAL as an inferior subtype of INTEGER. I had to think and choose how to declare each ... um ... whole number variable. From my recollection, CARDINAL won most of the time. 
Try this https://hackage.haskell.org/package/mtl-2.2.1/docs/Control-Monad-State-Strict.html or this https://hackage.haskell.org/package/mtl-2.2.1/docs/Control-Monad-State-Strict.html depending on your preference to work with `mtl` or `transformers`.
&gt; What's the type of `(# case 0 | i #)`? Whatever you want it to be, obviously. Just like `(# i | | #)`which doesn't disambiguate between, e.g. `(# Int# | Int# | Int# #)` and `(# Int# | Bool | Char #)`.
I wish developers could extend the ghc parser (like lisp macros), to play around with different syntaxes. 
Hi, thank you so far. I tried out Control.Monad.State.Strict as supposed by the others with no success. Well, the profiling shows that 99.8 % of the memory allocation happens in execSimulationSteps (also in SimulationStep.hs) which is defined as: execSimulationSteps numberOfSteps sim = execStateT (replicateM (fromInteger numberOfSteps) step) sim Further the function step is responsible for 92.2% of the allocation. Btw where could these 7.6% go? The profiling output (removing the subtasks) is as follows. However the allocation of the other modules by far do not sum up to 7.6%: individual inherited COST CENTRE MODULE no. entries %time %alloc %time %alloc ticks bytes execSimulationSteps Data.Simulation.Production.Simulation.SimulationStep 313 5 3.0 5.2 99.8 99.8 60 81238800 _queueState Data.Simulation.Production.Processor.Type 391 11500 0.0 0.0 0.0 0.0 0 0 isInventory Data.Simulation.Production.Processor.Ops 390 46000 0.0 0.0 0.0 0.0 0 0 asks Control.Monad.Reader.Class 389 126512 0.0 0.0 0.0 0.0 0 0 filteredOnId Data.Simulation.Production.Processor.Lenses 388 138020 0.4 0.4 0.4 0.4 8 6624976 gets Control.Monad.State.Class 387 69000 0.1 0.2 0.1 0.2 2 3680000 modify' Control.Monad.State.Class 319 46020 0.5 0.8 1.3 1.8 9 13248000 step Data.Simulation.Production.Simulation.SimulationStep 316 0 73.0 71.9 95.1 92.2 1454 1128058440 Yeah the simulation itself should not grow that much. It's an Integer-variable counting the steps. I tried before changin that to an Int-value. With no change of the memory allocation. But I'll try out disabling the statistics that keep track of these values to ensure that's not the problem. I'll let you know about the outcoming as soon as I did that.
Hi, thanks for your input. I tried out Control.Monad.State.Strict with no success. I also used the force function before and tried it out again (with BangPatterns for sim') with no success as well. 
Could be that the RandomGenerator which I split quite often allocates a lot of memory?
Also currently reading through your 3 articles, giving me another refresher on this stuff. Hopefully it'll stick this time :) Came here to say that I like the terms `unroll :: Term f -&gt; f (Term f)` and `wrapUp :: f (Term f) -&gt; Term f` for their resemblence of unrolling and wrapping up a loop/carpet/cinnamon roll of `f`s, where the fixed-point would lie in the center of the snail.
It's not super-polished (and consequently not on hackage yet), but https://github.com/pseudonom/poly-graph sounds like it may be relevant to you. It basically lets you describe the graph of your database and then generate arbitrary nodes in that graph. We use it to pretty good effect at work.
I have [mentioned this before](https://www.reddit.com/r/haskell/comments/4m68zp/the_infamous_editoride_situation/d3ug3z0), but I think it's important enough to repeat again. If we want Haskell to have state of the art modern IDE support, we MUST put first-class support for these things directly into GHC. [This video with Anders Hejlsberg](https://channel9.msdn.com/Blogs/Seth-Juarez/Anders-Hejlsberg-on-Modern-Compiler-Construction) makes the point really well. If we don't tackle the IDE problem at the GHC level, then the IDE people will be on a constant treadmill of keeping up with GHC in terms of lexing, parsing, language extensions, etc. It makes no sense to duplicate things that have already been done in GHC. If we continue to do so, then we will forever have sub-par IDE support. Haskell should be the best language in the world for IDE support. But if we want to get there, we have to make a major shift in the focus of the compiler.
I'm not sure which part you've updated, but the original example still doesn't compile. Here it is as written in the question: 1. updateEntity :: Entity MyRecord -&gt; SqlPersistT IO (Entity MyRecord) 2. updateEntity (Entity entityId entity) = do 3. (Entity entityId entity) &lt;- updateAndRefetch entityId -- shadowing entityId &amp; entity 4. case (entity ^. parentId) 5. Nothing -&gt; return (Entity entityId entity) 6. Just pId -&gt; do 7. parent &lt;- Yesod.get pId 8. if (isJust parent) 9. then checkEntity (Entity pId parent) 10. else return (Entity entityId entity) A first thing I'll point out is that `updateEntity` takes too much information in. The type is: updateEntity :: Entity MyRecord -&gt; SqlPersistT IO (Entity MyRecord) But you never actually use the `MyRecord` part of the Entity. Instead, you immidately shadow `entity` in line 3. The type signature would more appropriately be: updateEntity :: Key MyRecord -&gt; SqlPersistT IO (Entity MyRecord) I don't know what `updateAndRefetch` really does, but I suspect that, in a similar way, it gives us back too much information. It looks like its type signature is probably: updateAndRefetch :: Key MyRecord -&gt; SqlPersistT IO (Entity MyRecord) But, I suspect that the key in the returned entity will always be the same as the key that the `updateAndRefetch` was provided. So, it could be better defined as: updateAndRefetch :: Key MyRecord -&gt; SqlPersistT IO MyRecord Line 4 is missing an `of`. Line 9 is wrong because `parent` has type `Maybe MyRecord`. The last four lines should be rewritten as: mparent &lt;- Yesod.get pId case mparent of Just parent -&gt; checkEntity (Entity pId parent) Nothing -&gt; return (Entity entityId entity) Or, we could inline it a little more: Yesod.get pId &gt;&gt;= \mparent -&gt; case mparent of Just parent -&gt; checkEntity (Entity pId parent) Nothing -&gt; return (Entity entityId entity) Or, if we turn on `LambdaCase`, we can get rid of `mparent` with: Yesod.get pId &gt;&gt;= \case Just parent -&gt; checkEntity (Entity pId parent) Nothing -&gt; return (Entity entityId entity) But none of those are what you're looking for because you're still explicitly doing the pattern match. What you want is `MaybeT`. This is found in the `transformers` library in [Control.Monad.Trans.Maybe](http://hackage.haskell.org/package/transformers-0.5.2.0/docs/Control-Monad-Trans-Maybe.html). In my rewrite below, I'm going to assume that `updateAndRefetch` has the type signature that I recommended instead of the one it had in your original example. updateEntity :: Key MyRecord -&gt; SqlPersistT IO (Entity MyRecord) updateEntity entityId = do entity &lt;- updateAndRefetch entityId mresult &lt;- runMaybeT $ do parentId &lt;- hoistMaybe $ entity ^. parentId parent &lt;- MaybeT $ Yesod.get parentId -- This line is basically unneeded though lift $ updateEntity parentId return $ fromMaybe (Entity entityId entity) mresult This example showcases three common functions that are used when working with `MaybeT`. The first is the `MaybeT` data constructor: MaybeT :: Monad m =&gt; m (Maybe a) -&gt; MaybeT m a The second is `lift`, provided by [Control.Monad.Trans.Class](http://hackage.haskell.org/package/transformers-0.5.2.0/docs/Control-Monad-Trans-Class.html). The type signature specialized to `MaybeT` is: lift :: Monad m =&gt; m a -&gt; MaybeT m a Finally, we have `hoistMaybe`. This is found in the `errors` package in `Control.Error.Util`: hoistMaybe :: Monad m =&gt; Maybe b -&gt; MaybeT m b The way you can look at these is that: - `lift` promotes something that doesn't have the `Maybe` short circuiting ability - `hoistMaybe` promotes a `Maybe`, which doesn't have the other monadic effect - `MaybeT` promotes something that has both Let me know if you have any other questions. 
I feel like OP was probably talking about a semantics respecting syntax isomorphism. Otherwise this is fairly obvious.
Informally: Polarity is the idea that you can associate a polarity, "negative" or "positive" with any type or term in the language. In particular, "positive" things are values that can be broken down as patterns, like "S (S (S Z))" or "True" or "(x, y)" (where x and y are also positive). "Negative" things are computations, like a function, or a continuation, or a thunk'd value. Focusing describes a proof-checking/synthesis technique, or more generally an evaluation technique. The basic idea is that some proofs are immediately constructible (like a proof of a natural number of the form S n, where n is another natural number) but other proofs require evaluation (like a proof of a natural number of the form S e where e is a computation that hasn't yet been run). Focusing rules would be a set of rules that tell you how to transition from one evaluation context to another, by picking an appropriate place to "do more evaluation". This can be seen as explicifying laziness/strictness: a "thunk" can now be thought of as a "negative" type that needs to be evaluated under focusing to make progress, whereas a "strict value" would have an entirely positive type. 
I think that's whats great about this answer; the question seems hard at first: "How do I make two languages the same?" But it turns out the answer is incredibly simple. Proofs like these are really cool.
yeah that was what i was looking for. But every attempt focused only on the compiler (like giving it example data etc.) and i am more interested how the compiler and the runtime could complement each other. I don't know how lightweight the haskell runtime is, but roughly my idea was that the compiler automatically finds points for implicit parallelisation but makes most of the "optional". The runtime then can perform some bookkeeping to gather statistics about code paths and can then enable these when one processor is idle. I would expect that the potential points where to parallelise are few, so the overhead for bookkeeping and duplicated methods would not be great, but since it would be activated in runtime based on statistics the hard (impossible?) problem of fully implicit parallelisation would get easier. I don't know whether it is possible to actually hot-swap code or whether some conditional the runtime could set would suffice. Since this idea is rather simple i would think that it would have already been tried (or the implementation is really hard, idk)
But if you also want to handle language C, you must: 1. Write a compiler from language A to language B. 2. Write a compiler from language A to language C. 3. Write a compiler from language B to language A. 4. Write a compiler from language B to language C. 5. Write a compiler from language C to language A. 6. Write a compiler from language C to language B. It gets impractical very fast. Note: This criticism obviously also applies to the interpreter approach as well. What you can instead do is translate languages to an intermediate format. That way you only need to write a translator for each language to the intermediate format and a translator from the intermediate format to each language. I believe three languages would be the break even point for that approach.
Also known as "let A, B or C be your 'intermediary format'." 
Good point. I stand corrected. Although I do think that using an intermediate format can be easier from a practical standpoint. 
Brainfuck. Just compile to/from brainfuck for both languages and you got a bridge.
This is not possible. ETA: C is a Turing complete programming language, but C goes well beyond what you can express in an *arbitrary* Turing complete programming language. You can't open a file in Brainfuck.
What's your take on the other comments in this thread? The compiler idea seems to fit the bill.
This reminds me that [tombstone diagrams](https://en.wikipedia.org/wiki/Tombstone_diagram) exist. As Vulpyne mentioned, you can go from M × N (O(n²)) implementations to M + N (O(n)) by adding an IR, which is very useful. Also, there’s a notion of distance between languages, almost like a metric space. And it seems like the difficulty of implementing a compiler from one to the other increases quickly with distance, so it’s often easier to write two compilers (from `Source` to `IR` and `IR` to `Target`) than one monolithic compiler (from `Source` to `Target` directly).
[Patch](https://git.haskell.org/ghc.git/commitdiff/c079de3c43704ea88f592e441389e520313e30ad) by Dominik Bollmann.
Let's think of programs as functions of the universe. They take the state of the world as input, and produce a new world as output. The primary difference between C and brainfuck is that brainfuck doesn't have as high a level of access to the universe; it can't get at all the same parts as C can. But if you slap a serialization interface in front of it that serializes the entire universe, and deserializes an output universe, then the turing-complete-ness of brainfuck means that brainfuck can interpret the serialization in all the same ways that C can. Thus, it would be capable of all the same things as C. This does, admittedly require glue code: the serialization interface. But you can slap the same interface in front of every single programming language in order to produce a true isomorphism between them.
As per my other comment: If you pass the universe through the serialization interface, yes you can.
Whatever side-effects a construct in language A has, you can just declare the corresponding construct in language B to have them, too. Basically it is an implementation detail.
This is also true, which is why you have to think of programs as pure functions that take and produce universes. In this sense, you can definitely produce isomorphisms between any two languages by writing compilers between them, as long as you're able to find a way to appropriately pass them the universe.
What "corresponding construct"?
What's the input read, or output produced?
I somehow thought they were talking about Turing complete programming languages.
In that case, the serialization/deserialization layer would have to be something which modifies memory directly before and after running to pass and retrieve universes. Whatever the execution context of the language, there must be *some* representation of the computation being done. In an abstract sense, this representation defines the interpretation of the language and its turing-complete-ness. Manipulating this is how programs are able to do anything at all. It's no different in real C, except that C is faster and more practical.
I guess in SKI calculus, as abstract as it is, doesn't even really need a serialization. A lambda expression can just take a universe and return a different universe. Use SKI to write arbitrary programs.
C is not a Turing complete programming language?
It is, but when people say that they mean the purely computational subset, not the file I/O. Anyway, this discussion about words is pretty useless.
At this point you're surely just being intentionally dense. It's incredibly clear what was meant and you're objecting to completely tangential properties.
I had high hopes for haskell-ide but with the pace of GHC feature development the deck is stacked against it in the short-term and long term maintenance is pretty much impossible. I've said this elsewhere but I really hope GHC slows down so tool developers stand a snowball's chance of catching up.
&gt; Furthermore, only knowing that the number is non-negative is not enough most of time. Usually you need an upper bound as well. Yup, for this case there's: https://github.com/pseudonom/finite-typelits-bounded It also uses `minBound` and `maxBound` which solves your underflow concerns and often seems like the right semantics. 
Oh, but maybe the deepseq'ing is specious. The state type has a number of fields that look like so , _simulationOrderPriority :: Order bs pt -&gt; Order bs pt -&gt; Ordering or turn out like that on closer inspection. 
I think as /u/mightybyte said above, IDE support needs to go into GHC as a first class feature there.
Wrong bite ;)
You seem to have lots of unnecessary laziness in your code. What's the rationale behind having so many lazy fields in your records? For example this data type: data Simulation g id bs pt = Simulation { _simulationRandomGen :: g , _simulationGranularity :: !Time , _simulationPeriodLength :: !Time , _simulationOrderPool :: OrderPool g id bs pt , _simulationProcessors :: [Processor g bs pt] , _simulationCurrentPeriod :: Time , _simulationCompletedOrders :: [Order bs pt] -- ^ will be removed at the end of a period , _simulationWaitForShipUntilDue :: Bool , _simulationShipFullPeriodOnly :: Bool -- ^ wait until the order is due before shipping , _simulationOrderPriority :: !(Order bs pt -&gt; Order bs pt -&gt; Ordering) , _simulationVerbose :: Bool , _simulationIOSettings :: SimulationIOSettings bs pt } What's the point of `_simulationCurrentPeriod` being lazy, for example? And if I am not mistaken the strictness in `_simulationOrderPriority` has no effect. Then you have things like `!(Maybe (ProductType pt))` ad `!String`. The latter is pretty much useless, as this won't even guarantee that the first character is evaluated. The maybe type will only have its constructor tag evaluated, not it's content. My advice: don't use lazy record fields unless you know exactly what you are doing and have a good reason for doing it. Use strict types where it makes sense. Replace `String` by `Text` and lists by vectors when possible. And be extra careful with `Maybe a` and tuples. They are **not** what they seem to be to many beginners. data Simulation g id bs pt = Simulation { _simulationRandomGen :: !g , _simulationGranularity :: !Time , _simulationPeriodLength :: !Time , _simulationOrderPool :: !(OrderPool g id bs pt) , _simulationProcessors :: !(Vector (Processor g bs pt)) , _simulationCurrentPeriod :: !Time , _simulationCompletedOrders :: [Order bs pt] -- ^ will be removed at the end of a period , _simulationWaitForShipUntilDue :: !Bool , _simulationShipFullPeriodOnly :: !Bool -- ^ wait until the order is due before shipping , _simulationOrderPriority :: Order bs pt -&gt; Order bs pt -&gt; Ordering , _simulationVerbose :: !Bool , _simulationIOSettings :: !(SimulationIOSettings bs pt) } Also make sure that `OrderPool`, `SimulationIOSettings`, and `Processor` have strict fields. Haskell is, for better or worse, lazy by default. The evaluation of thunks is in many cases your own responsibility. Ignoring this and programming like Haskell was strict is a guarantee for space leaks. With some care you can create complex hierarchies of data types where everything is strict and where you can always be sure that there are no hidden unevaluated thunks anymore, as soon as the value is evaluated to at least weak head normal form.
very clear. Thank you for posting this.
Hi thx. I tried the Control.Monad.State.Strict with no succes. What ought the second link to be (they are both the same :/ )? 
[removed]
Facebook posts are not allowed on /r/haskell. If you believe your post should be an exception, please send the mods a modmail with a link to this post for further review *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/haskell) if you have any questions or concerns.*
Yes I did. However using the Strict pragma seems to solve the problem. At that point that confuses me a little, but it seems to work, see https://www.reddit.com/r/haskell/comments/4u1y1l/spaceleak_for_uknown_reason_in_statemonad_with/d5nj3e9
Ok, using the Strict-pragma is seems to work now. See: https://www.reddit.com/r/haskell/comments/4u1y1l/spaceleak_for_uknown_reason_in_statemonad_with/d5nj3e9
It seems to work when I use the Strict pragma, see: https://www.reddit.com/r/haskell/comments/4u1y1l/spaceleak_for_uknown_reason_in_statemonad_with/d5nj3e9
I can't find any info about registration, nor price for the event. How much is it and is the registation still possible?
What is your honest conclusion after writing this about haskell as a good language for writing video games? It's more of an experiment, right? No one will be writing 3D games in it, I'm guessing. 
haskell-src-exts, ghc-mod, intero backend (ghci-ng)
&gt; I initialze them with undefined in the respective Ops.hs functions (e.g. newOrder), and replace them later on during intialization of the data structures That's definitely not idiomatic Haskell. Turn them into strict Maybe's. Initialize them to Nothing. If they are ever, at any point, undefined or invalid, then Maybe is a good type to use to represent that fact. Use of undefined is not. The compiler can't help prevent you from making bugs when you use undefined instead of Maybe.
That's my point.
I feel that something like LLVM IR may be more practical
I'm kind-of curious as to why you split off the RenderPreserve behaviour off of the scale/translate etc. commands.
have a look at [hsqml](https://hackage.haskell.org/package/hsqml), or [qt](https://hackage.haskell.org/package/qt)
Not to mention /r/haskellquestions, which is the more appropriate reddit forum for these types of beginner questions. It's not terribly lively, but most questions still receive good answers within a reasonable amount of time. Of course, the rest of what you write applies there as well: more info needed to help effectively.
I totally agree. I also believe that Visual Studio Code will allow us to focus only on providing a proper language service written in Haskell and get a really slick IDE for free. See https://github.com/Microsoft/language-server-protocol and https://code.visualstudio.com/docs/editor/editingevolved (overview of IDE features).
I sometimes face the same error. The problem is you are using -&gt; instead of :&gt; in the type definition of the first route ( right after "genres"). I sometimes forget and type -&gt; too
Really a pattern in software is anything that is beyond a single expression. It involves two or more that can not be re-combined to create a single functional expression. Then in functional terms, every pattern is an antipattern. Funcional programming is about creating programs that are a single mathematical expression using algebraic or monadic combinators. And this is only possible when the main elements are functions. it is impossible when they are classes (see my comment somewhere else)
Yes. Of course. But. Brainfuck.
I think it will take a while for the change to percolate into better design. First, all the bugs need to be ironed out for it to be dependable, so it will be at least a GHC release AFTER the initial dependent types feature. The major everyday benefit of DT requires a deep refactoring of core abstractions, so that will take time too. I think the most immediate benefit will be for (no surprise) expressive DSL, which are actually quite common in any modest codebase, but maybe not the "core" of the project.
Care to sum up your understanding/feeling of why DT is so far away?
Even if there were, a lot of people frown on this lawless use of typeclasses, though some of those people will hold their nose and write such typeclasses on occasion.
When would you use something like this? An alternative is to pass in a lens...
Since you mentioned idris... would it be possible to take Idris' typechecker and put it into GHC? Completely subvert the "fix GHC's typechecker" step 
The hypothetical `Has a b` has no fewer laws than the commonly used `HasX a`, which is also none.
Please watch the talks or read the papers. There are good explanations and it's not for lack of effort. The best summary I can offer is that "totality" is *really* really important for DT languages and the semantics[1] of Haskell aren't really compatible with that. [1] Yeah, I know. Not really well-defined, and that's probably also part of the problem.
[removed]
Allow me a little rant, because I believe that dependent types are somewhat overhyped for general software development. I've only done a smallish project in Coq and some reading, thus am not exactly an expert (and I'd be happy to be corrected on any mistakes I make in the following), but even then some of the cracks surface quite quickly. **a) Induction and Coinduction** Pervasive laziness in Haskell means that we like to use a lot of coinductive types. But this is really annoying for proof purposes because coinduction, despite the nice duality with induction, is a lot less pleasant to work with. So, we can either (1) live with the resulting complications; (2) pretend that our types are really inductive and hope for the best; or (3) get used to working with strict data (and thus loose some of the natural composability of laziness). Neither option strikes me as particularly appealing. **b) Proving Is Hard** Most people find it fairly difficult to construct the informal proofs commonly used in mathematics. Formal proofs are a lot harder still because the machine will force you to prove every minute detail, whereas a human would find many things to be obviously true. This burden can be lessened by automation, decision procedures for fragments of the underlying logic, integration with SMT solvers and the like, but all of these would take significant effort to be integrated into Haskell (or invented in the first place). I would therefore assume that only small parts of the code for any nontrivial programme will be proven correct. While this is obviously better than the status quo, it also means that the level of assurance afforded by dependent types (which is their main advantage compared to tests) won't typically be too high. **c) Termination** As is well-known, a dependent type system is inconsistent (i.e. everything can be proven) unless all computations terminate, which is obviously not the case for general Haskell. There are a couple of options for dealing with this problem: ca) Leave it to the programmer to ensure that everything, or at least the terms they are using as proofs, terminates. This obviously makes it relatively easy to screw things up in more complex proofs; more so because 'proof code' is often never run (only typechecked), so you have to carefully look for infinite loops. cb) A syntactic termination check ensures that recursive functions are only called on syntactically decreasing arguments. Coq, Agda and Idris all use this method, but obviously no computable check can ensure that every terminating programme is recognised as such. Thus, there is a tradeoff between keeping the termination checker simple (thus bug-free) and ensuring that many 'typical' recursive functions can be encoded without jumping through the hoops of well-founded recursion. The situation for coinductive types (i.e. standard Haskell lists and such) and their analog, the productivity checker, is even worse. cc) Size types and other research-y stuff which afaik aren't ready for implementation in a mainstream-ish language. **d) Proof Irrelevance and Efficiency** When you start carrying around proofs with your data, you typically want to erase the proof terms after typechecking so that they don't incur significant performance penalties at runtime. As far as I know, how to do this completely is not entirely clear yet, although Coq's extraction mechanism seems to work reasonably well (and Idris is also working on this, right?). Moreover, you'd usually want nice definitions for naturals, integers and rationals, but these are horribly inefficient. So you either need to work with a model of the actual machine representations (very ugly) or have the compiler automagically replace the slow representation by a fast one (invalidating your model, but hopefully not in a terribly bad way). **e) Legacy Support** This point is very speculative, but I strongly dislike the current flavour of 'almost-dependent' type-level programming in Haskell. It obscures the simplicity of a proper dependent type system with all its proxies, singletons, equality constraints and derived hacks. So, I hope that the eventual implementation of full dependent types will make a lot of that unnecessary, but I'm not sure how inelegant the design has to be for compatibility reasons. With all that said, I definitely applaud the heroic work that goes into extending Haskell with dependent types. I believe that they can be a good tool for certain purposes, but they are not the panacea they are sometimes presented as.
Could you explain why coinduction is unpleasant for proof purposes?
What does "fully dependent types" even mean for something like Haskell? You would have to rework the core language entirely. I think the best Haskell will ever get is ad-hoc methods. Those methods will get more and more sophisticated (and more and more ad-hoc). But it is ultimately not a dependently-typed language. 
&gt; [...] I strongly dislike the current flavour of 'almost-dependent' type-level programming in Haskell. It obscures the simplicity of a proper dependent type system [...] I strongly agree with this sentiment. For a long time, now, I've felt that adding dependent types to functional languages is roughly the equivalent of adding OOP constructs to traditional languages (a la Perl, PHP, Python). It's ad hoc and messy. The end result is often still useful, but useful despite itself. It adds needless complexity to the language. &gt; Proof Irrelevance and Efficiency I feel this would be a huge deal-breaker for Haskell given the culture. Haskell is one of the few success stories disproving the common wisdom that "functional languages are slow". The compiler is meticulously tuned to churn out speedy code. 
If you want fully Dependent-Type types, why not use Idris, Agda, Coq, or F*? If those languages aren't what you want, what makes you think Haskell will overcome their shortcomings? Haskell will probably always try to do some type inference, which is at odds with the concept of fully dependent types. 
The project "native metaprogramming" in [summer or haskell](https://www.reddit.com/r/haskell/comments/4kp6zg/summer_of_haskell_2016_accepted_projects/) goes a bit in that direction. 
I thought fully dependent types was an undecidable problem.
Can you give a real example of Expr? Is expr some data structure or just new name for string?
&gt; for a sum type this leads to partial functions, something I want to avoid Good idea! &gt; it seems to go crazy when playing with indexed recursion schemes Please explain. 
Registration will open in a few days. HAL itself will be very cheap (approx. 30€, cheaper for students).
[removed]
No, it won’t. After some consideration, I now think it was an overstatement that I should delete. In general, I just need to carry the extra parameter around: data BinOp f = BinOp f f f deriving Functor A bit clumsy, but not that much clumsy. : )
Would this be avoidable using custom type errors in ghc 8? I ask because I am not completely sure about the limitations.
Is it still unsatisfactory? NB the problem you are trying to solve is equivalent to "the record problem".
If I understood you correctly, then you can achieve this with `newtype`s: newtype Left a = Left a newtype Right a = Right a newtype Op a = Op a data Ast = Binop (Left Ast) (Op Ast) (Right Ast) transform (Binop (Left l) (Op o) (Right r)) = undefined Note, that you can also derive `Functor`, `Foldable`, and `Traversable` instances for these `newtype` wrappers via the corresponding ghc extensions. - *edit:* Your choice of syntax suggests that you might want the field names as type level strings. You could use `DataKinds` and `PatternSynonyms` for this: {-# LANGUAGE DeriveFunctor, PatternSynonyms, DataKinds, TypeOperators, UnicodeSyntax #-} -- A functor `f` tagged with a phantom type `t`. newtype (f :@: t) a = Tagged (f a) deriving (Eq, Ord, Read, Show, Functor) -- Tagging a functor with the type level string "Field1". type Field1 f = f :@: "Field1" -- Introduce a new pattern, which behaves as the `Tagged` pattern, -- but specialized for the "Field1" tag. pattern Field1 :: f a → Field1 f a pattern Field1 a = Tagged a -- Example usage: getter for the int stored in the Const functor. getInt :: Field1 (Const Int) a → Int getInt (Field1 (Const i)) = i Note, that the exhaustiveness checker can't work with pattern synonyms yet, so you get a *pattern matches are non-exhaustive* warning for `getInt`.
Would somehow run it and feed it back obliviate that then ? I really wish we would have a way to talk 1st class about phases. (I think it's even important internally, there are logical phases in our programs)
In my opinion, the biggest thing it would simplify is compilers, which lean heavily on these sorts of advanced type system features. For other sorts of programs I think people are too quick to dismiss non-dependent solutions.
On the other hand, we could finally have a deserialisation function that can decide the return type based on the data it's given, making the types more durable and useful.
You could try [Haste](http://haste-lang.org/docs/) + haste-perch + haste-webgl
One annoyance is that the 'built-in' notion of equality is very rarely sufficient. Thus, you have to define a bisimulation, prove that it is an equivalence relation, and henceforth parameterise everything over that equivalence relation. Specifically, you have to prove for each function on your coinductive type that it respects the bisimulation, which is usually not that hard, but gets old quickly. For inductive types, on the other hand, you can usually get away with 'built-in' equality in the sense of syntactic equality of normal forms, which is much nicer to work with because every function respects it automatically. Further, productivity is a major issue. For consistency, you want to guarantee that the next constructor of your type can always be forced in finite time. This means that even a benign `filter` on potentially-infinite lists is problematic because it could filter out every element, in which case you'd never produce a list constructor. With inductive types, you run into similar issues as well, but (a) typically not so soon and (b) there is a standard technique called well-founded recursion which is comparatively ok to work with. For coinductive types, you need very heavy machinery; see [here](http://www.chargueraud.org/viewcoq.php?sFile=softs%2Ftlc%2Fsrc%2FLibFixDemos.v) for examples which use non-constructive techniques and are still quite horrible. For these reasons, I'd assume that Haskell doesn't get a mandatory productivity checker, which leaves us open to infinite loop bugs, but that strikes me as the lesser evil. Lastly, there are currently some quality-of-life issues with Coq's coinduction support, and since Agda and Idris don't seem to fare any better, I'm unsure whether there is a satisfying solution. Specifically, many common automation tactics don't work well within proofs by coinduction, and you have to employ various hacks to work around this. However, one might hope that these issues could be solved with smarter automation, and Haskell won't get tactics any time soon anyway.
Lenses might be worth looking into. A `Lens' a b` is a first-class value you pass around that indicates a particular field of type `b` contained in a record of type `a`. They have many nice properties including composition with the standard `.` operator. Given a value `theLens :: Lens' a b` you automatically get a getter `(view theLens) :: a -&gt; b` and a setter `(set theLens) :: b -&gt; a -&gt; a`. 
as if there is just that one point in possible design space. This is not what Haskell intends to be; &gt; Languages such as Coq and Agda avoid the * : * axiom because it introduces inconsistency, but that is not an issue here. The FC type language is already inconsistent in the sense that all kinds are inhabited. The type safety property of FC depends on the consistency of its coercion language, not its type language. &gt; If a consistent type language were desired for FC for other reasons, we be- lieve that the ideas presented in this paper are adaptable to the stratification of * into universe levels (Luo 1994), as is done in Coq and Agda. http://www.cis.upenn.edu/~eir/papers/2013/fckinds/fckinds.pdf
You could instead try to generate Haskell Core from Idris Core (called TT). That way you have the benefits of the Haskell runtime and libraries (provided you also implement an FFI) with the Idris typechecker. It turns out Idris is very welcoming when it comes to this kind of things. There's a bunch of experimental or toy generators for [PHP](https://github.com/edwinb/idris-php), [Bash](https://github.com/mietek/idris-bash), the [JVM](https://github.com/idris-hackers/idris-java), [OCaml IR](https://github.com/stedolan/idris-malfunction), [Erlang](https://github.com/lenary/idris-erlang), [Ethereum](https://github.com/vindaloo-thesis/idris-se), etc. as well as two official code generators for C and JavaScript.
Then you wouldn't be able to havr the infinite list type: `` IList = IList Int IList `` The tail would be `Has IList IList`. But I don't know if that is desireable.
Plus performance of the application. 
Being a user of both TLA+ and Haskell, I have to say I strongly disagree with the idea that TLA+ is "more accessible and user-friendly than Haskell." The two are basically incomparable, but, IMO, there is a reason it is a lot easer to get programs than specs from people.
&gt; there is a reason it is a lot easier to get programs than specs from people. I think that's just because most developers already know what programs are and what they're supposed to do, while very few know what formal specifications are and what they're supposed to do. In any event, a developer would become productive in TLA+ within two weeks at most, and would master it in a couple of months. I don't think the same could be said for Haskell (or Java or pretty much any PL out there). There's just so much less to learn in TLA+. The main obstacle, IMO, is unfamiliarity with the goal. But I think that [Amazon's experience with TLA+](http://research.microsoft.com/en-us/um/people/lamport/tla/formal-methods-amazon.pdf) -- especially how quickly it spread through the company (and now Oracle is undergoing this process, too) -- highlights TLA+'s user-friendliness as well as its contribution. I hadn't had a more pleasant experience with a new language other than, possibly, with some Lisps. In any event, if you compare TLA+ with other specification languages and formal verification tools (say, Z, VDM, PVS, ACL2, Isabelle or Coq) it wins on the friendliness front hands down. I guess you may find Alloy more or less friendly, depending on your personal preferences. BTW, what do you use TLA+ for?
Richard Eisenberg (author of `-XTypeInType`) here. So many wonderful points to respond to! So I've done it in a blog post: https://typesandkinds.wordpress.com/2016/07/24/dependent-types-in-haskell-progress-report/ Feel free to continue the conversation here, though.
Thanks very much for writing that post! I'm very much looking forward to reading your thesis when you get around to uploading your final PDF. Good luck!
Why do compilers lean on those advanced type-level stuff?
Definitely the easiest for simulation
`Reader r` has multiple instances, including a Monad instance. And if `r` is a Monoid, `Reader r` also has a Comonad instance. I haven't read all of Tomas Petricek's amazing document yet, but presumably monads model effects and comonads model coeffects, so perhaps it would make more sense to compare the Reader Monad with the Reader Comonad. And to a first approximation, there is no difference: it's the same Reader type, we just have a [bad habit](https://www.reddit.com/r/haskell/comments/2tbbxh/io_monad_considered_harmful/) of calling things which have a Monad instance "the Thing Monad" instead of just "the Thing". It just so happens that Reader supports both monadic and comonadic operations. You can even use both kinds of operations in the same computation: import Test.DocTest import Control.Comonad type PrimeReader a = [Int] -&gt; a knownPrimes :: PrimeReader [Int] knownPrimes primes = 2:primes -- add 2 to make sure largestKnownPrime is well-defined largestKnownPrime :: PrimeReader Int largestKnownPrime = maximum &lt;$&gt; knownPrimes divides :: Int -&gt; Int -&gt; Bool divides n k = (n `mod` k) == 0 isComposite' :: Int -&gt; Bool isComposite' n = any (divides n) [1..n-1] -- If a composite divides n, then a prime (namely the composite's factors) -- divides it too. So we only need to check the primes. isComposite :: Int -&gt; PrimeReader Bool isComposite n = any (divides n) &lt;$&gt; knownPrimes nextPrime :: PrimeReader Int nextPrime = do k &lt;- largestKnownPrime go (k + 1) where go :: Int -&gt; PrimeReader Int go k = do r &lt;- isComposite k if r then go (k + 1) else return k -- | compute the first n primes -- -- &gt;&gt;&gt; takePrimes 10 -- [2,3,5,7,11,13,17,19,23,29] takePrimes :: Int -&gt; [Int] takePrimes n = extract go where go :: PrimeReader [Int] go = do primes &lt;- knownPrimes if length primes == n then return primes else do prime &lt;- nextPrime extend ($ [prime]) go I would have written a shorter example, but I did not have the time :) A Reader computation is one which can be influenced by a parameter of type `r`. It supports a few operations: 1. `return` ignores `r` and produces the given value. 1. `(&gt;&gt;=)`, used above via the do notation, runs part of the computation and then uses the result in the remainder of the computation. 1. `extract` runs the computation using `mempty` as the `r` value. 1. `extend` runs a sub-computation using a modified context, here by adding `prime` to the list of known primes. That's just a list of operations supported by `Reader`. They could have been named differently, and indeed `Reader` has another function called `local` which is similar to `extend`. But it just so happens that there are many more type constructors which support operations similar to `return` and `(&gt;&gt;=)`, so the Monad abstraction is useful to write generic code which works with all of them, and the same thing is true with `extract`, `extend`, and the Comonad abstraction. 
When I use this pattern, I like to provide it in term of a lens: ``` class HasX a b | a -&gt; b where accessor :: Lens' a b ``` because at least then you have the three lens laws to lean on.
hm, seems to work for me...
Can someone eli5 what fully dependant types are?
&gt; I’d be happy to hear about problems you encounter. Schadenfreude is key to being a good programmer ;)
What can this do that the play/playIO functions from gloss can't?
How is `Expr` defined?
Thank you very much; that is some excellent information! I now have a much clearer picture of what Haskell's eventual support for dependent types is going to look like.
i think you have to qualify import qualified ExprT as E import qualified StackVM as S E.Mul -- the mul out of E S.Mul -- the mul out of S hth
In traditional language design, name resolution (i.e. renaming) is done in its entirety prior to any typechecking/inference. So with a naive approach like this there is a "phase" problem where there's not an obvious way to get information from typechecking back to the earlier renaming phase. TDNR https://prime.haskell.org/wiki/TypeDirectedNameResolution is one of many proposals to use types to disambiguate names. The state of the art wikipage is https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields but you should grab someone who has been following the discussion and get them to explain to you what's going on.
Thanks Richard!
I highly recommend a presentation by Edward Kmett: [All About Monoids](http://comonad.com/reader/wp-content/uploads/2009/07/AllAboutMonoids.pdf)
It's so awesome that you took the time to respond to [my query](https://www.reddit.com/r/haskell/comments/4uamzt/countdown_to_fully_dependent_types/) and give us a rough timetable. I've been routinely astonished at how fantastic the Haskell community is with networking together and supporting each-other as the ecosystem evolves. Thanks again for the elaboration and detail.
Say I have a machine that converts bananas and milk into milkshakes. Haskell's compiler guarantees that only bananas and milk can be put into the machine. Normally any old banana and any amount of milk is allowed in the machine. Dependent types allows me to now say that only bananas that are 4 to 5 inches long and 2 to 2 1/2 cups of milk can go in this machine.
Here's a slightly vague answer: they'll make our types *more expressive*. There are things—simple things—that I would like to say in my types but *can't*. Either they're just impossible or, perhaps more likely, it's too difficult to express and the resulting types too ugly. Dependent types will help there. It's hard to come up with good details because I don't have practical experience with dependent types—especially as Haskell will have them—but I have some ideas. For example, I have a library called [`modular-arithmetic`](https://hackage.haskell.org/package/modular-arithmetic-1.2.1.1/docs/Data-Modular.html) that lets us have types like `Int / 5` for integers modulo 5. It's quite useful—it makes my intentions clear and keeps me from forgetting to wrap my arithmetic operations—and, in the common case, totally usable in "normal" Haskell. However, what happens when I only know my bound at runtime? That's not an uncommon occurrence. It's a pretty natural extension of my modular arithmetic type, which I have already found useful, but there's no *good* way to do it right now. Another library I've wanted to write is the same thing but for words of different sizes. I want to have types like `Word 18` and `Word 36` and so on. Moreover, I'd like my library to automatically choose the smallest underlying `Word` it can to store these: a `Word 18` only needs 32 bits while a `Word 36` needs 64. If you go past a certain limit, it would just use `Integer` or whatever. This would be trivial to do at the value level with a normal function: size :: Int -&gt; Type size x | x &gt; 0 &amp;&amp; x &lt;= 16 = Word16 | x &gt; 16 &amp;&amp; x &lt;= 32 = Word32 | x &gt; 32 &amp;&amp; x &lt;= 64 = Word64 | otherwise = Integer Now, maybe it's just my lack of experience with type families, but I just couldn't figure out how to encode this pretty simple relationship. And even if I *could* have figured it out, it would have been pretty awkward—certainly uglier than the function I expressed above. But the underlying idea is simple and so the code for it *should* be simple too. This is the sort of thing where, I think, dependent types will really shine although, again, I don't have enough experience to know whether it's actually possible and what it would look like.
Dependent types unify and simplify a whole host of type system extensions. With dependent types you can program types just like you can program values. Among other things this allows you to do formal verification of your programs by using the type system as a logic.
Really good explanation! The post on semigroups (linked in the beginning) especially helped me grok the last piece of this for me. The concept isn't even that hard to understand, especially with all the examples the author gives.
That was great! Thanks.
&gt; However, what happens when I only know my bound at runtime? That's not an &gt; uncommon occurrence. It's a pretty natural extension of my modular arithmetic &gt; type, which I have already found useful, but there's no good way to do it &gt; right now. Sorry, I lost you here. What's the natural extension and why is there not a good way to do it?
&gt; However, what happens when I only know my bound at runtime? The [reflection](http://hackage.haskell.org/package/reflection) package can help with this. 
The reason why I want to do this is that I have a (mealy-style) state machine whose transfer function has the signature transf :: State -&gt; Input -&gt; (State, Output) where State is a *huge* `Vec`tor (basically a size-indexed linked list). Each call to `transf` only changes a tiny portion of the `State`, but even that part is computationally intensive (i.e. has thunks in it). The `Output` will be examined downstream and may cause the forcing of those thunks eventually, but only if those are in the *active* part, as the majority is *dormant*. I may have several thousand calls to `transf` in a second, so I cannot spend the effort to `deepseq` the entire `State`. But the funny thing is that scrutinizing `Output` will usually only lead to forcing the same part of `State` (i.e. visiting only the array elements under the same indices) each time. However I do not know (from inside the `transf` function, at least without actually analysing `Output`) which parts of `State` will be demanded. So I am interested in a parallel strategy that looks at the old `State` and the new `State` and forces all thunks in the new state at array positions where the old state has WHNF values, **but not more**. Having the *most probably* demanded vector entries forced early pays off (core-utilisation/latency-wise) downstream in my pipeline (or so I hope). You will probably ask why I don't choose a *better* data structure. This is an FPGA software simulation written in the [HDL CLaSH](http://clash-lang.org) so I am married to the provided types somewhat.
This won't work, as I do not know `n` at the point where I intend to run the strategy. My situation is also not a simple list but a nested `Vec`tor, so there is more branching going on, and a plain number won't do. See my response to /u/tinchos for a longer explanation.
Did you ever report back? I'm still interested in how that went.
It might be too much, but it could still help: http://blog.sumtypeofway.com/an-introduction-to-recursion-schemes
Looks interesting, though how would one determine what the user is clicking or hovering, given coordinates?
Yes. However, I think this is fundamentally similar to creating a data BinOp = BinOp using record syntax: both require “standalone” data type definitions. I somehow don't like it…emotionally, irrationally.
I don't think we should give up so easily on compile-time termination checking. We cannot, and should not, have automatic termination checking for every function. But with some additional work, and perhaps some new syntax and some help from the compiler, it should be possible to write compile-time termination proofs for many or even most Haskell functions.
I ask because while learning Haskell was very satisfying initially, I feel like when I'm actually in the trenches I find myself often doing things that would've been easier in an imperative language. I'm curious as to what your experiences are!
1. Haskell was the first functional programming language I've learned, so with Haskell I discovered the exciting new world. 2. Haskell helped me to understand that the most problems in programming are actually type problems. If you use correct types you will avoid most of the problems. 3. Haskell has definitely changed the way I write code in other languages. I try to write cleaner functions with no side effects now. 4. Thanks to Haskell I've started to learn some math. I use Haskell in some personal projects and to make some small utilities. I don't use it in my daily work because I don't have a full control of code.
Well, the only thing that I can think of is that you make a custom Strategy for Vector, something like simpleStr :: Vector Int -&gt; Vector a -&gt; Vector a where sparks only those computations whose index is in the given list, and leave the rest untouched. In fact, once you know what indices you need to update, you can spark one update computation for each one alone. I don not think there could be any way to know is an expression is in whnf or not (there is always IO)... So you should keep the positions that you may have changed and spark only those.
&gt; Do you use it (or the lessons you learnt from it) in your daily work in a way that justifies the time you spent learning it? No.
&gt; Haskell helped me to understand that the most problems in programming are actually type problems. If you use correct types you will avoid most of the problems. I agree with you, but somehow this sounds like "if all you have is a hammer, everything looks like a nail." &gt; Haskell has definitely changed the way I write code in other languages. I try to write cleaner functions with no side effects now. Indeed. I tend to separate pure functional code and side-effecting code. This has made it so much easier to unit test code also.
Interesting, thanks a lot. 
This relies many times on the ecosystem you work in. If you work with very imperative libraries or frameworks, you will have to do many things imperatively. Try writing small parts of your programs in functional style: * Try to reduce state as much as you can. If you initialize a variable/flag and then modify it several times, it can likely be refactored into a fold operation. * Replace your loops with maps and folds when it makes sense. It will happen more than you think. * Try to separate logic and state/IO. * Try to keep as much as you can immutable. Modifying state only at the DB level.
&gt; but somehow this sounds like "if all you have is a hammer, everything looks like a nail." For me it sounds like "if you have a nail, you need a hammer, not a screwdriver".
The natural extension is that he wants to have an `Int / n` modulo type, where n is known at runtime (f.x. it's read from a file). Since he wants to keep expressing the modulus in the type he needs dependent types (so that a runtime value can appear in a type). 
One of the big and quick wins for me was that Haskell type declaration syntax is so simple and consice that it makes an excelent pseudo code for talking about types in any language
I am not an expert by any means, so please take my words with a large grain of salt. I am still learning Haskell (by playing with a few side projects in my copious free time). To me, as /u/thesz said, it is a thinking tool. My day job is in C and low level systems. However, I find Algebraic Data Types very useful when thinking about a potential solution to a problem. I also find the Haskell emphasis on designing programs that satisfy a few algebraic properties (and hence has a few laws associated with them) to be extremely useful. I have often failed to find good algebraic properties in my programs but at least I try to think about them now. Ha! I am also careful about side effects. Surprisingly, with a lot of care, one could write neat programs in any language (like C) by trying to limit mutability to a few functions. There is no type system/compiler support, but one could still leverage the learning from Haskell to think and write better programs.
Like. Alternative title: "Overcoming Boolean blindness with Smart Constructors".
I agree. Whilst I do get a kick of out writing idiomatic functional Haskell code, I try and express it _correctly_ first (and any tests that are necessary), and then refine through iteration thereafter. Ultimately I think done is better than perfect!
It would take ages to be adopted. It's like situation with *Safe Haskell*, but even worse...
I have a question.. everybody knows that DT would kill Haskell's type inference, but is it proven (like, mathematically) that is not possible to do (any) type inference with DT or is that nobody has discovered a way to do it yet?
I use Haskell for work, so I suppose that is a "yes".
Very few packages have *Safe Haskell* annotations. And even GHC sometimes infers that modules are *Safe*, it's something which could change, which in turn would cause compilation issues downstream. For recent real-world example see: https://github.com/ekmett/pointed/issues/17 There was change in `bifunctors` which made them unsafe, caused `semigroupoids`to be also unsafe, and that in turn broke `pointed` which cared to be `Safe`. Situation would be different if there won't be any automatic termination checking, but to make everything to be properly annotated (and maintained) up to point where application development starts... I won't hold my breath. --- Also only time will show, how many things are actually terminating (IIRC not as many type families are actually injective as we thought), and how we'd express pre-conditions for that. But it would be definitely cool, if GHC would know that functions are terminating. Then we could force inliner to inline recursive functions (e.g. for generic programming, where we write recursive functions, yet over finite structures).
Absolutely! I've been using Haskell full-time professionally for the last 6 years. It is fantastic for building robust production systems. In the few places I've needed to work with non-Haskell code (usually Javascript) I frequently encounter bugs that I'm almost positive would have never happened with Haskell. I talked about one of them [here](http://softwaresimply.blogspot.com/2014/01/emberjs-is-driving-me-crazy.html), and [I'm not the only one who has seen this](https://www.youtube.com/watch?v=BveDrw9CwEg&amp;feature=youtu.be&amp;t=1208). Dynamic languages are always a maintenance nightmare and dramatically increase the QA burden. And even if you don't find a job doing Haskell, the people that you meet in the Haskell community are fantastically mind expanding. IMO this factor by itself makes Haskell worth it.
You can fake this with pattern synonyms: import qualified Calc import qualified Expr import Control.Lens class AsMul t a | t -&gt; a where _Mul :: Prism t a pattern Mul a &lt;- (preview _Mul -&gt; Just a) where Mul a = _Mul # a instance Mul Expr.ExprT (Expr.ExprT, Expr.ExprT) where _Mul = prism' Expr.Mul $ \case Expr.Mul a b -&gt; Just a b _ -&gt; Nothing instance Mul Calc.StackExp () where _Mul = only Calc.Mul The better way would be to define a common module with AsMul and have each of Calc and Expr import it and define instances in each module separately, their local constructors can be given different names as they are only used by the AsMul instance. This works better when you have constructors with the same number of arguments as the API doesn't have to give you the weird Mul (a,b) and Mul () patterns, though.
&gt; I think the most immediate benefit will be for (no surprise) expressive DSL Have you played around with Idris's `Eff` monad? It's powerful, but quite awkward. If this is the best dependent types can offer us in terms of DSLs, I am disappoint.
Err... Off the top of my head, no, bet I can probably provide some intuition. First of all, dependent types do not have _principal types_; that means that there isn't an obviously correct type for any well-typed expression. Second, since dependent types can be used as proofs, type inference becomes proof search, and its possible to write terms that are well-typed iff some turing machine halts.
It's proven that it's not possible to do complete inference with DT, but this isn't that big a deal for Haskell, since it's not possible to do complete inference with GADTs or rank-n types. What's not explored is how much inference you _can_ do, or how nice you can make the cases where you can't.
I use it every day. I can only think non-strict and pure now :)
[removed]
Consider the type: data Undeserialisable a where IntWrap :: Int -&gt; Undeserialisable Int CharWrap :: Char -&gt; Undeserialisable Char Deserialising this would have type String to... what? It isn't `Undeserialisable a` because that insists you can provide any outcome type from any input string, and you can't do that. This is currently the best solution so long as you wrap it in a Maybe and do runtime checks. Likewise, it can't be Int or Char, because then you can never get the other value. Dependent types would let you calculate the result type from the string, and represent that dependency at compile time.
No. I'm an unemployable lunatic now. :-(
Haskell has dramatically changed how I program in other languages. I write functions a lot more and methods a lot less. I default to map/filter/fold, and I try to write pure functions as much as possible. As far as actually writing Haskell though? I wish I could. It takes me 4x as long to do anything in Haskell. Half of that is caused by my inexperience but the rest is due to not having efficient trails beaten down by a large community. It also doesn't help that Haskell doesn't seem to have a great way to make GUI apps.
[removed]
I have noticed this recently too. The sizes of 'arbitrary'ly generated things are not always handled very well, especially if you generate lists of things. See this: https://hackage.haskell.org/package/QuickCheck-2.9.1/docs/src/Test-QuickCheck-Gen.html#listOf You may want to use genvalidity's https://hackage.haskell.org/package/genvalidity-0.2.0.1/docs/Data-GenValidity.html `listOf` function instead. It is much more sensible about sizes, see https://hackage.haskell.org/package/genvalidity-0.2.0.1/docs/src/Data-GenValidity.html#genListOf
Thanks for the reply. I've pushed my heap implementation up to [hackage](https://github.com/andrewthad/impure-containers). I wrote some docs so hopefully, it makes enough sense. If you browse the source, you can see the liquid haskell annotations. Concerning the question of mutable vectors in which all elements satisfy a predicate, I was able to get the example type alias to work. I tried to generalize the predicate using abstract refinements [in this gist](https://gist.github.com/andrewthad/dd1e0617c62ae183ec733108e38a4f46), but it did not work. Could you offer any suggestions? 
[removed]
I think it's a good line of work, but it makes perfect sense to me to provide the general-purpose layer first, and then to explore additional annotations and instrumentation.
I mean in the sense that when I was programming in Python it felt like most of my programming effort in terms of brainpower expended was going into fixing things that Haskell's typesystem would have caught.
I agree it does provide far more guarantees than a dynamically typed language would give, but it's still pretty far from guaranteeing algorithm correctness. But due to the type system, you are forced to program in a much more sane manner in general.
It made me a much better programmer in general. I mean, even when I do things like Java now, I produce a much better codebase now. Also, one of my open-source projects for Haskell got me my current job, although I don't get to do Haskell enough at it. I'd sum it up like this: if you want to become a better programmer, learning Haskell is definitely worth it.
I'd say that I've benefited a lot from my study of the language. But I'd also say that there's an opportunity cost involved in dedicating your time to any particular topic, and with Haskell there's probably a point of competency with the language after which you'd be better served professionally by studying something else. I think my study of the language went past that point. I generally do not use Haskell at work or for production systems. However, the rigor that I learned from writing Haskell software is something that has seeped into my programming regardless of which language I'm using. There are pretty deep lessons to learn about programming when you have such a harsh taskmaster looking over your shoulder as the Haskell type checker.
- Haskell forced me to get comfortable with recursion and modularity - Haskell taught me to think data first - Haskell made me naturally embrace all sorts of general best practices - Haskell exposed me to a new world of information such as reading academic papers and new fields such as PLT and compilers - Haskell made it easier for me to learn new languages - Haskell made it harder for me to work with OOP and other mainstream languages - Haskell made it harder for me to learn new domains as most are described and taught in imperative languages - Haskell required me to invest a lot of time and start learning programming "from scratch", where i could have had deeper domain specific knowledge have i stayed in a mainstream language All in all I really like Haskell and I think it gave me a solid foundation as a software developer, but I also think it made the road to feeling competent and doing more complex things longer - for better or worse.
What happened to `reducers` anyway? Is it still used or did it get superseded by something else?
&gt; At the end of the day, you can't make a good case for why the average company should drop PHP or Java. The case has already been made, and made well, many times by many people over the last 2-3 decades. Not to switch to a DT language, but to switch to a functional language. Basically every time functional languages are compared to non-functional languages, their defects/kLOC are lower, their features/kLOC are higher, their development time is lower, and their programmer productivity is higher. I see no reason "the industry" will see the advantages of DT languages, even if a good case is made. It's almost like "the industry" make more money by writing and fixing the same bugs over an over, rather than using tools that completely eliminate whole classes of bugs.
IIRC, full inference up to Rank 2 types is possible. (Although, OutsideIn only infers Rank 1 types). IIRC, Rank 3 inference has been shown to be undecidable (not just Turing compete). I'd like to see local Rank-2 inference, and global Rank-1 inference, but I don't think anyone has delivered that quite yet. This should be sufficient to type-infer anything that would be type-inferred by current GHC.
&gt; Deserialising this would have type String to... what? `(Undesialisable Int|Undeserialisable Char)` (using new anonymous sum type syntax). In particular, the would be the sum of all the GADT result types.
Seemed to me that they really were. I think this snippet is really the nub of the post: &gt; A solution ... is to introduce a newtype Prime and write a smart constructor for it: &gt; &gt; newtype Prime = Prime Int -- INVARIANT: must be prime &gt; &gt; prime :: Int -&gt; Maybe Prime &gt; &gt; Now you can rigorously test that any value produced by this prime function does in fact contain a prime. Then a Prime value can serve as evidence that the value within is prime. In this way we can relate a Prime value back to the integer it concerns and we have solved the Boolean blindness problem. But I could be wrong. What do you think the point of the post is?
Yes, lots! But Haskell itself is still elusive. I seem to be a perpetual beginner never breaking all the way through.
No, SafeHaskell doesn't have the benefits of a totality checker. 
What was awkward?
My justification, as subjective as it is, is in the rest of my comment. Lower opportunity costs for user of simply typed APIs.
[removed]
I use Haskell in my day job. I've enjoyed writing about and teaching Haskell. I might have been chasing a non-coding career path otherwise.
Runtime is in general very unstable as it is subject to your operating system's scheduler, power management, and a plethora of other factors. If you need stable runtimes then you should be using something like [Criterion](http://hackage.haskell.org/package/criterion). Otherwise the times given by the output of `+RTS -p` should be a bit more reliable. You may also find the RTS eventlog output to be helpful in understanding the runtime characteristics of your program.
An example for your argument is the "type-safe" filepath libraries, as filepaths are tricky. A counterexample is `servant`, the type-level programming and hard-to-read errors pay for the verification and convenience. I think the majority of the Haskell community is willing to experiment with libraries that claim to solve old problems with new types, so to speak. Either way, the benefit of implementing dependent types is that we can play around with them.
Sounds like a job for [acme-php!](https://hackage.haskell.org/package/acme-php)
I believe Dijkstra summed it up well: &gt; Simplicity is a great virtue but it requires hard work to achieve it and education to appreciate it. And to make matters worse: complexity sells better. When technical managers and programmers "don't even ...", it's generally the state of world--they see no reason to drop PHP or Java. In fact, quite the opposite, they tend to promote their favourite programming languages along lines of "I only like what I know." It matters not what the benefits/disbenefits are--and to be fair, if coding is just a day-in/day-out job, as it is for some, it's not really a concern.
The extension is called `TypeApplications` and yes, the ambiguity can be resolved this way.
Thanks! IALA (I Always Lament Acronyms), since RIETI (Reading is Easier than Inferring). 
[This blog series on type-safe artificial neural networks](https://blog.jle.im/entry/practical-dependent-types-in-haskell-1.html) looks related. I've only read a little, but the idea is that neural networks are represented as a series of matrixes, and dependent types make sure all their dimensions match up.
Works for this particular case. Does not extend well to larger more complicated (or infinitely large, like for symbols or nats or arbitrary depth ASTs, which can only be deserialised by pushing type level checks to typeclass powered runtime checks) sets of possible types. Now, there are other workarounds, but they're just that... workarounds. 
Yes, I'm now learning Elm for web development work, and it's been very easy to apply (almost) everything I learned in Haskell to this nice little language.
&gt; Basically every time functional languages are compared to non-functional languages, their defects/kLOC are lower, their features/kLOC are higher, their development time is lower, and their programmer productivity is higher. This isnt enough evidence for me. There are two major things you didnt mention: 1. The skill level of the developers 2. The projects completed As far as I can tell, functional programming is niche enough that the skill level of the developers is highger, and the projects completed are of very particular types. I have seen very few major projects in haskell, for example, and no comparisons between those projects and other major projects. Your claim is giant, and you claim that somehow its more profitable for people to *work slower,* which is absurd.
I'm not hugely familiar with FlowType, but from a Haskell/Elm/PureScript perspective, types are only half the battle. The other battle is mutation and side-effects. I, and probably most Haskellers, want to be able to tell if a function has side-effects, by looking at its type. TypeScript, ML, Flow, etc. all can't give you this, as long as you still have normal assignment. So Flow is probably really nice when compared with JavaScript, but leaves something to be desired for people who are used to immutability. I certainly won't argue that immutability is always the right way to go for every project, but I suspect it's the personal preference of many people here.
Servant is great and with the possibility of adding custom type errors it will be a killer app for DT.
Sorry if that wasn't clear but total time shown by the output of +RTS -p is what I was talking about. I will definitely take a look at Criterion!
In one of the comparisons, the Haskell Report was given to a graduate student with no previous programming experience and it was still a complete program that scored well against the non-Haskell implementations.
Haskell made me a [worse programmer](http://lukeplant.me.uk/blog/posts/why-learning-haskell-python-makes-you-a-worse-programmer/), but it certainly improved the quality of the code I write. :)
We aren't trying to prove programs isomorphic, but programming languages.
Thanks for the reply, but it does not really answer the question. Surely, one could even define a function `listOfStuffToString` and use that when appropriate. I never heard about the idiomatic `Show` but it certainly seems to be correct - I never understood why the `Show` instance of `Data.Map.Map` is `fromList [things]` but now it makes total sense. So thanks for that!
Almost exactly my experience. It's crazy how enlightening it is to learn about types and proper abstractions that are based on math. It makes your typical OOP code look like someone randomly chose some metaphor (i.e. inheriting traits in animals) to structure their app. 
This function and its whole module will be helpful to you. https://hackage.haskell.org/package/base-4.9.0.0/docs/Text-Show.html#v:showListWith The difference list type is pretty crucial for performance. HTH.
More importantly, the cost of bad technology, if widely used, will not drive the market participants bankrupt. In the Middle Ages, books were copied by hand by monks, yet neither monasteries nor book-writing-by-hand went "out of business". It was the best anyone had, so people were happy with it. In 2016, PHP is the most popular programming language for websites. It is just passively accepted that it's the best we've got and if your website costs ten times the amount it'd cost to make, then "oh well"; the market will just shill it out. Technological superiority isn't enough to gain popularity as long as the market is willing to bear the cost of an inferior solution.
Okay, that's pretty neat and not something I knew about last time I tried this. Maybe that module will be a reality now! This still seems like something dependent types should simplify though.
I haven't played with Idris since `Eff` was added. How is it awkward? It seems to me at first thought like it would be very natural
I'm of the opinion that if all you ever learn from haskell is algebraic data types + pattern matching then its already worth it. At the very least it will protect you from the OO zealots that insist on using subclass polymorphism for all forms of control flow.
Yeah, but not that many seasoned developers or big fans of types in /r/javascript
By more readable I meant the same as when in Haskell type signatures tell you a lot about what functions do. With flow, it's much easier to look at code and go "oh that's what it does".
Many models of the market are based on the same sorts of dynamical systems as physical law. And all of physics is based on minimizing energy states. The laws of physics don't bend to our desires. But they don't act irrationally. The same is true of markets. 
Domain deafness.
[removed]
So I got it building and running on my phone, but it hangs during init. Seems like it can't mmap memory for some reason (this is in osReserveHeapMemory). Have you seen this? Or have you successfully run haskell code on arm64?
Oh right. So sorry I didn't mention this: you need to pass `--disable-large-address-space` to `./configure` when building `ghc`.
It can be. With compound literals (introduced in C99), it's not too bad. At a call site, it winds up looking a little like you have named arguments, if you squint.
Indeed! You may be interested in this: http://goto.ucsd.edu/~rjhala/papers/refinement_types_for_haskell.pdf 
Yes, I learned a lot of functional programming principles from it. I still work in python but I'm able to include functional programming techniques such as from the python eggs 'funcy' and 'pymonad.'
I don't think you understand my point, respectfully. It's not about "bad" vs superior technology. It's about profits. If someone in the Middle Ages invented a machine that could copy books, monks wouldn't be copying them by hand, because they would need to charge too much compared to the machine guy. You can't just "oh well" away cost. Even rich people care about cost, and will seek to reduce it. I'm not saying everyone will start making web pages in Haskell, I'm saying that a consumer product on the shelf is cheaper if the software that helped create it cost a tenth to develop. If a firm can drive down the price of a consumer or producer good by using Haskell, that firm will succeed, without Haskell necessarily ever becoming popular.
It's not just about purity. I'm not familiar with how Lisp does IO, but I think this is another advantage to Haskell: that it actually has a sensible IO interface. Purity is the root advantage, but if we don't find pure solutions to stuff like IO, it doesn't matter if the core is pure, since this pure core needs to interact with the real world in order to be useful. Purity is not sufficient. It's just as important what interface the language provides to actually manifest your pure ideas into the world. Here's my take on the IO monad-way of doing things: https://news.ycombinator.com/item?id=12111953
Yes. Learning Haskell paid off mostly by using it to learn about features of static type systems and functional programming. I can use those concepts in almost any other language, like thinking about side-effects, laziness, purity, composition, designing simple models based on algebraic structures with simple laws, and many more. Currently I use C# at work and I use the above as much as I can. I also managed to introduce the use of types like Either and Maybe into our codebase which helped us a lot with design and maintainance (plus C#'s `from ... in ... select` works wonders for this)
Paging /u/snoyberg
Markets are, to some approximation, dynamical systems. They might be chaotic, but they still obey sensible sets of principles which govern their behavior. The world would descend into financial crisis if the US minted twice the number of bills. That's because supply and demand is an economic law. Speculation and "information warfare" are not exceptions to the rules. They are strategies people have developed. (And neither is particularly novel, for that matter). An influential person making a grand speculative claim in the right political climate will change people's perceived value -- even if only temporarily. Long enough for some to capitalize on it. Particle-antiparticle continuously pop into existence from the vacuum. They almost always end up annihilating each other a moment later. They are "speculative" particles, in some sense. But in rare circumstances, one of the two will get stuck on the far side of an event horizon. The pair cannot rejoin. The other particle is visible to us from Earth as Hawking radiation. This is not all that different from a profit made from a speculation. I'm not saying that kind of analogy is perfect. But it is suggestive. At the very least, it should demonstrate that a haughty "Come on, we're living in 2016!" reply doesn't do much for me.
I'd much rather write Haskell and compile it to Javascript with GHCJS. You can get much better code reuse between the front and back ends that way (assuming you're using a Haskell backend of course). If GHCJS was ruled out for some reason (perhaps code size), then I'd fall back to [PureScript](http://www.purescript.org/). Anything else is just a bug-infested nightmare in my experience. In fact, in the past month I've encountered at least 4 or 5 production bugs in JavaScript/CoffeeScript/TypeScript code that would have been prevented by Haskell or PureScript. 
This is a necro that's completely inelegant, but couldn't that do be replaced with fmap baz bar
&gt; Haskell helped me to understand that the most problems in programming are actually type problems. If you use correct types you will avoid most of the problems. Wait what? Most programming problems I've ever encountered are one of two sorts: People problems. The people you work with either don't understand what computers are capable of, what their system is capable of, what your team realistically could do, what *should* be done, even when it's technically possible, etc. Sometimes it's a matter of us, the programmers, not fully understanding the problem domain. Integration with other technology. As you go up the abstraction ladder, your reasoning gets less and less precise. I can say with good confidence how a C compiler will act when I give it my source code. I am much less confident with what Wordpress will do with my markup. Things get very hairy when dealing with poorly thought-out systems or when having to bridge between two systems designed to meet different needs. Type systems will do little to help you find yourself around odd-ball corner cases and limitations due to technological hacks put in place to keep business software working. Type errors are a close third, perhaps. But progress and preservation does little to alleviate the pains of people who don't know what they're trying to do and being stuck working with technology you have to pay consultants to understand.
I feel like side effects (besides mutation) are only like 5% of the battle.
Well, markets are driven by people and people are irrational. 
People are entirely rational. They are just extremely complex and impossible to model properly.
Not at all. In OOP a derived type possesses all the functionality (methods) that the base type has. `newtype` OTOH strips the base type from all external methods, and leaves it to you to re-add them (or add others).
If Haskell was the silver bullet, companies will adopt it immediately. There are enough companies out there which are aware of Haskell and that you can assume their bosses knows what they are doing (Google, Microsoft , Facebook for example) and haven't switch to Haskell yet. They must have their reason. I switched to Haskell in my company and after 2 years I still find sometimes that the cycle : compiling, understand the error message , fixing it is much longer than running some test, get a runtime error (with a clearer error message) and fix it. Where Haskell shines, is refactoring not getting things done quickly.
I mean, it's kind of funny. I don't think it was meant in a mean spirit...
Simple example: some guy decides that to store and use date as unsigned int unix timestamp is very effective. He does it in 90s and then system works without any problem till 2038 when everything breaks by "unknown" reason. I've experienced similar problems a lot in my career. Haskell will not even compile if you are using wrong types somewhere in your code. Haskell type system is different from any other language. You can't compare it to other statically typed languages. 
Another answer, and perhaps the most important, is that you should not be required to give up on the expressiveness and composability of a pure language to get dependent types.
With type-classes, this means 'myName ++ ""' could be radically different than just 'myName'. A bit too magical for my taste.
Actually I defined `readDateDay` wrong, it should have been: readDateDay :: (Nat y, Month m, IsValidDay d m y) =&gt; String -&gt; y -&gt; m -&gt; Maybe d Then I would expect to be able to prove, *at compile time*, that no `Just d` can ever escape this function without being valid. At that point I don't need any kind of runtime check from the type system, it was done on my code which will run at runtime.
`myName ++ ""` has type `String`, *not* type `Name`. When you add an apple and an orange, you don't get an apple or an orange, but you *do* get two fruits. I don't see the ambiguity.
TL;DR No, newtype declares that types are different whereas OOP declares 'is a' relationships Newtype is mostly not like OOP inheritance. If OOP class A inherits class B, then you're saying that an A _is a_ B. But if Haskell type X is a newtype of type Y, then you're saying that an X _is not a_ Y. For example, the declaration below says that a Prime and an Int are not the same type - that you cannot use an Int where a Prime is required. The declaration below declares a type Prime whose underlying representation is an Int. The idea is that the Int inside a value of type Prime is always prime - you'll have a function for making Primes that checks that: newtype Prime = Prime Int Now, `GeneralizedNewtypeDeriving` does introduce something that's a bit like OOP implementation inheritance. Suppose we'd like to do arithmetic on `Prime`, making it a member of the `Num` typeclass (this is a terrible idea though, see below). Prime, as written above, doesn't have a `Num` instance. GHC can derive instances for some typeclasses such as `Ord` automatically, but not `Num`. But if you turn on the GeneralizedNewtypeDeriving extension, you can say `deriving (Num)` on your newtype and GHC will use the Num code from the underlying `Int`. newtype Prime = Prime Int deriving (Eq, Ord, Num) That would enable you write code like this: Prime x = Prime 1 + Prime 3 Since obviously 4 isn't a prime number, it's clear that being able to add them like Ints is a bad idea, so it was a bad idea to derive Num in this case. There are plenty of cases where it does make sense to derive type classes from the underlying type though.
I think one reason Haskell doesn't do this is it'd make type inference harder.
I didn't downvote you and I'm sorry others did. I think my original comment was poor feedback - sorry for that too. You may be right that I'm missing the point (though, if so, I still am). Do you have an example of where you'd not use a smart constructor? What functions would you test?
Let's say you have a custom Show instance for Name. Now this: print myName is radically different from this: print ("My name is: " ++ myName) If I wasn't intimately familiar with the code, something like this would throw me off during refactoring. (Ignore the silliness of using 'print' with Strings. It's just a bad example.)
&gt; as long as you still have normal assignment Limit yourself to local variables and you have ST, which is pure.
I write Haskell code almost exclusively in my day job, so I'd have to say **hell yes**!!! I wrote some Haskell in my last job as well and I've been doing Haskell since 2008.
Ah, ok. Well why not just make it use the parent instance dictionaries? i.e., you can't declare an instance for a `newtype` if there is an existing instance for the root type.
&gt; If OOP class A inherits class B, then you're saying that an A is a B. But if Haskell type X is a newtype of type Y, then you're saying that an X is not a Y. I would amend that slightly. In Haskell "type X is a newtype of type Y means that X *has a* Y and further has *exactly one* Y and nothing more." So, it's basically a named container type for exactly one value of some other type.
&gt; I personally believe that Haskell is superior to imperative/impure programming languages, but a typical manager doesn't have the technical background to understand the reason for that superiority. A manager doesn't need to understand anything if he gets fired because a competing company can cut costs, while he cannot. A lack of profits will, eventually, either force him to implement something that cuts costs, or get a new job. &gt; Moreover, even if some company were to drastically slash software development costs by switching to Haskell, this information wouldn't instantly spread to all other companies [...] Never did I claim it would happen instantly, only that the pressure is there. It will take time, and I'm not sure how long. It's possible we have to wait for the old developers to die off. I'm not saying because I'm cynical, I'm saying it because I consider it a fact that many human advances, like in physics, couldn't happen until the people, that were raised on the old theories, died off. Humans, generally speaking, seem to have a hard time unlearning. &gt; Haskell developers are few and expensive, whereas Java developers get churned out by every school and PHP developers can practically be picked up from the streets. Doesn't that tell us something? Where does the extra money come from that companies use to hire the more expensive Haskell developers? Profits. Added profits are channeled back into salaries, and the premium on a Haskell developer salary tells us that the market is demanding these people more than the rest, I would argue. &gt; Suppose a manager works with a budget of 20M and he has commission a website for new product X. [...] The company that this manager is working for produces a product, and this product will be more expensive than the product of competitors if competitors can get the same thing done with a $2M budget. I don't assume *perfect* information, but you certainly seem to assume imperfect information on the part of the people is to buy this more expensive product in the end. If these people ever find out it's available cheaper elsewhere - and they have every incentive to speak to each other about it - the inefficient company goes bust. And that's a one-time thing. There is no cure for lack of profits. And while hoping that your customers, or competitors, don't find out that it can be done cheaper elsewhere might work in the short term, reality will eventually assert itself.
Yes, or `baz &lt;$&gt; bar`. I typically prefer imperative-looking code to `fmap`/`&lt;$&gt;`. 
Implementation detail, would the runtime actually need to pass the proof after it was proven? Seems to me that it would only need to prove the same size and call an "unsafe function that assumes it was proven". That function in turn might internally call functions that assume something that can be derived from the original compile-time proof.
Thanks! That clears things up a lot. And it's helpful to know about the problem with `RealWorld`. It seems like the best solution at the moment is to just make any vector code I try this on be polymorphic in its `PrimMonad`, which is usually what I do anyway. I've tried this out without making any of the `restrictedX` variants of the mutable vector functions, and it works as you said it would. One thing that is not totally clear (and it's possible there's a blog post or paper about this somewhere) is how LiquidHaskell decides when the refinements for things have to match. In the example you gave involving `id`, it's obvious that because `a` is universally quantified, the refinements must match. It's less clear that this is true in `unsafeWrite`: {-@ assume unsafeWrite :: PrimMonad m =&gt; x:(MVector (PrimState m) a) -&gt; ix:{v:Nat | v &lt; mvlen x } -&gt; a -&gt; m () @-} LiquidHaskell cannot possibly know what `unsafeWrite` actually does. Maybe it actually does nothing with the argument, in which case turning the `a` into `a&lt;p&gt;` is overly restrictive. Playing around in [this gist](https://gist.github.com/andrewthad/6c2bf12a3900202f9f5317b8daf4ffb7), it seems like the rule is to implicitly add abstract refinements to every occurrence of `a`, as long as `a` does not appear in something like this: `(a -&gt; a)`. I'm guessing that there is some algorithm that figure out where matching refinements need to be used based on whether type variables appear in positive/negative positions (or covariantly/contravariantly, my vocabulary in this area isn't very good). I'd like to understand this better since so that I can know when I can count on the default behavior being that right thing. Sometimes, like in `otherWrite`, it looks like it ends up being overly restrictive, which is fine. The main thing I'm concerned about would be cases where it was less restrictive than I expected it to be.
I think that's clear thanks. I would have used the term _smart constructor_ to for both `tryToApplyChangeSet` and `applyOperation` but it looks like you would not. Fair enough. And you would expose the `Note` constructor - for pattern matching I guess. I would normally make `Note` opaque because otherwise it's possible to add another function that violates its invariant (I could get pattern matching back through view patterns). But you want a bit more flexibility than that and are prepared to test the invariant at more points. Am I getting closer to understanding?
Good! No problem :-)
Idris is pure.
I don't really buy this. `3 meters + 5 meters` makes sense in the exact same way that `3 + 5` makes sense; it's not like adding `meters` somehow changes our understanding of what `+` means. That's silly. It's possible we just have a name conflict here and I'm simply talking about a different concept entirely, but as no language seems to have it, I'm not sure what I can call it. But I find myself perpetually wanting it. EDIT: ok never mind, that's an awful example of this; units are a totally different topic. Still, `3 trucks` and `5 cars` are `8 vehicles` in the same way that `3 vehicles + 5 vehicles` is `8 vehicles`. It does not make sense to magically redefine plus on trucks so that somehow `3 trucks` and `5 cars` is now only `7 vehicles`.
Does `3 meters * 5 meters` make sense? What about `3 meters * 5 kilograms`? `3 meters + 5 kilograms`?
&gt; It's possible we just have a name conflict here and I'm simply talking about a different concept entirely I think so. In F# there's [units of measure](http://fsharpforfunandprofit.com/posts/units-of-measure/), maybe that's closer to what you're looking for. But the goal of Haskell-style newtype is to abstract away from the underlying type. Ideally, such a type would be defined together with its relevant manipulation functions, and then even if you were to change the underlying type (and the implementation of the manipulation functions accordingly), user code would be none the wiser. Let me give you an example I encountered recently in a business application (using a single-case union in F#, but the idea remains). I had data transfer objects defined like this: // in Haskell, this would be a newtype that doesn't export its constructor type UserId = private UserId of int64 type User = { Id : UserId Name : string // ... } I don't want to be able to add or subtract `UserId`s, or convert them to string, or anything like that; all I want to be able to do is equality (which is implicit in F#, in Haskell I would have derived `Eq`) and retrieving the underlying value in the back-end. `UserId`'s underlying type was `int64` because in the first version of the app we were using SQLite, and that's the type of IDs in the .NET SQLite driver. Then we moved the back-end to MSSQL, which uses `int` as the type of IDs. I just changed the definition of `UserId`, and all my front-end code still compiled without a single change.
Exactly. Support for dimensional programming can be extremely useful, particularly in the physics domain. 
&gt; But could you please give more examples in which this would be useful? It looks like a principled idea, but I don't understand why this particular principle is more useful than, say, only allowing automatic conversion from the parent rather than to it. I mean that given any `Apple`, I can always give you a `Fruit`, but if you give me a `Fruit`, I can't somehow coerce that fruit into an `Apple`. All names are strings, but not all strings are names. Newtype is probably the wrong word, but whatever the concept is, it seems to have some core intuition. Creating an object of type `Name` is something I don't even know how to do--in fact, it's so hard I make my users do it for me! But once you give me your `Name`, I have no trouble turning it into a string and saying `"hello" ++ yourName ++ "!"` 
Well with `x : UserId` I don't want to be able to do `x + 2` either, which you say you want.
Very nice, thx for the info. I'll most likely register.
Facebook doesn't NOT use Haskell, they just don't use it for very much. But they ported their spam filtering service to Haskell last year.
I saw a project like this awhile ago but don't know if it ever progressed to a point where open sourcing or regular usage happened.
There is a nuance in the `unlines` behaviour: &gt; unlines ["asd", "qwe"] "asd\nqwe\n" &gt; intercalate "\n" ["asd", "qwe"] "asd\nqwe" As far as the list `show`ing goes, it is quite an interesting solution! :) How *correct* is it do use it though? Would that be frowned upon by a reader?
To illustrate, imagine a junior programmer is trying to remove a value from an object. Someone told them mutating data is bad, so they opt for the following to create a new object without the key, then assign it to a `const`. const one = { a, b, d } = { a: 1, b: 2, c: 3, d: 4 } console.log(one.c); // what is c? `one.c` is `3`. This could be a silent issue that appears weeks later when someone uses a utility function to iterate all keys and send them to a server. Imagine `one.c` is actually `one.password` and now you're sending the password somewhere unintentionally. Hopefully the example is motivating enough to show that a type signature for `one` would clear up any potential miscommunications about the value of `one.c`. Then there are the more common cases of misspelled functions or variables which is an incredibly common source of bugs. (note, this problem can also be solved with eslint, etc).
I can't stress the importance of this idiom enough: *If there are multiple sensible ways to implement a class for a type, make a newtype per implementation.* Just look at Monoid instances for Bool and Int.
In this case, the proof only exists at compile time, it has no runtime impact. This is an optimization that was introduced to Idris a few years ago; types are erased if it can be proven that they aren't used at runtime. Look up "type elision".
I don't agree that Scala has algebraic types, you have to simulate them with inheritance: sealed abstract class Color case object Red extends Color case object Green extends Color case object Blue extends Color Similarly you have to simulate typeclasses with implicit conversion, and my experience with large code is that it quickly gets difficult and time consuming to debug why a particular layer in a stack of implicit conversions doesn't trigger (it happens behind your back after all). It feels like something that was discovered by accident rather than intentionally designed, much like template metaprogramming in C++. Type inference is poor, which results in verbose code with lots of superfluous type annotations that makes code difficult to read and boring to write. Inheritance causes a myriad of subtle problems. Scalaz' fold method comes to mind as somewhat difficult to reach, because the compiler will bark up the wrong part of the inheritance tree and find the weird fold methods in the standard collections library instead. And let's not discuss CanBuildFrom. Side effects can occur anywhere without the compiler raising an eyebrow.
LiquidHaskell does not know what `unsafeWrite` does, but (as with standard/unrefined Haskell types) it does not care, as long as the two `a` arguments match. The reason why your `otherWrite` fails is that you ask Liquid Haskell to match the elements of `v` with a type that containt `101`, and then (in `testFunc` post condition) you ask Liquid Haskell to prove that `v` contains elements `x &lt; 100`. Note that at `testFun` you are making calls to the polymorphic functions `otherWrite` and `unsafeWrite` and at _all_ the calls you are instantiating the type variable `a` with the _same_ type `{x:Int | p}`. Liquid Haskell will infer the appropriate refinement `p` as the most restrictive predicate that is satisfied by all the occured values. In your example, since the refinement `p` should be satisfied by `101` it cannot be `x &lt; 100`. You can read more about how this `p` is inferred here http://goto.ucsd.edu/~rjhala/liquid/liquid_types.pdf Generally, positive position of refinements (postconditions) means that you can assume the refinement at the call site, but you need to prove that it holds by the implementation of the body. Dually, negative refinements (preconditions) means that you can assume them at the implementation of the function and you should prove that they hold at the call site. But when you have polymorphic types variables it means that the specification holds for _any_ refinement, thus you do not need to assume or prove anything, and you rely on Liquid Haskell's inference to find the strongest refinement that satisfies all the occurrences. BTW, there is a catch that seems you haven't run into yet, but since you may run into it, I should let you know. The above properties of polymorphic specifications (types) do not hold when the type variable is constraint, with type class constraints other than `Eq a`. If you have a constraint type, like `Num a =&gt; a -&gt; a`, and since `Num` type may have methods that randomly create values, Liquid Haskell will not infer an appropriate refinement to instantiate `a`, but will simply instantiate `a` with an unrefined (or simply/true refined) type. Just keep this in mind.
Couldn't they just contribute to the actual Wikipedia? That's my first stop for almost all knowledge these days, and I've rarely been disappointed by its coverage of mathematical structures. If the benefit of this is that all the structures are listed in one place, well, Wikipedia has lots of "List of _" pages, and would almost certainly be friendly to one more.
To explain why you want newtypes, [Joel Spolsky's post Make Wrong Code Look Wrong](http://www.joelonsoftware.com/articles/Wrong.html) isn't a bad place to start. Essentially, his idea is that it's not uncommon to have semantically different kinds of values of the same type: for example, you might have sanitized user input and an sanitized user input. You might have x, y and z coordinates as well as counts. The problem, though, is that humans are very fallible and are prone to making mistakes. So instead of making "wrong code look wrong", it's better to "make wrong code not compile". So you can say newtype Count = Count Int newtype X = X Int newtype Y = Y Int etc., and now what used to just be encoded into the prefixes of names is now encoded in the type system. And there's not even a runtime penalty for the additional safety. You can also use newtypes to avoid leaking implementation details of your library, and to get around the restriction that typeclasses are globally unique per type (for example, there's `Sum` and `Product` newtypes so `Sum 2 &lt;&gt; Sum 3 == 5` and `Product 2 &lt;&gt; Product 3 == 6`).
Haskell helped me to enjoy writing code again. It taught me that a lot of the little dumb problems I'm always dealing with when writing low-level user-space and kernel code in C (which is how I pay the bills) aren't inherent to programming itself but rather artifacts of the way C is designed, and it doesn't have to be that way.
ooh, like hoogle? What a great idea, let's build it!
This is very interesting and I enjoyed reading it! (one minor suggestion though, I think that adding type signatures would make it even better). 
Hmm, they are called "non-confluent" but I dont know what that means.
Alright, now I get past init, but I get an exception when trying to call a Haskell function from C. (I wrapped it with the "wrapper" ffi call and passed it into the C environment, then tried to call it and it crashes.) I tried both with and without the data layout changes in your D2378 patch. The assembly at the function pointer looks like this: -&gt; 0x1063000e8: ldr x16, #12 0x1063000ec: adr x17, #16 0x1063000f0: br x16 0x1063000f4: .long 0x01e1fbf4 ; unknown opcode 0x1063000f8: .long 0x00000001 ; unknown opcode 0x1063000fc: .long 0x063000e8 ; unknown opcode 0x106300100: .long 0x00000001 ; unknown opcode 0x106300104: .long 0x00000000 ; unknown opcode Which looks a little weird? I'm not sure, I don't know much about arm assembly, but I also couldn't find it in the GHC source.
Here you have only the definitions, just formalism, no explanations. Seems to be handy as a reference, if you stumble on some object you don't know in a problem and don't want to get lost in Wikipedia's explanations. Both seem complementary to me 
https://frontrow.workable.com/ !
Maybe this is the best option. But, I check and there's not even a button like function? I'll have to draw the button in the OpenGL canvas and set a callback for that area? Seems a bit archaic.
http://math.chapman.edu/~jipsen/structures/doku.php/partially_ordered_sets Please tell me about the transitive property in this link. Am I reading this correctly?
Basically inheritance where you're never allowed to override your parents' methods.
Making wrong code not compile is another reason. But it's by far not the only reason for newtypes.
There is a typo. It should be `x &lt;= y, y &lt;= z =&gt; x &lt;= z`.
&gt; I agree with you, but somehow this sounds like "if all you have is a hammer, everything looks like a nail." If all you have are carpenter analogies…
Tactics, the original statement you replied to (seemingly) was: &gt; I'm not sure the market is optimizing for anything. ..to which you invoked the physics-based models of market as support for their rationality. At this point in discussion, though, I guess we can all agree that existence of some proper-yet-hidden model has little bearing on the human notion of rationality. Again -- stating that (at the end of it all) there is a physics based explanation to everything (collective human behavior, for example) does little to add to this discussion. Precisely because this proper explanation is both unknown and extremely complex. Whereas the existent economic theories 1) appear laughable in their predictive and explanatory power, and 2) tend to serve matters of political expedience more than anything else.
If you replace `++` with `&lt;&gt;`, use `OverloadedStrings` and make an `IsString` instance for `Name`, it'll just work. I don't think it's a compelling example. Perhaps you could use a different example?
[Planet Haskell](https://planet.haskell.org/) aggregates some blogs.
NY Haskell is pretty good about posting videos of talks: https://www.youtube.com/channel/UCzNYHE7Kj6pBqq5h8LG9Zcg Composeconf also has a youtube channel of its own: https://www.youtube.com/channel/UCzNYHE7Kj6pBqq5h8LG9Zcg
A bit off topic, but why is there a difference between `newtype` and `data`? As far as I know, `newtype` are more limited, but in the other hand leads to free (speaking of cpu time) conversions. For me it appears as an optimization that the compiler may be able to apply automatically on `data` with only one field.
(apparently the second section of the article answers my question. Thank you) I started reading your serie, thank you, that's well written and interesting. However now I have a question. Can I write a polymorphic function `loadNeuralNetwork :: FilePath -&gt; IO (Network i j k)` which loads a network of unknown size, i.e. the size is only known at runtime, and still use dependent types? (Edit: fully rewritten in a more compact way)
the other reason that springs to my mind is to allow multiple instances of a typeclass e.g. Monoid instance for Sum and Product is there anything else?
As I mentioned: performance.
The less maths parts of Wikipedia are slowly rotting and becoming something bad behind the scenes, so I'd say it's good not to depend upon it if you can avoid it.
A very good reason to use `newtype`s is to manipulate the arguments of an existing type. ``` newtype Flip f a b = Flip { unFlip :: f b a } ``` ``` newtype (f :.: g) a = Compose { getCompose :: f (g a) } ``` This is important because `newtype`s can be used unsaturated (partially applied) whereas `type` synonyms cannot. That is, if `:.:` were defined as a type synonym, one would not be able to write an `instance (Functor f, Functor g) =&gt; Functor (f :.: g)`
Can you elaborate?
&gt; I'm never quite sure about the consistency of these lists, especially re. the more exotic definitions. Bah, there's bound to be some errors in the data here and there, but is that really a deal breaker? We could always add a "report an error" link instructing users to send us a pull request. &gt; So, yeah, the tool itself would help clarify these issues as well, but large part of its development would be a thankless typing job (it doesn't look like the entries are available in some machine-readable format). `-_-` Many times already, I've seen my NLP colleagues spend their days drudgingly annotating thousands of sentences, for the good of their science. I think it's about time I do the same for mine :) Let's do it!
Thanks! Too bad I'm in Sydney D:
I think they state elsewhere that module types that are not functors are "atomic," which fits into that surrounding statement nicely.
To be fair, I have no idea what broke here. Here's the `ViewController.swift` I use to start the `Counter.hs` sample. import UIKit class ViewController: UIViewController { override func viewDidLoad() { super.viewDidLoad() let args = ["MyAppName", "+RTS", "-DS", "-D", "-RTS"] var cargs = UnsafeMutablePointer&lt;UnsafeMutablePointer&lt;Int8&gt;&gt;.alloc(args.count) for (ix, s) in args.enumerate() { cargs[ix] = UnsafeMutablePointer&lt;Int8&gt;((s as NSString).UTF8String) } var count = Int32(args.count) print("Starinting hs_init...") hs_init(&amp;count, &amp;cargs) print("... started.") print("Starinting Counter...") startCounter(10) print("...Done") } } 
Indeed! We hope to be able to address the distant timezone remote employee issue one day.
These guys are pretty cool 👍
Should CVSS also be included (if available)?
Yeah, that sounds like very useful metadata to include, though I'd like to keep it as an optional field to make sure it doesn't scare people away from reporting an issue.
Do you have a website where you describe the kind of work that you do and the skill set that you expect the candidates to have?
Why not just deprecate them all on hackage?
Could you break out the naive Bayes classifier as a reusable component? IIRC, N.B. doesn't compute the evidence term, so doesn't need to marginalize out the fitting parameter (no nasty high-dimensional integrals then). I guess there are a number of further assumptions made by text classification, but it would be very helpful if you could make them explicit in the code. Thanks a lot and congratulations on breaking new ground!
EDSL can leverage the toolchain of the host language (editor plugin, syntax highlight etc.)
* It's not possible to express the metadata proposed here on Hackage * Like Stackage itself, the hope is to decouple the two distinct acts of authoring/maintaining a package, and reporting bugs on it * It's much easier for external tooling to deal with a Git repository tracking a YAML file than to query Hackage for all of this information * As an example, please see [exhibit A](https://github.com/commercialhaskell/all-cabal-metadata/) * Unless I'm mistaken, there's some level of ambiguity of exactly what the meaning of "deprecated" is on Hackage. This proposal is intended to give a clear definition and expected behavior for when a package version is included in the list
A template language that understands HTML is less vulnerable to injection attacks than one that treats the output as a string. So that's one advantage of BlazeHTML over Velocity.
Hmm, you're right! Will have to update the PDF! It appeared at [ICFP 2014](http://dl.acm.org/citation.cfm?id=2628161) 
Hi! Indeed there is a need for promoting Haskell in the Indian industry, especially among startups. It has immense benefits, especially for learning. It makes you reason and ask important questions about your problem domain, unifies business concepts with mathematics and most importantly it's fun to code with! They should teach Haskell in elementary school along with basic algebra! Anyways, I have messaged you my contact details. Let's collaborate and spread good word for Haskell
I'm afraid not. Where are you based at?
Scala unifies object oriented and functional programming. This is not because most of OOP languages have a limited type system, are imperative, first-order, etc that it is inherent to the paradigm. After all, most of functional languages are also imperative. So should we conclude that FP is about stateful behaviour? OOP is about coalgebras and sub-typing. None of this is in opposition to FP. On the contrary, it is very useful to combine both. For example, change the definition of datatype in Haskell and all your patterns have to be updated, ... unless you use classes ... which are implicit vtables with static dispatch. Bring in impredicative datatypes with classes and you get OOP in Haskell for the sake of abstraction :) FP is about modelling computation, OOP is about modelling data. Combine both and you get a language in which you can abstract both computation and data. Nice isn't it?
Wow this is great blog
Firstly, `newtype N = N Int` is roughly equivalent to `data D = D !Int`, i.e. `newtype` is automatically strict. However, that doesn't answer your question because you could modify your statement to read "... on `data` with exactly one strict field". The more important difference is that `N` is indistinguishable from `Int` in all but name whereas `D` isn't, as evidenced by their treatment of `undefined`: data D = D !Int newtype N = N Int i _ = 42 n (N _) = 42 d (D _) = 42 i undefined -- =&gt; 42 n undefined -- =&gt; 42 d undefined -- =&gt; undefined Thus, there is a (very subtle) semantic difference between the two data types and they cannot be transformed automatically into each other. (Example adapted from [the Haskell 2010 report](https://www.haskell.org/onlinereport/haskell2010/haskellch4.html#x10-680004.2).)
Because it's not deprecation; In essence, it's a notification mechanism for users so that they urgently attend to their programs / library / etc, so that they take actions on their dependencies (upgrade, do nothing provided it not affected, ..)
I'm somewhat new to these algorithms, so I apologize if the following seems newbish: what are the evidence term and fitting parameter in this context? Also, would wrapping my runBayes function in a State monad make it a reusable component if it added the newly classified text to the learning material?
Excellent proposal ! This is sorely needed to be able to communicate issues quickly and in a central place that can automatically harvested by scripts / tools (difference with a mailing list or reddit). Hopefully also raise the profile of haskell in business settings, to complete the entreprise ready tools like stack.
On the one hand I am super glad to see more Haskell job posts, as well as the fact that your team is growing enough to warrant multiple posts here. On the other hand, each time you post I keep thinking, "Why would I want to join a team of 4 MIT dropouts?" I imagine you are trying to express that you were good enough to get into MIT, but took the more monetarily lucrative entrepreneur route instead. There is nothing wrong with that, however, the message here makes me hesitate sending in an application to you. I'd focus less on your personal accomplishments, keep the "We're working part-time with well-known Haskell community leaders, including Well-Typed and FP Complete. We've raised $3.3M, so we're not going to go away overnight" part (it's nice to know one is applying to a somewhat financially stable and connected company), and show why the potential applicant will be successful right along with you.