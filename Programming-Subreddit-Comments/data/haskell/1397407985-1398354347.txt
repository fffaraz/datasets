The DSH project is still ongoing and work has *not* ended. See the list of updated links to papers above. 
Because I wrote it for a client who owns the copyright ... Like I said, there's a limited preview release available. If you're interested in participating please email me and give me details on your use case: http://web.jaguarpaw.co.uk/~tom/contact/
Awesome!
Is there a time schedule for the hackage release?
I'm still not sure what you're hoping for, but here's my guess in terms of Opaleye. Please correct me if I'm wrong and I'll rewrite it. I'm guessing you mean that `foos` restricts the Foo table to the rows where the id equals a particular parameter, and `bars` restricts the Bar table to the rows where the id equals another parameter. Then `notPossible` takes the results of these restrictions and looks up the Foo corresponding to a Bar. Is that right, or do you mean something else? If you mean something else can you give an example of the tables and the expected output? (NB If you find this syntax repetitious please remember that since most of this is first class the duplication can be refactored away. I'm just trying to be as explicit as possible for the sake of pedagogy.) foos :: QueryArr (Wire Int) Foo foos = proc fooId -&gt; do f &lt;- fooTable restrict &lt;&lt;&lt; eq -&lt; (fooId, fId f) returnA -&lt; f bars :: QueryArr (Wire Int) Bar bars = proc barId -&gt; do b &lt;- barTable restrict &lt;&lt;&lt; eq -&lt; (barId, bId b) returnA -&lt; b notPossible :: QueryArr (Wire Int, Wire Int) Bar notPossible = proc (fooId, barId) -&gt; do f &lt;- foos -&lt; fooId b &lt;- bars -&lt; barId restrict &lt;&lt;&lt; eq -&lt; (fooId b, fId f) returnA -&lt; b 
They're issues which demonstrate the violations of type safety.
Done! https://ghc.haskell.org/trac/ghc/ticket/8993
We should specify what we mean by 'typesafe' in this context. The claim, I think, is that they do not statically disallow the generation of invalid SQL. This is a different claim than "does not statically disallow queries returning types other than claimed" or "allows `unsafePerformIO` to be written without invoking it directly." All are things we should strive for, but it is better to be clear about which ones are in question.
I agree there can be different meanings of "typesafe". Always producing valid SQL is one of the important ones for me, though others are welcome to define their version of "typesafe" to exclude this case if they want.
Afaik the stable release of Haskell Platform doesn't work with latest XCode command line tools because Apple dropped GCC. Besides, the release management of Haskell Platform doesn't really inspire confidence. Where are our [6 month releases](http://trac.haskell.org/haskell-platform/wiki/ReleaseTimetable)?
There appears to be some work that explores these ideas further in this paper that I mentioned in another thread: http://arxiv.org/pdf/0903.3311v3.pdf I haven't read through the paper yet, but I hope it will explain my observations that the effects of monads[1] can be commutative or idempotent. For example, the commutativity property is that do a1 &lt;- act1 a2 &lt;- act2 return (a1, a2) is the same action as do a2 &lt;- act2 a1 &lt;- act1 return (a1, a2) The idempotence property is that these two are the same. do a &lt;- act return (a, a) do a &lt;- act a' &lt;- act return (a, a') I hope that paper will generalise thoughts along these lines. [1] in fact applicatives 
Already noticed that. Still, the [download page](https://www.haskell.org/ghc/download_ghc_7_8_2#macosx_x86_64) claims that the entire XCode is required: &gt; This is a bindist for Mac OS X 10.9 (Mavericks). The package was built with and **requires** Xcode 5.1 (in particular, the "Unix Development Support") to be already installed. You can find Xcode at http://developer.apple.com/. Hence my question. Did I miss something?
Thanks! I'll put it on my reading list and try to attack it after I burn out on trying to pick through DCCD again. :)
Try https://chrome.google.com/webstore/detail/hackage-fu/dnpldbohleinhdgfnhlkofpgkdcfcfmf?hl=en-US It puts a link at the top right to the latest version or tells you you are looking at the latest documentation. It is the only plugin other than Ad-Block I bother to run. =)
also can we get an Index link on the contents page please thank you Fuuzetsu I love you :) 
&gt; Snoyman, Lessa, and presumably others in the Haskell community have created impressive query DSLs Low hanging fruit. Persistence for example initially worked only with postgres. And esqueleto works only with persistent. Persistent itself is very basic, does not even support joins. You cannot compare these very basic and very specialized (1,2 database types) tools with LINQ that supports all major commercial and open source databases and a full set of sql features. And that's exactly what OP is asking for. &gt;On the Scala side of the fence ScalaQuery, Squeryl, sqlTyped, etc. were all the work of solo developers. All piggybacking on a industrial strength jdbc backend that provides a full support practically for all commercial and open source databases. We do not have such industrial strength database backend in haskell ecosystem. Someone has to write it. And based on your own examples that someone is either very rich Oracle or very rich Microsoft. You chose. 
Vincent is doing a lot of hard work to support cryptography in Haskell. However, some of his recent work is worrying from a language-theoretic viewpoint: His random number generation is impure, and this has actually bitten me in production work. I don't know why he chose to go down that route, but I really expect him to return to pure code. If I can't trust his type signatures, I can't use his packages in production.
Many thanks for posting the new URLs. These papers have great examples and I like the DSH implementation with the list comprehensions - for me it is so much more readable than the other solutions presented in this thread. I am looking forward for the new release!
Maybe a rel="cannonical" tag pointing at the latest version will help with this. https://support.google.com/webmasters/answer/139066?hl=en
Because it comes with a complex RTS and hard-to-reason-about properties ? Also it might not be ported to your target architecture, while C certainly is available.
No love for Cloud-to-Butt Plus?
Nice! So these distance fields, if I'm reading the examples correctly, are basically like CSG operations from old game engines? I've wanted to have something like this after using SolidWorks... though it's obviously a very different use case.
https://github.com/haskell/hackage-server/pull/196
This is the first time I see something about `esqueleto`, and it's also all I need to see. If I wanted the ability to shoot myself in the foot, I'd use `unsafeRawSql` or something.
I think we're in "soundness versus completeness" territory here. A sound EDSL will only allow valid expressions, at the cost of expressiveness. A complete EDSL will allow you to express anything possible in the language, even if that results in also allowing invalid (or unsafe, or undesirable) expressions. I really like that if my Haskell code compiles, it's very likely that the code is pretty much correct. If I start using unsound EDSLs, like `esqueleto`, that feature might vanish. And it will, because I am a puny human, and I make *a lot* of mistakes.
That page seems to suggest that rel="canonical" is for when the content is actually the same, but that wouldn't be the case for documentation for different versions of a library.
I really do appreciate what you have to say, but postgresql-simple has quite a few users other than myself, and a number of commercial users too. And I'm specifically aware of users successfully using postgresql-simple on OpenBSD, FreeBSD, and Windows... the only big issue on other platforms is getting `postgresql-libpq` compiled, which unfortunately I can't help with much as I don't really use platforms other than Linux and occasionally FreeBSD and illumos. Yes, documentation was initially lacking, and the first few versions were rushed, as it very much was a project to scratch my own itch. Things have since matured; the biggest documentation holes have been filled in, and a lot of functionality has been added, not all of which was an immediate direct benefit to myself. (According to sloccount, the initial release was 1459 lines of code, the most recent release is 3898 lines. 353 of those lines are deprecated and ready to be removed completely.) I'm not particularly aware of major bugs in postgresql-simple at the present time, other than that the `Hstore` module doesn't handle nulls inside of `hstore` values (oops, but not fixable without changing the API), and some suspected performance issues. (It's still pretty fast for most things, and it's almost certainly a lot faster than HDBC) Also, the notification module uses a polling loop on Windows. (But that's as much a problem with GHC as it is with postgresql-simple; unfortunately Windows is definitely a second-tier platform when it comes to doing IO.) There is a major outstanding bug in `snaplet-postgresql-simple`, which is the bug that keeps getting mentioned, but that's a separate package not written by myself. I hope to contribute a fix within the next several months. Would I like something like LINQ? Probably, but getting there is not easy. Also, as somebody who tends to use non-standard and at times esoteric postgres-specific features, I'm a little skeptical that even LINQ supports some of these. I've never claimed that postgresql-simple is the best you can do, even for a basic access library, just that it was a small step forward. Honestly, I think it would be a very interesting to see someone create `odbc-simple`, that is, start with mysql-simple and port it to obdc, possibly pulling in some changes from postgresql-simple. This is what Janne did with sqlite-simple. And I didn't know that much about libpq when I started, either. (Though admittedly libpq is a lot simpler than odbc, which along with notification support, is why I used it instead of odbc.) It probably wouldn't even take too long , a couple of days tops; just get something working that scratches your own itch. Then let it grow.
I found your solutions overly complicated. :( I've uploaded my solutions [here](https://gist.github.com/wting/3f724d1bf46441f45dab). You should also check out various Haskell solutions [here](http://www.go-hero.net/jam/14/solutions).
So that might give problems for people who need to google older docs. 
Can't we ask Google to fix this? Maybe they just don't know.
Surprisingly i am not one of those who wants linq in haskell or cannot use haskell with db i'm working with. I was just answering the question of OP. He should not expect a product similar in features to linq (especially if he needs to work with commercial databases) in haskell anytime soon. Since no one is paying to make it. It's sad that people lash out when you simply point to the obvious state of affairs. I'm used to it in other programming communities. But with haskell's motto, i would think it should not be too hard to simply say "yep, we do not have it". What's wrong with that? 
&gt; What is with the hostility? You are the one calling names and trolling and i am having hostility issues? The nerve you have. &gt;That does not make "we need SQL server people using haskell in order to get something like LINQ" logical. How is it not logical? It is a complex and boring to make product that one one will spend their time on, especially considering that you would have to support several different databases. Who is going to pay for it? &gt;and insist on dragging that into everything. The subject of LINQ and interfacing DATABASES is "dragging it into everything"? &gt;Just so you know, LINQ is not nearly as perfect as you are presenting it. Who presents it as perfect? I'm merely saying we do not have anything similar and will not have it anytime soon. (not within at least 2 years) 
Thanks, I'll be looking at your solution!
Having read your presentation on your library, I can see why you'd think that. In the end, however, I still think that you're comparing apples to oranges if you want esqueleto to have the same kind of type-safety as a modern HaskellDB. The difference is that esqueleto is very close to the bare metal, i.e., SQL, and that's its purpose. If you know SQL, you just need to know a few syntax quirks to get to know esqueleto. Most of the time you're able to know exactly which SQL query is being generated at a glance. A HaskellDB-like library, OTOH, is miles away from SQL. That means you need to learn a completely different idiom for writing your queries, and you have little control of the generated SQL. In return, you get a lot more type-safety. I do feel there's space for the two kinds of libraries. In my own experience, using relational algebra and hoping for the best didn't fare well. Also in my own experience, I've shot myself in the foot with esqueleto maybe once or twice since I wrote the library.
It's better to fix the problem for everybody than insist that every Haskell user installs that extension.
&gt; But with haskell's motto, i would think it should not be too hard to simply say "yep, we do not have it". What's wrong with that? Nothing, really. But you have complained about odbc support, so why not try fixing it? The *-simple style interfaces have their flaws, but IMO they are still a big improvement over HDBC, and there's a decent chance you could fix some of the deeper problems you've complained about while you are at it. And odbc-simple would be an invaluable contribution to the *-simple community, (currently consisting of bos's mysql-simple, Chris Done's pgsql-simple, my own postgresql-simple, and Janne Hellsten's sqlite-simple) which I hope will eventually shed light on how to create a better HDBC, maybe even as good as or better than JDBC. (Although, my estimate of a couple of days is probably a little off, as thinking back, I did have the benefit of starting with a pretty decent binding to libpq, whereas there's nothing obviously comparable for odbc on hackage. And apparently Grant Monroe put a month or two of effort into the libpq bindings, and was planning a higher-level interface but didn't complete that.)
If only it could be simple to fix odbc on linux :) The reason you guys can write ffi bindings to postgres and mysql drivers is because both of those databases have native linux drivers. MS Sql does not have one. Instead there's 4 levels of indirection: unixodbc, freetds, hdbc and hdbc-odbc. Trust me i tried. The situation there is beyond fixing. Heck it would be easier for me to write a rest service from java to haskell and use java's mature jdbc drivers. TLDR: it is not as simple as picking up native FFI calls to mysql driver and somehow re-purpose them to work with MS Sql from linux. 
Not really, although I’d say distance fields subsume CSG operations. Here are [a few examples](http://www.iquilezles.org/www/articles/distfunctions/distfunctions.htm).
So confused. What is this "the" function?
I don't really know anything about Idris other than it's dependently typed, but it looks like it might be a way to specialize a type. So maybe the Haskell equivalent of that reverse example would be `reverse ([1, 2, 3] :: Num a =&gt; Vect 3 a)` (assuming that you could do that). `Vect 3 _` is a type of list that contains exactly three elements.
`the` is generally how you explicitly annotate a type in the middle of an expression. It is just `id` with an explicit type param. ([source](https://github.com/idris-lang/Idris-dev/blob/master/libs/prelude/Prelude/Basics.idr#L10-14))
That makes me sad. From the homebrew website: "Trivially create your own Homebrew packages." How much work would it be? Do you know why it goes / had gone wrong? I'm not too fussed about the platform and I'd be prepared to put in 1 day's work to do this.
I've uploaded my solutions [here](https://gist.github.com/sheaf/10622394). I think I kept it simple enough (although I had to rewrite the minesweeper one, the version I submitted was terrible).
Not found. And then: "This list can't be viewed". Is it set to private? I want to hear!
For really simple clients with socks and tls support, people can use [connection](http://hackage.haskell.org/package/connection) either directly, or as a base to get socks+tls.
My understanding is that OP is asking for an SQL generator not a database connectivity library.
Or we improve the hackage search to cover module names. Then you can go straight to hackage and search "control.lens". Also, did you know you can use hackage as a search provider in your browser? (Like you can have google, wikipedia, etc)
they are 404 for me.
Good point. I think pages like http://hackage.haskell.org/package/hedis-0.4.1 and http://hackage.haskell.org/package/hedis are similar enough for this to work. But the actual haddock pages for the modules might be too different.
There is a tradeoff. I think most people are looking for the latest version (or perhaps the latest platform version?) so this change would better serve them. Those looking for older versions would not be served well by this change. But I think it is still an improvement over the current "rank a random module version" that is happening now
My favourite bug of all time.
How do I declare a function ? I tried foo : int -&gt; int Am I missing something?
I agree. I was just offering a workaround that works today.
I noticed that Idris uses some kind of type-directed name resolution. I'm surprised I tend to like it: Idris&gt; 1 :: [] Can't disambiguate name: Prelude.List.::, Prelude.Stream.::, Prelude.Vect.:: Idris&gt; the (List _) (1 :: []) [1] : List Integer Idris&gt; the (Vect _ _) (1 :: []) [1] : Vect 1 Integer 
Would Haskell have actually prevented this bug? It seems like it could have happened regardless.
Part of the gap is caused by different goals in the language design. LINQ itself abuses some abilities that C#/VB are willing to offer that don't sit terribly well with the larger Haskell audience to get that syntactic elegance. Notably you can reflect on a lambda to get its entire syntax tree in LINQ. This means you have to keep it around and that many LINQ errors only get caught at runtime, and further, that the compiler has its hands tied w.r.t. optimization. That said, something could perhaps be built with something like `th-desugar` and explicit template-haskell splices that'd feel more LINQ like. I don't think it'd be a nice API though.
I expect to be able to make a release of DSH in two or three weeks. As Torsten said: We completely reworked the query compiler which is now (a) Haskell-only (no dependency on C code anymore) and (b) should produce better SQL code more often. A word of warning, though: Expect an interesting query DSL to play with, but do not expect a production-ready database library for your web application. This is still academic software... :) To get you some impression of how SQL queries in DSH look like: This is query 14 (slightly shortened) of the TPC-H benchmark for analytical queries, formulated in DSH: relevantShippings :: Text -&gt; Text -&gt; Integer -&gt; Q [(Text, Text)] relevantShippings sm1 sm2 date = [ pair (l_shipmodeQ l) (o_orderpriorityQ o) | o &lt;- orders , l &lt;- lineitems , o_orderkeyQ o == l_orderkeyQ l , l_shipmodeQ l `elem` toQ [sm1, sm2] , l_commitdateQ l &lt; l_receiptdateQ l , l_shipdateQ l &lt; l_commitdateQ l , l_receiptdateQ l &gt;= toQ date , l_receiptdateQ l &lt; (toQ date + 42) ] highlineCount :: Q [(Text, Text)] -&gt; Q Integer highlineCount ops = sum [ if op == "1-URGENT" || op == "2-HIGH" then 1 else 0 | op &lt;- map snd ops ] q12 :: Text -&gt; Text -&gt; Integer -&gt; Q [(Text, Integer, Integer)] q12 sm1 sm2 date = [ tuple3 shipmode (highlineCount g) | (view -&gt; (shipmode, g)) &lt;- groupWithKey fst (relevantShippings sm1 sm2 date) ] 
If this came across as an offense, I'm really sorry. It wasn't meant to be! I'm talking about cprng-aes, which I have always used as a high quality CSPRNG. My applications usually require it to be deterministic and from one version to the next weird bugs started to pop up that I had a really hard time reproducing. It took me hours to find out that the type signatures are lying. The generator is not deterministic and performs side effects in the background. I really think that Haskell functions should be pure, even if that means that you have to pull some monadic state around. I hope you can understand that I was really angry that day (I lost both time and money) and quickly removed all dependencies with your name in them. And this isn't something I can report as a bug, because you as the author don't seem to be considering this to be a bug. What would the title of that report be? It is a deliberate interface choice. It doesn't conform to the usual Haskell conventions and to my own idea of high quality APIs, so at the time I had no choice and went with the less convenient DRBG library. I really hope that you return to a pure API, because other than the above cprng-aes is a great library. IMO there is no "acceptable tradeoff" when it comes to purity. Side-effecting actions must be either IOs or STs in Haskell.
I understand your point, but there are only very few architectures that can run asymmetric cryptography code but can't run the Haskell RTS. For those other (and probably better) options are available. There is at least one guy who [generates his C++ from Agda](http://www.youtube.com/watch?v=vy5C-mlUQ1w). For the achitectures where Haskell is available we shouldn't interface with C code. We have the option to write a high quality pure Haskell TLS library, and Vincent is already delivering it. Better help him improve that one rather than going back to the legacy we want to get rid of in the first place.
If I get your query right, it could be formulated as follows in DSH: foos :: Integer -&gt; Q [Foo] foos fooId = [ f | f &lt;- foos, fIdQ f == fooId ] bars :: Integer -&gt; Q [Bar] bars barId = [ b | b &lt;- bars, bIdQ b == barId ] notPossible :: Q [Bar] notPossible = [ b | f &lt;- foos , b &lt;- bars, fIdQ f == fooIdQ b ] notPossible' :: Q [Bar] notPossible' = [ b | b &lt;- bars, fooIdQ b `elem` (map fidQ foos) ]
Have you looked at acid-state? Where LINQ to SQL (or LINQ to Entities) attempts to map classes to SQL tables, acid-state just serializes your types to disk. No more munging or data transfer objects. It has type-safe RPC too. Instead of using SQL for everything you get to use the right data-structure that fits what you're trying to model, and it's fast. If you're looking for syntactic elegance try using lens operators w/ acid-state, doesn't get much better than that IMO. You can monitor your acid-state memory footprint using EKG. Here's a screenshot: http://bit.ly/1n6s7KR. As long as your thunks get evaluated the memory footprint can be pretty negligible if you use the right types (i.e. bytestring or text over string).
Interestingly, the function "`the`" seems to originate in Common Lisp. It appears in the [CL Hyperspec](http://www.lispworks.com/documentation/HyperSpec/Body/s_the.htm#the) as a "special form" with the same meaning.
It's beautiful :')
There's a difference in emphasis. Understanding what's in this blogpost made me realize just why Haskell's abstraction are so great. It's not about the neat math, it has real world advantages. It's not about getting to write denser code with less functions, it's about having to understand less code. It's about realizing that even when an abstraction is trivial, there's a potentially huge advantage of defining/using it. Introductory tutorials miss this point because they focus on the how and not the why, and the advantages of abstraction don't really show up when you only do small projects. The fact that this is so blatantly obvious to you and me illustrates how well this works. I think this article is mostly to convince people to get into FP, and to guide beginners into big-picture thinking. 
To put it another way, it's better for code to be simple than impressive.
How do you use that for types that don't have a trivial value? 
Right, I suspect that the various Scala query DSLs perform similar behind the scenes hacks (although not to the language itself as is the case with C#) in order to provide a concise SQL-like syntax while providing some measure of type safety (in so far as Scala itself is type safe that is ;-)) Persistent + Esqueleto looks to be the query DSL of choice for production systems as of right now, so it's not like there's a gaping hole in the Haskell ecosystem vis-a-vis type safe database access. Saying that, the various research projects and experimental libraries mentioned in this thread illustrate that maybe there's a demand in the Haskell community to take type safe queries to another level. Will be fun to find out, lot's happening in Haskell land...
Behold the power of dependent types!
This works ``1 `asTypeOf` (undefined::Int)`` if I understand your question.
I found out about it late, and wasn't able to devote the time to it, but I did solve Cookie Clicker Alpha. Details (and code behind a break) at http://agingnerd.blogspot.com/2014/04/well-drat-spoiler-alert.html
Thank you!
Oh yeah that makes sense. The value is never evaluated but the type is inferred. That makes it a first class version of scoped type variable annotations, right?
&gt; For example, a Category is just a typed monoid, where not all combinations type-check Probably better to say a monoid is like a degenerate category. There _is_ some intuition in the idea we've "stretched out" a monoid, but the whole complexity of categories comes in the structure of the 'type-check'. &gt; ... and an Applicative is like a monoid where we combine functors "horizontally" and here it really breaks down because there _is_ such a thing as a plain old monoidal functor, but there's much more going on. for one thing it has to be between two categories that each have their own internal notion of a product or tensoring operation. for another thing, in Haskell we don't just have "monoidal functors" but _closed monoidal functors_ which provide an abstraction not only of _tensoring_ but of _application_, and to talk about that one needs a notion of an "internal hom" among other things. You can understand Applicative without any of that. But if you want to understand the categorical stuff going on, then you _must_ talk about that. I'm all for discussing the power of abstractions, but we should be careful not to get so caught up in their _simplest_ elements that we don't also convey the power in the complexity these abstractions give rise to. 
Is this any different than GHC complaining about ambiguity?
its easy to write your own, yes, but the issue is making sure the "default" ghc formulae aren't wrong. and.... thats a full time monitoring task. theres 1-2 people who may take that up soon allegedly :) (but not me)
Well a groupoid is simpler than a group I think, and much more fundamental. My point however was more that category theory is something that lets us _model_ monoids, etc. So why start with monoids and then "twist the intuition" rather than starting with categories and giving the description precisely? And similarly we can then define groupoids and groups from this same framework? (Although now, in HoTT-style thinking we might in fact say groupoids come prior, and categories are just groupoids with some additional structure [i.e. non-invertible morphisms]..) (I also note that tekmo already concedes in the comments his approach is backwards. so i'm just suggesting his pedagogic experiment is not going the right way, and my comment on categories should be read in concert with my following comment, to make the more general argument that we have to look at how complex behaviours arise from simple rules, and not just the simple rules we start with)
After having carefully discussed his requirements with him I strongly get the impression he is after an SQL generator. After all, he says "no interest in going back to string based SQL". Either he already has his own way of running the SQL or he's in for a nasty surprise (as you point out), but in any case how to run the SQL wasn't his *current* question.
The point is in how to disambiguate. In Haskell you can disambiguate only by saying exactly which function you want (typically achieved via qualified imports), not simply by providing a type signature.
Off the top of my head, [Pretty Printing](http://hackage.haskell.org/package/FPretty-1.0/docs/Text-PrettyPrint-FPretty.html), and [Parsers](http://hackage.haskell.org/package/parsec-3.1.5/docs/Text-ParserCombinators-Parsec-Combinator.html) are examples of this. Pretty much look on Hayoo, and click around looking for anything with a signature a -&gt; a -&gt; a. Here's one for [Music](http://hackage.haskell.org/package/lilypond-1.6/docs/Music-Lilypond.html#g:11) for example.
This works for constructors but I don't think anything else. But for constructors its pretty neat. I think agda does this too.
I have actually realized after posting that comment on Tekmo's blog that S. Awodey in Category Theory wrote (p.12): &gt; In this sense, categories are also generalized monoids
Then you could just do `1 :: Int`. The point of `the` is to avoid inline type signatures.
While it is apparently not documented anywhere, it seems to be more general than constructors. For example, this works and picks the right concatenation function: \a,x,y =&gt; reverse (the (List a) (x ++ y))
[diagrams](http://projects.haskell.org/diagrams/doc/quickstart.html#philosophy) is another good example.
Except with typeclasses
Yeah, I also prefer to think of a monoid as a special case of a category. I was just experimenting with the opposite pedagogical approach for this post because I've seen some people teach it this way (i.e. a category is a "monoid-oid") and I wanted to try it out for fun.
None, yet. I want to finish fleshing out the skeleton of the book before I start making the work public. My plan is to commit my work in the open on Github, and I will blog about it once I begin doing that.
I would imagine that, if you're learning your cryptography material well enough, you should be able to port any existing algorithms to Haskell. One thing that might be of help is [the `State` monad](http://www.haskell.org/haskellwiki/State_Monad). I've never done any cryptography, but the State monad is nice for implementing known algorithms in general (in my experience) because it removes the pain of dealing with implicit state, and makes it a lot easier to read and understand your own code. I've implemented a couple of graph theory algorithms using the State monad and it has really helped me, so I thought I might mention it. In general, I don't think that learning the two things concurrently is a bad idea, because the topics are largely disjoint.
Some programmers enjoy reading a classic programming manual written for one programming language, and practicing the examples in a new language they're trying to learn. Have you considered following along Applied Cryptography, reading the C code, and translating it into Haskell?
Indeed. However, each time I disambiguate a typeclass function by an explicit signature, I have a strange feeling that something is wrong. Backward.
I'm doing the Matasano Crypto Challenge (http://www.matasano.com/articles/crypto-challenges/) usign Haskell, exactly for that purpose (and having lots of fun with it!)
The tricky bit is that you could presumably be storing trees of any height, ~~so `Tree depth Int` is the only possible return type~~ (I'm silly, see gelisam's comment below) From there it's on you to use pattern matching and attempt to determine the height manually, GHC can then be beaten into accepting the more specific type. So yes, it's absolutely possible, and doing this will ensure that you handle trees of any height. The syntax you where looking for by the way is {-# LANGUAGE GADTs, DataKinds, KindSignatures, TypeOperators #-} import GHC.TypeLits data Tree :: * -&gt; Nat -&gt; * where Leaf :: Tree a 0 Node :: Tree a n -&gt; a -&gt; Tree a n -&gt; Tree a (n + 1)
Are we talking actual in-process referential transparency violations or something being different across runs?
Now that is neat. I definitely have something to make progress on now. Thanks a ton!
Sounds like you want to persist a trie. There's a project called acid-state worth looking at, but I have not tried it personally.
Applied Cryptography is a great book, and I agree that it would be a good one to work through in order to do exercises in Haskell and learn about algorithms. Note, however, that algorithms are only one part of security as a whole, and Applied Cryptography by itself is nowhere near sufficient in learning about cryptography. The author realized this and co-authored a book titled Cryptography Engineering to help tackle the issue. See the following article for details: http://sockpuppet.org/blog/2013/07/22/applied-practical-cryptography/
Nitpick: `Tree depth Int` is not the correct return type, because `load` is not able to produce a value of type `Tree depth Int` for any `depth` you request; on the contrary, it is going to produce a value of type `Tree depth Int` for a fixed `depth` of its choosing. Therefore, in addition to using GADTs and type-level naturals to encode a dependent type, we also need to use existential types to encode a sigma type: data SomeTree a = forall depth. SomeTree (Tree depth a) load :: DBConn -&gt; SomeID -&gt; IO (SomeTree Int)
Codecademy does have a course creator, see http://www.codecademy.com/teach – they only support JS, Python, Ruby and Web though. You could always submit more exercises for exercism :)
This is only tangential but after playing a bit with TypeLits I’m not too sure on how to define a `Foldable` instance for such trees (flipping around the type parameters to fit the mold of `Foldable`). The first thing that came to mind was to do a type-level case analysis on the depth. While defining a `Foldable (Tree 0)` instance goes well, I’m not too sure on how to proceed for the general case. `Nat` is opaque (i.e. no data constructors) and GHC complains for something of the style `instance Foldable (Tree n) =&gt; Foldable (Tree (n + 1))`: Illegal type synonym family application in instance: n + 1 In the instance declaration for `Foldable (Tree (n + 1))' (I don’t know what to make of [this bit of information](https://ghc.haskell.org/trac/ghc/wiki/TypeNats/MatchingOnNats).) This is using 7.6.2, I don’t have access to anything more recent.
As a student, some suggestions: 1. If you can avoid it, don't give out skeleton programs. I had a teacher who did this in my systems programming class, and I learned next-to-nothing. It's better for students to have to look up how to it than for it to be there in front of them, even if it's something as small as defining a type signature for them. They'll learn more without a skeleton. 2. It looks like you're trying to cover too much for a standard-length semester, especially for a relatively low-level course (I notice the pre-requisites aren't too extensive). Maybe try to cover more basic things in greater detail. It's better for a student to come out with a perfect intuition about `Applicative`, `Functor`, `Monoid`, folds, maps and filters, etc, than to be acquainted with more complex Monads (`Parsec`, `Free f`, e.g), Lenses and Transformers but lack intuition about previous topics. 3. I really like the idea behind building the interpreter. I hope that you're able to do this to show off a glance of real program architecture, even if you're teaching them the basics of Monad Transformers as you do it. At the same time, playing into (2), I'm not sure you'll have time. It might be better to take a step back and give them a slightly less demanding final project. Looks like a fun class that I'd have enjoyed, just make sure that your students have a good understanding of the basics before moving on -- it's easy to forget how hard even the basic stuff was to grasp the first time around. I did a talk on introductory Haskell to upper-level undergrads last week and seriously underestimated exactly this. Basically, expect to move a bit slower than you might want to, and your students will come out with a solid understanding (and possibly love) of Haskell.
I appreciate the feedback and agree with it. I will stick to my original approach in the future. 
I suspect you can't since the `f` would be constantly changing. You could opt for an existential box, but I don't think you'll find anything too helpful in this approach 
Whenever I learn a new language (I'm a dabbler, it's just fun for me) I read along in some beginner's books and just reimplement in the new language. Since you wan to learn Haskell and are also interested in crypto, try both with an easier crypto text - no need to dive right into more complicated stuff like producing an elliptic curve library ;). I recommend [Hacking Secret Cipher with Python](http://inventwithpython.com/hacking/) - just follow along in Haskell instead!
I just chatted with Duncan Coutts in #haskell, and here are his comments: &gt; on a minor point, yes of course it should do build-tools, it's just never been implemented &gt; as for the global vs. minimal, yes I see it's an issue. One sometimes wants to start a sandbox with a very minimal set. &gt; the issue with making a minimal sandbox is simply knowing which packages should be in it, and we currently just base them all off of the global one. &gt; but that's not the only solution, it'd also be possible to copy a subset, if we know what subset to copy &gt; and I have a somewhat cunning plan along these lines (related to some other ghc-pkg/cabal improvement work) which might make that rather easier &gt; what I want is for ghc itself to come with multiple profiles, with one being the minimum (base + rts + deps), and that could be used as a basis for new envs
I would put QuickCheck and such into it's own section for Testing and verification. Building an Interpreter might be a better place to introduce free monads. The way things are set up, it is really fast paced. This does not feel like what is normally a 3 credit class. If every section was 1 week with 3 1-hour classes or 2 1.5-hour classes, the excessively ambitious sections are: higher order functions, type classes, building an interpreter. *Defining Types and Typeclasses* leaves me with an unsettling feeling. I think that you should spend more time on basic types, (possibly bringing in functor, applicative in the Maybe a, Either a b cases). If you're going to introduce Typeclasses, I would suggest focusing on very helpful or general ones, like Monoid, Ord, Eq
I would re-word the course abstract to be less sales-pitchy. I've heard the "Haskell will change your freaking life, wake up sheeple" thing before (probably out of my own mouth) and now it's just tired. Not sure how it sounds to someone unfamiliar with the language, though.
Attempting to learn both Haskell and cryptography at the same time is going to be difficult. If you only care about learning the basics (this is how this algorithm functions, here's how I use it to authenticate / encrypt / whatever), then I think it's totally reasonable and would recommend going implementing as much of applied cryptography as possible. If the goal is to learn how to *actually* implement crypto, then I think you'd be better off splitting the projects, or postponing the crypto piece. Reasoning about things like timing-attacks is more difficult in Haskell and is likely going to unnecessarily slow or stall progress on both fronts.
Not the easiest read, but worth it.
I don't think the script style is very useful for pedagogy.
No that isn't what it says. It could be worded better. The "Unix Development Support" is the command line tools.
I actually used haskell to do the problems for the coursera cryptography course. I had no problems with it. But I already knew some haskell. 
It works for me in this case, but I'm glad it's not commonly used.
interesting fact: a ~ forall r. Cont r a Either e a ~ forall r. Throws r e a which should be some hint as to why these make sense as ways to do computation in this fashion *Edit: this is sort of the culmination of the post, but I figure its worth putting upfront*
I don't see much pedagogical value in either approach unless one doesn't want to to read an introduction on category theory and an introduction on monoids. For the former, I'd suggest the first chapter of ["Category Theory (2nd ed.)"](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.211.4754&amp;rep=rep1&amp;type=pdf) by S. Awodey and for the latter, Wikipedia's entry on [monoids](http://en.wikipedia.org/wiki/Monoids). 
This! I'm also one of those who steer away from the HP, and basically just need `ghc` (which comes with `haddock` and the necessary set of GHC boot libs), `cabal`, `happy`, and `alex`. I really can't understand why the HP needs to come with all those other packages I could easily `cabal install` if I really need them.
How can `cabal install happy` fail when building it via Setup.lhs succeeds? I have run into this problem myself, but assumed that if one failed the other would inevitably fail also.
Somebody get this one a medal! And a beer!
Is there something that comes before this? It feels like a part of a series relating to previous concepts.
Btw, there have been talks to allow GHC's package database to allow to handle multiple package instances, as the next step beyond Cabal sandboxes: - http://permalink.gmane.org/gmane.comp.lang.haskell.cabal.devel/9655 - https://ghc.haskell.org/trac/ghc/wiki/Commentary/Packages/MultiInstances Otoh, the big elephant in the room may be the `ghc` package which is the main cause that GHC has to ship with most of its bundled packages. Fortunately, only few packages need to actually depend on `ghc` (mostly those doing dynamic Haskell code compilation/interpretation)
It's not the first functional pearl I see adopting this style ; I hate it with a passion.
Same. Just because Plato did it does not make it appropriate for computer science :p
Any reason to not use the existing python interface [cpython](http://hackage.haskell.org/package/cpython). Is there anything missing from there that didn't allow you do what you needed? Obviously, maybe you just wanted to do it yourself for fun, which is totally understandable.
Partly just for fun, but mainly because of its license. Besides, it hasn't been updated in over a year, and it's for Python 3.x which isn't available on OS X by default. But if it wasn't for the license, I'd probably have used it anyway. I wondered why he chose GPL-3 for a binding to something that's distributed under a liberal license, given that he doesn't seem to have a fundamental objection to liberal licenses (there are other packages of his that are MIT), but since it turned out to be easy enough to wrap Python myself I didn't bother to ask him.
Is it because it takes too long? Or because they just don't lay everything out at once? Normally I think this presentation works best for people just starting off. They are incredible hard to write well however since most people after they understand the topic throughly are no longer completely cognizant where the tripping points are for a beginner.
It has more to do with the fact that this is not what I'm looking for in a CS paper. And, let's be honest, good fiction writers are scarce in our trade...
It seems all the source code is part of the blog. Do you intent to package it separately and put it on Hackage ?
For a moment I thought you were talking about the curly typeface used; Cs and Ds and Es &amp;mdash; in superscript too! &amp;mdash; were all blending into one.
Good post, and great work!
Probably, if there's demand for it, and if someone steps up to help. I don't have time for another package to maintain all by myself. Note that there's still a long way to go from this code to a real Pygments binding, let alone a complete binding to the Python API. As of now, the code just covers my own needs and doesn't account for all the rest of Pygments' features and API.
I bet /r/compilers would appreciate an xpost.
I've been reading it and PFPL in a breadth-first method. Both books cover similar topics but from different angles, so I have found that doing a comparative reading is valuable. Skimming through both books and then diving into more interesting topics is also really useful since sometimes unfamiliar things can be learned by seeing them in sufficient separate contexts. That said, I'm going in with some familiarity with the style of reasoning used in both books and a little bit of experience building typed languages of my own. I'd recommend implementing Algorithm W, perhaps, to see an intermediate topic of those books in real code. You'll be familiar with its operation from Haskell and each piece is fairly obvious to understand on its own. Then later, as the books reach those topics you'll have a good place to stand.
Other options include the Rank-2 CPS equivalent: load :: forall a r . DBConn -&gt; SomeID -&gt; (forall depth . Tree depth a -&gt; IO r) -&gt; IO r And the dirty runtime check: load :: KnownNat depth =&gt; DBConn -&gt; SomeID -&gt; IO (Maybe (Tree depth a))
Nice article. I like how you managed to do this without learning Haskell, lol. Have you considered porting GHC to [Haiku OS](https://www.haiku-os.org/)?
I am not familiar with the "dirty runtime check" solution. How does it work?
The end goal is something like a persisted trie, but it's really an excuse to try to understand type-level naturals. Thanks for the suggestion though :) 
Mark Jones' "Typing Haskell in Haskell" is recommended as well, although I find it more difficult for two reasons: firstly, it types all of Haskell which is an extension atop Algorithm W and secondly it tends to describe the implementation directly and less the reasoning behind it. Still a good paper to take a read through while studying this stuff! http://web.cecs.pdx.edu/~mpj/thih/
Thank you!
If this is as solid as it seems from a brief glance at the tutorial, this is AWESOME. I can't wait to try it out on a fun toy multiprocessor or something. What are some of the limitations you've found? Is this ready for actual serious use? If not, why not, and will it be? 
What's this about a linaro arm64 ghc gsoc? (Saw this mentioned at the end of the article. )
Can definitely skip over the sections on subtyping, objects, and "featherweight Java". I think the most useful bit for me was reading "The Essence of ML" in ATTaPL which walks through a more fully featured implementation of a typechecker.
Chapters 1-11 you pretty much need for the rest of the book. After that, a lot of sections are more or less independent, except that chapters 29-31 depend (pretty obviously) on earlier chapters that they generalize. On first read through, I skimmed through 1-7 (having dealt with untyped languages before), read 8-11, and then jumped to chapters 22-24 because I was interested. Then went back through many of the other chapters, dealing with subtyping in a pretty cursory way.
Courses are generally not available on video. However, there are several individual talks available for free on the Skills Matter pages. For example, here are mine : https://www.skillsmatter.com/members/kosmikus#skillscasts
Something I never would have considered (b/c license). I guess I hear of a lot of people who don't use the [hmatrix](http://hackage.haskell.org/package/hmatrix) because of its GPL license. You can actually use it with 2.x python, if you use an older version of the package.
But that's standard typeface for categories.
Porting to a different OS is an entirely different kind of beast. "All" he was doing (not to discount the work which was very impressive) was bootstrapping an existing compile process to a different machine architecture. Unless it was a blocking change he could leave the code intact (allowing him to not know Haskell and do this)
I'm not sure and I doubt it. I found it as a google search result. If you find more to the series please let me know.
Ok, so I've made some progress along this route, but I think I've hit a block where I can only have one of two desirable functions. In both cases, I've defined my tree like this: data Tree :: * -&gt; Nat -&gt; * where Leaf :: a -&gt; Tree a 0 Node :: Tree a n -&gt; Tree a n -&gt; Tree a (n + 1) Now, I want a depth function and a "create from input" function. To satisfy the depth function (and give an existentially-qualified type to wrap it), I can do this: depth :: KnownNat n =&gt; Tree a n -&gt; Int depth = fromEnum . go Proxy where go :: KnownNat n =&gt; Proxy n -&gt; Tree a n -&gt; Integer go p _ = natVal p data SomeTree a = forall depth. KnownNat depth =&gt; SomeTree (Tree a depth) depth' :: SomeTree a -&gt; Int depth' (SomeTree t) = depth t That compiles and works nicely: *Main&gt; let t = Node (Leaf 3) (Leaf 4) *Main&gt; depth t 1 *Main&gt; let t' = SomeTree t *Main&gt; depth' t' 1 But, I also want to be able to create some sort of tree dynamically. To do this, I need to drop the "KnownNat depth" from my SomeTree, or I get errors (pasted in later). So, a version that can dynamically create stuff looks like this: data SomeTree a = forall depth. SomeTree (Tree a depth) withDepth :: Int -&gt; a -&gt; SomeTree a withDepth 0 n = SomeTree $ Leaf n withDepth x n = case withDepth (x-1) n of SomeTree x -&gt; SomeTree $ Node x x dynamically :: IO (SomeTree String) dynamically = do depth &lt;- readLn :: IO Int return $ withDepth depth "hi" And, that can work (I have Show instances for my trees that I elided): *Main&gt; dynamically 3 SomeTree (((Leaf "hi") (Leaf "hi")) ((Leaf "hi") (Leaf "hi"))) (((Leaf "hi") (Leaf "hi")) ((Leaf "hi") (Leaf "hi"))) So, that's neat. I can do everything I wanted to do. But, if I try to use my depth definition with the SomeTree that lacks the KnownNat restriction, I get this error: No instance for (KnownNat depth) arising from a use of `depth' Possible fix: add (KnownNat depth) to the context of the data constructor `SomeTree' or the type signature for depth' :: SomeTree a -&gt; Int In the expression: depth t In an equation for depth': depth' (SomeTree t) = depth t On the other hand, if I keep the KnownNat, I get this error: Could not deduce (KnownNat (depth + 1)) arising from a use of `SomeTree' from the context (KnownNat depth) bound by a pattern with constructor SomeTree :: forall a (depth :: Nat). KnownNat depth =&gt; Tree a depth -&gt; SomeTree a, in a case alternative at Main.hs:35:13-22 In the expression: SomeTree In the expression: SomeTree $ Node x x In a case alternative: SomeTree x -&gt; SomeTree $ Node x x Any ideas about how to reconcile these two functions? I read the haddocks on TypeLits, and got depth to pretty much work, but now I'm stumped again. EDIT: I've put my "working" snippet into a gist: https://gist.github.com/tsuraan/10761544#file-fun-with-typelits
I normally see the typeface used in [this document](http://wwwhome.ewi.utwente.nl/~fokkinga/mmf92b.pdf), for example the first normal paragraph on page 8, "By default, A,B,C ...".
We use Haskell Platform with no problems. And there is a big advantage to using it - you are using a standard dialect of Haskell, so a lot more things "just work". But if you are trying newer things I suppose having the platform installed globally might get in the way occasionally. The set of packages that come with the platform isn't *that* big. Where do you run into trouble? Another approach to help people who don't want the platform installed globally is simply to install it locally from Hackage. Is there any problem with that? 
FYI, on G+ [I discussed licensing of cpython with John Millikin](https://plus.google.com/113481334831388770945/posts/f5TG1W5AbJM)
Ok, so then we could do: newtype ContT' r w a = ContT' {runContT' :: (w a -&gt; r) -&gt; r)} ContT' r (Either e) a ~ Throws r e a This is obvious, since `Either e a -&gt; r ~ (e -&gt; r, a -&gt; r)` because that's how coproducts are defined, and with currying you get `Throws`. Great! I wonder how useful that would be. 
I really like stories like this - they're always so fun! It's all the more spectacular by the fact Colin isn't really a Haskeller. Thank you so much, Colin! When I saw that bug and merged it I was extremely startled at the subtle nature of it. It is definitely a contender for "most strange bug" I have seen in recent memory - a top contender to Gustavo Luiz Duarte's amazing [PPC64 *big endian* fix in Nov. 2013](https://github.com/ghc/ghc/commit/a4b1a43542b11d09dd3b603d82c5a0e99da67d74)!
Until now, we have only used CLaSH for small, perhaps medium-sized, circuits in terms of complexity. In terms of size, even the simplest circuits can fill up your entire FPGA. I'm currently in the process of porting over this code to the new version of CLaSH: https://github.com/christiaanb/DE1-Cyclone-II-FPGA-Board-Support-Package. It does: * Tone generation using the CORDIC algorithm * Interface to a PS/2 keyboard * Communication with an audio peripheral * Audio filtering to do simple high- and low-pass filtering (using an FIR filter) * FFT to analyze the audio * Display a so-called "Waterfall" plot of the audio spectrum on a VGA monitor. I hope to update the website soonish with an overview of the kind of circuits we have built with it, including code where possible. As far as limitations go: * Long compilation times and high memory usage on functions with alot of terms (e.g. a large vector literal). Although if you are used to synthesis tools, the CLaSH compilation time is not too long. * Limited manpower to fix bugs: it's just me right now, and I'm supposed to be writing my phd thesis * Currently no support for recursive functions, but that is amortized by builtin support for the usual set of higher-order functions (map, foldr, iterate, etc.). I usually don't miss them when I'm doing circuit design, and it has actually taught me to use higher-order functions more often in my own Haskell code. Note that recursively defined values are allowed, just not recursive functions. Adding support for this will take some time due to the me writing my thesis thing.
So I looked on ICFPs website... Is there anywhere you can sign up to go? Is it free/costs money (I imagine the later)? How much does it cost? Or is the kind of thing you get invited to? 
Good question. I understand that it costs money, in the region of $100, but that ICFP determine and manage this, and they haven't opened registration yet. I'll ask.
Hmm... Well, I guess you could move the addition to the context side. With enough extensions, the following compiles: class Foo a instance Foo (Tree 0 a) instance ( Foo (Tree n a), sn ~ (1 + n) ) =&gt; Foo (Tree sn a) Of course, your method implementations are probably going to recur on smaller trees, at which point you'll get a confusing error saying that it can't deduce `n2 ~ (1 + n)` in the recursive call, or something like that. I haven't figured out how to reason about those kinds of error messages yet. I don't like type-level naturals.
It's the first time I've seen `KnownNat`, what does it do? Maybe it only works for `Nat`s whose value is known at compile time, hence the name? Anyway, if you compute the depth in a normal way, without relying on this weird `KnownNat` magic, it works just fine. depth :: Tree a n -&gt; Int depth (Leaf _) = 0 depth (Node t1 _) = 1 + depth t1 depth' :: SomeTree a -&gt; Int depth' (SomeTree t) = depth t
This is a really nice interface for handling types. An interesting possibility that comes up in my mind is that [IHaskell Notebook](https://github.com/gibiansky/IHaskell) could optionally print greyed out types for expressions to the right of expressions as you type.
On the main complaint: if you're finding that you have to get the types right on too many expressions before the code can be run, it's a sign you're writing too many expressions at once :)
Ah, thank you! That's another thing lukexi pointed out just after I posted. :) Makes sense.
Can anyone outline the implementation of the following? "Writing a C-printf-style function where the first argument determines the number of arguments that are expected afterwards as well as their types."
FRP frameworks in Haskell are eager (I believe) so you could get the same debugger.
Here's an example https://github.com/liamoc/learn-you-an-agda/blob/master/Printf.agda
There's this https://hackage.haskell.org/package/Printf-TH, but it uses TH, not haskelly dependentish types
You can do it with GADTs and TFs, but the format strings are their own special type. You can make a quasiquoter for them though.
Hah this really is quite cool, and you beat me to it by at least a few months, although of course with a different language. It's something that I've wanted to do with GHCJS for a long time (but on a somewhat lower level, running all graph reduction under the stepper, not just FRP events/state). I think having tools like this available will really help sell the concept of FRP and make it more practical for bigger systems. The plan for GHCJS is to integrate memory profiling with this, so it's easier to find out what events and state updates are using up memory (since Haskell's laziness does make FRP systems a bit more prone to memory leaks). 
thx, i like the fetching script. my current workflow only opens hoogle/hayoo with the os x selection. The prefetch is a great addition.
as usual, [oleg has some good stuff on that](http://okmij.org/ftp/typed-formatting/)
From the discussion on the mailing list, it seems that a big benefit of putting other packages in Haskell Platform is that it makes installing the packages easier on Windows, where building external dependencies is non-trivial. Haskell Platform therefore currently includes a variety of common packages, such as HUnit, as well as packages that would otherwise be difficult to install (on Windows), such as the OpenGL packages. Beginners can easily install a Haskell development environment in which they can learn and do quite a bit before they have to start using Cabal.
FYI: I created an issue: https://github.com/haskell/cabal/issues/1777
Personally, I have not run into problems with Haskell Platform either, as I can always install newer versions of packages into the sandbox. (I am researching the minimal --global idea after seeing recommendations for it.) To anybody who avoids Haskell Platform due to problems, could you please let me know of specific examples, either here or in reply to the following mailing list post? http://www.haskell.org/pipermail/haskell-cafe/2014-April/113662.html As for your second question, it seems that many people avoid Haskell Platform and just install GHC, Cabal-Install, Alex, and Happy without issue.
I second this request. It's been claimed that the HP is problematic, but I wonder **which packages** people are having to upgrade past the HP version? And what problems have arisen when upgrading?
The FSF has a comprehensive [FAQ](https://www.gnu.org/licenses/gpl-faq) on their licenses, and then there is the [Software Freedom Law Center](https://www.softwarefreedom.org/).
It depends on your audience, it is of only limited use to introduce these abstractions without knowing a lot of examples anyways. More often than not the really hard or interesting questions are tied to very specific examples, "abstract nonsense" like groups are simply a by product of thinking about laws something you want to describe has. In the case of a category that could be "processes that are composable", monoidal categories are "multiple processes that can happen simultaneously and can be composed". If you now think of a group as a category with one object, such that every morphism is an isomorphism, it is clear that it formalizes reversible operations on one object (symmetries), whereas a monoid does formalize potentially irreversible operations on one object. The only sort of weird thing going from this informal description to the actual definitions is perhaps the inclusion of the "do nothing operation". The beauty of this approach is, that it allows you to speak about "structured objects" in other categories as simply functors from the abstract structure to some concrete representation, for example a topological group is simply a functor from the group seen as a category with one object and only isomorphisms to the category of topological spaces. And gives you also a notion of how to map such structured objects onto each other, i.e. the maps should be induced by natural transformations between those objects. A precise definition of what a natural transformation is, was the main motivation for the invention of category theory, but again the inventors had a lot of examples in mind before they came up with a definition. One problem I have with the way category theory is presented to people with no mathematical background in Haskell is that the presentation does often not distinguish between the representation and the abstract definition and deemphasizes the laws required because those can't be encoded or checked within Haskell. For example a Category is not "just" what the article claims, rather certain small categories can be represented in this way faithfully. A lot of other categories have no faithful representation in Haskell.
Bugs like Heartbleed are impossible by default in Haskell, due to [memory safety](http://en.m.wikipedia.org/wiki/Memory_safety).
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Memory safety**](https://en.wikipedia.org/wiki/Memory%20safety): [](#sfw) --- &gt; &gt;__Memory safety__ is a concern in software development that aims to avoid [software bugs](https://en.wikipedia.org/wiki/Software_bugs) that cause [security vulnerabilities](https://en.wikipedia.org/wiki/Vulnerability_(computing\)) dealing with [random-access memory](https://en.wikipedia.org/wiki/Random-access_memory) (RAM) access, such as [buffer overflows](https://en.wikipedia.org/wiki/Buffer_overflow) and [dangling pointers](https://en.wikipedia.org/wiki/Dangling_pointer). &gt;Computer languages such as [C](https://en.wikipedia.org/wiki/C_(programming_language\)) and [C++](https://en.wikipedia.org/wiki/C%2B%2B) that support arbitrary [pointer](https://en.wikipedia.org/wiki/Pointer_(computer_programming\)) arithmetic, casting, and [deallocation](https://en.wikipedia.org/wiki/Memory_deallocation) are typically not memory safe. There are several different approaches to find errors in such languages: see the Detection section below. &gt;Most high-level [programming languages](https://en.wikipedia.org/wiki/Programming_language) avoid the problem by disallowing pointer arithmetic and casting entirely, and by enforcing [tracing garbage collection](https://en.wikipedia.org/wiki/Garbage_collection_(computer_science\)) as the sole memory management scheme. [*[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed)*] &gt; --- ^Interesting: [^Buffer ^overflow](https://en.wikipedia.org/wiki/Buffer_overflow) ^| [^Dangling ^pointer](https://en.wikipedia.org/wiki/Dangling_pointer) ^| [^Vulnerability ^\(computing)](https://en.wikipedia.org/wiki/Vulnerability_\(computing\)) ^| [^Type ^safety](https://en.wikipedia.org/wiki/Type_safety) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cgtq2k6) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cgtq2k6)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Take a look at `Data.Dynamic.fromDynamic` for some inspiration.
Please start from Monad transformers and go from there. Everyone already knows how to do the easy stuff. The hard stuff is hard, which is why a class would help to learn it. For people who want to learn haskell, it's already easy to learn the easy stuff. 40 Helens agree. 
Thank you so much for the link. Indeed Skills Matter provides [a lot of free Haskell presentations](https://www.skillsmatter.com/explore?sort_by=desc&amp;q=tag%3Ahaskell&amp;location=&amp;content=skillscasts) similar to the ones you've recommended. My apologies for the false accusation.
Hi, I just had this idea last evening. Is there another lib on Hackage exposing such a behavior? I checked monads-tf, monad-control and that's not quite what they aim at doing.
Have you seen `formatting`? http://www.reddit.com/r/haskell/comments/233njg/blog_post_haskell_gets_static_typing_right_andres/cgtrk2u
Well done! Awesomely "visual" case of how purity makes things simpler to reason about.
Do I understand correct that `(=&lt;&lt;) c` would be the same as `b c`?
I don't understand what this does. Can you give more examples?
&gt; Though, of course, it's entirely John Millikin's decision what license he chooses to share his work under, Naturally. &gt; I have found it odd in the past that GHC is under a liberal license, and Python is under a liberal license, yet a comparatively small project to link the two is GPL. Exactly. The more, since most of the bindings seem to be generated automatically with `c2hs`. &gt; This has prevented me from using cpython in the past. Same here. Some of the arguments he used in our discussion on G+ did put me off as well. His sentiments on software licensing are a far too ideological for my liking. Besides, the lack of a public issue tracker and of a browseable public repository make me stay away from his projects as well. I can't assess how he handles bug reports, and I can't see easily how he frequently he works on his projects, so I find it quite hard to trust his works. 
No problem. **EDIT:** I actually replaced my README on github by this explanation. I too find my original example was lacking. The idea is that functions like readerAct :: Int -&gt; Int -&gt; Reader Int String readerAct x y = do z &lt;- ask return (show (x+y+z)) Are difficult to reuse, because they force you to run in Reader. MTL came with the solution: readerAct :: (MonadReader Int m) =&gt; Int -&gt; Int -&gt; m String readerAct x y = do z &lt;- ask return (show (x+y+z)) It allows readerAct to be use with a monad transformer, but it needs to introduce a new class per Monad, and to write an instance of each class for each Monad. So if I have 4 monads (Reader, Writer, State, Maybe), I will need to write 16 instances (that's what MTL does). My idea was to stay with the defined types of the transformers package, and to ensure composability, _always_ use the transformer versions. This goes: readerAct :: Int -&gt; Int -&gt; At (ReaderT Int) String readerAct x y = do z &lt;- ask return (show (x+y+z)) where the type is simply an alias for: readerAct :: (Monad m) =&gt; Int -&gt; Int -&gt; ReaderT Int m String So you are explicitely saying that your function needs a ReaderT Int behavior but can run in any monad stack provided it contains a ReaderT Int somewhere. (You actually say the same than with MTL, only without the need to introduce). This approach was the one I've been used so far. But what if your final monad stack looks like: fn :: StateT MyWorld (ReaderT Configuration (WriterT String (ReaderT Int IO))) () Then your ReaderT Int is pushed deep down the stack, and you have to insert 3 lifts to execute actions in it: fn = do count &lt;- lift (lift (lift readerAct)) lift (lift (tell count)) -- we log the number logged so far. It is a bit ugly, and everytime you want to execute one action in your stack, you have to look at it to count the number of lifts you should insert. Cumbersome. MonadPointer allows you do just replace whatever amount of lifts by: fn = do count &lt;- mpoint readerAct mpoint (tell count) -- we log the number logged so far. And if you _really_ can't stand the boilerplate introduced by mpoint, you may simply declare polymorphic variants of your stack-accessing functions: get' = mpoint get put' = mpoint . put ask' = mpoint ask tell' = mpoint . tell And then you get fully polymorphic accessors without having to write a single class (PointableIn, the class behind mpoint, is generic enough). And you can then rewrite the code as: readerAct x y = do z &lt;- ask' return (show (x+y+z)) fn = do count &lt;- readerAct tell' count The gotcha that remains is anyway already present in MTL: if you have twice the same monad transformer (e.g. if you have two ```ReaderT Double``` in the stack), then mpoint will address the uppermost. But I wouldn't consider good practise to have twice the same transformer, as it's not clear what each Reader is meant to be used for (you have newtypes for clarifying this).
This is exactly how I program in haskell - make your types specific enough and your functions short enough that if you can write an expression that seems vaguely correct it's probably going to be the right one. It changes programming into a game of trying to hook up all the types together. Type holes, a new feature in 7.8.1 help even more - just put an underscore or underscore prefixed identifier in an expression somewhere and when it comes to compilation time ghc will tell you which type you need to make it all work out. Combine it with something like SublimeHaskell, which compiles every save and shows you compiler errors, and you can find out the inferred type of any subexpression very easily.
Thanks, that explains it very well.
Interesting! I can't make any promises, but I shall try to port my CHIP-8 computer from Kansas Lava. Do you have a story for RAM? Anything explicit, or just a usage pattern that emits VHDL which is recognized by synthesizers to be mapped to BRAM?
yes
I've been having the problem of deploying Fay to a Heroku server - I settled for including the compiled JS in source control, which is suboptimal. But this sounds like it will allow me to integrate Fay into the existing `cabal build` process that happens on Heroku. Which is a big win in my book!
I work with a lot of JavaScript people and something that angers me is when they say "types", their default image is java and C. I keep trying to teach them otherwise, but they refuse to learn Haskell :(. One of them knows Scala but uses it as a better java.
I almost always start with `foldr undefined base list` and then go back and fill in the function, so I guess I do defer the typing somewhat that way :)
The `At (ReaderT Int) String` type alias doesn't work correctly when the type occurs in negative position.
Regarding having multiple Reader environments, my usual solution is to use a single composite state or environment. Then I can either: a.) use `zoom` from lens to run a computation in part of the state (or `magnify` to run in part of an environment b.) I can `makeClassy` lenses for the parts of the state/environment I want and make instances of HasFoo, HasBar for those parts for the composite state/environment. Then I just double down on the `mtl` approach data MonitorOptions = MonitorOptions { _monitorPort :: Int , _monitorHost :: String , _monitorEnabled :: Boo } -- deriving ... data Warnings = Warnings { _errorsAsWarnings :: Bool } -- ... makeClassy ''Warnings makeClassy ''MonitorOptions Now you can have a combinator that works with just the monitor configuration for something like `ekg`: startMonitor :: (MonadReader e m, HasMonitorOptions e, MonadIO m) =&gt; m () and you can have things that need warning configuration warn :: (MonadReader e m, HasWarnings e, MonadIO m) =&gt; String -&gt; m () and now if you mix these you can do it directly with `do` notation. The constraints accumulate on the composite environment, and you can make a big composite environment/state without too much boilerplate: data CompilerOptions = CompilerOptions { _compilerMonitorOptions :: MonitorOptions , _compilerWarnings :: Warnings } -- deriving ... makeLenses ''CompilerOptions -- or makeClassy if you want further nesting. instance HasMonitorOptions CompilerOptions where monitorOptions = compilerMonitorOptions instance HasWarnings CompilerOptions where warnings = compilerWarnings foo :: (MonadReader e m, HasMonitorOptions e, HasWarnings e, MonadIO m) =&gt; m () foo = do startMonitor warn "The sky is falling" If you can't be bothered to make the composite environment directly, you can use tuple and manually magnify/zoom: foo :: (MonadReader (MonitorOptions, Warnings) m, MonadIO m) =&gt; m () foo = do magnify _1 startMonitor magnify _2 $ warn "The sky is falling" I find this alternate version composes a lot less well for programs in the large, however, and tend to reserve `zoom` / `magnify` for when I need to get at one or more of several similar states in a larger state to run an action.
That's indeed the approach I'd choose if I only have to deal with Readers and States. (thx for the quick tuto on magnify, btw, I didn't quite get how to use it when I read lens docs ;) ) But I also planned on using MonadPointer to create &amp; compose actions using EDSLs (thx to free or operational). I targetted the MTL ballpark as I was aiming for some general composition method. Note that if there is for this also an available approach using lens I'd be very much interested to know it (ie. if there is a way to build getters from a full monad stack to a specific monad transformer).
Both type At mt a = (Functor m, Applicative m, Monad m) =&gt; mt m a and type At mt a = forall m. (Functor m, Applicative m, Monad m) =&gt; mt m a say the same thing. GHC _has_ to put a quantifier in the former, it wouldn't mean anything otherwise, so it does it for you. So there are two problems. 1.) Type inference in the presence of rank-n types just doesn't happen. This means that without an explicit type signature you're out of luck and GHC can't help you by suggesting one! 2.) Because of the quantifier if you take such a function as an argument. You're 'starting over' and passing it a new dictionary and getting it to compute its result. After all it has to be a valid value for _all_ m. This means that f you have something like type List a = forall m. ListLike m =&gt; m a to pick a simpler example, then you discover that `(++) :: List a -&gt; List a -&gt; List a` has to actually send in the ListLike dictionary to both of its arguments, compute them from scratch and get the answer. This means `xs ++ xs` gets no sharing, it has the same computation expecting a dictionary, but this isn't as good as having the same list. `xs` isn't a list any more. It is a promise to take a ListLike dictionary and give you a value of whatever type it has. Similar situations would arise with `At (WriterT w)`. These reasons are part of why in `lens` we have to leak some information about the choice of `Functor` used into the type signature.. Otherwise we lose sharing. (We also would lose the magic overloading between lenses, traversals, etc. but that is a separate issue.) Pulling the quantifier out of the argument so it can be shared across the entire computation lets you work polymorphically _and_ retain sharing.
&gt; Both [...] and [...] say the same thing. Ok, I just wanted to be sure about that. Thanks for the explanation about sharing, I learnt something. So you're suggesting me to remove the At alias and always use the full type? (Because when At is in a positive position this problem don't occur). Or just never try to use it in a negative position (which I did not intend to do at first place)?
Well extracting a particular monad transformer from a stack doesn't entirely make sense due to the way layering works there isn't any real guarantee it can be "extracted". `StateT` winds up with parts splayed across both the inside and outside of the stack. `ContT` has the underlying `m a` in both positive and negative position, etc. So I think you'll find that when you start digging into more of the corner cases that the notion falls apart.
You're right that it's not satisfactory. I'm trying to write something that would use type-level lists of transformers to be more general that At (which only works when you want to target one specific Monad) and I think it wouldn't have this problem. It would look like: test1 :: (MTSet [StateT Int, ReaderT Double] m, MonadIO m) =&gt; m String And the m is shared between all the instances, as this should expand to: test2 :: (PointableIn m (ReaderT Double), PointableIn m (StateT Int), MonadIO m, Applicative m) =&gt; m String But it doesn't work right now. If I try it with ':type' in GHCi, test1 type expands correctly to test2 type, but when I write this in my code as a type for the actual function test :: (MTSet [StateT Int, ReaderT Double] m, MonadIO m) =&gt; m String test = do x &lt;- mpoint $ helper 42 liftIO $ print x return (show (x::Double)) GHCi refuses to compile, saying that it Could not deduce (PointableIn m (ReaderT Double)) arising from a use of ‘mpoint’ (which is weird). I'm investigating. I also wanted to make a MTList class, which would, conversely to MTSet, statically ensure that the transformers of the type-level list appear _in that order_ in the stack, possibly with other between them.
&gt; So I think you'll find that when you start digging into more of the corner cases that the notion falls apart. The notion of using lens as a way to manipulate a stack of transformers? Yes, that was just a "what if" idea I had.
So you're basically saying the biggest benefit is mostly to Windows users who insist on suffering from the impedance mismatch of using GHC on a non-POSIX OS?
I remember that `foldl` has a type that spells "aba-aba", or in other words, foldl :: (a -&gt; b -&gt; a) -&gt; a -&gt; [b] -&gt; a And `foldr` is the same thing but the first two arguments swapped, so it spells "baa-aba", or foldr :: (b -&gt; a -&gt; a) -&gt; a -&gt; [b] -&gt; a Then when I want to write a `foldr` expression, I start by filling in just {- foldr (\b a -&gt; a) a bs -} as a comment which works as a mental reminder to figure out what goes where. As I design each subexpression, I can query GHCi about it's type and then finally, when all subexpressions typecheck, I insert them into the expression and remove the comment braces.
Here's the few I wanted to upgrade recently and had to fight with a HP2013.2 install on Windows: `hashable`, `unordered-containers`, `attoparsec`, and `text`. Needless to say this didn't decrease my disdain for the Windows platform.
If you used inductive naturals data Nat = Z | S Nat You could define the missing instance. (Probably for a custom class that is just like KnownNat.) There's an old wiki page somewhere with a caveat that type literals are not inductive and that can cause this kind of problem you are seeing. However, that wiki page's content is out of sync with the implementation, iirc. HTH
So I just removed the At type to correct that behavior. (To gain the same brevity effect, I added a MA alias which will hold until Monad becomes a subclass of Applicative) Regarding my MTSet class, I wouldn't exactly know why but it worked after I turned it into a closed type family that recurs to build a Constraint. I allows to build polymorphic actions that may require _some_ transformers to be present in the stack, without specifying all of them or their order. I like this solution but it requires the use of UndecidableInstances. I'm not very sure of the implications here. Should you have the time to make some extra comments I'd be very happy to read them. I'd for instance like very much to be able to encode the fact that the same transformer _cannot_ appear twice, to remove the OverlappingInstances... **EDIT:** Done! Thanks to the advices of [this page](http://www.haskell.org/haskellwiki/GHC/AdvancedOverlap).
Some guesses: - patterns aren't first class (though they're getting there) - pattern completeness/redundancy isn't checked (or checkable) though see Neil Mitchell's [Catch](http://community.haskell.org/~ndm/catch/) - some kind of bickering about lazy/strict sums is probably in order
&gt;One of them knows Scala but uses it as a better java. As intended. 
Inspired by this, I hacked together the following for zsh: #compdef ghc #autoload compadd `ghc --show-options | awk '{ print "\"", $1, "\"" }'` It seems to work. Someone who knows more about such things could probably improve on it though. **Edit:** see http://www.haskell.org/haskellwiki/Zsh for more full featured completion options. 
How is that intended? What was the point of the functional stuff? Of the better type system? Why not just use one of the other many Java Sugarers if all you wanted was a less verbose java?
Scala is not a functional language. In fact scala creator Martin Odersky and the commercial company he made to push and popularize scala are making a point to spell it out. Scala is a OOP language with added functional "niceties". Unrestricted state change is very important and is seamlessly supported and encouraged through entire code of scala programs. Functional programming is really used as a means to provide a nicer, more terse syntax working with collections (map filter fold) as opposed to for loops. It is a small, local scale programming. The larger design is still intended to be completely OO. Now there's a very vocal FP minority in scala community that coalesced around scalaz library. But they are viewed more as extremists.
I guess I'm one of the minority. I use Scalaz quite a bit and use Scala as a weaker Haskell. Why not just use Haskell? Cuz no one is using Haskell :(
I do use haskell in my daily job. I used to be a java programmer. But i did not sit an wait for an invitation. I took the matter in my own hands and now i'm in a better land. Haskell maybe is not ready to be used on the client (lack of GUI libs). But on a server side (web programming, REST services etc) it is mature enough. Everything i did with java on a server i now do with haskell. 
I've only really been able to find 2 jobs on Haskell (believe me, I'm looking) but they required senior level knowledge. So now I'm learning it (while learning language-agnostic senior stuff on my regular job) in hopes that one day I can jump into it. Been playing with Yesod a lot. Quite nice I have to say. Like Rails, without the ruby. 
Registration will open in about a month, and will be somewhere in the region of $100-$200 for a one-day registration, depending on whether you are a student or not.
As I mentioned in the comments, I wrote that `FreeT` code, and I directly stole that code from the same article of the monad reader, so it's not a coincidence at all that they are the same. `pipes` is also a special case of a free monad transformer: data ProxyF a' a b' b x = Request a' (a -&gt; x) | Respond b (b' -&gt; x) type Proxy a' a b' b = FreeT (ProxyF a' a b' b) That's really all there is to it. There are some minor differences (for efficiency reasons), but the above code is the semantic model for `pipes`. I also wrote a [similar post](http://www.haskellforall.com/2012/07/free-monad-transformers.html) on the original incarnation of this code, before it was merged into the `free` package.
Your approach may be closely related to the idea of Monatron. http://hackage.haskell.org/package/Monatron http://usuarios.fceia.unr.edu.ar/~mauro/pubs/monatron.pdf
I remember seeing this post a while ago, before getting really acquainted with the idea of Free monads/ transformers. That's actually super interesting. It's really cool that Free monads can encapsulate so many different types of behavior!
This definitely looks like an improvement over the `mtl`: * Concrete types for sub-components means errors get caught earlier in the development process and the error messages are closer to the actual problem. Contrast this with `mtl`, which defers errors until late into the development process (when you call `runXXX`) and usually the location given by the error message (at the point you call `runXXX`) doesn't match the location of the real error. * Instances scale linearly instead of quadratically I still personally prefer using the explicit `lift` style of `transformers`, but this is a decent alternative.
Thanks! I'll have a look.
`mtl` doesn't have to do anything special for `(Integral i) =&gt; StateT i` because the constraint is not part of the monad transformer.
I was thinking of the fact that functions Either A B -&gt; X is not quite isomorphic to pairs of functions A -&gt; X and B -&gt; X, which is the defining characteristic of a sum. Recently I've been musing about experimenting with an even less strict language to remedy this. Tel's point about pattern completeness is a good one though. That should be a compiler error, but it is more or less easily fixed.
Give credit to Mario Blazevic! He's the original inspiration for all of this.
The `case` expression for a given type should be equivalent to an elimination function like `maybe` and `either` though, if I understand correctly. So maybe as long as there's a sufficiently concise syntax for higher-order functions... EDIT: I'm not entirely sure where I'm going with this
Agree. This was an excellent blog entry.
have you loooked at [layers](http://hackage.haskell.org/package/layers-0.1/docs/Documentation-Layers-Overview.html)?
I opened a feature request in the ghc bug tracker to see what ghc devs think about this: https://ghc.haskell.org/trac/ghc/ticket/9005
The right direction :). What you are reaching for is [catamorphisms](http://eprints.eemcs.utwente.nl/7281/01/db-utwente-40501F46.pdf) and [church encoding](http://okmij.org/ftp/tagless-final/course/Boehm-Berarducci.html).
Right I basically only access Hackage with bookmark keyword search. Maybe I will add myself a keyword too for docs...
https://copr.fedoraproject.org/coprs/petersen/ghc-7.8/ repo has a build for Fedora 20.
the later only. a referential transparency would be a bug. specifically given a rng r and when using: cprgGenerate :: r -&gt; Int -&gt; (ByteString, r) the new r might be different across run
I get it that it's upsetting to have bugs or something surprising, but again, if you don't return it to me somehow how do you expect me to know about it and fix the usage. The only deliberate interface choice that I made, is that it would mix impurity every now and then in the rng, and return a new rng that is not strictly dependent on the old rng and the number of bytes pulled. So given a seed and a rng, it would not give a reproducible stream of randomness. Everything else is very likely to be a bug.
Thnxs for the post. I have plan for using Haskell + OpenCV on my next side project so will look into your github repo :)
I'd done some research a few years back that strongly indicated Python would benefit from Haskell's STM. Possibly a way to get rid of the GIL without having terrible performance problems.
Some of the new stuff is discussed on the [Haskell prime mailing list](http://www.haskell.org/mailman/listinfo/haskell-prime).
Note that the mtl is not just about positive `lift x n` times to reach the well-typed layer. It is also about lifting various operations (e.g: local) through transformers in ways that aren't completely trivial. 
As far as standard libraries are concerned, have a look at the libraries@haskell.org mailing list. The one thing that the committee did was, indeed, the Haskell 2010 standard. The committee consists of people *from* the community, so you could say yes, they interact with the community. They aren't up to anything in particular *qua* committee. Thus, Haskell 2010 was just an approval of those GHC extensions/deviations that everyone uses all the time anyways (even authors of other compilers). As for new language features, the canonical way for those to get famous is to get implemented as a GHC language extension.
Thanks! As I see, layers approach to automatic lifting is similar to mtl, but indeed it seems to solves the explosion of instances problem mtl has. It still requires OverlappingInstances, but this big problem is gone. This makes me wonder if MonadPointer approach could be adapted to use layers classes instead of transformers classes. Or conversely if the overlaps could be removed from layers instances for instance using a technique similar to [these](http://www.haskell.org/haskellwiki/GHC/AdvancedOverlap) combined e.g. with the static insurance that the monad to which you lift exists only one in the stack (it's what I use for MonadPointer).
Are you aware the Pypy project is looking into this direction quite a bit?
this would be great, +1.
Instead of checking `/etc/issue`, maybe it would be better to check `/etc/lsb-release` or `/etc/ubuntu-release` (or similar for other distros)
We are nearly full. Please [register](https://docs.google.com/forms/d/1mMTlVNnWQvac1oWSXWQLBO9GVvqv8a_fYTFrR8UNuDI/viewform) soon. After we reach capacity new registrations will be put on a waiting list.
http://morepypy.blogspot.be/2014/04/stm-results-and-second-call-for.html
Yeah, you're right.
One of the biggest problems I've had on Mac is when that unix package tries to update but it can't build due to some bug solved in a future GHC. I do appreciate that some packages which are nontrivial to build come packaged in the HP, but I can't say the same for non-system packages. I don't think I'll ever use the opengl stuff.
Well darn, already full. I shouldn't have hesitated for so long ;-(
Don't worry, there will probably be a few people who will cancel.
Please send a direct email or a reply (to me) on the thread. Thanks.
I knew I forgot a hackathon! I'm leaving Germany earlier that week to come back to the states but then heading back to Zurich for Zurihac after. Had I planned more carefully I'd have just come visit you folks for a week or so in between.
Changes to the Haskell standard pretty much don't happen. It's a lot of work, with little perceived gain. Changes to the Haskell language happen because people add features to GHC. Most of the interesting changes have papers that accompany them. The Haskell community is great. But, we are probably not where we should be in terms of involvement with GHC. SPJ is currently trying to transition GHC from a vehicle for research to a community supported project. Which is great! But, I think it does show that Python was always a community supported project, and GHC has traditionally been an open-source research project. That said -- there are community driven changes. We do often see library submission proposals: http://www.haskell.org/haskellwiki/Library_submissions And things like Functor, Applicative, Monad changes in GHC 7.10 have come out of the community, http://www.haskell.org/haskellwiki/Functor-Applicative-Monad_Proposal So, in summary, yes, you are seeing a hole in the Haskell ecosystem. We really need to create more transparency, policy, and tools for contributing. I'm not sure if anyone has taken leadership on that though. IMO, it needs to be someone's full-time paid position. 
Done! :)
Most Heartbleed abusers isn't as interested in application data, which uses up a significant amount of memory on application machines. In contrast proxies only have encryption data and what was sent (which is typically a more compact form of said application data).
Yes [there is](http://www.haskell.org/haskellwiki/ZuriHac2014/Attendees).
Aside from the bit on arrays versus lists, I think the points are very true. I'm not sure that this advice isn't already embedded in every tutorial out there anyway. 
If you make `f` a Functor then that's equivalent to `Applicative` (often called `Monoidal`). And `Applicative` implies `Functor`: instance Functor Ap where fmap f ap = pure f &lt;*&gt; ap But you need the `Functor` instance to map `Monoidal -&gt; Applicative`, so there's still a very slightly weakening in what you're proposing. I don't know that it exists anywhere. And all of the above might be implied in your emphasis that this does not instantiate `Functor`. :)
I doubt it. It's unclear what the laws would be if it isn't a `Functor`. Do you have any example where you might use it?
Here's another member it should have: phomotopy :: (a -&gt; b) -&gt; (b -&gt; a) -&gt; f a -&gt; f b Examples: * any Applicative * newtype ToIO x = MkT (x -&gt; IO ())
For people that are more curious about this, read the [original paper introducing applicatives](http://www.soi.city.ac.uk/~ross/papers/Applicative.html) which discusses why the `Applicative` class is equivalent to having `punit` and `ppair` if `f` is a `Functor`.
I don't think it's vulnerable actually. The crypto is in c.
Does not protect against cache timing attacks for example.
Do you have any laws in mind?
Was wondering about this after your post on scalable architectures. Thanks
You're welcome!
`phomotopy` doesn't give you very much since you have to provide what seems to be an isomorphism. For your examples of `Applictive` and `ToIO` at least, you can use a `Profunctor` i.e. have one covariant type variable, and one contravariant. class Profunctor p where dimap :: (a -&gt; a') -&gt; (b -&gt; b') -&gt; p a' b -&gt; p a b' Then you can extend to what I call a `ProductProfunctor` class Profunctor p =&gt; ProductProfunctor where empty :: p () () ppair :: p a b -&gt; p a' b' -&gt; p (a, a') (b, b') This is basically just an `Applicative` instance for `p a`.
I am currently implementing this as two classes: class Homotopic f where homotopy :: (a -&gt; b) -&gt; (b -&gt; a) -&gt; f a -&gt; f b class (Homotopic f) =&gt; Product f where punit :: f () ppair :: f a -&gt; f b -&gt; f (a,b) 
i would rather a how to learn cabal... I feel like nothing EVER compiles when i try to install something with it :( wat am i doing wrong :(
You actually get some interesting things when f &amp; g are not an isomorphism. For example: ppure :: (P f) =&gt; a -&gt; f a ppure a = phomotopy (\_ -&gt; a) (\_ -&gt; ()) punit
Here's an expanded version that checks the version number before calling your function, since my bashrc lives on a few servers as well, and I haven't had a chance (or permission) to update them as well. declare -a version=($(ghc --version | grep -Eo '[0-9]+')) if [[ ${version[0]} == 7 &amp;&amp; ${version[1]} -ge 8 || ${version[0]} -ge 8 ]]; then _ghc() { local envs=`ghc --show-options` # get the word currently being completed local cur=${COMP_WORDS[$COMP_CWORD]} # the resulting completions should be put into this array COMPREPLY=( $( compgen -W "$envs" -- $cur ) ) } complete -F _ghc -o default ghc fi I can't think of a way to remove that `$versions` array afterwards though.
Yeah this seems to be just `f a = p a a` where `p` is a `ProductProfunctor` as I described in another comment.
The thing is, pretty much every type of kind \*→\* should be an instance of Homotopic, unless you're doing fancy stuff with type families or whatever. For example: * newtype T1 a b = MkT1 (a -&gt; b) * newtype T2 a b = MkT2 (b -&gt; a) * newtype T3 a = MkT3 (a -&gt; a)
Thanks a lot! I've actually been struggeling with exactly the issues you are describing and your article was of great help. It's a pitty ghci does not seem to play nicely with c++ libraries, though. Could you elaborate a bit on your experiences with the build architecture? Why did you choose CMake? Have you tried using cabal for the entire build process (using c-sources)?
What is the intuition behind what you want phomotopy to do? The only implementation I could imagine is that for functors I would ignore the second argument and apply fmap to the first: phomotopy f _ = fmap f.
**f is not necessarily a Functor.** This is the important bit. If we constrain f to be a Functor, then Product merely becomes Applicative, which is not very interesting.
Are you using GCC? It isn't available by default on OS X, and before I installed it nearly every cabal install failed. This [SO answer](http://stackoverflow.com/a/21285413/1176156) sorted me out. There are several things that could go wrong though so you're probably best off asking a question on SO about your specific error.
Lots of newcomers have problems with Haskell's ecosystem. Let's try to help them out.
I also have many problems with dependencies :( 
Hop on [#haskell](http://haskell.org/haskellwiki/IRC_channel) and people can probably help you out. Depending on what you're trying to do there are better and worse ways to proceed. What I do, and others do, is completely sidestep my operating system's packages. I install GHC from the GHC site, then upgrade cabal using cabal, then install everything I want with cabal. I also [put ~/.cabal/bin in my PATH](https://github.com/chreekat/bDotfiles/blob/avery/bashrc#L109).
I'm working on it :) If you have fancy ideas for how to improve it, please go ahead and stick them on the Github issue tracker. I'm more than happy to have suggestions for what y'all think would make it better, even if they're sorta "I wish I had a unicorn"-style suggestions. I'm also hoping to use TypeHoles in 7.8 in order to fix my autocompletion so that it only autocompletes using functions which might actually make sense in context. It'd make this sort of programming even easier.
Are you using Cabal sandboxes? I just did a test using Haskell Platform 2013.2.0.0 and confirmed that I can install the latest versions of all of those packages in a a sandbox without issue. For anybody not using sandboxes: I highly recommend using them. Here is a good introduction: http://coldwa.st/e/blog/2013-08-20-Cabal-sandbox.html
http://hackage.haskell.org/package/singletons-0.10.0/docs/index.html
&gt; I've actually been struggeling with exactly the issues you are describing and your article was of great help. Care to share your issues? I'd love to hear about other's experiences -- so far it's seemed like I'm the only one trying to tackle this stuff. &gt; Could you elaborate a bit on your experiences with the build architecture? Why did you choose CMake? Have you tried using cabal for the entire build process (using c-sources)? I used to use cabal for the entire build process and stuff more or less worked with some tweaking (except for GHCI, that's never worked). The problem is, I can't provide that back to the OpenCV folks, and that's my ultimate goal. They use CMake, so I tied into their build process to generate, compile, and link these bindings. If my pull request sits for too much longer though, I'm going to give up on the process and just start developing it as a Haskell library. This would be unfortunate because I have other people depending on my C bindings already and trying to coordinate with them is going to become very difficult if I pull out of the OpenCV build tree. At the same time, I don't want to maintain my own fork of OpenCV either. Honestly, what I could really use is more man power... There's a lot to do, and only a couple of us (mostly me) working on it.
You say changes to the Haskell standard pretty much don't happen, but as I originally mentioned there are several sources saying the Haskell language standard is revised every year: http://www.haskell.org/haskellwiki/Future_of_Haskell -- "A new standard will be defined each year." https://ghc.haskell.org/trac/haskell-prime -- "New revisions of the language are expected once per year." http://www.haskell.org/haskellwiki/Language_and_library_specification -- "Language revisions are expected to be produced once per year, starting with Haskell 2010." It's again mentioned on Wikipedia as I mentioned in my original post. I also found this relevant discussion on the "yearly revisions" or lack there of: http://www.reddit.com/r/haskell/comments/1esmhj/yearly_revisions_to_the_haskell_language_who/
What you call "Homotopic" has been termed an "InvFunctor" [though really only on irc i think?], and I think really can be thought of as a sort of functor, categorically speaking (although of course it is definitely a special case of a profunctor). A homotopy is really a relationship between two functions, so I think your name is terribly misleading. Anyway, if you put the two together you should end up with something that's categorically a sort of monoidal functor (but, notably, _not_ a closed one).
Yes, I guess if you're living in a world where every type can be built with `-&gt;` and `(,)` then that sounds plausible. I find that somewhat surprising! It would still be true though that every such type is (isomorphic to) `p a a` for an instance `p` of `ProductProfunctor`.
Well, pick a type constructor and I'll give you an instance of Homotopic for you.
Yes, [u/SrPeixinho](http://www.reddit.com/user/SrPeixinho) basically wants to check pattern exhaustiveness: ghci&gt; let foo (Just x) = x ghci&gt; :set -Wall ghci&gt; let bar (Just x) = x &lt;interactive&gt;:4:5: Warning: Pattern match(es) are non-exhaustive In an equation for ‘bar’: Patterns not matched: Nothing If you want to turn the warning into an error, use *-Werror*: ghci&gt; :set -Werror ghci&gt; let baz (Just x) = x &lt;interactive&gt;:6:5: Warning: Pattern match(es) are non-exhaustive In an equation for ‘baz’: Patterns not matched: Nothing &lt;no location info&gt;: Failing due to -Werror. 
I'm agreeing with you!
Haskell platform may help? 
Unrelated, but what is the is_nat function supposed to do? It seems to take a natural number and map it to the exact same number. Z becomes Z S Z becomes S Z Etc. In which case it would just be the id function defined over natural numbers: is_nat :: Nat -&gt; Nat is_nat = id I might have gotten this wrong, not entirely sure about it.
I think you're right. Furthermore `is_even` is a partial identity. I can only imagine that what was desired was is_even :: Nat -&gt; Bool is_even Z = True is_even S Z = False is_even (S (S a)) = is_even a
...and, by extension: is_nat :: Nat -&gt; Bool is_nat = const True
That typechecks because incomplete pattern matching is not a type error in haskell, as functions are allowed to be partial(fail for certain inputs). You could use `Maybe` and `double_minus_one_is_even` would be `Nothing` for all inputs. What you actually want is really hard in haskell. You want to get *type* errors when *values* are wrong. Also, you want to be able to *prove* things. This is hard, and needs something called dependent types: Haskell has no actual dependent types, but it can fake them. Encoding proofs is hard, but it can be done. It requires advanced features like GADTs, TypeFamilies and DataKinds. It would be more natural to do it in a language with actual dependent types like Agda or Idris. 
How unusual. May I ask what your background is? I'm sorry to report that your mental model of how Haskell works is quite far from the mark. But your mental model, while not a good fit for Haskell, is quite consistent and interesting in its own right. By any chance, are you familiar with the [Twelf](http://twelf.org) proof checker?
Here are similar classes: http://hackage.haskell.org/package/invertible-syntax-0.2.1/docs/Text-Syntax-Classes.html
It is called `IsoFunctor` here: http://hackage.haskell.org/package/partial-isomorphisms-0.2/docs/Control-Isomorphism-Partial-Prim.html Which makes sense, they are functors in the category of isomorphisms.
Use separate types for even and odd numbers: data Even = Zero | SuccEven Odd data Odd = SuccOdd Even If you convert all your functions to use these two types then you will get a type error for your `double_minus_one_is_even` function. Edit: I forgot to mention that you also need: type Nat = Either Even Odd There is a sexier way to do this with type families, but I wanted to show the Haskell 98 solution
If you are on OSX then you should be using homebrew.
That is not what I wanted.
I was just wondering if the simply typed lambda calculus could be used for theorem proving, without the dependent types. I guess if you restricted it to only one type (`data T = N | L T | C T T`) and encoded a specific type (nat, int, bool, etc) as the identity function for the structure that represented that type, then it could work. I never heard of Twelf, I'll take a closer look, thank you.
Thanks, but the whole point in this is trying to prove something without dependent/parametric types.
Or you can just use the Haskell Platform installation [here](http://www.haskell.org/platform/mac.html). It even includes a script for fixing the CPP issue.
This is more of a problem with `yesod` than the Haskell Platform. `yesod` has a huge dependency graph and usually has build problems as a result.
What did you want `is_even` to do? Currently it is the identity on even `Nat`s and undefined on odd `Nat`s.
Yes that is what I wanted.
OK. It seems somewhat strange. Why not use the `Maybe` monad?
?????????
He is moved on to a new project. Which is understandable. For a man of his caliber chaining himself to one thing for the rest of his life is a waste if his talent. So whatever he would want to achieve will be done in a different language, not scala. Scala's ship has sailed.
The simply-typed lambda calculus encodes propositional intuitionistic logic, which isn't very powerful.
Also with restricted recursion added as a primitive * Interesting, could you cite an example of something that couldn't be proven/done with that system? 
I said in another post, I just want to test if I can use a system consisting of the STLC + restricted recursion + a unique type, `data T = N | L T | C T T` to make arbitrary mathematical proofs without the need of dependent (or even parametric) types. I am using Haskell because this system is pretty much a subset of Haskell, so I thought why not? And can't use monads or bools because those are obviously outside it.
What
Proving that the Int type is a ring.
You can't define a ring in propositional logic. For example, there is no way to encode a proof that addition is commutative without forall.
I see, but doesn't the addition of restricted recursion and the recursive type I cited make the system (considerably) more powerful than bare propositional logic?
I didn't really get that, are you talking about type-level recursion or computation-level recursion? And how restricted?
Thanks, I'll check your project out. I've also gotten GHCi working with simple applications. I'm not sure what the reason is but if cabal compiles the C wrappers, running GHCi isn't a huge issue. You can still run into problems with compilation and linking, but GHCi will work when you go to try it (esp. via cabal repl). However, when you build the C wrappers independently, like in my case, compilation and linking get very difficult. I expect its that ghc and ghci expect the produced archive to be in a particular format, but I haven't figured out yet how to get cmake/g++ to produce an archive in that format. I'm tempted to just have the Haskell bindings recompile the C wrappers... it increases compilation time, but it might get everything (but ghc-mod anyway) working.
Yeah, I think IsoFunctor makes more sense.
Why only isomorphisms?
'-' you don't get it. This is something else to learn so, for now, I'll try to implement the language myself on Haskell, shouldn't be really difficult. If it doesn't work I'll take a closer look on those, thanks.
Oh I accidentally deleted that other post trying to navigate to parent, how stupid. Answering here: Why do you think those languages particularly could help? I'll try to implement that language myself in Haskell for now as it is one thing less to learn. If it doesn't work I'll take a closer look on Twelf and, what is the name of that other language again? Thank you.
Twelf and Beluga. They're expressly made to prove properties of programming languages. Encoding your program in either would take a handful of lines of code, and then going through with the proofs would be really straightforward too.
I think you'd see it on double_minus_one instead
I you want to avoid dependent types, then Twelf isn't for you :) Here, I [rewrote](https://gist.github.com/gelisam/11060091) your code in Twelf so you can see if you want to learn more. If you do, here is a good [Twelf tutorial](http://www.cs.cmu.edu/~jcreed/flash/twelftut.swf). Note that just like in haskell, Twelf doesn't complain about `double-minus-one-is-even`, even though there is no `nat` for which `%solve` will ever find an `R`. The problem of figuring out whether a type is inhabited or not is an undecidable problem, and I don't know of any type system which even tries to infer an approximate an answer. There many languages (including Twelf) which allow you to prove that a particular type is inhabited or uninhabited, and the type system will verify that your proof is correct, but it won't attempt to prove it for you.
If you're done with it, perhaps I can borrow it? I like the "sheeple" part. :-)
I'm on Brigitte's team doing code tools ;) I'm not sure it has an user base among "real world" programmers yet, it's mostly academicians working on niche projects.
Is the idea that a proof is valid if and only if the corresponding function is well-defined on all inputs? That's quite an unusual (and undecidable) way to do proofs.
One of the page authors here. This was created for an independent study several years ago, so some content may be out of date. We tried to document our learning process as we went, in the hopes that it would be useful to other learners. Feel free to ask questions or point out any glaring inaccuracies.
&gt; to make arbitrary mathematical proofs My understanding is that proofs are what you get at the type level *if all your functions are total*. Your functions are not total (they fail quite often), so your programs do not correspond to any valid mathematical proofs. Furthermore, please correct me if I'm wrong, but if everything you have at the type level is Nat, I don't think your mathematical proofs are meaningful. What your types are saying, is something like this: *Hey, natural numbers exist* (Nat type). *Hey, if natural numbers exist, natural numbers exist* (Nat -&gt; Nat type). If you don't want to utilize Curry-Howard correspondence and total functions for strict mathematical proofs, the best you can do is to create some QuickCheck tests. They would not *prove* anything, but they would make you reasonably confident your programs have certain properties. But with partial functions, even most of your QuickCheck tests are likely to fail.
Yes, the whole issue is that I find type systems extremely inelegant. Code in functional language is pretty much written twice. I am struggling to accept that this is the "right way", or natural way to do it. I strongly believe that there is a pure, simple system that is as powerful as CoC and similars, but that can be implemented much more naturally, without involving all that type/value level separation that those languages do. And, as I believe that system exists, I'm trying to find it. In the end, I hope I'll have an aha moment where I figure out why that separation is "natural" and then finally "accept" dependently typed languages and type systems, in general. Anyway, yes, the current idea is that a type is just the identity function for a specific structure, at value level. Why would it be an undecidable way to do proofs? I'll take a closer look on your program and Twelf itself as soon as I finish this exercise. Thank you very much!
I don't get why everyone is talking about Twelf and Beluga, how are they different from Agda/Idris/CoQ?
They're expressly targeted towards reasoning about languages/logics, and they have a fair bit of logic built-in for common cases. Agda, Idris and Coq are general-purpose programming languages while Beluga and Twelf are logic frameworks (AKA language workbenches).
&gt; A proof is something like: even (double (nat a)) That's not a proof, that's a property you *want to prove*. You cannot do that in a mathematically sound way like this, but you can turn this into a QuickCheck property, in a very straightforward way (I mean, it almost *is* a QuickCheck property).
I don't get it. If it compiles, then you know for sure the double of any nat is in the shape encoded by even. So what is there left to prove?
Maybe we need something like [bro pages](http://bropages.org/) for haddocks.
Well, if you find type systems inelegant, you're not going to make many friends on /r/haskell :) &gt; a type is just the identity function for a specific structure I see! Does that mean that the only way to know whether a given value satisfies a given type is to run the identity function on the value and see if the function succeeds or fails? &gt; Why would it be an undecidable way to do proofs? Because of quantifiers. Given a concrete value such as `S (S Z)`, you can double it and verify that the result is even. You can even write a general procedure which doubles its input and crashes unless the result is even. But writing this procedure brings you no closer to a proof that double's output is always even: you still have no idea whether there exists a concrete value which will cause your procedure to crash. And of course, by the usual diagonalization argument, there is no general procedure for determining whether an arbitrary procedure has an input which makes it crash or not.
A comment/wiki section per package?
Well, my experience is that most libraries are in one of the following categories: 1. Standard or de-facto standard libraries; these are covered in great detail in all sorts of learning resources, and you can find plenty of explanations and examples there. E.g., mtl. 2. Bindings for existing C libraries. Examples can be derived easily by porting the original library's C examples to Haskell; the bindings are usually straightforward enough to do that given a bit of Haskell experience. 3. Libraries whose purpose is clear and straightforward from the types and documentation alone; you don't really need examples for those. 4. Libraries that actually *do* provide examples. Lens being a great example. Granted, sometimes you really need some academic reference to really "get" what a library is for. Anyway, for questions about specific libraries, there's a great community to fall back upon - try #haskell on freenode if you haven't already.
First: yeah, people should use examples in their documentation. [doctest](https://hackage.haskell.org/package/doctest) for the win! Second: you don't need examples nor tuturials in order to use a haskell library. [Follow the types](https://www.fpcomplete.com/user/chowells79/types-not-tutorials)! Third: I found a [test](http://community.haskell.org/~wren/unification-fd/test/) folder by looking at the darcs repo linked from the library. Does that help?
You could be right, but how could it be that I am trying to solve the halting problem if that system is strongly normalising?
Wikipedia [says](http://en.wikipedia.org/wiki/Normalization_property_%28abstract_rewriting%29#Typed_lambda_calculus): *A lambda calculus system with the normalization property can be viewed as a programming language with the property that every program terminates* Idris has the normalization property if you add the "total" keyword to your functions. That is, if you manage to make your program compile with Idris, it is guaranteed to terminate. The thing is that you won't manage to make Idris compile the functions in your original comment. I do realize the "even (double (nat a))" term is strongly normalizing, indeed. But if you want the compiler to always prove each and every one of those cases for you, the compiler would have to solve the halting problem, which is impossible. So... No general solution is possible. No solution of any kind is, strictly speaking, possible in Haskell - because Haskell was not designed from the ground up to work as a theorem prover. Haskell can give you both false positives (accept some non-terminating programs), and false negatives (reject some of the valid programs). Idris and the likes can only give you false negatives. Currently, your programs would fall under the "false negatives" category - Idris would not be sure if they terminate, so it would fail to compile them with the "total" flag. 
&gt; Is using a library always a non-issue to you guys? No. In fact navigating all the libraries is the single most common problem people have learning haskell.
But that is the point, if a function compiles then the property is proven. If it doesn't, then it is false. Compiling is the proof. I don't see why the halting problem would be an issue considering that system (simply typed calculus with one inductive type and restricted recursion) is guaranteed to always terminate. The halting problem is solved, the answer is always "halts". Sorry if I'm not getting something obvious here.
I have had to give up on at least a few libraries due to lack of examples. Many libraries are of low quality (I say this as an author with low quality libraries) and with time, you learn how to avoid code that is written in an odd style. 
The problem is not types and all their benefits, is the way they are done. I just have a feeling we could have the same things, except with a simpler implementation and no separation of levels. Maybe I'm wrong, but that's the point of the exercise. I still don't get your argument. As I see it, a simple pattern match is enough to answer if a procedure has an input which makes it crash or not. For example, (even (double (nat a))) Here, we know `nat a` must be either `N` or `L T`. This way, we know `double (nat a)` must be `N` or `L (L T)`. Both of those are valid inputs for `even`, so you know there is no valid term that satisfies `double (nat a)` that can't satisfy `even (double (nat a))`. Thus, "the double of every natural is always even".
If a function doesn't compile, that doesn't mean it doesn't terminate, it just means the compiler can't figure out whether it terminates, so it rejects it. The way you construct your programs, they would be rejected by Idris working in "total" mode, and they should hopefully be also rejected by GHC if you compile with -Werror: more on this [here](http://stackoverflow.com/questions/3804484/in-haskell-why-non-exhaustive-patterns-are-not-compile-time-errors). If you want a 100% guarantee that your program terminates, you'll always need to do some convincing - because of the undecidability of the halting problem, you have to help the compiler solve it for every particular case. That's why languages like Idris even exist. Nobody wants the dependent types, people want programs that provably terminate, and dependent types are a nicer way to write those.
MonadTran, I'm talking A, you're talking B. We're not understanding ourselves. Let's stop a little bit and agree with this point before proceeding: the Simply Typed Lambda Calculus is strongly normalising, so every term is always reducible to a normal form and thus every computation always terminate. So anything about the Halting Problem is irrelevant to the discussion - it simply does not exist for the subjected system. We agree with that?
&gt; Is using a library always a non-issue to you guys? I took the question as a challenge :) [Here](https://gist.github.com/gelisam/11068221) is an example of using (the latest version of) the unification library you linked. I did it by relying solely on the types, but I must admit that the type constraint for [unify](http://hackage.haskell.org/package/unification-fd-0.8.0/docs/Control-Unification.html#v:unify) was especially hairy!
Oh, I see, that is the problem. I thought we were not talking about Haskell anymore! I understand the whole conversation now, you are right. My bad!
Can you give an example of an implementation of phomotopy that doesn't ignore its second argument and for which f is not a functor, just to get an idea of what you were thinking?
types explicitly spell out how to use the module. just read the types. and think.
Sorry for the delay. For stuff like this, feel free to ping me on IRC for a faster response if I'm around. copumpkin is often on IRC as well, not sure about the other mods.
Even with "follow the types", a single minimal running example is still helpful. Finding the right node to expand out for "follow the types" can still be an intimidating first step.
Hmm, plus2 is applied to nat resulting in `λ (L (L N)) -&gt; (L (L N)), (L (L (L a))) -&gt; (L (L (L (Nat a))))`, which typechecks for any natural &gt; 2, as expected. Then `is_even` is applied to it, resulting in `λ (L (L N)) -&gt; (L (L N)), (L (L (L a))) -&gt; (L (L (is_even (L (Nat a)))`. There, `Nat a` can be N, so `is_even` is being applied to `L N`. It is not defined for `L N`, though, so it fails. Similarly, `is_nat a` is `N, L N, L L N...`. `plus2 (is_nat a)` is `L L N, L L L N, L L L L N...`. `is_not_one` has no problems typematching with `L L N` so it compiles... Correct? 
ah good, karal and joachim (i'm spelling their names wrong) are mentioned in the thread
So it looks like IsoFunctor is a better name than Homotopic. class IsoFunctor f where isomap :: (a -&gt; b) -&gt; (b -&gt; a) -&gt; f a -&gt; f b **Pretty much any type constructor** should be an instance provided it doesn't use type families or similar shenanigans. Here's an example: data T a = T1 a -&gt; Int | T2 (Char -&gt; a) | T3 (a -&gt; a) instance IsoFunctor T where isomap ab ba (T1 ai) = T1 (ai . ba) isomap ab ba (T2 ca) = T2 (ab . ca) isomap ab ba (T3 aa) = T3 (ab . aa . ba) Note that T is not a Functor or a Cofunctor or anything all that special.
You might also like https://github.com/dmwit/pi-eta-epsilon which uses `unification-fd` to implement the language described in [The Two Dualities of Computation: Negative and Fractional Types](https://github.com/dmwit/pi-eta-epsilon/blob/master/docs/rational.pdf?raw=true). You could start at the file defining the [syntax](https://github.com/dmwit/pi-eta-epsilon/blob/master/src/Language/PiEtaEpsilon/Syntax.hs) of the language in the paper, then look at the [evaluator](https://github.com/dmwit/pi-eta-epsilon/blob/master/src/Language/PiEtaEpsilon/Evaluator.hs) (which is the bit that uses `unification-fd`).
Hmm ELI5 that paper?
1. You need docs any time you have more than one argument of a given type. 2. Examples help you to figure out types when you're deep inside a Monad and everything is a synonym or new type. Even hard-core haskellers know that you still need docs. 
The short answer to why is because anyone can upload to hackage. The regulated standard libraries are a small subset of hackage. People are busy or lazy, or a project gets started and never finished. 
And conversely, not all libraries lack examples. Right now I will point to [aeson](http://hackage.haskell.org/package/aeson-0.7.0.2/docs/Data-Aeson.html) and [pipes](http://hackage.haskell.org/package/pipes-4.1.0/docs/Pipes-Tutorial.html), but there are several other counterexamples. 
One option not mentioned is to read the code to the library and see if the function you want to use is used in some other part of the library. This sometimes provides a good enough example to go by.
I think it's a big problem in a lot of academically-minded fields. Those with lots of knowledge can forget the difficulties of learning the subject matter in the first place - or they picked it up more easily that others, so don't think to consider that others might need more help/assistance than them. It can certainly be a problem in Mathematics (source: one of my current lecturers).
So, you don't consider this a bug?! import Crypto.Random import Crypto.Random.AESCtr main = do s &lt;- makeSystem let break = fst . cprgGenerate 16 . snd . cprgGenerate (1024*1024) if break s == break s then return () else putStrLn "What the hell is going on?!"
[The `ListT` monad](http://hackage.haskell.org/package/pipes-4.1.1/docs/Pipes.html#g:5) is the monad for looping over effectful lists. I even wrote a library for traversing directories using `ListT` here: https://github.com/Gabriel439/Haskell-DirStream-Library/blob/master/src/Data/DirStream.hs The primitive function for looping is: childOf :: (MonadSafe m) =&gt; F.FilePath -&gt; ListT m F.FilePath Doing a recursive descent is as simple as: descendentOf :: (MonadSafe m) =&gt; F.FilePath -&gt; ListT m F.FilePath descendentOf path = do child &lt;- childOf path isDir &lt;- liftIO $ isDirectory child if isDir then return child &lt;&gt; descendentOf child else return child `ListT`'s monad instance has a very straightforward translation to `pipes` operations: * `(&gt;&gt;=)` is `for` * `return` is `yield` The main reason I haven't released `dirstream` yet is that I'm experimenting with more prompt alternatives to `pipes-safe`. Edit: The reason this requires resource management is that traversing a directory tree requires acquiring and releasing a directory stream.
Just knowing the entry points to a library is an enormous first step. If you can't even figure out where to start to begin to read the types then you have to do this cognitive "topological sort" of which concepts and types in the library depend on each other, which can take hours or even days. It can be really exhausting for beginners. 
aeson is a great example. afair the example was contributed by chris done, a user of this library. edit: the point is, users can easily submit documentation!
That's the [Cont monad](https://hackage.haskell.org/package/mtl-2.0.1.0/docs/Control-Monad-Cont.html) there at the end.
Of course, following the types works. It just needlessly takes a hell of a lot more time than seeing and understanding a good, concise example. It is also less boring and less frustrating to follow an example. Just recently, I got up and running with pipes-binary quickly because it had one good example, which is all it takes. If it had not, it surely would have taken me one or two hours longer two figure things out, like it has in other instances. This is coming from an intermediate/advanced haskeller.
&gt; The Two Dualities of Computation: Negative and Fractional Types That sounds super cool. Thanks for the reference!
When we're expending that much energy, why not make a pull request to the original author with the examples we make, so they can go up on hackage as part of the documentation?
Sometimes the advice is "read these academic papers"! This is occasionally OK for me, depending on the topic, but I'm not going to do that for something I'm not extremely interested in. If you're really unlucky these papers can be behind paywalls too. My friends from university must be getting tired of me asking them to download the papers for me. 
A site I used a lot when just beginning to learn Haskell was zvon.org http://zvon.org/comp/r/ref-Haskell.html It has all the basic prelude stuff, and all the basic functions have a simple example. It's out of date, and probably useless to advanced people, but it helps a lot for newbies. It could perhaps be used as an inspiration for a new documentation. 
&gt; I think you accidentally a word there. Thanks, fixed!
If I remember well, the idea is that in a world where all functions are reversible, the typing rules look a lot like the laws of arithmetic addition: `a + b = b + a`, `a - a = 0`, `a + (b + c) = (a + b) + c`, etc. Also, something about unification and the laws of arithmetic multiplication, but when I read that paper, I didn't understand unification well enough to understand that part.
Glad to hear my horribly contrived examples are still proving useful! All of this stuff is CC-A-NC, but I'd be happy to re-licence the examples to package authors to include directly in their work. However, no one has asked if they can do that.. yet :)
Wait, you're not using the same notation in this comment and the previous one. I also feel like the two comments are describing very different algorithms. Thus, I must have misunderstood at least one of those two comments :) Here is the algorithm which I thought you were describing in your previous comment. When you said "we know `double (nat a)` must be `N` or `L (L T)`", I had assumed that `T` was a wildcard, and that your types were sets of patterns containing wildcards. To avoid confusion, I will be using `*` for wildcards instead of `T`. For example, given the following definition, the input type would be {`N`, `L *`} and the output type would be {`N`, `L (L *)`}. double N = N double (L a) = L (L (double a)) To obtain the input type, we take the left-hand-sides {`N`, `L a`} and we replace the variables with a wildcard `*`, yielding {`N`, `L *`}. To obtain the output type, we take the right-hand sides {`N`, `L (L (double a))`}, and we replace the function calls with a wildcard `*`, yielding {`N`, `L (L *)`}. To type-check a function composition `f (g a)`, we validate that each pattern in the output type of `g` is subsumed by at least one pattern in the input type of `f`. The algorithm which you are describing in this post seems much more complicated. Just so that I don't again misunderstand your representation: `Nat` is `is_nat`, right? So a type has the same syntactic representation as the source code for a pattern-matching function, that is, it's a set of pairs, each of which is a left-hand side pattern consisting of `N`, `L`, `C`, and variables, plus a right-hand side consisting of an arbitrary expression containing `N`, `L`, `C`, the variables bound in the left-hand side, but also arbitrary function calls. The rules for inferring a type, as far as I can see from your examples, are: 1. a variable `a` has type {`a -&gt; a`} 1. `N` has type {`N -&gt; N`} 1. if `e` has type {`p -&gt; v`}, then `L e` has type {`L p -&gt; L v`} 1. if `e1` has type {`p1 -&gt; v1`} and `e2` has type {`p2 -&gt; v2`}, then `C e1 e2` has type {`C p1 p2 -&gt; C v1 v2`} 1. if `e` has type {`p -&gt; v`} and `f` is a function defined by pattern-matching and one of its definitions is the pair {`p1 -&gt; v1`} and `p1` unifies with `v` under the substitution `s` and `v1` has type {`p2 -&gt; v2`}, then the type of `f e` is {`p2[s] -&gt; v2[s]`}. Or something. I probably got the details wrong :) 1. otherwise, or to avoid infinite recursion, `f e` may also have type {`p -&gt; f v`} 1. if the type of `e`, `e1` or `e2`, or `f` has more than one pair, then the rules above are repeated for each pair Following those rules, depending on which functions we choose to expand, the type of `double (is_nat a)` can be either of the following: * {`a -&gt; double (is_nat a)`} * {`N -&gt; double N`, `L a -&gt; double (L (is_nat a))`} * {`N -&gt; N`, `L a -&gt; L (L (is_nat a))`} Did I understand your algorithm this time?
Prefer pattern matches to `!!` such as: (faceValueS:quantityS:rest) = splitOn "x" str Similarly, instead of calling both `head l` and `tail l`, prefer to just pattern match in the arguments.
If you can get your serialise/deserialise to work with `lex` so that it composes well with read instances (which I think you can) the using read/show instead of custom functions might clean up some stuff.
The `changeValue` function is a perfect example of something that should be rewritten as a fold -- or a composition of `map` and `sum`
Not a specific comment, but something you'll want if you don't have it already is hlint
Tangentially, are you planning more NixOS posts? I'm really liking the experience so far, but it would be good to have more "real world documentation" from long-term users.
I think the normal advice (which helped me a lot) for people struggling with let vs binding is to not use do notation until the difference is intuitive. Use the operator directly instead.
More than happy to write about it more! Anything in particular you're curious about? We have a moderately interesting deployment story at work now, which I might be blogging about on our work blog. Subscribe to /r/nixos/my rss feed to make sure you don't miss that :)
For me it was the opposite. I learned by just using do notation until I could intuitively understand how the operators worked. I guess what worked for both of us was putting time into trying to learn it.
Trust the operator precedence. currentPlayer :: Game -&gt; Int currentPlayer game = mod (turn game) (playerCount game) coinValue :: Coin -&gt; Int coinValue coin = (faceValue coin) * (quantity coin) becomes currentPlayer :: Game -&gt; Int currentPlayer game = turn game `mod` playerCount game coinValue :: Coin -&gt; Int coinValue coin = faceValue coin * quantity coin 
Ah yes, like this: changeValue :: [Coin] -&gt; Int changeValue x = sum $ map coinValue x Right?
thanks! :)
Yup. Similarly, `bestMove` looks like you're trying to find the highest valued move based on some sort of criteria. The function for that is `maximumBy`. The invocation looks something like bestMove xs = maximumBy (comparing moveValue) xs You'll notice that when writing Haskell, one tends to *avoid* explicit recursion. Instead, we have a lot of special loops and functions that do what we want. (Such as `map`, `maximumBy`, `fold` and so on.)
that makes sense, thanks again
also put the {-# OPTIONS_GHC -Wall #-} pragma at the top of your file
I'm sorry I mentioned "let" vs "bind", that wasn't the focus of my argument. The point I was trying to make is that there needs to be an easier way to start using Haskell productively so that its value can be recognized more quickly. Otherwise you have to invest a LOT of time before you can do anything that's actually useful --- fine if you're in college, not so much if you're working. Examples/snippets in Hackage would help tremendously. I agree strongly with the OP on this. He (or she) is not alone.
Eh, it is just that writing code is fun and writing doc is boring
I am not sure about the algorithm, I'm just experimenting, so feel free to pick the one you think could work. If you know for sure that is an undecidable problem regardless of not being a turing complete language, please say so! My intuition and hope is that this works, but the system isn't able to prove certain mathematical statements. That wouldn't violate against Gobel's incompleteness theorem. (But if you actually don't know and is indeed trying to figure out the algorithm, it is just a simple way to fuse composed functions! I implemented the rest of language yesterday and today I'll try to implement that part, if it don't work it will become obvious soon!)
Oh yeah, I agree with both you and OP on this. My comment was mostly directed toward the one I responded to, saying that they shouldn't assume using the operators is always easier than using the do notation for beginners – there are probably beginners of both kinds and we need to accomodate to both.
Dear Haskell Beginner - I too share your sense of BEING CLUELESS: solution 1.)cabal install via at least three methods. 1.1)routine cabal - always use fixed dependency template or you can easily break the cabal itself. 1.2)download package and cabal install THE FULL PACKAGE which sometimes has examples or tests 1.3)download different versions and even different library/modules which may have examples 2.)search via search engine startpage or even g o o g l e. hidden examples are on course websites, bloggers, etc. 3.)search the haskell email lists/ threads/ discussion boards, etc. 4.)buy the books 5.)there is a long and winding LEARNING CURVE - maybe fpcomplete? 6.)remember that this is haskell and NOT PYTHON. there is no guarantee that the people like these answers are a.)not academics or researchers since haskell is a research language b.)specialized only in financial services and who cares whether you can do something useful in openCV or python vision c.)wrapping of C libraries are a lot of fun using FFI. your results may vary - VARY A LOT WARNING VARY A LOT. d.)haskell is secure and type safe and ??? and the HEARTBLEED BUG for C library can STILL BREAK YOUR WHOLE SYSTEM. e.)using pure haskell can be 'safe' but can result in SLOW and ineffective code. benchmark it against python (uggg) and using parallel code via hack up shell script and PYTHON WINS?? f.)there don't seem to be any requirements for hackage so no tests, no routine compile flags, no examples... no problem. WELCOME TO THE CLUB. 1.I can understand the domain and engineering - for I have engineering experience. 2.)sometimes I can understand MATH for I LOVE MATH 3.)sometimes I can understand the PROOF, even If I am not a theoretical MATH MAGICIAN - / academic 4.)sometimes I can use Haskell 5.)sometimes I can write 'toy examples', and pass the course exams - and no, I am older and have plenty of other problems to solve IN THE REAL WORLD and in REAL LIFE. 6.)sometimes I can't even get 'reasonable example' to run - because I AM CLUELESS - thanks a lot HASKELL. SO I AM STUCK LIKE YOU ARE, friend. 7.)sometimes I am able to get REAL WORLD example to run by combining and 'merging' and re-engineering and selecting via Zeller Differential DIAGNOSIS methods of various muti code base. 8.)sometimes I am just trying and use FUZZING and brute force method I have no clue as to why this template or some strange feature with BAD documentation on haskell works or does not work. 9.)using alternative HOL Coq and other languages to troubleshoot haskell itself. THE LANGUAGE ITSELF. 10.)You see. the ACADEMIC language is for GLASGOW people or even those who are SCOTTISH... go ahead and flame me. It is academic and as PROOF many of the hackage packages are labeled as EXPERIMENTAL. 11.)python 2x an...
Thank you.
Function application goes *always* before operators. That's easy to remember.
Well, it depends on why you are publishing the code to Hackage in the first place. If you are using Hackage as a second Github then there is no incentive to document it. However, if you are releasing your code because you really want others to use it, then you will enjoy documenting and communicating your ideas.
Specifying the 'xs' variable twice here is redundant as well. 
Why not just use Show and Read?
Yup. And changeValue = sum . map coinValue (or perhaps more efficiently, changeValue = getSum . foldMap (Sum . coinValue) .) But I was thinking this was a one-thing-at-a-time deal.
`show` is meant to give you a string you can paste into GHCi and it will give you back the object you `show`ed. (See for example how the show instance of Data.Map.Map uses `fromList`.) They are not meant for custom output/input.
I'm in a similar situation, and I tend to find after I write something that I've reinvented the wheel; there are existing library functions that would've made my life simpler had I known them. An example: your bestMove could, I think, be written import Data.Ord bestMove :: [Move] -&gt; Move bestMove = maximumBy (comparing moveValue)
&gt; liftM2 Google tells me this is something to do with monads. I don't have my monad wizard hat yet. :( 
It's a cute hack, but I think your code is more readable and there's no difference in efficiency. Without getting a wizard hat, liftM2 in this case is just liftM2 op f g x = f x `op` g x Technical discussion (wizard hat engaged!): liftM2 f mx my = do x &lt;- mx y &lt;- my return (f x y) In this case, we are using the monad `(Game -&gt;)`, that is, `m a` = `Game -&gt; a`. (Sadly, operator sections are not legal syntax in types, so you have to write it as `((-&gt;) Game)` which is very confusing to beginners.) instance Monad ((-&gt;) env) where -- return :: a -&gt; (env -&gt; a) return a = \e -&gt; a -- (&gt;&gt;=) :: (env -&gt; a) -&gt; (a -&gt; (env -&gt; b)) -&gt; (env -&gt; b) f &gt;&gt;= k = \e -&gt; k (f e) e -- follow the types! -- f :: env -&gt; a -- k :: a -&gt; (env -&gt; b) = a -&gt; env -&gt; b -- e :: env -- f e :: a -- k (f e) :: env -&gt; b -- k (f e) e :: b -- \e -&gt; k (f e) e :: env -&gt; b Desugaring liftM2: liftM2 f mx my = do x &lt;- mx y &lt;- my return (f x y) -- do { v &lt;- exp ; ... } -- ==&gt; exp &gt;&gt;= \v -&gt; do { ... } liftM2 f mx my = mx &gt;&gt;= \x -&gt; do y &lt;- my return (f x y) -- again liftM2 f mx my = mx &gt;&gt;= \x -&gt; my &gt;&gt;= \y -&gt; return (f x y) In `(env -&gt;)` monad: liftM2 f mx my = mx &gt;&gt;= \x -&gt; my &gt;&gt;= \y -&gt; return (f x y) -- inline &gt;&gt;= liftM2 f mx my = \env -&gt; (\x -&gt; my &gt;&gt;= \y -&gt; return (f x y)) (mx env) env -- beta reduce (apply the function) liftM2 f mx my = \env -&gt; (my &gt;&gt;= \y -&gt; return (f (mx env) y) env -- inline &gt;&gt;= liftM2 f mx my = \env -&gt; (\env2 -&gt; (\y -&gt; return (f (mx env) y) (my env2) env2 env -- beta reduce \y -&gt; and \env2 -&gt; liftM2 f mx my = \env -&gt; (return (f (mx env) (my env))) env -- inline return liftM2 f mx my = \env -&gt; (\env3 -&gt; f (mx env) (my env)) env -- beta reduce (env3 isn't used, so just thrown away) liftM2 f mx my = \env -&gt; f (mx env) (my env) -- move lambda to other side of = liftM2 f mx my env = f (mx env) (my env) -- alpha (rename arguments) liftM2 op f g x = op (f x) (g x) 
Why liftM2 and not liftA2? Aren't applicatives both weaker and easier to explain? (Then again, this might just be me dealing with the frustration of there being corresponding M functions to every A function because of historical reasons.)
Yeah actually I don't think it would iterate through the list twice. Looking at the source for sum and the source for map, sum asks for elements one-at-a-time, and map provides elements one-at-a-time, so sum just adds them up as map gets it. I was hoping something possibly along the lines of foldMap being possibly optimized for its associativity, requiring only log n operations instead of n. Maybe we can get an associative monoid folder we of these days.
C++ linking is nasty, even (or especially) when mixing and matching C++ compilers. The most effective way to handle linking into C++ from haskell (or ANY compiler/language but the original c++ compiler) is to expose the desired api with a C style interface via an extern "C" { code here} block. a good "in the large" example of this approach is any of the C++ files (.cpp) in LLVM-General https://github.com/bscarlet/llvm-general/tree/master/llvm-general/src/LLVM/General/Internal/FFI
Yup! That holds for multiple arguments, actually: -- say we want -- subtractAll a [b,c,d,e] == a - b - c - d - e subtractAll :: Int -&gt; [Int] -&gt; Int -- could be defined as subtractAll left rights = foldl' (\accum right -&gt; accum - right) left rights -- we can drop one subtractAll left = foldl' (\accum right -&gt; accum - right) left -- or both arguments subtractAll = foldl' (\accum right -&gt; accum - right) -- and still be defining the same thing -- we can also do the same with the anonymous function if we make the `-` operator look like a normal function subtractAll = foldl' (\accum right -&gt; (-) accum right) -- dropping one arg is the same subtractAll = foldl' (\accum -&gt; (-) accum) -- dropping the last arg gets rid of the \ and -&gt; since \-&gt; isn't legal syntax subtractAll = foldl' (-) 
Certainly agree. Not all is fine and dandy in Haskell land, and a lot of libraries would definitely benefit from examples. But then again, quite a few libraries do come with excellent examples - off the top of my head, I would quote Lens, Scotty, Happstack (most of the documentation here *is* examples), and ZeroMQ (though the Haskell examples are found in the documentation of the original ZeroMQ library rather than that of the Haskell bindings).
Fair points. The Haskell community does have a tendency to optimize itself towards experienced programmers, probably because so many of us are coming from a background full of frustrations with other languages, which can sometimes lead to a certain bias. And what is trivial and logical and what isn't is obviously highly subjective.
Nice concept, but it's a pity about the name :/
But more than that, they provide automatic serialization/deserialization.
Yeah.
(regular) lists in haskell don't use stream-fusion but buildr optimizations. use the `stream-fusion` package to get stream-fusion for lists.
Well, we don't have to reuse the name! :)
Not one that's meant to be used in production, no.
Thanks for the article! I like his comic from land of lisp.
&gt; not sure about the algorithm &gt; [not] a turing complete language &gt; [not] able to prove certain mathematical statements. That's the spirit! When I saw that your approach was based on calculating functions and determining whether those functions were well-defined on all their inputs, I was worried because I know that determining this is undecidable in general. But now that you have clarified that you are ready to limit the expressibility of your language, I'm no longer worried! After all, if we have conservative totality checkers, why not conservative well-definedness checkers? &gt; I'll try to implement that part, if it don't work it will become obvious soon Good idea. Even if it doesn't work out, trying to implement an algorithm which you don't fully understand is a good way to obtain an intuition for *why* that particular approach didn't work. That, in turn, might tell you how you could further constrain your language to make the problem easier to decide.
cool. You might want to take a look at how llvm-general does it engineering. NB: for c++ libs to play nice in GHCI, you need to link to them as shared libs not static libs. 
I'm not familiar with how Emacs renders text, but I'm sure it's perfectly possible to implement, if not already available in some GUI versions of Emacs. If someone would add ligature support to terminal windows, it would work perfectly well in Vim, Emacs, Nano, Bash, etc. OpenType features haven't really been used in monospace fonts, so it hasn't been a concern (so far).
Very cool! Is there a list of supported ligatures? I'd want things like `&lt;$&gt;`, `&lt;*&gt;` etc.
so far there's `&lt;-, -&gt;, =&gt;, &gt;&gt;, &lt;&lt;, &gt;&gt;=, =&lt;&lt;, .., …, ::, -&lt;, &gt;-, -&lt;&lt;, &gt;&gt;-, ++, /= and ==`. I'm open for suggestions for new ligatures and how you think they should look!
Operators with angle brackets are usually typeset so that angle brackets touch the inner symbols. The common ones are `&lt;$&gt;`, `&lt;$`, `&lt;*&gt;`, but people often invent others, like `&lt;$$&gt;`, `&lt;**&gt;`, `&lt;||&gt;` etc.
The problem with "just read the types" is that types define the *universe* of possible uses but tell you nothing about the *distribution of value* over those uses. Good examples not only show you where the value lies but also how to extract it cheaply.
Thanks for letting me know!
&gt; NB: for c++ libs to play nice in GHCI, you need to link to them as shared libs not static libs. I'm forced to link the wrappers both ways at the moment. GHC can't find any symbols the wrappers need unless the archive is linked statically, and running the executable already requires the shared lib. I'll look through llvm-general, thanks for the tip.
This looks really cool! It's a shame it's not on sublime, though.
Am I the only one who prefers to see ASCII symbols, plain and simple?
One of the integral problems is that several of those ligatures need wider characters to look good. Monospaced fonts tend to not have very wide characters.
`&gt;=` and `&lt;=` (from `Ord`) would probably be a good idea too. I also think /= should have the slash centered, like in the mathematical symbol.
I think a big part of it is habit. I too prefer ASCII symbols, but I know that if I got used to this, I wouldn't want to be without it.
Maybe, but I think I'd just prefer to keep things as simple as possible. 
My main problem with this is that I don't like the underlying font. If it was done in a font like menlo then i'd probably use something like this. 
That'd be rather confusing.
/= is a bit ambigous and I guess there's really no one true way to do it. Someone else would probably have solved it differently. My rationale was that after all, it's still a '/' followed by a '=', and it should still look like what it is, even to someone unaccustomed to Haskell or this typeface. The same problem is faced with &lt;= (and &gt;=). It should look like what it is, a '&lt;' followed by a '=', anything else is bound to be confusing to some. But perhaps they could be added as discretionary ligatures to be turned on at will..?
Your posts are really incredible. Every time I read one it reinvigorates my desire to further enhance my knowledge and usage of Haskell. Your posts have been quite motivating for me.
Of course, right-folding with an operator that's strict in its right argument makes for Very Bad Times.
Of course you won't feel smart enough. You've never seen it before! Learning Haskell is like learning programming all over again. I've been doing Haskell for a few years now, and even I have trouble following along in the reasoning above. I can, for sure, but I'd like to avoid it if possible. This is because I'm, like you, a learn-by-doing kind of person, so I've mostly learned how things work by doing, and figuring out the theoretics of it all takes me quite a bit of time and effort.
I think == should still have a notable intersection in the middle, to me it looks too close to = at the moment.
You're right, the == is kind of problematic. In a way I want to make it a triple equals sign ≡, but I don't want to go too far from the pure ascii presentation. I hope that in the future there will be more of these fonts and everyone can pick the way that works best for them :)
added, thanks!
Ok, quickly coming back to report that it works with gEdit, only problem being that it highlight matching `&lt;` and `&gt;`, which destroys the ligature for a splitsecond, but that's fine for me.
added!
thanks for letting me know. Ligatures are usually not expected in monospaced fonts so these kinds of problems surface. Hopefully support will eventually get better!
test with ghc 7.8, a lot of your linking woes will evaporate. (if not, please share examples that the new dynlinking regime fails on )
I'm on 7.8 already. Examples are in the OP, but I need to figure out a minimal case that reproduces the bug still.
Yeah, I like the spacing with ASCII symbols.
This is awesome, but it says nothing about `callCC`, which is IMO *the* problem when understanding continuations. With his interpretation I got: callCC f = ContT $ \_return -&gt; f (\a -&gt; ContT $ \_return' -&gt; _return a) &gt;&gt;- \b -&gt; _return b Which is relatively readable. I think I understand it, but if I tried to explain it it would end up like a burrito metaphor.
Oh wow, mind blown thanks!
Oh cool, I definitely have to learn more pipes stuff, I've only done a bit of it so far.
This is the first osx-only feature I've seen
Sorry for the rambling in this post, wasn't really able to piece together the ideas that well this time. I think I can put something better together from the replies I got though.
It was required for iOS. There's probably no reason it can't be done on Linux or Windows as well, I just don't think it was implemented/tested.
Arguably, just the ContT constructor, or cont function, and runCont/runContT are a more elegant way of using continuations than callCC. I may be mistaken though, but I think cont corresponds to shift and runCont corresponds to reset in delimited continuation literature, which is far nicer than callCC. I do remember someone telling me about some subtle difference between reset/shift and those functions though...
It's all really mechanical, no voodoo at all. You probably understand computers in a 'state machine' kind of way, and Haskell treats computers as big logic machines. Almost everything in my comment is basic string replacement. (\x -&gt; some_expression) (some_other_expression) is equal to (by beta reduction) some_expression with 'x' replaced by (some_other_expression) do { x &lt;- exp ; ... } is equal to (by desguaring of do-notation) bind exp (\x -&gt; do { ... }) Except instead of `bind`, we call it `&gt;&gt;=` and put it infix.
Yup. If you want to be fancier, you can even write it in a point-free style: changeValue :: [Coin] -&gt; Int changeValue = sum . map coinValue _edit: I see /u/kqr beat me to it several comments deeper into the conversation._
Check out the "interact" function. Also, you're going to see "point-free" programming a lot, so you might want to get some practice at it. For instance, your "processGame" function could be written as: processGame = serializeMove . selectMove . deserializeGame . lines 
Memo-tries come to mind.
Can probably just implement a new class which just delegates to the Show instance for the constructors that don't need custom formatting. class Show a =&gt; Pretty a where ppr :: a -&gt; String instance Pretty Foo where ppr (Twentieth _ xs) = ... -- your logic here ppr x = show x
Pretty cool. Makes it that much easier. I really like where Haskell is going these days.
Ah, I guess I left out an important detail. Many of the `Foo`s are recursive: data Foo = ... | Sixth Foo Foo | Seventh Foo String | ... Is there a way to adapt your solution to recursive ADTs? I agree that it is excellent for non-recursive ones. 
I suppose one idea would be to make a separate ADT for that one line, with a separate implementation of `Show`: data Foo = ... | Twentieth TwentiethHelper deriving Show data TwentiethHelper = TwentiethHelper String [Bool] instance Show TwentiethHelper where show TwentiethHelper s _ = show s -- The "Twentieth " goes up with Foo's implementation of show. Unfortunately, this adds an extra layer of pattern matching to all other functions that use `Twentieth`, so I don't think this is such a good approach. 
There *is* a clean way to do this, but... it's much simpler to use one of the following hacks. *Hack #1* (recommended): create a second type, very similar to `Foo`, except its version of `Twentieth` doesn't contain a list of bools. If you put that second type in a separate module, it can also be named `Foo` and similarly for the constructor names. You still need to convert between the two versions of `Foo`, but that conversion function should be much easier to write than `show`. *Hack #2*: wrap the list of `Bool`s in a special type whose `Show` implementation is the empty string. There will be an extra space and/or parentheses where the lists would have been, but there are ways to work around that. *Hack #3*: manipulate the `String` produced by `show` and remove the parts you don't like. *Not a hack* (but a lot of work): use [scrap your boilerplate](http://hackage.haskell.org/package/syb) to implement your own generic `show` function, except your version always asks the type of each argument whether the argument should be omitted. Wrap your list of bools into a newtype which says that it always wants to be omitted.
Vim2hs does this, to some degree. https://github.com/dag/vim2hs It converts \ into lambda and . into a circle.
I suppose Hack #4 would be to have GHC ``-ddump-deriv`` the derived Show instance and then modify the resulting code. Although imho forcing the Show instance do something that it's not designed for ( i.e. break: ``read . show = id`` ) is kind of hackish.
Thank you! These seem like good ideas. 
I agree that breaking the laws of `read` and `show` is a bit of a [code smell](https://en.wikipedia.org/wiki/Code_smell), but surely I'm not the first person to have this sort of problem. Is there a more canonical way of turning something into a `String` that is not the inverse of `read`?
Ah, using pretty is a great suggestion. Thanks!
Best practice is usually to leave `show` as outputting syntactically correct code, such that `read . show == id`. So in this case I'd suggest a separate type class/function, but I understand that doesn't help your problem at all.
I just tried it with gvim and it doesn't seem to work, unfortunately. Not just the ligatures though but all glyphs seem to be mixed up.
One comment above pushed me to try the font in Konsole (one of the popular terminal applications for Linux). And, funnily enough, it works without any issues. So any terminal application started in it will benefit from the font. Be it Vim, nano, etc.
Right. And once you start using the continuation as a function, the analogy breaks somewhat, because the type of `_return` is `r -&gt; m x`, while the type of `return` is `r -&gt; m r`.
I am using haskellconceal for vim, which substitutes some of these symbols for Unicode characters without changing the underlying text file(the line which the cursors is on gets shown as only ascii) http://www.vim.org/scripts/script.php?script_id=3200
So set those character font to the one where they occupy double width. You can even use that Source Code Pro L, just change symbols' places to their proper unicode codes. Edit: [voilà](http://i.imgur.com/5WyspIw) (let ((font "Source Code Pro LU 20")) (set-default-font font nil t) (set-fontset-font t '(8500 . 8800) font)) 
great blog post! could someone explain the bit about lifting into the free applicative thingy? i've never heard of that before.
Nice. Thanks!
Congratulations to everyone on a great release. This release addresses almost every pain point I've come across with cabal.
So it's recommended to always use a sandbox then?
Yes. You should also install the minimum number of packages globally too, to avoid dependency conflicts and cabal hell.
* Dependency freezing * Now cabal doesn't re-link executables (of any kind) unless something changed. Awesome, thank you cabal team!
Be warned the new `ghc -j` has caused me a number of issues during rebuilds on Mavericks (and the issue may extend to other platforms) when building both shared and static object files. I've had situations where I'll build, edit a file, rebuild and it'll get hopelessly confused and complain about missing symbols, but if I clean and rebuild or build without parallel builds it works fine. This is distinct from the other issues you get when building both on GHC where you `^C` out in the middle of a build and the .hi files get made but not the .dyn_hi files and you're forced to clean.
This is awesome.
Could someone please elaborate on "all the reasons [Unicode characters] never worked out in APL"? I'm not familiar with APL, nor is it obvious to me what those reasons could have been.
send it to neovim. i hope that project get's to anything, because it's so much more responsive to issues.
Sweet! Kudos to Mikhail Glushenkov who has done most of the hard work for the release.
Offered him a beer. No reply :(
You can do `cabal help` to see an overview of commands, and then `cabal help &lt;command&gt;`
Fair enough, although I think this could be (and sometimes is) dealt with by up to date, accurate documentation. It'd take a lot of effort by the maintainer though; which Haskell obviously doesn't have.
It doesn't work in Aquamacs at least, it just looks like Source Code Pro. Unless I'm missing some configuration option.
try emailing haskell@freebsd.org if you need a package included or upgraded. I have....it works! the freebsd community is attentive 
Vector is perfectly safe with the default flags as long as you don't use operations with the word unsafe or basic in the name. Use it! Safe Haskell has a very specific meaning wrt running Untrusted code to facilitate auditing. 
The text is a bit unclear. What I mean is that APL-style single-character operators never gained widespread support. I suspect the reason is that most people feel very uncomfortable with such terseness. Game of life in APL is: life←{↑1 ⍵∨.∧3 4=+/,¯1 0 1∘.⊖¯1 0 1∘.⌽⊂⍵} Which is very cool but also very intimidating. 
Yay! And so much yay for `cabal exec`!
Okay, so I have two follow-up questions (I'm still learning): 1) I've made it a habit to just use the Safe pragma whenever nothing prevents it. I find it helpful to have the compiler tell me whenever I'm about to do something that could potentially be harmful, and the guarantees listed [here](https://www.haskell.org/ghc/docs/7.6.3/html/users_guide/safe-haskell.html) all sound very good. So what's the specific meaning that you are referring to? 2) Coming from a C background, I see vectors a lot like C arrays. So I'd often want to build a vector incrementally, i.e. I'd first build a vector big enough for my data and then fill it up via update. Since Haskell is a pure language, there's theoretically a new vector begin created every time I use the update function. I'd imagine this to be a considerable bottleneck and I should probably use mutable vectors in this case. Is this correct, or does the compiler work some magic here?
If you don't manipulate the vectors in an unsafe way in your own code you might consider marking your module `Trustworthy` rather than `Safe`. That is a claim that you believe your module is safe for others to use, but that they have to trust you.
All the yay! `cabal repl` was.... not enough.
1) all the ways you can write "unsafe" haskell can only really be done deliberately, if you can accidentally write a well typed term thats unsafe, its a bug in the type system. Safe haskell is merely a tool to make auditing for those deliberately unsafe features in code you depend on easier! 2) you're touching on some sophisticated stuff (at least in full generality), but the short answer is that Vector provides both mutable and immutable apis. And the real answer is use criterion to benchmark different approaches and choose based upon your benchmarks! http://hackage.haskell.org/package/criterion seriously, when in doubt, write things both all the various ways, and benchmark them :) 
well the ports provided cabal is too old for sandboxes (I hope they move to 1.18 soon though...)
It depends what the string is for.
How about using the obscure "array" package, from the Haskell language standard? 
Indeed, this has helped me a lot while learning precedence. It tells me when there are extra parens, and lets me figure out why they are not needed.
You should get in the habit of seeing anything between infix operators as being a single item. It seemed strange at first, but once you start using it, the precedence rules become second nature.
It is a problem that comes at least from the Enlightenment. The idea that men learn by assimilating rational rules. That´s why still many people think that learning the grammar is the main task for learning a foreign language. Or for being a good musician the first thing necessary is to mastering music theory. That has been the dogma in any academic discipline since then. That is why academic books are so harsh and anti pedagogical. And... what was the question by the way? Haskell? Ah.... Haskell is an academic language. NOTE: People does not learn by assimilating rational rules. rational rules were not in the nature falling from the threes. People learn by examples. the rationalization and abstraction is a posteriori. Any person that learn must reproduce in his mind the process of the man that discovered it. Just give them examples. Or even better: give them your history of problems that you tried to solve with your library. I ever include a decent quantity of examples in my packages. It makes you and your code less mysterious and less appealing, but far more usable. the example way is the way of craftmanship. http://www.cs.utexas.edu/users/EWD/transcriptions/EWD04xx/EWD480.html
This looks great. One question, other that seeming unnecessary, what is the potential harm of developer doing 'cabal install' versus the recommended 'cabal build'? 
We're working on improving `cabal repl` so it "just works". For example, we'll rebind `:reload` so it calls back out to cabal to re-run preprocessors. This means reload will work even for e.g. .hsc files. We'll also add support for a &lt;target&gt; parameter, so you can control the context you get dumped into when you run `cabal repl`.
First it's conceptually a bit odd: why do you want to install a package that's in some temporary in-between releases state? Installing is something you'd do if you want other packages to be able to build against the installed package, which is no longer necessary with `sandbox add-source`. It's also telling cabal something different than `build`, for example for installation we ought to create docs, which isn't necessary for build. It comes down to expressing your intentions to cabal precisely: you typically want to build something to see if it compiles, you don't want to mutate some package DB somewhere (which is what `install` does.)
You dont need to program in that style. I would argue that he barely shortened the code, in exchange for which he had to pass through a few extra layers of abstraction. Sometimes the reward is worth it, however, so it is worth looking at. The idea is this: LiftM2 takes a binary function (a -&gt; b -&gt; c) and returns a "lifted" version of that function that works on monadic values instead (m a -&gt; m b -&gt; m c). What is this "monad" thing in this case? Well, lets take a look at the first parameter, `m a` and see what we are passing in... turn :: Game -&gt; Int So, the way that matches up is that `a` is `Int` and `m` is `(Game -&gt;)`. Semantically, I like to think of it like "An integer, assuming we provide a game." The second parameter has a similar pattern: playerCount :: Game -&gt; Int And so the `m`s match up! So how would I say what this is doing in plain old english? It takes a binary math operation, and two operations that would give you the parameters you needed if you had a game, and returns a function that takes a game, runs the extraction operations, then runs the binary operation, and returns the result. The reason it is done in a monad is because the same type of pattern works for many different things, and is thus generalized, but you can find out how that works as you go.
If the plan is to use sandboxes then there's no harm in just `cabal install cabal-install` by hand
It works™. `pkg install ghc` and wait 20 seconds. (Of course there are packages that won't compile but they are system-dependent like linux-cgroup etc)
Funny you should mention that, I'm going through the same process with FRP! I'm planning to use the reactive-banana-wx and I agree, it's very tough to find good examples. One tutorial that helped me was this [one](http://alfredodinapoli.wordpress.com/2011/12/24/functional-reactive-programming-kick-starter-guide/). Reactive-banana has a couple simple/good examples, I like their slots machine one. Still though, even then I had trouble connecting the explanations of FRP that I've read and the code given as an example. I think this is also a huge deficiency in some Haskell documentation.
OpenType has discretionary ligatures built in.
If it's just for debugging, why do you need to hide that piece of the output?
Woah, great work! I'm not familiar with emacs, where do I input the code? I now have a fresh installation of aquamacs.
Indeed! Many libraries have better docs on their github project README.md than in the cabal description field probably because the readme is a lot more convenient to edit. Would be great to be able to re-use this file for both GH and Hackage. IMO it's the Hackage top page that's the most important entry point into a library but that's usually in the worst shape because it's hard to edit this in .cabal.
for vim users check out [vim2hs](https://github.com/dag/vim2hs)
interesting, thanks for the advice
Not quite: e.g. `cabal install` performs stripping; `cabal build` doesn't.
That data is inconsequential, yet if you `show` it, it takes up the vast majority of the string. With it removed, the debugging strings are 5-ish lines long (80 characters wide, so 400-ish characters total), but with it in the strings don't even fit on my screen. It becomes very hard to find the relevant parts in this sea of inconsequential stuff. I've got a workaround (I went with hack #2 from /u/gelisam), but I'd still be interested in hearing about non-Show ways of displaying ADTs that don't require coding the whole thing up by hand.
How do I use this in GNU/Linux?
Now we finally have currency literals like we've always dreamed of, and it only involves abusing four language extensions to get them. {-# LANGUAGE PostfixOperators #-} import Acme.Pointful data Euros = Euros Pointful deriving Show data Dollars = Dollars Pointful deriving Show (€) :: Pointful -&gt; Euros (€) = Euros ($) :: Pointful -&gt; Dollars ($) = Dollars moneh = (3.14 €) :: Euros mollah = (2.71 $) :: Dollars
The definition of `Num (Chuwabra a b)` doesn't actually *have* to be that terrible. Since `[]` is an applicative functor, the num instance can be lifted to act component wise. 
Thanks. I'll have a look at the binary sizes. So this case might be one exception where 'cabal install' remains the right move. Particularly if you are deploying to a remote machine for testing and leaving it there if it passes. 
You can add/multiply them pointwise if you like: zipWithExcess :: (a -&gt; b -&gt; c) -&gt; (a -&gt; c) -&gt; (b -&gt; c) -&gt; [a] -&gt; [b] -&gt; [c] zipWithExcess f g h = go where go (x:xs) (y:ys) = f x y:go xs ys go [] ys = map h ys go xs [] = map g xs instance Num (Chuwabra a b) where fromInteger = Pointful . (:[]) abs (Pointful xs) = Pointful (map abs xs) signum (Pointful xs) = Pointful (map signum xs) negate (Pointful xs) = Pointful (map negate xs) Pointful xs + Pointful ys = Pointful $ zipWithExcess (+) id id xs ys Pointful xs - Pointful ys = Pointful $ zipWithExcess (-) id negate xs ys Pointful xs * Pointful ys = Pointful $ zipWith (*) xs ys Then &gt;&gt;&gt; 1.2.3 + 2.3 :: Pointful Pointful [3,5,3] [edit: fixed] 
This is just `Monoid` trickery, you will learn about them as you go. (hint: look at the definition of `Sum` [here](http://hackage.haskell.org/package/base-4.7.0.0/docs/src/Data-Monoid.html#Sum))
&gt; Pointful xs + Pointful ys = Pointful $ zipWithExcess (+) id negate xs ys Shouldn't this be Pointful xs + Pointful ys = Pointful $ zipWithExcess (+) id id xs ys ?
You're agreeing with him. Install as few as possibly outside of sandboxes.
Thank you for the clarification. Are you saying that the same APL program is less intimidating if the font is rendered with ligature? ;) Regarding Unicode compatibility: my Reddit client doesn't show most of the characters of your example correctly...
We'll fix `build` so it strips as well: https://github.com/haskell/cabal/issues/1795
The truth is no one really knows if advanced typography in code will be just eye candy or if it will actually make a difference - my guess is as good as yours! If APL would be designed from the start with multi-character-wide glyphs, I imagine it would be so.
I can see benefits of that, but it also means that `build` will unnecessarily take a bit longer to complete (during development you don't usually care about the executable size). You really want to strip only when deploying, so the distinction between build and install seems useful here.
Up at last : https://github.com/erikd/haskell-big-integer-experiment 
You can control stripping with flags (and we perhaps ought to have compilation modes such as `-c dbg` and `-c opt` so we can tell cabal what we're compiling for i.e. development vs deployment). `cabal install` isn't really good for deployment either, as you typically don't deploy on the development machine, but instead you want some bundle to copy to another machine. install makes you have to dig out the binary from where it put it on the local, development machine.
&gt; cabal install isn't really good for deployment either, as you typically don't deploy on the development machine, but instead you want some bundle to copy to another machine. Yes, but at least that puts all the executables in one well-specified place, and you don't have to dig them out of dist directories. I like the idea of a flag for development vs production build, though.
Feature request: could you make something pretty from `{-#` and `#-}`, they are such an eyesore.
 GHCi&gt; 10.100.1000 :: Pointful Pointful [10,1,1000] :(
For printing for humans I use the Buildable class from text-format
Yep.
Ack. good catch.
GHC already doesn't re-link if nothing has changed, so why is relinking avoidance needed in Cabal too? Just curious. We have a bug open in GHC about the relinking avoidance being not accurate on non-Linux platforms: https://ghc.haskell.org/trac/ghc/ticket/4451 
You most certainly do have in-process referential transparency violations.
So, what was the conclusion...? How does the performance/completeness compare?
In normal emacs it's either ~/.emacs or ~/.emacs.d/init.el
Can you at least add a link to the criterion report, since you [seem](https://github.com/erikd/haskell-big-integer-experiment/blob/master/bench-integer.hs) to have a benchmark in the repo?
I thought I heard a rumor once that Daniel Fischer was working on a gmp replacement, and that he thought he could write one in Haskell that's faster than the gmp itself. I haven't heard about this effort and where it is recently, however.
Try [Parallel and Concurrent Programming in Haskell](http://chimera.labs.oreilly.com/books/1230000000929) by Simon Marlow. It covers this sort of thing well.
I ran the test on my laptop, in case anyone else cares about interpreting the results. https://www.dropbox.com/s/08hkte8ipfr0lev/inttest.pdf or in the original HTML output https://www.dropbox.com/s/qfju5u1rv2ytn5t/new-bench-integer.html make check did however have one error. Here is the full output: ~/haskell-big-integer-experiment master ❯ make check ghc -Wall -fwarn-tabs -Werror -O3 -XCPP -XMagicHash -XUnboxedTuples -XUnliftedFFITypes -DTESTING --make check-integer.hs -IGMP GMP/gmp-wrappers.cmm -o check-integer [ 1 of 33] Compiling Simple.GHC.Integer.Type ( Simple/GHC/Integer/Type.hs, Simple/GHC/Integer/Type.o ) [ 2 of 33] Compiling Simple.GHC.Integer ( Simple/GHC/Integer.hs, Simple/GHC/Integer.o ) [ 3 of 33] Compiling Simple.Integer ( Simple/Integer.hs, Simple/Integer.o ) [ 4 of 33] Compiling New1.GHC.Integer.Sign ( New1/GHC/Integer/Sign.hs, New1/GHC/Integer/Sign.o ) [ 5 of 33] Compiling New1.GHC.Integer.Array ( New1/GHC/Integer/Array.hs, New1/GHC/Integer/Array.o ) [ 6 of 33] Compiling New1.GHC.Integer.Prim ( New1/GHC/Integer/Prim.hs, New1/GHC/Integer/Prim.o ) [ 7 of 33] Compiling New1.GHC.Integer.Type ( New1/GHC/Integer/Type.hs, New1/GHC/Integer/Type.o ) [ 8 of 33] Compiling New1.GHC.Integer ( New1/GHC/Integer.hs, New1/GHC/Integer.o ) [ 9 of 33] Compiling New1.Integer ( New1/Integer.hs, New1/Integer.o ) [10 of 33] Compiling New2.GHC.Integer.Sign ( New2/GHC/Integer/Sign.hs, New2/GHC/Integer/Sign.o ) [11 of 33] Compiling New2.GHC.Integer.Array ( New2/GHC/Integer/Array.hs, New2/GHC/Integer/Array.o ) [12 of 33] Compiling New2.GHC.Integer.Prim ( New2/GHC/Integer/Prim.hs, New2/GHC/Integer/Prim.o ) [13 of 33] Compiling New2.GHC.Integer.Type ( New2/GHC/Integer/Type.hs, New2/GHC/Integer/Type.o ) [14 of 33] Compiling New2.GHC.Integer ( New2/GHC/Integer.hs, New2/GHC/Integer.o ) [15 of 33] Compiling New2.Integer ( New2/Integer.hs, New2/Integer.o ) [16 of 33] Compiling GMP.GHC.Integer.GMP.Prim ( GMP/GHC/Integer/GMP/Prim.hs, GMP/GHC/Integer/GMP/Prim.o ) [17 of 33] Compiling GMP.GHC.Integer.Type ( GMP/GHC/Integer/Type.lhs, GMP/GHC/Integer/Type.o ) [18 of 33] Compiling GMP.GHC.Integer ( GMP/GHC/Integer.lhs, GMP/GHC/Integer.o ) [19 of 33] Compiling New3.GHC.Integer.StrictPrim ( New3/GHC/Integer/StrictPrim.hs, New3/GHC/Integer/StrictPrim.o ) [20 of 33] Compiling New3.GHC.Integer.Sign ( New3/GHC/Integer/Sign.hs, New3/GHC/Integer/Sign.o ) [21 of 33] Compiling New3.GHC.Integer.WordArray ( New3/GHC/Integer/WordArray.hs, New3/GHC/Integer/WordArray.o ) [22 of 33] Compiling Check.Helpers ( Check/Helpers.hs, Check/Helpers.o ) [23 of 33] Compiling New3.GHC.Integer.Prim ( New3/GHC/Integer/Prim.hs, New3/GHC/Integer/Prim.o ) [24 of 33] Compiling New3.GHC.Integer.Type ( New3/GHC/Integer/Type.hs, New3/GHC/Integer/Type.o ) [25 of 33] Compiling New3.GHC.Integer.Internals ( New3/GHC/Integer/Internals.hs, New3/GHC/Integer/Internals.o ) [26 of 33] Compiling New3.GHC.Integer ( New3/GHC/Integer.hs, New3/GHC/Integer.o ) [27 of 33] Compiling New3.Integer ( New3/Integer.hs, New3/Integer.o ) [28 of 33] Compiling GMP.Integer ( GMP/Integer.hs, GMP/Integer.o ) [29 of 33] Compiling Check.New3 ( Check/New3.hs, Check/New3.o ) [30 of 33] Compiling Check.New2 ( Check/New2.hs, Check/New2.o ) [31 of 33] Compiling Check.New1 ( Check/New1.hs, Check/New1.o ) [32 of 33] Compiling Check.Existing ( Check/Existing.hs, Check/Existing.o ) [33 of 33] Compiling Main ( check-integer.hs, check-integer.o ) Linking check-integer ... ./check-integer # | tee check.log Comparing GMP and Simple Integer operations: - Can convert from Int. - Can convert to Int. - Can create Integers. - Can add Integers. - Can subtract Integers. - Can multiply Integers. - Can negate Integers. - Can OR two Integers. - Can AND two Integers. - Can XOR two Integers. - Can hash an Integer. - Can shiftL Integers. - Can encode to Double. - Can quotRem Integers. - Can divMod Integers. Testing low level primitives for New1 Integer library: - Can add Words catching the carry. - Can add Words add a carry and catch overflow. - Can multiply Words catching overflow. - Can multiply Words add a carry and catch overflow. Testing low level primitives for New2 Integer library: - Can add Words catching the carry. - Can add Words add a carry and catch overflow. - Can multiply Words catching overflow. - Can multiply Words add a carry and catch overflow. Testing low level primitives for New3 Integer library: - Can add Words catching the carry. - Can add Words add a carry and catch overflow. - Can multiply Words catching overflow. - Can multiply Words add a carry and catch overflow. - Function quotRemWord works. - Function quotRemWord2 works. Comparing GMP and New1 Integer operations: - Can convert from Int. - Can convert to Int. - Can create Integers. - Can create Integers. - Can complement an Integer. - Can negate an Integer. - Can take abs of an Integer. - Can shiftL known Integers. - Can shiftL Integers by up to 256 bits. - Can compare two Integers equal. - Can compare two Integers not equal. - Can compare &gt; two Integers. - Can compare &lt; two Integers. - Can compare &gt;= two Integers. - Can compare &lt;= two Integers. - Can add two Integers. - Can subtract two Integers. - Can AND known pairs of positive Integers. - Can AND two positive Integers. - Can OR two positive Integers. - Can multiply two small Integers. - Can multiply large Integer by small. - Can multiply two Integers (old failures). - Can multiply two Integers. - Tests that have failed once for no good reason. - Can calculate product [1..n]. - Can shiftR known Integers. - Can shiftR Integers by up to 256 bits. - Get correct result at boundaries. - Addition results are minimal. - Muliplication results are minimal. - ShiftL results are minimal. Comparing GMP and New2 Integer operations: - Can convert from Int. - Can convert to Int. - Can create Integers. - Can create Integers. - Can complement an Integer. - Can negate an Integer. - Can take abs of an Integer. - Can shiftL known Integers. - Can shiftL Integers by up to 256 bits. - Can compare two Integers equal. - Can compare two Integers not equal. - Can compare &gt; two Integers. - Can compare &lt; two Integers. - Can compare &gt;= two Integers. - Can compare &lt;= two Integers. - Can add two Integers. - Can subtract two Integers. - Can AND known pairs of positive Integers. - Can AND two positive Integers. - Can OR two positive Integers. - Can multiply two small Integers. - Can multiply large Integer by small. - Can multiply two Integers (old failures). - Can multiply two Integers. - Tests that have failed once for no good reason. - Can calculate product [1..n]. - Can shiftR known Integers. - Can shiftR Integers by up to 256 bits. - Get correct result at boundaries. - Addition results are minimal. - Muliplication results are minimal. - ShiftL results are minimal. Comparing GMP and New3 Integer operations: - Can convert from Int. - Can convert to Int. - Can create Integers. - Can create Integers. - Can complement an Integer. - Can negate an Integer. - Can take abs of an Integer. - Can shiftL known Integers. - Can shiftL Integers by up to 256 bits. - Can compare two Integers equal. - Can compare two Integers not equal. - Can compare &gt; two Integers. - Can compare &lt; two Integers. - Can compare &gt;= two Integers. - Can compare &lt;= two Integers. - Can add two Integers. - Can subtract two Integers. - Can AND known pairs of positive Integers. - Can AND two positive Integers. - Can OR two positive Integers. - Can multiply two small Integers. - Can multiply large Integer by small. - Can multiply two Integers (old failures). - Can multiply two Integers. - Tests that have failed once for no good reason. - Can calculate product [1..n]. - Can shiftR known Integers. - Can shiftR Integers by up to 256 bits. - Get correct result at boundaries. - Addition results are minimal. FAILED [1] - Muliplication results are minimal. - ShiftL results are minimal. - Can encode to Double. - Can encode to Double (QC). - Can decode Double to (Integer, Int). 1) Comparing GMP and New3 Integer operations: Addition results are minimal. Falsifiable (after 5 tests): expected: True but got: False (mkInteger True [0x3,0x0,0x4],mkInteger False [0x3,0x2]) Randomized with seed 1475015401 Finished in 2.1453 seconds 128 examples, 1 failure Makefile:20: recipe for target 'check' failed make: *** [check] Error 1
This is exactly the problem [the `async` library](http://hackage.haskell.org/package/async) solves. The documentation for it even includes an example for parallel URL downloading. Edit: The specific function you want is `mapConcurrently`. If you don't want to wait for all results to be complete to begin processing them, let me know and I can write up a streaming solution for you.
I've heard this claim several times and I'm still *extremely* skeptical that we will ever be able to beat GMP in Haskell any time soon, sorry to say. That code is ruthlessly hand-optimized by experts, down to hand-written assembly, and to this day I have yet to see an explanation of *how* anybody aims to achieve this. I am certainly not an expert on the topic of big integers. But I'd at least like to know *how* and *why* people think this is possible, and I have not seen this addressed. Also, to be clear, these benchmarks do *not* take GMPv6 into account, I'm almost certain, which has a range of significant performance improvements for e.g. Haswell machines and other Intel processors. Details like this matter greatly. That said the preliminary benchmarks here show this as roughly competetive in some workloads, much slower in others, but overall an absolute win over `integer-simple` across the board. Which is great. We can definitely replace it with this it seems, once we're sure it's tested and can go in smoothly.
I assume the OP doesn't want to use `mapConcurrently` because it opens too many connections. See [this SO question](https://stackoverflow.com/questions/18896103/can-haskells-control-concurrent-async-mapconcurrently-have-a-limit). Also, I think a package was announced recently which could help with this, but I don't recall if it's mentioned in those answers.
Other comments point out that phomotopy could be in its own class and that some such classes exists on Hackage. Here's the very thing: http://hackage.haskell.org/package/invariant There's a lattice with four elements: covariant, contravariant, invariant, nonvariant. covariant (+) and contravariant (-) are unrelated. Invariant (+-) is the their meet (assuming meet ~ "both"); nonvariant (0) (aka "haunted", aka constant) their join. As noted elsewhere, that lattice does not assume any indexing (a la GADTs), just parametric type constructors.
So it looks like the speed is comparable for smaller numbers, but `gmp` pulls away for very large (2500 digit) numbers. That seems good enough for everything except solving project Euler problems.
Thanks for all this great info. I don't really expect cabal to do my copying for me. In my case it is going to several remote servers via scp, so I have a custom script for that. I just expect cabal to put everything in a predictable place. 'cabal install' is doing that ok for me (and stripping). 
I found Andres' talk Datatype-Generic Programming in Haskell (https://skillsmatter.com/skillscasts/3932-a-haskell-lecture-with-leading-expert-andres-loh) to be a good introduction to the types of things you can do with generics, including how automatic derivation of certain type classes is achieved.
You might find [this classical paper](http://www.hpl.hp.com/techreports/Compaq-DEC/SRC-RR-47.pdf) approachable. 
With a few primops for mulx adcx adox I think it should be fairly straight forward to write this in a high level style. The new instructions make it easier, not harder.
The keywords you are looking for are "deforestation" and "fusion." The former is the more general word, but "fusion" is pretty hip these days.
It has been there as a stub as long as I can remember and I figured out I would give it a shot.
Haskell must be the only language where a paper counts as "I'm not a PhD" answer.
Don't know if OP does, but after reading the first two paragraphs, I'm sure that *I* will enjoy it. Thanks!
If the class of functions is as simple as you have described, then this is a very, very special case of deforestation. A case in which the transformation always succeeds (which cannot be said of various "deforestation" or "fusion" algorithms in general). The class of functions is called "top-down tree transducers", and an algorithm to compose any two such devices into a single one was given almost 45 years ago by W.C. Rounds. For a more modern perspective on that specific technique, motivated specifically by functional program optimization, you might want to read http://dblp.uni-trier.de/rec/bibtex/conf/flops/Kuhnemann99. After that, the technique was generalized to broader classes of programs/tree-transducers, in, for example: * http://www.iai.uni-bonn.de/~jv/KV01.html * http://www.iai.uni-bonn.de/~jv/VK04.html (And if you are really brave, you could read my PhD thesis "Tree Transducer Composition as Program Transformation", http://www.iai.uni-bonn.de/~jv/Voi05.html) 
From the github Readme.md: &gt; This is still very much a work in progress. Patches and pull requests more than welcome. 
It's a work in progress. I'll have a look at that.
This is probably because for both the experimental package and `gmp` smaller numbers are just unboxed `Int#` or `Word#`, see [here](https://github.com/ghc/packages-integer-gmp/blob/master/GHC/Integer/Type.lhs#L109) and [here](https://github.com/erikd/haskell-big-integer-experiment/blob/master/New3/GHC/Integer/Type.hs#L18).
I'm not sure it counts tbh. 
I think that it does, and I like the answer. You don't need a PhD or even any academical background at all to read papers if you have a good approach.
`Typeable` is quite simple. Effectively, it could be written as class Typeable a where typeOf :: a -&gt; TypeRep where `TypeRep` is some object representing a type. You can think of it as a String to keep things simple. So `typeOf 1 == TypeRep "Int"`, `typeOf [1,2,3] == TypeRep "[Int]"`, `typeOf "abc" == TypeRep "[Char]"` etc. What this allows you to do is make casts safe. The function `Data.Typeable.cast` implements this safe cast by comparing the `typeOf`, and returning Nothing if there is no match. cast :: (Typeable a, Typeable b) =&gt; a -&gt; b cast x | typeOf x == typeOf (undefined :: b) = Just (unsafeCast x) | otherwise = Nothing Now `Data.Dynamic` is simply a value of any type, together with its TypeRep. This allows you to (effectively) use `cast` to get back its original type. If you understand existential types, then it is `exists a. Typeable a =&gt; a`.
Huh. I actually think that cabal reply is awesome as is. Thank you for a great tool!
I've found that Haskell papers are often like a very good (little) book or an article. They are very well-written, have great explanationary intro, but are still quite dense.
Nice trick!
Generally it is hard to write good examples for uses of continuation passing style because most such examples can be implemented using a narrower type. For example, basically every coroutine example can be implemented using free monad transformers (see issue 19 of The Mobad Reader to learn more about this), which are easier to reason about (and have simpler types) because they are not as powerful as ContT.
Yeah, I know. But this is useful to get a feeling of how continuations work, and that's what examples are for. I definitely understand continuations better after doing this.
The Ubuntu release and Easter both intervened sequentially so I got a bit behind. Replied now, and I appreciate the sentiments :-)
... Are you even paying attention to what *GMP actually does?* New instructions are such a small part of it by themselves, that's not the point. If that's what you think *actually* matters and what I was addressing, well, I really don't know what to say. Do not tell me it is "fairly straight forward" to my face when your 'solution' is completely worthless for me and a vast majority of other users. It's misleading at best and a bald-faced lie at worst, and ignorant of the actual use cases such a project will see. Let's not forget GMPv6 for example didn't *just* improve Haswell across the board. I just picked that off the top of my head for an example. Realistically Haswell is a *fucking terrible* example to point out because practically nobody has Haswell machines. It's something like the 1% vs the 99%. What Haswell can do is irrelevant to processors from 6 years ago, and they still matter. I honestly could not care less about those ADX instructions, they are worthless to me and 99% of people. GMPv6 also improved Core 2 users and Nehalem users, which do not have such advanced instruction sets. It also improved for SPARC users, and ARM users across the board. It also, from a non-performance-aspect, has begun offering carefully timed side-channel-resistant operations for cryptography, which offer extremely high levels of speed still. And we're beginning to utilize these features - so that last point isn't just randomly picked. What are your plans for winning there with the instruction sets available, and totally different hardware/bandwidth constraints? On a minumum of 4 platforms (PowerPC is needed) - each with almost *half a dozen microarchitectures* - and two separate APIs (fastest vs side-channel-resistant), by the way. Because that's what you're gonna need to do if you want this to be acceptable as an alternative. This is the part where this battle literally becomes *unwinnable*. You cannot win unless you cherry-pick the playing grounds so far it becomes practically pointless. And if you do that, **that is totally fair** - but say that, and *do not claim you are beating GMP*. GMP is not one algorithm on one processor. It is totally misleading at best and flat-out wrong in general. Yes, it is probably easy for me to fabricate Haskell using some random CPU features on one microarchitecture, that can beat GMP in one micro benchmark on one machine I have. That does not mean Haskell can, or will, beat GMP - ever - in the large, for the things it is targeting. So people need to stop claiming that, or they need to *adjust their claims to match reality*. It's one of the two. New instructions barely have anything to do with this.
Yeah, exactly, and there are plenty of other situations like this. So now instead of having to actually beat hand-written assembly, we only now have to beat machine-generated C code which has been automatically optimized by suites of tools to likely achieve high ILP and good instruction bandwidth/use based on a suite of various input parameters, and let's not forget this C code was *already* hand-optimized by experts, compiled by one of the best compilers in the world - it just *got even better than it already was*. That *really* makes me feel a lot better and more hopeful of our chances.
Daniel is fucking awesome no doubt, and if anyone can do it, I have no doubt it is him. I just want to put this on the record. And to be clear to everyone, this isn't a question of Daniel's skills - it's just that I've heard this claim *enough times already* I'm going to start demanding justification as to why when people bring it up. If Daniel could pop in here, that'd be epic.
Yep, this is definitely a problem. But this one can be fixed by some reworking of the integer-gmp internals, and Herbert made some optimizations this release cycle already in some spots for example (memory/primop related). Improving `integer-gmp` itself is not of out reach, and someone could do some good stuff here. The effort required to actually *beat GMP in Haskell* is, on the other hand, likely in the ballpark of a million times greater.
Thank you! I reworked the first half of that Wikibook chapter a while ago, trying to make it more accessible, but I didn't really try my hand at improving the examples back then.
That, right here, is why I love this community and language so much. I will read those carefully on the next hours. While I am doing that, @jvoigtlaender, as you have great knowledge on the topic, if not abuse, can I ask your advice on a particular problem? This will be kinda long so no problems if you can't answer, but we might negotiate a consultancy if you will. I am looking for a programming language that has those characteristics: 1. is not turing complete; 2. can be 100% optimised - that is, I want the ability to fuse any kind of recursive function; 3. is generic enough that you can express any - or, at least, as many as possible - practical algorithms; 4. is simple enough that an interpreter would be a few lines of Haskell and I can work on it myself. I want that for a project that is an online encyclopedia of code, for which I've been chosen to get some funds in a government contest, so this is really important. Haskell is obviously not what I need as the language is kinda complex, is turing complete and so on; I really need a language that I can understand fully and implement an interpreter/compiler for it with ease. Idris, Agda, CoQ and similars are interesting as they are the only languages I know where you can prove termination, but those are obviously far from simple. Core dependently typed systems are kinda simple, but the whole separation of two levels - types and values - make for a lot of duplicated code on the implementation (and are in general very hard to understand). So after a lot of thinking, I ended up on the simply typed lambda calculus with restricted recursion, pattern matching and only one base type, that one I mentioned. I believe I can encode Nats, Ints, Lists and so on using that type, and optimise them for native ints/lists, getting fast enough programs. I also believe I can encode pretty much any practical algorithm on it. I don't need IO, infinite threads, nothing of that - it is an encyclopedia of functions, not fully featured programs. My question is: now that you see the problem I'm facing and given your knowledge, what do you think of all this, and what path would you suggest me to follow? Edit: just wanted to say your thesis is looking very readable and spot-in. I'm choosing it over the papers as it is more complete. Thank you again.
&gt; which are easier to reason about (and have simpler types) and are inefficient, see [Asymptotic Improvement of Computations over Free Monads](http://www.iai.uni-bonn.de/~jv/mpc08.pdf). The solution is exactly CPS. Coincidentally, I wrote to Michael about this just today; so I'll ask you, too: why does `pipes` ignore that issue?
I [asked](http://www.haskell.org/pipermail/haskell-cafe/2014-April/113628.html) the same question on cafe recently, and got no answer.
Also, to answer your first point, any monad can be transformed using codensity (i.e. a CPS transformation). However, the continuation monad is more powerful than the codensity monad, so I wouldn't expect a CPS transformation alone to provide insight into the full significance of the continuation monad in general, in the same way that I wouldn't expect the `IO` monad to provide full insight into the significance of monads in general.
It's still pretty confusing .. are there better names than x, m, and r we could give in the newtype definition ? Or is Cont just so general that really x, m, and r could be literally anything. newtype ContT x m r = ContT { (&gt;&gt;-) :: (r -&gt; m x) -&gt; m x } 
Read carefully: I am talking about the functions without underscores.
Ah, you caught me :) I was a bit hasty. I was probably thinking of ways to optimize summing lists using parallelism, and knowing that the operation is associative allows for stronger parallelism strategies than ffor a non-associative operator.
&gt; If you understand existential types, then it is `exists a. Typeable a =&gt; a`. A value of type `exists a. Typeable a =&gt; a` doesn't make much sense -- it gives you some arbitrary type `a` that you know nothing about, but then asks you for a `Typeable` dictionary for it. There's no syntax for bundling a dictionary with a value, since GHC doesn't have first-class existentials in the first place, but a common pseudo-syntax in #haskell is `*&gt;`: `exists a. Typeable a *&gt; a`. (In fact, you could even define a data type that means the same thing with ConstraintKinds: `data k *&gt; a = k =&gt; T a`.)
Sigh. My point is exactly that underscores don't matter here at all. It's about the effects, not the computed value. I wrote a small benchmark to demonstrate the problem. (I'm a bit surprised that it manifests itself even on normal, right-associative lists though.) import Pipes import qualified Pipes.Prelude as P import Data.Foldable import Control.Monad.Identity import Control.Monad.Codensity import System.Environment l :: [Int] l = [1..20000] main = do args &lt;- getArgs case args of ["pipes"] -&gt; print $ (runIdentity . P.sum . traverse_ yield) l ["cps"] -&gt; print $ (runIdentity . P.sum . lowerCodensity . traverse_ (lift . yield)) l The cps version completes instantaneously; the non-cps version takes 5 seconds. Would you say that `traverse_ yield` is the wrong way to use pipes?
Glad to see GHCJS getting some attention. Lock free hash map looks exciting too. Good luck to everyone working on these projects.
I believe that this GitHub issue comment thread is applicable: https://github.com/Gabriel439/Haskell-Pipes-Library/issues/100#issuecomment-27433052 Baking Codensity into Pipes/Conduit incurs unavoidable constant factor slowdowns, so it is preferred to apply the codensity transformation manually when needed, just as you demonstrated here.
I found the problem. This is because `pipes` does not define an optimized `(*&gt;)` (which `traverse_` uses internally), so the default implementation (`liftA2 (const id)`) produces left-associated binds when used in conjunction with `traverse_`. It's easy to prove that this is the case: just replace `traverse_ yield` with `mapM_ yield` and now `pipes` is 5x as fast as the CPS version (using a new upper bound of `20000000`): $ !time time ./test3 pipes 200000010000000 real 0m0.345s user 0m0.340s sys 0m0.000s $ time ./test3 cps 200000010000000 real 0m1.510s user 0m1.352s sys 0m0.152s For comparison, here are the implementations of `traverse_` and `mapM_` side-by-side so that you can see that they only differ in the constraint: mapM_ f = foldr ((&gt;&gt;) . f) (return ()) traverse_ f = foldr ((*&gt;) . f) (pure ()) Also note that `pipes` already has a combinator for traversing `Foldable` containers, which is `each`: each :: (Monad m, Foldable f) =&gt; f a -&gt; Producer a m () So you can also write your program as: print $ (runIdentity . P.sum . each) l ... and that also runs in the same amount of time. So the only reason your example ran slower is because it (unintentionally) triggered deeply nested left-associative bind. Thanks for catching this and I've created [an issue](https://github.com/Gabriel439/Haskell-Pipes-Library/issues/116) to fix this.
Last week Nathan Smith (/u/name510) [posted an implementation](http://www.reddit.com/r/golang/comments/23aht5/i_wrote_a_go_program_for_solving_doublets_aka/) of [Word Ladders](http://en.wikipedia.org/wiki/Word_ladder) to /r/golang. After reading the program I realized it was simple enough I could try to recreate it in Haskell for my first program. So after about 5.5 hours (and a little [blog post](http://www.foobarsoft.com/2014/04/wordladders-in-haskell/) ) I'm pretty happy with the result. It was actually easier than I thought it would be. I'm glad I've been following this sub for a while and reading some of the programs people posted. But I have a couple of style / 'why' questions that I developed while creating it. Most every function I wrote has a where statement on it, which makes me thing I'm overusing them. They did make the code relatively easy to read, so I could see myself being wrong. I remember doing this (or the alternate, lots of 'let's) when doing Clojure too. I felt like I was abusing the guard syntax (using a pipe character in a function definition). At times I was writing relatively complex conditions, but it looks like those all got fixed so they look relatively simple now. The runWithTwoWords function looks like I should be able to clean it up somehow. The fact that I ended up with a where clause on each definition (to shorten the guards, it's all related) makes it look more complicated than it should. Are there any other issues or tips you guys can give me? Just in writing this comment I figured out two issues that I hadn't solved to my satisfaction. One involved function composition not working right (I needed a $ so the parameters were given to the right function) and one an odd IO issue (I needed to flush stdout, but when I did it before I accidentally types stdin so compiler complained).
deriving (Generic), and using gshow http://hackage.haskell.org/package/generic-deriving-1.6.3/docs/Generics-Deriving-Show.html as the basis for your show? Or would that be a bad idea due to gshow and show then giving different results?
I'm still missing how this would work. Could you explain further?
Note that I haven't tested this code Relies on the generic-deriving package {-# LANGUAGE DeriveGeneric #-} data Foo = ... deriving (Generic) instance GShow Foo instance Show Foo where show (Twentieth str _) = "Twentieth " ++ show str show x = gshow x
 graph &lt;- timeAction "Created the graph in " . liftM createWordGraph $ return dict -- This 'return' feels odd to me Yea, that `return`/`liftM` is pretty odd. Why not do the following: graph &lt;- timeAction "Created the graph in " . return $ createWordGraph dict The return is only necessary because you're trying to interleave this pure action between two IO actions (to time it).
That makes sense. Since my timing function needed an IO monad parameter, my thought was to lift my createWordGraph function into the IO monad, but that meant passing it an IO WordDictionary. Running the createWordGraph function pure and then lifting it's *result* seems much simpler and cleaner.
Yeah, I think the concurrent hash map would be of the most direct benefit to myself. Hopefully it'll also have an STM interface to it as well. We'll see. :)
Why on earth is darcs getting so much attention? Shouldn't everyone be moving to git by now? Darcs is inefficient, cumbersome and strange from the perspective of someone who is used to the 'scalpel' like control git offers. It's frustrating to me to see work that is essentially re-inventing the wheel.
Why on earth is agda getting so much attention? Shouldn't everyone be moving to ghcjs by now? Agda is web-unaware, constraining, and strange from the perspective of someone who is used to the 'swiss-army-knife' like plethora of gui libraries js offers. It's frustrating to me to see work that is essentially re-inventing the anvil.
I don't see your point. Agda is different enough to be interesting. xmonad's repo is 18MB with darcs, 876kb with git. Git is much faster, and more scalable. Git offers lightweight branches which offer EVERYTHING that the "multiple head" idea that darcs has, except it makes it harder to shoot yourself in the foot. Darcs is a step in between hg and svn. Go ahead, name some examples to prove me wrong and I'll address them.
I think you'd also need to code up special cases for all the recursive pieces: instance Show Foo where show (Twentieth str _) = "Twentieth " ++ show str show (Sixth fooA fooB) = "Sixth (" ++ show fooA ++ ") (" ++ show fooB ++ ")" show (Seventh foo str) = "Seventh (" ++ show foo ++ ") " ++ show str ... show x = gshow x This seems equivalent to /u/freyrs3's idea. 
Yeah, OK, manipulating primitives directly within main is pretty ugly (especially when the hard parts are stuck in a lambda in the middle), but you can build your own lightweight combinators to make the code easier to read, easier to test, and easier to write. This is the same principle in any language. download url = randomRIO (10000,1000000) &gt;&gt;= threadDelay &gt;&gt; putStrLn url urls = map show [1..25] untilEmpty chan action = fix $ \next -&gt; do empty &lt;- atomically (isEmptyTchan chan) unless empty $ do value &lt;- atomically (readTChan chan) action value next distribute n m = replicateM n $ async m waitAll = mapM_ wait chanOf :: [a] -&gt; IO (TChan a) chanOf list = do chan &lt;- newTChanIO mapM_ (atomically . writeTChan chan) list return chan main = do urlCh &lt;- chanOf urls tasks &lt;- distribute 10 $ untilEmpty urlCh $ download url waitAll tasks 
Huh. Today I learned. Well, I'm glad there is a reason. From the 2 weeks of usage I had assumed there were minimal to no differences. Thank you for the insight.
See https://github.com/haskell/cabal/issues/1177.
I'm looking forward to "Flesh out features of Hackage 2". All the listed features sound great, but I especially hope adjustable version bounds get some attention. It probably won't resolve the upper version bounds debate, but might make it less sharp.
You're welcome. And yes, as it turns out, that example doesn't demonstrate my point well, since the reason of the left-association was different. In fact, I now see that it'd be hard to demonstrate this with `Foldable` (assuming the instance defined through `foldMap`, not `foldr`), since it essentially does re-association for you. Here's an example of directly traversing a left-associative list, which also triggers the quadratic behaviour. l :: [Int] l = [1..20000] revListIter [] = return () revListIter (x:r) = revListIter r &gt;&gt; yield x main = print $ (runIdentity . P.sum . revListIter) $ l It's analogous to an "underscored" function in that it doesn't accumulate any values in its return result. Yet it creates a left-associated bind chain. Again, in real world, one may be traversing tree-like structures, where it's not obvious how they will be associated. Anyway, I now see that you're trading complexity for a constant factor. I just wanted to explain that bad behavior does not necessarily result from functions like `mapM`.
After years of lone contemplation on a mountain top, I've finally unlocked the secret of the monad and its ilk. However, til this day I haven't mastered the beast that is iterating over the indexes [0, size) using the array package. I am, however, glad that the array package allows me to use the alien civilization of Khaa's array 127i-based indexing convention, should I ever need it. ;) 
Good read, thanks!
Not yet. I think we'll set up a repository under the GHCJS organisation on Github ( http://github.com/ghcjs ), and of course I'll encourage the students I'm mentoring to blog about their progress.
camp? http://projects.haskell.org/camp/
I was under the impression that GMP had been wrapped by ghc to take advantage of its memory allocator. In any case if malloc fails something is probably very wrong and you don't want to just go on.
Perhaps you'll find this useful: https://github.com/adaszko/concurrent-ordered-set I plan to extend this from sets to maps if there's any sort of demand for such things.
Darcs offers a fundamentally different way to work with version control, which is worth exploring. Also git is frustrating to work with for anybody who has not spend hours upon hours of learning the horrible user interface. Just because some find it frustrating does not mean it is bad. Same with darcs.
Me too! I'm pleased it's been accepted. And yes, I think being able to adjust the version constraints should make a difference, both in the number of packages that just work, and in the debate on upper bounds.
&gt; Most every function I wrote has a where statement on it, which makes me thing I'm overusing them. They did make the code relatively easy to read, so I could see myself being wrong. I remember doing this (or the alternate, lots of 'let's) when doing Clojure too. Sounds perfectly normal to me, in fact the only things I don't write in 'where' clauses are 1) exported functions 2) functions used by more than 1 function in a module. I would recommend against your use of tabs, though as it causes your where clauses to march to the right quite a bit and the way layout interacts with tabs can be rather confusing at times. It looks very readable at first glance, though.
You're welcome!
And interestingly, many Darcs users have had a tough time switching to/learning Git. 
On points of style: - write `null neighborWords` instead of `neighborWords == []` - write `isNothing existingNode` instead of `existingNode == Nothing` These functions work even when the underlying types can't be compared. `updateOpenSet` has a misspelt `curretnCost` parameter which appears to be unused.
You can just use *Data.List.Split.chunksOf* to download the pages *N* pages by *N* pages, where you'll only run *N* async computations at a given moment.
What "underlying types" are you talking about?
`existingNode :: Maybe Node` — the underlying type is `Node`. `neighborWords :: [String]` — the underlying type is `String`. There is probably better terminology for this, but I can't think what it is.
Then your objection doesn't make sense. The "underlying type" of both `[]` and `Nothing` is `forall a. a`, so it can always be compare. `null` an `isNothing` are better stylistically, but that's it.
This is already possible: {-# LANGUAGE PatternSynonyms, ViewPatterns #-} import Data.Sequence pattern ViewL a b &lt;- (viewl -&gt; a :&lt; b)
In `runWithTwoWords`, why don't you just inline the where clauses? `length one` is just as readable as `lenOne`, so put the first one in the guard directly. You can do the same thing with the other two cases, only I would call them infix, as in `one ` \``Set.notMember`\` `dict`: it reads almost like English, if you ignore the `Set` there in the middle. So in `runWithTwoWords` you don't need where clauses. The other uses seem legit.
Ooh, I see. Fine then.
I'm looking forward to the concurrent / lock free data structures. [https://ghc.haskell.org/trac/summer-of-code/ticket/1617](https://ghc.haskell.org/trac/summer-of-code/ticket/1617)
I don't know what you mean by &gt; repeating the call to an mtt `N` times on a list (tree) (what's an "mtt"?), but the reason why quicksort is a good example of an algorithm which probably cannot be implemented using your scheme is that quicksort's recursive calls are on two halves of a transformed list, not on the tail of the input list. When the only recursive calls allowed are on a subterm of the input, termination is guaranteed because the input is finite and each recursive call is making it strictly smaller. If you allow recursive calls on the result of another function, such as the function which partitions quicksort's input list, then termination is not so obvious because you don't know whether the helper function is returning a value which is bigger or smaller than the input. In the case of quicksort, Agda has mechanisms which allows the implementation of `partition` to prove that it's returning lists which are smaller than its input, and those mechanisms are more complicated than just checking that recursive calls are only ever made on a subterm of the input.
The second and third parameters are the same as any monad transformer--by convention, `m`is used for type variables expected to be monads, and the final parameter is the usual parametric component like any functor, which can be any type. The only `ContT`-specific variable is the called `x` here, which is the final result type of running the `ContT` expression and can also be any type whatsoever. So yes, it really is that general. The more traditional type parameter names would be `ContT r m a` which I find more mnemonic (`r` for result, `m` for monad, and `a`following the convention of arbitrary type parameters being taken from the beginning of the alphabet).
In the case of your example, you can easily fix it by using `reverse` followed by `each`: print $ runIdentity $ P.sum $ each (reverse l) Yes, you can write non-performant code with any library if you try really hard. What's important is how easy it is to write performant code and how much the library steers you towards the correct solution. With the exception of `traverse_` (which I consider a fixable performance bug in `pipes`), most `pipes` code is highly efficient. Moreover, the library deliberately steers you away from using `mapM` and friends because it provides improved solutions (in the form of `Pipes.Prelude.{mapM/replicateM/sequence}`). There's another thing that I want to clarify, which is that the CPS transformation is not free. As another comment mentioned, it incurs significant constant factor overheads and it greatly complicates the internal implementation. One of the primary goals behind the `pipes` library is to be a teaching library, which requires understandable source code, and that is why I'm very reluctant to complicate the internals.
We serve as an umbrella organization over darcs. They get a separate allocation of slots and we then let them compete for more with all the other projects in the Haskell ecosystem. There have been many productive "big roundtable" sessions between the darcs folks and the maintainers of other revision control systems as part of the GSoC mentor summit. Ultimately, darcs' patch theory at least tries to put version control on steady theoretical ground, and it serves as an incubator for ideas, much like Haskell itself.
Err.. join is a method on Monad. Not sure what you mean.
According to GHC 7.8: &gt; 'join' is not a (visible) member of class 'Monad'
This change is not in GHC 7.8 and will not be merged due to compatibility considerations. Edit: As I see the same patch of Austin Seipp both adds join as an instance and Applicative as a superclass
Looks like http://ghc.haskell.org is not responding.
It made more sense originally because those were longer and there were quite a few of them. During refactoring, things ended up much simpler but I just left them as where clauses while I was moving things around so I wouldn't end up breaking even more. You're right I could clean those. Infix is a good idea too.
To clarify I was talking about reimplementing gmp for use by ghc. Yes I think optimizing for pre-Haswell x86 is a "waste of time" (relatively). Similar with SPARC and the half dozen other microarchitectures gmp apparently support. ARM w/NEON *is* important. For example, I think 5% better performance on Haswell or ARM has a benefit similar to supporting all non-{x86,arm) architectures. I also think the simplicity of a Haskell-only implementation is worth trading performance for. I even think dropping support for niche micro-architectures can be a net win. Side channel resistance *is* important and needs to be exposed at the Haskell level. But since side channel resistance isn't a composable property, and we need specific functions for things like elliptic curve scalar multiplication, gmp is only a part of the solution, and might not even be the most important one. We have seen again and again lately how relying on high performance software written in C is dangerous for security. Although gmp might be well tested, long term, lifting as much as possible up in a higher level language is the right thing to do. 
[doc for users](https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields/Design)
&gt; In the case of your example, you can easily fix it by using reverse followed by each: You totally missed my point. Again. Anyway, I got answers to my questions, and I've already spent too much time on this discussion, so I'm done here. I may write a post warning potential (and existing) users of your library, since I don't think this issue is widely known.
This entire discussion gives me the impression you are arguing in bad faith since you dismiss every point I make.
No it is not. I put it on a branch this morning for review/integration testing. It will hopefully be available in HEAD sometime in the next week or two, however, once Simon can go over it.
Everyone's so excited and the server can't keep up! 
Also, I'll take this space to advertise that my colleague Adam - who implemented this extension - will be giving a talk about it in London on the 28th, at Skills Matter: https://skillsmatter.com/meetups/6345-overloaded-record-fields-for-haskell
No, it did not. I put it on a branch this morning. It's not ready to go into HEAD, and that branch does not even build yet - because some minor changes to libraries need to happen. Furthermore, the patch needs to be reviewed by the Core Libraries Committee. Barring those things it will hopefully be done this week.
Join is now directly part of `Monad` with this change, along with a default implementation - that was part of the proposal all along.
The server is under stress right now because Trac is going bonkers for some reason. I'm investigating now, Haskell.org infrastructure is aware of the problem.
This is not going into GHC 7.8. It will only be available in GHC 7.10.
Impatient folks can look at the commit right now at: https://github.com/ghc/ghc/commit/88c9403264950326e39a05f262bbbb069cf12977
I would avoid making `Node` an instance of `Eq` and `Ord`, and use explicit functions instead. I would replace insert node with insertBy (compare `on` fullCost) node Similarly, I would replace (node ==) with ((==) `on` word $ node) and (node /=) with ((/=) `on` word $ node) [`insertBy`](http://www.haskell.org/ghc/docs/latest/html/libraries/base-4.7.0.0/Data-List.html#v:insertBy) is in Data.List; [`on`](http://www.haskell.org/ghc/docs/latest/html/libraries/base-4.7.0.0/Data-Function.html#v:on) is in Data.Function.
That's a really nice and simple explanation! Now next questions is when should we write our own typeable instance instead of deriving it? Is TypeRep internally stored as String? is there any performance penalty for using it?
Does GHC 7.8.2 play nice with clang on OS X 10.9 yet? -- I had hoped that all the preprocessor issues would be fixed by now but this thread brings on a sense of deja vu: https://github.com/haskell/cabal/issues/1740 
That would only work if all the downloads took the same amount of time, otherwise there would always be fewer than 10 active.
 data T = N | L T | C T T nat N = N nat (L a) = L (nat a) This doesn't do what you think, though. What it does is create a function that will throw an error at runtime if you pass it a tree containing any C constructors, and is otherwise the identity. There's no way for the typechecker to do anything with this. You can lift your data to the type level: newtype Void = Void Void -- uninhabited type newtype TN = TN Void -- still uninhabited -- here we use phantom types for the parameters newtype TL a = TL Void newtype TC a b = TC Void -- useful type for telling the typechecker about our types -- even though they are uninhabited data Proxy a = Proxy -- example: class Nat a where natInteger :: Proxy a -&gt; Integer instance Nat TN where natInteger _ = 0 pred :: Proxy (TL n) -&gt; Proxy n pred _ = Proxy instance Nat n =&gt; Nat (TL n) where natInteger x = 1 + natInteger (pred x) example1 :: Proxy (TL (TL TZ)) example1 = Proxy test1 = natInteger example1 -- test1 = 2 example2 :: Proxy (TC (TL TZ) TZ) example2 = Proxy test = natInteger example2 -- type error, no instance Nat (TC ...)
Darcs is drastically different, in fact. This is not the only way. Consider how easy it is to cherrypick with Darcs, in such a way as to not mess with the identity of the patch!
I have problems imagining what call you have in mind for quicksort on a list. Can you give a concrete "start expression" for quicksort on a list, with some functions from a macro tree transducer? Generally, the reason why I think quicksort cannot be done is what gelisam points out in the comment below.
on #haskell, I wrote: &gt; please don’t hate on me. i’m trying to implement “objects” with subtype-polymorphism. my approach is that I have a ‘data Dyn = forall t . Foo t =&gt; Dyn t` that models subtyping &gt; ecause i want to figure out haskell and learn something about subtyping on the way &gt; i’m using typeclasses as interfaces and suspect that they’re not really what i need here. the question is: what is? Thank you so much for your time!
The main hoogle only searches the haskell platform by default. It does index everything though, and you can make it search `hxt`, for example, by adding `+hxt` to your search query. FP Complete has their own instance of hoogle, which searches a larges base of packages (which includes hxt).
Wow. Using unsafePerformIO inside an Ord instance so that sorting becomes interactive. That's... a new one.
I am new to haskell, so good to know this, thanks.
&gt; In a way I want to make it a triple equals sign ≡ Please do. I am used to that from HaskellConceal
It is either madness or genius. Probably a bit of both.
Probably just madness ... I noticed that you actually can't kill the process when you invoke it from within ghci (`:main`) ... But the sorting procedure worked quite well ;-)
Seriously? That's great news.
I think ["Soonish"](https://twitter.com/icfp_conference/status/384052667460247552) is all we know?
I propose we name this 'dysfunctional programming'.
This is beautifully ugly code.
So you don't just want subtyping, but specifically dynamic typing (that is, downcasting), correct?
Ok, challenge accepted. If we're just using the Prelude then it can be defined pretty simply by writing a monadic version of the sortBy in about 35 lines. Could code golf it down a bit if you wanted. http://lpaste.net/103092
&gt; I'm surprised I didn't got any reply from the community to this job offer I'm... skeptical that there are many PhDs with 5+ years experience in distributed computing research in/willing to relocate to Italy reading /r/haskell. Maybe better luck looking for those people and then just teaching them haskell? Sounds neat though.
so am I, but it changes the character spacing, so vertical alignment gets messed up, I will see if I can switch to this.
Woah, relocating to Italy is probably enough to be a showstopper :) 
Software from KDE exists for OSX as well as Windows. Haven't seen any of those OSs recently, so I can not say whether the applications are stable. Anyhow, for those willing to try, http://community.kde.org/Mac#Installing_KDE_software_on_Mac_OSX
&gt; Now next questions is when should we write our own typeable instance instead of deriving it? Never, GHC 7.8 prevents you from doing so.
That's the only power operator defined for `Num`. (^) :: (Num a, Integral b) =&gt; a -&gt; b -&gt; a (**) :: Floating a =&gt; a -&gt; a -&gt; a (^^) :: (Integral b, Fractional a) =&gt; a -&gt; b -&gt; a
 try readLn :: IO (Either SomeException Int) instead of fmap readMaybe getLine :: IO (Maybe Int) is also pretty golden :-)
You might find the paper "[Haskell's overlooked object system](http://arxiv.org/abs/cs/0509027)" useful :)
Could the error reporting be improved by adding the metadata to a generic intermediate form, or is there something intrinsic to the problem that would prevent this approach?
I'm assuming the idea is to build something around the compare-and-swap instruction via the `atomic-primops` package so I wouldn't expect an STM interface to be in the cards.
Once this goes live, should we just use 'pure' instead of 'return'? Would 'return' be considered just for compatibilty whereas 'pure' being the better choice?
What happens to pattern match failures in do notation? Do they just error instead of allowing the type to decide how to handle failure? (That is what fail is used for right?)
Just had a quick peek at the ALU code, and it looks quite clear, well done.
You get a `MonadFail` constraint for do blocks with incomplete patterns (or not-syntactically-obviously-complete patterns or whatever).
Thanks!
Thank you, exciting news in the morning, just the three letters "bos" are enough.
Sadly, that breaks much more user code than the AMP. It should still happen. And, we should get it into a revision of the standard.
This is a pretty big limitation of the main Hoogle. If we knew were the function was we wouldn't be searching
I've been thinking about the "existential anti-pattern" and GHC 7.8. With typeable for typeclasses, it should be possible to build actual dictionaries of typeclass instances, and thus get a much nicer "COM syle" object system.
Edward has hinted to me in the past something like this might happen, if I remember correctly*. So stay tuned and maybe your hopes will come true. * Warning: I am not Ed or the Core Libraries Committee's representative. Caveat Emptor. Do not place bets. Do not hinge decisions or parties on it. Do not taunt Happy Fun Ball.
Yeah, I always feel conflicted about how to name the last type parameter for any monad transformer. `transformers` uses the convention of `a` for that parameter, but I really like `r` (for return value). However, for `ContT` it does make a lot of sense for the other parameter to be `r`.
&gt; Ideally, I'd be getting something like `Dyn Foo` instead. With `ConstraintKinds` (and `FlexibleInstances`), you can! data Dyn interface = forall rtty . (interface rtty, Show rtty) =&gt; Dyn rtty instance Foo (Dyn Foo) where (Dyn this) `doFoo` () = case this `doFoo` () of (updated, ()) -&gt; (Dyn updated, ()) &gt; How can I avoid having to implement `instance Foo Dyn` myself? How could I generalise? Maybe using Template Haskell?
I use darcs for my day job, and for the last 5 years it has done everything I have needed except get supported by github. darcs is stupidly easy to use. Given darcs vs git, I would pick darcs no problem. The problem is that it is darcs vs github.
&gt; Simple but powerful lens-based API :(
I have a number of libraries in-development that talk to HTTP endpoints using JSON, and while `http-client` works great it always sucks having to wrap something around it for JSON. A tad bit too low-level. This looks excellent, thanks Bryan.
template haskell can do this.
See: [this thread.](http://www.reddit.com/r/haskell/comments/1kpu1h/can_haskell_programs_be_compiled_such_that_the/) AFAIK, there's no way to have "solutions" in the binary produced by GHC. If you were printing a particular solution, you could have it done at compile time with template-haskell. This is a job for supercompilation, which is active research.
Can TH memorize the data-structure at compile time?
`readMaybe` is relatively new, I think. I keep forgetting it exists now so maybe OP did too.
Large dependency list, for one.
This looks like an excellent and easy to use library. Thank you for code examples.
does it do conditional requests and respect cache controls?
I don't mind as much, but couldn't the use of lens be factored out of the library?
While a lot of that code is pretty self explanatory (as Haskell tends to be), some of it is pretty dense and could benefit from some comments. I understand it was for a dissertation as well, so the code just has to make sense to you and your professors, but it would make it easier for me to read =P
This could be solved by using `lens-family-core` instead of `lens`
That tutorial was fantastic; I really appreciated the effort.
Thank you!!! Haskell's curl and shpider libraries were broken.
I see no reason to switch to pure. Even in the cases I wrote Applicative instances for something which was a Monad, I rarely used `pure` outside of Applicative context anyway. This isn't so much a question of which is better as it is style. In my case, I prefer `return` since it's closer to what people have traditionally expected and seen themselves in the context of `Monad`. That said, feel free to use `pure` where you would use `return` if you want.
My problem is that it's adds a lot of prerequisite knowledge. I've mentioned this on this sub before, but I'm not a fan of the idea of `lens` creeping into everything. I like `lens`, but I think it should be factored out of the libraries in most cases. Haskell is already a tricky language for beginners to pick up, and `lens` is unequivocally an intermediate/expert-level library. If all the best libraries used by the vocal part of the community (this sub, for example) require `lens`, the language will become much more hostile to beginners. I think that has the potential to greatly hurt the community.
This looks like something similar to lenses.
Isn't that exactly what lenses are? This intro is pretty readable and gets to a type exactly the same as cut's: https://www.fpcomplete.com/user/tel/lenses-from-scratch It even has your given law in the "lensToId" part.
&gt; Minimal complete definition: '&gt;&gt;=' and 'return'. I am not sure whether here is the right place to write this, but the phrase above should be changed to: &gt; Minimal complete definition: '&gt;&gt;=' or 'join'.
Does it really break that much code? That's not my experience. 
This is a good point, but it I think including a mini lens tutorial in the documentation of lens-based APIs is a fair compromise. We'd like to write concise code and lens lets us do that. We'd also like to use libraries so we don't have to roll our own implementation of everything. If we decide that lens must stay out of all libraries for beginners' sake then we have to settle for writing inelegant code wherever we use someone else's API. That sounds like the greater of two evils IMO.
As I see it, that boils down to "I like `lens`, but it shouldn't be used for anything that matters". It seems unlikely that such a stance can survive in the long term.
Understood. I always find it really neat that a lot of the fundamental building blocks in the Haskell ecosystem are so mathematically fundamental that they get coincidentally discovered time and time again. Best of luck in your explorations!
&gt; "I like lens, but it shouldn't be used for anything that matters" That's not what I'm trying to say at all. I'm not against the use of `lens` in projects, far from it. It's libraries that force you into it that worry me. The way I see it, `lens` is like Monads 2.0. When you're new to the concept you have a lot of trouble understanding it. Finally you break through, and use them for a bit. Now you understand them, and wonder what's so hard about them in the first place. Say what you will, monads have put a reasonable number of people off Haskell, even though they aren't that bad, and I think `lens` has the potential to do the same (though I of course hope it doesn't) if it becomes too pervasive. 
I have given up on trying to align things when I program. I just indent.
Well it's possible but it's not pretty. Either the field accessor or the lens would have to have a leading/trailing underscore, trailing tick, or whatever. makeLenses of course gives the lens the prettier name.
Hmm ... the one from `Text.Read`? ... I really didn't know about that one. So thanks :)
If I am not mistaken yours is O(n²) isn't it? I used `Data.Set` for it's tree based nature. Every comparison may take up to 10 minutes (two songs a 5 minutes each) so that actually matters. But I fear /u/Ikcelaks `sortByM` wins the challenge hands down ... didn't search good enough for existing solutions ...
It sorts the songs based on the manual comparison. But both of you are right ... I could have used some list sorting function. And `sortByM` is probably (if I am right that it is O(n log n)) the thing I should have used if I would have search a little bit longer :)
Yeah, I thought that might be a problem. The only way I can think to solve this is to have the two APIs in different modules, but that's huge duplication of code and effort, and working with both together would be a horrible pain in the ass.
Given the areas of research of Create-Net (Mobile and Ubiquitous Technologies, Smart Socio-Technico Systems), not offering remote work is a sin (-:
What complicates things is that it is improbable that any kind of hygiene that would allay your fears can be sustained. Given the usefulness of `lens`, I would not expect that library writers will disciplinedly factor out `lens` as an optional module and take the trouble of maintaining two parallel APIs, or that people can be convinced that `lens` is a set of tools only for advanced Haskellers writing applications. My point is that, while it is natural for us to be in two minds about weird and wonderful `lens` at this early stage, eventually we seem bound to, paraphrasing MitchellSalad just above, choose the lesser of two evils; that is, either to embrace `lens` and deal with how to get newbies used to it or to refrain from using it and to keep looking for alternatives. The parallel with monads is interesting. One difference I would suggest is that, even though the workings of lenses feel definitely more esoteric to the uninitiated, it seems easier to get basic working knowledge of them without being forced to reconsider everything you took for granted about program organization, as tends to be the case with monads.
Yeah, I realize my position has it's downsides, and probably isn't sustainable. I think what lens gains (in regards to learnability, in comparison to monads) in terms of quick working knowledge, it more than loses in breadth and complexity. [This](http://i.imgur.com/4fHw3Fd.png) is the go-to example of this. Another note on this topic: I may be an exception, but there's something slightly off-putting about seeing "Oh, just use `Lens' s a` for now, you don't really need to understand the full type." I understand that the tutorials need to say that, but personally it makes me uncomfortable.
Why does everyone keep saying that `http-client` is so "great"? Its API has a totally weird abstraction level for a library that aims to be used as a building block for "proper" HTTP libraries. Just one example: Why on earth do I have to put up with this `Manager` thing, if have already my own connection pooling in place? &gt; built on reliable libraries like `http-client` How was this "reliability" ever verified? Did it pass any HTTP conformance test-suite? Does it have a good track record, because it has been used for years by many users? Is there any chance, the other popular libs like `HTTP`, `http-streams`, or others will be ported to `http-client`?
Your `cut :: a -&gt; (b, b -&gt; a)` is equivalent to `(a -&gt; b, a -&gt; b -&gt; a)` which are the getter and setter of a lens, so yes what you're looking for is exactly a lens. I don't see why you need a typeclass though.
&gt; `ConstraintKinds` I think that this was exactly what I was looking for! Awesome! Thank you :) Those things are hard to google for if you don't already know them. &gt; Maybe using Template Haskell? OK, I was going to learn this next anyway. :)
Thank you for your well thought out answer! &gt; e.g. why the dummy unit arguments? Each method has exactly one argument in my type alias -- in this example, that argument is unit, producing a method with no (useful) argument. Similar for return types: I'm using unit to produce methods with no return type. &gt; https://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/ Great! As you were typing your reply, I was looking to find this exact link, but couldn't. Thanks! :) I'll read it again! I have played with your solution now, and I agree: it's simpler than mine for the example I gave; however I see some disadvantages, compared to what I started with, too. Those disadvantages are not at all contained in my original question, so please don't see them as criticism. * (+) It's really nice that you don't need the `ExistentialTypes`extension :) * (-) Your solution looses the ability to pattern match on the contents of an object. The encapsulation is perfect, something we wouldn't always want with immutable objects. Thanks again :) 
what's so cool about italy anyway. I can get pizza at my doorstep wherever I am. maybe if you had offices near the seaside, that'd be a gamechanger.. but even that's like.. meh what's so interesting about this job, and how many PhDs do you think there are? I mean, basically what you're saying is "come to italy, so you can measure how much energy a lightbulb is wasting with haskell !". I dunno, that's how I read your job posting
Oh, sounds fun!
The code I pasted is the same as the implementation in monadlist except it uses foldr instead of explicit recursion, and they both have the same time complexity which is the same as Haskell's sortBy: O(n log n). Like Ikcelaks said, doing (``S.toAscList . S.fromList``) isn't actually doing anything in the original code, and is actually going to be much slower than just sorting it as a list, since ``toList`` and ``fromList`` are both each doing a full traversal of which the equivalent effect is just to (``sort . nub``).
See here for how to "denlensify" an API. It's so trivial it's not even worth doing. http://www.reddit.com/r/haskell/comments/23q8kc/wreq_a_capable_new_http_client_library/cgzrc3u
Gotta love reddit! Great answers, thank you to everyone!
I wasn't aiming to have values which specify a focus - the typeclass effectively specified the only focus possible instead. I was thinking in terms of pulling the top layer off a zipper while being able to put it back - as pulling the top layer off the zipper equates to stepping one level towards the root of the tree, there's only one focus (though as you could be stepping to the parent through e.g. a left- or right- child, there's more than one way to deconstruct/reconstruct). The version with a lens in it was in response to kamatus first comment, and doesn't have a typeclass. The first version had a typeclass because (for the simple form of zipper I was thinking about) each tree ADT has it's own zipper type. Though it's probably a bad idea. 
Personally I believe there are far too many typeclasses in the world. I strongly encourage everyone to avoid them unless their introduction leads to a *huge* reduction in boilerplate.
Hmm ... ok ... than I guess you win.
Yeah, on the good old patch-tag :)
Well they tend to take a similar amount of time in my experience. Also, the point would be to avoid harassing the same server. unless you are exploring millions of pages it is a pretty decent first implementation for "controled" batched crawling, really.
&gt; I'm surprised I didn't got any reply from the community to this job offer, I am not after reading it ;-) (Before reading I was far more enthusiastic.)
Not yet, as they weren't necessary to get a 0.1 release out the door. Obviously caching support would be rather nice to have, and I plan to work on it soon.
What is `&lt;&amp;&gt;`? I checked [hoogle](http://www.haskell.org/hoogle/?hoogle=%3C%26%3E). It looks rather like `flip &lt;$&gt;`. Oh, I get it. I think it's not bad, though it's not quite always fixed in it's name. Sometimes its `#`, I think that's the case in diagrams, right? But it's been used before, and its still readable.
You're wrong, we are located in the Dolomites, the environment here is really fanstastic: https://www.google.fr/search?q=dolomites&amp;tbm=isch&amp;tbo=u Actually, from all the working places I've seen in Europe, there is no comparison in term of quality of life. Of course, one should like mountain activities! 
That's right!!
&lt;&amp;&gt; is in the lens library: [lens](https://hackage.haskell.org/package/lens-4.1.2/docs/Control-Lens-Lens.html#v:-38-)
Do you have a suggestion for a better channel to post in?
What made you turn back?
its common in F# where |&gt; is flip $ so I don't think its bad per se. I'm not sure haskell has all the operators you might want for left to right by default though
http-client has been used for years when it was still http-conduit.
What is WTF here? 
The phrase should be removed. Haddock generates it automatically now, although in this case it would need a proper MINIMAL pragma.
Yeah, there's definitely some restructuring of code and clarifying with comments that would go a long way to making it more understandable. The dissertation itself hopefully clears up a bit but I can understand people not wanting to read a 60 page document to understand the code. It's something I plan to return to, since even though I've finished university as of today I'd like to continue improving this.
It's OK. Left-to-right and right-to-left are duals of each other and I think we just have to get used to both! What's more important to me is writing using associative operations. For functions this amounts to preferring `(.)` over `($)` for right-to-left coding style and `(&gt;&gt;&gt;)` (from `Control.Category`) over `(&amp;)` for left-to-right. [1..10] &amp; (filter odd &gt;&gt;&gt; Prelude.foldr (+) 0 &gt;&gt;&gt; print) [EDIT: replaced "monoidal" with "associative"]
I don't understand what you would want it to do instead, since it is written in C it can't use any kind of exception mechanism. It also would break any abstraction if you had to check every arithmetic operation for memory errors. It would also be completely wrong to just ignore the error obviously. The only option I can think of is to provide error handling callbacks, but that is pretty much equivalent to just give the oportunity to use custom memory handlers. I don't think 99.99 percent of users will use the default behaviour, as I said ghc's runtime uses custom allocators for gmp see https://ghc.haskell.org/trac/ghc/wiki/ReplacingGMPNotes/TheCurrentGMPImplementation, you can bet that all the symbolic algebra packages don't use the default implementation (Mathematica for example). 
Hmm, I just built a library on top of http-client, and it was fine. The `Manager` exists precisely to abstract away connection management, so I haven't found myself wanting a different connection pooling mechanism. I've used http-client for years, and Michael has always been very helpful and responsive to questions and bug reports. (In fact, he added the `Manager` feature in response to a request of mine.) So yeah, it has been used for years. And finally, your snotty tone and use of scare quotes brings some of the worst of reddit concern trolling to a community that I generally like. Knock that crap off.
the default value `""`.
Technically it's `^.` and `^..` that use `mempty` for a default value when there are no targets of a traversal. `^?` fails with `Nothing`.
That's just `mempty` as bos points out below.
If you use \^? Instead of \^. You'll get back a Maybe based result. When you use \^. on a Traversal it has to give back an a even when there is no target, the way it does that is by using the Monoid instance for that type in that case. &gt;&gt;&gt; r^?responseBody.key "fnord"._String Nothing
Note the default is being supplied here due to the use of \^. It'd probably be more idiomatic and less magical for the example to show it via \^?
Reddit's Markdown has made the lens operators even less intelligible than usual :) Edward means `^.` and `^?` (and yes the example shows both `^.` *and* `^?`)
the "usual" right to left style is easier for pointfree style which has its advantages at times.
You can always use `&gt;&gt;&gt;` for left-to-right pointfree style, but it can be a bit awkward that `&gt;&gt;&gt;` is three characters instead of one.
Just curious: are there any differences between (flip ($)) and (flip (.))? 
&gt; What's more important to me is writing using monoidal operations. why? your example has much more noise than the original.
&gt; why? Because associative operations allow splicing out subparts. a &gt;&gt;&gt; b &gt;&gt;&gt; c &gt;&gt;&gt; d &gt;&gt;&gt; e &gt;&gt;&gt; f is the same as let middle = b &gt;&gt;&gt; c &gt;&gt;&gt; d &gt;&gt;&gt; e in a &gt;&gt;&gt; middle &gt;&gt;&gt; f but `a &amp; b &amp; c &amp; d &amp; e &amp; f` is not the same as let middle = b &amp; c &amp; d &amp; e in a &amp; middle &amp; f In fact the latter probably doesn't even type check. &gt; your example has much more noise than the original Not really. I only wrote it in that form because I wanted to make it as close to the OP's as possible. If I were writing it myself I would make the function the main object of interest, rather than the result of applying the function to an argument filter odd &gt;&gt;&gt; Prelude.foldr (+) 0 &gt;&gt;&gt; print If you want to claim that `&gt;&gt;&gt;` is more noisy than `&amp;` then I won't argue.
You literally reinvented `Store` type lenses.
with that reasoning, take monads out. take typeclasses out. take lazy evaluation out. take functional programming out. these make haskell hostile to beginners. If lens is a useful abstraction that users should learn, they should learn. Every beginning haskell programmer writes a monad tutorial already. There's nothing wrong with having them write a lens tutorial. Lens tutorial will be everywhere in social network and blog sphere. There will be tweets and reblogs and pins and likes and pluses. It'll only greatly improve marketing of haskell. 
&gt; with that reasoning, take monads out. take typeclasses out. take lazy evaluation out. take functional programming out. these make haskell hostile to beginners. It's a sliding scale. And I'm not in favour of removing `lens` at all, I never said that. &gt; If lens is a useful abstraction that users should learn, they should learn. While I agree on principle, look at Rust. Complete memory safety without a GC is extremely useful, but it's getting a lot of flak for being too "complicated." &gt; There's nothing wrong with having them write a lens tutorial. If it was for university or for a job, sure. But people learning Haskell for fun (the vast majority) don't *have* to do anything, so I don't agree with you. Adding intellectual prerequisites for something usually reduces the number of people interested, whether we like it or not. &gt; It'll only greatly improve marketing of haskell. I'm not sure I agree. We in the Haskell community laugh at the Monad Tutorial meme, but outside the Haskell community it's often seen as the embodiment of what's wrong with Haskell (and the community) - there's a large perceived intellectual burden before you can "get practical." A lot of people don't see a huge number of tutorials for something as "must be really useful!" but as "must be difficult, don't want to have to deal with that." EDIT: Once again, I'm *not* against lens, or libraries that use lens. I'm just pointing out a possible downside to lens "infiltrating" the whole library ecosystem.
Fair enough. :) Fixed. Couldn't type the backticks nicely on my ipad, but apparently backslashing works.
I agree that at their core they aren't complex. But I think that's the curse of generality - the more general something is, the trickier it can be to wrap your head around, but the simpler it seems once you understand it. Of course `lens`'s components aren't too far up the generality scale, but they're further than most programmers (especially non-Haskellers) are used to operating at.
Thanks for writing that out :) I expect you're probably right about `lens` being the "next big thing", I'm just slightly wary of the possible downsides of its spread, which doesn't seem to get talked about much.
yep, much better without the liftM and and leaves both styles as broadly a matter of taste. I stuffed up the monadic transformation because of my type-bingo style - use random combinations of fmap/liftM/bind until something type checks. Definitely easier to play type bingo left-to-right!
Ok, so I finally understand why (.) is better than ($) other than noise pollution. I actually find &gt;&gt;&gt; more evocative of what's happening than (.).
That's a very good point. `&gt;&gt;&gt;`, like F#'s `|&gt;`, indicates a directionality.
&gt; the more general something is, the trickier it can be to wrap your head around, but the simpler it seems once you understand it This is a good observation. `Control.Lens` is *incredibly* general and paradoxically it can be hard to see how all the parts fit together because they fit together so easily :)
I proposed this half a year ago, but few people seemed to care. I let it die silently, and every now and then I thought whether I should have been more vocal about it. Anyway, here's the link: http://www.haskell.org/pipermail/libraries/2013-December/021833.html
That makes the two examples so symmetrical its a bit spooky.
&gt; Is there something specific you're reacting to? It's not what I expected. Not at all. Not in the Haskell at least. So this isn't even `_String`-specific?
Exactly :) I think it also explains the Monad Tutorial Effect, since monads are in a sense the "introduction to generality" for many people. You struggle to understand them, but then as soon as you do they make so much sense that you just *have* to write a tutorial about them :D
If I try to install this, I get the message below from cabal. Should I care? cabal: The following packages are likely to be broken by the reinstalls: http-conduit-2.0.0.8 
Why is `FormParam` implemented as := :: FormValue v =&gt; ByteString -&gt; v -&gt; FormParam rather than := :: ByteString -&gt; ByteString -&gt; FormParam ?
There's no single word about Haskell, but there are 9 mentions of “cloud” alone. Eclipse, ANT, JUnit, SVN, Agile, PaaS, “capability to adapt to dynamic environments”. Ending with that resume idiocy. If I can draw some picture from that it's not convincing. I imagine that's not only for me. If it's not true and, like, you don't *actually* have to *touch* ant, then that's the first thing you need to state. 
That's happening because wreq depends on a newer version of http-client than http-conduit is built on. If you reinstall http-conduit at the same time, the problem will go away.
(In case you don't know how, instead of merely finding it too awkward: hold down single-quote and wait for a pop-up keyboard featuring backtick to appear.)
&gt; no big deal though :) Kinda it is actually 
Relocation isn't a showstopper because people get relocated to crappy places. Relocation is a showstopper because it's a huge pain in the ass, generally takes considerable time, and uproots peoples lives. This is a dealbreaker for many people, money and venue aside.
I'm leaving the minor cleanups and addition of improved `MINIMAL` pragmas until after the merge later this week.
I really like how the API leverages `lens`. I am aware this may not be a priority at this point, but have you done any performance benchmarks/optimizations yet (and is a rewrite of [pronk](https://github.com/bos/pronk/) in terms of `wreq` planned)?
Once `lens` becomes a GHC boot library (or merged into `base`), this won't be an issue anymore... just kidding of course :-)
Derp. Of course! Thanks.
FWIW as a haskell novice I have found lens quite easy to grasp. At least comparing to everything else you need to understand to enjoy Haskell.
&gt; For example, about half of the API of Data.Map could be replaced by just at and itraversed. The reason `Data.Map` doesn't just have an `at` function (it has been proposed and evaluated in the past) is that it is much slower than having specialized functions. &gt; Providing a non-lens API would require: Records, even if clunky, already come with these. Example ghci&gt; let opts = defaults &amp; param "q" .~ ["tetris"] &amp; param "language" .~ ["haskell"] vs ghci&gt; let opts = defaults { param = [ ("q", ["tetris"]), , ("language", ["haskell"])] } if I guessed the meaning of the lens operators and the docs of `param` correctly. 
The code for `(^.)` is embarrassingly short: s ^. l = getConst (l Const s) When you pass it an `l` that is a `Traversal' s a` it demands an `Applicative` for `Const a`, which exists only when `a` is a `Monoid`. So it looks for a `Monoid` for the thing you are requesting, and uses it to smash together the (potentially multiple) results of the `Traversal`. When you dig through the types its the only thing it can meaningfully do to answer the request that is a correct extension of the semantics for a `Lens` and which never crashes. `(^?!)` on the other hand, does the other thing you might suspect it could do, which is assume your traversal succeeds and gives you back the first answer, and calls error otherwise. The scary look of the operator is intentional, but it can be parsed morphologically as `(^?)` (which can gracefully fail) with an `!` on the end indicating you are claiming you know it won't fail and that you want us to trust you. Those are the only two "sensible" ways you interpret someoperator :: s -&gt; Traversal' s a -&gt; a and `(^.)` is the one that never errors. When you are working polymorphically the Monoid constraint will complain at you, and when you are working monomorphically over a particular type, presumably you know what you are asking for. By doing things this way `lens` enables you to use operators on many many more types of traversals than you'd expect at first glance. Almost any part can be used with any other part in `lens`. e.g. If you call `zoom` on a `Traversal`, it'll run your state monadic action on each part of the Traversal, and then use the `Monoid` to glue together the answers. That `("hello","world")^.both` must perforce return `"helloworld"` is a consequence of that pattern. 
I guess I don't understand cabal well. For example, if I try to install (reinstall?) http-conduit, I get the message below. But isn't the whole point of cabal to be able to do these kinds of updates without having to do a "force"? cabal: The following packages are likely to be broken by the reinstalls: http-conduit-2.0.0.8 Use --force-reinstalls if you want to install anyway.
`_String` is a `Prism` that checks to see if a `Value` in aeson is made with the `String` constructor. It can be viewed as a `Traversal` of 0-or-1 target in Value. e.g. if we just wanted the `Traversal` it'd be _String :: Traversal' Value String _String f (String a) = String &lt;$&gt; f a _String _ xs = pure xs Because it is a `prism` it is a bit more complex, so that you can can use it both to traverse a string if its present, but also to construct a `Value` from a `String` with `_String # s`. Given that String a^._String = getConst (_String Const (String a)) = getConst (String &lt;$&gt; Const a) = getConst (Const a) = a does the obvious thing. However, you are being surprised by SomethingElse^._String = getConst (_String Const SomethingElse) = getConst (pure SomethingElse) = getConst (Const mempty) = mempty = "" This is the only way these parts fit together. We could needlessly restrict the type of `(^.)`, crippling it and preventing the user from writing this case for no reason. However, this would require crippling roughly half of the API of lens and preclude many perfectly legitimate uses.
I'm putting GHC back to "To celebrate" list already for 7.10 - gets me everytime!
The problem is that GHC doesn't support [MultiInstances](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Packages/MultiInstances) yet, so you can't yet have the same package version of `http-conduit` registered multiple times compiled against different versions of `http-client`. Otoh, you can use `cabal sandboxes` to workaround the issue in the mean-time. 
I'm of two minds on this argument, but they are frankly both decidedly on the other side of the fence from you. The better angels of my nature speak to the fact that my goal with working in Haskell is to raise the level of discourse and to build new tools, because the ones I've had to use my entire career suck. To that end, I feel it is going to be necessary for people to learn things. Otherwise, we're going to be stuck right here doing exactly what we're doing now, forever. To that end, I'm happy to reach out to anyone I can and try to teach them things I think will be useful for them. `lens` raises the bar of abstraction, but it does so so that 2 years on you can still be using it usefully, and be finding new ways to twist the pieces around to fit new problems. Yes, there is a teaching challenge with it. There is with any new tool. I prefer to tackle the challenge rather than shy away from it, however. The fact that it _is_ pervading the ecosystem so thoroughly these days speaks to its utility, and the fact that the combinators in lens work so well for such a wide variety of applications rather dramatically decreases the net cognitive load on an end user in my experience. You don't need to learn how to use dozens of one-off combinators for each new widget. You just need to know there is a `Lens`/`Prism`/`Traversal` for it, and you can grab access to a ton of additional combinators based on other things you already know. Then there are the somewhat more sarcastic worse angels of my nature: https://www.youtube.com/watch?v=bkjmzEEQUlE But we don't let them speak in public. ;)
Please keep plugging for this. Many of us support it wholeheartedly. 
Regarding adding `alterF` (I think that was the last name proposed) to `Data.Map` would at this point just make `lens` use faster than we can make it from entirely outside of the library. I wouldn't advocate removing any of the other combinators.
Thanks a lot for this library, looks great! btw, are there (or is it planned) an exception-free version of getWith/postWith?
Sigh. Yet more weakening of the type system. Like any such increase in polymorphism, it can initially seem useful, but often leads to far more ambiguity and loss of semantic clarity than it's worth. I guess we'll see, with time.
I jesting pointing out that the simple thing people want 99% of the time is more inconvenient than it should be. On top of that you have to worry about the efficiency of the index translation.
&gt; shout if you are interested in this "road to Haskell", I can blog about that I for one am interested.
I think it's neat, however if you use heavily this method your program compilation will take ages. This is a problem when you compile often. Why not putting the result in a file?
&gt; &gt; For example, about half of the API of Data.Map could be replaced by just at and itraversed. &gt; &gt; it is much slower Sure, I was being a bit loose with language. I meant "replace" APIwise, not in terms of the other properties the library provides, such as high performance. &gt; &gt; Providing a non-lens API would require: &gt; &gt; Records, even if clunky, already come with these. That's not my point. My point is that that is the *minimum* a delensified API would have to provide. Record syntax isn't first class so I can't write the equivalent of `over`, for example. Furthermore, the only "lenses" that records allow you to get your hands on are the actual fields of the datatype. You can't write any sort of custom lenses.
There's a guide for Ubuntu users [here](https://wiki.ubuntu.com/Fonts). I think it should work for other distros though. Basically [grab](https://github.com/i-tu/Hasklig/releases/download/v0.2/Hasklig_0.2.zip) the fonts and stick them either in /usr/share/fonts or ~/.fonts and then run: ``sudo fc-cache -f -v``
Yes, you can add whatever caching mechanism you like on top of that.
&gt; That `("hello","world")^.both` must perforce return `"helloworld"` is a consequence of that pattern. Woah that's *very* counterintuitive! I guess anyone who doesn't like that is welcome to define their own operator which uses `MyNewConstWithoutApplicativeInstance` if they want it to fail at compile time.
Good to know, I'll blog about it then :)
My problem with lens-based libraries is that the error messages I get if I screw up are absolutely awful.
I checked it can be done: import Control.Lens newtype ConstNoApplicative a b = ConstNoApplicative { getConstNoApplicative :: a } instance Functor (ConstNoApplicative a) where fmap f (ConstNoApplicative a) = ConstNoApplicative a type GettingNoMonoid r s a = (a -&gt; ConstNoApplicative r a) -&gt; s -&gt; ConstNoApplicative r s (^^.) :: s -&gt; GettingNoMonoid a s a -&gt; a s ^^. l = getConstNoApplicative (l ConstNoApplicative s) *Main&gt; ("hello", 5) ^. _1 "hello" *Main&gt; ("hello", "world") ^. both "helloworld" *Main&gt; (Nothing :: Maybe String) ^. _Just "" *Main&gt; ("hello", 5) ^^. _1 "hello" *Main&gt; ("hello", "world") ^^. both [...] No instance for (Control.Applicative.Applicative (ConstNoApplicative [Char])) [...] *Main&gt; (Nothing :: Maybe String) ^^. _Just [...] No instance for (Control.Applicative.Applicative (ConstNoApplicative String)) [...] 
&gt; We could needlessly restrict the type of `(^.)`, crippling it and preventing the user from writing this case for no reason. However, this would require crippling roughly half of the API of lens and preclude many perfectly legitimate uses. Lens could theoretically provide a different operator that doesn't have this behaviour by using a different `Functor` for it: http://www.reddit.com/r/haskell/comments/23q8kc/wreq_a_capable_new_http_client_library/ch00xwc NB: This isn't a feature request, it's just an observation! 
[Hoogle link](https://www.haskell.org/hoogle/?hoogle=%3C%26%3E+%2Blens) including `lens`.
Back at ICFP Simon offered the core libraries committee the chance to decide the fate of `fail` in `Monad`. In general, I agree with the general sentiment of moving `fail` somewhere. For 7.10, we have a lot of big changes going in, but currently they all have the very nice property that they can be worked around without requiring CPP on the behalf of the end user. I'd really like to preserve this property, as it'll make it clear that these generalizations aren't the source of much pain, making it easier to push for other improvements in the future. Moving `fail` in 7.10 would break that rather nice property, but it is currently a guideline we're using, not a hard rule. With 7.12 we're talking about what it'd take for split base, and many of those potential changes _do_ require CPP at present. The real question is does it belong in `MonadFail`, or `MonadPlus`, or lifted up to `Alternative` or something at that level. Should we keep it in `Monad` for a release, and add `MonadFail` as a class with a member that defaults to the one in the other, so we have time to make the migration, or rip off the bandaid. I would like to take the time to carefully consider the options, rather than just assume it belongs as `MonadFail`, but I do agree that `MonadFail` is a very strong candidate for the right solution, and I'm open to discussion about whether it should be something we consider for 7.10 or 7.12.
And loading your list from the binary which might actually take longer than computing it depending on your computation.
This is another factor that makes me wary of the advancement of `lens`. My favourite part of Haskell is the type system, and letting it yell at me when things go wrong. In my normal code, it's yelling at me in heavily accented English, but with `lens`, sometimes it's yelling in Icelandic with an English word here or there.
It could, but when that is the behavior you want, it is very nice not to have to be redefine everything in lens that goes through `Const`, `Pretext` or `Context`. In general `lens` combinators ask you to ask yourself for an instance. The guideline we use is that any lens combinator when used with any lens type should work if it has a sensible principled way in which it can. This pretty much governs the entire structure of lens, as it is far less jarring than everything randomly failing to compile with inscrutable messages.
Of course, it can be done. However, all you've done is disable functionality. The central tenet of lens is that all of its parts should work together whenever they can. `(^.)` should work on an indexed traversal as well as a prism or an iso, and in that sense it does the only sensible thing it can for all of those cases. This way you learn the vocabulary once, and the applications for it keep expanding. We have precisely one combinator in lens (outside of the ALens/loupe machinery) that is a disabled version of another combinator. I invite you to try to find it. ;)
&gt; both decidedly on the other side of the fence from you. I definitely saw that coming :D I don't exactly disagree with anything you've written. It all makes a lot of sense, and ideologically I mostly agree. I'll say again, I have absolutely no problems with `lens` itself. &gt; the fact that the combinators in lens work so well for such a wide variety of applications rather dramatically decreases the net cognitive load on an end user in my experience. I mentioned this in a reply to /u/tomejaguar above - this is what I call the curse of generality, and I think it's in play here. The more general a concept is, the harder it is to learn, but once it's learnt, the simpler it seems. An unfortunate side-effect of this is that once you understand it well, it's possible to lose sight of how hard it was to learn in the first place. I think this is an issue the Haskell community struggles with more than most other programming communities. I see it over in /r/programming occasionally: someone will say they don't understand monads, and then someone will say "they're simple!" and try to explain. Then OP will reply... and just not get it. Which is what happens to a lot of Haskell newbies, and what puts off a lot of prospective Haskell newbies. Through no fault of its own, I think `lens` has the possibility to do the same. For those that know the library well, it's definitely a reduced cognitive load - you have little trouble fitting `lens` and the problem at hand together, and that's what allows `lens` to be so powerful. But to someone that isn't experienced with the library, it can be overwhelming. I mean no disrespect or anything similar by what I'm about to say (to anyone in our community), but I think the very experienced Haskell programmers can occasionally lose sight of these problems. Most of us laugh at the monad tutorial epidemic because monads aren't difficult for us, but I think the larger programming community views it differently - they see it as a problem with the language that we need to go to such length to explain (with varying degrees of success) such an integral part of using the language. We love monads, because we're over the hump of understanding. But to those on the other side of the hump, they're a necessary evil. Now, in the community's current state, I don't think `lens` is contributing to Haskell's wider perception - it seems pretty unknown outside of Haskell circles. And if every programmer in the world were to suddenly turn to Haskell and learn `lens`, I would be extremely happy. And I believe more people should try. `lens` is already a huge hit in the Haskell community, and with good reason. But call me a cynic (and I probably am), but I feel like `lens` has the potential to become a negative *from the perspective* of those looking in from the greater programming community. And while Haskell is occasionally caught up in its own world (and I don't mean this in a bad way), public perception is extremely important for the growth and uptake of a language. Of course, this is all speculative, and I'm probably just a rambling cynic :) Let me reiterate that I mean not to offend anyone, or cast a shadow on anyone's work (least of all Edward's) in this post. I'm just writing out what I've been thinking about for a while.
Isn't that the point? You're talking about a matter of style; the examples should look almost exactly the same, but reversed.
&gt; Let me reiterate that I mean not to offend anyone, or cast a shadow on anyone's work (least of all Edward's) in this post. I'm just writing out what I've been thinking about for a while. I think your thoughts are very cogent. Thanks for sharing them!
I believe in the C world things like int foo(char a) are called type declarations so that may be causing some confusion.
A silly suggestion has just crossed my mind: those disturbed by the fact that lenses compose in the "wrong" (sic) order with `(.)` might want to try `(&lt;&lt;&lt;)` and `(&gt;&gt;&gt;)` instead. If you squint a bit there is even a mnemonic, by associating the widening of the "&lt;" and "&gt;" glyphs with zooming in.
Even if I see why it's `""` and from where it comes, the big picture is: obj['nosuchkey'] is now ""
Hence why I wouldn't have included `^.` in the example docs for this usecase at all. 
That's '(eg. "`foo ^. bar . baz`" instead of "`foo^.bar.baz`")' for anyone that was confused.
From an epiphenomenological perspective I can appreciate your viewpoint. I just happen to believe, objectively from what I've seen in practice, that the "price" you see us paying is outweighed by the benefits we gain, and that it is perhaps a lot lower than you think. My inbox gets a lot of traffic from folks new (and old) to the Haskell community alike. I've also had many many reports from newcomers to Haskell that `lens` was what let them get over the hump and start hacking in Haskell effectively, or that it was what let them finally understand State, or what the point was for all that abstraction we throw about. Ironically if I had to say anything from the traffic in my inbox and on #haskell about it, it is mostly the old guard who gets disgruntled by lens.
How does moving fail require CPP? Couldn't a backwards compatibility package be defined that just re-exports MonadFail on a GHC where it exists, and defines a simple MonadFail and instance (Monad a) =&gt; MonadFail a for older GHCs?
And thanks for reading :) It's nice to be able to say stuff like this and not have anyone take it the wrong way.
&gt; Well, they also compose in the "wrong" order with respect to `(&lt;&lt;&lt;)` and `(&gt;&gt;&gt;)` too :) Indeed :) The main difference is just that we do not use `(&lt;&lt;&lt;)` and `(&gt;&gt;&gt;)` as often as `(.)`, and so the end result might look less jarring for those sensitive to that quirk. I don't see myself adopting the suggestion, but I might give it a try next time I have to write an expression with both lens composition and plain function composition.
&gt; I just happen to believe, objectively from what I've seen in practice, that the "price" you see us paying is outweighed by the benefits we gain, and that it is perhaps a lot lower than you think. I realize there haven't been any real problems so far. And I hope as much as you that there never are :) Of course you have much more experience than I do on this matter, having interacted with far learners of `lens` than I have. These are just the thoughts of a single beginner/intermediate Haskeller, and as I'm a sample size of precisely one, feel free to disregard everything I say :D Thank you for discussing this, it's given me some things to think about.
&gt; Indeed :) The main difference is just that we do not use (&lt;&lt;&lt;) and (&gt;&gt;&gt;) as often as (.), and so the end result might look less jarring for those sensitive to that quirk. Yes I think you might be right about that.
Link, please ^)
Is there a good introduction to MTTs?
&gt; Lenses are the next big thing in Haskell I love lenses, but the `lens` library specifically is just too huge for me to realistically expect people reading my code to understand. Or even myself.
`flip (.)` takes two functions as arguments and produces a third function. `flip ($)` takes one value and one function as arguments and produces a value.
What's with the "copyright Adam Gundry 2013" comment in the files? Obviously you are not personally retaining copyright if you're committing this to GHC.
I also think that it's weird even mentioning existance of \^. in main article, since it's like imagining in, say, python that doing `foo["bar"]` would return "" if bar doesn't exist. I find this operator to be quite harmful (and using "just skip if key that should exist doesn't" in general, it would be super-hard to debug why exactly end-result is empty).
it is documented as doing exactly that insensible (in my and your opinion) thing it does. in that sense it's only bad, that it gets boasted as something good.
I can think of a large number of reasons why memoization should not be implemented in the compiler, even though pure functions can afford the opportunity for the potential optimization.
To be fair, the tutorial does mention `^?` alongside with `^.`, and compares what you get with each operator in the missing key case. Perhaps it is just a question of emphasis.
Yes I find it harmful too. I don't quite find it WTF because we know from its type that it must succeed and that shoud give us pause to think what is really going on. But I'm not convinced that this behaviour is more helpful than harmful.
At the very least the type tells you it cannot fail. What's worse is `^?` in my opinion Prelude Control.Lens&gt; [1..] ^? traverse Just 1 
for the future, check in ghci: λ :t flip ($) flip ($) :: b -&gt; (b -&gt; c) -&gt; c λ :t flip (.) flip (.) :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; a -&gt; c 
how should it be decided, if it's worth it?
it couldn't be reimplemented (as, say, Left for Either) in `instance Monad`. it would have to be done in `instance MonadFail`. also every block with `fail` would require a `MonadFail` instead of a `Monad` constraint.
fail as a member of `Monad` moving to another class means that any instance that actually provided fail before or delegated it to an underlying monad has to move that code out to `MonadFail`. If users want to continue compiling with pre 7.whatever-we-put-it-in compilers, then they'd have to CPP around their fail definition and put another one in their MonadFail instance. instance Monad Foo where return = ... (&gt;&gt;=) = ... #if __GLASGOW_HASKELL__ &gt; 7XX instance MonadFail Foo where #endif fail s = ... Adding the `Monad a =&gt; MonadFail a` patch package only addresses the needs of monad consumers, not authors. I'm not saying it is insurmountable. I'm simply saying it deserves deliberation as it isn't a free change. There are also a couple of proposals in the works that might somehow apply here as well.
Yea, I'll second that. I'm not familiar with the topic and google is sending me to a variety of different papers.
Don't know if there is any benefit in extra annotations if you could just do `memo square` to achieve the same thing. (`memo :: HasTrie t =&gt; (t -&gt; a) -&gt; t -&gt; a` from http://hackage.haskell.org/package/MemoTrie-0.6.2)
It would be prohibitively expensive to memoize everything. It would be very difficult for the compiler to automatically find which functions are good to memoize. It's much nicer (for everybody) to just do it on your own. See [MemoTrie](http://hackage.haskell.org/package/MemoTrie) and [stable-memo](http://hackage.haskell.org/package/stable-memo). (I link both because they serve slightly different purposes.)
IMHO, "binding" a type signature sounds even more incorrect than declaring one. Perhaps "ascribe", or "specify"?
No need to make it a feature of the compiler/language if it is easily doable by a higher order function... 
http-conduit and pipes-http solve a different need than wreq. while there is certainly overlap, wreq is still using traditional lazy IO. the other use fancy new io abstractions. i _guess_ it should be possible for both http-conduit as well as pipes-http to incorporate/add on to wreq's (imo) superior api. i think tekmo would be very glad to do that, i am not so sure about snoyman though.
I see no point to adding this to the compiler, even using annotations from the programmer. It's just extra weight for us to implement (**and indefinitely maintain, mind you**), when you can write a `memo` combinator in like [10 lines of code](http://hackage.haskell.org/package/uglymemo-0.1.0.1/docs/src/Data-MemoUgly.html) today using `MVar` and `Data.Map`. And despite the name of that package, that implementation is actually quite clear and *less* ugly than the work needed in GHC to support it, I imagine. Similarly, a plugin for GHC is going to be much larger and a lot more work to maintain and care for, *and* harder to use (because now all clients must use the plugin) compared to this. On top of that, a plugin will be *less* understandable for possible contributors, and it will change with GHC endlessly. That `uglymemo` package has worked fine for years and I foresee absolutely nothing that prevents it from doing so for years to come. Even as a GHC developer myself, given the choice between maintaining these two things and using them in a wider setting - well, the choice is obvious. Trading more complexity and surface area in the compiler so you can get around a single `import` statement (or an extra 10 lines of code) is not a tradeoff worth making in this case. The 'solution' simply does not warrant the complexity budget.
I spend a lot of my time reinventing things (often poorly) that other people have already named and done better. It is a great way to find out new things, and when you _do_ get to know that the other thing exists you can read the literature on it already informed of an intuition as to how it can be used.
Yep ... Actually `MemoTree` is IMHO one of the coolest usages of laziness. Just create a tree with all solutions ever ... but only evaluate the parts you need. The only problem is that you have no way to control memory usage.
There's some code that did work in the file karatsubaSlice.hs in the top level of the tree. It requires the Arrays to reside in pinned memory which is actually not a good idea so I need to fix it so it doesn't need this. If you ckec back through the git commit history you should be able to find where this code was actually working.
I think F#'s |&gt; is more like (&amp;) than (&gt;&gt;&gt;) i.e. it's function application, not composition. F# does have a left-to-right composition operator though: it's &gt;&gt; , although I'm not sure if the type inferencer like |&gt; better.
Wow, I just discovered PatternGuards.
I'm curious why you describe this as a weakening of the type system? It certainly weakens type inference and makes ambiguity more likely, but that's pretty much inevitable given more polymorphism. The same arguments could have been made against the introduction of typeclasses (not that I think ORF will be anything like as revolutionary, but still...)
I think it is usual for GHC contributors to retain copyright on their source files, and I don't know who would receive it otherwise. (The University of Glasgow? Haskell.org? SPJ?) I'll happily be corrected on this, though. Obviously I have licensed my code under the GHC license (BSD), which is a separate issue (and makes the question of who holds the copyright more or less irrelevant anyway).
Great tips. Minor correction: it looks like the version of `sequence` defined in terms of `mcons` is ill-typed. Here's a similar version that more closely mimics `(:)`. sequence :: Monad m =&gt; [m a] -&gt; m [a] sequence = foldr mcons (return []) mcons :: Monad m =&gt; m t -&gt; m [t] -&gt; m [t] mcons p q = do x &lt;- p y &lt;- q return (x:y)
I think memoization is too implicit and fraught with significant memory leaks if used blindly. On the other hand, we already have an explicit way to do "caching" with lazy lists, e.g. just define squares = [ x * x | x &lt;- [1..]] and you have an infinite list of possibly evaluated values for every positive integer (="caching").
There's not really a reason to if you don't have to I guess, and even if you do upgrade it should be pretty painless I'd imagine. `http-conduit` was split in the 2.0 release and `http-client` was the result of that. It's just lower on the dependency chain so other libraries (such as `http-conduit` or `wreq`) can use and build on top of it easily without buying into the whole `conduit` ecosystem.
Except that the original text talks about declaring a type, and not a type signature, and that I'm not suggesting to talk about binding a type signature, but binding a type to an expression. Regardless, it would be interesting to see how the Haskell standard formalizes this.
I really don't follow your reasoning. /He/ made the choice of qualifying the usage of the (::) operator as a "type declaration" in his paper on Haskell. I am personally not sure whether the C standard ever uses these specific words (type declaration) to refer to something, but if it does like you claim, then why did he chose to use this same expression in his Haskell paper? Per your own claim, the author is the one creating this confusion then. Moreover, the code you gave is not a type declaration (again, whether that's a formally defined term in C land or not). It is either a function prototype (if you add a semi-colon after it), or an actual function definition if it's followed by curly braces and the body of the function, of course. So you got me pretty lost here... EDIT: Confused commenter with OP - replaced "you" by "he" where appropriate.
Or maybe I'm just an idiot that didn't understand your intent was to add weight to my remark... My apologies if so, it's late here.
`:i` isn't just for instance information. Moreover, `:info Functor` would be class information, not just instance information. Prelude&gt; :i Either data Either a b = Left a | Right b -- Defined in `Data.Either' instance (Eq a, Eq b) =&gt; Eq (Either a b) -- Defined in `Data.Either' instance Monad (Either e) -- Defined in `Data.Either' instance Functor (Either a) -- Defined in `Data.Either' instance (Ord a, Ord b) =&gt; Ord (Either a b) -- Defined in `Data.Either' instance (Read a, Read b) =&gt; Read (Either a b) -- Defined in `Data.Either' instance (Show a, Show b) =&gt; Show (Either a b) -- Defined in `Data.Either' 
this is one of the better haskell refs i have seen recently....thanks Stephen!!
&gt; There are obvious They aren't 
`wreq` certainly does *not* use lazy IO: it uses *eager* IO, and provides (a currently incomplete set of) simple left folds if you need to stream responses.
You don't need to apologize, and I didn't take anything as ungrateful. You're entitled to feel however you like about my choice to use lens, and I certainly knew ahead of time that it would be a controversial thing to do.
You don't have to switch. Michael just factored out the `conduit`-independent subset of `http-conduit` into `http-client` so that other libraries (like `pipes`) could use it.
I'm just thinking that it would be best to cover \^? and some other way that throws error (or returns error-info), rather than \^. in tutorial. Any way, if I'll play more with the library -- I'll better suggest a pull-request, not discuss it on reddit :)
You may want to read ["Stretching the Storage Manager"](http://research.microsoft.com/apps/pubs/default.aspx?id=67497) by Simon Peyton Jones, Simon Marlow, and Conal Elliott. It talks about the various approaches to memoization, and how there is no one size fits all approach and they extend the runtime system to handle a bunch of novel techniques to better support it. However, these are implemented in libraries, not as a core language feature as there is really no one-size-fits-all solution. For simple functions like your example, with simple inputs, you may get a lot of mileage out of Luke Palmer's [`data-memocombinators`](https://hackage.haskell.org/package/data-memocombinators), though.
&gt; Ironically if I had to say anything from the traffic in my inbox and on #haskell about it, it is mostly the old guard who gets disgruntled by lens. Yep, and here's a possible reason: http://ro-che.info/articles/2014-04-24-lens-unidiomatic.html
I happen to agree with you about it not being "idiomatic" Haskell. I actually wasn't trying to write an idiomatic Haskell library pursuant to the conventions we as an ecosystem used when it was first written. I was very much trying to be true to the abstractions that were presenting themselves to me. *Insert high-minded artistic-sounding mumbo-jumbo here* When I start working on a project I like to let it 'try to find its own voice'. I don't maintain `lens` with the same guidelines that I do something like `semigroups`. The latter has a very conservative stance, changes glacially slowly and is standards minded. With `lens`, there *already existed* all of these restrained, staid, rather boring packages that didn't push the idea, or worse pushed it in ways that frankly nobody can give me laws for, and I wanted to see where it went if I stuck to *a set of laws* and let everything else vary -- along the way bunch of other people came along for the ride, and it has dramatically improved my productivity and that of many others. I learned practical applications of obscure bits of category theory, internalized the meaning of profunctors, proarrow equipments, framed bicategories, representability of profunctors, learned more about what kinds of adjoint functors are useful for getting stuff done, provided some interesting feedback to the rest of the bidirectional transformation community, figured out that uniplate is just a library supplying a traversal, and that I could steal its vocabulary, found connections between lenses and zippers that had been alluded to but not explored, found new ways to express breadth first traversals... None of this would have been possible had I taken your suggested approach in that article. If I had it to do over again, I'd pretty much take the same approach I did the first time. I apologize if that sounds abrupt; it probably comes off rather dickish, actually. I respect your opinion, I just don't happen to share the opinion that `lens` would have somehow been better had I somehow found a way to make it fit existing conventions, and moreover, having built it, even with the benefit of hindsight, I don't see how I could have done it differently to meet the objections without producing a worse, less usable library.
There actually is a `(^#)` operator in lens that is crippled in the right way to make this work just for lenses. It is, however, perhaps too crippled, as it won't work for a mere `Getter`. These are the kinds of trade-offs you start getting for more restricted operators.
I think one of the hallmarks of good functional style is finding the narrowest (i.e. least featureful) abstraction that gets the job done. By minimizing features you maximize the ability to reason about that abstraction. To illustrate this, let me ask a different question: what can you NOT do with the IO monad? The answer is: pretty much nothing, since the IO monad operates at the maximum possible privilege and therefore is the most difficult abstraction to reason about because literally anything is possible. Despite its power, we try to avoid programming in the IO monad so that we get stronger guarantees about our code's behavior. Note that you can balance these two conflicting goals using [functors](http://www.haskellforall.com/2012/09/the-functor-design-pattern.html). Each component of your application should be written as a morphism in the category of least privilege and you can connect morphisms in diverse categories by unifying them to agree on a common category using functors.
You're welcome to switch to &gt; [1,2]^?partsOf traverse.filtered ((1==). length).traverse Nothing instead. ;) In all seriousness though, we are actively working on a way to make affine traversals (traversals of 0 or 1 targets only) that don't suck to use.
You can use `(^..)` to get the full list. `(^?)` is a (flipped) alias for `preview` which just gets the first... you know, as a preview. =) 
Probably depends on your level of experience, but for me its: On windows: I use Haskell-Platform; I think it helps to have more of a batteries included setup there, at least at first, because I'm not sure what I'll be able to compile. On linux: I had haskell-Platform, but wanted to be on more the edge of things. Recently I've completely removed haskell-Platform and have been enjoying just GHC + Cabal. Took some time to bootstrap everything but now I'm more comfortable with this setup.
First: Thanks for writing this ~~nicolast~~ Denis. In addition to solidifying your knowledge of Haskell by writing about it, it will surely help others wanting to embark on the same journey. If ever I'm confident enough in my abilities, I should follow suit. Second: Is it accurate to say that the value of `"Hello " ++ "World"` is `"Hello World"`? Try: Prelude&gt; let a = "Hello " ++ "World" Prelude&gt; :print a Prelude&gt; let b = "Hello World" Prelude&gt; :print b I'm only asking because the implications of lazy evaluation are rather important when learning Haskell. Indeed, lazy evaluation doesn't seem to be mentioned at all. *update:* credit/blame where credit/blame is due. Sorry nicolast!
I start with ghc and cabal. Then, I install alex, happy and doctest (and agda, and any other program which produces a binary). Then I copy `~/.cabal/bin` and `~/.cabal/share` to a safe location. Then I use a script to clear `~/.cabal` and `~/.ghc`, re-run `cabal update`, and put the binaries and their helper files back in `~/.cabal/{bin,share}`. After that, I only ever build in a sandbox, and if I ever install outside of a sandbox by mistake, I re-run the script to revert `~/.cabal` to its mostly-blank state. I follow all those paranoid steps because I've encountered some weird configuration-dependent bugs, and I want to make real damn sure that the packages I publish will work with a fresh install.
great to see the final approach in there too. not sure if it was in the prior version.
Definitely one of the best Haskell tutorial!
I started building binaries in sandboxes and using cabal's --prefix option to install into ~/.cabal.
Damn. After staring for a few minutes I noticed this too and thought I would have something meaningful to add to the discussion for once, only to discover I'm hours late to the party! Sigh.
This seems overwhelmingly long (the table of contents in the left column doesn't even fit on my screen). Any chance of breaking it up into smaller pieces, each with some sort of theme?
You are missing something. Read the types. And use GHCi to guide you to the next type, quickly. Or just draw a picture/commutative diagram. Some problems are complex, and require more mental horsepower than a person can handle. Fair enough. But that is why the abstractions are there in the first place. Draw the picture and you'll understand. That said, some packages seem to go out of their way to make things difficult (data types in hidden modules, so you don't have a choice but to download the source to figure anything out, since hidden modules don't have documentation built for them at all). I'm looking at you, Yesod...
While Lenses *themselves* aren't, several of the *other* concepts defined in edwardkmett's lens library are highly generalized "non-typeclass versions" of type classes defined in the standard library. Off my memory: Traversal &lt;-&gt; Traversable Setter &lt;-&gt; Functor Fold &lt;-&gt; Foldable
The benefit of the Haskell Platform as a base is that it gets you a lot of tricky packages installed right from the get go. This can matter *a lot* on windows where some of the platform is just outright arcane to set up.
The idiomatic style, models how such expressions are said in English (other languages too?): For example: a = sort $ map (\x -&gt; x*x) $ filter odd q is written this way because we say "a is equal to the sorted squares of the odd values of q". If you write this the other way: a = q &amp; filter odd &amp; map (\x -&gt; x*x) &amp; sort and try to speak it in that order: "a is take q, then take the odd values, square them, then sort them" it is quite awkward. The idiomatic order also matches mathematical conventions: usually we write the value being defined on the left, and most functions have their arguments on the right: x = f ( y * sin √φ ) Is read as "x is defined as f of y times sine of the square root of phi". (Though mathematics has plenty of exceptions.) Notice that, when using monadic expressions, the convention shifts, because there we are concerned about actions in sequence, and again it follows spoken language, which states actions in time order: getLine &gt;&gt;= query db "select from foo where name = ?" &gt;&gt;= print This would correspond to "Get a line from the user, then query the database for matching records, and then print them." When combining both imperative (monadic) and pure code, this can seem strange if you try to over analyize it, but it actually is the common way of saying such things: getLine &gt;&gt;= print . map (map toUpper) . words would be read "Get a line from the user, then print the upper cased words of it." Trying to read any of the other ways of writing this as speech leads to a pretty tortured reading! 
It might be worth showing you can still get an intermediate representation with a different instance, but you can't show everything.
I have bad news for you. There is even more to learn.
See John Hughes' 85 paper: [Lazy Memo Function](http://www.cse.chalmers.se/~rjmh/Papers/hughes_85_lazy.pdf), where among other things he discussed an implementation directly into the garbage collector. So, yeah, "not too difficult", but there has to be a reason why Haskell (and GHC) has not picked up this idea, especially given that Hughes was among the first designers of the language. 
Which hp packages are tricky on windows? When I've tried to advocate for the hp to include hard to install packages I was told that is not the point and that the hp maintainers don't have the resources. So, I agree it would be nice, but as far as I know (and I only glanced at the package list) the current package set is easy to cabal install even on windows.
I like the idea of tagging knowledge like this, however the in-lined examples are a bit lacking (understandable, given there's so many topics). Have you considered open sourcing the site so people can contribute more articles for the "See also" sections?
There's rarely a reason to nuke ~/.cabal, is there? But yeah, it's probably more elegant not to hijack ~/.cabal for installing these binaries.
If you want the latest language extensions, you have no choice but raw GHC. Luckily I don't have to compile it myself.
FWIW: I didn't write this article, just submitted it here.
Adding join as a method in Monad is quite a disruptive change since it causes name clashes if you gave defined join yourself. I changed many hundred modules in our code when join moved. The changes for MonadFail was limited to a few instance declarations. 
I'd be happy to help :)
oh. thanks for correcting me.
fwiw, `PatternGuards` are enabled by default in `Haskell2010`
Also network. Basically anything requiring external c/c++ dependencies. A very common problem is when multiple incompatible unix tools are installed on the path (from ruby, python, perl ...). Then hard to debug errors occur when building with haskell.
copyright assignment does not even work legally in certain parts of the world (ask the fsfe).
Perhaps add the 'assert False undefined' trick to the section on bottoms, that way you get a location when it is hit
Haskell already have memory problems as to ask it to remember new things. I'm joking, but the problem I see is that automatic memoization could have a (not easy to estimate) impact on memory usage.
The sources are on GitHub: https://github.com/sdiehl/wiwinwlh
FYI: There was a [related thread](http://www.reddit.com/r/haskell/comments/23224w/minimal_haskell_platform/) 9 days ago, with relevant comments.
Cabal runs `readDesc`, but not for `sdist`. It's a bug in Cabal, see [#630](https://github.com/haskell/cabal/issues/630). The fix in Cabal seems easy to me, basically you'd just need to replace `readPackageDescription` in `Distribution.Simple.sdistAction` with the `readDesc` hook. Looks like this issue was forgotten. Consider commenting on it, to give the Cabal maintainers a little remainder. Meanwhile, you can add your custom source files in `sDistHook`. This little example adds all files from `git ls-files` to the source distribution. import Control.Monad (liftM) import Distribution.PackageDescription (PackageDescription(extraSrcFiles)) import Distribution.Simple import System.Process (readProcess) gitFiles :: IO [FilePath] gitFiles = liftM lines (readProcess "git" ["ls-files"] "") addGitFiles :: PackageDescription -&gt; IO PackageDescription addGitFiles pkgDesc = do files &lt;- gitFiles return (pkgDesc { extraSrcFiles = files ++ extraSrcFiles pkgDesc}) main :: IO () main = defaultMainWithHooks customHooks where customHooks = simpleUserHooks { sDistHook = sdistWithGitSourceFiles (sDistHook simpleUserHooks) } sdistWithGitSourceFiles sdist pkgDesc bi hooks flags = do pkgDesc' &lt;- addGitFiles pkgDesc sdist pkgDesc' bi hooks flags I don't know whether `extraSrcFiles` is used in any place other than `sdist`. The Cabal documentation suggests that it isn't, but to be on the absolutely safe side, you may want to extend `readDesc` hook likewise, so that `extraSrcFiles` is consider for `cabal configure` and `cabal build` as well. I hope this helps.
Otoh, I like to use `pure` in a lot of places, because I find it a more indicative name that something "pure" is happening. I think it's really just a matter of style, as you say.
 a = q &amp; filter odd &amp; map (\x -&gt; x*x) &amp; sort &gt; and try to speak it in that order: "a is take q, then take the odd values, square them, then sort them" it is quite awkward. ... or "q's odd values, squared and sorted" :) 
Great work. I really seems like you are doing it the right way: no changes to the source files, no strange plugins, just choose that font if you want the eye candy, use a "normal" font otherwise.
Memoization can also have removal policies, although I can't think of a way to do it purely. [stable-memo](http://hackage.haskell.org/package/stable-memo) (which uses unsafe functions in its implementation) will remove mappings for keys that get garbage collected, and you also have the option of only keeping mappings around while both the key and the result happen to still be in the heap anyway.
This is what I worked out. Basically I got to your `errorProne`. Not sure why you think it's error prone though. Will some step fail to satisfy laws if you don't pass the same setter in both times? I don't think so. I doubt there's any way of getting the types to match up with only one argument. Lenses are like type level functions but you can only pass them in evaluated at one type level value at a time. * Step 1: notice that `mapping` actually has *two* functors, so you're going to need two setters. * Step 2: Write what you want (`mappingL`) as `mapping` is written but replacing `fmap` with `over ...`. mappingL s1 s2 k = withIso k $ \ sa bt -&gt; iso (over s1 sa) (over s2 bt) * Step 3: Check the type signature. Is it what you want? *Main&gt; :t mappingL mappingL :: (Functor f, Profunctor p) =&gt; Setting (-&gt;) s1 a1 s a -&gt; Setting (-&gt;) b1 t1 b t -&gt; AnIso s t a b -&gt; p a1 (f b1) * Step 4: Yes it is. Neaten it up. mappingL :: ASetter fs fa s a -&gt; ASetter gb gt b t -&gt; AnIso s t a b -&gt; Iso fs gt fa gb mappingL s1 s2 k = withIso k $ \ sa bt -&gt; iso (over s1 sa) (over s2 bt) * Step 5: Rejoice! mappingFirst :: AnIso s t a b -&gt; Iso (s,r) (t,r) (a,r) (b,r) mappingFirst = mappingL _1 _1 
I honestly think that you (Stephen Diehl) should write a book. Intermediate to advanced haskell isn't covered very well out there in literature and you have given a pretty broad scope of it here. Your writing style is great, and your JIT compiler writeup is also really good!
Probably a typo: l :: Num t =&gt; t l = view _1 (100, 200) -- [100,200,300] I am pretty sure that this should result in `100`.
Why I think `errorProne` is error prone: `errorProne _1 _2 anIso` applies `anIso` to the first element of the tuple in one direction but to the second element of the tuple in the other direction, thus failing to be an isomorphism. Or do I fail to understand what an `Iso` is? If I want to use `mapping` but assure myself that I'm only using a single Functor, I can write mapping' :: Functor f =&gt; AnIso s t a b -&gt; Iso (f s) (f t) (f a) (f b) mapping' = mapping and then use `mapping'` instead. I can't get rid of the duplicate parameter to `errorProne` so easily.
I've always found it rather hard to understand what `Iso` means, but it seems your reasoning is unlikely to be correct given that `mapping` *itself* has this type mapping :: (Functor f, Functor g) =&gt; AnIso s t a b -&gt; Iso (f s) (g t) (f a) (g b) and thus applies the `fmap` of one functor in one direction and the other in the other. I don't think `errorProne` can be any *less* correct than `mapping`!
I agree that splitting out something like `MonadFail` can be done with relatively little pain with enough forethought. e.g. if we decide what we want to do during 7.10, put in appropriate warnings, and pull the trigger in 7.12. Even the `join` change was given that much time.
when replaying events, it does not execute any action it just read the result of the execution of the action from the log. The action is executed one single time. The replay simply read the log. result &lt;- step $ do talk with the user, store and retrieve databases etc step $ dosomethingWith result result comes from the log if the process is restarted and the process for example had a timeout in the second step. "step" itself determines either if the result is in the log already or not. If it is not the case, the action is executed 
more examples always would help but of course space and time are limited :) I've been playing around with the approach and so far am happy with it (seems to fit my needs better than GADTs do). Oleg's tutorial/lecture notes (http://okmij.org/ftp/tagless-final/course/lecture.pdf) is excellent and very readable. (link can be found from the top level link in the article)
I usually use the HP, but this last time I decided that I wanted GHC 7.8 so I installed everything from source, but it meant I had to install an older version of GHC from the ubuntu repos to do the bootstrapping. It definitely was more challenging, but a fun exercise.
Regarding 1 I would like it to be the case that the types always tell you what the function does but I wouldn't call it a rule. 3 and 4 can be fixed by using lens-family-core. For 5 you could make your own lens composition operator (there is probably already one) that composes the opposite way. 
Today I learned about the RWS monad. Guess it figures that someone else would've already gotten tired of stacking those...
&gt; However, since anyone can add their own ^^. operator I wouldn't count on it
Edward Kmett seems to defend his lens library by saying that people shouldn't be scared of mathematics. However, I don't think that's at all the problem. Personally, I take no issue with comonads, Kan extensions or lax monoidal endofunctors, they all provide good insights that are helpful for structuring code and reasoning. Similarly, the concept of van Laarhoven lenses is fantastic and a great starting point for a record system. I love all that stuff, and certainly Kmett has done a great service to everyone by promoting these ideas; for sure he is one of the most knowledgeable people around on those matters. However I honestly can't stand the lens library. Certainly the proliferation of arcane syntax like `^.`, `.~`, `%~` isn't helping. Concise syntax like that is fantastic when used parsimoniously, but here I don't see the pay-off even after going through the process of internalising what all these symbols mean. I'm hoping someone takes the ideas of the lens library and uses them to design a more elegant library that encapsulates the mathematics at play without obfuscation or desecration of Haskell syntax. I'm sure everything is crystal clear to Edward, but honestly [this](http://i.imgur.com/4fHw3Fd.png) is just nasty. Maybe there are a few mathematical insights that need to be uncovered before simplification is possible, but I refuse to believe that the concept of lens is at all as contrived as that diagram indicates.
The issue of type signatures, in my opinion, comes from the fact that most users find continuation passing style transformed types hard to reason about. This is why there are so many type synonyms (to hide the CPS) and why it's not intuitive to see how lenses compose, and also why lenses compose backwards. Familiarity with CPS makes lens drastically easier to use, and I often use GHCi to query unsynonymed type signatures to figure them out better. Is CPS unidiomatic Haskell? I personally disagree. CPS is perfectly natural to me. Furthermore, I find lens makes certain tasks *more idiomatic* in Haskell. Like working with complex state in a monad, and deep tree updates. The issue of dependencies is a matter of politics and history. If Edward could push more of the common library support into their respective dependencies, then lens wouldn't have such extravagant dependencies. Had lens been around before all these libraries, maybe it would have been easy. Now, it's an uphill battle.
&gt; 3 and 4 can be fixed by using lens-family-core. Sure, most of the problems with `lens` can be "fixed" by using a different lens package, but that doesn't change `lens` that just avoids it :)
It's worth noting for 5 that one of the most common Haskell operators (&gt;&gt;=) is also backwards (left-to-right instead of right-to-left).
The difference between Lens/Yesod and what Roman calls idiomatic Haskell is a matter of taste. I.e. my reaction to Lens and Yesod is "ewww!" But it is just that, taste. For every valid argument for one style you can find a valid counter argument for the other style. The user-base of Haskell is growing and with it comes differences in tastes, everybody will have to live with that.
&gt; desecration of Haskell syntax Now that is a strong way of putting it! &gt; but honestly this is just nasty. Edward would probably do good in placing that diagram somewhere less prominent; all it seems to do is attracting undue attention. It is not like you need to know everthing in it in order do anything with `lens`.
&gt; I just hope that someone will write an idiomatic Haskell library as powerful as (or close to) lens, with perhaps a different set of compromises made. Before writing lens, didn't Kmett write a similar library with different tradeoffs? I remember he said something like that at the beginning of his [lens talk](http://vimeo.com/56063074). I'll re-watch it tonight and update this comment.
My experience has been similar. The one time I realized I needed a lens utility function not in lens, I expanded out all the synonyms and then found it pretty straightforward to write. It has also been my experience that CPSed Haskell is less common, and usually an optimization. Maybe it is lack of familiarity, but I find it harder to understand CPSed parsers vs State monad based parsers. 
I think that operators should only be used if they are composition operators in some category. I am saying this as somebody who has violated my own rule and regretted it. This rule tends to eliminate most gratuitous use of operators and keeps the good ones.
I think the biggest problems are 1 &amp; 2. The documentation makes an effort to correct this by pinpointing specific examples of what the type signatures translate to, but having lenses be *so general* is both a blessing and a curse at a certain point. For example, the `%=` operator is defined like so: ``` (%=) :: (Profunctor p, MonadState s m) =&gt; Setting p s s a b -&gt; p a b -&gt; m () ``` But really all it does is `modify` a specific piece of an overall state in a `MonadState`. It makes more sense if you tack on a specific piece of such a state that you'd like to modify, for example: ``` (_1 %=) :: (Field1 s s a b, MonadState s m) =&gt; (a -&gt; b) -&gt; m () ``` But even then, what is `Field1`? I really like the utility that `lens` provides and I use it a lot when putting together moderately complex algorithms, but it can be really tough to finagle the types to correctness given the overly general type signatures. But again, `lens` wouldn't be what it is without such generality. It's definitely a trade-off. 
&gt; For 5 you could make your own You can make anything your own. The point is that there's already ~~poin~~`(.)` with defined semantcs. Lens redefines it in unnatural way and arguments that by "it's like in java". (I wouldn't think there are technical difficulties with that, right?) And I like jokes, but April the 1st passed and it's still there. 
Everybody who agrees with this needs to check out the [lens-family-core](http://hackage.haskell.org/package/lens-family-core) and [lens-family](https://hackage.haskell.org/package/lens-family) libraries which cover most use cases and are much simpler. They are also mostly backwards compatible with lens.
The uninuitiveness of types (issues [1] and [2]) should be solvable with improvements to documentation and teaching materials (with [2] probably being more of a challenge than [1]). [4], the large dependency graph, is not as bad as it seems - beyond the two examples given by Roman, the `lens` dependencies are either core libraries everyone already uses anyway or category theory stuff required by the implementation. [5], backwards composition, can feel a little strange, but it is not an arbitrary decision - van Laarhoven lenses are what they are. From the five issues, I find [3] to be the most interesting one. When programming with `lens` it does seem like that for every core idiom in `base` there is a parallel universe version of it in `lens`. Fortunately, everything composes and so you can use both "languages" at the same time, but it can be a little awkward sometimes, specially when you combine lens composition and plain function composition in a single expression, and thus have to move your eyes in two directions to read the code.
The lens library certainly offers a lot of functionality and gives you a lot to learn. **1.** Let's work through what I needed to know to figure out exactly what this function could do without looking at the implementation (or ever having used it) Here is what I needed to know: `Traversable f` - `f a` is some type with a bunch of `a` that I can update applicatively without changing the overall shape of the value. `Prism'` - In lens a `Thing' s a`is a `Thing s s a a`. It means that the types don't change using this optic. `Prism` - In lens a `Prism s t a b` is an optic that supports two operations. `Prism`s are Optics that correspond to constructors in a sum-type. They can construct a `t` from a `b` and they can sometimes select `a` from an `s`. A simple example of this is the `_Just :: Prism (Maybe a) (Maybe b) a b` Prism. It is simpler to think of _Just having the type `_Just :: Prism' (Maybe a) a`. `APrism` - In lens the "A" version of an optic is the concrete version of the non-"A" thing. These are generally used in argument position to avoid having to use Rank-2 types. Any non-"A" thing can satisfy the type of the "A" thing. When reasoning about what the function does we can ignore the "A". So now we have `below :: Traversable f =&gt; APrism' s a -&gt; Prism' (f s) (f a)` Given a prism that effectively gives us two functions: `s -&gt; Maybe a` and `a -&gt; s` we get a prism for `f a -&gt; f s` and `f s -&gt; Maybe (f a)` Given that Traversable isn't going to change the shape of the list the only behavior I can expect is that this maps the "constructor" over the whole structure in the one direction and can only return a "Just" value in the return direction where all the `s` elements of `f s` were matched by the constructor. **2.** I really don't understand this point. People love generalizing and abstracting in Haskell. The trend is definitely moving away from clear, concrete types. **3.** Lens certainly has a lot of combinators. I recommend people deal with this by generally disregarding most of the operators lens defines. (At least at first) While they can be convenient in a pinch you don't need them. They are only short-cuts. When you do need to be able to read them there is a consistency to the naming convention. I could explain it but I think that it's outside the scope of this post. Fortunately for lens, very many of its combinators are just names from the base library with "Of" tacked on the end. Now you get to use your intuition for what the operator does and have it parameterized by an appropriate optic instead of fixed to a list or other structure. For the others the type is enough (once you know the basics) The key things to learn are the basic optics: Getter, Setter, Fold, Traversal, Lens, Prism, Review. There's a lattice that you internalize eventual once you realize that each of these types provides incremental features over the previous. [See Hackage](http://hackage.haskell.org/package/lens) * `Getter' s a` - Gets one `a` from an `s` * `Setter' s a` - Sets any number of `a` in `s` * `Fold' s a` - Gets any number of `a` in `s` * `Traversal' s a` - Gets or Sets any number of 'a' in 's' * `Lens' s a` - Gets or Sets one `a` in `s` * `Review' s a` - Can construct `s` from `a` * `Prism' s a` - Gets zero or one `a` in `s` / Can construct `s` from `a` You should start to see that these build on each other. **4.** Dependencies! Wow, so many of them. I know. They exist to avoid the orphan instance hell that other libraries can suffer from. The big dependencies are things that are in the Haskell Platform or were nearly (aeson). The little dependencies are clean implementations of various mathematical abstractions. The list can certainly be overwhelming. **5.** Backwards composition of lenses makes perfect sense when you think about them as modifying applicative update functions first and foremost, rather than trying to think of them as field accessors. **Conclusion** Haskell has a rich history of adopting powerful abstractions that are impervious to those new to them. I watch new users struggle with "What is a Monad?" "A Monad is an Applicative is a Functor? Which one do I need?" "Where did this &lt;*&gt; operator come from? I can't search for it!" Thinking of lens as a new mini-language doesn't seem that far off, but with a little guidance, I don't think that it has to be that overwhelming.
Wasn't aware - thanks for the tip.
Perhaps one way to look at it is that lens has given us a number of new idioms, substitutes of which did not exist before. For example, we did not have an acceptably usable idiom for working with (setting, getting, using, updating, etc) deeply nested data, which lens makes very easy to do. With this kind of standard-setting, it is not surprising that we've had to learn some new patterns. A lot of the mentioned "shortcomings" seem to have to do with implementation details that could not be avoided, either for usability or performance reasons, if you want to stay writhing the bounds of existing compiler features of GHC. I'm not sure what can be done here without losing the overloading power. I'm sure nobody would lose sleep over lots of power for free - trouble is in making it work! Of all the things, types for me are the big loss when using lens. I can no longer pick an arbitrary function and immediately see what it must be doing from its type (save for the most common ones which are now familiar). Instead I rely on examples. A reference card with a clear explanation for *each* type might perhaps be a nice aid there (but with Haskell-layman explanations and not just mathematical buzz words). In general, I am so far very impressed with lens (it's hard not to be) and look for opportunities to use it in my work. I miss it when I don't readily have it (eg in Haste, where I've been hand-writing lenses using lens-family). 
I'm a big fan of what `lens` allows me to do. I have some concern about the best way to present the functionality, but I've no doubt it will all be worked out with experience. My repsonses to the article: &gt; below :: Traversable f =&gt; APrism' s a -&gt; Prism' (f s) (f a) &gt; &gt; Despite having some (fairly basic) understanding of what prisms are, this signature tells me nothing at all This is not true. If you understand what a `Prism` is (and you understand that that `APrism` is just a way of passing a `Prism` as an argument whilst avoiding `Rank2Types`) then you understand what `below` does. A `Prism' s a` represents the fact that the type `s` is a sum type with `a` as one of its summands. If we have that, how do we represent the type `f s` as a sum type with `f a` as one of its summands? Well, the summand is that all the elements of the `Traversable` (of type `s`) are the `a` summand. &gt; A package implementing lenses that depends on a JSON parsing library and a gzip compression library sounds almost like a joke to me. This is the conflation of two meanings of the word "depend". `lens` "depends" on a JSON parsing library and a gzip compression library in the "cabal" sense that it compiles against them to provide features for them. It does not "depend" on their functionality to provide its own functionality. &gt; Backward composition of lenses. It’s a minor issue, and I wouldn’t mention it if it wasn’t a great demonstration of how lens goes against the conventions of Haskell. I was somewhat unsettled by this, but I've just come to live with it. It's not a big deal. Furthermore, as shachaf pointed out on `#haskell-lens`, it's not clear what "backwards" really means. &gt; (map . fmap) (+1) [('a',1),('b',2),('c',3)] -- is this composition backwards? The `map` applies to the list whilst the `fmap` applies to the tuples. This is "backwards" from what we would expect if we were extracting one of the elements of one of the tuples (e.g. `fst . head`).
Do you mean "if they are associative"?
In my experience any monadic code with sufficiently complex flow turns into CPS very, very fast. Just for the reason that it's trivial to pass continuations, and analysing return values all the time is ugly.
It doesn't redefine it. It uses the usual `(.)` operator.
Another point to bear in mind whilst we're criticising the library: The lens developers are not a bunch of cowboys who have thrown a mishmash of a library together without a care in the world. In fact a lot of thought has gone into making difficult tradeoffs. Before you criticise an aspect of the library please make yourself aware of the disadvantages of doing it another way.
The main caveat is that you'd want to flip quite a lot of functions ( `^.` for starters) in order to make it more pleasant to read. In any case, using `(&gt;&gt;&gt;)` would be a fun thing to try out.
You're right, I missed slideshow.md the first time around, which is where the meat is.
Lens takes me back to when I first learnt programming. I went through a phase where I had no real understanding of syntax or programming in general and I basically copied and pasted stuff (not always literally, sometimes from my head) into a file and bodged different bits together until it worked. For me lens is the same - it's really cool when it works but because I have no understanding of the types as soon as it breaks I'm stuck. I do the equivalent of hitting the back of an old telly and hoping it works. Now obviously this is mainly my fault - if I put some effort into learning the library and all the types and classes and whatnot I'd be a lot better off. The problem with this is it is really complicated due to the type synonyms and the operators which make me have flashbacks to perl. For things like Wreq and some other libraries it may be acceptable to have a lens interface because so many of the common use cases are easily covered by tutorials and it makes the code much simpler. However it's still a bit of a black box and one that will bite me if I ever don't get the types right. This isn't too much of a criticism of lens, more of a criticism of lens being forced (wrong word, I admit) on beginners. I think it is extremely clever and extremely useful. Unfortunately it is/feels too clever for me to keep in my head and I think others may feel the same way. Hope this wasn't too harsh.
Do mean something like: &gt; isoC = iso (fromEnum :: Char -&gt; Int) (toEnum :: Int -&gt; Char) &gt; over isoC (+1) 'm' 'n' &gt; over (_1 . isoC) (+1) ('m',99) ('n',99)
Ironically, I think the type signature you used as an example was one of the most understandable in the library.