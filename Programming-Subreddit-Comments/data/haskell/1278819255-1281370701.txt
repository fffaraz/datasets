This is tremendously awesome. It's been a concern in the back of my mind for some time that while it's definitely possible to make Haskell perform on a level comparable with C (e.g., see shootout results), there seems to be a very fragile feeling to it, where one is never quite sure whether one's program will be fast and efficient, or perhaps eat up several gigs of memory and then die with a stack overflow. We see very impressive sleek "high level" code that runs with amazing performance in various people's work on fusion, but there's always the sense that the example was chosen to just happen to hit the optimizations in just the right way. Neil's work here feels to me (and felt, back when I first saw him talk about -O9 at AngloHaskell many years ago) like the first really serious shot at fixing that problem, and getting not just good performance numbers for the problems people identify and set out to solve, but good robust performance in code written by average programmers who've never read about the internals of the compiler or library rewrite rules. And now it looks like he's made it quite practical as well. Very exciting!
&gt; one is never quite sure whether one's program will be fast and efficient, or perhaps eat up several gigs of memory and then die with a stack overflow I don't think people working regularly in Haskell have this issue (at least, anecdotally from work). The error bars are smaller.
Think of it like this--what does this pattern match do? match 11 with | x -&gt; ( . . . ) It binds the value 11 to the identifier `x` within the scope of the expression to the right of the arrow, right? Binding in general can thus be regarded as just a degenerate form of pattern matching, so why not combine the concepts? Also keep in mind that although Haskell is covered in a thick, crunchy layer of syntactic sugar, pattern matching is one of the very few fundamental building blocks that everything else is defined in terms of.
You are right that I exaggerated quite a bit. It would, though, IMO be hard to deny that performance is a lot less predictable in Haskell than in most other languages. I'd also mention that people working regularly in Haskell are currently taken overwhelmingly from the top 20% of programming ability. If Haskell continues to fail at avoiding success, this will change...
I agree, this is excellent work and a step towards easier adoption by new developers. I'll play with Supero a bit when I get the time :)
I haven't written any big serious, pieces of software but I usually assume that if I write things in a clear functional style, using standard library functions, and using sound algorithmic thinking, that GHC will do a good job making it efficient. Seems to be the case in my experience. Even things like strictness annotations that I pre-emptively place seem to end up being redundant far more often than not, when optimizations are turned on. 
I was hoping for something that showed some degree of insight...
Nice diagram, you know I wonder why there is no visual programming language based of off arrows that would generate Haskell code.
I actually prefer that "fun foo = function | [] =&gt;" syntax to haskell version, in haskell you repeat yourself or name things that are immediately pattern-matched. and he's not talking about monomorphism restriction but value restriction. you can get rid of the first one in haskell but you cannot get rid of the second one in ML.
For those playing along at home, what fas2 meant to say was projecteuler.net. 
Oh yes, sorry.
I think Harrop has said elsewhere that he is banned from /r/haskell.
I'm really glad to see yampa actively used. There was a time when I thought that arrow based frp and any superior research would die out and people would just continue to use there imperative gui bindings.
By the look of his latest blog [posts](http://fsharpnews.blogspot.com/) maybe he's got his hands full with mathematica...
I believe those rules can be derived from the free theorems of zipWith and foldr. It's pretty amazing the amount of program transformation that can be done just using definitions plus free theorems. This might make for an interesting Haskell refactoring tool.
Well, I guess I don't see why this is strange (or, perhaps it is more precise to say I don't see why this is a value). This has been possible for a long time. Here's some complete code (with no imports) that makes the same definition of `test` possible: class IntBool a where intBool :: a instance IntBool Int where intBool = 42 instance IntBool Bool where intBool = True This confusion between "value" and "polymorphic set of values, one for each type" was the reason for the dreaded MR.
Small typo - the last line of your example should be :- &gt; instance IntBool Bool where intBool = True 
Regarding the strictly enforced ordering of functions/arguments/files, which was a wart that particularly bugged me, I got some [interesting answers on SO](http://stackoverflow.com/questions/3162387/why-is-fs-type-inference-so-fickle)
It's strange because it's ad-hoc polymorphism on the type of a function's *result* (or on the type of a non-function term, but it's mostly the same thing in practice). For people used to C-style function overloading, which ignores return types, it does look pretty magical. I suspect that perl's "function behavior depends on calling context" stuff causes a similar reaction.
So I was wondering about such a tool and it actually exists, unfortunately I can not find where to download this tool or find the source.
&gt; although Haskell is covered in a thick, crunchy layer of syntactic sugar, pattern matching is one of the very few fundamental building blocks that everything else is defined in terms of. And let-binding is another primitive :) Under strict evaluation there's no need to distinguish let-binding from case-analysis. However, under lazy evaluation they have different semantics regarding when evaluations are forced; let-binding introduces lazy sharing, whereas case-analysis introduces dispatch for forced evaluation. This is one of the reasons why Haskell's lambdas resemble `let` and Haskell doesn't have the `function` keyword of ML-like languages. Sure, we could add some sugar for case-analyzing lambdas (and I'd like that), but it'd still be sugar. The only way to make it non-sugar would be to introduce a primitive distinction between strict lambdas and lazy lambdas, which would lead to rearranging the other primitives in order to reduce duplication. Not necessarily a bad thing, just different.
&gt; It binds the value 11 to the identifier x within the scope of the expression to the right of the arrow, right? Binding in general can thus be regarded as just a degenerate form of pattern matching, so why not combine the concepts? No, it forces the evaluation of an expression (namely 11) and binds `x` to the value resulting from that evaluation. Let-binding allows you to bind identifiers to unevaluated expressions in order to defer their evaluation.
I don't know what language you're talking about, but in Haskell: case e of { x -&gt; ... } does no evaluation of `e`. Evaluation is only caused if you have a (refutable) pattern. And I'm relatively sure that in most eager languages (ML, OCaml, F#), `let` causes evaluation, so the distinction doesn't exist there, either. Your other comment on this level has some errors, too. For instance, one reason that: let v = e in e' is different from: case e of v -&gt; e' is that the types of let-bound identifiers are subject to generalization. So: case \x -&gt; x of f -&gt; (f (), f 'c') -- also (\f -&gt; (f (), f 'c')) (\x -&gt; x) is a type error, while: let f = \x -&gt; x in (f (), f 'c') is not. Doing the same thing with `case` requires non-Haskell98 types. Also, `let` is recursive in Haskell, and definitions that make use of that cannot be translated to `case` without using `fix`. None of this has to do with evaluation order, and indeed, there's nothing `let` can do evaluation-wise that cannot also be done with `case`. In fact, the operational semantics of `let` is given in terms of `case` and `fix` in the Haskell 98 report (type-wise, that wouldn't work, though).
If the expression `case e of { x -&gt; b }` is forced, then the evaluation of e is forced regardless of whether `x` is used in `b`. If, however, the expression `let x = e in b` is forced, then `e` is only forced when it is necessary for reducing `b` to WHNF. Whereas in strict languages, the expression `let x = e in b` will force `e` prior to forcing `b`, which gives the same semantics as case-analysis but different semantics than lazy let-binding. Haskell has some odd corner cases with variable-patterns vs constructor-patterns and lazy patterns, but that doesn't affect the fact that case-analysis and let-binding serve two fundamentally different roles. The generalization of types in let-bindings is neither here nor there, and is a very language-specific matter in any case.
&gt; If the expression `case e of { x -&gt; b }` is forced, then the evaluation of e is forced regardless of whether `x` is used in `b`. No, this is false. It is true if you are talking about `case` in the GHC core language, but in ordinary Haskell, that case statement does no evaluation of `e` unless evaluation of `b` actually causes evaluation of `x` (or `e` itself possibly). It's quite easy to prove, too: Prelude&gt; case undefined of x -&gt; 5 5 it must be this way, too, because otherwise the translation given in the report would be wrong. One of the rules is: let p = e1 in e0 ==&gt; case e1 of ~p -&gt; e0 -- when the let is non-recursive If case statements always caused evaluation regardless of the patterns, then that would be an incorrect definition for: let x = undefined in 5
This tantalizing set of slides from what was apparently a fantastic talk raises the usual question: is there a video? (or at least an audio?)
...show me the code or it's a makeup.
Just a data point -- the most frequent cycle-waster in my naive code (for certain applications in a certain idiom at least) is realToFrac, which is just disastrous form a performance standpoint. I haven't investigated the alternative package, to see if it is the "right" thing, but what we have now is certainly almost never right.
Well apparently it's [here](http://www.csse.uwa.edu.au/~joel/vfpe/) but this looks like a completely different tool to me, I'm just waiting to get a reply from the person who gave me that link originally.
Hmm. realToFrac should be a no-op (or at least efficient) except for some more unusual type conversions. Would be good to file a bug.
It's [here](http://sourceforge.net/projects/frvr/) as described in [this post](http://imve.informatik.uni-hamburg.de/projects/FRVR)
I did look there but I couldn't find the source for the tool itself.
Check the [repo](http://frvr.svn.sourceforge.net/viewvc/frvr/) not the downloads page.
Yes I did check there, I couldn't find it, I can see the frvr project but not the tool for visual programming.
You've left me in despair.
why is that?
Start here, and follow the posts of Simon Marlow and John Meacham on Haskell Cafe for the most important parts of the thread. The full thread wanders between the Haskell Cafe, Haskell, and Haskell Prime mailing lists, with partial cross-posting.
That is stunning.
I really like the idea implemented in jhc that John describes here: "The underlying allocator is based on Bonwick's slab allocator[2] which works quite well for a haskell runtime, I have a slab for each type, so a slab of 'cons' cells, a slab of size 3 tuples, and so forth." http://www.haskell.org/pipermail/haskell-cafe/2010-July/080258.html 
Another great perl from this thread: "thinking about computational complexity in terms of O(cache line fills) rather than O(operations) is much more appropriate on todays architectures" 
Are there any fonts for OS X that support the HANUNOO LETTER RA character? Not even Code2000 has it.
Next: Writing QuickCheck properties to test stuff like the monad laws, by drawing ASCII-art [commutative](http://www.haskell.org/sitewiki/images/1/1f/MonadAssociative.png) [diagrams](http://www.haskell.org/sitewiki/images/5/5d/MonadUnit.png).
I was talking about Core, because you were talking about primitivity after getting rid of all the syntactic sugar. The H98 definition is merely one of those corner cases I mentioned in the previous comment. You'll note that the desugaring requires a lazy pattern in order to override the strictness of regular case-analysis and to reincorporate the lazy sharing that let-binding provides. That desugaring rule is necessary for handling pattern bindings in the correct way, but it says nothing really about primitive case-analysis nor primitive let-binding. In a broader PL theoretic setting, lazy pattern matching is rare and understudied compared to the long tradition of strict pattern matching. Few languages other than Haskell enjoy lazy patterns. The H98 report presumes a certain set of primitive operations that everything desugars into, but there is nothing magical about the specific set of primops they chose, nor about the primops GHC uses to implement them. Rather than having a strict case-analysis and lazy let-binding, they could have chosen lazy case-analysis (perhaps integrated with lambda-abstraction a la OCaml's `function`) and a `seq` primitive for forcing evaluation. So long as you have abstraction, application, lazy sharing, analysis, and evaluation it doesn't matter (formally speaking) how you chose to divvy them up.
I think my favorite part is where he says, "Nothing too abusive here", followed by a list of 101 language extensions.
I'm especially excited about the Clean paper because I haven't been following what they've been up to, and there's always great stuff there to learn/borrow from. As far as work I haven't heard about before, Invertible Syntax Descriptions also sounds very promising. Edit: Also, lots of performance stuff! :-)
When I saw yesterday Simon asking &gt; I haven't looked at the new jhc yet, but I have a question about the GC support: is it conservative or accurate? If accurate, how are you finding the pointers - a shadow stack? I thought the following exchange should be interesting... Thinking of this, the Supero paper (Rethinking Supercompilation) mentions the use of GHC Core for the back-end. It could be interesting to be able to go from Jhc Core to GHC's and vice versa.
Are those papers all already available? (I know some of them are.)
Are we soon going to see "Functional Rubies"?
I'm using dejavu font in emacs. there is no font setting (that I know of) in html, so it probably uses default one in browser. it worked in chromium and ff on gentoo and ubuntuu so I thought it was ok. it's hard finding these symbols. mapM_ (\c -&gt; putStrLn [c]) $ filter isLetter $ map chr [0..alot] then you have to do filter "does it look like a bar" on 50k symbols. and not all of them work in ghc (probably a bug)
Where is the GPU paper? I cannot find it
John has already done this - in particular I believe he at one point had a pass that would take his E representation (a variant of PTS as he says) and convert it to source code for GHC to consume (that is, haskell source, not core.) Of course there isn't really a great way to represent PTS in Haskell, so the result is full of `unsafeCoerce#`, but John said at one point it did work. I remember seeing it and trying to make it work back when LHC was a fork of JHC, but we got rid of it as we deleted a bunch of code. I'm not sure if it's still in John's source repository, but you may be able to find it in older releases...
finally! I've managed to impress dons:)
you can't compare disabling monomorphism restriction to using undecidable instances. I think I've explained the need for the more edgy extensions.
Wow, lots of excellent papers here it looks like! In particular: &gt; * Hoopl: A Modular, Reusable Library for Dataflow Analysis and Transformation A very good paper, but there needs to be more documentation on using hoopl than just the paper. :( We would like to use it in LHC, but besides this one paper, there is almost no hope to deciphering the library from types alone (maybe I should try and contribute hackage docs...) &gt; * A Systematic Derivation of the STG Machine Verified in Coq Oh my, perhaps now we can have a verified implementation of GHC's core? :) &gt; * Scalable Event Handling for GHC Can't wait until this is finished. &gt; * Nikola: Embedding Compiled GPU Functions in Haskell OK, I'm not even going to lie here - I've read like, 4 papers on GPU-based stuff in Haskell (OK, so there's the Obsidian work, Manual Chakravarty's work on a GPU-based `accelerate` library, and a few others I know for sure,) and yet despite that and all the promises, *there is no library or compiler or implementation to be found anywhere for any of them*, so while I'm sure the paper will be interesting, I'm not going to be really impressed until I have software I can build on my own machine. Seriously, if anybody involved in projects like this is listening - **I will write patches for you to get it working for me if need be**, but there's no hope of that if I *never* get to see the code. &gt; * Supercompilation by Evaluation Oh my. :) &gt; * Invertible syntax descriptions: Unifying parsing and pretty printing This just sounds really, really nice.
ACM will republish all of these but until then some are already available: * [A generic deriving mechanism for Haskell ](http://dreixel.net/research/pdf/gdmh_draft.pdf) * [Hoopl: A Modular, Reusable Library for Dataflow Analysis and Transformation](http://research.microsoft.com/en-us/um/people/simonpj/papers/c--/hoopl.pdf) * A Systematic Derivation of the STG Machine Verified in Coq * [Species and Functors and Types, Oh My!](http://www.cis.upenn.edu/~byorgey/papers/species-pearl.pdf) * Experience Report: Using Hackage to Inform Language Design * [Concurrent Orchestration in Haskell](http://code.galois.com/paper/2010/Hask-Orc-DRAFT.pdf) * [Scalable Event Handling for GHC](http://www.serpentine.com/bos/files/ghc-event-manager.pdf) * [Seq no more: Better Strategies for Parallel Haskell](http://www.haskell.org/~simonmar/papers/strategies.pdf) * [The performance of Haskell containers package](http://research.microsoft.com/~simonpj/papers/containers/containers.pdf) * [Nikola: Embedding Compiled GPU Functions in Haskell](http://www.eecs.harvard.edu/~mainland/publications/mainland10nikola.pdf) * [An LLVM Backend For GHC](http://www.cse.unsw.edu.au/~chak/papers/TC10.html) * [Supercompilation by Evaluation](http://research.microsoft.com/en-us/um/people/simonpj/papers/supercompilation/supercomp-by-eval.pdf) * Exchanging Sources Between Clean and Haskell - A Double-Edged Front End for the Clean Compiler * Invertible syntax descriptions: Unifying parsing and pretty printing
Typeramming. That is all.
Embeddeducible, Garbagenonym, Datatypendency, Studlymonators. Some good stuff. (The usual wading through crap to get there, though.) Edit: You know, Studlymonators wouldn't be a half-bad name for a Haskell club.
Shariables &amp; Garbageratypes
um, neither? why not try thinking of something a little more creative / catchy?
Oleg said that using CPS might provide performance benefits (he didn't test it), but when I ran some benchmarks on file reading/parsing the IterateeM.hs ran significantly faster than IterateeMCPS.hs when compiled with -O2. CPS was faster when using -O0, but that's a bit self-defeating. I'm not sure why this is the case, since CPS has apparently provided performance benefits in other cases, but it might just be that iteratees involve creating/deleting too many temporary closures to take advantage of CPS's benefits.
All I get is a request to log in. Is there a publicly accessible copy of this announcement somewhere?
[written by Johan Tibell] All the tests under libraries/unix/tests pass (except for an unrelated failure related to bug #3816). The last problem was that the I/O manager state was copied to the child during a fork. This meant that the child inherited the running status (i.e. running) of the parent even though the I/O manager wasn't actually running. Since we don't need to share any state between the parent and the child I simply created a brand new I/O manager in the child. Note that the old I/O manager did this incorrectly by inheriting pendingEvents and pendingDelays from the parent even though there were no longer any threads waiting for the file descriptors/MVars in these lists. If we do want to keep the parent state in child for some reason we could change the code so it rearms all the file descriptors (by making e.g. epoll syscalls) after a fork. Unfortunately forking isn't handled correctly in the case of e.g. epoll so the kernel state needs to be manually rebuilt (at least this is what libev does). What remains now is to * do a thorough code review, * improve the performance of the poll backend so it matches the old I/O managers select backend (to avoid performance degradation on any platforms), and * bash on the code to make sure there are no lingering bugs. The two attached patches (to GHC and to base) contains all the necessary changes and can be applied to the current GHC HEAD. 
This is actually solving a major problem in computer science: *"There are only two hard things in Computer Science: cache invalidation and naming things"* -- Phil Karlton
boxes-and-arrows
Thunctor.
Actually, the strictness of case is one of the few points where the semantics of Core is not simply that of a subset of Haskell. In Haskell case only forces enough evaluation to determine which pattern matches, while the Core let forces WHNF (and may only match on the outermost constructor). In Core case and let indeed have distinct operational interpretation - case is the only point which forces evaluation, and let is the only point where allocations occur (let-bound functions may not require allocation). let and case are quite symmetrical in surface Haskell, and mix together all your ingredients - these are equivalent f pat1 = body1; f pat2 = body2; ... f v = case v of pat1 -&gt; body1; pat2 -&gt; body2; ... as are these case v of pat1 -&gt; alt1; pat2 -&gt; alt2; ... let helper pat1 = alt; helper pat2 = alt2; ... in helper v Pervasive laziness seems to lead to this sort of semantic collapse, like the way data and codata are unified.
I got that too, I clicked on the "sign in as a different user" link (which I assume clears a cookie) and then I was able to visit the google groups link unauthenticated.
Hey dons, Just want to let you know I appreciate all the effort you guys put into GHC. Language has come a long way since I started using it 2 yrs ago thanks to you guys.
Is there a summary of what this changes compared to what we're currently using?
Composurrying
&gt; perhaps now we can have a verified implementation of GHC's core? :) ...assuming it isn't radically revamped in the meanwhile? ;) Or has the verified version hidden away so you don't get to see it? :D 
The paper: http://www.serpentine.com/bos/files/ghc-event-manager.pdf
I'm posting the following on behalf of Ivan Miljenovic. "FGL" because it's the current name and don't see any need to think of a new package name, "inductive-graphs" because it's about, you know, inductive graphs ... Why, what name would you prefer?
Bottomorphism.
&gt; In Core case and let indeed have distinct operational interpretation - case is the only point which forces evaluation, and let is the only point where allocations occur Yes. I know. :)
Why does rolling a custom monad outperform Control.Monad.State.Strict so much? Is it just because of the compiler hints on the custom version?
I've always thought Scala might be a nice way of bringing some FP sanity to Java applications but haven't had the time to learn it yet.
Is there a place on haskellwiki where such advices are linked to? This is really nice to know.
Firstly, it fixes any inlining issues; secondly, specialization of the return type means it can be unboxed (polymorphic tuples won't unbox). There were several things in this problem that were fixed: * PureMT instead of StdGen * iterateM worker/wrapper * mapM/iterateM unnec. traversals. * Non-inlining of C.M.State.Strict * Specialization of return types
Despite all the other optimizations applied this still makes me wonder if a Template Haskell based monad library for creating specialized State/Reader/Writer monads will be a good idea.
&gt; using Scala I can do GUI programming, What about gtk2hs? I am using vty to create some console GUIs too. &gt; web programming (using the Lift framework), Are snap and Yesod not ready for prime-time? &gt; and program mobile devices (using Android, though I gather that it's not yet a trivial exercise to get Scala code running on Android). Does the GHC iPhone port not work on the Android too? I seem to recall a startup games company targeting the iPhone and Android with GHC. What gives?
I wonder whether supercompilation is able to do the most of these optimizations automatically.
We really do need a Java bytecode back-end for Haskell... If I remember correctly there were issues, but "How hard can it be?"(tm). Worth looking into during a weekend or two.
I'd go for inductive-graphs. It's not taken yet, and it makes it clear what the package is meant for. Besides if it's really a *new* library, I see no reason to give it the same name as the old library.
I always suspected that Monad Stacks ultimately end up making code both more confusing for me to read and for the compiler to optimize. I like what Hudak did in his book for his IRL (Imperative Robot Language) which was stored in a Monad that he constantly added to the monad's capabilities rather than stacking pieces he wanted from existing monads. Monads do not generally "compose" very nicely, and all the lifting to get to IO can get rather confusing. Also the order in which you run a StateT ReaderT IO () is a little confusing as well. I would REALLY REALLY REALLY love to see an "optimizing Haskell" book someday, but I'm not good enough at it to write it yet. :-) 
Has anyone else noticed that the phrase "best-of-breed" only shows up in Java contexts?
I'm working on an approach based on type families, actually. Though using TH is an interesting idea.
As far as I'm aware, there's no simple way right now to write Haskell code for the Android. The most promising direction seems to be generating native code for the ARM processor (LLVM makes that easy), and then dynamically building the Java piece from build-time metadata. But that still leaves the task of getting the RTS running on Android.
Now to use this to create a GHC runtime where all the really low level bits are in inline LLVM.
am I the only one with broken slides (those balloons have black background, so they cover the code)?
Hmm...
No, it seems that infoq has issues with transparent images. You can view these slides online at [scribd](http://www.scribd.com/doc/28336206/Multicore-programming-in-Haskell) or you can download a [pdf version.](http://www.haskell.org/~simonmar/multicore-haskell-marlow-qcon2010.pdf) 
Ugh.
Actually, it's more Enterprise-ish or Corporate-speak, than specifically Java. Although I can see why you would make that mistake. It's like even they think their software is a dog...
I wonder if using high-level strategies on lazy structures has much overhead over writing a combined function that uses par and pseq directly, or if the optimizer handles it nicely.
Perls? Rubies? This is *Haskell* we are talking about, maybe "functional diamonds" is better? (Done in the style of "This is SPARTA")
Don't get too excited, this is only for ghc to embed assembly fragments in the LLVM code it generates.
So it's not possible to write inline assembly in normal Haskell code? it would have been interesting for injecting SIMD instructions into Haskell. I hope that having an LLVM backend means with can have it some how at some point, either as intrinsics or something like Mono.SIMD.
Phew. I am terribly relieved.
Darn! I interpreted it to be the other way around. :-( Ah well. I'm sure that is useful too.
Just a question for people who have been watching the screencasts: any suggestions? It's hard to tell if the pace is right. Do people want more or less explanation of details? And is anyone dying to see a particular topic covered? I take requests ;)
It seems like we can solve this problem using associative types, e.g. class StateMonad s a where data State s a :: * -&gt; * -&gt; * 
Hahaha Don't want to use the "save" function, eh?
The "doing stuff in haskell" part of monads I kinda get. The catagory theory part of it always seems to stump me. This looks like a promising article tho.
I think the pace is pretty good. What I didn't like much was when you said you were typing everything out manually because you think it is more instructive that way (in Part 1) but then told us (a few minutes later) that we should just take these lines as given for now and not to worry about what they do. I think it would be interesting if you covered how to add a new type similar to NicHtml or JqueryDay since it probably isn't possible for you to provide any conceivable type in exactly the way we want it to be in Yesod itself. Maybe you could use a set of tags along with a special input field as an example. Another interesting topic to me personally would be form validation, possibly including server side validators called via AJAX. I have just been writing a Python web app a few months ago for setting up a server program at work; it had to check e.g. if a path, user or group exists on the server or how much free space is available, if a path is owned by a given user,...; the kind of thing that is impossible to validate client-side. Not sure if those are supported by Yesod already but it would probably be a topic that needs to be supported sooner or later anyway.
It would be great to see some way of defining middleware functions that can be composed to handle things such as sessions, cookies, authentication, xss-filtering, and related in a neat way appMiddlware = handleSession . handleAuthentication . xssFilter . handleAuth processRequest myRequestHandler $ appMiddlware httpRequest
See, this is what it's about. People should always strive to be this sexy. A+.
I've no idea what that means, but I'll take it as a compliment and say thanks!
&gt; the task of getting the RTS running on Android I'm working on this as we speak. If you want to chat about it, I'm kmc on FreeNode. &gt; generating native code for the ARM processor (LLVM makes that easy) Potentially -- for now, we're just using unregisterized via-C, like the iPhone port.
Clojure likewise.
I love the huge let down at the end. All that work for zero opportunities.
Free money -- you get what you pay for.
You can have middleware at the [WAI](http://hackage.haskell.org/package/wai) level. For example, gzip, JSON-P and path cleanup is all performed as WAI middleware. I'm not quite certain what exactly your other middleware examples would do, so I can't be certain if WAI middleware is the right answer.
&gt; I think the pace is pretty good. What I didn't like much was when you said you were typing everything out manually because you think it is more instructive that way (in Part 1) but then told us (a few minutes later) that we should just take these lines as given for now and not to worry about what they do. Point taken. There's a tension between trying to explain everything and getting it to fit within something less than an hour. I would recommend that if there are particular points you think need explaining, you ask the question in the comments on the page so other users can see the discussion. I think your idea of a new type is a good idea, but I would start at an even lower level: how the GForm datatype works, and build up from there. I definitely would like to build it up to the point where it explains declaring a JqueryDay yourself. In fact, I think when I do this, I'll make the example an AJAX-validating field. I've added it as an [issue on github](http://github.com/snoyberg/yesod/issues#issue/5). Thanks for the input!
Full Ack to the error message thing... the verbosity should be configurable, though.
See [the LLVM Haskell bindings](http://hackage.haskell.org/package/llvm) as well as [Potential](http://intoverflow.wordpress.com/2010/05/21/announcing-potential-x86-64-assembler-as-a-haskell-edsl/).
I did save it. I just wanted to say "Hmm..."
I've been investigating something similar, but using a simple bump allocator for the first stage and storing distinct tags (rather than types) in pages after that. That lets me get a cheap subtract/check-based allocator for the common code path, but adds the overhead of checking to see if the page the object lies is tagged to the already slow path of evaluation in the absence of tag bits. However, since most accesses don't care about the tag itself, just the dynamic pointer tagging bits in the reference, this only comes up when you're going to stall due to the indirect jump _anyways_ and so shouldn't add overhead to the common path. So, bump allocate, and then after the nursery collection move either to another bump allocated page (if it doesn't seem like there will be enough of a given tag to give it its own page) or move it into slabs by tag. The slabs have the benefit that if you allocate them only for tagged values rather than unevaluated thunks you can elide the forwarding pointer and use a nice bit-mapped parallel mark-compact allocator, which has very low storage overhead, either storing the bitmask in the BIBOP or directly at the start of the page. This means that things like cons cells start out at tag + fwd + head + tail = 4 words, and compress down to 2 words + a bit in a page mask where the page is tagged with 'cons'. You can do garbage collection separately per thread with this model as long as you occasionally stop the world to collect garbage that has been handed off to other threads, but that is a bit bigger discussion than perhaps is suitable for random rambling on reddit. ;) This costs another bit in the page mask to know what values have been shared and thus will have to have everything they reference pinned during collection, but ultimately lets you get away without using either a read or write barrier for the common case!
I'd really prefer, in the long run, that libraries implementing important data structures be given names that describe what they do, and *not* creative / catchy names. One person's "creative" name is another person's "I can't find the library I wanted to use." As Haskell now has way too many libraries for everyone to be familiar with, and is rapidly getting more, things like this are getting far more important. (Granted, at least the word "graph" would likely be in the description, unlike the infuriatingly-common-in-Hackage packages like "skjdfhs" with description "Haskell bindings for skjdfhs") As I said in my submission for the form, the maintainers should choose to see this as an opportunity to improve the choice of package name.
If it worked, do you think he would have shared it with reddit? =)
Solid article. I particularly liked the parts about error messages and teaching. **Content**: B+ (I really only read the article for the Haskell bit, so I found the first part superfluous, but if you gotta advertise you gotta advertise) **Quality**: A
Indeed, but the constraint on 'a' gets in the way: Specializable a =&gt; StateMonad s a 
There is Brian Alliet's thesis work on [LambdaVM](http://www.cs.rit.edu/~bja8464/lambdavm/), but it looks a bit out of date. It would be great if it were brought up to date though, and made available to the wider haskell community.
The GPU paper is available here: [Nikola: Embedding Compiled GPU Functions in Haskell](http://www.eecs.harvard.edu/~mainland/publications/mainland10nikola.pdf).
Jane Street Capital (www.janestreet.com) is hiring in London if you are looking for more ways to apply functional programming to finance... ;)
Man up and use vim already.
It's acceptable only if you were stroking a Unix beard while pondering the article.
&gt; Time to get the Mac and Windows installers ready, and to prep the distro packages!! Anyone know if there will be a "Haskell Platform" for Mac **x64** version this time around?
Well, now it has been shown to work, yes, I think we can do this. We need more Mac **contributors** though. Anyone want to set up buildbots?
&gt; Anyone want to set up buildbots? Well if its not to involved or rather complicated, I'd be happy to give it a try. Any particular instructions to follow?
You probably want to join #ghc, and ensure your Mac server is up 24/7. The detalis are here: http://hackage.haskell.org/trac/ghc/wiki/BuildBot
I hadn't intended it to be advertising, but now that I read it again I see what you mean. "Enthusing" was what I had been aiming for. Thanks for the feedback - I think that's the first time I've had a blog post graded! :-)
It's possible, but not for this one. If the GHC project starts spitting out 64-bit executables tomorrow, I'll switch -- otherwise I have to set up my environment to build GHC, and who knows how long that will take. As it stands the Mac installer for this edition is likely to be a little bit late.
Is there an option for non-24/7 contribution?
Lift is a much more mature web framework than anything Haskell has now. Yesod is making a lot of progress though.
I thought the haskell platform gave you ghc. &gt; Haskell Platform is now available, the first encorporating GHC 6.12.3: If this is the case, why does configure give me an error for not having ghc 6.12.3 (linux installer)? Is this just because it's a release version or do I need to have ghc 6.12.3 installed separately?
I can't speak towards the performance issues, but my recent experience using mtl to make and use a monad stack was pretty easy. The hardest part was putting together the type,but after that the type classes took care of the lifting pretty seamlessly. If I call *ask* it digs down until it finds the ReaderT, if I call *tell* it digs down until it finds the WriterT and so forth. What was the most confusing was trying to figure out how to work with both exceptions (from IO) and Errors from Control.Monad.Error. But that is less about the transformer stack than the variety of ways haskell has for dealing with things that might fail.
The src installer for unix requires you have grabbed ghc 6.12.3 from http://haskell.org/ghc , or your distro. Other installers typically provide ghc for you.
Have you seen Ben Lippmeier's Gloss library? He used it for teaching beginners at ANU and UNSW to good effect.
Thanks for the response :) When you say "other installers" do you mean there will be other unix installers which include ghc or that installers for other platforms include it? 
The Windows and Mac installers include it. The distro installers for Debian et al include it. We just don't produce a generic standalone installer yet that bootstraps GHC, and I don't think we plan to.
Courtesy Chris Done, http://imgur.com/cUuCW
I wonder what jdh thinks of this.
I miss his trolling...
looks fabolous! but joking aside, I prefer the old version (colors seem less.. hmm, less something), but it should be stretched to full screen, not just a tiny strip.
Here's my take on the new design: [screenshot](http://imgur.com/9LHvk.jpg) [live version](http://dl.dropbox.com/u/623671/haskell_platform_redesign/index.htm)
Nice work.
For future reference when linking to Google Groups, log out of Google, then link, e.g. http://groups.google.com/group/ghc-io-manager/msg/d8633642bfd2f845
BTW, candidate main site, http://code.haskell.org.nyud.net/haskell-platform/download-website/
Nice design!
Psst, it doesn't look so good if your browser is taller than it is wide.
Yep, that's been pointed out. Haven't got a good background to replace it, but if someone has a nice idea.
Alternative that solves the association of the three operating system with the three properties of the Haskell Platform: [alternative](http://dl.dropbox.com/u/623671/haskell_platform_redesign/index2.htm)
The left margin doesn't shrink anywhere near enough, so "cutting edge" cuts off early, but apart from that i thought it looked pretty nice.
Is there any chance the HP team could work with the macports GHC maintainer (gwright at macports dot org apparently) to get hs-platform correctly integrated? 'Cause macports is quite falling behind, main is still 10.4 and devel is barely ghc 11
Ubuntu seems a complete mess. I asked on #haskell (freenode) and was advised to install cabal-install from sources (version 0.8.2) after installing ghc6, libghc6-dev-http and libghc6-dev-zlib. I guess this doesn't give me Haskell Platform!
Fedora is nice. Not near enough packaged, but cabal2spec makes things extremely easy. Need to get around to packaging the things I use yet though :( (xmobar and dependencies among others).
I don't think there are Ubuntu maintainers.
CSS diffs welcome.
so, is ddc in a state, that llvm compiler seems like the best move? is language finished? is there interpreter implemented?
Gosh, that's a bit surprising. Certainly looks that way though.
Possible to clone the page's code from somewhere, or do it raw by downloading the files?
Where will the results be available?
Ubuntu will include the platform for the next release in October.
DDC is still alpha although Ben is working his way through the thing doing a major refactoring and clean up. We're doing the LLVM backend because the the C backend is a bit hackish. There are still a bunch of things that need to be added to the compiler to bring up to a stage where is even supports most of basic stuff that Haskell does. As for the interpreter, there are not even any current plans to do one. No one is against the idea, just too busy doing other stuff. 
darcs get http://code.haskell.org/haskell-platform The web stuff is in the download-website/ dir.
Neat! Doesn't work for me, though: parsing profiling results printing output html file visual-prof: Prof2Html.hs:(61,0)-(73,19): Non-exhaustive patterns in function toColour
that looks promising, however I get the following error when trying to run it: Prof2Html.hs:(61,0)-(73,19): Non-exhaustive patterns in function toColour
Oh yes, that's a small problem. The program should run long enough (at least 1 tick), otherwise it divides by 0. I will fix it to output some useful error.
Is there a sample output somewhere? (That I can see without checking it out from git.)
I WANT THIS FOR EVERY OTHER LANGUAGE
http://imgur.com/SHTUhGd
Awesome, I'll be trying it soon :)
"Hskjdfhs: Haskell binding for skjdfhs C library." fixed that for you :-)
Doesn't work on Windows at all. Says it could not find command ".". I suspect line 125 of [this file](http://github.com/djv/VisualProf/blob/master/Prof2Html.hs#L125): run ++ ".hs &amp;&amp; ./" ++ run ++ " +RTS " ++ mode ++ " -RTS" Perhaps there's a portable way to run an executable in System.Process?
1. Hash tables are made for lookup, not just insertion. They usually also offer delete. Presumably, hash tables in the wild are searched more often than they are inserted into. 2. floor is a lousy hash function for doubles, generally speaking, though I suspect in this case (sequential integers) it is not at all bad. libstdc++ uses (4.3.2, cleaned up for readability): size_t hash(double val) { if (val == 0.0) return val; const char * first = reinterpret_cast&lt;const char*&gt;(&amp;val); size_t result = static_cast&lt;size_t&gt;(14695981039346656037ULL); for (size_t length = 8; length &gt; 0; --length) { result ^= static_cast&lt;size_t&gt;(*first++); result *= static_cast&lt;size_t&gt;(1099511628211ULL); } return result; } With mono 2.6.4 and F# 2.0.0.0, and GHC 6.12.3, hashing doubles turning them into strings and hashing the strings (yes, I know) takes GHC about 34 second and F# about 58 seconds on my machine. Using floor, those times are 31 seconds and 5 seconds, respectively. I haven't yet written a Haskell function that implements the libstdc++ hash function for doubles. What's the smart way to get the bytes out of a Double? (Or alternatively, what's the numeric type to use when you want double-width floating point that bytes can be extracted from? CDouble?) 3. (#3, stupid markdown) If Haskell had unboxed hash tables, would anyone use them? 4. (#4) How often do hash tables with floating point keys show up in the wild? Google code search shows: [254 string unorderd_maps](http://www.google.com/codesearch?hl=en&amp;lr=&amp;q=unordered_map&lt;string) [13 double or float unordered_maps](http://www.google.com/codesearch?hl=en&amp;lr=&amp;q=unordered_map&lt;\(float|double\)) [124,000 String HashMaps](http://www.google.com/codesearch?hl=en&amp;lr=&amp;q=HashMap&lt;String) [640 Double or Float HashMaps](http://www.google.com/codesearch?hl=en&amp;lr=&amp;q=HashMap&lt;\(Double|Float\)) HashSets of floating point values are a less common than HashSets of Strings, but not as infrequent (relatively) as HashMaps with floating point keys are to HashMaps with String keys.
I think HPC could *almost* do this already. It currently highlights execution code, and prints the number of entries per function. It would be a matter of hacking it to then colorize the executed code based on the entry count. No regexes or parsers will be needed.
But that will still show only the top level functions. For me it's useful to see the information for inner expressions. Also do ticks count depend linearly on number of calls? I think the former is more accurate. Of course it would be better to integrate this functionality in a more general tool like HPC, since now it's just a quick hack.
Oh, HPC counts ticks for all cost centers (I think ticks and cost centers are the same now). So you can use the HPC.Reflect info to get all that data programmatically. Maybe hpc markup could be modified to colorize based on the tick count, rather than just on whether the tick was entered or not, as it does now.
But can I program the typed type-level programming with types?
I don't know if there is a way to do it. Real World Haskell says that you need Cygwin.
Where's the post on Typing The Typed Type-Level Programming in Haskell post?
Eventually, you might as well move to Omega :-)
http://dmwit.com/background_radial_f15008.jpg Change `background:url(images/background.jpg);` to `background:url(images/background.jpg) #f15008;`.
You rock! Applied.
I managed to find the source of hpc markup which is buried in GHC's repository. And yes, it looks trivial to modify it to do the same.
There's a ppa for haskell-platform on lucid at https://launchpad.net/~justinbogner/+archive/haskell-platform
there are two things mixed in here: reusing regular values (and functions) at the type level and statically checking type level code. the first is great, less boilerplate to write. but the second? static types limit the expressiveness while providing some safety. it's a great thing for runtime things - you limit errors that happen to the poor user, that couldn't tell the computer from the keyboard. but type-level code is run at the compile time - there is always smart and handsome person present - the programmer himself, who can tell the difference between infinite loop and slow typechecking. so please, please let us use Peano and plus at the type level without forcing static typing on us.
I think you are free to keep using * everywhere.
`rawSystem` should do the trick
Indeed, you are free to keep writing all the untyped type-level functions you used to, with everything having kind *. The richer kinds will be there for you to take advantage of when and if you want. Note you will also be able to mix and match, say, using more expressive kinds for a small important bit, but plain old * for everything else.
Thanks
Try Agda or Coq. Dependent types are much nicer than these hacks.
Earlier this year I talked to [Iain Lane](http://orangesquash.org.uk/), who was doing most of the work on Haskell packages for Ubuntu 10.04. He also works with the upstream Debian team whose packages Ubuntu uses.
&gt; What are the most important monads for a beginner to know about? This would be StateT, WriterT, ReaderT, and ErrorT. Sorry for being pedantic, but those are monad transformers, not monads. Plain `State`, `Writer`, `Reader` and `Either` (not `Error`) are way more easy to understand.
I once said that monads are like saran wrap, but I was a little off. Monads are a bunch of kids pretending to be characters for "A Bug's Life." They run around on the yard with trash cans on their backs like exoskeletons. You're the new kid; you've just moved into town. **You**: Can I play? **Them**: No! You're not a bug! **You**: How do I be a bug? **Them**: Put a trashcan on your back! So you put a shell of sorts and play with them. But look! You want to hang out with the kids playing as a DIFFERENT bug! Let's go talk to them, shall we? **You**: Can I play with you? **Them**: No, you're the wrong bug. Go away! So you do the sensible thing and use liftM and get rid of your old shell so you can put on a new one. Seriously, monads are best learned by 4 year olds.
And here I was thinking I'd never see a metaphor worse than the space suit one. I stand corrected. :) I actually don't like focusing on the wrapping aspect. It doesn't come from the "monad", it comes from the data structure being used to implement it. If the data structure permits you to deconstruct it and extract the juicy insides, you can, if not, you can't. "Maybe" is a monad and you don't have much trouble getting into the insides, do you? Telling people that's the fundamental aspect is sending them down the wrong track entirely. Moreover, the wrapping metaphor arms people for understanding the "monad does one thing per do 'line'", but leaves the 0 case a weird special case (Maybe's Nothing), and does nothing for the multiple case (the list monad). The point of the list isn't about wrapping at all, getting the "insides" is trivial, its about how the "semicolon" is suddenly doing something new and different. No, the fundamental thing is that things that are usually "statements" are getting transformed into functions, which can then be manipulated like functions can be, called more than once, not called at all, etc. Urk, I'm working up to my own text here.
That's because some kids are nicer than others about switching teams.
The trouble with "what are monads?" is that it's sort of analogous to "what are objects?" There's a short answer, (objects are packages of data encapsulated with methods to operate on the data) but the short answer doesn't really tell you what they're about, what they're for, or why they're used, and, like objects, they're so basic and central that there's no short answer to "what are they used for?" other than "everything". Monads are simultaneously an abstraction of execution sequencing, of wrapping, and of collections. Does that help? Probably not. The article that really did it for me was [The Trivial Monad](http://blog.sigfpe.com/2007/04/trivial-monad.html). This mostly explores the "wrappers" aspect, but it's a good starting place. Or at least I found it a good starting place. I would judge that the most important monads for a beginner to know are probably: The trivial monad; List; Maybe; (State s); (Writer w). But my judgment is suspect here. EDIT: I thought of another one. For someone who's already familiar with parser combinators, (Parser s) is probably a great example of a monad. 
Monad transformers are monads too
I know both Java and Haskell and I thought this was too long and lacked focus. You don't need to mention category theory at all. Starting with: "I should begin with a few things that this guide is not about" is a good way to lose the reader. &gt;My goal today is to show how a simple class written in Java could be translated into equivalent functionality in Haskell using some monads, without getting into any of the theory stuff. I don't know if this is a good idea. Java and Haskell are completely different beasts and a lot of times the translation is not 1 to 1 from a class to a monad. That is why your intWrapper example seems convoluted. This should give the right intuition: &gt;A monad is a wrapper type around another type (the inner type), which adds a certain structure to the inner type and allows you to combine computations of the inner type in a certain way. This is really the full story. Taken from http://ertes.de/articles/monads.html The next paragraph: &gt;To achieve that, a monad makes values of the inner type indirect. For example, the Maybe type is a monad. It adds a structure to its inner type by allowing lack of a value, and instead of having a value 3 directly, you have a computation, which results in 3, namely the computation Just 3. Further, you have a computation, which doesn't have a result at all, namely the computation Nothing. &gt;Now you may want to use a computation's result to create another computation. This is one of the important features of monads. It allows you to create complex computations from simpler parts intrinsically, i.e. without requiring a notion of running computations. This is probably all you need to understand the intuition. For a deeper intuition, then read [You could have invented monads](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html) 
Monad transformers applied to monads, yes, not plain monad transformers. MTs are kinded `(* -&gt; *) -&gt; * -&gt; *`, not `* -&gt; *` like monads are.
By that argument, `State`, `Writer`, `Reader` and `Either` aren't monads either. They all have kind `* -&gt; * -&gt; *`.
Monads are a design pattern for Haskell, that is used for nice stuff in libraries. Like providing statements for IO programming. Well, that's obviously not all truth, but it should be good enough first approximation for Java programmer. ;)
Thanks for that information!
No, but no one is claiming they are :)
What's the problem? 'f' is a function with free variables. 'y' is constant, and bound in an outer scope. 
Consider "=" not as an assignment, but as mathematical equality. When you write `f x = x + y`, you're telling the compiler that "f applied to x is equal to x + y". When you write `y = 6`, you're saying "y is equal to 6". Therefore, you can derive "f applied to x is equal to x + 6". y will never change. "y = 6" is set in stone. You can't change your mind and write "y = 8" later. So the compiler is free to rewrite "x + y" as "x + 6".
&gt; The trouble with "what are monads?" is that it's sort of analogous to "what are objects?" Excellent point. When I started learning about OOP (well, at least the Java kind), I had a lot of VB/Perl/PHP baggage. I could define objects and do stuff with them, but understanding the mechanics and understanding how to properly use them were very different things. For example, I tended to create "god objects", and had no clear ideas on how to best compose coherent, decoupled classes. Over time, though, there were some Ah Ha moments. I see this again while learning Haskell, and anyone coming over from an OO language should keep in mind their earlier experience. As with starting out in OO, you'll learn to use assorted code constructs before you learn how to really think about them, but even using them poorly is a step towards understanding. I had to write a lot of crappy OO code before I could write less crappy OO code, and I expect to have to write a lot of crappy functional code to get those Ah Ha! moments.
&gt; y will never change. I think this is the key to where I'm screwing up. When I was first trying to learn about referential transparency, I was putting it in the context of languages that I already know. And in something like C, int f(int x) { return x + y; } isn't transparent, because `y` is mutable, and so if I call `f(5)` now, I might get 6, but later, I could get 10.
&gt; bound in an outer scope. This might be the other part... I think in my mental model of how things work, I tend to think of each function as being its own tiny scope, or something. It may be because of something like my response to fapmonad, where I'm just getting wires crossed.
Mmm another monad tutorial! I do approve his discussion about computational contexts.
The example was quite contrived.
&gt; While I organized the code, I notice the segfault was gone. I dont know still what was causing it, but Im not worrying very much, since its gone. That seems to be the wrong conclusion. It could be anything, and since it has been shown to be intermittent the right conclusion is probably that the bug is still there, but there just isn't a repro case for it yet...
I wrote this a little bit ago using Data.Tree. It's a functional solution, and I haven't really worried about performance or memory use, so it would be interesting to me to know what others think. [Have a peek](http://gist.github.com/484937).
I once had the pleasure of guest teaching for an AP computer science class. It was the last week of class, and the following year the curriculum was shifting from C++ to Java. Though the class had spent a year in C++, the way the curriculum was structured, not all of the students had done much with classes. So obviously my main task was to introduce Java's classes, relate them to C++ classes for those in the know, and explain them to those who weren't. "Hello World" in Java (as is often pointed out) is highly contrived. It uses a bunch of special keywords and introduces an abstraction that is way over engineered for the task at hand. Not surprisingly there was some resistance. Presumably some of these students later went on and became very capable and serious programmers. Likely some of them now believe Java to be God's Language, in some kind of Einstein-y sense. I frequently find myself telling this anecdote to people who are upset about the difficulty of learning functional abstractions. The difference is that (many) high school students *expect* that they might have a hard time, because they usually have no reason to believe to the contrary. Professional developers with a few years of programming behind them expect that they should *not* have difficulty, as they expect to be able to leverage their past experience to eliminate learning barriers.
Unfortunately, that will use up all your RAM if you try to run it against /, because it generates the whole tree in memory before it does the fold. Behind the scenes, it probably does something like a recursive mapM. EDIT: yep, if you look at the [source](http://www.haskell.org/ghc/docs/6.10.2/html/libraries/containers/src/Data-Tree.html), it gets ugly here: unfoldForestM f = Prelude.mapM (unfoldTreeM f) 
Well, I'm not at the point in my Haskell education where I can give performance pointers or memory advice. I appreciate your frustration. I hope one of the wizards will show up and explain how to do what you need to do, because I don't see why it should be impossible or even all that difficult.
&gt; be functional in the Haskellish sense (i.e., use no "unsafe" operations) It's more a typing problem than a functional problem. `unsafePerformIO` is not sound as you can create polymorphic references with it, and to a lesser extent `unsafeInterleaveIO` is A Bad Thing as its breaks causality.
I have started this and failed many times. I can't think of a haskell way to do the traditional normal generation routine of generating per-vertex normals and adding across vertices and finally normalizing. So something like for each normal in normals: normal = (0,0,0) for each triangle in triangles: vec3 normal = compute_normal (triangle) for each vertex in triangle: vertex.normal += normal for each normal in normals: normal = normalize(normal)
Using `unsafeInterleaveIO` strikes me as perfectly idiomatic and reasonably safe for this kind of problem? du dir = putStr . unlines . map format =&lt;&lt; mapM' (size &amp;&amp;&amp; id) =&lt;&lt; find dir mapM' = -- mapM but defined with unsafeInterleaveIO find dir = -- defined with unsafeInterleaveIO 
 filesize :: String -&gt; IO Integer isdir :: String -&gt; IO Bool du :: String -&gt; IO Integer du path = do 
I do. But this approach fails on the local hard drive on my laptop (60GB), so I don't need to go out to the cluster to break it. Memory *is* expensive: we have 5 PB of data, but 32 GB is still a fairly impressive amount of RAM on a host. There are loads of reasonable-size datasets that don't fit in RAM. Your own genome doesn't fit in RAM. If you'd like to write me a new kernel with a virtual memory system that lets me usefully search all those 5 PB of data without using operations Haskell deems "I/O", I'll buy you all the beer you want for the rest of your life. 
I'm glossing over symlinks and other complexities here. A seq or two would probably be in order to reduce the sum trees. This requires each individual directory (not the whole tree) to be brought completely into ram, but a more complex version using readdir could avoid that. -- assuming these are available filesize :: String -&gt; IO Integer isdir :: String -&gt; IO Bool direntries :: String -&gt; IO [String] du :: String -&gt; IO Integer du path = do dir &lt;- isdir path if dir then do ents &lt;- direntries path sizes &lt;- mapM du (map ((path ++ "/") ++) ents) return (sum sizes) else filesize path 
Yep, this is a good practical approach if I just want to "du". I'm looking specifically for the fold . map . unfold approach, though, because I might want to drop in a different operation rather than just taking sizes and sums. (My serious examples of different operations involve caching and concurrency, both of which introduce other kinds of I/O complexity, so I don't want to get into specific examples; I just want to be able to "do stuff" at the map stage.) Your approach gets away with using mapM because it interleaves the fold, map, and unfold operations, just as "find" would work fine if it pushed the "print" operations down rather than lazily building the tree. 
Yes, for this problem I would either use unsafeInterleaveIO to do the recursive diretory traversal lazily or I would do the whole thing in a more direct imperitive style in the IO monad. In the latter approach the traversal is imperitive but we still can use the nice standard data structures to accumulate information during the traversal. As an example, in the [tar package](http://hackage.haskell.org/package/tar) I use the first technique to pack up a directory tree into a tarball. The top level is just pack baseDir paths = preparePaths baseDir paths &gt;&gt;= packPaths baseDir The `preparePaths` does the directory traversal to get the list of files and `packPaths` reads the files. Both use lazy IO. It does the whole thing in constant space. The major custom functions are: getDirectoryContentsRecursive :: FilePath -&gt; IO [FilePath] interleave :: [IO a] -&gt; IO [a] The `interleave` function is like `sequence` but does the IO lazily. See `Codec/Archive/Tar/Pack.hs` in the [tar package](http://hackage.haskell.org/package/tar) for the details.
The answer is to give up on #3 There's no way to be space-efficient *and* to generate a representation of the filesystem using "standard data structures". This is true in all languages. If you were writing it in C, you wouldn't try to remove all IO from the main loop, right? Lazy IO is a symptom of muddy thinking. It indicates that the programmer is still trying to write procedural code. Sure, it might work for a while, but sooner or later you'll paint yourself into a corner that can only be solved via unsafe operations. And then, you're fucked. Here's how I would do it -- the algorithm of `du` is in a pure function, and the IO-related stuff is stored separately. Of course, dealing with the POSIX API, handling errors, etc, is much longer than the actual "business logic". But in a program with more to do than sum filesizes, `du` will be larger. Additionally, depending on the program, the `sizeof` and `lsdir` functions might be given their own typeclass. This would make the example longer, so I left it out, but if there's a dozen operations then a typeclass (or data structure full of closures) will be much easier to manage than a bunch of parameters. **EDIT**: there's a cleaner version of this in a following comment, which uses Ruby-ish utility functions to simplify the error handling and iteration. import System.Directory import System.Posix.Files import System.FilePath import Control.Monad import Control.Exception du :: Monad m =&gt; (String -&gt; m Integer) -- ^ Get size of some file -&gt; (String -&gt; m ([String], [String])) -- ^ Get (files, subdirectories) of some directory -&gt; String -- ^ Root path -&gt; m Integer du sizeof lsdir = loop where loop path = do (files, subdirs) &lt;- lsdir path filesizes &lt;- mapM sizeof files dirsizes &lt;- mapM loop subdirs return $! sum $ filesizes ++ dirsizes -- | 'du' for the filesystem duIO :: String -&gt; IO Integer duIO = du sizeof lsdir where sizeof path = do status &lt;- getFileStatus path return . toInteger . fileSize $ status lsdir dir = do contents &lt;- getDirectoryContents dir let valid n = not (elem n [".", ".."]) stats &lt;- forM (filter valid contents) $ \name -&gt; do let path = dir &lt;/&gt; name handle (\(SomeException _) -&gt; return Nothing) $ do status &lt;- getFileStatus path return $ Just (path, status) let directories = [p | Just (p, stat) &lt;- stats, isDirectory stat] let files = [p | Just (p, stat) &lt;- stats, isRegularFile stat] return (files, directories) main = do size &lt;- duIO "/" putStrLn $ "size = " ++ show size
I enjoyed this article. I'm not one to say about whether or not it was the best explanation of monads or speak to the contrivedness of the examples, but it was well-written and properly aimed at its target audience. I thought the part about contexts was very good (and nice "conversation with a friend" allegory).
Is it really cool to juxtapose these three sentences? &gt; If you were writing it in C, you wouldn't try to remove all IO from the main loop, right? Lazy IO is a symptom of muddy thinking. It indicates that the programmer is still trying to write procedural code. You say what not to do, and show an example of what to do, but I'm having trouble seeing the forest. Could you explain the big idea behind your version? Where is the non-lazy I/O? What's the "trick?"
Could you give some example source in one of those other languages you mentioned so that we can see exactly what you are trying to get at?
Is this supposed to be space-efficient? I've compiled code and launched the program. I've killed the process when it was taking 112M.
There's no version of System.Directory / FilePath / etc which uses bytestrings (AFAIK), so all those file paths are the default [Char] format. That means they're linked lists of Word32. If I had to write this for production, I'd probably use the FFI to call `stat()`, `opendir()`, etc. But for demonstration purposes, this is good enough. For contrast, trying to store the entire filesystem tree will blow the program up past a few gigs of RSS and get killed.
There is also the problem that getDirectoryContents isn't foldDirectoryContents. If you have a directory with a 100k files (e.g. thumbnail caches), you're going to keep a 100k lists-of-characters in memory, which gets quite expensive.
Lazy IO means that the IO is performed in what looks like pure code -- you'd have some value `[String]` which, when evaluated, actually goes out and queries the filesystem. Lazy IO is considered bad form because it's very difficult to reason about -- for example, using lazy IO to read a file makes it impossible to know when the file's handle will be closed. The link to Tom Moertel in the OP shows an example of lazy IO. He tries to represent an IO-generated tree in pure code, and fails. Rather than pause and think about a better solution, he reaches for `unsafeInterleaveIO`, which will cause his seemingly pure tree to be built via filesystem queries *as it's evaluated*. In my code, there is no lazy IO -- the tree is explicitly generated as it's traversed. This gives near-constant space performance (*near* due to [ticket #698](http://hackage.haskell.org/trac/ghc/ticket/698)) and makes reasoning about when some IO occurs very easy. The trick is that the core of `du` is separated out into a pure function. There's really nothing about traversing a lazy tree which *requires* IO -- it only requires a way to request subtrees. By making the code generic enough to work for any monad, it becomes pure and easier to read. Of course, the downside to this is that the algorithm then must be separated into two segments. For `du` this is easy, for others it might be quite complicated. In some cases, such as when directly manipulating pointers, using the `IO` monad might be required.
That's true -- for very large directories, a cursor-based solution will perform much better. Unfortunately, I think there's currently no high-performance binding to `opendir()` and `readdir()` -- the `directory` package only exports list-based functions. Hm...sounds like a fun weekend project.
Wow, that's a great explanation. Thanks!
It's going to be give up on 3 or "wait until someone else has published The Right Library". I have a little homebrew backup program that is based on: data FTrie m e a = FTrie (a -&gt; m (Either e [a])) [a] foldFTrieM :: (Monad m, Monoid b) =&gt; (b -&gt; ()) -&gt; FTrie m e a -&gt; (a -&gt; m (Either e b) -&gt; m b) -&gt; m b foldFTrieM force (FTrie next inis) f = go mempty inis where go acc [] = return acc go acc (a:as) = do ar &lt;- f a $ next a &gt;&gt;= either (return . Left) (liftM Right . go mempty) let acc' = mappend acc ar (force acc' `seq` go acc') as The Either is because getDirectoryContents might fail, I'm actually using my own ByteString-based getDirectoryContents, force keeps space-leaks away and the Monoid is used for statistics.
Here's a Rubyish example: class DirTree &lt; String include Enumerable def each (kids = Dir.entries(self).reject { |e| %w[. ..].include? e }).each { |e| yield(e = self + '/' + e) DirTree.new(e).each { |sub| yield sub } if File.directory? e } end end puts DirTree.new('.').map { |e| File.lstat(e).size }.inject(0) { |sum, size| sum + size } As you can see, I'm using the existing module Enumerable rather than writing my own traversal stuff: all I need is to put the domain-specific into "each", and then I can map, inject (fold), etc. to my heart's content, just as I could with lazy IO in Haskell. This isn't a particularly procedural approach: it's recursive, I'm combining higher-order functions, etc. I need to do I/O because my data is external to the program, and I need to do it lazily (on demand) rather than just doing all the I/O, then all the processing. Haskell's jargon "lazy IO" is something more specific, and is what I'm trying to find a substitute for. I want to write in a functional style to keep the code short and readable, and lazy IO in the broader sense is what lets me do that while still interleaving the IO with the computation as I need to. [Edited for clarity.] Your approach appears to work, but is made rather longer and less readable than the Ruby version by the fact that you have to write a bunch of new HOFs that explicitly recurse in just the right way rather than use the existing ones. 
I have a foldDirectoryContents lying around for a homebrew backup program, but I'm not going to polish it for release until I've finished my thesis. But if you've got some spare time I could of course mail you my version ;)
In [Ruby](http://www.reddit.com/r/haskell/comments/cs54i/how_would_you_write_du_in_haskell/c0uu4ne), I would do it with Enumerable, which provides the mappable/foldable interface I want; there would never even be a real data structure, just as with lazy IO in Haskell there would never be an actual list. 
I'd love to see it. 
Would it be heresy to henceforth call 'y' a function with no arguments in all of Haskelldom?
&gt; As you can see, I'm using the existing module Enumerable rather than &gt; writing my own traversal stuff: all I need is to put the domain-specific into &gt; "each", and then I can map, inject (fold), etc. to my heart's content, &gt; just as I could with lazy IO in Haskell The difference is, the version based on `Enumerable` will work ;) As somebody who uses Haskell every day, I am the first to admit that Haskell's library situation is absolutely dire. We have nothing like the rich set of utility libraries available in most other languages. For example, everything in `duIO` is contained in the `Dir` or `File` classes, in your example. The generic iteration I was too lazy to write would be in `Enumerable`. If such libraries existed in Haskell, the code would look much like your Ruby code. However, most Haskell users are too busy writing PhD theses on zygohistomorphic prepromorphisms to bother with things like "I want to back up this directory tree", which is why Hackage has nine pages of data structures but no way to parse .ini files.
How about something like this, where filesize is abstracted out? -- assuming these are available isdir :: String -&gt; IO Bool direntries :: String -&gt; IO [String] iterfiles :: (String -&gt; IO a) -&gt; ([a] -&gt; a) -&gt; String -&gt; IO a iterfiles f sum path = do dir &lt;- isdir path if dir then do ents &lt;- direntries path results &lt;- mapM (iterfiles f sum) (map ((path ++ "/") ++) ents) return (sum results) else f path filesize :: String -&gt; IO Integer du = iterfiles filesize sum
First, I'm going to point out that neither of these return the tail, but the reverse of the tail of the reverse. However, truetail1 = reverse . tail1 . reverse truetail2 = reverse . fromMaybe [] . tail2 . reverse both work like the definition reverse. However, because they rely on reversing the list before passing to the actual function, both will take infinite time on infinite lists. I'm afraid that I can't do more than venture a guess that the first definition somehow is more dependent on the value of the last calculation than the second, although I'm rather hazy on what that might mean. On second thought, maybe you should wait for someone more knowledgeable to help you out with your running time problem.
[Here's](http://hpaste.org/fastcgi/hpaste.fcgi/view?id=27904#a27904) a half-assed, custom, iteratee-like solution I came up with quickly. You can see that it first uses `find` to list the directories, then uses the monad instance of `Unfold` to annotate the leaves with file sizes, then uses `annihilate` to combine it with a fold that prints the desired information. I think that's about what you'd do in a genuine iteratee library, assuming you can't already. I ran it on a folder with all my programming stuff and papers/books/theses/etc. It's 11 G according to du, which completes in 1 minute 44 seconds. The Haskell version took about 3 minutes. That said, I also don't see what is wrong with `unsafeInterleaveIO` for this problem. Your specification is like saying, "how do I process an entire file space efficiently as if it were a pure `String` without using `unsafeInterleaveIO`?" The answer is that you don't, `getContents` and the like uses it internally. The only difference here is that there's not a predefined `getDirectoryTree` that does the lazy I/O for you.
I temporarily [put it online here](http://student.science.uva.nl/~rturk/GetDirContents.hsc) with it, you get *GetDirContents Data.ByteString Foreign.C.String&gt; :t foldDirectoryContents useAsCString packCString (liftM . (:)) [] foldDirectoryContents useAsCString packCString (liftM . (:)) [] :: ByteString -&gt; IO [ByteString] *GetDirContents Data.ByteString Foreign.C.String&gt; :t \f -&gt; foldDirectoryContents withCString peekCString (\n io -&gt; f n &gt;&gt; io) () \f -&gt; foldDirectoryContents withCString peekCString (\n io -&gt; f n &gt;&gt; io) () :: (String -&gt; IO a) -&gt; String -&gt; IO () I'd love to hear it if you could use it. (Or even polish and package it for hackage ;)
It is certainly a 0-ary function.
&gt; no way to parse .ini files. Really? There's _that_ low level hanging fruit available? I've never contributed to Haskell open source because I see such intimidating things on a regular basis that I just assumed that most trivial things were done a long time ago. Hmm...
The **List** package, which everyone should know about, gives a neat solution. Note that 1. style == 100% functional, 2. it's space efficient (the list is never all in memory at once), and 3. no lazy I/O or unsafeness of any kind. import Data.List.Class import Control.Applicative import Control.Monad import Control.Monad.ListT import System.FilePath import System.Directory import System.Posix.Files import Prelude hiding (filter) -- | Filter with monadic predicate filterL :: (Functor l, List l) =&gt; (a -&gt; ItemM l Bool) -&gt; l a -&gt; l a filterL pred = fmap snd . filter fst . mapL (\x -&gt; do flag &lt;- pred x return (flag, x)) recurseFiles :: FilePath -&gt; ListT IO FilePath recurseFiles thisDir = concatL $ files `cons` childFiles where concatL = join all = (thisDir &lt;/&gt;) `fmap` filter (\fn -&gt; fn /= ".." &amp;&amp; fn /= ".") (joinL (fromList &lt;$&gt; getDirectoryContents thisDir)) dirs = filterL doesDirectoryExist all files = filterL doesFileExist all childFiles = fmap recurseFiles dirs sizeOf :: FilePath -&gt; IO Integer sizeOf path = do status &lt;- getFileStatus path return . toInteger . fileSize $ status main :: IO () main = do let files = recurseFiles "." sizes = mapL sizeOf files print =&lt;&lt; foldlL (+) 0 sizes If you would like to dump the list of files, add this, but note that if you have both the foldlL and the execute $ mapL in your program, it will scan the files twice. execute $ mapL putStrLn files 
Doesn't the ruby version build up a massive array of sizes before it even starts summing them? If I'm reading it right, the Haskell version will sum up each directory before it moves on to the next directory. Incidentally, the ruby version will also attempt to add the size of each directory, as DirTree#each yields both directories and files.
Of course you're right: I wrote that wrong, which I'll blame on too much Haskell lately: in Haskell, if I composed a fold and a map, the lazy nature of lists would mean they'd be properly interleaved and never actually build the list. This kind of thing is why I really wish I could use Haskell for this in a nice way. To do it properly in Ruby, I'd fold the map into the inject: puts DirTree.new('.').inject(0) { |sum, file| sum + File.lstat(file).size } [And yes, directories are files that just happen to have the "directory" bit set. (If you replace .size with .blocks, you should get the exact same result as "du" gives you.)] 
&gt; [And yes, directories are files that just happen to have the "directory" bit set. (If you replace .size with .blocks, you should get the exact same result as "du" gives you.)] du reports a directory's size to be the summation of all its children. It doesn't try to actually get the size of the directory itself. Of course, according to a quick test, running `File.stat("somedir").blocks` returns 0, so the Ruby code will work anyway, it just feels a bit odd.
That's certainly more flexible; it doesn't ever build a lazy foldable data structure, though. I would have to experiment with it to see if I can insert the kind of weirdness I might want where the "sum" and "filesize" functions go. 
Yes, you're right; I originally referred to truncating in my post, but then I thought (falsely) Hey, that's just `tail`! Thanks for pointing out the terminological error. Thanks also for taking the time to speculateI was a bit startled not to have received any response after 2 days! I like the understatement of calling it a running-time problem. :-)
Following up on my other reply -- here's what the Haskell version looks like with Ruby-ish utility functions. As you can see, it's much nicer, and closer to your Ruby example. Man, I'd give an arm to have Haskell as batteries-included as mainstream dynamic languages. foldDirTree :: (a -&gt; String -&gt; IO a) -&gt; String -&gt; a -&gt; IO a foldDirTree k = loop where loop dir = foldDir dir $ reject (\e -&gt; elem e [".", ".."]) $ \a e -&gt; do let path = dir &lt;/&gt; e isDir &lt;- isDirectory' path let next = if isDir then loop path else return k a path &gt;&gt;= next du :: String -&gt; IO Integer du root = foldDirTree step root 0 where step n path = do size &lt;- fileSize' path return $! n + size and here's the equivalents to `Dir.entries`, `File.directory?`, etc: except :: a -&gt; IO a -&gt; IO a except a = handle (\(SomeException _) -&gt; return a) foldDir :: String -&gt; (a -&gt; String -&gt; IO a) -&gt; a -&gt; IO a foldDir path k a = except [] (getDirectoryContents path) &gt;&gt;= foldM k a fileSize' :: String -&gt; IO Integer fileSize' = except 0 . fmap (toInteger . fileSize) . getFileStatus isDirectory' :: String -&gt; IO Bool isDirectory' = except False . fmap isDirectory . getFileStatus accept :: Monad m =&gt; (b -&gt; Bool) -&gt; (a -&gt; b -&gt; m a) -&gt; a -&gt; b -&gt; m a accept p k a b = if p b then k a b else return a reject :: Monad m =&gt; (b -&gt; Bool) -&gt; (a -&gt; b -&gt; m a) -&gt; a -&gt; b -&gt; m a reject p = accept (not . p)
 import System.Process main = readProcess "du" ["-sh","."] "" &gt;&gt;= putStrLn 
The size of a directory per se is specific to the filesystem implementation, but on a traditional Unix filesystem, both .size and .blocks are nonzero for a directory. du returns results that include the size of the directory proper (i.e., the list of filenames as stored in whatever way the filesystem implementation stores it). On my Mac's HFS+ filesystem, my home directory per se is 2618 bytes and 0 blocks, indicating that the directory contents are stored "off the books" somewhere, while my NFS Linux home directory on the cluster is 12288 and 24. 
This looks neat, though I will have to uncross my eyes before I can actually play with it. So maybe my target solution will be to get a decent set of utility functions (and Iteratees) into Haskell 2011. 
I see. So du just adds the directory's own blocks count to the sum of its children and just doesn't list it separately?
Perzackly. 
the hardest part of this was working out why you were surprised. it's very hard to get "beginner mind" back...
even shorter: import System.Cmd main = system "du" 
What motivates you to call 'y' a function (with no arguments), as opposed to a non-function? I keep seeing people make such statements, and I suspect there's a confusion lurking behind the impulse. I'd like to find out whether that's the case, and if so, what the confusion is. I'll refrain from sharing my impulse, as I don't want to bias your response. I'd love to hear answers from any others who like this 0-ary viewpoint. 
I think you misunderstood ticket #698 - it doesn't affect whether something runs in constant space or not.
It's a unification (like electro-weak interaction). Having only one kind of things is better than having two.
&gt; unfold to generate a list or tree of all the files in the directory tree, map over them to get the sizes, and do a fold to get the total. This is not an accurate description of what du does. du must de-dupe files with multiple names ('hardlinks') to avoid double-counting them: $ mkdir a; du a 4 a $ dd bs=1M count=1 if=/dev/zero of=a/1; du a 1032 a $ ln a/1 a/2; du a 1032 a $ ls -li a total 2056 1040419 -rw-r--r-- 2 chkno chkno 1048576 Jul 21 23:39 1 1040419 -rw-r--r-- 2 chkno chkno 1048576 Jul 21 23:39 2 I understand you're looking at a simplified example, but this seems to just make the haskell implementation even more disproportionately inelegant. There's now a [hash table](http://flyingfrogblog.blogspot.com/2009/04/more-on-haskells-hash-table-problems.html "or other record-keeping structure") that either has to be accessed from the mapped function with unsafePerformIO, or you have to abandon map and chain the filesize finding operations to pass the growing list of seen inodes. In imperative style, this is a one line change (`if already_seen[this_inode]`), while it seems to restructure the haskell implementations much more dramatically.
Who care's about hash tables? Use Data.Map, Data.IntMap, or Data.Trie and be done with it. The only difference is passing your map around, exactly like the imperative version.
Has Mac OSX Leopard support been fixed? Guess I'll find out when I get home.
In haskell everything is a function...
That pattern is called a "hylomorphism", and is one of the more common recursion schemes. It can always be simplified into `fold . unfold` because the mapped function can be pushed into the arguments on either side. For pedagogical/clarity reasons it can be worth keeping it factored out, if you so desire. (It can also be worth factoring out a natural transformation to convert the functor you unfold into a different functor for folding, cf. [category-extras](http://hackage.haskell.org/packages/archive/category-extras/0.53.5/doc/html/Control-Morphism-Hylo.html).) Of course, the `hylo` function is pretty trivial, so I assume that can't be what you're after. A hylomorphism exists for every recursive type, though in this case you'd really want to make up your own simple type for directory trees instead of abusing lists. And since hylomorphisms exhaust the space of `fold . unfold`, if you can't do your other stuff with hylo then you can't do it with just folds and unfolds. Catamorphisms are the simplest refolds, and there are plenty of more complex recursion schemes which allow you to do things like generating/destroying multiple ply at a time. I can't say which scheme you want without knowing what you really want to do, but a bunch of this stuff is coded up already in the category-extras library, so you can look around there for inspiration. import Control.Applicative (pure, (&lt;$&gt;)) import Control.Monad ((&lt;=&lt;)) import qualified Data.Foldable as F import qualified Data.Traversable as T filesize :: String -&gt; IO Integer dirsize :: String -&gt; IO Integer isdir :: String -&gt; IO Bool direntries :: String -&gt; IO [String] -- | Open-recursive form of a file-system data FS r = File String | Dir String [r] instance Functor FS where fmap f (File x) = File x fmap f (Dir x ys) = Dir x (map f ys) instance F.Foldable FS where -- Fill in optimized implementations for all functions, as desired foldMap = T.foldMapDefault instance T.Traversable FS where -- Fill in optimized implementations for other functions, as desired sequenceA (File x) = pure (File x) sequenceA (Dir x ys) = (Dir x) &lt;$&gt; T.sequenceA ys -- | Generic pure hylomorphism. hylo :: (Functor f) =&gt; (a -&gt; f a) -&gt; (f b -&gt; b) -&gt; a -&gt; b hylo g f = f . fmap (hylo g f) . g -- | Generic monadic hylomorphism. hyloM :: (T.Traversable f, Monad m) =&gt; (a -&gt; m (f a)) -&gt; (f b -&gt; m b) -&gt; a -&gt; m b hyloM g f = f &lt;=&lt; T.mapM (hyloM g f) &lt;=&lt; g du :: String -&gt; IO Integer du = hyloM getFiles sumFiles where getFiles path = isdir path &gt;&gt;= \b -&gt; if b then Dir path &lt;$&gt; direntries path else return $ File path sumFiles (File x) = filesize x sumFiles (Dir x ys) = (sum ys +) &lt;$&gt; dirsize x To maximize performance we would specialize `hylo` and `hyloM` to `f ~ FS` so we could inline `fmap` and `mapM`, and then we'd want to do a worker/wrapper transform and add annotations so that `hylo` and `hyloM` are inlined at their use sites. It's this latter inlining which enables fusing away the intermediate `Fix FS`. Otherwise we'll still have to construct the intermediate structure, though we'll do so one ply at a time and interleaved with destroying it one ply at a time. *edit:* The reason why the latter inlining is necessary to completely get rid of the intermediate structure is that it's only "complete" functions like `du` which can do fusion, because it's only when we bring together the functions that use the constructors and the functions that match on them that the compiler can perform that case analysis at compile-time. Lists are a special case because they don't have branching recursion and they only have a single base case. Because of this restricted structure it's easy to make the compiler smart enough to see through the abstractions and glue the right parts together. And since lists are so common, all this work has already been done in the libraries. For less restricted data types it takes a bit more work, but it's still doable (barring certain list-specific fusions).
This module is astonishing.
I have to admit that I haven't gotten around to implementing proper meshes in my project yet. I'm just using lists of triangles, and for normals using the face normals. For your problem (if I understand it correctly) my first attempt would be something like this: {- Returns True if the given Vertex is one of those making up the given Tri -} vertexInTri :: Vertex -&gt; Tri -&gt; Bool vertexInTri = undefined -- FIXME provide implementation computeNormal :: Tri -&gt; Vector3 computeNormal = undefined -- FIXME provide implementation vadd :: Vector3 -&gt; Vector3 -&gt; Vector3 vadd = undefined -- FIXME provide implementation zeroVec = undefined -- FIXME provide definition (the zero Vector3) {- Calculate the normal for a given Vertex. The normal is the normalized sum of -} {- face normals for each Tri in which the given Vertex is a vertex. -} vertexNormal :: Vertex -&gt; [Tri] -&gt; Vector3 vertexNormal v = vnormalise . foldl vadd zeroVec . map computeNormal . filter (vertexInTri v) Your loop would then be mapping `vertexNormal` over a list of vertices (actuall you might want to swap arguments in the function to make that easier). I hope this helps in some way.
I'm pretty sure John Goerzen's MissingH package (which has probably been broken out into separate parts by now) included read/write for "config" files which were in fact .ini files.
Why complicate your universe with non-functions when functions naturally generalize to cover them? Category theoreticians don't have any problems with thinking of constants this way; the only folks I've encountered who do are computer scientists (intriguingly enough). Don't get me wrong, there are many important reasons to distinguish values from functions, but this isn't one of them. The reason to distinguish them comes from the different interpretations of functions-as-objects vs functions-as-procedures, which is the same distinction as closures vs code pointers in compilers, and the same as the distinction as exponentials vs morphisms in category theory. There is a close connection between the two interpretations (hence why the distinction is so often elided), but they are radically different. However, because Haskell silently converts between the two, this issue has no bearing here.
http://hackage.haskell.org/package/ConfigFile
why sum/get Files aren't written using Foldable and the other thing (can't remember the name)
If you want a high brow answer, it's time to break out the domain theory: First of all, `foldr f x  == ` for all `f` and `x`, as `foldr` pattern matches on `(:)` versus `[]`. This also implies that `tail1 ` is ``, which is a base case. So now, given any list element `a`, let's do a bit of calculation: tail1 (a:) = { unfold definition of tail1 and foldr } snd (True, if b then (a:as) else []) where (b,as) = foldr ...  = if b then (a:as) else [] where (b,as) =  = if  then (a:as) else [] =  This is an inductive step proving that, for all partial lists `xs` ending with ``, `tail1 xs == `. So we can break `tail1 [1..]` into a series of approximations, tail1 (1:) ==  tail1 (1:2:) ==  tail1 (1:2:3:) ==  ... And, since `tail1` is [continuous](http://en.wikipedia.org/wiki/Scott-continuous), we can can conclude that `tail1 [1..] == `. (All pure Haskell functions are continuous, unless somebody's pulling shenanigans associated with unsafePerformIO) With regard to `tail2`, if you really want to make your code more pointless, you can rewrite `comp2 f g` as `(f .) . g`: tail2 = foldr ((Just .) . (maybe [] . (:))) Nothing but that's hard to follow, so we'll note that ((Just .) . (maybe [] . (:))) = \a b -&gt; Just ( case b of Nothing -&gt; [] Just as -&gt; (a:as) ) Which is *not* equivalent to \a b -&gt; case b of Nothing -&gt; Just [] Just as -&gt; Just (a:as) Because if `b == `, the former expression is `Just `, whereas the latter expression is ``. This is key, because replacing `((Just .) . (maybe [] . (:)))` with the latter expression results in the non-productive loop you mention, and admits a similar proof of non-termination. But I digress. We can calculate the first few approximations: tail2  =  tail2 (a : ) = {unfold definition of tail2 and foldr, then refold definition of tail2 } ((Just .) . (maybe [] . (:))) a (tail2 ) = ((Just .) . (maybe [] . (:))) a  = Just  tail2 (a : b : ) = ((Just .) . (maybe [] . (:))) a (tail2 (b:)) = ((Just .) . (maybe [] . (:))) a (Just ) = Just (a:) So, via a similar application of induction and continuity, we can prove that `tail2 [1..] == Just [1..]`
&gt; As somebody who uses Haskell every day, I am the first to admit that Haskell's library situation is absolutely dire. We have nothing like the rich set of utility libraries available in most other languages. &gt; For example, everything in duIO is contained in the Dir or File classes, in your example. The generic iteration I was too lazy to write would be in Enumerable. If such libraries existed in Haskell, the code would look much like your Ruby code. However, most Haskell users are too busy writing PhD theses on zygohistomorphic prepromorphisms to bother with things like "I want to back up this directory tree", which is why Hackage has nine pages of data structures but no way to parse .ini files. I agree with you but I'm not worrying much about these missing libraries. Someone will write them once they need them and publish them somewhere; if they're good enough they'll make it into the Haskell Platform. I would be worried if the libraries were difficult to write in Haskell in the first place, which they're not.
Well it would just make everything a function. From a typeclass point of view this makes it typesafe, since you can only return a function. That's really it.
With GHC 6.12 you can derive Functor, Foldable and Traversable, which makes this a very nice short solution.
&gt; most Haskell users are too busy writing PhD theses on zygohistomorphic prepromorphisms to bother with things like "I want to back up this directory tree" Isn't `doliorules` 's module a bit zygohistomorphic, or at least prepromorphic?: annihilate :: Unfold n a -&gt; Fold n a -&gt; IO () data Fold n a = forall r. F (a -&gt; IO r) (n -&gt; [r] -&gt; IO r) data Unfold n a = forall s. U s (s -&gt; IO (Either a (n, [s]))) With a type signature like that, how could I resist applying `annihilate` to my file system? main = do (f:_) &lt;- getArgs annihilate (sizes f) process His main function isn't doing the same as `du` except in the sense of making a calculation based on a general tour of the filesystem, which is what `ijk1` wanted and used `du` as an example of. It was as well behaved as `du`, to judge, unscientifically, from the 'Activity Monitor' (it spiked a bit over, e.g. `~/.darcs/cache/patches`, I guess since it uses `getDirectoryContents` straightforwardly). And it was quite fast: $ time ./dolio ~/ real 3m32.351s user 0m35.975s sys 0m52.453s $time du ~/ real 2m10.954s user 0m2.251s sys 0m36.086s It seems as though a problem like this would involve hundreds of engineers, a diligent eye to allocation, and acres of C, etc. etc. Maybe it does, in the end, but maybe it mostly needs thought? 
It is, as he notes, executable as it stands. See below http://www.reddit.com/r/haskell/comments/cs54i/how_would_you_write_du_in_haskell/c0uu7rz
The downside is that it means giving up the unification of function *arity*. Is "everything is a function of arity 0 or 1" really better than "everything is either a constant or a unary function"? Personally, I prefer the universal currying perspective, even if it means that nullary terms are named differently.
Just a question: was the disk cache the same for both examples?
This was very instructive; thank you for posting it.
Doesn't it prevent the allocator from releasing memory back to the OS?
Where did you get `hyloM` from? I don't see anything like it in category-extras.
I disregard hard links in the Ruby and other implementations too because nobody in my world really uses them (except . and .., which we filter out) and, as you mention, they add complexity. The simplified du is an excellent example of the general pattern I want, which is a nested-IO unfold, and which [winterkoninkje points out is well-known in certain Haskell circles](http://www.reddit.com/r/haskell/comments/cs54i/how_would_you_write_du_in_haskell/c0uvqqo). 
Maybe its because Data Structures are easy with Haskell and IO is hard.
That is quite helpful--right now I've abandoned my attempts to make a raytracer and am making a bitboard-style chess AI (well first I'm implementing the rules of chess, the AI after that isn't too hard, especially in haskell), and I've gotten a lot farther so far. In your example, it has to look through the entire list of triangles in order to find the correct vertices, so your entire algorithm is something like O(t*v) where t = number of triangles and v = number of vertices. Anyways, the typical algorithm that I described is simply O(t). I thought of your way of doing it and put it away because I thought it'd be too slow. I thought of storing the mesh info differently, which would work kindof : You store the vertex and for each vertex you also store its associated triangles. This information can be determined from the file format I guess. Then you could use your method to compute all the normals. I guess that's the main reason why I can't get into this Haskell stuff, if I find a better way to do it imperatively I get stuck until I can find a better way in Haskell. I'm also a bit confused about the type system and how it infers things, as I make tons of type errors initially usually. 
Thanks! I was hoping that there was a bit of deeper knowledge that would make this work; I will need to spend some time finding and grokking category-extras. 
If you want, I guess, y can be thought of as a function that takes no arguments and returns 5. But I don't see a meaningful gain by doing so. Are you going to call 5 a function that takes no arguments and returns five? What's happening in f then, is f a function that takes a function as an argument, and then returns a zero-argument function that computes the sum of the zero-argument function passed in and the zero-argument function y from the outer scope? You could take it to that level, but I think it's a silly semantic game to do so.
[winterkoninkje's solution](http://www.reddit.com/r/haskell/comments/cs54i/how_would_you_write_du_in_haskell/c0uvqqo) is way more zygohistormophic and I feel it is more elegant. He makes this `du` problem feel like a simple instance of a monadic hylomorphism. It is also the first time I think I've seen a practical example of how one might use category-extras.
Parsing doesn't require any IO, and IO in Haskell is not at all difficult.
Are you sure it's correct? In ghci I get: fmap fmap fmap fmap fmap fmap fmap fmap fmap :: (Functor f, Functor f1, Functor f2, Functor f3) =&gt; f (f1 (f2 (a -&gt; b))) -&gt; f (f1 (f2 (f3 a -&gt; f3 b))) fmap fmap fmap fmap fmap fmap fmap fmap fmap fmap :: (Functor f1, Functor f) =&gt; (a1 -&gt; a -&gt; b) -&gt; f1 a1 -&gt; f1 (f a -&gt; f b) the second of which definitely is shorter. From my studies I recall a program which could make the (plain old) Hindley-Milner type inference infer a type which was exponential in the size of the program. Something with tuples, but I've forgotten the details.
Thanks! I understand why we call y a constant now.
Please test and report.
I'm sure it's not correct. It seems to approximate exponential growth but as you can see from the second graph the successive ratios (which should be constant for exponential) seem to be 4-periodic and hang around 1.5 after the first 7 or 8 fmaps. 
OK, at least there is a perception that IO is hard.
Where? IO in Haskell is no harder than in C; you've got a `Handle`, `hGetBuf`, and `hPutBuf`, which correspond roughly to `FILE*`, `fread()`, and `fwrite()`. Further abstractions are layered on top of these, just as (for example) glib and Qt provide abstractions on top of the C procedures. Most of the difficulty from writing `du` efficiently is the quality of OS system call bindings, which don't have much to do with IO except that (as environmental properties) they are contained in the `IO` type.
mom said if i continue applying fap i might go blind OH YOU MEAN fmap never mind
Exponential size is simple: a x = (x,x) a5 = a . a . a . a . a You'll get a nested tuple of size 2^n since every step doubles the size. You can even get doubly exponential: b1 x = (x,x) b2 = b1 . b1 b3 = b2 . b2 b4 = b3 . b3 b_n will have size 2^2^n. Printing type of b5 on GHCi takes some time. I wonder if it can go even faster.
&gt; In haskell everything is a function... I keep running into this belief, and I wonder about its origins. Do you remember where you got it?
&gt; It's a unification (like electro-weak interaction). Having only one kind of things is better than having two. Great, thanks! Exactly the sort of thing I was asking for. If you're willing, I'd like to explore this reasoning further. If we did decide that values of non-function type are 0-ary functions, then what order functions? 1st, 2nd, 3rd, ...? In other words, if 7 (or y) is a 0-ary function, what is the result type of that function? If the answer is, say, Integer, then we'd have to recall that Integer really means 0-ary function whose result type is Integer, ad infinitum. 
Does this unification really work in our setting, with *higher-order functions*? Do we collapse values of all types to 1st-order functions? 2nd? 3rd? Or do we eliminate only "0th order" (non-functions), and still have 1st, 2nd, 3rd, ...? And if so, what's the gain?
&gt; Are you going to call 5 a function that takes no arguments and returns five? and similarly with that returned 5, and so on? 
This one was built on Leopard and should work.
Exactly. This game isn't helpful to me.
There is a garbage collector recycling memory for you. Releasing memory to the OS would only come into play for a long running program with a spike in memory usage -- mostly it would not be necessary, which is why that ticket is still unfixed. I should really rename it or something, it does tend to lead to confusion.
Mainly here I'm really curious about how beliefs begin and spread and solidify. I keep hearing "everything is a function", but I don't hear people say "everything is a list", and `3` is really the singleton list `[3]`. Or "everything is a pair", and `7` is really `(7,_|_)` or some such. Or "everything is a `Maybe`", and `True` is really `Just True`. Personally I don't like to equate non-functions (number, bools, trees, etc) with 0-ary functions any more than I to equate them with singleton lists (or trees, ...) or non-Nothing Maybe values, etc. 
One that I hear a lot is that Haskell's `()` is a 0-tuple and that all other values that are not n-tuples, for n &gt; 1, are 1-tuples.
&gt; Why complicate your universe with non-functions when functions naturally generalize to cover them? Is it really simpler to say that we have an infinite number of orders of functions or that we have both 0-ary and 1-ary functions than it is to say that we have exactly one order of functions (1-ary)?
Category theorists like to model constants as morphisms from "the" terminal object. Unfortunately this doesn't translate into Haskell, since there is definitely a distinction between "morphisms from a terminal object" and "constants". For example, compare fibsVal :: [Int] fibsVal = ... and {-# LANGUAGE EmptyDataDecls #-} data EmptyType where fibsFn :: EmptyType -&gt; [Int] fibsFn _ = ... (Note that () isn't terminal, since there are in general *two* morphisms from it to any value (strict and nonstrict); the true terminal object is an "empty" type as above. There is no initial object.) Now fibsVal and fibsFn have different types, hence different denotational semantics. They also have different operational semantics: fibsVal gets memoised, fibsFn doesn't.
Ah, I hadn't really thought that far. Haskells *laziness by default* was what made me think of constants as functions that take no parameters. Because they are not evaluated strictly when used, they kind of "return" a value. But you're right: `(-&gt;)` identifies a function and every function takes exactly one argument. In Scala, there is a difference between the type `Int` and `() =&gt; `Int` (which means a function with no arguments that returns an Int). But that is only necessary because Scala is strict by default. So it's more like a laziness annotation than a different type.
I thought about it and I agree with you now.
Seems like encoding a type-level Ackermann function wouldn't be that hard.
Hmm... I think it was because I was originally taught 'everything is an object' in OOP. It helps to simplify, like Benutzername and a lot of others alluded to above. 'y = 3' can be considered as a function with no arguments that returns just 3. As for me, my mental model is that there are 2 kinds of fundamental things in haskell: functions and data. After reading the above comments.. I guess one could say that y = 3 is a constant(..or some data stored in y or..?), but it just seems to complicate *my* model instead of simplifying it. By the way, I've been a haskeller for only 3..5 months
It's just that I know 3 and [3] are different (a number and a list with one element), whereas for me functions are functions. Maybe it is because functions are assigned to values, while the rest (say data) isn't, because statelessness is encouraged. Assuming that 'y = 3' is a function that always returns one thing helps with that IMHO
The many really complex solutions and suggestions for this particular problem make me wonder if Haskell is really for me. 
There's no way for me to be rigorous! -- but if I reboot for each, I get: du first 3m8.783s then 2m0.166s then 3m12.500s ./dolio first 4m21.816s then 3m38.229s then 4m12.928s I knew time comparisons were going to be unfair to `du` in any case, since it's doing more -- or rather, I couldn't figure out how to get what they were doing to line up. It was just a question whether the other was going to go obviously wrong. There seem to be 825,293 files. 
I'm sick of these mother affine types! Some description of affine types: http://www.ccs.neu.edu/home/tov/pubs/affine-contracts/ But why the name? I'm familiar with affine transforms and linear transforms. I see that this is a bit more than a linear type. Is there some deep meaning here?
Yes, there is a connection. Girard introduced the term in connection with the geometry of interaction. [Here's](http://www.seas.upenn.edu/~sweirich/types/archive/1997-98/msg00134.html) an e-mail from Girard talking about the name "affine".
Sure, but note also that "the" terminal object is any object within an isomorphism of a terminal object. This too doesn't translate directly to Haskell since Haskell's nominal typing means that we can define multiple empty datatypes and can distinguish between them (something which is often called being [evil](http://ncatlab.org/nlab/show/evil)). You'll note that empty datatypes aren't terminal objects either. They contain multiple distinguishable values including nontermination and extensible exceptions. In order to avoid the problem of evil, it may be desirable to define a canonical terminal object from which constant morphisms emerge. In order to prevent the evil ability to compare it against non-canonical terminal objects, we must not give it an explicit name in the surface syntax. Therefore, your `fibsVal` does indeed have an (invisible) argument which is the canonical terminal object. We can even give that invisible argument a denotation since the function is brought into contact with its argument exactly when evaluation is forced (i.e., the argument is control flow). This whole arrangement is notably different than in strict functional languages. In OCaml I wouldn't be inclined to say that `f : a` and `g : () -&gt; a` are both nullary functions. Here, the operational distinction about when evaluation of side effects occurs is very significant. In Haskell, however, there is nothing preventing the compiler from recognizing that the argument to `fibsFn` isn't used and therefore compiling it to the same code as `fibsVal`. Excepting of course that empty datatypes aren't empty.
Here's one of the places where the silent coercion between functions-as-values and functions-as-procedures confuses things. HOFs are just functions like any other. Their arguments are functions-as-values, i.e. exponential objects. When I say `id 5` and `const id` those aren't the same `id`, they belong to different ontological categories. Category theory makes this a lot clearer IMO. Albeit, the reason for wanting to do this unification isn't always immediately apparent in common functional languages. In CCCs we only have to worry about exponentials and morphisms so it's obvious how to worry about them (i.e., don't). Once we move to languages like Andrej Filinski's [symmetric lambda calculus](http://www.diku.dk/~andrzej/papers/DCaCD.ps.gz), however, then we also have coexponential objects and so we need to take more care in distinguishing things. Similarly, when you move to languages like [chiastic chromatic lambda calculi](http://llama.freegeek.org/~wren/pubs/ccgjp_nasslli2010.pdf) the distinction becomes plain to see.
The reason why certain problems are easier in other languages is a result of the fact that Haskell is trying to do something much harder than what the other languages are doing. It's trying to be expressive while checking nearly everything up front and absolutely separating the concepts of execution and evaluation. The benefits you gain from purity and type type system are enormous, but they're exactly the things that are difficult to see when you are learning. If you're writing a small program like this, then Ruby is the right language, and Haskell isn't, but as the complexity increases, Haskell quickly becomes the better language. The whole question of doing I/O "on-demand" has not really been properly solved before but is receiving a whole lot of attention now (iteratees, etc). One of the traditional methods is lazy I/O, but it is fundamentally flawed, and you should not use it. The solution based on the **List** package I gave in the comments here, is, I think, part of the way of the future. **List** is very new and sadly, not well known yet. Even though it's currently "non-standard" (though I hope that changes soon), I think you'll at least agree that Haskell excels when it comes to expressiveness. 
You're conflating two different presentations. In the first presentation we have a notion of functions with arity, and there's no particular reason to think that arity must be a whole number rather than a natural number. Indeed, by allowing it to be a natural number we can simplify things by not needing to say things like "a binding can be either a function or a value" (hey wait, I thought all functions were values in FP??) In the second presentation all functions are maps from a single argument to a single output, and therefore the notion of arity is meaningless/uninformative. You'll note that in the category theoretic presentation a binary function is denoted by a (unary) morphism to an exponential object: `Haskf :: a -&gt; (b -&gt; c) ~ CTf : a -&gt; c^b`
&gt; Haskells laziness by default was what made me think of constants as functions that take no parameters. Oh! Thanks much. This bit of operational reasoning had not occurred to me as a source of thinking than even non-functions are "functions". I was looking for denotational reasons, i.e., what things mean rather than how an implementation might work. One of my recurring blind spots when trying to understand other's thought processes.
&gt; Maybe it is because functions are assigned to values, while the rest (say data) isn't, because statelessness is encouraged. Thanks for your comments. I'm guessing there's at least one deep confusion underlying this statement, and I'm very curious what it/they might be. * By "assigned to values", did you mean "assigned to variables"? * Are you identifying *functions* with *definitions*, or maybe with *top-level definitions*? * Did you know that bindings/definitions can be of function or non-function type? * And maybe mixing up *defintion* (immutably binding a variable) with *assignment* (mutably overwriting a storage location)? Honestly, I'm not criticizing here. I really appreciate your helping me understand what conceptions people get about pure functional programming and about Haskell in particular. 
&gt; I guess one could say that y = 3 is a constant It seems you divide your value world into "functions" and "constants". But every value is a "constant". A function you define is a constant, which happens to be of type function. 3 is a value of some Num type. It could even be a function (given the right Num instance), so let's choose something else. "Hello world" isn't either a "constant" or a "function", it's a constant string. (\x -&gt; x*2) is a constant function that multiplies by 2.
I invented it? it's a straightforward lifting of the non-monadic version. You can almost get it from `g_hylo` by using the identity comonad, it's distributivity law, the identity natural transformation, and using `T.sequence` as the monad's distributivity law[1]. But this doesn't quite get us there because it requires that we can refactor the monadic parts of the coalgebra into the algebra. Even if that's possible, it would hurt code legibility and maintainability. [1] Note that we can generalize `hyloM` if we take an argument `sequence :: forall a. f (m a) -&gt; m (f a)` for the particular f and m, instead of the Traversable constraint. This is more general because there are some functors which can distribute over particular monads but not over all monads, and so pulling the distributivity law allows us to monomorphize on `m`.
&gt; ... we can simplify things by not needing to say things like "a binding can be either a function or a value" (hey wait, I thought all functions were values in FP??) What would motivate such a statement? I'd expect simply "a binding can be of any type". And if the listener is confused about functions and non-functions, then one might explain that some types are function types and some are not, so "any type" includes functions and non-functions.
What result were you expecting? I am not managing to understand what you were surprised by..
Yeah, I added the definitions for those who fear language extensions and to make it clear that there's no magic going on here. Heck, we don't even need the standard definition of a type-level fixpoint operator.
I'm not sure how they could make the code any clearer. You'll note that Traversable is being used in the definition of `hyloM`. The only other way I could think of using Foldable/Traversable would be if we construct the whole `Fix FS` tree in one go and then destroy it all in one go. The specific question was how to code as if building a tree but without actually building it, so that approach wouldn't work for us (and this is the fused version of that anyways).
&gt; Hmm... I think it was because I was originally taught 'everything is an object' in OOP. It helps to simplify, like Benutzername and a lot of others alluded to above. 'y = 3' can be considered as a function with no arguments that returns just 3. Ah, it looks like you're conflating *functions* and (top-level) *definitions* here. Maybe because they're tightly connected in C programming, in which functions are always defined at the top level and are always immutable, whereas non-functions can be defined in a nested scope and historically were mutable (before const). &gt; As for me, my mental model is that there are 2 kinds of fundamental things in haskell: functions and data. After reading the above comments.. I guess one could say that y = 3 is a constant(..or some data stored in y or..?), but it just seems to complicate my model instead of simplifying it. By the way, I've been a haskeller for only 3..5 months How does the following explanation hit you? *All definitions* in math &amp; pure functional programming are constant, including definitions of function type? Moreover, definitions can have an infinite variety of types, including simple ones like `Integer` and fancier ones. And some of the fancier ones involve function types, e.g., `Bool -&gt; Integer` and `(Bool, [Integer -&gt; String])`. Your mental model of "2 kinds of fundamental things in haskell: functions and data" might start pinching soon. For instance, how might you classify `thing1` and `thing2` below: thing1 :: (Bool, Bool -&gt; Bool) thing1 = (True, not) newtype Id a = Id a thing2 :: Id (Bool -&gt; Bool) thing2 = Id not As an alternative to the function/data taxonomy (which probably suits C okay), you might try out this one: "There are some basic building blocks for types, and they build on each other to form an infinite variety of types of values". How does this revised statement strike you? 
Well, there is the distinction between function bindings and regular bindings (i.e., pattern bindings including simple variable bindings). So it depends on the tutorial and how it explains things like that. Some will go for the "everything desugars into case, let, and lambda" route, but others try to provide a higher-level intuition.
You're making the same mistake as geezusfreeek did [here](http://www.reddit.com/r/haskell/comments/cs3gw/so_im_eating_breakfast_this_morning_posting_about/c0uwvt7). The notion of arity includes binary, ternary, quaternary, etc. By saying that arity can only be 1 you're confusing the presentation in which arity is a sensible notion with the presentation where everything is a simple function. The universal currying perspective comes precisely from rejecting the presentation that gives "arity" meaning.
Don't fall for [that trap](http://www.reddit.com/r/haskell/comments/cs3gw/so_im_eating_breakfast_this_morning_posting_about/c0uwvt7) :)
I don't agree with that logic (as you may be aware :) When I say that `y = 7` is defining a nullary function `y`, that function would have the terminal object as its input type and would have Integer, say, as its output type. By saying that `y` is a nullary function I in no way mean to claim that `Integer = 1 -&gt; Integer`. This latter claim isn't even wrong because it is confusing two different ontological categories: types and maps, or objects and morphisms. In other words, saying that `y = 7` defines a function does not license the conclusion that 7 is a function. We could just as easily claim that since `f x = x` defines a function then clearly `x` is a function. I think you'd agree that this is absurd.
What you do, is call 5 a function from no arguments to Int. Not a function that returns 5, but that "5" is this particular function (and then + and * and so on operate directly between them, etc). It can be quite helpful in mathematics (in formal logic, it's simpler to just talk about function symbols than to repeat a bunch of arguments for a separate category of constants), but I think it's less so in programming. In programming it seems better to go that other way, and play the "silly semantic game" of saying that functions are just a sort of value, which can be passed around like any other value, and Int-&gt;Int is just another type.
The tracker (openbittorrent) for the torrent appears to have been offline for the past few hours. Might it be an idea to add a new tracker to the .torrent should it not reappear soon?
Just our luck. We've fixed the torrents to talk to an additional tracker, so if you grab the .torrents from the download page again, it should work. Direct link to the fixed torrents: http://hackage.haskell.org/platform/2010.2.0.0/torrents/haskell-platform-2010.2.0.0.i386.dmg.torrent http://hackage.haskell.org/platform/2010.2.0.0/torrents/HaskellPlatform-2010.2.0.0-setup.exe.torrent Or: http://natulte.net/random/HaskellPlatform-2010.2.0.0-setup.exe.torrent http://natulte.net/random/haskell-platform-2010.2.0.0.i386.dmg.torrent
&gt; Thanks! I understand why we call y a constant now. By "constant", do you mean unchanging (immutable)? Or something else. And when you say "y" here, do you mean the value denoted by the expression "y" (in the context of its definition)? Or the expression itself? Or maybe the definition that binds y? Or ...? I see the word "constant" surrounded by confusion in programming circles, especially when people come to functional programming after imperative. In pure functional programming all values (including functions, structures, etc) are *constant* in the sense of being unchanging, immutable. Maybe confusion arises because a function, while functions are immutable/unvarying in themselves, they are also wonderfully useful for modeling quantities that do vary over time or space or whatever. 
ta, seeding now.
&gt; ... saying that y = 7 defines a function does not license the conclusion that 7 is a function. We could just as easily claim that since f x = x defines a function then clearly x is a function. I think you'd agree that this is absurd. Ah, thanks. If I'm tracking you, then although we've defined "y = 7", it's *not* the case that y is equal to 7. (Since y is a function and 7 is not; i.e., they're even ontologically distinct.) A surprise to me! &gt; We could just as easily claim that since f x = x defines a function then clearly x is a function. Hm. I don't follow that reasoning. Once you desugar the definition, you get "`f = \ x -&gt; x`". So I'd expect you to say "We could just as easily claim that since `f x = x` (which is to say `f = \ x -&gt; x`) defines a function then clearly `\ x -&gt; x` is a function." And I'd agree. Or if you don't want to desugar, then I'd read "`f x = x`" and conclude that `f x` and `x` are equal and so of the same type, and in particular either both are of function type or both are not of function type. 
I was expecting "unbound variable y" or something; like y was out of scope.
I'm saying there's no gain in treating values as functions, not the reverse. 
Thanks! That's exactly the kind of explanation I was looking for. Specifically, I was wondering whether the different behaviours were just results of some particular choice in Haskell's evaluation strategy, or whether one really was irrefragably non-productive. (I have the distrust of variables of a mathematician who doesn't program much and loves combinatory logic; so what you write as a point-free alternative to `comp2` is what I actually had, but then I thought that it would be polite to put it in a more comprehensible form if I wanted anyone to answer my question. :-) ) I do wonder about the following (with the name changed to reflect [wibbly-wobbly](http://www.reddit.com/r/haskell/comments/crczc/tails_and_infinite_lists/c0uu8l4)'s observation): trunc3 = foldr (\a (b, as) -&gt; (\x -&gt; (True, x)) $ if b then a:as else []) (False, []) It seems to me that, following your strategy, one may write trunc3  =  =&gt; trunc3 (a : ) = (\x -&gt; (True, x)) $  = (True, ) =&gt; trunc3 (a : b : ) = (\x -&gt; (True, x)) $ if True then a :  else [] = (True, a : ) and so on; and yet trunc3 [1..] is non-productive, so I suspect that I am still not understanding. Would you be willing to show me where I've gone wrong?
line 27: Don't use C-like calling conventions. fib n = fib (n - 1) + fib (n - 2) Same in the main function. line 48: This isn't "baseclasses" it's something like case statements (correct me if I'm wrong there, but it isn't class stuff). I suppose it's a place to start. More than many can say when they encountered Haskell or other functional languages coming from OOP (those who give up and leave). It's an enjoyable language with a fun and interesting combination of features. I'm loving my time in Haskell-land, hope you do as well. This still makes me happy in how much Haskell can express in such a simple line: tri n = [ (x, y) | x &lt;- [1 .. n], y &lt;- [1 .. x] ]
How exactly does their power differ from Haskell + indexed monads? Those I understand well...
wow, this abstraction is very cool, thanks for sharing!
Nice. Mostly just terminology issues: line 8: this is not "pattern matching". line 32: (a . b) is function composition, not a "compound statement". line 37: plusOne :: a -&gt; a would not actually work. line 43: [Char] is a list, not an array.
Why is that strange URL there? nyud.net?!
 -- "plusOne :: a -&gt; a" also works No, you cannot use `(+)` without a `Num a` context (which is implied by `Integral a` BTW).
My understanding is that affine types are a way of controlling aliasing. So for instance, you could use affine types to release a resource to some consumer without worrying that they'll pass it around to many others. I'm not sure how to accomplish this with indexed monads, though I've tried (while working on [Potential](http://potential-lang.org) I became interested in finding a way to model pointers which couldn't be aliased).
Can the background colour (before the image loads) be made less ugly? Something closer to the colour of the image in the centre of the image (pale yellow).
&gt; Don't use C-like calling conventions. &gt; &gt; fib n = fib (n - 1) + fib (n - 2) How would you write this, then?
&gt; This isn't "baseclasses" it's something like case statements (correct me if I'm wrong there, but it isn't class stuff). [Guards](http://learnyouahaskell.com/syntax-in-functions#guards-guards)
If you're writing stuff, you should use literate haskell (lhs, reverts what's code and what's comment) rather than regular.
And the example isn't very useful as you could just do the same thing with toplevel matches (as in line 25). Using more complex tests (&lt;=/&gt;=0 would probably better demonstrate the principles.
Add a whitespace between function call `fib` and argument `(n-1)`. If written without whitespace, people might be mislead into thinking that function arguments are always enclosed in parentheses (like in C), which they are not.
Couldn't we just say that func = ... is syntactic sugar for func _ = ...?
Right. This is what I meant. Should have been clearer.
What's wrong with LYAH and the other good haskell tutorials we have? Also I see you like list comprehensions, I thought they were the best thing ever but I have since realised that that are really not as useful as you'd think unless you're doing fairly maths stuff.
&gt; Are you going to call 5 a function that takes no arguments and returns five? Of course, `5` in Haskell *is* a function in some sense, taking one implicit *type* argument and returning a value of that type. `(5 :: Int)` however, is definitely nullary.
Well, I think (parts of) Haskell's evaluation strategy is baked in to the argument in various ways; for example, there is a variant of `if` such that `if  then a else b` is not necessarily , basically this if evaluates `a` and `b` in parallel, and returns approximations on which they agree, and finally forces the evaluation of the guard once it actually is needed. I've been told this is actually useful for modelling hardware. Also, why not just call it `init`, like the Prelude does? :-) ghci&gt; init [1..5] [1,2,3,4] You can sometimes use GHCi to test strictness properties, for example you certainly are doing something wrong: ghci&gt; trunc3 (1:undefined) *** Exception: Prelude.undefined ghci&gt; tail2 (1:undefined) Just *** Exception: Prelude.undefined However, this kind of strategy is not without it's pitfalls, as the following example demonstrates. I think there is a hackage package for making this better, but I don't know off the top of my head what that is. (ChasingBottoms?) ghci&gt; (undefined, 42) (*** Exception: Prelude.undefined 
I'm not sure what mistake you're talking about, because as far as I can tell you're reiterating my point with (more precise) terminology, i.e., that treating values as nullary functions breaks the ability to ignore function arity. Which is why the "it simplifies things" argument is a red herring, because it just moves things around. "functions + values" isn't really more or less complicated than "functions * arity", and I prefer the former as a matter of taste.
Ahh, took me a minute to find what was wrong. Actually, I think my original explanation for `tail1` is wrong as well in multiple respects. I worked backwards from the correct answer and didn't check my argument quite as carefully as I should have. This is productive on `[1..]`: trunc3 = foldr (\a ~(b, as) -&gt; (True, if b then a:as else [])) (False, []) The issue is that the pattern matching `(b,as)` on  results in the output of the function being , and using an irrefutable pattern makes it lazy enough so that if `(b,as) = `, you still get `(True, )`
&gt; although we've defined "y = 7", it's not the case that y is equal to 7. (Since y is a function and 7 is not; i.e., they're even ontologically distinct.) A surprise to me! The element of type `Int` being singled out is ontologically distinct from the thing that's doing the singling, of course. But I'm not sure if it's justifiable to identify the literal "7" in the code with the actual value 7, either. So it might be the case that y is "equal" to "7", regarding both as nullary functions that single out equivalent elements of `Int`.
&gt; Personally I don't like to equate non-functions (number, bools, trees, etc) with 0-ary functions any more than I to equate them with singleton lists (or trees, ...) or non-Nothing Maybe values, etc. I actually *do* like to equate "non-functions" with functions, in the sense that 5 can be regarded as the function `5 f = f . f . f . f . f`, `Just True` as `\z f -&gt; maybe z f (Just True)`, `[3]` as `\f z -&gt; foldr f z [3]`, and so on. But I don't think that's what most people have in mind...
Thanks again for your answers; I'm really enjoying the discussion. As always, I've been vaguely aware of these general principles for a while, but it's one thing to be able to recite them, and another to see how they influence an (ahem) real-world calculation. &gt;  there is a variant of `if` such that `if  then a else b` is not necessarily ``, basically this `if` evaluates `a` and `b` in parallel, and returns approximations on which they agree, and finally forces the evaluation of the guard once it actually is needed. I've been told this is actually useful for modelling hardware. Are you talking about something like [Conal Elliott's `unamb`](http://conal.net/blog/posts/functional-concurrency-with-unambiguous-choice)? His posts always fascinate me, but remain over my head for now. &gt; Also, why not just call it `init`, like the Prelude does? :-) Because I don't know the Prelude very well? :-) &gt; You can sometimes use GHCi to test strictness properties, for example you certainly are doing something wrong: I agree that these are wrong in the sense that one should be able to take the `init` of `1:undefined` without trouble, but they fit right in with the abstract evaluations above ([yours](http://www.reddit.com/r/haskell/comments/crczc/tails_and_infinite_lists/c0uvtji) and [mine](http://www.reddit.com/r/haskell/comments/crczc/tails_and_infinite_lists/c0uyl54)), right, both of which predict that an input tainted with bottom, even if it's at the end, will always produce an output also tainted with bottom? Is it possible to write a `foldr`-based `init` that doesn't have this property? EDIT: Actually, I now believe that one *shouldn't* be able to compute `init (1:undefined)` (`Prelude.init` can't do it, either), because any sensible implementation would have to look at `undefined` to decide whether or not it's the empty list.
If y is not defined somewhere and in scope, that's exactly the message you'll get, e.g. Prelude&gt; let f x = x + y &lt;interactive&gt;:1:14: Not in scope: `y' 
Totally. But what I meant was something like [this](http://www.reddit.com/r/haskell/comments/cs3gw/so_im_eating_breakfast_this_morning_posting_about/c0utahz): I thought (for some stupid reason) that functions... got their own scopes or something. Like I said, I know that that doesn't make any sense... but it happens. I know better now.
But then you'd have to apply variables to some value to obtain their value - e.g. given ```x _ = 6```, you'd have to access the value of x using something like ```x ()```. It would also break the equivalence between functions defined as follows: f x y = x + y f = \x -&gt; \y -&gt; x + y The latter would become ```f _ = \x -&gt; \y -&gt; x + y``` which is equivalent to ```f _ x y = x + y```. 
We can make a categorical version of hylo hyloC (CFunctor f ~&gt; ~&gt;) =&gt; (a ~&gt; f a) -&gt; (f c ~&gt; c) -&gt; (a ~&gt; c) hyloC g f = f &lt;&lt;&lt; cmap (hyloC g f) &lt;&lt;&lt; g Now we see that hyloM is just the Kleisli instance of this. hyloM :: (CFunctor f (Kleisli m) (Kleisli m)) =&gt; (a -&gt; m (f a)) -&gt; (f b -&gt; m b) -&gt; a -&gt; m b hyloM g f = runKleisli (hyloC (Kleisli g) (Kleisli f)) The only thing we are missing is an instance for `CFunctor f (Kleisli m) (Kleisli m)`, but there is always one when `f` is traversable. instance (Traversable f) =&gt; CFunctor f Klesli Klesli where cmap (Kleisli f) = Kleisli (mapM f) Edit: edwardk says that this instance is "conflict ridden, it says that the only endofunctors you have on a klielsi category are traversable, which isn't the case."
You're confusing the value 5 with the function 5. ;)
&gt; Case sensitive - why does types has to start with uppercase and functions with lowercase. This was under *The Bad* section. I think this is something that Haskell did very right. It is too common in other languages to have inconsistent naming conventions. Enforcing case for distinguishing types and functions makes all Haskell code more consistent. &gt; Static typing sucks. The majority of Haskell would not work if it used dynamic typing. This is like saying you don't like it because it is functional. It is always enlightening to read other people's view of Haskell. Thanks for sharing.
&gt; The bad: &gt; ... &gt; In Haskell everything is carved in stone. Programming in Haskell is like proving a mathematical theorem. I disagree with almost everything else he says but these two sentences are spot on. However, I wouldn't put it in the "bad" column - it's what makes Haskell so great to use. 
oh okay thanks! again, I'm a beginner myself, so I'm sure this thing's filled with errors. I have a lot of problem with types
Thanks :) The terminology really kills me haha. 
He specifically asked for a few lines of syntax, he's currently reading tutorials as well. And yeah, he'll be using this for math a lot :) 
Possibly, but do you see how it is designed to scale the page downwards?
I do now. Fair enough. But the first thing I thought when I clicked was "wow! how did we go from all those nice designs to this?"
&gt; If I'm tracking you, then although we've defined "y = 7", it's not the case that y is equal to 7. Indeed; depending on how you interpret that "y" which is not equal to 7. We can consider such nullary functions as the limit case of the syntax for function bindings. That is, there's an invisible argument which is the unit of juxtaposition, and whose type is the terminal object. If "y" means the function from the terminal object, then no y is not equal to 7. If, however, "y" means the function from the terminal object applied to the value of that type, then actually y is equal to 7. If we wanted to minimize invisible things then we could give definitions using a syntax like `idx = x` and `constxy = x` (assuming the visible space, , shows up in your browser). Now, what about when we have two visible spaces next to each other as in `fx = e`? Well, if we consider the empty string (the one between those visible spaces) to be the only value inhabiting the terminal object, then we have that the denotation of `fx = e` is the same as the denotation of `fx = e`. We'd similarly like to be able to ignore issues of associativity for . Thus, we have that  is a monoid on patterns/expressions with the invisible pattern/empty expression as its unit. And consequently we have the descending chain of function bindings: ..., `fxyz = e`, `fxy = e`, `fx = e`, `f = e`. The difference in ontological categories is, effectively, the difference between `f` and `f`. The former is a function from the terminal object, whereas the latter is a value (namely the image of `f`). As mentioned elsewhere, most functional languages make the coercions between them invisible. If they didn't we'd all be driven mad by the verbosity. However, when dealing with monads (even the `Identity` monad!), we have to make those coercions explicit. In a more ideal language, we would have notation that allows us to locally redefine the visible space (aka `(&lt;*&gt;)`/`ap`), the empty string (aka terminal object), and the invisible prefix that licenses the presence of a value (aka `pure`/`return`)--- that way we can reduce the syntactic dissonance between pure code and monadic/applicative code. Another place this more-ideal notation would be a boon is by leaving the visible space and empty string alone, but locally redefining the invisible prefix to something like `realToFrac`. This would greatly simplify, e.g., the code for smoothing of probability distributions from observed counts. We typically want our counts to be natural numbers, but then we have to convert them before we can combine them to produce a real value in the unit interval. If we could make those coercions implicit, then we really could just write down the mathematical definitions and still maintain type safety! *edit:* Yes, I know all this is terribly heretical. So far as I'm aware, it also has nothing to do with what other people have in mind when they talk about variable bindings in Haskell as defining nullary functions. Whenever I've questioned those people about their justification/interpretation, they've usually given me an explanation like the operational one mentioned elsewhere, comparing the operational denotation of laziness to the way strict languages like OCaml (or C) implement laziness with the `(()-&gt;)` functor. Nevertheless, being fond of heresy, I do think my position is a soluble one and is one which helps to unify a number of threads throughout PL research. Beware the [pre/trans fallacy](http://www.google.com/search?q=%22pre+trans+fallacy%22).
No, the two operators have different types: unamb :: a -&gt; a -&gt; a versus if' :: Bool -&gt; a -&gt; a -&gt; a I guess it's a bit like `unamb`, except `unamb` has a precondition that says the two values must agree (unless one is bottom), whereas `if'` starts out evaluation like `unamb`, but once the two values disagree, it evaluates the boolean to figure out which one to select.
&gt; Couldn't we just say that func = ... is syntactic sugar for func _ = ...? Well, there's a catch: "`foo _ = ...`" already is syntactic sugar for "`foo = \ _ -&gt; ...`". Combining the two desugarings would get us into an infinite desugaring loop at the get go for every binding. 
Since we're explaining things to beginners we should also note that the view that "lazy IO is bad" is not shared by everyone, in fact there is a continuing and fairly lively debate within the community on the issue.
It's true, the initial colour could be made to be somewhat closer to the average colour of the final image, since it takes a second or so for the background to appear after the initial page load.
Functions don't take type arguments in Haskell. Type families give you something akin to functions over types, but 5 is really just a value, even in Haskell. 
&gt; Once you desugar the definition By that same reasoning, we could desugar "f = y" into "f = \ -&gt; y". The problem here is just a syntactic one. Fundamentally, the terminal object must be something baked into the language definition. And for the reasons discussed below, in Haskell it must also be ineffable. The unit type isn't terminal because it contains both `()` and bottom. Empty data decls aren't terminal either, because they contain multiple "bottoms" which can be distinguished (e.g., extensible exceptions, catchable nontermination, real nontermination). Because there are so many "bottoms", we must forbid users from writing *any* expression to stand in for the value of the terminal object. The only value of the terminal object is *the empty expression*. Since the notation "()" was taken for the unit value, we have no visible way to write the empty expression. Whence the confusion.
&gt; By saying that arity can only be 1 you're confusing the presentation in which arity is a sensible notion with the presentation where everything is a simple function. Presentation of what? If you mean of typed functional programming in languages like Haskell &amp; ML, then I'm guessing you mean the presentation where every *function* (not every *thing*) is a function of one argument. Or do you mean presentation of something other than Haskell/ML? I've been wondering throughout this discussion thread--especially while reading your comments--whether we're carrying different contexts/assumptions into this conversation. For instance, the notion of arity other than one does not exist in Haskell &amp; ML, though it does exist in other formalisms and it does exist in people's informal/vernacular thoughts &amp; words about Haskell/ML programming. I.e., the unit arity property of *all* functions is a property of the precise definitions of Haskell &amp; ML. As is the existence of values of non-function types and definitions/bindings for such values. (*Edit:* added previous sentence.) And yet some comments read as if non-unit arity were something actual &amp; precise. Have we been discussing different questions here? (Definition of Haskell/ML vs ?) 
&gt; Why complicate your universe with non-functions when functions naturally generalize to cover them? Similarly, why complicate your universe with functions when non-functions naturally generalize to cover them? For instance, every value (including functions) can be promoted to a list, tree, Maybe, pair, Id (identity `newtype` wrapper), etc. But then I wonder what type of thing is in those lists, trees, maybes, etc, and what type of thing is returned from those functions. And I do like static typing, which is lost when one wraps up everything into a universal type. For instance, we lose knowing that the list or tree has exactly one element, that the Maybe is a Just, and that the function is a constant function.
http://hackage.haskell.org.nyud.net/platform/mac.html claims 64 bit, but i see no evidence of 64 bit anything. anyone know anything?
I enjoy Church encodings as well. And yeah, I don't think that's what most people mean when they say everything is a function *in Haskell*.
&gt; By that same reasoning, we could desugar "f = y" into "f = \ -&gt; y". I wonder if we're talking about the same language. I don't think "\ -&gt; y" is even legit Haskell syntax. If it were, however, I think it would further desugar into simply "y" and thus would not have function type (unless y does).
Parametric polymorphism as found in Haskell is based on System F, a typed lambda calculus which represents polymorphic functions as taking a type argument, and that's the sense that I meant. For instance, the polymorphic identity function could be written (in a quasi-Haskell-ish syntax) as `(a  x::a  x::a)`. The capital-lambda abstraction is roughly equivalent to a `forall` in Haskell, with application of type arguments done implicitly by the type checker. On the other hand, if memory serves me, GHC Core uses an explicit System F-based approach. Type families provide type-level functions, which is not at all the same thing as binding type parameters for the type of terms within a scope. The type of 5 in Haskell is `forall a. (Num a) =&gt; a`, that is, a capital-lambda abstraction taking one type argument (that's an instance of `Num`), and returning a "5" value of that type.
I enjoy your ideas, especially the heretical ones! I just wouldn't want anyone to take them to be statements about Haskell (as it is). And the confusion I'm probing is the one that says in Haskell (as it is) "everything is a function". We have a precise meaning for "is a function", namely has type `a -&gt; b` for some types a &amp; b. And `True` does not have such a type. If the statement were "one can think of everything as a function" (or a list, tree, Maybe, Id, ...), then of course I'd agree. And as you point out, in some settings (other than higher-order functional programming), there are some advantages to doing so.
I found this example strange: -- this next one is cool whatisit :: Integer -&gt; [Char] -- [Char] means an array of characters, AKA a string whatisit n | n == 2 = "Two!" | n == 3 = "Three!" | otherwise = "I don't know!" -- that's just a cool way to do basecases -- first part is a condition, followed by an equal sign, followed by code It doesn't really show off a real use case for guards. If I were reading this without having seen Haskell, I wouldn't understand why this was better than the previous example you gave of pattern matching, that is, why you hadn't written it this way: whatisit 2 = "Two!" whatisit 3 = "Three!" whatisit n = "I don't know!" If you want an example that uses guards, perhaps fizzbuzz or something like it is a better example: fizzbuzz n | n `mod` 15 == 0 = "both" | n `mod` 5 == 0 = "fizz" | n `mod` 3 == 0 = "buzz" | isPrime n = "prime, lol" | otherwise = show n
Lightbulb moment! I finally get winterkoninkje's answer (probably unintended) to my question on how people can believe that everything is a function in Haskell. They don't believe it. They'd rather talk about something else, such as formalisms in which everything is a function. Or maybe they even have a different notion of what topic is being discussed. And I realize I often do the same thing. I hear a discussion topic that overlaps with one interesting to me (such as how do people form, spread, and solidify beliefs), and I focus more on the topic that interests me and less on the original topic. And if someone doesn't notice the switch, they can get confused. Wow -- this reason was in another blind spot for me. In this case, I made an assumption: &gt; Would it be heresy to henceforth call 'y' a function with no arguments in all of Haskelldom? I responded as if seydar were asking a question about the Haskell language. For instance, rather than about how Haskellers like to think about math. I really don't know the intention of that question. I focused on a possibly related question that I've been wondering about for a while now. 
Monad transformers are monads over Kleisli categories.
First off, if you want to try a haskell text editor, why not try leksah? It's a nice IDE for haskell. Although I do agree with you on The bad #2 (No rapid development.) The other ones I totally disagree, #1, static typing is safe and fast. I just prefer it, and sometimes I just forget it because of the inference. #3 The basic syntax (disregarding type declarations, do constructs, infix operators, imports and exports) you can directly translate between lisp and haskell, that is f x = mod 2 (div x 2) becomes (Define f (x) (mod 2 (/ x 2 ))), alternatively f = \ x -&gt; mod 2 (div x 2) become (Define f (lambda (x) (mod 2 (/ x 2 )))). And indentation in haskell actually follow very simple rules. When it comes to #5 and #6 I think GHCi is what you are looking for, although GHCi could be improved upon greatly. 
Yes, but 5 is just a value. 
 iter n = [x | x &lt;- [1..n]] means: iter n = [1..n] or: iter = upto
&gt; iter = upto Isn't it `enumFromTo 1`?
Yes, he just defined upto too above there.
PHP works the way you describe: every function gets its own completely isolated scope, and if you want to access a variable in an outer scope, you have to declare it, e.g.: function f($x) { global $y; return $x + $y; } One reason PHP does this is that you can create new variables on the fly without declaring them, so this prevents inadvertently accessing variables you didn't mean to access. The need for declarations to access non-local variables isn't particularly onerous in PHP because it doesn't support nested functions in any useful way. However, in languages that support nested functions and the associated nested scopes, requiring declarations to access a variable in an outer scope would be a pain, so such languages don't require them. This is true of all the languages that are based on the lambda calculus, like Haskell, the ML family, Scheme, and Lisp. 
This article is two years old and really not worth reviving. Haskell is a difficult language with a steep learning curve. Lots of people give up early, and that's understandable. But those who pretend this makes them experts in language design should not be encouraged in their delusions. 
I regularly encounter this perception. It's simply and outright wrong. There is also the perception that armageddon arrives in a couple of years. This is wrong too.
I wonder what happens when everything-is-a-function and everything-is-a-tuple meet up.
3 and 5 aren't prime?
 fac n | n &lt; 0 = error "You bastard" fac n | n &lt; 2 = 1 fac n = go 2 n where go acc 2 = acc go acc n = go (n*acc) (n-1) ...which at the same time exemplifies the wrapper/worker idiom.
It's a [caching service](http://www.coralcdn.org/), which I used for social networking links to avoid the situation we had last time where the servers became unresponsive due to load.
MacPorts has a 6.10.x 64 bit port. The downloadable one is a 32 bit version of GHC which will run on 64 bit Mac. It's more to reassure the Mac users they're getting the right GHC for now.
That's why I should read TFA :)
I thought the type-system (without UndecidableInstances) was at most capable of primitive recursion. Is this wrong?
I think most folks who are confused about Haskell aren't going to be thinking about how it desugars. To think about desugaring presupposes a certain level of knowledge about what's going on under the covers. Witness, for example, the folks who complain about Haskell having "too much syntax"--- due to thinking of `(&gt;&gt;=)` and other symbolic functions as "syntax" because symbolic functions are built-in operators in most mainstream languages. Anyone who looks at "x &gt;&gt;= y" and sees `(&gt;&gt;=) x y` or who looks at "f x = x" and sees `f = \x -&gt; x` has already formed a decent internal model of how Haskell works. And those people aren't going to call "f = 7" a nullary function definition nor call symbolic identifiers 'syntax' (unless they're being heretical). So far as I'm aware, "\ -&gt; e" is not legal syntax in any current variant of Haskell. However, it's easy enough to imagine an -XNullaryLambda extension to enable it. If we go on to assert that "\ -&gt; e" is identical to "e", then clearly this would be a silly language extension. However, I could imagine wanting those two expressions to be unequal--- for example, if you were writing a new Haskell compiler in a similar vein to JHC (i.e., to explore the low-level possibilities) and you wanted to have a syntax (rather than merely typing annotations) for distinguishing thunks from unboxed values, then "\ -&gt; e" could make a perfectly good syntax for thunking up an expression (for the same reasons the `(()-&gt;)` functor is used in OCaml etc). If your new compiler is using this representation internally, then why not expose it to the surface syntax via -XNullaryLambda? Of course, you'd have to work out all the details regarding `seq` and bottoms to be sure that you're not changing the semantics of anything else in the syntax; but that's true of every language extension. My point in raising "\ -&gt; e" wasn't to propose -XNullaryLambda, it was to counter your claim that *clearly* "f x = x" means exactly "f = \x -&gt; x". If someone is being confused about Haskell, then I do not think that translation is necessarily clear. If someone is told that the statement "i = 7::Int" *means* that "`i` is an `Int` because `7::Int` is", and someone is also told that "f x = x" *means* that "`f` is a function because it has an argument". It seems perfectly reasonable to think that they might conclude that "`x` is a function" because `f` is. With "i = 7::Int" we are told that the thing being defined has the same type as the RHS. With "f x = x" we are told that the thing being bound is a function. Why can we not apply transitivity of identity and conclude that the RHS, namely `x`, is a function? If you scoff, it is because you already have an internal model of how Haskell works and what the syntax means. Given *only* what is stated in the previous paragraph, there is nothing illogical about the conclusion. The reason we consider such conclusions baffling is because we cannot help but already know what the glyph "=" means in Haskell. To someone who is new to the language and who is used to definitions looking like "foo(x,y) { ... }", they have no reason to think the "=" glyph has anything whatsoever to do with mathematical equality. The first time I encountered the use of "=" to define functions in Haskell, I know *I* certainly thought it was a bizarre syntax, at best, and a misuse of assignment, at worst. But then, at that point in time I was thinking about pointers, memory stores, and blocks of machine instructions. Functional languages like Haskell do not render themselves easily into such a von Neumann ontology of what programming means and what programs consist of. While there were clearly connections between programming and mathematics, the notion that one could program *in* mathematics was beyond the pale. If someone told me then, "you can code in mathematics", I wouldn't even know what that could mean, let alone believe it. My suggestions about how "f = 7" could be understood as defining a nullary function should not, as mentioned below, be confused for trying to offer an explanation of what newcomers to Haskell are thinking. Many of those newcomers are coming from a pervasive background in strict, imperative languages. Since those languages distinguish nullary functions from non-function values, and since they are strict, it seems natural to try to understand laziness as nullary functions. But the major differences between Algol-like syntax and ML/Miranda-like syntax should not be overlooked. People raised on the Algol tradition really love their syntax. When Java was first coming out I remember people saying, essentially, that it was a powerful language because it has so many keywords for modifying variable declarations. Just last night I had an elder in a coffee shop regaling me with tales of Programming In The Old Days and about his skill in finding syntactic errors like missing statement terminators. These people will have to unlearn their love of syntax before they can recognize the simplicity of Haskell's. In the meantime, they will find all manner of creative ways to misinterpret simplicity :)
&gt; Since those languages distinguish nullary functions from non-function values, and since they are strict, it seems natural to try to understand laziness as nullary functions. And ironically, they're absolutely correct. With the Spineless Tagless G-machine all[1] data is stored as closures, including fully evaluated values, and including non-functional values from algebraic data types. We use essentially the same implementation for our 'immutable' values as strict imperative languages use for nullary functions. [1] Excluding unboxed data. Of course, those data aren't possible in H98 or even H2010. 
&gt; But then I wonder what type of thing is in those... Pushing on the category theoretic angle once more: perhaps it doesn't matter? The usual presentation of CT talks about objects and morphisms. Due to the nature of CT we tend not to say a whole lot about what those objects are, but we can take things even further. Fundamentally, there is no reason why objects are necessary. Their only purpose is to give name to the domains and codomains of morphisms. But what are the nominata? Because every object has a unique identity morphism, we could take those identity morphisms to give definition to the class of things which can be the domains or codomains of (1-)morphisms. If we did so, then we wouldn't need to postulate a separate class of "objects". First-order[1] category theory can be presented perfectly well by taking morphisms to be the only ontological entity. Similarly, in the untyped lambda calculus we take every expression to be a variable, and application, or an abstraction. But if we dig down through all the gymnastics of argument passing, what is it we're actually passing around? If we ignore all the argument passing then it seems that, clearly, we're not passing anything at all. And yet, all the algebraic datatypes we know and love can be encoded in this austere language. So it seems that whatever it is we're passing around must be something in all that argument passing. The only thing *in* argument passing is the flow of control, but that's hardly a "thing" at all. We may as well ask: what kind of thing is computation? Taking the dual viewpoint where there is only data and there are no functions doesn't help either. Data in memory is merely a collection of pointers to other data. Just like with the lambdas, we're forever chasing, forever being told "whatever it is, it's over there". It's even worse to ask what data means. It seems like we could say that the meaning of programs is what they could do in any given run[2]. But data doesn't *do* anything, it merely *is*. It can't mean anything ---can't convey any information--- other than that it exists. To ascribe any sort of meaning to it we must interpret it, must render it into a program. But what is a series of actions other than just another kind of structure, another kind of data? In order to say what data is or what data means, we might as well ask what language is or how language gains its meaningfulness. [1] In higher-order category theory we may run into problems, because we wish to distinguish 1-morphisms, which map objects to objects, from 2-morphisms, which map 1-morphisms to 1-morphisms. I haven't thought about it much. [2] Though this runs immediately into philosophical problems. What means "could"? What *could* a program do?
I mean presentations (in a technical sense) of mathematical functions in functional languages, Haskell in particular; though the ideas could be applied to programming languages in general. In mainstream imperative languages there is a strong bias towards preferring a presentation of mathematical functions based on arity, just because of how the language's syntax is set up. When discussing functional languages we could stick with that presentation, but the alternative presentation where every function maps a single input to a single output (SISO) is more desirable because it handles HOFs better. But in this SISO-based presentation, the notion of "arity" is rendered meaningless. If every function has the same arity, then arity fails to discriminate anything. I could just as well say that functions have a squobble, and then assign every function's squobble to be %. Nobody would care because squobble contains zero information (in both colloquial and information theoretic senses). There's also the problem that it's senseless to compare terminology from one theory or presentation to terminology from another. The notion of "arity" only bears meaning in the theoretical framework within which it is defined, namely the arity-based presentation of mathematical functions. Talking about arity in the SISO-based presentation is like talking about Newtonian gravitation in Einsteinian physics. We can't do it because the theory which gives meaning to the Newtonian conception of gravity is not a part of the Einsteinian theory. Newtonian gravity does not exist in Einsteinian physics; we can't copy it over, and any attempt to translate it will sever it from that which gives it meaning. In the same way, we cannot talk about the Christian God in Buddhist theology. We may have a term "(Christian) God" within Buddhist theology, but it is not the same thing as the term "God" within Christianity. The latter is "the creator of everything" according to the theory in which it is defined; whereas the former is "the creator of everything, according to Christianity", according to the theory in which it is defined. Clearly these terms do not mean the same thing. &gt; ...whether we're carrying different contexts/assumptions into this conversation. You only asked what motivates one to call 'y' a function with no arguments rather than a non-function. These are my motivations. You never specified the context in which one should be motivated to do so ;) I've never bothered to write them out before, and you seemed interested in pursuing them, so I do apologize for willfully subverting the context you were asking from. Though I do hope I gave some answers for the intended context in the ends of my most recent comments. Oh, the things we do to avoid the thesis :)
Hee hee. Well, yes, I suppose that's another answer to take from all this. Though I think this particular issue is mainly a problem for us dedicated Haskeller's, not for the Haskell newcomers I usually see raising the topic of nullary functions.
*To try to answer the question about the development of beliefs:* I think part of it has to do with newcomers hearing the old-hands saying it, and so it becomes part of the dogma they learn. In the untyped lambda calculus (and a few others) everything really is a function, because lambda is the only term constructor. This realization finally sinks in for someone and they proclaim, "everything is a function!" ... ... Or when trying to figure out currying, once they realize that a binary function really is just a unary function returning a unary function: "we can return functions! Dude, everything is a function!" ... ... Or, when the old-hands are teaching newcomers to think functionally, they try to retrain them to think with functions instead of data structures. What is a map/set/hashtable/array, really? Often it's just an implementation of a (mathematical) function, so why don't we use a (Haskell) function? That way callers can say `foo (\k -&gt; lookup k m)` but they can also choose a different representation of their function. Either as pedagogy or as a stroke of enlightenment: "everything is a function!"... ... I have actually heard folks who are already indoctrinated in FP say this sort of thing when trying to teach FP to others (including when "FP" means Haskell specifically). For the instructor, the statement is usually hyperbole, IME. They don't really mean that *every*thing is a function. They're just trying to get the point across that everything *could be* a function (instead of whatever it is). For the student, sometimes they get it, other times they get confused or experience premature enlightenment along the way.
Had a weird issue after installing this, and then did `cabal install --user bytestring-mmap`. I received this message: `ghc: could not execute: /Library/Frameworks/GHC.framework/Versions/612/usr/lib/ghc-6.12.3/ghc-asm`. So I took a look in that file, and the shebang was `#!/opt/local/bin/perl`, which doesn't exist. Changed it to `#!/usr/bin/perl` and stuff worked again. Should I report this somewhere, is it a platform issue or a GHC issue?
I thought it was: f.x. f ( f ( f (f ( f x ) ) ) )
This is great! I'm already using pandoc to generate pdfs from HTML for reading blogs and things on my e-reader. It supports epub, so this is fantastic.. (eSlick). Although.. I may end up sticking with PDFs just because running things through LaTeX makes everything that much more readable. Another thing I've been trying is using Readability to get a page with just the text content (it strips site menus and such) that then printing it to a PDF. Combing Readability with Hyphenator works really well for that. But it's great to have an easy way to get ePubs.
We're talking about Haskell. 
I find your "why don't we call it a 1-element list" argument somewhat disingenuous. It's clear why we don't call values one-element lists: they don't have the right type, and they aren't written with the same syntax as one-element lists. Compare: name sample type sample value 2-ary function a -&gt; b -&gt; c \x -&gt; \y -&gt; z 1-ary function a -&gt; b \x -&gt; y 0-ary function a x value a x name sample type sample value 3-element list [a] [x,y,z] 2-element list [a] [x,y] 1-element list [a] [x] value a x Now, if you asked why we don't call values 1-tuples, then I might see that as an argument -- but then, I would also consider calling a value a 1-tuple quite sane (just as I consider calling a value a 0-ary function quite sane).
Using [jhc](http://repetae.net/computer/jhc/) and [devkitpro](http://wiibrew.org/wiki/DevkitPPC). Take note it's only a hello_world template project, there are no haskell bindings for devkitpro sdk right now. I'm thinking about doing it.
Did you have a look at [cmdargs](http://hackage.haskell.org/package/cmdargs) ? I'm not sure "looking up" the library and reading its doc is more expansive than all the cons you describe for your approach. This bullet is interesting: &gt; Specify to the library exactly which options you want to accept, &gt; whether they take any arguments, and provide a usage note for the &gt; autogenerated help text. It is exactly what I expect to have to provide to a command-line parsing library, and it is what I still have to disseminate through your approach.
That's... a lot of compile errors, there.
I like digits-of-e1, where turning on the jhc garbage collector somehow causes a _compile_ error.
Note that the title says "Haskell scripts". For polished "professional" kind of code, that or some other library might well be the tool of choice. The context here was meant to be small programs largely for personal use, possibly thrown around different machines. Of course this is my personal experience, but: * I don't care about long options as long as I don't run out of letters. In the context of the post (environment variables are used for non-bool options) this won't happen for a while. * "unrecognized option" complaints are nice to have, but not really important to me most of the time. * Same for autogenerated help. Nice to have, but in most scripts, I don't care. What I do care about is not having to think about it. I just want to say "hey, did we by any chance get -d?" and be done with it. I need to get the list of non-option arguments. That's exactly what the five-liner provides. Plus a convenient (!) getenv function to avoid the complexity of option arguments. In essense, it's a smooth "upgrade" path: from files &lt;- getArgs do_something_with files to files &lt;- getargs x &lt;- getflag 'x' if x then do_something_with files else do_something_else 
I hink command line arguments is calling our for a quasi-quoter.
Have you seen cmdargs? A quasi-quoter is great when you can't get the syntax close to what you want with plain Haskell, but if you can (and cmdargs gets very close) there are lots of benefits to sticking with Haskell.
*Wow*! I actually didn't notice this one, but if you click on 'Compile Error' in the web page it will highlight the error. In the case of compiling digits-of-e1 with jhc using -fjgc, it actually triggers an internal GCC compiler error! Found 1 CAFs to convert to constants, 0 of which are recursive. ... [snip] gits-of-e1_code.c: In function ftheMain: digits-of-e1_code.c:9901: warning: dereferencing type-punned pointer will break strict-aliasing rules digits-of-e1_code.c:9933: warning: statement with no effect digits-of-e1_code.c: In function E__fMain_e: digits-of-e1_code.c:4077: internal compiler error: Segmentation fault Please submit a full bug report, with preprocessed source if appropriate. See &lt;file:///usr/share/doc/gcc-4.4/README.Bugs&gt; for instructions. jhc: user error (C code did not compile.) Lovely. :)
A lot of the problem is missing support for floats/doubles, which causes LHC to fail on several tests with compilation errors (we're not sure how to rectify this atm.) The remaining compilation errors are mostly because our HPT analysis is extremely memory hungry as it stands, although not as bad as it was a few weeks ago.
yes, it looks great, I haven't used it yet but am planning on it for my next command line usage. Thanks for making it! I guess I am just a big fan of DSLs- and when there is something used as frequently as command line args, then there is motivation to try to make one unconstrained by haskell syntax.
Other solutions have many virtues, but I'm surprised nobody has shown you how to do this with a lazy Data.Tree, just as you asked. One problem is that this particular module is rather sparse - it doesn't even define the fold foldTree f (Node a bs) = f a (map (foldTree f bs)) The second problem is that as you noticed unfoldTreeM produces the tree strictly. I don't see how to do it in the parameter alone, so we'll have to make our own version to wedge in an unsafeInterleaveIO unsafeUnfoldLazyForestM f bs = mapM (unsafeInterleaveIO . unsafeUnfoldLazyTreeM f) unsafeUnfoldLazyTreeM f b = do (a, bs) &lt;- f b ts &lt;- unsafeUnfoldLazyForestM f bs return (Node a ts) Now define a traversal function to taste. Mine takes a path to the pair of the file size and a list of directory contents. (boring definition at the end) entryInfo :: FilePath -&gt; IO (FileOffset,[FilePath]) Now we can find total size under a directory tree main = do [path] &lt;- getArgs contents &lt;- unfoldLazyTreeM entryInfo path print (foldTree (\a bs -&gt; a + sum bs) contents) -- foldl' (+) 0 (flatten contents) also works or the size of the biggest file main = do [path] &lt;- getArgs contents &lt;- unsafeUnfoldLazyTreeM entryInfo path print (maximum (flatten contents)) Both run in constant memory, staying within the default 11MB heap on my 36GB home directory. (at least compiled -O2 - I hardly ever compile without it) I imagine you could thread a set of inodes to cull duplicates. As for your points, Your #2 is abandoned as soon as you decide to produce an apparently pure value that secretly does IO. We don't have a standard idea of "functionalish". In Haskell the watchword is referential transparency. Explicit IO is pure in the Haskell sense, the lazy tree thing is a slightly questionable hack (can you meet your proof obligation for that "unsafe" - actually yes). Your #3 slightly misdiagnoses the problem. The Ruby code doesn't use standard data structures either, what it does use is standard interfaces for iteration, the "each" and "reject" and so on. (It also doesn't seem to be working in a pure way - your each block includes a direct call back to the top-level traversal, much as in frud's code). Haskell doesn't have any standardized interfaces for this, in part because lazy lists make great iterators in pure code. Iteratees especially are one attempt to describe a standard interface imperative producers of streams of values can offer, and even one which allows them to work with pure consumers. I can explain a bit more how iteratees can support things like "reject", if you want. hyloM/annihilate are also nice. ---- Here's my function for the unfold: -- returns fileSize for the path, and a list of -- directory contents if applicable, with . and .. removed. entryInfo :: FilePath -&gt; IO (FileOffset,[FilePath]) entryInfo path = flip catch (const (return (0,[]))) $ do status &lt;- getFileStatus path subdirs &lt;- if not (isDirectory status) then return [] else do entries &lt;- getDirectoryContents path return [path &lt;/&gt; d | d &lt;- entries, d /= ".", d /= ".."] return (fileSize status, subdirs) 
I agree entirely, command line options are a required part of every program, so need to be concise. If the conciseness can't be achieved by Haskell alone, you need a quasi-quoter. As it happens, cmdargs currently does have some duplication - you have to both specify the record, and then provide a value for it. Using quotes I could specify/default the record in one step, which would be a good idea. I'll look in to adding a quoter for the next version.
Mudge's doppleganger??
You actually only need a "right seminearring" rather than a full semiring. That way you can see a recognizing parser as one that extracts a right seminearring, and a more traditional attribute grammar as one that extracts an Alternative, which just adds a parameter (attribute) to your right seminearring. So then you can view a parser that extracts the first result, or one that extracts all possible parses as just different choices of alternative. See the RightSemiNearRing class in my 'monoids' library for more information.
We may not need left distributivity but we intentionally require addition to be commutative such that the regular expressions a|b and b|a are equivalent. We also need an identity one for multiplication. Your RightSemiNearRing class also requires it via the Multiplicative constraint, but the definition of near-semirings on Wikipedia does not. Regarding your comment on Alternative Functors, I have the impression that they lack the distributive law, right?
Would you care to comment why you think this is necessary?
The choice of whether or not to have multiplication be based on a monoid or a semigroup is one of background and intention. The definition that wins the edit war on Wikipedia isn't necessarily canonical. ;) The usual derivation of semiring has two monoids, the simplest change to take it to 'near' is to remove one of the distributive laws, this seems to be more common in computer science circles than in mathematical circles, but it is the computer science crowd that really first had a use for this notion. With regards to a|b and b|a being equivalent, that actually fails to hold once you start allowing for extraction of attributes and/or when you move beyond regular expressions into context free grammars and try to apply different parsing algorithms to the grammar. &gt; Regarding your comment on Alternative Functors, I have the impression that they lack the distributive law, right? Alternatives give you either a "left" (technically, right) distributive law or "left catch": http://www.haskell.org/haskellwiki/MonadPlus_reform_proposal Depending on your point of view, you can view some subset of those definitions as being an abuse of syntax or semantics brought about by collecting the laws post hoc. ;)
&gt; Alternatives give you either a "left" (technically, right) distributive law or "left catch": &gt; http://www.haskell.org/haskellwiki/MonadPlus_reform_proposal If Alternative (unlike MonadPlus) "seems innately bound to Left Catch, at least in spirit" then distributivity is gone. Of course, if it is distributive then it is distributive ;) and gives rise to a seminearring. &gt; With regards to a|b and b|a being equivalent, that actually fails to hold once you start allowing for extraction of attributes and/or when you move beyond regular expressions into context free grammars and try to apply different parsing algorithms to the grammar. Could you elaborate on why attribute extraction breaks commutativity? By parsing algorithms that break commutativity, do you mean those based on backtracking? Is it only a matter of termination and/or efficiency? Because I used to think of alternatives in CFG as commutative and of non-commutative choice operators in parser combinator libs as (incomplete) optimization. *edit: added 'incomplete' before 'optimization'*
In full generality, for the same reasons that I think that didactics should be taught as early as 7th or 8th class. Specifically, because it is beneficial both to the project a presenter represents and to the wider audience that is being reached via internet when a topic is presented in a way where you don't have to force yourself to pay attention, even if the programme committee and locally present audience might be well accustomed to such a style. You don't want to scare young academics away from your field. Now, that was snarky, but face it, a not insignificant number of scientists just plain suck at presenting, and presenting complex things isn't an excuse for presenting them badly. Also, one should learn something new that doesn't directly relate to one's field of specialisation every now and then.
"Although the ideas behind the algorithm are not new but based on proven results from theoretical computer science, there is no correctness proof for the equivalence of the Haskell implementation of the algorithm with its specification. It is therefore confirmed using tests." No references. No explanation. Just a good old fashion 'it works, here's some test cases to *prove* it'. This is where you lost me. 
I haven't looked into the course itself, but the description lists things I wouldn't consider relevant for a conference presentation (designing exam and homework questions, incorporating histories of science, teaching for transfer, planning a course). If ICFP presenters need help to not suck at presenting (which I can't confirm from previous experience), I guess I would point to something more specific like http://research.microsoft.com/en-us/um/people/simonpj/papers/giving-a-talk/giving-a-talk.htm. But, hey, I will actually take a look into the course you propose. :-) 
Hmm. I deliberately wrote: it (the equivalence) is *confirmed* (not proven) using tests. The subsequent paragraph elaborates on how the tests have been carried out. The linked HPC report contains the complete test program. I'm sad I lost you. Can I change the wording to win you back? edit: The equivalence is not confirmed by giving "some test cases" but using QuickCheck to check the property matcher regexp input == spec regexp input for many random regexps and inputs. Maybe I should clarify this, thanks!
Cool. I have a notion that the idea quasi-quoter would look similar to the output. sample = mkCmdArgs [$cmdargs| -h --hello=String World argument (default=world) |]
Unlike suspected in my previous answer, I think we do need left-distributivity. In the scene on infinite regular expressions we have used left-factorization to recognize a^nb^nc^n which makes use of left-distributivity. Another interesting example of this technique that does not involve infinite regular expressions is [the eachOnce combinator as defined by Sjoerd Visscher](http://www.haskell.org/pipermail/haskell-cafe/2010-July/080948.html).
Hmmm. Taking question-design and incorporating history as an example: Instead of saying "Recall the definition of Tim Obscure's equation of confusing fact, which in its generalised form solves the equation on the screen" (a whole slide full, which you then read out loud thrice, freely intertwining transformation steps with your proof), say "Recall that Tim Obscure solved Foo with (high-level view of the equation), which is great because he did half the work for me, we only need to do Baz and this darn slideful transforms into (next slide) because (intuitive high-level analogy quux). Proof is in the paper, if you like gritty details." That is, if you read those lecture titles in their concrete formulation, things don't seem to be applicable. Yet, every presentation contains questions and you have the choice every and each time again which level to ask and answer them on; everything has some kind of historic background which puts things into context and is a quick, easy and interesting way to answer a non-obvious "why the heck would I want to do that?"; you in general, want to get your points across instead of dictating the paper to the audience; and lastly you want to know how to layout everything so that people don't have to force themselves to stay attentive. SPJ's talk about talks is great, but it mostly addresses dramatic tension and how to avoid the nastiest of pitfalls. It doesn't go into the depth nor width (nor is long enough to do so) that is required to design (or analyse) whether a talk is acceptable, or actually great. Another reason to watch the lectures is understanding that most if not all of those topics you loathed as a student you don't actually loathe, you just hated how nothing the lecturer said made any sense. I actually went on and understood newtonian mechanics, having sufficient indication that those equations are neither random nor arbitrary. He at one time mentions an experiment MIT had done: They asked second-year students to take the physics exam they already took as freshmen. Virtually 100% failed, having forgotten all the material they could pass an exam with in a mere year. So, if nothing else, you don't want anybody there to ever forget what you taught them. Otherwise, they'll never remember you when it comes to choosing the next nobel prize winner...
Okay, good reasons (except for the last one, I guess) I might want to look into the lectures (which is what I wanted to know). Funnily, though, I can't really remember I loathed topics as a student. Those that I didn't pursue further, were not interesting enough. And I don't consider them interesting now. You will say, it's because the respective lecturers were not good enough. However, I think I have enough samples to conclude that there was no all too strong correlation between quality of the teaching and my affection to topics. 
I remember disliking some subjects, but I don't remember many of my lecturers making no sense. My clearest memory of being totally lost during a lecture is from an abstract algebra course, but I like that subject.
&gt; If Alternative (unlike MonadPlus) "seems innately bound to Left Catch, at least in spirit" I saw that claim on the wiki, but I don't believe it when it comes down to it. =) The choice of Alternative is bound to the choice of MonadPlus by convention. If the MonadPlus satisfies left distributivity then so does the Alternative. &gt; Could you elaborate on why attribute extraction breaks commutativity? Because the order in which results get recognized becomes important, because you can observe the order of the results based on your choice of Alternative result. Given (True &lt;$ char 'a') &lt;|&gt; (False &lt;$ char 'a'), reordering those will reorder the results from [True, False] to [False, True] or if viewed in Maybe, like you'd get with a Parsec-style parser, you will get Just True or Just False depending on the order.
It is possible to break commutativity by extracting attributes. It is also possible to allow for the extraction of attributes without breaking commutativity. For example, the set monad has commutative addition and is distributive.
Most academics already attend "how to write", "how to give a talk", "how to tutor", etc. style courses as part of their PhD study.
With all due respect, it does not universally show. 
My point was simply that most real world parsing engines violate that assumption of commutativity. You can always of course go off and work in that smaller quotient where it holds, just like it is occasionally useful to project into a full semiring, but I think it is useful to be able to care about the order of those rules in practice. It is easier to retain the distinction and discard it, than it is to try to somehow recover it once it is lost. (And appealing to Set to reorder your attributed results doesn't work once you have an infinite result set.) =)
Ok, I think I just misread your comment that commutativity &gt; actually fails to hold once you start allowing for extraction of attributes and thought you meant it fails automatically. I agree with your general point and it is indeed awkward to reorder results even if one does not care about their order. However, when considering the meaning of a regular expression, choice is commutative and sequence distributes over choice. So I wonder which requirements we may drop without loosing the property that the matching algorithm is equivalent to its specification. When you say that we &gt; actually only need a "right seminearring" rather than a full semiring. do you mean that the matching algorithm and the specification compute the same results even if the result type is not a semiring but only a right-seminearring? In the light of my [other comment](http://www.reddit.com/r/haskell/comments/cttz2/haskell_meets_automata_theory_a_play_on_regular/c0v9xkw) this would mean that regular expressions may not be rearranged in an intuitively meaning-preserving way, but at least the matcher and the spec would still agree on the result of matching structurally equivalent regexps.
It occurs to me now that by &gt; No references. No explanation. you may have meant that there is neither a reference to nor an explanation of the *matching algorithm*. If you meant that: the explanation is in the referenced ICFP paper and not repeated on the webpage.
If it didn't improve their performance the first time around, what makes you think taking another course will?
Two things, mainly: The lecturer of this series is a very, very good example of a very, very good lecturer himself, which very likely means that this course is better than any anyone already took. In this sense, the lecture is metacircular. The second thing is that with a concrete example in mind, points tend to come across much better. Hearing such a lecture while worrying about what topic to write one's thesis about vs. hearing it with a clear understanding of what one wants to present -- and possibly knowing which subtle points others non-specialists might have problems with, just having no idea what to do about that -- makes a huge difference in how one is then able to apply didactics. Well, and possibly repetition alone helps, too. It's not that the lectures are in any way boring (the opposite is the case) or would require tons of side-study, you could e.g. just watch them instead of a sports game or something.
&gt; When you say that we &gt;&gt; actually only need a "right seminearring" rather than a full semiring. &gt; do you mean that the matching algorithm and the specification compute the same results even if the result type is not a semiring but only a right-seminearring? Correct. For instance you can generate a result trie, even though you may only have a right distributive law for the trie. A good example of the use of right seminearrings in this general sort of setting can be found here: http://conway.rutgers.edu/~ccshan/wiki/blog/posts/WordNumbers1/ http://conway.rutgers.edu/~ccshan/wiki/blog/posts/WordNumbers2/ http://conway.rutgers.edu/~ccshan/wiki/blog/posts/WordNumbers3/ http://conway.rutgers.edu/~ccshan/wiki/blog/posts/WordNumbers4/ However, Chung-chieh and Dylan are interested more in properties of generated text than in those that result from parsing it. &gt; In the light of my other comment this would mean that regular expressions may not be rearranged in an intuitively meaning-preserving way, but at least the matcher and the spec would still agree on the result of matching structurally equivalent regexps. It really comes down to a matter of utility. If you are willing to assume that you will only have the meaning of your regexp observed by a semiring you can view more expressions as equivalent under that particular choice of quotient. However, if you accept that real world parsers emit matches and extract ranges from the result, rather than just return a boolean yes/no with regards to whether or not the expression matches, then fewer such regexps are equivalent under the broader class of right seminnearrings. I merely wanted to bring to your attention that the finer distinction could be made. ;) I recognize that both are useful.
Neat, but I still prefer parseargs over this or cmdargs
cmdargs can't actually parse arbitrary types, right? I looked at it for a program that needed to take multiple values for a flag (:: Maybe (Double, Double) and :: [DoublePair]), but cmdargs doesn't seem to have a way for me to supply the parser.
Or you can use skew binomial random-access-lists, and get O(log n) indexing and drop as well.
Heh, will have a look at those beasts. But here we have unique requirements, &lt;Use&gt; objects function as the cross-connects between defs and uses and occur in millions. Reducing them to 3 pointers is already a big feat, and in a proof of concept I could go down to 2 pointers (encoding the Value by waymarking along the chain). But that has proven prohibitively expensive :-( In this (LLVM) context every machine instruction counts, so my new 3-bit alphabet algorithm (in C++) did not show any improvements (yet) on relevant benchmarks. This is sad, because the constant factor of the algorithm did lower (it theory), but looks like the (minimally) added complexity already eats up the benefits. The chapter is not closed yet, though...
Do you have any single directory where "ls --color=never -1" produces more than 2MB of output (which could take up ~100MB as String), or have hardlink cycles in your directories? I see it take no more than 16MB (GHc 6.12.3, 64-bit debian 5.0.4 ). It's not especially space efficient, but the memory used at any point in the traversal should be proportional to something like the depth of the traversal, plus the total number of characters in entries in the current directory, plus the total number of directories of any later siblings of the directories on the path to the current directory. EDIT: I also see it getting suck in a symlink loop under /sys/devices / /sys/bus (the symlink recursion limit keeps it to a finite depth, but it would run for a very long time). If you replace all calls to getFileStatus with calls to getSymbolicLinkStatus (which doesn't follow symlinks) it won't get stuck like that, then if I slap an error handler around the getDirectoryContents calls (handle (\(SomeException _) -&gt; return []) $ getDire ...) it can finish weighing the accessible stuff under /. Still runs in 16 MB, in 11 seconds vs 3 for du / &gt; /dev/null.
&gt; I merely wanted to bring to your attention that the finer distinction could be made. You certainly succeeded ;) thanks! But the laws of right-seminearrings are *not* enough to show the equivalence of the matching algorithm and its specification from the paper. As an example consider the right-seminearring [String] (here defined by abusing the Semiring class, addition is not commutative and left-distributivity does not hold): instance Semiring [String] where zero = []; one = [""]; (.+.) = (++); (.*.) = liftM2 (++) We also need the following instance which associates each character to the singleton list with itself as a string as weight: instance Weight Char (Int,Char) [String] where symWeight f (_,c) = f c .*. [[c]] Now we can call the functions partialMatch and its specification from the quickcheck.lhs file to observe that they yield the matchings in a different order: ghci&gt; partialMatch (fromString "ab|ba") "aba" :: [String] ["ba","ab"] ghci&gt; partialMatchSpec (fromString "ab|ba") "aba" :: [String] ["ab","ba"] I agree that the broader class of right-seminearrings is useful but when claiming the equivalence of the slow and fast implementations from the paper, they are not enough. I wonder whether the algorithm can be changed such that it is equivalent to the specification for right-seminearrings but currently I don't see how. edit: The above code is for weighted-regexp-0.1.1.0, for version 0.2.* the Weight instance is simpler: instance Weight Char Char [String] where symWeight f c = f c .*. [[c]] 
This is really cool. I had been previously confused about what kind polymorphism was good for; the "natural transformation" example clarifies it well.
&gt; (can you meet your proof obligation for that "unsafe" - actually yes). I disagree. Using unsafeInterleaveIO for this task does not meet the proof obligation if you ever expect the file system to change. For example, if you wrote a backup server, and you wanted to compare yesterday's file list with today's, then when you evaluate yesterday's - surprise - you're really getting partly today's and partly yesterday's, depending on what evaluation has taken place before. Yes, you could use parallel strategies to force evaluation, but then you have to make sure there's a chain of forcing all the way up to IO, and your program becomes very brittle - this is not what Haskell is about. Any solution that uses unsafeInterleaveIO for this task starts the unsafe rot, and sooner or later it will come back and get you. Why do it, when it is completely unnecessary? (See my other posts.) 
well, that's just in the same sense that getContents or something is allowed - you at least know you haven't lost memory safety and so on, even if you can't promise the returned tree has any particular relation to the filesystem (not that any of these solutions make any decent guarantees about concurrent modifications). I don't like this sort of lazy IO myself, but when people have started playing with it I think it's probably best to demonstrate that it can be done, the excessive and unpredictable memory usage isn't inherent to laziness (even uses with pure values, perhaps), and also argue for more explicit approaches (which seemed to be covered reasonably well).
I need to read the paper in more depth. When I brought up the use of projecting to a right seminearring, I wasn't focused on the particular applications you already had in mind. I think that may be where the disconnect lies. The equivalence there is only up to that of a semiring (as evidenced by the differing result orders), which by providing you more laws, provides you with greater opportunities for transformation.
What is the status of hp2any? Is it still maintained?
This is so cool it actually cropped up in my dreams last night. Dreaming about kind polymorphism is when you know things have got bad.
I just noticed the option to enable SSE4 instructions, can someone explain this, did they write bindings to C to access SSE instructions in haskell or are they just binding to a C library that has an option to use SSE?
Okay, now I want kind polymorphism in GHC.
Outer/inner instead of left/right is the best intuitive explanation I've seen yet :-)
Another example is the Typeable class. It has 7 class duplicates for 7 different kinds, and I think even then it doesn't cover higher-kinds. See [Typeable7](http://haskell.org/ghc/docs/6.12.1/html/libraries/base-4.2.0.0/Data-Typeable.html#t%3ATypeable7)
Anyway, thank you for this fruitful exchange. Before, I did not know whether the algorithm is equivalent to its specification for more general algebraic structures than semirigs and you made me think about it. As to whether the algorithm can be tweaked to work with semi-nearrings, a simple idea is to flip some calls to (.+.) where appropriate. I tried that but did not manage to get equivalence for [String]. Especially, when searching in "ba" for a substring that matches (a|b|a)*, then the results are always returned in slightly different orders no matter how I tweak.
Great example!
This version is very bad with memory, so I will have to start reloading the server. Some sessions may be cut off.
From half a framework to 3-4 frameworks in just a couple of months? :-)
I didn't look at the implementation very carefully. I assume this is done with continuations? Or you just incrementally fill in information into the session?
It's built on top of the Automaton arrow from the arrows package, which is closely related to continuations.
reminds me a bit of seaside- would be cool to have a direct comparison with some of their demos. http://www.seaside.st/about/examples/counter
I wouldn't deem this half of a framework yet. Very neat though!
The 'counter' would be something like counter n = catchAuto $ proc i -&gt; do fo1 &lt;- linkForm "++" (counter (n+1)) -&lt; () fo2 &lt;- linkForm "--" (counter (n-1)) -&lt; () runHamlet -&lt; [$hamlet| $n$ %br $fo1$ $fo2$ |] (I have not tested this code)
Hm. Isn't that kinds \* -&gt; \* -&gt; ... -&gt; \*? I don't know if normal kind polymorphism can handle that. After uncurrying, it's (\*, ..., \*) -&gt; \* which could be handled. But that requires also product kinds. By the way product kinds would be nice in the monoid/monad example, since it could be written like type Hask = ((-&gt;), (), (,)) Hask :: (* -&gt; *, *, * -&gt; * -&gt; *) type EndHask = (Nat, Comp, Id) EndHask :: ((* -&gt; *) -&gt; (* -&gt; *) -&gt; *,...) instance Monoid Hask Integer where ... instance Monoid EndHask [] where ...
Is session state keept server side? If yes, maybe this [pice of code](http://hpaste.org/fastcgi/hpaste.fcgi/view?id=28328#a28328) will help.
Yes, it's being kept server side. That shouldn't be a problem in general though - the issue is that too much state is being kept. I have an idea of how to fix it.
You might want to read "Automatically RESTful Web Continuations" and check out its bibliography.
Its not totally forgotten, but put aside temporarily because of other projects  mostly unrelated time-consuming activities, unfortunately. Replacing hp2ps was in the plans form day one, but always at the lowest priority, since it is a quite usable tool for off-line analysis. Instead, I concentrated on the live profiling aspect. Nevertheless, Im always ready for user input.
It's an awesome name!
I am playing around with this library, see http://code.google.com/p/hs-ogl-misc/ (under io-layer/System/IO9) - the basic idea is that Haskell threads might expose themselves as "file servers" to each other (and maybe over network as well), and 9P seems to be at least very well worked out solution. Still very raw code though. 
Hello developers. Look at your framework. Now look at me. Now back to your framework. Now back to me. Sadly, it isn't me. But if it stopped using global state and started using automata, it could work like me. Look down. Back up. Where are you? You're on a boat. With the framework your framework could work like. Anything is possible when you're pure, functional, and lazy. I'm on a horse.
I'm still waiting for someone to make Python on a Plane.
Thanks, I'll take a look.
Psst. Hey. Are you still in the process of making that video available?
If I knew how I'd write one and call it Haskell in Heels.
[Someone already did.](http://web.archive.org/web/20070811133439/http://pythononplanes.com/)
Conceivably you could have a set up like so: TypeRep :: forall k. k -&gt; * class Typeable (t :: k) where typeRep :: TypeRep t instance Typeable Int where typeRep = ... instance Typeable Maybe where typeRep = ... app :: forall k1 k2 (f :: k1 -&gt; k2) (a :: k1). TypeRep f -&gt; TypeRep a -&gt; TypeRep (f a) instance (Typeable f, Typeable a) =&gt; Typeable (f a) where typeRep tf ta = tf `app` ta This is a noticeable change from the current `Typeable` stuff, though.
http://i.imgur.com/Dg04R.jpg
Max was about 3MB/s for a couple of hours, so not bad. New server held up just fine, it seems.
Huh. I could only find references to Django.
Modifying the original SDL Haskell bindings of-course (people should avoid using Haskell extensions in binding libraries!!!), I would not try to duplicate efforts and divide people :) If you know what I was doing last time then you can probably guess where this is going (hopefully, cross fingers).
I imagine it would have been worse without Bittorrent and Linux distributions mirroring the files. Do you have any statistics about the number of downloads?
this is not /b/, please stop.
just speaking from a seeders point of view, I've been seeding the torrents for mac and windows, combined they've only done about 5GB since release, fairly quiet.
I've been wanting that for a long time. :-)
Prelude&gt; "Haskell" == "OOP" False 
Dude, I'm just trying to get some perspective on whether or not people think Haskell is OOP. I'm of the belief that it is, and I don't see much of a difference between FP and OOP.
isn't [haskell](http://en.wikipedia.org/wiki/Haskell_(programming_language))
If you're serious about this (and not just flamebaiting), start by taking a look at this presentation by Simon Peyton Jones: [Classes, Jim, but not as we know them](http://research.microsoft.com/en-us/um/people/simonpj/papers/haskell-retrospective/ECOOP-July09.pdf) 
This isn't a matter of opinion. [Read this](http://en.wikipedia.org/wiki/Comparison_of_programming_paradigms)
I'm convinced it is. Thank you for taking me seriously, though.
If you want a discussion, why not start it yourself? Why is Haskell OOP?
 - Beeeeeeeeeep!
Haskell is OOP because EVERYTHING is of one type: a function. These functions have varying arity (like 0-arity, as constants could be), but they're all functions, just like how in OOP everything is an object. OOP isn't about classes. It's about message passing. As Alan Kay said: &gt; OOP to me means only messaging, local retention and protection and hiding of state-process, and extreme late-binding of all things. It can be done in Smalltalk and in LISP. (http://userpage.fu-berlin.de/~ram/pub/pub_jf47ht81Ht/doc_kay_oop_en) Haskell does basically all of that. Local retention: done. Messaging: monads. Late binding: LAZINESS. Hiding of state-process: Haskell does away with state entirely, so there's nothing to hide. Haskell type logic is basically classes in modern languages (which apparently Alan Kay added after hearing that his idea would be too slow), so all of that transfers neatly. len :: A -&gt; Int vs. len :: B -&gt; Int is just A#len and B#len.
I don't think haskell's laziness is what AK means by late binding. Being able to change any part of your app's code while it's running would be more like it.
I will once I go through the apache logs.
This is a pretty good tool for that purpose: http://www.hping.org/visitors/
Of course not, they aren't even the same type! Haskell :: Programming language OOP :: programming paradigm
I'd say no. OOP is about entities called objects which encapsulates state and send signals to each other. OOP language is a language which constructions allows easier using of OOP style and the OOP style is idiomatic/preferred in such language. In idiomatic Haskell there is usually no state and there is no signals. Haskell types are rarely encapsulated as well. Usually Haskell program have no 'state' but it passes it around and it uses functions as oposed to signals (difference is that signal have intend to notify something about event/request, function simply have input and output). Oh sure - you can write OOP program in Haskell as well as you can write it in C (gtk+ is a OO framework written in C).I'd say that OOP is easier in Haskell then in C and it might happen that it is easier that in OOP language. But it is still not idiomatic, 'natural' way of solving problem in Haskell.
At this moment the reference bears a notice and links. It will require some significant work to change everything, but I am just about to finalize a Python 2.6. reference (http://zvon.org/comp/r/ref-Python_2_6.html), and I can recycle a lot of code from it. I hope to get out revised Haskell one this year. 
Contrast this with the version on [the PC](http://imgur.com/J9Nkd.png) earlier.
I'm not working on on this project it but I hope more people help out and contribute to this project, even just submitting bugs would probably go along way.
Jhc is a pretty exciting project. It's always a pleasure to find news about its progress.
I just skimmed through it but it seems really fine. I think the most important thing when talking about monads is to talk about abstractions (functions and data types are commonly found abstractions in programming languages), which is central to a programmer's activity. That is what you call patterns. A particularly bad way to present monads is to directly talk about very precise implementation details and view of them. But I woudn't say a pattern is necessarily about type classes, methods and combinators. (E.g. woudn't [this](http://www.haskell.org/haskellwiki/Indirect_composite) be called a pattern?)
It wasn't about Haskell specifically. The talk I think you are talking about was by Tim Sweeney. He has done a couple talks which mention the benefits of approaching games from a functional point of view: * [The Next Mainstream Programming Language: A Game Developer's Perspective](https://docs.google.com/viewer?url=http://www.cs.princeton.edu/~dpw/popl/06/Tim-POPL.ppt) * [The End of the GPU Roadmap](https://docs.google.com/viewer?url=http://graphics.cs.williams.edu/archive/SweeneyHPG2009/TimHPG2009.pdf)
So, a tutorial about writing a tutorial about monads is also just a tutorial about monads. It seems that tutorial writing is monadic! 
It's important to use big words to impress the student. Ok, but seriously, this was really good. I'm still learning monads, but I think I finally understand them. I don't know how to put it in words. They just MAKE SENSE. Like, DUH! And better yet, I see that Monads aren't for every situation, but they are available as a tool. I shall practice my enlightenment tomorrow night. Tonight I'm gonna try to see a movie. Does anybody else eat donuts with chopsticks?
Style seems familiar. The one I was looking for had more example code, but now that I know the author, I will find it.
Wait! You also have to include a method to map a tutorial about a thing into a tutorial about monads explained with that thing.
Say M is monad and T is tutorial. Hence: joinTut :: T (T a) -&gt; T a as shown (trivialy generalized to tutorial about anything). Then if the thing can be changed to something else (like a -&gt; b) the tutorial can be always extended to cover it. I.e. if we have a method of coverting a to be and tutorial about a we can make a tutorial about b. Hence fmapTut :: (a -&gt; b) -&gt; T a -&gt; T b The only problem I see is that Monads are pointed. 
Having seen the tutorial being performed 'in action' (well - unfortunatly I already knew monads at that point) it worked well.
I really like this (somewhat ego-deflating) part: It turns out that realising that Maybe Int -&gt; (Int -&gt; Maybe Int) -&gt; Maybe Int can unify with m a -&gt; (a -&gt; m b) -&gt; m b IS NOT SOMETHING THAT MOST PEOPLE CAN DO TRIVIALLY. You can probably only do that because you've seen the pattern so many times you've memorised it.
I haven't tried Jhc yet, but I'd like to. I plan to write a LD_PRELOADed library for libGL and this post might be handy to do it with Jhc. Thanks for the post.
The pointed bit is not a problem; if you actually have a thing then you can turn it into a (bad) tutorial about such things just by describing it: describe :: a -&gt; T a
Don, thanks for posting this. It's #4 of a series, and I would welcome feedback on all chapters! I'm working on #5 (error handling monads) and #6 (state monads) as we speak.
Be sure to bring them here for community feedback.
**A monad is anything which obeys the monad laws** Why do you need a tutorial?
Will do!
I've read dozens of tutorials on monads and I still don't understand your joke. :(
Well to be fair, I think it is something that most people *can* do. It is just seeing that it is the same, you just have to put 'm' instead o 'Maybe'.
AND they have to realise that and b are the same (Int), and understand that both a and b can map^H^H^H 'be' Int (and understand what 'be' means in these terms^H^H^H^H^H^H^H^H this setting). And at the same time not confuse yourself with the higher order function (which an eager Haskell learner may or may not be understanding fully). And they have to realise and understand they unify, because if you tell them and they nod ok and vaugly see they look similar you're just starting to lose everything. My point is that it is something most people who want to use Haskell or think they want to use Haskell will be able to do eventually. Just don't assume they can do it _trivially_. And don't assume a student who has to learn Haskell can do it immediately.
Because when people ask to understand monads, what they want to know is _why should I be using monads_. Saying water is two parts hydrogen to one part oxygen doesn't help people know that they should use it to help keep their house-plants alive. I do note that I didn't actually mention the Monad laws (yet). I stand by that. Why add 3 esoteric rules to the learners memory before giving them some motivation for them being there?
I just had a vision about terrorist being tortured in guantanamo by reading monad tutorials.
Minor nitpick: In general, `map (\\x -&gt; [x]) mv /= [mv]`. Take `mv = [1,2]` as a counter example. So, you can't really take the "`-- definition of map`" step in the confirmation of the 2nd law.
I am totally looking forward to this. I've just started reading the back catalogue of "program calculation" papers and some of them are really fascinating. This looks like it'll be an excellent collection with bite-size ideas that I can concentrate on without having to wrap my head around a whole book.
Excellent tutorial series on monads. Thank you for your work. 
You're right. I'll fix it. Thanks!
You're welcome! Stay tuned...
OK, fixed now. Thanks again!
I understood the monad laws when I was explained that Kleisli arrows forms a monoid (`&gt;=&gt;` being the operation and `return` the identity). I think that it's the best way to feel monads.
sigfpe should write a book 'Haskell, How to use it and win an Oscar (though not necessarily in that order)' and win an Oscar (though not necessarily in that order). 
The solution was [announced on Friday](http://www.dougalstanton.net/blog/index.php/2010/07/30/cyber-security-teaser-challenge/) so I thought I'd write up the process I went through to get my solution. There's a lot of writing and not much code there (catering to mostly friends and non-Haskell users!) but I thought people might find my experiences interesting. The code is all in a repository at the bottom of the page if you want to see it in its original mess. I did a fair amount of reorganising and renaming for the blog post. :-)
Prize to the first person to get this on Hackage...
Of particular interest, the [flattr plugins directory](https://flattr.com/support/plugins) showing you how.
I can't say anything about Haskell, but Java `byte`s are signed, and thus sign-extended when promoted to `int`. So for instance, `byte` 0xE3 = -29 = 0xFFFFFFE3 when sign-extended.
I'd like to see some. However, I think it'll be hard to do things like the Java puzzles because Haskell tends tomake sense. Given only the type, what does the following function do and why? foo :: (a, b) -&gt; a Well, because of the way the type system can guarantee something, for this to be valid, the *only* thing it can do to be general is return the first value of the tuple. "What can this function do and what can it *not* do?" would be an interesting exercise given some types.
ah, that makes sense edit: although separately, if you're going to have a byte type, why the hell would you make it signed?
It's interesting the way some things get independently reinvented. Months ago, while tinkering with ways of abstracting incremental stateful interactive computations, I ended up with a minor variation on the same theme as the article's `Pipe`, and have since returned to the idea several times. The basic structure resembles a [Mealy machine](http://en.wikipedia.org/wiki/Mealy_machine); the `arrows` package includes [an arrow transformer `Automaton`](http://hackage.haskell.org/packages/archive/arrows/0.4.2.0/doc/html/Control-Arrow-Transformer-Automaton.html) that builds one on top of any `Arrow`. The `Pipe` type in the article, for instance, could be written as `Automaton (Kleisli Future) b c`. Conal Elliott built on top of the same structure as part of [some posts about arrow-based FRP](http://conal.net/blog/posts/functional-reactive-chatter-bots/), while chunking up the input stream and extending the automaton structure with "halt" and "need more input" states instead leads to the basic idea of Iteratees.
Yes, please. I want to flattr more Haskell blog posts.
I just added some comments at the original post. My hunch is that most instances of this discomfort with "function of time" comes from overlaying causal &amp; operational notions onto that phrase. I like the simplest precise (i.e., math) model I can find.
I almost wonder whether this overlaying comes from the imperative mindset. In imperative programming a "function" is very much about how and why. Often in mathematics, even, we get a similar interpretation (e.g., addition as a primitive recursive function on the structure of natural numbers). The acausal and non-operational notion tends to show up more in mathematics with some kind of external model, e.g., time-varying series, physics, probability,... Often in places where a "function" isn't assumed to be knowable, or even computable. I've noticed that people have the same kind of discomfort when encountering acausal functions in probability theory.
Unless you cheat, and use non-H98. :)
Is that article some kind of joke? For example, since when Yii "is quite popular for high performance Web development"
Could we get that on a log scale, too?
one thing that puzzled me when I started learning haskell (and still confuses me today) was the following expression: (.).(.) 
The author has no idea of the web development scene in Haskell otherwise he would have realized that HoH has only managed release 0.2 as a proof of concept. The only reason for it to be remotely like 'ruby on rails' (whether such similarity is a good thing though is kind of subjective) is that it has a name in the form of 'a on b' but the rest of the article shows that is was not a requirement in the selection. In-fact being based on arrows positions HoH as more unique than most of everything else.
What happened to Luke's flattr button? :'(
yes, it was. he was talking about humorously alliterative web frameworks
 let 2 + 2 = 5 in 2 + 2
what's a reading group? is when you read things and ask others about something? then #haskell is one big reading group.
&gt;the only thing it can do to be general is return the first value of the tuple Wrong. foo :: (a, b) -&gt; a foo (x, y) = undefined
what's wrong with "you caused me ah ha moments, thank you"? do we really have to web2.0 everything on this planet?
I'm not fond of "liking" or "not liking" things, but I like the flattr business model a lot. I don't have a facebook account whatsoever, so don't see my post as web 2.0 fanboyism. For me, it's like buying a beer to someone who took part of his/her time explaining something to me. If haskell bloggers can have a fancy restaurant at the end of the month because they took time to be helpful to a bunch of people, why not?
It's not just a note of thanks, there's money behind it.
As is so often the case in Haskell, the types help: (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c) so (.) (.) :: (a -&gt; (b1 -&gt; c1)) -&gt; (a -&gt; (a1 -&gt; b1) -&gt; (a1 -&gt; c1)) (here, I've renamed all the type variables in the instance of (.) which is the first argument, and I'll do the same with the second). Now, the second argument must have a type that unifies with `(a -&gt; (b1 -&gt; c1))`, so `a ~ (b2 -&gt; c2)`, and (b1 -&gt; c1) ~ (a2 -&gt; b2) -&gt; (a2 -&gt; c2) b1 ~ (a2 -&gt; b2) c1 ~ (a2 -&gt; c2) Here `~` means "is the same type as". So: (.) (.) (.) :: (b2 -&gt; c2) -&gt; (a1 -&gt; a2 -&gt; b2) -&gt; (a1 -&gt; a2 -&gt; c2) removing some of the now-unneeded numbers, we get (.) (.) (.) :: (b -&gt; c) -&gt; (a1 -&gt; a2 -&gt; b) -&gt; (a1 -&gt; a2 -&gt; c) `(.) (.) (.)` is just `(.) . (.)` written in prefix form. Hopefully you can see from the type what this function does: it composes a one-argument and a two argument function, by first applying the two-argument function to its two arguments, and then transforming the result. Of course, you could get ghci to do all the hard work for you :)
I live in Belgium; I'm not sure I can really make money this way legally.
There's a package for that: "Djinn uses an theorem prover for intuitionistic propositional logic to generate a Haskell expression when given a type. " http://hackage.haskell.org/package/djinn
I love their crystal clear discussion of the issues with embedding `let` and function application in section 3.
What would be the problem? You might need to pay tax, but just check with your tax office, and get a written statement if there is no clear rules. We did this when I worked with a ad/scam-site that paied members to click ads. (No, I'm not to proud about that, even thouhg the ad networks loved us.). The tax officials decided we wouldn't need to pay taxes for the "salaries" we paied, unless they exceeded $ ~100 / event. 
I can see it on the side of his site... Edit: oh ok, I see it as inactive on flattr's site.
And don't forget that a Moore machine is, if I recall correctly, just the cofree comonad over (-&gt;r)!
Oops, yeah. Forgot about bottom :/ .
If people contact me directly about [my blog](http://www.snoyman.com/blog/) to put it up, I'll consider it. I've always thought it would be a bit of hubris to put up something like that on my blog. Not that I mind others doing so, but just doesn't feel right.
Pays the bandwidth bills, for instance. :-)
That was my favorite part as well.
Is it really 400 pages long? And will it have the Monad chapter?
This fix using `Maybe` feel a little hacky to me. There ought to be some tying the knot solution that prevents the rebuilding of the automaton, like the difference between `fix f = let x = f x in x` vs `fix f = f (fix f)`. But I'm not smart enough to figure it out.
&gt; Often in mathematics, even, we get a similar interpretation (e.g., addition as a primitive recursive function on the structure of natural numbers). Perhaps "primitive recursive function" and even "recursive function" are sloppy &amp; misleading terms. It's not that a function is recursive, but rather that a function *definition* is recursive. (And iirc, "primitive recursive function" means there exists a primitive recursive definition.) Even functional programmers blur this distinction between functions and definitions or functions and expressions to denote the functions. I think this blurring happens more than with values of non-function types.
if ever I get married and have kids, I hope a coloring book version of this will be out by then. Of course, my hoping this makes it infinitely less likely that the former will ever happen :D
i like the "robot monkey operator" `(:[])` too (at least it is not confusing)
Without knowing the details, maybe hash-consing can be used. That way, user code don't have to return a Nothing to specify nothing has changed; the tree-building functions will figure out themselves. But a remark in the post seems to indicate it is a good thing to let the user return a Nothing.
Looking forward to it.
It seems that my solution is totally different (on the surface) from the official solution and others' attempts, but I got the "right" answer. Ideas from the experts?
I've been avoiding this for a similar sort of shyness. (I think my avoidance is a bit less principled than yours...) If you're into the idea behind Flattr, one way to defeat the shyness is just to think of it as doing your bit to help Flattr reach critical mass.
Or the coffee bills, which is arguably just as important.
Do you have your original and could you post it? hpaste is dead
There are not many Haskell books out there, so... The more, the merrier :) In fact, we* really need books on advanced topics and idioms. I really haven't been able to find any books of that sort about Haskell. [*] and by "we" I mean "I".
Avoiding repeatedly rebuilding the expression in memory should be fixable by writing a smarter Arrow instance, but I haven't looked closely enough to be sure.
We have a lot of [beginner books](http://haskell.org/haskellwiki/Books_and_tutorials#Books), just not many advanced books (RWH, Okasaki, that's about it).
Int8/Word8 ? 
Just imagine the unsubtle innuendo possibilities for a web framework written in Coq
Another data source about what programmers think of Haskell is "The Right Tool", which is a web-app that lets users rank the languages they know based on how well they believe various statements describe each language. http://therighttool.hammerprinciple.com/ http://therighttool.hammerprinciple.com/items/haskell
http://www.cambridge.org/uk/catalogue/catalogue.asp?isbn=9780521513388&amp;ss=exc
Even then, what could a function of that type do? unsafePerformIO and unsafeCoerce would provide routes into weird and wonderful behaviour, I guess.
This is actually quite an accessible article by Oleg.
Related work: [ML Modules and Haskell Type Classes: A Constructive Comparison](http://www.cse.unsw.edu.au/~chak/papers/modules-classes.pdf).
Thanks for the link. Two "t"s in "Elliott", please?
Oh, my apologies, Conal! :/
Brain bandwidth.
Sure. I was merely intending to get across the sort of mathematics I had in mind. However, there's a long tradition of debate between intensional and extensional interpretations of... well, everything. From an intensional standpoint conflating "the function" and "the definition" makes a certain amount of sense (whether doing so is right or wrong). The intensional perspective is, so far, extremely dominant in computer programming. Without intensionality, the idea of newtypes in Haskell would be incoherent; as would many other interesting problems in type theory. Mathematicians and philosophers tend to be fonder of the extensional perspective (philosophers especially), but even they quibble over stuff like this. I think part of the reason it's a bigger problem for functions than for non-function values is because functions are intended to *do* something. Ignoring C and the like, any given (non-function) value is usually quite apparent about what it represents. Thus, there's not much confusion between the representation and what is being represented; rather to the extent there is, we often can't tell. But for functions, we have something in mind which goes above and beyond any specification. And we constantly find bugs in our specifications not matching our intentions. We rarely talk about bugs in non-functional values (which aren't indicative of bugs in function definitions). I think, because we cannot express actions concretely except through operational definitions, and because our goal is often to convince a computer what to do, this is what leads to the confusion between intension and extension re functions/definitions. 
AAggh! I was away on a bit of vacation, so you guys beat me to it! Sorry for not putting up the final chapters on for that long, it's been really busy with the editing process and such. But they are on the way, no worries, the book will still be online after release. More info coming soon, I'll keep y'all posted!
I was so hoping to see cover art.
As chastisement for thine transgression, I hereby decree that thou must wear the name of "Don Stewartt" for a single day. :o)
http://code.haskell.org/~dolio/haskell-share/DU.hs
I don't agree with the conclusion: I didn't find it fuzzy! It was very readable and the idea of topless structures, and the presentation with the quad-tree, are very nice.
 foo (a, b) = unsafePerformIO (emailPornArchiveToGrandma &gt;&gt; return a) 
the boobs operator
I don't know if it's finalized yet but there's cover are on the [publisher's site](http://nostarch.com/lyah.htm) (thanks to cag_ii in the /r/programming thread for that link).
For those who didn't notice, this article is from 2004.
I'm incredibly happy to hear this, good on you. I mostly used Real World Haskell once I'd briefly worked through what you had online, and I'd love a print copy of your work :).
I'm pretty sure I've corrected the spelling of your name in someone's answer on Stack Overflow before, and I know I've made the same misspelling while Googling to find one of your older blog posts. *Clearly* this indicates a deeper problem with the semantics of spelling, and would be elegantly resolved by simply defining your name over continuous *t* rather than discrete letters, thereby avoiding such sampling errors.
I have high hopes in [this guy's](http://marcotmarcot.wordpress.com/) [new gc](http://socghop.appspot.com/gsoc/student_project/show/google/gsoc2010/haskell/t127230760695)
My rule of thumb is to flattr a blog post when I would be missing something if I had not read it. This can be anything: an "aha" moment (can't miss those), an installation instruction (saved me lots of time), a reference to an interesting paper (wouldn't have stumbled on that by myself), a funny cartoon (I laughed), ... 
What kind of techniques are you thinking of?
thanks :)
LYAH is my favourite beginning Haskell tutorial. Author makes the subject crystal clear. Looking forward to this release.
I think the easiest thing might be to just attach some extra data to your arrows so that when you combine f &gt;&gt;&gt; g, if the data says that there's no need to reevaluate, you don't.
That is one awesome domain name.
Maybe it's the same convention as "iff". "Elliot and only Elliot".
The titles of this post and the linked blog post are both misleading. It's a very preliminary version of a new Haskell web framework called "bird", inspired by the [sinatra](http://www.sinatrarb.com/) web framework in Ruby.
Is there a Haskell Web framework that lets one use on-disk HTML template files? Something like Ruby's Erb? What I've seen so far is either Hamlet (from Yesod) and Heist (from Snap), both of which require you to follow some formatting constraint. Or else they give examples of templated text that is inlined with the application code. But I've not seen framework example that reads a template from a disk file, does basic variable substitution, and emits the results. 
[yst](http://hackage.haskell.org/package/yst)? I'm sure both snap and happstack can do this too.
Hakyll has on-disk HTML template files, but they're pretty limited in their power, and it's a static site generator rather than a web framework. http://jaspervdj.be/hakyll/
And someone else has http://monad.io
The Yesod example is spot on, I'm impressed. However, I'm a bit confused: for Snap and Yesod you include a fully runnable example, but the bird example seems to be missing some things... like a main function for example. What imports are necessary, and what does the main function look like? Also, what are you using for serving? Hack, WAI, Snap, Happstack, rolled your own... we have such a plethora of choices in Haskell ;). Good luck on making some progress on this, it seems like it's more lightweight than Yesod, which could definitely fill a nice role. And good luck on the new baby, I know what you mean about lack of recreational programming ;)
The project is on github and has an example of running an app. Looks like there is a helper program that adds some of the boilerplate to the app for you. [bird on github](http://github.com/moonmaster9000/bird)
I am already sure this will be among the 3 best books on programming I've ever seen, supposedly competing with SICP and "Design concepts in programming languages". Richard Bird is that cool.
Just feedback about the presentation, rather than the project: Anyone programming web frameworks in Haskell has probably been doing this for a while. Attacking other frameworks for having a complicated "Hello World" may appeal to a newbie, but as an old hand I am distinctly underwhelmed. "Hello World" proves nothing about a framework. I am well aware that in the functional world, if I wanted to adapt your example to Snap, Yesod, or frankly anything it's probably about ten minutes of effort with a couple of wrapper functions. Have you seen what Django dumps out when you create a project? Yet I am not really bothered. There are reasons for that stuff. Compared to the work of creating even a simple website it's not a problem worth worrying about. Show me how I can do something _complicated_ more easily than in other frameworks. I am not trying to be critical of the framework, which I know nothing about. I'm just pointing out this sales tactic strikes me as likely to fall flat; my goal is to be helpful here, not negative.
No, he is not missing something. Bird itself is a Program which will take your source and translates it by copying your source into a "template" which will call your "get", "push"... and so on functions. Pretty simple. I think it is nice for little projects, but all you get is simple routing by Pattern-Matching (not to be confused with regexes). Yesod's type safe url approach and macro machinery on the other hand is much more appealing, especially for larger projects. 
Wow, there are a couple there I hadn't heard of. Thanks for that list.
Looks interesting, thanks!
The problem with this, if my guess is correct, is that this sort of sharing is not preserved by many (any?) operations. For instance: l = 2:l is represented by a single cons-cell pointing to itself. But, if you write: l' = map id l barring a rewrite rule changing that to `l' = l`, `l'` is not similarly efficient (I'd expect it to leak space in something like `print l' &gt;&gt; print l'`). The author is probably in a similar boat. You can probably construct a knot-tied, recursive automaton that will be efficient, but that efficiency won't be preserved by compositions. That kind of preservation requires stuff like `reallyUnsafePtrEquality`. But that isn't a reliable solution in Haskell, so performing the optimization manually may be the only way to go.
ROAM [1] might be a more interesting and flexible data structure than quadtrees, though I'm not sure quite what a lazy, pure functional implementation would look like. [1] ROAMing Terrain: Real-time Optimally Adapting Meshes, Duchaineau et. al. http://www.cognigraph.com/ROAM_homepage/
Unfortunately, I missed that one, and understood not a whole lot of what came next in those lectures. Which was disappointing. On a side note, I would have understood my programming tutorials a lot more if the undergrads running them had read this. Having a guy tell me 'monads are a box that you can't pull things out of' only confused me.
Sometimes you just need to do something simple. I think there's plenty of room to have a framework like this alongside the more complicated frameworks. I really like Sinatra for this purpose, so I can see why the author would want to reproduce this. One thing is for sure, it's exciting to see so many web frameworks popping up for Haskell.
I'd really want to see a book on high-performance Haskell.
Why should we use bird rather than the already excellent loli sinatra-like framework?
This reminds me of [a talk by Dan Piponi](http://vimeo.com/6590617) given at ICFP 2009. I did not find a companion blog post, but maybe the one you are looking for can be found by [searching for "monad" and "diagram" on sigfpe's blog](http://www.google.com/search?q=monad+diagram+site:blog.sigfpe.com).
It is probably this one: http://blog.sigfpe.com/2006/10/monads-field-guide.html
This one is exactly what I was searching for. Many thanks!
And thus it was spoken: 2010 is the year of the Haskell web.
Is bird usable enough yet that it should be added to the [wiki page](http://www.haskell.org/haskellwiki/Applications_and_libraries/Web_programming)?
See especially Milan Straka's nice work on this later in the thread.
as the author of bird, i would say definitely "no". i'm glad people are interested, but bird is still a pet project, very incomplete. 
as the author of "bird", i wholly recommend using loli :-). loli is much more feature complete. however, i will say that my plans for bird are a bit different from loli - for starters, i wanted to use the languages built in pattern matching for url routes instead of the "/resources/:resource" string interpretation approach of loli / yosed / snap / sinatra. loli also has it's own built in simple templating language, but my plans for bird include adding helpers to easily plug in any haskell templating language (Hamlet, BlazeHTML, HaXml, HStringTemplate, etc.). and lastly, as i stated, a big part of my reason for creating bird was to help me get some real software engineering experience in haskell and learn the language. i hope to use it in production someday (by day, i'm a ruby web developer), but if it never gets there, i will have at least learned something along the way.
hi michael, currently bird runs on hack, though i'm planning on making that a modular aspect of bird, so that you can run it on top of hack or wai, and choose which handler you want on top of that (happstack, hyena, snap, etc.). as wizit mentioned below, the "bird nest" command takes care of building all the boilerplate for you. one of my primary goals was to make bird feel like a lightweight scripting language - i never want the user to have to write a main function, or to write any of the imports, etc. i'll let the framework / bird command take care of all of that for you. basically, i just want to lower the entry barrier to haskell web programming as much as possible; perhaps bird will someday serve not only as a lightweight web framework for micro-web apps, but also as an entry point for web developers to the haskell language itself. and then of course it could also be a stepping stone to more serious frameworks like yesod. i said in my post, i think yesod is awesome, and could become the rails of the haskell world. keep it up!!!!
Yes it appears to have the monad, monad transformations, and zippers chapters.
http://www.haskell.org/pipermail/libraries/2010-August/014029.html specifically
I can't see the text of this on this link; instead I see links to attachments and "Skipped content of type multipart/alternative". [Here is a gmane link that shows the thread](http://thread.gmane.org/gmane.comp.lang.haskell.libraries/13444). In a related note, both [O'Caml](http://www.lri.fr/~filliatr/fsets/) and [SMLNJ](http://www.cs.cmu.edu/~joshuad/papers/thesis/Dunfield07_Type_Refinements_thesis.pdf) have had errors in their balanced tree implementations. The errors discussed in those documents were found by using more advanced type systems. I think this might be a library for which using Coq to verify an implementation and extract Haskell code would be worthwhile.
To be fair, the constants seem to have been chosen not solely based on whether or not they retained perfect balance in all cases, but rather based on how well they benchmarked under common usage profiles, and the constants are more or less acknowledged to be slightly flawed in the comments.
Not sure about happstack, but the snap examples are all inline HTML. My attempts at reading in from a file collided with assorted type conflicts. I couldn't find anything in the docs to help me understand how to do this, either. yst claims to be a static file generator; I'd have to poke around to see if it can be used to dynamically generate and return Web pages on the fly.
I am less concerned about slightly-out-of-balance small trees than I am about cascading errors leading to very out-of-balance large trees.
I never knew Haddock had a frames mode. There's a "mini" mode as well, e.g. http://hackage.haskell.org/packages/archive/snap-core/0.2.8.1/doc/html/mini_Snap-Iteratee.html
The synopsis link on the right hand side falls apart when opened on firefox on my machine. Looks like a css problem. The tab jumps out haphazardly and a panel opens, but that panel doesn't attach to the tab, and it isn't wide enough to hold all the text from the synopsis.
This looks a lot better than the original design. However I'm not a huge fan of orange links.
No worky?
I'm not a fan of the orange links either (it's not dark enough, for example), and especially not a fan of the constrained width. We have way too long type signatures for that design to work (and then I didn't even mention type family style contexts, which make it much worse)
Now can someone explain the diagrams?
actually i like the orange links more than the blue ones. But a little darker would be great. Sure, the synopsis is nearly unreadable due to the constrained width. But I don't find it with the type annotations very usefull anyway. Have a look at the fourth alternative from the survey: http://www.ozonehouse.com/mark/snap-xhtml/snap-core-alt/Snap-Types.html Much more usable. But still the overlap of the synopsis and the content is a little annoying.
This happens to me also, but only on one of the examples in the above link. Also I hope the missing source links are just for the demo? I use them a lot.
There's an idea I've been playing around with: Yesod has support for something called subsites. I've purposely avoided talking about it a lot, because it's complicated, but it allows encapsulation of functionality in a subpath of your web app. Yesod uses it to implement static file serving, authentication and CRUD. I would really like to have more interoperation in the web community. One way to achieve this would be to allow subsites written with one framework (say bird) we used in another framework (say Yesod). For example, maybe someone writes a simple blog application in bird, then includes it in their Yesod app. If you're interested in something like this, support for WAI would be the first step: it basically sets up a common language as a base. After that, I would say that Jeremy Shaw's web-routes package is the next step.
very nice!
in the Ruby world they have Rack instead of Wai. One can build entire features as "rack middleware". For example there is an authentication framework: http://wiki.github.com/hassox/warden/overview This allows multiple applications to share the same authentication. Edit: I see now that Hack does also have the concept of middleware.
You have to pass an option explicitly to haddock (--hyperlink-source) to add the source link. They probably just didn't pass it when building that site.
Neat talk! I really thank Galois for all the tech talks they've had (the static-analysis @ Mozilla talk was really awesome,) and I *really* don't mean to sound like a totally entitled prick (because at this point I'm sure I do,) but what about the other talks that have happened since Galois started putting things on vimeo? Were they not recorded? Need permission from the presenter before uploading? I've asked this before and the response then was some permission was needed by the presenter and that's a completely legitimate reason (people are busy!), I'm just impatient at this point since there seems to be no mention of it anywhere else, and the contrast between which talks happen and the availability of those videos is very weird. ;) If it typically just comes down to "when the presenter gives us permission, if they do" then that's completely understandable and just want to know. In particular I would really like to see these 3 which happened a while back but have yet to appear (although from the grapevine I've heard they were all recorded): * Mark P Jones' talk on Habit: "Developing Good Habits for Bare-Metal Programming" * Tolmach's talk on certified garbage collection: "Towards a High-Assurance Runtime System: Certified Garbage Collection" * Merten's talk on Agda: "Introducing well-founded recursion" Like I said, I don't mean to sound like an entitled dickhead, and I figure people @ Galois have better things to do than upload videos and reply to entitled-feeling dickheads on reddit, I just want to learn. :)
I prefer the current high contrast design and prefer the current left aligned documentation.
Just got an update. * MPJ, has made progress today. Should be soon. * Tolmach, that won't be public. * Mertens. Lots of discussion took place, so needs additional editing. * Klein/L4. Needs more work (but is coming).
Thanks for this comparison! I see your point that it's easier to make the everything-is-a conclusion in the case of functions than with lists, especially if one forgets that "2-ary function" etc are just informal notions (not part of Haskell), or if one is speaking loosely/informally in the first place.
Thanks a lot for the update! Sad Tolmach's presentation won't be available, but his paper seems to be (from what I can tell on the HASP page it's the same material he probably discussed there) at least. I also didn't realize you guys underwent an editing process for the videos - that'll just make them better and more worth the wait.
Check out these [latest results](http://bugs.darcs.net/msg11962) from Alexey's hard work. I think you'll be seeing much faster get in Darcs 2.8 in January, perhaps the first time that people really start to notice the progress Darcs has made. In the meantime, a couple of performance goodies in Darcs 2.5 this month...
ah, no, my problem isn't with the width of the synopsis (and i agree that the 4th version is much better) but the regular page width. it is "blog-style", with a huge margin on both sides (two times ~5 cm on my small laptop screen). This is good for large blocks of text, but bad for haddock, imho.
ok, so perhaps what people mean (operationally) is: "Everything is a thunk."
Oh yes, sorry about that. The Mailman archive got confused here. Thanks for the alternative link.
Hm, I was taught in high school that `y=3` is indeed a function. Given the same input (nothing) it always returns the same output.
But it takes no input. There is a difference between y = 3 :: Integer and y = const 3 :: a -&gt; Integer
When I say almost everything is a function in Haskell, what I mean is that where other languages implement some Special Magical Feature, Haskell is usually using functions in some seemingly-clever but ultimately very direct way. Specific datatypes that implement the monad interface implement them with functions, and those functions implement many behaviors that other languages treat very specially, such as nondeterminism or continuations. Many of the things that people have been led to believe "must" be baked into the language can actually be done by functions. "Mutation" is also "done" by descending into function calls (with scare quotes indicating I know that's not the right way to put it, but I assume everybody here knows what I mean here). Another task that other languages bake directly into the language but is done by Haskell with functions (or function calls, more precisely, though I still think this fits into "it's all functions"). Of course, it's not all functions. Typeclasses are baked into the runtime and you can't get at the functions that implement them, or use them to compose together something similar-but-not-identical-to typeclasses, or compose typeclasses with something else, or whathaveyou. I'm sure we could make a long list of extensions that have the same property. And that's why I qualified my opening sentence with "almost". And that's also why I tried to carefully phrase my first paragraph; a _given_ monad implementation is implemented with concrete functions you can point at, but the monad typeclass as a whole is not a "function", nor is it anything you can emulate in Haskell itself as a function. (Maybe with Template Haskell, but then I think you're not talking about "functions" anymore, and the same goes for type-level programming; functions are involved with that, certainly, but of a different kind, in both senses of the term.)
Hah, now I feel a little better about making that mistake... twice. :)
Of course, in a functional language everything is a value.
Hm, good point.
I'd certainly buy it.
I don't see how it is constructive to focus on this.
But the function y _ = 3 has the same result... they are equivalent in my eyes. So I would say it is a function that can take in anything and return the same result.
Yeah, I'm removing this. No need to help this unpleasant behavior.
That comes from the sloppy way mathematicians express themselves. They don't have to talk to computers. :)
Well, when a High School algebra class says `y=3` they really mean `f(x)=3`, which is indeed a function that takes (and ignores) input.
As does WAI. Hack seems to use middleware much more than WAI does; I use gzip, json-p and cleanpath as middleware as an example. However, I wouldn't implement authentication as middleware because, AFAICT, there's no way to implement type-safe URLs like that. If you've read any of my posts on Yesod, you'll know type safety is important to me ;).
formlets has a fairly straight-forward approach to the issue: the Form monad has a State Int monad inside that dispenses unique identifiers. As long as you use the same form, you'll have the same names. Yesod.Form and Yesod.Widget both copy this approach.
Except unboxed things, and things that aren't values :-) "Every boxed value is represented by a thunk", or "is a thunk", less formally.
Types aren't values.. You could maybe say operations on values are closed on the set of values? Even in dependently typed languages, where types can be "values" (sort of?), you still have syntactic elements, and other "things" which are not "values". This is true in OOP too, btw, where not really "everything" is an object. I think it's just hand-waving :-)
 y _ = 3 is the same as: y = const 3 And this 'y' cannot be used interchangably with: y = 3 Though up to some potential differences in memoization and definedness, they are isomorphic. Just like: (3, ()) is isomorphic to 3 (with the same restrictions as above).
&gt; it feels like this approach will never be able to compete with a program that modifies the same variable. People often get a "feel" for what is *not* possible. These are often wrong. It sounds to me like it should definitely be possible, at least in many specific cases, to specialize a "foldl" loop with an accumulator to actually mutate an accumulator in-place.
And what does that class mean by "x = 3"? I doubt we'd want to carry the "y=3" special case into something as systematic as a programming language, and especially not Haskell. And remember the "in Haskell" part of the original question.
Exactly! As Bertrand Russell said "Everything is vague to a degree you do not realize till you have tried to make it precise." Most mathematicians &amp; text-book authors have not had to make their language (and thinking) precise. And so they haven't done so. Russell is an exception.
These are modified versions of the original SDL bindings (SDL,image,mixer,ttf) to build on jhc and jhc with Wii target. I don't think cabal works with jhc properly so i've added a simple makefile that will automate the build of hs2c -&gt; jhc -&gt; install.
This is super cool, thanks!
This has been a often-requested feature, but I've had a hard time figuring out the right API. For everyone interested in this feature: please review the code and let me know if this is what you're looking for!
as in accumArray
coq already has implemented (with proofs) AVL trees so the only problem is extraction to haskell. unfortunately, currently coq cannot extract modules to haskell.
I always had the idea that types are compiled away, so you could say that everything that's around in a FPL *at runtime* is a value. 
Well, at runtime, you could say "values" don't really exist anymore too, because who knows what weird transformations the compiler will do to your code. Or, if they do exist, then surely passing around the type class instances as actual argument reifies the types at runtime. I think any definition of the sort of "all things in language X are Y" will be fuzzy and hand-wavy.
Are the following three functions equivalent? fibs1 = (1::Integer): 1: (zipWith (+) fibs1 (tail fibs1)) fibs2 () = (1::Integer): 1: (zipWith (+) (fibs2 ()) (tail (fibs2 ()))) fibs3 () = let result = (1::Integer): 1: (zipWith (+) result (tail result)) in result Sure, they give the same result, but the way they do that is pretty different (1: fast, result gets reused, but space leak; 2: sloooow, no space leak; 3: fast, no space leak, but recomputation every time). They denote the same value if applied to the right number of arguments, but I think it's also important to realise that Haskell is a programming language that's meant to be executed at some stage! I guess it all depends on what your meaning of 'is equivalent' is.
Great work. Yesod already hijacks the Html datatype for producing Javascript and CSS values internally, thereby getting the great speed advantages available from blaze. Do you have any dates in mind for a 0.2 release of blaze?
This is one of the problems I have with the current slew of functional languages (that I'm aware of). Extensional equality is not the same as observational equality. One of the places this came up in my thoughts recently was with regards to questioning whether `seq` still causes problems if we're in a total lazy functional language. If we eliminate nontermination (and ignore things like finite stack size) then we don't have bottom; thus, we get rid of a lot of the problems `seq` introduces for equality and parametricity. However, differences in strictness can still cause dramatic differences in the observational properties of functions--- e.g., asymptotic differences based on whether certain fusion or rewrite rules are applied. This is part of the reason why people have problems reasoning about complexity and space use in lazy languages, but it also seems like a more fundamental problem of our type systems not capturing everything we'd like them to. But then, how much observational equivalence should we build into our denotations? We'd like to be able to put out new library versions without always breaking the API...
This week, I promise! :-)
Do you have an intuition for why the blaze-builder performs better than binary, even for binary data (e.g. ByteStrings)?
My favourite example of this is dates. People think it is just days, months, years, leap years - but when you look into it, it doubles in complexity. Every time. Is February 30 acceptable as historical date? Yes, if you are talking about 1712 in Sweden. (time zones? no time zones? date lines? leap seconds? Calendar changes? A border change that brought a calendar change with it? different religions in the same city holding different calendars? ...)
I've answered your question in the disqus comments on the blog.
This is apparently fixed in 8.3, due out later this summer, though I can't find the bug report at the moment.
That was what I used at first, as well. The advantage of the LabeledArrow is that it deals with forms which change shape in response to input. For example, if you have a dropdown or radio button which gives you different subform options; or if you have a dynamically expandable list of inputs. This can all be solved, but it requires some manual intervention. An earlier version of HoH used a stack of Ints, so I could push a new id onto the stack, run through a subform, then pop the id off. This led me to think about generalizing this: why not just push the stack automatically, every time there was a branch? Following that idea led me to the `LabeledArrow`.
Makes sense. I have to say, the usage of base64 encoding is quite cool ;).
Why do you say it has server-side Haskell? The only hints I see in the headers are Apache as the server and "X-Powered-By: Phusion Passenger (mod_rails/mod_rack) 2.2.14", which **really** seems un-Haskell. Edit: Oh, I just saw that they mention Haskell on their blog. Still, I trust an HTTP header more than a blog post ;)
They are great. Except that you can't rely on getting the same dictionary given the same types. This means that while you can build Set as a binary tree with an implicit dictionary providing the ordering. It is harder to union sets, because you don't know that the two sets were build using the same element ordering, and you need to capture the ordering in the Set itself, otherwise you have this problem every time you go to access the Set.
So there's no such thing as a nullary function?
The web server needn't be Haskell for other parts of the backend to be Haskell.
Not in Haskell.
You're dealing with this a level above the level Conal's talking about. He's talking about how strings, integers, lists, and so forth are not functions of zero arguments.
You're completely right: the site you're seeing runs Ruby, but the interesting stuff on the backend is Haskell. If you're interested, make sure you've requested an invite!
That's not so mysterious. It's from [typLAB](http://www.typlab.com/) ([blog](http://blog.typlab.com/)), a startup with some former students from [my department](http://www.cs.uu.nl/). They are big Haskell fans. See the [team](http://twitter.com/silkapp/team) for example.
I would love to see similar diagrams for arrows.
There's a mention of [Salvia](http://hackage.haskell.org/package/salvia) at the Twitter link.
Don't forget Daylight Savings, which not every state recognizes. Part of the year, Arizona is an hour ahead of the west coast, and the rest of the time it's the same!
x = 3 is not a mathematical function (fails vertical line test). [edit: Realized I sounded rude, and that was not my intent. Just trying to put it in the context of a high school math class.]
And the Soviets forgetting to revert their DST some time after the revolution, keeping them one hour ahead up until the 80s/90s.
You should be embarrassed by your departments website...
If so, the answer to "why" looks clear to me. It would seem fairly obvious that anyone saying that plain values are functions of zero arguments is thinking either: (a) that the idea of curried multi-parameter functions extends naturally to thinking of values as being, in a sense, zero-parameter functions, or (b) that laziness, or at least non-strictness, can be partially explained that way (Conal's "operational thinking" answer). Of course (I hope) these people don't believe that these values have (-&gt;) types... but I refer to things in Haskell as functions with several parameters all the time, whilst still understanding that actually they are curried into unary functions. I suspect that the collection of results may have been biased against (a) above (and even (b) to some extent), from Conal's obvious belief that this view is completely mistaken.
I've wondered what the connection between implicits and implicit parameters is.
Right... so the point of this subthread is that in U.S. high school mathematics, "y" and "x" really are special, in that x is assumed to be "the" independent variable, and y assumed to be "the" dependent variable, for a lot of the work done at that level. This wasn't, I'm sure, intended to be relevant to Haskell. Just an interesting side note.
Hm. Count me in-terested.
And there is not such thing as binary functions either, but it can be helpful to think of functions that return functions as binary functions, and make a similar equivalence with ternary functions etc. Once you do that, nullary functions become just as natural.
He mentions several possible meanings, like the unification argument, the one you're talking about is just the one he explored at the most length.
I'm even more embarrassed by how difficult it is to find information there, let alone the [university's site](http://www.uu.nl/EN/Pages/default.aspx).
Except they don't. "3" is not a function that takes no arguments in Haskell. `\x y -&gt; x + y` naturally becomes `\x -&gt; \y -&gt; x + y`, but how would you extend that to "3" in Haskell, when 3 takes no arguments? You don't, in Haskell, which is exactly Conal's point.
Sebastiaan Visser and Erik Hesselink both contributed to the development of Salvia and both work at typLAB. They've also given talks at [Dutch HUG meetings](http://dutchhug.nl/Meetings/) and the [Dutch HUG Day](http://dutchhug.nl/DutchHugDay/).
I think by the time someone finishes reading the fine print on your argument, they've gotten bored and we'd all be better of not saying that everything or almost everything in Haskell is a function, when it's plainly false and doesn't add any meaning or make it easier to understand or write programs in Haskell.
&gt; I refer to things in Haskell as functions with several parameters all the time, whilst still understanding that actually they are curried into unary functions. The difference between this and nullary functions is that this actually helps you write Haskell programs. When you notice that your function definition looks like this: foo x y = bar 3 x y You know you can rewrite it like this: foo = bar 3 However, the nullary function concept raises more questions than it answers. What happens to ordinary values? Where does the null argument come from, where does it go? It posits a whole new set of evaluation machinery, unlike "everything is unary." At the end of the day, other than being a superficially pleasant thought, I don't see any benefit to the nullary function idea at all.
But note that we are not using Salvia at typLAB. In the early days of the company, about a year ago, Salvia was still a bit too experimental. For now it's all Happstack.
Hey, if we're going to to sling about complaints about verbosity, that's the longest "tl;dr" I've seen yet. (Tongue firmly in cheek.) It's the internet, it's my sacred right to be verbose.
I don't mind verbosity, I just don't see value in the nullary function idea and I don't find your expository defense of it enlightening or enthralling. Which is weird, because I recognize you and I'm pretty sure you know Haskell better than that anyway.
IMO, this is the biggest design flaw of formlets - with numerical field names, they don't work well with JavaScript or CSS.
Yesod 0.5 let's you override the id and name attributes if you so desire. More to the point, you can declare the Javascript and CSS within the widget and then use the numerical names, thereby avoiding any possibility of name collision *and* getting nice CSS/JS interaction. Oh, and I think the biggest design flaw in formlets (rather, the formlets package) is that you can't have inline error messages, something fixed in Yesod as well.
in anything?
Lambda calculus, I think.
Apparently there's a bit of a spike in permanent jobs citing Haskell in the UK This year, [according to itjobswatch.uk](http://www.itjobswatch.co.uk/jobs/uk/haskell.do)
0.15%? Break out the 1.125ml bottle of champagne!
Rock out!! Also, compare with * Scala, 0.135% * F Sharp, 0.14% * OCaml, 0.01% (!) * Erlang, 0.151%
Functions are to functional languages as objects are to... *objectional languages*? :-)
 newtype HighSchool = H (Double -&gt; Double) eval :: HighSchool -&gt; Double -&gt; Double eval (H f) x = f x instance Num HighSchool where (H f) + (H g) = H $ \x -&gt; f x + g x (H f) * (H g) = H $ \x -&gt; f x * g x fromInteger y = H $ \_ -&gt; fromInteger y f :: HighSchool f = 3 
Don't be embarrassed. It's just the way of things. http://xkcd.com/773/
&gt; OCaml, 0.01% (!) Wait, isn't Jane's Street in London? 
New York City.
Still beat by COBOL :( http://www.itjobswatch.co.uk/jobs/uk/cobol.do
&gt; My favourite example of this is dates. Yes, the [canonical examples](http://msmvps.com/blogs/jon_skeet/archive/2009/11/02/omg-ponies-aka-humanity-epic-fail.aspx) of very difficult, tricky data structures that programmers get wrong constantly are: * Dates and times * Strings * Numbers Actually getting those three things right is *far* more challenging than grokking all the silly math-looking stuff in Haskell.
And London. 
And Tokyo.
My point is that you do. A binary function uses 2 lambdas a nullary function uses 0 lambdas - so 3 extends to 3 - what's the problem? This is a mental model that "works", and that has direct support in the haskell syntax. That is why you can write f a b = a+b in spite of currying
I couldn't agree with this more. It's hard to find any resources for actually using Haskell in the real world. Most blog posts about Haskell are either written for newcomers (What is a monad?) or they're written for people who have at least a graduate degree in mathematics or language design. When I read [this post](http://www.reddit.com/r/haskell/comments/cs54i/how_would_you_write_du_in_haskell/c0uu7rz) I thought the term "zygohistomorphic prepromorphism" was a joke. It's not; I Googled it. While I don't agree with the [haters](http://www.reddit.com/user/redditnoob), their argument is not entirely without merit. Haskell is just a tool, like a hammer. Yet most Haskellers seem to spend their time talking about cool things you can do with the hammer other than carpentry (Arrows seem to be the fad recently). Carpenters don't care and most programmers don't care either. We need more tutorials about Haskell solving everyday problems (I think think [this video](http://vimeo.com/12354750) is great) instead of art for art's sake. 
And... *the Moon*.
How many of these are for jobs where Haskell is in significant use? And how many of them are from hiring managers who heard that you get the smartest Java monkeys if you require them to know Haskell?
&gt; It's hard to find any resources for actually using Haskell in the real world. Did you read, uh, *[Real World Haskell](http://book.realworldhaskell.org/read/)*? It certainly goes way beyond "what is a monad". What about ezyang's recent [six-part series](http://blog.ezyang.com/2010/06/the-haskell-preprocessor-hierarchy/) on wrapping complex C APIs for Haskell, or the other FFI stuff on his blog? I agree that more practical information would always be nice, but I don't think it's particularly hard to find now. On the front page of the Haskell Reddit right now I see articles about [buffered IO](http://blog.ezyang.com/2010/08/buffered-streams-and-iteratee/) (with C and Haskell side-by-side), [HTML output](http://jaspervdj.be/posts/2010-08-05-blaze-builder.html), runtime heap inspection, [graphics programming for the Wii](http://www.reddit.com/r/haskell/comments/cxma0/sdl_binding_for_jhc_released/), and [performance tuning in a real Haskell app](http://darcs-gsoc2010.blogspot.com/2010/08/gsoc-2010-progress-report-2.html), not to mention a few webapp frameworks. &gt; I thought the term "zygohistomorphic prepromorphism" was a joke. It's not; I Googled it. [Are you sure?](http://www.opensubscriber.com/message/haskell-cafe@haskell.org/13009107.html) It's a real thing (as real as any other mathematical abstraction), but anyone suggesting you use it for practical code is almost certainly joking. Don't confuse "articles about Haskell" with "articles about mathematics which use Haskell for notation".
If you want to know why *not* to work in finance, [this video](http://www.youtube.com/watch?v=u6XAPnuFjJc) is a good start.
The title didn't really give the link justice... I thought the essence of the article was: 'some people are good at Haskell, some aren't ... The problem with Haskell jobs is that most programmers/managers are of the latter, rather than the former.'
&gt; This is a mental model that "works" What does it buy you?
&gt;Did you read, uh, Real World Haskell? It certainly goes way beyond "what is a monad". Yes, in fact it's on my beside table right now. I think it's probably the best example of what I would like more of. I have to admit I haven't finished it yet though. I am about 3/4 of the way through and I hope to finish it as soon as the finals for my summer classes are finished. &gt;What about ezyang's recent six-part series... No, unfortunately I haven't read that yet. Although I did just read his blog about [Buffered streams and iteratees](http://blog.ezyang.com/2010/08/buffered-streams-and-iteratee/) and liked it. &gt;It's a real thing, but anyone suggesting you use it for practical code is almost certainly joking. Well thank god for that. &gt;Don't confuse "articles about Haskell" with "articles about mathematics which use Haskell for notation". To be honest, it's hard not to do this. Haskell uses mathematical nomenclature and many Haskell programmers are mathematicians. So you can visit the website of a Haskell programmer and read [a blog post with a title that appears to be about Haskell](http://byorgey.wordpress.com/2008/04/17/an-interesting-monoid/) but is actually just about math. To be honest, I don't want to read "articles about Haskell." I want to read "articles about real world programming which use Haskell for notation." I want to read an article about "how I built this house using the Haskell brand hammer and why that was the best tool for the job." Haskell is an amazing language and I personally like it a lot but it is going to go the way of the dodo if its value proposition can't be made clear.
&gt; I don't want to read "articles about Haskell." I want to read "articles about real world programming which use Haskell for notation." I want to read an article about "how I built this house using the Haskell brand hammer and why that was the best tool for the job." Try some of the [CUFP](http://cufp.org/) presentations. Here's the schedule from [2009](http://cufp.org/archive/2009/report/report.html) and from [2008](http://cufp.org/archive/2008/schedule.html). [This report](http://www.starling-software.com/misc/icfp-2009-cjs.pdf) from Starling Software was particularly interesting. [Here](http://lambda-the-ultimate.org/node/3331)'s an article on why a group at a giant investment bank chose Haskell for financial derivatives modeling. Honestly, though, there's not that much out there, because most of the people using Haskell are already convinced they like it, and assume the same from their readers, and thus write to an "articles about Haskell" viewpoint. I expect this to change over time. &gt; Haskell is an amazing language and I personally like it a lot but it is going to go the way of the dodo if its value proposition can't be made clear. Haskell is 20 years old and has been incredibly obscure for most of that time. It's seen a dramatic uptick in visibility in the past few years. So your warnings of imminent doom sound a bit silly to me. If five years pass and there's still a lack of "Haskell trip reports" despite the increased visibility, I'd say you're onto something.
&gt;Haskell is 20 years old and has been incredibly obscure for most of that time. It's seen a dramatic uptick in visibility in the past few years. So your warnings of imminent doom sound a bit silly to me. I doubt academia will give up Haskell anytime soon. I'm simply referring to mainstream Haskell acceptance. My personal opinion is that if Haskell (or one of its functional brethren) can solve the "multi-core problem" then Haskell will see widespread adoption and we can stand around giving each other high-fives all day. If not, then I think interest in functional programming will collapse back into academia and we'll be stuck with C++ or Java for at least another decade. 
I doubt Haskell will ever have more than a modest share of the FP world, even if FP as a whole becomes vastly more popular. It's a crowded field; Clojure, Scala, Erlang, and F# are all serious contenders, not to mention all the functional code written in Javascript, C#, Python, Ruby, etc. Successful features from Haskell will continue to make their way into more popular languages. I am happy with Haskell as a niche language, though I still hope that niche will grow. I want to use something good, not just something popular. I am happy to teach anyone who wants to learn, but if "mainstream Haskell acceptance" means convincing every Java and PHP user to switch, count me out. &gt; we'll be stuck with C++ or Java for at least another decade. "Stuck with" only in the sense that we're stuck with COBOL today. There are massive legacy codebases, but the world is already moving away from these two.
Agreed. It's just kind of an offhand comment in the actual post.
Good stuff ;)
I have a problem with this video and the TED talk that it's based off of. It seems to generalize human behavior from the results of a few studies in ways that aren't entirely warranted. One key component of the studies behind this discussion that isn't mentioned is time constraints. An important reward/risk + limited time induces stress, which of course is going to reduce creative performance. If people are given monetary rewards with large amounts of time in which to complete the task, I'm sure you'll find it an effective motivator.
Also true for many scientists. I know of some Physics PhDs who switched from research to finance.
Show me how you write or call a nullary function in Haskell. That's my problem.
I would have hoped there would be some blog posts or articles about XMonad or Gitit but a quick google has turned up nothing. 
Did you see "[The Design and Implementation of XMonad](http://xmonad.wordpress.com/2009/09/09/the-design-and-implementation-of-xmonad/)"?
If the financial district is bringing functional programming closer to mainstream, then I don't see it as such a bad idea. Disclaimer: I'm a computer-related MSE who will likely work my first few years on Wall Street so I can do interesting, less well paying things later in life without worrying about money.
From the best neural net model I have developed so far (my brain), I discovered that Haskell is more of a figure in religious more than a programming language. Every post related to haskell has exceptionally low down vote rate.
I hadn't, no. Thanks!
I am satisfied with his reasoning: &gt; So my best guess is that a statement like well, 3 is a constant function is not so much a reason as a rationalization. In other words, not so much a means of arriving at a belief as a means of holding onto belief in the face of evidence to the contrary
&gt; Successful features from Haskell will continue to make their way into more popular languages. I think this is a more likely route for Haskell to benefit the wider programming community. As a language, I suspect Haskell is simply too complicated for most industrial programmers to get their heads around it *with an acceptable level of effort*. There are legitimate arguments about adoption being held back by the unfamiliarity of the programming style and by non-technical factors and industrial momentum, but one cant rely on those to justify the low level of industrial adoption forever. At some stage, one has to conclude that Haskell is just too hard to learn for most working programmers. On the other hand, the *ideas* developed by the Haskell community (and the wider functional programming community) are IMHO a much more valuable contribution than any one language anyway. After all, supporting functions as a first-order language feature is no longer anything special in industrial languages and there is increasing awareness in the community of the possibilities offered by higher order functions and lambda expressions (or whatever your language of choice calls them). Haskell is a valuable demonstration of, and test bed for, other concepts like lazy evaluation, expressive type/effect systems, and partial functions/pattern matching. Perhaps in due course, more accessible versions of these ideas will start to appear in mainstream languages as well. &gt; I want to use something good, not just something popular. Exactly. But to me, being good includes being practical, not just having a mathematically beautiful foundation (if you know enough maths to appreciate it).
Is a thunk an unevaluated piece of code that the compiler is like "yeah yeah WUTEVAH i'll get to that later MOOOM"
To digress a little bit, could you please explain how you came to the conclusions you did: &gt; 1: fast, result gets reused, but space leak; 2: sloooow, no space leak; 3: fast, no space leak, but recomputation every time I didn't come to contradictory conclusions, but I would just like to learn your ways of inference.
What is wrong with financial industry? I personally work in financial industry. Bank employs arguably the largest portion of analytical/computing talents among other occupation. The problem that bank faces is not something trivial and done once only first and foremost we have quantitative modelling. This is the area that functional programming (particularly haskell) excels. Problem in this category is unique because it requires * continuously revolved to capture latest market behaviour * massive, real time, inhomogeneous data handling * rigourous mathematical models * high precision floating point computation * data store * absolute correctness * multi programming language environment * fast pace Bank has always been the pioneer in adopt some of the latest concept and technology like GPU/CUDA, F#, high performance mainframe, etc. Why the disrespect to bankers?
 ternary a b c = 5 binary a b = 5 unary a = 5 nullary = 5
That's just an integer, not a function.
Let's try this: because bankers live off other people's loss.
well of course you are right, but I have showed you an example where thinking of nullary as a nullary function improves a kind of consistency and thereby is a mental model that buy me something. If that does not convince you then I do not know what would. It is just a natural extension of the mental model that calls binary above a binary function - which it is not.
I assumed the intended meaning was that investment banking is socially useless/parasitic, not that the work was technically uninteresting.
You can't convince me, because it actually reduces consistency rather than increasing it. Let's take your functions again: ternary a b c = 5 binary a b = 5 unary a = 5 nullary = 5 I can rewrite the first three into something closer to what Haskell does internally, which is to simply bind the name to a function: ternary = \a -&gt; \b -&gt; \c -&gt; 5 binary = \a -&gt; \b -&gt; 5 unary = \a -&gt; 5 But what happens to nullary? nullary = 5 No functions there.
If you don't want a mortgage, or a student loan, or a car loan, or a business loan, or a credit card, or a.... Charging interest is a fact of life, and people should pay back what the agree when they borrow. All bankers do is make fancy models and tell people if they'd like to do business with them based on trustworthiness and ability to pay. Some bankers are frauds, but the vast majority of them work to oil the gears of the markets.
&gt; At some stage, one has to conclude that Haskell is just too hard to learn for most working programmers. I'm not convinced -- how many of them even try? I think the gap is not with intelligence but with motivation. It's not immediately obvious why FP is useful; FP advocates often don't do a good job explaining this, and don't necessarily care to. Learning a new way of thinking requires delaying gratification. And for most people, Haskell is about seven new ways of thinking all at once. (Despite being the poster child for "pure FP", it's got a lot of features uncommon to the wider FP world). You can't pick it up in an afternoon and start coding websites and 3D demos. So I think a lot of people get tired of feeling dumb, get bored of computing factorials and quit early. Those same people might be smart enough to learn it, and perfectly capable if (say) their boss forced them to. &gt; But to me, being good includes being practical, not just having a mathematically beautiful foundation (if you know enough maths to appreciate it). I agree. And I do think Haskell is a practical language -- once you've invested the considerable time and effort to learn it. I've solved many real-world problems in Haskell, but no one project would justify the years of learning. Whereas, for example, one web project could very well justify the time to learn Ruby+Rails or Python+Django.
trivial: ternary = \a -&gt; \b -&gt; \c -&gt; 5 binary = \a -&gt; \b -&gt; 5 unary = \a -&gt; 5 nullary = 5 As consistent as it could be. The number of lambdas correspond nicely to the arity. Again, what is the problem?
As Conal writes: nullary functions is part of an informal perspective that may sometimes aid intuition. That is all I claim.
The problem is that there's no lambda on nullary, because there's no function there, because 5 isn't a function. Period. You're saying there's only one kind of thing (function) but two different ways of making them: with lambdas and without, and that's just bizarre nonsense.
I can't help but feel that you misrepresent my position, and that you just do not get my (trivial) point. For example: &gt; 5 isn't a function. Period I have never claimed that 5 or nullary above IS a function. All I am saying is that there is a mental model that can be *useful* that calls nullary above a nullary function. Sure that breaks the axiom that "function === lambda", but that is ok. A mental model does not have to hold water from all perspectives to be useful. I am also arguing that if you are ok with calling functions binary ternary etc then nullary functions follows naturally as a base case. My final argument is that thinking of a function as binary, ternary etc has built in syntactical support in haskell. Have a nice weekend. 
Can't agree more. After all, all of the financial products are insurance of some kind. It protects you from the risk you might experience. 
that's exactly what they did not do. and even now, the german banks got the fancy idea to disrespect the law and to not hand out talk protocols to their customers. because they feel they can.
Not all of this is standard Haskell, but here goes: `unsafeCoerce` is very muuch magical, and highly unsafe. It's used to implement type-safe cast and dynamically-typed values. You can implement dynamic typing yourself for a fixed set of types (`data Dynamic = DInt Int | DBool Bool`) but not for every possible type at once. Importing `Foreign` provides direct access to memory, though you might consider it part of the FFI. `seq` is magical, in that it's polymorphic. You can write it for specific algebraic types: seq :: Bool -&gt; a -&gt; a seq False x = x seq True x = x There is an argument for putting `seq` in a type class (and I believe it was, in pre-98 Haskell). Even so, you wouldn't be able to hand-write the instance for `(-&gt;)`, as you can't reduce a function to WHNF by pattern-matching. `par` is even more magical, of course. The classes which support `deriving` are special, in that you can't add new ones in-language. In GHC, `ST` is implemented nearly the same as `IO`. You can make a pure-Haskell implementation (e.g. stuff the refs into an `IntMap`) that's O(log (num refs)) slower, except you run into typing issues and require `unsafeCoerce`. I think.
people could come down and do really simple stuff. it would open the doors for others. it won't be about big frameworks and all that shit, but instead simple enough to let others appreciate the innovation: - zipWith - map - foldl - sum that would be a brave reduction. google even got map/reduce "patented".
Was I unclear when I said "Show me how you write or call a nullary function in Haskell"? &gt; I have never claimed that 5 or nullary above IS a function. That's exactly what you claimed! You wrote `nullary = 5` and said that the number of lambdas correspond to the arity. But it's a lie, because typing `name = something` in Haskell's toplevel establishes a _binding_, not a _function_. If you don't take parameters, you're not a function! This should be intuitive, because Haskell is referentially transparent. Why would a 0-arity function need to exist in a language with pure functions and no side effects? The only reason you see them in other FP languages such as Lisp and ML is to produce a side effect.
See [Simpson's Paradox](http://en.wikipedia.org/wiki/Simpson's_paradox). Fuck it, anything named 'Paradox' is worth looking at as an example of something that denies intuition.
Oh, and then people will say "You can write toy programs in Haskell, but nothing of serious magnitude". We need nothing less than say a web browser to break that barrier.
&gt; I'm not convinced -- how many of them even try? Approximately as many as have learned Haskell. There's a very low threshold for how much trying it takes to exceed an "acceptable level of effort". Most programmers in industry are more than *capable* of learning Haskell, and the ones who truly aren't probably shouldn't be programming at all, but the time and effort required to actually do so is more than can be reasonably expected. Out of all software developers in industry, take the subset of those who want to spend significant amounts of time programming outside of work, the subset of those who actually have the time to do so, the subset of those who would rather spend months learning a new language instead of hacking on familiar stuff, and the subset of *those* who pick Haskell instead of some other niche language. There's not many left at that point, I suspect.
I support practical transparency regulation for financial institutions, and the enforcement thereof. Obviously I don't agree with dirty banking practices.
&gt; When I read this post I thought the term "zygohistomorphic prepromorphism" was a joke. It's not; I Googled it. Oh, [it is a joke,](http://catb.org/jargon/html/H/ha-ha-only-serious.html) don't you worry. 
Thanks for the great reply! The one that especially got me thinking was seq, I felt I should be able to write it. Deleted: ridiculous code that made no sense. At least writing that made me realise the task was impossible.
There are two kinds of "trying". Either one is willing to put in cursory practice to learn a new notation, or one is willing to put in hard work to learn a new way of thinking. This applies not only to programming languages, but to every other field from mathematics and science to analyzing literature, working prepress, playing video games, or entering personal therapy. The only pertinent issue at hand is that "trying" mainstream languages when you already have familiarity with mainstream languages is only an exercise in learning new notation. There are differences, but those differences are only things that show up after you've mastered the notation. They're also the sort of thing that novice programmers often scoff at, because they involve mastering new ways of thinking. This isn't an FP problem, it's a laziness problem. The Factory design pattern is exceptionally helpful, as is higher-order OOP where you use methods for control flow, as is monkey patching, as is implementation decoupling via interfaces. But these are considered "advanced" and "too involved" because newbies must expend some effort to learn to think in these new ways. The biases that these methods encounters is exactly the same as the biases that Haskell encounters, and I've seen it in academia and industry just as often as in the casual programming world.
Well, I guess [the barrier is broken](http://www.cse.chalmers.se/~hallgren/Thesis/wwwbrowser.html) then.
[OK, I found the bug report](http://www.lix.polytechnique.fr/coq/bugs/show_bug.cgi?id=2203). It looks like extraction of the existing AVL trees in the coq stdlib might not be possible, even in 8.3. I think Oleg &amp; Ken Shan have showed how to translate ML modules to Haskell typeclasses, but I don't know about how usable the resulting typeclass interface is. Finally, it might not be that tough to translate a module-focused coq FSets implementation to one using, say, sections, which extraction can deal with in my limited experience.
Yeah, says you! You're about the only person who could say that a zygohistomorphic prepromorphism was a practical solution to some problem, [and I'd believe it](http://www.reddit.com/r/haskell/comments/cs54i/how_would_you_write_du_in_haskell/c0uvqqo), no questions asked. :)
&gt; There is an argument for putting `seq` in a type class (and I believe it was, in pre-98 Haskell). It was in a type class, but it was still magic, because there was an instance for functions, which, as you say, can't be written manually. In fact, as I recall, every type was automatically a member of the relevant class, without it even having to be derived, which is a pretty unique situation.
Well, no. When you can buy insurance for a loss that you will never suffer then it's no longer insurance (put simply: if I buy fire insurance on my house then it's insurance. If you buy fire insurance on my house then it's an investment. If I invest in international stocks and hedge the currency then it's insurance. If you hedge the currency just to hedge the currency then it's... hell, I don't even know what it is).
Haha, I had exactly the same reaction when I heard that term.
&gt; `unsafeCoerce` is very muuch magical, and highly unsafe. Interestingly, `unsafeCoerce#` in current GHC can be defined in terms of `unsafePerformIO` - making them *both* very magical and highly unsafe.
maybe this is an experience for haskellers, not being taken seriously, being labeled "academia" and so on. first, i thought the same and was intimidated by overly abstract computation (as i imagined). but i like haskell because it makes normal, simple programming easier. with regular programming, you have to write for-loops, take care of the running variable and so on, you spread your manipulations across all the code. in haskell, it's more clear and you can write things down so easily. i guess "normal programmers" might be intimidated by haskell's complicated stuff, when in reality haskell can allow your for-loop to be condensed into two expressions, e.g. "sum prices". and it makes it easy to reason about your code, because of the explicit type signatures and because the flow of data is explicit. so haskell is also removing complications of conventional programming. this is the most useful aspect in my eyes. it could, as an example, be used to write plugins for conventional programs, those would be very valuable and very short and would show what is possible with the language. e.g. decoders for ethereal/wireshark. or simple filters, in mail processing or somewhere in a browser to post-process html files. conceptionally very simple, but still very useful. 
Sure, I might want a student loan (for the rest of it, I don't need to own a house or a car and I prefer to spend money I already have). I can get that from a local community bank or a credit union. I'm sure that big banks support the operations of smaller institutions to some extent, but most of their work has nothing to do with supporting small institutions or with individuals' direct needs. Off-topic as this may be, I'm very glad to see somebody in the Haskell community suggesting that being in bed with the finance industry may not be an unmitigated good. Now if only someone would say the same about the defense industry...
Maybe not, but capitalism is.
you should understand that i would not trust these statements from somebody in the financial sector, because they say the same when they're lobbying. i don't want to argue with you here.
If you want to do interesting things, why not start now? The thing about money is that if you think having some of it is good (and worth sacrificing intellectual satisfaction), more money will generally seem better -- people who work at financial companies aren't stupid, and when they hire someone, they generally have incentives in place to make sure that person doesn't quit in a couple years and waste the investment the company has made in training this person.
I'm not sure there's any evidence that language adoption has anything to do with how hard the language is to learn. If anything, programmers find Haskell hard because they believe it's hard, maybe partially because of formal-schooling-induced fear of mathematics. I don't find Haskell "complicated" compared to, say, writing *good* code in Java or C++ (it may be easier to write broken code in the latter two languages, but see below). &gt; But to me, being good includes being practical, not just having a mathematically beautiful foundation (if you know enough maths to appreciate it). I don't see how you can enjoy programming without enjoying mathematical structures, and I don't see how a language can be useful without having a rigorous foundation. Anything else and you're building bridges out of sand.
&gt;Successful features from Haskell will continue to make their way into more popular languages. I think this is most likely the case. &gt;I want to use something good, not just something popular. My desire for Haskell to be popular is not based on some desire to only write in popular languages (if so, why would I even bother?). I would like Haskell to become more popular because of the benefits that come with popularity: tool support, libraries, documentation, etc. These are all of the tangential factors that increase productivity in a language. I would also claim that these are the reasons that C++ and Java have remained so pervasive (along with the current state of computer science education). Almost every game you can name was written in C++ because all of the game engines and libraries are written in C++. Its less about wanting to use a popular language and more about not wanting to reinvent the wheel. &gt;the world is already moving away from these two. I hope so but I haven't seen any strong evidence of their death yet. I would really like to see a new systems language come along and supplant C++ because I can't use VM based languages in my industry (computer forensics). If I am writing an incident response tool for a livecd or an IR flash drive then I need certain language features: native compilation, cross-platform executables, and good performance. Haskell is one of the few mainstream functional languages with all of these features (can't use Clojure, F#, or Scala). I could use Erlang but Haskell is faster. I could also use OCaml but Haskell has a better community, libraries, and tools (I would also like to avoid any association with JDH). 
If that were true, population growth would have brought the entire system down centuries ago.
Would you care to elaborate on that?
You're kidding yourself if you think that's not the same rationalization everyone uses. "I really want to do environmental law, but I'll just spend a few years working in the Legal department of Conglo-mo and then I'll be able to live my dream..." Most people don't make it out.
So, "everything is a function" is supposed to be a metaphor that helps you learn how to program Haskell. I would not ask the question, "Is the metaphor absolutely correct?" but rather "Did it help me in those first few days to learn how to write good Haskell?" Do not read too much into it and just treat it like the good metaphor it is because, ultimately, who really cares if it is? All that matters is that you know how to write good Haskell code (and that you can write that good code).
I'll give it a shot. For capitalism to be a zero-sum game, the premise here would be (correct me if I'm wrong) that the total amount of wealth in the world is fixed and that any gains that are made would have to be balanced out by losses somewhere else; thus there is no such thing as wealth creation, only wealth redistribution. Somehow from this line of thinking we end up at the idea that the free voluntary exchange of goods is evil because one player is always taking advantage of the other and profit is evil. batkins has a point though. For example, lets go back 200 some years ago, back when we were on the continental, since our dollar is worth about as much as the continental was back then. Our population at the time was 2.5-ish million. We now have a population of over 300 million. Question: Do we as americans on average possess 1% of the wealth individually as we had back then? I think not. How many of them had cars? Air conditioning? And since that time, name one part of the world that has seen a decrease in their standard of living. There is a very strong case here for a net worldwide increase in wealth, which would mean the capitalism isn't a zero-sum game. 
&gt; I don't see how you can enjoy programming without enjoying mathematical structures, and I don't see how a language can be useful without having a rigorous foundation. I sympathise with both points to some extent, but something about programming that I personally enjoy a lot more than either of the above is building software that does something useful. Any mathematical rigour and formal structures are merely means to that end, unless one is an academic with a mathematicians mindset, interested in the study of the method for its own sake and not only for the results it produces. Obviously many Haskellers are. However, most practising programmers are not. &gt; Anything else and you're building bridges out of sand. Most software is not written in languages with the kind of theoretically sound foundations of Haskell, but that doesnt mean all that software is not useful. Indeed, I respectfully suggest that your implicit claim to the contrary represents the disconnect between a lot of Haskell advocacy and the rest of the programming world: most practising programmers simply have different priorities.
I think it's considered hard because it basically requires relearning how to program, which means an extended "gimpy" stage where you struggle to do even trivial things idiomatically. It's tough to push through that barrier and reach the daylight at the other side.
&gt; I'm not convinced -- how many of them even try? Is that a useful question? I think a more interesting question is, of those who try, what proportion succeed? &gt; Learning a new way of thinking requires delaying gratification. And for most people, Haskell is about seven new ways of thinking all at once. Of course learning new ways of thinking requires some investment, but many programmers have made a jump from one programming language and style to another language with a very different approach at some stage in their careers. The basic ideas of functional programming might take some getting used to for someone with a background in imperative programming. However, I dont see why it need be any more of a leap than, say, going from programming C (procedural, static typing) to programming Python (a lot of OO, dynamic typing). Indeed, many ideas and techniques relating to higher order functions and to controlling state, perhaps the two most significant concepts in functional programming, have also been implemented and are routinely used in many mainstream programming languages today. &gt; So I think a lot of people get tired of feeling dumb, get bored of computing factorials and quit early. Those same people might be smart enough to learn it, and perfectly capable if (say) their boss forced them to. But the question isnt just whether it is possible to learn it, in isolation, is it? Effort invested in learning Haskell is effort that wasnt spent learning a different language, or learning new skills to improve performance with a language already known. This is why I wrote with an acceptable level of effort before.
I'm happy to aver that most practicing programmers lack the pride in their work to make sure they have as much confidence in the quality of the artifact they're producing as is feasible with current technology. It's understandable (since there are few economic incentives for spending more time writing correct software when buggy software can be delivered sooner) but not excusable. Again, I can build houses out of plasticine and claim that I'm "doing something useful" and not getting bogged down in academic structural soundness concerns, and maybe nobody would even notice until the next tornado.
"name one part of the world that has seen a decrease in their standard of living." You're assuming that "standard of living" is an absolute notion rather than a relative one. I'd assert (and there's evidence for this -- too lazy to look it up now, sorry :-) that social inequality strongly affects one's quality of life. That is, it feels worse to be living in a tent when you can see the mansions up on the hill than if everyone is living in a tent. So if that's true, I think it's pretty obvious that a lot of parts of the world *have* seen a decrease in their standard of living over the past 200 years. 
If the idea of a 0-ary function makes sense at all, it would be very odd if you could make 3-ary functions with 3 lambdas, 2-ary functions with 2 lambdas, 1-ary functions with 1 lambdas, but 0-ary functions also with 1 lambdas (or more).
Nevertheless...that's not zero sum.
Here you are confusing nullary functions in kurtel's sense with unary functions taking a unit value. Without currying, an n-ary function is a function of one argument which is an n-tuple, so a 0-ary function would have to be a function taking a unit value - here all levels of the iteration are actually mathematical functions, just taking different types of arguments. With currying, an n-ary function has n arrow types before the final result type. Here the 0 case makes sense as well, but the 0 case of the definition is values. That is, when you inductively define n-ary functions to recover a notion of multiple argument functions from curried single argument functions, you find that all the non-function values fall under the base case. Whether that's actually helpful in coding depends on the extent to which the language is designed so that things which work for n&gt;=1-ary functions also are true of values in general. There are significant differences especially around sharing, but it does tend to be a bit more true in Haskell then most other languages - all names are defined with an =, rather than defun foo(args) {..} vs val foo = .., no possibility of side effects in referring to values, allowing mutually recursive definitions of values, etc.
The IO monad could mostly be written as a plain GADT, something akin to: data IO a where ReturnIO :: a -&gt; IO a BindIO :: IO a -&gt; (a -&gt; IO b) -&gt; IO b ForkIO :: IO a -&gt; IO ThreadId GetChar :: IO Char PutChar :: Char -&gt; IO () ... and so on for other primitives, whatever they may be ... or in a number of other similar ways (some of which preserve some algebraic properties more soundly). At this point the runtime system would merely contain an interpreter for the primitives of this language, to actually execute the described actions (main specifically). There's a bit of a question about how to handle FFI this way, and it would very likely be more difficult to optimise, but it would certainly involve less magic.
Conversely, the demonstrable fact that capitalist systems have historically been positive sum (at least in terms of raw material wealth, ignoring thorny issues about assigning "value" to positional goods) does not imply the nonexistence of profitable activities that are of overall net negative value, particularly when taking very diffuse negative externalities into account. Real life, as always, is tediously complicated.
we agree on something!! woot! :-) If we actually had a free banking system, I'd agree with vladley. The problem is that the entire system is one big cartel (in the U.S. it's the federal reserve system) who has the power to rob people of their wealth through (monetary) inflation on a whim. And then government on top of it encourages banks give loans to people they know can't pay them back. The banks need to take a hike. Frankly though, I'm more upset with the government than the banks, but they're both culpable. Regulations are a joke because there's a conflict of interest between government regulating banks and government wanting the banks to buy government bonds with printed money so that they can spend. Ok, I'm getting off-topic now, but I can't help but laugh that our "financial reform" institutionalizes bailouts and gives more power to the banking cartel (federal reserve). People thought Bush was bad (and he was, don't get me wrong, he was), Obama is Bush #3 and has managed to be even worse. ARGH!!
I was asking about your statement that "If that were true, population growth would have brought the entire system down centuries ago", and I still don't see why that might be; magical_liopleuridon's answer doesn't answer anything, given that quality of life is relative. So if you have a better explanation, I'd love to hear it.
"There's a bit of a question about how to handle FFI this way" Exactly -- it's not really the IO monad that's magic, it's FFI that is. And FFI is pretty magic in any language. Monads just give you a way to reason about the order in which FFI calls get evaluated. But the "magic" needed in GHC is essentially the same as the magic needed in your example.
My point exactly is that 0-ary functions make no sense in Haskell. If you can't rewrite it as a lambda it isn't a function.
&gt; Here you are confusing nullary functions in kurtel's sense with unary functions taking a unit value. I misspoke when I lumped Scheme and ML into the same category. In Scheme you can certainly define a lambda with no argument list: (define (foo) (+ 2 2)) In ML you need the unit parameter. In either case, you cannot define a function that takes no arguments in Haskell. &gt; That is, when you inductively define n-ary functions to recover a notion of multiple argument functions from curried single argument functions, you find that all the non-function values fall under the base case. I'm too tired and sick of this conversation at this point to tell whether or not you're agreeing with me so I'll just try one last time to hammer home what should be completely obvious. If you invent a notion of nullary functions that can be defined in Haskell with verbal trickery around passing the empty tuple, you're still left with: - a big pile of non-function values, or - two fundamental things rather than one, functions and the empty tuple, owing me an explanation as to why the empty tuple is special and not converted into a nullary function itself, and - having to explain how exactly Haskell is taking something I would consider a pure value like 3 and converting it into a function \() -&gt; 3 - when the magic empty tuple is being inserted by the language so that any sort of evaluation can work, or - how this helps you to write software in Haskell If you invent nullary functions by saying they are the base case of higher arity functions, you're still left with: - explaining why there's no syntax for defining a nullary function in Haskell, either at the top level or with a lambda, and - explaining why single arity functions aren't an acceptable base case for the induction instead (they do have the advantage of existing) &gt; Whether that's actually helpful in coding depends on the extent to which the language is designed so that things which work for n&gt;=1-ary functions also are true of values in general. For what feels to me like the fourth or fifth time in this thread, let me remind you that we are actually talking about Haskell. &gt; There are significant differences especially around sharing, but it does tend to be a bit more true in Haskell then most other languages - all names are defined with an =, rather than defun foo(args) {..} vs val foo = .., no possibility of side effects in referring to values, allowing mutually recursive definitions of values, etc. I'll go out on a limb and assume that these words actually have meaning and that through some personal problem of my own I just can't see it right now. WTF are you talking about? That is, what does this have to do with the (IMO settled) question of whether or not Haskell has nullary functions and the less settled question of how is that a useful mental model _for programming in Haskell_. I have no idea what you're saying here.
&gt; people who work at financial companies aren't stupid, and when they hire someone, they generally have incentives in place to make sure that person doesn't quit in a couple years Thanks for the tip. I'm forwarding this to someone who has just fallen into this trap. 
Agreed. The system as a whole can be positive-sum but there can absolutely be zero-sum or negative-sum activities within it. And actually, given that certain inputs into the system (I'm thinking mostly of fossil fuels) have been permanently spent and could potentially create catastrophic climate conditions, you could say that the system is zero-sum or even negative-sum in that respect. That is, we need to include nature in the system and then we see that a lot of the benefits have accrued to humans while being removed from nature (not all of them - some resources are renewable and some can be effectively managed as rising market prices reduce their use).
Dude, relative income disparity has nothing to do with it. I also think income inequality is a bad thing, but it doesn't change my point. If capitalism were zero-sum and we had a fixed amount of wealth (I take wealth, therefore someone else loses it; I lose wealth, someone else gets it), then wealth (actual, measured wealth) could not increase over time, as it demonstrably has. Sure, people can see their psychological quality of life decline due to income inequality - but that's not the same thing. The total amount of wealth is still increasing and there have been long stretches of time (the postwar US up until about 2000, for example) where everyone's wealth increased and inequality was insignificant. You're making a different argument than "capitalism is zero-sum."
I don't see why people assign such nefarious purposes to the Fed. Prior to its creation, our recessions were deep, devastating, frequent and long. See Panic of 1857, Panic of 1873, Panic of 1907 (those are the biggest, there were more in between, mostly at the rate of every five or six years). Let me guess: your alternative to the Fed is the gold standard. But we HAD that. For decades and decades. And the result was terrible recessions that took forever to recover from. The point of the Fed is not so much "the government is taking control of the private money supply," but rather "_someone_ is taking control over what no one controlled before." Under the gold standard, the amount of money in circulation is governed only by people's willingness to spend and by the supply and demand for gold. During a recession, people start hoarding money out of fear. So, say the recession starts in the construction industry, but it scares people. When people hoard their money and suddenly stop spending it, that has knock-on effects - unemployment, falling sales - even in other, unrelated industries. You WANT the government to have some say over the money supply here. If you print more money, you lessen the effect of this hoarding and weaken the vicious cycle of fear-hoarding-collapse-fear-hoarding... And I don't know why you think the Fed was created so the gov't could steal wealth from you via inflation. In the 70's, in fact, oil supply shocks had set off a wave of devastating inflation and Paul Volcker at the Fed spared no expense to choke it off with high interest rates, even to the point of creating a devastating recession. The Fed is crazed about fighting inflation. Inflation has been running at 2% for years. Even now, when it's dangerously close to becoming negative, the Fed is afraid to act any more for fear of creating inflation. See also: http://en.wikipedia.org/wiki/Gold_standard#Disadvantages
&gt; For example, lets go back 200 some years ago ... How many of them had cars? Air conditioning? Um?
I think you're talking past each other to some extent here--I alluded to this in another comment, but it's actually surprisingly tricky to define "wealth" in a viable manner. Clearly wealth must entail something beyond raw material resources, relating to specific *configurations* of matter. If I take a hammer and smash my computer into pieces I've reduced my own wealth in some sense, despite owning the same pieces of matter I did before. There doesn't seem to be anything written into the laws of the universe that defines the value of some configuration of matter; if you chase justifications far enough back you'll reach the tautological point of "this has value because someone values it". That's really the best we can do. But this quickly leads to the disquieting observation that many things are mostly valued, not for their own sake, but *having them when other people don't*. A term for this is "positional good", but in reality most things have some mixture of positional and non-positional value. Pursuit of positional goods leads to a prisoner's dilemma-style situation, where everyone would be better served by having none, but any individual benefits by acquiring more. Thus, introducing more of a positional good into the market can, seemingly paradoxically, simultaneously *increase* the total global material wealth, while *decreasing* the sum value of everyone's possessions. And given that subjective value is the only viable definition of value at all, it's not really clear that the former is useful or relevant. A trivial example of a purely positional good is *money*, where creating more of the good (also known as "inflation") can make everyone worse off. There are plenty of goods with significant non-positional value, and I really can't see how people today aren't vastly better off in absolute terms, but it doesn't really make sense to treat income equality as distinct from wealth.
What does wealth mean, and how do you measure it? I mostly agree with camccann's comments, particularly about "positional goods".
Even aside from considerations of inequality and quality of life, there are most certainly places in the world that have seen a decrease in their quality of life. Check out the history of [Haiti](http://en.wikipedia.org/wiki/Haiti) (and I mean up to 2009) or, indeed, that of many other nations dealing with the legacy of colonialism. 
&gt; I'm happy to aver that most practicing programmers lack the pride in their work to make sure they have as much confidence in the quality of the artifact they're producing as is feasible with current technology. Im sorry, but that is an incredibly patronising and insulting assessment of the programming community. There is a balance to be struck between various measures of quality and the time and resources invested in developing any given piece of software. Different projects demand a different balance in order to be useful, and there is nothing wrong with taking pride helping people to meet some need they have. To pick up your metaphor, are there not places in the world where building many cheap huts for basic shelter would save many lives, while building a single expensive but tornado-resistant hut would not? In any case, the programming language chosen is only one factor in the robustness of software. I dont know who develops the most reliable code in the world, but the guys who wrote the space shuttle control systems for NASA must have earned a place on any shortlist, having consistently produced software with error rates orders of magnitude below the industrial average for many years. They wrote that software in a language designed about three decades ago, which is closer to Fortran than to Haskell. A lot of safety-critical avionics systems have been written in Ada and its variants. Some high reliability telecommunications systems are written in Erlang. Obviously each of these languages has features that make it attractive for writing high reliability software, but more often than not it is the development process that really stands out when you look at how these organisations build their software. No language is going to tell you that youre implementing the wrong thing because whoever wrote the software spec misunderstood the real world requirement, for example. Of course, Haskell has its advantages for writing robust code, such as a relatively expressive type system. It also has theoretical weaknesses that many other languages do not, for example unpredictable run-time performance in the presence of lazy evaluation. So I think it demonstrates a rose-tinted view of the world, dare I say even a certain arrogance, to presume that anyone who chooses to use tools other than Haskell to develop their software cant take pride in the quality of their work.
I never claimed that Haskell was the best language for industrial software development -- only that it's foolish to reject Haskell because it encourages prioritization of "academic" concerns like correctness. Laziness vs. strictness is really beside the point, which is whether static typing is "impractical" or is actually helpful in achieving your practical goals. NASA is a great example to talk about here, because they offset their use of languages like C with an incredibly rigorous and labor-intensive review process. A higher-level language (and I don't necessarily mean Haskell) would reduce their costs by shrinking the size of the trusted code base. More to the point, very few organizations put the amount of work into quality assurance that NASA does, which is all the more evidence that it's the norm to do a shoddy job because that's what customers expect. Yes, you can build quality software in a language that's the wrong tool for the job if you put a huge amount of extra work for it -- but most people don't put in that work, and don't get a quality product as a result.
&gt; Let me guess: your alternative to the Fed is the gold standard Yes :-) &gt; But we HAD that more or less, I'll get to that &gt; For decades and decades. And the result was terrible recessions that took forever to recover from. Not in the free banking era. The gold standard does not cause recessions and it does not cause recoveries to take forever. A bust is a correction in the market from a misallocation of resources due to unsound credit expansion. All of the panics you cited as well as all the others fit this description. Free banking makes systemic unsound credit expansion unlikely, though another way to deal with it would be some really heavy regulation, like requiring banks to have full reserves. &gt; If you print more money, you lessen the effect of this hoarding and weaken the vicious cycle of fear-hoarding-collapse-fear-hoarding... ahhh, Keynesianism. All printing money does is reduce the value of one's savings. &gt; And I don't know why you think the Fed was created so the gov't could steal wealth from you via inflation. In the 70's, in fact, oil supply shocks had set off a wave of devastating inflation and Paul Volcker at the Fed spared no expense to choke it off with high interest rates, even to the point of creating a devastating recession. The Fed is crazed about fighting inflation. Inflation has been running at 2% for years. Even now, when it's dangerously close to becoming negative, the Fed is afraid to act any more for fear of creating inflation. I'm gonna have to call bullshit on most of that. Volcker is probably the best fed chairman we ever had, I'll give you that. And what he did was definitely the right course of action. As far as the oil supply shocks setting off inflation, the inflation (monetary expansion) had already happened. The reason we went off the gold standard is because we printed so many gold certificates, there were more gold certificates than there was gold.....so there was a run on our gold supply. In other words, we defaulted. It's not a failing of the gold standard, the government shouldn't have been printing certificates like that. After Volcker left, the Fed was no longer crazed about fighting inflation. These 2% numbers, you're getting that from the CPI? 'cause that's not believable. Check out the M3, you'll get a more realistic picture. I'd like to go into this further, but I just had an awesome pizza!! and I'm tired and need to take a nap, lol. For more info, go to: * mises.org * campaignforliberty.com * lewrockwell.com and read: * Ron Paul's The Revolution * Ron Paul's End The Fed 
It would be relatively easy to do this in cabal-install. It already builds a dep graph (see `InstallPlan`). The main difficulty is with managing the output of each build. You'd probably want to do each build in a separate process and just capture the output. Then instead of displaying the results of the build to the user, you'd list which packages are currently building. Yes, the install phase would have to be serialised. Volunteers/patches welcome.
I'd find a parallel "cabal build" much more useful, because my working set of installed packages fairly quickly becomes static, even on a new machine.
&gt; people who work at financial companies aren't stupid, and when they hire someone, they generally have incentives in place to make sure that person doesn't quit in a couple years and waste the investment the company has made in training this person. Like what?
&gt; I could also use OCaml but Haskell has a better community, libraries, and tools (I would also like to avoid any association with JDH). I sympathise strongly with the idea of avoiding association with JDH, but effectively holding his existence against an entire community seems a bit harsh!
Why exactly would parallel ghc --make be that hard? It seems that there's already a topological sort of the module dependencies in place: put that into a queue that can hand out compilation units and accept the results, and won't release a job until its dependencies are finished. Then you just forkIO some worker threads and go to work.
I get my information from economists, not from obstetricians.
I think that free banking is so patently crazy that I can't even address it as a point of view. We had the faintest taste of free banking two years ago (leverage ratio limits for investment banks were eliminated) and that was, ahem, problematic. We went off the gold standard for the first time in the Great Depression. Funny thing, what happened: http://fabiusmaximus.files.wordpress.com/2009/03/gold.png?w=717&amp;h=469 Yes, I'm getting 2% from the CPI. Larger money supplies don't necessarily mean inflation - look at the past few years if you disagree - so M3 is not really the place to look.
That's a picture of you, isn't it?
See nominolo's talk on [the topic](http://vimeo.com/6572966). GHC's somewhat crufty compilation manager is a bit hard to hack.
Prior art: Gentoo's portage system does exactly what you describe. (Except possible serialized install, but I would provisionally assume it does that too.) One additional thing: It knows what is error vs. what is standard output, and if a parallel build stops due to error it still displays the error immediately.
Edward mentions that its possible to statically enforce correct resource usage. Does anyone have a link or an example? Ive been looking for this for a while. 
+1 I was actually wondering about this because it seems like it would be relatively easy to parallelize the build process in a language like Haskell as compared to many of the others. 
Most of them involve large piles of money, I gather.
Look for Lightweight Monadic Regions by Oleg Kiselyov (should be the #1 google hit).
keynesians are not real economists.
&gt; Larger money supplies don't necessarily mean inflation Yes they do. An increase in the money supply = inflation by definition. An increase in prices are the effect of inflation. The CPI is a loaded number. Whenever it the number looks like it's getting too high, government changes how it's calculated to make it look like prices aren't going up. &gt; We had the faintest taste of free banking two years ago (leverage ratio limits for investment banks were eliminated) and that was, ahem, problematic. in a free banking system, banks don't get bailed out. Companies that behave irresponsibly should be allowed to fail and enter oblivion and banks are no different. 
Except it's yet another toy program (1994 version: ` Implementation time: approximately 1 man month.`). Looking at the screenshots, it was barely usable as a browser in its time, and totally unusable nowadays (last update: 1997). Admirable as it may be, it's an example of the large number of Haskell projects that live for the length of a thesis or two and do not leave academia.
Then select mises.org from his list. It's named after economist Ludwig von Mises and has published the works of a number of economists. Right now you're being like the mainstream programmer, who ridicules Haskell without ever giving it a real shot. Yes, the Austrian view of economics is different, and it's definitely not mainstream. But that doesn't mean it's wrong! magical_liopleurodon and I strongly encourage you to do some reading, and form an informed opinion, instead of a biased mainstream one. This article alone will answer many of your questions: http://mises.org/daily/3515 and there are resources aplenty to answer any more. Plus, they have a channel on freenode.
Could the pointless political arguments please go somewhere other than /r/haskell?
fair enough....one sec.....
shoot, I'm not sure if I can strikethrough the "keynesians are not real economists" comment. I've downvoted myself though :-D Going back to the realm of economics, I'm just gonna have to 2nd RoyalMonkey :-). 
Haha. No, I don't hold his existence against the OCaml or F# communities. I'm sure I have similar tastes to some douche out there and I wouldn't want that held against me. I mentioned JDH last because his presence in that community is really the least of my concerns with the language. OCaml just doesn't have much of a community behind it. So tools and libraries will always be a weak spot. There would need to be something demonstrably better about the language for me to switch from Haskell.
&gt; Most of them involve large piles of money, I gather. Well, as you say people who are attracted by money probably stay attracted by money, and as with everything pay does increase with experience. But I don't think there's anything more to it than that, and certainly some people do leave after a few years to go back to places like academia.
This is freaking awesome. Everyone should buy snoyberg a drink.
For a moment, I wondered whether I had eaten too many lambdas before realizing that the graphs are *right-to-left*. Confusing!
Why do all the Windows users download it twice?
download twice, compile once.
I'm pretty sure dcoutts knows that, he's one of the guys behind haskell overlay for gentoo.
unfortunately compiling (and building) is not a pure computation, so there's not that much advantage.
I completely agree. I'm always close to crying when I cabal build packages on our 8 core server. (And given how we push Haskell as the language for multicore apps, it's also embarrassing.)
Agreed. With all due respect to the author, I am not able rightly to apprehend the kind of confusion of ideas that could provoke such an orientation of the graphs.
Money is governed by supply and demand. If the amount of money increases but demand is falling, that doesn't create inflation. I don't care about the absolute size of the money supply - I care about its effect on prices. Banks ARE different because lending creates money. When a bank goes under, it collapses a portion of the money supply. Plus, there are confidence effects, a la Lehman Brothers. The best approach is to regulate and monitor banks so they don't fail. Nobody wants bailouts.
Aren't they? So you don't think debt-financed government spending to fight World War II is what got us out of the Depression? If you do, then you too are a Keynesian. If you don't, then what did it?
I know something about Austrian economics. It's based on moralizing rather than numbers. "We need to suffer through this recession because people made bad investments." But your comparison to Haskell isn't really accurate. Right now, there are a lot of vocal people who subscribe to your point of view. You hear it a lot at protests, from Ron Paul people, from the WSJ editorial page. It's a lot harder to support an older view that's based on analysis and doesn't reflexively call government bad.
&gt; Money is governed by supply and demand. correct &gt; If the amount of money increases but demand is falling, that doesn't create inflation. If the amount of money increases, the money supply is inflating. ie inflation is occuring. If demand falls along with the money supply going up, prices remain constant, but inflation is still happening. &gt; I don't care about the absolute size of the money supply - I care about its effect on prices. ok. &gt; Banks ARE different because lending creates money. Lending creates money, yes. Banks should still be allowed to fail. Plus, we have FDIC, so most people would be ok. &gt; When a bank goes under, it collapses a portion of the money supply. Plus, there are confidence effects, a la Lehman Brothers. Correct. &gt; The best approach is to regulate and monitor banks so they don't fail. It's one approach and it's a viable one. &gt; Nobody wants bailouts. I wish this were true. I'll say that very few people want bailouts and that it's very unpopular among liberals and conservatives despite their popularity with democrats and republicans.
Whoohoo my nick is associated with haskell - higher level nerd unlocked! 
&gt; Aren't they? Nope &gt; So you don't think debt-financed government spending to fight World War II is what got us out of the Depression? Nope &gt; If you do, then you too are a Keynesian. Well, I'm not &gt; If you don't, then what did it? Essentially, FDR dying in office. Truman was better than FDR and Hoover and he relaxed some of the crushing tax burden that FDR hoisted onto the people.
Wasn't just for him. It's good to know that the solution has worked before for everyone involved.
I wish cabal-install worked more reliably before we set out to improve its speed. We need import/export signatures from packages so when we actually try to install something it doesn't fail on silly mis-specified version dependencies. And no, the PVP wouldn't really help here, because it makes version dependencies be over-specified. Haskell encourages far more modularity which results in far more package dependencies than in other eco-systems. This is great, but it means it also requires more powerful package management than cabal-install can currently provide.
This has been noticed before. For example, the last release candidate there were 122,991 Windows downloads, but 20,632 unique.
[That's how MRTG does it](http://www.google.com/images?hl=en&amp;safe=off&amp;q=mrtg&amp;um=1&amp;ie=UTF-8&amp;source=og&amp;sa=N&amp;tab=wi&amp;biw=1066&amp;bih=591), for some reason.
Editable tags look cool! And taking maintainership.
ITYM "objectionable languages" :-)
it's not ideal to hook up webkit, but OTOH they've done decent work interpreting html: http://hackage.haskell.org/package/webkit
Oh, when you compare it to mosaic, it's quite competitive, indeed. ...but mosaic is no comparison to present-day browsers.
Aye, mebbes \*hangs head\*
Neat. But do you really need to test gcd? That seems like a heavyweight function, though it does seem to be the fewest characters. In a situation like this I'd prefer to use mod, as that's usually faster, but I guess then you'd have to compare equality to 0, and that's an extra character.
Way to hide the decline, dons.
Hmm. I was trying to work out what the rough daily rate of submissions was. If you think there's something else in there - here's the [raw data](http://www.galois.com/~dons/tmp/submissions.dat). Have at it. *Edit*: here's the "smoothed" data, and the average (5.4 / day): http://i.imgur.com/ahYKO.png
I was trying to be funny... and failed.
Ah. I thought it was more interesting that the rate is pretty steady (4-6 average) over 2 years.
Yeah, this is cleaner and faster: 2:(nubBy (((==0).).mod) [3,5..]) EDIT: but only this is correct: 2:(nubBy (((==0).).flip mod) [3,5..])
Yes, it is interesting. I haven't been subbed to /r/haskell for long, but there does seem to be a steady flow. You usually do a fair share, but I noticed that when you were traveling or something other people kept the submission rate up.
you forgot to import Data.List, by the same criterion you could just import some library that gives primes straight up Also that code is nice, the improved version in a comment is even nicer.
yep, just slightly more than I can keep up with...
[Zygohistomorphic prepromorphism](http://www.haskell.org/haskellwiki/Zygohistomorphic_prepromorphisms). Get It Right! :D
People asked for this a while ago. Here's the [raw data](http://www.galois.com/~dons/tmp/submitters.dat).
dons == dons' robot?
Remember: his Twitter handle is dons*bot*
I was looking for dons and couldn't find him, but then I realized I wasn't looking high enough!
It shouldn't :)
your name is suspiciously absent
log scale.
Wow, hadn't realized I posted so often.
dons, could you inform us how exactly you constructed this tag cloud?
http://tagcrowd.com applied to http://code.haskell.org/~dons/haskell-reddit.html (last 3200 submissions)
Would you like to explain it?
As I read this procession of posts, I get the feeling that we're taking high-school math and reinterpreting it in the types context. Very cool. :-)
 ((==0).).mod = \x y -&gt; (((==0).).mod) x y = \x y -&gt; ((==0) . mod x) y = \x y -&gt; x `mod` y == 0 So, the whole thing is: 2 : nubBy (\x y -&gt; x `mod` y == 0) [3, 5 ..] nubBy eliminates duplicates from a list, based on the provided relation. It should probably be an equivalence relation, but the above happens to work anyway. The list already has all multiples of 2 removed, so when it comes to each prime, it eliminates all elements of the list that are multiples of it, and continues, leaving only primes.
&gt; 2 : nubBy (\x y -&gt; x `mod` y == 0) [3, 5 ..] &gt; It should probably be an equivalence relation, but the above happens to work anyway. Actually I made an error there, assuming it's symmetric. Transitivity could break it as well with different implementation of nubBy.
Everyone seems to have that Norvig book.
What is "Individual submitters" on the x-axis?
As a meta-reddit post, this post has a certain quantum mechanical effect - by commenting on it, we are changing it.
Only because dons is included.
Each submitter has a position on the x-axis. And the number of submissions that submitter submitted is reported on the y-axis. Left-most submitter is dons, with nearly 2000 submissions. The name of the submitter is written on the same level as its y coordinate but quite shifted to the right. When multiple submitters have the same y coordinate, they are still written on their (common) y-height, still quite shifted to the right. Submitters name for the 1,2,3,4 number of submissions don't have their name on the graph. 6 submitters submitted each 5 submissions (they are named yairchu, rgreayer, RayNbow, nefigah, masklinn and Aviator).
next time put an optional field for a [nick]name, it would be interesting to know who said something particular.
Looking for curve fit to Zipf power law.
Very interesting results, thanks for organizing this!
Reminds me of FunMath by Raymond Boute: http://www.funmath.be
The statistics are good, but what I think is really fascinating in [the results](http://www.johantibell.com/files/state-of-haskell-2010.html) is the column on Haskell's weaknesses. A lot of people put a detailed comment in that box, and it's well worth scrolling down and reading a few. 
The `blaze-from-html` tool is a great addition! It might be a nice way to start using BlazeHtml, sort of the new way to "view source". Unfortunately, it broke on the very first site I tried it with (http://dutchhug.nl/), the tool is probably a bit too conservative.
By the way, you mention that it would be great if a web framework integrated support for formlets. Yesod doesn't have support for the formlets package, but Yesod.Form has an implementation of the same principle. In other words: you don't have to splice together strings in Yesod.
Ah, I'm gonna try it out on that site and see what's the matter, should be fixable.
Congrats on the release! Looking at the new package, it would appear that the new version of Html is not just a wrapper around a Builder, as it was in version 0.1. Based on this, it seems that it would be more logical for Hamlet to use blaze-builder directly. Does that sound about right?
It also breaks on some other sites I tried it with, but it might be that they have malformed HTML. What's your goal with the tool? Do you only allow (mostly) valid HTML, which would be easier. Or try to recover from broken HTML, like TagSoup and HTML5 parsers do?
Yes, it using the builder directly might give you a bit of a speedup since you lose some overhead.
The goal is to support as much as is possible, especially when the `-e` flag is given. I use TagSoup for this goal.
You're right, that's definitely worth reading. Some of the comments are quite funny, I particularly like: &gt; What's Haskell's weakness: *It's young age* &gt; &gt; What would you replace it with: *Go*
I've uploaded `blaze-from-html-0.2.2` on hackage, which should fix this issue.
Hmm, cancelling out the "it has insufficient zygohistomorphic prepromorphisms" comments against the "math is hard, let's do OOP" comments, a lot of what's left seems to say that Haskell needs more library documentation, more development tools, and more hype.
That's computational maths, not programming. Programming is applied maths, in such arcane heights that ordinary mathematicians can't fathom the rigour, any more.
This proves that I still have a life.
These data are just crying out to be in a histogram or ECDF plot. Oh, I see the data are available. Just a sec... Okay: * [Histogram](http://community.moertel.com/~thor/reddit/rate_of_new_articles_appearing_on_haskell_reddit/haskell-submissions-histogram-600.png) * [ECDF plot](http://community.moertel.com/~thor/reddit/rate_of_new_articles_appearing_on_haskell_reddit/haskell-submissions-ecdf-600.png) R source: library(ggplot2) submissions_url &lt;- "http://www.galois.com/~dons/tmp/submissions.dat" submissions &lt;- read.table(submissions_url, col.names = c("count", "date")) qplot(count, data=submissions, geom="histogram", binwidth=1, main="Articles submitted daily to the Haskell Reddit", xlab="Count of daily submissions (N)", ylab="Count of days with N submissions") ggsave("/tmp/haskell-submissions-histogram.png") submissions_ecdf &lt;- with(submissions, { xs &lt;- sort(count) uxs &lt;- unique(xs) xs_ecdf &lt;- ecdf(xs)(uxs) data.frame(count = uxs, count_ecdf = xs_ecdf) }) qplot(count, count_ecdf, data=submissions_ecdf, geom="step", main="Articles submitted daily to the Haskell Reddit", xlab="Count of daily submissions (N)", ylab="Percent of days with N or fewer submissions") + scale_y_continuous(formatter="percent") ggsave("/tmp/haskell-submissions-ecdf.png") 
I dunno...I think I sort of get functional programming, but I really don't get how proofs can be done in functional programming.