I am not convinced by the idea that they are "entirely incompotable". In my read, Edward has two broad complaints about non-global (That is, modular) approaches to typeclasses * They break reasoning by making the category of constraints not a pre-order * They potentially disable global refactorings. I'm not sure the second complaint is true, that if it is true I should care. But, the first is a major problem with "modular typeclasses" and implicts. But, I believe it *could* be resolved in a modular type class system because it is a local condition: you really only need that "the part of the category of constraints here that I actually use is thin" and I don't see why, in principle at least, that can't be achieved in a modular setting just by making that thinness a constraint that must be satisfied for compilation to happen. If you are in a coherent corner there is no reason why the system shouldn't be able to prove that for you even if coherence doesn't hold everywhere--it has all the information it needs. Of course, this complicates the picture as adding new instances might break things that depended on uniques, but that is how it goes in logic programming--and indeed is just the classic "hijacking" problem other languages have dealt with.
Fuuzetsu has done some great work on Haddock, so I trust his opinion that it wouldn't be easy to extend Haddock with this. My comment above is more a critique of his assumptions rather than his competence. This issue in particular does seem problematic: &gt; I think the first sentence in the Markdown documentation after the introduction explains it pretty well: “Markdown’s syntax is intended for one purpose: to be used as a format for writing for the web.”. As it turns out, Haddock is not ‘the web’. It just happens that most people see it in action once it’s nicely rendered into XHTML and up on Hackage. It in fact also has back-ends for LaTeX and Hoogle! Does it make sense to have inline HTML tags in LaTeX? No. Does it make sense to have horizontal bars in Hoogle? No. Sure, you could argue that these backends could just ignore it but it makes no sense to allow Markdown, used as a mid-point between plain text and writing HTML by hand for Haddock. Haddock already has its own markup structures that other back-ends interface with, one of which happens to be for the web. But yes, maybe this is a case of perfect being the enemy of good. Writing a whole new documentation generator certainly wouldn't be an easy undertaking.
Ah, got it. You were talking about an implementation detail--I should have gathered that from your statement. I thought "improper prism" was more of a mathematical property or relation as "isomorphism" or "partial isomorphism" are. Thanks for your explanation.
OK, good to hear you don't see any fundamental problems that would need a rewrite of Haddock. I also think Fuuzetsu has done awesome work but that it might be worth revisiting the argument about Markdown. With the kind of fixes you propose and maybe some other pragmatic choices I don't see why we shouldn't be able to map a useful subset of CommonMark (probably disallowing embedded HTML) to a (revised) version of the Haddock internal AST. I think we can provide a good mapping to LaTex and a simplified one for Hoogle (dropping some markup).
Correct me if I'm wrong, but isn't Writer on lists O(n^2 )? It would probably be a good idea to use [difference lists](https://hackage.haskell.org/package/dlist) or something like that instead. Also, is there anything that makes it incompatible with `base-4.8.0.0`? Nice little library, BTW :)
Aha, I was not aware of that. It was apparently also standardized in a revision of the C++ standard. Thanks for the heads up!
The docs on the protocol (JSON messages over WebSocket) can be found here: https://pusher.com/docs/pusher_protocol
While there are a few useful techniques here for selling such as avoiding self deprecation and focusing on the negative, I don't think any "business type" would be even remotely convinced by this talk. I think what they want to see are case studies and metrics: company X competes with company Y, X uses Haskell, Y uses &lt;safe-choice enterprise language&gt; and X delivers measurably better SLAs, shorter development times, fewer bugs, more features, faster turn-around on product requirement changes, improved ability to attract, train and hold programmer talent, etc. Of course, this data doesn't exist, and it's difficult to prove that individual anecdotes actually compare apples-to-apples, but I believe this is exactly what's needed to sell FP to the understandably conservative mainstream. I encourage anyone who feels that Haskell contributes to this sort of excellence to gather and share concrete data like this. 
I called it boilerplate because writing the code was a purely mechanical process. i.e. we had types which exactly matched the JSON of the Pusher protocol messages, and each one had to have toJSON and fromJSON instances defined. In other words, all the Haskell code for doing this was entirely defined by our existing protocol.
All it really does is the single quotes, the rest falls through to markdown. So actually all it does is turn single-quoted things into markdown link format, and then the result goes to pandoc. And actually it relies on haddock, since it creates links into haddock-generated HTML. It doesn't extract from haskell source at all. So it wouldn't really be a good replacement at all :) But I must say, single-quotes for linked haskell references and markdown for everything is a pretty pleasant way to write. I wouldn't want haddock to be markdown though, I'd just like a simpler more predictable haddock with less quoting.
Are the slides available anywhere? Thanks in advance.
Generics are a nice way to eliminate some boilerplate, but I've gotten burned by this in the past when the upstream library changed formats on me and I had persisted data in that format. This has caused me to lean in the direction of writing your own instances in the case of persistent storage.
&gt; if you or someone else has 7.10 running and can confirm that all the tests pass i'd be happy to change them $ ~/opt/cabal-install-1.22.2.0/.cabal-sandbox/bin/cabal install --run-tests -w ~/opt/ghc-7.10.1/bin/ghc [...] Running 1 test suites... Test suite test: RUNNING... Test suite test: PASS Test suite logged to: dist/dist-sandbox-49828586/test/stitch-0.2.0.0-test.log 1 of 1 test suites (1 of 1 test cases) passed. [...] 
I've always used Generics directly. Using `:kind! Rep MyType` in GHCI helps get used to the shape of the derived representations. It also took me a while to get sufficiently precise understanding of the typeclass resolution rules to understand how to write the correct instance heads, but that was in large part because I was new to Haskell at the time.
Aeson already will generate ToJSON and FromJSON instances for you using GHC.Generics. http://hackage.haskell.org/package/aeson-0.8.0.2/docs/Data-Aeson.html#g:11 Scroll down and look for "Instead of manually writing".
thank you!
Reproduced; OS X 10.9.5, GHC 7.8.3.
If ubuntu has features you need you can always run the nix package manager on ubuntu. Then you could use nix packages for most everything you need to control in a declarative fashion.
OP, have you reported this as a bug to GHC yet? If not, I can file it for you if you prefer.
On a modern wired broadband connection, fairly negligible. But not everyone has high speed connections. Mobile phones in particular are often in low signal areas where 1mb could take several minutes to load.
Technically there are two ways to extend a theory. You can add operations and you can add laws about the operations you know. Given any two such extensions to a base theory you can define the pushout of these two theories to get the theory that is subject to both set of modifications and all the implications thereof. We just don't have a vocabulary for talking about that sort of thing in a language like Haskell. All the laws are pushed out to the meta-theory where you have to reason about them by hand, and they aren't able to be used to supply default definitions for the members of that are defined in superclasses. This means that a fine grained class hierarchy in Haskell pays a disproportionate cost. All users who instantiate the class hierarchy have to pay for all the levels in it, but they just get bogged in boilerplate supplying definitions that are determined entirely by laws gained elsewhere in the lattice of features. Consequently, we, in practice, tend not to supply classes that merely supply a law, but rather try to bundle the law with the operations it affects, as this enables those default definitions to be applied in some small part at least. This explains for the most part why Haskell library authors are allergic to law-only classes in my experience.
So you just have to make sure that all your functions are in the same monad. which for multiple requests, they often are all in the either monad anyway. in which isn't any worse than the original CPS version. And then you implicitly turn (or explicitly return?) the tuple into a object with left/right fields. `kleisliCompose` is cool, but you're probably not using some of the more complex combinators generically right? Like a `bracket` or something.
Sorry, I never got back to you! I'm not really looking for any solutions, just for learning and evaluation. I appreciate the pointers and I'll definitely be looking into pipes-related stuff when I've completed some more basic stuff. I'm currently revising the tutorials to be less focused on direct comparisons to Go.
Follow-up question: why the difference between `traverse` and `traverse_`? Seems strange that they require different constraints. Same with `for` and `for_`.
Template Haskel can also be used: http://hackage.haskell.org/package/aeson-0.8.0.2/docs/Data-Aeson-TH.html. I remember reading somewhere that ToJSON and FromJSON generated by Aeson.TH have more descriptive errors if parsing fails.
I've never had much luck with using QuickCheck with classes, YMMV. As a library author I don't want to incur the dependency for my users, and I find that the style of specifying generic once and for all properties still requires me to manually talk about which particular instances I'm picking anyways, otherwise defaulting picks something like () where of course the equalities hold. =) Wow, `sort xs == reverse xs` -- passed 100 tests!
&gt; One-star kinds are the simple, self-sufficient types. A strong, independent kind who don't need no... something. (Little help here, category theorists?) On-topic: Nice introduction to types and kinds.
Yes, definitely. However, good fusion rules would need to be in place to make large monadic uses perform well. Combining many Dynamics simultaneously is currently much more efficient than combining them in a binary fashion; I've provided Reflex.Dynamic.TH for this purpose. Once the basic Dynamic operators are made pure and sufficient optimizations are in place, it will be great to have Applicative and Monad instances for Dynamic.
So a large part of this is basically "`Arrow` without `arr`", with enough product/co-product machinery that arbitrary packing/unpacking is possible without an `arr`-like fallback, that it could support arrow `proc` notation. This is something that I've wished for a lot of times before; I think it could be generally useful (with `proc` support) without any of the HDL angle. 
Couldn't the ordering function be a type index of the `Set` type?
&gt; Aha! How did you pick the range (10^758, 10^800)? I… I have no idea. I thought I used `length . show $ number`, but apparently I didn't. On a second thought: I've accidentally pasted the number twice (`(2*) . length . show $ number` is `728`) and then had a copy error (read `758` instead `728`). Sorry :/. Still can't reproduce the behavior on my system, so it seems OSX related.
Simple fix: * get mingwex.dll and gcc.dll i made from [here](https://www.dropbox.com/s/mblojew2a1j5st4/gcc%26mingwex_dlls.7z?dl=0) * now when running hmatrix in ghci either start it like this ghci mingwex.dll gcc.dll &lt;other options&gt; * or if you dont want to type those dlls every time, edit your *hmatrix-0.16.1.5.conf* file in your package database (it should be somewhere in *%appdata%/cabal* directory) and add those two dll to extra-ghci-libraries field: extra-ghci-libraries: mingwex gcc after that make sure you run *ghc-pkg recache* for changes to take effect. If you want to know how to make those dlls yourself, you can see [here](https://ghc.haskell.org/trac/ghc/ticket/3242) in comment 10 how to make mingwex.dll and you can make gcc.dll following the same steps on mingw\lib\gcc\mingw32\4.5.2\libgcc.a Also note that you can skip gcc.dll and use already provided one *libgcc_s_dw2-1.dll*. However if you use that one your ghci will crash every time you exit because of some bug with dwarf exception handling and ghci not calling *FreeLibrary* before exit. 
Everyone come to New Zealand for NZHac, stay for the amazing scenery! https://plus.google.com/u/0/photos/102252268895890191681/albums/6128979501909823985
You can make the errors a bit nicer for users using DataKinds with GHC 7.8: https://github.com/bennofs/hsqml-react/blob/36cc69fb3a3099ae320557e799da8007527888c2/src/Graphics/QML/React.hs#L459-L478 This will generate error messages such as: Could not deduce (QObjectDerivingError … () "Object type tag can only appear in member fields") arising from a use of ‘Graphics.QML.React.$gdmitraverseQObject’ ...
Isn't there a way to avoid requiring special treatment for Windows and instead have it automagically work out of the box like the other supported GHC platforms (OSX, Linux, Solaris, Freebsd, ...)? Having to add special magic to the `.cabal` files of packages just to workaround Windows issues makes Windows look like a 2nd class citizen rather than a tier-1 Haskell platform...
I would argue that when it comes to dealing with foreign libraries it kinds is 2nd class citizen. This is not so much fault with ghc as much as its with windows itself - windows its not posix compatible and hacks like mingw and cygwin will always be just hacks. Also most of problems on windows i have are with ghci. Problem is for linking executables ghc is using gcc from mingw but when loading libraries in ghci it uses some internal ghci linker that doesnt mimic behavior of gcc 100% and problems like this arise. And problem is mingw itself. For example there is no shared version of libmingwex.a library and GHCI cant load .a files so i dont see other way around this.
&gt; The monad bind operators are right-associative No, they are `infixl 1 &gt;&gt;, &gt;&gt;=`. Edit: Although, do notation does associate to the right. import Data.Monoid import Control.Monad.Trans.Writer data Tree = Tip | Bin Tree Tree deriving (Show) instance Monoid Tree where mempty = undefined mappend = Bin withoutDo :: Writer Tree () withoutDo = tell Tip &gt;&gt; tell Tip &gt;&gt; tell Tip withDo :: Writer Tree () withDo = do tell Tip tell Tip tell Tip main :: IO () main = do print $ execWriter withoutDo print $ execWriter withDo First one prints `Bin (Bin Tip Tip) Tip`. Second one prints `Bin Tip (Bin Tip Tip)`.
I think you've missed the point, which is that the direction in which `-&gt;` associates is unknown if you're a beginner.
I don't think making one-liners such as `euler8` slightly more understandable achieves much. I think splitting it in bite-sized chunks, and assigning them meaningful names would make it significantly more readable. Also, such one-liners don't occur all that often outside of Project Euler-style problems that are full of list manipulation.
Fair point! I had to choose a simple example to fit the blog post format. Refactoring functions into smaller pieces with good names is always an improvement. That being said, I think functions like `takes` occur all the time. And I prefer the way the Flow-based implementation looks, for the reasons I gave in the post. 
[I tried](http://www.reddit.com/r/haskell/comments/31zagw/why_are_we_naming_types_instead_of_instances_when/cq6m9lh). It's more complicated than you think, even for this case where you should be able to make it work.
You must have been unlucky with whom you asked, given that the following two lines in "configuration.nix" are enough to make your system able to send mails from the command line: networking.defaultMailServer = {directDelivery = true; hostname = "smtp.example.net";}; 
Every monad is specialised to do something in particular. The IO monad is specialised for dealing with side effects, and it's not very good at error handling. The Maybe monad is specialised for dealing with error handling, but is not very good at side effects. What you really want is a monad that is a combination of IO and Maybe. [This is where monad transformers come into play.](https://github.com/kqr/gists/blob/master/articles/gentle-introduction-monad-transformers.md)
A reasonable thought, but to put STM into Go even if you assume you're going to change the compiler would still rock the language to the core and require massive changes to core semantics. (Assuming we're talking about a _successful_ insertion of STM, rather than some of the failed attempts to put it into other imperative languages without modifications to the language.) If STM required the addition of a bit of code into GHC or an extension or two, it's still a qualitatively different level of support required. You can pretty much say the same thing about any GHC extension, for instance. `ConstraintKinds` had to be added to GHC, but in the world of Haskell that was a feasible project. In the world of Go the entire idea is so foreign to the language that it's difficult to even imagine what it would look like until you've changed it so much it's a different language. (And I would observe this is a smidge less unfair than it looks at first glance... Go _does_ have something that vaguely smells like type classes. Very, very vaguely, sure, but it's not _quite_ as crazy an idea as putting it into, say, C.)
I don't know how annoyingly restrictive the BBC's media playing technology will prove to be if you (a) prefer to use sensible technology, or (b) connect claiming to be from outside the UK.
Coming from someone who's just beginning to seriously grok monads: Monads are about abstraction. They're a container (eh) with an intimidating rep and a mechanism for interacting with the stored data. That's it, no semicolons, burritos, etc. Maybe is just an abstraction over checking for failure. In Python, I might do this to guard against None: def square_it(x): if x is None: return None else: return x*x Soon the `if x is None:` begins popping all over my code. It's gross, it's line noise, and it's not connected to what the function's doing. Instead, it'd be great if I could just catch a failure and propagate it: # only interested in even numbers for some reason def maybe_get_none_or_int(): x = random.randint(1,10) return Just(x) if not x%2 else Nothing def square_it(x): return Just(x*x) maybe_get_none_or_int() &gt;&gt; square_it The maybe monad lets us focus on *what* our failure is, rather than *how* to handle it. The *how* part is just punted off to the caller. That's their problem. `square_it` doesn't need to know how to handle a None or a Nothing. It simply says, "Give me a value and I'll give you the square wrapped in Just." Perhaps we'd like to catch the failure and attach a message to it: def either_get_error_or_int(): x = random.randint(1,10) return Right(x) if not x%2 else Left("got odd number") def square_it(x): return Right(x*x) maybe_get_none_or_int() &gt;&gt; square_it Now we can *where and how* our computation failed. The only things that changed were a couple of names (Just -&gt; Right, Nothing -&gt; Left) (of course, type signatures and handling by the caller, but whatever). But all a monad is taking a pattern that appears in your code, recognizing it, generalizing it, and writing the abstraction. It's just the same as when you learned functors. Instead of writing a function that takes a function and a Functor, dissecting the Functor there and applying your passed function then reconstructing the Functor, you just write `fmap` once and it will handle it for you. It's an abstraction over changing a value in a container. Applicatives are an abstraction over taking a function inside of a container and a value in another container and applying the function to the value rather than doing all the nasty dissection out in the open.
I chose to order the arguments that way for one reason: higher-order functions. You can already apply a function to a bunch of values with `map f xs`. To apply a value to a bunch of functions, you have to do `map ($ x) fs`, which isn't very intention-revealing. I prefer to do `map (apply x) fs`. 
&gt; You can always compose the function with doesFileExist afterwards. It's better to catch the exception rather than check with `doesFileExists` and pray that it doesn't get deleted before the call to `hFileSize`.
That is definitely related, but an important difference is that interfaces in OOP are more like properties of the values than properties of the types.
Perhaps that's the difference. I don't see function application as English prose (read left to right), but rather a mathematical construct read from the argument. Order of application in `(f (g x)) = (f . g) x` is read from the argument position out. 
Oh, come on! It wouldn't be "very difficult to understand". You would very quickly get used to it, like any other minor syntactic thing. I've used both styles. Both are perfectly acceptable.
Yes, like I said not *quite* 1:1 but close enough that once I realized it, Haskell made a little more sense in my brain. Part of wishes that Python provided a similar mechanism (well...you *can* do it) but I also know that such a thing would be abused to all hell and code would be a mess.
So I suppose you never write `f (g x)` either? It's just as "backwards" as `f . g`. Furthermore, since Haskell is a non-strict language (part of) `f` really does happen before `g`. In fact, `g` might not happen at all.
Nothing really stood out to me, just waxing poetically about the elegance and precision of FP.
Annoyingly, I don't have a problem with `f (g x)`. The parentheses make everything readable for me. It only becomes a problem when you have a lot of parentheses, or if you use `$` (like `f $ g x`). I'm aware that `g .&gt; f` doesn't really mean that `g` happens before `f` due to Haskell's non-strictness. I think it's worth being a little sloppy with the execution model in order to better understand how data logically flows through a function. Edit: For example, `(error "..." .&gt; const True) ()` evaluates to `True` without throwing an error. [The discussion from IRC](http://ircbrowse.net/browse/haskell?id=20387338&amp;timestamp=1427769961#t1427769961) has some more examples. 
Re precision, the was some emphasis on guaranteeing non-interference between code dealing with separate things. There wasn't anything technical: no talk of "types" or "monads", just a sense of the clarity and discipline required to cope with increasing complexity. Oh, and there is the denial that Haskell is "a language made by geniuses for geniuses", at least in the latter aspect.
Did SPJ really arrive to this interview in a haskellcopter? :-)
Oh, I think everything was reasonably articulated. There's not too much that is technically interesting you can say on public radio I think :) I did quite enjoy the reference to... was it the [Devonian](http://en.wikipedia.org/wiki/Devonian) period perhaps? The implication, if I caught it correctly, was that there's a lot of diversity in programming languages at the moment and a period of consolidation is to follow.
I understand your frustration. The accepted answer in the linked stack overflow question advocates using the `mtl` type classes to avoid verbosity. You absolutely do not need to learn those type classes before working with functions like `fsize`. Actually, they would probably hurt more than they would help here. &gt; Wanting to become a true haskeller, I want to not sidestep the monads but bathe in them - absorb them into my essence. I think it is great that you are going out of your way to fully assimilate important concepts like monads. However, I think you got sidetracked here when you recognized that `IO (Maybe a)` is two monads clumped together. You are trying to learn the proper "monadic solution", but there is much you can do with `Maybe a` before worrying about its `Monad` instance. I think this usage is more idiomatic than any solution using monad transformers or type classes: fileNotFoundError :: IO () printSize :: Int -&gt; IO () printFileSize :: FilePath -&gt; IO () printFileSize = maybe fileNotFoundError printSize &lt;=&lt; fsize I won't explain this code because I don't know how advanced you are, but I'd be happy to go into further detail. Note how this code doesn't attempt to do anything smart with the fact that `IO (Maybe a)` is two monads. It doesn't construct `MaybeT IO a` or try to use the `MonadPlus` class or do any other fancy thing with the types. The `maybe` function deconstructs the `Maybe` without caring that it has a `Monad` instance. Further, `maybe` makes it clear at a glance that you're working with the `Maybe` type constructor. The only type constructor that is treated like a `Monad` here is `IO`, and only once when it is composed using `(&lt;=&lt;)`. Granted, your use case is likely more complicated than printing the file size. My point is: deconstruct the `Maybe` using pattern matching or a function like `maybe` or `fromMaybe`. Don't worry that it's a `Monad` until you have to. A great deal of Haskell code is like this example. You do not need to use every advanced concept. In this case, more advanced concepts could severely hinder readability. "True haskellers" work at the level of abstraction that is appropriate.
&gt; Edit: For example, (error "..." .&gt; const True) () evaluates to True without throwing an error. The discussion from IRC has some more examples. I'm still trying to understand the claim here :) Reading this left to right as (I think) you're advocating I see `error` first, but this will never be executed. On the other hand, reading `const True . (error "...") $ ()` from left to right I see `const` first and immediately know something about the execution process. i.e. that the next argument I see will not be executed by `const`.
Cool package. I'm always for making syntax more suggestive, and especially backwards composition doesn't strike me as the best idea a mathematician has ever had. That said, you should consider that you're always erecting an additional hurdle for collaborators when you invent your own syntax for standard stuff: Where I can read `f . g . h $ x` just fine because everyone writes their code like this, I have to check what the operators even mean in `h |&gt; g |&gt; f |&gt; x`. Of course, in time I'll become as fluent in the new syntax as in the old, but that'll take a while. And unless Flow becomes insanely popular, you'll have to teach potential collaborators the new syntax before they can do anything useful with the code.
To be fair, I remember the composition operator felt like it was backwards when I learned it in a math class. But then I had another ten years of math classes. In the present, Haskell is understandable to me because it is like math.
This. Yes this. This is the functional programming way. +1 for maybe fileNotFoundError printSize &lt;=&lt; fsize
Monads are *not* a tool for avoiding side effects. In general, different monads have very little to do with one another, apart from the fact that they're type constructors M of some sort which support some operations whose types look like return :: a -&gt; M a and (&gt;&gt;=) :: M a -&gt; (a -&gt; M b) -&gt; M b (These are technically required to satisfy some relationships with each other which allow a certain degree of refactoring to take place.) The important thing however, is that these functions are defined specially for each type constructor M. Their implementation in one instance might have essentially nothing to do with their implementation in another. There is a type constructor called IO in Haskell which happens to be a monad, and which is used to describe effects. It *happens* to be a monad, but the fact that it's a monad tells you very little. If you have a sufficiently general definition of what "effect" means, then yeah, okay, you can get away with saying that monads somehow encode "effects" of different sorts, but this is largely a confusing thing to say unless you already know how general that definition needs to be. A much less confusing way to think about Monad is that it's a type class for abstracting over some operations of a particular shape that tends to show up quite often while doing functional programming. Like most abstractions in programming, it exists to save us from writing the same code over and over. In particular, for any library which happens to define an instance of Monad, you get to use all the stuff in Control.Monad, for free. You also get dozens of other libraries and algorithms which have been written in a way that was abstracted over a choice of monad. An example of just one such thing is the function called sequence: sequence :: (Monad m) =&gt; [m a] -&gt; m [a] For different monads m, this does different things, but usually quite useful things nonetheless. For the IO monad, where it has type [IO a] -&gt; IO [a], it takes a list of IO actions, and glues them together, producing an IO action which when executed, will execute each of the actions in the given list in turn, and produce a list of their results as its result. For the list monad, where it has type [[a]] -&gt; [[a]], it takes a list of lists, and produces a list of all possible ways to select one element from each of the given lists, i.e. a Cartesian product of sorts. If you want to put it in terms of "running a bunch of things in turn", you have to interpret "running" a list as "picking an element from the list in all possible ways". Is that an "effect"? For a parsing monad, where it might have a type something like [Parser a] -&gt; Parser [a], it takes a list of parsers, and produces the parser for their concatenation, that is, the parser which applies each of those parsers in turn, each one consuming whatever was left of the input when the previous finished, and collecting a list of all the items parsed. Moreover, it's usually nondeterministic, which means that if one of the parsers fails, it will backtrack and try a different parse from a previous parser. These are just three examples of monads, and one particular simple function built on the abstraction. Not having to write the sequence function for each new monad is a savings in terms of code we have to write when developing new libraries which fit into this abstraction. If you look through Control.Monad, you'll find a bunch of others. If we define n monads, and m things which work with an arbitrary monad, then we'll have potentially saved ourselves from writing O(nm) code. Now, the thing which really confuses people coming from imperative languages I think is not the monad abstraction itself at all (though many examples of monads are really quite hardcore applications of functional programming), but the fact that IO actions themselves are values. We could use another set of combining functions for fitting together IO actions altogether, there's nothing all *that* special about the fact that it's a monad, and indeed, there are a lot of things special to how we can put together IO actions which don't have anything to say about other monads. For example, IO supports concurrency and exceptions and finalizers and STM, and lots of other fancy control mechanisms that have nothing much to do with other monads. But okay, with all that counter-ranting aside, let's address your main concern. There are a lot of ways to take care of exceptional cases in Haskell. If you're using IO, and you're finding it frustrating to deal with exceptions that you'd rather just ignore until later, use [IO's exception system](https://downloads.haskell.org/~ghc/latest/docs/html/libraries/Control-Exception.html). throwIO :: Exception e =&gt; e -&gt; IO a is a function that produces an IO action that throws the given exception when executed. You'll note that most IO actions which open a file will already throw an IOException if the file is not found. There are also ways to catch thrown exceptions of course. One of them is: try :: Exception e =&gt; IO a -&gt; IO (Either e a) This operation will turn an IO action (which may or may not throw an exception of type e), into one which has the same effects, but will give Left err, where err :: e is the exception in question if it threw an exception, and Right x, where x :: a is the ordinary result of the action otherwise. So for example, we can try to read a file like this: do res &lt;- try (readFile "foo") case res :: Either IOException String of Left e -&gt; do putStrLn "There was an error opening the file:"; print e Right x -&gt; ... do stuff with x, the contents of the file ... Of course, if we expect the error and just want to produce Nothing in that case: do res &lt;- try (readFile "foo") case res :: Either IOException String of Left e -&gt; return Nothing Right x -&gt; ... do stuff with x, the contents of the file ... ; return (Just ...) You can also defer the handling of any exceptions out as far as you like, moving more of whatever you're doing inside of the 'try'. There are also other ways to catch exceptions, like the `catch` function, and defining new exception types is easy, you just add `deriving (Show, Typeable)` to your data declaration, and write `instance Exception MyType`, and you'll get a reasonable instance from the default methods. I suspect that something along these lines is right for you, but without being able to see more of your code, it's hard to say with 100% certainty.
Well, I guess I should bring my A-game if those are the other speakers! 0_0 Looking forward to it though :)
Which is my point. I'm giving up on NixOS not Nix ;-)
Sure. I have just not read any descriptions on how to do it well purely within the IO monad. Do you have links or otherwise references?
It's like when you use pipes in a shell cat foo | g| f or in OOP `foo.g.f` Depends if, in your mind you focus on the argument and transform it or focus in the result. With type inference it's easier indeed to compose *forward*, `f. ....` has a return type of `f`. When I was kid I remember thinking `f.g` was backward and I still think it is (you apply `g`, then `f`). I'm not saying one way is better than the other but I understand that some people prefer one way, and other the other way ;-) 
Website says 30th April.
To me, it looks like Flow is more concerned with how things get done than with what things are. takes n = map (take n) . tails Reads to me: Takes-N is the same as mapping (take n) to the tails of the argument. And the Flux version: takes n = tails .&gt; map (take n) Reads like: In order to Takes-N, you get the tails of the argument and map (take n) over it. So, I think this is exactly what Functional Programming is trying to avoid. I want to express things as to what they are in terms of value and not in terms of *how* to get to these values, which is what Imperative Programming does. Except when you use monadic combinators, then you are doomed. Thinking in terms of value instead of computation may help to get an easier reading on what you say is "backwards". 
I'm honestly confused as to what your complaint is about the Aeson haddock. It seems pretty much exemplary to me.
&gt; The following are equivalent: &gt; when (name == "John") (return 24) &gt; do guard (name == "John") &gt; return 24 but then: &gt; ghci &gt; (when (False) $ return ()) :: Maybe () &gt; Just () &gt; ghci &gt; (guard (False) &gt;&gt; return ()) :: Maybe () &gt; Nothing they are very much not equivalent and `when` is defined as: when pred act = if pred then act else return () they aren't even similar for aeson's `Parser`, because `guard`ing in a parser makes it short-circuit with `empty`
It assumes that the person understands how to use all the standard functions (`&lt;|&gt;`, `&lt;$&gt;`, etc.) to manipulate Aeson parsers. In reality, people just look at the example with several lines going like `&lt;$&gt; o .: "name"`, `&lt;*&gt; o.: "age"`, see how it corresponds to the structure of their data, and blindly copy the example. Naturally, when later they want to do *anything* more complicated, they become very confused because they don't even know where to look for answers. Spend a couple of days on #haskell answering Aeson questions, and you would see for yourself. Other parsing libraries get around this by just reimplementing `&lt;|&gt;` and the like and exporting them. It's not better, but at least it leaves people less confused.
I find it easier to read. I've spent 2-3 hours in F# and already know what |&gt; and &lt;| does. The Haskell composition is more tricky to remember for me.
Thanks, fixed. ^(I have a habit of writing code and not running it, to save time. Sometimes I even get away with it.)
&gt; import qualified Data.ByteString.Lazy.Char8 as BS &gt; BS.putStrLn (encode val) This doesn't look safe to me. Since valid JSON data can contain unescaped Unicode characters, which the Char8 abomination will destroy.
The 30th is certainly when I'm planning to turn up.
Another opportunity to pick your brain a little :) What other type classes correspond to (are?) property-like structures besides `Functor`? It seems a safe bet that `Contrafunctor`, `Profunctor`, and `Bifunctor` are likewise, because these are all just about providing evidence for the co/contravariance of a type in its type arguments, which can only be true one way, if it is. (Though I wonder whether and how this might relate to /u/pigworker's [axis of generalization](http://www.reddit.com/r/haskell/comments/31gcqd/degenerate_bifunctor_functor/cq257mr).) What about `Foldable` and `Traversable`? Like `Functor`, these can be automatically derived by GHC -- implying that there's a uniquely obvious way to do so and the compiler knows it. But unlike `Functor`, there exists an [adapter newtype](http://hackage.haskell.org/package/transformers-0.4.3.0/docs/Data-Functor-Reverse.html) to reverse the order of a `Traversable` -- and `lens` even has a first-class notion of `Traversal`s. So it seems that these are *not* truly property-like. But the fact that GHC can derive them suggests that they are "more property-like" than, say, `Monoid`. But I don't know what a more precise characterization might be. Given that every `Monoid` (which is obviously not property-like) gives rise to a degenerate `Applicative` and a degenerate `Category` (IINM), this suggests that those can't be either (which matches with intuition). While intuition suggests that `Representable` *should* be property-like. What about `Strong` and `Choice`?
The real reason to prefer left to right is for autocomplete.... When writing left to right, the IDE always knows the current type, and can suggest functions to apply to that. I don't miss much from my java days, but Eclipse was awesome at suggesting what to type after "something." (the dot is just another form of left to right function, er, sorry, "method" application). It would only be better in Haskell, like built-in Hoogle.
Ha, nope. Sorry to disappoint. Perhaps some day my little library will be just as radical, though.
As soon as you want to do something slightly more advanced the Haddocks assume you are able to follow the types on your own, as is the norm with Haskell documentation. While there might be a cookbook or more advanced tutorials out there, they are not referenced in the documentation. Aesons Haddocks are definitely among the best I've seen, but they are far from perfect. Not to mention that following the types gets harder when the JSON Value type has an Array constructor which takes as an argument an Array type which is a type synonym for a Vector of Values, something which caught me in a mental loop once, cross-referencing the types until I realised I had gone in a circle. (I can't recall any specific problems I've had other than that because I put stuff like that behind me quickly, but that has been my experience a few times.)
I wrote the current haddocks "How to use this library" after figuring out the library a few years ago. The original docs didn't explain too much apart from pretty good docs on the classes. I understand your frustration as I went through similar about the original docs. I think the best result for the ecosystem overall would be to contribute your docs back upstream and either update the haddocks or the `README`, as blog posts bitrot (or just disappear) and have to be found, whereas the official docs will be found by everyone. Not just in this case, but generally speaking. Also, Bryan is very open to more helpful documentation, see e.g. his other package [mysql-simple](https://hackage.haskell.org/package/mysql-simple-0.2.2.5/docs/Database-MySQL-Simple.html). :-)
Modern software development has largely left Windows behind. Consider using [VirtualBox](https://www.virtualbox.org/), [Vagrant](https://www.vagrantup.com/), or [boot2docker](http://boot2docker.io/) and use Haskell inside Ubuntu.
&gt; Bryan is very open to more helpful documentation That's really good to hear (usually I expect people to react negatively to my proposals to add explanations aimed at beginners to their docs – I don't know why, actually). &gt; I think the best result for the ecosystem overall would be to contribute your docs back upstream and either update the haddocks or the `README` Yep, I'd want that too. (I'll wait a few days for bugs to be noticed, and then start preparing a pull request.)
Very nice. I used the wunderground API for a small weather applet in Lua for conky, but two weeks ago I accidentally deleted it :( Anyhow, the wunderground API is really nice, I especially like the idea of personal weather stations (PWS) which can give you very localized weather info. I used it together with a geo-lookup service for those days when I'm on the road. Now I'm inclined to rewrite it in Haskell.. :)
Well, in this particular case I haven't actually compiled any of it either... Apart from the `DeriveAnyClass` example, which I asked someone with GHC 7.10 to compile for me, because I don't have GHC 7.10.
Thanks. I plan to extend this over time to cover more of their API.
The IDE doesn't even have to think in terms of the value being created. It can still think in terms of the value being used and just prepend functions. In other words, if I have a function like this: myFunc :: BigStruct -&gt; Bar I can write this myFunc bs = bs And when I hit the autocomplete hotkey it looks at the last symbol I typed, bs, and shows me a list of all functions that take BigStruct as a parameter. Say I select "foo", then it changes what I have to: myFunc bs = foo bs This is still starting with the value being used to drive the autocomplete. Then, based on whatever foo returns, your next autocomplete might give you a function `mkBar :: Foo -&gt; Bar`. When you select that it rewrites your expression to: myFunc bs = mkBar $ foo bs I do agree with you that the IDE could *also* go the other direction by working from the value being created. Both should be available.
Yes! Thank you! Excellent library, will definitely try to get to know it. Mind if I import it into my Prelude?
I've been having this same issue. Thanks for the solution. It works.
This example is not even remotely compelling. Why would anyone want to include a call that never gets evaluated? This is pretty much as contrived as the if (0 &gt; 1) then "Static typing can't do this!" else 5 example from the advocates of "dynamic typing". I.e. very contrived.
&gt; Monads are not a tool for avoiding side effects. Monads are used for, among other things, statically tracking effects. This recognizes the *importance* of effects and avoids dismissing them as "side" effects. --- Claw hammers are used for, among other things, driving nails though two pieces of wood. Claw hammers **are** a tool for building houses.
`IO` is where we stick all our unchecked exceptions. Oh, sure, we do allow you to throw exceptions in pure code, but we recommend against it. It's mainly there because asynchronous exceptions might be thrown in pure code, so if you want exception-safety you have to assume pure code can throw. But, try / catch / throw / finally / bracket / etc. along with the whole extensible exceptions (using existentials!) is one part of the "akward squad" that were tackled by introducing IO (and monads in general) to Haskell.
 * Parsec implements its own `&lt;|&gt;`, which mostly everyone is using instead of the version from `Alternative`. * Parsers from Aeson work differently from actual parsers (they don't consume input, for instance). Moreover, to be honest, before writing this tutorial I had no idea about how they worked. I was doing things by example. I don't have any statistics to back it up with, but I think that `Alternative` is virtually unknown to beginners, even those who know about `Monad`, `Functor` and `Applicative`.
Slightly off topic, what's the most complex (highest order) kind on hackage? There [justification for sixth-order](http://dl.acm.org/citation.cfm?id=969615) functions, do we have justification for similarly complex kinds? I'm not sure if poly- or data-kinded things should count.
The hidden choice here is: 1. Write Haskell in a way that maps to your current understanding. 2. Modify your understanding to be more compatible with idiomatic Haskell. The author wrote this having already chosen (1). I would argue for (2) as well.
I posted [this](https://reddit.com/r/NixOS/comments/31x2mz/giving_up_on_nixos_for_now/cq6nbzo) in /r/NixOS, but if anyone is interested in just using Nix to avoid building the same things over and over with cabal, come help with cabbage! I use it just as a cabal helper, no funny business.
`apply x` only becomes intention revealing once you understand `apply`. But then, `($ x)` is also intention revealing once you understand `($)` and sections, and has the advantage that understanding it only requires a basic knowledge of Haskell syntax and the Prelude, which seems like a reasonable bar to set for a Haskeller. 
Nothing about monads in particular gives you side effect avoidance of any kind. We could write all the same libraries that we write in Haskell without the Monad type class, and each of the things which might have been an instance of Monad might present a somewhat different set of combinators that may or may not include things corresponding to return and (&gt;&gt;=). We could even have an IO type and not have it be an instance of Monad. It's really this fact that we've chosen to have a type of IO action values with a notion of execution (carrying out the described effects), separate from the process of evaluation (reducing expressions to values for the purposes of pattern matching) which helps us avoid having arbitrary effects be part of expression evaluation. *Not* the fact that the IO type we've defined happens to be a monad. Conversely, in a language with arbitrary side-effects being part of evaluation, we could have type classes and define ourselves a useful Monad type class. We could then go on to define instances of that class which did or didn't make use of side effects in their operation. Where Monad helps us is after the fact, where we can define tools which are *not only* useful for combining IO action values together in various ways, but also for combining values of many other types from many other libraries, and save ourselves the trouble of writing similar combiners in otherwise rather different settings.
Data point: I'm starting to play with lenses and arrows, and I've never heard of `Alternative`.
The source for `Data.Word` imports a header file "MachDeps.h" that defines `WORD_SIZE_IN_BITS`. I assume that it uses that to define `Word` as the corresponding machine word. It seems that I haven't clearly communicated my motivation for using words. What I want to do is to transform this: lookup :: Vector Bool -&gt; Vector Int -&gt; Vector Bool lookup = backpermute Into this: lookup' :: Vector (Int,Word) -&gt; Vector Word -&gt; Vector Word lookup' indMask v = let (ind,mask) = U.unzip indMask in U.zipWith (.&amp;.) mask $ U.backpermute v ind So, I want to compress the lookups as much as possible, using masks. A `Word` is the largest such mask that can be used in one operation.
No. If we want to be able to do function application and control flow while statically tracking effects, monads are the nearly the *smallest* such object that we need. (We might be able to do without `return` and we *might* be able to punt on associativity of `(&gt;=&gt;)`, but then we'd just have magmas in the category of endofunctors instead of monoids; it would still feel quite monadic.) --- You can definitely use monads for other things as well. I never claimed you couldn't. Claw hammers can be used for things other than building houses. That doesn't mean they aren't a tool for building houses.
Definitely the 30th. I've edited the post.
&gt; I've always thought of typeclasses as "interfaces" (as in the OOP meaning) Be careful, down that road leads the [existential type class anti-pattern](https://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/). Maybe you are less likely to encounter it coming from python, but a lot of C++ / Java programmers fall into that trap when they want to deal with semi-heterogeneous containers. Instead, realize that an interface is really just a record where all the members are of function types.
`IO` that changed the `RealWorld`.
I apparently misunderstood what you meant to do, it would indeed make you transparently use an outgoing smtp server (smtp.example.net). Do you mean that you want to deliver mail to some local machines? 
The comments against MD that seemed valid to me was the requirement to be able to push documentation to a LaTeX backend (and another one, I forget) along with the HTML backend. How is this addressed?
I just want to be able to send email to the outside world from a machine without having to configure anything. On Ubuntu, on a Mac etc ... you do `ls | mail &lt;someone@example.com&gt;` and it just works. With NixOs, it doesn't.
I'll bite. &gt; We could write all the same libraries that we write in Haskell without the Monad type class, and each of the things which might have been an instance of Monad might present a somewhat different set of combinators that may or may not include things corresponding to return and (&gt;&gt;=). This is true, we could get by without the `Monad` type class. Every instance of `Monad` could simply define its own combinators. However, all those type constructors would still be "monads". They would still share the same algebraic structure that is common to all proper instances of the `Monad` type class. The `Monad` type class is simply a way to make the monad concept an explicit first-class entity within our code. When discussing monads in general, we are often referring to the structure rather than the type class `Monad`. Sure, the type class itself does not imbue type constructors with a magical ability to model effects. However, monads in general model a very important pattern that is difficult to describe informally. It is common to use the term "effects" to describe the goal of this pattern, because in many languages the same structure cannot be tracked statically at the type level. In those languages, the pattern is typically expressed through a series of function calls with side effects. The need to write code in this style is obviated by Haskell's higher-kinded polymorphism (obviously, it is also strictly forbidden). This is what people mean when they say monads are capable of statically tracking effects. No, the type class has nothing to do with side effects or runtime operations. Rather, many patterns that are represented implicitly through side effects in an impure language can be given explicit structure in Haskell as a type constructor. Those type constructors frequently are monads.
Pandoc?
&gt; Because the CommonMark input is parsed into a native Haddock DocH, all Haddock output formats are supported.
Wouldn't it be better to use the largest word for which this would work? If Word is 32 bits, use Word64 anyway?
I really do like that the author was bold enough to present his alternative approach. Sometimes communities get too used to the status quo, and a post like this could shake things up and establish a new status quo. Idiomatic Haskell in 2015 doesn't have to be the same as idiomatic Haskell from 1997. I found the F# style of writing things to be easier to write, but harder to refactor, but that was quite a few years ago. I'm a convert to existing Haskell style at this point.
If `Word` is `Word32`, then I can't use `Word64`. So, `Word32` is the largest word that I can use...
&gt; In GHC 7.10 and older it's more general, but nothing changes if you consider this less general version. Pretty sure you meant 'newer' here?
This paper is from 2010. They weren't called lenses until [2011](http://twanvl.nl/blog/news/2011-05-19-lenses-talk). In 2007, they were really the same things, but called "[functional references](http://twanvl.nl/blog/haskell/overloading-functional-references)".
If you want to play with this, something like this should work: git clone https://github.com/jgm/haddock -b cmark cd haddock cabal sandbox init cabal sandbox add-source haddock-library cabal sandbox add-source haddock-api cabal install --only-dependencies --enable-tests -w $GHCPATH # where GHCPATH is the path to your GHC 7.10.1 binary cabal configure --enable-tests -w $GHCPATH cabal build cabal test # binary is in dist/build/haddock/haddock 
&gt; many much more important issues to be concerned about. Yeah! Like what order in which lenses should compose! /s
Back when I first tried Aeson, the thing that was never explained, or that I missed, was that you needed type annotations for it to know how to parse from JSON. It might also be nice if the user were directed to an applicatives tutorial, since otherwise the notation is very cryptic to new users. 
Kind of tangential to what you're asking, but I'd recommend decoupling the keypress from the event it triggers. E.g.: data Direction = North | South | East | West | Up | Down data PlayerAction = Move Direction Float | NoPlayerAction -- Extensible with more actions Then you can have a `Map.Map Char PlayerAction` and associated function to translate keypresses to behaviors. In addition to giving you the option of configurable keys for the user, this centralizes your keypress parsing rather than distributing it according to what type of action results.
I had more or less the same idea when I wrote [functor-monadic](https://hackage.haskell.org/package/functor-monadic), though it had more to do with my frustration of not having a pure analogue for &gt;=&gt;, &gt;&gt;= and having to write things like inputLengthOdd :: IO Int inputLengthOdd = getLine &gt;&gt;= (return . length) &gt;&gt;= putStrLn . ("You entered this many characters: " ++) instead of something more 'pipeliny': (&gt;$&gt;) :: Functor f =&gt; f a -&gt; (a -&gt; b) -&gt; f b --flipped &lt;$&gt; inputLength = getLine &gt;$&gt; length &gt;$&gt; ("You entered this many characters: " ++) &gt;&gt;= putStrLn I agree with the author's (unstated) sentiment that prefix function application was, historically, a wrong choice and that something like *h . g . f* makes no sense if you want the application order to be *f, g, then h*. In that case, *f .&gt; g .&gt; h* really would read better. The problem, though, is this: right-to-left application fits well with the *f x y z* format. We could eliminate this with the author's *apply*, but that would result in *z `apply` y `apply` x `apply` f* - and then we'd have the very same problem of reversed order that we tried to eliminate with *.&gt;*.
I've done Markdown -&gt; LaTeX and HTML with Pandoc. Super easy.
That is a beautifully small diff. I love Haskell.
Nope. It is indeed not universally safe, but for a *different* reason. `encode` emits UTF-8. This can results in crashes on Windows, where the default code page is usually something nonsensical. On other platforms where basically everyone uses UTF-8, it will work.
I get what you're saying about operator names. My day job involves writing Ruby, which uses `&lt;&lt;` (shovel), `|=` (or-equals), `&amp;` (symbol to proc), and so on. I think libraries that define their own operator *grammar*, like lens does, are extremely powerful. But they're definitely an edge case. With respect to higher-order functions, I think we may be talking past each other. I think operator sections are great. I much prefer `(+ 2)` to `(\ x -&gt; x + 2)`, for instance. However I don't think `($ x)` (or even `(x |&gt;)`) is a very understandable section. 
Ooh, I like functor-monadic. Thanks for showing me that! (I see that you defined `|&gt;` too. Nice!) Even though my function application function ended up being `apply x f`, I don't want to write `apply x (apply y f)` or whatever. I created the function so that the operators (`&lt;|` and `|&gt;`) could have a name. 
I had no idea there were other ways to represent function composition. I've only ever been exposed to g ∘ f. Wikipedia has a section about [alternative notations for function composition](https://en.wikipedia.org/wiki/Function_composition#Alternative_notations). 
&gt; However I don't think ($ x) (or even (x |&gt;)) is a very understandable section. I think ($ x) is very understandable--it's no worse than (^ 2). It might have been a little surprising when I first learned it, but after a little thought it makes perfect sense.
&gt; However I don't think ($ x) (or even (x |&gt;)) is a very understandable section. I do, since they are both binary functions. I think it's must easier to read than naming a locally defined lambda. Maybe a common name would be good, but that is really only a replacement for left-sections. For right-sections you'd either need a lambda or flip. `($ x)` is just a readable to me at `(+ 3)`; I don't ascribe special meaning to the later and use a consistent "mental desugaring" often unconsciously.
Opinionated. Newb friendly. I like it!
Ah, that's a good system. I was worried about having pre-determined keys to it. I'm still not sure how to attach it, when handling though. I mean, should I even be attaching it to the transformations? If it's just the camera, do I just change the fov for zooming in/out? With lighting, do I have them for the pos, amb, dif and spec properties? Likewise with material? I could create data types for each property that can be changed respective to each node a handler can be attached it, but I'm not sure how well that'd work. 
I'd like to see visual programming taken to a new level with Haskell seeing that its a very comfortable way to make functions with controlled side effects. I think a visual chain of these functions could be really sexy. Combine this with hololens/oculus to represent code in 3d and add in hand gestures and you've gone way beyond minority report lame gui stuff.
This should be at the top. All the other answers are overly complicated.
Thank you.
that's a concern I'm happy you shared. can you explain a little about the details of that situation? I'd like to make a note of it for myself.
Yes, you're right that I wouldn't shrink the **input** vector by switching to `Vector Bool` instead of `Vector Word`. However, I would shrink the **indices** vector for my functions. I updated the question to show an example of this sort of functions.
&gt; It's rather puzzling to me, then, why it insists on working with ByteStrings and not Text. Because everybody always works with the final encoded representation when communicating on the wire – and it's more efficient to use that directly than to first encode to `Text` and then to `ByteString`. &gt; Parsing a String as JSON is something a beginner might want to do, therefore it's included. I don't think this is a sensible approach. If you're teaching a beginner, why teach them things that make no sense, and that imply further wrong directions for them to explore? (The closest I'd get is "maybe you want to encode to a string? here's what you should do instead".) &gt; &gt; it's terribly bloated and hard to follow &gt; It does everything manually. If you had had read further, you would've seen [...] Here's how you could do this more coherently: start with the *simplest* case that covers 99% of needs, and show the easiest way to do that. With that done, proceed to "if you don't have more elaborate needs, you're done, *otherwise* here's you you address them, one by one". By starting with the complicated, all-the-things approach, you're going to lose people and get them thinking this is all really difficult (which it isn't!) long before you get to the simplest case. I would start with the `GHC.Generics` approach, which is the simplest possible case (since there's no code to write at all!), and the one I recommend whenever it's possible. My not-so-vested interest here is in getting us to a *good* aeson tutorial that makes the library more attractive and approachable :-)
&gt; I never claimed you couldn't. Some context that may be relevant is that in the last few months I've seen half-a-dozen people claim precisely that "Monads are for side effects" in various places on the Internet, by which they clearly mean that they are for nothing more and nothing less than handling side effects, and this is often put in the context "monads" being a "hack" which is only used in Haskell because it's the only way that side effects could be uncomfortably jammed into the language. Even if you are technically correct, it's still important to be clear that "Monads are not [only] a tool for avoiding side effects.", precisely because a lot more people are running around claiming that they are _only_ tools for avoiding side effects than are running around claiming that they aren't useful for managing side effects. (Seen that too, but the post I'm thinking of was just someone spouting off who clearly had no clue and no interest in obtaining one. Many of the people who think that monad is only about side effects were trying to obtain one and got bad info.)
Think of all the functions we could apply with that much $
You'll see mathematicians use alternative notations for function composition. compose g f There are mathematicians who prefer this notation. Read more here: http://en.wikipedia.org/wiki/Function_composition#Alternative_notations
Not exactly true. http://en.wikipedia.org/wiki/Function_composition#Alternative_notations
The prior advice was imported from other languages, I'm still only scratching the surface on Haskell myself. :) That said, here are a few things on my short term learning list that *seem* like the right tools for the job: - Lenses seem essential for managing the sort of nested complexity you're looking at in a graph of game actors. There are a few different implementations, and Vinyl is either an alternative or a variant (haven't quite figured that out yet). - FRP in general seems like a winner for program flow. I'm getting a good intuition for the underlying principle (basically, everything can be expressed as signal processing) but haven't dug in and learned a particular implementation yet. - Might need to grok arrows before I can really evaluate the FRP options. (Might just pick one that's not arrowized...) - Monad transformers (`mtl`) for general convenience. Mostly I just need more practice here, I understand them in theory.
It's a legitimate alternative notation for function composition. http://en.wikipedia.org/wiki/Function_composition#Alternative_notations
Some good ideas been thrown around here. I second Tekmo's point on interoperability, but I'd add this: a state of the art IDE would be another big draw. And I don't mean one that just copies existing imperative language IDEs, but one that provides a new experience based on pure functional programming. Some ideas: 1. A graphical debugger based on diagrammatic representations of graph reduction (for pure functional code) and/or free monads (for code in the `IO` and similar monads). 2. Interactive code completion based on proof assistants, but dumbed down for the masses a bit. 3. [Language levels *à la* DrRacket](http://docs.racket-lang.org/drracket/htdp-langs.html)—provide a beginner-friendly option to turn off some language features in exchange for nicer error messages. (See also [Helium](http://foswiki.cs.uu.nl/foswiki/Helium/WebHome).)
also automating massive refactoring. 
Yup. I'd want something with cleanly separated recursion and corecursion, with dependent linear types, with subtyping... Gosh, what a brave new world.
It's not half bad, especially if the precedence can be sorted. I'm all for wearing the hair shirt and expressing what stuff is rather than how to get stuff, but the reality is that we don't know what stuff really is until we get there the first time. As an example, I used flow to think about [a nearby thread](http://www.reddit.com/r/haskell/comments/32474a/the_point_of_monads/) where the author was looking to find the size of a file and wallow in monads. Here's my personal journey solving this (a task I've never attempted before). λ&gt; import Flow starting with a FilePath and guessing withFile will be needed ... λ&gt; :t "test" |&gt; withFile "test" |&gt; withFile :: IOMode -&gt; (Handle -&gt; IO r) -&gt; IO r λ&gt; :t "test" |&gt; withFile &lt;| ReadMode &lt;interactive&gt;:1:1-30: Precedence parsing error cannot mix ‘|&gt;’ [infixl 0] and ‘&lt;|’ [infixr 0] in the same infix expression :( λ&gt; :t ("test" |&gt; withFile) &lt;| ReadMode ("test" |&gt; withFile) &lt;| ReadMode :: (Handle -&gt; IO r) -&gt; IO r :). hayoo "file size" gets me ... λ&gt; :t hFileSize hFileSize :: Handle -&gt; IO Integer λ&gt; :t (("test" |&gt; withFile) &lt;| ReadMode) &lt;| hFileSize (("test" |&gt; withFile) &lt;| ReadMode) &lt;| hFileSize :: IO Integer Time to check exceptions stuff. A minute in the errors library gives me λ&gt; import Control.Error λ&gt; :t ((("test" |&gt; withFile) &lt;| ReadMode) &lt;| hFileSize) |&gt; tryIO ((("test" |&gt; withFile) &lt;| ReadMode) &lt;| hFileSize) |&gt; tryIO :: Control.Monad.IO.Class.MonadIO m =&gt; EitherT GHC.IO.Exception.IOException m Integer More brackets were needed :(. But I have me an Either. Around here I waste a fair few brain cycles trying to incorporate a print. Eventually I plum for a runEitherT first ... λ&gt; :t ((("test" |&gt; withFile) &lt;| ReadMode) &lt;| hFileSize) |&gt; tryIO |&gt; runEitherT ((("test" |&gt; withFile) &lt;| ReadMode) &lt;| hFileSize) |&gt; tryIO |&gt; runEitherT :: Control.Monad.IO.Class.MonadIO m =&gt; m (Either GHC.IO.Exception.IOException Integer) λ&gt; :t ((("test" |&gt; withFile) &lt;| ReadMode) &lt;| hFileSize) |&gt; tryIO |&gt; runEitherT |&gt; (&gt;&gt;= print) ((("test" |&gt; withFile) &lt;| ReadMode) &lt;| hFileSize) |&gt; tryIO |&gt; runEitherT |&gt; (&gt;&gt;= print) :: IO () And I have it - yah! Having worked out how to get there, I can now say what it is. λ&gt; :t (&gt;&gt;= print) . runEitherT . tryIO $ withFile "test" ReadMode hFileSize (&gt;&gt;= print) . runEitherT . tryIO $ withFile "test" ReadMode hFileSize :: IO () bewdyful. I wish my brain could think, hmmm, I'll assume some IO Effect - let's say `(&gt;&gt;= print)`, I'll probably have an EitherT to unwind which will come out of trying an IO, so let's write `(&gt;&gt;= print) . runEitherT . tryIO`. And then the inner IO will be `withFile "test" ReadMode hFileSize` of course. Since it doesn't, I spend 90% of my time in ghci going from the inside to the outside working out how to get where I want to be. And flow seems a nice tool to stop me having to jump to the start of the line with each step in the journey. +1 
&gt; I still don't really understand why people prefer composing backwards. Are preferences supposed to be understandable? People simply have different tastes, experiences and personalities, which lead them to like and dislike differently. Of course, where the title of the link fails is in declaring that Flow objectively makes Haskell code more understandable. Using Flow won't make anything _objectively_ more understandable - but by the same token, it doesn't make anything objectively _less_ understandable either. Using composition is currently more prevalent in the Haskell community, and as such there'll always be a good argument for sticking to the status quo, but I can't believe that if something like Flow (or `&amp;` which has been introduced into the Prelude) became popular, anybody would really not be able to pick up a couple more simple operators. I know that personally, if I found `.` a problem in some contexts (like the long list pipeline in the link), that I'd make use of both `|&gt;` and `.` when appropriate. There's no dichotomy here - each can be used when it's appropriate, and neither subtracts from the other. Sometimes composition might be the right choice, and sometimes pipelining.
Yes.
Really, it's not different from any other language. Learn how to do user input, learn how to do conditionals, and learn how to do recursion. With those three things, start making programs. Calculators, games, whatever you feel like.
One difference with `compose g f` is that the variables in the type are more linear. Reading the type (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c requires jumping from the second argument, to the first, then the third and fourth Whereas compose :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; a -&gt; c can be read from left to right. This is probably not a strong enough difference to be an actual benefit, but thinking about why there is a difference here might lead somewhere interesting.
Ironically, that is dealt with with OPs suggestion as well...
"bar equals" "dollar" "period" there's nothing wrong with being explicit about the symbols. People who know what the symbols mean will internally translate "dollar" to "apply" or "application" or "run function" or whatever internal understanding of it they have, while people who don't know what the symbol means will still be able to copy what you are saying and understand it later on. :)
Not saying I don't value your effort, it's just that an "abstract computation device" being the "lowest common denominator" over "all/most types of computation" tells me absolutely *nothing* if I don't already know what you are talking about. It's literally just fluff words. [You are trying to solve a problem without solution.](http://two-wrongs.com/the-what-are-monads-fallacy) I think it's much more useful to talk about "You know those 'safe access' operators in modern languages with built in null checks, like `?.`? Don't you wish they worked with exceptions too? Wouldn't it be cool if the same mechanic could be used to thread environment variables through a part of your program that needs access to them? Monads can unify those things (and many others) very elegantly, so you can write *very* reusable components."
Or programs using lots of Weak pointers where objects suddenly get GCed too early / too late. 
The browser is the most important GUI framework nowadays. With haskell running in the browser we can make GUI frameworks for virtually all platforms and devices in one fell swoop. 
JVM backend for Haskell? I guess this would boost Haskell like hell!
Here's some (imperfect) data that helps explain why, from [Stack Overflow's developer survey](http://stackoverflow.com/research/developer-survey-2015). Almost 40% of respondents target the browser. (full-stack+front-end) Occupation Full-stack web developer 32.4% Student 13.6% Back-end web developer 10.1% Mobile developer (all) 9.1% Desktop developer 8.3% Front-end web developer 6.0%
Is it (mostly) for human consumption? Then use text. Is it (mostly) for machines to make sense of? Use bytestring.
Non-answer comment: Develop incrementally, testing everything out in ghci (or winghci) as soon as you've written it, thus minimising the amount of debugging you do. It's so much nicer to edit a fresh function that's wrong and which you've just been thinking about than find a bug in a wall of text.
Well, there are really two options to actually see what is going on (as opposed to reasoning,...). Either you use printf debugging or you use some kind of debugger tool. The problem with debugger tools in most languages is that you can not easily automate the setup you need to debug a certain piece of code (in the same way you could just leave commented out print statements in the code so it tends to take a lot longer to produce the same amount of data about the application run(s). That said, as you mention, sometimes it is better to be non-invasive and then there is no way around debuggers.
I had to google the "SPA" TLA, and got to [Single Page Application](http://en.wikipedia.org/wiki/Single-page_application), as in web-based application programming with no whole-page refresh.
Read the basics, then try to do your project. You will learn the most from actually making stuff with it. Not some other people's suggestion(s), but what you actually want to create.
I eagerly await the day.
I am not in a position to make decisions on how to spend the money, but... 1. I would like a dedicated team of programmers for merging updates and debugging GHC. 2. I would like to hire dedicated team of programmers to optimize GHC, and improve the quality of kind of object code GHC produces. 3. I would like to hire a dedicated team of programmers to port GHC to a wider number of platforms, including virtual platforms like Java. I would like to see GHC compile Haskell code directly to functioning Android apps. 
&gt;JVM backend for Haskell Good god no.
Everything that can be said on this topic has pretty much been said, but also recommend that you give [EclipseFP](http://eclipsefp.github.io/) a shot. It's built on top of RCP (Eclipse's GUI toolkit) and features a visual debugger which, however good GHC is, is leagues ahead in terms of usability. You can set breakpoints, look up values on the fly... the whole shebang. The only problem is that the setup seems to be rather fidgety. It might just have been me, but when I tried it, the latency was **insane**, with the program, on occasion, hanging for **one second** after each keystroke, doing god-knows-what with cabal or ghc-mod.
I heard about `Alternative` while working through the CIS194 course. It's been the only learning source to mention them
&gt; You mean like this? No, I mean non-local control-flow decisions, where a large container is built up, but lazily and selectively consumed.
I'd rather have binaries than JVM executables, and binding the language to the JVM is both dangerous and wrong.
&gt; There's no dichotomy here But, there is disadvantage. To go from new Haskeller to reading a community-maintained package on hackage, there are now more things to commit to memory. It's won't be quite double, but it could still be disadvantageous. Idiomatic code is good because it is more likely to be understood by the majority of the community, including new members.
&gt; It does not make much sense that simply flipping the first two arguments of a function would make you think more imperatively. Does to me. `forM` vs. `mapM` for example. `mapM` clearly takes its argument order from the map operation that is as old as functional programming. `forM` takes it's argument order from imperative structures ranging (at least) from BASIC's FOR to Java's enhanced for and all the variants in between, where the container or its generator is listed before the statements. Argument order can definitely effect how humans think about functions, even if there's a clear isomorphism: `curry . (. swap) . uncurry`
It really depends on the context. If you're chaining a bunch of operations, it can often read much cleaner from left to right (and top to bottom, since that style is easier to spread across multiple lines). There's still plenty of things where the normal right-to-left composition operator makes sense, and I find myself using that style quite a lot. But other times left to right just feels more natural. tl;dr If I'm just composing a couple of functions, then `(.)` feels more natural. If I'm building a pipeline over data, then left-to-right is usually cleaner. Also factor in that English, as well as most other programming languages, tend to flow that way as well. I can read and understand left-to-right code much quicker because I have neurons hardwired to process text that way.
Thanks :) @apply: I wasn't trying to imply you were; I was just thinking aloud about what sort of issues might arise if we really implemented the idea of reversing the order thorougly. I agree with you that `g . f` is "wrong" as far as Western scripts (Latin, Cyrillic) are concerned, because in them, the information flows left-to-right, but you see what sort of trouble you get when you have multiple arguments: if you started to write `f .&gt; g`, you'd also have to write start writing the arguments prefix for consistency's sake, e.g. `x |&gt; f`. It would make sense - you feed x into f - but then you start running into the same "reversed order"-problem of having to write `z |&gt; y |&gt; x |&gt; f` (assuming a right-associative `|&gt;`) to represent `f x y z`. I really would like some postfix notation for functions, or maybe even (God help me) 2D syntax like x ↘ y → f → g z ↗ But yeah... aside from that being pipe-dream and being impossible in Haskell, I don't think anybody has an idea of how such a notation would really work. I saw some Haskell-inspired language with 2D syntax once, but writing that with a keyboard seemed like an unbelievable amount of busywork.
&gt; we should just build a new dependently typed language Here's the counterargument: Why throw away all these great libraries just to get dependent types, when we can work our way there gradually? Personally, I'm looking for a DT language with no loopholes, a decent way to do foreign C bindings, with HoTT as the underlying type theory. I just want it all, no biggie.
Idris?
My day job involves writing a lot of Javascript. More specifically, it involves *debugging* a lot of Javascript for things that the most primitive of type checkers would have caught. GHCJS - if it lives up to expectations once it's ready - will let me have my cake and eat it. I lie awake at night and dream how much better my life will be once I can use a real language for my frontend UIs.
When debugging with printf statements I prefer designing a function with a custom #define to remove the chance for errors when going back to remove all the printf lines. For example, I use #ifdef DEBUG_PRINT inside the function surrounding a printf then it's really easy to disable. I'm sure you know this, but with respect to Haskell I have a question: Is there an analogy to preprocessor definitions and conditionals? I'm a beginner to Haskell so it might be a dumb question. I'm not even sure that a preprocessor make sense with pure functional. I guess it'd involve some complicated monad setup which would risk introducing more errors into the code (and wouldn't even truly count as a preprocessor)? Right now my only Haskell debugging technique is to test functions one by one in GHCI and sometimes divide and conquer.
Chris Allen keeps a great [Getting started with Haskell](https://github.com/bitemyapp/learnhaskell) up to date. I think a good next step is to write some simple command-line tools. Maybe port over some scripts you've written in sh/bash/ruby/python to haskell. At some point you need a project, and since Haskell is a "general purpose prog lang": you are lucky! The sky is the limit.
&gt; Not everybody communicates on the wire. I'm having difficulty thinking of a reasonable use case for converting encoded JSON to `Text` or `String`. I can think of a lot of janky ones though ... 
There is the language pragma `CPP`. An integrated preprocessor akin to it does not exist.
By the author of the above comment * http://www.haskellforall.com/2012/07/purify-code-using-free-monads.html * http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html * http://www.haskellforall.com/2012/07/free-monad-transformers.html
I saw something similar at Scala Days 2014, but apparently Haskell faces some limitations in that area because it only works with distributive monads? I'm not even sure what differentiates distributive from non-distributive monads.
what's a distributive monad? also, speaking totally generally, I'd think that writing "a pure AST with an impure interpreter" would be easier in Haskell than in Scala because of... purity.
Well, there is template haskell, it could generate different expressions based on a `debugPrint :: Bool` value, IIRC. Using CPP is rarely problematic in practice, though.
if I ever make a desktop app, it would probably be like LightTable: scripted in some compile-to-JavaScript language (e.g. clojurescript, or better yet Elm or Purescript or Haskell), bundling WebKit and whatnot. every "GUI framework" I've tried has been a world of pain in installation and understandability. maybe you're a game developer or experienced in these frameworks. but I think I'd prefer to write almost anything (like yet not a graphics-heavy game) with a "js-bindings"-type thing, however experimental, rather than "cpp-bindings-plus-a-random-novel-model-of-GUI-as-different-from-the-DOM-as-emacs's-is". 
You might use the Turtle library to write shell scripts as explained [here](http://hackage.haskell.org/package/turtle-1.0.0/docs/Turtle-Tutorial.html).
Postfix notation is indeed convenient, (I haven't ever written anything in Forth, but I think RPN calculators are far easier and faster to use than regular ones) but I don't see how that 2D notation would scale in any reasonable character set. Perhaps some graph assembling a'la LabView would be better?
(Warning: This post was intended to be short but became a bit of a rant.) Hopefully they will finally fix [this bug.](https://ghc.haskell.org/trac/ghc/ticket/2189) It is a pet peeve of mine, since everytime I get an idea involving unbuffered single character input I get reminded of a bug in the major implementation of my favourite language, that has been sitting there for 7 years. Since it's a bug on Windows and properly fixing it is not that simple, it is probably there to stay for quite a while ... In the meantime I will sigh and start up a Linux VM. Fun fact: The first time I ran into this bug was in the Functional Programming course at my university. One of the exercises was writing our own getLine using getChar. That's one heck of a first impression. ;)
Here's my stab at it. wordBits :: Int wordBits = finiteBitSize (undefined :: Word) convert :: Vector Bool -&gt; Vector Word convert = unfoldr convert' where convert' vb | null vb = Nothing convert' vb = Just (convertSmall wb, t) where (wb, t) = splitAt wordBits vb convertSmall wb = maskCombine . zipWith toMask shfts wb shifts = enumFromStepN (wordSize - 1) (-1) wordSize maskCombine = foldMap (.|.) toMask n True = bit n toMask n False = zeroBits I'm not sure if enum/zip is good, might be better to use one of the mapaccum variants. The way I'm done it, a partial word at the end is shoved into the highest value bits, it should be simple to change to show them into the lowest value bits if you want that instead.
Idris is a good example, but I'd like to preserve as many ghc extensions as possible.
I want to show people this joke, but they would not get it.
&gt; For anyone else who are very hands-on and learn-by-example type, how did you start getting into and later become proficient in Haskell? I got into Haskell by using it as my [scripting language](http://www.haskellforall.com/2015/01/use-haskell-for-shell-scripting.html), although back then it was much more verbose (but still worth it).
This actually works pretty well (I've tried it before, and in fact its existence is the reason for some of `elem`, etc. being in Foldable now. If `Set` adopted this style then we could have it use the `elem` from Foldable in log time. The Set case is a nice edge-case because bundling the instance actually works. Anything that changes the type of your Set already has to be a bespoke combinator, unlike say Eq (a,b) where you want to be able to fmap out the b without mucking around with Eq instances at the time. You can also always get away with `empty :: Set a` here, and just carry the dictionary when you have one or more entries. This means that the dictionary constraint is then removed from lookup, but kept on `singleton` and `insert`. The main pragmatic complication then is that GHC gets very bad at inlining that dictionary, so performance nosedived when I tried this our in earnest. Of course, you still want uniqueness of dictionaries to pull this off. Without it, you can't even 'insert the smaller dictionary into the larger' because otherwise you start randomly changing the semantics of the result set, but rather have to adopt some sort of "insert the left set into the right set"-style strategy, which gives pessimal asymptotics.
&gt; map ($ x) fs Seriously? Thats not intention-revealing? `apply x` is just like, wtf is `apply`
This right here. I wish I had more Haskell friends.
No, not even `Nothing` can fill the `Void`!
This thread has been linked to from another place on reddit. - [/r/programmingcirclejerk] ["How do you debug programs?" - The first step is prevention. \[Haskal\]](//np.reddit.com/r/programmingcirclejerk/comments/329if3/how_do_you_debug_programs_the_first_step_is/) [](#footer)*^(If you follow any of the above links, respect the rules of reddit and don't vote.) ^\([Info](/r/TotesMessenger/wiki/) ^/ ^[Contact](/message/compose/?to=\/r\/TotesMessenger))* [](#bot) 
With Idris, you wouldn't need half of those extensions, since they're crutches to imitate dependent types in the first place.
Yet the OP could have made some research. In base-4.8 there is `(&amp;)` operator: https://hackage.haskell.org/package/base-4.8.0.0/docs/Data-Function.html#v:-38- (the `|&gt;` in Flow). Why no `|&gt;`? I remember reading (IIRC Edward Kmett's on libraries@haskell.org) mail, that this operator is already *taken* by `Data.Sequence`. In fact with `&lt;&amp;&gt;` from `lens` package you can write left-to-right code: xs &amp; permutations &amp; traverse monadicF &lt;&amp;&gt; filter pred &gt;&gt;= traverse monadicG &lt;&amp;&gt; filter pred2 And as `lens` is in a sense *un-idiomatic Haskell* already, that piece of code wasn't too different :)
Posix is a standard for Unix-like operating systems. There's no reason why Windows *should* follow it. It's a technicality, but Windows even predates Posix - first version of Windows was in 1985 (with MS-DOS even older, of course, and the CP/M it originally mimicked even older still) about the time the Posix project first began, with the first Posix releases around 1988. At that time, Unix itself was fragmented, and even the earliest Posix standards (several independent standards for bits and pieces to be adopted piecemeal at first) did little to change that. Sure it would be nice if all development platforms were compatible, but you didn't really get that even *within* Unix until Microsoft was already dominant in the personal computer O/S market. Even before that, Unix and Microsoft were never the only options - DOS did copy from Unix, but there was no strong reason at the time why DOS or Windows should have chosen Unix compatibility over (most plausibly) IBMs mainframe OSs, or VMS or whatever. I understand why the Haskell devs target Posix and rely on Posix compatibility layers on Windows, I wouldn't do any different, but the resulting problems aren't the fault of Windows. 
Even if it was significantly harder to do it in Haskell (which I am not convinced of), why not just write the timing sensitive parts in C ? This would be much nicer than having the whole crypto stack in this language :/
That is cool!
I'd spend it all on trying to lessen the cabal hell problem as far as it would go.
Sorry if that seemed like a dig - we all do it! ('That film was fantastic.') I just had to mention it in the context of talking about preferences.
Yeah, but we're talking about lottery levels of cash. With that much money, surely you could hire a bunch of type theorists to write an exhaustive standard basis library... ;-)
Here's how I do it: {-# LANGUAGE CPP #-} import Data.Bits import Data.Array.Repa import qualified Data.Vector.Unboxed import Data.Word import Prelude hiding (length) #include "MachDeps.h" bitsWord = WORD_SIZE_IN_BITS :: Int -- Group-up the bits into a 2 dimensional matrix: _ x 64. groupUp arr = flip fromUnboxed arr (Z :. div (length arr) bitsWord :. bitsWord) -- Map each element to `bit` of its inner index -- Fold the inner dimension with Bitwise-OR toWords arr = foldS (.|.) (0 :: Word) $ traverse arr id (\f (Z :. i :. j) -&gt; if f(Z :. i :. j) then bit j else 0) This works, assuming the length of the bit-array is a multiple of `wordSize`. Example: import Control.Monad.ST (ST,runST) import System.Random.MWC (Gen,initialize,uniformVector) nWords = 4 :: Int n = nWords * bitsWord sample :: Vector Bool sample = runST $ (flip uniformVector n) =&lt;&lt; initialize (singleton 42) sample' :: Vector Word sample' = toUnboxed . toWords . groupUp $ sample This is all great, but the real problem is that it's completely unnecessary. The only difference between `Vector Bool` and `Vector Word` is in the indexing function...
DrRacket is awesome. Having "friendly" versions of error messages would also probably push more abstraction into Prelude—`map` on functors wouldn't be a problem anymore because the error messages wouldn't be scary! (IMHO it's not really that much of a problem right now, but...)
It's super easy to avoid impurity in Scala, F#, Ocaml, etc. - you just have to completely avoid certain language constructs, and use pure wrappers around impure libraries. Source: worked on a commercial F# code base with minimal shared mutable state. E: programming without static types is like trying to juggle; programming without enforced purity is like trying not to eat your own shoes.
I know about `Data.Sequence.|&gt;`. In two years of working on Haskell, I have never encountered it. Plus, Flow only claims to avoid collision with the base package. 
apply directly to the forehead
Just did that with a week's worth of code. Got it replaced and working (and better structured) in a day.
I have no perspective, it's the only language I've ever studied. I will say this: 100% of the people I've talked to who know Haskell well all love it. I cannot say the same for any other language. It certainly seems to vie for being the best, fastest, most effective, most satisfying language etc. and the community is great. The only downsides seems to be that the community is smaller than some others (even though it isn't tiny) and some existing resources that would otherwise be useful aren't in Haskell (Drupal has most of the tools we need already, if we wanted to work with PHP; so the trade-off for Haskell is having to reinvent some wheels if we want it all to work together in Haskell…). So, again, I'm *totally* biased, but I *think* Haskell might really be the best, and I want to believe that it's a great choice for our project. We can't afford to build the project twice though just to run the experiment and see whether it's the best overall situation…
Haha. I'm the OP :) Meanwhile, I have found a solution. It's unsafe and kind of a hack, but I can use **unsafeCast** from `Vector.Storable` to cast the `Ptr`. It seems to work OK for byte-vectors -- assuming the number of bytes is divisible by 2 or 4 for Word32 or Word64, respectively. However, `unsafeCast` fails miserably on vectors of `Bool`. So, the work-around for `Bool`s is to convert them to a [Bitstream](https://hackage.haskell.org/package/bitstream-0.2.0.4/docs/Data-Bitstream.html), and then to a vector of bytes with `Data.Bitstream.toPackets`. sample2 :: S.Vector Word8 sample2 = runST $ (flip uniformVector nBytes) =&lt;&lt; initialize (U.singleton 42) sample2' :: S.Vector Word sample2' = S.unsafeCast sample2 I double-checked the `unsafeCast` against my manual method of shifting bytes around. They seem to match. -- sample2'' == sample2' sample2'' :: S.Vector Word sample2'' = U.convert. toUnboxed . bytesToWords . groupBytes . S.convert $ sample2 bytesToWords arr = foldS (.|.) (0 :: Word) $ traverse arr id (\f (Z :. i :. j) -&gt; swap (j, f (Z :. i :. j))) groupBytes arr = flip fromUnboxed arr (Z :. div (U.length arr) bytesWord :. bytesWord) bytesWord = bitsWord `div` 8 swap :: (Int,Word8) -&gt; Word swap = fromIntegral.snd.swap3.swap2.swap1 -- These took a little while to get right :) swap1 (i,b') = let b = fromIntegral b' in if 0 == div (i+1) 2 `rem` 2 then (i,b) else (i, byteSwap16 b) swap2 (i,b') = let b = fromIntegral b' in if 0 == div (i+2) 4 `rem` 2 then (i,b) else (i, byteSwap32 b) swap3 (i,b') = let b = fromIntegral b' in if 0 == div (i+4) 8 `rem` 2 then (i,b) else (i, byteSwap64 b) Imports: import Data.Bits import Data.Array.Repa import qualified Data.Vector.Storable as S import qualified Data.Vector.Unboxed as U import qualified Data.Vector.Storable as S import Data.Word
I found it difficult to figure out what would be a good project cause I normally try out some gui based project first. I went the data statistics route and so far it has held my attention. I think this was the hardest language for me to figure out a pet project.
[Last time](http://www.reddit.com/r/haskell/comments/2x70o8/x_0_true/cozlsil), we established that division (and even just pred) was impossible in the simply-typed lambda calculus, and required either System F or the untyped lambda calculus. If you want to use the untyped-lambda calculus route, you can use the idea that "dynamically-typed programming languages are unityped", by defining a single static type containing one constructor for each of your runtime types. In the case of the untyped lambda calculus, there is only functions of one argument, but in order to output a value at the end, I see that you also use integers. data Untyped = Int Int | Fun (Untyped -&gt; Untyped) Now, we can define different operators on `Untyped`, corresponding to the different runtime operations which each runtime type is supposed to support. Of course, those operators will fail if they are called on a constructor which does not correspond to the expected runtime type. In the case of the untyped lambda calculus, there is only one such operation, function application. infixl 0 $$ ($$) :: Untyped -&gt; Untyped -&gt; Untyped Int _ $$ _ = error "Int used as a function" Fun f $$ x = f x The rest of your program can be easily converted to use `Untyped` by adding `Fun` in front of all your lambdas and adding `$$` at all your function applications. -- Conversion between int and church numerals intToChurch :: Int -&gt; Untyped intToChurch n = Fun (\ f -&gt; Fun (\ a -&gt; if n == 0 then a else (f $$ (intToChurch (n - 1) $$ f $$ a)))) churchToInt :: Untyped -&gt; Int churchToInt n = let Int r = n $$ Fun (\ (Int x) -&gt; Int (x + 1)) $$ (Int 0) in r -- Church number division div :: Untyped div = Fun (\ v0 -&gt; Fun (\ v1 -&gt; Fun (\ v2 -&gt; Fun (\ v3 -&gt; ((v1 $$ Fun (\ v4 -&gt; (v4 $$ v3))) $$ ((v0 $$ (((v1 $$ Fun (\ v4 -&gt; Fun (\ v5 -&gt; Fun (\ v6 -&gt; (v4 $$ Fun (\ v7 -&gt; ((v5 $$ v7) $$ v6))))))) $$ Fun (\ v4 -&gt; v4)) $$ Fun (\ v4 -&gt; Fun (\ v5 -&gt; (v5 $$ (v2 $$ v4)))))) $$ (((v1 $$ Fun (\ v4 -&gt; Fun (\ v5 -&gt; v4))) $$ Fun (\ v4 -&gt; v4)) $$ Fun (\ v4 -&gt; v4)))))))) -- Church number addition add :: Untyped add = Fun (\ v0 -&gt; Fun (\ v1 -&gt; ((v1 $$ Fun (\ v2 -&gt; Fun (\ v3 -&gt; Fun (\ v4 -&gt; (v3 $$ ((v2 $$ v3) $$ v4)))))) $$ v0))) main = do print (churchToInt (add $$ intToChurch 2 $$ intToChurch 3)) -- outputs 5 print (churchToInt (div $$ intToChurch 9 $$ intToChurch 3)) -- outputs 3 In you want to use the System F route instead, you'll need to give explicit type signatures to your helper functions to indicate how polymorphic they are. I have a really hard time understanding your definition so I can't show you what those type signatures would be for `div`, but if you had something like -- Occurs check: cannot construct the infinite type: t1 ~ t1 -&gt; a -&gt; t foo = \myId -&gt; (myId myId) 4 main = print (foo id) -- should output 4 You could tell GHC that `myId` is intended to be polymorphic by adding an explicit type signature, like this. {-# LANGUAGE RankNTypes, ScopedTypeVariables #-} foo = \(myId :: forall a. a -&gt; a) -&gt; (myId myId) 4 main = print (foo id) -- outputs 4 
&gt; for example representing contexts as right-nested pairs means that variable access is linear in the number of variables, which is just terrible In my implementation, all of the CCC interpretation, including this variable access work, happens at compile time (circuit *construction*), so there is no run-time overhead. And while the CCC terms can be simplified, I've seen no run-time benefit when those simplifications are turned on. Instead, I get a more circuitous (😉) construction of exactly the same circuits. I do perform additional optimizations that improve execution, including common sub-expression elimination (via hash-consing) and constant propagation.
Sounds interesting to me as well. Quite a different intent from ours, of course, since we were aiming at compiling Haskell itself with semantics intact.
Perhaps, though my project is about a generating a specialized/optimized machine *per program*.
You can find some help from [my blog](http://conal.net/blog/) now and [the video](https://vimeo.com/galois) when it's ready.
Source code reflection is the first step of the GHC plugin. From there, I generate the CCC term, which is specialized to the circuit CCC. The CCC part could be bypassed, but I wanted to see how it could go, and I like that it gives a sound framework for other non-standard interpretations of Haskell programs. I have several such interpretations in mind.
I don't know where it's going now, either. I'm bummed about Tabula going under, and I really hope that there's a way to continue the vision.
I agree that it's a detour, and one that I like a lot. As I mention in another comment here, &gt; The CCC part could be bypassed, but I wanted to see how it could go, and I like that it gives a sound framework for other non-standard interpretations of Haskell programs. I have several such interpretations in mind.
I see that the link above is directly to the slides PDF. You can get a little more context from [the talk's GitHub page](https://github.com/conal/talk-2015-haskell-to-hardware).
Well, there are other extensions that are not for faking dependent types :)
I've only seen you talk about combinational circuits. What is your plan for Haskell programs that need runtime recursion? Combinational is easy, it's adding state that is difficult. 
As a complete amateur, it seems to me like the reason pred is not implementable in the STLC is that it lacks coproducts. Why is everyone here talking about System F and higher rank types?
I have a fairly elegant way to specify and encode "stateful" examples, in the form of Mealy machines to represent synchronous stream transformers. Check out the later examples, starting on slide 63. Compilation is quite effective. I'll describe it in a blog post or two sometime. Unfortunately, I overreached in my talk, and ended up rushing through this material, so the video won't help much.
I suppose you can type-check it in System F, which means that it would be accepted by Haskell if you used proper type annotations on the polymorphic arguments of this expression.
It's already hard to mitigate in assembler, which today is actually pretty far from the metal. I don't think that's a reason to stop all research on crypto implementation in higher-level languages. Like anything else in crypto, you need to be aware of the cost of any given attack versus the cost of mitigating it. There are plenty of crypto applications where timing attacks just aren't important. And at the opposite end of the scale, some applications will anyway require specialized hardware implementations to mitigate timing attacks. Overall, I don't think the timing issue has any significant impact on the interest of Haskell as a crypto platform.
Hire back Simon Marlow.
Like many other datatypes, coproducts can be implemented in STLC, but this representation has the big disadvantage that their eliminators can only be used for a single return type. left x = \ccL ccR -&gt; ccL x right y = \ccL ccR -&gt; ccR y That is, given `x :: A`, we can represent the Haskell value `Left x :: Either A B` as its eliminator, a function which receives continuations corresponding to the branches `Left x -&gt; ...` and `Right y -&gt; ...` and runs the first one. The type of this eliminator is: left x :: (A -&gt; R) -&gt; (B -&gt; R) -&gt; R For a particular type `R`. This is the important part. In a language with polymorphism such as System F, you could make a single eliminator which supports any `r`: left x :: forall r. (A -&gt; r) -&gt; (B -&gt; r) -&gt; r and then we could use `left x` in much the same way we would be using `Left x` in Haskell. But in the simply-typed lambda calculus, at the moment where you define `left`, you have to pick a particular `R`. You can make this less restrictive by defining a bunch of different versions of `left` or by inlining its definition each time you use it, but that means you're still limited to eliminating your `left x` at only one place, or at most at many places which happen to need it at the exact same `R`. When trying to use this encoding of coproducts to implement `pred`, we can try to use `left x` to represent whether we have already skipped a `succ` or something, but to do that we'll have to eliminate the `left x` at two different places: once inside the loop (to determine whether to skip the current `succ` or not) and once outside the loop (to determine whether this is the `pred zero` case or one of the normal `pred (succ x)` cases). Since those two locations are producing values of different types (the `Either () Nat` to be used in the next loop iteration or the `Nat` to be returned by `pred`), we can't use the same `left x` in both places.
Would this help with putting latex into haddock? E.g. I'd like to be able to relate the parameters in my Kalman filter implementation to a mathematical description. I'm told my only option is to use an image.
That sounds like a reasonable plan. But I must agree with those who object to the insistence on sticking with `String`. There are different levels of "newbie". Anyone who has gotten to the point where they are ready to try aeson - which might be pretty fast for some people, or after quite a while for others - is ready to learn about the three main string types in Haskell. Sorry about the `import` line, but that's how you write aeson code. I'm also sorry about the continued existence of code that uses `String` and utf8string with aeson, but that's no excuse for endorsing it in a tutorial, unless the explicit purpose of the tutorial is to teach you how to maintain crufty legacy code.
`|&gt;` can be found by normal hoogle: https://www.haskell.org/hoogle/?hoogle=%7C%3E But true, seems there is `(&amp;)` in the table in the post, so OP did some research. But it confuses me even more, why to invent new names to already existing things?
You always see the [bindings of the current context](http://i.imgur.com/zA81BXL.png). I haven't really used other debuggers (as gdb) that much, and as such I really don't find this behaviour surprising or annoying.
That would be rad! A few points to be aware of, David: * org-mode markup syntax parsing has not be tackled yet. What we've got is robust parsing of the org-mode document structure. * I did about eight hours of code-cleanup and refactoring last night in preparation for working on a Free-monadic AST for org-mode markup next weekend. I should probably update the README to also highlight what's supported and not supported, currently, by the library.
And better Windows support. While avoiding success is part of the Haskell mantra, bad support is not the way one is supposed to do it.
I'm expecting the *current context* to contains everything which is in scope, not only the variables actually used by the current expression. So in your example, at the second break, the expression is `qsort left ++ [a] ++ qsort right` and the binding shows `a`, `left` and `right`but not `as` which is technically in scope but not for the debugger. It's really annoying to not be able to see (or force) the actual parameters of a function because they are *out of scope*. Being able to know the parameters of a call can be useful to, for example, test the function in isolation or add a new test case ? Or simply, to know the context of the function when you don't know how you ended in this breakpoint. Without looking at the code of main, could you try using only the debugger to figure what was the initial list to sort (and see how painfull it is) ? 
&gt; It assumes that the person understands how to use all the standard functions (&lt;|&gt;, &lt;$&gt;, etc.) to manipulate Aeson parsers. Wow dang, it assumes that people know what functions in the standard library do? Good golly, we must be expecting too much!!!
I think the "ML of DTs" is still a dream. Existing work like Epigram, Agda and Idris have certainly laid the foundation, but we're still experimenting. “ML” happens when you understand the problem (and solution) domain well enough to come up with something *definitive* that can survive unchanged for decades.
Anecdote: [FLTK](http://en.wikipedia.org/wiki/FLTK) history is interesting. Designed by Bill Spitzak as a basis for Nuke (compositing tool) at Digital Domain. FL being related to IRIS GL Forms written by Overmars, which is the inspiration for XForms (X11 port of Forms), the original toolkit used by XFCE (XForms Common Env.).
Choose your adventure: * You picked "well-specified": Isabelle/HOL * You picked "theoretically grounded": Agda * You picked "practically useful": Idris
&gt; Repetition isn't good, but without any repetition we get docs that are nice for experts (because they are concise and to the point), but completely unsuitable for beginners. And it'd be fine too if Aeson wasn't the only JSON library that people use now; but it is, and refusing to provide any help to beginners in these circumstances is a bad thing. No, I'm not, because I want my documentation to be documentation, not tutorials. Don't know what something does? Grow up and look it up in its own documentation.
Good shout, I'll update that shortly. Will wait a while in case this new publicity brings out any other corrections. The screencast was a big help for me so thanks for it. The article you just read was on my first Yesod site and your video was invaluable in putting it together. EDIT: that's now fixed, thanks.
I'd guess the devil is in the details. It is entirely non-trivial to guarantee the timing aspects of this, for *all* possible input A's. You have to switch to a memory model where you control alignment, indirections, cache misses, GC, and more. You'd have to throw away the RTS for this code and anything it calls. 
&gt; ...monads have very little to do with one another, apart from the fact that they're type constructors M of some sort which support some operations whose types look like [this]. While this is all true, let's not hide the fact that those type signatures are not random or opaque; there is some discernible meaning in them. The "bind" operation (`&gt;&gt;=`) can be read as: this type admits an operation that can naturally described as a sequence of steps. That is something that all monads have in common, by definition. Disclaimer: My original "aha!" moment for monads came after reading together the two classic wiki posts of /u/cgibbard, [Monads as containers](https://wiki.haskell.org/Monads_as_containers) and [Monads as computation](https://wiki.haskell.org/Monads_as_computation). So I owe a great debt of gratitude to Cale, even though he may since have somewhat disavowed those wiki posts as a good way to understand monads. :)
well, figuring out the perfect way to do GUI based stuff in Haskell will certainly entertain you for a few years at least :)
Thank you for writing these bindings! I appreciate how the docs for subtypes show the specialized signature of each function. To see what I mean, check out the bottom of [this haddock page](https://hackage.haskell.org/package/fltkhs-0.1.0.2/docs/Graphics-UI-FLTK-LowLevel-TextEditor.html).
Interesting. Mind expanding your README to explain what this actually does? I am neither familiar with what features Agda provides, nor able to track exactly what this provides simply by watching the GIF. I see it does something with holes, but moves too quickly for me to truly understand, given that I have to pay attention to both the status line and the line that's actually being edited.
I don't think this patch helps directly but we should definitely add embedded LaTeX support to Haddock as a core feature.
you don't like it because you're forced to use it. if you just starting out learning, and following a tutorial, it sounds like it would be helpful
what are the other ways of "writing an application". eg I've tried writing emacs modes that are as complex/interactive as se browser app, and it's a huge pain.
which decades i.e. do people use ML though? just curious, no opinion on the matter. I could see the argument for lisp, which survives in Elisp and clojure, but the latter is more of a "child" itself.
There's a `Pointed` [typeclass](https://hackage.haskell.org/package/pointed-4.2/docs/Data-Pointed.html).
You mean inject large worst case delays? That could work, but it's going to be slow!
So my question is why Pointed is not a superclass of Applicative ?
Well, [this branch](https://github.com/alanz/HaRe/tree/wip-7.10-parsed) was updated 9 days ago and when I last heard a couple of months ago it was "getting there". 
Things like ‘no laws’ have been mentioned, there is a discussion [here](https://mail.haskell.org/pipermail/libraries/2013-August/020687.html) where Edward Kmett says: &gt; However, to move it into its own class would require literally everyone who currently has an Applicative instance to clutter their code with CPPs. &gt; &gt; Even as the author of the Pointed class, I personally find that the benefit of the change doesn't warrant the impact of the change.
Hmm, I don't actually know whether the STLC encoding of coproducts counts as a "real" coproduct or not. Let's figure it out! So what is the definition of a coproduct? In category theory, given two objects `A` and `B`, their coproduct is a triple (`A+B`, `inl :: A -&gt; A+B`, `inr :: B -&gt; A+B`) satisfying the following universal property: if there exists a pair of morphisms (`fl :: A -&gt; X`, `fr :: B -&gt; X`), then there also exists a morphism `f :: A+B -&gt; X` such that `f . inl = fl` and `f . inr = fr`. In the STLC, objects are types and morphisms are functions. Okay, so let's fix `A` and `B`. Which STLC type corresponds to `A+B`? We said it was the eliminator, so `(A -&gt; R1) -&gt; (B -&gt; R1) -&gt; R1`, for a particular `R1`. I can already see that this is not going to end well: since the `R1` is arbitrary, this means we have several candidates for the coproduct. In particular, consider `X = (A -&gt; R2) -&gt; (B -&gt; R2) -&gt; R2)`, another candidate. It clearly has two injection functions as well, namely: fl :: A -&gt; X fl x = \ccL ccR -&gt; ccL x fr :: B -&gt; X fr y = \ccL ccR -&gt; ccR y The overall property of coproducts says that if our first `A+B`, the one with `R1`, was a true coproduct, then there would exist a function f :: ((A -&gt; R1) -&gt; (B -&gt; R1) -&gt; R1) -&gt; ((A -&gt; R2) -&gt; (B -&gt; R2) -&gt; R2) such that `fr` and `fl` could be decomposed into calls to `inl`, to `inr`, and to that `f`. But that's absurd! Or rather, it's absurd for some combinations of `A` and `B`. For `A = B = R2`, for example, I could use `f` to construct nonsense functions such as `coerce `: f :: ((R2 -&gt; R1) -&gt; (R2 -&gt; R1) -&gt; R1) -&gt; ((R2 -&gt; R2) -&gt; (R2 -&gt; R2) -&gt; R2) coerce :: R1 -&gt; R2 coerce r1 = f (\_ _ -&gt; r1) id id So you were right: the STLC does not have coproducts, because its version of `Either` is "not universal", in both the sense that it doesn't support arbitrary continuation types and in the sense that it does not satisfy the universal property of coproducts.
This may work: http://eclipsefp.github.io Emacs seems to have the best Haskell tooling (and a Win version), but it's quite a learning-curve by itself. Haskell Platform might not be "the way to go" anymore. See haskell.org -&gt; Download for instructions on installing MinGHC. That said, I must admit I have zero experience on Win platform. Maybe running Ubuntu in a VM may also be a solution for the time being: it is probably the most well tested platform together with OSX. Good luck!
For IntelliJ, HaskForce is my current favorite plugin. An aside: When switching between plugins, you should always delete the generated project files. Strangely after having installed the platform on Windows a few times I cannot reproduce your issues. At least `cabal init` should work flawlessly :/ I'm sorry for your experience and I too think that this is an area where improvement is badly needed.
I've had similar problems. [Here is my post where I got help](http://www.reddit.com/r/haskell/comments/2q7in7/windows_ide_that_actually_works/) from this helpful group of users. In particular, I got IntelliJ working with help from [this post](http://www.reddit.com/r/haskell/comments/2q7in7/windows_ide_that_actually_works/cn3p61c) and this is what I used during my stint with the language. Hopefully you will be able to setup one of the various IDEs and get started. They do NOT "just work" like most software, there are hidden hurdles everywhere. Ask for specific help when you need it. As for learning the language itself, it is not a vacation. I started reading "Real World Haskell" and all went fine for a bit over a hundred pages, then I started to get lost. I switched to the Haskell Wikibook and something similar happened, I started getting lost after a hundred pages or whatever (I don't actually remember). I read a good part of the Haskell 2010 report to clarify a few concepts, then read all of "Learn You a Haskell for Great Good" while testing out the examples, then went back to Real World Haskell with a better understanding. I was able to follow everything for a few more hundred pages and skimmed the rest as I was in a hurry to start using the language for real. And that's when it all went downhill. It's not that the language itself is complex, it's even quite small compared to C++, but it's just so damned alien to my way of thinking. I tried and failed to do various simple things that would be trivial for me in any procedural language. I would require a Haskell tutor or colleague with the patience to answer my simplistic questions every five minutes until I get the hang of it. I don't have that so I decided to put it aside and maybe give it another try in some months. I also wanted to look at Rust, so I switched my learning focus in that direction instead. Good luck!
My first thought was that whether the law is trivial to prove or not is orthogonal to whether this is a good law or not, but it turns out there is a very good reason for rejecting laws which follow trivially from parametricity. A good law would be one which forces the implementation to be correct. Using the `pure ` implementation from Applicative as a baseline, the following implementation would be correct: instance Pointed Maybe where pure = Just But the following would not: instance Pointed Maybe where pure _ = Nothing But a law which follows from parametricity would be trivially satisfied by both implementations, so it would not be of any help in disqualifying incorrect implementations. I think it's going to be difficult to find a law which simultaneously rejects the `const Nothing` implementation above yet accepts the following very similar implementation. instance Monoid a =&gt; Pointed (Const a) where pure _ = Const mempty Speaking of which, given Pointed, a version of Applicative with only `pure`, we would also need a version of Monoid with only `mempty`, wouldn't we? I'd call it "Inhabited". 
&gt;I'd call it "Inhabited". Too late, it's already called Default ;)
To complicate things without any gain, you mean?
Maybe ;-) My question is "how do we know there is not any gain?" 
I'm a bit confused. Do you mean that `const Nothing` doesn't break `fmap f (pure x) == pure (f x)` and therefore this law is trivial or the opposite ?
Maybe by "binding" Xenasis meant these differences Frege has from Haskell for easier Java interop: https://github.com/Frege/frege/wiki/Differences-between-Frege-and-Haskell
Off topic but someone may be interested in the paper [*A Representation Theorem for Second-Order Functionals*](http://arxiv.org/pdf/1402.1699.pdf) which discusses things like `Pointed f =&gt; (a -&gt; f b) -&gt; f c` being isomorphic to `Either (a, b -&gt; c) c`.
That's what I'm using now (it's really nice, but still not local and browser dependent and Chrome isn't at all the kind of performance usable for code-editing). Still, the site and the IDE are awesome so far. +1
AFAIK, the IntelliJ project files are generated by the respective plugin, so yes.
&gt; no matter how hard it is to set up at first, it'll eventually work on VS. That'd be true for most IDEs That is true for any development environment. The tricky bit is the 'no matter how hard it is to set up at first' bit, which can vary a lot depending on what you are trying to do in which environment. And I disagree that IDEs are always easier to setup.
Maybe I misunderstood your point, but I don't see the connection between the example and the problem with type classes specified by laws following freely from parametricity. To my understanding, the laws one wants to hold for all type class instances *are* the correctness specification for the type class instances. So if the only laws that one wants for an instance to hold are free by parametricity, then the type system ensures that only correct instances are possible. In this sense both of the mentioned `Pointed` instances seem correct to me. The laws are just not sufficient for an `Applicative` instance, because `Pointed` as specified by these rules is a real super class of `Applicative` and hence is more abstract. I think I'm probably missing something, since I've heard it multiple times now that type classes should not be specified purely by laws which follow freely from parametricity. Maybe these type classes are too general, and hence it makes almost no sense to write functions using them? But I still fail to see exactly why free-from-parametricity implies useless law.
I think [this](http://howistart.org/posts/haskell/1) is a pretty coherent guide to building haskell projects IMO. also, some links to DE setup can be found at the bottom of the page.
This makes kind of sense on its own, but I can't see how this is specific to type classes specified purely by laws following freely from parametricity.
EclipseFP isn't maintained, don't recommend it unless you've tried it recently and it's suddenly started working again. FPComplete web IDE and HaskForce for IntelliJ are the best current options. The future stuff to come from FPComplete's recently open-sourced `ide-backend` is probably the way forward.
*I noticed that this is more of a rant, so be warned of subjectivity.* I hate the whole Eclipse experience. It's just so darn slow and feels like it was invented in 1985. If I had the choice between Eclipse as an IDE or a text editor with make files, I'd choose the latter. IntelliJ is in totally different league compared to Eclipse.
Oh no, I haven't come back yet, I just decided to stop trying a couple of weeks ago and to look at Rust, another intriguing product that has just come out with a 1.0.0 Beta of the language. It's a hell of a lot more understandable. Yet interestingly, it includes plenty of recognizable functional features, but in an intelligible presentation. Not the same level of terseness and no function composition like Haskell, but high level functions, lambdas, maps, filters, folds... And it's main selling feature, resource management safety without garbage collection is rather revolutionary. It's too young to have the library support needed for serious work though. So it's back to Java and C++ for the time being. I will look at both Haskell and Rust again later, much later.
Looks to me like premature point-freedom is confusing the issue here. Spell it out completely and look again: instance Monoid a =&gt; Pointed (Const a) where pure _ x = Const mempty x I'd say this looks more like instance Pointed Maybe where pure x = Just x than like the `Nothing` definition, where `x` falls in a hole. Sure, `Const` is going to discard `x` anyway, but that's not relevant here. `Pointed` has one parameter, which corresponds to `b` in `Const`, not `a`.
Yes, this is basically what I meant. If I remember correctly, the list type `[]` is such a case. It has two `Applicative` instances: one for non-determinism; and one for "vector operations". But only the one for non-determinism is suitable for a `Monad` instance. So this problem seems not to be specific to type classes specified by free-by-parametricity laws. It is rather a general phenomenon of super-classes. Also it is usually circumvented by using `newtype`s for the alternatives, i.e. in the above example [ZipList](http://en.wikibooks.org/wiki/Haskell/Applicative_Functors#ZipLists).
Thanks! I have not posted much about it no. Most of what I did has been already tackled by other people: I tried Julia set fractals, sorting pixels of an image in various ways, playing with Epitrochoids and Hypotrochoids, whose equations are well explained in the [wikipedia article about the Spirograph](https://en.wikipedia.org/wiki/Spirograph)... And other things. If you feel like digging in the code, you can take a look at my [github repositories](https://github.com/Vetii). There is not much to see from the functional programming perspective, but I am working on it :)
Have brought the site down for a brief moment to update something, back up shortly.
I learned a lot from purity and stuff like that before enabling complex or even simple (like overloaded strings) GHC extensions that may cause type errors.
thanks for the reply. Wikipedia says it has a formal specification, Is that what you mean by: &gt; it has a definition, and it is mechanized in a logical framework also, what are the reasons you're transitioning into using SML for actual projects? rather than using Haskell or Idris.
How would an "empty" semantic even make sense to talk about without considering mappend? 
If things like this can work, then how useful can generic functions over all Pointed really be? How can you possibly meaningfully reason about generic Pointed functions? 
Do you have any screenshots of what FLTK looks like on {Windows, Mac, Linux}? The ones I found elsewhere online seem like they might be out-of-date.
My rule: does the package author have an issue tracker and respond to open issues? The more promptly they respond, the more well-maintained.
The ones you might have seen on the project page are pretty indicative. Here's one of [FLUID](http://i1-linux.softpedia-static.com/screenshots/FLTK_6.png) which is pretty standard looking. And here's a [slicker one](http://zynaddsubfx.sourceforge.net/images/screenshot05.png) but it probably has a lot of custom widgets. All of these will look pretty much the same across the platforms. On a Mac the menu bar will appear across the top. Also in case you're wondering one of the features of this API is that you customize widgets (draw them differently, etc.) directly from Haskell. You don't need to descend into C++. Quite a few of the demos demonstrate how to do this.
Maybe "trivial" wasn't the best word. I'm saying that if some implementations are considered "good" and others are considered "bad", and a law is automatically true for all implementations, then the law cannot be used to discriminate between good and bad implementations. So we might want to find some other law which does. `const Nothing` was an example of a bad implementation (here "bad" simply means that it's not what the current implementation of `pure :: a -&gt; Maybe a` is) which nevertheless satisfies the law, because all well-typed implementations do.
I look at the last update time. If it hasn't been touched in years then I take that as a bad sign. Well-maintained libraries usually have an update at least every few months if only to stay compatible with the latest GHC version.
There's not really a "canonical" way to judge it. A comment/rating system has been discussed in mailing lists in the past, but nobody has yet stepped forward and implemented it. You can go by a couple of pointers: * Number of downloads (although this only goes one way: a much-downloaded package is probably good, but a little-downloaded one isn't necessarily bad), * number of versions (if there's &gt;20, the author has probably put real work into the package, but the same caveat as above applies), * presence of an issue tracker/maintainer email.
It doesn't seem to work with lambdas: (\f -&gt; f . sum . map (f.f)) f x = \y -&gt; y x Produce no result. EDIT: But don't be discouraged, friend. I believe in you :)
My advice for those searching for a Haskell IDE on Windows: Abandon all hope, ye who enter here. I've used Leksah - which was quite good, for a while. Version 12 was actually quite good, but lacked an important feature (Cabal Sandboxes). I tried version 13 and 14 and they just had horrible issues. My suggestion: SublimeText 3, and give up on the hope of a lot of IDE features (you'll learn to live without them pretty quickly). If you want to get set up on Windows, this is how I suggest you proceed: * Uninstall the Haskell platform (important - look at where various things are installed, such as `cabal.exe` and make sure that all the folders are deleted too) * Download and install MinGHC: https://github.com/fpco/minghc * I strongly suggest getting some sort of Cygwin client for windows. If you use Git, the Git Bash is really good * Open up your command line, and execute the following, in order: - cabal update cabal install cabal-install cabal install alex happy cabal install aeson hlint cmdargs haddock cabal install ghc-mod (If any of the above don't install let us know - `ghc-mod` on Windows has been a bit of a pain lately) Here is a quick intro to what that does: * `cabal update` downloads the latest package index from Hackage, and `cabal install cabal-install` will install Cabal for you (the package management-ish system) * `cabal install _____` will install a module **globally** to be accessible by every project, this is important because modules have a hierarchy of dependencies and they will quickly become problematic if you install every package globally * All of the packages I suggested you install globally will be used by SublimeText to make it more IDE-like (though not quite a full blown IDE) If you ever need to nuke your global packages, just execute these commands: $ rm -R ~/AppData/Roaming/ghc ~/AppData/Roaming/cabal $ cabal update $ cabal install cabal-install That will delete Cabal (package manager) and GHC (the compiler) from your AppData directory. Now, whenever you want to start a project, I suggest you do this: * Create a folder called /src/ in your project, and inside put Main.hs (with `module Main.hs where` at the top) * In the base directory of your project execute `cabal init` in the command line - this will create a `.cabal` file which has the configuration that you need * In the base directory of your project execute `cabal sandbox init` in the command line - this will create a "sandbox" that will download packages your project needs. This prevents dependency clashes for different versions and it's easy to delete a sandbox with `cabal sandbox delete` and starting over Now let's say that you want to use the JSON parsing library `Aeson`. You look it up and find it on Hackage: https://hackage.haskell.org/package/aeson-0.8.0.2/docs/Data-Aeson.html You would import it into your project files by putting `import Data.Aeson` at the top of the `.hs` file you're working in. But this won't get you the package. You'll notice in the URL there is this bit: `package/aeson-0.8.0.2`. What you need to do is add `aeson` to your .cabal file. Open up your `projectName.cabal` file (made from `cabal init`) and look for this line: `build-depends:`. Append to the end of the list `, aeson -any`. Any tells it to use any version. You can start adding constraints if you find that problematic, e.g. `aeson == 0.8.0.2` . Now, in a command line you execute `cabal install -j`. This will "build" your project and also download all of the packages and their dependencies. Every time you want to start a project you can follow this workflow: 1. Create a directory structure 1. Execute `cabal init` 1. Execute `cabal sandbox init` 1. Add any dependencies to your `build-depends` in your `.cabal` file 1. Any time dependencies are added, use the commandline to build with `cabal install -j` Other than Leksah you won't find a Windows IDE that will do any of that for you. Leksah did, and was great, but had massive problems. It even let you jump to function definitions, had code completion, let you step through your code with a debugger (and more), all inside the IDE. But at the end of the day it just didn't work properly. You can get *some* of these features (e.g. code completion) if you use Sublime Text and the SublimeHaskell plugin. This you should be able to do pretty easy following the readme here: https://github.com/SublimeHaskell/SublimeHaskell EDIT: I thought I would add 2 things... first, while sublime won't build/compile if there are missing dependencies. This is why you need to run `cabal install -j` on the project folder. However, it will build every time you save, and compile an `.exe` binary into a folder called `dist`. Second, if you open up a command prompt in your project directory and execute `cabal repl` you will get a GHCi session which has all of the dependencies from your sandbox!
&gt; I've used Leksah - which was quite good, for a while. Version 12 was actually quite good, but lacked an important feature (Cabal Sandboxes). I tried version 13 and 14 and they just had horrible issues. &gt; had massive problems. &gt; at the end of the day it just didn't work properly. I might have some time at NZHac to work on Leksah. What was the biggest issue? Please add it to github if it is not already there.
I see. Thanks for clearing that up! The pure-issue had been bugging me for months now and I couldn't quite understand why Functor only had fmap.
I just recently wrote up [a small post](http://blog.infinitenegativeutility.com/2015/3/typeclass-refactoring-and-default-superclass-instances) on the motivation for [a previously proposed GHC extension](https://ghc.haskell.org/trac/ghc/wiki/DefaultSuperclassInstances) which would make typeclass refactorings easier by allowing superclass methods to be defined in subclass instance definitions—in effect, it would allow this change to be made without needing CPP. (Said extension was discussed way back when the Applicative/Monad merger was first proposed, and Conor McBride has a [version of it](https://personal.cis.strath.ac.uk/conor.mcbride/pub/she/superclass.html) in his [Strathclyde Haskell Extension](https://personal.cis.strath.ac.uk/conor.mcbride/pub/she/), but it hasn't made it into GHC proper.)
Note that there may be good solid packages that are very seldom updated because they have few dependencies so don't need to update just for the sake of fixing deps, and also they were designed to do one or two things well and just do those things. So I would suggest, as a rough heuristic, that the fewer the dependencies a package has, then the _less_ you would expect even a well-maintained package to be updated. Ditto in terms of source code size. This is to say that packages that are actively used and well-maintained may not be being actively developed, and it is good not to conflate the two. If a package is still undergoing "too-active" development in terms of pace of uploads, that may also indicate to me that it is too unstable and in flux, and I would be better served with something more settled. Outside of the "big web framework" choices where you necessarily need to pull in a lot, I am most inclined to look for packages either A) have few deps, a relatively small amount of source, and build cleanly or B) have a history of frequent-but-not-too-frequent uploads A further question I may want to ask is "is this the minimal thing that solves my problem" -- i.e. if I need to take responsibility for examining its source code, is it a giant amount of code, or a small enough amount that I can debug, edit, patch, etc. the code myself.
It all boiled down to 2 problems: * Post version 12 it was slow. I mean real slow. I ran it on a quad core i7 with 16GB ram. I would type some text and it would take a full second before the text buffer started displaying on the screen * It would crash silently - in particular the console that ran in the background would produce some silent error, and while the UI appeared to work, nothing actually happened in the background Unfortunately it's been at least 3-4 months since I tried it last, so I couldn't tell you specifically what errors I had. I know that I had quite good success with v12 and the only reason I stopped using it was because it couldn't do sandboxes. Though - to be fair - I also did try the EclipseFP plugin, and though performance was slightly better than Leksah, it was still terrible. I basically had to disable all the features that made it worth having an IDE in order to get it to run without lag. So it might be the Haskell setup I had at the time or something, but v12 of Leksah ran consistently well.
I am not sure if Wikipedia is referring to "The Definition of Standard ML" (which is a specification that is somewhat formal), or to the Twelf mechanization of Standard ML. In either case, both are great to have—and the fact that the latter was completed over the course of just several months by a team of I think three people is a testament to the effectiveness of the logical framework methodology. The LF version of SML's definition resulted in the discovery of a number of defects in the original Definition, incidentally. As for your second question, my reasons for using SML now are mostly personal &amp; aesthetic. Haskell's got a lot of great features that I'd come to rely on, and I miss them when using SML—but for me, a proper treatment of modularity &amp; abstraction, as well as a proper treatment of effects &amp; exceptions, is enough to convince me to use SML. (My use of the term "proper" for SML's effects treatment is controversial; to avoid a drawn-out discussion, I'll say that I actually agree it is not optimal, but I am totally unwilling to submit to the notion that the Haskell approach to doing effects is acceptable. Idris's effects stuff is interesting as well, but it does not address my concerns about treating effects with a syntactic type system, as opposed to a semantical one.) My reason for using SML is not really "The Language has a type safety proof in LF", since in practice, even though Haskell has had numerous instances of type safety being broken by some poorly conceived combination of features, you don't really run into this kind of thing in day-to-day use. For me, I prefer SML for three reasons: 1. Modules (Matter Most) 2. Effects (Don't Belong In Syntactic Types) 3. Strictness (Lets Me Reason By Induction) (Is Compatible With Partiality) The fact that SML is very nicely defined and mechanized is just icing on the cake.
I think Haskell would be very painful to use if it were strict-by-default, and you wouldn't really get almost anything in return for it. (The real win of strictness, in my view, comes from the fact that it is compatible with a broad range of effects, which simply don't work in lazy calculi.) Perhaps the folks using Mu (or whatever it is called) can weigh in.
yeah. this seems related to how Semigroup should be the superclass for a Monoid that adds mempty, rather than Default being a superclass to a Monoid that adds mappend. the arguments I've read being that there are laws that can be satisfied by those methods (like associativity). so when splitting, superclass methods should involve the ones with "more complex" signatures, more arguments or more diverse types, to try and get a law. or this is a useless criterion? also, stuff like magma exist in math (though I don't know what it's for), but is "Default" or "Inhabited" (saw that one in this thread) an abstraction anyone uses? you can access the known inhabitant I guess. I guess Void can't have an Inhabited instance, but every other type can (maybe?). or this may just be me being silly and misrepresenting the issue because I can't wrap my head around it.
Gah, can't believe that I missed something so obvious! Thanks for the feedback. This should be fixed now.
Were you using the prebuilt Leksah binaries? &gt; Post version 12 it was slow. I mean real slow. I ran it on a quad core i7 with 16GB ram. I would type some text and it would take a full second before the text buffer started displaying on the screen Did switching off background building make it faster? There was an issue preprocessing chs files while collecting metadata that might have explained this. I cannot remember if it happened only happened if the chs files were in the workspace packages or if it happened even when they were in system packages. I think the fix is in the Windows binaries. It could also be a problem with the Gtk message loop. Because we are using the single threaded Haskell RTS the message loop has to yield to Haskell threads and the way it does that is quite fiddly to get right. It may be helped a lot with some small tweaks. Switching to the threaded Haskell RTS makes it much faster, but less stable (one Gtk call to from the wrong thread and the whole thing crashes). &gt; It would crash silently - in particular the console that ran in the background would produce some silent error, and while the UI appeared to work, nothing actually happened in the background So you could still edit, just nothing could save the changes? That can happen if one of the open files gets locked. Leksah will let you continue editing, but would not let you save files. Background builds would also die. There used to be a bug where leksah-server would lock files and cause this issue, though I think this was fixed some time ago by switching some readFile calls from lazy IO to strict IO.
because its a moderately mature compiler backend where using the lovely llvm-general/llvm-general-pure libraries, it'd take a weekend for you to try out :) https://hackage.haskell.org/package/llvm-general
I struggled with Haskell for several years until I finally gave it some serious time. I was saved by the [minghc](https://github.com/fpco/minghc) project, which I think is the most straightforward way to compile and install all of the necessary stuff. As for an IDE, don't try it until you are serious with Haskell development. It probably requires an in-depth knowledge of build scripts and Cabal; Notepad++ and 'ghc make foo.hs' should satisfy your needs for now. Also, do MSYS before Cygwin. 
&gt; GitHub issues Adding to this, "Open Pull Requests", if there hasn't been a commit in months and there are open pull requests I consider the project abandoned. Also, how hard is it to just acknowledge a pull request. If you don't want it, just say so and close it. Don't just leave it open with no comments!
[23 days ago to be precise](http://www.reddit.com/r/haskell/comments/2zpd2e/i_created_a_web_app_front_end_to_the_pointfree/) and they are different apps which do the same thing.
First, to distinguish a syntactic (intrinsic) type system from a semantic (extrinsic) one—the former is the kind of thing you have in Haskell and ML (and Agda, Coq, Idris, Epigram, etc.). Syntactic type systems deal with intensional characteristics of **algorithms** and **code**; on the other hand, "semantic" type systems deal with behavioral/computational aspects of **functions** and **values**. Refinement type systems are an example of the latter (by the way, "refinement" doesn't mean "write down some invariants and send them through a SAT solver"). In practice, usually the semantic layer is imaginary (exists only in the minds of a person trying to prove something informal about code). Properties that could be encoded in semantic types are, "This algorithm is total", "this program never crashes", "this function only ever returns positive numbers". Can't you just encode this in a syntactic type system though? Yes, but you do so at great cost: not only do your terms end up having to carry proofs, but you also have to think of every possible property you'd want to prove about such terms in advance. The former is not so much of a problem anymore, due to the great work done on erasure by Edwin Brady and others. On the other hand, the latter problem, which is a prime example of how intensional type theories seem incompatible with modularity, seems quite concerning. So there are three things that might be done: 1. You may encode the required invariants into syntactic types. This is how you end up in a world where one implementation of a function might get assigned a different type from another implementation of the same function (up to observational equality). This kind of super fine-grained typing may be interesting from some perspectives, but it destroys the extensional nature of mathematics. This is the approach taken in Haskell, Agda &amp; Coq. 2. You may have a looser intrinsic/syntactic type system that does not deal with invariants like "This code cannot do IO", or "This code never throws an exception"; instead, all these features are available ambiently, and there's no internal method of distinguishing between code that uses one subset of these effects from another. This sounds bad, and it's not great, but in return, you get the ability to implement functions (i.e. operations which take equal inputs to equal outputs every time) using effects, which may yield more efficient code. For instance, you may write a memoizing function that uses reference cells under the hood, but then prove by hand that it is functional/extensional. In return for this great flexibility &amp; modularity, you pay by having to do these proofs manually or externally; it is standard practice to ensure that things assigned a "function" type are partial functions, but it is entirely possible that you may encounter an operation which is not functional at all (e.g. it observably returns a different result depending on the phase of the moon). Trade-offs abound. This is the approach taken in the ML family of languages. 3. You may take the loose approach above as far as your intrinsic type system is concerned, and then layer atop it a theory of type refinements, which deal with behavioral properties of code. It is at this level that you may express and prove the fact some operation is function, or that some function is total, or that some operation never uses IO, etc. The last approach is ideal from my perspective, but it has not been quite realized in a serious/practical way yet. You may consider Nuprl's type theory to be an example of #3, where the main Nuprl type theory is the extrinsic/semantic type system, and then the intrinsic type system is just the system of arities (abstract binding trees). I think that more practical balances may be struck, by perhaps using a stronger intrinsic type system than just arities, but it is something that will have to be experimented with over time. It is very much an "art" to figure out what the proper balance is.
Exactly. Issues with no comment are the canaries in the coal mine.
In a technical sense, you are certainly right—both Haskell and ML are given a direct operational semantics. Certainly they are *syntactic* in the sense I describe, but I think it may be too much to say that they are "intrinsic"; whilst the intrinsic+syntactic vs extrinsic+semantic distinctions seem to coincide nicely for some type theories (i.e. Agda vs Nuprl), perhaps the analogy is more tenuous for PLs, particularly impredicative ones. I'll have to ponder it... Thanks for your comments! EDIT: I have thought about it some more, and here is what I think. Regardless of whether there are nice categorical models—I am thinking of an "intrinsic" presentation as one in which each term may serve as a typing derivation. At first, it seems as though ML &amp; Haskell don't satisfy that requirement, but I would say that there is no serious difference between terms which may always be elaborated/reconstructed into a typing derivation, and terms which are typing derivations themselves; for more on this point of view, see Noam Zeilberger's dissertation. It is only a difference of notation. So sure, the presentation of the type system may be "intrinsic" or "extrinsic", but that's really more of a stylistic choice than anything else. The real difference is elucidated in cases where a term is not "just as good" as a typing derivation; by using this as the dividing line, you get a clear distinction between type systems which are "necessarily" extrinsic, and those which are "incidentally" extrinsic. Another perhaps more clear way to look at the distinction is to construe it as between type systems with a synthetic vs an analytic type membership judgement. Of the former kind is intuitionistic type theory between 1979 and 1986, and computational type theory à la Nuprl (and many systems of type refinements); and of the latter kind are ML, Haskell, Agda, Coq, Idris, Epigram, etc. Another way to look at it, which I quite like, is Zeilberger/Melliès' reconstruction of type refinements as being a behavioral type system which is layered over a structural one, where there is a forgetful functor from the former to the latter. The definitions of "extrinsic" and "intrinsic" that arise here, then, have little to do with predicativity, or whether a direct operational semantics is given, or whether there is a nice categorical presentation—instead they are just terms that are used to describe one type system relative to another when they are ordered in terms of these functors.
The last * is the kind of the concrete type that results.
You might also get some orientation by glancing at reverse dependencies http://packdeps.haskellers.com/reverse subject to most of sclv's caveats. 
Oh, so Frank Int Maybe has a return type of * Whereas data Frank a b = Frank {frankField :: b a} deriving (Show) has no explicit return type (it is implicit)?
I see the mechanics of it now, but I'm confused about the purpose. Also, now I'm not sure why the `mempty` implementation of `pure` is reasonable.
Even if there have been recent commits, pull requests older than a couple months are enough to make me pretty suspicious.
[10 days ago](http://www.reddit.com/r/haskell/comments/316s1q/announcing_blunt_a_pointless_haskell_tool/) too! 
I'm all for competition, but what does this do that [Blunt](https://blunt.herokuapp.com) (my tool) doesn't? Edit: Other than a better domain name, of course. 
&gt; Were you using the prebuilt Leksah binaries? Yes, the pre-built binaries here: https://github.com/leksah/leksah/wiki/download NB. The website is massively out of date! You should maybe redirect the download link to that page! &gt; &gt; Post version 12 it was slow. I mean real slow. I ran it on a quad core i7 with 16GB ram. I would type some text and it would take a full second before the text buffer started displaying on the screen &gt; Did switching off background building make it faster? I turned background building straight away (found it annoying). It was slow even without background building. Also, I have a feeling that it got slower over time. The longer I was using it the worse it got. &gt; There was an issue preprocessing chs files while collecting metadata that might have explained this. I cannot remember if it happened only happened if the chs files were in the workspace packages or if it happened even when they were in system packages. I think the fix is in the Windows binaries. &gt; It could also be a problem with the Gtk message loop. Because we are using the single threaded Haskell RTS the message loop has to yield to Haskell threads and the way it does that is quite fiddly to get right. It may be helped a lot with some small tweaks. Switching to the threaded Haskell RTS makes it much faster, but less stable (one Gtk call to from the wrong thread and the whole thing crashes). When I get some time I can give it another go and let you know if it's any better. But have my hands a bit full at the moment. Also can I install it into a sandbox? I'd never used anything but the Windows installer. &gt; &gt; It would crash silently - in particular the console that ran in the background would produce some silent error, and while the UI appeared to work, nothing actually happened in the background &gt; So you could still edit, just nothing could save the changes? That can happen if one of the open files gets locked. Leksah will let you continue editing, but would not let you save files. Background builds would also die. There used to be a bug where leksah-server would lock files and cause this issue, though I think this was fixed some time ago by switching some readFile calls from lazy IO to strict IO. Yes I could edit but not save or build. It would eventually happen with either v13 of v14 after some amount of time. Almost without fail.
Thanks. That at least confirms that something must be wrong with my setup. According to the task manager, most of the time is taken up by gcc, so it could have something to do with mingw's internals, or with maybe `ld`... but it's good to know that reinstalling/switching to Ubuntu can solve this. This has been going on for months and I was really getting afraid that Haskell might be inherently this slow.
`Frank Int Maybe` doesn't *have* a type; it *is* a type. (Types don't have types in Haskell.) `Frank Int Maybe` has a kind, which is `*`. Similarly, `Frank` has a kind: as you pointed out originally, it's `* -&gt; (* -&gt; *) -&gt; *`. 
FWIW, Blunt worked better for me. 
Maybe you're having disk problems? I don't know of what tools are available, but maybe you could run a disk IO benchmark and see if it looks extreme.
https://github.com/haskell/haddock/issues/382 
You still haven't answered me what exactly is mutated right now (in a non-auditable way) on Hackage (and I assume you don't refer to the mere act of uploading a new package version...)
 data Frank a b = Frank { frankField :: b a } doesn't have a kind...it's a declaration. But the type-level function `Frank` has a kind. Just like the declaration foo :: a -&gt; (a -&gt; b) -&gt; b foo x f = f x doesn't have a type. But the value level function `foo` has a type. `foo` is a function on values that takes two values -- an `a -&gt; b` and a `b` -- and returns a `b`. `foo 4 show` is a String. `Frank` is a function on types that that takes two types -- an `*`, a `* -&gt; *`, and returns a `*` `Frank Int Maybe` is a `*`.
I wonder what people use this for. If it isn't a trivial conversion you can do manually, the code is probably more readable in the original form.
I don't think eta-reduction flies for data structures. To answer your question: yes, that is valid syntax. That data structure describes a simple "container" that can hold values of type a. Here are a few examples: * Person2 "test" :: Person String * Person2 5 :: Person Int
[This StackOverflow answer may be relevant.](https://stackoverflow.com/a/19759070)
Are you using parallel building (`--jobs=...`)? If so, try again with `cabal install -j1 lens` to avoid parallelism. Also, do you notice the build getting stuck in particular package?
&gt; Don't just leave it open with no comments! ...or at least tag it and/or assign a milestone
That's a very stupid approach, imho. There are lot of stable libraries not needing an update every week. Parsec2 has a *single* upload. If you have an issue, just write an email to the maintainer.
Note that in GHC Haskell, there's a difference between: `f x = g x` and `f = g`. The difference is syntactic, but the latter form triggers the monomorphism restriction, yielding a different type.
Oh, interesting. Looking at [nodes](http://cdn.altrn.tv/s/5dacb80b-6ab7-e011-9727-0025902c7e73_3_full.png?format=jpg&amp;width=1900) name leaking I'd have say they were still using some legacy toolkit :)
&gt; I was starting from the hypothesis that we did not want const Nothing to be a lawful implementation for pure, and concluded that in order to forbid those, we would need a law which does not follow from parametricity. Ah, that makes sense. Thanks for the clarification. :) &gt;&gt; I think I'm probably missing something, since I've heard it multiple times now that type classes should not be specified purely by laws which follow freely from parametricity. &gt; &gt; I haven't. Do you remember where you heard that? I remember two situations where I've heard statements like this, but maybe they were also in the intention of further restricting the set of correct instances. 1. The parents of this comment: &gt;&gt;&gt; The usual criticism to a Pointed typeclass is that there are no laws that might hold. &gt;&gt; Does every typeclass needs some laws ? If Pure was a Pointed Functor, you'll probably have the following law : &gt; &gt; &gt; &gt; fmap f (pure a) = pure (f a) &gt; But this follows trivially from parametricity, I think. 2. The [proposal](https://groups.google.com/forum/#!topic/haskell-cafe/-zd0Kyhjqs4) for a non-free law for `Foldable` on the haskell-cafe mailing list: &gt; I think I now (unlike in the paper) can state a succinct law for Foldable that has desired properties: 1) It is not "free" -- it can be violated, and thus stating it adds semantic content. […]
I've used this trick once or twice because it will sometimes make things typecheck in a different order. A `C Foo` instance wouldn't type check (at the use site, I think), but with `a ~ Foo =&gt; C a` it did since the equality constraint was checked later. I'm sorry it's a bit vague, and I'm not sure if it's the reason here, but I thought I'd mention it.
Might yield a different type.
It's needed for the implementation. You need to be able to check whether you've finished replicating yet.
`cabal install -j1` has helped, but the compilation time is still on the order of hours. &gt; do you notice the build getting stuck in particular package? There's nothing that sticks out. The build process is just all-around slow. Based on what /u/davetchepak and /u/jamshidh said, I'm pretty much convinced that the issue is due to mingw and not cabal, although I haven't been able to pin down what it could be. According to the resource monitor, most CPU is taken up by Windows console host and gcc/cc1.exe
I can reason about Pointed Semigroups pretty well (to implement a sort-of cons) -- but re-implementing point in a pointedsemigroup class does not seem like the right way. Better to have a superclass whose laws talk about possible adjacent class relationships.
This is discussed on the [Haskell Wiki](https://wiki.haskell.org/Why_not_Pointed%3F) and I think the argument made there is pretty strong: &gt; some of these tiers are uninhabited by methods, they merely offer laws. Laws that newer users may not understand are critical to the correctness of their code, and which won't be pushed on them when they get a bag of constraints out of the typechecker. So the user can silently introduce code that relies on constraints it hasn't put properly on the type. Basically, you'd have -- Applicative adds some laws to the interaction of `pure` and `&lt;*&gt;` -- for example, pure f &lt;*&gt; x = fmap f x class (Pointed f, Apply f) =&gt; Applicative f where {- no methods -} and code that used `pure` and `&lt;*&gt;` without defining a type would say `(Pointed f, Apply f) =&gt; TYPE` yet probably be relying on those Applicative laws.
There is so much legacy leaking, scripting it is lots of pain and frustration.
I'm not sure that makes sense, for example: f :: [Int] -&gt; IO () f xs = print xs &gt;&gt; return () `[]` and `IO` are both pointed functors (obviously, since they are monads), but there's obviously no way to decompose `f` since it has effects in IO.
 data Const t a = Const t instance Monoid t =&gt; Applicative (Const t) where pure _ = Const mempty Const t1 &lt;*&gt; Const t2 = Const (mappend t1 t2) It's just an extension of the trivial Applicative instance: data Trivial a = Trivial instance Applicative Trivial where pure _ = Trivial Trivial &lt;*&gt; Trivial = Trivial Both of these instances follow all the laws for Applicative (although Trivial is also a Monad, which Const isn't). EDIT: Also, this instance helps build up understanding of what things are and aren't Applicatives via composition: data (f :&amp;: g) a = Pair (f a) (g a) instance (Applicative f, Applicative g) =&gt; Applicative (f :&amp;: g) where pure x = Pair (pure x) (pure x) Pair kf kg &lt;*&gt; Pair xf xg = Pair (kf &lt;*&gt; xf) (kg &lt;*&gt; xg) iso_from :: (Const t :&amp;: Identity) a -&gt; (t,a) iso_from (Pair (Const t) (Identity a) = (t,a) iso_to :: (t,a) -&gt; (Const t :&amp;: Identity) a iso_to (t,a) = Pair (Const t) (Identity a) -- so it should be obvious now under what conditions (t,) is an Applicative: instance Monoid t =&gt; Applicative (t,) where -- (mempty, x) pure x = iso_from (pure x) -- (t1, f) &lt;*&gt; (t2, x) = (mappend t1 t2, f x) f &lt;*&gt; x = iso_from (iso_to f &lt;*&gt; iso_to x) -- exercise: under what conditions is ((,,) s t) an Applicative?
`Const` is a kind of degenerate `Applicative`, but this degenerate case can be useful to restrict to. The examples I remember: `Const` is the `Applicative` you need to use with `traverse` to show that every `Traversable` can also implement `Foldable`. I.e. from [Data.Traversable](https://downloads.haskell.org/~ghc/7.8.4/docs/html/libraries/base-4.7.0.2/src/Data-Traversable.html#foldMapDefault) (7.8.4 because 7.10.1 source links are broken): foldMapDefault f = getConst . traverse (Const . f) In `lens`, it's what you need to use a van Laarhoven `Lens` as a `Getter` (this uses just the `Functor` part), or a `Traversal` as a `Fold`.
When I was playing with Church numerals, I found that Haskell could deal with much more complicated code if I used a newtype to know where I was exploiting polymorphism: newtype Church = Church { runChurch :: forall t. (t -&gt; t) -&gt; t -&gt; t } intToChurch n = Church (mkChurch n) where mkChurch x f a = if x == 0 then a else f (mkChurch (x-1) f a) churchToInt n = runChurch n (+1) 0 zero :: Church zero = Church (\f a -&gt; a) succ :: Church -&gt; Church succ n = Church (\f a -&gt; f (runChurch n f a)) -- this add reduces to the same untyped expression as your "add" -- but the types are much less crazy. add = \ch0 -&gt; (\ch1 -&gt; runChurch ch1 succ ch0) -- but div looks complicated, you'll have to figure it out. EDIT: newtype E x y = E { runE :: forall r. (x -&gt; r) -&gt; (y -&gt; r) -&gt; r } left :: x -&gt; E x y left = \x -&gt; E (\kx ky -&gt; kx x) right :: x -&gt; E x y right = \y -&gt; E (\kx ky -&gt; ky y) either f g x = runE x f g pred :: Church -&gt; Church pred n = either id id $ runChurch n (either (left . succ) left) $ right zero EDIT 2: newtype P x y = P { runP :: forall r. (x -&gt; y -&gt; r) -&gt; r } pair :: x -&gt; y -&gt; P x y pair = \x -&gt; \y -&gt; P (\k -&gt; k x y) fst x = runP x const snd x = runP x (const id) newtype B = B { iff :: forall r. r -&gt; r -&gt; r } true = B (\x _ -&gt; x) false = B (\_ x -&gt; x) isZero :: Church -&gt; B isZero n = runChurch n (const false) true -- there is certainly a more efficient way to do this that -- doesn't duplicate the work done by pred/isZero -- and perhaps a way that does `y` preds at a time? div x y = (\predY -&gt; fst $ runChurch x (\p -&gt; iff (isZero (snd p)) (pair (succ $ fst p) predY) (runP p (\z w -&gt; pair z (pred w))) ) (pair zero predY) ) (pred y) 
If the options you can find all suck (and when I tried, they really did) then ghcid might be worth checking out. Simple, robust, provides 50% of an IDE with 0% of the hassle. https://github.com/ndmitchell/ghcid
So I worked on it a bit today. It's working so far, but had to use the Windows cmd facility as Git Bash for some reason (while recognizing the PATH for the cabal executable just fine) doesn't want cabal to actually do anything, always erroring out with "GHC &gt;=6.4 is required but none was found" or something like that. Didn't matter the package being installed or where I initiated GB from, it always errored.
It's... working. This's a bit surreal. Excuse me, gotta take a breather... Great, many thanks. Fooh, alright, so I tried using the `-o2` option, which I saw a lot, and it looks like `cabal install -j` isn't the standard/normal way to build projects because I can't set options like `-o2` on `cabal install`.
[This Stack Overflow answer](http://stackoverflow.com/a/29596461/1274282) explains how to remove points from expressions manually. If you're curious about how the `pointfree` tool does it, you could try to read [the source](https://github.com/bmillwood/pointfree/tree/1.1/Plugin/Pl). (I have not read it, so I don't know if it's easy to understand or not.) 
Looks promising. With TheCriticalSkeptics 's help setting the other stuff up, this could actually work nicely (will just have to keep everything really simple with the directories and file dependencies and variable names and so on). ~~Edit: This's a recurring problem, it seems. No programs heavily dependent on cmd that I've tried recently (all for Haskell) which were supposed to be accessible through PATH have been accessible. Running `ghcid -blabla` in Windows cmd says command unrecognized and I can't find the ghcid executable in the Cabal directory.~~ Update: Fixed.
Instead we could say: "if `f` is both `Pointed` and `Apply`, it must satisfy the following law: `...`", and `Applicative` would be just `type Applicative f = (Pointed f, Apply f)`.
I think you misread my post. I said "several months" (3+) to "*years*", not "a week". I also qualified my statement with "usually".
This is a fun fad. 
Any advice on using/or-not Stackage as the default provider for Cabal instead of Hackage? For a newbie's peace of mind, at least...
Oh, yeah. replicate1 should probably use guards or either specifically take an int. Whoops. 
Now the burning question in my mind is, "What's stopping this from being a practical library function, and how do we fix it?"
&gt; now that I know how bad it really is, I'll probably just switch OSs For reasons beyond my control, I need to use Windows on a new computer that I've had for a few months. I finally gave up trying to use it for Haskell development when I realized how often I was dealing with problems instead of writing code (but I have no idea if your issue is related to Windows or mingw). I installed Arch Linux as a VirtualBox Guest OS and the experience has been great. I have Arch with my development setup on one monitor, and Windows on the other. I can copy and paste between the two OSs and access files bidirectionally. It is much more convenient than dual-booting, so long as you don't mind running Windows all the time. Since you're using Windows anyway, you might want to give it a try. It's certainly easier than replacing your OS entirely.
can you link to an article about this kind of thing? I've never run across this but it seems like it's something I easily might
You're right. I thought about it more and I think what I was describing is not the category with regular haskell functions, but rather the subcategory of pointed types and pointed morphisms. In that case it's really more of a definition for pointed morphisms rather than a "law", so not very useful here.
How about [heterogeneous lists](http://hackage.haskell.org/package/HList-0.3.4.1/docs/Data-HList-HList.html#t:HList)? data HList (as :: [*]) where HNil :: HList '[] HCons :: a -&gt; HList as -&gt; HList (a ': as) You can write `HList '[a,b,c]` instead of `HList (a ': (b ': '[c]))`, in much the same way you want to write `(a,b,c)` instead of `(a,(b,c))`.
These sorts of conditional laws are really problematic in practice. In theory the author of either of the instances may be unaware of the other. scalaz tried this for a while. We tend to avoid making laws that cut across class hierarchies. Declaring an instance of class (Pointed f, Semiapplicative f) =&gt; Applicative f would be explicitly stating the laws tying the unit from pure to the operation of (&lt;*&gt;). Mind you in a world where this existed, you'd still have most code getting inferred with the more general type, but needing a more specific type than it infers to reason about the correctness of the code, which is rather awkward. We tend to like to bring in laws with operations because then using those operations infers the constraint that includes the laws.
from a performance perspective doesn't this have the same overhead as `(a, (b,c))` (requires structural induction to access deeper elements)?
There are examples, just not good ones. =) Set is an example of where you can inject into the set with `pure = singleton`, but this invites users to write code like: `foldMap pure` to construct a set of elements. Unfortunately it relies on a `Monoid` to glue together `f a`'s and pure to construct one, and no law relates the operation of `mappend` and `pure`. I invite you to explore the wonders that are foldMap pure :: [a] -&gt; Set a foldMap pure :: [a] -&gt; [a] foldMap pure :: [a] -&gt; Maybe a and the rather complete inconsistency of these answers.
See the Pointed instance for Const in the `pointed` package. You don't want `Monoid` for `Pointed` for `Const`. Rather you need an even more lawless class like the one in `data-default`. class Default a where def :: a instance Default a =&gt; Pointed (Const a) where pure _ = Const def and if you bring in Pointed as a superclass of Applicative, you need to bring in Default as a superclass of Monoid with `def = mempty`.
&gt; Signatures are mandatory, to tell zipN there are no more arguments. This is a drawback.
That's the first version, the other versions infer the types.
Cool project, and a good exercise to learn to build haskell web apps. Also note you can configure vim or emacs to do pointfree/pointful conversions in-place which is more convenient than cutting and pasting into a web form.
I understand! In fact, I built my own little example Scotty app. It's called [Hairy](https://github.com/tfausak/hairy). I ultimately didn't use Scotty for Blunt since it's really only a single endpoint. After I made my comment, I poked around and found this in your project's readme: &gt; I built it as a way of learning more about building services in Haskell. It's really just a toy. Don't take it too seriously.
That being said, an alternative of `replicate` could use the following implementation: replicate' :: (Bounded n, Enum n, Eq n) =&gt; n -&gt; a -&gt; [a] replicate' n _ | n == minBound = [] replicate' n s = s : replicate' (pred n) s &gt;&gt;&gt; replicate' (2 :: Word) 42 == [42, 42] Note that the missing `Num` instance for `n` doesn't allow an unannotated literal.
Parsec predates Alternative, and now it does implement Alternative and define its (&lt;|&gt;) as a copy of mplus.
&gt; These sorts of conditional laws are really problematic in practice. In theory the author of either of the instances may be unaware of the other. In the case of Pointed and Apply there is no such problem, as both libraries have the same author. If you added the law to the documentation of both of your typeclasses, it would be impossible to accidentally violate it.
Yes, I know this. It's still a bit annoying to do import Control.Applicative import Text.Parsec hiding ((&lt;|&gt;), many, optional) every time I want to use Parsec. (Well, it assumes that I want to use `Control.Applicative` too, but that's always true.)
Just updated the readme with a description of a sample editing session. https://github.com/imeckler/auto
The problem is not the author of the classes but the instances. I wasn't aware of `Pointed` untill yesterday. I could have written a type and its `Apply` instance without being aware of `Pointed`.
This is a result of what is called "lifted products". That is, `(a,b)` can itself be `_|_`, which behaves differently from `(_|_, _|_)`. We could avoid lifted products. To do this, basically all product matches are compiled as irrefutable. i.e: case x of (a, b) -&gt; .... Would be compiled as: case x of ~(a, b) -&gt; .... Such that `a` and `b` become equivalent to `fst` and `snd` applications of `x`, and much like newtypes, the pattern-match is a no-op. This would mean that the behavior of `_|_` and `(_|_, _|_)` becomes indistinguishable, and so it can be said that tuples don't add an extra bottom. This raises a potential efficiency problem, that a tuple cannot be compiled into a storage cell of multiple thunks. Here `a` and `b` must be generated as new thunks, rather than being extracted from the tuple. It also raises a problem w.r.t the behavior of `seq`. What does `seq someTuple x` do? Does it force both sides of the tuple? Is it a no-op? If it is a no-op, a function whose result can be forced to yield some computation suddenly fails to compute when its polymorphic result happens to be instantiated to a tuple. IMO, it would bring a net positive, if seq becomes a method in a type-class, of which tuples aren't instances. Because it would allow what the OP wants. Also because `()` would not have `_|_` in its domain. That makes all sorts of reasoning about stuff easier (consider the Monoid instance of `()`). I also think function spaces `(a -&gt; b)` should not be lifted, allowing various optimizations. Currently optimizing `f . id` to `f` is not allowed because it might refine the function from bottom to an eta-expanded lambda! Eta-expansion is thus not a valid transform in general, which sucks.
Which is often a handy way of getting overlapping instances and fundeps to play well together: class Cls a b | a -&gt; b where {} instance Cls A B where {} instance Cls any C where {} Won't work, whereas: class Cls a b | a -&gt; b where {} instance Cls A B where {} instance (c ~ C ) =&gt; Cls any c where {} Will. (Not that this has anything to do with the `MonadThrow` instance.) 
To add to the discussions about using trace, that some people already mentioned, I have found myself doing sometime like the following. If I have code like this: (!) :: [Int] -&gt; Int -&gt; Int (x:_) ! 0 = x (_:xs) ! n = xs ! (n-1) and if I'm only interested how this function is called (and not which particular branch is used at each step) I add a trace call inside a guard at the top of the function, something like this: (!) :: [Int] -&gt; Int -&gt; Int xs ! n | trace (unwords ["in (!)", show xs, show n]) False = undefined (x:_) ! 0 = x (_:xs) ! n = xs ! (n-1) Calling e.g. `[1..10] ! 3` would result in: &gt; [1..10] ! 3 in (!) [1,2,3,4,5,6,7,8,9,10] 3 in (!) [2,3,4,5,6,7,8,9,10] 2 in (!) [3,4,5,6,7,8,9,10] 1 in (!) [4,5,6,7,8,9,10] 0 4 allowing you to see how the function is recursively called without adding traces within each branch of the function. The advantage of doing this instead of adding traces within the function is that it is (arguably) easier to remove in comparison to removing trace's at multiple branches. Note, however, that you might change the strictness of the function if you do this without thinking about what you want to show and how much of it you want to force in your call to `trace` (e.g. the second version will print numbers indefinitely if you call it with `[1..] ! 3` whereas the first version will work and return 4).
&gt; IMO, it would bring a net positive, if seq becomes a method in a type-class This would come at the expense of having to break back datatype contexts. =/ data Foo a = Foo !a !a would need to become data Seq a =&gt; Foo a = Foo !a !a behind the scenes as Foo :: Seq a =&gt; a -&gt; a -&gt; Foo 
It could make sense, except that `seq x` no longer looks like: case x of CTOR -&gt; ... And it might force *too much* when your polymorphic result happens to be instantiated to a tuple type.
I find that nice. The `Foo` constructor type suddenly hints about the fact it forces values.
I'm not sure I follow. Can you make an example?
Notice that the type `(Int, String, [Bool])` is kind of like a list of types, where the first element is the type `Int`, the second element is the type `String`, and the third element is the type `[Bool]`. Also notice that this is also kind of like the parameters in an ADT (e.g. `data MyADT = MyADT Int String [Bool]`). Now, imagine we could represent such lists as types. Then our elements would look like: `[1, "hello", [True, False, False]] :: '[Int, String, [Bool]]`. Now imagine we could do type-level list operations on them. We might be able to express things like: list concatenation (*combine these two types* or *I want a value that looks like the combination of two records*), asking if an element is in a list (*I want a value of a type that has a `String` as its second parameter* - note that this is very similar to row polymorphism!) Does this make a little more sense? Hopefully I'm answering the right question! There is a nice library for extendable records via heterogeneous lists called [vinyl](https://github.com/VinylRecords/Vinyl/blob/master/tests/Intro.lhs)
The main site says Flow requires "at least Cabal **1.8**". This might be a typo (I don't really know; still new).
Lazy ML (one of the design influences on Haskell) did this. I am told it made type error messages considerably worse, because passing the wrong number of arguments to polymorphic functions didn't lead to errors right away.
What do you mean by polymorphic lists? 
What do you mean?
The answers saying that the two formulations are equivalent are more-or-less correct. There are corner cases where they differ, but they don't arise often. For example Prelude&gt; let f x = undefined x in f `seq` () () Prelude&gt; let f = undefined in f `seq` () *** Exception: Prelude.undefined 
I'm not sure I understand. Why not just use an appropriate Applicative instance for Array and Streams? It looks like using &lt;*&gt; etc. already let us do this? edit: any number of *any* collections. I see now :) 
For one, `seq (a, b) x = seq a (seq b x)` is wrong. The latter is undefined if either `a` or `b` is undefined, but you need it to be undefined only if both are undefined. For two, I thought recently that we could implement unlifted pairs as an abstract type, like: module Pair (Pair(), pattern Pair) where data Pair a b = Pair_ a b pattern Pair x y = ~(Pair_ x y) But it was pointed out to me that this is also wrong. The reason is that unlifted things like this should not be irrefutable in the way that `~` makes patterns irrefutable. They should instead be like newtypes. If you look at the report semantics for newtype, it says that if you have: newtype N = N (...) then matching the pattern `N p` against undefined is the same as matching `p` against undefined. So the pattern `N p` is refutable iff `p` is refutable. It is not simply irrefutable, and `~(N p)` is not always the same as `N p`. For instance: newtype N = N (Maybe Int) f0 x = case x of N (Just y) -&gt; y N Nothing -&gt; 0 f1 x = case x of ~(N (Just y)) -&gt; y ~(N Nothing) -&gt; 0 `f0 (N Nothing) = 0` but `f1 (N Nothing) = undefined`. So, unlifted pairs should work this way, too. The pattern `Pair p q`, when matched against undefined, should behave like matching `p` and `q` against undefined; alternately, `Pair p q` is refutable iff either p or q is refutable. The way to achieve this that I can think of is to turn `Pair p q` into the view pattern `(\ ~(Pair_ x y) -&gt; (x, y)) -&gt; (p, q)` which eta expands the pair. but this starts seeming pretty heavyweight, and I'm unsure if GHC would optimize it to not be so. If it were built-in, presumably it could be all right performance wise. `seq` is still a problem, though. If you want to keep the specification that `seq` is strict in its first argument, then the proper definition is `seq p x = case p of Pair l r -&gt; lub (seq l x) (seq r x)`, where `lub` is required to return the more defined of its two arguments here. This probably means parallel evaluation. Alternately you could change the specification of `seq` to do nothing on unlifted pairs.
&gt; typeclasses are a very bad idea for "ad hoc overloading" Type clases were [basically invented](http://swizec.com/blog/week-20-making-ad-hoc-polymorphism-less-ad-hoc/swizec/6564) to do ad-hoc overloading. The way you've phrased things, it sounds like type classes are a failure.
Just Hask can't fill it.
`-o2` or `-O2`; case [matters](http://www.someecards.com/usercards/viewcard/MjAxMS01ZDhkYzU1MWNlZGUwMGM0). 
Ah, according to the knowledgeable answers here it's a special move in the fun, fun game of typeclass Prolog. 
I thought that would be obvious, given `minBound` on `Int` ;). 
Do you want the Kmett news or the Kmett news?
Ooh. No, I think I misunderstood big time. I was treating 1.8 as &gt;1.22 (decimal mark instead of a sub-version mark). \^\^' heh. I'm still trying to see how `.&gt;` is different from `&gt;&gt;&gt;`. `&gt;&gt;` from F# is simple, and Flow's `.&gt;` is the same, but the tutorials/docs all over the web give the impression that `&gt;&gt;&gt;` isn't really the same.
Out of curiosity, would it be possible to extend this further by using pandoc and supporting arbitrary formats specified by prefix (such as `md`)?
They're also not guaranteed to perform identically under optimization; the inliner (and rewrite rules?) distinguish between them. See the [docs for the INLINE pragma](https://downloads.haskell.org/~ghc/7.10.1/docs/html/users_guide/pragmas.html): &gt; Moreover, GHC will only inline the function if it is fully applied, where "fully applied" means applied to as many arguments as appear (syntactically) on the LHS of the function definition.
My God. That is awful. Like, every single other flag is all lower-case, and they come make the one flag with a homoglyph small/cap letter required capital? Why?! Thanks, works now.
I guess much of it can be covered by using `(&lt;*&gt;)`, with the major exception being containers with specific types (e.g. `Bytestring`), but 1. Once you get past 3 arguments, `f &lt;$&gt; a &lt;*&gt; b &lt;*&gt; c &lt;*&gt; ...` gets tedious (not that functions with that many arguments are generally a good idea), and 2. `liftM`, `liftM2`, etc. are an instance of the same (anti-)pattern.
Consider the following function: polymorphicTail :: [a] -&gt; Maybe [a] polymorphicTail [] = Nothing polymorphicTail (_:xs) = Just xs You can call it with a list containing many integers: &gt; polymorphicTail [1,2,3] Just [2,3] You can call it with a list containing many doubles: &gt; polymorphicTail [1.0,2.0,3.0] Just [2.0,3.0] You can call it with a list containing many strings: &gt; polymorphicTail ["one", "two", "three"] Just ["two","three"] But you cannot call it on a list containing integers, doubles and strings: &gt; polymorphicTail [1,2.0,"three"] type error The `HList` type improves upon ordinary lists in two ways. First, it does allow its lists to contain elements from different types: &gt; (1 `HCons` 2.0 `HCons` "Three" `HCons` HNil) H[1, 2.0, "Three"] And second, `HList` is indexed by a (type-level) list of types, one for each (value-level) element. This type-level list is a regular list, because it's elements all have the same "type": `*`, the "type" of types. (I'm putting the work "type" in scare quotes because the correct term is "kind") This type index makes it possible to give a much more precise type to operations involving `HList` values. For example, `polymorphicTail`'s return type has to by guarded by a `Maybe` because it cannot ask for its input to have at least one element, and so it has to account for the possibility of failure. In the case of `heterogeneousTail`, we don't need the `Maybe` because the input type is `HList (a ': as)`, encoding the fact that the input list must have at least one element by requesting the type index to be a list of at least one type. {-# LANGUAGE DataKinds, GADTs, TypeOperators #-} import Data.HList heterogeneousTail :: HList (a ': as) -&gt; HList as heterogeneousTail (HCons _ xs) = xs &gt; heterogeneousTail (1 `HCons` 2.0 `HCons` "Three" `HCons` HNil) H[2.0, "Three"] In practice, it is rare to need a list containing elements of different types, so most people should probably stick with `[a]`.
Is there a package I could download to try this out it interests me 
I don't think pandoc makes sense in this context. Pandoc is a big library with a huge number of dependencies. Haddock is something that is going to be part of every Haskell setup, so there is reason to keep its dependencies minimal. 
I certainly agree that you'd not want to force a full pandoc dependency, but more curious about the feasibility of making haddock "pluggable"– given some configured registry of prefixes, you could feed the comment into an arbitrary shell command via stdin and take the traditional haddock syntax comment via stdout.
A few things to note **How to do O2 with Cabal** Your concern that you can't use level 2 optimization with `cabal install` is misplaced. You specify all of your compile time options in your `.cabal` file. So I normally start with this in my cabal file: ghc-options: -O2 -threaded -with-rtsopts=-N Additionally, SublimeHaskell will detect when you have a `.cabal ` file in place, and whenever you hit save, it will build using those options. **The -j part of cabal install** So, with `cabal install -j` is basically `cabal install` but with parallel builds (https://ghc.haskell.org/trac/ghc/ticket/910) which is useful when you first compile a package as it can download multiple packages simultaneously. You can also force it to only execute one job at a time with `cabal install -j1`. Additionally, you must do this in a console whenever you add new dependencies to your cabal file. For whatever reason compiling with SublimeText won't go and download dependencies. **Stackage** I would not worry about Stackage until you actually run into your first dependency issues (or until you need to write your first production ready code). **-O and -o** You only have to worry about this when you compile directly with GHC. `-o` is how you specify the "output", while `-O` is how you specify the optimization level.
Excellent summary. Unlifted tuples were considered when Haskell was defined, but deemed to inefficient and cumbersome. 
&gt; [1, "hello", [True, False, False]] :: '[Int, String, [Bool]] I'm not sure I see the point in this. That code can be represented already by using tuples. Just switch [] to () there. Also what would the type of a concatenation of two of these "lists" exactly?
Going back (at least) to the venerable UNIX cc, `-o` chooses the output file (default: a.out) and `-O` chooses the optimization level. It is not "awful". 
Go with stackage for now. IT should minimize any installation problems. If later you want a package that is on hackage, but not stackage, you can switch.
That type rolls right off the tongue: add it to `lens` forthwith!
In the worst case scenario you can use `ST` to transcribe the imperative algorithm literally and then purify the computation when you are done. `ST` is basically programming with mutable `IORef`s, except that you can purify the computation when you are done so long so long as no mutable references leak (and the types guarantee this). You can even do the same thing with mutable arrays, too. As an example, [here is some code](https://github.com/Gabriel439/suns-search/blob/master/src/Shuffle.hs) I wrote for implementing the Fisher-Yates shuffle (a.k.a. the Knuth shuffle) just by transcribing the imperative algorithm. I use `Data.Vector.Unboxed.create` to open a temporary mutable window and then just read and write a mutable vector the same way I would in an imperative language (albeit more verbosely). The compiler then verifies that no mutable references leak from the purified computation, certifying that the overall transformation is pure. 
for those, you can only reason with the elements polymorphically/parametrically. if you have a list of any show's, the only thing you can do is show them. If you have a list of things that can be any type...the only thing you can do is get the length of list or anything that doesn't involve touching the elements in any way. for heterogeneous lists, you know you can treat the first element like an Int, the second like a Bool, etc., because you know that you have one. 
&gt; In general, is translating imperative algorithms to functional implementations supposed to be straightforward or non-straightforward? Non-straightforward. &gt; I got to a part where pseudo-code was presented that was very imperative in nature. It relied on nested loops, calculating based on an array that was written and rewritten repeatedly and using values based on array indices and associated values. Which part of that pseudo-code is rewriting which array? I can see the nested loops, but inside it I see references to paths and "snakes", not to arrays. Skimming through the paper, I notice that Figure 1 looks like straightforward dynamic programming. Do you know how you could re-create that array using a functional approach? If you did, would it be helpful for your main problem?
Cale Gibbard has answered a couple of similar questions. For a high level description see his penultimate paragraph [in this answer](http://www.reddit.com/r/haskell/comments/1hvymr/what_does_it_takes_to_get_acmicpc_to_allow/cayyixr), and perhaps [this one](http://www.reddit.com/r/haskell/comments/2cin7p/is_there_any_way_to_elegantly_represent_this/cjfvbpg) for a more detailed example.
I wish.
This is what Idris does.
Thanks for the heads up, luite2!
I don't think there is guaranteed a way to do it for all algorithms.
Correct. You cannot implement all algorithms efficiently without in-place mutation. For example: there is no known purely functional algorithm that matches the performance of the imperative union-find. 
Yes and no. There is a `log(n)` slowdown for eagerly evaluated languages. No result is know AFAIK in lazy setting.
Findability is a big deal. I like Blunt better, but I doubt that I'll use it only because I won't be able to find it. If you could *somehow* make it easier to find - a really good URL like this other tool, or SEO, or *something* - then I would prefer it.
Fun stuff! A while ago, I wrote a `zipWithN`, but with a slightly different approach: {-# LANGUAGE DataKinds #-} {-# LANGUAGE GADTs #-} {-# LANGUAGE KindSignatures #-} {-# LANGUAGE TypeOperators #-} {-# LANGUAGE TypeFamilies #-} infixr 5 ::: data List :: [*] -&gt; * where Nil :: List '[] (:::) :: [a] -&gt; List as -&gt; List (a ': as) type family (~&gt;) (args :: [*]) (ret :: *) :: * where '[] ~&gt; r = r (a ': as) ~&gt; r = a -&gt; (as ~&gt; r) zipWithN :: (types ~&gt; r) -&gt; List types -&gt; [r] zipWithN _ Nil = [] zipWithN f xs = case go f xs of (r, [] ::: _) -&gt; [r] (r, xs') -&gt; (r : zipWithN f xs') where go :: (xs ~&gt; r) -&gt; List xs -&gt; (r, List xs) go partial ((x : xs) ::: Nil) = (partial x, xs ::: Nil) go partial ((x : xs) ::: ys) = case go (partial x) ys of (r, ys') -&gt; (r, xs ::: ys') foo :: [Int] foo = zipWithN (\a b c -&gt; a + b + c) ([1, 2] ::: [3, 4] ::: [5, 6] ::: Nil) It forces you to work with heterogeneous lists, which is perhaps a little annoying, but I think it leads to quite a simple implementation.
Does it actually default to that or does it only generate a default config file with that option inside if there is no config file present from pre 1.18 days?
&gt; for heterogeneous lists, you know you can treat the first element like an Int, the second like a Bool, etc., because you know that you have one. Those are tuples.
`Char` is not a typeclass, it's a type... so the first thing doesn't really make sense. You can only have constraints like that with typeclasses, like `Num`, `Show`, etc. Saying `Num a` doesn't mean that `a` has the type `Num` (`Num` isn't even a type :o) it means that `a` is constrained to be an instance or member of the `Num` typclass -- that it implements +, -, *, abs, etc. One close thing to what you were trying to do is something like (b ~ Char) =&gt; ... which restricts b only be a type that is equivalent to `Char`. (This isn't standard Haskell, though, I believe.... ? ) But of course, this is a little silly to write, as you can just use `Char` directly in the type signature :p
No, your type signature is not valid. `Char` is a type (kind `*`) and not a typeclass. You might have missed a `~` though: the type `Char ~ b =&gt; b -&gt; Bool` is equivalent. Without knowing the nature of your quiz it's hard to tell whether it's `isUpperAlphanum` or `const True`.
Yes, you can think of heterogenous lists as tuples that you can interact with over a nice abstracted interface.. and with type level functions that mirror functions you'd use on lists, like cons, nil, uncons, etc. And... that's sort of the entire point that /u/gelisam was trying to make in the first place :) In any case they're a wholly different sort of thing with completely different use cases than the existentially quantified lists you are talking about :) 
You can simulate an updateable array with a tree data structure, which has log(n) depth so all array operations are O(log(n)) instead of O(1). (Assuming you buy into the myth that there are O(1) arrays. :) )
A lazily-initialized array is exactly how I was thinking of reimplementing Figure 1 in a functional way. I'm not sure what you mean by "iteration", so just to clarify: the array is initialized via a list of expressions which can refer to one another, and it's okay for an expression early in the list to refer to the result of an expression later in the list; you must simply be careful not to create a bottom value by having two expressions indirectly refer to each other. a1 = listArray (0,1) [42, a1!0 + 1] -- [42,43] a2 = listArray (0,1) [a2!1 + 1, 42] -- [43,42] a3 = listArray (0,1) [a3!1, a3!0] -- &lt;&lt;loop&gt;&gt; Because the semantics of the list of expressions is equivalent to a recursive let statement with one variable per entry: a1 = let {x = 42; y = x+1} in [x,y] -- [42,43] a2 = let {x = y+1; y = 42} in [x,y] -- [43,42] a3 = let {x = y+1; y = x+1} in [x,y] -- &lt;&lt;loop&gt;&gt; 
As for a single pretty printer library, I find myself writing pretty printers all the time : for log messages, instant messaging, user communication, etc. The trouble is that in my use cases, I wish to convey text information on a variety of mediums. For example, [here is a pretty printer](https://github.com/bartavelle/7startups/blob/master/Startups/PrettyPrint.hs) that I used to write to the console (using `ansi-wl-pprint`) and to XMPP. It would be nice that those pretty printers could support user-defined attributes, and perhaps expose a "classic" interface along with something a bit more flexible (to encode RGB colors, character sizes, etc) while retaining the well known combinators.
Like an [`annotated-wl-pprint`](http://hackage.haskell.org/package/annotated-wl-pprint)?
Yes, naïvely you can always achieve the same algorithm with `log(n)` slowdown. See tel's and augustss' posts. The result i was talking about is, that you cannot avoid that slowdown in every case _in strictly evaluated languages_. For non-strict evaluation, no result is known AFAIK. I can look for some references when I'm not on my phone.
That's exactly what I wanted, thanks. I'll need to the read the paper properly to follow the proof but I will do :-).
http://stackoverflow.com/a/1990580/502399
My approach to rewriting imperative algorithms in functional style is the following: 0. Check if a functional algorithm which serves the same purpose exists. 1. Find a suitable immutable data structure to work with. Imperative algorithms often make implicit assumptions that some operations are cheap. Operations' complexity of immutable data structures is usually different. For instance, immutable vector random access is O(1), but [random access update is O(n)](http://hackage.haskell.org/package/vector-0.10.12.3/docs/Data-Vector.html#v:update) vs O(1) for a mutable array in both cases. If you have to mix random reading and writing in your imperative algorithm, then an immutable vector is a bad replacement for arrays; if you can delay reading until the entire vector is ready, it may be ok. Algorithms which consume input and generate output sequentially, are usually the easiest to convert to functional style (you can use lists). 2. Replace imperative control flow with maps, folds, scans. Loop, updating something element-by-element -&gt; map. Loop with an accumulator -&gt; fold* (or scan* if you need intermediate values). Simultaneous iteration -&gt; zip* and unzip. Loops with conditions -&gt; dropWhile, takeWhile, partition, filter, etc. Switches -&gt; case expressions and function tables. Any input-output -&gt; move it outside. 4. Test on known inputs. Check if the performance is not too bad on different input sizes. 5. If nothing else helps, do it in ST, IO or other monad which can do state-transforming actions, and use a mutable data structure (MVector, MArray, etc). Once you've started using mutable data structures and using mutable actions in ST and IO monad, the implementation is imperative anyway. You may consider using a C implementation instead + FFI. BTW, you may check [this Haskell implementation](http://hackage.haskell.org/package/Diff-0.3.1/docs/src/Data-Algorithm-Diff.html) of the diff algorithm.
It's almost what I want, I would like something like : annotate :: a -&gt; Doc a t -&gt; Doc a t token :: t -&gt; Doc a t Then the `SimpleDoc` data type would require an extra constructor.
I'm not sure what would be the semantics of that.
When you read "X **and then** Y" instead of "Y **of** X" you are indeed thinking using imperative logic. As some pointed out already, function application in Haskell is lazy, this means that the reader may ignore the X in some sentences of type "Y of X", *e.g.*: const x $ y If you think imperatively, functions like *const* may never come to mind or will look strange: y .&gt; const x How do you read it? "Y and then ignore it and let it have the value X"? Monadic combinators are there exactly to express the sentences that you need to use "**and then**" in them, *e.g.*: foo &gt;&gt;= bar &gt;&gt;= baz You can read it pretty clearly: "Foo and then Bar and then Baz", you layed out a structure for your computation, implying in some sort of sequencing from left to right. To me, it is a lot different to think in values instead of thinking in steps. You are thinking in "data transformation" anyway, but it just feels different to describe the value you want *vs* describing the computation you want.
This is actually pretty much only sugar. Mostly syntactic, a bit semantic if you have an *extraordinarily* dumb compiler. That is, [Oleg already did it a decade ago](http://okmij.org/ftp/Haskell/partial-signatures.lhs).
aha: [liboleg](https://hackage.haskell.org/package/liboleg).
I think you can avoid the slowdown even in an eager functional language, if you have uniqueness types. They allow you to have an imperative array with O(1) update instead of O(log n), because you don't need to worry about keeping old versions around. (Not sure what's the right citation, but I learned this from /u/Felicia_Svilling.)
&gt; This isn't standard Haskell, though, I believe.... ? The relevant part of the [report](https://www.haskell.org/onlinereport/haskell2010/haskellch4.html#x10-660004.1.3) does not list `~` as a way to construct a constraint. It comes from GHC, which I think accepts with without having to enable any extensions.
&gt; You cannot implement all algorithms efficiently without in-place mutation. That might be a *little* strong. It depends on how picky you are about mutation. Updating pointer to a thunk to instead point to the result of evaluating the thunk is mutation, but it's referentially transparent. `LVish` also introduces other ways of doing referentially transparent "updates". If you consider that (thunk replacement) mutation, this is true in a absolute sense. There's a specific O(n) problem that requires O(n lg n) time without mutation. Other problems may or may not be subject to the same limitations. If you don't consider it mutation, the field is wide-open; there are no strong results either way. &gt; For example: there is no known purely functional algorithm that matches the performance of the imperative union-find. Absolutely; but, just because it isn't known doesn't mean it doesn't exist.
QuickCheck's Arbitrary has no properties. This can be annoying because I often have to poke into the source code to see what Arbitrary is doing for a particular type. On the other hand, QuickCheck's use of these lawless typeclasses works well enough most of the time and reduces boilerplating in tests. So I just wonder if I would go so far as to say that "ad hoc overloading" is a bad idea generally. [edit] and of course remember that QuickCheck Gen does not even satisfy the monad laws!
Of course it is mostly sugar. That's why Oleg's "solution" is not a solution. Sugar should be sweet, not bitter.
`(Int, String, Bool, Bool)` could be viewed as the concatenation of `'[Int, String]` and `[Bool, Bool]`. Yes, they are isomorphic (you can transform, uniquely, one to the other), so in that sense they are "the same," but there are valuable reasons to use type-level lists over tuples. - One can represent a form of subtyping using this, as the [vinyl library discusses here](https://github.com/VinylRecords/Vinyl/blob/master/tests/Intro.lhs#L118). - You can also say things like "if all the types of this type-level list satisfy a typeclass, then do...", as I showed in this tweet: [vinyl + postgres in one tweet](https://twitter.com/aaronmblevin/status/553628621382750208). This says: if all the types in the type-level list can be made into a `Field`, then we can create a list of database `Action`s. Being able to do this over arbitrarily sized tuples would be hard. - you can get close to something called row-polymorphism, which is used in [purescript](https://github.com/purescript/purescript/wiki/Language-Guide:-Types#row-polymorphism). This allows you to say things like: "I want a function that will accept any type, so long as it has a field of type `String` with the name `name`). You can think about representing this as a type-level lookup, ala `'[Int, String, Bool].contains(Int)`. Writing a generic `contains` method for arbitrarily sized tuples would be hard. There's lots of applications. Don't worry if this isn't working its way int your mind yet. It took me a while for it all to click. Your intuition that tuples are very close to heterogeneous lists *is* correct. However, when you want to start doing type-level programming, lists are much easier to deal with than tuples!
This is awesome news, I had several projects that have a copy of `wl-pprint`(it's a single file), now I can remove those.
Speaking of porting imperative algorithms to functional setting, I'm wondering if we have functional and efficient algorithms for graph operations. I need: 1. A heuristic based one, similar to A*. 2. Something that's effectively Dijkstra's. 3. Something that's effectively Floyd-Warshall.
This looks really useful. I especially like the semantics of `_ =&gt; type` ... "under some constraints".
But why not just implement instance as: ``` instance MonadThrow (Either SomeException) where ``` ? EDIT: Is it because we don't want other instances of `MonadThrow (Either a)` for any `a`?
Don't lists already act as streams in haskell? The n-ary nature of this function is neat, but I don't understand why we can't just do the normal thing of converting to lists, processing, and then converting back?
Syntactic overhead, mental load, maintainability (switching from String to ByteString or Text becomes more painful if you have to convert explicitly; OverloadedStrings doesn't cover all the scenarios).
It scrapes web pages for a table of contents for serial fiction. I had to use different approaches for sites with links and those that use dropdown menus, especially because the HTML is not well formed on fanfiction.net. I threw it all in a pull request hoping I could get some code review from more experienced Haskellers. 
Now its time to write the (-&gt;) instance :-)
`swap` lives in a much larger context: http://hackage.haskell.org/package/categories-1.0.7/docs/Control-Category-Braided.html 
 Found hole `_' with type: Map CoWorker Editor -&gt; Map CoWorker Editor That language sounds a bit weird. The hole doesn't have a type... it just *is* one.
Thank you very much for pointing that out! I’ll study `Braided` then /u/edwardkmett.
Huh, you're right, I was thinking of do notation and thought it had to be the same. That seemed very inconsistent at first but then I realized `value &gt;&gt;= f &gt;&gt;= g` has to be left-associative or it wouldn't work.
I'm struggling to understand your rather enigmatic "No result is know AFAIK in lazy setting". I sincerely hope it's not just a missing "n"! :D EDIT: Just to be not-totally-content-free, I suppose I should add: As far as I understand it lazy-vs-strict is only really relevant in non-total settings, right? I say this only because I sense that there's a tacit assumption in the Haskell community that you want your programs to be total. (Of course, I'm ignoring the whole thing about a more disciplined approach to totality by having enforced totality and using data/codata to achieve "infinite" OS-like behaviors without giving up totality.)
(EDIT: Sorry, removed EDIT markers and rewrote bits, just to make more cohesive post. Sorry if you commented in the mean time.) Entirely true and irrelevant from a complexity perspective! :) Yes, log_32 is effectively constant, but it's not about that. You could argue that mutable arrays in modern RAM are actually logarithmic if you're doing random access (hint: How do chips find the address you're looking for?). If you're really trying to optimize for absolute performance you should be aiming for linear/sequential *access*, because that can actually speed up your (sequential) processing time by factors that cannot be overcome by algorithmic improvements (unless we're talking exponentials). The point being that hardware can *predict* things if you're doing linear traversals and can actually start *prefetching* things you're going to need a few nanoseconds from now! ... which is massively faster than trying to predict random heap pointers. I believe that the prefetching in modern processors is even good enough to even detect things such as "strided" access simply because it's so prevalent in OpenMP-like code. It may not be obvious to everyone following along, but the point of this is that **bandwidth *hugely* exceeds latency** in the hardware world at the moment. That's why prefetching and aggressive caching strategies will almost always win against "algorithmic complexity" (up to a given, realistic, n) -- in practical terms, at this point in time. I guess my point is basically: Do *predictable* RAM access, and your life will be good. AFAIK this is still an unsolved problem in FP, but I believe that it *could* be solved with enough effort.
Exactly! I should have expounded on the whole myth of O(1) arrays, but I didn't, sadly. I should really have remembered to because I have been lectured *at* (loudly!) about these very issues earlier in my researches into CS.
I wouldn't be surprised if it could be proven that every generalized construct lives in at least one of your packages, lol.
That's a very interesting post! I haven't read it in detail (or necessarily fully understood it -- slightly tipsy at the moment), but it seems to rely on the assumption that lazy evaluation can "do things under the hood" which are not pure. So, I'm getting the sense that lazy evaluation could (*very* theoretically!) actually end up being as fast as an "impure" RAM machine given a language which was lazy, but was ultimately destined to be evaluated on an "impure" RAM machine?
I'm not sure the semantics of what you suggest is as clear-cut as you say. For example, is `IntAnd -&gt; IntAnd -&gt; Bool` the same as `(Int,_) -&gt; (Int,_) -&gt; Bool` or `(Int,_x) -&gt; (Int,_x) -&gt; Bool`? Also, if I have `type T = _a`, then is `T -&gt; _a -&gt; Bool` the same as `_a -&gt; _a -&gt; Bool` or `_a1 -&gt; _a2 -&gt; Bool`? Also, this is probably personal, but my intuition about the semantics of a type synonym is more semantic than just textual substitution. I tend to see it rather as defining another name that refers to the same semantic type.
Great point! Semantically, it doesn't matter if the underlying machine is doing actual mutations under the hood, as long as they're not observable from the semantics of the language you're programming in. Still, how to achieve that (in general), I think is still an unsolved problem! :) Just curious: Have you used LVars for anything serious? I saw a presentation by one of the authors -- can't recall her name ATM. It looked really intriguing.
Brian Hurt gave a nice talk on implementing doubly linked lists which touches on imperative algos in functional languages: https://www.youtube.com/watch?v=-HZ4bo_USvE
Any laws you can think of? :) I would suggest `swap . swap = id` but it looks like the Map instance breaks it. 
I too think that should be a law.
It does not rely on that assumption. The point is it's still an open question for lazily evaluated pure languages. But eagerly evaluated languages are *known* to be slower for some problems.
That haddock page is what sold me on the awesomeness of Haskell. True story.
&gt; Not all logs are created equal. In big-O notation, they very much are, and the research papers are generally very careful not to claim O(1) but rather "effectively" or "practically" constant times. Big-O notation hides a constant factor, so O(2n) = O(n) = O(n/2). But, logarithms are intra-convertable via constant factors log_2(n) = log_2(32) * log_32(n) = 5 * log_32(n). Therefore O(lg n) = O(log n) = O(log_32(n)) and O(1) != O(log_32(n)).
&gt; If you're really trying to optimize for absolute performance you should be aiming for linear/sequential access Using a [cache-oblivious algorithm](http://en.wikipedia.org/wiki/Cache-oblivious_algorithm) is sufficient. I've also heard that concise (?) data structures are valuable here, but I couldn't find I good reference for those.
&gt; Something that's effectively Floyd-Warshall. I like [this general shortest path](http://r6.ca/blog/20110808T035622Z.html) which is a generalization of Floyd-Warshall (shortest path). Dijkstras and A* really just need a loop over a priority queue, so they are relatively easy handle in a functional setting. But, with some [efficient matrix multiplication](http://en.wikipedia.org/wiki/Cache-oblivious_matrix_multiplication), I think you can get G-J-F-W-M-Y down to around the same big-O runtime as A* / Dijkstras. Of course, same runtime doesn't mean same performance, so bi-directional A* or JPS+ can still be much faster.
For sparse graphs, Dijkstra's and A* should be pretty straightforward to implement with a `Map Vertex (Set Edge)` adjacency list, `Map Vertex Int` for storing distances, and `Set (Int, Vertex)` for the priority queue. Time complexity should be O(ElogV), I think? Edit: The adjacency list could obviously be an Array/Vector, no need to use a Map.
Linky? :)
You sure that's the instance you want for tuples? Why not using an irrefutable pattern and make it a bit more lazy. &gt;:)
My Google-fu tells me it's (Gauss-Jordan-)Floyd-Warshall(-McNaughton-Yamada).
I've been using https://hackage.haskell.org/package/wl-pprint-text which is a port that uses text and claims to have some extensions. as well as being actively maintained, which is why I picked it. any reason not to just use that? also, discussion on different libraries: http://stackoverflow.com/questions/9761507/which-pretty-print-library btw, thank you for taking ownership, just wanted to provide some background, which you may have already seen.
Oh, it's right at the beginning of the article. *blushes*
I wouldn't think that `Map` ought to be swappable. That operation is certainly something different...
A typeclass without any methods? Edward Kmett not practicing what he preaches! :O More seriously: why do you consider this law-only typeclass OK, while generally advising against it?
I'd have concerns about reading that kind of code. If I saw f :: IntAnd -&gt; IntAnd -&gt; Bool I'd be quite surprised when `f x y` typechecked but `f x x` didn't.
Of course I was missing an 'n'. Sorry about it. I _think_ that the total vs partial debate does not matter here. Afaict in a total language the _results_ of functions does not depend on evaluation strategy. The algorithmic complexity might. 
I read ($ x) as "of x"
&gt; myth of O(1) arrays This is new to me, are you talking about this? http://www.ilikebigbits.com/blog/2014/4/29/the-myth-of-ram-part-iii (I just googled up this article, and learnt something new)
I wonder if HM is missing the general concept of "unspecified type".
&gt; Is there a better way for me to submit this than just under the FutureIO moniker? If you think your contribution belongs to to async, perhaps you could send them a [pull request](https://github.com/simonmar/async/pulls)? This way, either your idea is great and it gets incorporated in the official package and everybody benefits, or there is a non-obvious flaw in your proposal, the official maintainers points it out, and you learn.
&gt; I just wonder if I would go so far as to say that "ad hoc overloading" is a bad idea generally. I want to echo this sentiment. Type classes seem to work best when there is a "sufficiently unique" instance so that coherence makes good sense. But, when you have that, it's hard to see the overloading as "ad-hoc"; the overloading is quite principled--it's just not entirely parametric. This will not prevent me from abusing lenses and type classes for sane record update syntax. But, I'll be hoping for something better in the future. (Honestly, some sort of implicits system that is more powerful that `ImplicitParameters` but not quite as confusing as Scala's implicits would be welcomed, at least by me.)
You're welcome!
Here's another quick question if you wouldn't mind: is it worth much that my module compiles Safe instead of Trustworthy? Is that worth attempting a pull req?
In this case, the implementation of `Concurrently` in `async` is not using unsafe primitives. It's the other sections of the module that might be unsafe. Also, if you are using `async` indirectly, then you are intrinsically relying on the `Trustworthy` annotation on `async` being correct anyway.
&gt; our Mu compiler Does it have some Github or documentation?
Anyone can hack into hackage.org and mutate anything. What do you want me to explain? How will you detect that someone changes information on hackage.org?
It hit me as well, pretty curious stuff.
Yeah, that's one of the issues with the Haskell language which is that most language features are actually libraries, which makes it hard to just do a "tour of the language". You just have to familiarize yourself with the library ecosystem.
the law comes with a Rule, maybe that's why.
Someone worked it out through [equational reasoning](http://stackoverflow.com/questions/24481648/quickcheck-gen-is-not-a-monad). You can also write a QuickCheck property for the monad laws, and it will reliably fail when you run it on Gen itself. There's an explanation in the link on why Gen is good enough on the monad laws even if if actually breaks them. They could have written Gen as a simple state transformer, which would easily satisfy the monad laws, but this would not allow an easy way to generate random functions. This is explained in section 3.3 of the [QuickCheck paper](http://www.eecs.northwestern.edu/~robby/courses/395-495-2009-fall/quick.pdf).
Basic HM does not have type signatures at all. Once you have type signatures, it's not hard to add unspecified types. You just replace the unspecified type by s unification variable. Then there are lots of details, since some types you write can have them and some can not. 
A paper at least?
Nope. You can hear a little about it in this talk http://m.youtube.com/watch?v=hgOzYZDrXL0
Yes. And by iteration I meant a step in the initializing list that generates an index-element pair. ~~Although as far as I know, you can't have an element earlier in the list refer to an element later in the list since items later in the list have not yet been initialized. Although it is possible to initialze an array with a list of decreasing indicies.~~ As you initialize elements keep in mind that every element that "depends" on another element, and you must not initialize such that there are circular dependencies. (EDIT: correction, clarification)
What's this `~` operator? While we're on about that, what's `:-&gt;` and `~&gt;` mean, too? Seen all of 'em, understand none of 'em...
Perhaps my use of `listArray` instead of `array` obscured my point: it *is* possible for an element earlier in the list to refer to an element later in the list. Here are my examples again, with `array` instead of `listArray`: &gt; let a1 = array (0,1) [(0, 42), (1, a1!0 + 1)] in a1 array (0,1) [(0,42),(1,43)] &gt; let a2 = array (0,1) [(0, a2!1 + 1), (1, 42)] in a2 array (0,1) [(0,43),(1,42)] &gt; let a3 = array (0,1) [(0, a3!1), (1, a3!0)] in a3 &lt;&lt;loop&gt;&gt; 
You'll notice I haven't updated this package in a long time. =) 
I keep meaning to go back and add a ribbon categories.
Blessing and a curse. The base language was incredibly easy for me to pick up over the last year (I've even shoved it's foot through the door at my work in a few projects) but the ecosystem is vast with enlist about every abstraction I could think of without a super easy way for me to find it. 
Yay! [This](http://www.amazon.com/Functorial-Knot-Theory-Categorical-Deformations/dp/9810244436/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1429067377&amp;sr=1-1&amp;keywords=Functorial+Knot+Theory+%3A+Categories+of+Tangles%2C+Coherence%2C+Categorical+Deformations+and+Topological+Invariants+David+N.+Yetter) is one of my favorite books on the subject. &gt; One evening at a Joint Summer Research Conference in the early 1990's Nicholai Reshetikhin and I button-holed Flato, and explained at length Shum's coherence theorem and the role of categories in "quantum knot invariants". Flato was persistently dismissive of categories as a "mere language". I retired for the evening, leaving Reshetikhin and Flato to the discussion. At the next morning's session, Flato tapped me on the shoulder, and, giving a thumbs-up sign, whispered, "Hey! Viva les categories! These new ones, the braided monoidal ones." - David Yetter
How about the `hide` option? My phone is misbehaving, so I can't post the link. Google for "haddock options hide". That should land you at the Module Attributes section of the Haddock manual.
Oh, for some reason I didn't think this got into 7.10. Awesome! I think this will allow me to drop a use of NoMonomorphismRestriction in a weird case where I'm composing with a function that returns a private type...
Any feedback is greatly appreciated, even if it's as simple as syntactic style. I have very little experience writing Haskell code.
It doesn't help; "a" is still not shown. As far as I can tell, Haddock treats "other-modules" as "hide". Doing either one or both of these has the same effect. If I make both modules exposed, and don't use "hide", I can of course see them both in haddock. But the documentation for "Public" has no reference to "module Private" like there should be. I think this is just a bug in haddock. It can't deal with the "module Public(module Public)" thing, or something.
I found a workaround that's not too bad: module Public(module Public,module Private) where import Private b :: Bool b = False 
`~` is type equality. `:-&gt;` and `~&gt;` are just type operators, and are whatever the author defines them as (using the TypeOperators pragma). I think there might be an informal convention for `:-&gt;`. I almost always see it used as `type m :-&gt; n = forall a. m a -&gt; n a`, which is frequently useful. For instance, a higher order analogue of `fmap :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b` is `hfmap :: HFunctor f =&gt; (a :-&gt; b) -&gt; (f a :-&gt; f b)`. [This link](http://www.timphilipwilliams.com/posts/2013-01-16-fixing-gadts.html) gives some examples of the type `forall a. m a -&gt; n a`, though they use `:~&gt;` as the operator.
I'm not sure if you're reading it exactly right -- it's seeking then reading, not reading then seeking. But you're right in that it's inefficient - specifically in re-statting and re-opening the file. I think I could do this without statting and opening the file every time around, but rather just continuously trying to read the same file descriptor. But then I would not be able to extend to "tail -F" semantics, which sadly requires constantly statting the file (I have a comment on the gist that describes some of this).
I love you guys!
Documentation is only generated for 'public' functions and modules, that is documented and expected behaviour for nearly every doctool I'm aware off. Why have some functions or modules poluting the documentation, when the reader isn't even able to use them? In some other tools, you have the option to choose between internal and public doc, where the internal also shows private items, but I don't know if haddock is able to do that. 
They *are* public functions, so they *should* be documented. That's the point.
By the same argument, type signatures in general are only sugar, I guess.
or at least non-polymorphic ones perhaps...
Cool stuff, would be interesting to see what some of these other tools you guys had in mind. We've been using LTS very successfully for the past few months and stackage before that, they've been super useful in making building our apps painless. Compile times are still a PITA for yesod / persistent-backed apps (I'm a fan of rebuilding everything from scratch when it's time to de, but supposedly halcyon helps with it, will need to investigate. What's the use case of the web IDE though? Is there a specific demographic that would benefit from it? e.g. if you invested many years into the command line, vim / emacs, plugins etc, have a certain flow you're used to, would you gain large benefits from it?
Hmm... Okay I can see now that you are re-exporting Private... Yeah, then I would consider this a reportable bug. 
Must have missed the stackage-view post, that is awesome!
&gt; Distribution of binary package databases to your whole team, avoiding recompile time and ensuring consistent environments Awesome!
 f :: IntAnd -&gt; IntAnd -&gt; Bool f (a, 0) (b, '0') = True f _ _ = False then `f (5, 0) (1, '0')` typechecks but `f (5, 0) (5, 0)` doesn't. 
I wouldn't consider `(Int, _)` to be closed. Its meaning depends on the context it's used in.
You can use the HInotify package(and the Inotify feature of linux) instead of waiting in a loop. I've used it for the exact same purpose here(old code so I can't guarantee that it's bug free or elegant but it worked for me): https://gist.github.com/erdeszt/9861706
Seems like a good idea :-) In any case, my plate is far more than full at the moment, so it will be a while before I get to doing more radical things in `idris-mode`.
Here's something similar I've done with the `linux-inotify` and `io-streams` packages, though the particular semantics used here actually continues to follow a log file after it's been rotated. &lt;https://gist.github.com/lpsmith/10625b629fbf7dd1afbd&gt; This is also code that worked for me, not code that I wrote to be as robust as I could make it. One issue in particular arises from the fact that standard loggers and log rotation mechanisms may end up writing to a log file for a period of time after it has been rotated. This follower might miss any such writes to the logfile. But I've mostly used it in situations where I can guarantee that doesn't happen.
Many non lazy languages I come across implement both 'and' &amp; 'or' lazily. Often these are overlooked and I feel like these shouldn't be selling points.
Nope, and that's a pretty good indicator...
Is the title a pun? Incomplete -&gt; lazy.
&gt; This behavior is known as short-circuit evaluation and also available in many other programming languages. &gt; &gt; However, let us now consider a function and that implements a logical AND which operates not just on two values, but on a whole list of values. As I try to explain in the tutorial, the thing about lazy evaluation is that it is not limited to certain special operations that are hard-coded into the language.
I think the idea is that you can only see packages that are currently in Stackage. ~~That said, I'm not sure why this couldn't be made to work with Hackage, proper.~~ Reproducibility of builds is probably a requirement that causes this not to be doable with Hackage Proper. The next closest thing would probably be Nix + Hackage, but I doubt much is gained using that over this, especially since you'd still have to write the nixpkgs for packages.
We already open sourced ide-backend, which is a good basis for IDE-like tools, and we're providing lots of Stackage and LTS-based tooling, which I think plays nicely with an IDE. We also just opened sourced stackage-view, which could arguably be called "IDE-like." But if the question is, "are we working on an IDE?" the answer is no. "Commercial grade" is intended to mean high quality, and specifically targeted at commercial needs, such as working nicely with a large team and playing well with the full process from dev, to QA, to the CI build system and finally production. All of the currently planned tooling we'll be releasing will be both "free as in speech" and "free as in beer." While I can't rule out the possibility of some for-sale components in the future, that's not our intention right now.
When we started on the web-based IDE, we had a lot of companies tell us that it's what they wanted. The target audience would theoretically be: * Companies starting a new project with Haskell * No established tooling * Lots of developers who are relatively new to Haskell * Desire to train up quickly and start producing code While there are certainly companies who have found the web-based IDE hits that sweet spot for them, the larger number of companies we work with and interact with in the community tend to be past that point, where they have existing projects with other tools. Migrating to a web based IDE doesn't make sense for them, which is why we've been working on tooling that fits those workflows better.
I'm a bit disappointed to see the haskell center go, because I just started using it productively on my chromebook :p
I don't understand how to design Haskell programs anymore. How do I figure out if something should be a regular type like Either, or a type class like Swap? Is there a way to remove the distinction? This reminds me of /u/neelk's [complaint about Foldable](http://www.reddit.com/r/haskell/comments/30s1t2/proposal_make_semigroup_as_a_superclass_of_monoid/cpver7e): "it looks equivalent to a class with a toList method, but gratuitously higher-order".
I agree with this. Using a fully qualified name seems like the best route to go.
If you want a more detailed discussion, I would also recommend Simon Peyton Jones's [Implementing Functional Languages: A Tutorial](http://research.microsoft.com/en-us/um/people/simonpj/Papers/pj-lester-book/). The PDF is freely available. It goes into the details of how to implement lazy graph reduction and, even better, how to shift work from runtime to compile time. I haven't finished it yet, but so far, it's been very eye-opening for me.
If Cabal-1.22 isn't registered (`ghc-pkg list cabal`), the `cabal-helper-wrapper` will install a private copy because it needs the library installed to work, but it doesn't want to stomp on anything you have that might break by upgrading Cabal directly. GHC for Mac OS X doesn't register the `Cabal` library, it comes bundled with a version of `cabal-install` built against it. EDIT: Put this as an answer, as well.
That pdf is great. Read along, do the exercises, and before you realize it you have implemented a machine for evaluating haskell. It also tricks you into writing down some of the evaluation rules. In other words, it doesn't just hold your hand but it is also nicely paced. Pair that with typing haskell in haskell and you can find out first hand how evaluation and type checking work in haskell. Which is great because it demystifies so much and both documents are accessible to beginning haskellers.
So it seems the poster needs to `cabal install cabal-1.22.2.0` or `ghc-pkg register`. To ghc-pkg register you need the output of `ghc-pkg discribe {name of package}` according to this [post](http://stackoverflow.com/questions/14345571/reregister-existing-package-with-ghc-pkg). I will just link a pastbin of what I get for `ghc-pkg discribe Cabal-1.22.2.0`. It seems the issue was caused because cabal tried to install cabal-1.22.0.0 which is what cabal-install was built against. However, it seems that this will break ghc-7.10.1 and Cabal-1.22.2.0 (according to the error). So maybe he has cabal-1.22.2.0 installed but not registered? I will include what you have said in your post and ask him to try `cabal install cabal-1.22.2.0`.
You guys are amazing. 
That will probably break things in sandbox development. I would just let `cabal-helper-wrapper` install its private `Cabal` and not worry about it.
How pure? Does it have non-termination? If so, it breaks in the same way since (let _ = loop_forever () in fun x -&gt; x) loops forever, while \y -&gt; (let _ = loop_forever () in fun x -&gt; x) y does not. If you really mean super-duper pure (e.g., pure System F or something) then yes, eta holds. 
I'm planning on using it for an online documentation hub for PureScript - kind of like Hackage, but for PureScript packages (which use Bower).
Thanks for this report MusicalWatermelon! We are trying to make haskell-mode as user friendly as possible and this is current project goal. haskell-mode has a lot of functionality that is currently a bit hidden because it is either not key-bound or lacks documentation or requires special configuration to enable it. Our goal is that haskell-mode does not require any 'tutorial' or anything like that to get started (it just works) and the more advance features are discoverable from within haskell-mode. As the work is not going to do itself we kindly ask for help. MusicalWatermelon, when you are done can you describe issues and solutions you found in the wiki on our project site https://github.com/haskell/haskell-mode? Do not hesitate to report issues, we try to do best we can to fix them as soon as resources permit. We have also have an active IRC channel: #haskell-emacs at irc.freenode.net.
If you think of `(Int, _)` as a type it depends on context, but it's not quite a type.
If it's not quite a type then `type IntAnd = (Int, _)` seems like a strange thing to write :-)
Yeah, it'd be great to use a filesystem notification thing, but I also run my code on my mac. I wonder if there's a haskell library that works for both OS X-based watch facilities and linux. Edit: nice! https://hackage.haskell.org/package/fsnotify has anyone tried it?
Thanks, I wasn't aware of Typing Haskell in Haskell. For people following along at home, [here's the link](http://web.cecs.pdx.edu/~mpj/thih/).
Yes, I just enable 'all for the layers and it ust works.
Yeah, this is a bit confusing, because the point of this only comes after a few examples: it's not that `&amp;&amp;` is lazy, it's that `zipWith` also is...
Does short circuiting count as laziness? 
Is there any chance that the excellent folk at FP Complete might provide binary package database versions of LTS Stackage for common platforms? Imagine simply _downloading_ a consistent set of packages, with a browsable set of documentation thanks to stackage-view! Mmmm. 
That's a tricky case, I'm not 100% sure myself. My reasoning is that we want (full) normal forms to be unique, but a cyclic, infinite list has several different, but equivalent representations that have no redexes. Note that ordinary lambda calculus does not keep track of sharing, so it has no cycles -- the expression has no normal form in this case.
Why not just stick with Sublime? It's a perfectly good editor and integrates nicely with haskell.
Umm, "don't try to do the hard stuff with the latest official tool designed to do the hard stuff, supposedly". XD So what is one supposed to do, then? Is it a problem with .10.1 specifically? So 7.8.4 comes recommended, then?
Also not a type `type A a = (Eq a, Num a)`.
Perhaps I replied to the wrong comment, apologies if so! However, my point even stands with regard to short-circuiting operators like `&amp;&amp;`, etc. Any language with references and closures can implement its own thunks, and you can do stuff like this whenever you please, without waiting for the language designer to do it for you. (Not applicable to C, I'll grant.) Now, the roll-your-own thunks approach is not a full solution to getting in a strict language what you get in a language with pervasive laziness, but this kind of kid stuff (conditionals, short-circuiting operators, etc) is totally doable and not even worth arguing about. EDIT: The fact that a version of `&amp;&amp;` or `||` which always evaluates both its arguments is not desirable is an indication that we should be thinking of these things at the level of patterns, *not* functions. It sort of coincides nicely for a lazy language, and but I consider the short-circuiting versions of these operators in a strict language to also be nothing more than a notational convenience, in precisely the same sense as `if`. When you begin to consider fine-grained type structure &amp; effects, the fundamental differences between things that were convenient to conflate before become evident.
Can't you just quote the names with `'` and `''`? Then your users won't need to import them. For example, from `product-profunctors`: defaultPredOfVar fn = (''Default, [varTS "p", mkTySuffix "0" fn, mkTySuffix "1" fn]) ... defsN = replicate numConVars (VarE 'def) Users of the TH splices don't need to import `Default` or `def` themselves. (I used to force users to import those symbols but I realised that it must not have been necessary because the lens TH didn't force users to import anything. Then I discovered this mechanism.)
So much this. The best part is how well organized and discoverable the menu system is. I've used Vim for nearly 20 years; had made two failed attempts with Emacs using Evil for the Haskell support - have now been using Spacemacs a month and love it. I did have to overcome a couple challenges but the support on gitter is fantastic.
I more or less agree with you. There are certainly practical issues with how to make and call thunks, but that is really a syntax issue and not a semantic issue. The real power of laziness lies in the ability to tie-the-knot. Until you are doing that, you aren't really using laziness. ``You can only build a cyclic immutable strict data structure of a shape that's determined at compile time. -- http://stackoverflow.com/questions/5810163/creating-a-doubly-linked-list-from-a-list-in-ocaml
&gt; The syntax is not as pretty of course, but at this point the only dispute at hand is one over concrete syntax, and I don't think that's worth arguing about. [Syntax matters](http://c2.com/cgi/wiki?SyntaxMatters)
I think I agree 100%.
As I mentioned: &gt; Noting, of course, that it's a bad idea to use mutable variables as default arguments in Python, hence the None check at all. Context: http://docs.python-guide.org/en/latest/writing/gotchas/#mutable-default-arguments In other words, default values should be immutable. 
I didn't know None and [] had different mutability.
How do you guys make money? (Are you making money?) Honest question.
How would that be more concise?
Ok not concise, but maybe more clear? It'd help me distinguish faster, idk, I'm slow :/
I can see your point. I guess some kind of syntax highlight would help if it applies different colors to types and kinds.
Err on the side of making things "plain" data and functions. You only need functions to abstract over functions and data; you need "craziness" like ConstraintKinds to abstract over type classes and their instances.
I've found tying-the-knot to be hard to reason about, likely to lead to infinite loops, and hard to debug when it does... am I wrong?
Oh, and shout out to my cousin who writes great short stories in series: https://www.patreon.com/dreamscapes
And I'm super excited for all of that :D The better the Haskell toolset gets, the more likely I can convince management that it's a good idea to use on new projects. Thanks for the hard work!
Thank You for writing these wonderful tutorials. 
I remember someone writing that soon GHC will have &gt; * :: * which I guess means that kinds are types too?
Hopefully with the open sourcing of the back end, another will work its way into the community
You may want to have a look at Spacemacs. I was also a bit skeptical but found the "layers" system to be reasonably effective at packaging up configuration in digestible chunks. Moreover, the defaults interact quite well, something I was never quite able to achieve with my own configuration.
We're talking about getting something like that going with other members of the community. In fact, the Commercial Haskell SIG (Special Interest Group) has a tooling task force devoted to these kinds of things: https://github.com/commercialhaskell/commercialhaskell/blob/master/taskforce/tooling.md It's still pretty young, so now's a great time to get involved and help push it forward.
We are a profitable company. I'll leave it to the discretion of others to determine how much of our business dealings we feel comfortable disclosing publicly. (And it was surprisingly hard to resist the temptation to making up some absurd revenue stream.) __EDIT__ Aaron, our CEO, has [provided more details](http://www.reddit.com/r/haskell/comments/32njog/the_future_of_school_of_haskell_and_fp_haskell/cqeqxo6).
Spacemacs is the first haskell IDE where everything just works for me out of the box with zero configuration. And spacemacs' keybindings for general thins (open, save file, etc.) are very nice too. Takes a day or two to get accustomed.
This solved my problem, some more googling also gave this discussion of the same question: http://comments.gmane.org/gmane.comp.lang.haskell.glasgow.user/25495
The paper is here: http://dept.cs.williams.edu/~byorgey/pub/type-matrices.pdf
**TL;DR:** We’re implementing a system to significantly improve Hackage security. It’s based on a sensible design ([The Update Framework](http://theupdateframework.com/)) by proper crypto experts. The basic system is fully automatic and covers all packages on Hackage. A proposed extension would give further security improvements for individual packages at the cost of a modest effort from package authors. It will also allow the secure use of untrusted public Hackage mirrors, which is the simplest route to better Hackage reliability. As a bonus we’re including incremental index downloads to reduce cabal update wait times. And it’s all fully backwards compatible.
The Polish government secretly pays you every time there's a new comonad declared on github?
Do you mean that I've got the details wrong (I didn't actually test it) or that it's bad that it happens?
 Spacemacs is completely keyboard driven, but if you don't immediately complete a command sequence a window opens on the right side to show you available choices for the sequence you've started.
I dabbled with diff contextization recently, so I may say that it worth trying to make explicit more intermediate data structures that imperative algorithms squash into a mutable one. GHC should optimize and fuse many different maps/filters into one low-level loop reasonably well. Just make intermediate structures explicit, though it is somewhat painful in its own way compared to imperative state management: sometimes it's easy to forget what some intermediate structure does.
It might be of interest to go through which of the TUF-identified threats your proposal mitigates/doesn't mitigate and why/how.
Ah, *that* menu. OK, that sounds pretty cool.
True. There's a fair amount of tooling available around git. In particular, it's possible to be selective about the depth of the history your'e interested in (at no extra effort on our part), using the equivalent of `git clone --depth`.
Copying from the email thread, as that thread seems to be focused on incremental updates and not the meat of the TUF-based proposal: * * * I've read the blog post, and am still trying to understand the implications of TUF. However, it's incredibly difficult to give solid review of a system which is stated to be "based on TUF," without knowing what the delta implied by that is. For now, I can only ask simple questions: 1. Is there any timeline for the changes needed to Hackage and cabal-install? 2. Is there any idea of how much extra code will need to be maintained going forward for this? This is an important point, given that both Hackage and Cabal are having trouble keeping up with demand already. 3. Is there any mitigation of eavesdropping attacks on the authorization headers sent to Hackage by uploaders for digest authentication? 4. Is there any mitigation against a compromise of the Hackage server itself, either the code base or the system? Overall, I'm quite wary of a solution stated as "experts devised this, it's good, we'll implement it, everyone stop worrying." I take your point about crypto-humility, but I'm not confident that an approach based on TUF addresses that concern since it involves a new implementation (or copy-pasted implementation) of crypto primitives together with unspecified changes to TUF. Note: wary is *not* a code word for opposed, but I strongly believe that anything we do here warrants far more discussion, and that can't happen until more details are explained.
To unpack that a little :), the current 00-index.tar.gz already contains every cabal file for every version of a package. But, they're all in the one gzip file so they compress quite well. If the tarball is switched to append-only, the gzip compression from previous chunks won't carry over to a new chunk without at least some fancy, non-standard tricks.
&gt; concurrent/reactive programming Those two are very different. As a fan of FRP, I think it would be a good fit for your project, but only if you're interested in learning FRP. If you're more interested in finishing your tic-tac-toe project than in learning new ways of organizing your event-handling code, an imperative implementation would do just fine. Especially given that your only source of event is a single button, so you won't end up with a tangled mess of callbacks. As for concurrency, I really don't see why you would need it in this project. Surely `hArduino` allows you to poll the button without having to block the current thread? And turning an LED on or off surely can't be a blocking operation either? &gt; What kind of strategy and library is suit to this problem? While the hardware-oriented nature of your project makes it stand out, the overall structure of your problem doesn't sound particularly different from the more common situation of a game in which you'd be polling the keyboard instead of the button input and displaying to the screen instead of LEDs. If going for an imperative solution, I would run a game loop in which I would poll the button at the beginning of every loop, update the state in response to that input, and "draw the next frame" by sending the appropriate hArduino commands to turn the LEDs on and off to match the new desired LED pattern. If going for an FRP solution, I would have the same loop, but instead of updating a state I would feed the events into an FRP network producing a `Behavior LedPattern`, which I would then use to update the LEDs. As for which library: for FRP, I usually recommend reactive-banana, but any other FRP library should work too. You can find more information about the differences between the different FRP libraries in my [FRP Zoo](https://github.com/gelisam/frp-zoo#readme) project. You have many other questions, but this time I think I'll wait for feedback before continuing with my usual big-wall-of-text style of answer :)
Oh, I see what you are saying. You are saying there can be a function f :: IntAnd -&gt; IntAnd -&gt; Bool for which g :: IntAnd -&gt; Bool g x = f x x fails to typecheck. Yes, that is indeed strange behaviour. Sorry, looking back on the thread it is completely clear what you were trying to express, but I just didn't grasp it.
How is the public key of the timestamp signature distributed in such a system? As I understand it, you still need the public key to actually verify the signature. Also, what are the reasons that such a system could not use GPG? I have not used GPG much, but it seems to me that those trust chains could easily be created in GPG (you just need the root public key trusted by GPG, all the other keys could be signed by the root key and therefore trusted by GPG) without requiring a WoT and it would avoid having to implement our own signature checking.
That's right - I think your `g` expresses the point more clearly than I did.
Thanks for making me laugh.
&gt; if you browse their website and you come across something you can't start using unless you've emailed sales@fpcomplete.com, that's a revenue stream. That's a *potential* revenue stream. It's also important to know (roughly and proportionally) how much they make from each potential source.
Those aren't really type functions; they're normal typeclasses designed to provide specific vinyl functionality. `(∈)`, for instance, is a specialized synonym for `RElem`, which is "really" a provider for lenses into a `Rec`. You might find the [`Witnesses` module from earlier versions](https://hackage.haskell.org/package/vinyl-0.3/docs/Data-Vinyl-Witnesses.html) more relevant. The `Implicit` class will give you a GADT witness into elements and subsets of a list. E.g. `implicit :: Elem Z '[X, Y, Z]` will give you something along the lines of `There (There (Here))`, showing you how to get to the `Z` element. It works differently than closed type families, though. (Closed type families weren't available yet.)
As a first approximation of this, I tend to just use the highlighting search functionality of emacs.
Having interactive and extensible interfaces is IMO one of the next challenges of programming language design. You can have a look at interactive modes for dependently typed languages / proof assistants (Coq, Agda, Idris, Isabelle...). Without interactive editing, programming in these languages would be almost impossible. http://peaker.github.io/lamdu/ is an interesting project : the interface is really well done. However such an interface would need to be extensible to be useful, and this is quite difficult to do elegantly.
The signed tags sign the sha1 hash of the commit which includes the sha1 hash of the parent commit(s). So even without obtaining the parent commit(s) you can take the hash of the tagged commit and thus verify the signature. IOW, signed tags in git don't require history. (It's also possible and easy in git to create "orphan" branches that don't have history, so that people won't need to bother with --depth, although of course they won't get history.)
Funny that you mention vinyl because I'm actually working with vinyl right now. I had noticed that some of it's type-level list function are more general-purpose (like `++`) but that others like the RElem things are for more vinyl-specific purposes.
Mostly through engineering work for companies: building apps or customizing tool chains to interop with their infrastructure. That's why we can afford to give away a lot of core stuff. Give away tasty cake and some people choose to buy frosting. Those of you familiar with my background may know that I started my first "freemium" software business 30 years ago! Different software, of course.
Ah, the Dwarf Fortress model. Might have to try this, I've been dancing around the likely benefits of emacs for a while now.
 Voor point. You could try to make on of the mods aware of this..
&gt; Pervasive laziness gives you [code which is polymorphic in its strictness] for free How? It would be great if that was true, but since so many Haskell data structures and functions have both a strict and a lazy variant, it doesn't seem to be the case, at least not the way those libraries use the language. Tom Ellis and I spent a bit of time thinking about how it would be possible to implement [strictness polymorphic functions](http://h2.jaguarpaw.co.uk/posts/strictness-in-types/#polymorphic-strictness) in Haskell (which reminds me, I wanted to blog about an extension of this technique). As you can see, it needs a bit of legwork!
Given that I already had [a library for regular expressions in Agda](https://github.com/gallais/aGdaREP), I had a bit of [Fun(ctor)](https://gist.github.com/gallais/ab9ec22610d04d6f596d) and it leads to [cute examples](https://gist.github.com/gallais/67d6d3870a88f8dcc4db) in the simple case where I just check that the length of the word is even. Edit: here I'm constraining the values rather than their types but it'd be quite easy to adapt the definition.
Since you mentioned FRP and the arduino -&gt; https://github.com/frp-arduino/frp-arduino Here's the reddit post that put me in the know about this -&gt; https://www.reddit.com/r/haskell/comments/2wfdhr/frparduino_arduino_programming_without_the_hassle/
"Avoid success at all costs", which means that Haskell was successful. But, really, is it that important? 
These rankings are nothing to count on. As the Blub Paradox teaches us, you can't trust most programmers opinions on that.
Tiobe has always been considered really poor. Pretty much every other language popularity ranking lines up roughly with each other, and tiobe is vastly different. And some of the stuff in their list is clearly nonsense.
I am honestly confused how you could get them confused. 
Delphi &gt; Pascal &gt; Logo, Cobol, Postscript &gt; Scheme, Lisp, D, Ada, Fortran, Lua, Go, Erlang, Clojure, Haskell hmm....
Here's the thing, from my point of view. FP Complete are a company that make money from selling software. That's fine. FP Complete give away stuff for free. That's awesome. In particular, School of Haskell, FP Haskell Center and Stackage are awesome. My understanding is that the public-contribution stuff is all freely available and am having a hard time understanding where there's anything I should worry about here. I'm not particularly worried about Stack Overflow or GitHub either - two big players in the user-generated-content market. If some excellent Haskell folk make money from Haskell and from encouraging and assisting other folk to do the same, that's great, not sinister, and I _really_ don't need to see the company accounts.
Good point about them not being type functions. However, (I believe) you can achieve the same thing as `Implicit` using `∈`, as I do [here](https://gist.github.com/aaronlevin/8106e18b278f32ff4282#file-free-cqrs-hs-L31)
Tiobe ranks assembly higher than Go...
I love the idea of using TUF, but it does seem very strange that the code and associated materials (like notes and discussions) here aren't already published. Is that really true? What gives?
Correct me if I'm wrong, but that gives you one particular shape for a given RE, whereas the paper is about restricting the elements of a container that has a shape defined orthogonally to the RE.
I just read Friendship is Optimal on your recommendation. It was pretty awesome, I liked it. I'll check out blindsight next.
I use it because I have to convince GHC that the type that is quantified over (`c`, if that is the correct terminology) is actually contained in the list `[c]`. In a previous version I used the `Implicit` class (re-written), as can be seen [here](https://gist.github.com/aaronlevin/437ceac6acc344f86ef9#file-finally_crud-hs-L58). Swapping them out worked fine and I did try testing it and it wouldn't compile unless the elment was in the list (unless you mean something different when you say spurious `RElem` instances in scope).
It's not just poor. It's actively misleading for anyone who doesn't actually bother to give the methodology a second thought -- which I'd guess is *a lot of people*.
Proposal 1 and 2 share the second and third line while the first is modified to reverse the subtyping relationship. I would have found shared variable names and subtyping relationships with a modified "then..." line more readable. 
aren't supertypes called kinds in Haskell? If not what are the applications of subtypes in Haskell?
You should challenge yourself on this thinking. If the compiler happily accepts that you are, in fact, using `ST`, you are still programming functionally. You need to come up with some material gain in not doing sharing through destruction. For many algorithms, updating values in place is a great thing both on the readability and efficiency fronts, and if you're fighting hard to make it more "functional" then you should maybe step back a bit and think about why you are doing this.
It can't choose the type dynamically, but it can statically, e.g.: f :: (Int, _) -&gt; Int f (a, "0") = a f _ = 0 
Kinds are unrelated, as they are the 'type of types'. Limited forms of subtyping does exist in Haskell though, in the form of polymorphic specialization: e.g. `Int -&gt; Int` is a subtype of `a -&gt; a`.
In that ranking I find it strange that C is so far down. This might possibly be caused by lots of popular C libraries being mature at the time Github was introduced and staying on their old infrastructure.
Actually it is not. You can not use a function expecting an Int parameter in place of one able to handle parameters of any type. Nor can you use one only able to return an Int in place of one able to return any type.
They were referring to the (System F) “generic instance” or “subsumption” relation, usually spelled ⊑. It’s true that `Int -&gt; Int` does not unify with `forall a. a -&gt; a`, but `Int -&gt; Int` is an *instance* of `forall a. a -&gt; a`, because you can instantiate `a` to `Int` and obtain the type `Int -&gt; Int`, with which `Int -&gt; Int` unifies (trivially). Subsumption checking is largely the same as unification, except that function types are contravariant in their input type: `a -&gt; b` is a subtype of `c -&gt; d` if `a` is a *supertype* of `c` and `b` is a subtype of `d`. This relation is used to check type signatures: to check the type of `(e :: t)`, you infer the type of `e` and check that it’s subsumed by `t`. **Edit:** I had not read the article, which states some of this. 
While you bring up some valid points that I'm eagerly awaiting to see addressed by the TUF-proponents, I'm wondering why you're so fixated with Git. I'm starting to think that even if your concerns were all addressed (without using `git`), you'd still complain that the proposed solution doesn't actually run on Git. I'd rather see this discussed more abstracted, w/o tailoring the arguments from the start to require using `git`. There's threat-models, and we can surely discuss this w/o mentioning a concrete implementation. If it turns out that Git indeed covers all threat-models better than the alternatives proposed while outweighing its cons, then it would be rational to chose to build on top of `git`. But you seem to rather argue from the desire to use `git` as an end to itself.
Seems like Stackage is emancipating itself from Hackage more and more... I wonder what impression (or rather confusion) that gives to newcomers to see there's that thing called Hackage which uses one set of tooling, and then there's this thing "Stackage" everybody talks about, which seems to be an enhancement/subset/alternative/clone/??? to Hackage, and then big question which one should use, and why there's two such facilities to begin with rather than just one...
Haskell has no subtypes, for good reason. I'm just posting it here because /r/haskell might as well be called /r/functionalprogrammingtypetheoryandallthatjazz :)
This is a good point. I'd like to expand it a bit: *any* time we have multiple tools we can end up with confusion. How to install GHC itself can fall into that category, for example. We definitely need to be aware of that. Now for Stackage vs Hackage. I'll take you literally at first, and answer that I think there's a clear description we can (and to some extent do) give about the two projects. I actually think [my first blog post about Stackage](http://www.yesodweb.com/blog/2012/11/solving-cabal-hell) explains these layers well: Stackage sits on top of Hackage, in the same way that Hackage sits on top of Github (and other source control mechanisms). We should make clear to users the different options, and provide a sensible default when we think users will not be able to make a good decision. But I think you're really getting at a deeper issue: the wider ecosystems evolving around Hackage and Stackage. In this case, you're talking about `stackage-update`vs `cabal update`, but in general there will likely be other such choices between two sets of tools. I think this is somewhat inevitable, as we have different parties who believe the tooling should be developed differently. In this case, I would have much preferred to just include the `stackage-update` code in `cabal-install` itself, as I don't see a reason for it to *not* be included. I was going to bring that up next week, but [discussions with Duncan](https://groups.google.com/d/msg/commercialhaskell/PTbC0p_YFvk/ho4lhrWREyAJ) indicated that he'll be taking Hackage and cabal in a different direction, which I strongly disagree with. Therefore, I decided to make this Git-based approach available separately, and people can decide for themselves what they want to use. And to be honest, I wrote this because I woke up a 3:30 this morning for no good reason and was bored ;)
You need the full quote: &gt; Hackage sits on top of Github (and other source control mechanisms) The blog post explains this in more detail, the gist being that there's a spectrum of stability, where Github/BitBucket/DarcsDen are the most unstable source of packages and you move from there to Hackage, Stackage, and Haskell Platform, each adding a layer of stability.
Well said :)
Not signing the combined set of packages by some authority is a security issue in itself. This enables the mix-and-match attack, and thus a full build like what stackage does should be signed. (one could say that the version bounds used by cabal is a security issue in itself). So you should add signed commits to your github repo. Also, there must be a sequence of states that are signed to prevent rollback attacks. That's what most signature schemes forget. So don't sign your package, sign your package and include a hash of the previous package in the signature. Otherwise, great work! I think a git repo with keys and signatures is a good approach, you just need to get the sequences signed as well. 
It is frustrating to see again and again various proposals that ignore the core functionality in a tool they use every day. I have personally designs that are more efficient than git, better tailored to partial checkouts and composing large hash-trees, but I assume there is a limited amount of time and resources that can be used on this (thus the use of TUF?). Security is also about being practical and useful. If I can create a public mirror of hackage that improves its availability and its trust structure with a few simple commands (setting up a git mirror), then that is actually an important security consideration. So we can't make security too abstract as you suggest. But I agree - given not that much time and money, I could create something git-like (cryptographically secure hash-dag representing an unforgeable sequence of file system states) that is more efficient and tailored to this problem. I don't have that time though, so I fixate on git. Git has a glaring issue: SHA1. 
I really like more this GitHut rating http://githut.info/ It does show that during last two years haskell got beaten (in active repositories) by Go, CSS, Clojure, Tex, R and lately Swift. It also impresses me how much Java there is on GitHub (and how rarely I see it there, unlike others).
I discussed that in [this comment](http://www.reddit.com/r/haskell/comments/32wbv5/yesod_blog_announcing_stackageupdate/cqfac7f).
Fair enough, was just wondering.
What do those who criticize it propose as the practical alternative to Foldable? It is clearly of practical use to be able to write algorithms using it once instead of writing one version per current instance. Forcing everyone to learn what relatively exotic types like Coyoneda mean does not seem like a very practical solution either.
What generic programming can you do with `Foldable` that you can't do just by converting your value to its equivalent `foldMap` first? (i.e. converting it to a value of type `Monoid m =&gt; (a -&gt; m) -&gt; m`)
I don't quite understand the question, considering foldMap is a method of Foldable and wouldn't be available either if you got rid of it.
&gt; Since `Foldable` has no return type polymorphism Actually it's a bit more than that. I guess it's that in every method the typeclass parameter appears exactly one, as the first argument to a function.
&gt; Can you tell me about the issues with ide-backend-server? To be honest, I'm not now sure if ide-backend-server was the issue. I built stackage-view from default hackage rather than stackage (mostly to see if it worked). It built but fpview complained about no access to ide-backend-server. There's no ide-backend-server on hackage yet, and I wasn't sure which branch/tag to build from git. In the end I tried several (ide-backend-server/0.9.0, experimental, master). In each case, with an ide-backend-server executable available, fpview starts, but is giving me: "A problem occurred &lt;&lt;server died&gt;&gt;" when I choose a target and start the interface. I assumed that meant ide-backend-server, but I guess it could be referring to some part of the webserver stack At that point I ran out of free time (getting ghcjs installed had taken a big chunk of it).
[Here you go](http://hackage.haskell.org/package/type-list). But beware of bugs, I was supposed to fix Intersection long ago, but I didn't, sorry. Also it's interesting to know you're working on relational algebra; I was trying to extend Edward Kmett's [tables](http://hackage.haskell.org/package/tables) by writing a wrapper over Vinyl's rec to make in-memory relational databases. But after I tried to give it a spin in a bigger project I'm currently working on, I got absurdly long and memory hungry compilations (a'la compiling Agda's compiler), and gave up for now. [Here's](http://hub.darcs.net/mjm/relational) some half-baked code in case you're interested. EDIT: I fixed Intersection and made Length into a type family that returns a Nat (instead of a type class with a function that returns an Int). Also moved the repo to Github. Please notify me of any remaining issues.
To clarify, you're suggesting that instead of providing an instance for `Foldable`, you instead ask people to export *just* a `foldMap`-like method with concrete types? E.g., Data.Set.foldMap :: Monoid m =&gt; (a -&gt; m) -&gt; Set a -&gt; m Right? I guess one problem is that with just that, you might hit worse asymptotics for some folds. For example, if I have an ordered tree as my representation, taking the `max`imum can be done more efficiently than comparing the maximum pairwise over all elements. That said, to get that gain at the moment, you still have to implement a custom `max` function, but at least you don't have to claim a new symbol to do so... 
This is the closest thing to what I was actually looking for. Thanks. Just to give some thoughts on the different approaches: * `tables` is a cool idea, but is really more of a key-value lookup than a relational algebra implementation. It seems like primary and candidate keys get tracked at the type level. Doing RA operations like natural join or project seems impossible as the records are just standard Haskell records. * Your code seems similar to the `tables` approach. A lot of keys show up at the type level, but you're using vinyl records and somehow managed to get joins working (I can't study the code long enough to figure out how). One thing you're code does not (and cannot) do is optimize queries. * The approach I am taking is to not worry about the candidate key stuff yet. It will eventually be represented at the value level, so that a particular query may optimize at runtime. I am building up an AST of strongly typed RA operations, which can then be optimized, and then running the optimized version. So, for example, I can push selection or projection down inside of a join so that it has to deal with fewer rows. 
I wouldn't agitate against `Foldable` but I wouldn't be too sad if it didn't exist either. I'm responding to /u/Taladar's question about what the alternative to a typeclass is. The nature of `Foldable`, i.e. all occurrences of the type parameter occur once on the left hand side of an arrow, seems to lead to a situation where the typeclass provides "name overloading" rather than genuine "generic programming". 
Tables is more or less a collection of maps pointing to the same rows. I've got joins"working" by just combining Cartesian joins (implemented by just gluing rows together) with projections; I didn't get to the point when differentiating between columns of distinct source tables mattered, but I wanted to implement it by "tagging" the field labels beforehand. As for type level, well, the distinction between type and value levels becomes somewhat blurry with singletons and the like, but I do agree my version is much more static than what you seem to have in mind. Overall, I wasn't aiming for efficiency, this was supposed to be a proof of concept. EDIT: I admit I really like the lens approach I was trying to implement, and it saddens me it seems indeed to be incompatible with query optimization. Perhaps the way to get the best of both worlds would be the approach taken by REPA, of differentiating between "delayed" tables (which would be just functions) and "concrete" ones, that can be computed sequentially or in parallel?
My alternative proposal would be `BetterFoldable`, but I see the point with `Coyoneda` being slightly exotic. However, it seems worth it to me. I'm a purist, though, so YMMV.
&gt; I'm not convinced that the existential typeclass antipattern is about that, mainly because there's no existential type there! I may have misunderstood what the existential typeclass antipattern actually is! I thought it was something of the form class C a where foo :: a -&gt; ... bar :: a -&gt; ... I do think this is an antipattern, but I suppose it doesn't become "existential typeclass" until you wrap it in an existential! Personally I don't love any particular approach to the "foldable issue" and the typeclass is pretty much as good as any, probably moreso.
You know about /r/rational right? I think you'd like most of their recommendations. 
Wow, that visualization is excellent.
Could you sketch your argument on how this would work out in more detail? How would I recover typical operations from this formulation, for example?' And would the "free functorality" of the coyoneda formulation wipe out the impact of those laws on actually preventing bad instances?
Makes sense, thanks for your reply, and appreciate the good work y'all have been doing to improve the Haskell ecosystem!
To bootstrap the system you need to get the root public key(s) of the repo to the client somehow. In practice we'd just distribute the root public key(s) for the central community instance with cabal-install (of course using HTTPS at that stage actually is useful). Then all the other keys (timestamp/snapshot/target etc) are available from the server, signed by the root key(s). These keys can be changed at any time and if so the client just re-downloads them and check they're still signed by the root keys. The full details are in the TUF spec and paper (including root key updates). As for why not GPG, it's not that it couldn't be used, it's just that TUF doesn't need that level of complexity, any suitably secure signature algorithm(s) are suitable. This means we can pick ones with minimal dependencies which makes the system simpler to distribute. (On a closely related topic, we're not limited to one algorithm but can upgrade following the crypto algorithm lifecycle.)
That sounds like a good summary.
Will be open sourced soon(tm). Working on it!
It might be nice to mention the differences between this approach vs. just a polymorphic Order like: data Order c p = Order { orderId :: Int , orderCustomer :: c , orderProducts :: [p] } 1. It operates in a closed world (vs. open world) to where the reference-style is only drawn from our `Reference` type. (This is a difference that *can* be an advantage; open world might be desired in some contexts.) 1. It prevents a "mismatch" between the reference-style between a customer and a product. We only have one variable vs. two independent variables. Again, this can be an advantage, but it might also prevent cases where you want to so a DB-join with the customer table, but not DB-join the the product table. (For example, a customer, #items, summary across all orders for the month.) I like seeing this application of DataKinds + TypeFamilies; I don't think I've seen it in other articles.
Good point! It would have been worth that discussion, definitely. The problem I have with the full polymorphism example is it leaves the types `c` and `p` to the programmer's interpretation. When I see `Order c p` I think: "an `Order` that can hold a `c` or a `p` of any type" whereas what we're trying to do here is express the fact that the way we represent and understand objects depends on their source. Also, I really don't want someone thinking they can define a `Functor` instance of `Order c` ;) thinking aloud: I think one could use this style of tagging sources of referential data in combination with a free monad or finally tagless encoding of operations and HaXL to build a pretty cool ORM. Maybe. I haven't fully thought it out. Thanks for your comments!
Another approach to consider would be to model foreign key references as a `Constant` functor that encloses the referred-to type: {-# LANGUAGE GeneralizedNewtypeDeriving #-} import Data.Functor.Identity import Data.Functor.Constant data Customer data Product data Order f = Order { orderId :: Int , customer :: f Customer , products :: f [Product] } newtype Ref a = Ref { unRef :: Constant Int a } deriving Functor deref :: Ref a -&gt; IO a deref = _implementMe derefOrder :: Order Ref -&gt; IO (Order Identity) derefOrder (Order id customer products) = do c &lt;- deref customer ps &lt;- deref products return $ Order id (Identity c) (Identity ps) It might also be possible to design a `Ref` type that, instead of just being `Constant Int`, uses a free `Applicative` functor to record a heterogeneous list of primitive references *and* a function to use to combine their dereferenced values. Then you could meaningfully `fmap` and `&lt;*&gt;` over `Ref`, which would just record the functions and defer their application until you `deref`. Conceptually you could see this as extending the meaning of `Ref a` from just "reference to an `a`" to the more general "an `a` whose value depends on some references." And actually, if you had `Applicative Ref` you might be able to do away with the `f` type parameter to `Order` above: -- An `Order` that depends on some references example :: Ref Order example = Order id &lt;$&gt; refToCustomer &lt;*&gt; refToProducts refToCustomer :: Ref Customer refToProducts :: Ref [Product]
It's not an issue for sandboxes, since the downloaded package index in `~/.cabal` is shared among all of them.
This, essentially. (I think; I haven't thought a lot about it, but it's the right names, so it's probably correct. :P)
Why would it exclude `Set`? I just wonder if by using `Coyoneda`, you're only getting the same guarantees as you would if you had a `Functor` anyway, which is to say, what you get for free from parametricity. _Edit_: Oh right. &gt; quickCheck $ \s1 s2 -&gt; foldMap Sum (fromList s1 &lt;&gt; fromList s2) == foldMap Sum (fromList s1) &lt;&gt; foldMap Sum (fromList s2) Failed! Falsifiable (after 7 tests and 1 shrink): [4] [4] 
...and now that I reread your GP post and notice again that you'd mentioned HaXL, I noticed that my last bits are really HaXL-like.
But that still is not subtyping. The Liskov substitution principle does not hold. /u/Taladar was right: `Int -&gt; Int` is not a subtype of `a -&gt; a`.
as always, the story of mutability makes the most sense when treated like we do in Haskell :)
Servant is very new, so that might explain that. There have been a few posts about it here, recently, though.
&gt; Is it just me, or are some of these haskell libraries difficult to find? A good way to find all the modern Haskell web frameworks is probably to look at the reverse dependencies for wai which is the de facto standard for webserver/framework interaction in Haskell. It might include some other web-related packages of course. http://packdeps.haskellers.com/reverse/wai
Spock is pretty nice. WAI too.
I know what I'll be doing for the next few hours..
I'm sorry, I have been meaning to write and talk more about servant but have been busy for the last two months, getting used to life in the UK =) We did publish a couple of posts here on /r/haskell though. We will definitely make a bit of noise "soon", we're preparing the next release and hoping to have a lot more examples and what not. Also, servant as it is now has been released in december 2014.
I think /u/tailcalled is correct that requiring `foldMap` be a full monoid homomorphism discludes `Set` from being `Foldable`. The key is that `foldMap` should only need to satisfy the monoid homomorphism laws if the input containers are "disjoint", where the notion of disjointness is type specific. There was a recent haskell cafe thread about `Foldable` laws where I go into a bit more detail about this: https://mail.haskell.org/pipermail/haskell-cafe/2015-March/118471.html
Incidentally, don't these deconstruct-and-then-reconstruct-while-applying-upcasts eta-expansion based subtyping rules^1 closely correspond to definitions of `Functor` (resp. `Contrafunctor`, etc.)? E.g. `fmap f (a, b) = (f a, f b)`. After all the "covariant" of subtyping and variance is the same as the "covariant functor" of `Functor`, and upcasting a covariant type is the same as `fmap`ping the "upcast coercion", as you allude to in the article. ^1 I'm tired and probably not using terminology very precisely, sorry
There's certainly a relation there, but it's not quite so simple, as far as I can tell.
This is one of the most alluring job descriptions I've ever seen. I'm in no position to apply for a senior Haskell position, but I'm going to rank job descriptions against this one in the future. &gt;skedge.me values a sustainable work life balance. We know that building a company is a marathon, not a sprint. We value focused effort and productive output over last minute crunches and time spent in a chair. Yes please.
Spock is really nice.. I'm going through a tutorial to get a feel for it. Thanks for the recommendation. 
I'm still in undergrad so yeah :P
damn too bad i'm in the arm pit of northeast PA..
Thanks! And sorry for taking so long to respond :) &gt; GHC derives `Foldable`/`Traversable` my making the "obvious left-to-right choice". It is known. However, it seems like they are "property-like aside from the order" - that they *would* become property-like if you could somehow factor out the order. E.g. you have something like (for a given `T` type) `T -&gt; Functor T`, but `T -&gt; (Order -&gt; Traversable T)` (the `-&gt;` here is not the Haskell `-&gt;`). (And perhaps, in fact, `T -&gt; (() -&gt; Functor T)`.) What I'm trying to get at is that maybe this can be characterized in a finer-grained way than "is uniquely determined" (property-like) or "is not uniquely determined", but rather "uniquely determined up to *thing*" (which, in `Traversable`'s case, would be the order). Does this make sense? If it does, the next thing I wonder about is whether *all* structure (type classes) is uniquely determined up to some *thing* (can you always find a suitable *thing*)? My initial instinct is "probably not" (`Monoid` seems too loose for that?). My second thought is that perhaps you can choose the structure itself as *thing*, in which case it becomes trivial. But is there anything for other important `class`es analogous to the ordering in `Traversable`'s case, where if you make a choice along that dimension then it becomes unique? (`Representable` and the choice of representation of course seems like another example. My intuitions for `Strong` and `Choice` are much less well developed.) &gt; `Distributive` implies the existence of such an isomorphism but gives you no way to reference it, so that one is uniquely determined. Interesting! Given its status as `Traversable`'s dual, I wouldn't have expected this. How come one and not the other?
this isn't related to the lens package is it? "bidirectional programming" didn't sound like it, but I only read the abstract.
The tutorial was a great quick start, but I immediately went looking for the apparently non-existent API documentation.. I see that hackage reports "Docs pending". What's the status on API docs? This is pivotal. 
Apply.
No, I mean that I'm committed to being in Boston and can't relocate. Also, I don't have enough Haskell experience compared to, say, Coq.
Like? I'm undergrad... yet I've been programming Haskell daily since a year or two, and other languages for ~12 years... pretty much rewritten the whole prelude at this point, completed tons of things... yet I'm "out". I don't think a degree means much nowadays. Just commenting, though, I'm in another country anyway. Good luck!
Yes, but that doesn't use Show on its own, it uses Read too. 
Yeah, but I dislike Typeable even more than this antipattern. (WxHaskell uses the antipattern but relatively inoffensively, since the author was saddled with an OO model not of their design from the C++.) I'm sure Typeable has principled uses, but I tend to get put off anything that goes down that route as a style choice. It does feel like some folks on SO just want to use it because they can't be bothered to learn Haskell's type system. In fact, that's exactly the circumstances in which the existential typeclass antipattern is most offensive! I'm probably ill-informed since I've avoided Typeable as much as possible. It probably has some great scrap your boilerplate application I'm missing out on. 
My impression had always been that the bit about injectivity and surjectivity here was rather well known in the bx community. The more common phrasing has the extra caveat though, given a Lens from S to A * `put s` is either injective _or S is uninhabited_. * `get` is either surjective _or S is uninhabited_. Every value in A must be obtainable via get from some S otherwise we couldn't put it in and get it back out no matter how hard we tried and `get (put s v) = v` would fail.
The notion of a "lens" comes out of the bidirectional programming (or "bx") community originally. The lens library kind of discards those roots and explores a bunch of variations on a well-defined case that that community largely finds boring. =)
Foldable is there because Traversable logically implies it, but its functions can sometimes be optimised. 
I played around with something very, very similar to this a couple of months ago (and even had a draft blog post for the first part), and had an extension to it that I only got half way through implementing. It's good to see that the idea I was playing with wasn't just insane ravings :) First a bit of background - I used something like newtype ID a = ID Int where I was using DataKinds to have the phantom type track what the ID was for - so Order would have a field orderId :: ID Order and I only had the choice of no references / ID references. I can't remember if that version had extra magic or if it was actually `ID (Order r)`. The extension I was working on started with a `Cache r` type which held several values of type type CacheEntry r a = Map (Id a) (a r) where `r` was the reference type and `a` was the various types in my system and a typeclass that gave me access to inCache :: Lens' (Cache r) (CacheEntry r a) and then inCache . ix i with `i :: ID Order` would traverse over the order with that ID. From that I was able to build something that could go back and forth from `Cache IDRef` and `Cache NoRef`, which tied the knot through the whole cache in the `NoRef` case, so you could wander through the structure to your hearts content, but only for the read-only case. The idea was to be able to load data from our data store, then build an object graph we could work with without having to worry about IDs, and then convert back when we were done. Although converting back only made sense if we were updating the object graph. There was a little bit of plumbing involved, but most of it seemed like it would be able to be made generic fairly easily, and it was fun to play around with. I context switched at that point to looking at how to make it work for the read-write case. I looked a little at [the "Weaving a Web" functional pearl](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.445) but didn't make any progress there. I then started playing with typeclasses to model parent and child relationships with the Cache as implicit state, so I could handle cascading deletes etc.., managing inserts and just handling updates in general, and then I got busy with other things. I might resurrect some of this in the future, since I'm probably going to need something like it at work, but there's other things at the head of the queue now. Sorry for the rambling brain dump. If you keep doing things with referential data I'd love to read more about what you get up to.
I really appreciate this brain dump! lots of interesting ideas! I hope you write them up. Thanks for the functional pearl link, too.
What do you use coq for?
Well so for types that don't have binders in their intros, the elim is a fold, and as we all know, folding with constructors is the identity. but that's also precisely what eta expansion is: folding with constructors. E.g. we all know `foldr [] (:) = id`. The subtyping versions of eta expansion I proposed are folding with constructors pre-composed with casts, like `foldr [] ((:).cast)`. To me more precise, you pre-compose with cast in all the non-recursive (i.e. parameter) positions of the functor that your type is the initial F-algebra for. This is more or less like fmapping the total type with cast. And since one of the laws of fmap is `fmap id = id` we also expect that since `cast` is more or less a type-changing `id`, that `fmap cast = cast`. But things get tricky with the types that have binding intros.
There's also the part where equational reasoning is broken in languages with partial functions...
Do you guys do internships? 
`Applicative` is just a super class of `Monad` where every `Monad` is also an `Applicative`, because of that you can define `Applicative` in terms of `Monad` like: pure = return f &lt;*&gt; x = do f' &lt;- f x' &lt;- x return (f' x') There are functors that are `Applicative`, in which you can apply functions inside of the functor `(&lt;*&gt;)`, but aren't able to implement bind `(&gt;&gt;=)` from `Monad`. One example is `ZipList`.
I like that you're using NixOS, I don't want this distro to die.
&gt; Is there some way the compiler can tell me when a process will crash at runtime because an exception hasn't been handled? Yes, trivially: all programs can throw exceptions. Async exceptions can be raised in most straightahead Haskell code, along with OOM exceptions and other nastiness. Java's checked exceptions are often looked at as a failed experiment, because it turns out they compose pretty badly: even trying to define something as simple as a `.map` method runs into this. This keeps the abstraction ceiling for checked-exception-using code very low. OTOH, where Java would use checked exeptions, Haskell uses plain old datatypes like `Maybe` / `Either`. These compose pretty well!
This might be an unpopular opinion, but I'm glad you're breaking off from cabal-install and hackage. They're a mess, and they always have been. I glad somebody is stepping up to fix the situation. cabal-install is an embarrassment to the Haskell community.
I don't understand ATM, but I'm tired and can sometimes be very stupid. I'll get back to it after some sleep. 
I understand what you mean about it not composing well. Certainly through the Monad typeclass both `Maybe` and `Either` compose well. But take something like `readFile :: FilePath -&gt; IO String` from the Prelude. I could make use of `readFile` without the compiler telling me there's a problem. If the type of `readFile` was `FilePath -&gt; IO (Either IOException String)` then I would know that there's a chance this function could fail. I'd be forced to handle the exception at compile time. It's not that I'm not sure how it would be done, it's that it seems it's almost never done in IO. My example of `Data.Map`, it's not like we're given `lookup` and `unsafeLookup` - the library is so made that lookups will always return `Maybe a`. But in any library dealing with IO, it's almost as if the feeling is because we're in IO we should just expect exceptions. But some compile time errors would be nice. 
There are reasons that cabal at the moment doesn't do https. It would be good to teach it to do so. This requires some elbow grease and isn't a profound disagreement over approach. And in the future there are also plans for a delta based approach (though not a git one) to fetching the index, as has been discussed on the well-typed blog. In the meantime, if the bandwith is a killer (and I can't imagine it is but...) or if https feels like an important thing here (and it really isn't in my mind because poisoning the index with a MITM is a rather obscure attack vector compared to many of the other ones which continue to exist) then I guess you can use this tool right away.
&gt; One of the great things about Java was how it forced exception handling. If you call a method that throws an exception, you either have to handle it or your method has to throw the same exception. Which means that somewhere along the call stack you must handle it. Java has unchecked exceptions (i.e. all subclasses of `RuntimeException`), which are *not forced* to be handled throughout the call stack. Many Java libraries I use have their particular exceptions as subclasses of `RuntimeException`. I can only notice them either in the documentation (often lacking) or when an unexpected runtime failure happens. Sometimes, Java even encourages you to throw unchecked exceptions. For example, if you need to write the implementation of an interface method (e.g. `run` from `Runnable`, or `mapRow` from `RowMapper`), but your implementation uses methods that throw checked exceptions, your code won't be valid Java unless you (1) handle all of them inside the method implementation via `try/catch`, (2) change the interface method to throw the required exceptions, or (3) wrap the implementation in `try/catch` blocks that re-throw the exceptions inside a `RuntimeException` and catch the `RuntimeException` somewhere else. Also, some programmers are lazy; if their call stack is full of methods with `throws IOException, GeneralSecurityException, ActivitiException, SQLException, MyAppException`, it's tempting to bubble them all in a `RuntimeException` and handle it somewhere else, instead of having all those `throws` declarations. So, Java's exception handling actually lets you write code that may fail without previous notice at runtime. Similarly, in Haskell, some functions might code errors within `Maybe`, `Either` or other monads/datatypes… and some *throw* exceptions that don't get represented explicitly in the function's type. This is why the type checker (at least) can't warn/protect about thrown exceptions, IMHO. Even if you use functions that code errors explicitly in their types, you may use functions that throw exceptions and get rid of the enclosing "error-handling type" (e.g. `fromJust` for `Maybe`, `fromRight` for `Either`). This is equivalent to wrap checked exceptions inside an unchecked one in Java, and you end up with the same problem. I think the closest thing to mandatory error-handling would be to define all I/O functions as `IO (Either IOError &lt;rest of type&gt;)` instead of just `IO &lt;rest of type&gt;`, which would require explicit error-handling functions (even if it's just `fromRight`) in order to end up with a `main` function with type `IO ()`. At least, the compiler may warn about some error-throwing situations (e.g. non-exhaustive pattern matching).
Forget all that stuff. An existential type is just (in principle) a special kind of pair type, where the first element of the pair is itself a type, and the second element has the type given by the first element. It's as if we could say `(a :: *, a)` is a type, the first element of which is of kind `*`, and is named `a`, and the second element has type `a`. Only instead we write it `exists a. a`. Here are some such pairs in a fictional Haskell: (Bool, True) :: exists a. a (String, "Foo") :: exists a. a Of course, we could use other things in the body: (Int, [1,2,3]) :: exists a. [a] Now, what happens if you have a type, such as this: data Any = Any (exists a. a) What's the type of the constructor `Any`? Why just Any :: (exists a. a) -&gt; Any But if, as I described before, `exists a. a` is kind of a pair type, like `(Foo,Bar)` is a pair type, with `a` as the name of the first element which has kind `*`, then we ought to be able to sort of .. curry `Any`, right? So that its first argument is a `*`, named `a`, and the second argument is a value of type `a`. But that's just what `forall` is! curry Any :: forall a. a -&gt; Any But of course, we don't explicitly give the type argument to `forall`, and we don't explicitly have a first-element of the existential types using `exists`, they're silent/implicit/hidden. Unfortunately, we don't have `exists` in Haskell (a sad oversight), so we can't write the type declaration as I did above. However, we do have GADTs, and we can use them, with the "curried" form, like so: data Any where Any :: forall a. a -&gt; Any But this is kind of gross, why should we use GADTs? So instead there's a special syntax. The usually notation for type declarations says to put the name of the constructor, then the argument types in order. Hence `(:) :: a -&gt; [a] -&gt; [a]` is written as `(:) a [a]` in the type declaration. So why not do the same here? Only, we need to do something with that `forall`, because the variable it binds isn't present in the return type. Well, ok, stick it before the constructor name: data Any = forall a. Any a
Of course you know, but the great thing with GADTs is, that when case matching you get the original `a` in the body. That's occasional useful when dependently typing Haskell.
Aren't the two lenses from Example 1 *very* well-behaved?
&gt; Exceptions There are arguments on both sides of this debate and I don't really feel qualified enough to comment. Sorry. But addressing a few less central points: &gt;I recently noticed that there are two general "types" of Monads (with a whole bunch of exceptions and variations). The first is SomeMonad a that is specifically about the a. A Tree a, Maybe a, [a], ST s a etc. While the second is a Monad to which the a can often be superfluous, Reader r (), State s (), Writer w () etc. With some that skirt the line (IO, Iter, Rand). I'm sure there's some formal Category theoretic way of saying that. I'm not sure I agree with that entirely, or at least, that there is a non-fuzzy "formal Category theoretic" distinction. You can certainly have Reader, States, and Wrtiers where there a is significant. Take for example a process that calculates something but you want to log things during the calculation with Writer. &gt;I was looking at the async package today. The first thing I noticed was the more specialized type signature async :: IO a -&gt; IO (Async a) instead of a generalized async :: MonadIO m =&gt; m a -&gt; m (Asnyc a). The problem is that not all monads "split" very well. Take for example a producer from the pipes library. Splitting the flow of execution into two threads and having both try to read basically means it ends up being random which thread ends up reading the next byte of data. The ST monad can have similar problems. But, some monads do split. The classic example being Reader. You may with to look at async-lifted and the MonadBaseControl typeclass. 
Happy to see more haskell job postings lately (I have a feeling this is a rising trend?) . Gives me hope for when I graduate :P
For Myers diff in particular, you will need to save each version of the array in order to determine the actual diff, rather than just the size of it. This means you can use immutable arrays. I implemented the Myers diff algorithm in C for my [bytestring-delta package](http://hackage.haskell.org/package/bytestring-delta) (this only scales to a few thousand bytes due to the algorithm being O(n^2) for largely-differing strings, but it was fine for my use case). I have a nice big comment illustrating how to derive the edit script by climbing the tree of V-array versions: https://github.com/joeyadams/haskell-bytestring-delta/blob/master/bdelta.c#L267 Although Myers diff is array-based, you should be able to break this down into smaller pieces than I did in my C version. You could try breaking it down into the following functions and types: -- | Represents a single version of the V array from the Myers diff algorithm. newtype VArray = VArray (Vector Int) -- | Create a new V array given a D value, where the generating function is -- given k indices which range from -D to +D skipping every other. generateVArray :: Int -&gt; (Int -&gt; Int) -&gt; VArray -- | Access a V array by its k index. Return Nothing if the index is out of -- range. indexVArray :: VArray -&gt; Int -&gt; Maybe Int -- | Generate the next version of the V array based on the previous version and -- the old and new strings. The Bool will be False if the shortest edit string -- was found, True if another iteration is needed. nextVArray :: Eq a =&gt; Vector a -&gt; Vector a -&gt; VArray -&gt; (VArray, Bool) -- | Apply a function to a value over and over while the function returns -- True. Return all versions of the value starting with the value given, and -- ending with the value returned when the function returned True. This -- combinator is used to apply 'nextV' repeatedly. iterateWhile :: (a -&gt; (a, Bool)) -&gt; a -&gt; [a] -- | Given all the V arrays in reverse order of generation (longest to -- shortest), generate the edit script in reverse order. Remember to reverse -- the input and output lists when using! generateEditScript :: [VArray] -&gt; [EditInstruction] data EditInstruction = Copy Int | Skip Int | Insert Int Because this algorithm is so array-based, you won't get as much compile-time safety as you may have come to expect from Haskell. You're still susceptible to one-off errors. A couple ways to avoid this, neither of which are trivial: * Redesign the algorithm to use more functional-friendly data structures like lists and trees, and to avoid integer indexing. * Use a dependently-typed language (like Idris) to verify all of your indexing logic at compile-time.
Also, there are some cases (such as `Identity`) where the input containers are never disjoint.
I'm afraid using sqlite from haskell is probably not much better. Sqlite doesn't support concurrent writers at all; one write will fail. persistent-sqlite doesn't prevent multiple threads trying to write. Only solution is to avoid using connection pools and serialize all writes through one thread. Except, sqlite even lets reads cause a concurrent write to fail with BUSY. You have to enable the non-default WAL mode to avoid that failure mode. WAL doesn't seem to be a panacea for concurrency either, I was able to find [a case](http://thread.gmane.org/gmane.comp.db.sqlite.general/93116) when using it under heavy load, where a write breaks concurrent reads. I hope that at least postgresql avoids these kinds of concurrency problems?
Having checked exceptions might be useful but at the very least some asynchronous exceptions can happen anywhere and you certainly don't want to handle e.g. out of memory in every Haskell program explicitly.
Obligatory [Cartesian Closed Comic](https://ro-che.info/ccc/16) =)
In the mailing list I proposed making this an optional feature based on the presence of the git executable being present, otherwise reverting to the current behavior. I find the idea that cabal can only use Hackage as an upstream source by default to be surprising and distressing. It precludes a great number of potential improvements.
Thanks for saying this Duncan, I fully support it (though I don't know what it's in response to). Reasonable people will of course disagree on many things. We agree on the core points, which are the awesomeness of Haskell and need to make the ecosystem around it as good add possible.
To be fair, the tone of some of your replies, Michael, did seem mildy condescending as I read it. Given that you two do work together more closely than I had realized (until reading the comment to which I'm replying), the tone doesn't seem as out of line. In other words, I suppose I'd be more concerned with your responses if it had been the case that Duncan and you were strangers. In any case, I think this was a great idea, so +1 for the effort. I assume (and if that assumption is wrong, would request) that using github isn't mandatory (that is, any legitimate git repository url would do), in which case it seems feasible to host a git mirror on haskell.org (if people were that concerned with having the content on medium not controlled by the Haskell infrastructure team). Really though, having one less thing to personally maintain seems like a benefit for, not against the use of github. If they ever did disappear into the either, I highly doubt it would be without notice. Further, if it were without notice, I doubt it would be the case that either a) someone didn't already have a local copy somewhere, or b) the repo couldn't easily be recreated.
I'm on the west coast and can't relocate but would be interested in the SRE position. Are you guys considering remote?
&gt; because it turns out they compose pretty badly: There's actually checked exception systems that compose fairly well, but in all those experiments / languages you can *abstract over* the exceptions thrown by a function / method. It's really the lack of having and being able to use "exception set" "variables", that causes the problem in Java. This can even tickle the edges of dependent types, since you want to test (lack of) set membership or perform a set union where the set appears as (part of) a type. Java's checked exceptions are, IMO, a bit of a failure. But, I'm also not a fan of how Java did generics, either. Even Haskell is better at checked exceptions than Java although we don't use them by default.
Yup. It's a wart. I like the safe package for similar reasons. I shouldn't have to try at all hard to avoid runtime errors, I'm programming in Haskell! 
Indeed - the interest in FP and FP-oriented languages very much seems to be a trend. Facebook just went on a Haskell dev hiring spree. I went to Forward JS in San Francisco this year, and all the talk was about functional JavaScript. This is a valuable skill to have in the toolbelt.
Yeah, finally i new episode \o/ Looking forward to listen to it. Also thanks for the recommendations for books and videos on the last episode thread. Now I know a bit more about type theory :)
I'm glad to hear you like the show!
I know, thank you - I just tried to put on the table that this particular requirement could be rethought.
&gt; Now, let's ask two questions: &gt; 1. How do you prove such a statement? &gt; 2. Given a proof of such a statement, how do you use it as a premise in a larger proof? This, and the following examples in English and Haskell, more so than all the other replies in this thread, really resonated with me. Much appreciated
The talk you linked by Dreyer is one of the most clear expositions of parametricity I've ever seen. I say that even though I wasn't able to follow the entire mechanism due to not knowing all of the notation he was using. Can you recommend any papers/lectures/theses/books/etc to cover this same material at a similar depth but in more detail? I'd really like to be able to confidently hold and explain something like "parametricity subsumes type and abstraction safety and, genuinely, you need and want them both in a language". Dreyer links a lot of papers, but I'm not sure which ones would be most appropriate. I'm not even sure I can meaningfully differentiate "universalists" and "existentialists".
It appears inferring what Haskell would look like with dependent types is undecidable.
Or the standard way, which is: (&lt;*&gt;) = ap
I'd read Wadler's paper in the past but, yes, found it to be less illuminating than I would have hoped. It helped me understand *what* Free Theorems were and how they were generated, but it was a little difficult to motivate. I'll take a look at the OPLSS lectures, though. Thanks! 
In non-linear logics, the choice of additive or multiplicative `Sigma` is irrelevant, since they're inter-derivable. Additionally, both kinds of `Sigma` appear in the literature. Regarding computational witness, yeah, this is why I said that the first part is silent in the existential. Also yes, quantifiers in Haskell are parametric quantifiers, e.g. `forall` is a true parametric universal, rather than non-parametric `Pi`. But these issues are very very subtle and not at all necessary when trying to get the intuitions for Haskell's existentials (especially why they look so weird).
~~Pardon the lack of reference, but~~ I thought I saw it mentioned elsewhere that it should be enough to have an explicit import of Prelude at the end of your other imports? Update: Looks like it's mentioned directly in the [migration guide](https://ghc.haskell.org/trac/ghc/wiki/Migration/7.10): &gt; However, there's also a less-known trick/hack to moving the implicit import Prelude into an explicit one after all base-imports. I'd say look through the rest of that article to see if any other hints are helpful. You may also see if [base-compat](https://github.com/haskell-compat/base-compat) would help.
Yesod. Enough people use it that wrinkles are hammered out, the scaffolds are well made, more type-safe than Snap, the way apps are organized makes sense to me. The various typeclasses used to glue things together are nice because you can use the scaffolded App datatype and then extend with anything specific to your application. I've used Scotty happily, but principally for small stuff. Edit: I use Yesod for JSON API services if I think it'll exceed 500-1000 LOC. No reason to reinvent the wheel.
And what is wrong with just providing name overloading? I mean I could see the problem if it was about completely different operations but the various foldMap implementations are clearly related in their goals and usage.
I've always seen Yesod as too big for what I want to do, as I normally just write JSON api services, so I just use scotty for that. Although I'd love to hear other peoples experiences when it comes to writing JSON apis in Yesod. edit: a word
I tried Yesod but the sheer amount of metaprogramming scared me away.
Author here. I meant to post this a while back and forgot about it. Sorry there is no text just a code listing. Hope someone finds it fun.
Well you only have one default? That's sort of what a default is. However, as we all know, cabal can use any other source if it is configured to do so. I wouldn't want to use any source as a default that isn't available via community managed infrastructure however. No matter how problematic that infrastructure might be at any point, it has the great advantage that there is a commitment to maintaining it that is not tied to any particular individual or enterprise.
How do these frameworks apply over to less traditional (and in my opinion, more up to date/recent) approaches to APIs through a WebSocket connection? I'm working on a SPA where HTTP routes matter less than WebSocket messages.
Laws should provide the motivation that type class instances will be "mostly" unique, in order to justify the enforcement of coherency.
I started using Happstack back when it was the only choice. I have not switched because I have not seen anything compelling enough from the frameworks that have come around since then. If I just need to hack up something quick and dirty, then I can use the subset of features found in [happstack-lite](http://www.happstack.com/page/view-page-slug/9/happstack-lite-tutorial). If I need something with more safety, then I can used web-routes for type-safe URL routing, HSP for literal XML syntax, and reform, for type-safe form processing, and jmacro for embedded javascript. All of these things existed before Yesod come along, and use far less meta-programming magic. These days, I would seldom drop down as low as Happstack as a starting point. I tend to start with [clckwrks](http://www.clckwrks.com), which provides more essential infrastructure like authentication, basic CMS via markdown edited in the browser, themes, plugins, etc. I would then add a custom plugin that adds whatever site specific features I need on top of the common core components. Happstack has a comprehensive documentation in the form of a [book](http://www.happstack.com/docs/crashcourse/index.html) which is available online, as a pdf, or as an ebook. There are few things I would like to see. One is the completion of the new low-level HTTP server -- hyperdrive, which will support the newer 'standards' like http 2.0, web-sockets, etc, which are finally starting to stabilize. Also, now that GHCJS is becoming useable, I'd like to see the don't repeat your types approach demonstrated [in this video](https://www.youtube.com/watch?v=K2jdUlhX_E8) flushed out into a full isomorphic virtual dom based server+client side framework. That also means hacking on a few more features of [acid-state](http://acid-state.seize.it/), such as replication using the [Raft Consensus Protocol](https://raftconsensus.github.io/) and transaction logging to S3 (or similar). At present I am figuring out how to best develop and deploy applications using nix. That includes using hydra (part of the nix family) for continuous integration, and nixops for deploying to the cloud. I hope to start a blog/video series on this topic in the next couple weeks.
WAI
It says they aren't in the post
What's does community managed infrastructure mean?
Stephanie Weirich is the bomb!
Template Haskell doesn't feel right. I know it's supported officially and all, but it just doesn't feel like the best way to solve the problem. 
Yesod. I tried using Spock and I didn't really understand what I was doing wrt to connection pools/sessions etc. The simplicity of Yesod was great. 
We have a big Snap-based project, but the choice was made by previous programmer long time ago. If I would choose now, I would choose something WAI-based and with type-safe URLs and rendering, like Yesod. For APIs I used Scotty (on a different project), but plan to migrate to Servant, which is a much better solution in terms of type-safety and code-size.
Stackage update pulls from https://github.com/commercialhaskell/all-cabal-files.git Why should I trust that will be around in a year? Is that on a domain we have control over? Is it in a github account that is widely enough shared? Where is the hardware that manages that? Where is the code, and is it all public? If the people managing it decide to stop, where is the process for transitioning? "Community managed infrastructure" means, to me, that, however haltingly and partially and with whatever problems that exist, we have attempted to have some answers to questions like the above.
The Github account has 7 different maintainers on it. All of the code is public. No, the domain is not controlled by us, but that's trivially solved if deemed a real issue by having a redirect from another domain. I would argue that it's a moot point given how much more reliable a service Github has proven to be. Since all of the code is being run by publicly available services, there is no reason to believe it won't be around in a year. Have most of the questions you've just asked been asked about Hackage? Actually, I have a very concrete example of that process *not* working on Hackage: [in this issue](https://github.com/haskell/hackage-server/issues/308) it seems that there's no process by which to fix a bug on the main server in the absence of the primary maintainer. Where is the "process for transitioning?" It seems illogical to apply different standards to something just because it's not hosted on a haskell.org domain name. I would argue that having something hosted on a public server that has shown a great record of stability\* is superior to having something that is run by a small team just for Haskell. \* By stability, I mean that- AFAIK- Github has *never* changed the manner in which Git repositories are hosted. An HTTPS or SSH URL has stayed constant for the entire history of Github, and shows no signs of changing any time soon.
How do you write a `Set` instance? I can't work one out.
I've been reading some of David Barbour's stuff over the past few months. He does seem to have some very interesting ideas, but I often find his language hard to understand. There seems to be a lot of missing context in what he says. Perhaps it's [the curse of knowledge](http://en.wikipedia.org/wiki/Curse_of_knowledge). His Awelon project looks interesting, but I'm afraid if he got hit by a bus tomorrow all of his work and ideas would just vanish. It would be fantastic if he could do a talk, or at least an article, summarising all of his work in semi-layman's terms, without so much hidden context.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Curse of knowledge**](https://en.wikipedia.org/wiki/Curse%20of%20knowledge): [](#sfw) --- &gt; &gt;The __curse of knowledge__ is a [cognitive bias](https://en.wikipedia.org/wiki/Cognitive_bias) that leads better-informed parties to find it extremely difficult to think about problems from the perspective of lesser-informed parties. The effect was first described in print by the economists [Colin Camerer](https://en.wikipedia.org/wiki/Colin_Camerer), [George Loewenstein](https://en.wikipedia.org/wiki/George_Loewenstein) and Martin Weber, though they give original credit for suggesting the term to Robin Hogarth. &gt; --- ^Interesting: [^The ^Sense ^of ^Style](https://en.wikipedia.org/wiki/The_Sense_of_Style) ^| [^Naïve ^realism ^\(psychology)](https://en.wikipedia.org/wiki/Na%C3%AFve_realism_\(psychology\)) ^| [^Dunning–Kruger ^effect](https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect) ^| [^Shoshin](https://en.wikipedia.org/wiki/Shoshin) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cqha32d) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cqha32d)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I did 7k+ LOC in Snap and then did it Yesod. I didn't find much of difference, personally. I ended up adding and removing so much from both that those differences started to become negligible. Funny thing is I think Yesod is better for small stuff due to how loaded it is. It's easier to get started and you don't have to worry about adding in more deps. However, when my app started to grow I found myself removing a lot of the defaults and changing deps to better fit my domain. That took a lot more time than I expected.
Spock for my small hobby project. I started out with Scotty but the lack of sessions brought me to Spock.
If it should get "big", I go for Yesod. Because it is well documented (important for transferability of code), it is fast enough (2-10ms responses are the norm), it has the features (i18n, routing file, pluggable with lib, mountable subsites!!, type safe everything, hot code reload in devt mode, scaffolds), and the community is very helpful (and big enough). If the project will stay very small (just a small web api), I'll choose something like Scotty, Spock, servant, postgrest, depending on the task at hand.
Awesome writeup, thank you!
Maybe [this](https://github.com/frp-arduino/frp-arduino) will help. good luck with the chickens!
Where do you draw the line between a "framework" and a "library"?
I didn't realize from this page that it was official part of Raspbian (but it seems to be indeed). Also, I'm still interested in any feedback and encountere issues using a Raspberry Pi as well as the status of GHC and Ardinuo. 
I usually prefer frameworks with lean dependencies (and ones whose authors take the PVP serious, so that `cabal` doesn't break every other week), so as a rough heuristic: $ cabal install --dry yesod | wc -l 136 $ cabal install --dry snap | wc -l 91 $ cabal install --dry spock | wc -l 71 $ cabal install --dry scotty | wc -l 64 $ cabal install --dry snap-server | wc -l 34 $ cabal install --dry happstack | wc -l 29 $ cabal install --dry warp | wc -l 28 So I usually avoid the cabal-headache that `yesod` poses as best as I can (and I openly advise against `yesod` whenever somebody asks me, and rather point to `happstack`, `scotty`, or even `snap` which also have a more stable API). I usually just need a HTTP server that doesn't get in the way, so I tend to use something like `snap-server` or `warp`.
I don't know much about chicken but apparently, they go back home before it's dark. That's how "commercial" chicken door timer works. 
It's a haskell-level definition how a web-server and a web-app work, so that you can split a server from an app. Previously you had to write your web-app as an executable which does the whole job, with WAI your app looks like `Request -&gt; (Response -&gt; IO ResponseReceived) -&gt; IO ResponseReceived` function, and you can run it with any WAI-supported webserver, like Warp. Another benefit (apart from reusing web-server) of WAI is that it's easy now to add "middleweres" to your apps, so you can use, for example, few frameworks, or have a middleware-implemented authentication layer that works with any WAI app etc. Check out how many stuff you can add to your app, no matter which framework you choose (Spock, Scotty, Yesod, Servant) because they're based on WAI protocol: http://hackage.haskell.org/package/wai-extra
The way ghc changes it seems CPP is the only option. :(
Pretty much. https://www.youtube.com/watch?v=zhsYbc7T8yE
ah, I only target back to 7.6 so I haven't had this issue
I missed `yesod devel`, too, when I stated using Scotty. It does look like you could use [wai-handler-devel](http://hackage.haskell.org/package/wai-handler-devel), but I [could never](http://stackoverflow.com/questions/21985379/wai-development-server-and-cabal-sandbox) get it working :/.
Re 2, If the existential has a context it is exactly represented by a pair. So thinking of it as a pair is not a bad idea.
Such cooperative creatures!
Outstanding. I mean, there's no way I'm moving to New York, but this is a solid start. I can see a future where I don't have to sign up to write C++ and write Haskell anyway because fuck 'em, it gets the job done. But seriously, New York. No. Try Mexico or Brazil. Try anywhere that isn't New York (or San Francisco; SF wants to be NY-west and I think they've succeeded; abandon ship). I like Haskell, but I don't like Haskell more than I like legroom. In my city.
A framework is something that calls your methods. A library is a collection of methods you call.
I agree that using a whole RPi (or even an arduino) might be overkill for your project, but if the goal is also to have fun then that's not a problem of course. I have compiled a few small programs on the Raspberry Pi. It works well, but compilation takes a long time due to the relatively slow processor and small RAM of the Pi. To access the GPIO pins, you can use the [HPi](https://hackage.haskell.org/package/HPi-0.4.0) library. (Full disclosure: I wrote it). In the chicken coop project you could use it to read the value of a light sensor (possibly through an A/D converter or Schmitt trigger), process the value in Haskell and then tell a servo to open up the door of the coop. Don't hesitate to ask if you have any questions! 
I use Yesod. I have tried to use scotty on one project, but I ended up redoing it in Yesod. As of late, I have been experimenting with using lucid (for HTML) and clay (for CSS) instead of Yesod's Shakespearean templating languages, and that has been kind of fun. 
vcache is mostly complete at this point as I understand it, and the current focus of his development is on the wikilon development/runtime environment. I'm also developing a native code compiler for Awelon Bytecode for my honors project. It's at http://github.com/klkblake/abcc, but it is very very incomplete (the type checker mostly works).
&gt; That also means hacking on a few more features of acid-state, such as replication using the Raft Consensus Protocol and transaction logging to S3 (or similar). Oh boy this sounds good!
Yesod. Because of the type safe features and fast start. However, its use of TH has been uniquely challenging, since my webapp runs on the user's system and so portability matters. If I had the choice to make over, I would not use yesod for this use case (wouldn't mind using it in more traditional server-side use cases). I have also come to strongly dislike the shakespearean templating languages. I find them troublesome when refactoring and abstracting, especially problematic when one needs to embed haskell code inside the templates (since there are many warts in their parsing of haskell), and generally a layer I can do without. (Also they reject templates containing tabs, which is counter to my coding style.) I'd like to switch to lucid or something. Has anyone integrated lucid into yesod's widget system? It seems that the ToWidget site Html instance could be used, but I have not figured out how to hook that up to lucid's use of blaze.
If `yesod` is only usable with LTS Haskell, then it's rather sad, and yet another reason for me to discourage its use. I'm not sure where you get that lack of tooling was "widely acknowledged" as the cause of `yesod` failing to play well with the Hackage ecosystem. Especially since other large package-ecosystems have managed to avoid the constant breakage I saw with `yesod`, did they have secret tooling at their disposal? I rather see the problem with package authors leaving off upper-bounds and then relying or rather abusing facilities such as LTS Haskell as kludges to workaround their recklessness. So as to short-sightedness, try extrapolating what a mess Hackage would degenerate into if every package would drop their upper bounds and LTS Haskell would become a necessity for getting to ever compile anything, and being a centralised index, how it would scale in order to cover all non-abandoned packages.
We use **Snap** extensively for our API servers and web applications. I personally love Snap (monadic combinators for request / responses is very intuitive to me) and the snaplet design. We've authored quite a few Snaplets too. Yesod felt like it was branching away from Haskell idiom, to me, when I tried it. I also don't like template haskell very much, debugging stuff is difficult, particularly when your project is quite large and you're modifying a lot of the fundamental pieces (or trying to debug why something isn't behaving the way you thought it would).
Huh? All of the big ones mentioned in here are "actually a web framework". Snap, Yesod, Happstack, etc...
I have seen this package but as far as I understand it only allows to "control" an Arduino, which I still don't really see what the point is. An Arduino is a controller not a controlled board.
Ok, on a computer now (though it doesn't have Haskell installed, so this code might need some rejiggering, and it's still inefficient because you can use a more efficient data structure than lists): instance BetterFoldable Set where traverseish f (Coyoneda s g) = fmap (\list -&gt; Coyoneda (fromList [0 .. length list]) (list !!)) $ traverse (f . g) $ toList s
There are very legitimate ways to explain why TH "doesn't feel right." (See: [What's so bad about Template Haskell?](http://stackoverflow.com/questions/10857030/whats-so-bad-about-template-haskell)) But I also think sometimes Haskellers like to drink the kool aid and decry TH because it's the cool thing to do. We'd rather wear the hair shirt of pure functional programming than rely on icky unsafe TH. But sometimes, compile time meta programming *is* a great option that cuts down on boilerplate and can even *add* safety. For example, generating lenses guarantees that they will abide by certain laws. Generating JSON instances guarantees that JSON produced can always be correctly consumed. Generating Yesod stuff makes sure you have defined appropriate handlers for every route. tl;dr I agree but also disagree
It allows you to put your on programs on the Arduino. You can do input and output and all the operations. When the program is transferred to the Arduino it can operate on its own. 
I actually disagree about quasiquoters vs combinators. Yesod's routing DSL is beautiful. The persistent models DSL is beautiful. Shakespearean templates are pretty okay. Not my favorite, but still better than combinators imo. Haskell has no good story for type safe string interpolation without TH. Combinator approaches to solving the problem are verbose, you have to [close quote append interpolation append reopen quote] at every interpolation site.
GHC Generics seem like a better way to generate instances, though. 
&gt; But, I'm also not a fan of how Java did generics, either. Well, except for only allowing type parameters of kind `*`, Java did pretty much the same as Haskell. The difference is that Java has a "type" tag for every value at runtime, so erasing them doesn't work very well. That's not so much a failure of the generics, though, as it is a failure of everything else.
Glad to see type-safe routing spreading to the Haskell web frameworks! Spock is even re-using the path-pieces library that Yesod split out. Servant still has the advantage of including query parameters in the route type and also the response type before it is turned into a content type.
That's a really bizarre assessment. The "enterprisey" audience doesn't particularly care. I like WAI for real practical reasons. The same reasons I liked having a standardized API in every other language I used: I can use everything and not have to worry about adding a whole extra layer of proxying complexity for no reason.
&gt; web based I'd say that's the problem.
outdated
My impression (as an outsider to the BX community) is indeed that they find very well behaved lenses boring and try to find other sets of laws that are still preserved in compositions. I'd like to learn why Haskell lenses require the PutPut law. It is certainly helpful to know that PutPut is preserved in lens compositions to use it for reasoning when it is satisfied. But it is also well known that the other two laws are preserved in lens compositions even if PutPut is not satisfied. So dropping it as a requirement for Haskell lenses would allow for reasoning with more lenses. I wonder if other parts of the lens package actually make use of PutPut and if anything would ~~brake~~ break by dropping it as a requirement. The generalizations that don't have a backwards transformation (like traversals) seem unaffected by an additional law on put functions.
All implications we use have been well known in the BX community. What is not so well known (albeit not new) is that certain implications on put functions alone determine the get function uniquely. 
&gt; When the program is transferred to the Arduino it can operate on its own. Are you sure of that ? According to this [post](http://www.reddit.com/r/haskell/comments/19uwf5/harduino_control_your_arduino_board_from_haskell/) (from the author) &gt;That's correct. That's why the tag-line is "Control your Arduino board from Haskell", not "Program your Arduino board using Haskell." &gt; Unfortunately, getting a GHC (or any other Haskell compiler) run-time on Arduino would be quite challenging, so directly programming Arduino using Haskell is currently a distant dream. Maybe things have changed since that.
ghc-mod when it's not being finicky? That, or hdevtools.
To be fair, plenty of people told them a web based IDE was not going to cut it. The people asking for IDEs mostly already have existing workflows and need tools that work with that, not completely external (and insanely slow) tools.
It's fabulous. It just works. No.cabal hell. All the syntax and error highlighting you might want, with incremental compilation and github integration. Only two things stop it from being my main development environment: no ghci equivalent, and it's web-based. Other than that, it's the bees knees. 
A bit like the Haskell Centre at FP Complete does, you mean? :) 
^ The epitome of constructive. 
The TH in `yesod` is for things that I don't know how to do as nicely at the type level (all the SQL part for example). That said, `servant` demonstrated that you can have routing that way (and it's awesome), so I probably just lack imagination. 
I think there may be some confusion of levels here... When you say "lift" are you referring to the action of the functor on objects? If so, you may read the following. The domain and the codomain of a Haskell functor are not types, they are kinds. Usually, the domain and codomain are both `*`, but it would be possible to give a more generalized signature that operated on different categories. Anyway, the object `f` for which we come to know `Functor f` in Haskell is itself the action on objects. Remember that the objects of the domain are inhabitants of a kind (usually types, then). That is to say, the action of `f` on some object `a` is precisely `f a`. On the other hand, the morphisms of the domain are Haskell values (in the case of `*`, then these are functions between two objects `a,b :: *`). It's easy to get twisted around and think that the objects are going to be Haskell values, and then that there should therefore be something like `lift :: a -&gt; f a` or something, but it's not how it works. The objects are types in this case...
I think you're talking about this https://github.com/ndmitchell/ghcid 
I'm so glad I could help! I remember having the same problem myself, and every once in a while I also get things twisted around.
I used to have that problem with everything but Happstack, which is why it was my first go-to framework. But, most of the others are more consistent now if you build with cabal sandbox. The only one I still have trouble with is yesod.
indeed. Even though he writes a lot and has impressive, Gabriel-levels of documentation. Can't wait for it to trickle down. 
How do you set it up to work with vim? Are there instructions somewhere?
I've been looking at MVC stuff in Haskell, but the cool part about vcache is that I think it seems more low-level and makes (or should, I haven't used it) it easy to persist state across runs, among other things. without a whole database. I think. http://www.haskellforall.com/2014/04/model-view-controller-haskell-style.html https://hackage.haskell.org/package/vcache-0.2.5/docs/Database-VCache.html#t:VCache
There may not be something wrong with the web-based IDE, but we still need one that isn't web-based.
Neither does SBT. I have a bash script that calls SBT and pushes the errors to a file which vim then reads. I assume I would need to do the same here but that's semi easy.
This is really great. I'm glad that type-level lists are becoming a "stable" feature.
I'd argue that dropping the law allows _less_ reasoning, but more lenses. ;) With the PutPut law in place you can reason that "no information is lost", and that the lens itself is effectively "linear". You can express the lenses that can be written in linear logic, even. The problem is that the foundations we build upon all make pretty fundamental use of that law. You can weaken that foundation, but then you lose the interpretation of lenses as costate comonad coalgebras. We don't use that but that encoding for traditional lenses follows a _consequence_ of the backend that we do use. That encoding was useful for finding prisms, defining traversals, letting us talk about indexed constructions, type-changing, etc. _all_ of which are outside of the vocabulary of the bx community. So, yes, we could go chase after implementations that permit more lenses but less reasoning about them, but the way that we found these other things was by exploring the variant with the `PutPut` law, and I don't know how to recover them as anything other than an ad hoc construction with a weaker `PutPut`. (Mike Johnson and Bob Rosebrugh _have_ made some nice attempts at moving in a more general direction.) e.g. the way I think of lenses these days is in terms of generalizations of a scheme for isomorphisms. type Iso s t a b = forall p. Profunctor p =&gt; p a b -&gt; p s t to different notions of strength. type Lens s t a b = forall p. Strong p =&gt; p a b -&gt; p s t type Prism s t a b = forall p. Choice p =&gt; p a b -&gt; p s t subject to the linearity conditions above, but `Strong` and `Choice` are both notions of 'strength' for a `Profunctor`, just with regards to different monoidal categories. I find this viewpoint endlessly more satisfying than the viewpoint that is encumbered by get/put. Why? Because it gives rise to prisms, but it _also_ gives rise to looking for lenses into things like functor composition stacks when you pick the category to be [Hask^Hask] and the tensor to be functor composition. (So you can write a "lens" in this generalized setting that picks out the `Reader s` structure from `State s` because `State s` is the composition of `(-&gt;) s` and `(,) s`. You can find other interesting monoidal categories to construct lenses over. For instance, we know Monads don't compose, but we can make a category where the objects are Monad transformers, and the arrows between them are monad transformer homomorphisms. In this category we can define the tensor of two monad transformers `s` and t` to be `(s * t) m a = s (t m) a`, and then we can build lenses using the aforementioned notion of strength. Now lenses into monad transformer stacks let you run simpler monadic actions in a larger monadic context. This is nice, because the usual style we convince people to write mtl code in is to work abstractly: foo :: (MonadState s m, MonadReader t m, HasFoo s, HasBar t) =&gt; m Int which is grossly inefficient unless inlined, because it has to look up how to do anything with `m` in a dictionary, but here you can write a concrete action: foo :: StateT Foo (ReaderT Bar) Int and embed it into an arbitrary monad by using the appropriate notion of a lens and iso. (e.g. we can commute `(StateT s * ReaderT e) &lt;-&gt; (ReaderT e * StateT s)`, but some other transformations are directional. So, I too find the "well behaved lenses boring", but I find them boring because they are a special case of a much larger construct that has been productively producing interesting structures, and I don't know how to weaken that structure in an analogous way to the PutPut weakening while preserving any of its interesting properties. FWIW- The `lens` library doesn't care about the laws in general. You can work with "improper" lenses or prisms with that library all day and the operations all make operational sense. The thing that you lose, however, in the presence of a weaker PutPut, or even merely idempotent variants is the ability to know that the combinators in `lens` are canonical. Right now they are equivalent to any other combinator that achieves the same thing. In the presence of the weaker PutPut law "the" unique implementation often explodes into 2 or more, often dozens, of alternative implementations which now yield different answers. The power of reasoning seems to be definitely on the side of the construction that avoids reasoning about such operational minutiae.
https://github.com/syl20bnr/spacemacs/ has a vim setup and very good (very very good) haskell support out of the box.
You just have it print out the requests and you send back hand-written packets back? That's pretty hardcore.
The effect is pretty impressive (quite hypnotic), and the code is very understandable for those that already have some Haskell and diagrams. Nice ! :-)
For vim, theres plugins like: * https://github.com/bitc/vim-hdevtools * https://github.com/eagletmt/ghcmod-vim * https://github.com/eagletmt/neco-ghc * https://github.com/lukerandall/haskellmode-vim
I don't actually want an IDE, but if I did, I'd want one that was free as in freedom, not free as in freemium. 
Note that you can also expand `foldl`, so that's one more way to write it. There is whole part of obfuscation that is about transforming programs into more complex-looking ones while retaining their semantics. Its main use is to slow down reverse engineering, or even discourage it.
`product /= product'`. What happens on the empty list in the first case? What happens in the second? That being said I think you're looking for eta-transformation.
Like all things, it's also in [lens](http://hackage.haskell.org/package/lens-4.9.1/docs/Control-Lens-Zoom.html) :) How would it work if you had `(Resource1, Database1, Database1)`? Or for something that isn't a tuple ? I suppose that you should write a `Wirable` instance.
I'd rather have less safety than have TH. I actually like that Snap doesn't have type safe routes in the yesod sense, because they seem like the complexity outweighs the benefit. Maybe I just dislike TH because it's so unfamiliar, idk.
er, I put this in ghci and they act the same. Do you mean on the inside they are different?
Doesn't work with local tooling. Unconventional licensing. Reinvents *everything* instead of just integrating the Haskell-specific tools into an existing IDE that is already very good for, say, SQL code or which hooks into your Vagrant setup. Oh, and as several other people already said, it doesn't work without an internet connection. On the other hand, the only drawback to local tooling is that you need to install it. I'm not sure I'd call that a drawback.
Haskell style is to use camelCase, so `myReverse`, not `my_reverse`. In your lambda, what is the type of `x`? What is the type of `acc`? What is the type of `++`?
 λ&gt; let product ys = foldl (\ acc x -&gt; x*acc ) 1 ys λ&gt; let product' = foldl1 (*) λ&gt; product [] 1 λ&gt; product' [] *** Exception: Prelude.foldl1: empty list The both don't behave the same on empty lists, so they are not the same function.
I suspect he means the vimmish mode: [How to enable vim or Emacs key bindings](https://fpcomplete.desk.com/customer/portal/articles/1294419-how-to-enable-vim-or-emacs-key-bindings)
Sure, so the tutorial you had was simple enough to follow though I didn't understand how id had the right type but whatever. The main problem was doing something more complicated. I was looking at your blog code and I didn't really understand these type SessionVal = Maybe SessionId type BlogApp = SpockM SqlBackend SessionVal BlogState () type BlogAction a = SpockAction SqlBackend SessionVal BlogState a runBlog :: BlogCfg -&gt; IO () runBlog bcfg = do pool &lt;- runNoLoggingT $ createSqlitePool (bcfg_db bcfg) 5 runNoLoggingT $ runSqlPool (runMigration migrateCore) pool runSpock (bcfg_port bcfg) $ spock sessCfg (PCPool pool) (BlogState bcfg) blogApp I didn't understand how I could modify any of this code for myself. I suppose reading a wai tutorial would help? Whereas using yesod I didn't have to think 
I think the best thing to do for you now would be to look at the type of your my_reverse, type the following in your ghci : :t my_reverse Not the result you expected, right ? See ephrion answer as to why this is the case. You also get an unhelpful error message (unhelpful to you yet, with a bit of experience it becomes much clearer) because of a particularity of Haskell : the numeric literals are polymorphic, that is when you write "1", the program sees it as "fromIntegral 1" which could convert 1 to any numeric type, in other words to any type that is an instance of the Num typeclass. Here the context expect a list and you give it a number, but because of what I just said, it doesn't tell you that, it tells you that it can't convert your number into a list because there is not instance of Num for [a] (the "list of a" type)... A bit confusing, on the other hand this flexibility is very nice sometimes, especially in EDSLs.
just nums, I don't think it matters what the type is, it just reverses a list
I didn't mean for 'full' to be attached to just isomorphic. What I meant to say is: a full fledged, isomorphic, virtual-dom based server+client side framework. Where 'full-fledged' means 'has all the important stuff' and where 'isomorphic' refers to being able to write code once but run it on the client or on the server. Similar to this, but in Haskell, http://nerds.airbnb.com/isomorphic-javascript-future-web-apps/ Does that clear things up? 
The code you need is my_reverse ls = foldr (\x acc -&gt; acc ++ [x]) [] ls About the error, I think the reason you get it is because GHC thinks x is a list because you use it as second argument of (++) that has type [a] -&gt; [a] -&gt; [a]. Probably you are passing a ys that is a list of ints and GHC gets confused because x should be a [a] but you are passing an Int.
Oh, I see. I had not expected to see that usage of "isomorphic" on this side of the tracks. Thanks for clarifying.
Leksah, Eclipse FP and various newer attempts are so ignored by everyone that they are not even in this Comic.
If SublimeHaskell actually worked it would have been a perfect IDE for me...
Snap is more like a library than a framework. 
I think with `lens` you would probably tackle the problem like this: userLookup :: (HasResource1 c, HasDatabase1 c, MonadReader c m, MonadIO m) =&gt; Int -&gt; m User ordersLookup :: (HasResource1 c, HasDatabase2 c, MonadReader c m, MonadIO m) =&gt; Int -&gt; m [String] composedLookup :: (HasResource1 c, HasDatabase1 c, HasDatabase2 c, MonadReader c m, MonadIO m) =&gt; Int -&gt; m String where `HasResource1` etc come from the `makeClassy` and friends. You can use this with `MonadReader` and `MonadState`, and if you use classy prisms for errors you can also use it with `MonadError`. If you enable `ConstraintKinds` you can set up aliases for some of the constraints to make things less chatty. It's a nice way to get rolling with some code quickly. I understand that some folks like to be more explicit about their monad transformer stacks, but in that case both the `lens` and the `wiring` based solutions are off the table - although it looks like you could use either approach for a prototype and then make things more explicit later. As a sidebar for those who like being more explicit about these things - and because we use hearsay and folklore for library discovery - [mmorph](https://hackage.haskell.org/package/mmorph-1.0.4/docs/Control-Monad-Morph.html) is excellent, well-documented, and probably just what you want. [hoist-error](https://hackage.haskell.org/package/hoist-error) can also be pretty handy when you need it.
Happy to explain to the best of my ability; it's two ways to present a family of signatures (not related to generativity, as far as I know). In short, the terms in this context mean roughly the same thing as they mean in mathematics. (i.e. two different ways of defining families) The first way to define a family of sets (or signatures, or types) indexed in `I` is as function `Set^I` (this is parameterization). The second way to define a family is in terms of display maps (fibrations) from some total space (i.e. `Σ[ X : Set ] X -&gt; I`); this is fibration. Here's a proof by Paolo Capriotti that the two are equivalent: http://www.paolocapriotti.com/pages/display/display.html Anyway, for type classes the choice is clear: parameterization must be used, since you need the classes to be predicates `C a` such that the resolution algorithm can find a suitable implementation by inspecting `a`. On the other hand, with modules there has not been any pre-commitment to a particular resolution mechanism, so we are free to choose the approach that results in the least amount of churn when we move stuff around. And it turns out that fibration is better in this respect; it lets us build up composite signatures without privileging any one particular piece of the signature unduly over the others; then, mutual coherence of signature components may be ensured using sharing constraints (as opposed by causing it to be the case by construction, through ornate/brittle &amp; carefully arranged nested signature parameterizations &amp; instantiations). Harper &amp; Pierce give a good explanation of why fibration is more practical: https://books.google.com/books?id=A5ic1MPTvVsC&amp;lpg=PA323&amp;ots=PopFhJcC4s&amp;dq=%22fibration%22%20%22parameterization%22&amp;pg=PA323#v=onepage&amp;q&amp;f=false
Right -- I think it's totally reasonable to say "I wish this particular error was captured in the types, in a way that reminded me to handle it." The trouble is that not everyone agrees about where that line should be drawn, so there's a little friction between different libraries. It would be cool to see a language that let the user move that line in a fine-grained way -- eg. "This program should handle all file-related exceptions, except in the logging system" -- but I haven't come across any practical examples. 
I hadn't tried darcs before but I heard it has a better large-binary-asset story than some other VCSs so today I gave it a shot. Guess I heard wrong. I checked in a 600MB Ubuntu `.iso`, recorded it, used `tail -b +10` to remove a few kB from it and recorded it again. In the several minutes my computer took to do this, darcs memory consumption shot to 15GB (my computer only has 8GB RAM; good for OSX not crashing or having to kill the process). I can't understand this. All darcs seems to do is record a gzipped-patch file with a plain-text hex representation of the binary file. There's almost no work involved. It should be able stream the process very simply with constant memory.
https://www.youtube.com/watch?v=cefnmjtAolY&amp;feature=youtu.be&amp;hd=1 very good video. 
Whatever works!
or a config that combines them harmoniously and includes an install script. https://github.com/begriffs/haskell-vim-now /selfpromotion
`reverse` can be implemented with `foldr`. In fact, any function on lists can be theoretically implemented with `foldr`, although the implementation might not necessarily be understandable. However, in this case there is a pretty simple implementation of `reverse` in terms of `foldr`.
Isn't there a tension between: 1) community widely adopts closed-source web-based tool 2) FOSS is good ?
cool makes sense 
I've started using eclipse fp for a project lately. It's kind of nice. I'm liking it more than spending hours trying to get emacs to do what I want anyway. 
I tend to believe it's because Agda is very miserable to use without editor support.
FP Complete is no longer pursuing the web-based IDE. Instead, we're working to bring the "no cabal hell", "syntax and error highlighting", "incremental compilation" etc to your local editor/setup of choice. Stay tuned... For now I'll just say "no cabal hell" can be approximated by adding http://stackage.org to your workflow.
There *is* something `(.)` can do that parentheses can't: *be a function.* Toy example: λ. let ops = [(+7),(*3)] λ. :t ops ops :: Num a =&gt; [a -&gt; a] λ. let composeList = foldr (.) id λ. :t composeList composeList :: [b -&gt; b] -&gt; b -&gt; b λ. :t composeList ops composeList ops :: Num b =&gt; b -&gt; b λ. composeList ops 1 10 **Edit:** Added a step for clarity. **Edit2:** Not sure why I called it a map. :P
Yes, which is why responding to questions about "will acid-state be able to replace a database" with "it can just buy more RAM" isn't a good idea.
Yes exactly, the fact that Agda-mode is developed and maintained as part of Agda itself is a huge point. So why not do the same with GHC?
Which is why I went on to explain when that would be a practical choice and what paths we are considering for when it is not.
Thank you. For bothering, for trying. 
Thank you for writing this up. It's a lot of food for thought that will take me some time to digest. For now, I only have a question to your final remark. Why do you consider the combinators in the lens package canonical? In general, there may be more than one put function for a given get function even if PutPut is required. What is special about your combinators that makes the put function unique? 
&gt; I understand that some folks like to be more explicit about their monad transformer stacks, but in that case both the lens and the wiring based solutions are off the table Using `zoom` and `magnify` is explicit.
You should use `traverse` instead of `folded`, because it limits you to a `Getting` or something. I don't personally mind writing these.
Does anyone actually use Yi or Leksah to do real work? Maybe other Haskellers are fine with Emacs or Vim as a full IDE.
Would reactive-banana be relatively easy to sit on top of your bindings? 
I'd be in favour of this. I've always found it weird to type out `currentUser . _Just . userRole` (or `traversable` instead of `_Just` as I now have learned!)
Generate docs, generate cabal packages, run tests out of a generated dist, sanity check my packages (copyright year, linked fields etc). http://github.com/ndmitchell/neil is the code, http://neilmitchell.blogspot.co.uk/search/label/neil are a couple of posts about a few of the things it does.
[I made this for you.](http://stackoverflow.com/a/29742635/1463507)
It's only a matter of time before trains will have wifi. Here in the netherlands they already do, although it does not work well yet :( You could also use internet tethering with a mobile phone.
Huh, cool, it's like the utils section of my .emacs but packaged in an external tool. Never even considered that option before.
I don't want to be a downer, but what is the point of darcs when there is git (and hg)? Seems like there is no advantage, and git is versatile enough to enable pretty much any workflow.
My main application has switched frameworks two times. I started out with Shed (https://hackage.haskell.org/package/http-shed) in 2008, I used a homegrown serialization library based on Read/Show instances(!). Performance was not good. In 2009 I ported to Happstack and acid-state, big performance win and cleaner code. In 2011 I ported everything to Snap, keeping acid-state. For my new projects I now use Snap, mainly because that is what I have fresh in memory. So I have customer data dating all the way back to 2008, that has survived 2 major migrations (plus one major version upgrade of SafeCopy that changed the binary format).
Yi is actually [extremely old](https://github.com/yi-editor/yi/commits/master?page=157). Not that that makes it good, but being too young is not the problem.
I'm a darcs developer and I think you did hear wrong, I don't think we've ever advertised a good large-binary-asset story. Apart from the memory usage, you'll end up with a 1.2GB patch file (modulo any compressibility) from the change, which won't really be very pleasant to work with. We'd like to fix that but it'll require a repository format change so won't be trivial to do.
Darcs is simpler to use than either git or hg (fewer commands). It also requires less merges when collaborators work on separate files (because the patches often commute).
We've discovered your repo after we've started, so we decided to do our own thing from scratch. However we definitely will have a look at your code :) We'll see if we can provide the compression functionality - but this is last prio for us as well.
Hah, I forgot about them :) I was mostly thinking about people who prefer to explicitly `lift` rather than using the `mtl` classes.
They have wifi, it just doesn't work. My mobile (and contract) doesn't support tethering, but I'm not sure I'd want to use the amount of bandwidth required for an IDE over my mobile data plan anyway.
If you use _Just instead of folded or traverse, you can still use it to get a Prism
&gt; Getting everybody to agree on API/features would be a goddamn nightmare. I'm not so sure, every attempt at IDE features for haskell provides the same information: jump to definition, type at position, completion info, and mostly they do it by wrapping GHC API. If GHC provided a higher level API to do this then I'm sure people would use it (and contribute extensions if they needed them). As things are, I'm expecting tools to coalesce around ide-backend in the near-mid term; it's had a lot of engineering behind it that doesn't really need duplicating.
I know, I was just joking :) Most people respond with Snap or Yesod or Scotty or Happstack (or ...), which has a lot more batteries included, hence the jab ;)
I use SourceTree (a great GUI wrapper around git) and essentially never worry about commands, don't even need to know them for the most part. Not sure I understand deeply the concept of patch-centric vs file-centric but I think I might wonder how much that really matters in actual practice, even if it's theoretically better. 
After looking at the [WxHaskell reactive-banana adapter](https://hackage.haskell.org/package/reactive-banana-wx) I don't see why not. WxWidgets and FLTK have the same event-loop/callback model and the Haskell bindings are as close to C++ as possible.
Here are the first of many links that come up in a simple Google search for how to make a 4x4x4 led cube with an Arduino controller: * http://www.instructables.com/id/LED-Cube-4x4x4/ * http://www.instructables.com/id/The-4x4x4-LED-cube-Arduino/ * http://www.techmadeeasy.co.uk/2013/01/21/make-your-own-4x4x4-led-cube-with-an-arduino/ * http://www.freetronics.com.au/products/cube4-4x4x4-rgb-led-cube#.VTTZ7M6221k
Yes. But to clarify, the latter. While I'm at it, I should fess up to the stipend typo. We haven't won the lottery.
The [contributors page](https://github.com/dmbarbour/awelon/graphs/contributors) for awelon shows that most of the development occurred during the first half of 2014, and not much has happened since then. The [contributors page for vcache](https://github.com/dmbarbour/haskell-vcache/graphs/contributors) shows that it was mostly developed during January and February.
This is a very important step to bring sane package management to Haskell. The next steps could look like this: * Packages with relative paths * Caching of binary packages * Bringing package dependencies to the type system level (so you can safely mix different versions of packages in the same project and an IDE can highlight package conflicts while you type) Something similar already exists in the Java world with Maven/Ivy and OSGi. For example when you develop an Eclipse plugin, you can mix different versions of OSGi modules, as long as you keep their respective types separate. Only when you bring the types together, you have to cast/ convert to a type that both versions of the module share in the same version (typically from some stable base packages). Haskell could even go one step further and take the target platform into consideration: e.g. Intel x64, ARM, JavaScript. So you could reason about a distributed system spanning browser, server backend and IoT sensors in an enhanced Haskell type system.
Would you like to share that code? I use vim as well, and would be interested to know how to feed it such a list of errors from outside.
&gt; It would be extraordinarily challenging, and quite possibly impossible, to compile generic Haskell code to run on most Arduinos. GHC certainly isn't up to the task, as the chips just don't have nearly enough RAM and Flash memory. Do you mean compiling directly on the Arduino or cross-compiling and executing it ? I'm not expecting to install GHC on the Arduino itself , so I'm happy to cross-compile (as long as it's doable). 
Thanks !. Not sure I want to up or downvote :-)
You can get a pocket mobile hotspot for pretty cheap now if phone tethering doesn't work for you. That doesn't help the data plan issue, of course, but I would do some empirical testing of that; your train time might turn out to be less data-expensive than you think. That said, I wonder if FPC might be able to come up with, say, a server-in-a-docker-image solution without too much effort.
That's the problem, you have the wrong attitude. You have to let emacs get *you* to do what *it* wants.
+ It knows about the history and [won't fuck up merging](http://r6.ca/blog/20110416T204742Z.html); + It's 100000x easier; + It's consistent, most commands works for *any* patch you like (not only the last); + Zero setup on server; 
The static analysis for this kind of partial evaluation is usually called *binding time analysis*, for those who want to read more deeply into this. Binding time analysis is a form of *abstract interpretation*, which is the general term for trying to tell something about runtime behavior with only static analysis.
[I asked a similar question a few weeks ago](http://www.reddit.com/r/haskell/comments/31e8ek/i_want_to_build_an_android_app_but_im_an_amatuer/)
Instead of using a light sensor you could just use a clock and look up a table on line of sun rise in your area each day. That would be more reliable of you had some cloudy weather or an artificial light near by. 
In this example, `(^?)` is already limiting it to a Getting (First a). Can you give an example of when using traverse instead of folded for the implementation of (?.) would be advantageous? I have a hard time wrapping my head around the actual benefits of that generalization.
A git commit is a snapshot of the working tree at that point in history (together with some metadata). darcs records changes to files. Thus if you examine a git commit in .git/objects, you get a load of files. A darcs patch is stored as a collection of atomic "hunks" (renames, added/removed lines).
I'll just leave this here: https://github.com/haskell/haskell-mode
I think your specific use case is perfect for lenses, and I'm sure someone here will show up very soon to show you how to do so (I wish I could), but for now, have a read of [this](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html). If lenses are too far for you, why not take advantage of the [Num typeclass](http://hackage.haskell.org/package/base-4.8.0.0/docs/Prelude.html#t:Num)? instance Num Stats where Stats h m + Stats h' m' = Stats (h + h') (m + m') Then your first example would be completely correct.
For your first example I would probably use lenses. You could make your ``Stats`` type a monoid if you want to concat them as in your example, but in my opinion it doesn't feel right. Using lenses you could write this: data Stats = Stats { _health :: Int, _mana :: Int } $(makeLenses ''Stats) Now do the same for ``Player`` and you can do this: heal :: Int -&gt; Player -&gt; Player heal hp = over (stats . health) (+hp) Lenses are extremely useful when you want to do nested changes to datastructures like you do here. Regarding JuicyPixels, have you looked at the [helper functions](https://hackage.haskell.org/package/JuicyPixels-3.2.3.1/docs/Codec-Picture-Types.html#g:6) that are defined in the library? If you look at the definition of ``pixelMap`` it requires its arguments to be of the typeclass ``Pixel``, so I would guess that's why its not a Functor. I think you should be able to accomplish what you want with either the ``pixelMap`` or one of the ``fold`` functions.
I was referring to the operations of the other combinators that take a given lens/traversal/prism and do something with it, not to the get/put operations themselves. In a world with the PutPut law you can't care if a combinator does a `get` then `put`, then `get` again and `put` again, or if they fuse it into a single pass by choosing an appropriate `Functor`. In `lens` we always fuse things together into a single pass with an appropriate choice of `Functor` rather than multiple rounds of `put`/`get`. This choice is valid because we know you can't care how many passes we take. The PutPut law gives me the freedom to optimize. Now, I didn't make a claim that the `put` law was determined by `get`, but in the presence of `PutPut` what is an example of such an ambiguous `lens`? I'm not able to construct one off hand. e.g. It strikes me that the counter-example in http://www.cs.cornell.edu/~hpacheco/publications/fm14.pdf isn't admissable in lens. Why? If you put in a shorter list you destroy positional information that is needed for a subsequent put to pass the put get laws. put (put s []) (get s) /= put s (get s) In lens we're often effectively using a representation of a lens as an isomorphism: exists c. forall i. S_i &lt;-&gt; (c, A_i) c is a "constant complement", determined up to isomorphism by the operation of the getter. This view is only valid in the presence of the `PutPut` law, without it you get something much weaker.
Generally Haskell does indeed demand that you state the properties you want of your types. It is not *automatically* clear that `(+)` on `Stats` ought to be pairwise addition, although a reasonable person might guess as much. Thus, it's required that you explicitly demonstrate that this is what you meant. (By the way, this is why people like typeclasses with laws since they help to mandate what it *actually means* for a type to "have `(+)`". Abstract algebraicists at this point start whinging that `Num` doesn't appropriately demonstrate the structure that lets it earn its chops as an algebraic ring or whathaveyou, but gentlepeople can agree to pretend that we know exactly what it means to have a sensible instantiation of `(+)` :) Now, what you've done in Scheme is not so dissimilar, if much less explicit. When you call `(+)` on stats in Scheme you are recognizing that your `Stats` "type" "has addition" as it *inherits* it from the underlying list structure. This can be done in Haskell too! {-# LANGUAGE GeneralizedNewtypeDeriving #-} newtype Stats = Stats (V2 Int) deriving ( Eq, Show, Num, Ord, Read , Ix, Generic, Storable, Binary, Serial , NFData, Hashable ) -- you get the point... What we've done is noted first, via the `newtype`, that `Stats` shares structure with `V2`. Then we've lifted all kinds of instances up from `V2` to stats. Now what we've lost is the information about the semantics of the elements of `V2`. This is one place where lenses come in handy: health, mana :: Lens' Stats Int -- implementation deferred With this interface we have maximally abused the shared structure between `V2` and `Stats`. This is fairly similar to the Scheme version—the "appended" information denotes what the first and second elements of the vector "mean" and then operations arise from shared structure. Here's `health` and `mana` btw. health :: Lens' Stats Int health inj (Stats (V2 h m)) = fmap build (inj h) where build h' = V2 h' m mana :: Lens' Stats Int mana inj (Stats (V2 h m)) = fmap build (inj m) where build m' = V2 h m'
Okay, this is an interesting answer. So, with `newtype`, I can demand any instance from the base type? How would you use this technique to solve JuicyPixel's API issue? Would you create a newtype from Vector?
A quick note about the `Image` type from JuicyPixels: It can't be an instance of `Functor`, because its parameter is restricted to Pixel types. This is done by design so that when you have an `Image` it is guaranteed to be an actual view-able image. Implementing `fmap` would forfeit this guarantee. You can always use `imageData` to access the underlying `Vector`, which *is* a `Functor`, `Applicative`, etc.
Yes, and my guess would be that the instances doesn't exist because they don't abide by the laws of the typeclasses. The point of a type system like Haskell is to disallow things that doesn't make sense. That is why for example a ``Set`` doesn't have a ``Functor`` instance and instead it defines its own map function with ``Ord`` constraints ``map :: (Ord a, Ord b) =&gt; (a -&gt; b) -&gt; Set a -&gt; Set b`` and a warning that the mapping might change the size of the set. However in my experience its almost always possible to minimize boiler plate with some simple combinators. Please do show some more examples where you are having trouble with excessive boiletplate and Im sure that I or someone else can show you how to reduce it. 
One option is to add a Num instance for tuples, and take advantage of newtype deriving. -- add an instance of num for pairs instance (Num a, Num b) =&gt; Num (a, b) where (x,y) + (x2, y2) = (x + x2, y + y2) ... newtype Health = Health Int deriving (Eq, Ord, Num, ...) newtype Mana = Mana Int deriving (Eq, Ord, Num, ...) type Stats = (Health, Mana)
I understand all that you said but ... if **you** needed an image type on your game and you knew you would need to fold over it, traverse, and so on, what exactly would **you** do?
When you are setting (ie. with `.~`).
Ah, in that case then yeah I'd probably have a thin wrapper over `Vector` without restricting the types like was done here. I'd push the demand that the types inside the vector are "pixellike" out to the operations which actually treat an `Image a` as an image. That would let me lift lots of structure from the underlying `Vector` without issue. But it may be that this is where things began with JP and it moved to the more restricted type for some reason or another.
That's *probably* okay in an application (as opposed to a library) but it's just begging to be hit by an orphan collision.
Memory required for the commit aside, if darcs works on changes, wouldn't it ultimately just write a small commit that is composed of taking out the last few bytes?
Is git?
You can use PatternSynonyms to make this a bit nicer: {-# LANGUAGE PatternSynonyms #-} newtype Stats = Stats' (V2 Int) pattern Stats h m = Stats' (V2 h m) health :: Lens' Stats Int health f (Stats h m) = Stats &lt;$&gt; f h ?? m mana :: Lens' Stats Int mana f (Stats h m) = Stats h &lt;$&gt; f m
I see but how do you apply that to my problem practically? ...
Aww. And you got my hopes up that this was either present or forthcoming.
[She](https://personal.cis.strath.ac.uk/conor.mcbride/pub/she/higpig.html) can do some of these things. However its approach does not scale for large projects.
Ah, then I'm glad to have been mistaken. Thanks for making that clear. Does it make sense to set up a repository for working on TUF-for-Haskell? Or would all the work just go into existing projects?
Since darcs actually tracks changeses, you get branches for free. If you commit two separate changes, they will be recorded as completely individual "branches" in the history, and can be handled, applied and unapplied individually. Git requires you to manually create branches to deal with that.
Yes, it's one of those things that seems like an annoyance at first, but it's actually a really powerful concept in its own right! It also does things like allowing you to trivially abort a failed merge (unlike, say, SVN did) and trivially get back to a working state. Oh, and a second feature of git that I haven't seen anywhere else is the reflog. (Maybe darcs has something similar. It's been ages since I used it.) I believe this is fundamentally predicated on git tracking "content" instead of "patches", but there's no denying that it's incredibly useful to rewind everything to a state before you did "bad thing X".
My guess is that /u/hastor wants to know that if they clone a repository from a server, they actually get what they should be expected - not code that has been tampared with. Git in this respect is basically a Merkle tree, so if the top commit sha agrees, then so do all the children.
&gt; Git requires you to manually create branches to deal with that. In practice this usually happens with trivial spelling commits (and such) and you can just cherry-pick these over to master and rebase your original branch. (I realize that this isn't quite as automatic, but it *is* mechanical and can be automated if you really want to.)
I know 4x4x4 is possible, but in my case 2 pins multiplied by 4x4 lines plus 4 pins would look for positive digital output, because each LED lamp has to light up two colors, not only one. I can't handle so many pins without some more materials that I can't afford. Thank you for searching and suggesting, but this time I'd go with 3x3x3 and try that later. :)
You may want to use `type` instead of `newtype`. `type` defines an alias. So you might have: type UserId = Int type GroupId = Int addUserToGroup :: UserId -&gt; GroupId -&gt; IO ()
If you can provide a `Iso` to the stored values then you can use [my `total` library](http://hackage.haskell.org/package/total) to pretend that you are pattern matching on the `Prism`, like this: _Stats :: Iso' Stats (Int, Int) _case &amp; on _Stats (\(h, m) -&gt; ...)
This is why `over` (from `lens`) is more flexible and general than `fmap`. With `over` you could provide a traversal to the pixels: pixel :: Traversal' Image Pixel ... and then you could use `over` to map a function over each pixel: over pixel :: (Pixel -&gt; Pixel) -&gt; Image -&gt; Image The nice thing about `over` is that `fmap` is just the special case where you provide the setter `mapped`: over mapped :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b So in a sense, `over` generalizes the `Functor` class. When you need to automatically infer the instance using a type class you supply the `mapped` setter, but when you need a monomorphic map you just provide a monomorphic traversal as the argument. For just a little extra verbosity you gain a lot more flexibility.
My first draft had &gt;= £13K, but then I was told the precise figure: I replaced the 13 but foolishly failed to remove the K. Other than that, the figure is accurate. That's a per annum figure, and the UK does not consider it taxable income. Our institution pays monthly. Any earnings arising from work as a teaching assistant will supplement that stipend. If the student obtains an adequately paid internship, we can suspend the studentship and the stipend for its duration (typically three months), effectively extending the window for completing the PhD. Such a stipend package is fairly standard for PhD studentships in the UK.
The example you mention indeed does not satisfy `PutPut` so it only shows the ambiguity for well-behaved lenses (not *very* well-behaved ones.) Here is a small example showing two different very well-behaved lenses with the same `get` function: type Source = (Bool,Bool) type View = Bool get :: Source -&gt; View get = fst put1 :: Source -&gt; View -&gt; Source put1 (_,y) v = (v,y) put2 :: Source -&gt; View -&gt; Source put2 (x,y) v = (v, (x == y) == v) Both `put1` and `put2` satisfy `PutPut` and form a very-well behaved lens with `get`. As you point out, `put` functions in very well-behaved lenses change some part and leave a complement constant. The complement does not need to be unique though. In the above example `put1` preserves the second component and `put2` preserves equality between the two components of the source pair. In the paper linked from this post, we give a similar example with a pair representing picture dimensions. When changing the width it is reasonable to preserve the height of a picture. It is also reasonable to preserve the aspect ratio and both updates are very well-behaved.
That is true. You did not mention `type` in your post, so I thought I would mention it. 
What is the point of git when there is darcs?
You might want to look at the [`newtype`](https://hackage.haskell.org/package/newtype) package.
Haskell doesn't only happen in Hask, it's just that `Functor`, as it's written, captures a Hask endofunctor and this is probably the most useful one. With type families and GADTs you can pretty easily construct functors between other categories. With constraints laid upon the type variables you can pretty easily write functors to subcategories of Hask. As a dumb example consider the category of natural numbers where an arrow `n ~&gt; m` implies that `m` is a multiple of `n`. {-# LANGUAGE DataKinds , TypeFamilies , TypeOperators , GADTs , KindSignatures #-} data Nat = Z | S Nat type family Plus n m :: Nat where Plus Z m = m Plus (S n) m = S (Plus n m) data Vector :: * -&gt; Nat -&gt; * where Nil :: Vector a Z Cons :: a -&gt; Vector a n -&gt; Vector a (S n) append :: Vector a n -&gt; Vector a m -&gt; Vector a (Plus n m) append l r = case l of Nil -&gt; r Cons a as -&gt; Cons a (append as r) data n ~&gt; m where NilA :: n ~&gt; Z ConsA :: n ~&gt; m -&gt; n ~&gt; Plus n m -- Witness to the fact that `Vector a` is an object map from Nat to a subcategory -- of Hask, Hask[Vector]. nmap :: (n ~&gt; m) -&gt; (Vector a n -&gt; Vector a m) nmap x v = case x of NilA -&gt; Nil ConsA q -&gt; append v (nmap q v) There's also the entire language of contravariant (Hask, endo)functors. But even for just covariant Hask endofunctors I'm not sure of any other general purpose terminology besides `Functor`. There's `Mappable`, I suppose, but I'm not a huge fan of that as it tends to have an (overly strong, perhaps) association with list mapping and that stops making sense with types like `Const e` and `Cont r`.
&gt; because... well... you know. I don't...
Yes, you can define this by using the `traverse` of the underlying `Vector`. If this is all that the poster wanted to do, then `lens` would not be necessary. The problem is, we need a traversal over the whole pixels rather than the pixel components (or at least, that is how I interpreted the comment I was responding to). So the traversal would have to construct the pixels from the components while stepping through the underlying `Vector`.
With [Halcyon](https://halcyon.sh/), the *first* build could be instantaneous, too. :) Assuming, that is, that you aren't the first person in the world to run it, and assuming someone is hosting an S3 bucket for e.g. LTS Haskell collections. Nudge, wink.
Is there a github equivalent for darcs? EDIT: oh, I've found it, I'm quite curious why I can't stumble upon [hub.darcs.net](http://hub.darcs.net/) with simple google searches. 
It should, and this is what it does for text files. But for binary files it doesn't do any diffing so stores the entire old file and the entire new file.
Sometimes, you just need to frameworks to get out of the way. ;)
&gt; You still had to ... Here's it without the ... instance (Num a, Num b) =&gt; Num (a, b) where (x,y) + (x2, y2) = (x + x2, y + y2) (x,y) * (x2, y2) = (x * x2, y * y2) abs (x,y) = (abs x, abs y) signum (x,y) (signum x, signum y) (x,y) - (x2, y2) = (x - x2, y - y2) fromInteger x = (x,x) 
Thanks. Sorry if this sounds inane, but I don't understand in which was you use `type`. I'm trying to make FUNCTOR fit with Haskell's Functor, but I can't quite do it. Are you using `type` analogously to `*`, s.t. every object in the category is a concrete type and every arrow a type constructor `* -&gt; * -&gt; *`? Or is `object` an ordinary function that returns the *values* of a given type/set `S`, and `arrow` a function `S -&gt; S -&gt; ArrowSet`?
So, a sufficiently polymorphic framework is isomorphic to a library via [IIoC](http://www.thev.net/PaulLiu/invert-inversion.html). I wonder if this has any deep connection to [holomorphic projections](http://www.pnas.org/content/111/11/3961.long).
This smells like pattern conjunction, which smells useful.
Note again that the "need" to implement them is a pretty direct consequence of explicitness of types. In Scheme you can "dynamically cast" between a value and its representation (there's no practical difference except in your mind anyway). You also have to make sure, by hand, that the thing you're mapping/folding over is what you think it is. If the extra explicitness is painful then you'll probably have to get used to facing it anywhere you work with static types.
I know, I know. I'm not denying the numerous benefits of types, but when you have a deadline and you find yourself not being able to do things in time the balance starts weighting... 
Making the first build nearly instantaneous is definitely the direction we are going for. Halcyon declares itself to be "a system for installing Haskell apps". In contrast, stackage-cli is targeted more at *developing* Haskell apps and libraries (e.g. manage your cabal.config, manage your sandboxes). That's one reason why there are some differing design decisions in play here.
Also [Control.Lens.Wrapped](https://hackage.haskell.org/package/lens-4.9.1/docs/Control-Lens-Wrapped.html).
Oh, I agree. I'm not actually one to defend types to the end of the earth. It's ultimately just a trade-off. In my mind I feel it's a bit like choosing to "run blind" in the recognition that sometimes you can get away with really clever timesavers while "no one is looking". It takes some skill with type kung-fu to learn to replace those clever tricks with type-principled surrogates. In the mean time, dynamic languages can often offer fast wins at the potential risk of long-term slowness.
Yes changesets and snaphosts are isomorphic, therefore being based on changesets can't be a selling point.
And people who don't like it forget that LOTS of people like it.
But it can, because the tooling evolves around the philosophy shaped by the underlying structure. Can you get branches for free in git? Sure. Do you? Nope.
I'm aware, but as you probably know, cheap is not free.
And is the opposite of what you typically want to do, ie give it a valve if it has none
I think the problem is you're trying to do X, and you thought "I know, I can write this as a traversal". So you asked about traversals, and all you got was a bunch of mumbo jumbo about `lens` and type-constrained traversals. You should just be asking how to do X. The reason why this particular traversal seems so complicated is because the `Image` type was structured to prevent arbitrary traversals. You're fighting the API here. Whatever it is you're trying to do, `JuicyPixels` probably offers an easier way to do it. If you really prefer to use an image type that is `Traversable`, don't use `Image` from `JuicyPixels`.
Perhaps rather than trying to wrap/unwrap a Bool you should consider a simple sum type: data Status = Active | Inactive Then you could pattern match directly against those constructors, which would still give you the benefit of not confusing data types. Given a helper function: toggleStatus :: Status -&gt; Status toggleStatus Active = Inactive toggleStatus Inactive = Active Your example becomes: let newFilt = filt { filterIsActive = toggleStatus $ filterIsActive filt } If you need booleans in some cases, you could write a [Boolean Instance](https://hackage.haskell.org/package/Boolean-0.2.3/docs/Data-Boolean.html) or just write a simple helper function.
This is not about juicypixels...
Actually, SPDX gives short monikers to just about every recognized open source license in existence. We could have Cabal support the SPDX standard, at least in naming conventions. Just have it pull and parse the SPDX license standardization file.
I don't believe they do. It is self-evident that people like it. Do you really think people who dislike staging believe that it was added to git because nobody wanted it, and then kept and advertised and bragged about because nobody likes it?
Anyone else getting [weird formatting](http://imgur.com/34A26Ms) that makes the announcement hard to read? Chrome, MS Windows 7
Devil's advocate: Months later, you need to maintain that thing you wrote on a tight deadline. Because you (ab)used dynamic types, you can't be certain that assumptions you made about one piece of code won't completely break some other piece of code. Sure, you could have written unit tests, but presumably that didn't happen since a) you are on a time crunch, and b) writing tests would have forced you to write code that you've already written before, which seems to be a sticking point. Hopefully, your project is small enough that you don't have to worry about such long-lasting effects. 
If it were slightly lighter I think it would be OK. The syntax highlighting doesn't seem right though. Everything after the `#` operator is coloured differently perhaps as if it were a comment.
Thanks for the protip!
A git hash represents a merkle tree, so yes (as secure as SHA1).
You could be right. I've not tried the combinatorial approach outside of Haskell, so I don't have sufficient experience to form a valid opinion there. 
&gt; We could have Cabal support the SPDX standard, at least in naming conventions. I like it, although it doesn't cover the "or later" variation requested by the OP. Also, we'd need a plan to deprecate the names Cabal is currently using that don't match SPDX and transition to SPDX with a minimum of breakage. &gt; Just have it pull and parse the SPDX license standardization file. Reading the .xls or .ods isn't really that easy, but we could do. It looks like they already have some Java code to do the generation of their web pages, we could probably lean on that for generating a SPDX/Licenses.hs file or somesuch.
*ahem* I don't see why the approach shouldn't scale; the existing implementation certainly doesn't. SHE has limited information with which to crunch source text and suffers from the way preprocessing happens before module chasing, making it hard to propagate information from module imports at that stage. The possibility certainly bears thinking a little harder.
[This is](http://i.imgur.com/g1KppTN.png) what your header looks link at my resolution. Maybe disable text wrap some way ?
Next steps: - Create a merkle-tree of all compilation output. - Create a git repo holding (public key, configuration, build target, merkle-tree-of-artifacts) tuples. The hash of the public key can be the branch name to make access control and distribution easier. All commits are signed by the public key belonging to the branch. Compilation output (merkle tree) can optionally be committed and published to the repo after a build. - Make it possible to configure a trust structure based on which keys you believe in (pull those branches from the repo). Then for fast builds, compare the merkle tree and pull artifacts that have the same hash among the trusted keys. - .. from some object store (S3 for example). 
Oh, boo hoo. Let's get realistic, shall we?
I'm sorry. Are you actually being *serious*?
Have you tried reading the linked message?
do you use linux? I'm on windows and even with sandboxes which i always use i constantly have compile issues :( i'd love to consult some windows haskellers, maybe i'm doing something wrong.. 
That is very interesting, though I don't understand the last two points - what's ~&gt;? And how is nmap a "witness" to anything? :o
Welcome to a whole new century where computers have actually been invented.
Some of the Agda developers are committed to making Emacs work well. Nobody has any investment in ensuring that only Emacs works well. Put up evidence that support for other editors has been resisted and I'll gladly (a) apologise and (b) invite those resisters to desist.
&gt;Put up evidence that support for other editors has been resisted The point is what you said: people cared about Emacs integration - no known hostility to alternatives.
You should then be more careful with your positioning of the word "only". Slack usage of "only" is a common source of unintended and problematic interpretation, as I have learned on occasion to my cost. You state that there is a commitment to "Emacs-only": that is to exclude editors other than Emacs. The truth, as you now acknowledge, is that Emacs has the only commitment. There is neither commitment nor hostility to other editors.
TIL. Is there any chance you could write a tutorial for `lens` as you use it ("how to program imperatively ..." seems to be just whetting the appetite)? Considering your previous tutorials, this one would be great.
Yeah, I could try to write something up. The rough summary is that I try to avoid using lens for code that I intend to show to non-Haskell programmers (one less thing to explain when introducing the language). However, when I do use lens: * I use the `lens-family-core`/`lens-family`/`lens-family-th` libraries (for smaller dependencies) * I only use associative operators, which rules out most lens operators, including `(^.)` Usually those two rules eliminate most of the objections to lens (huge dependency graph and operator soup), while still keeping all the nice parts (nice mathematical laws and huge flexibility with minimal code). You lose out on a few things (like `Prism`s), but it covers 90% of my use cases for lens.
can you lump people into being nonsensical.
I think it's only impossible if you assume a *finite* state space. I was thinking of a generalization/simplification of Mealy machines that removes the finiteness assumption. Like von Neumann style sequential computers, programmable logic devices can have a lot of memory and access to more memory, hard drive space, etc. 
What's depressing is that the discussion of the services a language might provide (in its static semantics) should be so tightly coupled with editor preference. We should be figuring out how to make type-assisted development editor-agnostic. I doubt it's super-tricky. Indeed, it's happening.
you could use DataKinds and KindSignatures to make a fancy wrapper data BoolType = Active | SecondType | ThirdType newtype BoolLike (t :: BoolType) = BL Bool type IsActive = BoolLike Active not' :: BoolLike t -&gt; BoolLike t not' (B b) = B (not b) let newFilt = filt { filterIsActive = not' $ filterIsActive filt }
Dang, neat. Thank you for the example; that gives me a pretty good foundation for learning more about DataKinds and KindSignatures.
You may want to look at the Repa package, it's well designed to work with image data (or even higher dimension data) in the ways you're describing. there's also examples available showing how to use it with JP IIRC.
i just tried installing spock from cabal in a sandbox and no dice :(
Well, the haskell version of a mailbox is arguably just a `Chan` or a `TChan`, both in the standard libraries.