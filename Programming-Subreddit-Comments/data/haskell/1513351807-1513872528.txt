You don't need a patch to pacman, this what you put in the PKGBUILD file. Statically linking Haskell dependencies means you don't need to care about extracting dependency information.
The problem only uses US-ASCII. I don't see a reason not to use ByteString if the programmer wants to, in the same regard, if he wanted to use Text. Is it really a reason to get queasy? It's code that's written to demonstrate a point.
So unsafe == performance cost? That's an odd definition of "unsafe". Sorry if I seem obtuse, here. I haven't done any dynamic linking in Haskell, and I don't use Arch, so my distro may have solved this problem differently. (Debian/Ubuntu)
Being pedantic: `mkStdGen` doesn't return two numbers, it returns a `StdGen`. If you try to print the standard generator, or let GHCi print it by virtue of gentering `mkStdGen` on the REPL, then the pretty-printing of StdGen is just showing it's internal state.
You can't inline because it is unsafe which is what GHC does to get good performance. The optimization you don't use is unsafe, not that dynamic linking itself is unsafe.
Could you post a small example of what you're trying? A Gist or repo?
The problem is that in this case shared libraries don't reduce the burden of updates. This is an empirical fact, as shown by the fact that minor updates of binaries cause cascading updates all the way down. Because of how inlining works in haskell, it is _not possible_ to swap out lower level dependencies and keep the top level binary intact. Rather, the purpose of dynamic linking is to reduce executable size when many binaries all share the _same_ dependencies already. That's just how the tech works.
Wonderfully timely blog post, thanks for mentioning ‘rank2classes’, I didn’t know about it. I have been delaying writing the TH myself because I know it’d be a time sink vs manually updating instances as types change. Problem solved!
Is that true? I seem to recall lots of people talking about Ubuntu and CentOS Haskell in disparaging ways, from both sides of the aisle. I don't get nearly as many opinions like that from, say, the Python community. Why is that? At any rate, my point is not to say that Arch is right about their policy. It's that, for whatever reason, Arch folks see Haskell as a problem. We could see that as our problem, too, and try to find mutually acceptable solutions, or we could just talk about how stupid the Arch folks are. I saw a lot of the latter in this thread, and felt that it was a bit dismissive and unfair. 
Sure. And, it turns out, that cascading update thing is a problem too, if the people in that thread can be believed. Fixing shared libraries to work like they do in other languages might be one way to solve that. Or not. I don't know. But I do know there's a problem, and it's not all with the Arch folks. Unless you don't think this user experience is a problem for you: https://www.reddit.com/r/archlinux/comments/7jtemw/which_package_do_you_hate_the_most/draiws4/
It IS true. All the non-arch distros statically link pandoc and don't have these issues. Source on the disparaging remarks thing, I don't see any but haven't looked all that hard. And there isn't much Haskell can/should do besides make support tickets with arch telling them to fix it. I don't think the arch guys are stupid, they just made a bad call with how they setup pandoc. 
Types change and people change
Also... sorry for yet another question, but is there any simple way to do it that makes the calculus inconsistent? For example, with unrestricted recursion (fix/mu/rec, whatever you call it), or perhaps type:type, is it possible to express induction and extract the second element of Sigma? If so, how? Of course, it wouldn't be ideal, but, between programming in an untyped language, and in one with an expressive inconsistent type system, I'd take the later. I'm gonna write *a lot* of code on that language (for Ethereum projects), so I'd rather have a legacy of "inconsistent CoC" than a legacy of untyped lambda calculus.
The spec doesn't say that, or that it's Linux encoded. The implementation relies on `readInt` to ignore any `\r` a Windows encoded file might produce. That wasn't my main point however. My point is that because of its good performance, a lot of Haskell people use `ByteString` over `Text` (e.g. the Cassava CSV parser). Since -- ironically for Haskell -- there's no compile-time checking of whether a `ByteString` is full of UTF-8, US-ASCII, or Latin1 bytes, it's possible, in a team project, for corruption silently to occur if someone accidentally imports `Data.ByteString.Word8` instead of `Data.ByteString.UTF8` (and even that latter package has its issues with `length()` for example). If the point of Haskell is to be a _safe_ language, we should all be using `Text`. The fact that we don't leads to lots of bugs the compiler cannot catch, which is antithetical to Haskell's philosophy. 
When FPCO built stack they did seek input from a number of us who had built our own build systems (and probably others?) to deal with vastly more complex build setups than cabal was designed to deal with. We also have never been an FPCO client. We developed two different build systems over the years but never had the time to really make them general purpose although we did open source them. It always greatly annoyed me that I had to be in the build system business. It was often suggested to me by others that what I really needed to do was to build a single library with all the code and a single executable with all our functionality but I always considered that to be an insane suggestion. There is no reason to abandon good software engineering principles because of the shortcomings of an existing tool chain. I will certainly concede that I had goals that most people did not. I don't maintain many libraries. I have a single repo with more than two hundred different haskell packages that I wanted to be able to build easily. I would never want a situation where a new engineer needs to do more than clone and run the build command to build our entire codebase. Today if I was doing this from scratch I would probably build this on top of bazel somehow with stack, so we can more easily deal with non-haskell dependencies (c++ libs) and the fact that I now have a considerable amount of scala and python. But I have done exactly zero work on what might be required to do this. I just have significant exposure to bazel via tensorflow. These are all highly opinionated choices and other people may want to go in their own direction and I am happy if what they do works for them. The fact that with stack/stackage I could let someone git clone; stack setup; stack build and build our entire codebase was major leap forward. Because we had our own build system that used GHCs multiple package repos I never used cabal sandboxes or freeze that everyone else (maybe not everyone) was using as I couldn't just feed it the root of our repo and have it build hundreds of packages. I am certainly happy to see cabal moving in this direction. I am unlikely to move back as stack has moved far quicker into things like integrating with docker which is now a core part of my workflow. This is also probably as it should be. The space of build systems and tooling is an area worth exploring. Other people undoubtedly have vastly different problems and goals than I do. I am happy that whatever workflows cabal implements work for them. I am also happy that stack actively wanted to solve problems that I had.
But Haskell is rather big language. And should be parsed differently with different LANGUAGE pragmas enabled. Especially if you want to produce helpful error messages.
Right, but the people in the Arch thread generally didn't fix their issues, or they fixed them by finding alternatives. That's not positive for Haskell. In that thread, I see mention of pandoc, xmobar, and shellcheck as troublesome. One bad package could be just the packager; three bad packages with a common base technology stack looks less like a bad packager problem, especially with a distro like Arch which has earned some respect in the Linux community. FWIW, I did look for some sources, but the ones I had in mind seem to have made changes which aren't as disparaging of distro packages. That's cool. Credit where credit is due. 
The entire GHC compiler isn't going to be rewritten to make life easier for one distro. Arch should obviously ship static binaries. No other distro has the problems Arch does, because no other distro pretends that dynamic linking means the same thing for GHC as for GCC.
Well written, well phrased!
Folks let me know what you think of it and if you find anything that is not accurate, typos etc. Thanks!
Take the Haskell definition: data Rose a = Node a (List (Rose a)) But what if we define `List` to be contravariant, e. g. as `data List a = List (a -&gt; Bool)`? Then `Rose`is negative recursive, and doesn't have an eliminator. Basically, we can only use externally defined type constructors if the inductive type being defined only appears in positive positions. At the same time, we must be able to automatically generate an induction predicate for `List (Rose a)`. This predicate will an induction hypothesis for the `Node` case, and it expresses that the induction predicate holds for all elements of a `nodes : List (Rose a)`. So, we must also implement a generic way to compute induction predicates, which is far from simple (but might actually be simpler than going for dependent pattern matching &amp; termination checking instead).
Nvm, found it: https://news.ycombinator.com/user?id=coolsunglasses
Helium, Utrecht University's simplified Haskell uses Parsec for it's parser: https://github.com/Helium4Haskell/helium/tree/master/src/Helium/Parser This is perhaps no surprise as one of Helium's original developers was Daan Leijen who wrote Parsec. Alex, is this going to be for something connected with Tidal?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Helium4Haskell/helium/.../**Parser** (master → 6096fd2)](https://github.com/Helium4Haskell/helium/tree/6096fd209823d81ffda80ad540e0482705bc6878/src/Helium/Parser) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
I'm in the same boat as you. Fortunately I asked how people install GHC on the survey: http://taylor.fausak.me/2017/11/15/2017-state-of-haskell-survey-results/#question-15 Method | Count | Percent :-- | --: | --: Total | 1211 | 100% Stack | 781 | 64% Package | 238 | 20% Platform | 107 | 9% Source | 49 | 4% Minimal | 36 | 3%
The issue isn't with using a non-Stack/non-Cabal package manager. I'm on Gentoo and refuse to use anything that's not in some repo there because I've already had the experience in the past of trying to maintain a system with multiple install/update methods. I've even dropped my inkling to see about contributing to the ide-engine because their stance is a flat "We have a lot of custom dependencies on specific git repos/commits. Stack allows us to capture this, other methods do not." (and the proposed alternative is just Nix). The issue is that Arch tries dependency trees on a binary distro. I definitely agree that they're going about it wrong (though [this page](https://www.archlinux.org/todo/remove-static-libraries/) and /u/merijnv's comment on the original post show they were doing it just fine until they changed the defaults without knowing the way we do things), but be careful not to lump all system package managers in with pacman.
There were roughly 40,000 downloads of the platform in the months of Sept-Nov. Of those, 75% were Windows users, 20% mac users, the remainder linux. Downloads of Core vs. Full are about evenly split on all platforms.
&gt; Isn't this more a problem with Arch Linux than with Haskell? Yes. &gt;Obviously I'm not saying we shouldn't strive to fix the situation This seems to be a problem predominantly for Arch linux users who *aren't* writing Haskell, i.e. people installing `pandoc` alone. 
&gt;How does the Haskell community propose to solve these problems, if not with dynamic libraries? `cabal new-build` actually works pretty nicely for what it's worth. Something like Nix/hydra? Users' binaries would be automatically updated when a new library version comes out. 
Dynamic libraries are still useful since updating one library only requires you to rebuild all its dependents, not the entire Haskell universe that a particular executable depends on.
[Here you go!](https://gist.github.com/anonymous/085af8b97f1b503f4e26b533dc93eedb) That's part 1 of today's [Advent of Code problem](http://adventofcode.com/2017/day/15), if it seems like a totally random computation.
As was said before, "fixing" GHC to support swapping dynamic library dependencies is *just not possible*. You simply cannot have inlining across packages with this feature, and *many* Haskell functions are marked as must-inline. You can't just break the operational semantics of Haskell because your package manager doesn't understand how Haskell handles dependencies
I really appreciated the diagrams, also. Very helpful!
Just a quick note: I feel most of the times the problem isn't how light a solution is... it is just that the field, as a whole, tends to express those things using languages (both human and programming languages) that I don't understand. 99% of the things I end up understanding turn out to be pretty simple in retrospect, they were only hard because of communication issues...
&gt; Right, but the people in the Arch thread generally didn't fix their issues, or they fixed them by finding alternatives. That's not positive for Haskell. Well I'm not surprised, since Arch introduced a problem that is best solved by using Haskell specific technology that a non-Haskell user is unlikely to be familiar with (using stack / cabal directly). Look man I'm sorry but I don't think you are reading my comments through. This is a problem Arch basically invented for themselves by making a bad decision with regards to how to handle Haskell packages. Haskell actually solved Arch's problem BEFORE Arch introduced it! Haskell does static linking by default which would give Arch users ZERO problems, but Arch specifically chose to OVERRIDE the working and sane default. Like I generally don't like to point fingers but jesus is it damn well necessary in this case. This is 100% truly and completely Arch's fault.
&gt; three bad packages with a common base technology stack looks less like a bad packager problem, That's faulty reasoning. Taken to the absurd, the packager could have logic that says `if languageIsHaskell then barf`, and that would clearly be a packager problem specific to Haskell. Haskell has accommodated solutions to this problem (static linking; even dynamic linking as long as the packager keeps the versions in check, which Arch is failing to do). You keep suggesting that maybe it's Haskell's responsibility, *but there is no action Haskell can take.*
Thanks for the links. &gt; And it's still not enough, because we really need proper computational behavior of transports and equalities as well when working with QIITs/HIITs Define an eDSL like I did with my OTT? I had some [thoughts](http://effectfully.blogspot.com/2016/10/insane-descriptions.html) on how to cheat and encode induction-induction in Agda by emulating very dependent types and I *think* there is a room for QIITs with some not the worst computational behavior, but do not have time to dive into that. By the way, is it much harder to handle full HIITs rather than QIITs? Do you pursue univalence? &gt; Inductive-inductive types are perfectly linear. They just allow type and point constructors to be introduced in any order, and allow them to refer to any previous type/point constructor. Ah, that's a nice way to put it.
Actually, Michael, I'm satisfied that the committee is using their best judgement.
&gt; So, we must also implement a generic way to compute induction predicates, which is far from simple I think it's intimidating, but not that hard. `List` becomes `All` and we just need some ornament that does this generically. Never did that, though.
Because all your data types must be strictly positive in their parameters and this is has to be ensured by construction. That suddenly means that you have to write in an annoying eDSL which is hard to read and comprehend, which mixes the concepts of a parameter and a fixpoint, which has annoying limitations and it's not clear whether there is a way to transform some higher-level language into this cryptic encoding.
I would write this in terms of concatMap, where it’s relatively easy and still feels map-like. 
Benjamin Pizza? Lol. Great post tho.
It is unsafe *and* there is a performance cost. It has a performance cost because the compiler can't inline as much as it would if the binary were statically linked. It is unsafe because Haskell’s ABI is not designed to be stable. Even rebuilding the same code can change the ABI of the resulting binary. So if you release a new version of a library, you also need to rebuild every package that transitively depends on it. This negates one of the advantages of using dynamic linking in the first place – the ability to update a library without affecting the packages that use it.
&gt; By the way, is it much harder to handle full HIITs rather than QIITs? The system appears to work just fine for HIITs; I could sit down and implement it in Haskell and I am 95% sure it would do the right thing, because the very large amount of dependencies in the translations make our current solution pretty much the only viable one modulo cosmetic differences. The problem is in the formalization. There is no known way to have usable internal syntax of type theories in book HoTT, because the syntax needs to be set-truncated (to a QIIT from a HIIT), but you can only eliminate from set-truncated types to types which are also sets. So we can't define e. g. the standard interpretation of the syntax which interprets contexts as types, because the type of types is not a set. So instead we use a metatheory with UIP, and thus our formalization does not take place in HoTT. &gt; Define an eDSL like I did with my OTT? I sometimes use a homegrown [cubical type theory](https://bitbucket.org/akaposi/elims/src/2e49c06624b3d81ae3bfa41b4ced7a089e77dd4c/agda/cubicalDeep/Lib.agda?at=master&amp;fileviewer=file-view-default) written in Agda rewrite rules. It works remarkably well in a number of cases but it's often too much strain on Agda, and also it might not even make sense in the first place. But in general, now I think cubical is clearly better than OTT-like solutions.
Can someone help explain how this second use of fmap in 'finished' is working? I know this is usually where Applicative/Monad come in, but these are working with fmap and I can't understand how. curried :: Num a =&gt; Maybe (a -&gt; a) curried = fmap (*) (Just 10) &amp;nbsp; finished :: (Functor f, Num a) =&gt; f (a -&gt; a) -&gt; f a finished = fmap (\x -&gt; x 10) -- reduces to Just 100 if you call 'finished curried' I'm not sure how the type signature of fmap in finished is being fulfilled since it seems to be doing f (a -&gt; b) -&gt; a -&gt; f b, and why it's neither unneeded since curried did a lift from Int to Maybe Int, nor is it giong too far and lifting again to Int -&gt; Maybe Int -&gt; Maybe (Maybe Int). I also can't get this to work outside of a lambda. Doing something like fmap curried 10 throws an error. Thanks for any help. 
If you want useful error messages you're essentially going to have to be able to parse the entire language regardless of what pragmas are enabled and then if a mismatch occurs between what the parser recognizes and what it's "allowed" to recognize (ie someone uses a feature that's not enabled), then you have that subsequent error message fire. Unless there's some other method I'm completely forgetting about?
I find that very surprising. I can count the number of professionals I know developing Haskell on Windows with one hand. From my university time, I don't even recall anybody using Windows for any kind of programming activity. Where do these Windows downloads come from, and what do these people do / use Haskell for? Do you have insights into the geographical locations from which the downloads originate, their user agent and language? This would be very interesting to know.
The type signatures may seem mysterious but if you write out the type of fmap and replace the a's and b's with they types specific to these functions they all work out. This can be done as follows: General Type Signature fmap :: (a -&gt; b) -&gt; f a -&gt; f b Curried function's fmap fmap (a -&gt; (a -&gt; a)) -&gt; Maybe a -&gt; Maybe (a -&gt; a) Finished functions fmap fmap ((a -&gt; a) -&gt; a) -&gt; Maybe (a -&gt; a) -&gt; Maybe a Can you see that in curried you are fmapping (*), which has a type signature of (a -&gt; a -&gt; a), over a Maybe a thus it produces Maybe (a -&gt; a). Then you are fmapping (\x -&gt; x 10) over this which means that x will be bound to (* 10) since we partially applied (*) to 10 in curried. Thus x 10 or (* 10) 10 produces 100 still wrapped in Maybe. This is one of the patterns that applicative abstracts using (&lt;*&gt;). Checkout it's type signature and see if you can use it and fmap to produce the same output as these two functions. I hope this helps let me know if you have any more questions! 
I'm still reading this, but it's awesome. I was researching functional graphs recently, and didn't want to read any papers, so I mostly read through fgl and a few other libraries. There are a few typos, like EdgeNode and Forest is left out in the second example (if you want each example to be a stand-alone file), and Undiscovered is used instead of U in the first. 
https://bugs.archlinux.org/?project=5&amp;string=pandoc https://bugs.archlinux.org/?project=5&amp;string=shellcheck https://bugs.archlinux.org/?project=5&amp;string=xmobar https://bugs.archlinux.org/?project=5&amp;string=ghc-libs https://bugs.archlinux.org/?project=5&amp;string=ghc No mention of this problem in any of those places. I'd consider "filing a bug" to be an "action Haskell can take that it hasn't already taken". Unless I've missed something, in which case, I'd be quite happy to be proven wrong. 
I guess I'm asking why everyone is so much quicker to point fingers than they are to offer to help.
Hm. I do think that's odd, but it's worth noting that the discussion has been had on other forums several times by now, IIRC.
**Because there is no help to offer**. Haskell already has a solution for this. The Arch maintainers are aware of it, and have chosen to ignore it. There is nothing else Haskell can do.
What should we do?!? The only thing we can really do to fix it is send a pull request or an issue to Arch that literally undoes a thing they did completely intentionally. That's THE solution.
You can program over such structures also with `generics-sop`: mapFormTemplate :: forall f g. (f ~&gt; g) -&gt; FormTemplate f -&gt; FormTemplate g mapFormTemplate eta x = to (htoI sopG) where -- unfortunately we need the type annotation here. sopF = hfromI (from x) :: SOP f '[ '[Text, CardType, Text, Day] ] sopG = hmap eta sopF See https://gist.github.com/phadej/55464aead55c85176e0b2b93ba790845 for full gist One clear benefit is that there are `hcmap` and bunch of other combinators in `generics-sop` so, you can e.g. cmapFormTemplate :: forall f g c. Proxy c -&gt; (forall a. c a =&gt; f a -&gt; g a) -&gt; FormTemplate f -&gt; FormTemplate g 
&gt; terminating eventually, we'd like stronger guarantees. Dhall terminates, it's all about termination. Half of its README is about not crashing, terminating, never hanging. That doesn't seem like a proper objection. 
Cool! I’m not so familiar with generics-sop and I certainly didn’t know it had functions for dealing with heterogeneous containers like that. Thanks for the tip!
Thank you.
In the gist, why still wrap explicitly each field in the record? When using generics-sop, I usually define a record with normal, unwrapped fields, and get the wrapped fields version "for free" when moving to the generic representation.
The gist borrows definitions from the OP here. Also I can see that you could write (in near future): data FormTemplate f = ... deriving (FFunctor) via (SOP2 '[Text, CardType, Text, Day]) Another point, is that Andres is working on static-opt case, which should kick in, so the generated code for `mapFormTemplate` won't have `SOP` etc. structures mentioned at all! That said, I too write code which works on e.g. `NP`, *it depends* :)
Thank you! I was not sure this is a bug. Created ticket [https://ghc.haskell.org/trac/ghc/ticket/14588]. Am I doing it right?
I love to model graphs as comonads, because the algorithms get short and concise, yet all optimization is hidden in the co-bind. Like https://samtay.github.io/posts/comonadic-game-of-life.html uses comonads on 2d regular graphs. http://blog.higher-order.com/blog/2016/04/02/a-comonad-of-graph-decompositions/ is also a good read, but Scala..
With the advent of proper semigroups, it probably makes sense to distinguish distributive and unitary, as they actually wind up hair-splitting apart fairly often. My first efforts in this direction followed the sort of "fuck it do what I mean" model. Unfortunately, once I started caring about inverse semigroups, all those fine distinctions actually mattered. =/
That was a huge help; I was thinking about what was comprising the arguments to the second one completely wrong, thanks a lot man.
&gt; basically unanimous Key word: _basically_.
There's http://hackage.haskell.org/package/haskell-src-exts . Or did you want something using Parsec in particular? I don't understand why that would be important.
For circuits, in general, the vertices are the wires. The logic gates are the edges. This way it is much easier.
&gt; Data.ByteString.Char8 already has a readInt function that's plenty fast By the way, the `readInt` I included was just a direct copy paste from ByteString to drop returning unneeded things to make it faster. Probably should have put that in the comment. I also rewrote it from scratch on a branch and made it 2x faster on positive or negative numbers (tested with criterion). I might contribute that somewhere. As is currently S8.readInt is pretty much hitting the ceiling for speed as far as I can tell. &gt; One small note, I'm using drop 1 to drop the \n character. I didn't test this, but I assume dropWhile (=='\n') would have been negligibly slower, and probably a bit more correct. The new line is a trailing new line, that's why the Rust version has a != test. So your version is producing a different result message at the moment. &gt; The fastest parser I presented is a few times slower than the Rust parser, but I think this is ok Is it okay? I think we can just accept that we lost the parser comparison by far, but that there are optimization opportunities, which I explored a bit. I think most people thought the cereal/binary packages were OK, until store showed you can go faster than that. This does matter when parsing e.g. high frequency stock data protocols, cereal/binary made one client of ours tempted to just give up and switch back to C++. So I like once in a while these comparisons that make us reconsider what should be considered "top speed", constrained by safety and convenience. &gt; The main loop performs wonderfully. All in all, it performs quite well and I'd say it's all reasonably idiomatic Haskell. The only real "trick" is to use an unboxed mutable vector, but that's fairly commonplace in mutable algorithms. I agree! The actual main loop is straight forward and fast. By the way making it run in ST instead of IO shaved off 10ish milliseconds. But it require freezing and thawing, this is mostly due to the timing code being in IO. &gt; Please please please test your benchmarks with LLVM! It's very often faster, and I think it'd be good to know if we should be striving to make it the default. GHC demands llvm 3.7, I tried installing that with home brew and setting the PATH but GHC still doesn't recognize it. It would be nice if anyone on OS X who has llvm working would share a guide!
... except that edges only go two ways. I would argue that the most elegant way I've thought of is this: A *bipartite* graphs, with components on one side, and nodes on the other. In an ideal electrical circuit all points directly connected by wires are equivalent, so each of those equivalent classes get a vertex on one side. As for components they each get one node on the other side, and connect its ports to the 'node' side, with identifying labels if necessary.
&gt; The new line is a trailing new line, that's why the Rust version has a != test. So your version is producing a different result message at the moment. Actually I'm pretty sure your version is interpreting an extra zero at the end. There are only 1057 numbers in the file, but there is one extra blank line at the end. This is the reason for the different results. &gt; Is it okay? I think we can just accept that we lost the parser comparison by far, but that there are optimization opportunities, which I explored a bit. I think most people thought the cereal/binary packages were OK, until store showed you can go faster than that. This does matter when parsing e.g. high frequency stock data protocols, cereal/binary made one client of ours tempted to just give up and switch back to C++. So I like once in a while these comparisons that make us reconsider what should be considered "top speed", constrained by safety and convenience. I think this is a valuable question, but also a completely different question. The previous OP was worried that making Haskell compete with Rust would require unidiomatic code. The parser was not in the way of being competitive, so I think the focus needed to be on idiomaticity (is that the right word?). Just a different take.
The problem is selection bias. That survey was answered by mostly Linux users. [According to another comment in this thread](http://reddit.com/r/haskell/comments/7jpbgj/ann_haskell_platform_822_released/draw57l), the platform is downloaded mostly by windows users. We don't actually have a representative sample in either survey, so both statistics mean very very little.
I think these sorts of studies are great and really interesting, so thanks all for sharing... &gt; In conclusion, it's not hard to write fast Haskell that's also idiomatic! I don't necessarily disagree, but optimizing Haskell code is itself an optimization problem involving developer time as well as the performance of the code. I think in this case we see several iterations by several developers, to converge on a solution that is fast and that you call idiomatic. I suspect what is happening here is you've gone to considerable effort to attempt to prove the point that what you've done is easy. I find optimizing Haskell efficiently involves a good amount of guess-work guided by intuition and past experience (followed be benchmarking, repeat...), and that it is not often worth the time to go back and e.g. figure out which of the several bang patterns we added we're unnecessary in all versions of GHC we care about.
&gt; I think in this case we see several iterations by several developers I actually started by just checking out the challenge page and doing it myself, having only looked at the prior two iterations enough to know there was an idiomaticity problem. Once I had something decent with attoparsec, I looked at Chris's solution to steal his timing code. That's why my `loop` looks so syntactically different from his `while`, even though they're operationally almost identical. The main inspiration I took from Chris's code was to ditch attoparsec for simpler bytestring parsing, but I don't think this was necessary, and I do think it would be an obvious next step should someone find it necessary. This isn't to dismiss your point. Obviously I had more prior experience with the problem, having read anything at all about it. But I would be comfortable claiming that all you need to know for this problem is that haskell supports unboxed mutable vectors, and that accumulators are a common source of space leaks. These are pretty entry level performance optimization techniques in Haskell, so I don't think it would be hard for any intermediate Haskeller to arrive at a solution similar to mine.
not willing to answer any of the substance in the question but instead focusing on the form. next, you'll be correcting grammar ...
Why should I learn Haskell?
Looks like a very helpful thing to do!
Something you may want to look at is [Cedille/CDLE](http://homepage.divms.uiowa.edu/~astump/papers/cpp-2018.pdf), which is (for the most part) CoC extended with dependent intersection types, implicit products, and a heterogeneous equality type. In it, you can get dependent eliminators, including induction. You can also prove parametricity and a few other interesting things. The theory is barely more complicated than the CoC, it doesn't require termination or positivity checking, doesn't require an implementation of unification, etc. As far as pattern matching goes, you might be able to make a [first-class patterns](https://hackage.haskell.org/package/first-class-patterns) library, but I don't know how compatible that is with dependent types in general.
Among other things, it: * Forces you to explicitly mark side effects, so code is easier to understand and debug * Teaches you how to encode various invariants using types * Presents a new paradigm of programming that can improve your style in other languages.
He said simple-ish Haskell.
Do you: - enjoy learning new things, especially programming languages? - are not satisfied with your current language of choice or practices? - want to learn about different approaches to building software? If so, consider learning Haskell. It's fun, It's practical, it's educating. Feel free to contact me if you want to learn Haskell and need a little help getting started or just want a small taste of Haskell.
Some extensions were added so they can disable features, but they work by turning them off. For example, the `MonomorphismRestriction` extension is on by default.
Yes, it's *all* with the Arch folks. They just don't seem to understand how Haskell/GHC works, sadly. (In all other respects I'm a very happy Arch user, but the GHC/Haskell situation is what *might* drive me away for good. It's an unmitigated disaster, frankly.)
`unsafeThaw` may be used here to remove the copy I suppose.
I am wondering why `EitherDecode` from `aeson` returns `Either String a`. Why not `Either Exception a` ?
One possible answer is related to lambda expressions. You have to remember the syntax for them. The `=` should be a `-&gt;` symbol. Also names of variables cannot start with an upper case letter. So the identity function can be written as \x -&gt; x
AKA a netlist
I don't know if this belongs here, but I am puzzled on how to use Haskell on Arch Linux. I have been trying to make Haskell work with stack-bin but after a while reverted to the old setup with hundreds of haskell-* dependencies in pacman. How do you do it? Note: I need Agda for school, and I want to further experiment with Haskell on small projects (like Advent of Code)
I can speak for the author of aeson, but here are my thoughts: `Exception e =&gt; Either e a` says that "whatever exception type you choose, if there is a failure I will produce one of those". That's pretty hard to implement. `IsString e =&gt; Either e a` is reasonable to implement, but the cost is type errors when the function is used in a polymorphic context, as GHC can't figure out the type: GHCi, version 8.0.2: http://www.haskell.org/ghc/ :? for help &gt; let foo :: IsString s =&gt; Either s Int; foo = Right 5 &gt; either (const 0) id foo &lt;interactive&gt;:4:21: error: • Ambiguous type variable ‘b0’ arising from a use of ‘foo’ prevents the constraint ‘(IsString b0)’ from being solved. Probable fix: use a type annotation to specify what ‘b0’ should be. These potential instances exist: instance a ~ Char =&gt; IsString [a] -- Defined in ‘Data.String’ ...plus one instance involving out-of-scope types (use -fprint-potential-instances to see them all) • In the third argument of ‘either’, namely ‘foo’ In the expression: either (const 0) id foo In an equation for ‘it’: it = either (const 0) id foo There is always a conceptual cost to overly general code, and sometimes a technical one too.
Great post! Do you know `alga`? https://github.com/snowleopard/alga 
Thanks Stephen! Yes d0kt0r0 has made a parser which allows a nice subset of Tidal (called 'minitidal') to be used in the web browser. I'm wondering whether we could get all of it supported.
Ah I assumed that would be a wrapper around ghc internals, and therefore not possible to compile into javascript, am I wrong?
&gt; Exception e =&gt; Either e a says that "whatever exception type you choose, if there is a failure I will produce one of those". That's pretty hard to implement. I don't quite get it yet. The correct complete type is actually `FromJSON a =&gt; ByteString -&gt; Either String a` and I was wondering about `(FromJSON a, Exception e) =&gt; ByteString -&gt; Either e a`. My understanding is that it would mean "You will receive either an exception or a FromJSon instance after decoding this bytestring." Why would this be harder to implement ?
I'd love to see a Haskell port of that blog post.
Perhaps. Where in the book does it say this, though?
This is something that tripped me up a lot when I started learning haskell. If I write a function `Exception e =&gt; blah :: ByteString -&gt; e`, I am claiming that I'll produce a value of type `e` for **any** type `e` that is an instance of `Exception`. The caller of your function chooses what `e` is, not me.
Do you have performance benchmarks?
Animo78 just want to expand on this answer a bit since it might be unclear to you what's happening. You had as your main case doubleChar (x:xs) = x:x:doubleChar xs In other words the result is twice the stuff on the left adjoined to the result for doubleChar xs. For the foldr you can pretend as if the doubleChar xs was computed in advance. And then just need to teach the problem, given that result for doubleChar of all but one character how do you handle that one more character. How to adjoin. The function is `(\x result -&gt; x:x:result)`, where result is effectively already computed and the `a:a:` adjoins. So keeping with your notation. You also have to provide the base case, what to do with the empty string. Which you did with the "". So you get: doubleChar = foldr (\x result -&gt; x:x:result) "" He used "b" instead of "result". There is also one more change that might not be clear in istandleet's answer. You had doubleChar "" = "" In your induction case you are thinking of the input to doubleChar as an array / stream of characters not as a unit (string). Why create the shift in your base case. So generally you would write for your recursive version doubleChar (x:xs) = x:x:doubleChar xs doubleChar [] = [] so then the foldr version becomes: doubleChar = foldr (\a b -&gt; a:a:b) [] The foldMap version is more complex in that you are thinking of strings as a Monoid and as a Foldable simultaneously. You are abstracting away the `(:)` operator. Not sure if that one is worth expanding on. 
[removed]
It's definitely not a wrapper around GHC internals, it's an independent, standalone parser. As far as I know it should be possible to compile to javascript via ghcjs, though I've never tried it personally.
Just flagging so you'll get a message. Expanded answer at: https://www.reddit.com/r/haskell/comments/7jz6ss/help_porting_a_recursive_function_to_mapfold/drc2oo0/ 
Thanks will have a look!
Quite helpful thanks ! I guess one other possible way would have been to return a custom exception. Something like: FromJSON a =&gt; ByteString -&gt; Either DecodeException a
Okay, that's convincing to me. I think I mischaracterized your post to make the point I wished to make. FWIW I also prefer to write smallish parsing tasks using take/split, etc, finding that it's often more understandable and less buggy. I think the simplicity of parser combinators is often overstated.
You can still use GHC but you have to pass the `-dynamic' flag, otherwise it won't compile.
Some more articles added: command-line arguments in Haskell, monadic do block, etc.
Blaze-html 
I find parsing combinators to be a similar comparison to LaTeX vs Word. It's harder to get something really simple done, but the complexity scales waaaaay better and by the time you want to get anything reasonably complex done, it's far easier in LaTeX. Plus, if you use some libraries a lot, they tend to get easier to use to the point where you might as well throw it in for even trivial stuff. One of the reasons why I think some simple Haskell code ends up being done by way overpowered methods; not that it's a bad thing, of course.
`stack new` now by default creates projects which use hpack, so you'll want to edit `package.yaml` instead of `{projectname}.cabal` to add dependencies.
That's a pretty surprising change... 
Oh! Thanks for the info.
Perhaps u could solve it in fp-way.
This change affects you only if you use default project with just `stack new`. Just as you said. If you specify project template explicitly you don't see any extra files added by stack. I think in general it's a good idea to stick to some existing project template or create your own. For example `hs-init` tool (sorry for shameless plug) creates projects with `.travis.yml`, changelog, readme with badges, etc. Might be a better replacement for just `stack new` :)
Is there a good reason why hs-init seems to be installed with the security nightmarish "pipe shell script download to shell" method instead of stack or cabal-install?
drop all haskell related stuff you installed with pacman and [just install stack](http://haskell-lang.org/get-started).
Thanks very much! Most typos should be fixed. They're not intended to be stand-alone files, I just split them to make them a bit more digestible.
No I don't. Thanks a lot for the link I'll have a look at that repo.
Why do you think it's a bad idea? I don't think it's much secure than `stack install` because with the `-XTemplateHaskell` you still can remove your whole system during compilation. `hs-init` is supposed to be small executable script which you can just copy under any folder in your `PATH` and run it. Installing through piping shell is just a convenient way to copy this script.
Interesting I wasn't aware of this way of modelling graphs. I'll make sure to read those blog posts, thank you!
It depends on how you want to do it. Haskell doesn't have a scipy equivalent, but it's not that hard to implement something like digit recognition using good old fashioned PCA and linear regression, which we do have libraries for. What makes this hard, and machine learning difficult in general, is preparing the data adequately. The algorithms aren't that crazy at all.
&gt; sorry for shameless plug Not so shameless then, is it ;)
It is a bad idea because nothing ensures that the script remains the same even if you look at it in your browser before executing it. Someone malicious could e.g. manipulate the web server so it sends a harmless looking script to brower user agents and a malicious one to curl/wget/... Even if that is not the case, the script could change in any new version to something buggy that accidentally deletes something ( see e.g. this steam bug on how quickly that can happen https://github.com/valvesoftware/steam-for-linux/issues/3671 ). Using this method of installation gets laymen who do not know any better used to executing code directly off the internet from pretty much any website or Github README file.
You should get a warning about this: &gt; stack build Warning: WARNING: E:\code\rattletrap\rattletrap.cabal was modified manually. Ignoring package.yaml in favor of cabal file. If you want to use package.yaml instead of the cabal file, then please delete the cabal file.
I experienced this the other day. I love stack but this made me a bit grumpy! 
Thanks for your feedback! This was very helpful information. Could you please open issue on github with proposed better solution? I don't know how to implement current behavior with `stack install` because it would be nice to have some embedded configuration which won't be changed at all (thus, it's not desirable you have some `config.yaml`). One reason to implemen installing through shell script is because it's done this way for `nix` (rather popular and considered to be good technology in Haskell world): https://nixos.org/nix/
[The State of the Haskell ecosystem](https://github.com/Gabriel439/post-rfc/blob/master/sotu.md) explains what common programming tasks and domains Haskell is appropriate for
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Gabriel439/post-rfc/.../**sotu.md** (master → 9aec61c)](https://github.com/Gabriel439/post-rfc/blob/9aec61cb08bded38ea454b52b5b1a3bb7426db17/sotu.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply drc9hkz.)^.
I would probably just install it without defaults with stack install and then ask those settings on boot and put them into a config file somewhere in ~/.config or potentially even just git-config since they all seem to be github related. What is your objection to a config file?
Ideally, I don't want to do anything. Just have a library Image-&gt;Text and it should be dam quick. As there are commercial libraries doing that there should exist one in Haskell. If I would do it, I probably would think first about using wavelets and then a neural network on the coefficients perhaps. Or at least not start with PCA and linear regression. Do you (or anyone else) want to write a handwritten/OCR library in Haskell in collaboration?
To be fair, I always use `stack new &lt;blah&gt; simple`, so I would never have encountered this since it appears to be a change to the default template, **not** the `stack` executable itself. 
To answer your last question, you can simply delete the `package.yaml` file from your project's directory. Then `stack` will just use the `.cabal` file, and won't overwrite its contents on compilation. 
Oh, so hpack is used by default now in stack. News to me too, thanks!
Getting hot code reloading/swapping in Cloud Haskell-like systems would be very cool. Right now, spawning remote processes requires that the same version of code is running on both the spawning process and the remote process. It uses a system of "static pointers," where statically known values are given pointers that are stable across executions of the code. This means that I can serialize a static pointer to a function, send it across the network, and it can be deserialized on the receiving end and executed. In Erlang, I can just send a bunch of code to a node and it'll run it. This makes updating a network much easier.
 Prelude&gt; :show language base language is: Haskell2010 with the following modifiers: -XNoDatatypeContexts -XNondecreasingIndentation Prelude&gt; :showi language base language is: Haskell2010 with the following modifiers: -XNoDatatypeContexts -XExtendedDefaultRules -XNoMonomorphismRestriction -XNondecreasingIndentation
fltk is easier to install on windows: https://hackage.haskell.org/package/fltkhs http://github.com/deech/fltkhs
&gt;Right now, spawning remote processes requires that the same version of code is running on both the spawning process and the remote process. Not in transient-universe. Nodes can have even different architectures. For example a web browser and the server can communicate with the same cloud primitives.
I don't get the point of this response? Also, do note that GHCI will by default enable a few extensions when you use it in lieu of compiling with GHC, most notably `-XExtendedDefaultRules`. The behaviour discovered by the OP still shows up when one simply compiles the program without using GHCI.
Okay, so why does the tech work that way? Is this a Haskell thing or a GHC thing? I actually ran into this in the past and didn't think much of it but I once made something in Haskell because Haskell made sense for that and found that the binary was ginormous so I tried the dynamic linking and that reduced it to acceptable size but every cabal update indeed made it fail with a linker error so I just integrated it into portage which automatically rebuilds it when any of those libraries on my system update and calls it good. What's the architectural reason that GHC cannot do what other compilers can regarding this?
By their very nature monadic parsers trade speed for convenience/power. For instance `string "foobar" &lt;|&gt; string "foobaz"` should only parse the `foo` prefix once but you run into the halting problem since monads are based on chained functions. Arrows were partly invented because of this and -XApplicativeDo tackles mostly the same problem. Though if you really need to parse huge amounts of data in a complex format you should just go with alex/happy or the equivalent for other languages.
The direction of your research seems very exciting and interesting. You may want to contact authors of [Declarative Programming over Eventually Consistent Data Stores](http://gowthamk.github.io/docs/quelea.pdf) and pick their brains. I'm not in any way affiliated with them but the FP crowd seem very open to questions and academic inquiry. 
What about using [gloss](http://hackage.haskell.org/package/gloss)? It's a nice and easy to use wrapper around GLUT. The mouse events are fairly easy to handle. The [playIO](http://hackage.haskell.org/package/gloss-1.11.1.1/docs/Graphics-Gloss-Interface-IO-Game.html#v:playIO) function should allow you to perform the IO you need to do your audio analysis/communicate with another thread that handles it.
Interesting. Do you have any references that demonstrate this to be true?
I personally don't think it's a big deal, but you could link a specific github version (which you've hashed), and have the command pipe do a sha1 before execution - this solves the highlighted problem and is still a copy/paste/done job.
As an Arch user myself, I found the same issue. Also, I like cabal, the "new-build" command is pretty awesome, so I added this info in the wiki explaining how stop having dynamic linking (not depending on those haskell-* packages): https://wiki.archlinux.org/index.php/haskell#Building_statically_linked_packages_with_Cabal_.28without_using_shared_libraries.29 Basically, you can use the ghc from the repos but you need to build your own cabal, other than that it works pretty well.
One thing I'd like to see is a way to "distribute data structures". We have plenty knowledge about in-memory composition: we know how to construct data structures using recursive data types, sometimes with functor layers and recursion-schemes to consume these structures. That said, as soon as the data structures cannot fit in RAM, it's easy to imagine having a tree where a leave is a pointer to a remote node holding the rest of the structure. Such a remote node means we need to run some IO (not Traversable) to observe the full structure; and it's unclear how we can keep recursion-scheme style of computing when consuming such structures: we could either `pull remote data locally` or `send the computation remotely and retreive the result`, a mere fold cannot make such decisions. In short, some combo of Spark and Data-Type-A-La-Carte would be nice (I have pieces of code here and there). Happy to discuss this further over Hangout if you're interested.
This is a complex topic, and open source OCR is not very deceloped, even without limiting your search to Haskell. You can have a look at Tesseract maybe, it is quite old but can be manipulated in command line gives good result for scanned printed documents.
This is definitely a field I am interested in. I'd quite enjoy a `fn :: Image -&gt; Text` interface. But realistically speaking I'll likely have different priorities in the foreseeable future than to invest much time implementing this. That being said, if you decide to develop it in the open, be it alone or with others, I will likely want to follow it and test it once there is something to test. And maybe jump in with little bits of contributions in the meantime if I see an opportunity that fits well for me. Could you ping me with, e.g. a GitHub link, if you decide to start development?
Did Haskell turn out to be faster in the end? https://www.reddit.com/r/haskell/comments/7jr2yy/haskell_and_rust_on_advent_2017_maze_challenge/dr8wx40/
Actually, thinking further about this a bit: I think I would try to implement this with a temporal/recurrent neural network: Meaning, I would simulate human vision by having a fovea where the network sees the sharpest, and present it with a gradually decreased information density from that center. [Illustrated by this picture](http://research.nvidia.com/publication/2017-09_Latency-Requirements-for). This would ensure that we can have a constant and relatively low amount of input neurons, and at the same time we can process unbounded image size and resolution. (Having to calculate coefficients might not even be necessary, just inputting dark/light/rgb intensity might be enough for this.) I would then allow the neural network to simulate saccades ((rapid) eye movements) by interpreting a few output neurons as move down/right +/-, with intensity as the amount to jump. Maybe even zoom in and out. So it can decide what part of the picture to focus on. And each iteration of the network it can output a single letter that it fully recognized, or a `noop`if it's still unsure and wants to process further. And maybe even an EOF when it is confident the text is over. (And maybe all those outputs' intensity could be signal for the NN's confidence, meaning that we could underline certain words as 'uncertain' if some of the letters have low confidence scores attached.) And of course I would loop back some output neurons as inputs to allow it to 'think' in-between iterations. What do you think of this approach?
[The original arrow paper](http://www.cse.chalmers.se/~rjmh/Papers/arrows.pdf) gives monadic parsers as motivation. Technically it is probably possible to write a monadic parser with c-like performance but that would rely on manual left factoring and avoiding most abstractions over the core contaminators neither of which is very fun.
However, hot code swapping is a completely different subject and it would be something very nice to have.
I have created a meme, thanks.
&gt; to all compiled languages, and they solve it by embedding the version number in the shared library's name. Does Haskell not do this? If n We should contact the devs and open a conversation with them: https://mail.haskell.org/mailman/listinfo/arch-haskell
We should contact the devs and start a conversation with them: https://mail.haskell.org/mailman/listinfo/arch-haskell
Here is my solution using `STUArray`: https://github.com/lan17/alg_cmp/blob/master/advent_of_code/day_5.hs From my own brief benchmarks its about 10ms slower than OP solution in Haskell, but is a lot more brief. I am somewhat proud of this because it is my first Haskell program that does side-effects! My simple C++ solution is almost 2x faster than either one, however :P https://github.com/lan17/alg_cmp/blob/master/advent_of_code/day_5.cc
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [lan17/alg_cmp/.../**day_5.cc** (master → 175f1b6)](https://github.com/lan17/alg_cmp/blob/175f1b628a81e6df082499e88c2d66b11506a248/advent_of_code/day_5.cc) * [lan17/alg_cmp/.../**day_5.hs** (master → 175f1b6)](https://github.com/lan17/alg_cmp/blob/175f1b628a81e6df082499e88c2d66b11506a248/advent_of_code/day_5.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply drcp7bf.)^.
Is there somewhere to follow your progress on Duet by the way?
Agreed, interesting, ambitious. I feel mixed on it.
Really? I thought the whole point of referential transparency is that it doesn't make a difference whether or not you inline. I can't seem to find this "must-inline" anywhere either.
Referential transparency only works if the expressions themselves aren't being changed. i.e. if a library defines `foo x = x + 1`, but the next version changes it to `foo x = x + 2`, then it is not safe to change which version of foo exists beneath an already-compiled module, as the module may have inlined the other version. The `{-# INLINE foo #-}` pragma can be used to tell GHC that `foo` always needs to be inlined; the `{-# INLINABLE foo #-}` pragma *suggests* that `foo` be inlined, and small functions will very often be given one or the other pragma implicitly.
From the docs about INLINE: "In general, GHC only inlines the function if there is some reason (no matter how slight) to suppose that it is useful to do so." Doesn't sound very *must* to me. Why can't we choose at compile time, not to inline values from external libraries? Alternatively, why can't the compiler produce a list of all external library values that were inlined, so we know whether or not an update to an external library will require a recompile?
&gt; Doesn't sound very must to me. You're right. But the only cases where it won't be inlined are those cases where doing so is essentially identical to not inlining. In practice, the function will be inlined anywhere that it can be beta reduced. &gt; Why can't we choose at compile time, not to inline values from external libraries? You can. Just use `-O0`. The performance of Haskell without inlining is insanely slow though. There is no way I would recommend using Haskell in production without inlining. &gt; Alternatively, why can't the compiler produce a list of all external library values that were inlined, so we know whether or not an update to an external library will require a recompile? There's not much reason to do this. For one thing, it's very rare that you take on a dependency on a library and end up with *zero* of its code inlined, so you'd end up rebuilding in most of the same cases anyway.
Other compilers do not online functions from other packages, but this is essential for the performance of purely functional languages like Haskell. Any decent FP optimizer is going to rely on this.
Nice post! If you haven't seen it before [this](http://r6.ca/blog/20110808T035622Z.html) is pretty awesome.
What would the custom error type be? A newtype over String is not adding much. An ADT which has values to represent all possible parse error would certainly be more interesting. For example: MalformedJSON, MissingKey, InvalidType. And you're right. The design would probably have been better if Aeson could report structured errors, and multiple non-fatal errors. 
Inlining isn't just about removing function call overhead. It is about enabling large scale rewriting optimizations. Idiomatic haskell will have lots of functions from external libraries composed. By inlining those functions, rewrites can be preformed that remove use of intermediate data structures entirely. One common example of this is _fusion_: https://wiki.haskell.org/Correctness_of_short_cut_fusion The existence of these large-scale optimizations that rely on inlining to expose rewrite opportunities is what allows _idiomatic_ Haskell code to also be performant.
Learning a bit about reflection. [Duplication and Partial Evaluation - For a Better Understanding of Reflective Languages](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=09F2BAF42EFBB9401CA73EBD492AE5CE?doi=10.1.1.132.372&amp;rep=rep1&amp;type=pdf) [Collapsing Towers of Interpreters](https://www.cs.purdue.edu/homes/rompf/papers/amin-popl18.pdf)
[Advanced BDD Optimization](https://www.amazon.com/Advanced-BDD-Optimization-Rudiger-Ebendt/dp/0387254536) to get a better understanding of state of the art variable reordering techniques. [GPU Gems, Jade Edition](https://www.amazon.com/GPU-Computing-Gems-Jade-Applications/dp/0123859638) to see how their GPU-based cuckoo hash table works, so I can adapt it to working with SIMD for bulk inserts for BDD `ite` and hash-cons computations.
I'm trying to do some things that I don't think I can, but I'd like to check that. I have a data type data Iso a b = Iso (a -&gt; b) (b -&gt; a) and a data type data NT f g = NT (forall a. f a -&gt; g a) (forall a. g a -&gt; f a) I'd like to be able to have a function that takes qualifying Isos and returns the corresponding NT, so for instance I have a new datatype data AddUnit a = AddUnit (a, ()) and the functions toUnit (Identity a) = AddUnit (a, ()) toId (AddUnit (a, ())) = Identity a and I have Iso toUnit toId. I can't write convert :: forall f g. (forall a. Iso (f a) (g a)) -&gt; NT f g because of something or other about impredicative polymorphism I can't write anything to take the first or second functions from Iso and make them :: forall a. f a -&gt; g a I think I've hit a dead end here, but I'd like to not give up if it's not hopeless.
The Green Mile by Stephen King
The final poll is available here: http://freeonlinesurveys.com/p/Cw1FRJ2G?qid=1017541 
&gt;Ideally, I don't want to do anything. Just have a library Image-&gt;Text and it should be damn quick. And perhaps train my handwriting with a GUI. As there are commercial libraries doing exactly that there should exist one in Haskell. Commercial implies funding. Who's paying for this development work? Why not just wrap the working library?
I'm not informed about pure haskell libraries - but there are [bindings to opencv](https://github.com/LumiGuide/haskell-opencv). There would be also a [tutorial in python](http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_ml/py_knn/py_knn_opencv/py_knn_opencv.html) for handwritten text. I suppose one could use the bindings to reimplement this in haskell. Disclaimer - I didn't try these, they are on my todo list ;)
I needed clarification of this in a first read so here goes: Inlining functions from dynamic libraries is unsafe because the inlined code doesn't change when the library changes, even though the function that was inlined or one of the functions it relies on may have changed and therefore the semantics may not have been preserved by inlining. Is this correct?
I'd also like to know. Also, in case your question is slightly different, I wish cabal2nix recognized hpack's package.yaml file when present. Or if we had an hpack2nix. And/or if those could be integrated to enable $ nix-shell -p 'haskell.packages.ghc822.developPackage { root = ./.; }'
This slow paced but helpful style really attracts me to keep reading. Reminds me of the extremely slow paced APL tutorial at http://tutorial.dyalog.com
Artemis by Andy Weir (author of The Martian).
My understanding is that this is only the case for Haskell Text rather than String.
Unlike Python or Nickle, for example, the last example in "Mutability and Purity" may give a wrong answer on a 32-bit machine due to overflow. It may depend on the version of GHC you're using (I don't really understand how integer type defaulting works now), but it will certainly happen if anything in that loop happened to acquire type Int earlier on. I don't have a 32-bit machine handy, but &gt; let n = 1000000 :: Int32 and then substituting n in the example illustrates the problem nicely.
[I patched `cabal2nix` a couple of months ago](https://github.com/NixOS/cabal2nix/pull/304) to support generation of cabal files from hpack package.yaml files. I think version 2.6 or above should include it. You can explicitly tell it to regenerate the cabal file with `cabal2nix --hpack`, or if no cabal file is available it will attempt to use hpack by default.
This looks really useful. I'm considering creating a web site that displays and tracks the liquidity/market depth of Bitcoin (for various markets), and this library looks really useful in that context. I'm wondering, though: what would be an idiomatic way of representing sell and buy orders, respectively? There's a duality there that makes adding an additional type parameter inelegant (since there are only two options: buy/sell). In other words, how would we represent a) having BTC and wanting to exchange it for USD (placing a sell order in a BTCUSD market) versus having USD and wanting to exchange it for BTC (placing a buy order in the BTCUSD market)? So, an order would have three components: 1. currency you wish to sell (e.g. "USD") 2. currency you wish to buy (e.g. "BTC") 3. exchange/venue on which you want to place the order (e.g. "Coinbase" or "Bitstamp") and parameters 1) and 2) would be inverted for a buy order vs a sell order. 
This is cool, thanks !
Though I haven't worked with Erlang, the static pointers "limitation" seems to be easily fixed with a reasonable CI/CD strategy. I think binary compatibility is only part of the story, when protocols, internal data representation might change between releases.
I don't understand why they are representing currencies with lifted `String` instead of using different types for each one. This way `exchangeRate` would be type safe! 
Can’t you just represent this with a pair of ExchangeRate values, or an ExchangeRate and buy and see offsets?
How is it not type safe? The benefit of this representation is I can easily add my own LambdaCoin tomorrow without changes to the library. You could do the same thing with uninhabited types for all currencies but there’d be more user overhead for every instance. 
"Visual attention" modelling was a pretty hot topic in machine learning until not so long ago, see e.g. https://arxiv.org/abs/1412.7755 , and it's a fascinating problem in its own right. I feel handwriting recognition is a somewhat simpler problem than general image recognition, because the sequences we are trying to predict are generated by few tens of symbols at most.
I think you'd want to start with the OpenCV bindings : https://github.com/LumiGuide/haskell-opencv . 
Very interesting! I don't know much of this field, but I follow C.Meiklejohn's work with interest; he works on/with declarative/functional languages and among other things he recently published a "Next 700 programming languages" paper on distributed languages http://christophermeiklejohn.com/publications/pmldc-2017-preprint.pdf 
I think you're right: it would make sense, for e.g. a BTCUSD market, to use `ExchangeRate "USD" "BTC"` to represent the limit price of buy orders, and `ExchangeRate "BTC" "USD"` to represent the limit price of sell orders. 
Regarding rationals versus integers, I've been doing some humble research into faster representations of rationals that maintain the safety but maybe perform a bit better. We currently have two pervasive types in Haskell, Rational and Scientific and Scientific is much slower than Rational https://github.com/haskell-perf/numbers There aren't a lot of (new) papers on improving Rational speed that I could find, just a bunch from the 60s-80s. However, the symbolic way of representing arithmetic in e.g. Mathematica is quite attractive to me. It doesn't reach near floating point but it does offer more attractive ways of doing things. E.g. pi is just represented as "pi"; "sin(pi/2)+sqrt(2)" evaluates to "1+sqrt(2)" and no further, it makes these reductions using knowledge of the mathematics, not by approximation. If you graph it or use it where you need to approximate, then it will be approximated according to some scale. But you don't have to deal with that because you just need them to work algebraically. In Duet I want students to have an Integer type, a Rational type, but also be able to express geometry for drawing things on screen but I don't feel like exposing them to floating point when I could go the symbolic route. Consider e.g. `1 + 1/3 + 1/3 + 1/3 == 2` evaluates to True in Haskell but `(1 + 1/3 + 1/3) + 1/3 == 2` evaluates to False. In Mathematica both are True. The last thing a learner wants is to have their basic intuition crapped on, so I'm leaning strongly towards symbolic numerics.
Very nice overview of `containers` and `fgl` libraries, thank you! I think the question "How should I implement a graph algorithm in a functional programming language?" still doesn't have a fully satisfactory answer. The existing data structures and algorithms are generally harder to understand and harder to get right compared to the imperative equivalents. And, as you rightly point out, the reading material is pretty much limited to a few academic papers and blog posts like yours...
Thanks! I am glad you enjoyed it. 
Yes, this is why. No type safety is lost. And also, constraining the kind of these type indexes to Symbol, instead of leaving their kind polymorphic, improves error messages. 
I should start blogging about it soon! The most recent upload is here: http://chrisdone.com/toys/duet-epsilon/ (historical ones are available under [/toys](http://chrisdone.com/toys/)). Epsilon introduces the structured editor which is still WIP.
Doesn't using a representation using symbolical numerics imply a prechosen basis of what are the representable numbers? For example, `sin(pi/2) = 1` is not purely a symbolical transformation, it must be defined somewhere else, or derived from other pregiven properties.
Without any type signatures it will default to arbitrary precision `Integer`s. &lt;interactive&gt;:1:1: warning: [-Wtype-defaults] * Defaulting the following constraints to type `Integer'
Thank you. I get an error while executing 'stack install --system-ghc cabal-install' though: -- While building custom Setup.hs for package network-2.6.3.2 using: /tmp/stack7523/network-2.6.3.2/.stack-work/dist/x86_64-linux-tinfo6-nopie/Cabal-1.24.2.0/setup/setup --builddir=.stack-work/dist/x86_64-linux-tinfo6-nopie/Cabal-1.24.2.0 configure --with-ghc=/home/brecht/.stack/programs/x86_64-linux/ghc-tinfo6-nopie-8.0.2/bin/ghc --with-ghc-pkg=/home/brecht/.stack/programs/x86_64-linux/ghc-tinfo6-nopie-8.0.2/bin/ghc-pkg --user --package-db=clear --package-db=global --package-db=/home/brecht/.stack/snapshots/x86_64-linux-tinfo6-nopie/lts-9.18/8.0.2/pkgdb --libdir=/home/brecht/.stack/snapshots/x86_64-linux-tinfo6-nopie/lts-9.18/8.0.2/lib --bindir=/home/brecht/.stack/snapshots/x86_64-linux-tinfo6-nopie/lts-9.18/8.0.2/bin --datadir=/home/brecht/.stack/snapshots/x86_64-linux-tinfo6-nopie/lts-9.18/8.0.2/share --libexecdir=/home/brecht/.stack/snapshots/x86_64-linux-tinfo6-nopie/lts-9.18/8.0.2/libexec --sysconfdir=/home/brecht/.stack/snapshots/x86_64-linux-tinfo6-nopie/lts-9.18/8.0.2/etc --docdir=/home/brecht/.stack/snapshots/x86_64-linux-tinfo6-nopie/lts-9.18/8.0.2/doc/network-2.6.3.2 --htmldir=/home/brecht/.stack/snapshots/x86_64-linux-tinfo6-nopie/lts-9.18/8.0.2/doc/network-2.6.3.2 --haddockdir=/home/brecht/.stack/snapshots/x86_64-linux-tinfo6-nopie/lts-9.18/8.0.2/doc/network-2.6.3.2 --dependency=base=base-4.9.1.0 --dependency=bytestring=bytestring-0.10.8.1 --dependency=unix=unix-2.7.2.1 Process exited with code: ExitFailure 1 Logs have been written to: /home/brecht/.stack/global-project/.stack-work/logs/network-2.6.3.2.log [1 of 2] Compiling Main ( /tmp/stack7523/network-2.6.3.2/Setup.hs, /tmp/stack7523/network-2.6.3.2/.stack-work/dist/x86_64-linux-tinfo6-nopie/Cabal-1.24.2.0/setup/Main.o ) [2 of 2] Compiling StackSetupShim ( /home/brecht/.stack/setup-exe-src/setup-shim-mPHDZzAJ.hs, /tmp/stack7523/network-2.6.3.2/.stack-work/dist/x86_64-linux-tinfo6-nopie/Cabal-1.24.2.0/setup/StackSetupShim.o ) Linking /tmp/stack7523/network-2.6.3.2/.stack-work/dist/x86_64-linux-tinfo6-nopie/Cabal-1.24.2.0/setup/setup ... Configuring network-2.6.3.2... configure: WARNING: unrecognized options: --with-compiler checking build system type... x86_64-pc-linux-gnu checking host system type... x86_64-pc-linux-gnu checking for gcc... /usr/bin/gcc checking whether the C compiler works... yes checking for C compiler default output file name... a.out checking for suffix of executables... checking whether we are cross compiling... configure: error: in `/tmp/stack7523/network-2.6.3.2': configure: error: cannot run C compiled programs. If you meant to cross compile, use `--host'. See `config.log' for more details I followed the exact instructions from your chapter in the wiki. Do you have any idea what goes wrong?
I'll take 10ms for readability any day of the week. Good job.
This is a good summary. It's not about any one particular feature; it's the holistic approach of doing things in a reasonable (as in, able-to-be-reasoned-about), composable way. Tbh I wish we'd drink even more of our own koolaid: conflating induction/coinduction/laziness is sloppy, and unfortunately a lot of our learning materials explicitly encourage this conflation.
One old example is `HList` package which allows you to solve so-called _extensible records_ problem in kinda naive way. You can check usage examples on this slide for my Haskell lecture: * http://slides.com/fp-ctd/lecture-15#/15 Relatively modern usage example is to use `Symbol` type in the solution for `OverloadedLabels` problems. You can check discussion in this ghc-proposal: * https://github.com/ghc-proposals/ghc-proposals/pull/6 Basically, you can probably solve problem with duplicate record fields by having functions with types like this one: f :: HasField "foo" r Int =&gt; r -&gt; IO ()
For my meaning, it's definitely a case of adding in "pregiven" knowledge. My examples were intended as short-hands for some kind of AST. The type might be e.g. data Exp = Ratio Exp Exp | Int Integer | Pi | Cos Exp | Sin Exp | Mult Exp Exp | Div Exp Exp | Add Exp Exp | Minus Exp Exp Then you have `evaluate :: Exp -&gt; Exp` and `approximate :: Exp -&gt; Double` (for example). In WolframAlpha, `sin(pi/2)` [produces `1`](http://www.wolframalpha.com/input/?i=sin(pi%2F2\)) because it has some knowledge of `sin`'s [basic trig function properties](http://mathworld.wolfram.com/TrigonometryAnglesPi2.html), but give it e.g. `sin(pi/7)` and it gives [a decimal approximate result](http://www.wolframalpha.com/input/?i=sin(pi%2F7\)), indicating that the best exact representation it has is `sin(pi/7)`. 
typo: "hypotetical" in the [first paragraph of the Positions section](https://ren.zone/articles/safe-money#positions)
Thanks, great material! I especially liked the race and concurrently examples. I had not seen the use of race to perform multiple synchronous events and stop when one evaluates.
If you don't know about DataKinds, then please read about that extension first. If you do: `Symbol` is the type-level version of `String`, and `Nat` is the type-level version of (non-negative) integers. `'String` (or rather `'[Char]`) and `'Int` don't work because values of type Char and Int aren't constructors like `Nothing` or `Just`, so we can't use the usual trick of automatically generating the type constructors data Nothing data Just a corresponding to the `Nothing | Just a` value constructors.
Would this expression fail at compile time or at runtime? exchange (4 :: ExchangeRate "USD" "JPY") (2 :: Dense "BTC")
compile time, IIUC
Oh shit, my bad.
This was a really great and informative read, thanks for posting this! Also, I think I might steal this technique for writing my next talk.
I would highly recommend the safe-money blog post for learning about this.
&gt; conflating induction/coinduction/laziness is sloppy, and unfortunately a lot of our learning materials explicitly encourage this conflation. Apparently some Haskellers [like it that way](https://lobste.rs/s/vnlion/anatomy_haskell_based_application#c_qnmo3h)
Those insider improvements are now live for everyone, after Fall Creators Update. This is also affecting Java and possibly also Go. And GHC itself is definitely on the Microsoft radar here. So hopefully it will soon go away completely. In the meantime, I'm already doing large compiles on GHC 8.0.2 and surviving the experience. Another small quirk I came across is that warp server fast static file responses using Linux file descriptors wasn't working on WSL. So I turned that off locally for our application that uses warp as its HTTP engine. I haven't yet tried turning it back on after FCU.
It also makes my life much more pleasant, and I *don't* work for Microsoft. Thanks!
Small nitpick: The first example with callbacks still waits on the response to the first request before sending the second request.
Thanks for safe-money and this great introduction. In the discussion of floating point and rational representations, I would have liked to see Decimal (http://hackage.haskell.org/package/Decimal) mentioned. Am I right in thinking this is a third approach ? I have the impression it performs well. (Anecdotally, when hledger migrated from Double to Decimal, it got faster.)
[removed]
The very next example shows concurrency. It starts with: &gt; It's pretty lame that we need to wait for our first HTTP request to complete before even starting our second.
This is awesome. I like the approach of writing either pseudo code or code in another language, and then writing a syntactically similar snippet in Haskell to show that the Haskell code is either just as clear or clearer, but comes with some advantages. That seems like a really useful way to show a non Haskell programmer some benefits of using Haskell immediately without having to teach them the language from the ground up.
Well at least you could use M36 at all, if that's the only persistence available. As in our case, where a PostgreSQL database is the way that multiple copies of the app running in parallel keep their data in sync. We could only use M36 for some new module or subsystem if it's capable of storing its data in PostgreSQL, not on the filesystem. We actually do have a primitive filesystem emulator backed by PostgreSQL for legacy features that still need a filesystem-like interface, but I don't think it makes sense to build M36 on top of that. But anyway, that's just the worst case. I would be very surprised if there isn't some much better way to wrap a SQL database as an M36 backend than dumping all the tuples into a flat table. Besides being a better way of using the SQL DB when that's the only game in town, it would also provide a better migration path for people who are stuck with tons of data and code already sunk into the SQL quagmire.
They're type-level strings enabled by `-XDataKinds`. They have kind `Symbol`. Each string is a different type which don't unify: &gt; (Proxy :: Proxy "a") `asTypeOf` (Proxy :: Proxy "a") Proxy &gt; (Proxy :: Proxy "a") `asTypeOf` (Proxy :: Proxy "b") &lt;interactive&gt;:14:38: error: • Couldn't match type ‘"b"’ with ‘"a"’ Expected type: Proxy "a" Actual type: Proxy "b"
I don't understand "exceptions" in Haskell. It seems like, unlike most other languages, there are several different practices/implementations of the concept in common use. It also seems like there are many times where exceptions/errors are "unchecked" (not represented in the type system). I find the latter point very confusing, considering the language's focus on safety elsewhere. If I want to write robust, safe code, how should I model the concept of "exceptions", and how should I interoperate with other code that uses a different model?
Hello developers! I am new to the Haskell community (by the way, is there a name for it?) and would like to learn about beginner friendly projects that encourage open source contribution. I am currently going through the cis194 assignments. I also intend to participate in HSoC! Any help would be appreciated.
I haven't seen much in depth, though it looks like Control.Monad.Error and Control.Monad.Exception would have what you're looking for. The basic theory seems to be to use some sort of sum type like Maybe or Either as your output. If given non-error input, you do the function as normal, wrapping it in a Just or a Right at the end, but if the input's bad, you return Nothing or Left "some string describing the error" or maybe some whole data type (maybe a heterogeneous list containing the input for debug purposes?). This way, you have a bunch of a -&gt; Maybe b or a -&gt; Either String b which you can then chain together with monad tools like bind and Kleisli composition. 
Here are my personal tips: Profile and see what part of code needs optimization, then try to use more unpacked data structures (remove pointer indirections). Very often using Vector instead of Lists is a good idea. Throughput versus Latency problems in Haskell. For a consistent frame rate or animation the latter is important. Generally, GHC is optimized for throughput. Simon PJ once said to avoid haskell for latency critical stuff on Stackoverflow. There are some tricks to improve the it though. The general advice is to reduce the size of the retained set for latency problems so that GC runs more quickly. Prefer concrete types, for example, use `State s a` instead of `MonadState s m =&gt; m a` when possible. GHC may optimize `State` to an assembler loop not much different to what a C++ compiler would produce, but `MonadState` (unless specialized) will be passed as a pointer to a record with pointers to methods. The -XStrict pragma: e.g., “{-# LANGUAGE Strict #-}”. It is somtimes a quick way to check if non-lazy evaluation might help for a module. “-llvm” sometimes helps. For very memory hungry programs (e.g., running or typechecking Agda programs), this might help: "+RTS -s -M11G -H11G -RTS -A1G" or even “-A2G”. Both assume you have 16GB RAM. If you need to compile/benchmark a lot, a faster desktop PC and using ramdisk for the stored files is a good idea. E.g., my ramdisk has read speed of 11GB per second. Lastly, there are things like improving sharing, memoization, etc, which are tradeoffs between CPU and memory. 
Are there any further papers on using Wavelets for feature extraction? I found [this paper](http://ieeexplore.ieee.org/document/7499039/).
I also found that section really confusing. What exactly does operating in the I/O monad bring to the table that an imperative language wouldn't get? Isn't the point of callbacks to launch both requests at once?
GHC Haskell not only has a powerful type system but also a powerful kind system. Kinds are the types of types. `Symbol` is the kind of type-level strings. These are very useful when you need to know about the value of string at compile time. Some examples of uses for `Symbol` include: * [Help descriptions in `optparse-generic`](https://hackage.haskell.org/package/optparse-generic-1.2.3/docs/Options-Generic.html#g:2) * [Servant API types](http://haskell-servant.readthedocs.io/en/stable/tutorial/ApiType.html) * Various experiments in record systems * Various `Generics` systems which aim to make accessor and constructor names of datatypes available at compile time * The new [safe-money](https://hackage.haskell.org/package/safe-money-0.4.1/docs/Money.html) package
Thanks! Good to know about -Wtype-defaults. I haven't tracked the recent changes to this stuff. I don't think the default has always been Integer, though. Also, as I said, given how much of the standard libraries still use Int by default I think Int contamination is pretty easy in real programs.
If you want a good sort of "universal" encoding for this you might want to look into the old work by Petter Potts on [exact real arithmetic with nested linear fractional transformations](https://www.doc.ic.ac.uk/~ae/papers/potts-phd.pdf). This was the approach I was taking when I last cared about really accurate math. In particular playing around with what Bill Gosper calls [Hurwitz numbers](https://perl.plover.com/classes/cftalk/INFO/gosper.txt) and which can be generalized a bit to allow for a nice closed form representation for pi. Things get complicated, though, when you try to make the system maximally lazy, keep as much stuff in some sort of normal form where the (generalized) Hurwitz pieces and meromorphic functions are concerned and avoid space leaks. The problem with the approach you give here is that it has all sorts of gaps in its coverage that you're trying to cover piecemeal by adding rule after rule after rule.
Reducing symbolic expressions can work but also has some drawbacks. Namely, it can quickly become exponential. You tamp this down by making computations take a DAG format instead of inlining all of the subexpressions but doing this properly and generally is hard. Mathematica does seem to handle it nicely though so it’s worth looking closely there. 
On the flipside I can imagine there could be a snippet of Python that looks clean and readable but may look nasty in Haskell.
&gt; The -XStrict pragma: e.g., “{-# LANGUAGE Strict #-}”. It is somtimes a quick way to check if non-lazy evaluation might help for a module. Oh really, have you find this to work well in practice? Since issues are sometimes caused by [not being lazy enough](http://blog.ezyang.com/2011/05/an-insufficiently-lazy-map/), rather than not being strict enough, I would have thought that enabling it on an entire module would introduce as many problems as it solves, thereby making the results unclear.
Yesterday it made a complex program run 22% faster. I used this pragma to one of the modules that allocated a lot. For me it's more a way to profile; e.g., okay, this made the program 22% faster, so there is definitely a way to improve the program (e.g., reduce laziness of a function). Also, it's what Idris is doing on default (e.g., strict evaluation is the default).
Correct, sorry for the confusion. Typically you need a JIT to pull off inling from dynamic sources.
SDL2
Regarding "-fllvm" see also [this posting on reddit](https://www.reddit.com/r/haskell/comments/7k4wsx/my_take_on_haskell_on_advent_2017_maze_challenge/).
&gt; Why a Haskell programmer might want to use GHC Were there competing Haskell implementations at the time? I always thought GHC was the first...
This posting inspired another post [on performance tips](https://www.reddit.com/r/haskell/comments/7kf0qq/collection_of_advanced_performance_and_profiling/).
Hey, why is there no mention of [exact-real](https://github.com/expipiplus1/exact-real)?
Thanks for the links! I'm super into any links like this that explore exact arithmetic. The latter Hurwitz numbers seems really neat that you can choose the precision in a streaming fashion. The Potts paper is substantial so I'll have to print that out. I appreciate the attached Miranda code. :3 It's true that adding rules in an ad-hoc fashion is a bit lame. I think that's what Mathematica does. I think it's a mix of symbolics with a big bag of rules and when needed they'll compute approximates: &gt; When you type in an expression, the Wolfram System automatically applies its large repertoire of rules for transforming expressions. These rules include the standard rules of algebra, such as x-x=0, together with much more sophisticated rules involving higher mathematical functions. &gt; &gt; The general principle that the Wolfram System follows is simple to state. It takes any expression you input, and gets results by applying a succession of transformation rules, stopping when it knows no more transformation rules that can be applied. E.g. if you ask `sqrt(2) + pi &lt; 2pi`, it can tell you the "difference" is `sqrt(2)-pi`. Pedagogically, while this isn't efficient to run (or probably to implement), you can show "steps", like in [Duet's evaluation stepper](http://chrisdone.com/toys/duet-delta/). So I do think symbolic numerics has a teaching advantage. Probably the rules aren't as simple as lambda calculus + case, though. :-) Meanwhile, the long form goal of a nice precise arithmetic package in Haskell based on something like the Hurwitz numbers talk is definitely up my alley. Cheers!
I know I might be down voted. Basically there is no good option of doing GUIs in Haskell. Also, there is no single advanced GUI program besides the outdated Hoodle (low level GTK code hinders code readability) and Xmonad.
See also the thread on [advanced performance/profiling tips](https://www.reddit.com/r/haskell/comments/7kf0qq/collection_of_advanced_performance_and_profiling/) for haskell
One of the many web frameworks and run it inside Electron.
What are some problems caused by conflating those? Also, is the distinction it about, for a concrete example, the fact that the list type in Haskell has (at least) three different interpretations: - as the inductive type of finite lists; - as the coinductive type of possibly infinite streams (which in a way form a superset of lists); - as a lazy data structure to implement streams?
Sure, when someone writes a "What Makes Python Unique" blog post they can use those.
&gt; the fact that the list type in Haskell Haskell doesn't have a list type, but I'm going to assume you mean the type constructor `[]`. &gt; as the inductive type of finite lists This is blatantly wrong, i.e., everyone knows this doesn't agree with Haskell's semantics. &gt; as the coinductive type of possibly infinite streams (which in a way form a superset of lists) This is still wrong, albeit for subtler reasons. `undefined` is most certainly typable with type `[a]` for any type `a`, but it is not “a stream”. &gt; as a lazy data structure to implement streams? Haskell types are *never* inhabited by data structures. They are inhabited by computations, some of which return data structures.
This is a frequently asked question, but we don't have an FAQ in the sidebar, so I'll repeat my usual answer. The GUI situation in Haskell is unfortunate, and has been for a while. As explained in "[A History of Haskell: Being Lazy With Class](http://haskell.cs.yale.edu/wp-content/uploads/2011/02/history.pdf)", section 11.3: &gt; People interested in [Haskell GUI libraries] rapidly split into two groups: the idealists and the pragmatists. &gt; &gt; The idealists [...] sought to answer the question, “What is the right way to interact with a GUI in a purely declarative setting?” This question led to several quite unusual GUI systems: [...] Fudgets, [...] Haggis, [...] FranTk, [and] Fruit [...]. &gt; &gt; Despite the elegance and innovative nature of these GUIs, none of them broke through to become the GUI toolkit of choice for a critical mass of Haskell programmers, and they all remained single site implementations with a handful of users. It is easy to see why. [...] developing a fully featured GUI is a huge task, and each system lacked the full range of widgets, and snazzy appearance, that programmers have come to expect. [...] &gt; &gt; Meanwhile, the pragmatists [created bindings to] widely available GUI toolkit libraries. [...] &gt; &gt; To this day, the Haskell community periodically agonises over the absence of a single standard Haskell GUI. Lacking such a standard is undoubtedly an inhibiting factor on Haskell’s development. Yet no one approach has garnered enough support to become the design [...]. To summarize, creating a full GUI system from the ground up is an enormous amount of work (native look &amp; feel! fancy international keyboard input! right-to-left text support! accessibility! and so many widgets!), and so it just makes sense to use bindings to existing libraries, such as [gtk](https://hackage.haskell.org/package/gtk), [hsqml](http://hackage.haskell.org/package/hsqml), and (the easiest of the three) [fltkhs](http://hackage.haskell.org/package/fltkhs). The downside is that those libraries use the same API as those upstream libraries, and that API is very imperative, so it doesn't fit very well with the rest of the Haskell ecosystem. For this reason, the Haskell ecosystem is [considered immature](https://github.com/Gabriel439/post-rfc/blob/master/sotu.md#standalone-gui-applications) in the domain of standalone GUI applications. That being said, if all you want is an application with buttons, there are other ways to make that happen. You can [draw your own buttons in gloss](https://github.com/gelisam/frp-zoo) or using other OpenGL libraries. You can use [brick](https://github.com/jtdaugherty/brick) to write a terminal-based app with ASCII art buttons. You can generate HTML buttons using [`reflex-dom`](http://hackage.haskell.org/package/reflex-dom) or [`threepenny-gui`](http://hackage.haskell.org/package/threepenny-gui). Those libraries are all more more idiomatic than the libraries which offer bindings to an existing GUI library.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Gabriel439/post-rfc/.../**sotu.md#standalone-gui-applications** (master → 9aec61c)](https://github.com/Gabriel439/post-rfc/blob/9aec61cb08bded38ea454b52b5b1a3bb7426db17/sotu.md#standalone-gui-applications) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
&gt; Throughput versus Latency problems in Haskell. FWIW, you have to have some *serious* latency requirements for this to be a problem in any GC'd language (Haskell actually being among the better ones in this regard). &gt; Prefer concrete types When the specializer is doing its job, this isn't an issue. Problem is convincing the specializer to do its job... The easy way is to add `INLINABLE` to anything you want to be specialized, and GHC will make sure to specialize it at every single call site.
Thanks! The latency issue was a startup doing a internet based messaging system. Thing DBUS but online. Haskell latency was horrible in October 2016. They consulted with Simon Peyton Jones on Stackoverflow. As far as I remember the posting (lost the link). He said that Haskell is not good enough, they had to switch programming languages. Now the startup isn't using Haskell and it was a sign to me (and maybe others) that GHC has some issues.
&gt; Plus, if you use some libraries a lot, they tend to get easier to use to the point where you might as well throw it in for even trivial stuff. In the case of parser combinator libs, this is what I mean I've moved away from after fixing a few bugs in what ought to have been trivial parsing tasks. Of course it's all a judgment call.
I don't recall the business logic required, but I do remember that their latency requirements were *extremely* strict, such that Go was the only GC'd language I know of that would have fit (due to its GC compromising on just about every other desirable GC feature to reduce latency). They needed a maximum of like 10ms latency *at all times*, which is just crazy for a GC.
There's [this desktop movie player written in Haskell](https://github.com/lettier/movie-monad) with an [accompanying guide to how it was built](https://lettier.github.io/posts/2017-08-30-haskell-gtk-video-player.html).
Do you remember the company name or their stackoverflow post? I tried to google it but could not find it.
To clarify a bit, numeric loops especially are where -fllvm shines. Basically, if the code will compile to a somewhat C-like block, LLVM can really help optimize the numerical and the indirections.
The company was called Pusher, IIRC.
It has been Integer since at least 98: https://www.haskell.org/onlinereport/decls.html#default-decls
Thanks, makes sense. I guess the interoperability was more my concern. It seems like different libraries use different mechanisms. Including program termination (thinking here of the old `http-client` "non-200" policy), and I find it hard to understand how to bring that all together in the calling code. Or even what the "best practices" are…
Lots of concerns: Hosting/Operations, DevOps, durability, existing infrastructure sharing, parallelism, compliance - to name a few. 
Anything can look “clean and readable” if the syntax doesn't reflect the complexity of the language's semantics. This seems to be the entire point to operator overloading in some object-oriented languages (C++, Python, etc.).
Please ask us here if you want to know any more details, we're also on Twitter at https://twitter.com/monadic_waw/status/942335124011716608
My latest discovery of this kind is [tqdm](https://github.com/noamraph/tqdm). This lets you change the Pythonic for x in xs: # ... to the still very Pythonic for x in tqdm(xs): # ... and get a progress bar for free with all the usual fun stuff: a bar that fills up, an indicator of how long it's been going, an estimate of how long until it's full, all that good stuff. As an API, it is gorgeous: essentially just one function that you drop in the right place and it Does What You Want. I have *no* idea how to design a similar API for Haskell that fits into idiomatic Haskell loops/`map`s/`filter`s/etc. nearly as cleanly. What a shame!
Right, and conflating these has lots of problems because you end up in a situation where termination depends on your evaluation model. In contrast, in a language like Agda which discriminates between these notions, you can safely guarantee totality independently of your evaluation model. Hence, Agda can compile quite easily to JS (strict) and Haskell (lazy). Same with Idris. There's also just the safety aspect -- it's easily to make things loop by accident in Haskell. Moreover, this conflation and leaky non-termination cripples a lot of potential optimization, since reordering things in the presence of non-termination can change the meaning of your program. It's not even safe to do something as simple as rewrite the functor law: `fmap f (fmap f) = fmap (f . g)`! It's also a major reason supercompilation is impossibly difficult instead of relatively straightforward. The downside is it does place a slight burden on the developer -- `[1..5]` and `[1..]` are no longer the same kind of thing (although arguably they never were, it just wasn't represented in the type system). But in my experience it's not that painful in practice. It's unlikely that any of this will change in Haskell, but still, it's good to be aware of the cost for playing sloppy here.
What do you think about the gtk bindings?
I would use the [GTK bindings](https://hackage.haskell.org/package/gi-gtk) produced by [haskell-gi](https://github.com/haskell-gi/haskell-gi). You can be up and running with a GUI rather quickly. Both [Gifcurry](https://github.com/lettier/gifcurry) and [Movie Monad](https://github.com/lettier/movie-monad) use the bindings produced by haskell-gi. Gifcurry had originally used [gtk2hs](https://github.com/gtk2hs/gtk2hs) but was then later ported over to haskell-gi. Here is a small example using [Haskell Stack](https://docs.haskellstack.org/en/stable/README/). &gt; stack.yaml flags: {} extra-package-dbs: [] packages: - '.' extra-deps: - gi-gtk-3.0.18 - haskell-gi-base-0.20.8 resolver: lts-9.1 &gt; gi-gtk-test.cabal name: gi-gtk-test version: 0.1.0.0 build-type: Simple cabal-version: &gt;=1.10 executable gi-gtk-test main-is: Main.hs build-depends: base &gt;=4.9 &amp;&amp; &lt;4.10 , gi-gtk ==3.0.18 , haskell-gi-base ==0.20.8 hs-source-dirs: . default-language: Haskell2010 ghc-options: -threaded &gt; Main.hs -- Taken from https://github.com/haskell-gi/haskell-gi {-# LANGUAGE OverloadedStrings, OverloadedLabels #-} import qualified GI.Gtk as Gtk import Data.GI.Base main :: IO () main = do Gtk.init Nothing win &lt;- new Gtk.Window [ #title := "Hi there" ] on win #destroy Gtk.mainQuit button &lt;- new Gtk.Button [ #label := "Click me" ] on button #clicked (set button [ #sensitive := False, #label := "Thanks for clicking me" ]) #add win button #showAll win Gtk.main To build it. stack setup stack build stack exec gi-gtk-test 
I would recommend jsaddle-webkit2gtk over Electron. It's similar, but your Haskell will run as a native process, which will make it ***much*** faster.
I'm looking forward to implementation of ideas from this paper: * https://github.com/paf31/the-future-is-comonadic But I'm disappointed as well by the current state of GUIs in Haskell... :(
But by trying to store the data from one database in another you only make all of those problems worse...
I have studied wxHaskell, GTK, Qt, Cairo. I guess SDL, OpenGL, etc. have the same problems. For me it is, the more you know on advanced GUIs the more horrible you see are those library. One problem is that they have 10.000s to 100.000s of lines of code what you can do in a fraction (1000 to 5000??) lines. The problem is that the design is very flawed. As for, how good are the BINDINGs to those horrible flawed libraries? I guess if the libraries are that flawed even good bindings in Haskell don't help. A part from that, I find the jsaddle-webkit2gtk package very interesting.
Are you the author of jsaddle-webkit2gtk?
I think it is generally considered bad practice for pure code to throw exceptions, so if you are writing pure code that might error out and you want those errors to be catchable, you should use `Either` or `Maybe` or similar. `IO` is more controversial. Some things like asynchronous exceptions are pretty hard to model with sum types, so those end up basically having to be exceptions. There also are generally more possible things that can go wrong in `IO` code than in pure code. Due to the above some people argue that you shouldn't bother with sum types for `IO` code and just raise and catch exceptions, and then when you see `IO` code you should always assume it might fail, and perhaps read the docs or similar. Other people believe that just because sum types in `IO` don't deal with ALL possible errors, that you can still get a lot more safety and reasonability and so on by using sum types for as many synchronous exceptions as possible, so `readFile` will potentially return something like `Either FileError String` where `FileError` contains things like the file not existing or lack of read permission and so on. I think the latter is generally the more popular opinion, but unfortunately there isn't as much of a consensus on that.
No. That would be Hamish Mackenzie.
&gt; GUI system from the ground up Can I cite you in one of my papers? I guess it would really help as quick way to say there is a gap in the state of the art. I am doing an advance GUI system from the ground up in academia. I guess it is some work but doable. The upside is that with a very good design things get so much more elegant, short and easy. There is really no sense in throwing around C pointers in Haskell and wxHaskell and GTK are doing exactly that. If anyone is interested in doing very advanced GUIs in academia, please write me.
What I researched, wxHaskell is better. But there is little difference in that the underlying libraries wxWidgets and GTK are that flawed. 
 fstIso :: (forall a. Iso (f a) (g a)) -&gt; (forall a. f a -&gt; g a) fstIso (Iso a _) = a sndIso :: (forall a. Iso (f a) (g a)) -&gt; (forall a. g a -&gt; f a) sndIso (Iso _ b) = b convert :: (forall a. Iso (f a) (g a)) -&gt; NT f g convert iso = NT (fstIso iso) (sndIso iso)
Haskell IO is asynchronous by default. The only other languages that have this property that I know of are Erlang and Go.
The easiest way is to use [FLTKHS](https://github.com/deech/fltkhs), but it doesn't look as nice as Qt or GTK+. That said, FLTKHS is easy to get started with, if you follow the instructions. If you want something more powerful, I can suggest [Qtah](http://khumba.net/projects/qtah/). Both Qt and FLTK have a GUI designers you can make use of.
The easiest way is to use [FLTKHS](https://github.com/deech/fltkhs) by /u/deech, but it doesn't look as nice as Qt or GTK+. That said, FLTKHS is easy to get started with, if you follow the instructions. If you want something more powerful, I can suggest [Qtah](http://khumba.net/projects/qtah/). Both Qt and FLTK have a GUI designers you can make use of.
…both of which are possible with Electron, too. Speaking of jsaddle, how does it handle sync callbacks? Say when you need to preventDefault() or stopPropagation()?
How do you use GHCi with Electron? GHCJS does not have interactive support, so reloading and testing requires recompiling, which is much slower than GHC reloading interactively. Also, jsaddle-webkit2gtk is demonstrably much much faster than GHCJS.
I was playing with benchmarks in the `containers` package and hated that after each change, stack had to rebuild half of `criterion`'s dependency tree – more than 20 packages! Then I came accross `gauge` on the criterion issue tracker, swapped it in, and tada! it has only a single, direct dependency on `containers`! =)
https://github.com/tonyday567/perf I started to collect some tips and tricks in the readme, but its way incomplete. I'd especially like to add some tips for the compile chain: how to clean up and read core, check for rules firing and such. 
How about this for a first approximation: import Data.Foldable for_' :: Foldable t =&gt; t a -&gt; (a -&gt; IO b) -&gt; IO () for_' xs f = do let len = length xs for_ (zip [1..] (toList xs)) $ \(i,x) -&gt; do putStrLn ("step "++ show i++" of "++show len) f x 
I've tried to improve the situation a bit with my library [`frpnow-gtk3`](https://hackage.haskell.org/package/frpnow-gtk3), which gives some high-level constructs on top of the low level [`gtk3`](https://hackage.haskell.org/package/gtk3) bindings.
It's a proprietary system. Suffice to say it works similar to how you use jsaddle to develop ordinary webpages (run the app in ghci/ghcid, communicate with browser via a special channel etc). Remember, Electron is essentially a browser with a few special APIs. So any approach you use to develop webpages with Haskell you can also use in Electron.
It's a proprietary system. Suffice to say it works similar to how you use jsaddle to develop ordinary webpages (run the app in ghci/ghcid, communicate with browser via a special channel etc). Remember, Electron is essentially a browser with a few special APIs. So any approach you use to develop webpages with Haskell you can also use in Electron.
Ok, cool. I'd written fstIso and sndIso, but I was trying to just substitute the definitions into convert rather than keeping them separate. Turns out if I just had put them in a where clause, it seems to work w/o signatures for fstIo and sndIso, so those don't clutter everything up. Thanks for the help! As a side note, I do find it funny that my first impressions coming into Haskell were "Awesome! No need for those annoying type signatures if they're not necessary!" and now nearly all of my experimenting with Haskell's just with those weird higher rank and dependent things. 
Haha I went through this the other day as well. At first I thought stack was being janky.
Oh sorry &gt; One of the many web frameworks and run it inside Electron. This implied GHCJS to me. I'm not sure what "many web frameworks" would work with Electron other than GHCJS, since jsaddle doesn't currently have an Electron backend. Plus, I don't see an advantage to Electron over webkit2gtk. The latter is quite a bit leaner.
&gt; Very often using Vector instead of Lists is a good idea. For what it's worth, using vectors instead of lists is usually an indication you're using lists wrong. Lists are *not* for storing homogeneous data. I have a blog post with one [nice example](http://blog.vmchale.com/article/elgot-performance) of how to use lists, but I'm sure there are more out there. 
Or use [Threepenny][1] with Electron. [1]: https://github.com/HeinrichApfelmus/threepenny-gui/blob/master/doc/electron.md
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [HeinrichApfelmus/threepenny-gui/.../**electron.md** (master → adcacef)](https://github.com/HeinrichApfelmus/threepenny-gui/blob/adcacefc1712866aa18a608d1b2b05ab9243f706/doc/electron.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
&gt;There is lazy vs strict problems Usually this requires actual benchmarks (in *full* context) in my experience. &gt;Benchmarking I just use criterion. &gt;performance tips My #1 tip is to learn functional data structures. Haskell is a garbage-collected language. But using functional, lazy data structures will get you much closer to low-level languages. And more importantly, it will enable you to write the code that Haskell excels at. Is Haskell ever faster than Rust? Not really. But Haskell can make it impossible to write equally concise, fast code.
Is your last sentence a typo? Haskell can make it impossible....? Your #1 tip is contrary to my research as an intermediate haskell user in academia. My quest has largely been to avoid lazy structures. But this might be my domain of Haskell work. And to battle to get parallel execution in Haskell which is quite hard to do in Haskell (despite claims that Haskell is "best in class"). I might have to learn to use functional data structures more.
This is a wonderful link, exactly what I was searching for.
Well I did some reading on this thing now which I actually never considered much and truth be told that's a bit of a fine print of Haskell that is never mentioned. Being unable to use dynamic libraries like you're used to is a bit of a hefty disadvantage you don't normally read about to say the least. It makes it hard for Haskell code to in general be distributed in the traditional sense and that's a disclaimer that doesn't seem tob mentioned often with the downsides of a lazy implementation.
This makes me feel like I will never “learn Haskell” like you can learn and understand Javascript or C, lol.
Well just to be clear: you *can* use dynamic linking with GHC. But its purpose is to reduce footprint among Haskell binaries that share Haskell library dependencies. What you *can't* do is swap out said dependencies dynamically with different versions. In any case, static linking has not been a problem for the many distros that ship Haskell packages. I've never encountered a scenario where the inability to swap a Haskell library with a different version without recompiling was a problem.
&gt; Speaking of jsaddle, how does it handle sync callbacks? Say when you need to preventDefault() or stopPropagation()? Depends on which jsaddle runner you are using: * `jsaddle-webkit2gtk` and `jsaddle-wkwebview` (for macOS and iOS) override `window.prompt`. * `jsaddle-warp` (for web browsers) uses `XHR.send`. * `jsaddle-clib` (used with JNI on Android) lets you take advantage of fact that Android allows sync callbacks from JavaScript to Java. 
&gt; Well just to be clear: you can use dynamic linking with GHC. But its purpose is to reduce footprint among Haskell binaries that share Haskell library dependencies. What you can't do is swap out said dependencies dynamically with different versions. Yeah, I gathered as much; that's sort of the real benefit of it I guess and why software is often distributed like that. Update once, fix anywhere. &gt; In any case, static linking has not been a problem for the many distros that ship Haskell packages. I've never encountered a scenario where the inability to swap a Haskell library with a different version without recompiling was a problem. So it seems strange to me that you consider this a hefty disadvantage. It happens quite often that a major security vulnerability is found in a particular library and then you just update the library and that is that. Having to trace everything it is statically linked in to manually recompile all that would be a lot of work.
&gt; Having to trace everything it is statically linked in to manually recompile all that would be a lot of work. Is it really? I primarily use Nix, so this sort of thing is fully automated there. I would have figured that any decent packaging system could automate this sort of thing easily.
Forgive a little self-promo but of the GUI options I think [FLTKHS](https://hackage.haskell.org/package/fltkhs) is still relatively unique. Besides the [executive summary](https://hackage.haskell.org/package/fltkhs-0.5.4.1/docs/Graphics-UI-FLTK-LowLevel-FLTKHS.html#g:2) of what it brings to the table it's the only one that allows you to (1) define new widgets directly in Haskell seamlessly, (2) retrofits the GUI builder to generate Haskell at build time meaning you can inject Haskell code directly into the GUI builder resulting in much tighter integration with existing Haskell code, this is quite unlike GTK/Qt where you have to read the Glade/Designer file at runtime and do icky things with strings, and (3) lets you completely override the C++ event loop so something more declarative is possible. 
Well I use portage which also tracks what links against what and when ABIs break. But the point is that it still requires extensive recompilation if you want to update simply a single shared library. This might not be much because not much of your system is made in Haskell but so much links against say LibreSSL and if half your system had to be rebuilt every time libressl updates that'd be cumbersome I guess. In the case of a binary system half the packages of your system would have to be redownloaded. There's another problem in that Arch is actually strangely better suited for this because of its rolling binary nature where they have no promises on ABI breaks. Point release systems like Debian and Fedora generally promise that within a single major release no shared libraries ever change ABI so that whatever you locally compile on your system keeps working until you upgrade to the new point release as well as being able to install binaries that target the ABI of that specific version and from what I understand keeping ABI of a shared library the same in Haskell after a security fix is essentially impossible.
That's why haskell binaries should be statically linked. You don't have to redownload an entire dependency hierarchy, just the leaves; and there's no ABI issue to worry about. It's not ideal, but I don't think Haskell would be very practical without the inlining, and it's been pretty low cost to me so far, professionally.
&gt; I have no idea how to design a similar API for Haskell that fits into idiomatic Haskell loops/maps/filters/etc. nearly as cleanly. What a shame! If you want a progress bar, then you'll needs effects in IO. You very likely want this because you're reading from a file, so it would make sense to use something like conduit. With those assumptions in place, it's fairly trivial to write [something like this](https://gist.github.com/rdnetto/90f23ea17960b50236199c120fbd37d6). The only downside is that you need to know the length of the input stream up front to compute a percentage and ETA, but that's unavoidable.
Ah right. What you said that made me realize that `fstIso` and `sndIso` actually don't need to be higher ranked at all, `fstIso :: Iso a b -&gt; a -&gt; b` is sufficient. Now what they DO need to be is defined on the top level, so that they can be used polymorphically. Since they do not close over any of the parameters passed in to `convert`, putting them in a `where` clause is equivalent to putting them on the top level, so as you noticed that also works. I changed my earlier comment to show what the true issue with the more direct approach is, as it is not actually the fault of type inference. It's actually about the scope in which `forall a.` is quantified over for `Iso` vs `NT`, it might be worth reading over.
Javascript?
Yes and the confusing part is that the syntax looked like Javascript. 
&gt; Haskell types are never inhabited by data structures. They are inhabited by computations, some of which return data structures. This is a very operational point of view. If it's useful to you, fine. But there is a consistent denotational semantics here. Doesn't make much sense to try to "correct" a statement from a perfectly valid understanding into another perspective that is no more valid, but just more operational. If you don't see how the denotational perspective makes sense, that's worth spending some time thinking about. The idea is to stop thinking about which operations are performed in which order, and just look at how the meaning of an expression is composed from the meaning of its subexpressions. The meaning of an expression never *is* a computation; computation is the process of *finding* the meanings of expressions.
&gt; But there is a consistent denotational semantics here. Did they make Haskell total when I wasn't looking? &gt; The meaning of an expression never is a computation; computation is the process of finding the meanings of expressions. What's the meaning of a computation that never ends?
Is this work commercial, academia or hobby?
&gt; `(fmap g) . (fmap h) = fmap (g . h)` IIRC that's due to typeclass-less `seq`, not due to `_|_` in general. Also I don't buy the idea that totality makes compiling to JS and to Haskell any easier, since just because evaluating `x` is guaranteed to terminate says little to nothing about how long it will take, so you can't just switch freely between strict and lazy evaluation unless you want to surprise users with potentially massive pauses that only appear in one compilation target. For similar reasons as above I don't buy the idea that it gives you many new optimization opportunities. Now of course it is still useful to increase safety and decrease the number of mistakes you can make that still type check.
I am working on advanced GUI is academia. I looked at the library and found it very, very difficult. It seems quite low level even compared to GTK. If you would help me implement a cool example, I have a very good and simple example implemented in both wxHaskell and SDL2. I think it is impressive, but I am biased. If you help me slightly in porting it to FLTK I think you would help promote the library.
I am well aware that these "interpretations" (that I did not mean in a formal sense) don't match whatever the type constructor `[]` actually means. But I certainly get some mileage out of thinking about ("interpreting") `[]` as a good enough approximation of any of them. Furthermore, when we write programs, we usually assume that variables are bound to computations that terminate and return a value, which is as good as a simple value for many purposes. The fact that Haskell types are inhabited by computations rather than values matters only because the computations may not terminate or throw exceptions. So that sounds more like a criticism of Turing-completeness and unchecked partiality (well known thorns) rather than not having proper inductive and coinductive types. 
&gt; So that sounds more like a criticism of Turing-completeness and unchecked partiality (well known thorns) rather than not having proper inductive and coinductive types. No. Turing-completeness is useful. What has to go is the conflation of computations with the values that they might produce.
Yeah, ok, that explains a lot wrt my errors. Although, if that's true, the shouldn't I be unable to write p :: forall f g. (forall a. Iso (f a) (g a)) -&gt; (forall b. Iso (f b) (g b)) -&gt; NT f g p iso1 iso2 = NT (first iso1) (second iso2) where ... if the foralls are in fact being used in two different ways (maybe I'm just misinterpreting?) also, slight nitpick, you'll want split x = (fst x, snd x) for the second function. I assume that's what you meant, but not quite what you typed. 
&gt; Did they make Haskell total when I wasn't looking? No. Denotational semantics exist even for non-total languages &gt; What's the “meaning” of a computation that never ends? `_|_`, which is an extra symbol in your set of values that specifically denotes computations that don't terminate.
&gt; Denotational semantics exist even for non-total languages When did I say it doesn't? I've understood [Stoy's book](https://www.amazon.com/dp/0262690764/) just fine. &gt; `_|_`, which is an extra symbol in your set of values that specifically denotes computations that don't terminate. I'm perfectly aware. But your collection of “values” isn't a set. It's a space with a non-discrete topology, and functions must be continuous.
As I'm sure you are aware, the denotational semantics for Haskell uses a special value, bottom, to act as the value of an expression which does not terminate or fails with a runtime exception. This is sufficient to describe the referentially transparent subset of Haskell, including the existence of (opaque) values for IO actions. If you are looking for a semantics that includes unsafePerformIO and such, or if you want a complete equational theory for I/O actions, you'll have to look elsewhere. Nevertheless, this is sufficiently useful and understandable to be the default way of thinking about Haskell expressions.
Nah, the simplest one is this: Edges are either positive or negative (meaning the value is negated). When more than one edge reaches a node, it means that it is an and gate. For or gates, just negate both inputs and output edges.
&gt; As I'm sure you are aware, the denotational semantics for Haskell uses a special value, bottom, to act as the value of an expression which does not terminate or fails with a runtime exception Of course, and this is inconsistent with the statement “Haskell has a type that denotes a collection of lists”.
Happy to help. Can you DM me a link?
JavaScript is not concurrent. Everything runs in one thread, so if one computation blocks you are stuck
This was academic. I wrote it when I had to make a GUI frontend for a program i produced for a Linguistics [research project](https://github.com/george-steel/maxent-learner). I released my wrappers as a separate library because I thought they would be useful to others here.
Funny thing is java too does not have a good UI (if you do not count web UIs) Swing and javaFX, despite huge financial investment from many corporate gians (Sun, Oracle, IBM etc) never become widely used. 
What are the features Go compromises on, out of curiosity?
Fixed! And actually what you have there is essentially going even further in the direction of being more doable / solvable. Now you don't even have to essentially use `iso` twice, as you already have two available to you. The issue with the direct approach is sort of the exact opposite, it's that you are have only one `iso` and only use it exactly once.
Huh. Could have sworn I remembered otherwise. Thanks much for the correction!
Thanks! I guess I'm wondering why it is necessary to do the concurrency stuff in the next section to get both requests issued at the same time, then? Sorry for the confusion.
This article was very enlightening to me: https://blog.plan99.net/modern-garbage-collection-911ef4f8bd8e
When will you start on June 11? Morning or afternoon?
&gt; FLTKHS I did just now.
I agree to an extent with a lot of what you're saying, and it sounds like you're firmly in the Bob Harper camp of things, but I disagree with how you go about presenting it. Dogmatically presenting concepts as if they're the obvious, one true holy way to think about things rubs me the wrong way in general but it especially strikes me as silly when we're talking about a research field that's barely in it's infancy. Types... Don't really have a single true, canonical notion. For example, I personally think that viewing types as sets of values is unnecessarily restrictive but that's just, like, my opinion, man. I also think that completely disregarding 99% of how we talk about programming languages and their properties just because we don't have a "perfect" way to express subtle nuances involving laziness, bottom, etc... Seems a bit like throwing out the baby with the bathwater. What do we win by saying "Haskell doesn't have a type that denotes lists"? How is that better than saying "all of Haskell's lifted types see inhabited by bottom"? And more importantly, suppose we do decide to be more "accurate" and "precise" about the terminology... How would you describe what Haskell calls types in a nice, clean way?
Are these not problems with implicit partiality/nontermination rather than the more specific concern of conflating coinduction and induction? I'm not sure what the functor laws illustrate. They just don't hold for free even if we forget about nontermination, so one has to check them case by case anyway, but it's also easy to check.
If you're willing to cheat, you can use the Debug.Trace library to make wrapper functions that wrap around common functions such as fmap, foldr, etc, and unsafely prints out information while they're run. (I mention this for the sake of completeness. This is essentially a terrible idea)
Hmm, yeah looks like you hit an issue there. No, I didn't suffer that last time I compiled my own cabal with stack (a few weeks ago).
&gt; it sounds like you're firmly in the Bob Harper camp of things Unlike Bob Harper, I'm totally fine with using a denotational semantics rather than an operational semantics. But the denotational semantics must not hide the structure present in the programming language. For instance, if you want a category whose objects and arrows are denoted by types and programs, respectively, then this category has to be enriched in such a way that we can usefully talk about things like whether two programs are equivalent or whether one program is asymptotically faster than another. In other words, you must not use a denotational semantics to sweep fundamental issues under the carpet. But I fear that a Haskeller would perceive this as being fundamentally the same as using an operational semantics. &gt; For example, I personally think that viewing types as sets of values is unnecessarily restrictive but that's just, like, my opinion, man. Of course, in the presence of general recursion and first-class procedures, types cannot denote just sets, as Dana Scott discovered. In general, types denote spaces, but, even in a call-by-value language like ML, only `eqtype`s denote discrete spaces. In Haskell, no type could possibly denote a discrete space. &gt; I also think that completely disregarding 99% of how we talk about programming languages and their properties just because we don't have a "perfect" way to express subtle nuances involving laziness, bottom, etc... We [do have ways](http://www.cs.bham.ac.uk/~pbl/cbpv.html) to talk about such issues. Haskellers just willfully ignore them. &gt; How would you describe what Haskell calls types in a nice, clean way? Um, [domains](https://ncatlab.org/nlab/show/domain+theory)?
This is the blog post explaining the whole story: https://making.pusher.com/latency-working-set-ghc-gc-pick-two/
Well, part of it anyway. They left out [all the compromises Go makes in exchange](https://blog.plan99.net/modern-garbage-collection-911ef4f8bd8e).
Fortunately, nobody made such a statement and everyone understood what I meant by "the list type in Haskell".
Which GUI framework ?
This lie would be easier to believe if you hadn't [said](https://www.reddit.com/r/haskell/comments/7kd1zn/what_makes_haskell_unique/drdw0h6/): &gt; as the inductive type of finite lists
&gt; Can I cite you in one of my papers? If you want, but I doubt reddit comments have much academic cred! &gt; I am doing an advance GUI system from the ground up in academia. There has been many attempts in the past, what's your spin on it?
Use wxHaskell. It has native look and feel on all platforms.
I need a Parsec combinator that will parse everything between two bars. "||wow|" would parse ["", "wow"] "|hello |world|" would parse ["hello ", "world"] The function would have a signature something like this betweenBars :: Parser a -&gt; Parser [a] betweenBars parser = undefined My attempt was to use the 'endBy' function betweenBars :: Parser a -&gt; Parser [a] betweenBars p = (b *&gt; endBy p b) where b = char '|' But I found that it would quite often parse an empty string at the end of the last bar. Any ideas?
There is old Haskell code for it as well, but it is pre-Haskell 98.
You could write me your full name in a private message. What is my spin? Ie in my Phd. You wrote that GUI tool kits are "very imperative". I think the problem is primarily that: 1) GUI tool kits are extremely bad designed and 2) very bad implemented. I.e., 1) hurts more than 2) and that they are imperative is less an issue for the functional world (not place here to explain the details). You wrote that there have been many attempts in the past. I don't know of any very good ones (but looked only the last 7 years back). I think [Verifying Functional Reactive Programs with Side Effects](http://types2017.elte.hu/proc.pdf#page=47) is not bad, though. My/our spin is to use the current state-of-the-art methods that exists today (but I think the functional world is mostly not aware of), improve on it and make an academic result of it. (I.e., academic goals not industry ready stuff.). There is no non-Haskell code in it. GUIs are written generically. For example, you write one GUI (with the GUI builder implemented in Haskell), you write one program, you get a standalone GUI AND a web GUI from one code base. One part is improving on FRP. But sacrifice native look and feel. I guess there is no sense in writing or explaining more. I think at some point I should have a showable result and discussing then would be easy. I guess people are not interested in the results anyways (people seldom are interested in good stuff; ie., most of the programming world rejects functional programming.) PS: If anyone wants to join building very advance GUIs, please write me. If you are intermediate/advanced on benchmarking, profiling and/or transforming single threaded code to parallel code, it would be easy to help. 
(I am not willing to cheat.)
Stop managing version bounds. Your story is the epitome of how version bounds are a lot of work for little payoff. Include your stack.yaml files in your packages so people can know how to build them if necessary. Get the packages in Stackage so there is a known set of packages against which they will build. But there is no reason for you to do all this busywork to satisfy some ideal of what a hygienic cabal file looks like. 
*If* you're using conduits or pipes, then yeah, it's possible to slot something into that API. But I do a lot of looping in Haskell that isn't conduit-based!
H., Threpenny is really, really great! What makes it so special is that you don't have to write and javascript code. I love it! But after the basic example you somehow want to change the looks/design and then I somewhat had the feeling I need to write stylesheed/CSS or something. Anyways, I have a very nice demo that I implemented. I wrote it in wxHaskell, SDL and are now probably porting it to FLTKHS. If I had a question to ask via DM or e-mail, would you help in porting it to Threepenny? Should take around 15 minutes or so.
As for libraries in Haskell. There is also the very [Haskus System](http://haskus.fr/system/). It allows to run GUIs directly in the frame buffer without any window system (e.g., without X server).
Don't all Stackage packages need to be on Hackage though? I'm pretty sure for example, Hackage requires a bound on at least `base`. Should my cabal file contain no version bounds, and is this legal on Hackage? Also, how do I locally build and test against multiple stack snapshots? I can only seem to be able to put one in the "stack.yaml" file. 
I am doing GUI research in Haskell. Also, I would recommend wxHaskell currently (it is more high level than the other systems like Gtk or fltkhs. And now need to write CSS for the layout. But note that wxHaskell is not fully type safe. It uses unsafe coercions which could lead to runtime errors that are not statically caught. 
Are you affiliate with Gloss or do you know the library well? I have a quite nice demo (only a few lines of code) that works in wxHaskell and SDL2. Probably will port it to FLTK and/or Threpenny in the near future. I would love to port it to Gloss too.
I'm poking around [Pong example!](https://github.com/ekmett/lens/blob/master/examples/Pong.hs) from the lenses lib. Just wondering what's the sane approach to work with the codebase, having to run `stack run mainapp` after each edit to see changes on screen seemed a bit too inefficient to me.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ekmett/lens/.../**Pong.hs** (master → 24793e1)](https://github.com/ekmett/lens/blob/24793e1f1b89c900ca891ddbebe31f67b8976cdf/examples/Pong.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
I am not affiliated with Gloss. I have used the library for a few projects so yes I would say I know it quite well as a user. Are you using both wxHaskell and SDL2 for this demo of yours, or is your demo working with each of them independently? Gloss will not replace the wxHaskell part, it is not a GUI library where you get buttons, tabs, text inputs and everything. It is more like a simpler version of SDL2. You get a window and the gloss takes care of rendering/event loop. The only thing you need to provide are (from Memory): - Name and initial dimensions of the window - Refresh rate - Initial model (whatever the state of your program is) - Event handling function - Update (on every frame) - A rendering function This setup makes it really easy to get something going in minutes! Hope it helps :)
So, I'm not sure if this is correct, but I've been doing this: * Pick a version of Stackage that builds * Remove all your version ranges from your project and only include the package names The outcome: * If you had version ranges, your build might break in stackage and you'd have to fix it. But it might break prematurely * If you don't have version ranges you have to fix it anyway, but it won't break prematurely * Tests ... seriously unit tests and specs on everything Why: * I maintain heaps of JS packages too and they run the silly Greenkeeper/David/dependency auto upgrade pull request nonsense and it just takes up *so much time* and people randomly change their libraries and break your build anyway.
Thanks for you answer! I would love to implement it in Gloss, if I know where to turn to if I get really stuck. Thank you. I know that I don't get any GUI parts and I don't need that anyways. I implemented the code twice. One program uses only wxHaskell. The other program uses only SDL2. They are completely separated from each other. The Gloss version would only use Gloss (not wxHaskell, not SDL2). Only the visual result is always the same. (The framerate is a bit lower in wxHaskell than in the SDL2 version.) I will get back to you in the next few days, if I get into trouble with Gloss.
Good luck! :-)
I'd be quite interested in reading a comparison between Stackage and JS (you mean NPM, right?) workflows.
Let’s see if someone says I’m doing stack wrong.
The last time I heard about this, it was said that there were explicit rules for how to round monetary values, and that if you preserved them as exact fraction that you would be in violation of financial law... is that no longer the case?
I generally try to use [GHCid](https://github.com/ndmitchell/ghcid).
I believe this is discussed in the article, you are given the choice of how to round your values depending on your use case, and safe-money will give you the result, and tell you how much residual there is. You do exact computations using the Dense representation, and then use round/floor/truncate etc. depending on what's valid for your jurisdiction.
Stackage only supports adding packages to nightlies so that they appear in upcoming major releases. Stackage's strict versioning is a pretty big reason to have version bounds in your Cabal file; so that people using older Stackage snapshots can include your package in their `extra-deps` and still get proper bounds checking.
&gt; The only downside is that you need to know the length of the input stream up front to compute a percentage and ETA, but that's unavoidable. In cases for which actions are composed applicatively, it is possible to keep track of the step count because it is static. I have a [small package](http://hackage.haskell.org/package/plan-applicative-2.0.0.1/docs/Control-Plan.html) which uses this technique.
What does that even mean? You want to take a non-Haskell binary and make it available as an `Application`?
Concurrent is not the same thing as parallel. It is absolutely possible to write concurrent JS, because concurrency doesn't require parallelism. (Think: time slicing on a single core CPU.) It's also not completely true that "if your code blocks anywhere, you are stuck". While there is only one thread for JS code, the runtime may employ more than one thread to handle the asynchronous tasks that get spawned. So it is possible for one async task (e.g. a network fetch) to stall, and not deadlock the rest of the JS.
Great, I look forward to trying it out! I've found the ghci debugger to be underpowered. I'd really like to be able to control which part of the evaluation tree to evaluate, but ghci only gives me a imperative style, do this, then that.
But some optimizations are easier with lazyness, like replaceing `const a b` with `a`, which wouldn't work when b is bottom in an eager language.
With this debugger you get no control over what is evaluated, and because it’s after the fact, it feels a bit imperative and a bit not at the same time. 
I use GHCid for all backend works (which has no user interface to see result after editing, so it's basically just the type checking and linting information), mind elaborating how does it work with ui apps like [Pong!](https://github.com/ekmett/lens/blob/master/examples/Pong.hs) mentioned above? Thanks.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ekmett/lens/.../**Pong.hs** (master → 24793e1)](https://github.com/ekmett/lens/blob/24793e1f1b89c900ca891ddbebe31f67b8976cdf/examples/Pong.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
Note that async exceptions can be raised at any time and in any code, including pure code.
* Remove upper bounds and you have a lot less maintenance. I only add manual upper bounds if I have a reasonable expectation it will break. * I have a script ‘neil sdist’ that I use for releasing. It’s still manual, and what you describe would be perfect, but it’s now under 2 mins for me, so not worth the effort. * You add to Stackage once, in a configuration file, and then never worry about it again. A 2 mom process and you also get contacted when you need to revise your package for package upgrades. 
People using older stackage snapshots can include any packages in their extra deps and have proper bound checking by checking if the thing actually compile end-to-end (aka `stack build`), not by checking version bounds from cabal file.
Other than the fact that "it compiles" isn't as accurate as curated version bounds, and and that I don't believe Stack runs the tests of your dependencies; `stack solver` is a common command used for building stack.yaml files from Cabal files and for being able to add package deps without thinking about versioning, and it depends on packages having version bounds.
I'd be really interested in reading such a comparison. If you'd like me to review it before you publish it, I'd be happy to. One clarifying question for your description above: it sounds like you're talking about building an application, not a library release for Hackage. Is that correct?
Thanks for the link. That was an interesting paper to read.
That's right. `git http-backend` in this case.
You need bounds on `base` to upload on hackage, but it doesn't means that they should be perfect. it's common to put `base &lt; 5` or `base &lt; 6` to satisfy the hackage need for base to have bounds yet not have to run the versioning treadmill. Relative to old compiler compat, in a perfect world it would be nice to support many old compilers, but realistically: * only a tiny handful of real people are using old compiler (most old compilers bug report is due to the matrix build which doesn't reflect how people use it). * it doesn't really affect stack user, since stack knows how to upgrade the compiler, so effectively you have more people using the latest and greatest stable. * Since it sucks *your* unpaid time to support old compiler, don't force the N compiler rules, just do it on a on-demand basis: someone is either paying for support, or you choose to support an older compiler from a user's plea.
Thanks for the reference and link.
I maintain libraries only in the stackage ecosystem but both libraries and executables in the NPM ecosystem.
Well I'm saying that that that is the best approach is an unsong disadvantage of lazy languages that isn't much talked about. You say it is a low cost but the inability to use the normal model of distribution that has been essentially universally selected in FOSS for its many disadvantages in memory, storage, security and even hackability in terms of LD_PRELOAD and stuff is a rather big downside. I'm saying that if it works like this then Haskell is unsuitable to ever use as a main language that most of your system is built in like C, C++ or even Rust would be because it would mean you'd have to either redownload or worste rebuild on a source-based system half of your system on minor updates—security related or not—with Haskell to get the benefits of the update in shared components which with C just involves only updating a simple shared library that is used by everything.
&gt; ... and stuff is a rather big downside. I have yet to encounter a scenario where this was true. None of those things has ever been a limiting factor due to static linking for me in production. &gt; you'd have to either redownload or worste rebuild on a source-based system half of your system Is this really a complete deal breaker to you? That's very surprising. Having to rebuild when dependencies change is fairly cheap in all practicality.
We were planning to start around 10 am
&gt; only a tiny handful of real people are using old compiler If Haskell continues to be adopted at the rate it is now, this will soon stop being true. Even with Stack making it easy, it's common for more enterprise-y places to have a policy of remaining on old versions of stuff so as not to "waste" time upgrading (unless there's a critical security risk). We still use 8.0 at my job, and I wouldn't be terribly surprised if that remains true after 8.4 is released. In fact, I'd wager the majority of people who are always on the latest Stackage lts'es are probably the hobbyists, not the businesses.
&gt; I have yet to encounter a scenario where this was true. None of those things has ever been a limiting factor due to static linking for me in production. Well you can't deny that if you have a lot of Haskell applications that say use libcurl and a massive security problem is found in libcurl that is fixed that it would require updating all of them instead of just the system libcurl only? Or if you link them to readline and readline gets new features that it would require updating all of them instead of just the system readline to get the new features. &gt; Is this really a complete deal breaker to you? That's very surprising. Having to rebuild when dependencies change is fairly cheap in all practicality. I wouldn't call it a deal breaker when very few things are built in Haskell but when your entire system would theoretically be made in it if you had to rebuild half your system on every library update that would. I mean I might as well do an update right now to illustrate it: misj@note $ emerge --sync &amp;&amp; emerge -uUD @world [ebuild U ] app-arch/tar-1.30 [1.29-r3] [ebuild R ] app-arch/xz-utils-5.2.3 [ebuild U ] sys-apps/util-linux-2.31-r1 [2.31] [ebuild U ] dev-libs/libxml2-2.9.7 [2.9.6] [ebuild U ] x11-libs/gdk-pixbuf-2.36.11 [2.36.10-r2] [ebuild U ] dev-cpp/glibmm-2.52.1 [2.52.0] [ebuild U ] net-vpn/tor-0.3.2.7_rc-r1 [0.3.2.7_rc] Would you like to merge these packages? [Yes/No] Let's single out xz-utils; that seems like a nice one; let's see what .so it brings: misj@note $ equery files xz-utils | grep \\.so$ /usr/lib64/liblzma.so Okay, so let's see what's in /usr/bin that links against it: misj@note $ for f in /usr/bin/* ; do if ldd "$f" 2&gt;/dev/null | grep -q "liblzma.so"; then echo "$f"; fi; done /usr/bin/appstream-builder /usr/bin/appstream-compose /usr/bin/appstream-util /usr/bin/archivemount /usr/bin/bsdcat /usr/bin/bsdcpio /usr/bin/bsdtar /usr/bin/cmake /usr/bin/cmake-gui /usr/bin/cpack /usr/bin/ctest /usr/bin/grub-mkimage /usr/bin/grub-mknetdir /usr/bin/grub-mkrescue /usr/bin/grub-mkstandalone /usr/bin/kate /usr/bin/kcmshell5 /usr/bin/kdecp5 /usr/bin/kdemv5 /usr/bin/kde-open5 /usr/bin/keditfiletype5 /usr/bin/kiconfinder5 /usr/bin/kioclient5 /usr/bin/konsole /usr/bin/kpackagelauncherqml /usr/bin/kpackagetool5 /usr/bin/ksysguard /usr/bin/lzcat /usr/bin/lzma /usr/bin/lzmadec /usr/bin/lzmainfo /usr/bin/meinproc5 /usr/bin/tor /usr/bin/unlzma /usr/bin/unxz /usr/bin/xz /usr/bin/xzcat /usr/bin/xzdec And this is just /usr/bin and these are jsut the ones that are readable because setuid executables generally arent and this is just one library and this is on a very minimal system. I'd to for all of those to have to be rebuild if one library is rebuild. I do thik that this disadvantage makes a system that is primarily built with Haskell infeasible. I don't think it's as easy as to blame the Arch Maintainers; you get similar problems of massive updates with lazy languages regardless; the alternative is just not keeping an up to date system which is obviously not what Arch is about.
I was referring to [this discussion](https://www.reddit.com/r/haskell/comments/5oq6p8/on_fmap_fusion/). As for switching between strict and lazy, honestly I've never had an issue with it. I know it's supposed to be a thing, but as long as you make `foldr` always strict in the accumulator and branches always lazy, most stuff tends to turn out ok. As for optimization, it's not the only difficulty in supercompilation, but as far as I can see it is definitely the showstopper. For deciding whether it's a good idea or not, just throw some heuristics at it and call it a day, as we do for many other optimizations.
Honestly, it may just be a bias I have since I use Nix (which makes this completely effortless), but updating a hundred packages seems like a complete non-issue to me. As long as you have an automated system to make sure it's done for you (hopefully one that provides prebuilt binaries, like Nix), I just don't see a problem with having to update a bunch of stuff. Furthermore, this seems more like a reason that Haskell maybe shouldn't be used to design an entire system, with Haskell-only tools throughout, and doesn't seem all that relevant to the typical business use-case of Haskell, which is to build a single Haskell product and deploy it.
They are, but I think most of the difficulty in confronting the totality issue is that so much of our ecosystem (especially the more Prelude-ish stuff) constantly conflates the various interpretations of List. &gt; I'm not sure what the functor laws illustrate. They just don't hold for free even if we forget about nontermination, so one has to check them case by case anyway, but it's also easy to check. I wasn't referring to checking them - I was referring to using it as a rewrite rule for optimization!
&gt; Honestly, it may just be a bias I have since I use Nix (which makes this completely effortless), but updating a hundred packages seems like a complete non-issue to me. As long as you have an automated system to make sure it's done for you (hopefully one that provides prebuilt binaries, like Nix), I just don't see a problem with having to update a bunch of stuff. Well I recently had to rebuild my entire system for the new GCC PIE ABI on Gentoo and that took about 7 hours. A large part of that was the 2 hours taken for Chromium. I do an update about once per day and if that would take like 4 hours every time instead of the usual 5 minutes my computer wouldn't be available for other things; I mean it works fine in the meanwhile for web browsing with the venerable MuQSS scheduler and all but if you need it to build something else or compute some simulation that's going to take a while. &gt; Furthermore, this seems more like a reason that Haskell maybe shouldn't be used to design an entire system, with Haskell-only tools throughout, and doesn't seem all that relevant to the typical business use-case of Haskell, which is to build a single Haskell product and deploy it. Maybe but it should be a disadvantage mentioned to people that with everything they write in Haskell they contribute to this; As you can see the people in Arch are very annoyed with this in how much they need to download. Now you say that this is an Arch packaging problem which in part is true but the solution to the problem entails "not staying updated" which of course goes against Arch. If they statically linked it and stayed just as updated it would still entail downloading just as much and that's just a major downside of lazy implementations that's apparently unsung.
&gt; I do an update about once per day and if that would take like 4 hours every time instead of the usual 5 minutes my computer wouldn't be available for other things NixOS updates for me take about 5min because binaries are cached upstream before releases are published. Granted, there is a massive cascade of downloads occurring, but I don't really have to pay attention to this and it's not really in the way at all. &gt; If they statically linked it and stayed just as updated it would still entail downloading just as much This isn't quite true. Downloading final binaries is quite a bit less than downloading the sum of their dependencies. &gt; that's just a major downside of lazy implementations that's apparently unsung. I don't understand what laziness has to do with this. This problem stems more from inlining.
Well, it is still based on HTML, so, yes, if you want to have complete control over styling, you need to apply some CSS. But that can actually be fairly painless. For instance, you can use a CSS framework like "Foundation". [Example source here][1]. That gives you a nice default style and some methods for layout. There are also some combinators for [binding to flexbox][2] on Hackage, thanks to Jeremy Barisch Rooney. [1]: https://github.com/HeinrichApfelmus/threepenny-gui/blob/master/doc/hal-2017/src/E02_html_and_css.hs [2]: http://hackage.haskell.org/package/threepenny-gui-flexbox &gt; If I had a question to ask via DM or e-mail, would you help in porting it to Threepenny? Sure, I'm happy to answer if you have a question.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [HeinrichApfelmus/threepenny-gui/.../**E02_html_and_css.hs** (master → adcacef)](https://github.com/HeinrichApfelmus/threepenny-gui/blob/adcacefc1712866aa18a608d1b2b05ab9243f706/doc/hal-2017/src/E02_html_and_css.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
When using Stack I never put any version bounds into my .cabal files but I've been wondering why .cabal files have support for version bounds when the package versions are pinned down via resolvers and extra-packages. Also why do you only remove upper bounds and not lower bounds as well? 
&gt; NixOS updates for me take about 5min because binaries are cached upstream before releases are published. NixOS doesn't statically link everything, but it does treat every update to anything as an ABI-breaking update, since there's no trustworthy convention to determine when an ABI is broken. Granted, there is a massive cascade of downloads occurring when I update, but I don't really have to pay attention to this and it's not really in the way at all. (EDIT: Oh the one thing that doesn't force ABI updates is the Linux kernel. The kernel is separated from everything else, and NixOS trusts that everyone is adhering to Linux's ABI). Well maybe you don't mind the cascade of downloads but it's clear the cascade of downloads are one of the biggest thorns in the eye of the Arch user community here; in the end some people still have data caps. &gt; This isn't quite true. Downloading final binaries is quite a bit less than downloading the sum of their dependencies. Well yeah I suppose there are two directions of the problem; the one where the library used is updated and the one where the final binary is updated. In the former case if you want to stay updated it doesn't matter much but in the latter case with static linking you only need to redownload the final binary but that is of course purely because the library is baked into the binary so it's larger and you're still re-downloading all the equivalent logic. &gt; I don't understand what laziness has to do with this. This problem stems more from inlining. Because these inlining optimizations from what I read are required to make lazy evaluation remotely performant. In eager implementations inlining is just eliminating call overhead but in lazy implementations it opens up all sorts of re-ordering and optimizations without which it wouldn't be fast.
Seems to not work for monadic functions. It's generating a `Show m` context which doesn't make sense.
What is matrix build? I tried googling and found https://docs.travis-ci.com/user/customizing-the-build/#Build-Matrix which actually sounds like a good idea to me. 
If you are into program correctness, then it could be interesting to look at how to combine QuickCheck with techniques from Jepsen or TLA+. I'm happy to elaborate, if there's interest.
Those entreprise-y place shouldn't rely on free labor from maintainers. If they need a specific thing (like old version support), they should ask the maintainer and/or fund the support one way or another (money, bounty, help, etc..) Or course there's no hardwired rules, if a package is really at the bottom of a deps tree, then it's quite likely that more versions is a good thing, but also more likely that you have more maintainers, people willing to help and less incentive to change the package much.
yes, program can use all sort of hints, but that means nothing about the end result. It's possible that a solver working on date of upload would works just as well. Only a full end-to-end compilation + running tests tells you that everything is working as expected. But ultimately I don't have to think about versioning, because a stackage LTS version is known to work together by having the good Stackage folks running a compilation+tests end-to-end and "blessing" the results so I don't have to. Sure it doesn't cover extra-deps or packages not on stackage, but surely that's just another reason to put more resources in the whole system (e.g. more packages in stackage) more than just sinking everyone's time in unproductive manual bounds checking&amp;editing&amp;curating
My mum works in a company that sends consultants to deploy SAP in firms, and oh god no let's not try to write a whole ERP. It's going to take decades to achieve a design, an architecture that's will last years. The best thing we could do is to implement parts of an ERP, perhaps as a web app, like Salesforce did (and it worked out very well for them). 
Inlining is more about the purely functional stuff than anything to do with laziness. Fusing data structures, specializing type classes, case analysis, the list goes on; these can all be applied just as well to strict languages like PureScript.
&gt; Also why do you only remove upper bounds and not lower bounds as well? Why wouldn't you use version bounds to forbid dependencies which you know don't work?
I work for Pronto Software which is a leading Australian ERP, and you wouldn't want to write an open source ERP. There are millions of lines of code involved.
Note that I use eager and lazy specifically here and not strict and non-strict. The language might be strict in its semantics which pertains to the guarantees of termination given but the evaluation strategy is not eager then. These optimizations you speak of re-order the evaluation order which is indeed permitted in a pure language but the evaluation strategy is no longer eager at that point. Eager vs lazy is an implementation detail in theory.
haskell-src-exts (used to) produce quite large JS bundles. Please report if that's not the case anymore!
&gt; Stackage only supports adding packages to nightlies That's not quite correct. It [is possible to add packages to current LTS series](https://github.com/fpco/stackage/blob/master/MAINTAINERS.md#adding-a-package-to-an-lts-snapshot).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [fpco/stackage/.../**MAINTAINERS.md#adding-a-package-to-an-lts-snapshot** (master → 0e0bcc0)](https://github.com/fpco/stackage/blob/0e0bcc070f2c648684780366118d1270d8b9a84f/MAINTAINERS.md#adding-a-package-to-an-lts-snapshot) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
&gt; you know don't work While I try to have meaningful version ranges for my packages, I usually set as the lower bound the version I am using when creating a package. It is hard work to "know" a lower version will not work!
absolutely no-one argued for that; Everyone agrees that if you know for sure that something is not working with a dependency then it should be specified in the cabal file as some kind of version bounds This is all about not knowing (aka the future) for upper bounds, or not willing to iteratively look for older versions (aka the past) for lower bounds. It's mostly the case that lower bounds bit rot unless they are periodically tested which is quite costly in term of compilation, so I have to agree with /u/haskelll here, lower bounds are mostly useless.
That's one step closer to something like an actual `ghcdb`. Cool!
Huh. I don't get any empty strings at the end - can you show some example input where it happens?
Interesting idea, maybe it could be useful to generate a changelog from a range of git commit messages by scanning them for certain keywords? I guess it should be easy to automate this.
&gt; mastering the concept of a monad is perceived to be a toughest achievement of a Haskeller — and there is some truth in that. Some. I really think it's inadvisable to talk like this. There are lots of challenges in software development and lots of challenges in software development in Haskell specifically. Let's not promote the idea that "mastering monads" is somehow the pinnacle of one's achievements.
Thanks for the idea, I have removed containers in a PR and replaced by a list backed inline Map implementation of 10 lines :)
`selda` done a great [example](https://github.com/valderman/selda/blob/master/ChangeLog.hs)!
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [valderman/selda/.../**ChangeLog.hs** (master → a67832b)](https://github.com/valderman/selda/blob/a67832be54b3e2496fa37a070c3aae27774cbf3e/ChangeLog.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
Yes, well a really popular and awesome one is https://www.npmjs.com/package/commitizen for node, which actually walks the person commiting through setting a commit message that is compatible with the changelog tooling so that *every* change is tracked.
Wow, that's fantastic! =)
Making GHC target WebAssembly. Haven't had the free time to update readmes / post progress anywhere since the last reddit thread about it, so I'll talk a bit about the current state here. LLVM is on hold. We have to get [tail calls](https://github.com/WebAssembly/tail-call) accepted into WebAssembly before we can use the LLVM backend. It seems like the process for this will be to open a PR on that repo containing an implementation of the opcodes in the spec interpreter and a description in the overview file. If that's accepted, hopefully it will be merged into the [spec repo](https://github.com/WebAssembly/spec), and then will find its way into browsers. In the meantime, we're marching forward with the "unregistered via-C" backend. This backend exists primarily to port GHC to new platforms using little more than a C compiler. I've benchmarked this backend targeting x86_64-linux with Reflex's benchmark, and it's about 3x slower than the native code generator. Not bad, all things considered. I'd kill for GHCJS to be that fast. Anyway, this is proving to be the path of least resistance in the meantime, but it's struck with a nasty issue with the C standard. For reasons loosely touched on in [#8965](https://ghc.haskell.org/trac/ghc/ticket/8965) and [#11395](https://ghc.haskell.org/trac/ghc/ticket/11395), GHC's C FFI declares extern C functions with `void symbolName()`, with no type information. As far as [I can tell](https://bugs.llvm.org/show_bug.cgi?id=35385#c17), the way this is used is in violation of the C standard, especially with respect to variadic functions. Fixing GHC to emit proper extern declarations is a bit hairy, but I *believe* this may actually be the last thing standing in the way of GHC generating valid WebAssembly code, RTS and all. *(Sidenote: GHC thinks it's succeeding already, but this issue is actually manifesting as invalid wasm object files. This is because the LLVM backend for WebAssembly is kind of unstable and often breaks in silent ways. Debug builds of Clang / LLVM help, since `assert` is not skipped, but in this case it's just completely silent. Silent issues like this have plagued this project)* Next steps will be trying to run such code, and fixing the [syscalls](https://github.com/WebGHC/musl) one by one until it works. Hopefully there won't be many of these that need implementing (someone should probably `strace` a simple Haskell program to get an idea of which syscalls we're going to need). Then we can hopefully build a jsaddle backend, and jsaddle apps (such as reflex-dom apps) will just work as is! So realistically, I'd say we're actually kind of close! Insofar as it is easy to predict.
I'm using it for a threepenny-gui app. ghcid calls not main, but an alternative function that immediately renders the part of the app I'm currently working on (so I don't have to click anything to get back where I was). And it also starts a reload in a browser with 'xdotool' at the beginnings. This gives me a pretty convenient developer workflow.
[Website for learning Chinese.](https://clozecards.com/) There's a tremendous amount of interesting Chinese material on the internet (podcasts, stories, grammar examples, flash cards) but it's difficult to tie it all together. Haskell works great as a glue for this purpose.
&gt; In the meantime, we're marching forward with the "unregistered via-C" backend. Doesn't that use `setjmp` and `longjmp` to do tail calls? As far as I know they are unsupported by WebAssembly.
That sounds very intriguing, got any example that I could have a look? Especially the function that ghcid calls.
Not everyone uses stack - I don’t at home. The lower bounds are useful for auto upgrading. The upper bounds just mean the package needs upgrading, so I do that instead. 
I thought it was trampolining, based on some of the C I was seeing generated. I may have misunderstood what I was reading though...
Yep, that’s my bad. I can fix that easily enough - needs to add a kind check. 
Thanks for that, it was an enjoyable read and a good summary! I've also been trying to create some slides to present Haskell and functional programming practices to the company I work at, so I was wondering if I could borrow some of your examples (I'll cite sources at the end of the presentation)?
I think tracking changes at the commit level, or tracking all changes, are both not the ideal thing for a user. I maintain mine by hand - I’ve never considered it much effort and often valuable to write the changelog entry first. 
Changelog plugin for vim, and 30 seconds after each merged feature. Take the time to do it, you're not a machine
And am I supposed to believe this is a [coincidence](https://www.reddit.com/r/haskell/comments/7bdyib/debugdo_does_this_already_exist_andor_should_this/)? :-P Joking aside, I also extended my debugging code to functions so you could use `[d| ...|]` and it would print out all the function calls. One thing I didn't realize until reading your example was that you can include the type-signatures in there too! I actually wrote some Elisp to let you select a region and then hit `M-x haskell-trace` to wrap it up in the debug wrapper. If the debug package is using that namespace and will remain stable, I could possibly add support into Intero. I think you could select a declaration and run e.g. `M-x intero-debug-region` which would automatically add `{-# OPTIONS -package debug #-} {-# LANGUAGE TemplateHaskell #-} import qualified Debug` to the file and wrap the region in a debug wrapper. Once done you could hit another key to strip them out. Additionally, it would be nice to get the trace file somewhere containing e.g. line/col and the trace info. Then you could view the trace from within Emacs without having to launch a browser.
This time is: [Duet epsilon](http://chrisdone.com/toys/duet-epsilon/). (Last time was [Duet delta](http://chrisdone.com/toys/duet-delta).)
Paging /r/terrorjack
Right. My point is that it is bad practice to do so. 
Thanks for doing this. The giant dependency footprint of `criterion` has always been a minor gripe of mine, particular because I almost never use html/csv/json export features of criterion, so it's nice to have a lightweight alternative. I'll probably use this for future projects.
I'm working on a [code formatter] for ATS. It's quite difficult, but it's good for learning (both ATS and happy/alex).
https://matrix.hackage.haskell.org/
[(en) slides](https://clementd-files.cellar-c2.services.clever-cloud.com/scala-io-haskell.html)
It totally depends on what you are writing. Suppose your `main` function initializes your app and then shows a menu to a user. So you can hack together some other function that initializes the app, but also simulates user choice of some menu item. When ghcid reloads everything, you'll be able to see what currently interests you - and start interacting with app only from that point. But it's up to you how granular you want to make this approach. 
First of all, thank you for an open feedback, and I am grateful for that. About your statement "There are lots of challenges...", definitely so, and I couldn't agree more! Software engineering is a vast subject, ranging not just the concepts of the programming language in use, but encompassing various architectural interfaces, their effects, architectural degradation, etc. So, obviously, "monads" but form a small part of the whole subject. Within the topic of Haskell, I personally found understanding monads a difficult subject, if not the pinnacle, as you rightly mentioned. I believe there could be many more like me, but, yes - that's a guess by a long shot. Since there is a lot more in that article, and the statement you've commented upon was right in the first paragraph, I chose to remove that from the article, lest somebody gets put off by that statement, and not read the rest :-) Thanks again!
Is this library only useful when you know all you currency combinations at compile time (pairs, triples, whatever...)? Or can I use it to write a personal accounting software (where every account and every transaction can have different currency, but we should be able to move money in any possible way between them)?
It's clearly a fine strategy with the topic at hand (reducing the cost on maintainer). I think the no preemptive lower bounds is just simpler, since it doesn't force the user for an early upgrade that is maybe not necessary, and down the line your lower bounds is likely to not be valid anymore anyway (provided you don't test with every versions at each code revisions, which we agree is really costly)
I write haskell for a living. I would never want to write an ERP, regardless of what language I was using. The problem domain itself is just inherently complicated. Think about what a written description of an ERP would look like. It's not three pages of requirements, it's hundreds of them. And if you can hardly describe how the system should behave, how could you hope to model it well? I try to work on smaller problems because it's much easier to understand the system you're trying to model.
Yes, definitely!
You need to know currencies ahead of time, but you can build exchange rates on demand if you have a way of looking them up, at least as far as I can tell. 
Question: Does the new WebAssembly backend mean that the garbage collector works the same way on native and browser? GHCJS implements garbage collection a bit differently than GHC, and this causes my `reactive-banana` library to behave differently than on native GHC — it actually breaks completely.
[this](https://www.xcontest.org/alpha/live/) - the backend is in haskell. I'd like the frontend to be in some FP as well, but gave up on that. 
- As you may already be aware, version bounds are somewhat of a touchy subject around here. You have a few options: no bounds, only lower bounds, or all bounds. I'm not going to make a suggestion here. Instead I'll just say: Don't feel obligated to support more versions of dependencies than you want to. If someone wants to build your package with GHC 7.4, let them open a bug report. You can probably save yourself a lot of headaches by focusing on recent versions of dependencies. - If you're already building your package on Travis CI, it should be pretty straightforward to set up automatic Hackage uploads on Git tags. For example, I have [a script](https://github.com/tfausak/rattletrap/blob/bfa7834b71d1b8d02e066aadd3e70f33010320ad/tools/upload-package.hs) that does that. - Adding packages to Stackage is easy and you only have to do it once per package. For instance, here's [the first package](https://github.com/fpco/stackage/pull/533/files) I added to Stackage. 
* inline-c bindings to an MP3 decoding library: https://github.com/ocramz/mpg123-hs * An extensible library for interoperating with cloud API providers (for now google cloud storage serves as a worked example) https://github.com/ocramz/goggles * `accelerate` backend for `sparse-linear-algebra` : https://github.com/ocramz/sparse-linear-algebra/tree/master/accelerate
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ocramz/sparse-linear-algebra/.../**accelerate** (master → fb8a5db)](https://github.com/ocramz/sparse-linear-algebra/tree/fb8a5dbef3f7bf24327d8673af028576e9ea8914/accelerate) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
I'm not sure what you mean by this. Are you looking for an automated way to produce change logs or consume them? For producing them, I am of the opinion that anything automated is probably not the way to go. As a user, I don't want to read a list of commit messages as a change log. Even a list of issues and pull requests isn't much better. I want a high-level overview of what has changed from my perspective. I don't know of any way to do that automatically. For consuming them, you could either use a structured format like JSON or XML, or stick to a common format like [Keep a CHANGELOG](http://keepachangelog.com/en/0.3.0/). For what it's worth, I manage my change logs on GitHub. It's worked well for me. Here's an example: https://github.com/tfausak/rattletrap/releases
Brilliant. (json1, json2) &lt;- concurrently (httpGet url1) (httpGet url2) useJsonBodies json1 json2 Would love to see this as the first example on the front page of https://www.haskell.org/ . This sort of thing says more about Haskell than calculating primes IMO. 
&gt; We still use 8.0 at my job Well, there is no LTS with 8.2 yet so that is entirely reasonable and I wouldn't count one version behind the latest version as old. Usually it doesn't require huge amounts of effort to support it either.
&gt; You could write me your full name in a private message. No need for secrecy: my name is Samuel Gélineau. &gt; You wrote that there have been many attempts in the past. I don't know of any very good ones (but looked only the last 7 years back). I was thinking of the attempts listed in the paper I quoted, such as Fudgets. &gt; My/our spin is to use the current state-of-the-art methods that exists today (but I think the functional world is mostly not aware of) Now you're making me even more curious! Any links, or terms I can google to learn more? &gt; One part is improving on FRP. I would also be very interested to hear about that! Which aspect of FRP are you improving on? FRP is still a very active area of research, there are many variants which improve upon the basics in various directions. &gt; I guess people are not interested in the results anyways (people seldom are interested in good stuff; ie., most of the programming world rejects functional programming.) Most of the programming world, perhaps, but not /r/haskell! Please do tell us more when you feel ready to reveal more.
&gt; &gt; Denotational semantics exist even for non-total languages &gt; When did I say it doesn't? You implied something along the lines by saying "Did they make Haskell total when I wasn't looking?" in response to "But there is a consistent denotational semantics here." &gt; And with this denotational semantics, Haskell doesn't have a list type You clearly understand denotational semantics better than I do. Please explain your claim. My understanding is that the problem with set-based denotational semantics comes from higher-order functions; otherwise it captures laziness just fine. Why would the requirement that these functions be continuous present a problem? Why is the collection represented by the Haskell-ish pseudocode"`denotList denotValue = [_|_, []] ++ map (:denotList denotValue) denotValue`" not a collection of values denoted by Haskell's list type?
&gt; As suggested, I removed that statement from the first paragraph. Thanks again! Thanks! I think that's a wise move.
I'm on the final phase (hopefully) of [nginx-haskell-module](https://github.com/lyokha/nginx-haskell-module). Currently this features synchronous and asynchronous tasks (executed by strongly typed, pure and impure Haskell handlers), and services, i.e. asynchronous tasks that run during all the Nginx worker process' lifetime. Recent features include *shared* services, i.e. services that run exclusively on an arbitrary Nginx worker and use Nginx shared memory to store their result, and controllable service termination that allow making persistency actions on a worker's exit.
I'd love to have a solid idea for a new Haskell project... perhaps something locally hosted. Doesn't even need to be inventive or original, just something to keep me engaged and thinking functionally.
[Website for learning Japanese](https://github.com/dfordivam/hhsrs). Trying to combine flash cards (SRS) with a nice reading app.
Thanks!
I'd see it as a "which issue is more dominant" test. Is a lack of strictness the bottleneck? Or is too much strictness the overall bottleneck? Switching on Strict is a much faster way to check than banging everything. I certainly wouldn't leave it on, myself; I would go through and start looking for the strictness problem spots. But it's a quick convenient heuristic to see whether or not further investigation is worth doing.
I think you mean /u/terrorjack the /r/ links to subreddits, not users :)
Your example is very non obvious in it's usage of lists as a control structure (it doesn't even mention lists in the post). Also, out of curiosity, it seems like you're saying that lists should only be used as well behaved builders in Haskell; why, though? Surely they're useful for more than that?
I'm doing a lot of internal work on my library [evenful](https://github.com/jdreaver/eventful). It turns out the library is useful already and there are even a few users, but I think I need to use the lessons I've learned building and using that library to make a better and more flexible core API. It also turns out the docs for event sourcing are arguably more useful than the library code since the concepts are fairly simple once you've ironed them all out, so I'm going for a more documentation-first approach this time around. I'm also working on a library for integrating [AWS X-Ray](https://aws.amazon.com/xray/) into WAI, yesod, and persistent applications. So for it is working great for our app at work. I've written the code in its own package of our monolithic repository at work, so once we have ironed out the kinks we can just move that directory to a new repo and put this up on Hackage.
Can you explain the motivation? I don't understand why this would be useful. Thanks
The last sentence wasn't a typo, it was just worded a bit odd. I think they were saying "Haskell can achieve code that is so clear and also performant that if any other language tried to match it, they fall short either in clarity of code or in speed." It's not really about avoiding lazy data structures as it is using the right one for the job. Being a functional language, Haskell follows tradition and makes it *really* easy to work with lists. Hence they often tend to get used for everything, even when it doesn't make sense. However, the containers library has some great data structures; there are some great stm structures as well, and of course the famous vector library. As far as data structures to look at (for Haskell), I'd suggest: * Maps * Vectors * Queues (semaphores are a very common usage for these, especially for concurrency) * STM/Concurrent variations of the above I didn't mention trees because, as it turns out, a lot of these data structures tend to be implemented with trees or other exotic data structures. The above will get you pretty far and improving on choices from there will really boil down to a stronger understanding of the complexity of certain tasks and what time complexity you need for certain things to remove bottlenecks in your code.
Would a reverse proxy work?
Don't worry, this sort of thing is definitely upper 1% knowledge. Even knowing of it's existence isn't necessary to use the library, much less understanding all of the fancy stuff that goes on underneath. You can definitely master and use Haskell for real world stuff without knowing or caring about any of this :) --- The problem I have with JavaScript and C is that they pretend that complicated things are easy and tend to hide the details unnecessarily. For JavaScript, using it is super easy, but explaining it's evaluation model, how equality works in it, how it's object system works? Way too complicated. With C, there's the basic level, then there's function pointers, then there's undefined behavior, then there's the really scary stuff where you start having to care about differences in CPUs, memory order, and other super technical nuanced things. But nobody ever talks about that stuff, nor is it really visible. You just have to sort of remain blissfully unaware until you run into those issues. With Haskell, those tricky technical bits tend to be fully visible and open, which makes it seem scarier... But you still don't really need to know them, just like most people get by in JavaScript and C without knowing all the nasty dark corners of the languages.
/u/ndmitchell, I don't have an account to comment on your blog, but regarding your browser open question: On Linux, if xdg-open is used, it required the related xdg package to be installed (this isn't a problem. It's either already installed on a distro, or the distro is user unfriendly enough that the user will know to install it). Xdg-open, when given a file, will open in the user's default file browser; however, when given a file url, it will open in the user's default web browser. In short, you do have to make sure the file has the `file:` URI on Linux
I'm still working on a reincarnation of my hobby programming language. The compiler is now done insofar as there are no open questions regarding how I could do something I need to do. The runtime is now my big construction site - Figuring out a good instruction set architecture and memory management because I want a stack of mostly pure Haskell values, but I also want (multiple levels of) indirections for pointing around wildly. Got a solution working on paper involving a stack of pure values that can also point at a heap of reference-counted values. I've learned so much with this. Ultimately, I hope to get back to generating levels somehow... now that OBLIGE for Doom is unfortunately being discontinued. :(
I don't do web development at all anymore, but I think this is one of the most interesting projects there are atm!
I'm new to using Haskell for hobby projects. Currently [porting the Elixir backend to Haskell](https://github.com/srid/slownews/pull/13) and learning the Haskell way of doing things along the way. This has been great fun.
[The Friendly Orange Glow](https://www.amazon.com/Friendly-Orange-Glow-Untold-Cyberculture/dp/1101871555) (history of the PLATO computer-aided instruction system), [The Sculptor](https://www.amazon.com/Sculptor-Scott-McCloud/dp/1596435739), and [Soonish](https://www.amazon.com/Soonish-Emerging-Technologies-Improve-Everything/dp/0399563822).
Relatedly I compiled a set of examples from the book in a more readable way https://github.com/chrisdone/ats-examples
You don't need to know the currencies ahead of time if you use `SomeDense`, `SomeDiscrete` and `SomeExchangeRate`. As mentioned in the article, these are a bit trickier to use due to `RankNTypes` being necessary when learning about the currency identifier and scale at runtime. But, it's perfectly possible to do so. What will happen is that you will deserialize the data stored, say, in your database or network as a `SomeXxx`, and then you will use the respective the `withSomeXxx` on it before manipulating that value somehow. That way, the currency identifier and/or scale present in the `SomeXxx` value will be promoted to the type level within a limited scope. 
You'll need to provide the flags you gave GHC as well. Hopefully, you're building with `-O2`. Your definition of `Node` can be improved a little: data Node v d = Node { val :: v, info :: d, prior :: !Int } Making `prior` strict in the presence of `-O2` will also cause it to be unpacked, which will help a little.
a desktop application for searching Magic (the Gathering) cards, like magiccards.info https://github.com/sboosali/magic-card-search 
&gt; free labor Exactly. Conversely, if they do give back, say by open sourcing some subset that depends on a given package, they can add it to a stackage lts and publicly declare their dependency; which makes the maintainer more likely to support an older compiler versions for the dependent package. 
I'm working on another test tool for a Mission Control System and for this I am currently creating bindings to MCS which uses C++ omniORB CORBA implementation. I use [hoppy](https://hackage.haskell.org/packages/search?terms=hoppy) for the bindings to C++ (and therefore the CORBA implementation) and have the first basic GUI (in [gtk2hs](https://hackage.haskell.org/package/gtk)) already running. CORBA callbacks are also already working, I have received the first live telemetry in the tool. CORBA Exceptions still make some problems and it's very rough, but a lot of challenges are already mastered. Still missing a Haskell CORBA implementation though...
Can you link to a project of yours that does that, maybe include the xdotool script too?
I was not aware of this plugin. Please can you share a link?
I was getting so confused by this. It came out of nowhere. Also, the cabal file looks poorly formatted now. I liked the old one better.
Yea we'll be using the entire RTS from native GHC, including the GC. I'm surprised GHCJS's GC breaks your library completely. I thought it behaved pretty much identically to native GHC.
My team uses Nix, not Stackage, which has supported 8.2 for quite a while
Yes, I participated in that discussion, what I said is still true. For any functor instance that is lawful when you forbid `_|_`, the following is ALSO lawful modulo `seq`: (fmap g) . (fmap h) = fmap (g . h) So as I said the issue is entirely typeclass-less `seq`. I strongly disagree with the rest of what you said, and here is something of a "proof" as to why optimizations that are unsound in the presence of `_|_` are still unsound when totally is ensured: class NotBottom a where notBottom :: a instance NotBottom SomeType where notBottom = extremelyExpensiveComputation The above typeclass can be defined for every inhabited type, so every optimization before that was unsound because it caused unnecessary evaluation of `_|_`, can also be made unsound by replacing that `_|_` with `notBottom`. Unless you can give evidence to the contrary I am very convinced that totality *alone* is almost useless for optimization. Now this is not to say that totality itself is useless, currently mistakes made in Haskell code can either lead to a compile time error, a runtime error, a runtime performance regression, or runtime nontermination. Totality would move anything in the last category to the first, which is wonderful, particularly since the last category is not a small one by any means.
It's not *quite* ready to use that as test data, but I think it could be quite useful once I start doing more work :)
Some typos: `Node` is missing a `Show` instance and `t'` should be indented more in `insertTreap`. The program is too lazy: `splitTreap` and `mergeTreap` only do a bit of work (looking only at the root) and then return a `Tree` with thunked fields. At runtime, when we compute `heightTreap` on some thunked tree which is the result of a sequence of `insertTreap`, roughly the following happens: - pattern matching in `heightTreap` needs to look at the thunked root; - so we evaluate `insertTreap` (which only produces the root); - if it returns `Tree`, the process repeats recursively with `heightTreap lt` and `heightTreap rt` because `lt` and `rt` are thunks. In short, there is a lot of back and forth between `heightTreap` and `insertTreap` (which in turn needs to make calls to previous `insertTreap` in a similar manner). Thus, the biggest win comes from making the tree fields strict. data Node v d = Node { val :: v, info :: d, prior :: !Int } data Treap v d = Leaf | Tree {node :: !(Node v d), left :: !(Treap v d), right :: !(Treap v d)} Whenever you evaluate a treap, the whole structure will be forced. For example [`Map` and others from containers](https://hackage.haskell.org/package/containers-0.5.10.2/docs/Data-Map-Internal.html#t:Map) are also data types with strict fields. `splitTreap` returns a pair, which is also lazy. Rather than defining a strict pair type, you can also just use pattern matching (`case`, not `let`), `seq`, or bang patterns. let (lt, rt) = ... -- we actually don't need a bang here because `t` is already strict in `lt` !t = Tree { node = node tree, left = l, right = lt } in -- Here's a funny trick: !t = tree { right = lt } (t, rt) -- same in the other branch Also note that we always need to access the `node` field to do anything. Using the `{-# UNPACK #-}` pragma avoids a level of indirection (although there is a trade-off because the `Tree` constructor becomes bigger). Also unpacking `prior` helps a lot (unpacking fields of types with a small representation often helps); polymorphic fields cannot be unpacked. data Node v d = Node { val :: v, info :: d, prior :: {-# UNPACK #-} !Int } data Treap v d = Leaf | Tree {node :: {-# UNPACK #-} !(Node v d), left :: !(Treap v d), right :: !(Treap v d)} Some more algorithmic improvements are also possible, by doing `insertTreap` in one go instead of one `splitTreap` (which allocates a lot of intermediate pairs) and two `mergeTreap`.
I am teaching GHC to always perform [pointer tagging](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/HaskellExecution/PointerTagging), even for more than 7 constructors in a datatype. Reviewers so far like the idea, I am looking for help in benchmarking. At work I am in the process of introducing [Clash](http://clash-lang.org) and [Rhine](https://github.com/turion/rhine) to open-minded colleagues. Stream processing is cool.
Doesn't really matter. Any system will have to support two versions at least for a transitional phase anyway. I think it is reasonable to stop supporting 8.0 once 8.4 comes out though (i.e. once compatibility issues between 8.0 and 8.4 become an issue).
Great! I'm surprised as well. :-) But apparently, the semantics of [weak pointers in GHC are tricky][1], and I am using that trickiness for the implementation. [1]: https://www.microsoft.com/en-us/research/wp-content/uploads/1999/09/stretching.pdf
Huh. Reflex uses those quite a bit. Weird
This is great! I did almost all the changes you suggested (stricting fields, and UNPACK) and it brought down the time to 0.91s and memory to ~700 MB. I really need to understand laziness better. By the way, isn't the memory requirement too high, like the array size is 4e5 ~ 0.4 MB, while the entire memory requirement is 700 MB. Why is this happening?
Thanks ! Stricting really helped the speed. :)
I've got a symbolic differentiator on the backburner that I keep meaning to abstract, but I've mostly just been spending my hobby-haskell time trying to figure out this weird thing that I know works with functions and trying to generalize it to arrows, which should be simple enough. Now to figure out that weird dependently-typed-cofree thing...
The weak pointers described in the paper come with more functionality that one would naively expect from a weak pointer. It looks like reflex requires only the "naive" part, while my implementation style requires the full functionality.
Make the data constructor strict in the spine and use `foldl'` instead of `foldl` for a dramatic increase in performance. I'm writing a blog post up on it now
Before someone gives you an answer, have you tried determining how much space your treap should use? Pointers and all? It's a good exercise.
Working on the 1.0.0.0 release of my concurrency testing library [dejafu](https://github.com/barrucadu/dejafu/pull/157). It's been a little slow going because I'm also writing my thesis and procrastinating a lot (hey, writing docs isn't the most fun task!) but it's almost there now. This release has some big internal changes which bring support for bound threads (`forkOS`) and significantly reduced resident memory usage for most tests. It's not exactly new functionality, but I've also tidied up the main library interface a fair bit and removed a lot of almost-duplication.
Working on learning the language! I'm not 100% new to the FP paradigm but I am new in completely functional languages. Over the last few weekends I've dabbled with this and Clojure (which I understand is more lisp flavoured). Out of the both, my brain has clicked easily with Clojure and I'm having a more difficult time with Haskell (which has been for the most part a task in frustration), but I know over time it will get better as I learn more and more. 
Well, since I do not copy any trees, I think there will be exactly one copy of the data. That means, assuming `v,d` are 4 bytes each, around (4+4+4) for Node and 4+4 for pointers =&gt; 20 times the length of the list. But the actual memory is quite high, even when the fields are made strict. 
Wow! You wrote a blog post about this, I can't thank you enough (:'( I don't have reddit gold). I'll surely read every last line of that blog. Thanks!
[removed]
Est-ce que quelqu'un a une liste de boîtes françaises qui font du Haskell ou de la programmation fonctionelle ? :-)
I've actually been meaning to write something like this for a while -- thank *you* for the test case! :D
That 700MB seems to be the number of allocated bytes throughout execution, whereas the memory requirement would be closer to the maximum residency (you can find that statistic by running with `+RTS -s`). I see about 660MB allocated/6.4MB residency. I haven't evaluated the total allocated bytes, that still seems like a lot, and I'm not sure how much of it can be cut down by the algorithmic improvements I mentioned. However, I can explain the residency. Note that it's 4e5 `Int`, which is boxed, so that's two words. A `Treap` has one `Node` for every value, that's one word to represent the constructor, plus the contents of the `Node`. Since the `node` field is unpacked, `Node` is represented as a 5-field constructor, each field takes one word (pointer or unpacked `Int`). So that's 6 words more. (nullary constructors such as `()` and `Leaf` don't take space, there's just one word for each constructor that gets reused). 1e5 values, 8 words per value, on a 64-bit machine (8B words) that's precisely 6.4MB.
Je connais [FretLink](https://www.fretlink.com) (mentionné dans ce talk), [LexiFi](https://www.lexifi.com), [OCamlPro](https://www.ocamlpro.com) et [BeSport](https://www.besport.com). Il me semble aussi avoir vu passer des mails sur Mitsubishi R&amp;D (en Bretagne IIRC) mais je ne sais plus si c'est de la prog fonctionnelle ou beaucoup de méthodes formelles. J'ai aussi un vague souvenir d'avoir entendu parler d'une boîte sur Paris qui fait beaucoup de Coq et qui absorbe des anciens de PPS quand ils quittent la recherche après. Faudrait demander à /u/perthmad par exemple s'il voit de quoi je parle.
&gt; Your example is very non obvious in it's usage of lists as a control structure (it doesn't even mention lists in the post). That's a fair point. The blog post wasn't made for the comment unfortunately :) Basically, we use a (special) hylomorphism to both build up and tear down a list at the same time. This ends up being just a for loop! You can try out a more straightforward example in GHCi with sum [1..100] It's a bit trivial, but this is equivalent (in Haskell) to what would be a `for` loop in an imperative language/style, thanks to optimizations the compiler can do. &gt;it seems like you're saying that lists should only be used as well behaved builders in Haskell; why, though? Surely they're useful for more than that? They are indeed. You can use them to fill an array efficiently. Lists are immensely versatile structures, but when you want performance they end up being bad for large collections of objects. 
https://www.meetup.com/fr-FR/haskell-paris/ ça peut-être
Learning Haskell mostly. A couple of projects I'd like to take more seriously as I get comfortable/proficient in the language: 1. Work on improving/fleshing-out/supporting one of the main GraphQL libraries 2. A safe-by-design MTA; recent experiences with managing my own messaging infrastructure have made me want an MTA that has the least amount of configuration possible and cannot be configured to be exploitable by known attack vectors.
I've been checking out eventful for some potential projects/prototyping at work. :)
This looks very promising, thanks for sharing! I tried to try it with nix-shell -p 'haskell.packages.ghc822.ghcWithPackages (p: with p; [(callHackage "debug" "0.0.1" {})])' --run 'ghci' And I got tar: */debug/0.0.1/debug.cabal: Not found in archive tar: */debug/0.0.1/debug.json: Not found in archive builder for ‘/nix/store/csgyr48lcn8b10jih0d45l44nm7s4klw-all-cabal-hashes-component-debug-0.0.1.drv’ failed with exit code 2 Would a nix aficionado know why?
Ha, j'aimerais bien y passer mais je suis aux États-Unis pour l'instant ! :)
(I posted this as a comment to the article. I thought I'd share it here as well) You missed an important example, in my opinion: the "cached-io" package (this is just one of them, there are several implementations). This enables caching any external data fetches (i.e. IO actions) in Haskell. So you give it 1) a function that fetches a resource (which is rather expensive to call), and 2) a cache timeout, and it gives you a new IO action that only does the actual fetch every (at most) &lt;timeout&gt; seconds, and the rest of the time just returns the value from the cache. This works for all data fetches, regardless of whether it's reading a file from disk, fetching data from an URL, reading system time, etc.. As far as I know, this isn't possible to implement in any other popular language, because they don't offer a generic way to describe this data fetch (as an IO action does in Haskell). In most other languages, a data-fetching function would just look like a regular function that has the side effect of fetching external data, so there would be no way to pass this as an argument to a function that caches the result of the data fetch, and only performs the actual fetch every &lt;n&gt; seconds.
Keep plodding along at [hnes](https://github.com/dbousamra/hnes/). Gave a talk on it last week
I'd really like to see more work in correctness testing/proving of implementations of distributed protocols/algorithms/processes. E.g. define a DSL (through Free or whatever) which can then be used to implement an actual service (i.e. messaging, timeouts,... backed by an IO implementation), or from which a PlusCal/TLA+/Spin/... model can be extracted or some kind of Coq/HOL/... model, which can then be model-checked or upon which proofs can be made. I always disliked the step from e.g. a Spin or PlusCal model which can be tested but not 'executed' to some implementation, which leaves room for interpretation or implementation errors. If need be, would be willing to cooperate/help out/...
On fait de l'Elixir chez [Kbrw](https://www.kbrw.fr/). A part ça, j'ai aucune idée du reste du paysage. \^^
I'd like to make a Haskell webserver that manages credentials (among other things) and exposes a git repository that can only be used if the user authenticates correctly.
Well, I threw together a quick bit of code for the problem. Here is the basic version: module Main where import Data.Ord import Data.Bits import Data.List import ParSeq newtype Filled = Filled { unFilled :: Integer } deriving (Show, Eq) newtype Corner = Corner { unCorner :: Int } deriving (Show, Eq) adjacents :: Int -&gt; Corner -&gt; [Corner] adjacents d (Corner i) = [Corner j|e&lt;-[0..d-1], let j=complementBit i e] adjacentsFree :: Int -&gt; Filled -&gt; Corner -&gt; [Corner] adjacentsFree d (Filled s) (Corner i) = [Corner j|e&lt;-[0..d-1], let j=complementBit i e, not (testBit s j)] fill :: Filled -&gt; [Corner] -&gt; Filled fill (Filled s) = Filled . foldl' setBit s . map unCorner maxPath :: Int -&gt; [Corner] maxPath d = go (Filled 1) (Corner 0) where go f c | null cs' = [c] | otherwise = (c:) $ maximumBy (comparing length) $ map (go f') cs' where cs' = adjacentsFree d f c f' = fill f cs' maxLength :: Int -&gt; Int maxLength d = go (fill (Filled 1) $ adjacents d (Corner 0)) 1 (Corner 1) where go !f !a !c = case cs' of [] -&gt; a' [c'] -&gt; go f' a' c' _ -&gt; maximum $ map (go f' a') cs' where cs' = adjacentsFree d f c f' = fill f cs' a' = a+1 main :: IO () main = print $ maxLength 6 This runs in about 13.1 seconds on my machine. 27 43,960,906,464 bytes allocated in the heap 30,118,712 bytes copied during GC 60,088 bytes maximum residency (3 sample(s)) 26,776 bytes maximum slop 3 MB total memory in use (0 MB lost due to fragmentation) Tot time (elapsed) Avg pause Max pause Gen 0 42271 colls, 0 par 0.062s 0.120s 0.0000s 0.0002s Gen 1 3 colls, 0 par 0.000s 0.000s 0.0000s 0.0000s INIT time 0.000s ( 0.000s elapsed) MUT time 13.031s ( 12.992s elapsed) GC time 0.062s ( 0.120s elapsed) EXIT time 0.000s ( 0.000s elapsed) Total time 13.094s ( 13.112s elapsed) %GC time 0.5% (0.9% elapsed) Alloc rate 3,373,498,817 bytes per MUT second Productivity 99.5% of total user, 99.1% of total elapsed Then I tried parallelising it using the "parseq" "primitive" discussed [here](https://www.reddit.com/r/haskell/comments/73umrw/another_parallelism_primitive_parseq). maxLength2 :: Int -&gt; Int maxLength2 d = go (fill (Filled 1) $ adjacents d (Corner 0)) 1 (Corner 1) where go !f !a !c = loop f' a' cs' where cs' = adjacentsFree d f c f' = fill f cs' a' = a+1 loop !f !a (c:cs) = b' `parseq` max a' b' where a' = go f a c b' = loop f a cs loop _ a _ = a The stats on GCd and fizzled sparcs on this look quite discouraging. 27 31,988,757,216 bytes allocated in the heap 53,861,600 bytes copied during GC 1,475,912 bytes maximum residency (28 sample(s)) 219,520 bytes maximum slop 21 MB total memory in use (0 MB lost due to fragmentation) Tot time (elapsed) Avg pause Max pause Gen 0 2053 colls, 2053 par 3.297s 0.191s 0.0001s 0.0004s Gen 1 28 colls, 27 par 0.000s 0.002s 0.0001s 0.0002s Parallel GC work balance: 66.81% (serial 0%, perfect 100%) TASKS: 18 (1 bound, 17 peak workers (17 total), using -N16) SPARKS: 81532328 (41778 converted, 0 overflowed, 0 dud, 76378185 GC'd, 5112365 fizzled) INIT time 0.000s ( 0.001s elapsed) MUT time 26.188s ( 1.701s elapsed) GC time 3.297s ( 0.194s elapsed) EXIT time 0.000s ( 0.000s elapsed) Total time 29.484s ( 1.896s elapsed) Alloc rate 1,221,527,721 bytes per MUT second Productivity 88.8% of total user, 89.8% of total elapsed gc_alloc_block_sync: 109759 whitehole_spin: 0 gen[0].sync: 336 gen[1].sync: 2803 But the performance is quite satisfactory: A 6.9x wall-clock speed up on a 8 core CPU.
But I think I figured something out by copy-pasting some abandonware that almost does what I want, and modifying the parts that don't. #haskell
Thank you very much for your generous contributions, feedback and help in bugfixing. As a fruit of your labour, let me present to you v0.7.1.0 of both Allure of the Stars and LambdaHack!!! with the following changes: - add amazing cave and item (actor, blast, organ) descriptions - package for Windows as an installer and also as zip archives - fix a crash from SDL frontend under some OpenGL drivers (no thread-safety) - add WWW address to the Main Menu, for other sites that may run our JS blob Keep it coming!
Grammatical nitpick: Throughout the article, you use "intension" when you mean "intention".
- [kale](https://github.com/parsonsmatt/kale), a framework for writing extremely convenient CLI tasks. Beginners welcome! It should be getting pretty close to ready for a release. - [persistent-typed-db](https://github.com/parsonsmatt/persistent-typed-db), a library that provides a phantom type around `persistent` models and connections to statically rule out querying the wrong database. Soon Hackage `persistent` and `esqueleto` will support it :D - Hacking on PureScript a little bit. It's a very nice project to jump into.
This is such a common request that I've *really* wanted to make a Haskell Project project -- a listing of project ideas, potential mentors, and relevant GitHub/etc links. This itself is a good project to get started with -- it's a fairly basic CRUD app and would benefit the community. I'd be happy to mentor it as well :)
For automatic (less error prone) production of changelogs. Thanks for suggestions
The example in the post would be implemented with [Promise.all](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise/all). IO in JavaScript is inherently concurrent. You have to go out of your way to block a scheduler while waiting on IO.
I am writing more complete decoders for GitHub webhook payloads so we can write GitHub bots and services in Haskell .
Really looking forward to this.
I would like better GraphQL support too!
I'd use gtk2hs and Reactive Banana. You can tie a GTK Signal to a reactive banana Event using a few lines of code, which gives you inputs from button presses. Then you can tie a reactive banana Behavior to a GTK Attribute for output. The calculator functionality maps button events to an internal state Behavior, and one aspect of this Behavior is then displayed in a text box at the top of the app.
Seeing all the recent performance discussion, especially with people trying to write Haskell and encountering performance issues, I thought a "checklist" would be neat. I came up with the headings and threw in some links to get it started. I'm all done for keyboard quota for the day, but would love to add more pros to the sections and inline some of the advice links, all up to date with recent knowledge. Will merge any PRs to flesh this out! :-)
Your nixpkgs must have a recent-enough version of all-cabal-hashes in order to know about this package.
Yes, I kept digging and found out what you wrote as well. I've come this far: `nix-shell -p 'haskell.packages.ghc822.ghcWithPackages (p: with p; [(callCabal2nix "debug" (fetchFromGitHub {owner="ndmitchell";repo="debug";rev="master";sha256="foo"; }))])' --run 'ghci'` But it seems for some reason it doesn't even try to fetch the github repo, let alone complain about sha256 being foo. Just drops me into ghci without the package. Any ideas about what may have gone wrong?
I think that Atskell would become the best language ever
I mean sure. But since the pure code is being called from an `IO` block. They can be treated as though they are morally only occurring in the relevant `IO` action. 
Rien à Montréal, donc probablement rien de ce côté de l'Atlantique :(
I enjoyed this. I hope Haskell 2020 gets a new prelude.
In the short term, I'm: * finishing off my `reflex` blog series * putting together some code to do state-machine backed property-based testing of web applications by combining `reflex-dom` and `hedgehog` * getting together some prototypes for using `reflex` to work on the backend with sockets / websockets / servant * helping out with the GHC / WebAssembly work where I can
&gt; Pay 1$ or more and get [...] "Learn You A Haskell for Great Good!" That seems to be the only relation with Haskell... and it's a [free book](http://learnyouahaskell.com/) anyway, so including it in the bundle only artificially makes the bundle seem like a better deal than it is?
&gt; In short, you do have to make sure the file has the file: URI on Linux Having moved to open-browser it doesn't seem required - it works great on Windows and Linux. 
dead link
kale looks nice!
When I wrote it I believed I was ripping-off/being-inspired-by Hood/Hoed. However, what I've done is certainly closer to what you did, and I had seen your thing very briefly, so I imagine it was percolating around in my brain too! The debug package is only 2 days old, but yes, I intend it to be stable and supported - ideally I'd like to get everyone collaborating on it (a pipe dream for many projects). You also need to add `ViewPatterns` which might be a detail that changes, although given how elegant they are for what I want, perhaps not. The debugJSON is an implementation detail, which may change, but if you want it to be mostly-stable or add-only, that's probably feasible between minor/major versions. I don't have line/col information though, since I just pretty-print the results, but it seems feasible to add it - https://github.com/ndmitchell/debug/issues/7. Intero integration is always welcome, although in all honesty it might be worth waiting for a few real-life users saying "this actually has value" first :)
I'm just about riled up enough to make a GHC proposal.
We'd probably have to start a deprecation cycle in GHC 8.6 (too late for 8.4), thus committing to the change being in Haskell2020. Since 8.6 won't likely be released until late 2018-mid 2019, Is 1-1.5yrs of deprecation warnings long enough? Also, we'd have to do an analysis of code on Hackage to measure the breakage, which could be too large.
You should see if the ghc guys will replace the builtin debugger with this!
Good taste! I am a bit into computer for education, having read mindstorm and is currently reading turtle geometry. Heard about PLATO but never really study it.
Fixed (hackily) in 0.0.2.
Or take lessons from this and vamp up the built in debugger. Their debugger is way more powerful, but I just don't fundamentally believe that command-line debugging tools are of much use - too hard to use at exactly the point you want to explore.
works for me
Using gloss I see. Yes, GHCI would usually be a good way to iterate, quicker than rebuilding each time. Not all frameworks work with GHCI. It looks like this was tricky but doable with gloss at one time. Try it and seek help from the gloss docs/maintainer if there's trouble.
Thanks for the feedback, I corrected that. I wonder how I did that!
Backwards compatibility can be helped on a module-by-module basis by having a `Prelude.Deprecated` module that contains whatever was removed or changed from the new Prelude. It would be relatively easy to write a `sed` script that did the bulk of the work -- import the deprecated functions and explicitly import the Prelude and hide any shadowed versions. I'm OK with this being long term. But to go into Haskell 2020 (or whatever the new version is) without fixing the Prelude would be a wasted opportunity. 
The details haven't been fleshed out in a full proposal yet, so there's some controversy, but the new caret-bounds should help with this (and I believe they're compatible with the latest cabal and stack both at this point). In particular, you should be able to say `foo ^&gt;= 4.5` if that's the lowest version you've tested against, and it will imply upper bounds that are "soft" and tooling will eventually be able to automatically bump.
FYI there is an interesting WIP haskell ERP system on github, a few years old. Maybe someone here will remember its name.
Need to check for space leaks as per http://neilmitchell.blogspot.co.uk/2015/09/detecting-space-leaks.html?m=1
If you follow the space leak techniques from http://neilmitchell.blogspot.co.uk/2015/09/detecting-space-leaks.html?m=1 it detects the space leak immediately (finding it is the foldl takes a little while longer, but changing that removes it).
Fuck yeah
That's a great technique. I'm having some trouble fixing a memory leak at work, and trying this has been on my list ever since I first saw that blog post. Thanks for writing it!
*drat* I wish I spoke French. Hope you give the English language version of your talk soon :) .
Is the stuff about explicit export lists up to date? I thought not being exported meant the compiler could delete the definition if unused (which won't affect performance, except compile performance), but has no effect on whether it inlines it locally or not. I'm not sure what to make of the "more flexibility regarding calling conventions" handwaving and jhc reference. It seems the real advantages are from being local definitions, not unexported, and INLINEABLE can address that. Also I wouldn't mind seeing another explanation of lag vs. drag vs. void. Every time I have to go back to the original ghc profiling paper to remind myself what exactly those are, and while it seems the usual strictifying exercises address drag, I'm not so sure about lag and void.
lovely! Could you add a few examples with the code? Might make it easier to read. Adding benchmarks to the examples would be even better.
If someone modified this to work with an emacs frame, i would be so happy. Someone please make my dreams come true.
i think it's the formatting and the fact you can use it as a reference that makes it worth it.
Is it simplified-only? The examples make it seem that way. It should be possible to convert automatically, though I assume it's easier if you start from traditional.
don't see anything worth paying for.
Did not look into the details, yet, but have you also listed AUTOBAHN? A genetic algorithm for strictness-analysis.. I only read the paper and have not used it myself.
You don't make libraries proposals as GHC proposals. You make them to the core libraries committee through the libraries@ list...
Good to know, thanks :) I'd guessed that Prelude being part of the Haskell Report would be somewhat different?
&gt; The partial functions and foldl in the prelude really take away from Haskell's charm. Especially for newbies. Eh the partial functions are a bigger problem. And "for newbies" is honestly not the most interesting criterion. 
Sadly, LYAH is a poor learning resource to the point where I'm almost convinced it's actually worse to read it than not.
I don't think I'm following your argument. Why would Python (for example) not be able to take a function that produces a result using IO and cache it? I'd write up a quick example but I'm on my phone :p
I'm learning too! From venting my frustrations to more experienced, helpful Haskell programmers I've been assured that they continue to experience frustration. I am also assured that the benefits are tangible and worth the effort. I've been trying to plod along with the https://adventofcode.com challenges this year in Haskell to get some more exposure. It has left me wanting to flip tables at times and also highly elated as I realize I've started to grasp some new concepts and techniques I'd never encountered before. Having http://haskellbook.com is also helpful as is having someone with whom you can commiserate/vent/talk to about Haskell. Don't despair! It's a journey and not a destination!
it's definitely not the best but it's good as a quick intro to haskell. i read this first and then went to cis course.
By default Haskell doesn't (and probably shouldn't) assume that your IO can all be done in parallel. What if the first JSON request was a POST to create some resource, and the second was a GET to some related resource that was affected by the POST? Doing them at the same time would be a great way to break your logic. What the example means by asynchronicity is that while those HTTP requests are being sent, the program is free to do other things in another 'green thread'. For example, if this is happening in a web server running on WAI/Warp, other requests could be served while one request waits for that HTTP call. In other languages, you might have to use other language builtins (like goroutines or Node's event loop), have your code run by some other application (e.g. Apache has multiple threads to run the same PHP script concurrently for different requests), or use raw OS threads/processes (like forking in C).
I don't see the point. The market is saturated and at the end of the day, it is not really that important what technology was used to build the system. What matters to customers are the features, safety and user friendliness. It would surely be a nice project for educational purposes. Disclosure: I work with Workday ERP.
LYAH is certainly outdated and has an off-colored flavor, but to say it's worse than nothing is simply false. LYAH was good enough to bridge the gap for complete FP noob like me. I've gotten more from LYAH than what I paid for ($40). The other option to Haskell at the time of release was RWH, which was far too difficult. I don't recommend LYAH as the go to resource, but some people might want more resources from different perspectives. If so, then $1 is a steal.
Weird. I just looked at the open-browser implementation and it just seems to call a plain `xdg-open "url"` in a sub shell where url is just a raw string. I'd expect that to open files in a file browser unless they had the `file:///` URI 🤷‍♀️ Well it works fine, apparently, so meh
You're missing the `{}` as the last argument to `callCabal2nix`.
It was one of the better resources for it's time, but Haskell learning materials have improved a bit since then and are getting better. Every single time I log into the IRC, I see at least one person who picked up LYAH and proceeded to get so completely confused about how Haskell works that the others on the irc end up inevitably a) undoing all of the learning they picked up from LYAH, b) actually teaching, c) recommending different learning materials instead. I'd much rather see those learning materials advocated for more than LYAH; just because it's not utterly irredeemable (although, personally, it comes close) doesn't mean it needs to be the go-to learning resource that people think of when they want to learn Haskell.
What changes would you make to ats to turn it into atskell? Besides making the code look less ugly, that is...
dang it anyway
See mama our contract as to what to do I what is the video but a game about this little our rumba guy goes around the rock welcome much honey all right soon find a good fucking Evan is still one spot I'm not entirely sure I got it open or you're talking like Mickey Mouse
Making another WebAssembly backend for GHC, doing it as my undergrad thesis. Details [here](https://github.com/TerrorJack/asterius)
That worked, thanks! Strange that it fails silently without any warning or error.
Win. Cool stuff. Thanks for the explanation
Hmm, yeah handing a partially applied function to `ghcWithPackages` is not useful.
Nope, same process for Prelude.
This paper seems to be exactly what I was looking for. Is there anywhere I can make questions about it (other than directly mailing the authors)?
Awesome, thanks for the links! That haskell book looks interesting, I think I'm gonna buy myself an early christmas present. I'm looking forward to the day when I am not thinking about the syntax and more about the problem I'm solving. So far so good, just got over my first brainfuck which was "Why oh why can't I just divide two ints and plug them into a function that wants a float?!". After studying the number types I see the error of my ways, and also what I was taking for granted in my other languages! 
An HTML parser according to the WHATWG spec. Definitely already a solved problem, but it's a good way to relearn the language (and learn beyond the brief look my college course contained) that will hopefully eventually lead into a terminal-based browser that actually feels (somewhat) modern.
As far as I'm aware, that's you're best bet. Aaron Stump is the lead behind the project. If it helps, there's a reference implementation [here](http://cs.uiowa.edu/~astump/cedille-prerelease.zip). Bases on an [earlier](http://homepage.divms.uiowa.edu/~astump/papers/from-realizability-to-induction-aaron-stump.pdf) paper, I made an implementation myself, which you can find [here](https://github.com/AHartNtkn/IotaTT). Feel free to ask me about implementation details, if you want.
Haskell bindings to [OpenZWave](https://github.com/OpenZWave/open-zwave). The goal is to use an FRP library (currently [reactive-banana](http://hackage.haskell.org/package/reactive-banana)) to define various automation rules around my home. I have everything setup and running, but dealing with dynamic graphs in FRP is not as simple as I originally thought, and so my "beautifully declarative smart home configuration" is currently a mess that's almost impossible to read. But I'm getting the hang out it! Hoping that I'll have an API I'll happy with before the year's up, and will be able to write up my experiences (binding to C++ with [hoppy](http://khumba.net/projects/hoppy/), adventures in FRP, and dynamic code loading are all things I'd like to write up)
Honestly giving ATS friendlier syntax would be a game-changer. And typeclasses instead of a module system would nice. 
&gt; but it really hurts when we talk about how great our performance is and then people run into String, foldl, etc Well, don't talk about performance then! I've found Haskell to be consistently slower than Rust and consistently faster than Ruby/Python. It's not terribly complicated. &gt;It's not less painful than when we brag about how safe we are, and then have folks writing head etc. Most people that care about safety learn about that fairly early on. And with `head` at least it's pretty clear it's unsafe if you have any idea what you're doing. 
For the first point I suggest Weeder - it can spot excess export lists.
I completely agree that partial functions are a problem (although I don’t think debating whether `foldl` or partial functions are a bigger problem is particularly productive). What puzzles me is that you say you’ve never used `foldl`: Did you never implement functions such as `minimum`, `maximum`, `sum`, `product`, …? At least in my projects, these kind of functions are quite common and `foldl'` gives me a way to implement these kind of functions in a very performant way while implementing things like `sum = foldr (+) 0` will result in a spaceleak for no good reason. So I’m really curious if you have a different way of implementing these kind of functions or if the need for them just never arises in your projects.
Thanks for bringing that term and paper to my attention!
&gt; while implementing things like sum = foldr (+) 0 will result in a spaceleak for no good reason Really? I don't think that's true. I just use recursion schemes in most cases. It benchmarks to the same thing. 
Ah, I see what you mean. You have a point, but I still disagree on a couple things: first, in the presence of _|_ unsound is actually unsound, but in the presence of "expensive", it's really not: you may perform more work than needed, but ultimately, you still get the right answer. That's not unsound, just suboptimal. And yes, touche, there's nothing I can argue mathematically here: it's not technically an always win, therefore you can construct an example where the "optimization" will be slower than the original. Practically speaking, though, I'm still on my side: you can say the same about nearly every optimization; that doesn't mean we don't use optimizers because someone can manage to construct a pathological case. In practice, when you have an expensive computation that you know might not be necessary, you just make it explicitly lazy, just like when you have a computation you know will be used, you make it strict.
The blog post was quite literally caused by someone running into a space leak with `foldl`. Obviously we can *just not do the bad stuff*, but as a language/ecosystem that prides itself on knowing better, our standard library could reflect that better.
Try the following example ``` import Data.List main :: IO () main = do print (foldl' (+) 0 [0..1000000000 :: Int]) print (foldr (+) 0 [0..1000000000 :: Int]) ``` The first one runs reasonably fast while the second one takes a few seconds and then runs out of memory on my system (compiled using GHC 8.2.2 and with `-O2`) The reason for this is that the `foldr` version will walk all the way to the end and accumulate `1 + (2 + (3 + …)` and only then start to reduce things. Taking a look at the Core produced by GHC for the relevant loop is also helpful: ``` $wgo $wgo = \ w_s54a -&gt; case w_s54a of wild_Xv { __DEFAULT -&gt; case $wgo (+# wild_Xv 1#) of ww_s54d { __DEFAULT -&gt; +# wild_Xv ww_s54d }; 1000000000# -&gt; 1000000000# } end Rec } ``` One can see how it has to recursively `case` on the second argument to produce the result which results in linear space usage.
&gt; That's not unsound, just suboptimal. If the time it will take to finish is literally longer than the heat death of the university (which for my hypothetical class is absolutely the intention), then calling it suboptimal is pretty dishonest. &gt; you can say the same about nearly every optimization But you really can't, not to the same degree. If someone found a piece of code where GHC's optimizer caused a slowdown of that magnitude it would be considered a bug and promptly fixed. So I still completely disagree, in terms of optimizations and evaluation strategies totality still means basically nothing.
I'm not really familiar with the book, but I'm curious what you think is close to irredeemable about it. I scanned a few sections from the web site, and didn't see anything that struck me as terrible. The title and overall tone is annoying to me, but that's a matter of taste, of course.
[removed]
Thanks! But you'd actually have to set up some "green threading" elsewhere for that to happen in Haskell, yes? It isn't that Haskell will automatically go do reductions in another part of your program while it waits for the first http request to complete?
You're welcome, and good job to you as well! If your `isSquare` test involves calculating that same floor of sqrt, you can save it for reuse by replacing the `Bool` with a `Maybe Int` that's `Nothing` when the `Bool` would be `False`, `Just k` when `k^2` is the length. `flip fmap exactSquare $ \n -&gt; let xss = possibleInputs n; ys=.....`. This will make `solve` give a `Maybe [Int]` instead of a `[Int]`, but rightfully so.
/u/NorfairKing, could you use [`wai-app-file-cgi`](https://github.com/kazu-yamamoto/wai-app-file-cgi) for this?
Mitsubishi à Rennes, c'est de la méthode formelle il me semble. Pour ta boîte sur Paris, c'est peut-être [Prove &amp; Run](http://www.provenrun.com/) ? Par contre, ils font pas de Coq à ce que je sache, ils montent leur propre assistant de preuve propriétaire.
I would start from Haskell and just add unboxed data and unboxed collections as first class. This would make much of strictness analysis and strictness annotations redundant. And I would add also the proof system. . 
Well done! I didn’t think of using a lambda and partial application like that. As far as I can se, it should work (if you remove the else-clause in inner() and de-indent “return cached” a single time). What’s the reason that `getGoogle = cacheIO(...)` doesn’t just store the result of `cacheIO()` in `getGoogle`?
Does reactive-banana use "unsafePerformIO" etc? Which code should one worry about with regards to breaking on GHCJS?
&gt; Then I came accross gauge on the criterion issue tracker It's a shame hackage couldn't give you this information.
"This is the laziest goddamn customer service you've ever seen."
Oh, I wasn't actively looking for a benchmark library. I just came across [this issue](https://github.com/bos/criterion/issues/166) and thought "Gauge? What's that?"
[removed]
Yes, reactive-banana uses `unsafePerformIO`, but that's not what is breaking. In my case, the issue is that the GHCJS implementation of the module [System.Mem.Weak][1] does not match the [specification][spec]. [1]: http://hackage.haskell.org/package/base-4.10.1.0/docs/System-Mem-Weak.html [spec]: https://www.microsoft.com/en-us/research/wp-content/uploads/1999/09/stretching.pdf
*All* pure code is ultimately called from an IO action (namely `main`).
I am currently working on a Customer Relationship Management (CRM) web application written in yesod and prostgresql DB. The company using it is a small Web/Print-Agency. They manage all the customers, suppliers, orders and also timetracking and reminders. Sure, the development is agile ... they get what they need :) I develop it on ubuntu but the production haskell app is deployed to an OpenBSD box. 
Do you know why IntMap does so many more allocations than HashMap? Is HashMap better optimized, or does IntMap use different algorithms/structures that just need more allocations? I see that IntMap is mostly faster, so I will stick with using it, but I am curious if the allocation numbers indicate that there are optimizations to be done in IntMap that might make it even faster?
Added!
I agree - ideally, each each would have some actual Haskell package in the repo that demonstrates it. The trouble with some of the Wiki advice is that you don't have any evidence accompanying it to know what extent to which it was true for what version of which compiler.
I use VS Code with [Haskero](https://marketplace.visualstudio.com/items?itemName=Vans.haskero) plugin. The installation process is pretty smooth. It has autocomplete and even "insert type to function".
Good question. I updated to recent weigh which shows us additional columns: max bytes allocated at one time and the final live bytes after GC. The results are: https://github.com/haskell-perf/dictionaries#from-list-int-keys Which indicates that they both use roughly the same resident memory, but the `IntMap` seems to do a lot more allocations of intermediate data structures. It allocates total way more and does way more GCs as a result. Couldn't speculate on why this is without more investigation.
Yes, I scrapped that for parts to use in the end :)
Allocations should not be a red flag in Haskell. An allocation is essentially as cheap as incrementing a variable in Haskell. What matters is that you don't generate too much garbage. `containers` compensates for its high allocation rate by sharing data between structures very often. My understanding is that HashMap allocates at a lower rate, but those allocations are more costly / often generate more garbage. As with everything, it's about tradeoffs. I usually reach for `containers` before `HashMap`, because modifications with `HashMap` are usually much slower. However, `HashMap` does provide constant time indexing, so there are times where it's appropriate. Then of course there's mutable hash tables, which in most languages are the best mutable map in almost all cases. But I've heard that the mutable hash tables in Haskell are all much slower than they should be, though I haven't investigated this at all.
I just use Dante in Emacs. It's pretty low-tech, but it serves all my basic needs and works with any tooling infrastructure you may be using (I do get creative with my tooling :P). In the long term, I hope that `haskell-ide-engine` will become the de facto solution. I think a lot of people seem to agree with me on this, so it's hopefully just a matter of reaching stability and releasing everything to Hackage / Stackage.
I'm currently using MonadError in IO code so we track errors as much as possible. You have to accept that inside IO anything is possible so you can't account for all possible errors. In pure code you don't really need to worry about async exceptions, rather you should worry about them at the point that this pure code is forked. Lastly, it is confusing. https://simonmar.github.io/posts/2017-01-24-asynchronous-exceptions.html https://www.google.co.uk/amp/s/www.fpcomplete.com/blog/2016/11/exceptions-best-practices-haskell%3fhs_amp=true
I am using WinGHCI and Sublime Text 3. And it's terrible. WinGHCI offers fewer features than even the command line version; it doesn't even have auto-completion. What is more, it crashes frequently. In particular, trying to interrupt running compiled code has better than even odds of requiring a WinGHCI restart. It truncates pasted lines. Etc. Sublime Text is a fine editor. Neither the default Haskell nor the SublimeHaskell mode work well for me. The default mode is feature sparse: no auto-completion or integration with ghc. What's worse it contains several bugs that consistently mismark comments and codes. Even while trying to avoid the more commonly mismarked constructs (which are perfectly legal Haskell and accepted without complaint by ghc), a line turning red or green for no reason is common. The SublimeHaskell mode is in principle better and has good integration with GHC and related tools. When it works. Which for me is rarely as it is enormously finicky regarding installation location and version of certain tools. Last time I tried to use it, it either popped an error message every time I tried to save a Haskell file. Or it just crashed ST on startup. But it may still be better than the default mode. Let me try installing it again... That I (or anyone) would still chose to work in such an awful environment, rather than the smooth, sophisticated IDEs offered by other languages, bespeaks what a wonderful language Haskell is.
Remember that allocations in Haskell are basically as cheap as incrementing a pointer. What matters is the size of the working set; much more so than the amount of garbage generated.
I don't think those benchmarks are entirely fair to the `intersection` case. I use `intersection` and `union` a lot, and `containers` absolutely kills `HashMap` here, since repeated use takes great advantage of sharing. But the benchmark is only testing a single `intersection`, which isn't going to be very representative. In my experience, the only time I was glad to be using `HashMap` was when I was creating it with `fromList` and effectively using it read-only. Any time I wanted to modify immutable maps, `containers` has performed better; but I use `union` and `intersection` a lot. I'm fairly surprised at your benchmark's insertion results; I would have expected `HashMap` to be extremely wasteful with `insert`.
Thanks to all the many contributions to Stackage Nightly over the last months, which made this release possible now. :-)
Have you tried something on Windows Subsystem for Linux? 
This sounds like a solid idea. Probably just something like Yesod w. Persist and like a Postgres db?
All that may be true. The idea behind `haskell-perf` is to *show* it. If you get time to submit a PR to make the benchmarks more accurate or comprehensive, that would be much appreciated! 
I use VS Code with ghcid, no real IDE features besides type checking but it is very fast and copes well with changes to the file system (for example during git rebase). I tried the haskero plugin that was mentioned elsewhere but found it a bit too slow.
I work on my side project that is a website for meeting foreigners and language exchange: [difriends](https://www.difriends.com/), it is 70% written in Haskell and Yesod and increasing (the old PHP/Node backends are going to be exterminated). Currently I am replacing the chat/messaging, websockets in Haskell are great. The current Node implementation is very buggy, it seems that I have no talent for dynamic typing. Hopefully the new implementation will be great.
I use and like `haskell-mode` for emacs, but honestly it's.... kinda broken on all newer GHCs, and it has been for a long time. I still use GHC 8.0.2 because it just doesn't work on anything newer :/
Please please add a list `dedup` (remove consecutive duplicates) function that requires the list element type to be `Ord` and have the documentation recommend it over the O(n^2) `nub` for production code. `nub` is to removing duplicates as bubble sort is to sorting; even if something is only `Eq`, if there's even the slightest change of there being more than 20-30 of it in a list from which one wants to remove duplicates, it's worth making it `Ord` to save someone time debugging unnecessary slowness down the line.
What are you looking for in regards to benchmarking? What worked best for me was to backport my changes to the latest release (8.2.2). Then I organized two bindists (one with, one without my changes) and just run the benchmarks of various libs. (aeson, containers, unstable-containers, ...) with both. That way you won't have to deal with the issue of packages being incompatible with GHC-HEAD left and right and can use whatever build tool you prefer. Nofib is also a great way to ensure you don't introduce regressions.
I agree! I've been looking at it lately because of your blog posts and, man... I just can't get over how horrifically terrible it looks to write. I'm also a big fan of modules like 1ML, although I admit I only like them in theory as I haven't used 1ML enough to get a feel for those vs Haskell typeclasses. It would also be really interesting to see what would happen if we had both 1ML's module system and Haskell's typeclasses...
I don't think it's completely irredeemable, but it's a very low quality learning resource and does a fairly terrible job of actually teaching people Haskell. The fact that it's ranked so high in search results, that so many people seem to recommend it without reservation, and is it even still the first result in the Haskell sidebar after all this time? That's the ridiculous part, to me. With as much emphasis as the Haskell community gives to having high quality libraries, it irks me to see RWH (essentially completely obsolete at this point) and LYAH (almost functionally useless as a learning resource) being some of the two most famous Haskell learning resources. People finish reading the book and end up having learned nothing, but feeling like they learned a lot. Then, when they fall flat on their face trying to do Haskell, they realize they're essentially still at square one and either quit and perpetuate the "you need a PhD to write hello world" myth or start learning it the hard way through the IRC and random blog posts; maybe they'll get lucky and start reading a better beginner resource after wasting their time on LYAH. Why the hell would that method be one of the top most recommended learning methods of a language? I know we like to joke about avoiding success at all costs, but I'm pretty sure that's not how we're supposed to be doing it...
spacemacs + intero OR VsCode + HIE. 
I use `haskell-ide-engine` with VSCode. What do you use to auto-format the import lines? 
Nothing. Though there's a PR to add this to brittany, which is what HIE uses to format code.
I use Emacs with `intero-mode`. Works great, you just have to restart it from time to time when hlint turns off. 
I think you forgot to count the pointers in Node, but you're right that clearly it doesn't explain everything. Next step, make a heap and/or allocation profile and see where your memory is going.
Vim on one monitor, a terminal and Safari with Haddocks on the other. I use straight terminal Vim, not gvim or anything like that. I used to futz with Emacs, `haskell-mode`, Intero, `evil-mode`, and at least a half dozen other Emacs packages. I was spending too much time futzing with all that stuff, for questionable returns. I miss some things about Emacs but Emacs with default key bindings is painful. I don't miss getting that menagerie of packages to work, and I don't miss having some buffers in Evil and others in Emacs bindings and having to constantly shift my brain that way.
If you implemented then you can certainly help me, if you don't mind! I have a CoC implementation and my goal is to figure out the least things I need to add to it to have inductive types. I'm trying to grasp the paper and still a little bit confused with the details. Before anything, just a question to be sure this is what I'm looking for: that system allows us to express O(1) pattern matching too, right? Because from the examples it seems to me that this implementation of inductive types is not more efficient than church-encoded types for pattern-matching... So, some questions I do have: For one, it starts with the CoC, but rather than having just `λ` and `Π` (which is what I'm used to), it also has `Λ` and `∀`, which look like the same as the formers, except, I guess, they're used to introduce kinds, and can be erased? Is that correct? In any case, is that distinction important? Then, after talking about how `∀` is part of the version of CoC he starts with, he then says he will add implicit products to it, and once again uses the `∀` symbol. So, what is going on? Is that the same thing mentioned previously, or is it something different? Or, in short, I'd absolutely appreciate if you could tell me exactly how many constructors there are, and what the introduction and elimination rules do, in English... I understand most of them but some things are not clear, like, for example, why do we have `{p}` on the value of a dependent type, if `{p}` is always `β` which is just `λx.x`? Also a little bit confused about the role of `t − t ′` and `ρ t′ − t`. Don't worry about answering them all, I'm slowly grasping things as I read the paper again and again, but some help would certainly speed things up...
Cool. The PR is being actively discussed right now: https://github.com/lspitzner/brittany/pull/83
I'm still waiting for someone to pick up the glove and write a "lyah considered harmful" article.
Thanks for the response. I'm still interested in specifics about what's wrong with the book, though. That it doesn't cover the library ecosystem? That's definitely true. It seems to just barely dabble in the most basic of the abstractions that the modern Haskell community takes for granted. But it doesn't seem to justify statements like "... end up inevitably a) undoing all of the learning they picked up from LYAH" I'm actually interested in whether you see something wrong with the content or pedagogy, rather than just the scope. Is it really misleading like this? How so?
I hope the author, Oskar Wickström, doesn't mind me posting this here.
The side project practically this whole year has been trying to find time to basically rewrite [hashabler](https://github.com/jberryman/hashabler/): - actually fix the correctness issue by codifying that Hashable instances form a code (a prefix code specifically) - add a bunch of utilities for testing, so hash functions can be automatically given a signature, we can calculate statistics about hash quality automatically, etc - generic deriving for Hashable - a million other little things
&gt;Would LOVE to get auto completion etc. in atom I have many nice things in my Haskell environment, but not this. I use vim and a bunch of command-line tools, however. 
Good question :) I think the underlying problem is that Persistent tries to do too much in providing JSON instances for database datatypes. I've previously tried getting it to let me define datatypes elsewhere and eventually gave up - just too much hassle. My recommendation would be to let database datatypes be database datatypes, and model your datatypes that go out over the wire separately. I know it sounds like duplication, but you'll frequently need slightly different forms for the database and HTTP comms anyway, so you might as well bite the bullet.
I think the missing of the database schema management can not convince people that is "Fast and Fearless"
Good point; do you know any good practices/libraries for this?
Who is this directed at? End users shouldn't need to look at commit messages. Developers will need to look at commit messages, and moreover already have access to those.
I read most of your comments here. Can you elaborate on why the libraries you mention are horrible and badly designed?
Thanks for the tips! I have just visited the [nofib landing page](https://ghc.haskell.org/trac/ghc/wiki/Building/RunningNoFib) and started benchmarking.
Yay, pandoc is at v2.0.5 now!
Hello! I run the default stuff in my emacs config from dantes github page in emacs 25.1, but the flycheck doesnt work for me, and the keybinds for looking up info on functions doesnt work either. Do you have any experience with this issue? Cheers! 
Is there a way to introduce type alias for non trivial types? i can introduce type alias like type Name = String, hower i'd like to introduce an alias for a compound type like (a-&gt;bool). So i tried type premise = (a -&gt; bool) but that was not allowed. It would really make my code easier to understand and faster to write, since i can than define functions as f :: premise -&gt; conclusion.
Yes, simplified only. Automatic conversion to traditional is on my TODO list. Hopefully it'll still be useful even with the occasional error.
Someone programmed a whole office suite that is comparable to MS office suite and wrote it in 20k lines of code. The MS Code is maybe what? 300 million lines of code for the same thing. Basically we have all horrible accidental complexity that can be elegantly solved with good designs. If you design a GUI system from the ground up you need this *Very good designs*. For example, look at random cairo source files for vector graphics. Then look at the Haskell Rasterific Package. The latter is exponentially better designed. This is a good comparison that "math wins" in many cases. Basically we just need to do the same thing for a GUI system; e.g., what Rasterific is to Cairo we need "Concise new GUI" what wxHaskell/GTK/etc. do. 
You can parametrize type synonyms: type Premise a = a -&gt; Bool f :: Premise a -&gt; a -&gt; a
How do I modify **only** the `???` part (without adding parameters in front of the `=`) to make this code typecheck? Lambdas haven't really been explained in the book up to this point, not have Functors or `fmap`, so I'm wondering how I'm supposed to solve this without some of the more advanced but yet unfamiliar-to-me techniques that a Haskell geek friend of mine mentioned to me when I asked him. **[Here](https://i.imgur.com/2j1bTvR.jpg)** are some of my failed attemps. **[Problems 1 and 2 on the same page](https://i.imgur.com/KlQd6XF.jpg)** were quite simple to solve for me but I feel stuck on problem 3. Any help?
So, in other words, you are against technical debt and suggest that clearer design with a more powerful language like Haskell will help, do I understand correctly?
Emacs + evil + dante and ghcid. A kind of similar setting to [this](https://github.com/soupi/minimal-haskell-emacs) but a bit more customized.
But your pure code is not going to be throwing any async exceptions itself, or at least I shouldn't be. 
I believe the difference is between "encode" and "represent". The encoded program is _not_ the original program, but instead has all these singleton witnesses lying around that need to be reflected back and forth! And computation on the type level needs to be "faked" with an entirely separate lifted function space. The goal of things like `DC` as I understand it is to represent accurately the direct meaning of dependently typed program, rather than just a translation of one.
In my opinion Persistent is very good and straight forward to use. Includes database migrations and a easy to use interface
Thank you for posting this. I feel Haskell is a great choice for the web. I'm using Servant for API, Persistent for db and Blaze for generating type safe HTML. Still just plain CSS and JS for now. It's really nice to come to Haskell after spending years in Python frameworks. Not saying they don't do the job but they are much more error prone. Usually once I'm done compiling Haskell my web functionality just works. 
See [Chris Allen's criticism of LYAH](http://bitemyapp.com/posts/2014-12-31-functional-education.html#learn-you-a-haskell).
&gt; I believe the difference is between "encode" and "represent". How meaningful is this distinction? When translating to an internal language, things inevitably get encoded/represented differently. After all, the goal is to reduce the surface language to a set of basic constructions. Perhaps your argument can be rephrased that if the Core supports dependent quantification, the encoding of surface Haskell is more direct and lightweight. But I see this as a spectrum, not a clear distinction. Are there Core-to-Core transformations that are easier to do in System DC? Then I would understand that this change is to simplify compiler internals.
For me "clearer design" is much more important than the language. But yes, I guess correctly. Besides Haskell there are dependently languages etc. or other good languages like lambda prolog (ELPI, teyjus).
Except me and many others want to use Haskell for front end / GUIs. So we won't let Haskell just be resigned to server side only. 
I have it installed and have experimented with it. Being able to use a decent Linux IDE for Haskell is nice, but I miss some of my native apps.
spacemacs looks awesome, thanks!
Also note that it is has merely been hypothesized that elaboration into singletons is sufficiently expressive to encode Pi types. See section 8 of the nokinds paper you linked. The approach of a dependent core not only seems more manageable, but there's a path to showing it implements full Pi types. There's no work showing full Pi types in the singletons approach, and the 2012 paper (http://repository.brynmawr.edu/cgi/viewcontent.cgi?article=1009&amp;context=compsci_pubs) leaves a number of questions open even as a first step, and I think not all of those have been addressed?
There's a common statistical fallacy at play here in reasoning, though I don't know if there's an official name for it (sampling bias?). LYAH is still one of the most popular resources on Haskell -- not least because it is short, it is written in a friendly manner, and it is free. Beginners will inevitably be confused in some proportion about Haskell -- especially those who show up to IRC with questions, almost by definition. (I.e. why would they show up to IRC with questions if they were not confused?). Since LYAH is the book they're most likely to have encountered and read on their own, then the overwhelming probability is most beginner questions will make reference to LYAH. This can't tell us _anything_ about the quality of the book as a resource! I guess this is an instance of the general fact that anecdata can only show occurrence, not frequency or correlation.
I use Beam which allows me to do this effortlessly because `beam-core` compiles in GHCJS just fine. But it's true that you often want slight differences between your model and the data in the view (at least consider the security implications of transmitting full DB models), so I can see benefit in not using DB models directly. In that case you'll just have to create a different type optimized for the view-side. Another interesting approach (which may or may not work in your setting) would be to use `generic-lens` and merge one record into another based on field names.
Are allocations cheaper in Haskell than in other langauges?
Much. Haskell's allocator is optimized for the fact that people allocate ADT constructors all the time. Allocating is quite literally as cheap as bumping a pointer.
It's the pedagogy that's the main issue; I should've clarified more about that, sorry. The scope is fine, as it's essentially a very standard and basic approach to intro Haskell (I'm not sold on the order of presentation but that's certainly not it's big issue). The material also isn't necessarily wrong, either, but that doesn't make it a good learning presentation, just not an overly harmful or misleading one. It's main teaching approach generally consists of: shiny pictures, horrible analogies, and a few snippets of code that you can copy and paste into a repl. It makes your brain go "aha" but no actual connection was formed in the brain, and so the concept doesn't stick nor did you actually understand it. I don't recall anything ever being presented in a way that would motivate need or knowledge, either. Concepts are introduced once and I can't really see any spots where they're reinforced, which is another crucial component to learning. People need to do things, have a motivating need for an abstraction or solution method, solve problems, write down things, and actually reason through questions. Overall, it seems about as effective at actually teaching Haskell as reading popsci teaches physics.
So hyped when I saw this! Though when I actually went to use it, I got: &gt;stack build Downloaded lts-10.0 build plan. AesonException "Error in $.packages.cassava.constraints.flags['bytestring--lt-0_10_4']: Invalid flag name: \"bytestring--lt-0_10_4\""
So what's the catch? Does it make garbage collection more expense?
Try updating your stack to 1.6.1 ?
As u/sclv mentioned, this bug is fixed in Stack 1.6.1. If you can't upgrade, consider using [my fork of Cassava](https://hackage.haskell.org/package/Cassava-0.5.1.0). See [this issue](https://github.com/haskell-hvr/cassava/pull/155) for details. 
That worked! The hype is real again!
I consider LYAH one of the best resources for learning Haskell. It was my first exposure to Haskell and reading it was a breeze. I did not end up with a _single_ misconception after reading it (unless these misconceptions are still with me 5 years later and I cannot recognize them, which I doubt). I will recommend LYAH to every student who is interested in Haskell, and I am not aware of any other free book with the same quality of material and presentation.
Not at all!
I agree that databases and schemas are a central part of web development and choosing technology, but it was left out of the talk intentionally as the talk before mine was about persistence in Haskell, specifically.
&gt; ils montent leur propre assistant de preuve propriétaire. :(
We have our persistent models in the shared project so we don't need to duplicate code. I got fast-logger to build by using [this small change](https://github.com/TaktInc/logger/commit/dc0e5d622ea6c891d6d42bf3d8e051d94c202896). In cases where there was no workaround for GHCJS, we fall back to conditional inclusion and compilation. In your cabal file: if !impl(ghcjs) build-depends: store In your code: #ifndef ghcjs_HOST_OS instance Store.Store Foo #endif
That's a great point; I thought about whether to include that anecdote or not because of it. Ultimately, I see questions asked by people learning from LYAH vs other resources to be disproportionately lower in quality, understanding, and so on (although they only appear so to me, and I don't have any hard numbers to back that up). I also see less people stay on the irc long term who have done LYAH, and see more people who have learned from Haskell programming from first principles stay in the long term (although, again, that's merely on indicator out of many and certainly not an accurate way to measure the success of learning materials). Although, of course, the same statistical biases remain in play here along with others. I can only really objectively critique LYAH on it's pedagogical methods and ability to present material in a way that's proven through educational theory and practice to be effective; both of which I have major issues with.
Have you read through Bird's intro to Haskell book, or Hutton's, the CIS194 course, or the Haskell programming from first principles book? I'm curious as to how they stack up to you vs LYAH :)
The catch is that it requires some pretty serious compaction in the GC to avoid fragmentation, as it won't otherwise get to go back and reuse freed memory. AFAIK, this isn't a major cost, as we want compaction anyway.
... you forked a popular library over a flag? Haskell people have really got to stop with the "fork/reimplement at first sign of danger" mentality.
It wasn't the first sign of danger. The maintainer knew about the bug for more than a month by the time I forked. (See [stack#3345](https://github.com/commercialhaskell/stack/issues/3345).) I forked the library because the maintainer refused to change something that demonstrably caused problems: &gt; [I don't see any point to ineffectively workaround non-compliance bugs in Stack](https://github.com/commercialhaskell/stack/issues/3345#issuecomment-322148628) &gt; [I totally agree that this problem should be fixed, but the problem isn't at my end [...] There's no doubt in my view, that the bugs were clearly at Stackage's &amp; Stack's end](https://github.com/commercialhaskell/stack/issues/3345#issuecomment-322395708) &gt; [I don't mind to have them included, as long as it doesn't cause me to do anything I wouldn't have done anyway [...] being part of Stackage or not has no relevance to me](https://github.com/fpco/stackage/issues/2842#issuecomment-327744738) &gt; [I'm afraid I'll have to redirect you to the Stack issue tracker](https://github.com/haskell-hvr/cassava/issues/150#issuecomment-322317153) &gt; [I'd suggest you try shaming Stack dev into releasing a point release instead](https://github.com/haskell-hvr/cassava/pull/155#issuecomment-333310786) What else could I do?
I have been micro benchmarking GHC for simplexhc, and I've found some strange behaviour. I'm going to write this up as a blog post, but the short of it is that GHC does not scale correctly on certain kinds of input programs, and it is likely because of the way STG is code generated. https://ghc.haskell.org/trac/ghc/ticket/8272#comment:20 Sorry for the horrible formatting, I'm on mobile right now!
It wasn't "over a flag", it was "over a flag that triggered a stack bug rendering the package unbuildable". The fork made this library buildable during the interim when the new version of stack hadn't been released yet (and still serves this purpose if for whatever reason people can't upgrade their version of stack). I fail to see how any of this is a bad thing. As people upgrade to the new stack, the fork can die, having served its purpose.
One gripe: `foldl` is (almost) useless for lists, but it's perfectly reasonable to use it to consume sequences, sets, etc.
Right. There's another factor at play too -- someone who shelled out cash for a lengthy book vs. someone that browsed a free webpage. Again, the quality filter makes these things really hard to compare :-/
Of course with free software people can fork away, but I do think it’s better to change the name of the fork quite substantially to avoid confusion. As it is now, we have cassava and Cassava, which is just confusing. 
Good point. For example there's Roman's naming convention for the temporary forks he's made in the past: https://hackage.haskell.org/package/temporary-rc https://hackage.haskell.org/package/regex-tdfa-rc
You can just run ghci proper from the terminal!
I'm sorry. I regret the name I chose. I should have gone with something clearly different, like `cassava-without-the-broken-flag` or `data-csv`. I was trying to be cute and instead was stupid. I was also trying to show that case-sensitive package names on Hackage are bad, but it wasn't the appropriate way to make that point. As Mitchell Wrosen said on my behalf [here](https://github.com/haskell-hvr/cassava/pull/155#issuecomment-350496246), I will gladly deprecate `Cassava` in favor of `cassava` if the flag name is fixed. 
The GHC 8.2.2 release is complete.
I have to say, the attitude of "being part of Stackage or not has no relevance to me" is a good way to get your code forked.
True! I think what we want is `foldStrict` and `foldLazy`. `foldStrict` being `foldl'` for lists and `foldLazy` being `foldr` for lists. `Set` and `Seq` etc can define what's right for them in that case. `foldl` vs `foldr` vs `foldl'` vs `foldr'` is difficult to know on a case-by-case basis, and requires you to know what data structure you're working with for performance reasons. I think a `foldStrict` or `foldLazy` name that could intelligently choose the directionality of folding based on the underlying data structure might help with this.
I don't get it. Would reverting the flag have broken some new Cabal stuff or something? How is breaking the newer tools better than breaking the older tools?
As far as I can tell, reverting the flag would not break anything. But I could not determine why the flag was changed in the first place, so perhaps I'm missing something. 
The changes in `Validation`, were quite a surprise, but understandable. Thanks!
Yep, but that's really easy to do. For example- doJsonStuff = do json1 &lt;- httpGet url1 json2 &lt;- httpGet url2 useJsonBodies json1 json2 forkIO doJsonStuff someExpensiveCalculation Now `doJsonStuff` is running in its own green thread, and `someExpensiveCalculation` can run while the HTTP requests are happening. If you only have one core, then `someExpensiveCalculation` and `useJsonBodies` will contest for processor time as you'd expect, but Haskell's scheduler will allow both of them to make progress. This is different to e.g. Nodejs, which does not pre-emptively switch between tasks.
I think the basic idea why elaboration into singletons is sufficient is in the paper [Singleton types here Singleton types there Singleton types everywhere](https://www.iro.umontreal.ca/~monnier/comp-deptypes.pdf). The target language λH in that paper is similar to Haskell today: dependency at the type level and above, but not at the term level. They show how you can mechanically translate a program which uses dependent types at the term level into that calculus.
I think you must be. It sounds like some other tooling depended on the change, and it was stack which was out of date.
Hey that's me! My middle is W. ;)
Have you tried running a minimal Linux distro in a VM for that purpose?
Thanks for the suggestion. I have indeed played around with running a Linux distro in a few VMs, including the one bundled with some versions of Windows. It worked ok, but some of my essential X apps, like Emacs, felt a little sluggish compared to native apps. Still, that may be on the whole what I should transition to for Haskell dev work. 
Oops! Fixed. 
Containers finally has the `(!?)` op for maps!
I use `ghci` and that's about it. `:load`, `:type`, `:info`, and `:browse` are all very handy. I use Atom but just for fuzzy search and autocomplete and other basic text features. Oh it also has quick integration with `stylish-haskell` which I like for tidying up imports and pragmas. Aside from that, haddocks open in chrome.
Try intero instead.
Today I finally bit the bullet and figured out how to use DevelMain from emacs intero (quick reloading of yesod apps). Very happy with the result. 
&gt; It seems to have faster insertion, but slower lookup. Oh, thanks for pointing that out, my eyes had glazed over by then and I saw what I thought I should be seeing rather than what was there. I guess I should consider doing some more benchmarking of my use cases, I'll report back if I get a chance to pull anything useful for public consumption together.
You probably do not want your DB models to go over the wire (OTW) 1:1. Maybe at first for a small app, but as it grows many OTW models will be bigger, smaller, or just different from your DB models. Having a schema for what you will pass OTW is really nice, and `haskell-servant` facilitates this. Using that you get nice API docs and models that you can use on the client as well. Here an example with GHCJS/Miso: https://github.com/FPtje/miso-isomorphic-example
I've been reading up on things like this lately and what I've found so far has been pretty cool (hopefully any lurking wizards will correct me if I got something wrong) One thing that I found interesting is that a dependently typed core is, theoretically and practically, easier to implement than a non-dependently typed one. When there's a difference between terms and types, you have to track that, manage it, and do all the bookkeeping. With dependent types, there's no difference so it's much simpler on the compiler side of things. A dependent core is waaaaay simpler than the hackish Frankenstein we have currently as Haskell's current internals (well, it's not really as bad as I make it sound....). Currently there's: * The "normal Haskell 98" typesystem * A bazillion extensions to typeclasses, type families, type in type, gadts, insert-feature-that-mimics-some-subset-of-dependent-types, etc But, we can get all of that just by having a dependently typed system. No extra layers of glue and duct tape required :)
Forking and reimplementing are completely different
agree with /u/alanz (?) that &gt; I think this all points to the need for a proper grammar/spec for cabal files 
This unspecified new tool that no-one has heard of or seen, could have waited couple of months to help with smooth transition.
&gt; What else could I do? From that thread, it looks like it was a Stack bug with regard to parsing of Cabal files. So fixing the bug or trying to collaborate on work to shore up the spec for Cabal files would be options. Maybe that's just me. 
If you want type safe HTML, look at my lib: https://github.com/knupfer/type-of-html Blaze is only somewhat typed and not that fast...
I think temporarily manually editing the flag name makes sense, but: &gt; After some discussion with others via email and slack, the following is now clear: @hvr has a tool that generates these flag names, and likely did not initially realize that they would cause problems for stack. 
Seems like the flag was automatically generated
(What were those changes?)
My guess, is that there would be greater performance and better error messages. (But I don't know anything about implementing with dependent types, this is just a normal benefit of specialising/distinguishing things that are otherwise similar). 
Seems like a combination of [filtered](http://hackage.haskell.org/package/lens-4.15.4/docs/Control-Lens-Fold.html#v:filtered), [has](http://hackage.haskell.org/package/lens-4.15.4/docs/Control-Lens-Fold.html#v:has) and [only](http://hackage.haskell.org/package/lens-4.15.4/docs/Control-Lens-Prism.html#v:only) could do the trick. Something like a ^. key "pets" . each . filtered (has (key "type" . only "Dog")) I haven't tested it, though.
I'm *guessing* the other tool was Cabal 2.0. Given the choice between supporting a new Cabal-the-library and an old Stack, I'll choose the new Cabal since Stack depends on it anyway.
&gt; I have both linux and windows workstations Only losely related to your questions, but might be interesting: https://www.youtube.com/watch?v=gu7tIFJjHhU
Note that most of the bazillion things you list are not in core. The current core -- system FC -- is pretty tight as is. I don't think that the DC stuff is necessarily any simpler than FC -- just different. One of the interesting results of this line of work, afaik, is that all our weird zoo of stuff, though it can be used to "fake" dependent types, isn't actually just "a poor subset of dependent types" -- its different, and it enables different sorts of techniques and idioms. So even in languages with dependent types -- like Agda and Coq -- you still get different sorts of typeclass approaches bolted on on-top of them. They're not in the core (just like they're not in the core of GHC), but they're still way handy! See for example in the stuff Richard has worked on the desire to express _both_ Pi and Forall, to indicate if the term is relevant or not at runtime.
Huh, I was thinking about doing something like that with Pokemon (target level would be something on the level of pkmncards.com, it'd be nice to have automated queries, w/o futzing around w/ whatever the Haskell equivalent of curl is), but ended up getting too bogged down with trying to simulate the game logic. If you end up making it a generic card searcher, given some custom database, I'd probably use that, although that sounds like more work than just optimizing it for Magic, I'm aware. 
It'll be able to import custom cards, and insofar as I want those cards to be able to add additional fields or different frames (like a planeswalker), it might be compatible with other card games like Pokemon (but I don't know much about it). the scope is definitely just on Magic the Gathering, but importing cards in some some text format are one of the features I want for version 1.0 anyways, since I make custom cards. You can watch the repo on githib, I should get it done in a few months when I have time, so let me know if what I end up with works for you. 
&gt; that system allows us to express O(1) pattern matching too, right? I do not know of any work related to pattern-matching in the system. One of the main goals of the project is to explore the benefits of different kinds of encodings beyond Church. In the original paper I linked you, it talks about both Church and Mendler encodings. There's also work on Parigot encodings in the system. These produce semantically identical types with different computational behavior, some being more efficient than others. See [this](http://homepage.cs.uiowa.edu/~astump/papers/stump-fu-jfp-2016.pdf) paper. Beyond that, I can't fully answer your question. &gt; Also, the normalization proof is on the original CDLE paper, right? Because this system seems a little bit different (and simpler) than it. The normalization proof appears in [this](http://homepage.divms.uiowa.edu/~astump/papers/from-realizability-to-induction-aaron-stump.pdf) paper. If you're interested, you can listen to [this](http://typetheorypodcast.com/2016/12/episode-6-aaron-stump-on-cedille/) podcast, which talks about the changes to the system. To make a long story short, as it turns out you don't need as much to get induction as Stump initially thought. You can ignore the original CDLE paper since it doesn't reflect the current theory. &gt; I guess, they're used with kinds rather than types, and can be erased? &gt; Then, after talking about how ∀ is part of the version of CoC he starts with, he then says he will add implicit products to it, and once again uses the ∀ symbol. So, what is going on? The first ∀, standing for impredicative quantification, only allows quantification over kinded types. The second ∀, standing for implicit quantification, only allows quantification over typed terms. They are actually part of different grammatical classes in the formal grammar of CDLE, which is why the ambiguity is acceptable. Note that, when erasure is mentioned, this means erasure into an untyped lambda-calculus. Untyped means all types are erased, and since impredicative quantification quantifies over types, any type that's applied to a Λ (and the Λ itself) is erased. Implicit products are defined in such a way that their arguments and Λ are erased, but they're talking about terms, which can't be erased without implicit products. &gt; do I need it? You need it in order to get recursion. Take the following two encodings of the natural numbers; cNat = ∀(X : *) . X → (X → X) → X Nat = ∀(P : cNat → *) . P(0) → (∀(n : cNat) . P(n) → P(suc(n))) → P(x) That second encoding is used to derive induction. In that second encoding, that first ∀ is impredicative quantification, while the second is an implicit product. Let's consider the number 2 in both encodings; Λ(X : *) . λ(z : X)(s : X → X) . s(s(z)) Λ(P : cNat → *) . λ(z : P(0))(s : ∀(n : cNat) . P(n) → P(suc(n))) . (s - 1 (s - 0 (z)) Proving induction fundamentally relies on proving that every erased Nat is a cNat. So that first becomes `λ z s . s(s(z))` and that second becomes `λ z s . s(s(z))`. Without those extra `-` arguments being implicit, they couldn't have been erased, and we couldn't prove them identical by reflexivity. You can get dependent eliminators for non-recursive types without these implicit products (which is enough for sigma types, for instance), but that won't get you most examples that you'd want. &gt; {p} is always β which is just λx.x β is like refl but for the heterogeneous product. It is **not** `λx.x`, but it is erased to `λx.x`. The reason we have a variable {p} is the same reason there's an `e` in the type of the J-rule J : (T : *) (a : T) (b : T) (C : (a : T) (b : T) → Id T a b → *) → C a a (Refl T a) → (e : Id T a b) → C a b e even though identities only have one canonical inhabitant, being `refl`. For heterogenous equality, we also have the K-rule. &gt; Also a little bit confused about the role of `t − t ′` and `ρ t′ − t`. They are separate things. That first is implicit application, where that t' will be removed during erasure. The second is essentially equality elimination, where a term of type `P(x)` can be turned into a term of type `P(y)` if we have an identity between `x` and `y`. &gt; I'd absolutely appreciate if you could tell me exactly how many constructors there are [...] The system has two layers. The first is the untyped lambda calculus, which everything can be erased into. This just has lambda expressions. The second layer has; 1. Pi types with (lower-case) lambda introduction terms and an ordinary application eliminator that operates on terms. 2. Impredicative quantification (∀) over *kinds*, with (upper-case) lambda introduction terms and an application eliminator that operates on *types*. 3. Implicit quantification (∀) over *types*, with (upper-case) lambda introduction terms and an application eliminator that operates on *terms*. 4. Dependent intersection types (ι) over types, with an introduction rule denoted `[a, b]`, similar to pairing with sigma types, and two elimination forms `a.1` and `a.2`, similar to projection with sigma types. 5. A heterogeneous equality type (≃) with one introduction form `β` that witnesses reflexivity, and one elimination form ρ which implements what is essentially a transport of a predicate across an identity proof. Everything in the second layer can be erased into the first (see Figure 3 of the first paper I linked to).
interesting ... But will this result in worse compile time?
I have a split screen setup. I use neovim on one screen. On the other, I have a python gui that wraps a 'stack ghci' process and capture its input/output and display various kinds of outputs (after some processing in some cases, for example, I can toggle errors/warnings if I want to concentrate on either one of them). It looks like [this](https://snag.gy/ZnmbvI.jpg) The neovim sends a command to the python script via a tcp socket and uses the error lists that the script outputs to a configurable file. Using it I can navigate the error list using the vim's native process for the same. I can also send short arbitrary commands to the ghci, whos output will be shown in the gui itself. I can also get autocompletion and type info of the visually selected code using ghc's `type at` command. The biggest advantage for this set up is that, you don't have to worry about version compatibilities between various components involved. If you can get 'stack ghci' working in your project, then this will work. 
&gt; -XTypeInType with singletons is sufficient to encode any dependently typed program. I was skeptical of this, so I tried to write `Sigma` using `singletons`. It seems to work: https://gist.github.com/LightAndLight/c2e09008d74337f81f10042923368153
I use a similar setup. Just neovim instead of vim. Instead of a terminal on other, I have a [python script that runs a 'stack ghci' command](https://www.reddit.com/r/haskell/comments/7ksw96/how_is_your_haskell_work_environment/dri0uom/).. 
That's pretty cool, can you explain more?
Monad and all its constituents are gone, because they were not lawful. Applicative remains, but is a bit more awkward to work with until the kinks in ApplicativeDo are worked out.
The `format` sub-command exists, but it's not documented. (See [issue 2460](https://github.com/haskell/cabal/issues/2460).) Try `cabal format`. 
ah, right. I mostly use Applicative "syntax" (the `&lt;$&gt;` and `&lt;*&gt;` ) anyway. 
Do you have to do everything with lenses? filter (\ pet -&gt; pet ^? key "type" == Just "Dog") (json ^.. key "pets" . values)
Error messages do not come from Core. And by performance do you mean compiler performance or that of generated programs?
Compiler performance, but yeah, not error messages if there is still significant translation from fully dependent (source) haskell (again, these are vague thoughts). 
- https://github.com/haskell/cabal/blob/master/boot/Lexer.x has lexer definition - The outer grammar is specified in https://github.com/haskell/cabal/blob/76183b4ce6e9ffbd5246494fe560200d422313d9/Cabal/Distribution/Parsec/Parser.hs#L159-L206 - IMHO it doesn't use the best formalism, and can be improved - e.g using the one in https://dl.acm.org/citation.cfm?doid=2633357.2633369 - then the alternative outer parser can be implemented, using e.g. trifecta + http://hackage.haskell.org/package/indentation-trifecta or megaparsec (it seems it has similar combinators for above formalism) (based on my previous experiments, it shouldn't be longer than 100 lines) - I'll be very happy seeing a patch adding a test-suite, parsing all of Hackage + nasty examples, using built-in and more readable "spec" implementation, and comparing their results. - Inner, stanza + field grammars are specified declaratively in https://github.com/haskell/cabal/blob/master/Cabal/Distribution/PackageDescription/FieldGrammar.hs - Here too codified spec would be great, third (in addition to parser and prettyprinter) interpretation of `FieldGrammar` can be written: documentation generator (with hard-coded corner-case examples), so the parser can be tested against those examples. - Currently testing has to rely on roundtripping of parser/prettyprinter + known regressions. (and obviously all other tests which read .cabal files). - The glue part is unfortunately ad-hoc in https://github.com/haskell/cabal/blob/master/Cabal/Distribution/PackageDescription/Parsec.hs. Fortunately it's very simple. Nevertheless, should be documented. - With all that, one could produce a "Cabal-like file Grammar" document / chapter in user manual. That's one goal. All help will be appreciated. - Also, `stack` could start using `Parsec` class in Cabal-2.2 https://github.com/haskell/cabal/blob/76183b4ce6e9ffbd5246494fe560200d422313d9/Cabal/Distribution/Parsec/Class.hs#L39. It's understandable to avoid `ReadP`, but `parsec` should be fast enough. (Maybe at some point, we'll have Backpack http://hackage.haskell.org/package/parsers in use!) - The class itself is in-flight right now, so no yet documented. (the problem is how to describe fields which are parsed differently in different cabal-spec versions)
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [haskell/cabal/.../**Parsec.hs** (master → 76183b4)](https://github.com/haskell/cabal/blob/76183b4ce6e9ffbd5246494fe560200d422313d9/Cabal/Distribution/PackageDescription/Parsec.hs) * [haskell/cabal/.../**FieldGrammar.hs** (master → 76183b4)](https://github.com/haskell/cabal/blob/76183b4ce6e9ffbd5246494fe560200d422313d9/Cabal/Distribution/PackageDescription/FieldGrammar.hs) * [haskell/cabal/.../**Lexer.x** (master → 76183b4)](https://github.com/haskell/cabal/blob/76183b4ce6e9ffbd5246494fe560200d422313d9/boot/Lexer.x) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
It's undocumented because it doesn't work. It will work in cabal-install-2.2 (to be released around same time as GHC-8.4). It works in `cabal-install-head`.
&gt; beginner friendly projects that encourage open source contribution Here are [some Github issues](https://github.com/issues?utf8=%E2%9C%93&amp;q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22+haskell) that you could give a try. (There are some non-Haskell results too.)
By spec implementation you just mean something readable without necessarily being efficient? Yeah, I'll take a look, you should make a ticket with this information if you haven't already, so that someone who is more familiar with cabal might see it. 
And the backpack-ification of recursive descent parsing libraries is definitely exciting. &gt; Instances exist for the parsers provided by parsec,attoparsec and base’s Text.Read I'd like to do something similar to `parsers` for chart parsers, but there is such a broad range of providing sharing, beyond the monadic ones in Earley (bind create the reference) and Frisby (bind increments a counter for identifiers). I don't think they can all be abstractrd over, in some typeclass or.signature. https://hackage.haskell.org/package/Earley-0.12.0.1/docs/Text-Earley-Grammar.html#t:Grammar https://hackage.haskell.org/package/frisby-0.2.1/docs/Text-Parsers-Frisby.html#t:PM 
A question: If packages in this comment: https://github.com/fpco/stackage/issues/3044#issuecomment-351007435 had upper bounds, then the maintainers would be pinged two weeks earlier. Would then `either-5` entered LTS-10 or not?
To make it clear. GHC-7.4 and recent versions of dependencies aren't mutually exclusive. You can use https://matrix.hackage.haskell.org/package/lens with GHC-7.4. What should force you to use only newer GHC, are the features in GHC ifself, e.g. you want to use TypeApplications (GHC-8.0) or Data.TypeLits (GHC-7.8) or GHC.Generics (GHC-7.4) ...
[removed]
Thanks, @sjakobi . Will definitely give it a look.
Author of `-XTypeInType` here. I claim that today's System FC is expressive enough to represent any dependently-typed program using singletons. /u/vilhelm_s links to the paper (not mine!) proving this, in theory. However, as /u/sclv argues, this is only an encoding. The big problem with the encoding is that it will affect runtime performance, as a dependently-typed program would have to sometimes convert between an unrefined datatype and its singleton, at runtime. This is terrible. Furthermore, singletons are gross. See the [Hasochism](http://homepages.inf.ed.ac.uk/slindley/papers/hasochism.pdf) paper, which was essentially a 12-page plea not to just rely on singletons when implementing dependent types in GHC. From an implementation point of view, I don't think DC is simpler than FC. Yes, current FC does have duplication between terms and types, but this duplication is boring: it's the functions that, say, compute free variables and perform substitutions. However, implementing DC will undoubtedly require a few hairy algorithms to be implemented, which will raise the overall complexity (even if we can scrap the boring duplication present today). This isn't an argument against DC at all -- just my assessment of the challenge. As a last point, DC won't, by itself, clean up the extensions. That's all about the surface language, none of which interacts meaningfully with the Core language.
[And soon for `IntMap`s too!](https://github.com/haskell/containers/pull/454) :)
Two embarrassingly long-running patches to GHC. One to fix [#12919](https://ghc.haskell.org/trac/ghc/ticket/12919) and many (10ish) related bugs, and one to fix [#14066](https://ghc.haskell.org/trac/ghc/ticket/14066) and several (5ish) related bugs. The first patch was complete months ago, but it had a large regression on compile times of type-family-heavy code. I haven't had the time to fix the performance, but I will. The patch affects how GHC evaluates dependently-typed type families when the kinds of arguments are themselves type families (the way it does this in 8.0 and 8.2 is just plain wrong). The other patch uses implication constraints to better track which type variables are in scope where. This is currently done by various ad-hockery and is wrong sometimes. The new version will be right, in contrast. :) This is mostly working, but I'm still working through the testsuite. 
Very interested in dynamic code loading. Nothing I tried worked well, from existing libraries to the GHC API directly. Simon Marlow's `ghc-hotswap` doesn't seem to work on Windows, but I'm going to try it next week. 
No, there is nothing in Cabal 2.0 that has to do with double hyphens in cabal files. It is just HVR being obstinate. I hear that he has an automated tool that generates them, but that tool could easily change. The flag looks really ugly to me.
It was a bug in stack, and it is fixed. However, there is no good reason to deviate from the subset supported by older stack.
However, AFAIK this tool has not been changed, the package has not been fixed, and this is probably just the beginning of users running into the problem.
Also, my recommendation is to not spend any time struggling with Haskell on Windows. Just go with Linux or Mac if you have that option.
Heli beat me to it! https://hackage.haskell.org/package/queryparser https://hackage.haskell.org/package/queryparser-vertica https://hackage.haskell.org/package/queryparser-hive https://hackage.haskell.org/package/queryparser-presto
Not sure if its considered the best, but [Haste.App](https://hackage.haskell.org/package/haste-compiler-0.5.4.1/docs/Haste-App.html) is interesting. You actually write the server and client as one program with essentially two different main functions. You can define functions in the server side that get used by the client side, and Haste takes care of the serialization back and forth.
Any results to share ?
I'm using Neovim on Ubuntu with a few plugins, including `intero-neovim` that I help maintain. It's good stuff!
/u/vaibhavsagar and /u/ephrion, If you're curious about the result, see https://github.com/NorfairKing/wai-git-http
I wanted to try Haste out in the past! However I could not get it to build on my machine so I never got anything working :(
I don't :) but I was hoping for some "obvious" way of doing this which ended up with me digging in lenses for a disproportionate amount of time. Probably it's just me but I think Haskell is severely lacking in this area. I see why people think it's not a a good tool for quick prototyping. 
Thanks i will give this a shot. 
&gt;haskell-servant facilitates this Can you elaborate on this? It sounds super interesting and like a very good solution! I know that one can generate API docs, but how would one proceed to generate OTW models from DB models? I have not found anything touching upon that in the Servant docs! 
Given this is not the case, since Cabal (even HEAD) doesn't do anything special with the flag name (and if someone was talking about doing special with flag format, it would have likely been discussed on the issue tracker), it doesn't looks good for this "new tool" excuse.
How is `Beam` compared to Persistent? I am reading through the docs now but it would be nice to get a comment from someone having used it :) Can you point me somewhere I can find out how the `generic lens`-approach works? I have not used it before, but it sounds like it could perform the marshalling without me needing to write and maintain a function for each data type, which would be great!
I store all 3 in one repo under one folder. This way both reflex and haskell parts can refer to Shared folder rather than pulling an independent package, and I do not duplicate the code. Initially we also split front and backend to separate repos, but then quickly realized that such divide is superfluous. Smallest change in one repo usually comes with its counterpart change in the other. It really is one application. 
It looks like this is the way I'm heading at the moment :) How do you do the marshalling between DB model and business model? On this course the problem I want to solve is having to write and maintain a new function for each marshalling
Thanks you for your response. I will try to find a smooth way to marshall between DB models and shared models, and hopefully that will work out nicely. I'm super grateful to know that this can be done though!
That is a good approach, and the one I am using as well. The only part I am confused about is how to best declare the models. Where are your models declared? Do you have several copies of the same model for db/client? 
Not sure if one exists already, but it is definitely feasible to create it. 
Curiously, the most time of the compilation isn't spent in the type checker, but in the simplifier... Compiletimes with -O2 are quite noticeable.
It's almost like some people make irrelevant changes for the sole purpose of making Stackage look bad...
The list is surprisingly long those days, and this is probably a bit too easy that a package of yours has gone there and forget. Hopefully it can be slim down at some point with some effort (maybe a flag day to fix your skipped-tests !). As to maintainers that consciously put or/and hold their packages there, this is for sure completely harmful.
[removed]
Absolutely agree. Like it or not, Stackage has become a de facto standard in the community, and respecting this should take precedence over any personal "matter of principle". I would also suggest that for popular/core libraries no single person should be able to make such deliberate interface (flag format) changes, or exclude anyone else from fixing design issues.
Combining complex lenses doesn't seem conducive to making a prototype to me. If you feel the library is lacking you could always expand it yourself.
Port the cabal file to hpack ( https://github.com/sol/hpack ) and stop maintaining it manually.
This is amazing, can't wait until it's fully working. From the post, it sounds already quite usable, except for this: &gt; my advice is to unpack or add submodules for dependencies that require cabal to run `hsc2hs` or `./configure` What does "unpack or add submodules" mean? There are a *lot* of common dependencies that require cabal to run `hsc2hc` or `./configure`.
Here you go: https://github.com/haskell-servant/example-servant-persistent/blob/master/src/Models.hs You can change the From/ToJSON instances to match your OTW needs. You could even combine several models into one OTW JSON if you need. &gt; how would one proceed to generate OTW models from DB models? So it's not fully "auto generated", but i argue its often not what you'd want. There is a way to have Aeson generate your From/ToJSON instances for you at compile time. More about that here: https://hackage.haskell.org/package/aeson-1.2.3.0/docs/Data-Aeson.html
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [haskell-servant/example-servant-persistent/.../**Models.hs** (master → ee65996)](https://github.com/haskell-servant/example-servant-persistent/blob/ee6599612bf14f5f77ea8ca18cca35124ae3d6e3/src/Models.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
I'm not complaining about the library. I'm just stating the same task is easier in other languages :) Prototype == getting the fasted way possible to a usable result. Haskell never got in my way before, it did today. That's my personal opinion and probably my personal problem as I stated above. Read what you want in between the lines :) 
From what I can see, GHC isn't afforded similar considerations by the various libraries upstream of it, and they deal with these kinds of issues by keeping an eye on what is going on upstream / occasionally participating in upstream development. This includes Haskell libraries (the boot libraries) and things like LLVM. Ideally all projects would be mindful of not breaking things for their users, but I'm struggling to think of another situation where it would make sense for a downstream project to assert that their needs should take precedence over the principles / priorities of the upstream project.
The android tarball unpacks to only about 0.5 GB. I'd certainly be willing to devote that much space on my SD card to have an onboard GHC I could use in termux. How hard would it be to "dogfood" this and get an android tarball that runs on android? Of course, in that case it would be even better if GHCi would also work.
The task becomes much easier when you're not trying to manipulate JSON, which is probably why this was so hard. Is there a reason not to load the array in to a list of converted types and filter than instead?
&gt; I ​sent out an e-mail to LLVM-dev, asking if it is possible to fake GHC-like "custom call stack on the heap" within LLVM. I am not sure if you are already aware of it but someone is [working on support for custom stacks in LLVM](https://icfp17.sigplan.org/event/hiw-2017-native-support-for-explicit-stacks-in-llvm).
Honestly man using `filter` is pretty darn obvious. The second I looked at your problem `filter` jumped out at me as an obvious solution. A poor workman blames his tools and all that.
So DC is about runtime performance of generated programs. This clears things up, thank you!
I mean it seems to me as though the obvious approach to wanting to filter something is going to be `filter`, it would have been the first thing I reached for. I really don't think it's fair to criticize Haskell's ability to quickly prototype based on your issue here.
&gt; Probably it's just me but I think Haskell is severely lacking in this area. I see why people think it's not a a good tool for quick prototyping. I'm sincerly curious what feature of other languages do you have in mind, as most languages don't even have lens?
Sure it's not fair, but I can see why people tend to think so. Never happened to me before. Again personal opinion. feel free to ignore.
I'm not criticizing lenses or anything. Don't mean to offend anyone, or Haskell itself. I've been doing great with quick prototyping in Haskell up to now. This time I hit what was for a me show stopper. I can see the same experience stopping other people too. How about: (json["pets"] as? Array&lt;JSONValue&gt;)?.filter{where: $0["type"] "Dog"}
Please stop being part of the problem. Thanks.
Well, as mentioned in the linked cabal issue, cabal does not push flags into dependencies, as this would produce different hashes and as such would break down at non-reinstallable packages (or that's my current understanding). However cabal will push down flags into dependencies that are source local. As such if you unpack or add a dependency as submodule, cabal will push down the `--with-hsc2hs` and `--cross-compile` flags. I consider this a rather severe defect. The best solution so far seems to be to just whitelist non-reinstallable packages, and pushing down flags by default. But I haven't gotten around implementing this in cabal just yet.
Here it is - https://github.com/binarin/personal-finance/blob/master/lib/UILive.hs
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [binarin/personal-finance/.../**UILive.hs** (master → 26bff2d)](https://github.com/binarin/personal-finance/blob/26bff2d652663c884a39a11ab9a6c23ea92dc606/lib/UILive.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
I'm not sure I understand what your intention is here. Do you want to run GHC on your Android phone in termux? In that case you are not looking for a cross compiler, but for a regular full stage2 compiler that run on and targets the same host. And yes, you could probably build that with the cross compiler in principle. TH might need some additional hackery though.
How about f key = over (at key) (maybe 0 (Just . (+1))) 
Wow beam looks really nice 👍
But if it's never happened to you before, then clearly Haskell has been going quite well for rapid prototyping so far, seems weird for one issue to flip you so hard.
Yes. I apologize if my complaint came across as hard. I love Haskell so much that I go out of my at work to promote it. Nothing of the following happened but in my mind the scene played out something like this: Boss: Let's give Haskell a shot. Please to this small task and sees how it plays out. Also be sure to explain to other developers what you are doing. Me: Sure * struggles a disproportionate amount of time to extract trivial information from JSON * * can't fully explain what is going on to myself, let alone others* Me: Yeah... actually... 
The example corresponds to not using lens. Which is what they were talking about. I don't understand what you mean. 
Equality saturation on term rewriting systems for automatically optimizing specifications of distributed algorithms. The idea is that you write down a term rewriting system whose type of terms represents the combined state of every node in your distributed system. Then you annotate the rewrite rules with weights (for probabilistic situations), times (to model the fact that, e.g., sending a message over the network is slower than ticking the program counter forward once), and optional labels (labelled transitions represent "observable" transitions that cannot be optimized out). There are a few basic equality analyses like transitivity, plus you can ask the user for equalities on terms that can be used as additional equality analyses. Bidirectional transformations [are inhabitants](http://groups.inf.ed.ac.uk/bx/TyDe16.pdf) of bisimulations, and I suspect bidirectional transformations are recursively enumerable, so you might be able to have an equality analysis that does superoptimization by enumerating bidirectional transformations of a given type. I think you actually want a coarser form of equivalence between term rewriting systems: [testing equivalence](https://www.sciencedirect.com/science/article/pii/030439758790065X), but I'm not sure if there's a way to recursively enumerate testing equivalences. The main utility function I can think of for this is some kind of bounded Monte Carlo simulation to compute observable messages per unit time or something like that. I'm kind of curious what /u/edwardkmett thinks of this, since I originally found out about equality saturation from a comment of his from like 4 years ago.
I could send you a link to my already published paper. But most people (including our reviewers) didn't understand the new papers yet. So I first have to have complex examples as reviewers complaint that there is no proof that it "scales to real world applications" and also that the description of the approach is too technical for people to understand. My view currently is that FRP is not the solution but *part of the solution*. Current approaches in Haskell fail in my eyes because with FRP you can't build GUI systems that are as nicely designed (from a systems standpoint) to the current state-of-the-art (with is anyways unknown in the Haskell world, I guess). I will contact the Fudgets authors, thanks again for mentioning them (I already knew them, but anyways). If anyone would like to cooperate on some part of GUIs, please write me a DM. If you are very good at Haskell that is all that is needed. For instance, implement machine learning for hand writing recognition. I would like to also support pen input from graphics tablets. 
If anyone would like to cooperate on some part of GUIs, please write me a DM. If you are very good at Haskell that is all that is needed. For instance, implement machine learning for hand writing recognition. I would like to also support pen input from graphics tablets. 
This is a very nice example! Thanks! 
Thank you. This is as good as it gets
I have some Typeclass and instance declaration class (Eq aState, Show aState) =&gt; Game aState where initialState :: aState data TicTacToe = TicTacToe {board :: Board, currentPlayer :: Player} deriving (Eq,Show) instance Game TicTacToe where initialState = TicTacToe (replicate 9 Empty) X When using "initialState" ghci cannot derive an instance of show althoug the TicTacToe Intance does exist. *Main&gt; initialState &lt;interactive&gt;:31:1: No instance for (Game a0) arising from a use of ‘it’ The type variable ‘a0’ is ambiguous Note: there is a potential instance available: instance Game TicTacToe -- Defined at TicTacToe.hs:43:10 In a stmt of an interactive GHCi command: print it 
Thanks!
And it's just the Right Way to do it. But if being Right isn't enough of a reason, then, yes, you could cite performance as a reason. :)
You mention &gt; not all (&gt;&gt;=) and (&gt;&gt;) can be do’ed Can you explain what you mean by that? Any `mv &gt;&gt;= f` can be translated to: do v &lt;- mv f v And any `mv &gt;&gt; f` can be translated to: do _ &lt;- mv f
Thanks, that explains the problem in more detail. But I was asking about the work-around. What does "unpack or add submodules" mean? How do I do that? Does "unpack" just mean unpack the tarball, and if so, what do I do with it after it is unpacked? And what is a "submodule"?
Yes exactly, working ghc and ghci on an android device (tablet in this case). I have a feeling it would require much more than "TH hackery". Seems to me the entire GHC build system is designed to work only on specific target platforms, and just having an existing GHC for a different platform is far from enough. Isn't that the whole point of this project?
I did, but first it's stack only, and second, it has some really annoying UI properties. My development flow is basically write a type signature, make a hole, type check, refine the hole, type check, refine, etc., and Intero doesn't have a "type check" button - `C-c C-l` actually moves my cursor to the repl buffer, which was really, *really* annoying. No, I don't want to try to run my `_`-ridden code in a repl!
In motive, sure. But in terms of the problem it generates for the Haskell community, they're the same: they generate a confusing number of incomplete options instead of contributing back to make one option complete.
If you do want to find out how to do this with `lens`, it looks like [this may help](http://hao.codes/haskell-advent.html)
atom + ghc-mod + terminal window + nix for haskell packages
That's great! Thanks!
I only have 2 ideas at the moment: 1) implement the [Fudgets Demos](http://www.altocumulus.org/Fudgets/demoform.html) and 2) implement [Hoodle](https://github.com/wavewave/hoodle). 
I haven't heard of Validation before. What is it?
This is great! It's the only JSaddle tutorial I can find on the Internet! We definitively need more tutorials on JSaddle. Many thanks! 
My work on GUIs is about: * "math wins" instead of the technical depth (i.e., outdated, badly designed code such as wxWidgets or GTK) * "math wins" allows very concise code * very generic programming (e.g., program once, get desktop and web GUI for free) 
A GUI for [pandoc](https://pandoc.org) would help make it accessible to people who fear the command line. And the interface is already built: the GUI would just need to build an `Opts` structure and call `convertWithOpts`. 
Nice idea! I fear that people will say "Oh, that's a trivial GUI, doesn't scale".
The problem is, that `initialState` has type `Game a =&gt; a`, meaning that it works for all types `a`, which have a `Game` instance, and ghci does not know that you want `a` to be `TicTacToe`. In your particular case, there is only the `TicTacToe` instance for `Game`, so `TicTacToe` would the only valid choice for `a`. But usually using a type class makes only sense if one has multiple potential instances in mind, so let's assume there's another instance data FooBarBaz instance Game FooBarBaz where … then from just writing `initialState` it's not clear whether you wan't something of type `TicTacToe` or of type `FooBarBaz`. To specify this information you can write the expression `(initialState :: TicTacToe)` instead of `initialState`, which resolves the ambiguity. Often it is not necessary to specify the type, as it is clear from the context (rest of the program), but by entering `initialState` into ghci, there is no context and `initialState` is basically your whole program, so you must provide this information yourself. Note that in other cases it's perfectly fine to work overthis problem only arises, because `initialState` is not further restricted through its context, by which I mean that when you enter `initialState` in ghci, then `initialState` is basically your whole program
Have you _seen_ JS "wat"s or C undefined behavior? There's quite a bit of complexity lurking, ready to teleport your foot into the path of your bullet. The difference I find with Haskell is that these surprises are actually useful (and not that surprising after a while).
It sounds like you want a GUI that shows that your approach to GUIs works well. I would definitely want to see a) How well it handles inserting/deleting elements in a large list/table/ widget (does the performance scale) b) How confusing it gets to debug events triggering other events,... (think button that triggers action that disables the button, performs an action (e.g. a new version of some server-side data), fills a widget elsewhere with the result, re-enables the button,...) before I was convinced that it is suitable for general GUIs. Maybe something like a GUI to display monitoring data from Livestatus ( https://mathias-kettner.de/checkmk_livestatus.html ) might be good to show off both of those. Alternatively some sort of remote file browser or perhaps an LDAP client showing the LDAP objects.
It will be difficult because, ideally, the common case is just a "select file", (optional) select input format", and select output format" However, customizing things should be able to be done without dropping into the cli... The tricky bit is going to be hiding that complexity, yet making it easily accessible, I think :)
Wow! This is really helpful to push the project further.
Dear H., if you have any ideas for good GUI demos that show real-world scalability, please check out: https://www.reddit.com/r/haskell/comments/7l203h/what_haskell_programslibs_need_a_gui/
In case you have an idea to show real-world scalability with GUI Demos, please check out this follow up post: https://www.reddit.com/r/haskell/comments/7l203h/what_haskell_programslibs_need_a_gui/
In case you have ideas for GUI programs that show real-world scalability, please check out: https://www.reddit.com/r/haskell/comments/7l203h/what_haskell_programslibs_need_a_gui/
I love the idea of declarative GUIs. Have you seen purescript-flare? https://david-peter.de/articles/flare/ 
It is basically Either, except that instead of stopping upon hitting the first error, it will accumulate all of the errors. you can find it in edkmetts either library. A practical use for it is building a smart constructor. e.g. mkVehicle :: String -&gt; String -&gt; Validation Errors Vehicle mkVehicle model year = Vehicle &lt;$&gt; mkModel model &lt;*&gt; mkYear year mkModel model = case model of "Civic" -&gt; Success model ... _ -&gt; Failure "Invalid Model: " ++ model mkYear year = ... -- Check if year falls between 2013 and 2017 If I were to call the function with `mkVehicle "Blerg" "300"` I would get back 2 errors. You can also use `&lt;*` and `&lt;$` to check for more complicated rules, such as: validateVehicle :: Vehicle -&gt; Validation Errors ValidatedVehicle validateVehicle (Vehicle model year) = ValidatedVehicle model year &lt;$ isVehicleCombinationValid model year -- Checks to see if that combination of vehicle and year makes sense. &lt;* someOtherRandomTest model year You can convert these into sum types for even better safety if thats viable for the domain, but I'm just too lazy to type them out for the example. A real world example of using this would be to validate a user filed out form. If they forgot to fill out all of the required fields, you can return a list of all empty fields, instead of just 1 field at a time like you would with Either.
This stack package from AUR seems to work fine: https://aur.archlinux.org/packages/stack-bin I'll update the wiki.
I guess I'm old school. I just use terminal vim with a few plugins and customizations but nothing crazy. I pretty much always use it in conjunction with tmux. I'll always have a web browser open to documentation as well.
I don't think data FooBarBaz instance Game FooBarBaz where would be allowed, because the class definition of Game a has a context that requires a to be part of the Eq class. But FooBarBaz is not. Moreover, initialState should be of type (Eq a, Show a) =&gt; a -&gt; String, right?
I do not use persistent. The Shared folder is only for wire transfer between backend and frontend. I just code sql by hand. Never had a problem with this approach. And it is not only simple but also very flexible. As for the potential typos and errors I'm willing to deal with them in this particular instance for the sake of simplicity. 
Interesting, thanks!
If moving cursor to other window is your only issue with intero, you can ask on github project page for a switch to stop doing it. Then you could set that switch and for you the C-c C-l would not move the cursor.
Maybe, but I don't wanna be "[that guy](https://xkcd.com/1172/)." I'm happy to report actual bugs, but I don't want to barge in and ask for functionality to cater to my wishes. In any case, upon some further inspection, it seems a lot of dev tools have had difficulties updating to handle GHC 8.2.x. I tried the Atom IDE stuff, and it all wanted 8.0.x as well. I guess there were just some big changes in compiler messages and the ecosystem needs time to catch up.
[Image](https://imgs.xkcd.com/comics/workflow.png) [Mobile](https://m.xkcd.com/1172) **Title**: Workflow **Title-text**: There are probably children out there holding down spacebar to stay warm in the winter! YOUR UPDATE MURDERS CHILDREN. [Explanation](https://www.explainxkcd.com/wiki/index.php/1172) **Stats**: This comic has previously been referenced 7 times, 0.3381 standard deviations different from the mean ______ [xkcd.com](https://xkcd.com) | [xkcd sub](https://np.reddit.com/r/xkcd) | [Problems/Suggestions](https://github.com/benreid24/xkcd_bot/issues) | [The stats!](http://xkcdredditstats.com) 
I've made a [toy chat server](https://github.com/soupi/msg) a while ago. Maybe you'd like create a gui chat client for it?
Ah, thanks ! I didn't know about that :)
`at key . non 0 +~ 1`, run over an uncontained key, modifies its value to 1. Note that decreasing its value to 0 using this lens deletes it from the map.
Is this specific to an increment? I'd like to be able to apply any modification `a -&gt; a` function to the existing value if it exists!
It isn't. `non x :: Eq a =&gt; Iso' (Maybe a) a` replaces `Nothing` by `x` going in, and `x` by `Nothing` going out, so `\d f -&gt; at key . non d %~ f` can be used as `Int -&gt; (Int -&gt; Int) -&gt; HashMap Text Int -&gt; HashMap Text Int`.
These slides are so pretty! I may have to overthink beamertex...
I didn't understand most of the code on the slides but the premise is spot on. I think Uncle Bob's perspective is unfortunately pervasive however. Is there a transcript to go along with these slides somewhere? Perhaps I'm missing the exposition for mere mortals.
That doesn't seem related. Laziness is still useful with inductive types. Having lazy inductive types is useful. 
@nicolast is talk record available somewhere online?
&gt; Moreover, this conflation and leaky non-termination cripples a lot of potential optimization, since reordering things in the presence of non-termination can change the meaning of your program. It's not even safe to do something as simple as rewrite the functor law: (fmap g) . (fmap h) = fmap (g . h)! It's also a major reason supercompilation is impossibly difficult instead of relatively straightforward. This is a result of seq not being behind a type class. &gt; The downside is it does place a slight burden on the developer -- [1..5] and [1..] are no longer the same kind of thing (although arguably they never were, it just wasn't represented in the type system). But in my experience it's not that painful in practice. Yes they can be. They can be potentially finite coinductive lists. &gt; It's unlikely that any of this will change in Haskell, but still, it's good to be aware of the cost for playing sloppy here. There is actually a path for changing this. We would just have to add a way to talk about unpointed domains and within those you'd have your conductive/inductive types. As an added benefit, we'd get way stronger free theorems.
I particularly love the fonts. What fonts are those?
An interactive Haskell debugger would be great. The timing feels great when something like [the debug package](http://neilmitchell.blogspot.com/2017/12/announcing-debug-package.html) is trending on [this very subreddit](https://www.reddit.com/r/haskell/comments/7kjf5g/announcing_the_debug_package/?st=jbfgteh8&amp;sh=06493b02) :). A related idea would be to create a nice UI for the REPL. There have been a couple of projects like this (IHaskell using Jupyter and [Hyper Haskell](https://github.com/HeinrichApfelmus/hyper-haskell)) but it's always interesting to see alternate takes on the UI design/implementation.
(Just want to say thank you very much, that made perhaps 99% of this thing absolutely clear to the point I think I can implement it. Don't have much to add, I'm making a lot of progress here.)
I think they mean to vendor the dependencies, like a git sub module, but idk
I'd just use functions. I don't think you can really avoid it: sometimes your over-the-wire representation will correspond to some number of data models in your back end representation - sometimes you'll want to leave sensitive fields out. There isn't a principled way of deriving one from the other.
Haskell on Windows is good. I don't know how autohotkey does it, but doesn't seem too hard. I was looking at adding hotkey registration to workflow-windows (which provides the "right hand side" of the key bindings, ie keyboard shortcuts, clipboard access, mouse clicks, Etc). https://github.com/sboosali/workflow-windows https://github.com/sboosali/workflow-types https://msdn.microsoft.com/en-us/library/windows/desktop/ms646309(v=vs.85).aspx Also, file an issue so I don't forget about it, and so you can track it. I was definitely going to get around to it in the next year. 
Oh, I'm refactoring the workflow packages, so you should file the issue here https://github.com/sboosali/workflow The goal is to provide some portability by taking an intersection of different operating systems and window managers' features. But there are significant differences: like the standard keyboard, the standard mouse, dealing with Ctrl + Alt on Windows and Linux versus Command and Ctrl on OSX, dealing with OSX's "group windows under an application" model versus the "each window is separate, but they may have the same executable", etc. 
It would be great if someone wanted to pick up development on it!
The example with filtering is how I'd do it, and isn't the equivalent in other programming languages (maybe with a loop) how you'd do it too? 
I think a really good base would be to start looking at algebraic representations of graphs. We have a few really cool posts and stuff that was linked on the subreddit in the last week about that. There were also some pretty good demonstrations of traversal algorithms implemented using those representations; that might be illuminating. As it is, your graph representation is what stands out the most to me as being weird, but that's only a gut feeling and I'm not quite sure how to pin that down further from there.
Yes, our work is declarative GUIs. But one thing we would like is not to have "web based GUIs" or "desktop GUIs" or "html layout" or "desktop layouts". But to have generic, declaritive GUI definitions where the rest (generating web based GUIs/html, etc.; generating desktop Apps) is done automatically by the library. The hard part that we want to contribute is how to have really static type safety in the presents of such generic programs. For instance, wxHaskell doesn't have static type safety as there are "unsafe" coercion that in case of type errors throws runtime exceptions. 
I would like a link to your paper as well.
It would be interesting how it handles using custom widgets in a very large list/table. Does it create all widgets simultaneously or lazily on demand?
This is great news, and it should make platform distros much nicer too!
Very nice, looks really promising. Going to file an issue tomorrow as soon as i can
Works perfectly. Thanks again!
sweet. btw, it's currently pretty messy and underdocumented, but I've been using it daily, and I'll clean it up whenever it gets released. 
- IP address: [`remoteHost`](https://www.stackage.org/haddock/lts-10.0/wai-3.2.1.1/Network-Wai.html#v:remoteHost) - MAC address: can't get it from HTTP - Browser and OS: [`requestHeaderUserAgent`](https://www.stackage.org/haddock/lts-10.0/wai-3.2.1.1/Network-Wai.html#v:requestHeaderUserAgent)
For context, b) can get really confusing with things like Qt's signal/slot system if you make slots trigger extra signals a few times in a row since there is (or at least was when I last used it in Qt 4) no real way to inspect the signals that are firing with a regular debugger, you get no equivalent of a stack trace to give you context for debugging.
That does sound like it has its uses. Thanks for the explanation.
Anyone able to get [this](https://marketplace.visualstudio.com/items?itemName=phoityne.phoityne-vscode) debugger for VS code to work? I couldn't even get the sample project to work and even if I did I'm still not sure how that would help me with my projects.
You're welcome! I think lens-aeson could benefit from some "higher-level" examples of how to search within a value, and also of using the `Plated` instance to rewrite / transform whole trees. By the way, `Fold`s from lens are monoids (though it's not obvious) so you could do something like a^..key "pets".values.filtered (has (key "type"._String.only "Dog" &lt;&gt; only "Fish")) to keep both dogs and fishes. In any case, `filtered` accepts a regular predicate, so we could build it in non-lensy ways if needed. 
If there's a talk for this, I hope it's at least 1000 hours long in order to explain the slides in a way I can understand :)
Higher lever examples would be great. Thanks for the monoid tip, it's great! 
Wavelets are very common in image feature extraction where a suitable wavelet is selected and then only the wavelet coefficients are fed into the neural network (or whatever machine learning approach one is useing). https://en.wikipedia.org/wiki/Discrete_wavelet_transform#Example_in_Image_Processing
You are probably right. It just feels cumbersome to manually put all that together. I'll bite the bullet and get it done though. Thank you for your input!
Brutal
Nice! Would anyone know where I can find the changelog (major features and changes) either for the current alpha release or what is expected to be the changelog for the full release? In theory these two are rather similar, aren't they?
I still think that using filter should be the first thing considered when shown this problem, and thus there shouldn't be any struggling. I don't know what Haskell could have done to make this easier, you were given `filter` and the necessary lenses, what more is needed?
Do you think you could help me set up atom for haskell use? I'm just starting to get into Haskell and would really appreciate it!
http://todomvc.com/ Personally I'd be interested in the above mainly, I think. Apart from much simpler hello-world/introductory examples. TodoMVC is an approach with which I could compare with many already existing GUI frameworks, e.g.: - reflex-frp: Live: https://tolysz.github.io/reflex-todomvc/ Source: https://github.com/reflex-frp/reflex-todomvc/blob/develop/src/Reflex/TodoMVC.hs - miso: https://github.com/dmjio/miso#todomvc
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [reflex-frp/reflex-todomvc/.../**TodoMVC.hs** (develop → cd15a37)](https://github.com/reflex-frp/reflex-todomvc/blob/cd15a37b0e6decf42840967ce5fba6a03cf278fa/src/Reflex/TodoMVC.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply drjjf41.)^.
Yes. In other words, a lion share of the work might be UX and design related instead of just encoding a UI.
Yes indeed! I'd so much like a GUI framework that utilizes haskells already in-build laziness to achieve snappy-by-default UIs! E.g. evaluating most GUI-related thunks on a secondary background thread. I am so surprised that current HS UI solutions are strict-by-default: I've tried reflex-dom, brick, threepenny-gui, to name a few. Maybe just no one attempted to work out the semantics yet?
The fledgling [release notes](https://downloads.haskell.org/~ghc/8.4.1-alpha1/docs/html/users_guide/8.4.1-notes.html) are a good place to start.
[The user's guide](https://downloads.haskell.org/~ghc/8.4.1-alpha1/docs/users_guide.pdf) has a changelog entry.
Yes please! A well-featured GUI debugger would be most appreciated. :) I'm going off on a tangent here, but could it be possible to perhaps reuse Google Chrome's Developer Tools as a Haskell debugger? It already supports connecting to a remote socket for debugging, and it supports source maps as well, so it's not like it could only work with Javascript. Maybe we can fool it into being a Haskell debugger. And maybe it's much easier to connect these two than having to write the GUI from scratch?
That seems like odd reasoning. If I create a project that depends on stack, will I get to be the one that gets to decide whether changes in stack are made for good or bad reasons?
one of the difficulties of Haskell is that there's _always_ a way to get it done with sufficient type-hackery and template haskell, and not always a lot of drive to make things simple - it's pretty easy to end up writing quite a lot of type-astronaut code for something that would be minimally longer written simply and directly.
As the suffix "alpha" implies, this is a very bleeding edge release with very little guarantees regarding API stability (c.f. [new GHC schedule](https://ghc.haskell.org/trac/ghc/wiki/WorkingConventions/Releases/NewSchedule)). Put differently, a package that works now with GHC 8.4.1-alpha1 may not necessarily work with the final GHC 8.4.1 release. In order to support early adopters in testing GHC 8.4.1-alpha, there's a new [Overlay Hackage Package Index](https://github.com/hvr/head.hackage) which provides packages patched for unreleased GHCs (currently GHC 8.4.1-alpha1 &amp; GHC 8.5/HEAD). See its [README](https://github.com/hvr/head.hackage#README) for instructions on how to use it; there's also a shell script included which automates common workflows. Don't hesitate to ask if you have questions! As usual, there's already an (incomplete &amp; work-in-progress) [GHC 8.4.x Migration Guide](https://ghc.haskell.org/trac/ghc/wiki/Migration/8.4) you can consult and maybe even help complete.
+1 on this. The `head.hackage` overlay has been incredibly useful in smoke-testing the release thusfar.
i can't advocate this suggestion enough, i struggled with haskell on windows for quite some time.. although today it is WAY better than it used to be so whoever has actually been working on that i salute thee
What exactly do you mean by "Uncle Bob's perspective"? He seems to have a lot to talk about, after all... Regarding some of the code: * Slide 9: `GeneralizedNewtypeDeriving` is ridiculously powerful. You could add `deriving IsString` to use string literals with `OverloadedStrings` for instance, while still staying type safe and being guaranteed the same runtime representation (`Text` in this example). * Slide 10: Note how the `a` doesn't show up on the right side. However, we can cheaply define new types of messages, or define utility functions that work for all `Message a`. I use phantom types all the time and found they help me catch a metric ton of logic errors at compile-time. By the way, `Message a` could be a `newtype` as well. Note however that some goofus could come along and define a `Message Int`. * Slide 11: No idea what's going on here. Still, uninhabited types are useful, for instance to indicate that a function [won't return](https://www.fpcomplete.com/blog/2017/07/to-void-or-to-void), or in the context of the Curry-Howard correspondence, which is how proofs are done in a dependently-typed language like Agda. There, you would define a predicate or relation to not return a Boolean, but instead a type which is either inhabited (typically the unit type, corresponding to truth), or uninhabited otherwise. Lots of uses for `Void`. * Slides 12-14: Remember `Message Int`? It turns out we can define a closed set of types which are accepted as a type variable, this is what `DataKinds` and friends is all about. For instance, `Driver a` would actually stand for `Driver (a :: *)`, where a is of "the standard kind" so to say. However, using `DataKinds` we get `Sober` and `Drunk` *values* and *types* (indicated by the apostrophe), which are of *kind* `Sobriety`. Thus we can restrict what types can stand in for `a`. * Slide 13: Generalized Algebraic Data Types are so-called because they allow to be more flexible about the return values of data constructors. Example: -- Requires GADTSyntax. These just make the types of -- value constructors more explicit. data Maybe a where Nothing :: Maybe a Just :: a -&gt; Maybe a -- Requires GADTs. In a function that takes this type as -- argument, you can now pattern match on types. data MaybeNum a where Nothing :: MaybeNum a JustN :: Int -&gt; MaybeNum Int JustD :: Double -&gt; MaybeNum Double * Slide 15: You can regard type families as functions which map types to types. In my opinion, it's easy to go overboard with these, and they can easily make APIs unwieldy. The [wiki](https://wiki.haskell.org/GHC/Type_families) knows more. I find it useful to use `kind!` when working with type families, that is: &gt;&gt;&gt; :kind! TypeOf TInt TypeOf TInt :: * = Int The `Refl` constructor is short for "reflexivity", which is how you construct "proof objects" in dependently typed languages to show that the left-hand and right-hand sides are "the same" in a propositional sense (which is in contrast to definitional equality, which we define using `=`). Here, `Equal` is a proof object that two types are the same. That is a weaker notion of what can be proven "by reflexivity" in a dependently-typed language, where *types* can depend on *values*. * Slide 16: This is where "singleton types", that is, types which are only inhabited by one value, come in. This is how we can "fake" dependently-typed programming (to some extent!) in Haskell, for instance using the [singletons library](https://www.stackage.org/nightly-2017-12-20/package/singletons-2.3.1). Justin Le has written a series of blogposts about dependently typed programming in Haskell, for instance [this one](https://blog.jle.im/entry/fixed-length-vector-types-in-haskell.html). Truth be told, just about every proponent of this programming style probably has experience in languages like Idris or Agda or whatever, and if you're interested in this kind of thing I would have a look at them. In my opinion, at least in 2017, learning dependently typed programming in Haskell is like learning functional programming in C++.
You don't necessarily have to encode all of the CLI interface as a GUI interface. Perhaps you just build the common/simple options, and if a user is reliant upon it enough to want more than the GUI offers, perhaps the CLI wouldn't feel so scary for them at that point because they are already familar with how pandoc functions at a high level. Git tools are a great example of this. Most of them only encode a small core subset of functionality, and if you need advanced features, you are expected to learn the CLI.
With that quote Uncle Bob reminds me of Rich Hickey – the guy never saw a proper type system but proceeded to bash static types nonetheless.
As /u/spirosboosalis noted, I meant git submodules. I should have been a bit clearer about this here. `cabal unpack &lt;pkg&gt;` will unpack the given package in the current directory. Similarly adding a git submodule, would put the packages source into the current directory. As such cabal considers them source local and will forwards flags.
What exactly do you mean by "Uncle Bob's perspective"? He seems to have a lot to talk about, after all... Regarding *some* of the code: * Slide 9: `GeneralizedNewtypeDeriving` is ridiculously powerful. You could add `deriving IsString` to use string literals with `OverloadedStrings` for instance, while still staying type safe and being guaranteed the same runtime representation (`Text` in this example). * Slide 10: Note how the `a` doesn't show up on the right side. However, we can cheaply define new types of messages, or define utility functions that work for all `Message a`. I use phantom types all the time and found they help me catch a metric ton of logic errors at compile-time. By the way, `Message a` could be a `newtype` as well. Note however that some goofus could come along and define a `Message Int`. * Slide 11: Uninhabited types are useful, for instance to indicate that a function [won't return](https://www.fpcomplete.com/blog/2017/07/to-void-or-to-void), or in the context of the Curry-Howard correspondence, which is how proofs are done in a dependently-typed language like Agda. There, you would define a predicate or relation to not return a Boolean, but instead a type which is either inhabited (typically the unit type, corresponding to truth), or uninhabited otherwise. Other examples include its use in the `pipes` and `conduit` libraries, to indicate that all inputs and outputs have been handled (together with `()`), or for instance this interesting use in the context of [continuation passing style](http://www.haskellforall.com/2012/12/the-continuation-monad.html), where you can check that a chain of continuations has "no more holes left" if the return type unifies with `Void`. Lots of uses for `Void`. * Slides 12-14: Remember `Message Int`? It turns out we can define a closed set of types which are accepted as a type variable, this is what `DataKinds` and friends are all about. For instance, `Driver a` would actually stand for `Driver (a :: *)`, where `a` is of "the standard kind" so to say. However, using `DataKinds` we get `Sober` and `Drunk` *values* and *types* (indicated by the apostrophe), which are of *kind* `Sobriety`. Thus we can restrict what types can stand in for `a`. * Slide 13: Generalized Algebraic Data Types are so-called because they allow to be more flexible about the return values of data constructors. Example: -- Requires GADTSyntax. These just make the types of -- value constructors more explicit. data Maybe a where Nothing :: Maybe a Just :: a -&gt; Maybe a -- Requires GADTs. In a function that takes this type as -- argument, you can now pattern match on types. -- Note how in a function expecting a 'MaybeNum Double', pattern -- matching on 'JustN' would be ill-typed. data MaybeNum a where Nothing :: MaybeNum a JustN :: Int -&gt; MaybeNum Int JustD :: Double -&gt; MaybeNum Double * Slide 15: You can regard type families as functions which map types to types. In my opinion, it's easy to go overboard with these, and they can easily make APIs unwieldy. The [wiki](https://wiki.haskell.org/GHC/Type_families) knows more. I find it useful to use `kind!` when working with type families, that is: &gt;&gt;&gt; :kind! TypeOf TInt TypeOf TInt :: * = Int The `Refl` constructor is short for "reflexivity", which is how you construct "proof objects" in dependently typed languages to show that the left-hand and right-hand sides are "the same" in a propositional sense (which is in contrast to definitional equality, which we define using `=`). Here, `Equal` is a proof object that two types are the same. That is a weaker notion of what can be proven "by reflexivity" in a dependently-typed language, where *types* can depend on *values*. * Slide 16: This is where "singleton types", that is, types which are only inhabited by one value, come in. This is how we can "fake" dependently-typed programming (to some extent!) in Haskell, for instance using the [singletons library](https://www.stackage.org/nightly-2017-12-20/package/singletons-2.3.1). Justin Le has written a series of blogposts about dependently typed programming in Haskell, for instance [this one](https://blog.jle.im/entry/fixed-length-vector-types-in-haskell.html). Truth be told, just about every proponent of this programming style probably has experience in languages like Idris or Agda or whatever, and if you're interested in this kind of thing I would have a look at them. In my opinion, at least in 2017, learning dependently typed programming in Haskell is like learning functional programming in C++. While probably doable, the language certainly doesn't encourage "this kind of thinking". 
It depends what you actually want out of the `ghc` that is built. If you want the full experience, you clearly want cabal or stack and all the other tools as well. And those will bring in their own expectations on how the file system and operating system works. My motivation for this project was never to really put a ghc on iOS, Android or Raspberry Pi. I have a rather powerful desktop with a much less restricted OS; I should be able to use that to build software for those platforms. I know that others have put GHC onto a raspberry pi, and compiled *on* a raspberry pi; I don't think the experience has been very good though. GHC is a very powerful compiler and needs a lot of resources (CPU and Memory) these days. Right now I consider iOS, Android and Raspberry Pi to be application platforms, not development platforms.
Also just realized that the update of containers puts it at 5.10.2 which now has patterns for Sequence. It is now super trivial to convert anything using `[]` to `Seq`. You can use `fromList`/`toList` to convert existing code piece by piece, and with the patterns, you can do pattern matching as well: case listObject of [] -&gt; ... x:xs -&gt; ... case sequenceObject of Empty -&gt; ... x :&lt;| xs -&gt; ... Both do the same thing, just one uses `Seq` and the other uses List.
I think both `TDD` camps, the one where "T" stands for "Type" and the other where it stands for "Test", are on extreme ends of a spectrum. Most probably, even when we eventually get `-XDependentTypes`, we won't use them all the time. Certainly for some critical program logic, and it's good to have these tools in the box. But things get complicated quickly. One fundamental issue I see after having invested some time in learning Agda, is that proofs in type signatures are somewhat anti-modular. The way you generally prove things is by reducing some propositional equality to some definitional equality, by applying laws and implicit normalization steps (which are done for you by Agda, by "following the definitions", basically). The problem is that you have to know how functions are defined, so you can decide whether you should pattern match on the first or second function argument, for instance. Should this function definition change in the future, it might be that your proof won't type-check anymore. A property test isn't that brittle. Time will tell what should be the sweet spot between productivity and formally-verified correctness, I guess.
 As /u/jared--w mentioned, you want " inductive graphs". These provide a "match" function that's like pattern matching, which decomposes a graph into a "node's context and the rest of the graph", for some arbitrary (but obviously deterministic) node. For example: match :: Graph -&gt; Maybe (Context, Graph) match { } = Nothing -- the empty graph match ... -- using pseudo-syntax for undirected "graph literals" -- Context "b" ["a","c"] represents a list of Edges: [Edge "b" "a", Edge "b" "c"] data Context = Context { cNode :: Node , cEdges :: [Node] } type Node = String -- data Edge = Edge Node Node data Graph &gt;&gt;&gt; match { a - b, b - c, c - a } Just (Context "b" ["a","c"], { c - a } &gt;&gt;&gt; match { c - a } Just (Context "a" ["c"], { }) &gt;&gt;&gt; match { } Nothing -- the graph is empty, we can terminate, -- having processed every node and every edge, exactly once Thus, just like pattern-matching on a list makes it smaller and smaller, and us your recursive function terminates, so does pattern matching on the "match"ed graph shrinks it. See: this awesome tutorial: https://futtetennismo.me/posts/algorithms-and-data-structures/2017-12-08-functional-graphs.html and then the `fgl` package: http://hackage.haskell.org/package/fgl-5.5.0.1/docs/Data-Graph-Inductive-Graph.html 
&gt; * MAC address: can't get it from HTTP Thank god.
Google Haskell "open world assumption". If there will really only be one instance ever, then you can write -- At the very top of the file {-# language FunctionalDependencies #-} class (Eq aState, Show aState) =&gt; Game aState | -&gt; a where initialState :: aState This could theoretically be useful, but it's rarely what you really want; more likely you want to get rid of the class in this case.
&gt; browser name/version, os name/version? Usually to get these in a robust manner you use a UserAgent parsing library. 
I generally recommend that beginners *avoid* using type synonyms. They're less useful and more confusing than beginners tend to expect. I almost only use them in combination with `TypeFamilies` or `RankNTypes`, and you're most likely not ready to play with those extensions yet.
Do note that the Validation Applicative has been available in [transformers](https://www.stackage.org/haddock/nightly-2017-12-20/transformers-0.5.2.0/Control-Applicative-Lift.html#t:Errors) for a while now, though well-hidden. There is actually a [blog post](http://teh.id.au/posts/2017/03/13/accumulating-errors/) describing its usage.
Right -- there are two equivalent ways to define addition of natural numbers, and neither are interchangeable wrt proving things.
Agree here; please not another Electron crap you need to actually hack in React/JS to use. That could be a render target but not just lazily be 'it'. 
Thanks for your comments, and very thought-provoking ones! Let me start with the last two points: &gt; There is mention of function being a monad which is not correct.... Functions are monads themselves, and of course, they can return monads too. Newtypes such as Reader, State, etc. rely on functions being monads (newtypes are just wrappers to their underlying forms - the compiler removes the wraps and replaces them with their underlying forms before generating code). Since any monad has an associated context, the function (as a monad) as an associated context of the operations it does on its argument. And, just as all monads have a "value" associated with their contexts (i.e., value of Maybe is a value after Just or Nothing, value of a list is "empty" or the list of individual components of a list, etc.), the value of a function-monad is the result of the evaluation of the function itself. Thanks, and you gave an idea for my next article in the blog - Functions as Monads. This is a concept that was also difficult for me to understand in the beginning. &gt; let bindings can bind any type... Yes. True. I just did not mention that there because we were talking of only monadic code in that article. Thanks for catching this, I should also mention that it in that article. And now for the "not all &gt;&gt;= and &gt;&gt; can be do'ed" part, thank you for catching it! I worded it wrongly: I should have put "not all.... can be _easily_ do'ed", by which I just meant that novices could possibly prefer the &gt;&gt;= and the &gt;&gt; notations than using the do blocks. This is the impression I got during Haskell meetups that I attend, but, yes, the words there in the article give a completely different meaning! Thanks, and I just removed that confusing part from there. 
I guess I should be a little bit more clear - I'm not really looking for any given solution to this problem, I'm specifically trying to frame the problem in terms of recursion-schemes, as a learning exercise. The graph traversal stuff outlined there seems neat, but honestly it's kind of overkill for this toy problem. I could just model the graph as: data Pipes = Pipes Int [Pipes] deriving (Eq,Show) And then get unique nodes via : doit :: Pipes -&gt; [Int] doit (Pipes n ps) = go [n] ps where go xs [] = xs go xs ((Pipes a as):zs) = if a `elem` xs then go xs zs else go (a:xs) (zs ++ as) My current solution is to just use a catamorphism to transform my `PipeTree` into `Pipes`, and brute force it, because it's really quite easy. My question is how to model that without using explicit recursion.
Thank you.
I just tested it real quick and it looks like Errors suffers from the same space leak as discussed here: https://www.reddit.com/r/haskell/comments/7hy4ml/validation_leaks/
Graph traversals without inductive graph representations won't have even an implementation with explicit recursion, they need something auxiliary like tracking everything visited. Once you can write it with recursion, then you can host out that recursion with the right recursion scheme. And since we loop on the `Just (_, graph)` until we reach `Nothing`, I feel like you'll want something with short-circuiting, like apomorphism, not a catamorphism. I don't know that much about either graphs or recursion schemes, but I think that this is the cleanest solution. 
Good point, I didn't now that. Another issue might be the `Monoid` constraint, where `Semigroup` might suffice. More broken transformer code :(
I noticed that too, the font sizes and the entire layout is pretty aesthetic. And thank god that even though he used a "funny" blackboard font, at least it's not comic sans.
&gt; Newtypes such as Reader, State, etc. rely on functions being monads I see what you mean. A subtle point - functions themselves are not monads. A monad always will have a kind `* -&gt; *`. Functions have a kind `* -&gt; * -&gt; *` i.e. they take two type parameters - one for the argument and one for return type. So a function itself cannot be a monad. In case of Reader/State, we fix one of the type parameters and then make it a monad. For e.g. `State s a` is equivalent to `s -&gt; (a, s)`. But `State` is not a monad, `State s` is. Or in other words, if you have a function `a -&gt; b`, `((-&gt;) a)` is a monad. This means when you use a do block for state monad, the state type `s` is fixed. You cannot have one part of the do block working on state type `a` and another working on state type `b`. Nice work with the blog, Haskell needs a lot of entry level and more importantly mid level tutorial materials.
But can you fault them for it? They are responding to what they've seen, and what is most popularly called a typesystem by about 90%+(?) of the programmers in the current day and age, e.g. C's, or Java's type system. It's not like, even if they've heard of Haskell, Hindley-Milner (or god forbid Idris/dependent types) that they would be immediately and trivially convinced by their power and their potential. Most of us spend years learning methods to exploit the expressivity in these systems, don't we? And therein lies an accessibility issue: AFAIU, with `-XDependentHaskell` for example one could in theory encode any and every bit of business requirement in the types, and one would no longer need any tests. But even exploiting _all_ the expressivity of Haskell is beyond me at this point (let alone using dependent types) having learnt and used Haskell for 3-5 years. How about test-driven development? I've learnt that in a matter of days/weeks/months, depending on how we count. It's conceptually orders of magnitude simpler. It lets me write quite very stable code with much less effort at this point of my knowledge. #### Example time I'm using reflex-dom at the moment, because I want to use Haskell and have a GUI for my apps at the same time. I wanted to add a very simple function: clickOutside :: forall t m. MonadWidget t m =&gt; El t m -&gt; m (Event t ()) Which would, as the name and signature implies, emit an event every time the user clicks outside the element. Now, with T(est)DD, ensuring the stable and regression-free behavior of this code is just about trivial. Writing a few test cases, trying to find edge cases where it could fail. specify "clickOutside" $ do (outsideEl, (insideEl, _)) &lt;- el' "div" $ do text "outside" el' "div" $ text "inside" outsideEvs &lt;- clickOutside insideEl evCount &lt;- count outsideEvs sample (current evCount) `shouldReturn` 0 (clickOn insideEl &gt;&gt; sample (current evCount)) `shouldReturn` 0 (clickOn outsideEl &gt;&gt; sample (current evCount)) `shouldReturn` 1 (clickOn outsideEl &gt;&gt; sample (current evCount)) `shouldReturn` 2 (clickOn insideEl &gt;&gt; sample (current evCount)) `shouldReturn` 2 (clickOn bodyEl &gt;&gt; sample (current evCount)) `shouldReturn` 3 (clickOn htmlEl &gt;&gt; sample (current evCount)) `shouldReturn` 4 That's it. If this test passes, and as long as it stays green, I can be reasonably certain that this function still works as expected. But I have no idea how I could even begin to encode such behaviour in the type of the function. Maybe someone amongst you will know? #### Why am I writing this up? I think I am because I would like so much if us, as a community placed more effort on testing or at least testability. - For the sake of learners, who come from less expressive type systems and still want to write stable Haskell with the help of tests even during the years when they learn the ins and outs of more expressive types. - For the sake of seasoned devs even whom might be stumped every now and again on how to encode a piece of invariant on the type level. Value level tests, I think you'd agree, are much preferable in such a meantime instead of no checks whatsoever. And the tests can be discarded once a type-level solution is found. Many Haskell devs seems to shun (unit-)testing, but I think that's entirely unfair. Especially in light of my prior points. Unfortunately the above test code for reflex is only pseudocode. I've taken inspiration from other already working systems to come up with the fictional API. And now I'm finding myself at a crossroads: - Make reflex-dom unit-testable somehow. I've already made a few attempts in this direction, nothing very fruitful has come of it yet. I'd likely have to familiarize myself and learn much of reflex internals to make this work, so this path is less than ideal in terms of time-efficiency for me at the moment. (Related: https://github.com/reflex-frp/reflex-dom/issues/175) - Wait for /u/dalaing to finish implementing https://github.com/dalaing/reflex-testing/, or at least until it arrives at a reasonably usable state. But even still, AFAIU, his project won't quite be suitable for the kinds of tests that I'd like to write. To put it as an analogue: at the moment I'm more interested in HSpec-level assertions instead of QuickCheck-style property-based tests. Why? I guess that could be the topic of a similarly long post, but it's very much along the same lines as my points above. - Look for some other Haskell GUI library/framework that allows for more testable code. I'd be surprised to find one, but please, by all means, surprise me if you can. Any other ideas? Also, any thoughts in general on what I wrote?
I really like this idea.
Going off my original comment somewhat, although this curiousity seems unrelated, I had the following code that worked newtype Pair a = Pair { getPair :: (a,a) } vecTup = Iso toVec toTup where toVec :: Vec Two a -&gt; Pair a toVec (VCons a (VCons b VNil)) = (a, b) toTup (a, b) = (VCons a (VCons b VNil)) toNat :: forall f g. (forall a. Iso (f a) (g a)) -&gt; Natural f g toNat iso = Natural (first iso) (second iso) where first (Iso f _) = f second (Iso _ g) = g then running toNat vecTup in GHCi worked, but when I changed Pair to a type rather than newtype as such type Pair a = (a,a) gave the error: the type synonym `Pair' should have 1 argument, but has been given none I tested it to make sure it was a properly kinded type, running :k Natural (Vec Two) Pair =&gt; * So that seems to be all correct, but even just trying let f :: Natural (Vec Two) Pair f = undefined in GHCi gave the same error. 
&gt; Do you have an example where this approach is necessary? Not giving up control over type families when deriving classes. Consider `Product Identity Identity`, isomorphic to `Pair`: data Pair a = a :# a We can get many instances for free (`GeneralizedNewtypeDeriving`) by defining `Pair` as a *newtype* over it: ^(for technical reasons, `Distributive` can not be derived yet) newtype Pair a = Pair' (Product Identity Identity a) deriving newtype (Functor, Applicative, Monad, MonadFix, Distributive, Representable, ..) pattern (:#) :: a -&gt; a -&gt; Pair a pattern a :# b = Pair' (Identity a `Pair` Identity b) This inherits properties of its underlying type which is **not always the right default**! The ‘default’ representing type of `Product Identity Identity` is `Either () ()` Rep (Product Identity Identity) = Either (Rep Identity) (Rep Identity) = Either () () What an ugly type! What we want is `Bool` and the price we need is to establish an isomorphic to `Either () ()`. Or establishing an isomorphism between `Either () ()` and other type: data PairIndex = Fst | Snd How do we use this isomorphism? Pass it as a phantom parameter to an *adapter* (note that the representing type of `NewRep (iso :: old &lt;-&gt; new) f` is the `new` representation! newtype NewRep :: (old &lt;-&gt; new) -&gt; (Type -&gt; Type) -&gt; (Type -&gt; Type) where NewRep :: f a -&gt; NewRep iso f a instance (Representable f, Rep f ~ old, Iso iso) =&gt; Representable (NewRep (iso :: old &lt;-&gt; new) f) where type Rep (NewRep (iso :: old &lt;-&gt; new) f) = new index :: NewRep iso f a -&gt; (new -&gt; a) index (NewRep fa) = index fa . to @old @new @iso tabulate :: (new -&gt; a) -&gt; NewRep iso f a tabulate gen = NewRep (tabulate (gen . from @old @new @iso)) and this allows us to derive `Representable` underlying `Product Identity Identity` with any isomorphic representing type, such as `Bool`: data Iso_Bool :: Either () () &lt;-&gt; Bool instance Iso Iso_Bool -- instance Representable Pair where -- type Rep Pair = Bool newtype Pair a = Pair' (NewRep Iso_Bool (Product Identity Identity) a) deriving newtype (.., Representable) 
When I was last playing with equality saturation I had a few vaguely related thoughts: First, equality saturation can be viewed as a propagator network problem, like datalog. Build "tables" for each of the node types with functional dependencies but do union-find on keys when you have a key conflict rather than reject the insertion. This can then cause a cascade of other union-find collapses of other keys, helping to bound the table sizes. The next trick is encoding all of your rewrite rules into some sort of Rete-like rule engine that can run over this representation, and encoding all the alpha and beta memories as other propagator network "tables" of the same sort. It unfortunately lacks datalog's termination guarantees when it comes to the expansion step. We're forced to simply hope that it terminates. (Rete isn't datalog, and creates new node ids as it works for some rewrite rules.) Now the relevant bit. The rule search for Rete is pretty much first come, first serve. Another possibility is to try to pick rules that tend to lead to an optimized program more often than picking arbitrary rules. After all, if it terminates any rule firing order is equivalent to any other, but if it doesn't and we have to cut it off early, then it'd be better to have explored _useful_ rules. You could do this by usinga technique as simple as tracking how many times a rule has participated in the selection of a successful program by tracking the provenance of the successful program (also a propagator problem!) Tracking provenance through union-find comes up in congruence closure problems and there are good algorithms for it. Vilhelm Sjoberg's dissertation at U.Penn has good links to all the relevant papers. The trick then is weighting your rules. You can't just use probability of occurrence based on how many times you've seen it used so far, you have to at least bias it a little with something like [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) to make every rule able to fire. The nice thing about this approach is that you can in theory compile very very large rule sets to evaluate with this sort of stochastic Rete. This brings us to the next bit involving Monte Carlo simulation. =) Stanford has a project called [STOKE](http://stoke.stanford.edu/) which is a stochastic super-optimizer using MCMC searching for searching for programs. It operates on raw assembly code to avoid the "WYSINWYG problem" where optimizing in an intermediate language may or may not produce faster programs once all the instruction scheduling, register selection, spilling, etc. is taken into account. It proceeds by first feeding the program a bunch of random inputs and recording the outputs, and does an MCMC walk through randomly selected mutations to your program, accepting some moves that move 'downhill' by making some outputs invalid and all moves that move up hill to achieve "detailed balance." With that, whenever all of the outputs match, then it performs a detailed proof to check that the two programs are equivalent, and if the program is cheaper than the old one under whatever metric you are using for latency, speed, size, etc. it then emits that as the most recent optimized version before going back to the grind. Modified to fit this environment it'd be applied to syntax trees to produce new rules to feed the rete engine. They become big rules with lots of side-conditions that rarely are available, but if they are, they lead directly to a more optimized program space. Let it run for a few days and your compiler gets smarter. All of this presupposes you're willing to wait a really long time for the optimized program to be spat out at least the first time. In any event, that is the only use of MCMC I've really thought about for Equality Saturation.
If your project breaks due to a change in stack, and you open a PR that is purely an improvement (no downsides), then we will merge it.
flare isn't particularly "web-specific" even though it happens to render to a web page, it's based on Gabriel Gonzalez's typed spreadsheet.
Fantastic discussion!! Thanks for engaging! Yes, you are right. May be the wordings should be that a "partially applied function" is a monad, or, in other words, a curried function is a monad. Basically, a function that _awaits an argument_ is a monad. Now, this is a function anyways. Let me give an example, just to clarify what I mean: &gt; f = (+2) -- A curried function &gt; g = (+3) -- Another curried function &gt; :{ | sumplus2plus3 = do | a &lt;- f -- "Extracting value" from the function f monad | b &lt;- g -- "Extracting value" from the function g monad | return ( a + b ) &gt; :} &gt; &gt; sumplus2plus3 5 15 &gt; Clearly, we have a monadic do block in there, and both functions f and g are awaiting an argument: so, you are right - the kind of monads must be * -&gt; * (as should be the kinds of Functors and Applicatives too, as all Monads are implicitly Functors and Applicatives). Thanks for those encouraging words an the end!! It goes a long way to push me to add more articles as I gain more and more experiences in Haskell!!
The get precise `insertWith` semantics something like: at key %~ \x -&gt; (+1) &lt;$&gt; x &lt;|&gt; Just 0 works. To get the semantics where 0 isn't a possible value, so you can use it for, say, maps of maps of maps where default values occur, use `non`. at key . non 0 +~ 1 This was shown below, but, this isn't the same meaning as the `insertWith` example given on two fronts. In particular the first value you see out of it will be 1, not 0. Second if the modifying function sets the result to 0, the key will be deleted from the map. Usually this works out for the best, but the semantics are subtly different than what you are asking for. `insertWith` is a strange beast full of side-conditions.
The left and right direction are very important and absolutely not something we can forget about. For some data structures it seems like you almost want 4 or even 6, as you have a left fold that starts on the left, a left fold that starts on the right, a right fold that starts on the left, and a right fold that starts on the right. And the first and the last of those 4 will probably want strict versions. 
Handy command for starting new projects: `stack new yourprojectname simple-hpack`
I think a todo app (as mentioned here already) would be the starting point, but something non trivial like a crypto exchange browsing app; list of exchanges, list of coins, mini charts (updating) , large charts updating, etc. It's absolutely non trivial to make but it has everything you encounter while making a complex GUI including server latency, randomly updated numbers and charts, very long lists (lazy loading). Most of these apps (mobile and desktop) have quite a lot of issues (most people don't see but as a programmer I do) related to these real world issues. 
Well Haskell is definitely a better language for this than the crap one used in ethereum.
You can't partially apply type synonyms. So basically `Pair` alone is basically never valid Haskell.
&gt; Yes they can be. They can be potentially finite coinductive lists. My bad, you are correct. I think I have a different (possibly misunderstanding?) perception of this than most, as I've never used this "possibly finite coinductive list" and it doesn't seem useful to me. Here's how I think about these (in Agda): -- inductive list data List (t : Set) : Set where [] : ⊤ → List t _∷_ : t → List t → List t -- what in my mind should be called Colist but people call "Stream" data ActualColist (t : Set) : Set where ][ : ⊥ → ActualColist t _∷_ : t → ∞ (ActualColist t) → ActualColist t -- what people call "colist" and I never use data SomethingElseEntirely (t : Set) : Set where [] : ⊤ → SomethingElseEntirely t _∷_ : t → ∞ (SomethingElseEntirely t) → SomethingElseEntirely t Yet, even the [Agda standard library uses Colist to refer to the third structure](https://github.com/agda/agda-stdlib/blob/master/src/Data/Colist.agda), so I'm clearly in the minority here, but I simply don't see how that's the proper categorical dual of `List`: to me, it seems like it should be the second. &gt; There is actually a path for changing this. That's great to hear! Still, I've brought it up before and received lots of pushback, so I'm not entirely sure it's even just a technical problem. A lot of people seem to just disagree that making this distinction brings significant tangible benefits.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [agda/agda-stdlib/.../**Colist.agda** (master → fa530e4)](https://github.com/agda/agda-stdlib/blob/fa530e4add7f39d32944aa0413fef4cc3208f79e/src/Data/Colist.agda) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
I agree, this was a really great read, and very pedagogical. I learnt a lot, and all the extensions used felt completely warranted (unlike a lot of teaching material, these examples showed not just what you can do, but *why* you'd want to do it) which made me pay more attention to the examples. More of this, please :-)
While mbw_rdt has written a lot of good explanations for the different points, if you're looking for a complementary explanation, there's a Strange Loop 2017 talk by Stephanie Weirich on [dependent types in Haskell](https://youtu.be/wNa3MMbhwS4) which includes singletons, among other things. I've seen the Refl bit in an Idris talk (YouTube - [Idris: practical dependent types](https://youtu.be/4i7KrG1Afbk))
Excellent talk from Stephen Diehl at Functional Work yesterday night in FW’s office (London the 20th December). I saw that FW was recording the talks. they might release the talks soon.
So very true. Since this is a hobby project I do not feel any rush, and that quite often leads to me being very non-pragmatic and not getting anything done. 
Ah, thank you.
Thanks for the response!
Thank you
Is `over` equaliavent to `(%~)`?
\o/
Agreed about development, but Haskell is useful for so much more. Currently, all the scripts, utilities, file conversions, etc., that I write on Android are in Python. Why not Haskell, like on other platforms? I doubt any of that would take more than a few seconds to compile on my tablet. I'm not sure what they wanted out of GHC on the raspberry pi. But for something you carry around in your pocket all day, having a working GHC, even a slow one, would be huge.
Got it. Thanks!
Sorry, maybe I'm wrong but this emphasis in types and the overlooking of composition is something that has responsibility in the bad programming practices (under my point of view) that uses Haskell as a typed OOP language. Instead of creating "functional patterns" that is, combinators and EDSLs, Most if not all "real world" haskell is just a translation of OOP patterns. And I'm sorry again. All in all, Haskell is not a better OOP language than OOP languages. That's the reason of the failure of Haskell in most practical applications. Type safety at this level of making wrong states irrepresentable is not worth the pain in practical terms. Reusability, composability and maintainability does. And this is only possible with composability and EDSL's. 
What is `letseq`? It's not defined anywhere, but it's used to define `letrec`.
Did you miss slide 18?
Thanks, at least he mentions composition, but little more, I'm afraid. I see some monad instance for the shake of restricting IO only, but no combinators for the workflow problem at this level, like state change, permissions, delegations. I do not mention lower level problems like events, checkpoint and restore execution state, rollback etc. That would be something a real programmer of workflows would die for.
I really don't understand the decisions behind solidity, to me, nearly everything decision behind this language in this high stakes + not much code environment seems plain wrong. I was super surprised when in dabbled in ethereum to encounter this kind of language. 
Why is branded as "summer school" if it's structured as (longish) conference? Can we get learning tracks agenda earlier?
I "shun" unit-testing, at least in the frameworks-and-mocks sense: I so rarely need what it offers that it's not worth remembering how to do it in the cases where it would be useful. It's so easy to separate my code into a) business logic functions that are testable at a simple inputs-and-outputs level, or better still the code is obviously correct on its face b) a stack of high-level-effect to low-level-effect interpreters that again can be tested in a simple inputs-and-outputs way or the code is obviously correct on their face c) low-level effect plumbing that can only meaningfully be tested at the integration level, that I just so rarely have the call for a mocking framework or anything like that. Those frameworks tend to be "viral" in the sense that they violate the language's usual invariants, so using one would add a cost to understanding *all* my tests, a cost that isn't justified in terms of the benefits to a small number of tests that actually need such a framework. So I prefer not to use them at all. (GUIs may be an exception to the rule; I don't work on GUIs and they seem to have quite different constraints to ordinary request-response style programming. So your answer might just be that not many people are working on GUIs in Haskell and therefore Haskell has poor support for GUIs. If that's true then it's at least partly a bootstrapping problem and, optimistically, may resolve itself as Haskell generally and Haskell GUIs specifically become more popular) In principle I'm all for testability to provide an easy on-ramp for people coming from other languages, or less experienced programmers generally. But I'm not going to spend my own unpaid time (and can't justify spending my contracted time) writing tools or frameworks that I'd never use myself. I think the only practical solution here is for organisations that have an interest in providing an easy on-ramp to Haskell to commit appropriate resources (i.e. funding, blogs and/or code contributions) to creating that on-ramp. I'd happily accept a contract at my usual day rate to, say, make reflex more amenable to unit-testability. But who would be willing to pay for that? (Note: I'm actually a Scala programmer, but I think the same issues affect both languages)
I'd say [Dhall](https://hackage.haskell.org/package/dhall-1.8.2/docs/Dhall-Tutorial.html) would be a very nice candidate! If dhall had a gui for editing (atleast a subset) of dhall expressions, it could be quite easily reused by many other applications. For example, Pandocs confgs can be read from dhall file with little effort, giving you a "gui for free".
You certainly use your phone different to how I use my phone. But I can see the motivation. I guess we might get there eventually...
I like to use toy examples that encapsulate the essence of a "real-world" problem. So far, I have come up with CRUD.hs and BarTab.hs [from the reactive-banana examples page][1]. [1]: https://wiki.haskell.org/Reactive-banana/Examples
I like to use toy examples that encapsulate the essence of a "real-world" problem. So far, I have come up with CRUD.hs and BarTab.hs [from the reactive-banana examples page][1]. [1]: https://wiki.haskell.org/Reactive-banana/Examples
Commented! https://www.reddit.com/r/haskell/comments/7l203h/what_haskell_programslibs_need_a_gui/drkgoqa/?st=jbgi9i8p&amp;sh=16f01fad
I'm not sure if I would describe it as "cool", given that it doesn't work in GHCJS. :-) Anyway, weak pointers are created by mkWeak :: key → value → Maybe (IO ()) → IO (Weak value) This function will establish a relationship between `key` and `value`, namely that the key will, from now on, keep the value alive. You can *discard* the weak pointer afterwards, and the relationship will still hold!
Linking to the blog posts would be useful for those of us who want to learn more about recursion schemes. Hint hint. :)
[Here's the original paper about this technique](http://okmij.org/ftp/Haskell/tr-15-04.pdf), which I coincidentally discovered just last night!
I did, but it was sort of hard to see. Edited post to make link larger.
If you have android and you install [termux](https://termux.com/) your life will be changed.
/u/arvindds , I wanted to thank you and pay respect for your engaging with the comments above and incorporating feedback! That alone is something not many have mastered! 
&gt; But can you fault them for it? Maybe you don't know how to do it. I certainly don't know how to do it. But **I'm not the one writing books or being paid for seminars on how to write code**, that would be 'Uncle' Bob.
I only know a teensy bit of Idris, so I might be totally wrong here, but here's my guess on what the situation might look like - Today you have SemVer (sometimes enforced by the package manager) that works on the type level. However, SemVer actually operates on the level of semantics, not types, and it is up to the programmer to take care that they don't accidentally change semantics. If dependent types become popular, I think there will be an additional notion like SemVer. Identical semantics (SemVer) → Identical (or more) theorems (ThmVer?): given two definitions of the function `f` and an arbitrary proposition `P(f)`, what are all the theorems that can be proved? If there is a theorem that could be proved with the older definition but can't be proved with the newer definition, bump your major version. This will probably have to be done manually but maybe you can have a QuickCheck-style generator of propositions to test your new and old implementations. 
Yeah something seems to have been changed. Same happened to me. Instead added the dependency in package.yaml.
&gt;some rather esoteric questions The first two don't seem *that* esoteric to me. I don't have any great tips for how best to use parser combinators, but I know Idris just switched to megaparsec so you might have some luck reading the source code. &gt;How can I implement position tracking for error reporting I know megaparsec tracks this automatically. You can annotate your AST with it reasonably easily. 
I think some people are missing the point. It's not to bash static typing or to avoid writing tests. So often we read about bugs and think, 'that would have never happened had they/we used haskell, because the types would have caught this, like, months ago'. Like-minded people of Uncle Bob don't like to believe this because well, 'we could have written more tests'. The aim of expositions like this talk, of course, to get this across: with a powerful type system those 'behavior' bugs become 'mechanistic'. Sure, types may over-complicate things... But that's not why they are not useful
As I attempted to note in my main post, i'm trying to build my own parser combinator library for use. Though, out of the things i'm not wanting to bend on.. that's probably one of the most likely things for me to actually bend on. Thanks.
OK, just some incomplete thoughts here: &gt; lexer or no lexer You will have lexing code in there one way or another, but IMO having a completely separate lexing stage (like you would with, say, yacc+lex), is a bit of a performance hack and absolutely not necessary in a non-strict parser-combinator approach. After all, writing both parser and lexer using the same building blocks is perfectly possible here, and so what you really get is a lexer that is seamlessly integrated with the parser. How strong and explicit you make the split is more a matter of style and expressivity, although it's also worth noting that the more contextual your language becomes, the less useful a very explicitly separate lexer becomes, because the lexing rules will often depend on context that can only be determined by performing at least part of the actual parsing. &gt; How can I implement position tracking for error reporting? Depends on the kind of error you want to report. For parser errors, all you need to do is track source position in your parser monad; the most straightforward way is to build your parser monad as a newtype or type alias over a monad stack that has a `StateT SourcePosition`, and then have all your parsing primitives update the source position as needed. For errors in later stages of processing, you need to tag your data structures (AST and the like) with source positions pulled out of the parser monad during parsing. The simplest possible way is to just attach an extra field to each constructor of your AST of type `SourcePosition`, however this is inconvenient when you want to synthesize AST rather than create it using the parser. One strategy for this is to make the AST polymorphic over the source position type, so that you can write most of your code in a source-position-agnostic way, and defer handling of source positions in errors to user code. You can also use Comonads as an abstraction for this, although for some reason it doesn't feel right to me, can't really put my finger on it though. A radically different approach is to pseudo-expressions / pseudo-statements into your AST that merely wrap other AST values while adding source position information, e.g.: data Expr = LitExpr Value | AppExpr Expr Expr | SourcePosition SourcePosition Expr The advantage of this is that the programmer overhead is constant (you only need to write source position extraction code once, no matter how many constructors your AST has), and it is easy to inject or strip source positions from the AST without disturbing anything else. &gt; How can I effectively partition my parser combinator library away from my actual parser? splitting them into different files seems to take away some of the operators set up in the combinator library. This isn't supposed to happen; operators are just syntax-sugared functions, and if you export them from the parser combinator module, they should be visible from outside. Hard to tell what's going on, but I would wager that for some reason they are not being exported. A common source of such errors is importing things from elsewhere and failing to explicitly re-export them. For example, your parser combinator might have an `Alternative` instance, from which you get `(&lt;|&gt;)`, and that operator will be visible inside the parser combinator module because you imported `Control.Applicative`, but the module uses implicit exports (no explicit export list), so only things defined in the module itself get exported, but not things defined in modules it imports, meaning that `(&lt;|&gt;)` isn't exported. Just a shot in the dark though. Other than that, putting the parser-combinator stuff into a separate module is the canonical way to do this; you should really try to get this working, because it's how we do things in Haskell, and you need to understand the module system. It's not super complex, btw., although a few things might not match your expectations. Be sure to re-read the sections on modules, imports, and exports in whatever learning materials you've been using.
No problem! You can also read the source for megaparsec of course - it should give a good idea of how to make such things fast :)
type WeekNumber = Int rainfall ::WeekNumber -&gt; Double rainfall n = fromIntegral (n `mod` 7) * 3.9 countWetWeeks :: WeekNumber -&gt; Int countWetWeeks n | n &lt; 1 = 0 | rainfall n &gt; 10 = 1 + countWetWeeks(n-1) | otherwise = countWetWeeks(n-1) How do I rewrite countWetWeeks using only list comprehension and not recursion?
This is an extremely helpful response! The lexer section definitely clears that up, and the modularisation section... yeah. that's definitely what's happening. Thanks there! As far as the position tracking section however, hat leaves me with more questions, more stemming from my inexperience than anything else. I do not understand &gt;building a monad alias over a monad stack that has a `StateT SourcePosition` Nor do I understand these differerent approach suggestions. Could you possibly explain them?
I will take you up on that recommendation!
This is interesting but still not the point. Meanwhile i have explored my example further and found that m0rphism already had the answer, but confused me with his example that did not derive Show. If his example class would derive Show, than there would be a initialState :: TicTacToe initialState :: FooBarBaz so i need to specify which one to call.
There was so much good stuff this week that I had a hard time picking what to feature! Do yourself a favor and check out pretty much everything in this issue, from the featured articles to the links in brief. 
Build list of rainfalls, reject those that do not meet criteria, count items in array?