One of the comments on the Trac is interesting. It appears this has existed in more obscure forms since 8.2.2, so 8.6 may simply be presenting the bug much more often.
It's pretty hard to blame release cycle length, per se. A bigger problem seems to be the (substantial) challenge of getting "real-world" testing of pre-releases. Hackage HEAD is somewhat helpful, but it's not the easiest thing in the world to use and I think most people still aren't using it. The bug in question is subtle enough that it managed to escape GHC's extensive test suite, so I don't really know how it *should* have been caught.
There are probably a few techniques we could identify (it might be interesting if you posted an example), but I've blogged about one here: [http://brandon.si/code/translating-some-stateful-bit-twiddling-to-haskell/](http://brandon.si/code/translating-some-stateful-bit-twiddling-to-haskell/) . Probably better than reader is to use the Reader-like Monad instance \`((-&gt;) ())\` which someone pointed out to me somewhere. &amp;#x200B; Also if the stateful form of the algorithm makes things much more terse or understandable, you can always write your version in the \`State\` monad; that's what it's there for: &amp;#x200B; do ... when (isFoo bar) $ modify someMutation
`ByteArray` and `MutableByteArray` are unboxed arrays; they don't contain any references to heap objects so there is nothing for the GC to do. The `vector` library knows how to pack and unpack data of different types for you, so that you get a richer API than the primitive one implemented by GHC (reading and writing bytes and words of different size).
WOW! I guess real-world testing only begins to happen when stackage nightly switches to the new release and people run all the test suites, etc. Definitely a compiler should not be used in production until there's an LTS for it. &amp;#x200B; Should perhaps the GHC team build all of hackage or stackage locally and run the tests before each release? Sure, there will be upper bound problems, etc., but that could potentially help catch bugs like this before the release.
Never use features under `MagicHash`, unless you *really* understand what you are doing.
But when you do really know what you're doing, hopefully the runtime isn't producing incorrect results ;)
yeah (I'm playing around with writing yet another typed paths pkg), and that's my approach.
Maybe the expectations around the stability of major releases should just be adjusted? It seems perfectly reasonable that a major release would need wide scale testing in uncontrolled environments, and as you note, hackage HEAD isn't accomplishing this. So, the least cost solution to this issue seems to be simply adjusting communication. 'Bleeding edge', 'relatively untested', and 'proven stable' are definitely three different categories, and the current release schedule as written doesn't seem to clearly distinguish between the latter two.
this was discovered before the Stackage nightly was released. it was just a property test in a package (`flac` by `mrkkrp`) whose author promptly added the compiler version to their continuous integration. not that Stackage isn't useful, just that this was much simpler. see this travis file: ``` language: c sudo: false cache: directories: - ~/.cabal/packages - ~/.cabal/store matrix: include: - env: CABALVER=2.4 GHCVER=7.8.4 addons: {apt: {packages: [cabal-install-2.4,ghc-7.8.4], sources: [hvr-ghc]}} - env: CABALVER=2.4 GHCVER=7.10.3 addons: {apt: {packages: [cabal-install-2.4,ghc-7.10.3],sources: [hvr-ghc]}} - env: CABALVER=2.4 GHCVER=8.0.2 addons: {apt: {packages: [cabal-install-2.4,ghc-8.0.2], sources: [hvr-ghc]}} - env: CABALVER=2.4 GHCVER=8.2.2 addons: {apt: {packages: [cabal-install-2.4,ghc-8.2.2], sources: [hvr-ghc]}} - env: CABALVER=2.4 GHCVER=8.4.3 addons: {apt: {packages: [cabal-install-2.4,ghc-8.4.3], sources: [hvr-ghc]}} - env: CABALVER=2.4 GHCVER=8.6.1 addons: {apt: {packages: [cabal-install-2.4,ghc-8.6.1], sources: [hvr-ghc]}} before_install: - export PATH=/opt/ghc/$GHCVER/bin:/opt/cabal/$CABALVER/bin:$PATH install: - cabal --version - ghc --version - travis_retry cabal update script: - cabal new-build --enable-tests --enable-benchmarks --flags=dev - cabal new-test - cabal new-haddock - cabal sdist notifications: email: false ``` https://github.com/mrkkrp/wave/blob/master/.travis.yml
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [mrkkrp/wave/.../**.travis.yml** (master → 3481ee2)](https://github.com/mrkkrp/wave/blob/3481ee2a93507104109e8a84f04df6882cb803f0/.travis.yml) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e71p1xr.)
Thanks, I think I misspoke a bit. I meant to say that really large scale testing begins when people begin working including the packages in stackage nightly but that's too late because the compiler has already been released. It would be great to go though that process before the release. Have something like "stackage pre-release" .
`--allow-newer=base` does not work for you?
I haven't understood it as a fundamental question, the way it has been asked. Many people answered with examples of concepts that are used with monads and explained why a value/type with abstract context is not canonically extractable in the general case. 
It probably does. A secondary reason is that I have sometimes tested late GHCs betas on my code, gotten things to work, but then the final GHC release breaks a few more things. It's a small concern in the grand scheme of things, but it does hurt my motivation to try it out, since I might as well wait and fix everything at once.
Well, that's why `cabal` has the [`--allow-newer` flag](https://cabal.readthedocs.io/en/latest/nix-local-build.html#cfg-field-allow-newer) which gives you the flexibility to control which version constraints you want to ignore at your own risk. ...and btw, Hackage Trustees have been busy over the last weeks adding those "restrictive base upper bounds" all over the place via [revisions](https://github.com/haskell-infra/hackage-trustees/blob/master/revisions-information.md) to ensure that the constraint solver can do its job and users have a good experience on Hackage. 
In ghc 8.4.3 `Semigroup` became a superclass of `Monoid`. Most code that did not limit the `base` upper bound and used `Monoid` broke during that change.
My experiences at a younger, smaller shop echo a lot of this. After doing contract work for several months, I was brought on in late 2015 as the second member of the backend team at slant.co largely because of my interest in Haskell. We were still primarily a Node shop then, and remained so for another two and a half years until the cost of adding new features had gotten high enough that the downtime of a total backend rewrite was no worse. The "looming threat" of a Haskell migration had been a running gag in the company since before I arrived, which was actually beneficial: it made the idea more familiar, made it clear that this wasn't a passing fad interest, and provided numerous opportunities to explain the benefits. By the time we made the call, the CEO was only mildly nervous and trusted our judgement on the matter. It was still tense for a few weeks, because "we have structures that compile" is hard to recognize as progress from the outside. But once the core architecture was done and we started building routes, eyebrows started going up because frontend requests that used to take a day or two were getting done in 20 minutes. It was also soon apparent that the new routes were considerably faster (though that was partly the result of a tandem API redesign allowing for much simpler DB queries). The last bit of doubt melted away about a month after live deployment, when our ops manager (who had also been vocally dubious) noted that the Haskell server had been running error-free since it went online. We have a couple of route types still on Node; after we got the core migration done, other projects took priority. Notably, we're working on an ML product, which we started building in Python because (a) that's what all the literature and tooling supports and (b) at the time, frontend was still catching up with the API changes, so the Haskell backend wasn't live yet -- and we didn't want to push our luck. But after a month of mounting frustration, we finally gave up and moved everything that didn't need direct access to Tensorflow into a Haskell service. That migration only took a few days, at which point the CEO came over, commented on how we were smiling again, and told us that from now on if our instincts say build it in Haskell then we build it in Haskell. **tl;dr:** the big hurdles were initial approval, and a significant initial time investment before being able to demonstrate any kind progress to outside observers. After clearing those, the benefits rapidly became obvious to everyone.
Ok, but wouldn’t it still be broken with ghc 8.4.3 just the same with the upper bounds in place??
yes, i strongly agree about that. systems that notify authors, and/or authors become involved in updating their package, before the compiler is released, are really helping out the community (just like releasing open source in the first place). I actually couldn't test GHC HEAD for my stuff because `cabal-install 2.2` didn't seem to work for `ghc 8.6` (but I don't know if I just didn't run the command or update something correctly, hopefully `ghc 8.8` will work fine with `cabal-install 2.4`. like others have said, playing jigsaw with updating dependencies is annoying, so it's nice if `Hackage` or `FPComplete` can coordinate on that (rather than bumping a dependency's `base`, and so on). my point was just that it was one person who found this, /u/mrkkrp, on their well-tested, small package! (so, quick to compile, few dependencies to confuse things, etc). btw, `head.hackage` seems interesting: ``` repository head.hackage url: http://head.hackage.haskell.org/ secure: True root-keys: 07c59cb65787dedfaef5bd5f987ceb5f7e5ebf88b904bbd4c5cbdeb2ff71b740 2e8555dde16ebd8df076f1a8ef13b8f14c66bad8eafefd7d9e37d0ed711821fb 8f79fd2389ab2967354407ec852cbe73f2e8635793ac446d09461ffb99527f6e key-threshold: 3 ``` (http://head.hackage.haskell.org/), but I've never used it. Can I just add that in my `.project` file? (pinging /u/Athas too, since they seem to have used it).
I couldn't run `cabal` on `nixpkg`'s `ghcHEAD`. i.e. `cabal new-build -w ghc-8.6.2018. ...` (which was `cabal-install-2.2`), but I don't know I just didn't install and/or update something correctly. will I be able to build development versions of the next compiler lease with `cabal-install-2.4`?
&gt; Ok, but wouldn’t it still be broken with ghc 8.4.3 just the same with the upper bounds in place?? For the version bounds of base, yes, there is two kinds failure. Say you have X as a dependency (which uses `Monad` as above) of Y and you try to build Y. * Without the version bounds building Y will build everything up to X and then fail when it tries to build X with a confusing error message about `Monoid` or `Supergroup. * With the version bounds, building Y will fail immediately because cabal cannot find an install plan the satisfies the constraints. However it will specify that it failed *because* it could not satisfy the constraints on `base` for package `X`. However, is the package that would not compile with GHC 8.4.3 due to some other package not compiling with ghc 8.4.3, having a upper in place *will* result in a valid install plan and no upper bound will result in a confusing error message.
Well, generally you need a recent enough `cabal` snapshot that supports the given GHC version (cabal encodes knowledge about GHC and thus needs to co-evolve w/ GHC), in general this means using `cabal` HEAD. Consequently, if Nix packages GHC HEAD, they ought to package `cabal` HEAD as well.
See https://github.com/hvr/head.hackage#how-to-use
Assuming one builds with stack, they would also immediately find that the package is incompatible, right? I remember the experience back when I used cabal and I wanted to install some packages but it couldn’t make a build plan. Then I’d download their dependencies’ sources and patch them (sometimes only upper bounds, sometimes more than that) and that experience took a lot more time and effort than the scenario that you are describing which would happen in cabal without upper bounds.
Yeah, `cardano-sl` is still on 8.2.2 as well. My usual policy is to wait until a .2 release to avoid bugs, but I think I might have to start hanging back a major version in production projects. I'd be in support of a 8.4.4 release if there are good bug fixes to be had :)
Does it matter for the server when it reads a confusing error message?
that makes sense, thanks!
Aha! So the mistake I am making is like asking why Functor has no bind: if it had, it would not be a Functor but a Monad.
Interesting. If I knew haskell and not scala I would have applied!
Yes, the bug is present at least in GHC 8.0, 8.2, 8.4, 8.6. Three circumstances in which it can be triggered: * use dataToTag# and a type with at least 2 constructors. This is the root source of a bug. * use deriving Ord on a type with at least 9 constructors and -O. This is because the automatically derived Ord instance uses dataToTag# for large types. * use deriving Ord on a type with at least 9 constructors and -O and some code from containers package. This one happens only on GHC 8.6, because only this version simplifies the code aggressively enough to the previous point.
&gt; imagine leaving someone to implement a basic lens, and when you turn back around they are reading books on category theory to understand the underlying code Edward Kmett wrote :)
No it doesn't but not everyone wants to use stack. I use stack at work but for my own code I refer to use a different build system for Haskell code. As such I am really kind of sick and tired of the "everyone should just use stack" mantra.
Thank you for sharing this and I hope you share more of what you encounter. I noticed that getting people hooked is a long process which needs a lot of patience. The combination with producing something of value seems to help keep people excited.
Pretty much what infonoob said - it's not too difficult to get a visa for them, not too costly to pay some salary. We look for interns with some experience plus we are somewhat picky - this allows us to treat them as full-time developers. Good interns can come back to stay with us later full time.
Well, you are in a good place to start (/r/haskell), next year you might know enough Haskell to land an internship with us.
Yeah, they could have named it that, but it sounds better as "Keep your laziness in check", at least in english because of callbacks to some slang. Then, they could just clarify in the paper or talk or something
u/andrewthad could talk on this more, but at work he wrote a data structure that has exponential space requirements, but because of laziness, it never gets fully (or even mostly) evaluated, so we get the benefits of its lookup properties without the drawback of needing an insane amount of memory. in a strict setting, this would not be the case.
I'm not sure I get what you mean here. A syntactically invalid expression can't narrow the list of auto-complete results anyway, because types can't be inferred. In some random OO language, that'd be like trying to 'dot autocomplete' a variable that wasn't declared yet. After you've filled in enough of a function to infer the types of some terms, this is no more or less complex than typed hole fits (which is admittedly fairly complicated, but some enterprising folks solved this problem for us already, so good on them). Figuring out the fixity of a term is based on the leftmost character in the term anyway, and it's a fairly simple bit of logic. If you want to invoke auto-complete on a term before you've typed anything at all, then sure, there are lots of scenarios where that's not going to work, but it doesn't work for exactly the same reason it doesn't work in any other language.
We will update the minor as soon as 8.6.2 is released. By that time, more people can get their packages back in shape into the snapshot. There will still be some time until the LTS containing 8.6, so by that time such bugs won't exist, I hope.
There is the distinction of *undefined* and *unspecified* behavior for this reason.
[removed]
PS: There is a typo in the tile, but reddit doesn't allow to fix typos in the title of postings.
Btw speaking of work, do you happen to be the author of SRC? Am just going to use it at work! Funny coincidence :)
`CW` is not special. The [definition](http://hackage.haskell.org/package/sbv-7.12/docs/Data-SBV-Internals.html#t:CW) clearly shows it is a pair of kind (more or less an enum) and a sum type. The only think special about CW is the [show instance](http://hackage.haskell.org/package/sbv-7.12/docs/src/Data.SBV.Core.Concrete.html#line-234) which, importantly, is not derived.
One possibility is to familiarize yourself with one or more ways to work in an imperative style in Haskell and then once you've mostly rotely translated the code move on to phase two where you take advantage of Haskell's awesome re-factoring experience to gradually transform the translated code into a more functional style as you become more comfortable with it. Finding opportunities to program in a style similar to what is [described here](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html) by Gabriel Gonzalez (/u/tekmo) is one of my great joys in life (and totally pure) but there are other styles as well including things like [TVars](https://hackage.haskell.org/package/stm-2.5.0.0/docs/Control-Concurrent-STM-TVar.html) or the [ST Monad](https://hackage.haskell.org/package/base-4.12.0.0/docs/Control-Monad-ST.html) that can come in handy and at least start to blur the line on purity.
Perhaps for me this is easier in dependent types; a kind of dependent pair, where the first element is a type and the second is an element of the type \[(Char, 'd' : Char), (Integer, 3). What I didn't see that in SBV this is solved without static type safety; e.g., you call 'fromCV' and you may get a runtime error there: let myDict = getModelDictionary x let newValue = myDict ! "newValue" let newValueAsInteger = (fromCW newValue) :: Char \-- Char leads to runtime error (in my example) while Integer works! &amp;#x200B;
I guess this bug is another good candidate for a putative 8.4.4, though I'm not hopeful that such a beast is ever going to exist...
I don't know where you got this info ... number of constructors is definitely irrelevant. If you're interested in more details please follow the ticket.
Here's my personal but incomplete list of issues I'd like to see fixed in GHC 8.4.4: - Suppress `ignoring (possibly broken) abi-depends field for packages` warning one way or another (maybe backport https://git.haskell.org/ghc.git/commitdiff/1626fe600672d3dabcf95d11a6c16da5f5ec1068) - https://ghc.haskell.org/trac/ghc/ticket/15436 (Compile-time panic `Prelude.!!: negative index`) - https://ghc.haskell.org/trac/ghc/ticket/15260 (`touch#` being optimized away sometimes resulting in segfaults; NB: this merely needs to **forward-port** a fix from GHC 8.2 into GHC 8.4) - https://github.com/haskell/haddock/issues/837 (segfault in Haddock due to GHC lexer) - https://github.com/haskell/haddock/pull/892 (invalid argument (invalid character)" on non-unicode Windows) - https://github.com/haskell/haddock/pull/934 - https://github.com/haskell/text/pull/229 - Backport http://git.haskell.org/ghc.git/commit/8f3c149d94814e4f278b08c562f06fc257eb3c43 - `stm-2.4.5.1` (contains `flushTBQueue` hotfix, see https://github.com/haskell/stm/issues/16)
Yes, I am :)
What level of Haskell knowledge and experience are you looking for? Knowledge wise: intermediate, working, PhD in category theory, &amp;c. Experience wise: projects, playthings, work experience, &amp;c.
At my previous employer we used https://github.com/haskell-mafia/mafia . &gt; Not trying to repeat a mantra, just we’ve discussed how losing the upper bounds improves thing for users of both stack and cabal. Ah so this is where you misunderstanding comes in. Removing upper bounds helps stack at the *expense* of making cabal usage more painful. Stack provides a curated lists of packages that work together. That means whoever puts that list together can make sure the upper bounds (or lack of them) work together. Cabal users however need the upper bounds for cabal to be able to generate install plans. 
Intermediate is usually enough. No paid work experience is required as long as you can write reasonable code and explain your motivation when writing it. We use our code sample task + video interview to decide.
&gt; Cabal users however need the upper bounds for cabal to be able to generate install plans. We may split that into several use cases: * Good-A: Suppose I'm using GHC `X` when `Y &gt; X` is already released for a while and Cabal correctly chooses the un-updated library versions. Upper bounds are good here. * Good-B: Suppose I'm using the new GHC `Y` and am considering which dependency to choose and Cabal correctly and quickly tells me which packages work with my setup and which don't. * Bad-A: Suppose I'm using the old GHC `X`. I've done `cabal update` and now I can't install my usual dependencies at all because of the infamous cabal hell. At this stage I do the ceremony of removing my package database and re-installing Haskell with the official GHC packages (and not the Haskell Platform due to bad past experiences). When I do that, experience tells that most likely hackage is better stabilised for successfully creating build plans for the newer GHC `Y` so I go for that route. * Bad-B (continues from Bad-A): I'm now using the new GHC `Y`. Some important dependencies don't install due to upper bounds. When I go to their github repos some are already updated there so I install from there. For some, it's only fixed in a pull-request. Some require more maintenance, maybe just hiding an open import, but all of the hassle adds up. I end up spending a unexpected block of several hours to get my Haskell setup working again. Certainly the upper bounds helped use cases Good-A and Good-B. But from my experience and intuition those wouldn't be common.
The answare is very simple, it will be a game, and this website just provides public informations to this game. The passcode is (maybe) necessary because we will make as it's a mailer program and you can send a "mail" to the "gamemaster" (the gamemaster is a program) for help. 
Err, why 5.13? You lost me...
&gt; I can't install my usual dependencies at all because of the infamous cabal hell. Cabal hell is mostly avoided by using cabal sandboxes. The original design of user global cabal install was badly broken. 
&gt; When we restrict what we can do, it’s easier to understand what we can do. So nice to see that such principles are appreciated! I'm working on the alternative prelude called [`relude`](https://github.com/kowainik/relude) and one of the goals for this prelude is to eliminate partial functions like `head`. So we could have function of type `head :: [a] -&gt; Maybe a` but instead we decided to go with the following type signature: head :: NonEmpty a -&gt; a This design decision makes the library less beginner-friendly (people might not expect such type of the `head` function) but on the other hand it makes the interface more type-safe. * http://hackage.haskell.org/package/relude-0.3.0/docs/Relude-List-Reexport.html#v:head Another reason for this: if you have function of type `head :: [a] -&gt; Maybe a`, you can't express `head :: NonEmpty a -&gt; a` using your `Maybe`ised version of `head`. But if you have `head` that works with `NonEmpty` it's quite easy to implement `head` that returns `Maybe a`. And `relude` even has the `viaNonEmpty` function for that: viaNonEmpty :: (NonEmpty a -&gt; b) -&gt; ([a] -&gt; Maybe b) See the docs with examples here: * http://hackage.haskell.org/package/relude-0.3.0/docs/Relude-List-NonEmpty.html#v:viaNonEmpty
A Monad is always a Functor, so that's a bit redundant.
It would also break a law I would expect: `pure . unpure = id` ;)
In principle I agree with the idea, but IMO the Haskell type system is just not robust enough to make tracking complicated domains and ranges feasible. I mean what is the correct type signature for `stdDev`? The usual formula is undefined when the container is has less than 2 elements, but it’s *also* undefined when all of the items in the container are the same value (you’ll get a divide by 0). And this is not a hypothetical ivory tower bug—one of the only crashes I’ve ever had in production was due to this exact issue! So ya, I’d love to write the real type signature for stdDev, but doing that in Haskell is just not feasible. You’d really need to build your entire stats framework with proper dependent types, and that’s... *hard*.
It does allow you to fix typos in comments though :)
could you link a CI config file of yours that uses `hackage head`?
Sorry, nothing public I can link you to. I keep a separate `cabal.HEAD.project` with the `repository head.hackage` stuff in it, and a simpler `.travis.yml` file. So mine go something like this: ``` .travis.yml: matrix: include: - apt: blah blah hvr-ppa blah cabal-install-head blah ghc-head env: PROJECTFILE=cabal.HEAD.project script: - cabal new-build ---enable-tests --enable-bench --project-file=PROJECTFILE - cabal new-test ---enable-tests --enable-bench --project-file=PROJECTFILE cabal.HEAD.project: packages: blah etc repository head.hackage url: http://head.hackage.haskell.org/ secure: True root-keys: 07c59cb65787dedfaef5bd5f987ceb5f7e5ebf88b904bbd4c5cbdeb2ff71b740 2e8555dde16ebd8df076f1a8ef13b8f14c66bad8eafefd7d9e37d0ed711821fb 8f79fd2389ab2967354407ec852cbe73f2e8635793ac446d09461ffb99527f6e key-threshold: 3 ``` 
So, it's a nice example of dynamic programming: compressing the result space using memoization. While this can be useful as built-in functionality of the language, it's not something that cannot be done explicitly in a strict language. 
This is true for floating point numbers, but not Int or Integer, which I assume this code is using. The comment above saying to use `takeWhile (\d -&gt; d*d &lt;= n)` is the better answer IMO, since integer multiplication for both Int and Integer will be a single, lower latency instruction, which also avoid the conversion from Int to Double to Int.
Yes, sorry, I meant &lt;4.13. 
Nope, what I said is correct - try compiling the program from comment:10 and you will see the wrong behavior only with at least 9 constructors, you can see that the deriving code using dataToTag# with -ddump-deriv. I got this info because I investigated the ticket.
Number of constructors is a red herring. Please read the ticket more carefully. PS. I'm osa1 on trac.
If I compile the program from comment:12 with 9 constructors, it gives LT, if I use 8 constructors, it gives EQ. I know the root of the problem is dataToTag#, which needs just 2, but it's really rare to use dataToTag# directly, in most cases it comes from derived Ord code. I'm monoidal on trac.
In my experience qualified imports are very common. Especially the following patterns is used a lot: ``` import Data.Map.Strict (Map) import qualified Data.Map.Strict as Map ``` (This imports all functions under the `Map` namespace and the type `Map` is directly usable.) And this is even suggested in the documentation of the `containers` package.
Thanks that explains it well
Preferring qualified imports is a practice that I would recommend, but I'm always too lazy or forgetful to do it myself :P That's probably why they're not used more.
Your third point seems redundant. It should probably be replaced by derived instances of `Enum`, which use `dataToTag#` to implement `fromEnum`.
It's a red herring *and* it's practically relevant. Y'all are both right.
I'm not quite sure I understand where the divide by zero is coming from, but in any case: You can use the `gdp` idea here to write the type you want; naming the list of values going in to `stdDev` gives you a mechanism for talking about constraints on those values, too. Something like this: -- newtype a ~~ name = Named a data AtLeastTwo s data NonzeroVariance s checkAtLeastTwo :: ([Float] ~~ samples) -&gt; Maybe (Proof (AtLeastTwo samples)) checkNonzeroVariance :: ([Float] ~~ samples) -&gt; Maybe (Proof (NonzeroVariance samples)) stdDev :: Proof (AtLeastTwo samples) -&gt; Proof (NonzeroVariance samples) -&gt; ([Float] ~~ samples) -&gt; Float You can get fancier by stuffing the `Proof` terms into phantom type variables or constraints to make the user experience better, but this is the basic idea.
I put the third point to note that for some programs ghc-8.6 gives a bad result, while ghc-8.4 gives a correct one. Here's an example: https://github.com/haskell/containers/issues/568#issuecomment-425771435. This one relies `Ord` but I agree that for `Enum` the issue can also show up (and most likely for other classes).
Multiplying an `Integer` is not a single machine instruction. It's a huge loop. The naive version is O(n²), but it can be done in approximately O(n lg n) asymptotically. For this case, the `takeWhile (\d -&gt; d * d &lt;= n)` does work, but if you want an actual square root algorithm that does work over the whole range of `Integer` (or better, `Natural`, since that can't be negative), you'll want something like this. nsqrt :: Natural -&gt; Natural nsqrt n | n &lt;= 1 = n nsqrt n = go 1 where go x | x `seq` False = undefined go x | xsq &lt;= n &amp;&amp; xsq + x2 &gt;= n = x | otherwise = go $ div (xsq + n) x2 where xsq = x * x x2 = 2 * x The iteration condition is equivalent to `x² ≤ n &amp;&amp; (x+1)² &gt; n`, but it reuses the x² and 2x terms that will be used for the next iteration. Side note: If you have a function that can quickly find the base-2 logarithm of a `Natural`, then a better choice for the initial guess would be `go $ bit $ div (nlog2 n) 2` rather than `go 1`.
&gt; I'm not quite sure I understand where the divide by zero is coming fro Ah, looking back I was using mean distance from the mean, not standard deviation, my bad. Anyway the point is with smart constructors, yes you can constrain your API, but unlike actual dependent types, the information gained from the check is not actually available to the compiler at the use site, so it’s still sort of an “incidental” safety rather than a rigorous verification.
That thing exist, only has different names depending on the nature of M: extract, unsafePerformIO, fromJust, memoize....
&gt; unlike actual dependent types, the information gained from the check is not actually available to the compiler at the use site, so it’s still sort of an “incidental” safety rather than a rigorous verification. Sure, it's really a question of whether you need verified code (in which case... stop using Haskell, the type system is unsound!) or whether you just want the compiler to check that your complex domain-specific preconditions have been met (in which case, the `Proof`-passing style suffices)
Well I think it's a bit more than that: it's that with the smart constructors approach, you have to already know the pathological cases ahead of time, whereas with actual dependent types, you *do not* need to know the pathological cases ahead of time, you will simply bump up against the type checker and then discover them as you try to write your formula and realize you need to add more preconditions. In my case, the "the data can't all be the same number" case is something that did not occur to me when I was writing the original formula, so making smart constructors would not have helped because it didn't occur to me that I needed one for that.
I use qualified imports everywhere. At first, I just started doing it in code that others might read. Eventually, I realized that it was helpful for me too.
Another insidious problem with unqualified imports is that the compiler doesn't warn you about redundant ones. Which means that if you recompile that module, then the only-dependent-in-name module will be recompiled, even though it doesn't need to be. So I typically use unqualified imports only for Prelude-style modules.
That depends on whether there are smart constructors downstream of you. If you end up dividing by a result and your division requires an explicit assertion that the number is non-zero, I expect that you'll think for a few seconds about how it can be zero.
It might be relevant to write up how to do a few common, no brainer use case. Like a simple, goal oriented, summary of [Haskell-nix](https://github.com/Gabriel439/haskell-nix) I am convinced non-stack tools are useful, but occasional users should expect to be able to run simple self-contained commands, to play straightforwardly, without getting a phd in ad-hoc building tools. Stack is excellent at this : if I see a project with a stack file, I know I can build it. I know I wont waste my time with how to get the environment right. Is the simplest way to get 8.6.1 installed in a shell what you mentioned `nix-shell -p haskell.compiler.ghc861` Do you lookup entries from [here](https://github.com/NixOS/nixpkgs/blob/master/pkgs/top-level/haskell-packages.nix) ? Is there a similar one stop shop to an entire toolchain, say emacs + ghc 8.6.1 + ghc-mod + a vetted package set ?
True, but you said "it would not be a Functor", which isn't correct.
you mean it's not a middle finger?
Because it adds noise to every identifier you qualify. &gt; ....is to inspect its type in GHCI. The compiler is your copilot. You can also use `:i` to get more comprehensive information about an identifier. Some editors have integrated support for getting such information from the compiler. The only time I add a qualified import without thinking about it is if I know the module can heavily conflict with another commonly imported module (e.g. Data.Text or Data.List.NonEmpty).
Is that the same thing as Void?
&gt; Because it adds noise to every identifier you qualify. This is where I disagree. I see the qualified module name prefixes not as noise. Only in the mentioned example of working with a DSL in a module, such as writing a HTML template, it would be noise, because then you are giving &gt;90% of the identifiers in the module the same prefix. Yes, Haskell's compiler is super-charged, but if I do not have to consult it at all because I already know what something refers to, wouldn't that be nicer?
Prior to this ICFP (2018), I was not understanding an advantage of algebraic effects. I thought the point was to try to "recover" commutivity of effects; something not present in monad transformers. I then proceeded to effectively dismiss the system; since State+Maybe is never going to be the same a Maybe+State. Instead, the goal is to separate the *use* of effects from the *handling* of effects. So, Throw/Catch (Either?) and Throw/Fail (Maybe) can be used exactly the same way `throw`, without tying them to a specific way of handling it. From a categorical perspective, I think this is exposing the individual functors in the adjunction instead of just the Monad formed. I think by new perspective was due in no small part to the Koka presentations at both HOPE and TyDe. Koka itself uses a (row polymorphic) list of (optionally parameterized + optionally dynamic) effects, so effects must be discharged in a specific order, but it's becoming clear to me that while Maybe+State and State+Maybe will never be the same, we might be able to write a system so that *some* code can be written once to work with either handler order. (If we never get there, the separation of responsibility between the effect signature and the handler is still an advantage.) The HOPE presentation was how we can use linearity of handler to determine how they can be safely composed, with hints about how things like `finally` are related to recovering a linear handler from an affine one. [This pearl at TyDe](https://icfp18.sigplan.org/event/tyde-2018-typing-representing-and-abstracting-control-functional-pearl) was also quite interesting. While it's in Idris; it seems like it might be possible to use inside the Koka compiler -- effectively it provides a way to go to/from CPS style without introducing extra eta or beta redexes -- where effects are basically translated to/from CPS; handlers call `resume`, but that's very closely related to `reset_i` depending on how far up the effect stack you had to go. Anyway, this is just a mild ramble and probably not closely related to the paper, but I do encourage anyone not already enamored with algebraic effects to take a look at some of the Koka papers; I'm not sure what it was about them, but it made things make so much more sense to me than the `Eff` type in Idris.
please make that config available through some nix (overlay?). those tooling issues are a recurring pain.
As a type, yes. Haskell spells it `Void`, though most logics / type theories spell it \_|\_ (but in Unicode). In the Haskell report though, \_|\_ (pronounced "bottom") is used as a *value*. Specifically, the value assigned to unbounded recursion, imprecise exceptions, `undefined`, calls to `error`, etc. Any place Haskell wants to be unsound.
There's nothing special about `CW`; no heterogeneous lists are used: It simply embeds all possible `Kind`s. (i.e., different types of values that can be symbolically represented.) Note that the use of `getModelDictionary` and `fromCW` is now considered old-style and is deprecated; partially because of the problem of getting a dynamic run-time error like you mentioned in your comment. To access values in a type-safe manner, use a query and the `getValue` command. There's an example at the top of this [file](https://hackage.haskell.org/package/sbv-7.12/docs/Data-SBV-Control.html)
For PVP compatibility within a major version, you must import *everything* qualified or explicitly. Otherwise, you might be broken by a dependency *adding* a new symbol. If you do *any* unqualified "wildcard" imports, you *should* use an upper bound of the next **minor** version. This is why I import everything qualified or explicitly *except* for imports from other modules in the same package. --- On the flip side, when symbols are moving around between packages, it might be easier to support a wider range of dependencies *in practice* by using an unqualified "wildcard" import. I've only put in one patch into recursion-schemes, but ISTR there were a lot of unqualified "wildcard" imports and at least one symbol that came in from a "different" import depending on the specific versions of the dependencies we were compiling against. For example, `(&lt;&gt;)` moving from `Data.Monoid` to `Data.Semigroup` or the like.
The `==x.y.*` style is actually preferred over a `&gt;=x.y &amp;&amp; &lt;x.y+1` bound fairly universally in cabal files. Neither the first bounds (on `base`) or the last bounds (on `integer-gmp`) can be turned into a `==x.y.*` style bound, so they aren't.
Do other languages/compilers also face such bugs when it comes to optimisation and inlining? How do more "battle tested" languages deal with such stuff? Any learnings?
And, is there no test suite for such stuff that should've caught this? Would the recently released `inspection-testing` library have been of any help here?
I think every major production compiler has bugs in its optimizer. It is possible to formally verify compiler transformations, but doing so for an entire optimization pipeline in an industrial-strength compiler would be a huge undertaking.
Dante doesn't need any special nix config to use. It'll often work out of the box.
GHC has an enormous test suite run in a variety of different ways by a validation system. Every commit is validated, and a more thorough validation is run for every release. Unfortunately, this particular bug managed to slip through the cracks. When it's fixed, the test suite will be expanded to (hopefully) ensure that the bug doesn't return.
Well, the depends who you ask. I know a lot of people who would consider bottom a middle finger. 
The `Maybe` version would just ignore the payload.
Maybe it helps your intuition for foldl to see the foldl-via-foldr rewrite? (The real `or` is defined via foldr to fuse, this is a tradeoff of no stack allocation vs short circuiting) or = foldl (||) True or ls = foldr (\cur -&gt; (. (|| cur))) id ls True or xs = go xs True where go (x:xs) = (\cur -&gt; (. (|| cur))) x (go xs) go [] = id or xs = go xs True where go (x:xs) = go xs . (|| x) go [] = id or xs = go xs True where go (x:xs) acc = go xs (acc || x) go [] acc = acc The last version has much easier to understand strictness - `foldl` always consumes the entire list skeleton and the list ellement strictness purely depends on the step function. Constructor specialization then transforms this to or xs = go xs True where go (x:xs) acc | acc = go_s xs | otherwise = go xs x go [] acc = acc go_s (x:xs) = go_s xs go_s [] = True which makes it obvious that we only force list elements until the first True. Anyway, I totally aggree with you. Accurate strictness would have to be recorded as a (turing complete) function from result strictness to argument strictness. But that's hard to reason about both for people as well as tooling. 
But others might not. Having up-to-date, one-point-click solution is great. I am using Dante as well. After upgrading to a new emacs version, I have no idea why, say, flycheck, does not recognize packages referenced in my cabal files. error: Could not load module ‘Text.Parsec.String’ It is a member of the hidden package ‘parsec-3.1.13.0’. You can run ‘:set -package parsec’ to expose it. (Note: this unloads all the modules in the current scope.) Use -v to see a list of the files searched for. although I can `stack build` [this project](https://github.com/phadej/idioms-plugins) correctly. It would be great to factor out basic configurations. Everyone is spending time to get basic editor support, preciously sticking to brittle config, when there are only a handful of configurations, and the integration presumably has already been done by someone else.
Need this. I'm very lost as soon as we move into DataKinds, GADTs, TypeFamilies etc. I think I understand things in principle, but if the book at all talks about where the footguns are and what use cases are good and not good for type level programming would benefit me greatly and I would gladly pay a modest amount of money for that knowledge.
&gt; and I have to ln -s which ghc ghc-8.6.1 apparently Huh? Why did you have to do that? Here's what's supposed to work without issue: $ git clone https://github.com/phadej/idioms-plugins $ cd idioms-plugins/ $ nix-shell -p cabal-install haskell.compiler.ghc861 [nix-shell]$ cabal update [nix-shell]$ cabal new-build And in fact that *does* work without issue for me, on, NixOS, Ubuntu, and macOS. So I'm very surprised that you're having issues, and *strongly* suspect it has something to do with the `ln -s` thing that you probably didn't need to do. The only warning I get similar to yours is this, but it still works fine: Warning: Unknown/unsupported 'ghc' version detected (Cabal 2.2.0.1 supports 'ghc' version &lt; 8.5): I don't quite think that nix-shell command line is simple enough for drive-by, occasional users; the syntax is a bit arcane, and `nix search` doesn't help discover it like you'd hope it would. But it's not that bad. Here we're relying on cabal's version solving rather than a stable package set, which is fine for most drive-by usage. I will freely admit that trying to use the complete Haskell infrastructure in nixpkgs for building and pinning libraries with Nix itself is absolutely ill-suited for someone not reasonably familiar with (or wiling to invest time into) Nix, because of how unobvious it is to figure out how to use it at first. But once you've got the basics it's still pretty easy. # default.nix { pkgs ? import &lt;nixpkgs&gt; {} }: # nixpkgs can (should) be pinned here if you like pkgs.haskellPackages.callCabal2nix "idioms-plugins" ./. {} bash$ nix-build &gt; Stack is excellent at this : if I see a project with a stack file, I know I can build it. I know I wont waste my time with how to get the environment right * This has been far more true with any halfway decent `default.nix` file than `stack.yaml` for me. As long as `nixpkgs` is properly pinned, and the author actually tested stuff on your OS, `nix-build` is far more likely to be reproducible than `stack build`. This is because as long as nixpkgs works on your OS, literally the entire build is reproducible, down to the system dependencies like the C compiler and libc.
&gt; It would be great to factor out basic configurations Sorry, I have nothing to factor out :P I have literally zero dante config these days. &gt; Other languages have really superb editor integrations, I dont see why it's not the case for haskell (afaik, I might not be up to date on that one) Because editor integration is absurdly hard. The only languages with great editor integration are the ones backed by multi-million/billion dollar companies. haskell-ide-engine is showing a lot of promise though, punting the bulk of the work off on those big companies via the language server protocol. Once HIE gets into a released state (no idea what the blockers are; it works quite well), I think easy editor integration for Haskell will be within reach.
Have you tried telling the parser the fixity of `^.`? [One of the fields here seems relevant.](https://hackage.haskell.org/package/haskell-src-exts-1.20.3/docs/Language-Haskell-Exts-Parser.html#t:ParseMode)
Wicked! [The book](https://www.patreon.com/isovector) is coming along in full force these days if you want to check it out!
There was once a [non-breaking, backward-compatible proposal](https://mail.haskell.org/pipermail/haskell-cafe/2015-June/119942.html) that would have allowed: import Data.Map (Map) as M instead of import Data.Map (Map) import qualified Data.Map as M ... but it looks like the [bikeshedding killed[(http://bikeshed.com/) it dead dead dead. Maybe it could be looked at again?
Ya, that does make sense. I don't know, maybe I should try this approach more. Still, you end up with all sorts of issues from there being no real semantic basis for the proofs - for example, if you use smart constructors for finite naturals like `Fin50` to say "I have a number less than 50," you end up with the problem that there's no actual connection between `Fin20` and `Fin50`, so you have to make up a new rule that says "ok ok, `Fin20` is convertable to `Fin50`", and then it just compounds as you encounter `2 * (x :: Fin20) :: Fin 50` so you have to drum up another rule that says "ok ok, 2 * `Fin2` is also a `Fin50`" and it becomes death by a thousand cuts. You're not playing a game by the rules - you're making up the rules as you play. Of course, you can try to do this properly with fancy type-level stuff, and maybe it's gotten better in newer GHCs, but honestly my experience in attempting this sort of thing in Haskell has been terrible.
Which package did you notice to do that? I try to pay attention that we use accurate bounds such as `&lt; 4.13` over most packages bundled with GHC, e.g. see - http://hackage.haskell.org/package/unix-2.7.2.2/dependencies - http://hackage.haskell.org/package/deepseq-1.4.4.0/dependencies - http://hackage.haskell.org/package/ghc-8.4.3/dependencies - http://hackage.haskell.org/package/filepath-1.4.2.1/dependencies - http://hackage.haskell.org/package/directory-1.3.3.1/dependencies - http://hackage.haskell.org/package/ghci-8.6.1/dependencies - http://hackage.haskell.org/package/parsec-3.1.13.0/dependencies etc.. 
That did it. Thank you so much, I don't know how I managed to overlook that! 
I was assuming that `network` is a boot package. Perhaps it isn't, but is maintained by the Haskell… ehm… community? Same with `time`, `bytestring`, `Cabal`, `containers`, `text`, `mtl` and generally any library that is managed at github.com/haskell, they all use &lt;5.
Wow, that's a very impressive commitment! Do you get IDE help for that? But wouldn't it make sense for you to use `PackageImports` *instead* of the comment? Same number of keystrokes and now your comment is code!
&gt; Only in the mentioned example of working with a DSL in a module In a large Haskell project there is a tendency to write most of the code with most of the same imports over and over. That's why qualified imports turn into noise. Noise in the sense of redundant information. Tutorials and examples by their nature can't demonstrate that. &gt; Yes, Haskell's compiler is super-charged, but if I do not have to consult it at all because I already know what something refers to, wouldn't that be nicer? You already consult the compiler at the most critical stages of creating a program. May as well take advantage of it at all stages.
&gt; same number of keystrokes No. The package name goes into the import statement, like so: import "package" Module1 import "package" Module2 import "package" Module3 Which means the package name gets repeated on each line, as opposed to a single comment at the top of the import group.
What does "lifted" mean in this context?
thanks!
Well, I was sure it was a way to qualify "Boxed" values, such as `Int`, versus "Unboxed" ones, such as `Int#`. But I'm now unable to find any link which confirm this, so I may be wrong. By the way, In this context, I was meaning all types which are "Boxed" and capable of being `undefined`
If you put it in parenthesis, it looks a little like a bottom `(_|_)`
As I understand, "lifted type" is a type with added `_|_`. [This might be helpful.](https://stackoverflow.com/questions/39985296/what-are-lifted-and-unlifted-product-types-in-haskell#39986010)
Comma splice in the first sentence... should be a decent paper
You get unused import warnings, regardless. I don't follow.
People do it by convention because if you are just using limited portions of base it is 'unlikely' that a base upgrade will break you. If it turns out to, you can patch it on hackage with a revision. If 99% of the time this isn't an issue, 1% of the time it is, then by taking the more correct tighter bound you're just multiplying your workload by ~100x and making it harder to test the new release of ghc when it finally comes out. I tried it the other way for a while, but it became a serious maintenance burden. I adopt an at least vaguely related practice. When I incur a dependency on my _own_ packages I use the first digit as a bound, and try to maintain compatibility with myself by bumping the first major digit when I know I'll break myself downstream.
Using GHCi is a pretty unreliable way of testing performance. Compile the code and see what the result is.
You mentioned using ghci to determine which solution is faster. That sounds fishy; I think that you need to actually compile the solution and benchmark it properly using criterion or gauge or so. Alternatively, profile the solution and look at core. Otherwise it really is just guessing what is causing it. One such a guess might be that the packing and unpacking of the lazy pair is slow. If so then using a strict pair might help. But again, this is just guessing. You need to actually benchmark it to figure out what is happening exactly.
&gt; I would have assumed h' to be faster as it only travels over a structure once where as h travels over the structure twice, however ghci says otherwise. It may be faster with sufficiently large inputs, but that may not be showing up in your tests. `FoldAndMap` is going to be allocating tuples and lazily accumulating thunks, which can slow things down pretty hard. 
It's just the beginning, it was like that live. I don't think the sound crew were quite prepared for Simon's energy levels.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [LumiGuide/haskell-opencv/.../**Features2d.hs#L41** (master → 65b9f38)](https://github.com/LumiGuide/haskell-opencv/blob/65b9f38677aaf7f0bb63261b4ddf20802dc911f8/opencv/src/OpenCV/Features2d.hs#L41) ---- 
Why do you say that `h'` only traverses the structure once? If `xs` has *n* elements, `traverse k xs` will take *n* steps to create the function `h :: m -&gt; (t b, m)`, and then once you call `h m`, the implementation of `h` will take *n* more steps in order to compute the answer.
I think it helpful to think of bool being a set of three elements: True, False, and Bottom. The first two one can be constructed with their data constructors, Bottom represents a computation that results in Bool, but does not terminate.
Suppose your program manipulates many different types of numbers, and you decide to use the type system to avoid errors. Do you use `newtype`: newtype Dollars = Dollars Int newtype Age = Age Int Or phantom types: newtype Value a = Value Int data Dollar data Age or `DataKinds` and `TypeSignatures`: data ValueType = Dollars | Age newtype Value (a :: ValueType) = Value Int or something else? Are there concrete reasons to prefer one representation over another?
What advantages does Cabal has over stack? I know Cabal was hell and apprarently got better with stuff like new-build, but as regular stack user I am not sure if stack is basically a superset of cabal's features
Algorithm 1 of this paper is what I'm working on right (in the little spare time that I have): [A PERSISTENCE LANDSCAPES TOOLBOX FOR TOPOLOGICAL STATISTICS](https://hal.inria.fr/hal-01258875/file/PersistenceLandscapes_arxiv_v3.pdf) &amp;#x200B; I think the State monad will make things a lot easier.
Will further references to `cborg`, say in `build-depends`, then reference this source version?
This is awesome news! This was the last blocker stopping me from migrating a project to cabal in a company environment because we have multiple packages that can't be uploaded to Hackage.
It also breaks [𝗛𝗮𝘀𝗸](https://wiki.haskell.org/Hask) as a category.
Looks great. I'll try in a year ).
idk (i just saw this myself while skimming the new docs, and I'm traveling so I can't try it), but.. iiuc, a `source-repository-package` gets vendored, as does anything under `packages` or `optional-packages`. and the latter do accept tarballs: &gt;Package locations can take the following forms: &gt; &gt;They can specify a Cabal file, or a directory containing a Cabal file, e.g., packages: Cabal cabal-install/cabal-install.cabal. &gt;They can specify a glob-style wildcards, which must match one or more (a) directories containing a (single) Cabal file, (b) Cabal files (extension .cabal), or **(c) tarballs which contain Cabal packages (extension .tar.gz).** For example, to match all Cabal files in all subdirectories, as well as the Cabal projects in the parent directories foo and bar, use packages: */*.cabal ../{foo,bar}/ &gt;[STRIKEOUT:They can specify an http, https or file URL, representing the path to a remote tarball to be downloaded and built.] (not implemented yet) also, I assume that if both `source-repository-package` and `optional-packages` reference (directories containing) cabal files of the same name (e.g. `cborg.cabal`), that `optional-packages` takes precedence. can someone confirm this when they've tested it out, or if I've missed some thing in the documentation?
There is recent research about verifying the validity of various parts of the compiler pipeline, but were still at the level of things like "ensure that the semantics of a program fragment do not change when we apply this peephole optimization". Here is an interesting recent paper about a *research agenda* for making more of the compiler optimization pipeline machine-written and verified: &gt; This paper outlines an agenda for making compilers better by incrementally removing hand-written parts and replacing them with components derived using automated theorem provers, formal semantics, and data-driven tools. Although making this happen will require solving many interesting research problems, we have focused on the engineering advantages of the proposed approach, which we believe are clear and significant. https://arxiv.org/pdf/1809.02161.pdf
i assume so (as long as its the right subdir, with a `cbor.cabal`).
wow he finally updated his slide template to something readable :D
DataKinds is an optimisation over the general idea of using phantom types, in the sense that it restricts the "values/types" that the phantom placeholder is allowed to take. So, it's newtype vs phantoms. Newtype - each type gets a unique data constructor and you will not end up with ambiguous types. However, if there are common typeclasses or functions that you need for each such type, you'll end up with boilerplate. The boilerplate is probably solvable by the recent `deriving via` but I would caution against it (GHC deriving stuff has caused me far too many headaches than its worth). TH is another way to solve the boilerplate. Phantom - no boilerplate for common functions and typeclasses BUT you will need to put explicit type annotations at a number of places. For example, can you tell me the **concrete** type of `Value 10`? At a number of call-sites even the compiler can't. 
By default I would go for the first solution, it's the simplest one. What distinguishes the two latter solutions from the first is that it makes it explicit that the underlying representation of both ages and dollars is the same. It can be useful if you want to define operations on both dollars and ages, but nothing else (whereas with a typeclass you would have to define separate instances for both `Dollars` and `Age`). However if at a later point you decide to change the representation of ages but not dollars, then it's going to take more work than if they were separate in the first place. Since you mention dollars, we can imagine a library to handle currencies. Then a `Value` indexed by a type may be appropriate. We can provide a few common denominations, but that also allows users to define less common ones themselves: they can just declare a new `data ISK`. Furthermore, it seems reasonable to use a common underlying type for all of them, and in that regard `newtype ISK = ISK Int` would be somewhat more redundant.
Thanks, but I guess it's not really possible. I had 500MB of swap already - any more isn't allowed by their system. I just need a box at home I can build stuff on, I guess. But it's a pity you need so much computing power to build GHC!
&gt; https://news.ycombinator.com/item?id=18118874 i posted this to hn 
Stack snapshots sometimes stay on outdated versions of packages for a long time, because of the constraint that all packages in a snapshot must be mutually compatible. Cabal only ensures mutual compatibility for your (transitive) dependencies, so you'll have more recent package versions.
They will be! But not finished yet. 
Your analysis is incorrect: traverse takes time proportional to the size of the structure (which may be larger than the number of elements n), but the function that h' creates will still have size n. Concrete example: trees in which nodes may either store a value of type a, or no value at all. You can then create a tree that has n leaves storing values of type a, but an arbitrary number m of nodes. The h' solution then requires m+n steps whereas the h solution requires 2m.
Of course the type `Task c k v` is exactly the type of free `c` generated by the `PStore k v` functor, at `v` where `PStore` is the paramterized store comonad, data Pstore i j x = PStore {pos :: i; peek :: j -&gt; x} deriving Functor See "[A Representation Theorem for Second-Order Functionals](https://arxiv.org/abs/1402.1699)". So `Task Applicative k v` is `FreeApplicative (PStore k v) v`. `Task Monad k v` is `FreeMonad (PStore i j)`, which a the type of [Van Laarhoven free monad](http://r6.ca/blog/20140210T181244Z.html), which is in turn just a different representation of the same free monads that are used in [data types à la carte](https://www.cs.ru.nl/~W.Swierstra/Publications)/DataTypesALaCarte.pdf), which I presumed is where the `à la carte` in the paper's title comes from, though it seems Swierstra isn't cited so I guess not. 
Cool - we had no idea - and this was not the reason behind the title. 
Something that hasn't been mentioned is how a large number of libraries on Hackage weren't necessarily designed to be imported qualified. This in part results in extra verbosity or duplication when qualifying imports, for example: ``` import qualified Data.Config as Config Config.newConfig Config.updateConfig Config.resetConfig ... ``` Which is annoying to read, in comparison to the following: ``` import qualified Data.Config as Config Config.new Config.update Config.reset ``` `containers` is a consistent example of the latter, despite the typical usage requiring two import statements: ``` import Data.Map (Map) import qualified Data.Map as Map foo :: Map -&gt; Map foo = Map.insert "foo" 1 ```
There is a very cool tool for such things – [require](https://theam.github.io/require/). With `require` you're able to write require Data.Map instead of import Data.Map (Map) import qualified Data.Map as Map and this will be the same.
In Haskell, `Sing x` witnesses that induction works for `x`, so it's the same. The third version is also definable in Haskell, but there's no point because there's no `ind`.
&gt;or if my comments are just academic nonsense Surely that would never be tolerated on /r/haskell!
This is a slightly more practical post with some insight into what we are actually doing, and how Haskell's refactoring and maintenace story is saving us time in the future.
Just out of curiosity -- is it purely an extension to Shake, or...? (We're on the lookout for a new build system, but nothing seems to *really* hit the sweet spot so far...)
I took _noise_ to mean 'distracting from/diluting the main purpose of what you are looking at'. &gt; People trade off these values differently. Most definitely! The reason for this post is to find out what the community at large things; e.g. to find some common ground. Of course, discussions related to style will always be subjective, and that's fine. I completely agree that 'qualified imports make for more code to read' is a valid argument. It is, in my humble opinion, not a very strong one, but that is another matter. &gt; Maybe another way to put this is that Haskell library authors are somewhat more likely to see their task as being to build something like an embedded DSL (though maybe a very lightweight one), and so fall into your third exception. This might be very true. Interesting contemplation! :-) 
Oh my... This (libraries not designed to be imported qualified) explains why using qualified imports is not completely friction-less. =/
For who's curious: The [etymological meaning of 'Monad'](https://en.wiktionary.org/wiki/monad) means 'something ultimate and indivisible'. It has this name exactly because, in general (i.e. using only the monad functions) two monadic values can be combined, but never be separated afterwards.
Doesn't that stop us being able to recurse arbitrarily? I can see how it might generalise the computation, but wouldn't it get us further away from modeling a build system?
Oh okay thanks, that was exactly the answer I was looking for.
I never thought I'd be nostalgic for Comic Sans.
Am I missing something, or would it be possible to cheat and pass values of type \`Proof (AtLeastTwo samples)\` and \`Proof (NonzeroVariance samples\` that are totally unrelated to the final, input argument \`\[Float\] \~\~ samples\` (obtained from some other values)? &amp;#x200B; Your example is my first encounter with \`\~\~\` so I'm probably missing something?
Wooh! Super cool!
Cool, sounds really promising! :)
Looks similar (at least in part?) to what the [ether package](https://github.com/int-index/ether) was trying to achieve? &amp;#x200B; Although the usage of deriving-via is very neat! Such a simple improvement in user experience and suddenly a whole host of ideas become much more practical with the reduction of boilerplate.
&gt; The reason for this post is to find out what the community at large things; e.g. to find some common ground. All, I see. What I think, in this broader point, is that as a community, we should consider spending less time having strong opinions about the way other people make choices in writing their code. This new culture of absolutes, of "partial functions are always bad" and "I'm a better than you for using strong types"... well, it kind of sucks. Please don't add unqualified imports to that list. I would like for Haskell to be seen as a friendly and welcoming community again, who are doing something we love and having fun and building cool things, the way it was ten years ago. So I think I may be opposed to your goal as a matter of principle. Sorry.
Nice! Are you aware of [haskell-indexer](https://github.com/google/haskell-indexer)? It provides similar functionality via the kythe format. A demo is running [here](http://stuff.codereview.me/lts/9.2/) for lts-9.2. The UI for kythe is more bare-bones but my understanding is that kythe is a standard and that this is a demo UI. Maybe the two projects could merge their efforts? Either way, I'm delighted to see cross-package code navigation getting traction in Haskell!
One UX comment: I would suggest that clicking on a symbol name's at definition site automatically opens "find references".
Yes. Just like with the `packages:` field of a `cabal.project`. If your `cabal.project` points to a local path or some source repository for some packages, then any use of those packages in the projects "governed" by that `cabal.project[.local]` file will be resolved to the local path or source repository.
No reason to apologise! Questions about style, like this one, indeed can quickly become battlegrounds for bikeshedding. But that's not my intent: I want to find out, as a beginner-intermediate Haskell programmer wanting to do more serious projects in the language, what the actual societal idioms surrounding the language are, rather than blindly copying what I know from other programming communities. "It's a matter of style that is very personal but Haskell developers do not fight each other over it." is a perfect answer to my question. :D
`extract . pure` has the type `a -&gt; a` whose free theorem tells us that the only values inhabiting it are identity or bottom. So extract does say something about extracting a pure value due to free theorems.
Nice writeup and library. leveraging \`deriving via\` to take care of monadic boilerplate looks very useful. &amp;#x200B; I'm curious why all the effects are layered over \`IO\` using \`IORef\` as opposed to \`ST s\` and \`STRef s\`? Wouldn't local state be more general? There are many situations where you have pure computations with a bit of local state, after all.
Thanks! Actually, I did not know about Kythe and haskell-indexer when I started to work on Haskell code explorer. I will look if it is possible to convert the output of Haskell code explorer indexer to Kythe format.
Good suggestion! I will definitely add that feature.
Warning: rambling, and maybe derailing the discussion. Sorry! This is cool. I played with something similar recently when noodling around with what eventually ended up as rio-effect. To me this still has the "problem" effect handlers have to be implemented as newtypes with instances. Sure, you can derive a bunch of stuff with deriving via, but it still rules out "anonymous" interpreters. E.g., you give me some `forall m. HasWriter "letterCount" (Occurrences Char) m =&gt; Char -&gt; m ()` and I want to run it with `'a'` and some intpretation of `HasWriter` that is entirely local to the function I'm working in. If there are no instances of `HasWriter` that are suitable, I have to define a new top level monad (or monad transformer). I find this tedious, but more importantly, code is strewn all over the place. In the context of testing or mocking, it's nice to have the handler almost inlined into the test, making it very apparent what the test implementation is doing. For a concrete example, this is what I want to write: test :: Test test = it "calls GET" $ do calledGet &lt;- getLatestRedditPost &amp; handleEffect ( \case GET -&gt; put True &gt;&gt; return dummyPost POST -&gt; fail "POST shouldn't be called!" ) &amp; ( `execStateT` False ) assert calledGet Here I've got some `getLatestRedditPost :: forall m. MonadHTTP m =&gt; m RedditPost`, and I want to make sure it calls `GET`. It's a contrived test, but not a million miles away from something I might want to do. In the `transformers` world, this very specific implementation of `MonadHTTP` has to be moved out of the test, into a top level monad transformer. I find this very unfortunate: newtype CalledGet m a = CalledGet ( StateT Bool m a ) instance MonadHTTP ( CalledGet m ) where get = put True &gt;&gt; return dummyPost post = fail "POST shouldn't be called!" runCalledGet :: CalledGet m a -&gt; m Bool test :: Test test = it "calls GET" $ do calledGet &lt;- getLatestRedditPost &amp; runCalledGet assert calledGet I mention this here because `capabilities` doesn't seem to really change this fact. I guess capabilities is just trying to solve different problems than the ones I have.
In my view, ether stays very much within the realm of the intensional vision that type classes describe the monad stack. In that sense, it is a kind of tagged mtl. In some respect we are trying to reach similar goals. But the design ends up being significantly different.
I'll answer to this one with a link: https://github.com/tweag/capability/issues/13 (TL;DR: I'm very much in agreement, but there is some design to do, and we didn't get to it yet)
There is also a more general [`ReaderRef`](https://github.com/tweag/capability/blob/f1529d502d7c6a3c3eb0feaf488973999b3f6297/src/Capability/State/Internal/Strategies.hs#L194) combinator available that uses [`MutableRef`](http://hackage.haskell.org/package/mutable-containers-0.3.4/docs/Data-Mutable.html#t:MutableRef) (which includes `STRef`).
Brilliant stuff. What JS/UI library are you using?
Thanks ! UI is an \[Ember.js\]([https://www.emberjs.com/](https://www.emberjs.com/)) app.
I've used turtle and shelly for some bash replacement before. I would recommend :)
Hmm, &amp;#x200B; so what happens if I do: &amp;#x200B; let p1 = checkAtLeastTwo \[1.0, 2.0\] p2 = checkNonzeroVariance \[1.0, 2.0, 3.0\] in stdDev p1 p2 \[0.0\] &amp;#x200B; ?
&gt; Please don't add unqualified imports to that list. It's not *just* a style issue. There's a specific, technical [compatibility tradeoff](https://www.reddit.com/r/haskell/comments/9l0a0y/why_are_qualified_imports_not_used_more_often/e73fwgs/) that you are taking, too. You [SHOULD](https://www.ietf.org/rfc/rfc2119.txt) use qualified imports, and if you don't you should have *specific reasons* the advantages you get from unqualified imports are more valuable that the disadvantages.
Explicit imports can be used instead of qualified to retain compatibility and short names in this case. That said, I'd really love to as the Scala / Agda feature of being able to rename a symbol as you import it explicitly.
No, `extract . pure` has type `(Monad b, Comonad b) =&gt; a -&gt; a`. So, first you'll get an ambiguous type error. Second, because of the type class constraints you don't get strong enough free theorems to guarantee it is `id`. I can't come up with a useful (counter-)example right now; the various free constructions actually do guarantee this is `id`, so it'll be something a bit more twisty.
You need to make things noisier to get it to typecheck (the lists have to be named, and `p1` / `p2` are `Maybe` proofs, so you have to add some applicative noise). But in any case, here is what happens when you do the wrong thing vs the right thing: testBad :: Maybe Float testBad = name [1.0, 2.0] $ \l1 -&gt; name [1.0, 2.0, 3.0] $ \l2 -&gt; name [0.0] $ \l3 -&gt; let p1 = checkAtLeastTwo l1 p2 = checkNonzeroVariance l2 in stdDev &lt;$&gt; p1 &lt;*&gt; p2 &lt;*&gt; pure l3 {- typechecking fails here ^^, with • Couldn't match type ‘name1’ with ‘name’ ... Expected type: Maybe (Proof (NonzeroVariance name)) Actual type: Maybe (Proof (NonzeroVariance name1)) -} testGood :: Maybe Float testGood = name [1.0, 2.0, 3.0] $ \myList -&gt; let p1 = checkAtLeastTwo myList p2 = checkNonzeroVariance myList in stdDev &lt;$&gt; p1 &lt;*&gt; p2 &lt;*&gt; pure myList {- ok, passes the typechecker -} 
Is it possible somehow to convince GHC/cabal to build only some of the dependencies dynamically and then shove remaining statically?
Do you mean to dynamically link some and not others or something else such as dynamic resolution of version?
I have filed an issue in which to compare this with `haskell-indexer` (I filed it just before I found this reddit post): https://github.com/alexwl/haskell-code-explorer/issues/2
* dynamic dependency resolution * backpack support * won't download new compilers (sometimes one persons advantage is another's disadvantage)
Have you found a suitable replacement for Turtle or dumped the whole idea?
You would still use `Task c k v v` where appropriate. The idea is that you could build up your `Task c k v v` value in stages via `Task c k v x` values for various `x`'s, which can then be combined using `Applicative` or `Monad` combinators as appropriate.
Have you checked that GHC optimizes the code as well as it optimizes `mtl` code? I recall that performance was one of the main concerns with freer monad approacches.
Those constraints are not on the `a` so they do not give you any way to smuggle out any information that is not the argument or bottom.
Sure, there are reasons. I don't mean to say that this is just about personal taste. (In general, I don't accept "never breaks when a dependency changes" as an absolute requirement, though.). Quite the opposite, I think there are important values in both sides of the decision, which is precisely why this should be decided thoughtfully and deliberately. I just don't see "why aren't other people making the decision I like?" as a productive question to demand people answer, particularly when the answer is nitpicked over word choice when it is offered.
Sure. Haskell is big enough that there's not really just one community, and we don't always agree. :) For example, I use Haskell for math education with children. My usage patterns are going to be very different from consultants building web apps, and they will be in turn different from postdocs doing programming languages research... and THEY will be still different from hobbyists doing live-coding musical performances. It's a rich, diverse world out there.
I have been using Turtle for a while now to do this sort of thing, esp. working on a project that was my first largeish haskell project. Code is not great, but in case it helps in any way: [https://github.com/joelmccracken/reddup](https://github.com/joelmccracken/reddup) I find it much better than writing shell scripts, or writing scripts in whatever language you might otherwise use for the task (Perl, Ruby).
Ah, now I just understood what you meant previously :) (aside: would love to achieve the fluency talking with types at some point, but for now the example worked for me;)) Thanks!
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [tweag/capability/.../**Strategies.hs#L47** (master → f1529d5)](https://github.com/tweag/capability/blob/f1529d502d7c6a3c3eb0feaf488973999b3f6297/src/Capability/Reader/Internal/Strategies.hs#L47) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e75s38d.)
Oh! That's confusing :)
I have used shelly before, it's not bad.
[cabal-rpm: RPM packaging tool for Haskell Cabal-based packages ](http://hackage.haskell.org/package/cabal-rpm)
I am not sure what reasons lead to this situation, but I strongly suspect there are more than one. Not a long time ago, having a project to simply compile was an feat worth celebrating, and always at the peril of some global mutable database. For many, it was not a problem at all. I think we still have a thing or two to learn from other languages who put a strong emphasis on usability and make it a clear priority, along with tooling.
It's not "production" but I've written some work scripts in Haskell using turtle and think it was worth it for what I was doing (can find one [here](https://github.com/cah6/analytics-launcher-script)). I think that it gets to be more worth it as soon as your scripts start doing more complicated stuff. Sure, juggling filepath types and all can be annoying, but to me it's worth it for how easy it is to manage async actions or just mapping over a list of things. If your scripts are a "constant source of maintenance headaches", chances are you'd be able to deal with some silly compile-time type mismatches to get more high-level niceties.
I was just thinking about the type you write down and it looks very much like a van Laarhoven optic with (a) a generalized constraint and (b) an implicit "source"! -- Removed constructor for comparison type Task c k v x = forall f. c f =&gt; (k -&gt; f v) -&gt; f x type Lens s t a b = forall f. Functor f =&gt; (a -&gt; f b) -&gt; s -&gt; f t Looking at the paper, the interesting constraints are all subclasses of `Functor` (or `Functor` itself). Some more threads of thought - 1. A lens has a 1-to-1 mapping (unlike a Traversal) -- the Functor constraint doesn't look very useful in the paper because the dependency chain is linear. 2. It may be interesting to see what happens if you try a profunctor style formulation of the build system. What is the build system analog of a Prism? 3. We do not have any good laws for monadic optics (AFAIK), which is why they're missing from `lens`, whereas `Monad` and subclasses are quite useful in the build system context. I have no idea what to do with all this...
Try updating your nixpkgs maybe? What distro are you on? What version of cabal-install do you get in that shell? Believe me, Stack doesn't "just work," more magically than Nix. It has just as many or more obscure failure points like the one you're experiencing here. But they are obscure, and so is what you're getting here.
I would actually appreciate debugging this issue, opening issues on the appropriate repo. This is the kind of bug I'd like to prevent people from having :)
I wish `cabal` or some tool on top of it would just provide a one-stop solution that produces a static Linux binary, a Windows installer, a OS X application and maybe more, in an opinionated way for all the simple cases.
The source almost become explicit in the definition of `Tasks`, and begins to look even more lens-like. Still unclear if this is really related.
I've used `shelly` for a long time, but recently had to remove it from a project in favour of [`typed-process`](https://hackage.haskell.org/package/typed-process). My particular application was making many rapid shell calls, to the point where the `Sh` monad environment wrapping was becoming a bottle-neck. Overall though, I think these libraries are great for adding power to any otherwise nasty task (shell scripting).
Thanks for sharing, I really enjoy hearing about your experiences.
Looks super neat, but here's a low-level question. I don't recognize this `@` syntax: a &lt;- get @"a" I've been out of the loop for a while, maybe it was added recently? Or maybe I'm just blanking.
I'm very interested! Three questions: 1. What are some projects on which interns have worked in the past? Interested to learn more about what interns would be doing at Tsuru. 2. On what scale would the benefits/compensation be? 3. Can I apply now to start in May/June 2018?
I am glad the book is coming out, but it is really sad to hear all the trouble u/bitmyapp had to go through. &amp;#x200B; Best of luck! Me and many others greatly appreciate all the work you put into the book. It's been my go to recommendation for Haskell and Functional Programming for years.
I think it's now possible to cross-compile a `musl`-based static Linux binary, based on [this](https://github.com/nh2/static-haskell-nix) work. Another choice is compiling stuff in an `alpine` container, but iirc ghc on alpine is pretty old, so you need to compile a relatively new ghc first. Though I'm not really familiar with `Lamdu`, and if the shipped artifact has external dependencies that aren't available as static libs, attempting an all-static build may not be the right choice here.
I've switched to Haskell + Turtle for programs like this and I haven't looked back. Readability being subjective I find the advantage I get from Haskell + Turtle are cross-platform, natively-compiled tools. The type checker saves me a tonne of time. Great concurrency and parallelism story. And my scripts don't bit rot or fail in surprising ways. I don't write bash anymore -- I don't have time enough left in my life to bother! Haskell has disadvantages but I think bash has far more, most of them surprising or not well known: http://tldp.org/LDP/abs/html/gotchas.html A simple comparison: http://www.scs.stanford.edu/16wi-cs240h/slides/turtles.html 
&gt; The final agreement that we arrived at was based largely on the terms Julie demanded. [...] Julie got what she wanted and would benefit from the book’s final publication What was the final agreement? That Chris would sell Julie his rights in the book? Who will be the author(s) listed in the final book? -- I purchased the Haskell Book. I totally love the content and intent behind the book, but its verbosity put me off it. I am looking forward to Christopher Allen's next book which hopefully won't have this shortcoming. :-)
Two other Haskell books to keep an eye out for: * [The Joy of Haskell](https://joyofhaskell.com/posts/2018-08-06-announcement.html) - seems to have had some disagreements over direction as well. * [Intermediate Haskell](https://intermediatehaskell.com/) - though IIRC one of the authors stated somewhere that they no longer have sufficient time to complete the book in time.
Yes, we thought about this generalisation, and even had it at some point in our implementation. However, it appeared to be unnecessary for our build systems abstractions, so in the end we chose to use a simpler type. We might go back to the generalised version in future if it proves useful. As you rightly point out in another comment, the generalisation allows for easier composition of tasks. Both of the papers you mention are on my to-read list. Thank you for insightful comments!
Wow... it is heartbreaking to read the story. I mean we all can disagree each other but lawsuit against each other? I don’t know the full story, but I am disappointed how it ended up. 
Small point of clarification: lawyers getting involved does not imply a lawsuit. I’m not sure of the specifics, but in my experience lawyers are consulted in situations like these to help two parties mediate disagreements and help them come to equitable terms. For example, I consult an attorney before signing any contract of new work, and my employer has certainly contracted an attorney to draw up the contract served to me, but that doesn’t imply a necessarily adversarial relationship.
I haven't searched enough for a suitable replacement to give you an helpful reply. Maybe because I am such a big supporter of Gabriel's work (if he didn't succeed to bring something that I love, who would ;-) 
Ah my bad. Thanks for the clarification. It was me overthinking.
Thank you! Ideas and suggestions are welcomed!
&gt;I usually have a custom prelude that exposes a subset of Turtle. could you link to it?
It is interesting to hear that! I get the `a = k` bit but I'm not sure why you say `s = a`? Are you also setting `t = x = v` to arrive at that?
If you inline `Task` into `Tasks`, you will get something like: type Tasks c k v = forall f. c f =&gt; k -&gt; Maybe ((k -&gt; f v) -&gt; f v) which can shown to be isomorphic to this type: type Tasks2 c k v = forall f. c f =&gt; (k -&gt; f v) -&gt; k -&gt; Maybe (f v) The latter is particularly close to lenses, with `s = a = k`: type Lens s t a b = forall f. Functor f =&gt; (a -&gt; f b) -&gt; s -&gt; f t The isomorphism between `Tasks` and `Tasks2` is shown in this blog post: https://blogs.ncl.ac.uk/andreymokhov/the-task-abstraction/ (note that it uses a slightly different naming).
Well this is not such a great example (the application has a quite narrow and specific use case) but here for instance I have been forcing myself to depend on Turtle in the Main module only. In the library code I won't use it at all. To achieve that I had to write a minimal `Options` wrapper myself (that I do find useful): https://github.com/CIRB/cicd-shell/blob/master/src/Shell/Options.hs It is much smaller than the [original](https://github.com/Gabriel439/Haskell-Turtle-Library/blob/master/src/Turtle/Options.hs#L205) and does not put yourself in a corner (https://github.com/Gabriel439/Haskell-Turtle-Library/issues/237). I have also decided to avoid Turtle.Format and prefer to use [formatting](http://hackage.haskell.org/package/formatting) instead. There is nothing wrong with the Turtle module except that single letter functions won't really fit well particularly in library code (outside of the Main module). 
&gt; Accidentally the whole functional programming &gt; Spend 5+ years teaching yourself haskell &gt; Want to share your knowledge with the world &gt; Teach a girl haskell and programming so she can help you make a book &gt; Halfway through she stops talking to you &gt; You think to yourself, what is she doing? &gt; She's parading on twitter president style about functional programming, haskell, gender, and white people. &gt; She won't help finish the book &gt; You work diligently to finish something you deeply care about &gt; She runs off with [this](https://joyofhaskell.com/images/chris-auth.jpg) to make another book on the same topic. &gt; She doesn't finish that one either &gt; She tells you to lawyer up &gt; Starts a consulting company in the meantime with outrageous monthly fees. &gt; Proceeds to talk shit about you on twitter, epic style. &gt; tfw you were the only one who cared about the readers the whole time. &gt; tfw book cucked &gt; tfw another sjw ruins everything again The absolute state of Chris Allen. Can't say I'm surprised, and I can't imagine he is either. 
Haskell Almanac looks like a great follow up. I wish I could pre-order it now.
Haskell Almanac looks like a great follow up. I wish I could pre-order it now.
&gt; Hackage Trustees have been busy over the last weeks adding those "restrictive base upper bounds" all over the place via metadata revisions to ensure that the constraint solver can do its job and users have a good experience on Hackage. Thank you! This work is appreciated. I'd love to move our maintenance culture more towards package authors testing against ghc alphas, betas, and release candidates, so that they can relax their upper bounds prior to the actual ghc release.
Nice point, though I do wish `Nat` and `Natural` were in `base`, as `Age 5 - Age 6` should result in an `underflow` not `Age 18446744073709551615`.
[From Julie's side](https://twitter.com/argumatronic/status/1041519054098505728)
&gt; That Chris would sell Julie his rights in the book? Going by Julie's comment "_I don't have any more rights in it_" ([ref](https://twitter.com/argumatronic/status/1041519060553330688)) seems like that's what happened
Given that this isn't the first time that Chris has been central to a damaging Haskell community conflict, I believe it's more likely that he has been acting in bad faith than Julie has. I can update my beliefs given evidence that either party has lied about the we way this has gone down.
That is Chris Martin, who is authoring a different Haskell book with Julie (as far as I know).
Oh right, I remember him now. The name collision got me confused. Much nondeterminism.
1. Looking at merge requests of the most recent interns, 29 in total, some got dropped later, cosmetics, minor bugfixes, minor performance stuff, new feed spec, more performance stuff, some money making research. One more intern works on strategies and simulation code. 2. 700k-ish JPY/month + private language lessons in office + something else that I'm likely forgetting 3. Sure
I also prefer the freedom and color of the old ones.
One more to add to the list: [Haskell in Depth](https://www.manning.com/books/haskell-in-depth)
As an aside, I also want to thank the reviewers again; the reviews I received were quite good and helped me substantially improve my submission in some critical ways. This is not meant as a slight against anybody at all. I just think double-blind review is better for the community as a whole.
It's just a statement about bayesian priors. Here's another prior I have kicking around -- when someone makes a point about an agreement including a confidentiality clause (as the linked article does) then it makes one wonder why they're so concerned about it.
Oops, I meant May/June 2019, but you probably got it. Thanks for answering!
It seems like the bigger CS conferences that use double-blind review mostly do "light double-blind", which mostly means that authors do a best-effort attempt at blinding the paper but (1) do not have to blind the name of the projects they are building / extending, and (2) are not prevented from sharing their work online. So yes, in some cases one or more author's identities can probably be inferred. But that doesn't take away the positive benefits of double-blind review for the other authors out there.
The NLP conferences have adopted a "double-blind with anonymity period" policy that I really like. Basically, in the month leading up to the submission deadline and during the first few weeks of the reviewing process, authors aren't allowed to "advertise" their work by posting it on arxiv/social media/etc. Outside the anonymity period, however, anything goes. See for example http://emnlp2018.org/calls/papers/. The Haskell community is much smaller than the NLP community, however, so I'm not sure how something like that should translate over.
Curious as to why the confidentiality clause would bind you and not the other party? Weren't you negotiating as two equals? 
So firstly, my beliefs are at least partially rooted in evidence. Search for the "haskell.org debacle", which was pretty destructive. And secondly, yes! Even if my beliefs are a complete fiction, facts are the best route to change them. The alternative is to make me change my mind by telling me a lie. The phrase "you can't reason someone out of a position you they were not reasoned" is only true if the "someone" continues to think irrationally. You might doubt my ability to be rational for whatever reason, and that's okay, I just hope you won't discourage people from engaging with me just because you my initial beliefs are poorly informed.
One other thing that i believe was not yet mentioned is that we unfortunately do not have qualified (re-)exports (yet), i.e. the ability to abstract over the boilerplate that is writing import qualified Data.Map as Map import qualified Data.IntMap as IntMap import qualified Control.Lens as Lens unless you count CPP and its `#include`. And without that, keeping things (i.e. the qualified names) consistent may be annoying - but you really want consistency for those, at least per-package/project. as for the CPP approach, i have a long-running experiment at https://github.com/lspitzner/qualified-prelude/ which seems to have its merits. See the readme there for the trade-offs. For anything bigger, I use the same approach, but with a custom include file. See for example [the imports "header" in the brittany project](https://github.com/lspitzner/brittany/blob/6dc5561d0847aac42f26fbed2e3b1a6bba23c75a/srcinc/prelude.inc). Of course that approach also adds one indirection to qualified names - the reader will have to consult the `prelude.inc` or however you call that file to find out what the qualified name resolves to. But that is still O(1) instead of the O(n) where n~number of unqualified imports.
I don‘t know the living costs or tax rules in singapore but that salary would be extremely low in my country.
&gt; Why are qualified imports not used more often? I was highlighting that the above symbols containing their parent module name was one reason qualified imports are not more prevalent. They look ugly, repetitive, and encourage people to explicitly import or dump everything into the local namespace.
It's probably per month. I would expect living costs in a big city to be high.
&gt; I'm not sure if a "double-blind except for those whose work is too well-known to be hidden by obscurity" is an improvement? I think it is: the main purpose of double-blind is to mollify reviewer bias, not to give the author some impenetrable notion of crypto-anonymity.
Is this a remote-OK position? If not, Visa sponsorship possible? 
See https://github.com/haskell/haskell-ide-engine/issues/851
2019 also works, but if you bring time travel technology you will be much more valuable as an intern.
What singleton pattern? 
An anonymity period is likely not a good fit for the Haskell community, but your particular objections strike me as not very good. 1. The anonymity period doesn't reduce total publicity, it just delays the publicity by a month or two. It also serves to prevent publicizing false information by getting more results peer reviewed before advertisements. Admitedly the need for peer review to avoid poor research is much less in the Haskell community than in more empirical communities, so this seems like a valid reason for the Haskell community not to need it. (Although I have seen a few FP arxiv papers pop up on social media streams that have false proofs/misleading experiments as pointed out by people more knowledgeable than me.) 2. The purpose of the anonymity period is not to prevent a malitious reviewer from uncovering the authors, but to prevent the unconscious bias in the same way as double blind peer review. In particular, reviewers will continue to read their social media streams while reviewing papers, and by not having famous person X endorse paper Y that they're reviewing while they're reviewing it, they won't be subject to that bias. Again, this is much less of a problem in the small Haskell community than in larger communities like NLP.
So with double-blind in theory one could mimic his style to get accepted.
If you can do good work and also explain it so well that the reviewer thinks you are SPJ then you deserve to get published. I do not think any one would dispute that.
OKey I guess may be too much of noise on the Social media might have a similar effect as putting the authors name. I was not thinking of social media. But putting on authors home page I would not count it as that bad.
I organized the ML workshop this year, which was also not double-blind (it was not double-blind before and I didn't think of changing it). I thought about it more due to your message, and I think that I agree with you: there are good reasons to go with lightweight double-blind for all scientific events with submissions and selection, not only the "major conferences" that are already following the practice. I'll recommend that we use lightweight double-blind in the future. From the perspective of an organizer, PC member or reviewer (rather than a submitter's perspective), lightweight double-blind is more comfortable than single-blind (as a reviewer, you have to worry less about unconscious bias, so you feel better), but it also incurs more work on the PC chair whenever external reviewers are involved (checking for conflict of interests is more delicate and often has to go through the PC chair). The overall consensus in the Programming Languages community is that double-blind is totally worth it, but I can see how smaller events have not moved yet. Compared to other ICFP colocated workshops, the Haskell Symposium is more selective (rejects more submissions) and uses more external reviewers, so double-blind reviewing would bring more reviewer's comfort but also cost a bit more extra work. P.S.: A minor nitpick, but I think that the quote you have of the 2013 chair report is from Chung-chieh Shan, who was the chair at the time (and whose blog you are pointing to), not Stephanie Weirich.
I think there's some [stuff in the `vector-space` package for this](https://hackage.haskell.org/package/vector-space-0.14/docs/Data-AffineSpace.html).
But 5k is kind of low
"Legally binding" =/= "Physically impossible"
Thanks for an interesting and well-written article! It's great to read about parts of the ecosystem which I'm not familiar with, especially when they are presented as a thoughtful critique.
&gt; Sometimes I wave to people I don't know. It's very dangerous to wave to somebody you don't know, because what if they don't have a hand? They'll think you're cocky. "Look what I got motherfucka!" – Mitch Hedberg [Source: Mitch All Together](http://getmitchalltogether.tumblr.com)
does this mean Haskeel needs a Generic Typer system 
&gt; I can update my beliefs given evidence Funnily enough it seems like the data that "the other side" is providing is only available to certain people. Julie seems to have blocked a whole bunch of Haskellers on Twitter (including me) despite - as far as I can recall - not having had any negative interactions with her. It makes for a fun kind of echo chamber where arguments are no longer even made to convince anyone of anything, because anyone who isn't already convinced is not allowed to see them. I've no opinion on the conflict itself, but this particular tidbit seems somewhat funny to me.
I haven't been following its development so I'm not quite sure what you mean by "haskell platform", is that just GHC + cabal-install? But indeed, I feel Stack is quickly falling behind, with cabal-install closing the gap and stack lacking backpack support. Again imo the stack solver is a valuable resource and a lot of respect to the team for maintaining it - but it should not require a separate monolithic and incompatible package manager which seems only to obfuscate the underlying cabal commands
We have a GPL'd library somewhere in transitive dependencies and have to build and ship all the project libraries dynamically, with all the RPATH mess etc. It would be somewhat simplier to manage only a tiny part of the deps that way.
y tho
To solve the "overlived resources" problem, is there a way to use some type of phantom type-variable, on the lines of the state monad/s.
see also previous reddit thread at https://old.reddit.com/r/haskell/comments/9gi9wt/haskellbook_author_publicly_speaks_out_on_legal/ 
Thanks! I think it is possible to add LSP support to `haskell-code-server` (at least the following methods: ‘textDocument/hover’,‘textDocument/definition’,‘textDocument/references’). However, I'm not sure if LSP protocol supports 'type of selected expression'.
&gt; ... Julie attack me online with spiteful insinuations and untruthful comments about me ... It's worth pointing out that bitemyapp has a history of [doing that which he's trying to paint himself the victim of](https://old.reddit.com/r/haskell/comments/7yfdei/haskell_ecosystem_requests/duii6p7/) and given the history of both parties involved Julie's account of the events is lot more credible than bitemyapp.
Presumably, [linear types](https://github.com/tweag/ghc-proposals/blob/linear-types2/proposals/0000-linear-types.rst) will eventually solve this problem.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [tweag/ghc-proposals/.../**0000-linear-types.rst** (linear-types2 → b9e28d9)](https://github.com/tweag/ghc-proposals/blob/b9e28d91a1ec34dcdf06d24c8e214e08eb7501b3/proposals/0000-linear-types.rst) ---- 
&gt; So! How about it? Can we gouge out the other eye, and transition from single- to double-blind review for the Haskell Symposium? Nope. I could spot an SPJ paper from miles away no matter how much tippex you put over his name. And even with less warm and fuzzy authors: People read each other's preprints. 
One variant we considered is to switch to: type Tasks c k v t a = forall f. c f =&gt; (k -&gt; f v) -&gt; t -&gt; Maybe (f a) Here we generalise both in the way that /u/roconnor suggests above, and also by changing `k` to `t` with the following intuition: given a callback `k -&gt; f v` for fetching values of "primitive" dependencies indexed by keys `k`, and a "target" of type `t` (which could, for example, be a pair of keys `t = (k, k)`), the function returns either `Nothing` if there is no information on how to build `t`, or the resulting value `a` (which could in this case be `a = (v, v)`). But this didn't seem to work well in the context of build systems, since the caching could only be done at the level of primitive keys `k` rather than on the level of targets `t`, which we felt was non-compositional.
From the proposal: &gt; The Eff approach of bypassing &gt;&gt;= combinator is quite embarrassing How so? You mention lack of laws as a drawback, I would stress that it is quite an important one. Monad laws allow you to refactor without knowing implementation details of the monad. By having the type change at every `Dsl.&gt;&gt;=`, all bets are off. Either you give up on refactoring, or you need to make sure the various instances have compatible semantics. Have you considered mtl-style classes?
[removed]
According to everything I've read on the proposal, it will not. In the presence of exceptions, the linear guarantee degrades to affine. I still think it will be useful for other things, just not for this.
If you need SPJ's guide to write a paper well, you're not writing as well as SPJ.
&gt; I guarantee we're an ok bunch. Serial killers confirmed.
I was hoping for conferences to be flooded by SPJ-indistinguishable papers and speakers but ultimately it was a tongue in cheek remark
From the FAQ linked above: &gt; We want reviewers to be able to approach each submission without such involuntary reactions as "Barnaby; he writes a good paper" or "Who are these people? I have never heard of them." The point is not to make a SPJ submission unrecognizable. As you are reading, you may well think "oh, this writing is very clear and understandable. I bet SPJ is the author." But that isn't a problem; the point is that you reached "this writing is very clear and understandable" starting from a less-biased position. *Eliminating* bias is obviously not possible, especially in a small (but growing!) community. But double-blind reviews do *reduce* bias, whereas single-blind reviews do not even attempt that modest reduction.
Yes, that’s the whole point of double blind testing - that if you do good work you get accepted.
At the haskell symposium, I was already familiar with over half of the presenters' works. That's just from browsing reddit, the GHC trac, and the Haskell Summer of Code. I feel like double-blind wouldn't really buy much in this situation, but I don't think it would hurt either.
Writing style and clarity, alas, has little to do with the qualitative substance of the paper: If it did, we could, in good conscience, hand over the field to English majors. Writing like SPJ doesn't imply that you also contribute as much as SPJ on a technical level. As such, if you get an anonymised paper and it reads like SPJ, being accustomed to SPJ, you might get sloppy reviewing the technical details. You wouldn't become sloppy if a name that is not SPJ is written at the top. The best-vetted papers probably are those written in Greek, interspersed with barely intelligible Chinglish. These things cut both ways. ---- What is the actual goal, here? Increase the quality of peer review, or diversity of contributions? I don't think the former is actually a problem in the Symposium -- we aren't in medicine, we don't have to ward off this sneaky rascal called placebo effect. The latter doesn't need blinding but can also be addressed by other means, e.g. increasing outreach, having feeder journals. A travel expense fund. Lots of things. And, of course, the single most effective and durable method of all: Increasing diversity within the community at large.
That's basically what the regions approach I reference in the blog post does. To quote what I said there: &gt; While it works, the idea never really caught on, in my opinion because the cost of juggling the types was too high. &gt; &gt; Interestingly, regions isn’t too terribly different in concept to lifetimes in Rust. And perhaps more interestingly, I believe this is an area where the RAII (Resource Acquisition Is Initialization) approach in both C++ and Rust leads to a nicer solution than even our bracket pattern in Haskell, by (mostly) avoiding the possibility of a premature close.
Just a guess, but Chris probably can't answer that.
Great library and article! It's somewhat obvious, but I just wanted to point out that once you start using a library like this, you'd immediately define things like: ``` type CanCountLetters = HasWriter "letterCount" (Occurrences Char) tellLetterCount = tell @"letterCount" ``` To hide implementation details and make your code much more conversational.
Well, the [haskell-lsp](http://hackage.haskell.org/package/haskell-lsp) library exists to make that process easier. But I think having it as an additional source of information in the existing haskell-ide-engine might make more sense. And I guess I should actually try and run this thing some time to see how it works. Too busy preparing for HaskellX though right now. 
Yeah, by the current public information on this whole thing it's impossible for me to parse what really happened, and it surely looks like one of the authors (and I don't know which one) tried to screw over another author. I was willing to buy the book, but since it's quite obvious that something shady happened, I'll stay clear of it.
The article says conduit is coroutine based. How does a coroutine look like in Haskell?
No
Unfortunately it’s not a remote position. We’ve had success with remote candidates but would like to keep enough engineers in the office to promote an engineering-driven culture. Visa sponsorship might be possible depending on your country and circumstances. For example, if you’re in the OTP period after you’ve finished a degree that’s workable.
If I may plead my case before an answer... + I have asked category theory questions here before, with this and other accounts, and gotten great answers that I am grateful for! + There is no other good place on Reddit to discuss category theory. + Sometimes a discussion, a back and forth, is indeed what is needed; Stack Exchange will not do, especially for noobs like me. + This is no ordinary programming sub; its posts vary widely from the usual career and social concerns, concrete syntactical minutiae, compiler discussions, and so forth that you would find on any language discussion, to some hardcore type theory and category theory concerns that tend to draw interest from Haskellers! I think there are a lot of people with interest and expertise in those matters who can benefit from or contribute to such discussions. I'm so grateful this sub is here. + Much of this is in the official sub description! "Practical stuff, theory, types..." Topos theory may be on the edge of typical Haskeller interests (I wouldn't know that well; I am a complete beginner!) but I think it's there.
&gt; why not Some enjoyable experiences that would emerge if this idea was followed: * Airplanes that drop half a kilometer every now and then while the engines restart * Your favourite PC game keeps restarting in the middle of a heated boss fight * Programs restarting arbitrarily while you're trying to get work done for a deadline In all these cases you can "restart and recover" but I think the impact on user experience is clear.
Looks nice. Awesome documentation.
Another possible issue when using a streaming library on top of `ResourceT` is that functions like `take` (for streams) might delay some finalizers from running. For example when creating a combined stream out of the first ten lines of all the files in a directory.
You mean the law is not written into the fine structure of the physical universe and automatically enforced down to the level of elementary particles, rendering it physically impossible to break? TIL! \&lt;/humor&gt;
Is there a more begginer friendly documentation on the idea behind this package? Something like a commented example of usage? I'm very interested but I'm kind of not getting a good picture from the code documentation alone.
I'm no expert on the compiler, but I think _very difficult_. It's not just a language extension, it's basically every library out there. Also, you can't avoid non-termination without drastically changing the language, so there will still be some possibility of bottom values.
There is /r/CategoryTheory, though unless CT takes over the conversation (unlikely), I doubt there's going to be any pushback over it, given the overlap of interests. Relax, no need to pre-argue your position :)
Any time I see people get burned by the .1 release, I remember Magneto's immortal words: "In chess, the pawns go first." https://www.youtube.com/watch?v=yhSYEMEEG5c
The motivating part of the documentation definitely needs work, sorry about that. The library is an elaboration of the last section of http://kataskeue.com/gdp.pdf where you can also find motivating and a series of examples. The source repo for the paper (https://github.com/matt-noonan/gdp-paper) also contains a `gdp-demo` subdirectory, which is a small stand-alone project with several usage examples that you can tinker with.
&gt; Double blind cannot really prevent a malicious reviewer from finding out about the authors in any case. No amount of controls in this sort of process can protect against bad faith. Even people acting in good faith can and do exhibit bias, which is why there are still controls even when all the reviewers are pillars of integrity. 
Right now you have to it manually, there is nothing in the library to that
Thanks! Though that sub is so inactive that its tenth-newest question is a year old. If there's an ongoing discussion of those topics going on outside the math Stack Exchange, it's definitely not there! Sometimes a sub exists nominally but in practice it's no place to get or give help. :(
Btw, I created GUIs with Rasterific: [http://www.cs.swan.ac.uk/\~csetzer/transfer/ppdp18Temporary8jqm7tRs/demoPPDP18AdelsbergerSetzerWalkingshaw.gif](http://www.cs.swan.ac.uk/~csetzer/transfer/ppdp18Temporary8jqm7tRs/demoPPDP18AdelsbergerSetzerWalkingshaw.gif) New GUI toolkit will be more sophisticated in the future. We also cited Rasterific in our paper: [http://www.cs.swan.ac.uk/\~csetzer/articles/PPDP18/PPDP18adelsbergerSetzerWalkingshawFinal.pdf](http://www.cs.swan.ac.uk/~csetzer/articles/PPDP18/PPDP18adelsbergerSetzerWalkingshawFinal.pdf)
I bought the book and loved it, and will buy your next book whenever it comes out.
There are a two variants of this that work without compiler support: - Enforce nested scopes via RankNTypes, ST style. You can accommodate overlapping resources by promoting longer lived ones to a parent region. `regions` implements this with a ref counting scheme. - Use an indexed state monad that keeps track of individual resources. `regions` adds some annoying type noise and promoting is mildly awful. The indexed state monad version is much worse and almost requires proper dependent type. Example from idris: readAndAdd : ConsoleIO io =&gt; (vec : Var) -&gt; ST io Bool [vec ::: State (Vect n Integer) :-&gt; \res =&gt; State (if res then Vect (S n) Integer else Vect n Integer)] C++ RAII types are a much simpler solution, usually implemented by having a state counter per stack frame and a transition table that maps states to callbacks which free resources when returning or unwinding. That implementation doesn't really work in haskell thanks to laziness and sharing. It might work when combined with linear types since we only have one reference and only have to check when an exception is thrown? Maybe thunks could be transitively be marked so we only have to check gc roots? Not sure.
I hope my comment didn't come across as pushback. If we're allowing general category theory posts on this sub (which is perfectly fine by me), then I think the mods should make it explicit in the sidebar, so that is accurate :). If they want to add it with an extra "\* Only if the post wouldn't be better suited to Stack Exchange/Twitter etc.", that's fine by me too.
&gt; It always struck me as ugly boilerplate to have to define a whole new class with a destructor If you're just going to use it once, then, yes it's boilerplate. For anything handle-like it's usually fine to just use a unique_ptr with a deleter. I also think there's a generalized unique_resource (I think that's what it's called?) proposal for C++20(?) which is a simple value wrapper, where you can add your own finalization. 
&gt;The motivating part of the documentation definitely needs work, sorry about that. Don't worry, we all lack time to do this. :) Thanks for the pointers.
I am really not sure what the benefit is. A contravariant functor from GrIn^op to Set is exactly the same as a covariant functor from GrIn to Set. Maybe the author likes presheaf categories which are typically of the form Set^{C^op} ? Also, you probably get more result when posting this on /r/math. 
Yes. I should've made clearer that I understand that they are different names for exactly the same thing, in the tightest and most literal possible way. But the authors' language, for example the language I quoted from each, seems to very clearly be trying to communicate something to me about there being something "natural" (in the informal sense) about the contravariance--to point me toward that intuition. I feel I don't understand it, that there is something I'm not getting.
This is a fantastic tool! Great job! I spend a lot of time browsing package docs. This tool will make this activity much more pleasant. Can't wait for the time I'll spend browsing through this. Thank you /u/rfbacon :D
I don't think that will happen, because: - I wouldn't want my airplane to crash during gc pauses, so it's good to have at least 3-5 copies of the program running at any time. Then rebooting in a round-robin fashion is fine. - For PC video games, restarting the game every frame shouldn't be an issue as most, if not all state, is outside of the program. I'm not sure what the third issue is all about. 
To be clear... This is not an 'alternative' to monads. It's an alternative typing for imperative computations that use \`do\` notation. Monads help me do things like 'flatten a nested list', or 'collapse a tree of trees of numbers into a tree of numbers'. That they can also describe imperative computations given a good choice of data structure is moderately interesting, but is not the most interesting consequence.
That's what they all say
&gt;Does every function call interacting with external world return an Either? Basically, yes. &gt;How do the left values of these eithers compose? They don't.
&gt; like GUI apps .. you got me. I totally forgot that GUI apps still exist :-D
This seems to be a common misunderstanding. I've dedicated [an item](https://ghc.haskell.org/trac/ghc/wiki/LinearTypes#Dontlinearguaranteesdegradetoaffineinthepresenceofexceptions) in the linear type FAQ to this issue. As a matter of fact, linear logic already has exception (in the form of `⊕⊤` monad, which is distinct from the affinity comonad `&amp;1`). It has no `catch\` however. But `bracket` is completely fine! That's the rational behind the [`RIO` interface\](https://github.com/tweag/linear-base/blob/d5b6b0d34ccf48fae15ebc99ff7ddbb8c9e23fbb/src/System/IO/Resource.hs). It allows out-of-order, exception safe, typed, explicitly closed, safe, resources. Of course, linear types come with their own set of restriction which you may or may not find acceptable. But they can indeed solve these problems.
Agreed. I think it is actually "an alternative to monads in some use cases", especially Free / Freer monads, Eff monads, monad transformers, etc.
I mean, you've put up a curtain between "opinion of the conflict" and "opinion of the immediate environment in which the conflict takes place". It's not a very material distinction. Why make it?
I think `Eff` monad also requires those language extensions (if not more) for resolving the same problem we are resolving. https://hackage.haskell.org/package/freer-simple-1.1.0.0/freer-simple.cabal
First, some context so that you would know where I'm coming from: the majority of code I wrote in my life is Haskell code, and I don't have any experience with languages where qualified imports are the norm. And as it happens, I dislike qualified imports a lot, and I see a lot of people arguing for qualified imports and behaving basically as if the opposite preference was invalid or clearly stupid or naive. For that reason I can't stop myself from writing a rant about them. Please don't take it as "you're wrong", but more like "here's how somebody might feel very differently from you". This said: whenever somebody pushes for qualified/explicit imports everywhere, I feel very confused, because I really can't relate to wanting this. I will go through your stated reasons one-by-one: &gt;Because we want to import some custom operator(s). In this case, it makes the most sense to use `import Data.Monoid ((&lt;&gt;))` or similar, to *explicitly* import the operators we actually need. I don't see how operators are different from "normal" functions and why a lot of people name custom operators as a troublesome feature of Haskell. When I read code, I'm constantly surrounded by things that have to be looked up, and the majority of them are not operators but domain-specific functions and types introduced elsewhere in the code. If looking things up was a high-effort activity, I would die. Writing the import explicitly doesn't help me because the meaning of the operator still remains obscure and I have to look it up anyway. (Then I press `Ctrl-C y`, Hoogle pops up, and most of the time the operator I'm looking for is on the first page.) One thing that I have to concede is that for whatever reason people will keep fighting to change others' code style instead of spending half an hour on setting up jump-to-definition and Hoogle/Hayoo [search shortcuts](https://www.minterest.com/how-to-create-custom-search-engines-google-chrome/). Given how prevalent this is, I would suspect that there's some good reason for this, but I don't see it. &gt;Because we want to import some types that see common use. In this case, an explicit import seems also the way to go. (`import Data.Text (Text)`) I don't think I ever worked with a person who didn't know where `Text` comes from. Again, this might be something specific to my circumstances. If your team is new to Haskell, you might decide to adopt an explicit imports policy, yes. I will also concede that if you wonder "where does this come from" for every type you see, then qualified imports do take less time than looking up things in Hoogle. &gt;It is *immediately* clear where a function comes from when you read the file. Why is this important? I mean, sure, sometimes it's important, but not very often, because what a function does is more interesting than where it comes from. If the name makes it clear, you don't care. If the name doesn't make it clear, you swear and look up the function, which hopefully doesn't take much time for you. If you're using a very Github-heavy workflow, or if you have to read code in other places where your editor's support is not available, that's one more reason why you might be swayed to use (or demand) qualified imports more. But this doesn't apply to imports from Hackage libraries – only to internal imports. &gt;There's no disambiguities of multiple-exports, which can only be solved by qualifying one of the modules. (And if you do that, which one of the two will you qualify?) Picking one that is more often used qualified in the code (which is easily checkable with grep) – or even picking one at random! – seems like a better decision than always qualifying both out of fear that you might introduce a minor inconsistency. &gt;In tutorials I've read, I very commonly see code examples where everything and the kitchen sink is imported This is something I agree with you about – in tutorials the audience is different and the set of tradeoffs should also be different. &gt;If you have a module with twenty different import statements at the top, the only way to find out what type a function like `get` or `connect`, `singleton`, `fromList`, `insert` have, is to inspect its type in GHCI. If I found myself in a codebase where such functions have non-obvious types (e.g. `fromList` has any type other than `[a] -&gt; Something a`), I would also qualify them, yes.
&gt; For PC video games, restarting the game every frame shouldn't be an issue as most, if not all state, is outside of the program. What games are you playing? Games are absolutely filled with loads of state. The position, velocity of every entity, health of enemies and the player, the AI's current objectives and desires... I can think of dozens of things in even the most trivial games...
Indeed, and I think this is worth particular attention: People usually think of double-blind as some kind of 'gold standard', but it's **really** contextual whether it actually has any noticable effect.
Another article about a non-problem on a overhyped functionality on a over-blogged library that add little value to haskell.
lololol Do you think that the selection process can be fair even with double triple or quadruple blind selection? The industry is completely corrupt with mambo-jambo-cool-kids and fake startups, but academia is even more corrupt.
&gt; IMO, subtracting ages should give you an AgeGap not an Age This won't make the problem disappear though, you can still say `Age 1 + (Age 1 - Age 3)`
Btw, what's Rasterific's performance? IIUC it's a pure Haskell array crunching? Is there a OpenGL/Vulkan backend?
That's just like, your opinion, man. I agree that a number of FPComplete posts are about problems I don't encounter (or already know how to "fix") and they do hype things, particularly the over-blogged conduit family. I'm not sure I would say over-hyped, has the amount of hype in the Haskell community seems lower than (e.g.) the JS community in general. But, I actually like this article. *shrug*
Both Rust and Go have recoverable exceptions that they call panics. I do not think they can be removed from the language. Haskell libraries could choose not to use them so much though.
[removed]
&gt;There is no other good place on Reddit to discuss category theory. I bet category theoretic question would be welcome on r/math
As far as I can tell from your question, you're asking why we want, _in general_ to think of presheaves as contravariant functors C^op -&gt; Set for some category C, rather than as functors say E -&gt; Set for some category E which is the opposite category of C. It just so happens that in this case the specific category you're asking about is the one that gives the shape of a graph. The general answer is that this works out nicer, in general, in topos theory. In particular, a very important operation is taking the yoneda embedding of a category, which sends each object in C to a representing presheaf. The representing presheaves in the yoneda embedding are precisely functors C^op-&gt;Set given by the partial application of the Hom functor. Another general instance where things work out nicer is in taking the _slice_ category of a given topos, which takes a specified presheaf x:C^op-&gt;Set and sends other presheaf p:C^op-&gt;Set to the set of natural transformations [p,x]. Again, slicing works out nicer if you use contravariant functors. There's a general reason for this as a whole, I think, which is that the geometric intuition of a category of presheaves starts with taking sheaves out of the opens of some topological space T, and the sheaves associate coherent data with each open of T. In this setting, morphisms of opens in O(T) are inclusion maps, and we have the property that inclusion maps in O(T) thus give rise to _restrictions_ on associated data in a corresponding (pre)sheaf. Certainly you _can_ turn everything around and still make it work -- but the motivating example of sheaves on the opens of a topological space should give an intuition for why most constructions, generally, work out nicer in that particular direction. (and in general why we should tend to think of morphisms in the source category as inclusion maps).
We're definitely open to such posts here.
Let me add one other "twist" -- on the one hand simplicial sets as given by Delta^op-&gt;Set where Delta is the simplex category -- are amazing beasts and great examples of why the contravariant convention works so well. On the other hand, it turns out that when doing things like generalized nerve constructions, what one wants _internally_ is _not_ a simplicial object, but instead a _cosimplicial_ object representing a functor Delta-&gt;Set. My intuition for this is still not great, but my sense is that you want the covariant structure because you want a "beefed up" analogue of the simplex category in such a case, rather than a topos generated by gluing objects _indexed_ by such a category.
Since you're saying this, I'm assuming this a view held by a majority of the mods. Could this explicitly be added to the sidebar? There is already a list with "[..], types, [..]". Could category theory be added to that list, perhaps after types?
Of course a functor from GrIn to Set is the same thing as a contravariant functor from GrIn^op to Set. The advantage of thinking of them as contravariant functors is that (1) the category GrIn^op is (isomorphic to) a full subcategory of the category of graphs and (2) every graph is built by glueing together copies of the objects in that full subcategory isomorphic to GrIn^op. So really, the category of graphs has a more meaningful connection to GrIn^op than to GrIn. Let me elaborate on (1). The object _Vertices_^op of GrIn^op corresponds to the graph _V_ with a single vertex and no edges, and the object _Arrows_^op corresponds to the graph _A_ which has two vertices and a single arrow between them. You might think I could have said exactly the same about GrIn, without the op, but the point is that morhpisms of graphs between _V_ and _A_ are exactly as the morphisms in GrIn^op, **not** like the morphisms in GrIn. (Check this!) Now about (2), what this means is that any graph _G_ can be formed as follows: first "explode the graph" by taking the disjoint union of a copy of _V_ for each vertex of _G_ and a copy of _A_ for each arrow in _G_, then glue it back together to get _G_ again: for each arrow _a_ from _x_ to _y_ in _G_, you glue the starting vertex of the copy of _A_ you used for _a_ to the copy of _V_ you used for _x_, and similarly for _y_. So, in summary, the category of **contravariant** functors from C to Set (1) includes a copy of C and (2) every such functor can be formed by glueing together copies of the objects in that copy of C. (1) is false in general for **covariant** functors, and then (2) makes no sense for **covariant** functors. I don't know how far along you are in your study of category theory but (1) is (a basic consequence of) the Yoneda Lemma, and (2) is typically stated as "any presheaf is canonically a colimit of representable presheaves". And finally, let me suggest that if you have math questions in the future you ask in a math subreddit (say, r/math), rather than in a programming subreddit.
The category of presheaves is the free cocompletion of the index category. That is, it contains the original category and has all colimits such as coproducts and coequalizers (which you can essentially think of as being a form of quotient). This is probably what Spivak was referring to. Thus, in ArShp, you should essentially think of Vertex as being your "generic one-vertex graph", Pure Arrow as being your "generic two-vertex/one-arrow graph", and src/tgt as being the two graph homomorphisms that can embed Vertex into Pure Arrow. By taking the free cocompletion, you're adding all the graphs that can be built out of these two graphs.
I'm guessing something using continuations, as in: data SimpleCoroutine c a = Continue (c -&gt; SimpleCoroutine a) | SDone a where - `Continue k` represents a coroutine that's "paused" because it needs a value of type `c` to continue , and can be "resumed" by calling the continuation `k` with that value; - `SDone x` is a coroutine that's finished with a result `x`. A coroutine that "asks" for two words and returns their total length would be: totalLength :: Coroutine String Int totalLength = (Continue \s1 -&gt; (Continue \s2 -&gt; (SDone $ length (s1 ++ s2)))) This is a bit too limited though, because coroutines are supposed to be able to "yield" intermediate values. We can extend it like so: data Coroutine y c a = Yield y (c -&gt; Coroutine a) | Done a where `Yield i k` represents a Coroutine that yielded an intermediate value `i :: y` and can be resumed by calling `k`. (Note that you can recover `Coroutine` by setting `y` to `()`) This could be further generalized, but I hope you get the basic idea; I hope I got it right someone please correct me if I forgot anything! I got this general pattern from reading about `Iteratee`s, should be easily googlable if you're interested.
Thanks :) 
Category theory is extremely broad, so I'm not sure if all category theory related stuff can be qualified as Haskell-related, which is why I'm asking for explicitness here. Right now, the way I read it, it says "Haskell-related theory", which seems to indicate that topics related to semantics of pure lazy languages would be on-topic, whereas discussion of ML module systems (falling under type theory) without any connection to Haskell would be off-topic.
Agreed. I have been bitten by this when scraping email headers (only, hence `take` was involved) from my maildir directory, and ran out of file handles. This was not straightforward to debug either and I only found the exact location by poking around in ResourceT 's internal map. I came to the conclusion to prefer `bracket` when a static resource life time scheme is sufficient, and use `ResourceT` as locally as possible.
Indeed "theory" includes relevant category theory, to be sure, not all category theory. This is a relevant question because it covers relevant (relatively general and elementary) material. So just like not _all_ type theory, but just relevant type theory is in scope, the same holds for this sort of mathematical discussion. It's fine, and the topics listed are suggestive not exhaustive anyway. I don't see why you're being obstinate about this.
Sorry I didn't mean to come across as obstinate. This post struck me as off-topic, so I was seeking a clarification, is all. It isn't a big deal, I will stop here.
It’s true that double-blind does not prevent reviewers from guessing the authors based on writing style or familiarity with what people are working on. That’s unrealistic for a tight-knit community. However, the other advantage of double blind is that it gives newcomers cover from the “who are these people?” response. That seems laudable to me. 
&gt;I know the "baby" way to understand graphs: just use a simple inclusion functor from GrIn to Set, where GrIn has two objects Arrows and Vertices and two nonidentity morphisms source and target from the former object to the latter. If this is the baby way to understand graphs, I think I may be a zygote.
"This seems unlikely, given &lt;X&gt; &lt;Y&gt; and &lt;Z&gt;, but I'll grant it's not completely certain" That's a very normal human statement that most people make frequently, and there's nothing wrong with feeling that way or expressing it publicly. We all have to come to conclusions in absence of perfect evidence, it's totally ok to do that as part of a public dialogue, and in fact, preferable to staying silent, because others can help bring new evidence you may not have considered, and conversely, others may be operating with even less information than you are, so sharing it helps them come to their own conclusions as well. The strategy is, actually, quite similar to the scientific method, and is more like walking around claiming the Earth is round, stating why you think that way, and leaving the floor open to competing evidence. 
This is great! I’ll definitely use this next time I’m thinking about bits instead of pen-and-paper.
I've experimented with an [alternative design](http://hackage.haskell.org/package/streaming-bracketed) that should support prompt release even with functions like `take`. The idea is having a decorator that sits above the streaming transformer (the one from [streaming](http://hackage.haskell.org/package/streaming) in my case) and control how functions on streams are "lifted" to the decorator, for example installing additional finalizers as necessary. The solution is not a general one though, and it's tied to `Stream`s that sit just on top of `IO`.
Also, I didn’t hear that `:doc` was implemented until now :O
Does anyone have data on other, more mainstream languages? I have a hunch that other languages aren't like this. I feel like Haskell is at high risk of maintainership woes due to a tendency to have prolific authors, which are a blessing to have but also leave an ownership/vision void when they leave or burnout.
This is super interesting! I've often thought that the increased *programming* productivity of Haskell would mean that the *non-programming* aspects of language community would be weaker. For example, a Michael Snoyman or Edward Kmett could be vastly more productive with code in Haskell, but they're still just one person wrt writing documentation, mentoring folks, producing guides, etc. Haskell lets a single person do quite a lot of code, but it also has all the problems you expect on a single-person project.
I think I almost get it! But could you tell me a bit more about this "free cocompletion"? Like for example, could you characterize its adjunction?
I don't have data on this, but I've been a part of the JavaScript, Ruby, Go, and Clojure communities. And it deffinitely feels the same. The main difference being people don't stick around for as long so organizations end up owning packages.
Is that an accurate metric? I saw u/Tekmo thank all of the people that updated his packages to work with GHC 8.6 on Twitter. Is it possible that the load is a little more distributed beyond just the core package creators? Or are they doing the bulk of the work in maintaining their own projects?
ghci&gt; 1 :: Hex 0x0000\_0000\_0000\_0001 &amp;#x200B; Pro-tip: you can use \`default\` to change the default numeric type from \`Integer\` to \`Hex\`. &amp;#x200B; ghci&gt; default (Hex) ghci&gt; 1 0x0000\_0000\_0000\_0001 &amp;#x200B;
This is a problem I struggled with in Haskell and was wondering if Rust has a solution for it. Here's the problem: ioFunc1 :: Text -&gt; Either E1 R1 ioFunc2 :: Text -&gt; Either E1 R2 What would be the type-sig of a function that calls `ioFunc1` and `ioFunc2` internally? It would also need to return an `Either`, but what would be the type of the left-value? How does one join `E1` and `E2` at the type-level? The only way is to manually create a new `E3` and "map" all possible values of `E1` and `E2` to `E3`. Very, very tedious. IIUC Rust has not solved this problem. Instead it has choses to use strings and ints as the error/left types, which do not have this "compositional" problem, right?
The free cocompletion is given in one direction by the yoneda embedding. it doesn't necessarily have a left adjoint, but if it does, then the category is called _total_ (https://ncatlab.org/nlab/show/total+category). However, I don't think that this helps give an intuition for it, tbh. What does is rather understanding the yoneda lemma itself.
As far as I know, all the maintainers at the top of the list have a lot of co-maintainers and spread the work of maintainership around quite a bit, or have handed off many of their packages to others entirely (e.g. don stewart).
Ryan, Oleg and Eric are on this list largely because they've seriously help me spread the workload I have across multiple maintainers already. I don't think this chart presents really actionable intelligence on that front. As I spread things to more people you just get more people in your peak.
Time for me to push the other 40 packages or so I maintain on hackage into stackage to skew the index further. ;)
Indeed, I'm starting to get the feeling that this example is going to be great for understanding Yoneda like never before as I hammer it into my head. But wait! I don't see how anything can be said to be "free" in any sense unless it has a *right* adjoint! That's just what freedom is, as far as I've seen anywhere before. In what sense is this "free cocompletion" free?
I don't think u/KissMeImClueless was talking about adjoints to the Yoneda embedding. Rather, since you mentioned that the presheaf category is the free cocompletion, he or she wanted you to describe the free-forgetful adjunction corresponding to the cocompletion operation. That goes naively (I'll say why this is naïve later) as follows: Let Cat be the 2-category of small categories, functors and natural transformations and let CocompleteCat be the 2-category of cocomplete categories, functors that preserve colimits and natural transformations. Then there is a 2-functor Cat --&gt; CocompleteCat which sends a category C to the category Fun(C\^op , Set) of presheaves on C and it should be the 2-categorical left adjoint to the inclusion (or forgetful) functor CocompleteCat --&gt; Cat. But this doesn't quite work because of size issues. To contain these presheaf categories of small categories, we need CocompleteCat to consist of large categories (indeed, small categories can only be cocomplete if they are preorders! --which is a nice exercise). But then there is no inclusion functor CocompleteCat --&gt; Cat, because Cat only contains small categories. I don't think there is any really good way to solve this issue, so while free cocompletion really wants to be left adjoint to the forgetful functor from cocomplete categories to all categories, it isn't literally so. This is just a "global problem", locally we're fine: the presheaf category construction does have the universal property it would have as a left adjoint to that inclusion of cocomplete categories into all categories, namely, for any small category C, any cocomplete category D, and any functor F : C --&gt; D there exists a functor G : Fun(C\^op , Set) --&gt; D for which G composed with the Yoneda embedding is naturally isomorphic to F, and such a G is unique up to natural isomorphism.
Could potentially adjust for this by measuring the number of commits by each author in each package's git repos. This of course makes the measurement a lot harder and still has a lot of problems (like counting people who haven't committed in 5 years, whether the size of the commits count, inability to rule out large automated changes like applying a code formatter, etc.).
Wow, `default` is useful in this case! I'll write to README. Thanks. 
Sure, if you're OK with \`Maybe Age\`. Just wanted to point out that separating \`Age\` from \`AgeGap\` won't solve the fact that the operations are partial by nature.
If part of the issue is that `ResourceT` is more powerful than needed as many users only need static resource tracking, I wonder if there’d be value in an `Applicative` transformer `ResourceA`. 
It is work, but not as bad as that chart suggests. Most package maintenance work falls into two categories: * Regular maintenance that affects all of your packages, (i.e. supporting newer versions of GHC and supporting Stackage if you signed up for it) * Maintenance related to users (i.e. bug requests, feature requests, support, etc.) For the former category, other people tend to help a lot. As I mentioned in my tweet, a lot of the work is done for you by people who use your packages. They'll typically open pull requests and you just have to accept them and upload the package. The latter category can be broken down into two sub-categories: low-maintenance and high-maintenance packages. Usually you have just a couple of high-maintenance packages at any given point in time because that's all you can handle and once they transition to low maintenance packages you move onto authoring new packages. However, in this category most of the work tends to be done by you unless you explicitly invite contributions from others. Also, as a package maintainer you have several safety valves, such as: * Asking other people to adopt and/or comaintain your packages to relieve your workload * Removing your package from Stackage (assuming that you originally signed up for it) * Not doing any new work, but still accepting and merging pull requests * Completely abandoning the package (always an option of last resort) So usually the work tends to balance out to whatever you feel you are capable of handling.
Haven't looked at it yet, but this sounds like exactly what I need! I'm writing a compiler using recursion schemes in my AST, I want each pass of the compiler to reduce the number of constructors in the ADT as it simplifies. Maybe this can do the trick.
Yes it is. /u/krrrch can't possibly have assumed that was an annual figure!
I'm using something like your \`CleanupRegistry\` for years, see [http://hackage.haskell.org/package/io-region](http://hackage.haskell.org/package/io-region), and I don't think it encourages usage of outlived resources. Actually my experience is exactly the opposite: you can easily nest regions, transfer ownership and even pass regions around (they are first class citizens,) so you are much more flexible then with \`resourcet\`. It is very easy to achieve neccessary level of granularity with dynamic regions.
Why aren't all your packages in Stackage? From the discussion here it seems to me there's no reason for a package not to be part of Stackage if it's still maintained. Being in Stackage actually helps reduces maintenance overhead by not requiring you to maintain version bounds and you get additional CI for free.
Tangential question - does hackage publish package meta-data &amp; stats in a machine-readable format anywhere?
You mean ioFunc1 :: Text -&gt; Either E1 R1 ioFunc2 :: Text -&gt; Either E2 R2 
To be fair, I'm a maintainer of only a few of yours packages on Hackage. Ryan does *a lot*. So in this sense chart is skewed for me. OTOH there are 26 or so servant-* packages
Edited - thanks!
Time going into maintenance of N packages is not linear in N. Active development is. Updating to new version of common dependency (GHC, base, containers, template-haskell, or say profunctors) is "easy" as the puzzle is solved only once, and then applied to everything else. Also as the experience of maintenance grows, people find their ways (which work for them) to maintain their "catalogue" (from how to write changelogs, CI setup to actual code practices). Speaking of which: Travis-CI (or any easy to setup CI service) really helps. For last few major GHC versions I simply made a PRs to all of my packages (adding GHC-8.6 job and relaxing bounds) and waited them to turn green. Some turned red, but those are "interesting" ones.
Stackage *forces* one to keep up with dependencies. Keeping your package compatible with newer stuff (i.e. when code changes are required) might not be necessary. As concrete example: after deprecating the package in favor of something better, it's a good idea to remove it from Stackage. Note: the package will remain working, it only won't allow newer dependencies. (Some say that package is broken then, I disagree)
Absolutely, you're right. Many of the top authors have been trying to spread their work among other people. Indeed, DonStewart and BryanOSullivan are not active anymore. However, that highlights an interesting point. BryanOSullivan's departure from the scene left a void and many of his packages languished for a while. From my perspective, this isn't a criticism of Bryan, but rather a criticism of prolificness of a few people who then expand beyond their capacity to dev, or simply leave the community (also happens). But indeed, I wanted to include in this (maybe a subsequent spreadsheet), time-related data about who actually contributes changes (via e.g. github commits) but that requires more work. I think the GitHub API might offer "top contributors" info about a repo, but it'd be nice to have it "over the past 6 months", for example.
Definitely, I think that'd be really interesting and actually useful to gather. Commented more [here](https://www.reddit.com/r/haskell/comments/9ls1b4/burnout_risk_relationship_of_hackage_maintainers/e79lndk/) in a sibling post.
&gt;Both Rust and Go have recoverable exceptions that they call panics. It's generally considered bad form to actually try to recover from them though, which is why they're not termed exceptions. The idea is that they should generally only be used for errors from which the thread or program cannot recover, and hence there's no need or expectation to write "panic-safe“ code. This contrasts with Haskell exceptions, which at least in the prelude are used heavily; Rust and Go's file IO functions are actually more type safe than Haskell, in that at least they return values representing the possibility of error as opposed to silently throwing exceptions not captured in their type.
I think async exceptions are just a fact of life when dealing with the possibility of doing IO, in particular when the control flow depends on the result of some IO computation. I believe instead we have plenty good tools in Haskell (in fact, perhaps too many) for dealing with exceptional behaviour.
How `error` from go and `io::Error` from rust are different from `SomeException`? And why it is so common to ignore errors in `defer` and `drop` if it is type safe?
I agree, I wrote [in another comment](https://www.reddit.com/r/haskell/comments/9ls1b4/burnout_risk_relationship_of_hackage_maintainers/e79lndk/) using contributions from GitHub would be a good way to see that better. It just takes more pulling of data.
Indeed, I should update the data to GitHub commits. Megarepos make that a bit harder, but I could give it a go.
Yes, that was my point. But still many packages aren't authored by me. I help maintain them. A lot of servant (there're also alp, jkarni, fizruk), universe-* (dmwit). postgresql-* (bos, lpsmith), edit-distance, lattices (max bolingbroke), github (many, jwiegley before me), quickcheck-instances (aslatter), OneTuple (johon dorsey), these (isomorphism), log-* (scrive), strict-base-types (simon meyer), lucid (you, Chris Done), http-api-data, swagger2 (fizruk). What's Interesting about Haskell community, is that it's quite old. We have generations of maintainers. I think I'm in the youngest atm, but hopefully not in the last.
I don't feel like Go actually does this all that well because it doesn't force you to handle errs. But I have made the point before myself that Rust is more typesafe and has better standards in this regard. All I was saying is you can't get rid of exceptions - they are still needed in some cases, and Rust and Go haven't. I would love to see a Prelude and, for example, a solid network library, that just return Eithers but it doesn't require any compiler work that I know of.
&gt; those numbers don't include stackage downloads Actually they don't include stack downloads at all, right? Stack fetches from the S3 mirror even if its added as an extra package. I would love to see download statistics from this somewhere. Its frustrating that I can't get any sort of approximation of how much usage a package is seeing anymore.
For a lot of things I prefer the second approach. Not just for numbers but any place I need to make new distinctions among what would otherwise be the same type (so there is also a non-phantom parameter for the contained type.) Its a bit more convenient in that you can derive all the instances in one place, and make generalized wrap/unwrap/rewrap functions and lenses (special cases of [`coerced`](https://www.stackage.org/haddock/lts/lens/Control-Lens-Combinators.html#v:coerced)). The savings on boilerplate is quite significant I've found. When I find myself reaching for the straight newtype approach is generally when I want to make sure its sufficiently *difficult* to convert between types. If its very important to not be able to accidentally convert one to another, then the explicitness of a named newtype constructor may be called for. 
Not exactly. No, they haven't solved this problem, but they don't use strings or ints either, at least when it comes to IO. They just have a bunch of convenient handlers around Result type (they version of Either). More on that here: https://doc.rust-lang.org/std/io/struct.Error.html Also, it isn't completely true that Rust doesn't have exceptions. The code can call panic!() macro, and this "panic" can be caught and handled, but this approach is generally discouraged. Basically, panic is their version of 'error' from Haskell.
Haskell noob here (favorite language so far though!) The other languages I've gotten into, java, c++, a little c. I just haven't heard of package management woes in those languages the way I here about them here. Why can't haskell have something super clean and simple to use like maven? What is this driving force behind this complexity, is it just that haskell is sort of a rapid pace test new features language and that tends to break things? 
This is my use case as well! Although in some cases I want to add constructors, for example, type application/abstraction when going from the untyped AST to the typed AST.
The ability to have one thread interrupt another is useful when implementing parallel pure computations where a certain one succeeding tells you that the other computation is not needed.
Wouldn't this be possible with the [Trees That Grow](https://www.microsoft.com/en-us/research/uploads/prod/2016/11/trees-that-grow.pdf) approach? You can't exactly reduce the number of constructors, but you can use the type family to restrict what stage in the compiler that constructor is used for. (I think?)
I love Trees That Grow/Shrink for this! I have compilers which introduce new constructors in analysis/optimization passes as well as remove constructors (e.g. rewrite curried lambda applications to fully saturated applications or partial applications when compiling, doing erasure analysis along the way for a dependently typed core language). Every time I've tried to do "real work" with a base functor + cata/para/etc. approach, I seem to get drowned in boilerplate a little too quickly. Maybe I need to give it the same chance with e.g. pattern synonyms that it takes to make TTG really nice to work with.
Can't read the doc. It's in the bin? Would love to.
Really big packages are often maintained by companies who developed or depend on them. That seems pretty different to most of the Haskell packages at the moment, though I have seen talks that claim that Facebook and Google use Haskell internally.
Personally I've had far more complexity and problems with maven (mostly because it didn't install dependencies of the packages I installed using it). What complexity are you referring to? Maybe I have a blind spot or know a solution.
Byte arrays (pinned or not) are not traced because there's nothing to trace (they don't hold pointers). Non-pinned versions (`newByteArray#`) are still moved though. If you need no moving + no tracing, you need a pinned byte array. Note that if you allocate large enough byte array it automatically becomes pinned. [This blog post](http://www.well-typed.com/blog/2018/05/ghc-special-gc-objects/) has some details on this (see large object and pinned object sections).
Compacting functions is not easy because you need to serialize code. Most code has relative and absolute jump targets, simply serializing the bytes of the function's code is not enough. So you need to somehow disassemble while you serialize, and for every jump instruction serialize the target code. If a jump is absolute, in the serialized code you need to make it relative, and I'm not sure if this is always possible (e.g. perhaps absolute jump instruction is smaller than relative jump instruction). There may be other complications too -- this is just one problem comes to my mind. However, you should be able to compact a `StaticPtr` of a function. I just tried and for some reason this is currently not allowed. I believe it may just be a bug. A ticket in GHC trac might do it. 
I generally have more people helping me out with the git side of things on github. There things may have upwards of 20 people with commit access. The one thing I have maintained a relatively tight grip on is the release to hackage process. This is because I trust Ryan and Eric's judgment implicitly when it comes to cutting an actual release, and the general thought is that if I were hit with the bus factor they'd be able to push things further downstream into other hands. Cutting the actual releases is a relatively small percentage of the overall workload.
They also don't include most hackage downloads as they are backed by a CDN and so we only see a tiny fraction of our own downloads.
https://www.reddit.com/r/haskell/comments/9lx6zo/haskellcafe_call_for_contributions_haskell/
In Rust, you would end up using the `From` trait to say "I want a return value which has an error which can be converted from both `E1` and `E2`."
If you don't want to read the paper then EZY's posts are the most digestible. Try [this one](http://blog.ezyang.com/2017/01/try-backpack-cabal-packages/).
I know async exceptions are useful, my point was that I'd be willing to sacrifice this usefulness if it meant functions could only fail in ways spescified in their type signatures.
Suppose there are a number of things that are usefully expressed as an enumeration. But some of those things have additional data associated with them in other contexts: data AsEnum = A | B | C -- Option 1: type system guarantees correctness data AsUse1 = A1 | B1 Int | C1 data AsUse2 = A2 | B2 | C2 Char Double data AsUse3 = A3 Int | B3 Int | C3 -- same semantics for Int in A3 and B3 -- Option 2: must guard correctness at run time type Use1 = (AsEnum, Maybe Int) type Use2 = (AsEnum, Maybe (Char, Double)) type Use3 = (AsEnum, Maybe Int) I assume most experienced code bases use Option 1, despite the namespace proliferation? (*T*(U+1)* names, for *T* things and *U* uses.) Are there other solutions to this representation problem? 
Why do you lurk here?
I found a convenient way of representing different stages of the AST with a single datatype. With GADTs we could even restrict some AST nodes that belong to an early-stage representation. It's basically a labelled trees that supports Kmett's recursion schemes. I found fixplate and attr to be a little bit complicated. Criticism is welcome. [https://pastebin.com/zSwJy1VE](https://pastebin.com/zSwJy1VE)
Thanks for your kind response. At least my knocking on the door allows some people to express some criticism without being expelled from the spaceship.
Because I'm very interested in Haskell, and I program in Haskell. Because Haskell would be an incredible tool with an incredible future if not for the problem that is going derailed and being driven by stupidity.
Yes. It adds some value. I think so.
And what do you think entitles you to call out "stupidity" ? Where is your constructive criticism, which are the virtuous examples we should all be following? 
For me a shell is something I essentially live in for my day to day activities, not just something I open every now and then. So I have tmux for every shell, lots of programs, customizations, etc, and use quite a few programs. I don't directly use bash scripting utilities inline very often, and when I do, the stringly interface ends up being more useful than not just because of how Linux does everything. Plus, the current syntax of bash or zsh is fairly well optimized to make doing the common routine tasks as simple as possible. I think I would die of old age if I had to write "exec" or array syntax of lists of strings just to get anything done. I do like Haskell a lot, and Shelly/turtle and other libraries are quite interesting, but replacing a shell for day to day use would require extremely tight integration with the existing ecosystem or a *very* comprehensive replacement which is extremely difficult to do. Take a look a oilshell for a fascinating take on building a modern shell language and ecosystem :)
&gt;Most code has relative and absolute jump targets, simply serializing the bytes of the function's code is not enough But you don't really have to serialize any assembly, really; You just need a pointer to it. That's why I suggested that the closure would be compacted **along with a static pointer**. The documentation for `Compact` already states that "The serialized data can only be deserialized by the exact binary that created it...", so, if you just remember where the assembly was (via a pointer of some sorts), you could expect to find it at the same place. I'm really curious because if it were possible to do this, Haskell would gain a superpower that would only be a dream in impure languages.
People who do ambitious, useful, insightful simple and elegant programs that solve real, hard, open problems and make them work in the real world. To summarize: good programmers.
While 5k is indeed very low, the range goes to 10k, which is quite good. I guess most would probably be around 8k.
[As I pointed out before](https://www.reddit.com/r/haskell/comments/8dudce/make_great_again/), the type class `PolyCont k r a` is mere "`k` is something that can be turned into `Cont r a`." Your `&gt;&gt;=` is not much different than `&gt;&gt;=` of the Cont Monad. So it is a Monad in disguise. You did not escape the yoke of Monad the Great. But it's interesting that many effects can be represented by changing `r` of `Cont r a`. I think it's worth some investigation on how practical this is to represent the effect system. How does this compare to the RIO pattern rising recently? import Control.Monad.State newtype Dsl r a = Dsl { runDsl :: (a -&gt; r) -&gt; r } deriving (Functor) instance Applicative (Dsl r) instance Monad (Dsl r) instance MonadState s (Dsl (s -&gt; r)) instance MonadIO (Dsl (IO r)) instance MonadIO (Dsl r) =&gt; MonadIO (Dsl (s -&gt; r))
What are Algebraic effects? Total fp noob here so be gentle.
&gt; How much do you make doing open source work? What do you mean? The "doing open source work" part seems to imply that we should only include the portion of our revenue which comes from our open source work, whereas the choice of answers seems to imply that we should include all of our revenue sources, e.g. that someone who works on closed-source projects by day and open-source projects by night should pick "a competitive salary in software" even if none of that salary is for their open-source work. Or does that option exist for e.g. Mozilla employees who are paid to work full-time on an open-source project? If the former: for me, contributing to open-source projects is something I mostly do in my free time, but I also contribute at work whenever the situation requires it (e.g. when we encounter a bug in an open-source dependency). That's still a small portion of my work day, so I guess I should multiply my "a competitive salary in software" salary by that fraction and obtain something like "enough to make a meager living"?
I think it's a reasonable suggestion for the Haskell Symposium committee to consider. I trust them to weigh the pros and cons of it and come up with an answer that they feel is best for the Haskell community.
I wrote a series of posts quite some time ago titled "Coroutines for Streaming": https://www.schoolofhaskell.com/school/to-infinity-and-beyond/pick-of-the-week/coroutines-for-streaming/part-1-pause-and-resume Glancing through, if you read Part 1 and Part 2, they seem to explain the idea of what a coroutine is decently. Part 3 onwards gets more into how they can be used for streaming.
I don't really find the argument presented against ResourceT (by its own author, no less) to be very convincing. Perhaps I'm just too used to it, but * as stated, the performance hit is generally negligible * it doesn't seem all that complex to me * it would be nice to solve the "overlived resources" problem, but simply "not using ResourceT" does not by itself solve this problem In the absence of a better solution I'd say ResourceT is quite reasonable to use whenever you want to manage resources, and I disagree with the admonition to use it only when absolutely necessary. If we're looking for something better to replace ResourceT, there's probably something we can learn from [Racket custodians](https://docs.racket-lang.org/reference/eval-model.html?q=custodian#%28part._custodian-model%29).
You can also read my blog post about Backpack: * https://kowainik.github.io/posts/2018-08-19-picnic-put-containers-into-a-backpack
The important thing to remember is how do notation is desugared. When you have code that's like: ``` main = do a &lt;- f x g a h a ``` iirc it gets desugared to something like: ``` main = let a = f x in (a &gt;&gt;= g) &gt;&gt; (a &gt;&gt;= h) ``` So `a` is only bound once and thus only calculated once. Here are a few other links that are good &amp; marginally related, for code that works outside of monadic composition: https://wiki.haskell.org/Memoization https://wiki.haskell.org/Tying_the_Knot
I thought higher-kinded types weren't supported in Java; does this prove me wrong?
main = f x &gt;&gt;= \a -&gt; g a &gt;&gt; h a -- actually
You should be able to put that in a compact region and restart the game.
Thanks, it explains a lot!
amazing!
First of all, shouldn't be m &lt; 4 and not y? Else, I would probably write this as a list comprehension like you did
When I first saw the code I assumed it was just someone having fun, but it seems like an official Mojang product? What is the motivation for this? While I know lenses can be more than this, in a language with mutable state and chainable field accessors, many of the cases where lenses are used in Haskell don't apply to Java. Does anyone more knowledgeable than me have any idea why they would want to do this?
You can emulate it, here is the version for Optional (aka Maybe). The haskell equivalent would be something like type family App w k data MaybeKind newtype MaybeBox t = MaybeBox (Maybe t) type instance App MaybeKind t = MaybeBox t unboxMaybe :: App MaybeKind t -&gt; Maybe t unboxMaybe (MaybeBox m) = m boxMaybe :: Maybe t -&gt; App MaybeKind t boxMaybe = MaybeBox type Functor f a b = (a -&gt; b) -&gt; App f a -&gt; App f b maybeFunctor :: Functor MaybeKind a b maybeFunctor f = boxMaybe . fmap f . unboxMaybe genericFunction :: Functor k Int String -&gt; App k Int -&gt; App k String genericFunction func v = func show v test :: Maybe Int -&gt; Maybe String test = unboxMaybe . genericFunction maybeFunctor . boxMaybe
&gt; Does anyone more knowledgeable than me have any idea why they would want to do this? Perhaps the code is a research project to see if it not only can be done, but done in a reasonably "real world" codebase. In other words: "because they can".
&gt; Using [3*m+1| m &lt;-[1..3]] seems clunky Looks marvelously succinct to me. There's all kinds of ways to express this three-element set, possibly using multiple `map` calls and `take` from an infinite list, but there's just no reason to. The only thing clearer would be `[4,7,10]`, but that would definitely be missing the point.
Here's the biggest thread about this: https://www.reddit.com/r/programming/comments/9lyplq/microsoft_open_sources_parts_of_minecraft_java/ Here is the link to the article from MS/Mojang themselves: https://minecraft.net/en-us/article/programmers-play-minecrafts-inner-workings Scroll a bit down to "DATA FIXER UPPER" to read more about what OP posted. In short: Microsoft/Mojang open sourced several libraries they used in Minecraft (under an MIT license). The source in OP's link is called "DATA FIXER UPPER" (“The name is so stupid that we had to keep it,”) and **it helps Minecrafts convert old data formats into the current one**.
As I've [commented before](https://www.reddit.com/r/haskell/comments/9m2o5r/digging_reveals_profunctor_optics_in_mineacraft/e7bntex/), this lib is part of Minecraft - it takes care of how old data formats (for maps) are converted into the current format.
Modeling effects in continuations are not news. What is new in this library is the ad-hoc polymorphic continuations, or the ability of derivation. For example, as you mentioned, you can create `instance MonadState s (Dsl (s -&gt; r))` for one state, and this library proposed an approach to create or one state with another state, with IO, with Yield, or with any other DSLs, together.
Sorry for the misunderstanding about your point! I thought you are proposing "a superior Monad alternative." Above example shows how your approach can be reproduced within `mtl` style. I can define `MonadGetLine`, `MonadPutStrLn`, etc. classes and implement `MonadXXX (Dsl r)` for each `PolyCont XXX r a`. Then, it achieves the same goal - multiple effects used together. The advantage is in modeling effects in continuations? -- You need to satisfy each constraint by implementing `PolyCont Instruction r a` -- for each `r`. dslBlock :: (PolyCont (Return IOError) r Void, PolyCont (Return ()) r Void, PolyCont MaxLengthConfig r Int, PolyCont GetLine r [Char], PolyCont PutStrLn r ()) =&gt; r -- You need to define each instruction as type class, then -- implement them for each `m`. mtlBlock :: (MonadExcept IOError m, MonadExcept () m, MonadMaxLengthConfig m, MonadGetLine m, MonadPutStrLn m) =&gt; m ()
[http://hackage.haskell.org/package/arx](http://hackage.haskell.org/package/arx) if you use Nix, nix-bundle uses it under the hood.
There are two problem in MTL approach: * Performance, as mentioned in [Extensible Effects: an alternative to Monad Transformers](https://dl.acm.org/citation.cfm?id=2503791) * Expression problem, which will be demonstrated below: --- Suppose you have defined a `MyMonadicDataType`, and the following instances: * `instance MonadExcept IOError MyMonadicDataType` * `instance MonadExcept () MyMonadicDataType` * `instance MonadMaxLengthConfig MyMonadicDataType` * `instance MonadGetLine MyMonadicDataType` * `instance MonadPutStrLn MyMonadicDataType` Then, you defined a `MyTransformer` and want instances for `MyTransformer MyMonadicDataType`. You have to define at least 4 instances: * `instance MonadExcept e m =&gt; instance MonadExcept e (MyTransformer m)` * `instance MonadMaxLengthConfig m =&gt; instance MonadMaxLengthConfig (MyTransformer m)` * `instance MonadGetLine m =&gt; instance MonadGetLine (MyTransformer m)` * `instance MonadPutStrLn m =&gt; instance MonadPutStrLn (MyTransformer m)` --- In contrast, `Control.Dsl` based solution need only one instance: PolyCont k r a =&gt; PolyCont k (MyTransformer r) a 
Such useful
\&gt; *as I understood it* every assignment is actually a function Not quite. You can think of \`firstResult = heavyComputation someParameter\` as setting \`firstResult\` to be a pointer to a "recipe" for how to compute a value. \`secondResult\` and \`thirdResult\` similarly become pointers to "recipes" that will hold a reference to the \`firstResult\`. If you start a timer before the let expression and print out the time passed after the let expression, you'll actually find the let was evaluated very quickly. &amp;#x200B; \`putStrLn secondResult\` forces \`secondResult\` as the value is actually needed. To evaluate the recipe that \`secondResult\` points to the reference to \`firstResult\` will get chased and then that will be evaluated (after \`someParameter\` and \`heavyComputation\` are evaluated, of course). The recipe will be replaced with their values so now \`someParameter\`, \`firstResult\`, \`secondResult\` point to values. \`putStrLn thirdResult\` forces \`thirdResult\` which will chase the pointer to \`firstResult\`, but now it will find not a recipe but a value at the end of the the pointer, so evaluation should be fast. &amp;#x200B; These data structures that can be either a value or a recipe for making that value are called "thunks", which is a word you will see from time to time, especially when people are talking about performance. &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
1. There are two built-in transformers. You can also find the specialized version of those transformers in same source files: * https://github.com/Atry/Control.Dsl/blob/0.2.x/src/Control/Dsl/Cont.hs#L58 * https://github.com/Atry/Control.Dsl/blob/0.2.x/src/Control/Dsl/State/State.hs#L80 2. Check out http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.368.3832&amp;rep=rep1&amp;type=pdf 3. There is another approach without `RebindableSyntax` but I did not test yet if it go beyond the ability of GHC's type inference. You can just use `Cont` monad for `PolyCont` like this: `do { toCont Action1; toCont Action2; toCont Action3 }`. You may considered `toCont` as boilerplate like `send` function in `Eff` approach. 
Flattered to see my post from r/programmingcirclejerk being cross-posted here (and actually upvoted!).
Thanks a lot, it's much clearer now
Ok, let's say I too have noticed in (some parts of) the community the psychological traits you mention. But then again, "be the change" and all that, right? Rather than being a salty ***** on reddit, if you really care about the language, blog some Good Advice, write some Good Packages! While we all know that this language encourages some cerebral and over-engineered solutions (still, what's the problem with ResourceT? You didn't say that of course), being sanctimonious on social networks while contributing nothing to the ecosystem makes you just look like a troll.
I suggest using the definition of scanl to expand it for a few iterations and evaluate it by hand. It's a good exercise.
Just in case you want more useful array syntax in Haskell: * https://gist.github.com/ChShersh/40ad502ac68c4ae7e34e77af14f085bf
Out of curiosity, what are some applications you had in mind?
It sounds doable with some support in the runtime system. 
you sir are truly a 10x prgorammer *chef kiss*
Monads and monoids are different things. (But don’t worry about either of them yet.)
This is, hilariously enough, one of the easier-to-understand uses of `Refl` and `eqT` that I’ve seen thus far.
[removed]
&lt;insert here that line about monads simply being monoids on the category something something&gt;
I tried doing that: &amp;#x200B; Assuming: scanl f q ls = q : (case ls of [] -&gt; [] x : xs -&gt; scanl f (f q x) xs) fibs = scanl (+) 0 (1:fibs) =&gt;0 : scanl (+) 1 fibs =&gt;0 : scanl (+) 1 (scanl (+) 0 (1:fibs) ) =&gt;0 : scanl (+) 1 (0 : scanl (+) 1 fibs) =&gt;0 : 1 : scanl (+) 1 (scanl (+) 1 fibs) =&gt;0 : 1 : scanl (+) 1 (scanl (+) 1 (0 : scanl (+) 1 fibs)) =&gt;0 : 1 : scanl (+) 1 (1 : scanl (+) 1 (scanl (+) 1 fibs)) =&gt;0 : 1 : 1 : scanl (+) 2 (scanl (+) 1 (scanl (+) 1 fibs)) =&gt;0 : 1 : 1 : scanl (+) 2 (scanl (+) 1 (scanl (+) 1 (scanl (+) 0 (1:fibs) ) ) ) =&gt;0 : 1 : 1 : scanl (+) 2 (scanl (+) 1 (scanl (+) 1 (0 : scanl (+) 1 fibs) ) ) =&gt;0 : 1 : 1 : scanl (+) 2 (scanl (+) 1 (1 : scanl (+) 1 (scanl (+) 1 fibs) ) ) =&gt;0 : 1 : 1 : scanl (+) 2 (1 : scanl (+) 2 (scanl (+) 1 (scanl (+) 1 fibs) ) ) =&gt;0 : 1 : 1 : 2 : scanl (+) 3 (scanl (+) 2 (scanl (+) 1 (scanl (+) 1 fibs) ) ) =&gt;0 : 1 : 1 : 2 : scanl (+) 3 (scanl (+) 2 (scanl (+) 1 (scanl (+) 1 (scanl (+) 0 (1:fibs))))) =&gt;0 : 1 : 1 : 2 : scanl (+) 3 (scanl (+) 2 (scanl (+) 1 (scanl (+) 1 (0 : scanl (+) 1 fibs)))) =&gt;0 : 1 : 1 : 2 : scanl (+) 3 (scanl (+) 2 (scanl (+) 1 (0 : scanl (+) 1 fibs) ) ) =&gt;0 : 1 : 1 : 2 : scanl (+) 3 (scanl (+) 2 (1 : scanl (+) 1 (scanl (+) 1 fibs) ) ) =&gt;0 : 1 : 1 : 2 : scanl (+) 3 (2 : scanl (+) 3 (scanl (+) 1 (scanl (+) 1 fibs) ) ) =&gt;0 : 1 : 1 : 2 : 3 : scanl (+) 5 ( scanl (+) 3 (scanl (+) 1 (scanl (+) 1 fibs) ) ) Although seems to work, but its quite long and I don't get intuitively how would I deduce on seeing " scanl (+) 0 (1:fibs)" that it produces fibonacci numbers
That's great! Can somebody also port the `this` behavior, so I can finally understand it?
This : "a monad is a monoid in the category of endofunctors, what's the problem?" source : http://james-iry.blogspot.com/2009/05/brief-incomplete-and-mostly-wrong.html
The performance of Rasterific is okay, but not good. For small trivial drawings it's 60 frames per second, for medium-complex drawings of 800x600 size you get 18 frames per second, for large drawings it's about 7 frames per second. However, it is easy to improve the performance of Rasterific: faster abstractions, faster sorting and parallel processing gets you quite far, and I have done that to some degree. Ultimately, it is known that libraries like Rasterific can linearly scale in the number of threads. So with a modern AMD CPU you could get 64 times the speed using 64 threads. Many conjecture that graphic cards will be less (or even no longer) important as we go to 10 000 core machines and up. (I've heard you can rend a single 10 000 core machine from amazon already.) The reason is that is simply takes to long to move data from the CPU to the graphics card and back, so it is faster to do everything in the CPU right away.
Sometimes life is like that. It seems that you know exactly how it works and why it does what it does.... but the answer just doesn't satisfy you on an emotional level. This comic might be relatable :) https://www.smbc-comics.com/comic/2010-06-20 If it makes you feel any better, most people wouldn't be able to deduce on seeing that that it produces fibbonacci numbers.
Indeed, m&lt;4. Didn't have exercise visible when written. Thanks
Noted and edited
I think you can probably do this with only `OverlappingInstances`, no need for `Typeable`. Untested code ahead: class JsArray a where type JsArrayEl a jsArray :: a -&gt; [JsArrayEl a] instance JsArray Int where type JsArrayEl Int = () jsArray n = replicate n () instance {- OVERLAPPABLE -} JsArray a where type JsArrayEl a = a jsArray x = [x]
&gt; `[3*m+1| m &lt;-[1..3]]` seems clunky ghci&gt; length "{y| y = 3m+1, y E N, m &lt;4}" 26 ghci&gt; length "[3*m+1| m &lt;-[1..3]]" 19 
&gt;Dependencies: acme-left-pad (&gt;=2), ...
&gt; javascript &gt; elegance
Not the length, thought there'd be some way to preserve to y = bit rather than place the expression before the bar
If you want, you can introduce a `let` clause for `y`, but it has to come *after* `m` is in scope, not before it: list comprehensions are evaluated left to right. &gt; [y | m &lt;- [1..3], let y = 3 * m + 1] [4,7,10]
Isn't that just repeating yourself? That would be definitionly clunky to me. Of course you can preserve that (/u/amalloy's answer), but I'd never do that unless I was using it twice.
Thanks
Thanks, I hate it
I read about monoid and its somewhat similar to vector spaces from linear algebra. It seems to be obviously intuitive.
The idea has been around a while. https://github.com/functionaljava/functionaljava/tree/series/5.x/core/src/main/java/fj/data/optic
Thanks you, This is what I wanted
Might be repeating but closer to set notation exercise (line with braces) than list comprehension I managed
For an arbitrary monoid, what's the second "seed"? In `fix $ (0:) . scanl (+) 1` the two seed values are `0` and `1` and you can vary them independently for different, but related series.
I agree - I’m a layperson and I still think I can spot an SPJ or a Wadler or a McBride paper a mile off, because they each have such a distinctive style. Most authors aren’t SPJ or Wadler or McBride, though, so I think blinding would still be effective in a majority of cases
Yes!! You’ve got it. Monoids generalise vector spaces. A vector space is a monoid which also supports scalar multiplication.
There are probably some good answers here https://stackoverflow.com/search?q=%5Bhaskell%5D+scanl+fibonacci
yeah, but that just seems less cool. \`OverlappingInstances\` is an abomination
Thanks! 1. Aw, I missed that! Sorry! 2. After reading the paper, I found benchmark code and result: [http://okmij.org/ftp/Haskell/extensible/index.html#MTL-drawbacks](http://okmij.org/ftp/Haskell/extensible/index.html#MTL-drawbacks). It seems the difference is subtle - depends on how effects are layered - and sometimes GHC can make MTL win through better optimization. I think it would be better to go through benchmarks. 3. I think `toCont` is entirely possible, and I prefer that boilerplate than `RebindableSyntax`. (It's only the matter on my preference.)
name one such value it's added.
http://hackage.haskell.org/package/singletons
lmfao lots of the "best" programmers don't get paid s***. Like Richard Stallman, author of gcc, gdb, Emacs, etc. Or Torvalds, author of Linux (obviously), git, etc. How much do you think Simon Marlow was getting paid for his great work on GHC HQ? Much less then the average Facebook employee writing PHP to solve trivial — eh, I'm done.
There's actually recent paper about generalisation of this approach: * [Ghosts of Departed Proofs](https://github.com/matt-noonan/gdp-paper) We just need to wait until it becomes mainstream and widely adopted. And blog posts like this one about RAII definitely help the situation!
Can't this "problems" with the bracket pattern be fixed by these two solutions? * Make the allocate and release functions local to the bracket function so they cannot be called from elsewhere. * Make the allocate and release functions private to the module and only expose the bracket version. 
This is a really nicely worded comment, I really appreciate it :) Looking forward to reading that paper later today.
I don't *think* you can use overlap with associated types. I haven't tried compiling it either.
Hi, there have been some posts of people looking for master programs for Haskellers about a year ago ([https://www.reddit.com/r/haskell/comments/6i6gk6/master\_programme\_with\_haskell\_prospects/](https://www.reddit.com/r/haskell/comments/6i6gk6/master_programme_with_haskell_prospects/) and [https://www.reddit.com/r/haskell/comments/5r088i/where\_to\_study\_master\_in\_computer\_science\_as/](https://www.reddit.com/r/haskell/comments/5r088i/where_to_study_master_in_computer_science_as/)). Maybe that's a good starting point for looking for PL- and formal-methods-focused programs.
In GCed languages, we're only interested in when things are created, and we don't want to think about when they're destroyed. In non-GCed ones, destruction is explicit by necessity, so the language is sort of built around this pain point, with many syntactic and semantic features helping alleviate it. In those languages, once we make peace with this burden, RAII is saying "now that we have all the infrastructure and habits to cope with this, why not use this for resource management too?" Put another way, it's not that Haskell has no easy way of solving this easy problem. It's that the problem is hard by nature, and Haskell doesn't treat it above other hard problems. 
I agree with your points. My favorite description of GC was "an abstraction that lets you pretend like you have infinite memory." Obviously, this would be a _leaky_ abstraction, but it does make clear why GC can't be used for resource management. Just one clarification: I definitely didn't mean to imply that this was an easy problem. Ownership and lifetimes are one of the hardest things to learn in Rust.
&gt; Instead of adding Nothing to a function’s codomain, why not simply restrict the function’s domain to the set of valid inputs? Another riff on the same idea from [a recent post by Matt Parsons](http://www.parsonsmatt.org/2018/10/02/small_types.html): &gt; ### Constraints Liberate &gt; &gt; Runar Bjarnason has a wonderful talk titled [Constraints Liberate, Liberties Constrain](https://www.youtube.com/watch?v=GqmsQeSzMdw). The big idea of the talk, as I see it, is this: &gt; &gt; &gt; When we restrict what we can do, it’s easier to understand what we can do. &gt; &gt; I feel there is a deep connection between this idea and Rich Hickey’s talk [Simple Made Easy](https://www.youtube.com/watch?v=34_L7t7fD_U). In both cases, we are focusing on simplicity – on cutting away the inessential and striving for more elegant ways to express our problems. &gt; &gt; Pushing the safety forward – expanding the range – does not make things simpler. It provides us with more power, more options, and more possibilities. Pushing the safety backwards – restricting the domain – does make things simpler. We can use this technique to take away the power to get it wrong, the options that aren’t right, and the possibilities we don’t want.
Thanks. That helps. I tried searching Google with same query but didn't get satisfactory results.
This looks more about ownership and borrowing rather than RAII. You only need the former for the properties you want. For example, in a `bracket` implementation with ownership semantics, the `bracket` would own the resource allocated by the allocation function and only pass a reference to it to the bracketed function. Then it'd move the resource to the freeing function.
please flag this as NSW.
Thanks! I found this one as well for reference [https://www.reddit.com/r/haskell/comments/5zl482/what\_graduate\_program\_do\_research\_groups\_in/](https://www.reddit.com/r/haskell/comments/5zl482/what_graduate_program_do_research_groups_in/)
Interesting approach! I too would prefer the `toCont` boilerplate to `RebindableSyntax`, but that's a matter of taste I suppose. How do you compose effectful statements with different answer types? For example [`Yield`](https://hackage.haskell.org/package/control-dsl-0.2.1.1/docs/Control-Dsl-Yield.html#t:Yield) has answer type `[x]` while [`State`](https://hackage.haskell.org/package/control-dsl-0.2.1.1/docs/Control-Dsl-State.html#t:State) has answer type `(s -&gt; x)`. Now, if I use both `Yield` and `Get` in the same do-block, what happens? Are we going to have two levels of `Cont`, i.e. `Cont (Cont (s -&gt; r) [x]) a`?
FTI the PDF of the paper is available here: https://github.com/matt-noonan/gdp-paper/releases/download/june-2018-draft/gdp.pdf
in-dry-tone("Yes. That's precisely what I mean.")
&gt;RAII avoids a use-after-free completely here No, it's not. You can still do MyResource now_here = std::move(my_resource); my_resource.use(); I see this mistake very often in constructors. You can avoid this by deleting move constructors and assignments, by it makes things hard to use: you can't return the resource from a function nor pass it as an argument. 
Your example works because you just print stuff, not accessing actual memory or files. If it were a std::unique_ptr for example, this would crash the program. No copy constructor involved: generally, you can't copy resources. Let's make things clearer: void f (MyResource r) {} MyResource r = create_it(); f(std::move(r)); r.use(); // r was consumed by f by is still accessible
\`deriving via\` works as long as the "model" type is \`coerce\`able to the type we we're deriving an instance for. It actually works via coercions.
A good example of the kind of fiddling one has to do in Haskell can be found in the odbc package I wrote: * The type of `HSTMT s`: [carries a ST-like s](https://github.com/fpco/odbc/blob/master/src/Database/ODBC/Internal.hs#L841-L842) and then * Functions like [withStmt](https://github.com/fpco/odbc/blob/master/src/Database/ODBC/Internal.hs#L315-L320) use a `(forall s. HSTMT s -&gt; IO a)` to avoid the statement escaping scope, as in this blog post. * But like written in this blog post, having an `s` in everything is kind of annoying as an API to use. * So then [the Connection type](https://github.com/fpco/odbc/blob/master/src/Database/ODBC/Internal.hs#L68) has three levels of safety measures put upon it: * An `MVar` to ensure multiple threads can use it (and [close it](https://github.com/fpco/odbc/blob/master/src/Database/ODBC/Internal.hs#L199-L213)) safely. * A `Maybe` so that once it's closed, it becomes `Nothing` and [any function attempting to use a `Connection` will throw an exception.](https://github.com/fpco/odbc/blob/master/src/Database/ODBC/Internal.hs#L282-L293) * `ForeignPtr` so that once garbage collected (or finalizer explicitly called) [the finalizer will call](https://github.com/fpco/odbc/blob/master/src/Database/ODBC/Internal.hs#L177) the proper freeing function from the ODBC C library. It's a lot of work I need to remember to do. With my experiments in Rust so far, it's definitely more comfortable to have pervasive ownership and lifetime tracking.
&gt; Can we gouge out the other eye Well that's quite the metaphor!
Your RAII example is just as broken as bracket: {-# LANGUAGE RankNTypes #-} import System.IO import Control.Exception.Base (bracket) -- Use a phantom variable newtype Resource s a = Resource a -- Going well... allocateResource :: IO (Resource s Handle) allocateResource = Resource &lt;$&gt; openFile "myFile.txt" ReadMode -- Yep, everything's good freeResource :: Resource s Handle -&gt; IO () freeResource (Resource handle) = hClose handle -- Nothing to see here, using that good old ST style trickery withResource :: (forall s. Resource s Handle -&gt; IO a) -&gt; IO a withResource = bracket allocateResource freeResource -- Perfect right?? useResource :: Resource s Handle -&gt; IO () useResource (Resource handle) = hGetContents handle &gt;&gt;= putStr -- uuuuuuh... wait, that string is lazily read, so if I return that string and -- use it, I'll end up reading from a closed handle! But I should be able to -- query a bracketed resource to return a valid value, but now it's broken! unsafeUseResource :: Resource Handle s -&gt; IO String unsafeUseResource (Resource handle) = hGetContents handle -- Okay, now we can just return the file handle unwrapped itself evenWorse :: Resource Handle s -&gt; IO Handle evenWorse (Resource handle) = pure handle -- We can just convert between any type with a phantom vairable. completelyBroken :: Resource Handle a -&gt; Resource Handle b completelyBroken (Resource handle) = Resource handle main :: IO () -- still works as intended -- main = withResource useResource main = do nope &lt;- withResource (pure . completelyBroken) useResource nope -- throws runtime error The first thing you're going to want to do is hide that `Resource` constructor inside a module somewhere so we can't pattern match against it. We want to be wrap a value inside a `Resource` without being able to unwrap it, so it can't be used outside the scope. `Resource` is then a monad. Essentially what you want is the `Identity` monad, but with an `execIdentity` and no `runIdentity`. The other difference is you need `IO` so `Resource` should be a monad transformer. If you're interested, I could write a `ResourceT` (or perhaps better called ScopeT) transformer that should suit your needs and provide an extra layer of protetion. It will still be broken if you use an `IORef` or `STRef`, but if you're doing that then you're purposefully trying to fuck yourself.
Awesome. Love the super simple set theory intro and the grand finale which I won’t spoil. Can’t wait to see this in Haskell’s DB APIs.
copying your code where you fix the Haskell version at the end and adding one modification gives: {-# LANGUAGE RankNTypes #-} import Control.Exception (bracket) data MyResource s = MyResource newMyResource :: IO (MyResource s) newMyResource = do putStrLn "Creating a new MyResource" pure MyResource closeMyResource :: MyResource s -&gt; IO () closeMyResource MyResource = putStrLn "Closing MyResource" withMyResource :: (forall s. MyResource s -&gt; IO a) -&gt; IO a withMyResource = bracket newMyResource closeMyResource useMyResource :: MyResource s -&gt; IO () useMyResource MyResource = putStrLn "Using MyResource" breakCode :: MyResource a -&gt; MyResource b breakCode MyResource = MyResource main :: IO () main = do resource &lt;- withMyResource (pure . breakCode) useMyResource resource Your example is not broken because of the phantom type variable. That's sound, and the packages you linked show that. Your bug is that fact you can pattern match `MyResource`, and most ways in which you would normally fix this have a similar problem. As long as you can return some lazily read pure value from whatever resource you have required, you can break your scope when you read from it.
That won't work because of the type family conflict.
Thank you for this, didn't realise that a more complex expression on left was acceptable (just assumed Uni exercise best format, not looked) and agree it's the better style.
Hi! Author of the haskus packages here! I'm glad to see that this work inspires some people! I have been working on documentation lately, see: https://docs.haskus.org/eadt.html One of the other issues I had with the haskus package was the amount of boilerplate. Could you be more specific? Given that I don't have to use labels, i would think it is the opposite! Moreover while I don't use TH yet, your `mkVarPattern` (for example) could be easily implemented for EADTs too, so there is no fundamental difference here. Am I missing something? Unfortunately, I ran into compile-time performance issues with many constructors. Compile-time performance has been an issue for me too but I haven't invested much time to enhance it. I would love to see benchmarks to see how far I am from row-types and other similar packages.
I'm pretty sure /u/snoyberg knows that the `Resource` constructor should not be exported.
But the problem is a little more nuanced than that. If you were to use `hGetContents` inside the bracket, and return the string, the string is read lazily, so you can read from the file after it's been closed, even with a phantom variable.
Very cool! I wonder if you could implement this using a source plugin.
Would it work with functional dependencies?
GHC 8.6 ([release notes](https://downloads.haskell.org/~ghc/8.6.1/docs/html/users_guide/8.6.1-notes.html#plugins)) introduced [source plugins](https://downloads.haskell.org/~ghc/8.6.1/docs/html/users_guide/extending_ghc.html#source-plugins) which can do some [cool things](http://oleg.fi/gists/posts/2018-07-06-idiom-brackets-via-source-pluging.html) like having singleton lists with parentheses ([ Just 1 + Just 2 ]) work as idiom brackets liftA2 (+) (Just 1) (Just 2)
How can I model enriched categories in Haskell? There seem to be no libraries or anyone talking about it
This can already be done using monad comprehensions and rebindable syntax: [https://gist.github.com/sjoerdvisscher/f36676a8ba88fcfa86fce389255b74dd](https://gist.github.com/sjoerdvisscher/f36676a8ba88fcfa86fce389255b74dd) Still I would love to have this feature built-in!
The main source of boilerplate I encountered with EADT was writing the `VariantF` instances; having to write the base case and recursive case over `VariantF` was a lot of work. With row-types, the `metamorph'` function abstracts this away. I further simplify things with the `varFAlg` function. For example, from my tutorial module: ``` instance (Forall v (OverList a a' r)) =&gt; OverList a a' r (VarF v) where fmapList' f = varFAlg @(OverList a a' r) (fmapList' f) ``` Something I disliked with EADT was using type classes for every operation I wanted to write. I missed being able to write a function for `cata` that simply specified cases (obviously this is /possible/, but the (potention) incomplete pattern matches are unsafe (and the warnings frustrating)). Using row-types gives me the ability to specify cases explicitly without incomplete pattern warnings. There is some additional overhead from specifying labels and separating out the functions, but I feel there are cases where its simpler than using type-classes. For example, again from my tutorial module: ``` result4 :: List1 String result4 = cata (caseonF r) exList1 where r = #nilF .== (NilF' -&gt; Nil) .+ #cons1F .== ((Cons1F' a x) -&gt; Cons1 (show @Int a) x) ``` Using a similar technique, we can get behaviour similar to default/overridable instances by explicitly handling a set of constructors one way, and the rest another way. ``` alg :: Alg (List2F Int) (List1 String) alg w = case multiTrialF @("cons2F" .== Cons2F Int) w of -- Explicit handling of specified constructors Left v -&gt; caseonF r v -- All others handled by fmapList' Right leftovers -&gt; fmapList' (show Int) leftovers ``` As far as compile-time performance, EADT with many constructors (I think I had 15 or so when I decided to look for something else), was taking somewhere around 30 seconds to reload in ghci. With row-types, I haven't had to wait more than a second or two so far.
I've only read part of "Trees That Grow", but it's on my list. Without knowing that much about it, I think one advantage of this technique is that the user doesn't have to write any type families or include additional type parameters in their data types.
The answer type is actually `s -&gt; [x]`.
Maybe you can write up an example demonstrating that problem because I don't think "the phantom variable solution is just as broken as the bracket solution" captures what you're trying to explain.
How the heck do these two things mesh? &gt; The final agreement that we arrived at was based largely on the terms Julie demanded. It was supposed to be an amicable resolution to our problems. Julie got what she wanted and would benefit from the book’s final publication and the readers would finally get a finished product. -- Chris Allen &gt;&gt; Is there some way of buying HaskellBook such that you get part of the money and it does not all go to Chris Allen? &gt; [no, not anymore](https://twitter.com/argumatronic/status/1042434413580083200) -- Julie Moronuki I'm pretty sure that nobody would demand or want to be written out of the profits of something they've toiled over (Unless they developed strong ethical objections to it or something, which probably doesn't apply here)... So it sounds like this isn't "largely the terms Julie demanded" or her "getting what she wanted". This is probably a compromise, and it shouldn't be portrayed as Julie's total victory. According to Julie she has "no involvement or stake in the book or say over when it might ever be printed" any more, and she doesn't get profits from the sales anymore, so... reading between the lines, she probably sold her stake in the book for a one-off sum to be rid of the trouble of continued involvement.
The "why" isn't impossible here. I'm going to slightly reformulated and generalize a bit: xs = x0 : scanl f x1 xs Ignore what f, x0, and x1 are. What really matters is what happens when you feed scanl its own tail. In this case, it's capturing the pattern of "create a sequence by starting with x0 and (f x0 x1), then every subsequent xn is f applied to the two previous values." That's just how the knot-tying fits together there. And hey, the Fibbonacci numbers fit that pattern, though you have to be careful with your base cases. You can vary the whole thing, of course... xs = x0 : x1 : scanl f x2 xs Now you're starting with 2 values, and calculating the next one from the combination of the current and the entry 2 back. I can't think of anything offhand that uses that pattern, but it's a thing. But given that extra example and the type of scanl, let's try drawing some extra conclusions. scanl applies a binary function function to two values. One is the "current state" in the sense that it will be the next value in the output list. The other value is read from the input list. When the input list is related to the output list, you get a sort of feedback loop. It has a tendency to diverge (at least if f is strict) unless you add a delay of some sort to the feedback. In this case, that means adding extra values to the start of xs. So there is some sort of intuition available here. Feeding scanl it's own result creates a form of feedback loop. Pre-populating elements of the result adds a delay before it operates on its own output. That delay can be critical in preventing the computation from diverging if it would end up depending on its own result. Now, that is a slightly different formulation than the one you started with. But the idea is the same - scanl is operating on its own output, but with a delay. The only difference is whether the delay is also part of the output. For the Fibbonacci sequence, both work. For other sequences, the difference might matter. 
[removed]
I didn't actually consider including it, since I haven't seen it used very much in practice. Funny enough, I [pushed a commit](https://github.com/snoyberg/trio/commit/05ca97a4cdc5f39d6318949192bb2503f82aea93) to my experimental trio repo to use it a while ago.
myself, only the first. it's simpler, I can control representation more easily, and instances are more convenient. i.e. you can easily switch out Int for (say Word8, as suggested) without breaking the rest of your types or forcing a splitting. and, instances can be derived in the clauses like `deriving (Monoid)` without line-by-line `deriving instance Monoid (Value ...)` each time.
I think the "CloudHaskell" (or "DistributedHaskell"?) project did stuff like this, but I also think it's been abandoned.
This? I was under the impression that this was a pretty well known gotcha in Haskell. ``` {-# LANGUAGE RankNTypes #-} import Control.Exception (bracket) import System.IO data MyResource s = MyResource Handle newMyResource :: IO (MyResource s) newMyResource = do putStrLn "Creating a new MyResource" handle &lt;- openFile "tempfile" ReadMode pure $ MyResource handle closeMyResource :: MyResource s -&gt; IO () closeMyResource (MyResource h) = do putStrLn "Closing MyResource" hClose h withMyResource :: (forall s. MyResource s -&gt; IO a) -&gt; IO a withMyResource = bracket newMyResource closeMyResource useMyResource :: MyResource s -&gt; IO String useMyResource (MyResource h) = do putStrLn "Using resource" hGetContents h main :: IO () main = do s &lt;- withMyResource useMyResource putStrLn s ``` Output ``` Creating a new MyResource Using resource Closing MyResource *** Exception: tempfile: hGetContents: illegal operation (delayed read on closed handle) ```
Nice! So what do we learn from this? I guess that anything that holds onto a resource transitively must be tagged with `s`.
&gt; As long as you can return some lazily read pure value from whatever resource you have required, you can break your scope when you read from it. This point really helped the problem click for me, thanks!
Is this different from monadic regions? http://okmij.org/ftp/Haskell/regions.html (Looks the same to me, but then I’m not familiar with the details of RAII so cannot really comment.) 
Ah yes, this is basically the same as their suggestion to use lists, but instead use the free monoid, i.e. `Free { runFree :: forall m. Monoid m =&gt; (a -&gt; m) -&gt; m }`. (I have this packaged up in [fmlist](https://hackage.haskell.org/package/fmlist), which has `fromFoldable` and `foldMapA` too.)
Very cool! From a performance perspective, it would be nice to have the option of a right vs a left \`foldMap\`. The standard \`foldMap\` is implemented as a right fold. This is fine for many uses, but often aggregations like the ones mentioned in this post benefit from a strict left fold. see notes here: [http://tech.frontrowed.com/2017/09/22/aggregations/#strictly-speaking](http://tech.frontrowed.com/2017/09/22/aggregations/#strictly-speaking)
Off topic, but I'm amused how often I see this kind of communication pattern online. Each person continues to feel *clarifications* the other is making are instead direct disagreements. Person A: Claim Person B: Response to claim Person A: I didn't say &lt;thing from response&gt; Person B: I didn't say *you said* &lt;original thing I said&gt;, I was just *sayin'* Person A: I didn't say *you said I said* &lt;original thing you said&gt;, I was just restating my point, in case someone *else* thought that...
This is great. I'd love to see more of this genre of blog post. 
Are you saying monad comprehensions are as powerful a monoid comprehensions?
Aa a beginner in Haskell, actually practical examples is what I need, so thank you very much :)
Slightly OT: is there a *simple* ELI5 guide to Nix? Something like this is for Git: http://rogerdudler.github.io/git-guide/ Something that will help install the mental model for what's going on, what's actually on your system, etc.
Oh cool. Then in [my thing](https://www.reddit.com/r/haskell/comments/9mdhv0/comprehending_monoids_with_class/e7eguos/), you'd just add: fromFoldable' = Cont . flip foldMap'
Yes even with ghc 8.6 the compiler itself is still constraint by MAX_PATH due to gcc and binutils. That said I'm trying something now that may fix it. Hopefully have something to post soon. 
So what does the author mean when they give an example of something that they can do with monoid comprehensions that they say can't be done with monad comprehensions?
It might be limited, but turn your enrichment into a `ec :: k -&gt; Constraint`, and use constraint kinds and the constraints library to expose `forall d c. ec (cat d c)` in addition to the normal [Category](http://hackage.haskell.org/package/base-4.12.0.0/docs/Control-Category.html) stuff?
I don't think that particular quote from the article is actually true. By the author's own admission: &gt; On the other hand, we can encode a monoid comprehension with a list comprehension in a straightforward way. All we need to do is wrap the comprehension in a fold, and wrap each source in a toList (both from the Data.Foldable module): &gt; &gt; q3 = { g x y | x ← xs, y ← ys, f x y } &gt; q3 = fold [ g x y | x ← toList xs, y ← toList ys, f x y ] &gt; &gt; so monoid comprehension offers no gain in terms of pure expressive power; but besides the slight reduction in syntactic overhead and the added flexibility given by heterogeneity, it still has several advantages over monad comprehension, as we will see below. Plus the only example they give is something my version handled fine.
I had the exact same thought: [monoid-comprehensions](https://github.com/WhistlePayer/monoid-comprehensions)
As soon as lazy IO is used, all bets about resources are off the table anyway, so perhaps a different example would be more suitable. With `unsafePerformIO` being used, one can hide the most miraculous behaviour behind values as trivial as `Bool`; it is a hack to disable the type system, so it is not too surprising that it breaks safety gained by a type trick like `forall s .`.
Good luck running anything more complicated than Pong at an acceptable frame rate. Demanding games these days have to worry if things are in the *CPU cache*. Disk access is like a million times slower than loading RAM into the cache. Not to mention that many games require at least 2 GB. Transferring that much data to and from disk in 16.667 ms isn't possible. And it would absolutely destroy your disk if you're doing it every frame even if it were possible.
What does "turn your enrichment into a ec :: k -&gt; Constraint" mean? To me something feels off if I were to use Control.Category to model a category X enriched in category V. Category can be enriched in V only if V is monoidal, but class Monoid in Haskell and Control.Category don't really have overlaps: former models things as objects which can be combined, latter only talks about morphisms. Sure, I can model compositions of *morphisms* in Haskell with Control.Category*,* but enrichment requires that I model a morphism from a monoidal product of two special objects (which actually represent morphisms in another category) to another special object. &amp;#x200B; &amp;#x200B;
My group at ULisboa focus on Programming Languages, mainly for distributed computing. If you need more information, you can reach me at alcidesfonseca.com. http://rss.di.fc.ul.pt/tools/
&lt;interactive&gt;:5:33: error: Conflicting family instance declarations: JsArrayEl Int = () -- Defined at &lt;interactive&gt;:5:33 JsArrayEl a = a -- Defined at &lt;interactive&gt;:8:50
Thanks for letting me know, Il definitely have a look into that too! 
 $ cabal2nix --shell . &gt; shell.nix $ nix-shell 
I am curious what happens with GHCJS, does this turn into something horrid? Once a long time ago I tried experimenting with structs to replace \`IORef\` filled records in reflex. If I recall rightly I didn't see any performance benefit.
Rust might be a better choice to bridge the gap between C++ and Haskell
I never looked at what would happen in GHCJS, I'm guessing it wouldn't help much. GHCJS doesn't really get to unbox anything.
I don't like syntax that suddenly means something totally different.
I think this would be a fascinating project, but it would also be a **serious** research project and may be an unacceptable risk if you need a major portfolio piece to show upon graduation. That said, someone wrote a Quake3-ish game in Haskell in 2005 for an undergrad thesis project: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.435.1865&amp;rep=rep1&amp;type=pdf The thesis is titled "Functional Programming and 3D Games", if that link doesn't work. I assume that student had a full year to work on his project and it I don't know how much prior Haskell experience he had going in.
The `s` in `Object s` is the region like in `ST s` or `RealWorld#` for `IO`. This lets you work with structs locally in an ST computation. https://www.youtube.com/watch?v=I1FuZrDhOEk provides a crash course on why the library is structured the way it is. It also includes a rather choice sound bite of SPJ in the background threatening to change the language to make the things I'm doing here illegal! The template haskell bits were added shortly thereafter by Eric Mertens to make it a little safer to use. My longer term goal here is to take some stuff from Ryan Yates for mutable constructor fields and remove the need to go out to kind #, at which point I'll be able to write this sort of stuff more or less directly in normal Haskell. Summarizing the purpose of the library, if you try to write a mutable linked list in Haskell with something like data List a = List a !(IORef (Maybe (List a)) then you run into the fact that even strict the IORef is secretly a reference to a MutVar. So now you have a pointer to a mutvar which points to a maybe which points to your list rather than a cons cell that points directly to the next list entry. Here I use the fact that a `MutableArrayArray#` has things that live in `TYPE 'UnliftedRep` points directly to other `MutableArrayArray#`s in `TYPE 'UnliftedRep`. The `unsafeCoerce#` stuff for `Null` and the associated pattern synonym is to create an artificial `Null` pointer. The use of `SmallMutableArray#` is because I don't want page marking for small objects and there is no `SmallMutableArrayArray#` forcing me to fake it. Unboxed fields are the weird thing they are because I can't unpack things into `(Small)ArrayArray#`, and so I have to allocate a `MutableByteArray#` to hold them. (This indirection is one that using Ryan's TStruct / mutable constructor fields stuff I'll be able to avoid. It is the major source of remaining cost.) As for why no cas for slots, I don't have one offered to me by the compiler.
I did my CS undergrad at Uppsala University, Sweden. Haskell was a big part of my education and still is for current CS and Math undergrads from what I've heard. The people in charge really proud themselves with the fact that they are pretty much the only ones offering a "pure", classical Computer Science education while the other top schools in Sweden are all technological institutes and thus CS is an engineering degree with a big focus on the computer engineering side of things as well as engineering math and all that jazz. Might be worth looking into the CS MSc and see what they're offering.
&gt; What does "turn your enrichment into a ec :: k -&gt; Constraint" mean? That is, represent the additional qualities of the hom-sets with some type class. If you also used a type class to model "being an enriched category", then that would a a separate type class from the one(s) inside the `Constraint`. &gt; To me something feels off if I were to use Control.Category to model a category X enriched [...] Yes, I meant for you to create a separate type class / record for enriched categories, but it would share some aspects, since enriched categories are categories. &gt; Category can be enriched in V only if V is monoidal If you want to explicitly model that, you can, by using an entailment between a Monoid instance and V, or by including it is the context of a type class or constructor in your model.
Some reasons that I can think of: 1. Traits (and derive) are closer to type classes and deriving than interfaces in C++. 2. Rust iterators are lazy and closer API-wise to Haskell compared to C++. 3. Rust actually has relatively ergonomic ADTs and pattern matching.
I think that's selling Rust short. * Rust's trait system is almost *exactly* like (vanilla) Haskell typeclasses * Trait objects are Haskell existentials, just with an implicit conversion * Rust has Enums, which are basically ADTs. They make *a lot* of things that are awkward in C++ easy, For example, writing an AST for a language, painful with objects, easy in Rust. * Errors are handled through an algebraic type (Option/maybe or Error) rather than through exception throwing, and are threaded together monadically. * Functions are first class (albeit with an extra level of indirection through Trait Objects), and many idiomatic ways of doing things use higher order functions. * Both languages use Hindley-Milner style inference with proper parametric polymorphism, not the unsound mess of copy-pasting that C++ templates are. Moreover, I think there are a lot of moral similarities. Both languages have type/memory safety as opt-out rather than opt in. Both use immutability as the driving force behind powerful abstractions. The abstractions are different, but I think they come from a common place. Sure, Rust doesn't have higher-kinded polymorphism, and the memory management means that closures are a bit awkward. But to say there's nothing significant in common between the two is a stretch.
Not in great detail, only at surface level. I'm theory, it should address these kinds of problems, but I've heard conflicting reports about whether the proposal will reliable clean up resources in the presence of exceptions. In any event, my understanding is switching over to linear types for this will require a fairly large amount of coffee change.
Got it, thanks! 
More than once, I've considered writing a source plugin to give me access to core for exactly this usecase. Doing something across module/package boundaries is always the problem I haven't solved in my head.
The original version of React was written in SML, but then ported to JS. In fact, if you want a taste of what that'd look like, ReasonReact is pretty much React in OCaml.
Yeah, and React + Redux is pertty much the Elm model
Awesome! Can't thank you enough for this!
Does **sequenceA** behave the same as **sequenceM**? What is the recommended way to process infinite lists of IO effects lazily? (Using sequence results in memory leak)
Light grey on white is a horrible colour scheme. This is painful to read.
My concern is that wouldn't the automatic garbage collection mess with the performance?
This is the talk: https://www.youtube.com/watch?v=1PhArSujR_A Continued in part 5. He rewrote some parts of Wolfenstein 3D.
Nice! Great to hear Haskell is soaring to new heights :)
The question is really what you want to do with it. You can always discard the payload of option 1 and guarantees a well-formed representation. If you want to add uses in the future you could use this over-complicated mess (this way you get new instances for free): {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE DataKinds #-} {-# LANGUAGE StandaloneDeriving #-} {-# LANGUAGE FlexibleContexts #-} {-# LANGUAGE UndecidableInstances #-} data Tag = AT | BT | CT deriving Show data Use1 data Use2 data Use3 type family Payload (use :: *) (con :: Tag) :: * type instance Payload Use1 AT = () type instance Payload Use1 BT = Int type instance Payload Use1 CT = () type instance Payload Use2 AT = () type instance Payload Use2 BT = () type instance Payload Use2 CT = (Char, Double) type instance Payload Use3 AT = Int type instance Payload Use3 BT = Int type instance Payload Use3 CT = () data AsEnum use = A (Payload use AT) | B (Payload use BT) | C (Payload use CT) deriving instance (Show (Payload use AT), Show (Payload use BT), Show (Payload use CT)) =&gt; Show (AsEnum use) discardPayload :: AsEnum use -&gt; Tag discardPayload v = case v of A _ -&gt; AT B _ -&gt; BT C _ -&gt; CT Or just use a type class that converts to the tag type. That's really all you could do with it generically.
When I said "cannot be expressed with a monad comprehension" in that paragraph, I meant "cannot be expressed with a _pure_ monad comprehension _using the extended syntax for grouping and ordering_." I will change the text to clarify that :\^) Sure, you can encode it (albeit possibly with the wrong space efficiency). But I'd argue that as soon as you start folding things using nontrivial monoid instances, you are just using a monoid comprehension in disguise, even if there is an actual list comprehension in there somewhere in the encoding.
Great explanations, thank you for explaining this and your haskell workflow as well! Would like to see more posts like this in the future. 
That makes sense. I'm surprised though as I have been looking into migrating one of our projects to cabal 2.4 and running \`cabal v2-build\` just worked. But now I realize that I never told Cabal which resolver to use and yet it was able to pick versions of my dependencies. I looked at the dependencies of project and many had very few version bounds so I assumed cabal would somehow use something like Stackage under the hood. I'm still a bit confused. &amp;#x200B; &amp;#x200B;
&gt; the same as their suggestion to use lists, but instead use the free monoid Worth noting that list [happens to be](https://stackoverflow.com/questions/13352205/what-are-free-monads/13357359#13357359) the free monoid :\^)
How credible is that number that 80% of Haskell users were Stack users? Is this number about to decrease now that cabal [appears to be catching up quickly](https://old.reddit.com/r/haskell/comments/9l5gyi/fyi_cabal24_supports_sourcerepositorypackage_for/)? &gt; As the world’s leading provider of Haskell tools and services, FP Complete is committed to contributing more than its fair share to the community. These encouraging survey results just reinforce our commitment. ... Based on the survey results we will continue and even enhance our commitments to Stack ... [As I've complained about this in a past thread](https://www.reddit.com/r/haskell/comments/9fefoe/if_you_had_the_ultimate_power_and_could_change/e5w6aeh/) I feel like having two imperfect tools promoting different file formats is hurting Haskell adoption in the long run. Can we please pick either Cabal or Stack, and deprecate the other one? I don't really mind which one but, I mean, it's admirable that Cabal is now catching up to Stack but with fpcomplete's renewed commitment to Stack picking Stack seems to me that Stack is where the smart money is going.
Thanks for the insights! &gt; The structure is only traversed once, and there is no intermediate list. Doesn't your solution allocate many intermediate continuations before making them all disappear in one go with `runMonoidComp`, thus basically constructing an intermediate data structure in memory, similar to an intermediate list ? Which means it would have the same space (in)efficiency, as in quadratic instead of linear or constant in case of a Cartesian product query. I'm not very familiar with Haskell performance evaluation though, so I may be wrong. &gt; The real innovation in this article is the use of monoidal-containers instead of magic syntax. I'd agree with that. I also wanted to show that while monads are great for many things, they don't give you anything over plain monoids in the specific context of expressing queries over collections of data – trying to shoehorn them there just makes things unnecessarily complicated and inefficient.
Except that strictly speaking, [they aren't](http://comonad.com/reader/2015/free-monoids-in-haskell/).
Do you have more information or a source on fpcomplete's renewed commitment? Thanks
&gt; How credible is that number that 80% of Haskell users were Stack users? I would question that too. I've been writing Haskell for 10 years and doing so professionally for 5 years. I read /r/Haskell reasonably regularly but somehow I missed this survey. &gt; I feel like having two imperfect tools promoting different file formats is hurting Haskell I agree. &gt; seems to me that Stack is where the smart money is going. And there I disagree :). 
There's only one way to find out: measure it! I wouldn't be surprised if all solutions perform the same, as the intermediate structures could be compiled away.
Why are you using different sets of packages / different compilers for two components of the same thing? Is that supported? Anyhoo, the obvious answer to your question would be to add the missing superclass instance. Does that not work in this case? `Semigroup` was recently made a superclass of `Monoid`, but aside from that they're basically the same in both `base` versions. 
But WHAT are profunctor optics?
My own anecdotal experience is that everyone I physically know has switched to Stack, and prefer it over Cabal. My experience from [my own project](https://github.com/diku-dk/futhark) is that my colleagues breathed a sigh of relief when we switched from Cabal to Stack a few years ago. This was pre-`new-build`, though. I have been playing around with `new-build`, and it is quite good. It feels a little snappier than Stack - I think it spends less time querying stuff, doing rebuilds, and also for some reason seems to link faster. I might well switch to it for my own development. I will probably still recommend that all my users compile and install via Stack, since it reduces the amount of moving parts, and ensures that my code builds reproducibly (I might also use `cabal freeze`, but that does not ensure they have a sensible version of GHC). The frustrating thing is that, despite all of Cabal's increasing excellence, the user experience still has lots of superficial warts that bother me. For example, I'd like to switch back from `package.yaml` now that the .cabal file format has grown common sections, but the insistence on maintaining explicit `exposed-modules` is just pointless busywork for me. Similarly, the [treatment of package environment files](https://ghc.haskell.org/trac/ghc/ticket/15541) is also a bit of a footgun (and I'm [not alone](https://github.com/haskell/cabal/issues/4542)). Also, I find it silly that `cabal install` by default installs to a location that *nobody* who is not already a Haskell programmer has on their PATH (`~/.cabal/bin`), whereas Stack uses `~/.local/bin`, which seems fairly common for other languages, too. Such UI concerns are of course subjective, and I cannot argue that my preferences are objectively better than those of the Cabal authors. They probably designed it in the way that *they* like. However, it is my impression that the Stack developers are a bit more responsive to user's UI concerns, and *in particular* to ensuring simple defaults for new users. While I'm not a beginner, nor are my co-workers, some of the *users* of my software may not be Haskell programmers at all, and I want to ensure that they can build and install my code with a minimum of bother, option flags, and configuration changes neded to the build system. It's a shame, because *in principle* I prefer Cabal. Stack so far just works better for me in practice.
Latest versions work with almost anything. That isn't the real problem. One big thing that version bounds help with is on older compilers when you need to find a build plan that isn't bleeding edge, because, say, the latest version of one of your dependencies wasn't backported to that compiler. With stackage the story is that for old compilers you'll use an old snapshot, but there when you _do_ need something for which a later version was released with support for that compiler, but which happened after the last snapshot on that release, you're in for a rough time. If you don't have hard version bounds you can wind up with situations where on older compilers you'll get builds that fail in mid-build, rather than complaining about no successful build plans. These are the scenarios that are unaffectionately referred to as cabal hell, because once you're there, you already have a bad batch of deps built. When this was done before `new-build` you were basically just hosed and had to `--force-reinstall` your way out of the problem. Nowadays, its still obnoxious, but at least it is controlled.
Yeah, it's almost a perfect fit for the FP crowd. I say "almost" because, the way react presents it, `setState` is pretty effectful. However it feels better if you think of `setState` as a tail recursive call to a `render :: State -&gt; Render GUI` function. To make it more functional, you can also store a rendering function itself as a React state rather than a data structure. So then setState is basically `render :: Render GUI`, and the `view` function can simply be `this.state()`.
&gt; Doesn't your solution allocate many intermediate continuations before making them all disappear in one go with runMonoidComp, thus basically constructing an intermediate data structure in memory, similar to an intermediate list? I don't *think* the allocations of continuations will be any different at all from the allocations of closures with your system. You allocate a closure for every lambda you make when you call foldMap (eg once for every element of a parent foldMap when you call foldMap within a foldMap). I'm pretty sure these correspond *exactly* to the continuations made by Cont. In fact I'm pretty sure that this Cont based solution would really just be the exact same encoding, except for some newtype (un)wrapping.
I need compatibility with 7.10 at least. 
Here's an example that does not require any additional language extension: https://github.com/hadolint/hadolint/blob/e8dd20d2d33e4d3fa9f1b5e64c75630137249f36/src/Hadolint/Formatter/Format.hs#L16-L26
Ignoring stack vs cabal, I'm surprised other language ecosystems aren't taking lessons from Stackage (does this idea originally come from Nix?) where every package is guaranteed to build with every other package. It just makes life so much simpler. The alert system for updating dep versions is also very helpful.
What is preventing you from upgrading to 8.2.x? It your code open source/can I help? 7.10 is nearly 3 years old at this point...
https://prime.haskell.org/wiki/Libraries/Proposals/SemigroupMonoid#Writingcompatiblecode
Debian stable ships 8.0.1 for instance.
More like &lt;hyperbole/&gt; since the framework cult is universal in javascript and compiler extensions in haskell are most of the time needed, nobody in haskell fills their source-code with directives unnecessarily or because someone else said it was cool. JS devs on the other hand are adept of TDD: Trend Driven Development. 
I had really high hopes for Reason+React; still do actually, but the adoption has been slow. The ReasonML vim integration via Merlin is still better than what I have with Haskell: proper type-based autocomplete with signatures in the completion menu.
Unless it's a critical application where timing is crucial, which games are not, the GC is not a problem. Almost all game engine nowadays are written in C++ and it has a GC. The trick would be, I guess, in exploiting monads to have multithreading to paralelize computing 
I did it. For some reason `$ curl --connect-timeout 5` [`https://oscoin.io`](https://oscoin.io/) `curl: (28) Connection timed out after 5000 milliseconds` No issue by HTTP connection
Do you think its design is suitable for implementing an OpenGL/Vulkan backend? If so I might do that. For Lamdu we've went with a graphics-drawingcombinator branch modified to use FreetypeGL for font rendering, and there we do want smooth and fast rendering. In part of streamlining and separating our UI library I'm considering switching to Rasterific if I could make it work well.
You are right, it seems that in the Cartesian product case your "machinery" has the same overall behavior as a properly implemented monoid comprehension. For example, looking at: q f xs ys = runMonoidComp [ f x y | x &lt;- fromFoldable xs, y &lt;- fromFoldable ys ] it means essentially: q f xs ys = evalCont $ (cont $ flip foldMap xs) &gt;&gt;= \x -&gt; (cont $ flip foldMap ys) &gt;&gt;= \y -&gt; return $ f x y which itself means: q f xs ys = flip foldMap xs $ \x -&gt; flip foldMap ys $ \y -&gt; f x y = foldMap (\x -&gt; foldMap (\y -&gt; f x y) ys) xs = { f x y | x &lt;- xs, y &lt;- ys } ...the corresponding monoid comprehension. So it's basically a very fancy way of composing calls to `foldMap` :\^) The Alternative instance seems to only be used for guards. It's the one that attracted my attention first as it makes it look like you are nesting an arbitrary number of continuations at runtime, because of: MonoidComp a &lt;|&gt; MonoidComp b = MonoidComp $ cont $ \f -&gt; runCont a f &lt;&gt; runCont b f So I'm not sure if the equivalence with monoid comprehension holds in the presence of guards, due to this implementation.
Why does it matter what Debian stable ships? I'd expect anyone using Haskell at an industrial level to be a little less lazy than to use the compiler which shipped with their distro. Where I work, we pin the versions of everything being used with Nix, so that all developers on the team have at least a reasonable expectation that the bugs in production are going to be the same as the bugs which occur on their machine. It also means that if we really need to fix a bug in GHC or GHCJS, we don't have to wait for a complete development cycle for our patches to get in upstream: any patches can be applied in our Nix derivation for the compiler, and the resulting compilers get placed in our company's nix cache so that they don't need to be compiled separately on each dev's machine.
If interested in React / Haskell / Elm, you might find [miso](https://github.com/dmjio/miso interesting as well). It
If you are teaching a class and you don't have control over what's installed on the machines in the lab (except for OS packages you can ask the IT department to install globally), it's a good idea to write code that can be compiled using old versions of ghc. In Agda we regularly have lecturers asking us to keep supporting older versions of ghc because that is what they are stuck with at their institution.
C++ isn't garbage collected
You're absolutely right. Had to check, but I assumed it was integrated with the language since I worked in a project that had one implemented. My mistake
&gt; I've been writing Haskell for 10 years and doing so professionally for 5 years. I read &gt; /r/Haskell &gt; reasonably regularly but somehow I missed this survey. Me too. Had no idea this survey was happening, and I'm reasonably watchful of the Haskell community (I just don't use Twitter). This is why I generally take FPCo surveys with a boulder of salt when it comes to Stack-related topics; they have a pretty clear selection bias, since it's far more likely for Stack enthusiasts to be reached.
Thanks a lot! It does clear things up! I somehow missed that we were talking about a third category, **Graph**, whose objects are graphs and morphisms are morphisms of graphs. So, the correspondence that you mentioned is given by a functor from **GrIn**^op to **Graph** (as you nicely detailed in the previous answer). I'm now having trouble understanding the following statement: &gt; [T]he category of contravariant functors from **C** to **Set** (1) includes a copy of **C** [...] How can the category of functors (whose objects are _functors_, from **C**^op to **Set**) include a copy of **C** (which is a _category_)? (I'm also failing to see the relation of this statement with the previous fact – the functor from **GrIn** to **Graph**.)
&gt; How credible is that number that 80% of Haskell users were Stack users? It lines up with the state of Haskell survey that I ran through Haskell Weekly last year. One of the questions was: "What is your preferred build tool?" Of the 1,167 responses, 849 (73%) selected Stack. https://taylor.fausak.me/2017/11/15/2017-state-of-haskell-survey-results/#question-23
&gt; EDIT: the problem is fixed by following the advice and adding `FlexibleContexts`. That's only because you're punting the responsibility of finding the instance to the caller of `test`. As the error said, it's inferring this type: test :: forall c (t1 :: * -&gt; *) (t2 :: * -&gt; *) t3 t4. (Monoid c, Foldable t1, Foldable t2, Alternative (MonoidComp c)) =&gt; t1 t3 -&gt; t2 t4 -&gt; (t3 -&gt; t4 -&gt; c) -&gt; (t3 -&gt; t4 -&gt; Bool) -&gt; c Notice the `Alternative (MonoidComp c)` constraint. You're still using `Alternative`; you're just asking someone else to provide the instance. `FlexibleContexts` allows you to ask for a constraint on a specific type, whereas normally constraints can only be used on type variables. Anyway, here's how a MonadComprehension with a guard could be translated into `do` notation: test xs ys g f = runMonoidComp [ g x y | x &lt;- fromFoldable xs, y &lt;- fromFoldable ys, f x y ] test xs ys g f = runMonoidComp $ fromFoldable xs &gt;&gt;= \x -&gt; fromFoldable ys &gt;&gt;= \y -&gt; (if f x y then pure () else empty) &gt;&gt; return (g x y) So `Alternative` is only used in the desugarring of guards to say `else empty`. In ordinary bindings in a comprehension, only `Monad` is used, so a comprehension without guards will not require the `Alternative` instance.
That's great to hear, thanks for the notice.
Most people in the industry seem to favor \`stack\` over \`cabal\`, let me be a little controversial and say that I majorly dislike the fact that there is a \`.cabal\` file in my project when I am using \`stack\`, or the fact that cabal needs to even be installed. Just to give some context, you will not find a \`.sbt\`, \`.ivy\` or \`.ant\` file in a \`gradle\` project, so why should there be cabal file in a stack project?
The Purescript community is developing 'psc-package' on the same concept of having package sets
&gt;This is the situation and I don't see a way to unify everyone under one tool any time soon. And stuff like this is what makes Haskell a hard sell in enterprises. Even \`Rust\` has made installation trivial with their \`rust-up\` and \`cargo\` tools, I just cannot stress how much important it is to have installation and tooling to be absolutely trivial. If it is going to take my co-workers 2-3 hours of unnecessary time to figure out how to setup a hello world project then the effort is lost right there. &amp;#x200B;
I just want to chime-in and add that tooling and IDE support are the biggest road-blockers to introducing Haskell to co-workers. There are enough tutorials, books and guides. What we need is really trivial method of installing and trying out Haskell. If it is going to take someone more than 15 minutes to set-up Haskell on their PC and execute hello-world then no bueno. &amp;#x200B; Then there is IDE support, it goes not help when people keep recommending vim or emacs to people who are only used to pretty IDEs like IntelliJ, regardless of how superior your vim and emacs setup might be, nobody is going to take that effort and we need to accept that.
I assume you got these results by turning optimizations on (and thus enabling rewrite rules)? I think there is value in having solutions that produce the right algorithm right off the bat without having to rely on aggressive compiler optimizations to achieve the right complexity. It will be faster to compile, won't make quick debug builds without optimizations turned on unnecessarily slow, and will be less brittle – AFAIK list fusion works beautifully for simple examples like this, but easily breaks down as soon as you start modularizing code or having non-trivial patterns (such as non-linear usage of variables).
&gt; I assume you got these results by turning optimizations on (and thus enabling rewrite rules)? No. I used `-O0` and observed no output from `-ddump-rule-rewrites`. This should be the standard emission. And it's not exactly a complicated thing. It's just "Each binding introduces a loop function; each guard adds a case statement to the most recent loop."
I think GPUs are dead if we have CPUs with many cores (e.g. 120, 1000, 10 000). Rasterific is primarily slowed down because there doesn't exist a high performance sorting library in Haskell. You can write a fast parallel sorting library which would help the speed of Rasterific and help Haskell in general as a language, as other libraries would also benefit. You can write this fast sorting library based on Vulkan/GPUs or even just bind Julia sorting to Haskell via the C ffi. This would be a good idea. If you want to learn, check out the papers of the STEPs project: http://www.vpri.org/writings.php 
A super, super trivial method of trying haskell is the online repl: [https://repl.it/languages/haskell](https://repl.it/languages/haskell) But yeah a proper IDE would be good. 
I think GHC will throw warnings about the `Data.Semigroup` import being redundant. I *believe* that's why [the proposal recommended](https://prime.haskell.org/wiki/Libraries/Proposals/SemigroupMonoid#Writingcompatiblecode) `import Data.Semigroup as Sem` and `instance Sem.Semigroup ...`
Interesting! So it looks like GHC has a special optimizing scheme for desugaring list comprehension. Looking at the Haskell 2010 language definition [section on list comprehension](https://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-420003.11), indeed it does not mandate a particular desugaring – it only requires that the desugaring be equivalent to the one based on `concatMap`. Smart move! OTOH, as soon as we use monad comprehension it does have to go back to the simpler scheme, and thus to [the worse performance](https://www.reddit.com/r/haskell/comments/oolyt/why_are_guards_in_the_list_comprehension_faster/c3iu3yu/?context=1). I tried it and it uses `&gt;&gt;=` as expected. 
A profunctor optic is a profunctor optoac in the category of endoprofunctors.
Ir does throw the warning, but I can live with that :)
&gt; &gt; This is the situation and I don't see a way to unify everyone under one tool any time soon. &gt; And stuff like this is what makes Haskell a hard sell in enterprises. Right. That's why no one uses Java in enterprise: the constant struggle of whether to use ant or maven or gradle.
There doesn't need to be a cabal file. You can use a `package.yaml` file. And `cabal` (the binary) doesn't need to be installed, though `Cabal` the library does. This is because `stack` builds off of `Cabal`. If you're interested in something that reimplements the cabal build logic without depending on the library, take a look at [pier](http://hackage.haskell.org/package/pier); it's early, but it's promising, and the places it falls short suggest reasons to depend on `Cabal` and areas of potential improvement.
This is why you hate TypeApplications, right? \&lt;/troll&gt; More seriously, syntax stealing is a real concern, and good designs try to minimize it, but * When have you _ever_ wanted this syntax for any other reason? * Compared to quasiquotes, this is lighter weight and doesn't break all the tooling.
&gt; There are enough tutorials, books and guides. I feel like this is a comment we wouldn't have heard even a year ago. Could it be true? Have we turned the corner on community-supplied pedagogical materials? Doesn't this comment still feature prominently in user surveys?
When you use gradle you don't need maven or ant installed. You don't even need gradle installed, most projects use a gradle wrapper. 
It would be an absolute disaster for libraries to stop specifying their dependencies in a way that is compatible with both formats. Please don't push for this. You will be creating real problems that are much harder to solve with the current fragmented community. It's worth a lot of sacrifice to keep everyone at least nominally on the same page here. If you mean your binary project that you only use internally, then okay, sure. Go for it. Someone else already answered with a suggestion of how.
Same story here. This is the first I'm hearing about the survey, probably because I don't pay much attention to FPCo/Stack stuff at all. Social groups tend to be internally consistent and self-selecting. If you look at my team and other Haskellers I regularly talk to, you'd think the entire Haskell world was all aboard the Nix train...
https://ro-che.info/ccc/26
Starting with: fibs = scanl (+) 0 (1 : fibs) Pretty sure `fibs` is infinite. Get an arbitrary non-negative integer `k`, and take `(!! k)` on both sides of the equation fibs !! k = scanl (+) 0 (1 : fibs) !! k Now let's focus on the right hand side. We know that `scanl` performs a 'prefix sum', including the initial element ghci&gt; scanl (+) 0 [12, 300, 51000, 7800000] [0,12,312,51312,7851312] If we consider adjacent elements, after carefully avoiding any off-by-one errors, we can see that for any non-negative in-bounds `i`: scanl f z xs !! (i + 1) = f (scanl f z xs !! i) (xs !! i) So if `k &gt;= 2` scanl (+) 0 (1 : fibs) !! k = (scanl (+) 0 (1 : fibs) !! (k - 1)) + ((1 : fibs) !! (k - 1)) = (scanl (+) 0 (1 : fibs) !! (k - 1)) + (fibs !! (k - 2)) = (fibs !! (k - 1)) + (fibs !! (k - 2)) That is, fibs !! k = (fibs !! (k - 1)) + (fibs !! (k - 2)) I hope this proof at least partially fixes the 'but why?' problem.
&gt;How credible is that number that 80% of Haskell users were Stack users? I think it's very possible that 80% of Haskell users are Stack users, if you count everyone on equal terms regardless of how long or heavily they use Haskell. I greatly doubt that holds true if you consider total usage instead of distinct people. Here's why. There's been a deliberate effort by the community around stack for a long time to advertise their tool as the only reasonable one to start with -- to the point that they once tried to create a rogue Haskell homepage that flat-out told people to use their tool as the only option. While the rogue homepage is mostly dead, they have continued to reach people. I've recently been surprised a few times talking to newcomers to Haskell who think of "the Haskell community" as being comprised of spaces that I didn't even know existed, and they always seem to be dominated with dogmatic and even aggressive advocacy for Stack, almost like it's a mark of the community. So there are a lot of people trying out Haskell, and ending up with stack because they are insistently led to believe that it's just what Haskell uses, and only fringe crazy people do anything else. This is in addition to some real advantages that Stack has, not instead of. We could definitely benefit from some attention to the beginner journey for other tools, as well.
Have you tried [https://github.com/haskell/haskell-ide-engine](https://github.com/haskell/haskell-ide-engine)
Isn't it just: 1. Install stack 2. `stack setup` 3. `stack new hello-world` ?
While this is nice, it can be done by rewriting the pure computation into a monadic computation that works for any monad, then selecting an interruptable monad. Often, rewriting to use to `Par` monad is already required to get good parallelism, so this really isn't the onus some claim it is. Using this approach also prevents async exceptions from polluting / infecting / affecting the rest of the language.
I tried, but installing it to support all GHC versions took a painfully long time and it still didn't work correctly. Also still no project wide references and code completion is no where as smart as IntelliJ's intellisense. I just feel you cannot replace an IDE with an editor, it's just a pipe dream. 
I'm not a Haskell programmer - in fact I don't think I ever heard of Haskell before today. I'm trying to convert the following into C# but the syntax is very confusing to me: rectTriGrid :: Int -&gt; Int -&gt; RectTriGrid rectTriGrid r c = RectTriGrid (r,c) [(x,y) | y &lt;- [0..2*r-1], x &lt;- [xMin y .. xMax c y], even (x+y)] where xMin y = if even y then w else w+1 where w = -2*((y+1) `div` 4) xMax c2 y = xMin y + 2*(c2-1) Could one of you lovely people help me make sense of this, or give me an idea of how it would look in a more ECMA style language? ( [Source from here](https://hackage.haskell.org/package/grid-7.8.9/docs/Math-Geometry-Grid-TriangularInternal.html#t:RectTriGrid) )
Out of curiosity, what is the size of a reflex JS output?
Any survey hosted by FPComplete is biased towards users of stack, for reasons that should be self evident. I (a cabal user, btw; and not for want of trying stack) neither knew that there was a survey, nor would have cared to participate in one hosted by FPComplete. Yesod is a mess, stack has failed me whenever I needed anything off their particular beaten track, and ResourceT is only useful as a trigger warning. Haskell doesn't need better adoption; it's a research language. Consistency would be contrary to success.
I browse /r/haskell all the time *and* follow FPco on Twitter, and I wasn't aware of this survey. A good way of getting the word out for next time would be to get in touch with a mod of this subreddit, who can sticky the post.
Diversity is a good thing. One could argue that `stack` has accomplished a very important goal; it gave `cabal` competition, which forced that tool to improve much faster than it was.
I think it comes from the file system hierarchy from freedesktop.org: [https://www.freedesktop.org/software/systemd/man/file-hierarchy.html](https://www.freedesktop.org/software/systemd/man/file-hierarchy.html)
I strongly disagree with the grandparent's comment that there are enough books and guides. Documentation in general is much weaker than other languages, and there are still no intermediate Haskell books released (though such books are in progress)
Yes, these two surveys with a similar selection bias line up with each other. Let me quote u/ElvishJerricco from https://www.reddit.com/r/haskell/comments/8tc8pr/fp_complete_launches_new_blockchain_auditing &gt; We've pointed this out to you before. Both of those surveys show severe selection bias. A poll by the Stackage devs, and by an independent Stackage contributor who has alienated contact with non Stackage supporters on the platform he used to advertise the poll is about as biased as it gets. &gt; Any survey by a party that is partial to a particular view is going to have selection bias. In this case, it's pretty extreme. It should not be surprising to think that fp complete has much better outreach to Stackage users than to non Stackage users.
That sounds quite fuzzy.
I've been somewhat successful with a running IHaskell on a server: no installation needed, and some interactivity with immediate feedback, possibility to show pictures. Would recommend to a friend.
Add a sufficiently broad dependency on the `semigroups` package and you're good to go back to 7.4, and IIRC there are some plans that will work back to 7.0. Is that sufficient?
\&gt; Haskell doesn't need better adoption; it's a research language. Disagree for sure, Haskell is by far my favorite language out there for industry/practical stuff, I would love to see more adoption to improve library support, documentation, tooling, ability to hire people, and so on.
I'm getting a little tired of Haskell Weekly's survey results being sniped like this. I am very eager to remove any potential selection bias in the Haskell Weekly survey. I think you are already aware of that because of [this comment](https://np.reddit.com/r/haskell/comments/8uw9hw/psa_cabal_breaks_with_yaml0831_on_ghc_710_and/e1lp9na/?context=3). I developed last year's Haskell Weekly survey in the open. Nobody brought up bias as a potential problem in [this issue](https://github.com/haskellweekly/haskellweekly.github.io/issues/102) nor anywhere that I saw. In addition, nobody brought up bias in [the announcement thread](https://np.reddit.com/r/haskell/comments/7a3fad/first_annual_haskell_users_survey/) either. I am doing my best to continue last year's tradition and develop this year's survey in the open. If you are concerned about bias in the upcoming 2018 state of Haskell survey, please make your voice heard in [this issue](https://github.com/haskellweekly/haskellweekly.github.io/issues/206)! I want Haskell Weekly's survey to be a valuable resource for the entire community, not just "Stack people" or "Cabal people" or "Nix people". 
&gt; I can't floss my teeth, man. I can't get into the flossing thing. People who smoke cigarettes, they say "Man, you don't know how hard it is to quit smoking." Yes I do. It's as hard as it is to START flossing. – Mitch Hedberg [Source: Mitch All Together](http://cynicsaccelerants.tumblr.com)
It's big. Really big. My app is somewhere around 8MB, although the Closure Compiler can really help with that.
Doesn't everyone just use maven?
Thanks good to know
That sounds really bad, although it's only useful to compare minified code to other minified code, how large is it fully minified?
While funny I can't help but feel that this has nothing to do with my comment.
I can see how this would be pretty confusing if you're looking at the language for the first time. `where` indicates that the subsequent declarations hold inside the function above. `where xMin y = blah blah` means "there is a function xMin, with argument y, that does blah blah". `xMin` and `xMax` are declared right there and used in rectTriGrid. `where w = ` defines w inside xMin, etc.
~2.5MB I think. 
V8 outputs runtime stats in Chrome these days, those include parse&amp;compile times. From my experience JS of this size could take a second to compile, I'd be interested to see real results from GHCJS.
Well I don't want to come into your sub and insult your language, but this is some crazy puzzle speak right here! So, *for example*, if xMin were a javascript function, it would look something like this: function xMin (y, w) { if (y % 2 == 0) return w; else return w+1; } But w is actually declared in the next line, and then the following (final) line is the xMax function which takes two arguments, right?
Shit I thought we could get it down to &lt;1MB already.
It might be possibly to bring that down a bit with some compile flags - I've made a PR that adds them [here](https://github.com/jappeace/awesome-project-name/pull/2) - although minification and compression are always important / compound the wins that you get from adding those flags.
It isn’t light for me 
Interesting post, thanks! Do you use the same workflow with bigger projects (e.g. several files, using stack tool) or something entirely different? 
&gt; and also for some reason seems to link faster. That's pretty much the only thing I don't like about stack. It forces you to use the default linker and linking with that takes a lot of time, you can see in small project that like 3/4 of build time is waiting for the linker. With cabal you can use the LLVM linker which is often literally 10x faster. 
For quick prototyping on a single package I do: $ cabal2nix --shell . &gt; shell.nix $ nix-shell For complex projects I use: https://github.com/Gabriel439/haskell-nix You might also be interested in this live demo of me bootstrapping a Haskell server from scratch: https://youtu.be/NQJVNvxgDqg
Function composition. I have this function add3 :: (Monad m, Num a) =&gt; a -&gt; m a add3 = return . (+3) And say I make this one now f n l = ((++) . add3) n l I don't understand how the function composition acts like this: f n l = (++) (add3 n) l Instead of this way, f n l = (++) (add3 n l) which should give an error.
Excellent question, Mr. Troll. I tolerate `-XTypeApplications` but don't like this. Why? I think it's because of the intuition that symbols matter in a way parens usually don't. This intuition goes back through a lot of languages: sigils in perl, `*` and `&amp;` in C, and so on. Also visible type applications echoes the underlying core language, IIRC. The only time I can think of `x` and `(x)` meaning wildly different things is in a lisp-1 where `x` is a function and `(x)` is calling it with no args. But that's in a language where everyone knows and agrees that parens are special.
Shameless plug https://gist.github.com/androidfred/a2bef54310c847f263343c529d32acd8
That's just the format it parses, IIUC, since it's a standard file format for Haskell packages. I would be surprised if hpack support required changes beyond the parser and frontend.
You can go back to the definition of `(.)`: `f . g = \x -&gt; f (g x)`. So in this case, you have ((++) . add3) n l = (\x -&gt; (++) (add3 x)) n l = ((++) (add3 n)) l = (++) (add3 n) l = add3 n ++ l
On the other hand - both `npm` and `yarn` use `package.json` as the manifest source of truth, and add their own files on top of it (`package-lock.json` and `yarn.lock` respectively). The `cabal`/`cabal-install`/`Cabal` thing is really confusing, I'll grant you that. If everything were designed from scratch, I think we'd have something like `manifest.toml` instead of the `.cabal` file, and `Cabal` and `cabal-install` might not share a name.
Tip: \/u\/sudomakesanwich will be notified because you are replying directly to the OP.
I did not know about this survey and even if I have known would not have participated. This is not because of any problem with stack or FPcomplete but I generally do not participate in surveys of this kind. I have always felt that the marginal utility of it is rather small. Whenever things get complicated as is the case with a build tool for a language like haskell, I believe there should be opinionated attempts to push promising ideas some of which conflict with each other. A survey of this kind hardly helps in this case. If this can be done with little conflict (some amount of it is unavoidable) it is better. I was using stack for most of the builds but have moved back to cabal once the new-build thing happened. With the freeze file in place, a global freeze is not very important as long as the dependencies are fairly standard. With backpack and multi-component packages (a work in progress), for me, cabal has a lead now, if at all you want to compare the them. 
thanks!
You can't just "remove" selection bias like that. You have to actively avoid publicizing the survey on any you-centric social media like Twitter, but your followers will do that for you and generate the same effect. And even if they don't, merely seeing your name attached to the survey makes those you've alienated far less likely to respond. I hate to say it, but you're just not in a position to post a survey without selection bias w.r.t. to the Stackage ecosystem, because you've taken such a strongly partisan position. Strongly partisan people beget selection bias.
I didn't see this year's survey, but I saw last year's survey and avoided it entirely because it was strongly associated with someone with a strong partisan position, and I wanted no part in it.
This is odd to someone like me who codes 100% in the terminal. Haskell’s tooling is the one of the few things that keep me coming back to it. Very few languages can be compiled and interpreted, on top of a pretty stellar REPL. The language is very hard to grok, but the tooling is so good that it motivates me to write more Haskell. I know you specifically mentioned people who use standalone editors and IDEs but I felt compelled to praise Haskell in this one area.
Great. I was going to ask for some implementation of the concept and some examples. but I found it here. http://hackage.haskell.org/package/CC-delcont-0.2.1.0/docs/Control-Monad-CC.html The examples are at the end of the page.
The amount compression helps surprised me somewhat. I know it's ascii subsets and all but that's still over 5:1 without the flags. Could some sort of naive lto deduplication for repeated blocks help?
Haskell only stores the result if you've given it a name, like you did in the first version. The two versions you've given will compute the same result, but the second one will perhaps take more time while the first one will perhaps take more space. Now, because the two versions will compute the same result, GHC is free to transform the second version into the first one (Common Subexpression Elimination) and might actually do so at some optimization level.
Thank you! I understood it perfectly.
Perhaps I'm going to show my ignorance here, but I feel like this blog post is getting into the weeds a bit and doesn't really tell me anything about what the state of cross compiling haskell in 2018 is. I guess if I'm working on whatever this guy's project is this would be valuable. But I don't know what it is, he doesn't actually even say in the blog post, I guess I'm supposed to just know who he is and therefore know what his project is. As a casual who is moderately interested in using haskell for Linux + iOS + Android (vs Swift) I'm curious about a few things: 1. Is it possible to do this? I thought it was, there seems to be some old instructions on the Haskell Wiki about cross compiling to iOS anyway. 2. If it isn't, what do these updates mean for me? 3. Again if it's isn't, when can we reasonably expect to be able to cross compile? Anyway it would be cool to see a blog post meant for people who don't really give a damn about how `cmm` is built and just want to know if it works and how well it works :P
1.3M for a runtime is still a bit on the large side. However it's a small price for not having to deal with JS.
Yes however modern front-end packaging tools, mainly webpack/npm are unnecessarily complex. And elm doesn't rely on them. 
That is the size of the whole thing - the runtime, the app, and all of the dependencies. There is a good chunk of it that does come from the runtime, which means that the size of this output has grown pretty slowly as the app and the dependency list has grown. It's good enough for me, at least until WebGHC becomes a thing :) 
Was it [this](https://www.cs.utexas.edu/~cannata/cs345/Class%20Notes/10%20Haskell%20Programmer%20Evolution.html) ?
Does it happen to be The Evolution of a Haskell Programmer? https://www.willamette.edu/~fruehr/haskell/evolution.html
WOW Thank you very much!
WOW Thank you very much!
Well that is quite reasonable and valid criticism. So let's try to elaborate a bit. Is it possible to cross compile to iOS and Android? Yes, with some limits. http://hackage.mobilehaskell.org contains some slightly older prebuilt GHCs (8.4); and there are some articles on https://medium.com/@zw3rk on how to actually do the setup, so I won't repeat that here. What do these updates mean and why should you care. To get dead strip support for mach-o on iOS you want the llvm-ng backend. To improve the performance of that backend being able to serialize/deserialize `cmm` helps a lot wrt. to profiling. The minimal ghc topic is slightly related to the idea of making GHC a multi-target compiler (`-target`). The assumption is that you will still need to distribute the base libraries with such a compiler for all supported platforms, and as such you are interested in cutting them down to the bare minimum; with the work that went into `hadrian` to produce relocatable GHCs, this should eventually allow to build a ghc distribution with support for multiple targets that can be used by simply extracting a tarball. I'll take this as a vote for more practical posts. NB: If the 8.6.1 release would not have suffered from such a severe bug, I would have tried to provide prebuilt cross compilation binaries for 8.6.1. However as it stands I'll wait for 8.6.2. Building them currently requires about a calm weekend.
Lessons? Like putting messages similar to "Hackage documentation generation is not reliable. For up to date documentation, please see: http://www.stackage.org/xxxx" all over the hackage db?
This has actually progressed quite far. There will probably be some more in-depth information on this soon.
Doesn't this attitude, too, reinforce partisanship? It would be good to take user surveys cast by parties you don't agree with, precisely to deflate those biases you are so concerned with.
Well, in my experience if it doesn't builds (or stack thinks that it won't build) it will be simply missing, at the same time simply hardcoding the same packet from hackage just works.
See [reflex-platform](https://github.com/reflex-frp/reflex-platform). It puts together the GHC work done by Angerman and others with a lot of Nix work by folks at Obsidian Systems (disclaimer: my employer) and others and provides a platform for cross compiling Haskell apps to web, Android, iOS, and desktop. For something even more batteries included see [Obelisk](https://github.com/obsidiansystems/obelisk).
&gt; Thanks a lot! It does clear things up! I somehow missed that we were talking about a third category, Graph, whose objects are graphs and morphisms are morphisms of graphs. Well, not really a third category, since you can *define* Graph to be the category of functors from GrIn to Set. &gt;I'm now having trouble understanding the following statement: The category of contravariant functors from C to Set (1) includes a copy of C [...] &gt; How can the category of functors (whose objects are functors, from C^op to Set) include a copy of C (which is a category)? This is probably related to the Yoneda embedding, as you already alluded. Not just related: that inclusion of a copy of C *is* the Yoneda embedding. In general it works as follows: given any object x of C you can form the functor y[x] : C^op → Set defined by y[x](c) := C(c,x) ---where C(c,x) denotes the set of morphisms from c to x in C. The Yoneda *Lemma* says that given any functor F : C^op → Set, the set of natural transformations from y[x] to F is in bijection with F(x): Nat(y[x], F) = F(x). In particular natural transformations between different y[x]'s are just morphisms in C: Nat(y[x], y[t]) = y[t](x) = C(x,t). So then the Yoneda *embedding* is the functor C → Fun(C^op , Set) given by sending x to y[x]. &gt; (I'm also failing to see the relation of the quoted statement with the previous fact – the functor from GrIn^op to Graph.) That functor is precisely the Yoneda embedding! Remember that Graph = Fun(GrIn, Set). The graph I called _V_, with only one vertex and no edges, is y[_Vertices_ ^op]; and the edge _E_ is y[_Edges_ ^op]. Does that make sense?
Yes, that may happen because the packages in the resolver that are incompatible with the one you want are not being included in your build, or the package maintainer has not actually put it up on Stackage.
Cool, thanks!
Look at `accelerate`!
Why would anything leave memory?
 &gt; I never understood why people say Rust has anything to do with Haskell. If you look at the software architecture level, then there're quite a few similarities. If you're comfortable writing Haskell programs, then you will fight a lot less with the borrow checker in Rust. &gt; Just not that it's a bridge between C++ and Haskell. I think it really is. I've a strong background in C++ and have written several Haskell programs and learning Rust was pretty much easy going for me, because I could take my knowledge of C++ and Haskell. 
Just my 2 cents, but no new-build support is not what I would expect from any IDE-like tooling deserving this title.
\[These slides\]([http://penrose.ink/Penrose\_DSLDI\_slides.pdf](http://penrose.ink/Penrose_DSLDI_slides.pdf)) explain a lot of ideas behind this DSL.
Thanks al lot for your answer! I have enhanced EADTs accordingly (and the documentation: https://docs.haskus.org/eadt.html). 1) Now with Template-Haskell and haskus.Utils.EADT.TH we can use ``eadtPat 'ConsF "Cons"`` to generate pattern synonyms 2) I have added the equivalent of your ``varFalg``: instance (AlgEADT (MapList a a' r) r) =&gt; MapList a a' r (VariantF r) where fmapList' f = algVariantF @(MapList a a' r) (fmapList' f) 3) I have added the equivalent of ``caseonF`` (which already existed for Variant, but not for VariantF/EADT) for safe pattern-matching (doc: https://docs.haskus.org/eadt/safe_pattern_matching.html): showCont l = variantFToCont l &gt;::&gt; ( \(ConsF a r) -&gt; show a ++ " : " ++ r -- no explicit recursion , \NilF -&gt; "Nil" ) 4) I have added the equivalent of ``multiTrialF`` which indeed was missing: alg x = case splitVariantF @'[EvenF Int, OddF Int] x of Left v -&gt; variantFToCont v &gt;::&gt; ( \(EvenF a l) -&gt; "Even : " ++ l , \(OddF a l) -&gt; "Odd : " ++ l ) Right leftovers -&gt; "something else" 5) I'll try to find some time to improve performances
I think it does make sense! I've checked that the Yoneda embeddings of the two objects in GrIn^op and they do correspond to two graphs, _V_ and _E_. The Yoneda embedding of _Vertices_^op gives the graph with the following vertices and edges: - y[_Vertices_^op] (_Vertices_) = GrIn^op (_Vertices_, _Vertices_) = {id} - y[_Vertices_^op] (_Edges_) = GrIn^op (_Edges_, _Vertices_) = ∅ ... which is indeed the graph _V_, with a single vertex and no edges. The Yoneda embedding of _Edges_^op gives the graph with the following vertices and edges: - y[_Edges_^op] (_Vertices_) = GrIn^op (_Vertices_, _Edges_) = {s^op, t^op} - y[_Edges_^op] (_Edges_) = GrIn^op (_Edges_, _Edges_) = {id} ... which is indeed the graph _E_, with two vertices and a single edge. Thank you very much for the answers and for your patience! I hope one day I'll understand your second point as well ("any presheaf is canonically a colimit of representable presheaves"), but that seems like a long adventure ahead :-)
Thanks! This is what I ended up with {-# LANGUAGE RecordWildCards #-} convertPrivateKey :: Other.PrivateKey -&gt; PrivateKey convertPrivateKey Other.PrivateKey{..} = PrivateKey {private_pub = convertPrivatePub private_pub, ..} convertPrivatePub :: Other.PublicKey -&gt; PublicKey convertPrivatePub Other.PublicKey{..} = PublicKey{..}
For me the first implementation makes sense so it seems like it is a skill you can acquire. It's a shift from thinking about algorithms operationally (as a series of steps) to thinking about them structurally. I won't try to explain what it does since I'm guessing the books does a better job. That being said, using direct recursion becomes very rare in real code. Usually you construct your algorithm from smaller, already written algorithms. For example the way I'd do splits is `splits l = zip (inits l) (tails l)` where `inits` and `tails` come from `Data.List`.
Oh that's awesome, didn't know about those functions. What do you think of using helper functions? Just tried to whip one up, here's what I got: [splits-helper](https://i.imgur.com/IqpZiVh.jpg)
I really don't like maven's convention over configuration model. But, yeah, we've drifted toward maven for most things.
Awesome news!!!
And you can still break it like the haskell one: int main() { MyResource my_resource = create_it(); my_resource.~my_resource(); my_resource.use(); } I mean its bad code, but so is the haskell one; the compiler does not stop here.
It's shameless self-promotion but I wrote some similar posts a while ago :) \[folds\]([https://jozefg.bitbucket.io/posts/2014-12-27-folds.html](https://jozefg.bitbucket.io/posts/2014-12-27-folds.html)) \[extensible-effects\]([https://jozefg.bitbucket.io/posts/2014-07-15-reading-extensible-effects.html](https://jozefg.bitbucket.io/posts/2014-07-15-reading-extensible-effects.html)) \[logict\]([https://jozefg.bitbucket.io/posts/2014-07-10-reading-logict.html](https://jozefg.bitbucket.io/posts/2014-07-10-reading-logict.html)) \[concurrent-supply\]([https://jozefg.bitbucket.io/posts/2014-11-26-conc-supply.html](https://jozefg.bitbucket.io/posts/2014-11-26-conc-supply.html)) \[operational\]([https://jozefg.bitbucket.io/posts/2014-12-25-operational.html](https://jozefg.bitbucket.io/posts/2014-12-25-operational.html)) \[pipes\]([https://jozefg.bitbucket.io/posts/2015-06-01-pipes.html](https://jozefg.bitbucket.io/posts/2015-06-01-pipes.html)).
You just made my day ! &amp;#x200B; I've looked for this article a thousand times without even thinking to ask /r/haskell \- And I only remembered the back-and-forth : Junior and Professor do it merely the same way, and Graduate, Post-doc and other Church-encoding inclined are utterly incomprehensible.
That's sweet, thank you
Helper functions are all around, as it's really hard to write tail-recursive functions in most cases without a helper. They usually are named `go` in a `where` block. Your new solution might also work with `map`, although reversed: splits' n xs = map (`splitAt` xs) [1 .. n] Backticks allow using a function as infix (like an operator), and placing an infix function before an argument partially applies it at the second position: x `f` y = f x y (`op` y) = \x -&gt; x `op` y (x `op`) = \y -&gt; x `op` y --- By the way, please consider pasting code as text by indenting with 4 spaces instead of an image.
Would something a la `HVectElim` help? It's at least a start for trying to consume something in a way that is parameterized by a type level list. ```haskell type family HVectElim (ts :: [*]) (a :: *) :: * where HVectElim '[] a = a HVectElim (t ': ts) a = t -&gt; HVectElim ts a ```
To be properly pedantic, they are only "not" the free monoid in the presence of bottom (ie, a lazy language). Has to do with Haskell types being domains, so there's an absorption that happens that screws up the laws. Of course, with a bit of squinting and loose reasoning, that doesn't mater...
One thing that might help is trying to reason through a recursive definition *definitionally* instead of *operationally* like you describe having difficulty with. For example, I would read `splits` aloud in English like: -- There are no ways to split an empty list, -- or a list of one element. splits [] = [] -- (1) splits [_] = [] -- (2) -- The splits of a list of at least one element splits (x : xs) = -- (3) -- consist of a split between -- the first element and the rest, ([x], xs) -- followed by : -- all splits of the rest of the list, but with -- the first element added back onto the left side. [ (x : ls, rs) | (ls, rs) &lt;- split xs ] Or reason through it in terms of substitutions on a short example: splits [1, 2, 3] = case (3) ([1], [2, 3]) : [(1 : ls, rs) | (ls, rs) &lt;- split [2, 3]] split [2, 3] = case (3) ([2], [3]) : [(2 : ls, rs) | (ls, rs) &lt;- split [3]] split [3] = case (2) [] split [2, 3] = replace split [3] with [] ([2], [3]) : [(2 : ls, rs) | (ls, rs) &lt;- []] = replace [... | _ &lt;- []] with [] ([2], [3]) : [] = replace x : [] with [x] [([2], [3])] splits [1, 2, 3] = replace splits [2, 3] with [([2], [3])] ([1], [2, 3]) : [(1 : ls, rs) | (ls, rs) &lt;- [([2], [3])]] = replace [f x | x &lt;- [y]] with [f y] ([1], [2, 3]) : [(1 : [2], [3])] = replace 1 : [2] with [1, 2] ([1], [2, 3]) : [([1, 2], [3])] = replace x : [y] with [x, y] [([1], [2, 3]), ([1, 2], [3])] But the reality is that you won’t be writing direct recursive functions most of the time in real Haskell code, unless you have an unusual recursion pattern or you need particular performance characteristics. You need to learn how to do it as a matter of understanding how to structure code, but most patterns in business logic can be covered by `map`, `zipWith`, `filter`, `foldr`/`foldl'`/`foldMap`, `traverse`, `takeWhile`/`dropWhile`, `scanr`/`scanl`, `mapAccumL`/`mapAccumR`, and so on. This function could be implemented much more simply from composable parts as ``map (`splitAt` xs) [1 .. length xs - 1]``, or `zip (inits xs) (tails xs)` (although that includes empty splits). When trying to solve a problem like this, you should break it down into simpler parts and solve those subproblems first; see if a library function already exists for that part, or implement it yourself as a learning exercise. 
You happened to pick a *slightly* more complicated recursion function, as it also contains list comprehension in there as well (the `[ list values | method of generating list values ]` part) I think the hardest part about recursive functions is that when they are used you often aren't operating on individual elements of a list on their own, but rather working on future values that have not been processed yet. When you want to access these future values in iterative code, you directly index them (or in your example provide a starting point for a range). Access to those future values are based on your current position. Recursion accesses those same future values based on evaluation of the code you're currently executing, just at a later stage. This is one of those skills people talk about when they say "when you learn haskell (or rather functional programming) that you'll have some sort of revelation." Its not some amazing epiphany, you won't start seeing the Matrix all around you. What you will start to do is look at data processing as a set of termination states and a single, concise processing step. &amp;#x200B; As for the "code golf" comment, you're not wrong. Many times you'll read other's code just like your example. Its very easy for seasoned haskellers to understand exactly what that code was doing, but like /u/Darwin226 showed, the `map` version was much simpler and easier to understand for everyone. The big problem people have is that you are introduced to recursion in situations where you're really just doing iteration. Once you add in lazy evaluation, list comprehension, and multiple non-terminating states you can start to get lost. Hopefully it won't stop you from continuing to use haskell and FP.
Your blogpost as well as /u/lubieowoce's reply have been very educative. Thank you!
Nice project! Here's a deployed service if anyone's curious: http://ec2-34-218-78-155.us-west-2.compute.amazonaws.com:6868/
Because game worlds are inherently littered with mutable state and are typically made up of lots of areas? Trying to create a game without mutability is extraordinarily wasteful with memory and will lead to large amounts of fragmentation and nondeterministic slowness. Haskell is just not the right tool for the job. Save it for HTTP servers and other things that don't inherently have much state.
Thanks for the thoughts, definitely won't keep me from using it, was hoping to get some insights into how it all works and how to improve readability. Everyone has had some very good feedback
This is really helpful, thank you!
Servant server supports http/2, but is there any way to use http/2 in servant client? 
I think /u/evincarofautumn gave the main answer you should pay attention to. Instead of trying to trace through step by step what's happening, pay attention to the *meaning* of the subexpressions. Incidentally, I want to emphasize an answer to this: &gt;So my question is whether or not this is an acquired skill or if recursion tends to be murkier? Recursion tends to be harder to get at first, but it pays off. It's far less clear to me, for example, that your iterative version is even correct! It depends on the behavior of indexing expressions at the boundaries, and since I don't recognize the language, I don't know how those are defined. (Assuming your code is correct, they must be defined differently from Haskell's range expressions, so that \`1..i\` actually excludes \`i\`?) The recursive solution expresses a simple fact about spliiting lists: of the ways to split a list, all but one involve splitting the tail, while the remaining one involves splitting off the head from the tail. With a moment's thought, that's obviously correct. Not that I didn't say I understand exactly what operations are performed in what order. I don't! In general, that tends to be harder to reason about with recursion. But it's it's not *necessary* to know that the resulting code is correct. Instead, you may assume the function is correct on smaller inputs, and you need only verify that the recursive use *is*, in fact, on a smaller input, and that it correctly builds on that result (which is assumed to be correct) to produce a correct result in this case, too. This is why it's easier to understand that it's right, because I can do that reasoning at a higher level.
Continuing to engage in hopes of changing the situation is a valid choice, sometimes, I agree. But it would get exhausting to try to keep up with all places where people are saying and doing things you want nothing to do with, just to cast a dissenting vote in case they should happen to take a poll and mistake your silence for evidence that you don't exist. I understand the feeling of working hard at something, and then seeing it dismissed without a suggestion for how to fix the problem. But if you want to gather reliable sampled data in a complex world, you have to work to find a sample that avoids bias. That's a hard job, indeed, in a world where all of us have bias of some kind. People spend their professional careers trying to do it and still often get it wrong and don't recognize how their limited views bias their samples -- for example, consider polling organizations that assumed for many years that most people have land-line phones, and so accidentally excluded a huge part of young adults from their polls. Nevertheless, that's what it takes.
Eh, this is what the question thread is _for_. Drop by anytime if you want to get deeper into Haskell, because it's a lot of fun. I think people may have read your colloquial style as taking potshots at the language, which might explain the downvotes.
Yeah I think it's the `AffineSpace` class. I have also heard the term "torsor" used for this kind of structure.
&gt; coffee change (n.): Money paid to programmers so that they may buy coffee, which they then transform into code.
Yup, (`M-x`) `align-regexp`. Another handy one is `sort-lines`. These little text-editing functions go a long way, and they work reliably; IDE-like features are nice, but their complexity tends to make them unreliable in my experience. I’ve only just barely started using Intero on the regular, and I’m still not entirely sold: it was fiddly to set up correctly, and doesn’t yet work with GHC 8.6. (I want `QuantifiedConstraints`!) 
Have you submitted an issue for this on github?
I know it's a joke about prefix vs infix but the Scheme influenced sophomore would def write a tail recursive version 
As i said, put it in a compact region. Keep that in shared memory. Restart program.
Haha, that's correct! The literal translation in Finnish is 'Worm game' so I didn't stop to think about it. &amp;#x200B;
For my extremely simple use case at least, it was very straight-forward and painless. Brick looks like something I could use in the future though.
Thank you! That helps a lot!
By the way, there's no data dependency between `x` and `y`, so if you're comfortable with `Applicative` you could also write `outer s = (+) &lt;$&gt; (liftIO $ someIO s) &lt;*&gt; (hoist generalize inner)`.
I think this article about coercions would complement this post nicely: http://reasonablypolymorphic.com/blog/roles/
In this case, I basically didn't trust that survey results wouldn't be misused. Things like [this](https://www.reddit.com/r/haskell/comments/8tc8pr/fp_complete_launches_new_blockchain_auditing/e1878ox/) from StackLover / StackSucks / haskdev / vedksah / whatever account they were using at the time were the kinds of things I figured might happen, and there was an accidentally-public-comment-that-was-immediately-deleted that points to a potential [link](https://imgur.com/a/v5mCK) between some of the parties involved.
Restarting a program isn't free, even if you keep stuff in shared memory. I'm doubtful that this would maintain a decent framerate- and I think some of the bottlenecks would be stuff like not being able to maintain an OpenGL context or open window for that matter- and is really no different from keeping everything in resident memory. That shared memory pool will get used just like resident memory, complete with fragmentation and bad cache coherence if you're sloppy. The architecture of "have a pool of workers and restart them occasionally" doesn't really work for games. It's fantastic for servers, however, because reliability is more important than performance in that context. The thing is, you can just about always throw more hardware at servers, so real world constraints don't really apply. Worker pools might work alright for a game server, but not for anything client-side, where you constantly have to render a bunch of shit to a framebuffer, play sound, and handle input. You can never expect users to throw more hardware at the problem, so ideally, you squeeze everything you can out of even the weakest machines. And going back to the original comment, there's also the issue of airplanes having limited hardware constraints. It's not exactly a good idea to require 5x the hardware to have 5 copies of the program running at all times. What about the off-chance that all 5 programs need to be restarted at the same time? It would be far better to have a single program running that operates tightly within real world constraints and has real failsafes built in instead of coordinating 5 copies of the program and pretending that airplanes are stateless and have no side effects. But hey. I'm willing to be proven wrong. Make a Minecraft clone with Haskell that restarts every frame. If you can make it run at 60 FPS, I will bow down to you as a master programmer.
If the two projects don't collaborate with each other then there's an problematic conflict of interest at work here and Stack is in a disadvantageous position by being dependent on Cabal. Where can I read up about the various reasons you refer to that hinder collaboration?
This is exactly the kind of problem that hurts Haskell and that I was trying to point out [in this comment](https://www.reddit.com/r/haskell/comments/9fefoe/if_you_had_the_ultimate_power_and_could_change/e5we3ql/): &gt; To me, **the build tool you use should be a minor detail and be interchangeable with each other!** but unfortunately with Stack and Cabal for whatever reasons this isn't the case yet. 
Since Haskell Weekly's reputation is tainted as it appears to be seen as partisan (and I tend to agree). And on the other hand given that http://www.haskell.org is well respected and has a reputation for being non-partisan, maybe www.haskell.org could be convinced to organize community surveys?
TL;DR: Recursive or not, have the approaches ready and use them when needed. Others have mentioned ways to reason about recursion (like thinking about *results* of sub-expressions rather than the execution). But I want to point out that the recursive solution and the iterative solution do *not* present equivalent algorithms. So it's somewhat unfair to blame recursion *per se*. In this case the iterative solution uses slicing, whereas the recursive solution prepends elements to sub-results. Indeed, recursive and iterative, whatever they mean, tend to produce differing approaches. I do hope that others' comments have helped you make sense of the recursive. Still others have presented examples using `inits`, `splitAt` etc, which is indeed more clean. These approaches are what they are: approaches. There should be no need to say things like: &gt; It just seems like a lot of code written recursively could become code-golfy, opaque and hard to maintain/change. Wondering if there's a better way. Because it sounds like it implies that 'recursive' is 'worse', which just throws a tool out of your toolbelt. The use of the words 'seems' and 'could' does not mean otherwise. There are other things to consider, such as decoupling the recursion strategy (*how* you split cases into sub-cases) and the computation (*what you do* with the subcases) (It's called recursion schemes, but whatever). But it is still pretty recursive, and the mindsets like 'I already have the results of the sub-cases' are even *more* useful.
i use it "outside the ivory tower". just spent the past few hours porting a few random packages to build for ghcs 7.10, 8.0, 8.2, 7.4, and 8.6, without warnings: ``` -------------------------------------------------------------------------------- instance Monoid Occurs where mempty = Occurs Nothing Nothing #if MIN_VERSION_base(4,9,0) mappend = (&lt;&gt;) ---------------------------------------- instance Semigroup Occurs where (&lt;&gt;) = mergeOccurs ---------------------------------------- #else mappend = mergeOccurs ---------------------------------------- #endif mergeOccurs :: Occurs -&gt; Occurs -&gt; Occurs mergeOccurs = ... ``` no one's forcing you to bump your your haskell compiler, if you don't want it to ship with `Semigroup` in `base`. Even GHC 6 with 2008 Hackage is a superior programming language / environment than most today.
i use it "outside the ivory tower" lmfao. just spent the past few hours porting a few random packages to build for ghcs 7.10, 8.0, 8.2, 7.4, and 8.6, without warnings: ``` ---------------------------------------- instance Monoid Occurs where mempty = Occurs Nothing Nothing #if MIN_VERSION_base(4,9,0) mappend = (&lt;&gt;) ---------------------------------------- instance Semigroup Occurs where (&lt;&gt;) = mergeOccurs ---------------------------------------- #else mappend = mergeOccurs ---------------------------------------- #endif mergeOccurs :: Occurs -&gt; Occurs -&gt; Occurs mergeOccurs = ... ---------------------------------------- ``` no one's forcing you to bump your your haskell compiler, if you don't want it to ship with `Semigroup` in `base`. Even GHC 6 with 2008 Hackage is a superior programming language / environment than most today.
I'm the author of http2-client. Currently this integration is not done, but there is no major blocker. I've been discussing this exact topic with Alp no later than yesterday to get some understanding of servant-client. I'll likely start PoC-ing something soon but if you have time I can sketch you and walk you through how I would do that integration and let you do you.
My idea was that you'd always hoogle for `LeftHandType -&gt; a`, that's what my last sentence was about. For arithmetic for example you'd be much more likely to want `LHT -&gt; LHT -&gt; LHT` and slight variations thereof, for example suggesting `take` as infix operator when the lhs is an integer is probably uncommon. That's why I said you need to figure out how to query, cause the most polymorphic query probably leads to plenty of false positives. I agree that can still be a hard problem but you don't have to solve it completely in a v1 : )
Could you expand on your experiences with nix? I'm nowhere near a power user but my experience with nix has been beyond blissful so far. I'm aware it's not very user friendly yet, re the UI, other features are way more user friendly than any tool I've used before.
Where are you getting 80%? [This page](https://www.fpcomplete.com/state-of-haskell-2018) lists the survey as having had 1100 respondents and (of those respondents I assume) 465 stack users which works out to ~42%. That seems pretty reasonable to me, given how new users are commonly pushed towards using stack. It's certainly a lot more reasonable than 80%.
Note that `splits l = zip (inits l) (tails l)` has two more elements, those that contain `[]` at the either end. It's equivalent to ``` splits [] = [] splits (x:xs) = ([], xs):[(x:ls, rs)|(ls,rs) &lt;- splits xs] ```
Every time I try to use it I end up sinking hours upon hours into it with little to show. Last time I tried to use it, I was building the obsidian reflex web development. I added their binary caches and built it and it seemed to work. Okay. Two days later I'm on my laptop and I perform the same steps, and a few hours later I notice that it's been trying to compile ghc for the last four hours. Other times I'll try it and it seems to work but then I'll get runtime errors. Like the time I tried to build tensorflow to do a machine learning course. Worked until it came to some missing shared object file, then bailed. I've even had random failures like when there are tests enabled and one of them fails spontaneously, only to succeed on the next try. Why are tests even enabled in this nix build? I've had issues where the locale in nix build is somehow different from the host machine I'm running causing weird terminal glitches. And the amount of compiling that ends up happening is frightening. I run gentoo so I'm not opposed to compiling my system, but man nix takes it to the next level. You change one little thing and then every dependency needs to recompile itself all the way up. I really like nix in theory but man in practice it has not worked out for me at all.
It's already there: https://github.com/haskell/haskell-ide-engine/issues/558 
&gt; &gt; crazy puzzle speak &gt; I didn't meant to offend Might back off the word "crazy" next time you aren't intended to "offend". https://mic.com/articles/146806/stop-saying-crazy-and-insane-if-you-care-about-mental-illness#.oX8iYcFUL
The link to [structs](https://github.com/ekmett/structs) seems incorrect.
`x \`op\`` is the same as `op x` -- it's just that when `op` is typically used infix, you write the section with the backticks to keep it in the same position. 
That discomfort you feel at seeing recursion isn't entirely "wrong": when you see direct recursion, it often means you're missing out on a higher-level description of the problem using maps and folds. While you should definitely be able to understand a simple algorithm using direct recursion (all the lower-level algorithms are defined with it after all), it's not necessarily the first tool you want to reach for. &amp;#x200B; Mind you however, this attitude should go doubly for explicit loop iteration.
For people using old reddit, that's type family HVectElim (ts :: [*]) (a :: *) :: * where HVectElim '[] a = a HVectElim (t ': ts) a = t -&gt; HVectElim ts a
I wrote this largely to help people get started with Haskell, and I'm particularly interested in what the Haskell community feels about this use-case of providing a great environment for scripting. As I said on the piece itself - all comments, corrections and suggestions are very welcome!
That's worthy of its own top-level posting. I'd like to see the answers myself. My best guess is that, very broadly speaking, Monads tend to aggregate values, starting from small values and growing into larger ones, and Comonads tend to start with their full values and disaggregate them down into smaller values. From that perspective, seeing more Monad than Comonad ought to make sense. However, in that way information theory tends to be, when you really start digging into things, it gets fuzzy again. Comonad includes cellular automata, which are turing complete within the bounds of their space. I'm not sure if you can have a Comonad CA with unbounded internal space, but I'm pretty sure you can, which would make such CA instances implemented via a Comonad instance mathematically just straight-up Turing Complete. And there's an important sense in which all Turing Complete abstractions are equal in power, so if you push the limits mathematically there probably isn't a way to distinguish them. So there's a fundamentally intuitive, opinion-based element to that question anyhow, and whether or not the problems we tend to encounter in our real universe really are suited to one paradigm or another. Another perspective is the possibility that by either biology or historical intellectual accident, we're biased in the monadic direction in computer programming. I see certain similarities between that idea and the study of non-Turing Complete programming languages; one could argue that the general programming world's resistance to non-TC languages is historical accident, in that had we started with them, they'd seem natural and we would tend to be horrified by every time we needed to step outside of their domain. One could imagine another intelligent species being biased in that direction, or even ours, had programming been much more controlled by mathematicians for longer before it escaped out into engineering.
Well, I don't know what's going on there. maybe open a bug report? I have installed stack on many computers in many operating systems and didn't run into issues like that. This sounds a bit like an edge case?
GHC doesn't reliably perform Common Subexpression Elimination. Partially, it's because CSE can trade time for space, since you need to hold onto the result of the common subexpression until all uses finish. If it's a useful optimization in your case, it's one you should do manually by storing the result in a variable.
How could I go about manually storing it? Bang patterns?
Discussion from earlier this week: https://reddit.com/r/haskell/comments/9mm05d/2018_haskell_survey_results/
Some are technical, PVP vs Curation, default behaviour, installing ghc or not, etc. Some seems to me like they are personal: [example 1](https://twitter.com/hvrgnu/status/939464291941605377), [example 2](https://twitter.com/snoyberg/status/754905806848438272).
I don't think there's a fundamental theoretical reason for this, it's just that more practically useful things turn out to be monads than comonads. There is a notion of comonads as objects (in the OOP sense) though, as evidenced [here](http://www.haskellforall.com/2013/02/you-could-have-invented-comonads.html); as elegant as it seems, though, there are some caveats with this, since comonadic objects don't capture all the subtleties of the usual object-oriented paradigm (particularly inheritance, runtime polymorphism, and "open recursion").
&gt; why is the monad so much more used than comonads [...]? A comonad has access to some context, including the current value, and possibly the value of some neighbours and the shape of the neighbourhood. You can implement a monad which gives you access to side-effecting functions returning the current value, the value of some neighbours, and the shape of the neighbourhood. So if you already know about monads, it might look like you don't really need comonads. If you do know about both monads and comonads, then you can pick whichever one is most appropriate for your problem, and if your problem can be expressed in terms of a comonad, your code will probably be more elegant than if you use a monad which returns the context as a side effect. But even then, your code might be less reusable, because we have a lot of infrastructure for combining monads and monadic computations, but very little infrastructure for combining comonads and comonadic computations. So I think comonads aren't much used because if you know monads but not comonads, there isn't much of an incentive to learn about comonads, and even if you do know about both monads, comonads, and the tradeoffs between them, then there is an incentive to use monads instead of comonads.
Thanks for this!
Nice writeup! Might be worth mentioning Turtle and Shelly as well
Thanks! Yeah I omitted them mostly because I haven't used them; I was going to try Turtle but ended up deciding that as a beginner I should be exploring the base libraries rather than having everything conveniently in one place if you see what I mean. Upon reflection it would be more helpful for other beginners if I link them though.
I've added an extra section and given you a shout-out, cheers!
Because all the coalgebras in a cartesian monoidal category are trivial.
I mean that Functor and Applicative Functor are useful and so are Comonads but Monads are (seemingly?) used more often. As Monads fall between the more general functor and applicative functor on one side and the more specific Comonad (and others?) on the other side, it seems to be that the level of Monad in the type class hierarchy looks "not too general, not too specific, just right".
Most Monads are not Comonads and vice versa.
 main = do let firstResult = heavyComputation someParameter secondResult = doSomething firstResult thirdResult = doSomethingElse firstResult That manually stores the common subexpression in the variable `firstResult`, so it would only be calculated once.
I am so happy you did this! It will be my foray into both more Haskell and scripting!
I'm glad you like it :D
...the markdown \*tokenizer\* is built in to the renderer?!
Really good! Cross platform, concise, and informative for both beginners and old-timers.
Yeah. ``` Works as pre-formatted block in new reddit, but not old-reddit. Well, actually old-reddit treats it as inline pre-formatted if it doesn't have a paragraph break. ``` Works as pre-formatted in both new- and old- reddit, even with paragraph breaks. It's very weird.
Did you also try hindent/stylish haskell vs brittany?
None and Some are in the latest version iirc :) they're at least at HEAD of current master :D
None and Some are in the latest version iirc :) they're at least at HEAD of current master :D
The answer is probably more practical than theoretical. First, the monad combinators used in Haskell—and the `do` notation—both make it easy to write programs using a monad in a way that resembles imperative programming, which is a familiar style for most programmers. It's possible to structure programs using comonads, but process is less familiar. Second, most interesting comonads are inherently inefficient in Haskell. For most comonads, `extend f` will call `f` multiple times, and those calls can't share any of their work. Simple algorithms that are O(n) using certain Arrows become O(n^2) or worse when expressed with equivalent comonads. If it weren't for the performance problems, people might eventually find situations where comonad style is helpful and it would become more familiar. For now, we haven't found a compelling use case.
Monads are useful for dealing with effects. A monadic computation of type `a -&gt; m b` turns a value of type `a` into a result of type `b`, with the effect `m`. This effect can be "a list of possible alternative results" (`[]`), "an IO action that gives a `b`" (`IO`), "a computation that might fail" (`Maybe`), "a computation that needs another value to work" (`Reader r`), "a computation that requires mutable state" (`State s`), or whatever else. Being a monad means that effects combine nicely: there's a "none" effect (`return :: a -&gt; m a`), you can add to it (`bind :: m a -&gt; (a -&gt; m b) -&gt; m b`), and they work together as you would hope. Comonads are useful for dealing with contexts. A comonadic computation of type `w a -&gt; b` takes a value of type `a` in a context `w`, and produces a value of type `b`. This context can be "some additional input value" (`Env e`), "a collection of possible inputs, with one in particular chosen" (`Store s`), "inputs that could have been" / "a mutable value's history" / "some other intuition that works" (`Stream`), "okay honestly I don't know what this does" (`Traced m` -- requires `Monoid m`) or whatever else. Being a comonad means that the context augments things nicely: you can ignore it (`extract :: w a -&gt; a`), you can keep it around (`extend :: w a -&gt; (w a -&gt; b) -&gt; w b`), and it works as you would hope. Accumulating effects seems to be more common than dealing with values in a given contexts, so monads are more commonly used than comonads, so they have more general support. Also, comonads are co-things, and co-things are harder to think about than things. You can think about a monad by what it *is*, but you need to think about a comonad by *how it can be used*, which is a bit trickier. (I've found focusing on An additional note: there can be some overlap between these. For example, the `Reader r` monad is equivalent to the `Env r` ("Env" for "Environment") comonad. Though `Reader r a` /= `Env r a`, we do have: a -&gt; Reader r b = a -&gt; r -&gt; b = (a, r) -&gt; b = Env r a -&gt; b so their respective computations are the same.
Woah that comment!
&gt; Interesting comonads require a modal/linear type system while monads don't. &gt; [..] &gt; programs have multiple input types (the types of its free variables) but exactly one output type (the type of the term). &gt;Systems based on classical linear logic do have a full on duality but I don't know of any real languages based on that. I don't understand what you mean here. You can always tuple up all the input types into a single type. What do free variables have to do with linear type systems? Why do interesting comonads require linear types? Your comment seems equally mysterious to me as the top level comment 😅.
The ! modality, which allows unrestricted use of a variable in linear logic is a comonad. Unlike in Hask/Set, there can be meaningful 'splits' of values allowing non-trivial comonoids, which means that reader/cowriter can have non-trivial structure as well.
Monads don't live between applicative and comonad, both monad and comonad are functors with some extra structure, but haskell is missing the bits that would be needed for a 'coapplicative'. (It'd make the language boring to have them in full generality, though, as having arbitrary coexponentials and exponentials collapses the world down to a poset, leaving at most one arrow between any two types!) There are uses for comonads in Haskell, but by their very nature you're not going to use them to structure your entire program around one like a monad. On the other hand, you may well use lots of little comonads. My favorite usecases are for things like resumable stream processing. There I get a lot of mileage out of the comonad instance. I also tend to use lots of little 'coreader' style comonads to hold onto metadata about things like names and avoid having to name some kind of extract operation on them. Finally, the code in `lens` makes weird use of the comonads for Bazaar and Pretext to ensure the code works in as many situations as possible. We spotted a lot of the constructions in lens by thinking through the consequences of the view of a lens as a "costate comonad coalgebra".
I think stack and vscode with haskero or HIE make a pretty good up-and-running-in-15 minutes IDE combination for windows users. I disagree about the guides and books. There's a ton of introductory material that goes from beginner to simple monad transforms. Then there's a good chunk of advanced topics that are covered pretty well. But there's very little at the intermediate level, the level of "how do I write a complete web application". There's the Yesod book, and that's about it. And that's the level you really need to get people actually using the language. 
I didn't quite understand your comment but googling the terms led me to [What does a nontrivial comonoid look like?](https://stackoverflow.com/a/23858109/2682729) which was very helpful in understanding what you meant. Thanks!
It's not so much that Monads are more useful, is that things that are Monads are often more restrictive, and so *the fact that they are Monads* matters more. The key realization is in the function `extract`. To really be a Comonad, you have to be able to erase the Comonad part of the value at will... *and then you're no longer in the Comonad!* This is simply not something you can rely on for Monads. Indeed, a *lot* of Monad use is for times when you simply can't just reliably leave the Monad with a useful value. Indeed, every Monad that you can simply leave at any time without feeding it a value from outside **is also a Comonad** \- where extend is return and extract is just the reliable means of leaving. But then you've left. You're done, and probably moving on to other things. Those things may also be very Comonad like, but since you can always leave them right away as well, you don't think about their Comonad-ness.
Hold up, monads and comonads are not more general or restricted than each other. Rather, they are both parallel forks from Functor. Monads are a restriction on Functor, but Comonad is *not* a restriction on Monad.
I did, and like them all to some degree, I recommended Brittany for two reasons: 1. It seemed to make "better" choices with some code to maintain or build off formatting that was already in place - this is of course highly subjective on my part 2. It's the one used in [Haskell IDE Engine](https://github.com/haskell/haskell-ide-engine) so, assuming it gains adoption, that styling of code might be more widespread - but this is just my hunch based on what I've seen in other programming communities I wouldn't call either of those good reasons, but it comes down to needing to pick one to simplify the choices being made - I wanted to go to the opposite end of the pool with regards over-offering; because personally I think it can be quite overwhelming when learning a language to have to make all these choices.
My feeling is that not everyone finds the academic slant appealing, so it's good to get a variety of entry points. I largely wrote it in reply to the comments of my colleagues (at a Java shop) hence the focus on Haskell as an easy tool to get up-and-running with and straight into IO which inevitably is where most of the jokes rest...
Wow... that hostile discussion over at https://github.com/fpco/stackage-curator/issues/22#issuecomment-233163897 &gt; This rubbish of needing to be compatible with cabal-install because the Hackage overseers have stated it just makes me want to stop working with Hackage. Maybe you should take a step back and consider whether you want to continue being part of the problem with encouraging further fractures in our community. Now things start to make a lot more sense to me. =( Quite frankly that makes we want to stop using Stack.
It's from the blog post linked &gt; Most applied users (80%) use the tool Stack which is new since that time and &gt; We learned that our thousands of hours contributing to open-source components have not been wasted, and noted with special pleasure that 80% of commercial Haskell users are using Stack, an open-source tool initiated and lead-maintained by engineers from FP Complete. Given the numbers you mention it appears that fpcomplete is deliberately trying to make it look like Stack is more popular than it actually is which given [u/cdsmith's comment](https://old.reddit.com/r/haskell/comments/9mm05d/2018_haskell_survey_results/e7jomyq/) doesn't surprise me anymore.
Obviously
Note that you will need to login.
Would they work in Affine type systems such as Rust?
All the acadamic work is put into Haskell is to make it a safe and productive language. So as a community we should be showing how it can be used in real world applications without needing a PhD.
Do you think comonads will see more use once `LinearTypes` lands?
Aren't almost all of those just one guy? I guess it's impossible to prove, but the posting patterns and the fact that each one popped up after another one disappeared make it pretty obvious that it's just one person circumventing either bans or bad reputation.
There's also some reason to believe that this person colludes with fpco, evidenced by [this link](https://imgur.com/a/v5mCK) from [a recent /u/dalaing comment](https://reddit.com/r/haskell/comments/9mm05d/2018_haskell_survey_results/e7k7yqy) It won't actually prove that fpco requests or even privately condones this person's behavior. It just shows they've got some kind of strategic relationship in this person's crusade. Makes me uncomfortable.
That is likely indeed -- perhaps there is one anti-Stack sockmaster and one pro-Stack one, or perhaps there is just a single troll behind them all, but as you say it is hard to tell. Also, in the past I have seen at least one instance of what I believe to have been staged conversation between two sockpuppets. 
I highly doubt fpco wastes their time doing that.
They don't have to like what the guy is saying to have some kind of relationship with him. It might just be so they can ask him to tone it down.
I mean yea it's possible this person doctored that comment and deleted it after only a few minutes in hopes that someone partisan might happen to view the thread at the perfect time and screenshot it and hold it against fpco, but that doesn't sound like a strategy anyone would expect to work. I think it's more likely that there's *some* relationship, even if it's not the one that the comment suggests.
The first version is incomplete. What do you do if the result is `Nothing`? You can complete it like the `maybe` function in `Data.Maybe`. The second has the wrong type, it should be `Maybe a -&gt; Maybe (Maybe a)`. The third one does not make sense, since `Maybe` is a parametric type, not a type constructor. If you just want to remove the `Maybe`, because you are sure the result is `Just`, you can do: f Nothing = error "Something` f (Just x) = x This function exists and is called `fromJust` in `Data.Maybe`.
I'd be careful with accusations like this. One image isn't a lot to go on. The comment itself could be a fake to draw conspiracies, or the relationship might be nothing like that comment implies, or any number of other ways it could be misleading. I'd say it's more responsible to give the benefit of the doubt and assume the guy is a lone troll. I kind of agree that the comment makes me instinctively uncomfortable, but I'm not going to draw any conclusions from it.
When I was a beginner, Turtle was very helpful because I could find all the things I needed in one place, it was all integrated together, etc. Now, Turtle is still nice because I can look at the implementations of the things I'm using to see what libraries it is using internally. I also had a hard time exploring the "base libraries", because I did not know where to look. Perhaps someone could enlighten me here. &amp;#x200B; &amp;#x200B;
Thank you for clearing it up.
What new drama happened?
Downvoted. There is way too much meta-discussions and drama already. You have the possibility to respond to the specific replies you find questionable instead of doing this. Also you provide no proof. Again if you just respond to the specific posts you'd have proof add well. You're feeding trolls. 
If videos fail to display in firefox with "Sorry because of its privacy settings, this video cannot be played here", what worked for me is to open it in chrome. (Disabling adblock/do not track/tracking prevention in firefox did not help.) I struggle to understand why Skills Matter makes it such a pain to watch a video, let alone download it now to watch at a less distracted time. Some of my best early Haskell learning was spj videos watched offline during long flights.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/K1SVFqZ.jpg** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
I dunno if that's "coordinating" so much as "conversing." I don't see anything sketchy about that kind of communication (though the topic a little disheartening)
Here are three idiomatic ways to handle these kinds of basic concerns with Maybe. The first, and simplest, is to use `fmap` or `&lt;$&gt;`. Example: add3 x = x+3 myMaybeFunction = fmap add3 myMaybeFunction (Just 2) &gt; Just 5 myMaybeFunction Nothing &gt; Nothing Here, fmap uses Maybe's Functor instance to apply a function to a Maybe value. This is useful if you have a simple operation that needs to operate on a value that may or may not exist, and you want to save worrying about what to do if it doesn't exist to some other place in the code. The second way to deal with this is to use the function `maybe` from `Data.Maybe` to provide a default value if the input is Nothing. Say, for example, that you wanted to always default to outputting 0 in a numeric calculation: import Data.Maybe (maybe) myMaybeFunction = maybe 0 (+3) myMaybeFunction (Just 5) &gt; 8 myMaybeFunction Nothing &gt; 0 This option is good if you know you have a discrete default case and you want to 'get rid of' handling Maybe right there in your code. The third is the nuclear option, which gives you full control over exactly what you're doing. This option uses case syntax and destructuring to explicitly handle the values in scope. This gives us a lot of control about what to return, but puts all of that control into this single function. It potentially introduces more complexity than the other options, but is sometimes necessary. In this example, we'll add three, but only when the input value is even, otherwise, we'll just return Nothing. myMaybeFunction x = case x of Just n -&gt; if even n then (Just n+3) else Nothing Nothing -&gt; Nothing We could also forgo the use of case, and do this: myMaybeFunction (Just n) = if even n then (Just n+3) else Nothing myMaybeFunction Nothing = Nothing This is largely a matter of style, but the idiom tends towards the use of case because it makes for clean indentation. The argument for using case instead is that using pattern matching directly on multiple cases of input means you have, as far as indentation is concerned, multiple function declarations, which confuses the eye and sometimes screws with text editor 'folding' features. There are other ways to 'deal with' Maybe values, but in my opinion these are the most appropriate to use for someone just starting out. 
I also did not see this survey. They ought to do a better job of publicizing it and holding it open.
Hackage doc generation was broken for a very long time. The message was absolutely true until sometime in the last year or so. Even now, Stackage usually has more reliable documentation than Hackage, and a vastly better search function too.
Nice pun with email from list
Pandoc, the Purescript compiler
'reader' from mtl would work too I think.
FWIW I was able to watch a few of the videos with Firefox, and I even have Adblock Plus (in addition to Firefox's built-in tracker protection).
I've tried to play it. It seem as if sometimes it doesn't react to my key presses right away and I have to press twice.
Anything that involves deep refactoring. For those language, maybe even an upgrade from one version of GHC to another. Most languages suffer from an inability to make large aggressive changes. Haskell's purity, strong types and culture of small law abiding abstractions give it super powers in this regard.
http://pbfcomics.com/comics/skub/
Personally, i was shocked by infinite lists and partial application of functions when learning. Impossibility of implicitly making NPE is also cool. And this short article is cool, too: https://philipnilsson.github.io/Badness10k/escaping-hell-with-monads/
There are seemingly fewer interesting comonads than monads. On the other hand, if `Comonad` is enough for your task then that buys some interesting and useful benefits. For example, comonads compose nicely, unlike monads.
&gt; There is way too much meta-discussions and drama already. You have the possibility to respond to the specific replies you find questionable instead of doing this. Responding to individual comments is not necessarily a good strategy: it extends pointless, and often noxious, discussions needlessly, and risks granting credibility to accounts that do not deserve it. &gt; Also you provide no proof. Again if you just respond to the specific posts you'd have proof add well. Evidence can be easily obtained by reading this subreddit regularly and observing there are accounts which almost exclusively post polarising comments of dubious usefulness about cabal-install and Stack. Also note I intentionally picked my examples from the past: that all accounts I mentioned by name are now deleted, suspended or otherwise inactive. &gt; You're feeding trolls. If the message you take from this thread is "don't feed the trolls", I'm more than happy. Consider it as a call to, before engaging with a provocative comment, evaluate whether it is likely that the other party is actually a good faith contributor. 
Nothing exciting has happened. I'm just tired of seeing the same nonsense over and over in this subreddit for the past three years.
Teach yourself to fish! $ ghci &gt; f (Just a) = Just a &gt; :type f &gt; f a = Just a &gt; :type f &gt; f (Maybe a) = Just a &gt; :type f
Fair enough. Though it's entirely plausible that Taylor was unaware of that conversation. He isn't an FPCo member, or an official Stackage maintainer, after all.
The "HaskDev" accounts are annoying, so I ignore them. It would be nice if no one ever engaged, so they'd get bored and go away, but in the meantime I'm sure censorship is a good idea.
It is kind of embarrassing for the rest of us that you actually had to come here and spell that out. Let's hope this conversation here is enough to kill off that meme for good.
I tend to agree. A general rule to curtail that kind of thing would require unpleasantly heavy-handed moderation. Furthermore, skilled trolls do well at following the letter of the law while ignoring its spirit.
concat ["recruitment", "@", "karamaan", ".", "com"] I can't figure this one out... how can I still apply? :)
I kind of figured they weren't linked to FPCo :) The person who sent me the screenshot mentioned that the comment was not up for very long before it was deleted, so I can't see much benefit to writing it from the shit stirring side of things to writing it (although maybe they were just mentally unwell).
For what it's worth, someone sent me that screen shot along with a story behind it. Apparently they saw the comment, they took a screenshot, then responded, and then the comment was deleted - so it was only up for a very short period of time. I don't see much benefit in sending a message aimed at causing trouble if only to delete it a few minutes later / as soon as it gets noticed, but it is possible that they thought they'd gone too far in their trouble making or were just mentally unwell. Their persistence with the sockpuppet accounts might also point at the latter.
Cabal hell is definitely impressive ;)
Thanks! One has to have some fun every now and then.
This is more efficient than fizzbuzz!
lol who is anti Stack? I've luckily managed to slide across these posts 
I'm not sure it's that impressive, but I still like /u/mzero's [Haskell Amuse-Bouche](https://www.youtube.com/watch?v=b9FagOVqxmI) as a mild introduction.
As noobie I got the point that I should use stack, now I am running linux distro and I installed haskell trough package manager, I am kinda force to run ghc -dynamic flag, is that desired or I should use slack to install haskell, ghc and all the rest. Coming from the Javascript world, I install node and npm, I use npm to install packages/global packages. Is it the same with haskell ? I install slack, ghc and then use dynamic flag or I should just install slack and install everything through slack. Thanks for the intro, it did help me. 
Chrome 68 and Firefox 62 displays same error message. Only Safari 10 works for me.
I can vouch for this company as a great one to work for.
I have yet to hear any criticisms leveled against Cabal or Hackage that don't apply in equal (usually lesser) measure to every other package management system I've ever used. Pretending npm, pip, or maven can hold a candle to `cabal new-build` is outright laughable.
&gt; now I am running linux distro and I installed haskell trough package manager, I am kinda force to run ghc -dynamic flag Is Arch your Linux distro?
Let's ignore situation that was 1 year ago. Right now I see it as FUD and attempt to move users to commercially sponsored competitor website. This line is still there on a package published 5 days ago. If I push a merge request to one/all of the affected packages - do you think they will be accepted?
That is a problem specific to Arch. Arch insists on using dynamic as default for their Haskell packages, regardless of the headache it causes to those who write Haskell code. You are better off uninstalling all Haskell packages you got through Pacman, [installing Stack](https://docs.haskellstack.org/en/stable/install_and_upgrade/) and using it to manage everything else. (And if you weren't going to use Stack, my suggestion would have been to remove the Arch packages anyway and install GHC and cabal-install through the official distro-agnostic binary packages.).
I agree strongly that this is a good development environment for windows users and requires little set up... But if you can get HIE to install inside of 15 minutes, you have one hell of a set-up.
https://letsencrypt.org/ is free and super easy.
Another stylistic alternative is to remove the `if` using a guard, to avoid repeating `Nothing`: myMaybeFunction (Just n) | even n = Just (n + 3) myMaybeFunction _ = Nothing Or use `do` notation in the `Maybe` monad: import Control.Monad (guard) myMaybeFunction maybeN = do n &lt;- maybeN guard (even n) pure (n + 3) There’s no hard-and-fast rule here, it’s just a matter of what looks clearest to you in a given situation and is consistent with the style of the surrounding code. 
My rule of thumb for differentiating “effects” and “side-effects” is whether they’re *observable* by some observer that is “safe” with respect to some property. So for example, mutating an `IORef` is a side effect from the perspective of another thread if they can read the reference and observe that you updated it; but mutating an `STRef` is not, because it’s fully encapsulated (without `unsafeCoerce`, &amp;c.). Same goes for performance concerns like timing and memory, which can’t be measured from pure code in Haskell.
OP here, to expand on the title: I think [postgREST](https://github.com/PostgREST/postgrest#supporting-development) is a shining example of how effective Haskell can be on the server side, in addition to being a cool/visionary idea overall. It recently came up on HN (weirdly enough in a discussion around [GraphQL](https://graphql.org/)) that postgREST is donation supported -- I'd never seen any donation notes or anything like that anywhere so I had no idea. Let's show some support for one of the haskell's most awesome F/OSS projects out there braving the elements in in the wild corpse-laden wasteland of production-quality software.
Seems the link leads to a 404
I'm all for donating to open source projects, but why postgREST in particular, as opposed to anything else?
Yep!
As far as I know Stack compiles source and nix uses binary packages. If you're using nix you can install Stack via 'nix-env -i stack' and then use Stack.
I think that is the easiest solution, but I am not sure how to manage IDE tools to use nix shell to run. I have to dive into that. Until then I am fine with stack managing everything. The only thing which is annoying is that I have 180 packages tied to stack. So everytime I do pacman update it is just all haskell haskell haskell haskell haskell haskell .... 
Hehe maybe uninstall them, and use Stack directly for those. Over the years I've found it less trouble to use local packages for development in Python, JavaScript, Ruby and now Haskell. As for using a Nix shell - I think to some degree this is the wrong thread! My only guess is that if you jump into a Nix shell and fire up your tools from there, they will inherit that environment.
I initially had the same reaction. But on the other hand it would be a lot more work to make an index of all Haskell open source projects used in the wild, and to have everyone set up some donation medium. So I don't mind that someone highlights postgREST in particular, especially given that it's not been done before AFAICT.
Could you try again? The link works for me? It's a link to postgREST's github page: https://github.com/PostgREST/postgrest
I covered this in my other comment in this thread but only really because it's a good example of very useful and practical Haskell in the wild, and the fact that I found out it was donation-supported recently. There are likely a bunch of other F/OSS haskell projects that deserve support with varying levels of visibility both internal and external to the community. postgREST has a lot of community-external visibility and I think it could/should be supported as a "people are doing productive things with haskell" example.
lol some anti Stack person downvoted this. Who hurt you friend?
Coming from C, C++, C#, Java and Python: I was pretty impressed by QuickCheck (and it's property testing) How to write a prop test function is still somewhat of a mystery (I'm still a student), but I recognize the power behind it.
Indeed, I didn't know about that conversation. I'm happy to be corrected. 
This is the link in the post. https://github.com/PostgREST/postgrest%23supporting-development The links in your comment work fine. 
I'm planning to give a similar talk in my local community, but am not convinced with any comment in this thread. The problem with Haskell is that the benefits are clear only when you have already been bitten by an unprincipled, dynamically-typed code-base and are yearning for something better. Or if you already know Haskell, and end up maintaining / refactoring some old code. Neither of these situations delivers a wow factor to a sizeable audience. I remember seeing some Rails "propaganda" videos back in 2006 and was blown away by the ease and speed of doing web dev. I'm looking for a similar demonstrable wow factor wrt Haskell. 
This is kind of the fundamental problem with tools / ecosystems that are optimized for building small things from scratch (which Rails excels at) vs. tools / ecosystems that are optimized for complexity / evolving large systems over time. The vast majority of programming is on existing systems that have existed for at least a year or two (that's just how things work -- new projects are easy to build, don't take many programmers, and if they are successful, persist and gain more and more programmers), and so tools / ecosystems that are better at the latter are much more valuable in the long run, but they both don't play well to tiny "look what I did" demos or audiences that haven't already experienced the pain of dealing with legacy code (which is most code). But if you are giving a talk to people who can't appreciate any of that... I dunno. Do some fun type hackery? Servant? 
Nice and useful post. Be aware that you don't actually need "--install-ghc" option in stack scripts. This is [on by default](https://github.com/commercialhaskell/stack/blob/master/ChangeLog.md#v161) since stack 1.6.1 Also in stack scripts I've been using "script" command instead of "runghc" as described in the [stack docs](https://docs.haskellstack.org/en/stable/GUIDE/#script-interpreter). Not sure what exactly is the difference between the two, though :-) &amp;#x200B;
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [commercialhaskell/stack/.../**ChangeLog.md#v161** (master → 7574e33)](https://github.com/commercialhaskell/stack/blob/7574e33f119125f7d2517a13e26489b89696f0ed/ChangeLog.md#v161) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e7pdgqu.)
&gt; cotutorials Would a cotutorial be a blog post where you start out understanding something, but you're utterly confused after reading it?
Yes, that's me. Thanks for the kind words! Regarding donations, to be honest I haven't thought about it, and I don't think this came up before. Since there's never been (and hopefully never will be =P) less than 2 maintainers, I wonder what the reasonable options are here.
So, every blog post with "monad" and "tutorial" in its title?
Is there a way to provide examples that will work in both cmd and powershell? It seems like I'll end up with two for Windows just for exit codes.
Consider the (e, a) comonad. If we had interesting comonoids, then when we do `duplicate :: (e,a) -&gt; (e,(e,a))` instead of just copying the 'e' it could `split`it. Instead of `extract :: (e, a) -&gt; a` just dropping the `e` on the floor, it would use the counit of the comonoid to eliminate it. In Set these are boring. In a linear type system the parameter `e` would be equivalent to one that was !'d somehow. In something like a vector space or another more interesting setting you have actual comonoids with structure though, and these particular structures become more interesting. Note: this limitation isn't just for comonads, consider the reader monad. (e -&gt; e -&gt; a) -&gt; e -&gt; a -- has to 'duplicate' the 'e' to pass it as both arguments. It theoretically is using the comonoid for every element in Hask to do so. In a linear type system the 'reader' monad needs its argument to be a comonoid. You'd need to have something separate for such strong comonads. Consider, (e, a) as a comonad again for some comonoid e. There you can support `(e, (a,b)) -&gt; ((e, a), (e, b))` -- but you need to split e with the comonoid to do so.
From the outside, yeah. What I mean is that when you’re taking about effects, you have to be precise about who can observe them. By these terms I’d argue that an `ST` computation is still *effectful* (internally), and an `IO` computation that mutates an `IORef` internally is *not side-effectful* (externally).
As someone who hasn't really written any Lisp apart from mucking around with their Emacs configuration, I wonder if it would be possible to retrofit a similar macro system into GHC...
Even if there isn't a *real* reason to favor postgREST over other active projects, choosing a specific project leads to a more effective call to action. People are more likely to respond to a specific plan than a more abstract "let's donate to open source Haskell projects".
Well, you could always do cmd /c '&lt;command&gt;' or powershell -Command 'command' and it will work in both. It's a bit limited, but should be sufficient for the examples you've given. 
trying to learn haskell... this type thing is confusing me. when do i use =&gt; vs -&gt; ? 
Consider the `(+)` function ``` :t (+) (+) :: Num a =&gt; a -&gt; a -&gt; a ``` =&gt; is when you want to add a constraint to a type. Think in java (you want the generic type a to implement 'Num' so you can actually add it. -&gt; is the function type signature. a -&gt; a -&gt; a Function&lt;a, Function&lt;a, a&gt;&gt; 
Compiled Haskell binaries are large, especially when there are nontrivial dependencies. Is there some way to tell how much each library dependency contributes to the total binary size? (Not talking about GHC runtime, debug symbols, etc., just the libraries that are statically linked in.) Ideally there would be something similar to [webpack-bundle-analyzer](https://cloud.githubusercontent.com/assets/302213/20628702/93f72404-b338-11e6-92d4-9a365550a701.gif) from the Javascript world.
I am looking for the correct technique to remove this boilerplate. Consider a sum type ``` data ParseResult a = Result Input a | UnexpectedToken Char | NoOrConditionMatch | UnexpectedEOF | Error String deriving (Show, Eq) ``` The Functor only cares about applying `f` to the `Result` type, how can I just pattern match to get a 'catch all' without needing to re type everything out ``` instance Functor ParseResult where fmap :: (a -&gt; b) -&gt; ParseResult a -&gt; ParseResult b fmap f (Result input a) = Result input (f a) fmap _ (UnexpectedToken c) = UnexpectedToken c fmap _ UnexpectedEOF = UnexpectedEOF fmap _ NoOrConditionMatch = NoOrConditionMatch fmap _ (Error s) = Error s ``` I cant do something like this as it needs to return a `ParseResult b` ``` instance Functor ParseResult where fmap :: (a -&gt; b) -&gt; ParseResult a -&gt; ParseResult b fmap f (Result input a) = Result input (f a) fmap _ all@_ = all ``` 
How about `DeriveFunctor`?
Yeah, you're right. In this instance you can unwrap the reader and turn it into a function `SomeData -&gt; Int`.
Yeah and I don't want to complicate things also -- things can often change when money gets involved and unless people in servant really want it I don't want to rock the boat! Thank *you* for working on Servant!
My three go-to examples: 1. `newtype`. I can write `newtype UserId = UserId Int` and `newtype PostId = PostId Int`, and without adding runtime overhead (both `UserId` and `PostId` are represented by a machine integer at runtime) I have type safety and never use a `UserId` where I meant to use a `PostId`. A system I used to work on had some records with three different types of identifiers and woe befall you if you used the wrong one in the wrong context. Was this enforced? No. There was a loose convention of parameter names but that was it, and unwinding some of the bugs from that made for a really interesting time. You cannot get that sort of machine-checked safety (by which I mean, "an `Id` will never be used in the wrong way in production) in a dynamic language. 2. You can extend this idea with phantom types. I can write something like `newtype Money a = Money { inUnits :: Int }`. Now a `Money USD` is different from a `Money EUR`, and you can't add them, or compare them, or do anything unless they're the same currency. You can then extend this idea by writing a type like `ExchangeRate from to` and writing a function like `exchange :: ExchangeRate a b -&gt; Money a -&gt; Money b`. At an old job, the team sitting next to me accidentally applied exchange rates back-to-front in their data warehousing platform, and reprocessing all the data properly took a long time. In a language like Haskell, it's easy to make that bug impossible. This idea is explored thoroughly in the `safe-money` package. 3. Your database library might let you write something like `with connection.transaction: ...`. Haskell can do better. I can set up the types to control database access in a way that's almost impossible in other languages. I can write a function like `runTransaction :: Connection -&gt; DB a -&gt; IO a`, where `DB a` is the type of "database actions that return an `a`". I can set it up so that the only things you can put into a `DB a` are database queries, inserts, etc. This avoids bugs where you do other IO (like send an email) inside a transaction, then your transaction rolls, back but that email's already gone out and you can't take it back. With something like the `DB a` type above, the compiler will force you to do your database stuff, leave (and commit) the transaction, and then you can go do your emailing within the regular `IO` type. Similarly for concurrency; the `stm` library has its own type `STM`, and a function `atomically :: STM a -&gt; IO a`, which allows natural-reading code like `atomically $ do { ... }`.
Have you seen the servant example, where the handler has no code beyond `return`, because all the encoding/decoding comes from the types and is set up for you? https://haskell-servant.github.io/posts/2015-08-05-content-types.html
If you enable the DeriveFunctor extension, you can just add `Functor` to your deriving clause and that'll take care of it. This also works for: Foldable, Traversable, Generic, Data, and Lift.
For now: `=&gt;` is only ever in the first position, and only if you are talking about sum type classes / instances (if there's more then one they need to be parenthesized and separated by commas). `-&gt;` any other time.
I use stack-static on AUR, I find that's the easiest way to get started. Don't install any of the dynamically linked packages in community, just use stack provided by stack-static. It will save you a world of headache.
[removed]
I'd suggest using [https://hackage.haskell.org/package/hmatrix-gsl-0.19.0.1/docs/Numeric-GSL-Fitting.html](https://hackage.haskell.org/package/hmatrix-gsl-0.19.0.1/docs/Numeric-GSL-Fitting.html)
Haskell newbie here. Wrote a function and trying to infer the types using GHCi. Turns out the below function's type constraints can never be met. What am I missing here? Why did this function even compile? divby n p | n `mod` p == 0 = [p] ++ (divby (n `div` p) p) | n `mod` p /= 0 = [n] primFacs n = [ divby n p | p &lt;- [2,3..sqrt n]] When I query the type of primFacs in GHCi, I get the following *Main&gt; :t primFacs primFacs :: (Floating a, Integral a) =&gt; a -&gt; [[a]] How can anything be Floating and Integral at the same time? 
That does sound really good! I had the idea that hmatrix was abandoned, fir some reason
Yeah, I think it's best to keep the current situation, at least for me. I quite like that no money is involved. If people want to donate to other servant devs, they should feel free to talk to them about it. Something that some servant devs did once or twice in the past though is that a company who's happily using servant but would really really like to see some feature added or some limitation lifted, sponsors development time to address that very problem they're bothered with. We're of course not talking about the same amounts of money anymore, but this way it benefits everyone and just feels more balanced overall (to me anyway).
There isn't one? that's exactly What being supported by donation means...? Does it evoke something else for you? Some projects aren't actively looking for donations which is fine too. I thought postgREST was one of those until I saw they were trying to get some support via patreon to fund ongoing development. I don't even use postgREST but the idea is appealing to me and I thought it was a good example of Haskell in the wild. They didn't have that paragraph until about ~2 days ago. I requested that they add PayPal as a method of donation because I didn't want to use patreon/become a monthly patron but did want to donate *something*. I then thought "this project has 11k stars on github, if I wanted to donate maybe there are others as well who didn't know they even took donations at all". Presto-posto
I want it now!
https://github.com/lexi-lambda/hackett
Point taken, but it's missing a few crucial features.
It's great in practice. You use a macro like [error_chain](https://rust-lang-nursery.github.io/error-chain/error_chain/index.html) to generate an `Error` type with `From` instances for all the errors your libraries might throw, and it Just Works™ :)
&gt; &gt; What's the difference between being "donation-supported" and having a paragraph in your README which says "You can help development by donating"? &gt; that's exactly What being supported by donation means...? I would have thought that being "donation-supported" at least implies that there's a demonstrable improvement in the software that's directly linked to receiving donations. I think suggesting donations for a project that probably 95% of readers here never use is a bit ill-calibrated. Here's a related idea that I think would be great: you could compile and curate a list of all popular Haskell projects that accept donations and link it here. That at least would have broader appeal.
&gt; have to define a whole new class In my experience, that only happens when wrapping C libraries. When creating new things, I pretty much always already have a struct/class.
Also, my hackles are raised by the word "completely" in title, as if if we don't donate then the project will fade away.
Those are really good tips, I'll have a read :)
Haskell actually has loads of reserved words. More than Java, for example.
I'm a bit confused by the amount of not helpful answers. Admitted there are also somewhat useful ones by now.. but my point is: Please don't consider concepts like "fmap", "Functor", or anything related to "type classes" before the Haskell basics are mastered. OP should look up about fundamental concepts of Haskell, or rather of "functional programming" in general, like in particular "algebraic datatypes" and "pattern matching". For a problem like the one in this post, I can only recommend to start your study of Haskell by following the very first steps of any structured Haskell tutorial. Unfortunately, Haskell and the underlying "functional programming" paradigm are a bit like maths in that some things can only be learned incrementally and concept built on top of each other, but also in that there is some really fundamental core language features (including the datatypes and pattern matching) that are best studied in their entirety, otherwise you won't really be able to use Haskell at all. Those are some reasons for why a tutorial might be particularly helpful for the beginning. Though, after learning a few foundations, the explore-and-play-around approach is totally valid. The next goals in particular might be: * understanding the basics of pattern matching, being able to use pattern matching with tuples, booleans and integers * learning what the Maybe type "means" * getting somewhat used to the syntax of using the Maybe type in types, expressions and definitions (i.e. in pattern matching) * getting a feeling for the "list" datatype by studying examples, and learning about datatype definitions and how "Maybe" is defined and how a "List" type could be defined (as well as in what sense the standard lists use custom syntax) * etc.. Maybe is actually a neat type in that it is about the simplest standard datatype in Haskell that does not come with any kind of special syntax (not counting do-expressions), while numbers, lists, strings, tuples and bools do (for bools there's if-then-else and guards), so learning how to use Maybe is a very useful step in learning the whole language.
Hey, /u/mavavilj, you seem to be learning Haskell for more than a month already without understanding how to use the most fundamental basics of the language correctly. As you might have noticed, in your questions in this subreddit you get mediocre answers at best, and a lot comments with "answers" that don't actually cover your actual problem stated. As an example take all the comments in this thread that mention things like "fmap" and "functor" when your actual problem is merely some lack of knowledge about basic pattern matching in Haskell. What I would recommend to you is, to start studying some structured tutorial to the language. I've heard [Learn You a Haskell for Great Good](http://learnyouahaskell.com/starting-out) might be a good one. (I personally learned some basics in direct dialogue with a class-mate that already knew the language and got a more firm introduction to the whole thing through a book called "Real World Haskell".) The auto-didactic approach you seem to follow has its problems since most resources are not really fit for complete beginners. When you know some basics, approaches like "implementing some bigger project and using documentation and online forums" might be adequate, but to start out, no-one is going to give you a personal tutorial lesson when you could just read the right book instead. Also Haskell and its use of the "functional programming" paradigm are a bit like maths in that learning it is somewhat incremental and many concepts depend on mastering some more basic things first. Thus, to really get into it as efficient as possible, try going through a tutorial from start to finish, step by step, in order, and make sure to experiment with every small thing until going to the next, to make sure you're able to use and reproduce on your own what you've learned. Then, when you still encounter problems, you have the additional benefit of being able to use accurate terminology or phrases from the tutorial when phrasing your questions, thus people will understand you and you'll get more useful answers.
I am trying to rewrite a function using guards, but for some reason I get an error: parse error on input `where'. Here is my code. Any advice? parseRe :: String -&gt; (AST, String) parseRe str | null rest = (ast, []) | head rest == '+' = (Plus ast ast2, rest2) | otherwise (ast, rest) where (ast, rest) = parseSq str (ast2, rest2) = parseRe (tail rest)
What I usually do to solve this is to give my function a concrete type and then it will tell me why that particular type won't work. primfacs ::Int -&gt; [[Int]] primfacs :: Double -&gt; [[Double]] I'm on my phone but it should give you an error that is like "No instance Floating Int from use of div", and vice versa for Double You may have to do the same to your divby function to fix it first if it's type is not what you expect 
&gt; I would have thought that being "donation-supported" at least implies that there's a demonstrable improvement in the software that's directly linked to receiving donations. I think suggesting donations for a project that probably 95% of readers here never use is a bit ill-calibrated. Ahh I see what you mean -- certainly postgREST is not the kind of tool most people who frequent this subreddit would use -- point taken. Oooh that's also a great idea, I will do that, hopefully I can get something put together by next week -- I wonder if I could automate it by pulling from hackage? Then maybe I could mix the github stars, hackage rating/download count to create some sort of ranking? Could make a fun weekend project (in haskell of course).
I like to show off how easy Persistent is to use to generate DB relationships and then instance for ToJSON/FromJSON.
Pattern matching.
https://github.com/lexi-lambda/hackett/issues/24
It’s a form of a ‘generator’. Generator is the term used to describe the reverse arrow used in list comprehensions. 
For what it’s worth, a *lot* is implemented. The big missing things are better libraries, a more performant implementation (likely involving getting rid of laziness), and type errors that don’t suck (likely involving the implementation of an actual kindchecker; Hackett is currently higher-kinded but dynamically-kinded). Sadly, these are all nontrivial, and I only work on Hackett in my spare time, which I don’t currently have very much of. Still, progress is slow but steady, and that’s what counts, so have patience!
&gt; The best way to contribute would probably be to learn Racket and syntax/parse and get digging! I can confirm that learning a little bit of `syntax/parse` usage can go a long way towards writing most macros in Racket. It can be daunting at first, but with a little help, you'll find it is actually quite intuitive.
&gt;The main source of performance penalty is actually network namespaced, those could be reused and sandboxing would be cheap. Just need to be done :) &amp;#x200B; Newer kernel became much faster doing this. One should probably re-evaluate this.
I still struggle to answer the question of “how can I help?” to this day. The problem is that most of the people asking me that question are competent Haskellers, but they aren’t Racketeers, or at least not Racketeers familiar with the details of the macro system. This is a problem, since most of Racket is *very* well-documented, but the macro system really isn’t, which is a shame, since I think the macro system is Racket’s most impressive, interesting feature. The basics are decently explained, but the really advanced stuff is not introduced well, and unfortunately, Hackett is built on a lot of that really advanced stuff. Therefore, when someone asks me “how can I help?”, I often feel like all I can say is “learn the Racket macro system, and then let’s talk”. Sadly, I can’t really point people to any resource I love to actually accomplish that. It’s sort of like the state of the Haskell landscape before *Haskell Programming from First Principles*: when people ask for resources to learn Haskell, we could suggest little things that might help with the basics (LYAH, CIS 194) but definitely aren’t a complete pedagogical package that will guide people to where they want to be. Someday, maybe I’ll try to write a book about this stuff, if no one else does. It’s an incredibly daunting task, though, and I have too many things on my plate as it is.
Yes, but it all depends on the candidate and their situation.
I'm studying Haskell, but I tend to forget small details such as this. I tried searching from google the answer to how the parametrizing of this works, but couldn't find an answer.
Maybe you could try to re-acquire the necessary details then and put some basic skills to use. As soon as you've *actually used* the language to archive something you'll perhaps have better retention and an even better understanding of it all. There's for example [this list of problems in the Haskell wiki](https://wiki.haskell.org/H-99:_Ninety-Nine_Haskell_Problems) (many of which can be pretty hard actually); those might help if you can't find any other goals of what to implement.
&gt; Hackett is built on a lot of that really advanced stuff. Just curious. How did you go about learning the Racket macro system in order to accomplish this? Scouring source? Tutoring from some Racket gurus?
The user account onboarding email process for the wiki and hackage are spam management measures.
*Lots* of experimentation over a very long time, as well as periodic reading and re-reading of the documentation, plus some posts to the mailing list here and there. The Racket macro system is not undocumented, far from it, the documentation is just very dense, and it works far better as a reference than a tutorial. As I learned more and more, though, it became easier to understand the dense prose, and to realize why certain features would be useful. There was a long “ramp-up” of sorts: I started by writing small, simple macros, then slowly built up to more complex things. This is a long process, though, and a self-directed one, in which I surely made many mistakes and went down many false paths. I found it interesting because I find macros and macro systems sort of *intrinsically* interesting, and I was writing practical Racket programs along the way. I think many people who want to contribute to Hackett do not want to go through that same slog, and I don’t blame them.
&gt; Any Haskell library authors or open-source application developers looking for a few PRs? Yes! We actually prepared announcement with call for participation! During first two weeks we resolved a lot of issues and merged a lot of PRs. But there's still a lot of work to do! * https://kowainik.github.io/posts/2018-10-01-hacktoberfest
This is a tool I wrote. Posting it here for anyone who'd be useful for! Feel free to share your thoughts on what it does/how it does it / what you think of the Haskell code itself!
Yes! Smos does, see [https://github.com/NorfairKing/smos/blob/development/TODO.smos](https://github.com/NorfairKing/smos/blob/development/TODO.smos) There's lots of low-hanging fruit there. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [NorfairKing/smos/.../**TODO.smos** (development → ba2e1fb)](https://github.com/NorfairKing/smos/blob/ba2e1fbe9f33a06b85d89ccff642e59a779b7f73/TODO.smos) ---- 
Both sound like good ideas!
Oh, sure. My comment was definitely not intended in a negative way. It's just a really exciting project and I hope it becomes a big success :).
No. Other systems with any reach don't do fine without an onboarding process (unless they have a \_ton\_ of automation and editors to catch spam after the fact). &amp;#x200B; We've tried to remove the onboarding process from the wiki, substituting various plugins, and every time, gotten hit by a wave of spam. When we reached out for advice, we were told by people who help manage other substantive wikis that they have also found no better solution than manual onboarding. &amp;#x200B; The fact that we have gotten spam on hackage, which cannot be hit by any standard scripts, but needs to be attacked by something custom-crafted, further indicates that our sites have enough "google juice" that we can't just rely on the good behavior of community members in the face of dedicated spammers from outside the community.
Alright, this is interesting! In a year or so we'll see how wrong I was.
which systems? 
I can't recall the last time I had to ask a human to approve my account. For instance, right now I registered on npmjs and [Crates.io](https://Crates.io) and they both only required me to click an email verification link. Just in case, by "manual onboarding process" I didn't mean that there are no humans involved in dealing with spam, just that you don't literally have to go and fill some form and then wait until somebody approves you.
This is really nice, I'm really looking forward to use it. However: &gt;Smos will be written and configured in Haskell, like Xmonad. I see the following disadvantages with this approach: * Reduced user base. Only a small fraction of the potential GTD users are Haskell programmer. * Inflexible. In the sense that I need to compile the program instead of just re-start it for configuration changes to take effect. Moreover, it is inconvenient and sometimes impossible (e.g. in restricted corporate/production environments) to have the GHC toolchain installed to be able to make changes to the configuration. I think it is fine to provide the software as a (Haskell) library so that *power users* can *extend* it, but for *configuration*, I would prefer if something simpler where used, like TOML (or dhall if more power is desired) that does not require development skills/toolchains to make changes to the configuration. &amp;#x200B;
(Author here) \&gt; I think it is fine to provide the software as a (Haskell) library so that *power users* can *extend* it, but for *configuration*, I would prefer if something simpler where used \[...\] This is a very good point. The fun part of the current approach is that we can have both. Making a build of \`smos\` that can deal with a configuration file (and making that build the default executable) allows for both types of users. (Starting with a config-only approach and retrofitting an xmonad-like configuration is much harder.) Long story short: \- 1. You're right. \- 2. What you describe is possible and not hard per-se. \- 3. I would like both options. P.S. If you \_really\_ want this to exist, you can fund its development: [https://www.patreon.com/NorfairKing](https://www.patreon.com/NorfairKing) 
&gt; Rewrite the frontend from a mess of templates to Vue.js (with TypeScript, of course), making editing easier in the process. Have you considered using Miso or Reflex-DOM instead ;)
Let see if I can remember them all. Haskell: module where import qualified as data type newtype deriving class instance let in if then else case of do Java: package import public private protected class interface extends implements volatile transient static final int char long void bool short double float byte try catch finally throws throw if else for while until break continue switch case return goto Nope. My Java list is longer than my Haskell list; I guess I must have missed some in one or both lists.
https://cliapp.store/ is a related project (a donation portal for CLI tools), maybe you could team up ?
Sounds like a "sigh, guess *I* need to write the tutorial" situation..
&gt; I ended up going with Vue because hiring Vue people is easier, the ecosystem is more mature, and I don't have to be afraid of random problems that can't be solved without significant expertise. That's reasonable, but also a little sad, the reason the ecosystem is less mature and that more expertise is needed to solve certain issues that might come up, is precisely because people use Vue.js and the like instead of giving it a try. Now I'm not saying you are in any way morally obligated to use a Haskell frontend framework of course. It's just that it would be nice if more people chose to do so, particularly those closer to the Haskell community. &gt; I don't have any good evidence that "smart" or "elegant" or "theory-heavy" or "expressive" languages are better than dumb ones when it comes to metrics that I actually care about – like productivity or maintainability. Strange to hear that from someone in the Haskell community. I have personally found myself significantly more productive with Haskell than in other language, and I have seen fewer bugs in projects I've done in Haskell. The code is more concise and quicker to write and more general, it is less likely to have bugs, and it performs pretty damn well (obviously outperforming any dynamically typed solutions). I have also found the code to be far more modular and extensible and maintainable. &gt; Some things (optional typing, maybe?) might be a win-win, but it doesn't mean that if you add even more typing, you'll have an even bigger win. You can obviously take typing too far, but I would absolutely argue that Haskell does not. Even for fairly quick scripting I have basically never found Haskell's type system getting in the way (i.e correct code that would work in a less strongly typed language that GHC rejects).
In addition to having "normal" reasons (hiring, etc) for not wanting to pick anything Haskell-like, I do also have deeper reasons – but I won't be able to articulate well right now. Just for the record, though, I can perfectly relate to the sadness that occurs when superior technology ends up being unused due to &lt;petty reasons&gt; (creating a vicious circle) and then nobody seems to be virtuous enough to break the circle.
[Hit me up!](https://github.com/aurapm/aura)
This is a bug-fix release addressing several issues (including some crashes) that have been reported against 8.4.3. With luck we will also see a 8.6.2 release in the coming days as well.
I think the title is wrong since the announcement leads to 8.4.4, but I am really happy and excited to see this bug fix release. Thank you very much Team GHC :)
Sigh, I could have sworn I fixed that title. Let's try this again. 
At some point I'd love to hear your deeper reasons. I have found front end dev in Haskell to be fantastic.
\o/
I've noticed some inconsistencies in online documentation. For example, if you look at [Lens (Hackage)](https://hackage.haskell.org/package/lens-4.17/docs/src/Control.Lens.html) the source is cross-linked whereas [Lucid (Hackage)](https://hackage.haskell.org/package/lucid-2.9.10/docs/src/Lucid.html) doesn't have that. Is Stack also using Haddock under the hood? Both [Lens (Stackage)](https://www.stackage.org/haddock/lts-12.13/lens-4.16.1/src/Control.Lens.html) and [Lucid (Stackage)](https://www.stackage.org/haddock/lts-12.13/lucid-2.9.10/src/Lucid.html) have cross-linked source. Is this a Haddock problem or Hackage problem?
Besides `DeriveFunctor`, which would actually be my first goto, you could do some refactoring so that there are fewer "top-level" cases: data ParseResult a = Complete Input a | Failed ParseFailure deriving (Show, Eq) data ParseFailure = UnexpectedToken Char | NoOrConditionMatch | UnexpectedEOF | Error String deriving (Show, Eq) instance Functor ParseResult where fmap f (Complete i x) = Complete i (f x) fmap _ (Failed f) = Failed f Basically, whenever you want to manipulate a number of things uniformly, you should consider a type that has exactly those things in it.
I hope this does well, thanks for the effort.
I wonder if this fixes the windows Access Violation issue many people are getting
What's the difference between this and yi? Nonetheless, very cool - I heavily use emacs / org-mode but wish it had types.
(" A secure, multilingual package manager for Arch Linux and the AUR. ")
Cool ! Thank you very much to all GHC contributors for this ^_^
Smos is not a text editor, for one.
Good opportunity to point out https://github.com/hasufell/ghcup which lets you manage/update GHC versions distro-agnostic.
No, that patch wasn't backported. It's only in 8.6+.
cool! it installed and launched successfully for me. some comments: (1) the help command, or actually the default prompt itself, should have more information. in particular examples, and a reminder about the incompatibilities between (say) Haskell98 and Frege. like I did: frege&gt; import Data.Char frege&gt; all isAscii "frege" and it errored with a type mismatch type error in expression "frege" type is: String expected: [Char] and I was confused until I remembered that Frege uses Java primitives. since the REPL already has a `:load` command, maybe you could even just print few phone-specific hello-world URLs, or ship a few short files with it to help the user get started in the Editor pane. fwiw, the code from http://mmhelloworld.github.io/blog/2013/07/10/frege-hello-java works just fine. (2) the console doesn't scroll to outout, so on the first command, I thought it was really slow or was hanging. (3) the screen was several characters too narrow for most lines, so the extra lines breaks made it hard to read (especially without color). (4) there's no separator (or color difference) between the prompt and the output, makes it slightly harder to click on it, and whatever you're typing into the prompt always looks subtly messier. (5) so, (1)-(4) are minor. the fact that this runs on a phone is incredible. Can you access Java (or Android) classes from it (like are they in scope? My urge with the app is to access some non-trivial phone-only functionality (ideally, something that doesn't require the user to accept additional permissions play install the app). it would feel much more real if I could, for example, call getCurrentBatteryLevel :: IO Int or getPhoneName :: IO String and perform pure-Haskell manipulations on the value. 
wow thanks, this is a great idea! 
Beginner here, doing the Haskell book right now and I don't understand GHCI. In this exercise : &gt; If the type of kessel is (Ord a, Num b) =&gt; a -&gt; b -&gt; a, then the &gt; type of &gt; kessel 1 (2 :: Integer) is &gt; a) (Num a, Ord a) =&gt; a &gt; b) Int &gt; c) a &gt; d) Num a =&gt; a &gt; e) Ord a =&gt; a &gt; f) Integer I would have said e), but GHCi tells me (Ord a, Num a) =&gt; a Where does the Num a constraint come from given it was constrained to b in the first place ?
Good point. I was going to use `Show` and `Read`, but was grasping for an example without that kind of obvious connection.
Thanks for telling me a patch exists. Do you have a link to more info?
Maybe crunchy data, pivotal, or another group that has postgres committers on staff might take interest in contributing commits to postgREST or promoting the project further.
https://ghc.haskell.org/trac/ghc/ticket/15154 The bug exists in every version of ghc before 8.6, but you normally would need a very large amount of stack space usage to trigger it. In this case it was triggered by another bug. 
A couple of comments 1. I'm not sure what's specific to *reverse* mode. Forward mode would be `x -&gt; (y, dx -&gt; dy)`, which also fits into this pattern. 2. The reverse mode maps between the cotagent spaces, so to preserve sanity we should probably write `x -&gt; (y, d_dx, d_dy)`.
It comes from 1, which has type Num a =&gt; a. Example: if the type of id is a -&gt; a, then id 1 has type Num a =&gt; a, even though there is no Num in the type of id.
https://www.haskell.org/ghc/download_ghc_8_4.4.html is currently returning a 500 for me...
It says that it "wraps around try.frege-lang.org website". 
I’m not sure what the whole up tack thing is. Can we literally type this?
Can I somehow import a function that I wrote in another script?
A good ol' parameterized data type seems appropriate for this: data MyType a b c = A a | B b | C c type Use1 = MyType () Int () type Use2 = MyType () () (Char, Double) type Use3 = MyType Int Int ()
I intended no subtletly. I was merely giving an unthinking stream of thought on where I’ve seen dx imply a covector before. The exterior derivative has very little bearing on anything I was discussing, but it is neat stuff if you want to check it out anyway. The notation d_dx feels like it implies a second derivative to me.
I don't mean you're being subtle. I mean correspondence between the notation of differential geometry and that of classical multivariable calculus is subtle. By "d_dx" I meant the tangent vector "d/dx". I'll have to think this through a bit more. 
Certainly! Create a \`Common.hs\` (or whatever) containing your function, and then in another script in the same directory add \`import Common\`.
Awesome, thank you!
&gt; So next time you’re debating your science fiction geek friends about time travel and the integrity of the space-time continuum, remember that there’s no paradox in time travel as long as the universe is a fixpoint of the function obtained by pulling the back references out as parameter. Thank you! Now I know that the universe can be bottom ;)
No, sorry. That's a mathematical symbol used to represent the result of a computation that never finishes. There are many ways to write Haskell expressions that have that value (that is, that never terminate if forced). Some of them are: `undefined`, or `let x = x in x`, or `error "Boom"`.
Nice. I can retire my alternative https://github.com/mitchellwrosen/ghc-switch. Would it be possible to get your project into the `haskell/` namespace on github?