I can certainly give you some concrete numbers. How useful any of these numbers are, well that's up to you to figure out: * Total amount of Haskell coders over the life of the project: 5. * Current amount of Haskell coders: 2. * Current total amount of lines: 23088 (according to `sloccount`). * 2015 page views so far: 12.4k * Registered users in the project's database: 8k We work via Trello, and I was hoping I could tell you the amount of cards we've archived, but unfortunately there doesn't seem to be anyway to pull out that information. We work in weekly sprints, and this sprint currently has 22 cards "in production" (slightly higher than normal, we had a bad week previously). The project is [Fynder](http://fynder.io) - you can see an example of (part of) our product there by hitting the "View Schedule" button half way down the page. Happy to give you more random numbers, if you have anything specific. Edit: [here's a Github graph of activity](http://i.imgur.com/u626zdg.png) Edit: Before I was hired to write Haskell professionally, I did my own little study about moving from Perl to Haskell. I [blogged about that, here](https://ocharles.org.uk/blog/posts/2013-07-26-a-comparison-between-perl-and-haskell.html).
[A rising tide](https://en.wikipedia.org/wiki/A_rising_tide_lifts_all_boats) [sinks ships](https://en.wikipedia.org/wiki/Loose_lips_sink_ships), or something like that.
I never knew this was valid Haskell syntax :)
I think that's actually existential - but because it occurs in both positive and negative positions, I can actually do useful things with it.
Well, if you turn up warnings in your GHC high enough you'll see what is happening. That where clause is shadowing the definition of `(*)` from `Prelude` [in the `Num` typeclass] with a locally defined `(*)` with only one clause. The local `(*)` only knows how to multiply 3 by 3, any other inputs will result in a pattern match failure and crash. Haskell does support defining operators infix, which is unexpected if you come from many other languages, but I'm sure feels totally natural if you come from maths or as a blank slate. Both the incomplete pattern matching and the shadowing are available as warnings from GHC.
&gt;I'm in the process of convincing IBM management to evaluate Haskell as a language for building IBM cloud services. If you need to “convince management” to “evaluate” a decades-old technology you've already lost the argument. It's not as if they have any “objective” evidence in favour of Java being a particularly good language for cloud services that you have to overcome. If they knew their job, the management would be *directing* you to investigate whether or not to support services written in haskell. I would.
You can also do things like `let 2 = 1` or `let 'x' = 'y'`, though the binding doesn't seem to have any effect on the program.
That’s because the binding is lazy. If you write `let !'x' = 'y'`, you’ll get an exception about non-exhaustive patterns.
A description of non-strict semantics which doesn't use the term CPO anywhere! Nice!
It's not even *ex falso* since they just straight shadow the operation we know of as multiplication. Instead, this is a case of alpha equivalence gone humanely wrong! :)
It's allowed because you gave an explicit type signature - if you remove it, GHC will complain that the type is ambiguous (because it can't pick a monomorphic type).
Sure, that's the real mechanics of it, but you can also read it as: Assuming 3 * 3 = 10, then (3 * 3) + ((3 * 3) + 1) = 21, with "*", "+", and "=" having their normal meaning. The shadowing is a much more sane situation, though. From 3 * 3 = 10, you can prove that there's only a single natural number, and that number is (equal to) 0.
With all it's problems, and all the beauty of Haskell, and as someone that nearly joined IBM 3 years ago, (I was the first employee hired by TGCS, which used to be IBM RSS), I'm going to say that with all the existing investment of IBM in the JVM as a platform you'd be better off going with Scala. You might even look at Idris for some specialized work, and you might look at Frege for green-field projects, but Scala will be every bit the workhorse Java is now and let you use higher kinded types and even dependent types when desired. It's not pure, but Java interop is not really nice in a pure language, IMO; large swaths of existing libraries just do not have a reasonable pure interface.
WHNF out of the blue, tho, and never even expanded -- despite a good preliminary explanation of normal form!
in my current work environment, i'm seeing a lot of receptivity from the java stack focused team members getting quite intrigued by haskell, by dint of getting exposed indirectly via a team member who understands haskell/fp very well. point being, you already have smart engineers. Just make sure you have a member of the team who can be their resource for learning and levelling up and such. Then just focus on shipping neat product with a team who's excited both to build a new product and learn a fun new tool along the way!
You can create a transparent reference to an arbitrary expression in D. It's just that `x` in your code is not that reference.
Isn't there a bit of difference? In Haxl, there is an interpreter function `runHaxl`, and from the look of the material much labor is expended to keep the user from constructing `Haxl` values that could permit the user to 'observe' the difference, as the docs say, between `runHaxl $ liftA2 ..` and `runHaxl $ liftM2 ...` . You can insert an arbitrary IO action with [`unsafeLiftIO :: IO a -&gt; GenHaxl u a`](http://hackage.haskell.org/package/haxl-0.1.0.0/docs/src/Haxl-Core-Monad.html#unsafeLiftIO) and various shenanigans are anticipated in the docs for the next function `unsafeToHaxlException`. But these are marked and frowned upon. It seems there is considerable labor to keep them from parting, except in irrelevant ways. This is thus a somewhat sophisticated version of the familiar case where everything is supposed to wash out in the interpreter, which 'quotients' the material, etc etc. `Concurrently` and `runConcurrently` don't seem anything like this, though maybe if I think more what you mean by 'degenerate case' I will see it. 
Yeah, but sadly OP needs things that business types will eat up. Fact/good research doesn't matter, just lots of people (better if they have reputation in a very successful and famous business) claiming that Haskell was the reason for decreased dev time, less bugs, and overall better bottom line.
Ermine is also a candidate for the JVM.
&gt; JavaScript/TypeScript with functional influences. Could you elaborate on that? That sounds interesting! &gt; The Haskell community is a place where I constantly feel like a beginner, this is a very good thing. I am constantly mind-blown by the language and the community. Granted, I am a freshman in College who had little exposure to FP but still, this is incredibly refreshing!
Hi Simon, As you know, at LumiGuide we're in the final weeks of a project where we developed a bicycle parking guidance system for the city of Nijmegen and the city of Utrecht in the Netherlands. The distributed system handles around 9000 parking places and is comprised of optical sensors, physical image analysis servers, lots of network equipment like cables, routers and modems, various displays deployed throughout the city and cloud-based management servers. Apart from the python-based image-analysis-server (which we are planning to rewrite in Haskell in the coming two months) everything is implemented using functional programming. Some facts: * Number of software engineers: 2 (we also do all the dev-ops / system administration work). * Project duration: 5 months. * 10k lines of Haskell, 1500 lines of Nix (also 6k lines of Python) (this doesn't include whitespace and comments). I expect that the number of Haskell lines will double in the coming three months. * Haskell is used in the browser: GHCJS is used for building and your blaze-react library as the GUI framework. * Haskell is used for the front- and backend of the central management cluster. * Nix is used for our build system. * NixOS is used as the OS for our development workstations as well as for all our physical and cloud-based servers. * nixops is used for deployment. 
If you like music you could do some real time midi synthesis. I like it because it shows another use case of reactive programming, not only the GUI one. [Reactive-balsa](https://hackage.haskell.org/package/reactive-balsa) lets you transform midi events from a keyboard or other input devices.
And the reason for the negative vote is?
Having go-to-definition links in the source code in HTML is something I am after. You might be interrested in this discussion: https://mail.haskell.org/pipermail/haskell-cafe/2014-November/117104.html Nemnem in Haskell: http://robinp.github.io/nemnem/ It's available in Agda: http://www.cse.chalmers.se/~nad/listings/lib-0.7/README.html In Coq: https://coq.inria.fr/distrib/current/stdlib/ 
How do I generate a transparent reference to the expression `double(2)` in the above code? How do I factor out an expression *in general* to a name and use that name transparently in place of that expression where-ever it is used?
Don't try reactive arrows without type checking at home kids, unless you are /u/cameleon 
I believe that Christian B did a wee bit of work towards making cabal sandboxes relocatable this fall. But I could be wrong. 
&gt; modern haskell In GHC (not **Haskell**), which doesn't have a JVM backend.
FranTk was one answer, but that's not purely functional. How are you doing it?
Let's analyze your `f` function. The `f :: Integer -&gt; Integer` says you receive an `Integer` and produces a result that is also `Integer`. By the other side, let us see the type annotation of `return`: `return :: Monad m =&gt; a -&gt; m a`. It says that it "packs" a generic type `a` in a monad `m`. So, that's why the compiler is complaining and /u/qZeta recomended you to drop all `return` and `do`. Concerning your second function, it has 5 arguments (one `IO ()` and four `Int`) and returns a `Double`. Now, let's look your definition: `ternarySearch f a b tau`, which has four arguments. The problem starts there. My advice is: stop thinking that Haskell is C. `return` has a meaning in C and another in Haskell. I also recommend you to watch the types of the elements you use. GHCi has two very useful commands for it: &gt; :t (/) (/) :: Fractional a =&gt; a -&gt; a -&gt; a &gt; :i (/) class Num a =&gt; Fractional a where (/) :: a -&gt; a -&gt; a ... -- Defined in `GHC.Real' infixl 7 / **EDIT**: [This section of the book "Real World Haskell"](http://book.realworldhaskell.org/read/using-typeclasses.html#typeclasses.wellknown.numeric) may be useful for your current task.
Thanks a lot. I did not know of this report. I think it is going to be helpful.
There's [halcyon](https://halcyon.sh/) tool that does something similar to what you want. It caches locally all compiled binaries, so next time you create a new sandbox is simply copies from cache. But yes, it would've been great if cabal acquired some features of nix / halcyon. 
Thanks for the feedback. That sounds like the kind of development/ops environment that I am aiming for ;-) Best of luck with LumiGuide!
I wrote a small command line tool to automate this to an extent: https://github.com/abhinav/haskell-sandman. It should be possible to add the package-subset functionality you mention in your blog to it since `InstalledPackageInfo` contains all the information we need about the package and its direct dependencies. 
They have: it's statically typed while I'm highly dynamic. I didn't mean to be patronizing. And it's true: I didn't read opening post carrefully. I'm sorry.
Oh, very nice! Must take a look at this. By the way, in your example there are a few references to `sandbox` (such as `sandbox new common`) where I think you meant `sandman`? (And yes, I would guess that extending it to deal with subsets should be easy enough.)
Oof. You're right. Fixed.
You are right to point out that Scala is another option, and possibly the more fruitful one. I'm still wondering how Scala and Haskell relate in practice, i.e., whether the mileage from using one or the other varies in large scale projects. In my opinion, the only real (but very big) reason for Scala is that it is compatible with the Java ecosystem in companies. However, in the long-term I want to use an language the supports *effective reasoning* about systems, ranging from informal reasoning to full formal verification. I'm not convinced that Scala makes a good candidate for pursuing this vision. In particular, the compatibility argument becomes moot, as it is exactly the stateful nature of Java code that makes it so incredibly hard to verify systems built on it. The goal of enabling effective reasoning about systems is what motivates me. See ["Out of the Tar Pit"](http://shaffner.us/cs/papers/tarpit.pdf) for a very clear explanation of the benefits of following this direction. I also think that the constant attack pressure on our systems should be reason enough for us to really start building systems that are as correct/secure as possible by construction. Security is crucial and *it cannot be tested*. In that regard, I'm very glad to see projects such as [DARPA's HACMS](http://bit.ly/1BntWqF) going for a clean-slate approach.
It's not at all dangerous. The monomorphism restriction is about efficency. The `enum` definition does not look like a function, so one might expect it to be evaluated only once. But at runtime it might actually be a function (of the dictionaries), so it will be evaluated at each use.
By overloaded I meant typeclass constraints. If you want the exact rules, read the Haskell report. :)
In general, I'm interested in all numbers that help understand the relation between different languages, and in particular the relation of Haskell to other languages. The paper &lt;http://macbeth.cs.ucdavis.edu/lang_study.pdf&gt; referenced by /u/bergmark is a good example of how such numbers can look. I was also hoping that there are some teams that have converted legacy codebases to Haskell. E.g., the Strats @ Standard Chartered team. There one could compare code size, the relation between bugs fixed and numbers of features developed, the relation of bugs reported vs. developers employed. Another interesting metric would be number of lines changes required per feature request, as this might be indicative of how expensive maintenance work is.
@ /u/ndmitchell could you perhaps give some numbers from the Standard Chartered experience?
How does it handle (the analogue of) the diamond problem? For example, if both `lens` and `classy-prelude` depend on `mtl` and you mix both of them in?
There's not really been much conversion of legacy code.
Well, as of right now it doesn't. When adding a package into the current sandbox, it'll skip the package if it or another version of it already exists in the sandbox.
My [cabbage](https://github.com/acowley/cabbage) tool does this. It uses Nix to hash things and maintain the store, but the package database for each project is assembled by pulling things in from the store. It works pretty well, but the use of Nix is a hurdle for new users. I think it should be rolled straight into Cabal, as it’s not actually a huge extension of anything.
Very slowly.
Thanks! I don't know how long I'll keep it up for (for now I'm just learning for the sake of learning), but I'd always appreciate any sort of feedback on what I'm doing right/horrendously wrong. So far, the community has seemed quite helpful and friendly, which is awesome and encouraging.
Thanks for the explanation. I never knew you could shadow functions in typeclasses. Sounds like a can of worms :D
OK, here's a bit of feedback for you then! * Regarding installing ghc-mod, it's certainly not you being stupid. Cabal's errors can often be rather hard to comprehend, especially before you have experience with it. I'm glad you got it working in the end! * Regarding Hoogle, I would suggest the [one hosted by FP Complete](https://www.fpcomplete.com/hoogle?q=Integer%20-%3E%20Integer%20-%3E%20Integer) instead of the one on haskell.org. For some reason the former searches more packages than the latter, and as you can see it throws up what you want for `Integer -&gt; Integer -&gt; Integer`. * Regarding `$` I would probably suggest you to avoid using it for now. It's not more complicated than anything else in Haskell, but it's not really fundamental so internalising when precisely to use it might take time best spent learning something that will pay more dividends. (Disclaimer: I almost never use `$`) * Regarding your `Functor` instance, you were very close! What you need is actually newtype BinaryFunction a b c = BinaryFunction (a -&gt; b -&gt; c) instance Functor (BinaryFunction a b) where fmap a (BinaryFunction b) = BinaryFunction (\x y -&gt; a (b x y)) Can you see why? Can you now implement the `Applicative` instance? (It does exist :)
It's [surpisingly common](https://www.haskell.org/hoogle/?hoogle=%3C%7C%3E). ;)
&gt; `[minBound .. maxBound]` I never understood why this is not a standard `Enum` function. 
You're right that `colors` can be defined more simply. If the Peg type derives Enum and Bounded then `colors` can be defined as `[minBound .. maxBound]`.
Because Bounded and Enum have no dependency on each other. This "function" (not really, because it is not a function but data) does not obviously belong in either place.
Ah yes, I was aware of `zipWith` but I didn't think to use it. In Racket, `map` is polyvariadic, so it's effectively `zipWith`, but I didn't really make that connection until you pointed it out. Thanks!
Perhaps you are better off working in threepenny-gui, it has frp and gui built in. It's by the same author as reactive-banana.
I'm still in the design phase, so I might still get surprises during the implementation, but my main idea is as follows. When using FRP with gloss, using [gloss-banana](http://hackage.haskell.org/package/gloss-banana) for example, there is a main driver function which provides FRP events and accepts an FRP behavior describing what to draw. Here is a simplified type for this driver function. playBanana :: (Event Keyboard -&gt; Event Mouse -&gt; Behavior Picture) -&gt; IO () As a user of gloss-banana, you are expected to write an FRP network which converts those events into the required `Behavior Picture`. I would like to have a similar API for GUI programs. playGui :: (Event GuiEvent -&gt; Behavior Gui) -&gt; IO () Taking [inspiration from React Native](http://www.reddit.com/r/haskell/comments/2wii6n/haskell_google_summer_of_code_proposal/cot75v9), this `Gui` type would describe a virtual "DOM" representing the GUI we want to build, and `playGui` would diff subsequent values of this type to decide which widgets to create, update and destroy. To make things simple, suppose our GUI library only supports buttons, whose only events are button clicks. It also only supports two kinds of container widgets, one for laying out the widgets horizontally, and one for laying them out vertically; to avoid complications which arise when using lists, let's pretend that each container widget only has room for at most two child widgets. I can represent any GUI from that library using the following type: data Gui = Empty | H Gui Gui -- Horizontally | V Gui Gui -- Vertically | Button String More interestingly, I can represent any GUI event from that library using the following type: data GuiEvent = LeftEvent GuiEvent | RightEvent GuiEvent | ButtonClick In a sense this type is too general, because it represents any event which could possibly occur in any app using that library, not just the events which could occur in one particular app. As a result, the exaustivity checker will not help us confirm that he are handling all the events of our app. But the gloss events suffer from the same problem and we don't care, we simply filter out the events which we don't care about in order to create custom FRP event streams which are specific to our app. We can do the same thing with GUI events. For example, here's a tiny two-buttons app which has a "hide" button which disappears when clicked, and a "reset" button which re-creates it if needed. -- reactive-banana utilities filterEq :: Eq a =&gt; a -&gt; Event a -&gt; Event () filterEq x = filterJust . fmap (\x' -&gt; if x == x' then Just () else Nothing) -- individual widgets and widget configurations hideButton :: Gui hideButton = Button "hide" resetButton :: Gui resetButton = Button "reset" initialGui :: Gui initialGui = hideButton `V` resetButton hiddenGui :: Gui hiddenGui = Empty `V` resetButton -- reactive-banana network mainGui :: Event GuiEvent -&gt; Behavior Gui mainGui guiEvent = gui where -- extract the events we care about hideEvent :: Event () hideEvent = filterEq (LeftEvent ButtonClick) resetEvent :: Event () resetEvent = filterEq (RightEvent ButtonClick) -- the behavior of our application in response to the above events gui :: Behavior Gui gui = stepper initialGui $ (hiddenGui &lt;$ hideEvent) `union` (initialGui &lt;$ resetEvent) main :: IO () main = playGui mainGui So, that's the kind FRP-friendly API for GUIs which I have in mind. I think I have a pretty good idea of how to implement this, but we'll see!
Ok. At least it's not going backwards ;)
&gt;It's not just state; things like toString() and getClass() allow you to violate parametricity, so you don't get any theorems for free. Sure but those are easier problems to solve than say hacking on a very new language still in flux (Idris) or having to do a lot of java/haskell interop. I've not done professional haskell development, so I can't compare to it, but restricting our team to (what we call) the "scalaz safe subset" of scala has had huge productivity gains over our .NET projects. 
Have you had many people applying?
I just set up the top-level snap package with git submodules. This makes it much easier for newcomers to get 1.0 up and running today. Just clone the repository, run the pull.sh script, and then do `cabal install`.
The baby step troubles with even getting a useful IDE setup reminds me of an article about the bazaar and the cathedral, and how the bazaar produces usable but hodge-podge solutions; in many respects I think PLT Scheme or the now Racket has always been a cathedral. It's a team of people working on one product which includes language, libraries, interpreter, editor in one cohesive package. Reading this is like my experience with Common Lisp and Scheme in reverse. I went through similar "oh, what *now*!?" friction (as I like to call it) when trying to learn Common Lisp. The setup is hard, the libraries are all over the place and Lispers are like cats, they don't herd. When I came to Scheme I chose PLT Scheme and was amazed that I just installed it and I could do sockets and threads and have an integrated IDE and render vector graphics trivially or pictures in the REPL. I immediately dropped Common Lisp for good. In the cathedral model you can make opinionated decisions and just say "this is how we do things" and integrate everything accordingly. When I work on Emacs's haskell-mode, almost every feature is optional, has to contend/compete with other ways of doing the same thing that users like to use, and it really stops you being able to just make it "work" the first time because there's too much darn variation and choice. What platform are you on, which compilation server thing are you using, what modes are enabled, regular haskell or literate haskell, oh you're using nix? okay, let's support that… hoogle doesn't work for you? Okay, how about opening the hackage page for docs, etc. You don't have a cabal file? Okay, then … Haskell and Common Lisp are the bazaar model. The infrastructure is made by people contributing things here and there by itches they scratch and things of interest to them. Of course, even Eli Barzilay, one of the foremost PLT contributors has said in the past that he doesn't use Scheme for real work and uses Common Lisp. While PLT/Racket has most of what you need, it doesn't have the critical mass or the performance and general investment of a "real world" language that's "going places" (yet?). But for a beginner language I would definitely recommend it over Python, JavaScript or Haskell because it has what beginners need: something that just *works*. Newbies don't really care about writing beautiful maintainable programs if they can't even do something immediately gratifying. 
The first is only a sugared version of the second in the absence of recursive bindings. In fact do let a = a + 1; print a means let a = a + 1 in print a and the binding for `a` is a recursive definition whose evaluation never terminates.
With point free code I personally tend to draw a line around: (f .) . g Looks like that is: (id .) . id :: (a -&gt; c) -&gt; a -&gt; c Right?
&gt;if not impossible, to establish the guarantees of parametricity and referential transparency from within the language itself. As it is with Haskell because of ⊥, but we pretend that doesn't exist and we can still exploit the benefits of parametricity just fine. Correct me if i'm wrong, but it's more realistic to look at equational reasoning as a spectrum rather than a boolean value. Sure Haskell will give us more guarantees than scala, but some effort can reduce the gap quite a bit. 
I would rather say that it is because `let` bindings are recursive in Haskell, while in other languages they are not (for example F#).
On day 2 you start playing around with wanting to make prettier code for \a b -&gt; f a b - g a b You can define an instance for `Num` that lifts numbers over function arguments: instance Num b =&gt; Num (a -&gt; b) where (-) = liftA2 (-) (+) = liftA2 (+) (*) = liftA2 (*) negate = fmap negate signum = fmap signum abs = fmap abs fromIntegral = return . fromInteger Now your definition \a b -&gt; f a b - g a b can be rewritten f - g because f - g = liftA2 (-) f g = \a -&gt; f a - g a Iterating that again you get `\a b -&gt; f a b - g a b`, which is what you wanted.
The only numbers I have are C# vs F# and there on two projects I've seen a 66% reduction in lines of code. We write in Haskell as a first preference so little conversion to Haskell. Comparing between teams is not that helpful. My experience is a there is more variability in team quality than language choice. 
That's kind of disgusting. Besides, adding something like that just to get *slightly* nicer syntax defeats the whole point of making it more readable.
Yep, I hear you loud and clear. I'm a longtime programmer, but I only discovered Racket relatively recently. At heart, however, I'm really a programming language nerd, and the Racket model of being something of a programming language toolkit has been immensely appealing. I've spent a lot of time playing with (and improving upon) Typed Racket. Macros are almost *too powerful* to be practical, and though I think Racket is actually by far the most promising Lisp I've seen with the power to possibly make it into the (relatively, as obscure languages go) mainstream, it definitely currently sits in its place as a research language and a teaching tool. I have plenty of qualms about Racket, but because it's Racket, I can usually just change what I want. That is both the solution and the problem. :) Anyway, I've always found Haskell's general approach to programming a very appealing one, and frankly, I'd be interested in stealing some of that functionality into a Racket-based language. Typed Racket is awesome and impressive, but it's a completely different model from Haskell's or ML's (of course, it's designed that way, but the point stands). From what I've done with it, I like Haskell a lot, but I'm so new to the language that my understanding of how it would scale to writing larger programs is simply nonexistent. I'm not sure if I'll get to that phase through this project or not, but if I do, I hope to write about it along the way!
Oh, don't get me wrong, I'm impressed it's possible, and I'm sure in certain circumstances it would be helpful. I just don't think doing all that is something I'd like to do for my tiny use-case. :p
Ryan Trinkle's [ghcjs-setup](https://github.com/ryantrinkle/ghcjs-setup) repository is a pretty straightforward way to get set up. It's simply: git clone https://github.com/ryantrinkle/ghcjs-setup cd ghcjs-setup ./ghcjs-setup
Hi, O anonymous presumed-student.
*ahem* closure under composition
This is L.C. in 331 Loving Haskell so far! I might have to start browsing this subreddit.
&gt; This is because of Haskell's referential transparency, which means you can swap a variable with its definition anywhere and it will evaluate to the same thing. Okay, I can see how that might motivate the implementation of "let" being the recursive one. However, we can still do this: do let a = 2 print a let a = 3 print a This doesn't quite break the referential-transparency rule, but it certainly seems to bend it, as the question of what is *the definition* of a, is seen to be not such a simple one.
Haskell. :)
You're basically right. If `var1` is bound in `expr1` (like `expr1 = f var`) then you can't translate the let-binding into a lambda binding. However, using `fix :: (a -&gt; a) -&gt; a` you could do: let var = f var in expr2 ===&gt; let var = fix f in expr2 ===&gt; (\var -&gt; expr2) (fix f) This still won't always translate that way: `fix` is less polymorphic than recursive let. Recursive let can also be generalized by the MonadFix typeclass, to give you recursive do-bindings, which are in fact sugar for the function `mfix :: (MonadFix m) =&gt; (a -&gt; m a) -&gt; m a`. [this link](https://downloads.haskell.org/~ghc/7.8.3/docs/html/users_guide/syntax-extns.html) discusses the algorithm in "7.3.11.1. Recursive binding groups" which I assume would be roughly equivalent if you did want to implement recursive-let in terms of `fix`.
[That's right](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html) 
F#
be careful... you might never leave..
Why disgusting?
Haskell, of course. Haskell has first-class procedures.
I'm just being (mostly) facetious. Carry on. ;)
OCaml would be a strong contender. 
I have been experimenting with **rust** for the past few days and so far I like it. It supports algebraic data types, type classes (they call them traits), and HM type inference. It also has a similar feeling of rigorousness where the compiler catches many mistakes at compile time.
I also have a dormant project called [category-syntax](https://github.com/gelisam/category-syntax#readme) with a similar goal, namely to use Template Haskell to create a special syntax for associative categories, cartesian categories, and so on. Does that look like what you were planning to build? By the way, the reason I'm using a monadic syntax instead of arrow syntax is simply that arrow syntax is not supported by TH yet.
let and lambda have different typing rules. Compare: Prelude&gt; let x = id in (x (), x "hi") ((),"hi") Prelude&gt; (\x -&gt; (x (), x "hi")) id &lt;interactive&gt;:3:17: Couldn't match expected type ‘()’ with actual type ‘[Char]’ In the first argument of ‘x’, namely ‘"hi"’ In the expression: x "hi" In the expression: (x (), x "hi") Let-bound things can be polymorphic, while the variable bound by a lambda is monomorphic. edit: I've updated the example. The old example was comparing `let x = (x / 3, div x 3)` with the lambda version, in case you want to replicate the confusion discussed below.
I am envious of your insightful gut.
I'm participating in the 7 Day Roguelike challenge this week, and building a complete roguelike game, &lt;http://joeyh.name/code/scroll&gt; Going pretty well!
I like C++, more so now after learning Haskell than I did before; I get really antsy writing template code without being able to constrain types though. I've occasionally used python and I really hate it. I feel like Wile E. Cyote, just keep going like nothing's wrong until I look down and it turns out that everything was terrible from the beggining. Also python developers are somehow *even worse* than Haskellers for naming things (from Flask import g).
Desugar, then alpha-convert one of the lambdas. That'll make it clear how you have two different "a"s there -- you are doing variable shadowing. You might even get a warning from GHC with -Wall.
Julia and F# are fantastic, and Python &amp; C# are okay.
On the one hand, that's entirely fair. On the other hand, making the first usage of WHNF a hyperlink back to somewhere that it's explained would still be useful for random internet folks who stumble across this first. "Part of a series" doesn't necessarily imply "you have to read the rest first," and the more it can be made otherwise, IMO the better! And, boldly hijacking your attention for an unrelated query: is `reactive-banana` likely to see official SDL2 bindings?
So here's where I disagree -- the platform, if we can get it working in a state that people are comfortable recommending it, should be a "batteries included" system as far as libraries, package management system (recall one key purpose of it was to just make sure ghc installs came with cabal binaries), etc. Which only leaves IDE out of the picture -- and that's because we sort of have a wealth of options, and the ones with better GHC integration have all been a bit fiddly to keep working, and its been improvements in the GHC api that have changed that. My advice to beginners tends to be -- don't worry about an editor, anything that lets you turn of tabs and only use spaces is ok. Some people like really fancied up setups, others are happy with just syntax coloring in vi or whatever. It would be nice to say "ok, you want an editor, just install this, done!" And maybe one day we will have such a "decent default" for beginners -- but the problem is that beginners don't like to feel like they're at the kiddie end of the pool -- they want to immediately start using whatever full-featured setup they think is suited for "real development" -- and I don't blame them. So imagine if there were _also_ nice well-supported eclipse and jetbeans modules for racket, and fancy emacs modes, and etc. At a certain point, beginners to racket wouldn't just use the standard editor, but instead they'd go off trying all these things and running into corner cases etc too :-) On the other hand, if we had a "default editor" suited both for beginners and at least _some_ serious developers, then I could imagine that getting some traction... (the problem being it would have to be _some_ editor to pry existing haskell devs away from emacs or vi, whatever our poison of choice).
I believe you can do {-# LANGUAGE StandaloneDeriving #-} deriving instance Enum Peg deriving instance Bounded Peg but I haven't tested this myself.
You can always add those instances yourself, though that would be a pain. You know what, I'm also going to show you how to write the macro in Haskell, because who doesn't like Template Haskell? https://gist.github.com/tbelaire/c46f407c9b13e555daa7 (This is silly, but it's a fun exercise in template Haskell.) I'll be happy to explain what's going on, or you can just poke at the code.
Oh my gosh, I wish I thought of that before writing all that Template Haskell code. Who am I kidding, any excuse to muck about with template haskell is fun. :)
Template Haskell is definitely on my radar for something to check out if I get more proficient in Haskell. Of course, I'd *really* like to see an S-expression-based Haskell, but that's a very different idea. :)
I'd say C++11, but after learning haskell, my style of C++11 follows a much more immutable style. I'm just glad for the days I don't have to see "On Error GoTo SomeLabel"
&gt; I’ll spare you the gory details, but to make a long story short, it worked! Relevant XKCD: http://xkcd.com/979/ "Figured it out for myself!" _Thread closed_
[Image](http://imgs.xkcd.com/comics/wisdom_of_the_ancients.png) **Title:** Wisdom of the Ancients **Title-text:** All long help threads should have a sticky globally-editable post at the top saying 'DEAR PEOPLE FROM THE FUTURE: Here's what we've figured out so far ...' [Comic Explanation](http://www.explainxkcd.com/wiki/index.php/979#Explanation) **Stats:** This comic has been referenced 559 times, representing 1.0108% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cpc85ej)
&gt; Moreover, it is pretty much the only instance that makes sense for that type. Pretty much? Genuinely curious here -- are there arguments for other instances? And if not, why is this beauty not in `base`? :D
"Further, the optimization is ad-hoc and each new optimization requires the addition of new constructors, as well as the updating of every primitive combinator to handle every combination of constructors." I think the idea I'm trying to go for would be to provide a framework for opportunistic normalization.
It'll be at least a week or two before I finish what I'm working on now, but my next project will benefit from this. I'll definitely be trying it out to see how well it does what I need. 
&gt; That's kind of disgusting. Well... coming from J, it looks pretty nice.
&gt; My gut tells me that this is because / probably performs non-integral division, and GHC doesn’t like that I’m throwing the precision away. There seem to be several points where your instincts lead you straight to the source of the problem. &gt; That doesn’t work. I’m not sure why, but I’m almost certain it has to do with order of operations. &gt; Nope, that still complains about types. Oh, this is where typeclasses come in, right? I seem to remember the relevant typeclass is called Eq, let’s see if I remember the syntax. This is impressive to me. I think I was quietly cheering at the order-of-operations part.
Ah, well, it helps to have done *some* Haskell before, even if it was a tiny amount. I don't think my intuition would have been that solid if it was my first time even looking at the language. :P The non-integral division was actually the most obvious, I thought, mostly because I'd already been wondering if I would need to round the result if it *weren't* integral when I was solving that problem.
I use J for fun. The more you play around and learn the primitives the easier it is to read other code. The primitives are terse already, and function-level programming makes it even terser without sacrificing readability (IMO). Hooks and trains are reminiscient of an implicit use of SKI style combinators.
It's handy to know the general form `instance (Num b, Applicative f) =&gt; Num (f b)`, as well as `instance (Monoid b, Applicative f) =&gt; Monoid (f b)`, which both have trivial definitions. Unfortunately we don't have either general form in practice because of different reasons, but in some cases both can be extremely helpful. For instance the case analysis in Fizzbuzz is made very elegant from the fact that functions that return strings are appendable, using the definition `f &lt;&gt; g = \x -&gt; f x &lt;&gt; g x` (or `liftA2 (&lt;&gt;)` for the general definition)
Or do let x = doSomethingPure ... Either way, `hlint` will [print a warning](http://lpaste.net/840391900006449152) if you use `x &lt;- return value`.
I'm pretty sure you can also give functions the Num instance based on Cont's applicative, though I doubt anyone would call it an obvious choice.
You can also build up functions as natural numbers, church style, or do several other things, the type above is the only one that is definable within Haskell 98 (once you remove Show/Eq as superclasses from Num)
But it's also minimal enough that you can easily write code you thought you could reason about but that just isn't valid code. And basically no type system. C seems to me like the most un-Haskelly of the low level languages.
I like C. Not many built in features, but many ways to use them and very barebones. Prettymuch every combination is straightforward and with unions you can achieve semi good static typing in most code.
You don't use the same method of reasoning about functional code for imperative code.
I've played around with [Rust](http://rust-lang.org) a bit and enjoyed it. Waiting for the language and library ecosystem to stabilize a bit before putting it to real use. I like the look of Red Hat's [Ceylon](http://ceylon-lang.org/) as a JVM language (it also compiles to JavaScript). It has a static type system with interesting features like [union and intersection types](http://ceylon-lang.org/documentation/tour/types/), but doesn't throw the whole kitchen sink in like Scala. [Interoperation](http://ceylon-lang.org/documentation/1.1/reference/interoperability/java-from-ceylon/) with the rest of the Java ecosystem is very smooth. I've heard good things about [F#](http://fsharp.org/). Still waiting for first-party Linux support!
&gt; NumInstances Handy, thanks!
C++ (11 and newer) and from the up-and-comers: Rust
Oh so many things... But I used to do things like a &gt;&gt;= b &gt;&gt;= (\x -&gt; return $ f x) Before learning about liftm... f `liftM` a &gt;&gt;= b
The most groundbreaking feature of Rust by far, in my opinion, is its support for linear resources. The fundamental idea that something exists not just in a classical, mathematically self-consistent sense, but in a constructive, intuitionist "this exists, there is one of it, and it is *here*" sense. The reason this is so powerful is because of how strongly it relates to the real world. In the real world, abstracting over resources to pretend they don't exist is creating an expensive fiction -- and it is fiction. In the real world, there is actually a location in memory, which holds some specific data, it is changeable, and copying is a non-trivial operation. Moreover, the typical haphazard, non-linear treatment of resources won't translate into quantum computing: quantum state is fundamentally not clonable, which in some sense means the universe has a linear type system and forces you to abide by it. Rust has a bunch of other things which are super convenient, like Cargo, but ultimately, the fundamental reason Rust is so good is its linear treatment of resources. Everything else is just... super nice to have, but not profound.
`concatMap` is not the same as `concat . map`. Due to laziness even the latter only requires one traversal of a list -- but in the course of the traversal it will produce intermediate values from the map to get fed into the concat. The former function _fuses_ the two passes, and so in fact does allocate less.
Well, that totally depends on the context. But according to [my StackOverflow profile](http://stackoverflow.com/users/1139697/zeta?tab=tags), I like C++, C and JavaScript. Lets talk about all three. **JavaScript** is a nice little language, if you restrict yourself to the small nice core. However, I've answered the following question: "Why is 1 + '1' = '11' and 1 - '1' = 0 in JavaScript (Coercion)?". Ugh. Also, different implementations (Spidermonkey, Trident, …) have different quirks, and sooner or later you give up and use a framework. Still, it's the only language that one can run in all browsers _natively_. Yes, one can compile to JavaScript, even in the browser, but those add an additional layer of potential failure. Then there's the beast **C**. I've wrote the code for my master's thesis in C, mainly because I used PETSc (which is awesome, btw) and MPI. C gives you control. There are no exceptions, no overloaded operators. You can write polymorphic (yes, you've read that correctly) object oriented code. Drivers and portable libraries are usually written in C. You're completely in charge. Ok, there's this weird coercion feature, `union`'s are usually used wrong, types are sometimes a little bit weird, and the preprocessor is often abused. Also, many C learners usually try to do the right thing, but end up with undefined behaviour. They don't notice it for a _looooooong_ time, until they need to present the code to their tutor. And then hell breaks loose\*. Now take all good parts. The speed, the control. You've got a hold of them? Great. Add RAII, exceptions, classes, objects, templates et voilà! You've got **C++**. Almost. Because the bad parts from C are still in there. Hidden, lurking in the shadows, waiting to strike. Also, the ~1000page standard is filled with the term "undefined behaviour". You thought it was easy to trigger that in C? Hah! There are at least two additional ways in C++ for every way to create undefined behaviour in C. However, things are slowly getting better. C++11 introduces type deduction mechanisms like `decltype` and `auto`, gives you even more control about moving (rvalue references) and it gives you _lambdas_. Yeah! C++14 gives you polymorphic lambdas, which are basically templated lambdas. That's cool as hell, but not yet supported in stable distributions on LTS systems. Template metaprogramming is a great way to lose time, although `constepxr` makes some things easier nowadays. Either way, what I like about all three languages is the maturity of their Frameworks. jQuery has been around for almost 10 years. PETSc has been around for at least 16 years, MPI for ~20 years. Boost is a beast on it's own, heavily maintained. Qt has been around for ~20 years. Also, whenever you encounter a problem, it's very likely that someone else had this particular problem and solved it. So, why do I use those languages? To be honest, I don't use JavaScript that much anymore. I used it for (a) browser game(s). Usually, I just use my JavaScript-Fu whenever a friend of mine wants to add a new effect or has problems on his/her page. I also don't use C, unless I write something _really_ close to hardware or in a embedded system. But it's part of C++, so I use at least a little bit. I mostly use C++ for work. Why don't I use Python, Ruby, or …? I've tried Python. I can _read_ Python. But I'm usually faster in C++ or Haskell if I need to get something done. Same holds for Ruby. Also, I don't like duck typing anymore. I used to like it when I started with PHP ten years ago. Other languages haven't been on my radar yet, but I'm usually quite fast in learning the basics, e.g. a friend of mine learns Rust currently, and whenever he has a question, he usually asks me, I have a look at the language specification and answer him. Ok, that's it. I really need to get back to work and stop writing essays on reddit. PS: I showed my employer Haskell's quickCheck yesterday. He really liked it. So we're probably going to use a hspec/quickcheck-inspired C++ framework for our unit tests. ----- \* It's funny to see students struggle – because it's always the same. It usually starts with "but it worked at my PC!", followed by "I use Windows at home", and then "what do you mean, I can't use `arr[N]` after `malloc(N * sizeof(*arr))`"?. Then I usually explain how to use `gdb` and `valgrind`. Fun times.
I haven't seen much of a typed approach to preventing XSS attacks, nor proper sanitization/sand boxing of output data. Compared to unityped AngularJs, I'm not sure the comparison is favorable. More research needed!
Yup, Rust for me too. The lifetimes stuff is brilliant.
It's hard to understand what you refer to given that most frameworks have no output sand boxing or sanitization.
You're going to have to tell us what exactly goes wrong when you try to `cabal install` it.
Just took a stab at it, it wants `base &lt; 4.7`.
There's nothing unsafe about removing the monomorphism restriction. It wasn't there from the start, and I think it will go away again in the future. 
Is there a good reference for how exactly to take advantage of fusion, i.e. in what cases the compiler will do it for you? For that matter, is there any sort of general overview on the types of optimisations GHC performs?
Fair enough. I have added a link at the mention of WHNF to the previous tutorial. :-) &gt; And, boldly hijacking your attention for an unrelated query: is reactive-banana likely to see official SDL2 bindings? Unlikely, I am afraid, because I don't use SDL. I think there are unofficial SDL bindings, though.
pylint helps enormously. Other than that, having a team that uses the sane types for things and documents well is very important. I do long for a type system some days though.
There's been a quite a few yes, the response has been great. We're making an effort to talk to everyone since we're going to be growing rapidly.
Well, yes you have a point. But I wouldn't recommend to any beginner to use Template Haskell to write algorithms the same way they would use macros to write algorithms in Lisp. In Lisp, everything in the source code becomes a list, and it is very handy to have macros for stitching lists together without requiring intermediate list-evaluation steps. But in Haskell since everything is a function except for data types and classes, you should probably only use Template Haskell for generating code from data type declarations or class declarations, like how the lens library creates lenses, or how some parsing libraries generate parsers from data type declarations.
For the tooling: I got myself a [free \(academic\) FP complete account](https://www.fpcomplete.com/business/blog/free-academic-accounts/). It's a hosted IDE for Haskell, running completely in the browser. A special version of hoogle is already integrated and they have a curated list of packages (no cabal hell). Never looked back. If you can constraint yourself to the packages they have and if you have always good internet connection, it's a really great experience. They integrate well with GitHub, so you are not locked in to their platform and can switch anytime. I didn't to Haskell for a while. To refresh my knowledge, I read (and bought) the [book on PureScript](https://leanpub.com/purescript/read) (FP language similar to Haskell). It's really well written, and I found it refreshing to get a different persepective on some of the strange parts of Haskell (especially record syntax and modular effect / IO monad system). The PureScript people have basically rethought Haskell (if to the better or to the worse, time will tell). And it's a really nice and welcoming community (of course, same is true with Haskell).
There's implementations for QuickCheck in C++: http://en.wikipedia.org/wiki/QuickCheck
&gt; get ye Flask &gt; You cannot get ye Flask
I really like the way Scala works here. Once you have [sbt](http://www.scala-sbt.org/) (Scala's not so simple build tool) installed, everything is fine. Just create an empty *build.sbt* file, add in the project name and the dependencies, and you are set. Just start *sbt*, it will resolve the dependencies, pull everything and build the project in a fresh, independent sandbox. In the background there is a lot of caching, but you don't have to bother with it, it's really completely transparent. When you want to run the program, run the tests, need a repl, watch for changes and then re-compile: sbt does all this for you. When you need an IDE: Just start IntelliJ, open the sbt file and you are set. It uses sbt in the background, so no need for some "double bookkeeping", generating project files or whatever. It's all handled by sbt. sbt can even pull sources and documentation for the dependencies and IntelliJ will use them. Of course it's not everything perfect there: Error markers in IntelliJ are not backed by sbt, so IntelliJ will predict errors in code that it will then compile fine. And extending sbt is still quite complicated (although the documentation gets better all the time). But it's a much better experience than with GHC / Cabal, especially for beginners.
The `HarmTrace` package doesn't even provide the correct versions of its own project's dependencies. It seems to require `HarmTrace.Base.MusicTime` which is no longer provided in the latest version of `HarmTrace-Base`. You could probably fiddle around with things and get it working. You'll have to consider whether it's worth your time given that it's an unmaintained project.
How you describe TH are how macros are supposed to be used in Lisps as well. (With the sole exception of branching, where you have to use macros in Lisps but can get away without TH in Haskell.) Another common misconception is that you are supposed to litter your Lisp programs with macros. Just like TH, they make the code harder to reason about and don't compose as well, and just like TH you should only use them when you have to because they unambiguously make the code a lot easier to deal with.
A nice trick. But seeing it reminded me about a line in a movie I have seen a long time ago: [“Let us redefine progress to mean that just because we can do a thing, it does not necessarily mean we must do that thing.” ](http://www.reellifewisdom.com/let_us_redefine_progress_to_mean_that_just_because_we_can_do_a_thing_it_does_not_necessarily_mean_we_must_do_that_t)
OK. Thanks for the information. That makes sense.
I sometimes use Haxe for games and such when I want cross platformness. But I miss immutability by default...
Haskell actually has something that is comparable to DrRacket, namely Kronos Haskell. I would even go so far as to say that the interaction model of Kronos Haskell/IHaskell/IPython Notebooks is superior to DrRacket. The concept of cells with self-contained units of code that can produce rich output is brilliant. I've been learning Haskell (again) for the past couple of weeks, and although I'm a longtime Emacs user, I haven't even started to setup my Emacs Haskell development environment (again). I installed Kronos Haskell, wrote some Haskell code to try it out and was instantly hooked. It's so much fun! IMHO IHaskell (packaged like Kronos Haskell) should be the top priority for everybody who wants to make Haskell more approachable to newcomers. And it should be one of the first suggested downloads on haskell.org. 
I thought typing really helped for sanitization issues. Unsanitized strings would simply be types as such, thereby needing explicit conversion to sanitized ones that can then be used further. It makes it a lot harder to forget sanitizing m.
A denotational semantics of spells!
 Prelude&gt; :i fmap class Functor (f :: * -&gt; *) where fmap :: (a -&gt; b) -&gt; f a -&gt; f b ... -- Defined in ‘GHC.Base’ Prelude&gt; import Control.Monad Prelude Control.Monad&gt; :i liftM liftM :: Monad m =&gt; (a1 -&gt; r) -&gt; m a1 -&gt; m r -- Defined in ‘Control.Monad’ Oh well now I feel like an idiot. Having said that, I think I got into that habbit from the other liftM's... it just made more sense in my head to use liftM for all of them, rather than fmap for the case with only one argument. So, perhaps I will ammend my initial posting to this. I used to do things like function = do x &lt;- a y &lt;- b z &lt;- c let val = f x y z return val Where as now I am more likely to write. function = f `liftM3` a b c
I'd really enjoy seeing if you could bolt a more robust type system with inference onto C, though I suspect that would end up being Nim.
Since this is so powerful it makes me wonder, whether this kind of type is a good basis for building games/game engines in general, with the less powerful ones being useful special cases. I dare to say that it could be magical. ;) Also I think I just gained a little bit more insight into continuations. Basically: Nicely done, may the continuation of your life be free of type errors.
I prefer Haskell as imperative language even without lenses. I write quite a bit of networking/runtime system/concurrent code, so it's basically lots of IO all over the place and programs really nicely. I think lots of people somehow get convinced that IO should be avoided at all cost, but very imperative and IO heavy coding in Haskell is really pleasant in my experience. You still get to have the type system, ADTs/pattern matching, and having first-class IO is also lovely.
I personally don't agree with exporting `get`/`set`, because it doubles the surface size of your API. This is noisy, leading to a bigger cognitive burden (I have to hold more symbols in my head), more typing, and harder to navigate documentation. If people really don't want to expose constructors and field accessors, then I'd suggest exporting lenses.
This seems very ugly. At work we have an annotation ABSTRACTCON you can attach to a data type which (to a first approximation) means you can't do anything which is likely to break if a new field is added to the record. That mostly enforces record updates and selector use, which will gives you a lot more freedom.
What is the difference between gl and OpenGLRaw?
There is more documentation than code right now, so don't put your expectations too high :)
The annotation approach sounds interesting. How does that work exactly?
I wasn't trying to say `get`/`set` in place of lenses; I think lenses would be a fine solution here. I just didn't want to add a significant cognitive load to the article by introducing (what some consider) a very advanced topic.
Make your spells a nonlinear Lie group defined by a high-dimensional tangent Lie algebra. Hard mode.
&gt; doubles the surface size of your API. This is noisy, leading to a bigger cognitive burden (I have to hold more symbols in my head), more typing, and harder to navigate documentation. Initially I agree with this sentiment, because I prefer to just run `C-c C-i` on `Settings` and I will see data Settings = Settings { apiKey :: Text , hostName :: Text } Pop up in my Emacs, then I can go ahead and construct this value. But upon reflection arguments based on *existing tooling* (e.g. Haddock and GHCi) can have counterarguments in terms of *improving* tooling. Suppose we improve tooling so that we can properly hoogle packages we're using. Now built ontop of that we make some convenience/enhancement to `:i` which displays e.g. data Settings -- Constructors mkSettings :: Text -&gt; Settings -- Setters and getters setHostName :: Text -&gt; Settings -&gt; Settings getHostName :: Settings -&gt; Text Alternatively if there were lenses: data Settings -- Constructors mkSettings :: Text -&gt; Settings -- Lenses hostname :: Lens' Settings Text I think that'd be a best of both worlds, it'd keep the data type resistant to change but also be convenient in terms of discoverability and documentation. Haddock could also be more clever about things like this.
This seems nice. Does it address this issue? &gt; The reason for this is that by exposing field accessors, users will be able to write code such as: &gt; &gt; (mkSettings myAPIKey) { hostName = "www.example.org" } &gt; &gt; This ties your hand for future internal improvements, since you are now required to keep a field of name hostName with type Text. By just using set and get functions, you can change your internal representation significantly and still provide a compatibility layer.
Parametricially polymorphic means that it works across many types (polymorphic) but behaving the same at all types (parametric). If you have a closed hierarchy of types, you can actually encode that as a massive sum type (if nothing else). So, a "polymorphic" function that ranges over just a closed hierarchy is really just a monomorphic function; even if it uses it's language's polymorphic systax. At the very least, you have to range over an open set of types (say, anything that has a certain typeclass instance, or an open inheritance hierarchy rootied at a certain type) before you are polymorphic, much less parametric.
Let me know when you write it! :)
The renaming of record selectors bit is subtle and important. If you don't deliberately alias the record selector then it maintains its "magic" even if it isn't exported as part of the data type. This can lead to Haddocks which look as though the record selector is merely a "getter", but it actually can be used to set
Hah! I think about the _main_ reason I comment on reddit is to slightly disagree with posts. So you're probably not alone in this treatment :-) I probably adopt this tic then even when I don't disagree so much, and just have related thoughts spawned by a post. In this case, my disagreement was narrowly with the idea that Haskell has a "oh, what now" problem in general -- rather, my point was that it really _only_ has one in the arena of IDE tooling as far as I'm concerned. Beyond that, yes, it was just my rambling. In any case, it is a tic of mine well noted, and I'll try to watch out for it, especially when I don't actually disagree that much :-P (edit: also, I realize, if I say "I disagree with X" the implication is that if you also said "Y and Z" I probably agree with them, and I am honing in on a nuance. If I begin with "I agree with Y but.." then the implication is more often that I _disagree_ with more. I can see how this doesn't necessarily get taken in the way I intend, however.)
&gt; When you want to run the program, run the tests, need a repl, watch for changes and then re-compile: sbt does all this for you. So versus cabal (and leaving aside the IDE issue) what do you think the key elements are here? Is the fact that you can use sbt interactively important? I.e. we can do `cabal test` and `cabal repl` now... Or is it that we would be served by having a `cabal incremental-make` mode or the like, and, I guess, a `cabal run` mode?
So, can I learn this in ... 2 days? ;)
This 7drl adventure has really helped me understand continuations better too. Before, they seemed pretty trivial in a haskell context as used in eg openFile. Now I think I get where the power can be. I need to sit back down and see if the Cont monad makes more sense..
The install is pretty hairy, first I had to patch it's cabal file with http://pastie.org/pastes/10021068/text I also had to install a couple of C library dependencies (package names for Fedora distro): gsl-devel, lapack-devel, blas-devel With that out of the way it fails with the following http://pastie.org/pastes/10021097/text 
`OpenGLRaw` 1.5 was limited to supporting OpenGL 3.2 and a handful of extensions. There was `OpenGLRawgen` at the time, but it spit out an `OpenGLRaw` that had subtle differences, so it mostly wasn't used. I needed to write a bunch of code that worked with OpenGL 4.1 rendering `OpenGLRaw` rather completely useless to me, so I took some code that Gabríel Arthúr Pétursson had written that autogenerated bindings based on the xml specification from Khronos, mashed it up with some ideas from `OpenGLRaw` and spat out `gl`, which binds all of OpenGL 1.0 - 4.5 as well as the mobile profiles, and all extensions that ever existed by every vendor. Along the way we went and figured out how to autogenerate some decent documentation. Since then, `gl` kickstarted development on `OpenGLRaw`, which adopted the same approach as `gl` to autogenerate the modules. Now that `OpenGLRaw` has adopted the `gl` approach the line is starting to blur. Differences at this time: * `gl` uses shorter module names, because otherwise we run into build problems on Windows. (Apparently having 780 module names that are all long kicks us over command line lengths on windows!) `OpenGLRaw` keeps the old names. * As far as I can tell, `gl` supports more extensions. This may just be that `OpenGLRaw` figures out how to shoehorn a couple hundred more of them into fewer modules. It may be that they have to support fewer modules to build on windows: I haven't checked. They build ~520 modules though. * `gl` is automatically lifted into `MonadIO`. This actually sounds like it'll change in the next release of `OpenGL` / `OpenGLRaw` in response to our work on common `StateVar` package for `quine`/`sdl2`/`OpenGL` based on my `foreign-var` package, which is now shipped as `StateVar` 1.1. * `gl` uses pattern synonyms or all the #define's in the `OpenGL` specification. This lets you use them in both argument and pattern position and ensures that all the names are googleable. This limits `gl` to modern GHCs (7.8+) but rather greatly improves the performance of pattern matching on GLenums and the convenience of writing code with them. So what is the difference? As you can see, as `OpenGLRaw` gradually copies all the things `gl` brought to the table, the differences are less and less prominent. At the time of the release of `gl` the differences were as between night and day.
I just got used to the compiler feedback in Haskell. When I make a change, there's nothing preventing me from doing stupid errors in Python. And I don't want to consult the docs (which I don't have) or write test cases to catch the stupid stuff. I guess, I got spoiled.
You can but that will obviously create orphan instances which come with their own set of problems.
At ~17:50 or so it switches from talking about the Maybe monad to Haskell (at thoughtbot) in general.
If anyone was interested in the *Maybe Haskell* book but was turned off by the price, or is just looking for a deal, they have a 50% off discount linked from the show notes.
What do you mean? I know what a Lie algebra is, but can't really make sense of your comment. You probably mean something like "implement a commutator function that given two spells x and y returns the 'difference' between casting x first and then y and casting y first and the n x as a new spell", but how do interpret antisymmetry and the Jacobi identity. I don't think there is an obvious way to add or subtract spells.
Not really. Usually there are only two "dots", but in the combinator there are three or them. How would you interpret the middle dot? On a second thought: Forget it, I think I really don't want to know that detail.
My first Haskell project is [public](https://github.com/stephen-smith/ai-contest-2010/blob/master/MyBot.hs), too. Yes, that's a 250 line Haskell function. I *hope* I can write things a little better now.
&gt; Just because Java does it doesn't mean it's wrong. Heresy! /s
Same. I at least like that lenses project in their types what's going on and strongly differentiate between getting and setting and setting/getting.
In the podcast they talk a little about swift and some of it's functional features. The give the example of [argo](https://github.com/thoughtbot/Argo) the swift json library modeled after Aeson. If you scroll through the readme on the github you can see their version of applicative functors.
No. But I consider changing something that existed to be far less common than adding something new. I'm also in the enviable position at work of having all the code available to modify, if the need arises.
&gt; I prefer Haskell as imperative language even without lenses. Same here. I'm working on a BitTorrent client in my free time, and apart from some purely-functional parser and message generator code, it's about modifying a dozen of MVars and managing sockets from different threads. It's still a lot better than what this program would be like in most other languages IMO. My biggest issue are 1) exceptions 2) laziness that leads to memory leaking code, it's hard to reason about laziness and fixing a leak is basically a trial-and-error process.
Do you have any good guides or deeper explanation of linear resources in Rust? Is there anything preventing us from implementing it in Haskell?
I'm not sure how I'd want to implement that sort of thing considering right now I'm just using Racket's Scribble tool to generate these posts. I guess if you wanted to, you could star the repository on GitHub and track updates there. https://github.com/lexi-lambda/learning-haskell
Lua because of LuaJIT: IIRC the most memory efficient and speedy JIT compiler for a high level scripting language out there. Lua's also pretty reliable in my experience and will run almost anywhere. It's not too popular but it seems like it's gaining popularity quite quickly and you can certainly "do stuff" with it, even though it wasn't designed to run standalone like (I think) Python and Ruby were. In terms of the actual language though, hard to say, maybe Go, Rust, or even Ruby.
Pretty much the only thing in Haskell I don't like about IO is the lack of a good LHS/RHS distinction, so if x and y are references you can't do `x = y + 1` with either lenses or mutable variables (though I found a terrible way to fake it with lenses). It's really such a tiny sacrifice in the grand scheme of things.
Hmm... so if we had "sandboxing by default" plus "shared sandboxing builds to reduce rebuilds" then that would solve the main issues you think? (again, IDE aside?)
Short answer seems to be that they're looking for clients to try out some Haskell projects. If anyone here has a need for some consultants you should see if they're game! (As a side note, I love how quickly they turned this stuff into a pitch: "We've got engineers who have deployed Haskell in production and one of them has even written a book!")
SDL, but not SDL2, and the SDL bindings (for Haskell, not r-b) have stale package dependencies. Didn't realize that the current SDL bindings weren't yours in the first place, though -- assumed they were official since they're listed on the `reactive-banana` home page, which was silly of me, especially considering it's a wiki page. :)
Well, for starters I realized I was actually thinking about the properties of Lie groups, not algebras, it might not matter though. It's not really relevant to the way OP has structured their system, but it's not hard to imagine constructing spells as a linear combination of some base spell components. You don't actually need to be able to add and subtract them per se, you just need some operation with the same properties. Likewise you can construct the bracket function in this way, you are not restricted by the thought of casting the spells individually, but the idea of combining them into a third spell. It's mostly just a goofy idea, it would have some benefit if you randomly generated spells: you could have different groups cast spells that are from different groups, and still get them to interact with each other properly since you can take the product of the two systems to get a new one which contains the spells from both.
?
I was about to say C++, but then I saw your comment and thought that you're right. C++11 changed the game for me. Otoh, I still don't understand why anyone would prefer C over C++.
So using incremental numbers for LTS between GHC releases will not work? At least if one was using LTS 3.x it would be understood that 3 means GHC 7.10 and the .x would indicate different LTS releases. No need to wait for GHC releases for .x LTS releases.
Scala allows a lot of evil things by default that you would need a lot of language extensions to do in Haskell. In my spare time I often do evil things, and I do that in Scala.
Rust and Scala
&gt; I'm also in the enviable position at work of having all the code available to modify, if the need arises. That's an important difference. We have the same: making a breaking change to an internal library has a much lower cost than breaking an open sourced library. Often you can just chance all the use sites, and the only cost is people having to know that this thing changed.
&gt; Of course, even Eli Barzilay, one of the foremost PLT contributors has said in the past that he doesn't use Scheme for real work and uses Common Lisp. This isn't accurate about Eli, who certainly used Racket for "real work" while working as a full-time member of the PLT team. He worked in Common Lisp a long time ago -- maybe that's what you're thinking. I think you're also wrong about the performance/investment/etc of Racket, but that is of course a more subjective topic.
link? That sounds awesome!
sometimes in my free time I fantasize about making a heroes three of might and Magic clone. I also thought about what a nice API for making spells, or even plugins generally, would look like. interactivity and persistence are awesome, I was just thinking about how to have different views into {{World -&gt; World}} to support constraining spells and analyzing spells and generating Arbitrary spells and crafting non-game-breaking spells in game with spell "combinators". (maybe with lens? I should leant more about lens). I like light table's use of global mutable state. you might want a "physics" that imposes some cost on different primitive spells (not necessarily mana), you have to watch out for possibly "recursive" spells like wishing for wishes, etc. maybe spells would need to be total functions of some... state? although, infinite combos can be fun, if they're not too easy. anyways, that's just my brain-vom. since I'm not git-annex-guy I don't have the skills to prototype it yet though ;)
I think you meant to reply to /u/edwardkmett.
This is interesting, because my system *does* use combos of spell components for casting the spells... See my other comment above.
Christiaanb here. Yes, Cabal 1.22 has preliminary support for relocatable packages, which is actually undocumented because I'm a bad patch writer. Regardless, this preliminary support for relocatable packages partially enables relocatable sandboxes. What's this partial part you might ask? Well, only the .cabal-sandbox directory is relocatable, not the sandbox config file. So basically the part that takes the longest time to build is relocatable. So, how do you go about the creating such a relocatable .cabal-sandbox directory? easy: * cd &lt;project_dir&gt; * cabal sandbox init * cabal install --dependencies-only --enable-relocable The created .cabal-sandbox directory is relocatable anywhere on the machine, and across machines if ghc is installed in the same directory. Even dynamically linked libraries will work on Linux and OS X because the libraries will use relative RPATHs. All that's left to having truly relocatable sandboxes, is to make the .config files also relocatable. And I should write some documentation for the next Cabal release.
I did!
His rising sea was an analogy about how doing foundational work and putting things into the right frame of reference eventually makes problems easy. "Learn to swim or you'll drown in abstractions" is definitely a corruption of that premise. ;)
Sorry, when I said component I meant like, smaller more basic spells. To form a Lie Group you would need your spell type to 1) form a group, and 2) also be a smooth manifold. Forming a group is pretty easy, you need a function `foo :: Spell -&gt; Spell -&gt; Spell` that accepts *any* two valid spells and produces a valid spell, is associative (`( a foo b ) foo c == a foo (b foo c)`), has exactly one spell that is an identity, and has an inverse for every spell (this is the harder bit). I'm not sure how you'd go about making your type also be a smooth manifold, I know basically nothing about topology.
Thanks for the response, I'll try porting my 3d vis library to gl and see how it goes. Have you considered merging the gl project with OpenGLRaw?
Maybe! I'm not familiar with quasiquoters; is it something which parses a string into a Haskell value of a particular type? Are there any disadvantages of using that over TH which I should know about? Using do notation instead of proc really isn't that bothersome at all.
I'm quoting what he wrote on a mailing list a while ago and I had archived it in a portion of my brain of notable opinions. Maybe he has changed his habits now.
I'm glad that the overlap isn't causing bad feelings. The gl -&gt; ghcjs -&gt; webgl pipeline would be incredibly useful as both a personal and evangelical tool.
Also, rust uses linear, or more accurately, affine types, for memory/resource management. Haskell has a garbage collector, so it doesn't need that (at least for the memory part).
&gt; "But duh, generics are complicated, so we might as well put unsafe casts everywhere." and dynamic typing, b/c with `interface{}` basically you are doing dynamic typing.
Oh I see a lot of that from beginners, yes. Lines and lines of redundant let statements in a do block when they could have used `fmap (this.that.something else.firstThing)`.
Perhaps one flavor of solution for this `Settings` problem is to exploit the fact that a product of monoids is also monoid. module MyAPI ( Settings , hostName , apiKey , makeAPICall ) where import Data.Monoid data Settings = Settings { _apiKey :: Last Text , _hostName :: Last Text } instance Monoid Settings where mempty = Settings mempty mempty Settings a b `mappend` Settings c d = Settings (a &lt;&gt; c) (b &lt;&gt; d) hostName :: Text -&gt; Settings hostName = Settings mempty . Last apiKey :: Text -&gt; Settings apiKey = flip Settings mempty . Last {- Example use: &gt;&gt;&gt; makeAPICall (hostName "www.example.com" &lt;&gt; apiKey "whatever") myFoo -} makeAPICall :: Settings -&gt; Foo -&gt; IO Bar makeAPICall settings foo = makeAPICall' internal foo where internal = applyDefaults settings makeAPICall' :: InternalSettings -&gt; Foo -&gt; IO Bar data InternalSettings = ... applyDefaults :: Settings -&gt; InternalSettings applyDefaults settings = ... An alternative encoding here would be to use `newType SettingsBuilder = SettingsBuilder (Endo Settings) deriving Monoid`—which is in fact isomorphic to the setter/lens solution (`Endo (setHostName "www.example.com") :: Endo Settings`). That's my general feeling about what OO folks call the "Builder Pattern" as well—it's products of monoids. (It would be good to have some sort of `DeriveMonoid` feature, BTW...)
C++, Python, and Rust. The first two do everything I could ever possibly need, and the third was because I wanted smarter C++.
Two things I've noticed developing Haskell (and software in general) professionally: 1. Libraries that hide their internals are ok until the moment you need to examine the internals for debugging or custom code purposes. Then they become untenable as upstream dependencies and have to be forked and hacked up in a suitable way. If the library goes very far out of its way to design an abstract interface, this can mean too much work and the library gets tossed. 2. It is ludicrous to bump a library version without considering the ramifications. Minor versions don't take too long to verify so long as they are non-breaking changes, but when you are developing with around one hundred or more dependencies, the odds of a major version bump not breaking *something* is extremely low, even if the code *compiles*. The cost of upgrading dependencies is already high enough that changing interfaces is a comparatively small penalty to pay. The benefits of a concrete interface, minus the cumbersome contortions of an abstract interface to be some compromise of flexibility makes it not worth it. Furthermore, on the upstream side, if the cost of breaking the interface is not outweighed by future benefits, why is the change even being made? I have wasted hours hacking up libraries that thought they were being clever by hiding their guts from me, and I have outright thrown out libraries and condemned them loudly and violently to my coworkers for being too clever to bother. If you must make a distinction to avoid the contempt of users who assume far too much about major version changes, at the very least expose your .Internal modules and a .Unsafe interface.
agreed on the .Internal modules -- I'm a big fan of abstractions, but it seems _nuts_ to me to actually prevent people who "promise" they know what they're doing from going in and messing around as need me. It also seems to be murphy's law that the things hidden in an "internal" module virtually never change, despite the effort to hide them, while the exposed "public" surface area ends up being a _lot_ more volatile -- sometimes in fact _because_ of the need to make sure it presents "enough" of the structure available from the internal module :-)
So you would say a function that is parameterized on a type, taking a sum type over it as an argument, would be monomorphic and not polymorphic? 
Yes, I loved that joke. Though I'm of course referring to more than just OverlappingInstances (or whichever extension it was - I don't remember) :)
Oh, wow, this is good news! Nice work! There's a bunch of open tickets about this; https://github.com/haskell/cabal/issues/1196 , https://github.com/haskell/cabal/issues/2302 , possibly others ; maybe you should comment on them :)
ghc will always use its built-in packages, such as base. To tell ghc to use a sandbox, you can use the package-db flag: ghc -package-db=.cabal-sandbox/*-packages.conf.d For some reason, some of the tools in the ecosystem use `-package-db` (with a single dash) while others use `--package-db` (with two dashes). To tell ghc to look at source files in a particular directory, use the `-i` flag. So for the current directory, that would be `-i.`. If you import a module called "Json.Parser", ghc will then look for it at `./Json/Parser.hs`. *edit*: I previously wrote "-packages-db" and "-I", which were both off by one character. Okay, one character out of one is pretty bad :)
That works well when every setting has a default value. But in many cases, they don't. With the `Monoid` approach here, I think you'd end up converting compile-time errors into runtime errors.
Sorry, not working: ghc -packages-db=.cabal-sandbox/x86_64-linux-ghc-7.8.3-packages.conf.d --make exampl.hs -o example &lt;command line&gt;: cannot satisfy -package s-db=.cabal-sandbox/x86_64-linux-ghc-7.8.3-packages.conf.d (use -v for more information) 
It's "package-db", not "packages-db".
&gt; mapAccumL Cool, haven't seen that one!
Odd that Scala already has disjunctive patterns but Haskell does not.
https://ro-che.info/articles/2014-03-05-cabal-sandbox-tips
That may be the first "real world" use of scan I've ever seen. Quite elegant :D
Funny that you mention: &gt; In that case the first error, was the real one, but I usually find that it's the last one which is "correct". My experience is sort of the opposite. I normally scroll up to look for the first error. Often, when I do something that doesn't type-check, I create two errors. One will be where I am trying to create a value and the other will be where I consume the value. (Sidenote: this is one of several reasons that top-level type annotations are a good idea). In your case, flipping the arguments looks like it put you in a similar situation: a type error for each of them. I will offer one piece of advice. When you get a super difficult type error, try adding type annotations to make things monomorphic. In your case, the function begins with: &gt; evalState 0 sorted (mapM ... And if you change that to &gt; evalState (0 :: Int) sorted (mapM ... I think that the type error would be a little more bearable. You get better at feeling the error messages over time though.
No. I'm saying that a function parameterized over a type, with a restriction that the type is from a closed inheritance hierarchy / type family is not really polymorphic, since it is equivalent to a a function which is not parameterized over a type, but *instead* operates of the sum type equivalent to the hierarchy / family.
I agree that the error messages you got are hard to comprehend. Having the compiler try all the argument permutations seems like a useful thing, at least for a small number of arguments. On the other hand, I think it would then have to present that information _in addition_ to the already long list of eye-glazing material, because maybe the problem wasn't actually argument order, even though one permutation seemed to work. Your chosen solution looks fine to me. It is very straightforward to read and understand. I would only do a cosmetic change by naming the state action in a where clause. Other than that, I think /u/Noack78 also has good solutions.
&gt; you can't do x = y + 1 with either lenses or mutable variables `x &lt;~ (+) &lt;$&gt; view y &lt;*&gt; pure 1`?
I think the argument permutation could be suggested in the same way that typo (for function not found) are.
This post is just the right size for introducing important themes but not drowning the reader. Well done!
Hi, the answer [turned into a blog post](http://semantic-domain.blogspot.co.uk/2015/03/abstract-binding-trees.html). 
Or be prepared to make your own ;) 
You can't ever pattern match on Void because it has no values. 
I'm glad to see more mentions of hpc. I think program coverage is a great metric for the quality of your test suite. Mutation testing ala [jester](http://jester.sourceforge.net/)/[MuCheck](https://hackage.haskell.org/package/MuCheck) is an even better test for your tests; but coverage is (often) easier to set up and is somewhat easier to explain. I also enjoyed the very practical QuickCheck techniques. I'm always hesitant to see IO injected into a QC Property, but that's probably just the remnants of how I initially thought of QC vs. unit testing -- though I think your use of IO could simply be a closed/pure usage of ST. The SmallInt/newtype technique is actually more useful with other types; Int8 / Word8 already have Arbitrary instances, so they can be used for small-ish values. I would have liked to see more about smallcheck, but it doesn't seems particularly well suited to the problem. Thanks for the submission!
Well I was using the Void type from the article newtype Void = Void Void but of course it doesn't matter whether its `f (Void a)` or `f a`. It's the same argument.
As someone who is interested in programming languages, I must admit that I hope you're right.
&gt; This two errors thing, is in my opinion, one of the major practical problem with type inference Type annotations should only be elided when they are obvious. Otherwise, they serve to document the intent of the programmer to both the compiler and future programmers; but they are better than simply comments since the compiler verifies them, which prevents "accuracy drift".
The ScopedTypeVariables extension is benign so you can activate it whenever the compiler recommends it. It is merely an extension because it was not around the last time a standard was created.
This particular definition of `Void` is misleading. It does have an inhabitant. For an uninhabited `Void` f a = 0 and f a = 1 are different functions. How can you distinguish them?
The point of this definition of `Void` is that there is no finite value with that type (because to build a `Void`, you need to provide `Void` first) -- but of course you can definite an infinite value void = Void void If you suspend your disbelief and admit that there is no value of type `Void`, then you cannot observe the difference between your two functions `f` and `g` above: to show that they are distinct, you would need to call them, and thus provide an argument (and we assumed you cannot). If you cannot observe that they are distinct, you may as well assume they are equal. This is the intuitive argument behind equating things at any type `Void -&gt; foo`. It also works in theory, sometimes (depending on how you formalize `Void`, equality proofs, and the equality theory you work with). Now there are two independent difficulties: 1. in Haskell, all types are inhabitated (by various forms of non-termination) so not all identities that hold in a pure setting are true in Haskell; you will not be able to work around the fact that you cannot define an empty type 2. the definition of `Void` given in the article is wrong in a language (pure or not) that allows recursive (coinductive) values A better definition would be data Void = Void (forall a . a) which allows to define absurd (Void x) = x and is correct even in a pure, strongly normalizing language. Finally, note that while the high-school algebraic identities hold in categories (and in their pure internal languages), some isomorphisms between types (or categorical equations) correspond to *invalid* numerical identities, such as (∀a, a × 0^a = 0). See the article "Remarks on Isomorphisms in Typed Lambda Calculi with Empty and Sum Types" by Marcelo Fiore, Roberto Di Cosmo and Vincent Balat, 2002.
`f` and `g` are equivalent to `absurd`, I think. Try passing each of them a value of `Void` and see if they produce different results. Interestingly, /u/edwardkmett's `void` package [implements `absurd` differently](http://hackage.haskell.org/package/void-0.7/docs/src/Data-Void.html#absurd); it forces its argument with `seq`, which I interpret as "calling your bluff": -- | Since 'Void' values logically don't exist, this witnesses the logical -- reasoning tool of \"ex falso quodlibet\". absurd :: Void -&gt; a absurd a = a `seq` spin a where spin (Void b) = spin b And I suspect the comment is related to your "mistake," but I'm not sure precisely how to link the two.
Ah! Gotcha. Thanks for the explanation I see what you are saying
&gt; When we started talking about Folders’ MVP, I knew right away what two pieces of our stack would be: Haskell, and Node.js. &gt; Of the two, Nodej.s is the more obvious choice — it’s still new(-ish), widely popular, and reuses tech skills between server and browser. Even Walmart has made waves by using it successfully during the Black Friday weekend, unquestionably the most critical sales period for a USA-centric retailer. Choosing it for Folders over alternatives was a no brainer in 2015. I pretty much spat out my coffee when I read this. Just goes to show—It's all about fashion and social proof. Right now Haskell is fashionable, and it may continue to be for a few years. But the fact that Haskell is enjoying some degree of (nascent) industry success at the moment clearly has less to do with its merits than its fashionability, which is a shame, since it really is quite practical for a lot of stuff... The person who used Ruby in 2011, Node in 2015 and Haskell in 2016 is not the ally you want! There are also a few other unfortunate statements in here: &gt; It was created by world-class language design and compiler research geeks busy pushing back the limits of knowledge about type theory. I agree with all of that except “pushing back the limits of knowledge about type theory”. Haskell was applying prior art in type theory and PL design to great effect, not pushing on any limits. Other parts are good though; for instance, I agree that the tech stack is pretty important. But I think having programmers who know how to design, build and maintain reliable cloud services is *way more important*, at least in my line of work. A type system can do pretty much anything that you design it to do, but you still have got to know a lot of other stuff besides type theory to run a web service properly. And also, there are indeed a lot more eager Haskell programmers than there are jobs too... But how many of them are "senior" level is more questionable. I think anyone who wants to take advantage of this talent pool has got to recognize from the start that training, mentoring etc are *crucial*. It's not like Java or C++ where you can throw a dart and hit a mature expert in that language; but if you're willing to put in the time and resources, you can probably build a great Haskell team.
I think you left out an “if” when asking what happens if those functions are distinct.
I haven't tried it with Haskell, but I prefer it to other editors for Javascript, Java, and Objective C.
I also don't think &gt;It was created by world-class language design and compiler research geeks busy pushing back the limits of knowledge about type theory. is accurate at all. A group of researchers are hardly just "geeks".
I tried it about a month ago. Showed promise, but did not seem like it was quite there yet. I went back to Emacs.
You are right, but can still write `f b a` instead of `f a b` and the compiler can still detect it by what I call *argument permutation*. The name is probably wrong but the problem (and its solution) still exits. ;-)
So, I didn't mean to imply that the technical differences don't matter at all. They most certainly do. But for now, I'm working off the assumption that the advantages of Yesod, Happstack and Snap are mostly the advantages of Haskell and static typing. They're great frameworks, don't get me wrong, but the kind of bugs that we're getting in the nodejs bits tend to be like those you mention: typos, "overly dynamic" APIs (my personal favourite are functions that behave totally differently if you pass them an array, the content of the array directly as arguments, and whether or not the 2nd argument is a presumably-callback function) and whole classes of bugs that just aren't possible with a compile-time static type checking pass. But right now, at this point in our development as a startup... the hiring bit trumps everything else. Once we start having a "storied pedigree" and all that stuff the usual Haskell goodness will start to have a huge impact on everyday productivity. No doubt you'll be more and more right as time passes. 
We're making the bet that the talent pool is not quite as shallow as others imply. The number of hackage packages and daily uploads does seem to indicate there are folks out there ;) As to making our own, sign me up. Suggestions?
I partially agree about "abusing the tuple", however the code was more to illustrate my point about the "swapping the arguments" problem. I only partially agree because your solution is also abusing tuples. Let's say I come up with the question saying, I have a pair `(Product, Int)` and I need to do *extend* it the with running balance , transformed as percentage so I get the following : T-Shirt, Red, 1, 20 , 20, 27 -- 20/75*100 T-Shirt, Blue, 2, 50, 70, 93 -- 70/75*100 Jumper, Red, 3, 5, 75, 100 -- 75/75*100 I'll get lots of answer giving me `[(Product, Int)] -&gt; [(Product, Int, Int)]`. But someone will come also and says, "hey, you are abusing tuples" , you shouldn't use `(Product, Int)` but instead introduce an new `Product'` class. My function becomes `[Product'] -&gt; [(Product', Int)]`. We now have the same problem with `(Product', Int)` and in fact I should have tree types `Product`, `Product'`, and `Product''`, etc ... There are lot of know problems with this approach. One is it's lots of boiler plate and another is record fields name clashes. The solution I know is to use `Hlist.Record` but I'm not sure we want to go that far. 
I feel like the "Use Haskell to attract skilled engineers" line isn't really helping here, since it leads to companies mostly hiring for experienced Haskellers, leaving little room for newcomers to become those experienced programmers.
There is also the [Language Extensions](http://www.stephendiehl.com/what/#language-extensions) chapter of "What I wish I knew when learning Haskell" that provides some basic guidance and of course the [Language Features](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ghc-language-features.html) part of the GHC Users Guide has something to say about each extension too.
GHC shouldn't have to "know" that they have the same instance. The problem with orphans is that which one gets used is unspecified and may result breaking invariants if two different ones are used on the same data; standalone deriving (I sincerely hope) is deterministic based only on the datatype definition, so two different derivings will always coincide.
Even if you use a weaker notion of equivalence than extensionality (my goto is operational equivalence as defined by "There exists no single holed context such that exactly one of the two expressions resulting from plugging the hole diverges.") the two `Void -&gt; Int` functions are equivalent, right? Also, IIRC, `reverse . reverse = id` is true in an eager language but false in a lazy one.
I find developers are in surplus now and the jobs are scarce.
Very true. `mapAccumL` is the unsung hero of list manipulation.
I wonder if someone could try to sell me on some of these testing frameworks? I've been perfectly happy writing things like testFoo x = unless (foo x == 42) $ error "Oops" It's easy to comment a few lines in your `testMain` to run some specific tests if you're working on a bug, etc. I suppose I sometimes start defining helper functions that start to do some of what `HUnit` is doing when I'm actively working on a bug, but beyond that I haven't had the desire for any framework libraries. Besides writing code that prints status dots to screen is one of the small pleasures that keeps me going.
&gt; Even if you use a weaker notion of equivalence than extensionality (my goto is operational equivalence as defined by "There exists no single holed context such that exactly one of the two expressions resulting from plugging the hole diverges.") the two Void -&gt; Int functions are equivalent, right? shc :: (Void -&gt; Int) -&gt; (Int, Int) shc f = 17 `divmod` (f (error "Won't happen")) f :: Void -&gt; Int f = const 1 g :: Void -&gt; Int g = const 0 main = do print $ shc f print $ shc g &gt; reverse . reverse = id is true in an eager language but false in a lazy one. That's beside the point, but yes-ish. Let's assume I'm talking about `Data.Text.reverse`. I don't want to say two functions are the same even if they are denotationally equivalent if they are not significantly similar operationally. This is a more precise version of "performance is a feature/requirement". Within a family of denotationally equivalent functions, I want to choose one of the Pareto frontier of resource efficiencies.
So quickcheck does all sorts of wonderful things that you should really try out, but in terms of plain old unit tests, it's really just about not duplicating common patterns. If you are only ever unit testing pure functions that have easy to construct arguments and return values, then what you describe is fine. But often times things get nastier... I can't speak for the rest, but hspec has really good support for defining pretty complicated setup / teardown, logging, etc. It lets me write stuff like https://github.com/dbp/hspec-snap/blob/master/spec/Main.hs#L143 , which I find convenient. Really, they are just libraries like any other. If your tests get complicated, you should factor code out. If the code is general, make a library so others can use it...
In real life (I mean production code), I end up with `Product''''''''''` :-). I need (in the real world) to transform csv and/or denormalize sql tables to be used then to generate reports or graphs (in Word or Excel). it's a common pattern to add each intermediate result in a new *column* and end up with 10 to 20 columns. For example I need to generate a report displaying for each customer it's top 5 products (including price, quantity, price, total amount etc ...). To do so in Words, you need a csv containing for each customer (a row) a column for each field to display. The word template will look like Customer : &lt;customer&gt; Rank | Item | quantity | price | total amount ----- +--------------+----------+-----------+-------------- 1 | &lt;product1&gt; | &lt;qty1&gt; | &lt;price1&gt; | &lt;total1&gt; 2 | &lt;product2&gt; | &lt;qty2&gt; | &lt;price2&gt; | &lt;total2&gt; ... 5 | &lt;product5&gt; | &lt;qty5&gt; | &lt;price5&gt; | &lt;total5&gt; You'll end up with a csv containing customer, product1, qty1, price1, total1, product2, ..., product5, qty5, ..., total5 It's a really simple report and already 26 columns ! I have tons of such reports written in R (that kind of stuff is a breeze to write in R) and I'm writting new ones in Haskell, but I'm struggling, because that type of tasks (and everything related to R dataframe) is not considered 'real world' problem by the Haskell communautity. 
I know that [Edsko de Vries](https://github.com/edsko) is using Atom for Haskell. Has a couple projects on there related to it.
Well, it seems that with the particular project I tested this on it didn't work in that it could not find a number of libraries even though those libraries definitely were in the sandbox. On a newly created sandbox for new minimal project it did work. Now I'm trying to find out what's different. 
A lot of people, me included, take the term geek as either neutral or positive. Not negative. I very much doubt the author meant it in a derogatory fashion.
Nah, it's a blogger-edit-link. OP has posted the correct one here: http://www.reddit.com/r/haskell/comments/2yy9gv/haskell_inverse_functions_friday_13th_sorry/
 filmToString :: Film -&gt; String filmToString (title, cast, year, fans) = printf title cast year fans
&gt; the person who used Ruby in 2011, Node in 2015 and Haskell in 2016 is not the ally you want! If something becomes fashionable and then people adopt it and it also turns out to be good, then they might not leave it again! I'll take allies and users where I can find them :-) now, vis a vis &gt; I agree with all of that except “pushing back the limits of knowledge about type theory”. Haskell was applying prior art in type theory and PL design to great effect, not pushing on any limits. I think you are right and wrong here in various degrees. It really depends what you think of as "type theory" -- if it really is about only core formalisms, then sure, Haskell began with some _very_ established knowledge about type theory. (And the main contributions of GHC were initially very much about compiler design, the g-machine, efficient optimizations, etc). However, this _does_ miss the unique contributions to type systems that have been introduced via research on Haskell -- starting with type classes, proceeding through MPTCs and Type Families, and then their interaction with GADTs and the interplay of various tradeoffs in inference to be made as that design space is explored. So there may be more "advanced" type systems vis a vis this or that feature, and they may be more carefully understood, and most type system features originated elsewhere, but in pushing for a "production ready" assemblage of a _lot_ of interacting type features, GHC research really _has_ involved pushing on some places in type-theoryland. One case in point is the development of system FC (http://research.microsoft.com/en-us/um/people/simonpj/papers/ext-f/) -- a messy process, to be sure, but certainly real research pushing elements of type systems to new places.
Training your own is a big time investment, but it's the model we chose and it's working pretty well so far. Make good use of interns and part time workers by breaking up self contained tasks with well defined input/output (think of them like forked threads: communication is expensive so you can't afford to micromanage). University and even motivated high school students are a great untapped pool because they care most about gaining real experience without the immediate need for high salary. For example, writing tests, designing web front ends, and generating documentation are all perfectly suited to "non-experts", and make great learning experiences for them. To pull it off, you have to think of yourself as providing a service to them just as much as they are to you, otherwise it's just another startup taking advantage of bright-eyed amateurs. 
How? They return different results.
&gt; It's that I don't consider what we've been discussing here a "real world" solution. What we've been chatting about here is suitable for a one-and-done quick stab at data manipulation, but completely unsuitable for production-grade data manipulation. You seem to oppose "real world" to "one and done quick stab". Unfornutately, not every company have the time nor the need for "production grade" solution. Sometimes, I need to generate a report for "tomorrow", the code is disposable and I probably shouldn't be using Haskell but I do. &gt; The question you posted generated responses about elementary Haskell list manipulation because it looked like you were asking more about that, which is a common sort of question posted here, than the problem you mention now (which is news to me, I mean, I get the connection but your initial post doesn't make your higher level concern clear). My question was about elementary Haskell. I was interested in the general problem : "how to use map and accumulator at the same time ?", but the main question was about GHC checking for argument order. &gt;Now that you've alluded to it, it's even more clear that the answers you got in response to this post are not that useful to your core problem. I got the answer to my question which is `mapAccumL`. &gt;Seriously, I strongly encourage you to post a somewhat higher level question about your direct problem I will, but I don't know how to start. 
Whether it's positive or not, a geek is a mere enthusiast. I would be a haskell geek by that definition. A type theory or programming languages researcher would not.
LOL. When I saw the title of the thread, I was wondering "what the heck is STring. Some sort of state monad transformer applied to an algebraic ring structure?"
Ah. But they don't. For neither of them can be called. This is technically not true in Haskell because Haskell is shitty as a logic. (It's inconsistent). To make it true, you have to do some careful reinterpretation of all the terms. You have to talk about total functions and you have to ignore bottom-valued elements. I'm not sure what definition of Void this article uses. But ideally, there ought to be no way to construct a "fully-defined" element of Void. In Haskell, that means any object of type Void must has some sub-component that is bottom-valued.
The one used is the one in the article. newtype Void = Void Void and it doesn't have to have a non-bottom denotation to be passed to each of these function.
Ah. Good point. Again, Haskell is a terrible logic.
hypothesis: Haskell is a less terrible relevant logic (I would love to see somebody push that idea into a genuine result :-) )
Not really a good idea, for many of the same reasons orphan instances aren't a good idea. A type class instance isn't supposed to be *just* an interface, but rather *the* only way for a type to implement those methods; coherence is the only thing that keeps `Set` working, for example. With first-class functions, you don't need a separate language syntax to have *just* an interface; you just have a (possibly polymorphic) record type with all the functions and nothing else. A implements "interface" B is simply witnessed by a value of type "A -&gt; B" -- so you can "mixin" whatever you need. If you are willing to use a GHC extension, you can even make that parameter implicit. I do think Haskell could benefit from structurally-typed (anonymous) record types, which could make static duck-typing easier.
The problem here is that `error` isn't a real function in terms of the category theory; it inhabits the type `String -&gt; 0` = 0^String which should equal 0 and be uninhabited. If you step away from Haskell and into some strongly normalizing language, then there isn't a function `error` and the identity holds.. To look at the math again, if you allow divergent terms, then all types with divergent terms need `_|_` added to them, and `Void` isn't `0` but rather `1` (the divergent term), and `()` is really `2` (`()` and `_|_`), and then you need to talk about the [lattice of bottoms](http://stackoverflow.com/questions/6379458/the-concept-of-bottom-in-haskell) and continuity of functions which gets even more complicated.
&gt; A type class instance isn't supposed to be just an interface, but rather the only way for a type to implement those methods Is this a property of Haskell or of type classes in general? I know Idris allows for named instances, so a type can implement a type class in multiple ways, and specify which instance to use at the call site. &gt;coherence is the only thing that keeps Set working, for example. Couldn't set be implemented like [this](https://ocaml.janestreet.com/ocaml-core/111.28.00/doc/core/#Core_set), with the comparison function being part of the type? &gt;With first-class functions, you don't need a separate language syntax to have just an interface; you just have a (possibly polymorphic) record type with all the functions and nothing else. Could you expand on this with an example? It'd be great to be able to do something like that in Rust, C# or F#, although I suspect the lack of HKT could be an issue.
The Mercury compiler, when it reports that e.g. the actual type of argument X in a call is foo while the expected type is bar, it also reports that foo is the expected type of arguments Y and Z. If argument Z also has a type error, with actual type bar but expected type foo, the user should not have too hard a time figuring out the unintended argument swap. This fixes the problem better than having the compiler print out all the possible permutations, because there could be *many* such permutations if more than one argument has been misplaced. And modifying the compiler to print this bit of extra information wasn't hard either.
You might want to try installing [web-mode](http://web-mode.org/) for emacs, the defaults are much nicer. 
Hey, I just noticed that all your posts have currently either 0 or 1 karma. That's quite bad and means that the community doesn't think of your posts as constructive contribution. There have been some comments regarding this: &gt; I never comment on text on websites but seriously: what is up with that text? I started laughing too hard to read. -- http://www.reddit.com/r/haskell/comments/2yhu3b/haskell_equational_reasoning/cp9wo60 &gt; I seriously cannot deal with it. Every time I click a link on that site I don't even bother checking the content. -- http://www.reddit.com/r/haskell/comments/2yhu3b/haskell_equational_reasoning/cpaapaa And that infamous one, which I won't quote here. However, all three comments have a common message: your design/style/colors make your posts hard to read. Try to use less colors, `&lt;pre&gt;&lt;code&gt;...&lt;/code&gt;&lt;/pre&gt;` for code (which you can style with CSS or external highlighters) and don't use `&lt;b&gt;`, except for important things (even then, `&lt;em&gt;` or `&lt;strong&gt;` are usually better, since they are semantic tags). Also, you should proofread your posts, or let someone proofread your posts. The very first sentence contains two superfluous commas. I'm bad with commas, myself, but I try to fix, that. Additionally, you can use MathJax nice math rendering. All in all, work on your design and your writing style, then you should get more appreciation from the community. (By the way, why do you post articles from early 2014? Was there some event and they're relevant to that event?)
 &gt;As to making our own, sign me up. Suggestions? Depending on your gender: either an adult consenting man or an adult consenting woman and a lot of time.
You can always try to outsource parts of the job. But not the core business. Outsource the creation of tools that make your small core staff more productive. So basically instead of getting more people in the core business, you invest in increasing the "impact factor" of the people you already have. Log file parsers, collaborative tools, a fully automated integration environment, a graphical profiler, there are lots of possibilities there.
&gt; In Haskell, types can't be parameterized based on terms. Will Backpack change this? It's inspired by OCaml's modules.
&gt;This would be a kind of duck typing: if it walks like a Monad and quacks like a Monad, it's a Monad. If it's a Monad then why not declaring it as a Monad ? My major concern with duck typing is, it hides an implicit interface and I'm not how hiding it is good. You have things which quacks, other which walks and some how do both. We can call them quackers, walkers and qwalkers ... If I call quack on the argument of a function (and expect it to quack), I am implicitely expected it to be a quacker. I should probably write this information somewhere (at least in the documentation). In Haskell I can add to my type signature `Quacker a =&gt; ...`. It's a way of documenting it, AND the compiler can check (meaning the documentation is defactor in sync with the code). That's one of the reason I like and use Haskell. In the opposite way, if I create an object/structure which can quack, what's the problem of defining it as a `Quacker` ? I understand that with languages like C++/Java where inheritance is part of the definition of a class and can't be modified afteward it is a problem. But with Haskell where class instanciation be added on demand, I don't what are the benefits of not instanciating a class where needed. 
Reddit does have an API but it's large and doesn't have a tutorial and I have no experience doing anything like this.
If you go the json route you get to use the wonderful Aeson library, which makes it really convenient to deserialised the json into native Haskell data types. It seems like I decided then that `http-client-tls` and `http-streams` was the easiest way to get the HTTP going, with `hoauth2` for the authentication. I have a kind of lengthy and absolutely nowhere near finished text on this. If you want to read it, hit me up with your email. I don't want to publish it officially because, well, it's not up to my standards at all.
/u/intolerable-bot was written in haskell, and it's [on github](https://github.com/intolerable/intolerable-bot).
Check out Vinyl for now. There's another library, Frames, which is still in the works I think. It's based on Vinyl and provides a Haskell data.frame type.
Another question to think about if you want to improve the quality of your posts: who is your target audience? You are using a very didactic style, as if you were taking somebody by the hand and showing them something new. What does this hypothetical person know already? Which new things are you teaching them? Somebody who needs to be explained such basic things as what is a multiplicative inverse is unlikely to be able to understand your final section, in which you state, without further explanations, that "if f(x) is a differentiable function, and f'(x) is continuous, and f'(a) /= 0, then [...] d/dy f^-1 f(y) = 1/(f'(f^-1 (y)))". In which, by the way, you are using the f^-1 notation to mean something completely different from the x^-1 notation for the multiplicative inverse you have introduced earlier. Perhaps it would be more appropriate for you to target your peers? You could write a post explaining how you have just learned something new, perhaps in the style of an [anti-tutorial](http://gelisam.blogspot.ca/2015/01/haxl-anti-tutorial.html). I'm still hoping this term will catch on :) Yet another question: what is the main point of your post? A large fraction of your post is spent talking about the derivative of a function. However, you never explain how derivatives are related to inverse functions, except perhaps in that cryptic unexplained theorem at the end. If that theorem is your main point, then perhaps you should refer to it at the beginning of your post, and then introduce each topic in relation to the theorem, and finally tie everything together by using what you have introduced in the previous sections to describe the theorem in more details. With the theorem just tacked on unexplained like this, it feels like an unimportant appendix which most readers will probably gloss over without reading. Finally, I'd like to end on a positive note. By continuing to post here despite the downvotes and the negativity, you have shown that you are very persistent. That's very good, because it takes a lot of practice to learn to write clearly. Richard Dawkins and Simon Peyton Jones are my role models, precisely because they are so talented at expressing themselves clearly.
I would like to congratulate you for the quality of your problem exposition. It's rare to have end-users so thoroughly explain their need, in a way that makes it easy to check whether a proposed solution fits the deal. Having a discussion of related work on top of that is excellent. If there was a "best question" award, you would deserve it.
its important to emphasize its vinyl&gt;0.5 in particular. https://hackage.haskell.org/package/vinyl link to frames github https://github.com/acowley/Frames (i've been meaning to play with it myself, but life gets busy sometimes) the nice thing about the design in modern vinyl is that the core idea is simple enough that you can EASILY write your own customized version within your application or use case. Even u/jonsterling agrees on that front. I personally have some ideas for what might be a "post-modern" vinyl-like where more type inference can flow backwards, but thats sometime I'll talk about another day
&gt; Lose the type safety Hmmm, if that's a choice then Haskell becomes less compelling
You should try the `Frames` code -- I hear its about ready for use: https://github.com/acowley/Frames /u/acow gave a talk touching on it at Compose if you want to see some exposition: https://www.youtube.com/watch?v=2-JFkv9-JOQ If Frames wasn't around I'd either use Vinyl directly as suggested elsewhere on this thread, _or_ (and this is "bad" I know, but I believe in cheating when the win is sufficient) just keep everything as string _as long as possible_ and do all my feasible merging and munging in a "stringly typed" way, and only "parse out" to proper numeric, etc. types at the moment when doing so is actually necessary. If that was insufficient, I would maybe even use an ADT for a union type and do something like `data CSVElem = CSVString String | CSVInt Int` etc. Again, there are now much better solutions, and you should certainly give `Frames` a look -- but in a mail merge you are reading _in_ text, and emitting _out_ text, so maybe keeping things as text in the intermediate portions is not necessarily so bad as one might imagine :-)
That's a solution, but I'd better stay with R then ...
Just wanted to add CTRex to your list of libraries to consider. I don't think that it's actively developed right now, but it's basically an improved HList. It probably is a poor fit for large records though. There's some really good and easy to read docs: https://wiki.haskell.org/CTRex
Use Ermine? I think it has better record types. Haskell's record types are not that great. Tuples are definitely *not* what you want, based on your description of how merge works, i.e. not purely positionally. Records are closer to what you want, but you will be using a lot of different ones. SYB / Generics should let you significantly reduce the amount of code you write for each instance, but there will be some. There's probably some sticky bits, but I really think HList.Record is what you want. I can't speak to performance, but the types shouldn't be that much longer than the tuple types you are already using: `(String, Int, Double, Int, Double)` =&gt; `Record (String : Int : Double : Int : Double : [])` I'm unfamiliar with dataframe. Vinyl (suggested by others) looks like a potentially better HList.Record. That said, I would be extremely uneasy with your workflow, and I'd be looking for a way to remove the less statically typed aspects from the flow (CSV, Word) and update them to something else so I wasn't constantly discarding and reconstructing type information (Persistent, Pandoc template(?)). Also, while keeping all intermediate results might have made sense in Excel, it doesn't really make sense in Haskell, since there data structures are all persistent, and older versions are accessible as long as they are in scope. Your individual steps don't need to get bigger, but you should feel comfortable shortening the tail sometimes e..g going from `(a, b, c, d) -&gt; (a, b, e)`. And, now that I say that, it's clear your problem is equivalent to strongly-typed scope/environment management like you might do in a strongly-typed implementation of [SECD](http://en.wikipedia.org/wiki/SECD_machine), like we did in the Agda workshop at ICFP last year.
&gt; Will Backpack change this? Unlikely. Types depending on terms is the realm of dependently-typed programming languages. Dependently-typed Haskell is probably coming (in the form of many GHC extensions); there sees to be quite a bit of interest and not a lot of resistance. But, it's certainly not part of the backpack changes.
Even though the inputs and outputs are csv, therefore text, they have a *semantic* (like units, currency, etc) and this where the type systeme is usefull. Multiplying 2 quantities together doesn't make sense, whereas multiplying a quantity by a price gives an amount etc ... I'll have a look at Frame.
I'm pretty sure there's fundamental problems with the data.frame concept, but in conjunction with elegant functional style workflows provided by packages like tidyr and dplyr I haven't found a compelling argument against them yet, especially for one-off statistical modeling tasks. "6 months later you have no idea how many columns data has, and worst, you don't know if quantity is coming originaly from file1 or file2." Considering it would take about the same amount of time to do "data1 %&gt;% glimpse" and "data2 %&gt;% glimpse" "data1 %&gt;% summary" as to look up a piece of code etc. I'm not convinced by the source code must be data annotation argument. It reminds me of all the people saying not to use "auto" in c++ just because it's nice to have that "int x = 6" annotation there. I'm sure there's issues with data.frame, but I'd like to see a deeper discussion of what those are. 
&gt; Record [Tagged "product" String, Tagged "quantity Int] Time for an infix type alias? `type (:::) = Tagged`?
Yes indeed.
In theory yes. In practice, everything is intermediate untill I decide to display it in the report, or if you prefer everything is potentially an output, which is why it's easier to keep every column and only display what's needed.
For what it's worth you could try defining an orphan Monoid instance for Compose, and some helper functions to wrap/unwrap mulit-argument functions for you. Other than that, I have no idea.
&gt; This shouldn't be too hard by iterating over the parameter lists of the two functions You might want to think harder about that part of your plan. In Haskell, type-level programming doesn't look like value-level programming. You probably think it should be easy to write a function which iterates over two lists of types, compares them pairwise for equality, and returns a single list containing the union of the two. However, types are not like ordinary values, you can't just slap an `Eq Type` constraint and expect to be able to compare types for equality. The `(~)` equality constraint does not work like `(==)`. With `(==)`, you give two values and you get a bool telling you whether the values are equal or not. It's trivial to negate the bool in order to get a test telling you whether two values are distinct or not. But with `(~)`, you're not getting a boolean back. Instead, it's a pessimist check: if the compiler can conclude that the types are equal from the information it currently has, then the constraint will succeed, but if there isn't enough information, the constraint will fail. So this time we can't simply negate the result of the test, because the test could have failed for two different reasons: (1) the types were definitely different, or (2) the types were the same but the compiler could not figure it out. For example, here is a case where the compiler does have enough information to confirm that `String` and `String` are the same type: fancyEq :: (Eq a, a ~ b) =&gt; a -&gt; b -&gt; Bool fancyEq x1 x2 = (x1 == x2) useFancyEqDirectly :: Bool useFancyEqDirectly = fancyEq "foo" "bar" But in the following situation, even though we're again only using `String`s in the end, the fact that `indirectFancyId` doesn't reflect that fact in its type is enough for ghc to reject the program. indirectFancyEq :: Eq a =&gt; a -&gt; b -&gt; Bool indirectFancyEq x1 x2 = fancyEq x1 x2 useFancyEqIndirectly :: Bool useFancyEqIndirectly = indirectFancyEq "foo" "bar" But in this case, the fact that the constraint fails does not mean that `a` and `b` are necessarily distinct types; indeed, as `useFancyEqIndirectly` demonstrates, they could both be instantiated to `String`. What you're trying to achieve may or may not be possible in Haskell (I'd guess it isn't), but it's certainly much harder than you seem to think. *edit*: Note that the reason why doing something like that is hard in Haskell but easy in dynamic languages such as Python is that in python, your type-inspection code would execute at runtime, so you have access to concrete values and concrete type representations which are readily comparable. In Haskell, your type-level code executes at compile time, and must therefore work on abstract types, about which we don't know anything yet. It's a much harder task.
/u/acow is also [giving a talk](http://www.meetup.com/Boston-Haskell/events/221132070/) focused just on the Frames stuff here at Boston Haskell in a couple of weeks.
Well, one could try to introduce a kind difference between functions and values with some GADT wizardry like this: data Function as where Value :: a -&gt; Function '[ a ] Function :: (a -&gt; Function as) -&gt; Function (a ': as) (&lt;++&gt;) :: Function as -&gt; Function as -&gt; Function as Value a &lt;++&gt; Value b = Value (a &lt;&gt; b) Function f &lt;++&gt; Function g = Function $ \a -&gt; f a &lt;++&gt; g a then perhaps something like this to get back to the value level: class FunctionClass a where type Result a :: * getFunction :: Function a -&gt; Result a instance FunctionClass '[a] where type Result '[a] = a getFunction (Value a) = a instance FunctionClass as =&gt; FunctionClass (a ': as) where type Result (a ': as) = a -&gt; Result as getFunction (Function f) = \a -&gt; getFunction (f a) (disclaimer: this is just an offhand snippet, I don't have a working compiler available at the moment and I haven't checked if it works) Though as much as I like Haskell + GHC extensions, I can't help but to notice it's a bit like making Haskell a dependently typed language by nailing extra legs onto a dog, unfortunately. EDIT: Using singletons for the type lists might help GHC's type inference not to choke on code using tricks like that. When (undefined :: sing [foo, bar, baz]) is not enough, one can try something in the lines of data ListSing as where SNil :: ListSing '[] SCons :: sing a -&gt; ListSing as -&gt; ListSing (a ': as) class ListSingleton as where listSingleton :: ListSing as instance ListSingleton '[] where listSingleton = SNil instance ListSingleton as =&gt; ListSingleton (a ': as) where listSingleton = SCons (undefined :: sing a) listSingleton EDIT2: I just found this: https://www.fpcomplete.com/user/bennofs/number-of-arguments-of-a-function
Please read the excellent [“Storage and Identification of Cabalized Packages”](https://www.vex.net/~trebla/haskell/sicp.xhtml), by Albert Y. C. Lai.
I've written up a fairly extensive answer for you [here](http://nbviewer.ipython.org/urls/gist.githubusercontent.com/gibiansky/3683af9a874934923d73/raw/1afd237499cb5bbe083e34c3e9a49ffd04aefd1e/reddit_answer). I go a little bit deeper, and investigate several possible options for both your original question and your deeper motivation. If any of it is confusing, go ahead and ask, and either I or someone else may be able to answer more in depth. In the process I managed to find a bug in IHaskell (the software used to create that document) relating to fixity declarations :) Useful!
yeah, Function is like a Vinyl Rec. kinda. except Function is kept "curried" rather than "uncurried".
This solution worked with my small example, but it feels kinda stupid and I don't know why. Maybe someone can point out the downsides. {-# LANGUAGE OverloadedStrings #-} import Data.String import Data.Monoid instance IsString s =&gt; IsString (a -&gt; s) where fromString = const . fromString data Person = Person { name :: String, age :: Int } email :: Person -&gt; String email = "Hi I am " &lt;&gt; name &lt;&gt; " and I am " &lt;&gt; show . age &lt;&gt; " years old." 
Maybe things have changed, but when I put together http://code.haskell.org/~aavogt/HList-benchmark/ about 8 months ago, I had to limit the size of CTRex records relative to the other options because I was getting impossibly long compile times (runtimes were fine).
This is where I got the idea from ;) Actually, Vinyl was my introduction to dependent typing in Haskell as I recall.
Haven't played with Idris yet (nor any other dependently typed languages), so forgive my ignorance... The "ord" in your suggested solution... could that be considered a value-level representation of the "Ord t" constraint? Such that e.g. insert and delete require that you're using the same (value-level) "Ord t" instance...? What the would type of "empty" (the value of the empty set) be in this scheme? I assume it would be polymorphic in the "value" of the Ord constraint?
Different language specifically for reporting tasks. I believe /u/edwardkmett put it together.
Would you recommend Ermine over Haskell+Frames for similar tasks (I use Haskell mainly as an ETL)?
But then you can't use it as a technique to replace loops ... right? You need the church numbers to apply a function "n" times anyway. So, for ex, my `div` still normalizes to an exponential sized term.
We can always Church or Scott encode the type to a form similar to what you give here, even with recursion. http://hbo-kennisbank.uvt.nl/cgi/nda/show.cgi?fid=2381 That said, once we know we have an ADT, we know information that is annoyingly hard to recover from the Scott or Church encoding, e.g. that only one of those cases (Int -&gt; r) or (Bool -&gt; Int -&gt; r) matters, that we don't wind up evaluating both, and then taking one, etc.
Okay do you have any resource on how this encoding works exactly?
Thanks.
What would you gain that would justify throwing away so much information?
Not yet. I'm actually working on implementing it myself very soon as part of adding efficient numeric support for `annah`. You'll just have to wait until that is done.
Okay, thank you! :) Btw disregard the last message, I swear I read it was posted by you.
Sure, answers to those questions are in the tutorial, readme, and video in greater length, but the TL;DR is that `Frames` does two things for you: 1. It has a little bit of template Haskell to guess the types of a data set using a parser that can be easily extended with user types. This way you don’t have to write down a type listing off a dozen fields when the data file already does that (eg with a CSV header). 2. It uses column-based storage for maximal efficiency regarding memory use and column subsets. In contrast to most common non-Haskell data frame approaches, it works great with streaming via `pipes`, and gives you type errors that your editor can warn you about when the code you’re writing doesn’t match the data you have on disk.
It took some doing, but `Frames` is built on `Vinyl`, so I’d definitely start with `Frames` as you won’t be coding yourself into a corner if you write code in the row polymorphic style encouraged by the `Frames` tutorial. It’s all compositional, so you can peel off any layer that causes trouble.
Interesting, I'll have to read over your post more carefully later. Can every ADT be encoded like this?
&gt; What the type of "empty" (the value of the empty set) be in this scheme? I assume it would be polymorphic in the "value" of the Ord constraint? Yes, it would. You could also, at least in theory make singleton sets polymorphic in the choice of ord.
Thank you! A very interesting paper, and I'm glad to have a name to search for now.
I might be missing something here but from my perspective the tagged union representation is much simpler than your function representation.
A simple and efficient binary representation is probably the main reason. Encoding an ADT as a record with a tag word and some number of data words is in some sense obvious, as well, because you have a dependent product of the tag and fields.
Looooool. typo i missed out. cheers for that @tomasp !
s/Haskell/GHC Experimental GHC (and other compiler) extensions get merged into the real language from time to time, usually when they have proven their usefulness and stability over a long (very long in Internet time) time. Also want to make sure adding a given extension wouldn't make the language too complicated to work with for new users, etc.
Sorry, I meant a simpler Core language, not a simpler binary representation. You have to store, evaluate, and typecheck closures anyway, and you want to optimize those as much as possible; so if it's possible to optimize closures well enough that the ADTs end up being dealt with efficiently, then it seems better not to have to store, evaluate, or typecheck ADTs as well. Of course, if the only way to approach the same efficiency is by basically finding the ADT-like terms and dealing with them specially, then it's a waste of time.
According to the Hinze-James paper I cited in the blog the high-school algebra identities hold in bicartesian closed categories. I will add a few sentences elaborating on this.
[**@aaronmblevin**](https://twitter.com/aaronmblevin/) &gt; [2015-01-09 19:05 UTC](https://twitter.com/aaronmblevin/status/553628621382750208) &gt; postgresql-simple + Vinyl in 1 tweet: &gt; &gt; q :: RecAll f rs ToField =&gt; Rec f rs -&gt; [Action] &gt; q RNil = [] &gt; q (field :&amp; t) = toField field : q t ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://www.np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
That's the theory. In practise, barely any of the modern highly used libraries work without a slew of GHC extensions; getting anything to work in other Haskell implementations is very much a minority sport; the Haskell committee which defines the "language proper" is moribund; and beginners end up having to get to grips with Haskell as used by the GHC-extension using majority to benefit from the necessary ecosystem of libraries.
There's also a guide here: https://www.fpcomplete.com/school/to-infinity-and-beyond/pick-of-the-week/guide-to-ghc-extensions As for when to use them, I'd say avoid as much as possible in libraries that are meant for uploading to Hackage (though when you already are using something very GHC specific like Template Haskell or type families, then restricting the use of "lesser" extensions like tuple sections or monad comprehensions makes little sense, I guess) but for personal code anything goes. Just read the docs thoroughly first, as GHC sometimes suggests extensions like ImpredicativeTypes or IncoherentInstances for misdesigned code. As for why are there so many, well, it would appear that GHC is sort of "the" Haskell implementation, and noone bothers to define a new language standard including these.
what's "second-order lambda calculus"? concrete types and polymorphic types (forall)? types and kinds?
&gt; the Haskell committee which defines the "language proper" is moribund Arguably, less time has passed between the current standard (Haskell2010) and now than between the previous standard (Haskell98) and the current standard (Haskell2010). I still hold hope that Haskell will not become an implementation-defined language, but the people doing the work on the standard, updating the core libraries, and contributing to hackage do not seem (for the most part) to be concerned about that.
I feel like the second part of this is perhaps a bit overengineered - isn't this what -XRecordWildCards is for? {-# LANGUAGE RecordWildCards #-} data Person = Person { firstName :: String, secondName :: String, gender :: Gender } f :: Person -&gt; String f Person{..} = genderTitle gender ++ " " ++ firstName ++ " " ++ secondName And there's no problem doing it with both a Person and an Event at the same time (although I suspect you'd have to use -XRecordPuns instead if you had two Persons).
A lot of libraries that rely on extensions can still be used from Haskell98 or Haskell2010 code without knowing the difference. I always stick to the standard, and I can still benefit from the majority of libraries which do not.
Also Text.Printf would be easier to use to write the formatting in than concatenating a whole bunch of string constants and values. The lack of compile-time type-safety can be a little bit of an issue but that just means you need to actually run your code at least once before pushing it to production!
The case/case optimisation is incredibly useful and much harder to recover without explicit constructors. The other issue is large ADTs require a selector for each branch whereas Core has the DEFAULT pattern which reduces code size. 
Right, from a purely denotational view, we can always come up with enough lambdas and foralls to represent an ADT. But, operationally speaking, representing *data* that way isn't great; you end up trashing your processor's L1 I-Cache just as bad as laziness trashes the D-Cache. Now, if the "data" really directs/encodes your control-flow you are going to trash the I-Cache anyway, so the Scott encoding can actually be a more direct way of programming without losing much (or sometimes even gaining) performance, and you might even save some stack space.
This encoding requires O(N) traversal to implement things like zip, doesn't it? The more direct encoding (where `List` still appears in the parameter types) allows a very direct translation of pattern matching code, allowing at least the same asymptotic.
Second-order lambda calculus is more commonly known as System F. It's basically the simply-typed lambda calculus plus universal quantification (i.e. "forall")
Nice article. I've been whiling away my afternoon by adding various other list operations to your "Yes, we can!" Haskell code. They all work pretty well, but I'm not entirely happy with my `tail` implementation. I'm wondering whether you know a nicer way to do it. First, some utilities: first s _ = s second _ t = t -- Empty list &amp; cons operation for List type nil = \c n -&gt; n cons a x = \c n -&gt; c a (x c n) Now, `head` is easy: head :: List a -&gt; a head as = foldr as first undefined -- Above uses reordered foldr arguments, as in article `tail` takes a bit more work. I had the idea of using a `foldr` whose intermediate result is a pair. The first item in the pair is a list of everything processed so far. The second item is the tail of that list. tail :: List a -&gt; List a tail as = bs where (_,bs) = foldr as step zz zz = (nil, undefined) step a (cs,_) = (cons a cs, cs) The above mostly works, but it isn't lazy enough. It always works all the way through the given list, so it is slow for big lists, and it hangs on infinite lists. We can fix this by replacing the tuples with functional pairs: the *pair* x, y takes a 2-parameter function f and returns f x y. Revised implementation of `tail`: tail :: List a -&gt; List a tail as = (foldr as step zz) second where zz f = f nil undefined step a x f = f (cons a (x first)) (x first) The above works just fine, but it still seems a bit unwieldy for something as simple as finding the tail of a list. Any thoughts?
&gt; Aren't language extensions basically proposed and specified with a similar level of detail as an actual part of the language standard though? I'm not sure the documentation of the extensions is quite like this. It's definitely detailed, but (naturally) there's more focus on how it ties in to *GHC* and affects *Core*, rather than how it affects the standard. &gt; I mean as opposed to languages where new versions just implement new features without any sort of documentation on how the new features are supposed to behave? *shrug* Are any languages really like this? Most do some sort of specification up front, right? C++ extensions often get a research paper. Java has JSRs; Python -- PEPs. Even the coming-any-time-now Perl 6 had a (rather unusual) design process. I certainly don't fault the *GHC* developers for focusing on *GHC* rather than the language specification.
[This blog](https://ocharles.org.uk/blog/pages/2014-12-01-24-days-of-ghc-extensions.html) has a great series of posts on language extensions.
does it mean that haskell is not good for building parsers?
The Rust code was also tested with the parser results passed to Rust's `test::black_box` function, which prevents the compiler from optimizing away those results. The only code that could be optimized out is code that isn't used during parsing - so code that should be optimized out in a real application.
Empty data declarations are standard in Haskell 2010, IIRC
Chances are if you profile the attoparsec parser there's probably some room for improvement somewhere.
http://stackoverflow.com/questions/10845179/which-haskell-ghc-extensions-should-users-use-avoid/10849782#10849782 That list is a little old (I would edit it if I had the knowledge), but I've been referencing it a few times.
I might be dumb because it's before coffee, but I don't get this comment at all. Anyone mind explaining what it means? How would I be able to use a library that relies on an extension when I don't have that extension?
Well you're talking about a list type defined in terms of a fold; what I had in mind was what is apparently called the "Scott encoding" (see ekmett's comment), which is a very direct encoding of the case destructuring. nil = (\fnil fcons -&gt; fnil) cons x xs = (\fnil fcons -&gt; fcons x xs) The tricky part of this encoding is expressing generic list operations like `foldr`, because although they are straightforward to write, they will not typecheck in Haskell (they would require recursive types). foldr f z lst = lst z (\x xs -&gt; f x (foldr f z xs)) -- Occurs check: cannot construct the infinite type: -- t1 = t0 -&gt; (t3 -&gt; t1 -&gt; t2) -&gt; t4
Many of the commonly used language extensions affect how code is written within a module, not how the code is called. That means you can benefit from overloaded strings (for example) in code you write, and users of the code won't know the difference. This is obviously not true of extensions that affect data types - RankNTypes and TypeFamilies come to mind - if you then choose to export those data types. At least as far as I am aware.
Oh, yeah, as long as you have the extensions available, you don't have to explicitly use them in your own code. I see. But users of the library still has to have the extensions available, no? So if program foo is built on the bar library that uses extensions, then foo can't be compiled with any other compiler than GHC?
What about something like Twan van Laarhoven's [Funlist](http://twanvl.nl/blog/haskell/non-regular1)? I feel like they fall under "recursive types", but it isn't immediately clear to me whether non-regular types like these can be encoded in this way.
I would rather use alias. alias ghc="ghc -package-db=/path/to/*.packages.conf.d" alias ghci="ghci -package-db=/path/to/*.packages.conf.d"
 newtype FunList a b = FunList { runFunList :: forall f. Applicative f =&gt; (a -&gt; f a) -&gt; f b } We call this type, the finally-encoded variant of FunList, 'Bazaar' in `lens` (after we upgrade it to make it a 2-parameter indexed comonad, and allow lens-style indexing): newtype Bazaar p a b t = Bazaar { runBazaar :: forall f. Applicative f =&gt; p a (f b) -&gt; f t } type FunList a = Bazaar (-&gt;) a a and it is a rather high powered workhorse for us. `FunList` on the other hand, assumes we can always associate the applicative to the one side, just like list makes the same assumption that we can always associate to the right. `Bazaar` here can often productively produce results where the original `FunList` will sometimes necessarily get stuck. It also has a lot in common with FMLists. newtype FMList a = FMList { runFMList :: forall m. Monoid m =&gt; (a -&gt; m) -&gt; m } which can be more obviously related by looking at newtype Whatever p a b t = Whatever { runWhatever :: forall f. (Applicative f, Contravariant f) =&gt; p a (f b) -&gt; f t } whereupon b and t are provably uninteresting, and if you fix p = (-&gt;) you recover something interchangeable with `FMList`. type FMList t = Whatever (-&gt;) a () () Now, these are all tricks, but... There is a more general principled understanding of how you can always provide initial algebra semantics for a GADT that is due to Ghani and Johann in ["Initial Algebra Semantics Is Enough"](https://personal.cis.strath.ac.uk/neil.ghani/papers/ghani-tlca07.pdf). It had to be implemented in pseudo-code prior to PolyKinds. Now much of it can be implemented directly.
You just get a much bigger eliminator form for these.
It is your second case, "Some types are not strictly positive", that is the real killer here. Everything else has at least some kind of workaround.
I had a look at the code and it looks promosing. ## I like : + The idea of a common effort in the `data.frame` (plyr?) direction + `Frames` are sort of an interface and at the end is just `Int -&gt; Rec`. + it uses `Pipes`. Not I prefer `Pipes` over `Conduits` but more that I don't know which one to chose, so the choice is done for me ;-) ## I'm not sure about: + The use of data (csv) as input to the compiler (via TH), but I guess I don't have to use it. + The use of `Vinyl`. AFAIU the main difference between `Viny` vs `HList.Record` is the introduction of *universe*, which ideally can be defined once and reused for all sub-record and simplify type signature. However, this collapse as soon as you start *merging* (in a R way, or joining in a SQL way) frames. The result of outer joins transform some types in `Maybe`. Which means that the merge of to data.frame from the same universe, doesn't belong to the original universe. Moreover, I found the `these` package quite neat to perform join/merge operation (it's called `align` in `these` and works beautifuly between `Map`s). I'm not sure how `Frames` could represent nicely the result of a merge operation. (In R it's different, everything is a `Maybe`). ## I don't like: + (and I a really don't) : The reuse of the name `Rec`. If you are building something on top of `Vinyl`, PLEASE don't introduce names which collides which `Vinyl` namespace. It's confusing and IMO unnecessary. Please change the lines type RecF = Vinyl.Rec type Rec = RecF {-Vinyl.ec-} Identitfy to type Row/RecF/Whatever = Rec Identity before it's too late. ##Conclusion I found `Frames` really interesting but I'm still not sure if the efforts shouldn't be put in selecting and building tools around (and make them speak together) existing packages. I'm talking of `vector`, `container`, `these`, `pipes`, `reducer`, `cassava`, *-simple`, `vinyl` and/or `hlist` etc ... 
The C parser he used is a highly "dynamic one". I wouldn't pay too much attention to those numbers.
The dependencies are probably compiled with optimisations already, depending on their cabal files. E.g. attoparsec has ghc-options: -O2 -Wall in its cabal file.
&gt; …depending on their cabal files. So if an author forgets `ghc-options`, does one end up with sub par performance, unless one uses `cabal install … -O2`?
Yeah, I guess. Although I'd be surprised if there were many libraries whose authors care about performance but don't put `-O2` in their cabal files.
Yes, that's what I meant. Doesn't help my code run on other compilers, but makes it *much* easier for people to learn enough Haskell to understand my code.
To clarify: Normally, an `a ~ b` equality constraint fails because `a` and `b` could be instantiated at distinct concrete types, such as `Int` and `String`. My example was demonstrating that the fact that they *could* be instantiated at distinct concrete types did not mean that they *had* to be instantiated at distinct concrete types, and thus that we can't simply negate the result of a type-level equality to require two types to be distinct. The way in which ghc determines that `a ~ b` should fail is not by trying to find distinct concrete types such as `Int` and `String` at which to instantiate `a` and `b`. Instead, ghc tries to use the information it already has about the abstract types `a` and `b` to prove that they are the same, and if it can't manage to find such a proof, the equality constraint fails. As a simple example, here is a case where ghc knows that `a ~ b` and that `b ~ c`, and combines those two hypotheses to prove that `a ~ c`. fancyId :: a ~ b =&gt; a -&gt; b fancyId x = x transitivityDemo :: (a ~ b, b ~ c) =&gt; a -&gt; c transitivityDemo = fancyId Since ghc doesn't try too hard to prove that the types are the same, it is indeed possible to get into situations in which an `a ~ b` equality constraint fails even though other factors ensure that the types at which `a` and `b` are instantiated will always be the same. Do you have a good newbie-friendly example of that?
&gt; (b) you're referring to some quote where I'm talking about Scheme -- not Racket -- and I always said that if it's Scheme or CL, then CL wins since it's way more practical. Well then chrisdoner recalled correctly since the quote was : &gt; Of course, even Eli Barzilay, one of the foremost PLT contributors has said in the past that he doesn't use Scheme for real work and uses Common Lisp. Which might be a bit more emphatic but seems to coincide with your opinion though as you say, nowadays you really use **Racket** for real work.
Thank you very much. I love learning about how one can implement the basics of a library oneself instead of just reading a tutorial on how to use it.
I suppose in case of insufficient inlining this optimisation may be hidden behind function calls so... always good to be aware of the existence of `concatMap` even if writing `concat . map` directly is probably just as efficient.
If you have an actual quote, I'd be interested. I know Eli quite well, and I'm confident this isn't an accurate statement about his work in the past ~10 years.
Cross-posted from /r/purescript but thought this might be of interest here as well. :) https://www.reddit.com/r/purescript/comments/2yrbos/pure11_an_experimental_c11_backend_for_purescript/
Yeah, I must have overlooked LambdaCase, which fits right in with the other quality-of-life stuff. Why people subject themselves to typing non-ASCII stuff for anything bigger than toy projects is beyond my comprehension, but to each their own. ;) Maybe I'll understand when I finally get around to learning a better-than-QWERTZ keyboard layout.
I'd be inclined to agree conceptually, but my limited experience from trying out some type-level programming, as well as observing what others are doing in that area, is that things have a tendency to either not work out at all or require some really convoluted encoding. Could very well be that I'm just incompetent, though. (I was also familiar with fundeps from relational algebra before stumbling upon them in Haskell land, which may have skewed my opinion on this matter.)
Thank you very much. I'll probably set something up at some point in the nearer future, but in the meantime, if anyone finds this list useful, please do with it whatever you want.
There are a few ways to think of Bazaar and how it relates to Traversals. We can think of Bazaar as a Bob Atkey style 2-parameter parameterized comonad. The easiest way to see why it comes up in `lens` is to think about the two ways we talk about lenses. We have `lens`, which uses: type Lens s t a b = forall f. Functor f =&gt; (a -&gt; f b) -&gt; s -&gt; f t type Lens' s a = forall f. Functor f =&gt; (a -&gt; f a) -&gt; s -&gt; f s with a couple of laws and we have `data-lens`, which offers just the 2-parameter version of things: newtype DataLens' s a = Lens (s -&gt; Store a s) with equivalent laws. Now, let's compare `Lens'` and `DataLens`' s -&gt; Store a s vs forall s. (a -&gt; f a) -&gt; s -&gt; f s flip the latter forall f. Functor f =&gt; s -&gt; (a -&gt; f a) -&gt; f s then push the constraint in: s -&gt; forall f. Functor f =&gt; (a -&gt; f a) -&gt; f s then wrap it in a newtype s -&gt; Context' a s This indicates that a data type like newtype Context' a s = Context' (forall a. (a -&gt; f a) -&gt; f s) is a form of the `Store` comonad. In this view a `Lens'` is a `Context'` comonad-coalgebra. Now, we wind up needing an extra index when we talk about type-changing lenses. So let's upgrade Context: newtype Context a b t = Context (forall f. Functor f =&gt; (a -&gt; f b) -&gt; f t) Now `Context` is isomorphic to the 2-parameter parameterized Store comonad. You have `iextract :: Context i i a -&gt; a` etc. And if Data-Lens had a 4-parameter version of its lens type you'd be able to do the same flip trick to show that s -&gt; Context a b t is isomorphic to the equivalent s -&gt; IStore a b t version you'd get from `data-lens`. If we repeat the above experiment using something like type DataTraversal' s a = s -&gt; FunList a s we'd get a `data-lens` style treatment of Traversals. This is what Russell O'Connor did in his ["Functor is to Lens as Applicative is to Biplate"](http://arxiv.org/abs/1103.2841) paper. He calls `FunList` here the `CartesianStore`. But if we upgrade all the way to the type-changing version, and take `lens`-style Traversals and play the same game by flipping type Traversal s t a b = forall f. Applicative f =&gt; (a -&gt; f b) -&gt; s -&gt; f t to get s -&gt; forall f. Applicative f =&gt; (a -&gt; f b) -&gt; f t we see Bazaar there on the right hand side! So this is how `FunList` is related to `Bazaar`. They aren't isomorphic, however, except for finite traversals. For finite traversals, you can finish 'left associating' everything to get it in the form suitable for FunList. For infinite traversals you can never finish getting out a's and incurring 'b' obligations to start producing the 't' at the end with a FunList, while you can start producing `f t` productively out of the `f b`'s in a tree structure when `f` is sufficiently non-strict using: `(a -&gt; f b) -&gt; f t`. Now to get to your question. How do you convert? In lens we use `partsOf` (and `unsafePartsOf`) to convert a `Traversal` into a `Lens` to the list of the results. (You are expected not to change the size of the list.) Given a FunList a b you can extract the [a]'s very easily by running down until you get to the `Done` case. You can also take a `[a]`'s of the same length and run through ignoring the `a's you are given by the `FunList` and supplying each to the function at the end. This is basically what `partsOf` does with the `Traversal` form. A more direct way to go from a `Traversal' s a` to `s -&gt; FunList a s` is to exploit the `Applicative` structure of `FunList` and pick 'f' = `FunList a` when walking `forall f. Applicative f =&gt; (a -&gt; f a) -&gt; s -&gt; f s` So now you need to give me `a -&gt; FunList a a` and the traversal will give you `s -&gt; FunList a s` ! so what is that? Recalling `FunList`: data FunList a b = Done b | More a (Funlist a (a -&gt; b)) Now, (\a -&gt; More a (Done id)) :: a -&gt; FunList a a is the trick pass that to your Traversal and you get out a FunList comonad coalgebra! Now, so all of this without caring about `s` and you get the conversion to/from FunList. Now this isn't an isomorphism: FunList a b -&gt; Bazaar' a b is total, but Bazaar' a b -&gt; FunList a b requires reassociating to the left, and so may fail for infinite traversals. You'll introduce bottoms that weren't there originally by starting in Bazaar and round tripping through this. So, is there an "initial" encoding which doesn't introduce these bottoms? The answer is yes, but you need to quotient your handling of the result. I invite you to explore: https://github.com/ekmett/lens/blob/master/src/Control/Lens/Internal/Magma.hs#L62 How can we discover types like `Bazaar` and `Context`? Also, there is an interesting result, in that we can build both `Bazaar` and `Context` using very very simple building blocks. newtype Free p f a = Free { runFree :: forall g. p g =&gt; (forall x. f x -&gt; g x) -&gt; g x } is a neat construction. `Free Functor` is a universal property encoding of `Coyoneda`! `Free Monad` is actually a perfectly legitimate encoding of the free monad. `Free p` is also a McBride style indexed monad, just like how Dan Piponi noted normal `Free` is a McBride-style indexed monad. Now, in "Kleisli arrows of Outrageous Fortune" McBride gives a construction to convert between his one-parameter indexed monads and Bob Atkey's 2-parameter version. He calls it `(:=)`, I'll call it "At": data At a i j where At :: a -&gt; At a i i Coyoneda f is isomorphic to f _if_ f is a Functor. but if its not, all bets are off. Free Functor (At a b) t = forall g. Functor g =&gt;(forall x. At a b x -&gt; g x) -&gt; g t = forall g. Functor g =&gt; (a -&gt; g b) -&gt; g t = Context a b t Note this is kinda weird, the parameterized monad works over the 'a' parameter here, but the parameterized comonad works over the t parameter. (If you go back to the Magma module linked above you'll see that `Molten` performs this flip letting us build/compose on one parameter and use it as a comonad on the other.) And we can do the same construction with Free Applicative (At a b) t to generate `Bazaar`! This also means that if you look at it right, s -&gt; Free Functor (At a b) t is a 2-parameter indexed Kleisli arrow where the indices tell you the types you need to change to come back. That may have wound up a little long for a reddit reply. =)
The real issue is that the standard becomes increasingly difficult to implement as they keep adding language extensions. Extensions like `OverloadedStrings`, `MultiwayIf`, `TupleSections` are pretty harmless because they are so easy to implement, but other extensions like multi-parameter type classes seem like they are doomed to be `ghc`-only.
ahhh Made a mistake put the correct code
done.
Yeah, what hspec does is give a way to combine different types of tests into one test suite. So if you want to write tests that use Webdriver, or tests that use quickcheck, or tests that use snap, etc - it gives a way of putting those all into one test suite that has uniform reporting, etc. I actually initially wrote hspec-snap as a standalone library (hspec prior to 2.0 didn't support enough to be able to use it). Being able to integrate with hspec allowed me to drop a lot of code that kept track of failures, successes, the tree of test descriptions, and get some improvements along the way (like colored output). It's not huge, but then again, it's not hard to use either...
Also remember as /u/Yuraz said, attoparsec uses backtracking where the other libraries do not and the author probably should have just used the binary package.
`Frames` wasn't built on `Vinyl` to begin with as there was some lag in development of the latter while we all thought long and hard to convince ourselves whether or not the big change was worth it. After the `Frames` design was proved out, the `Vinyl` change was merged, I ported `Frames` to use `Vinyl`, and at this point I think you're right that the name overlaps are confusing. I don't understand your last paragraph at all, though. `Frames` leans on `pipes` for streaming, `Vinyl` for extensible records, and `vector` for dense arrays. It exactly is selecting and building tools around those libraries. Is your objection just that `Vinyl` was chosen instead of `HList`, and `pipes` instead of `conduits`?
This is correct, there is really no need to use attoparsec at all. I was curious about the performance when translated to a non-backtracking parsing library for strict bytestrings ([cereal](https://hackage.haskell.org/package/cereal)) and the result seems to be nearly identical to the rust version. [Source for anyone interested](https://github.com/Codas/nom_benchmarks/tree/dev).
What does `LambdaCase` have to do with non-ASCII stuff? 
As awkward and error-prone (due to partiality checking giving many false positives, and lack of any termination checking) it is, this compiles in GHC: import Data.Type.Equality data Nat where Z :: Nat S :: Nat -&gt; Nat data NatSing n where ZS :: NatSing Z SS :: NatSing n -&gt; NatSing (S n) data May a where Nil :: May a Is :: a -&gt; May a type family m :+ n where Z :+ n = n (S m) :+ n = S (m :+ n) congruence :: m :~: n -&gt; f m :~: f n congruence Refl = Refl additionSndDef :: NatSing m -&gt; NatSing n -&gt; (S m :+ n) :~: S (m :+ n) additionSndDef ZS ZS = Refl additionSndDef (SS _) ZS = congruence Refl additionSndDef (SS _) (SS _) = congruence $ congruence Refl additionSndDef' :: NatSing m -&gt; NatSing n -&gt; (m :+ S n) :~: S (m :+ n) additionSndDef' ZS ZS = Refl additionSndDef' ZS (SS _) = congruence Refl additionSndDef' (SS n) (SS m) = congruence $ case additionSndDef' n m of Refl -&gt; additionSndDef' n (SS m) commutativity :: NatSing m -&gt; NatSing n -&gt; (m :+ n) :~: (n :+ m) commutativity ZS ZS = Refl commutativity (SS n) ZS = case commutativity n ZS of Refl -&gt; Refl commutativity (SS n) (SS m) = case additionSndDef n m of Refl -&gt; congruence $ case (additionSndDef' n m, additionSndDef' m n) of (Refl, Refl) -&gt; congruence $ commutativity n m type family n :- m where n :- Z = Is n Z :- (S m) = Nil (S n) :- (S m) = n :- m neutral :: NatSing n -&gt; (n :+ Z) :~: n neutral ZS = Refl neutral (SS n) = congruence $ neutral n cancelling :: NatSing n -&gt; (n :- n) :~: Is Z cancelling ZS = Refl cancelling (SS n) = cancelling n whatever :: NatSing n -&gt; NatSing m -&gt; ((m :+ n) :- n) :~: Is m whatever ZS ZS = Refl whatever (SS n) ZS = cancelling n whatever (SS n) (SS m) = _hole -- insert a proof that subtraction can be pushed through equality
Perfect, a thorough reply like I was hoping for. Thanks for writing all that up. I've got to get some rest before taking a crack at this, but it seems like I've got some stuff to go through when I get up. Looks like I picked a good moment to stop lurking around here and say something haha.
Well, I've tried to answer your questions about what is offered, but something is getting lost in translation. If the answers here weren't clear, perhaps one of the other presentations on the design can help. I do appreciate the feedback on type names! EDIT: The names have been changed! `RecF` is gone, and `Rec` is now `Record`.
Check out [Data.MinLen](https://hackage.haskell.org/package/mono-traversable-0.9.0.1/docs/Data-MinLen.html) from the mono-traversable package for an implementation of what you're talking about.
There are two packages doing something like this : plugins and hint. I haven't used either in depth, so I can't attest to their stability, but I think they're close to what you want. 
IIRC vim is open source
I'm not sure if taking on a long term project is best... maybe taking on week-to-month long pojects when starting out is nice, so that you can quickly shift to new concepts and apply new ideas. a one or two month project would be nice too. cloning vim might be one or two year project though, so you might just get bogged down without learning new concepts. 
Are you looking to create a toy yourself, or a serious project that will legitimately replace vim? If the latter (wishful thinking on my part), then I will tell you the *key* to making that actually happen: backwards compatibility. You need to be able to make use of the massive years of labor that went into determining how Vim should behave in countless situations with different languages, user preferences, and so on. That is, you need to create a means to execute existing vim scripts. You need to implement all the vim functionality that they use, *compatibly*. Otherwise, you're not talking about a "long-term" project, you're talking about a project that requires more labor hours (and domain-specific knowledge) than any individual could possibly accomplish. A massive group effort (in an area that isn't likely to attract a massive group, I think.) Probably you just want to create a toy editor for practice though. It's sad for me because I don't like using vim at all but I don't have anything better.
&gt; Part of the reason for Haskell's existence was to provide a single language encompassing what was well-known at the time about non-strict functional languages so that research groups would have a common starting point for their projects. &gt; Putting new features behind language extension pragmas allows a stable common language while giving research groups a way to build their new ideas into the common platform so they can be more widely used. This helps everyone get an idea of whether these research-based ideas are as sound and useful as they were hoped to be. That makes sense. But was a need for extensions anticipated, or did they think they were going to experiment in another way? Because, though I don't know the implementation or the thought behind it, pragmas are essentially declared in *comments*... and that looks very much like a hacky after-thought. 
That looks much cleaner than mine.
I'd like this to be a long-term project. Yes, compatibility is on my mind, too. Thanks :)
I have some generic advice since it sounds like this is your first large project: start small and get something really simple working quickly (i.e. within 1 week, maybe 2 weeks) and then iterate on that.
If you're interested in actually getting a useable editor, I'd suggest not starting your own and instead contributing to yi. I don't know your background, but if 100 loc is the largest Haskell program you've written, I would guess that you will have a LOT to learn before you get a useable editor. You'll pick those things up a lot faster if you're learning from other people with more experience than you.
Is either team taking on interns this summer? I'm not asking about a Facebook internship, but rather if there is an opportunity to work in Haskell as a Facebook intern.
I agree with this -- as someone who has watched literally dozens of these projects start and fail, it is a rough path to go down. Even with years of time and lots of friends helping, all already experts in the language you choose to do the implementation... still an epic task. The feature-set of vim is exceptionally non-trivial. Source: I have run the #vim channel on Freenode for over a decade, I have watched, discussed, and even helped on dozens of these vim clones. Come say hi! 
&gt; `cabal install` defaults to `-O1`, […] together with (GHC manual) &gt; At the moment, `-O2` is unlikely to produce better code than `-O`. seems good enough. Thanks for the information.
&gt; I seem to remember seeing some library that abstracted over writing "GUI" apps in the terminal, but I can't remember what it was called Maybe you are talking about [vty-ui](https://hackage.haskell.org/package/vty-ui)? It puts a widget system on top of the ncurses like [vty](https://hackage.haskell.org/package/vty).
Personnaly I would use the approach of [those bindings](http://hackage.haskell.org/package/ncurses) (i.e. use .chs files). This makes it relatively easy to mix C and Haskell, and thus reuse C examples. For instance it should be straitghforward to port [sandy](http://tools.suckless.org/sandy). Still the .chs approach implies an additional learning curve but the ncurses package is really a good starting point.
 map :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
How is memory management done?
I just don't see how this is worth it. `fmap` is already in base so the change would be for purely aesthetic reasons whilst breaking old code and forcing library authors to perform menial updates. 
Hi, author of nom here :) I would say that haskell is very suitable at building parsers. The attoparsec version was very easy to write and is easy to read. The new cereal version is very readable too. Usability of parser libraries is really important, since people will resort to manual parsing (and all the errors it implies) if they cannot understand them :)
&gt;EDIT: The names have been changed! RecF is gone, and Rec is now Record. Thanks :-)
Well, they have to be careful to not change the behavior at all. I believe that kind of perfect compatibility comes with its own challenges. 
vi is also open source, and the code is at least a lot shorter than vim, though I haven't looked through it so I can't attest to its readability. If I were starting a vi-like editor, though, that's probably what I would use as my point of reference. http://ex-vi.sourceforge.net
See the example I posted above. OTOH, I would love it if someone did the change and then tried to build every package on hackage so we could see how many packages this would actually affect. 
I see, thanks for the example. Haven't thought about ambiguity resulting from imposing lesser type constraints on the context from which `map` is used.
I agree that breaking code is an important issue. I think it could be partially disarmed by providing an automatic refactoring tool, which uses the ghc api to replace the uses of the old `map` with `(map :: (a -&gt; b) -&gt; [a] -&gt; [b])` or an `lmap` specialized to lists. The latter case would be more difficult, because, it would have to watch out for scenarios where there is a user-defined `lmap` in scope. In these cases it could for example generate `lmap'`, `lmap''`, etc. until it finds an unused identifier. Breaking backwards compatibility is of course still a burden on maintainers, but I think such an approach could make the idea a bit less painful.
I'm surprised this wasn't part of BBP in some way. A lot more dubious changes did get included, but this was left out? Someone in the thread calls for a principled look at Prelude -- isn't that what BBP's aftermath was supposed to be already?
From what I understand, map wasn't included in BBP because it's historically been a larger matter of debate than adding Foldable and Traversable to Prelude. The more dubious changes from BBP were to avoid breaking code, and while BBP helps make prelude more principled, it obviously didn't address everything. For example, I think that they're planning on leaving Num alone instead of replacing it with monoids, groups, rings, fields, etc.
You mean like [HaRe](https://github.com/alanz/HaRe) :) ? We're not quite ready for release yet but we're getting closer everyday.
That's exactly what I'm planning to do - sort of making the state of the project reflect my own knowledge of Haskell. :)
The huge amount of code in Yi makes the idea of having to wrap my head around all of it rather scary, is all.
You don't have to wrap your head around all of it at first. Just talk to the maintainers and find some issues that they think you can tackle.
With my naïve translation to the binary package ([source](https://github.com/Codas/nom_benchmarks/blob/binary/haskell-cereal/src/Main.hs)) the results are not very encouraging: benchmarking IO/small time 1.225 μs (1.215 μs .. 1.235 μs) benchmarking IO/big buck bunny time 12.93 μs (12.80 μs .. 13.07 μs) Still by a factor of 200 faster than just running the cereal version on a lazy bytestring though (using runGetLazy). Ideally the input would be a strict ByteString given to the parser in chunks as they are read from the file... Anyone interested?
I don't think I really understand how that would simplify things.
Thanks, exactly what I was remembering. I was thinking of writing something like [Thoughtbot's Pick](https://github.com/thoughtbot/pick) as a library to use in Turtle scripting.
Just a quick update on this: I added the package subset functionality a couple days ago.
Pandoc was my learning exercise. I just never stopped working on it. 
You might want to check out [hspec-discover](https://hackage.haskell.org/package/hspec-discover), which automatically loads and runs test cases based on a certain file convention. You could do something similar. hspec-discover works as a custom pre-processor. {-# OPTIONS_GHC -F -pgmF hspec-discover #-} 
Thinking aloud: essentially pure expressions are commutative and have inverse, while expressions with effects usually do not. But commutativity and inverse-ness are not the only mathematical properties.
Thanks for all your replies thus far. I'll take it from here and have a closer look in TH and hspec-discover to see, which and how it can fill my needs.
Also, this approach works unusually well in Haskell because the language is so refactor-friendly. You don't have to get anything correct the first time because doing a complete overhaul is so easy, so you spend less time second-guessing yourself.
From the README: Uses native C++11 reference counting (std::shared_ptr) for relatively lightweight automatic memory management and under "Future Ideas": Compiler options for memory management
Awesome. Good luck On the slightly related front, you might find edwardk's trifecta library super nice if you want to play with parsing tools that have very human friendly error messages
I think it should be mandatory for large changes like this. If someone proposes changes like this to the Prelude I want a precise write-up the details the impact on Hackage.
Not required, but an advantage. In this role I expect you to be able to do autonomous, broad ranging work. Areas may include DSL design and implementation, type system embeddings, FFI-like API design, compiler or language extensions or tools, functional optimisations, library design, as well as "everyday" Haskell software design and implementation in a wide range of areas. It's a high level of expertise I'm looking for, and I welcome any qualified applicants.
That makes more sense. So the point is more to have the experience and background that a PhD entails? Or a PhD in PL(T) specifically? I guess both is ideal, considering the DSL/type system/etc. stuff.
This is an interesting point. The list syntax certainly gives the impression of concrete data rather than a computation.
This is basically the same suggestion as my sibling comment. Basically: for every character, you have a couple different properties (constraints, attacks) that you want to group together. The most natural way to do that in Haskell is to use a record or simple data type: they're easy to understand, manipulate, and program against. If it were me, I'd start with something like this: characters = [ Character { name = "ken", attacks = ... } , Character { name = "ryu", attacks = ... } ] Down the road, it may make sense to do some fancy `HInt`-type dynamic loading or HSpec-style discovery of modules with the right 'shape' -- but this is much simpler to start with, and even once you've gone down that road, the `Character` record would still be useful for things like redefining combos at runtime without having to recompile the entire program.
I've been planning to write a Vim-alike in Haskell as well, for the learning, but also for ideas I have about state machine stuff. The suggestions in here are things I've been stumbling through for the past few weeks - ncurses, vty-ui, etc. Maybe we can become friends, or the most bitter of enemies ;)
This is what it looks like with refinement types: http://goto.ucsd.edu:8090/index.html#?demo=Vectors.hs
Nice. Is all the boring stuff relegated to the SMT solver or is there something missing from the code?
I've heard about HaRe about a year ago, but unfortunately got distracted before I had the chance of playing around with it ;). Is there a recent feature list somewhere (outside of the source code)? I currently can't build it because the `haskell-token-utils` dependency fails. I guess this is due to me using ghc 7.8.4 and the HaRe roadmap saying: &gt; The token management utilities haskell-token-utils are too brittle, and will not be updated for GHC 7.8.x and beyond. &gt; There are substantial changes coming in GHC 7.10, which will form the basis of the new token management, based on ghc-exactprint - *edit:* The haddocks of [Language.Haskell.Refact.HaRe](http://hackage.haskell.org/package/HaRe-0.7.2.8/docs/Language-Haskell-Refact-HaRe.html) contain information close to a feature list: &gt; - `ifToCase` Convert an if expression to a case expression &gt; - `duplicateDef` This refactoring duplicates a definition (function binding or simple pattern binding) at the same level with a new name provided by the user. The new name should not cause name clash/capture. &gt; - `liftToTopLevel` Lift a definition to the top level &gt; - `liftOneLevel` Move a definition one level up from where it is now &gt; - `demote` Move a definition one level down &gt; - `rename` Rename the given identifier. &gt; - `swapArgs` 
Hmmm, then I guess I was mistaken, then. Also, you might be able to achieve the same thing for your first solution by using a lazy pattern match, like this: tail :: List a -&gt; List a tail as = bs where ~(_,bs) = foldr as step zz zz = (nil, undefined) step a ~(cs,_) = (cons a cs, cs)
My Haskell Lookup Table experiences have been great. I will recommend it to all my friends.
Blog spam. http://www.reddit.com/user/elcric_circle/submitted/
Excellent presentation. Graphics easy to understand. I learned a lot about Bloomberg. I had no idea they are as large as they are.
vty is a nicer library than curses.
While working with Haskell is definitely a plus, would't it be more economical for me to invest in whatever Goldman Sachs is using and taking the [ £350k per year they pay their Strats](http://news.efinancialcareers.com/de-en/174674/take-get-hired-goldmans-hot-strats-group/)? Or in other words: Your job add is a little bit strange. You only state your demands, but not what you are offering. For me this looks like you want some Haskell developers on the cheap.
No problem, happy to see that people are working on such tools :)
&gt; shared_ptr So cycles are memory leaks... I'm interested in whether there is a interesting space between manual memory memory management and full garbage collection. Rust is exploring this and but it doesn't look like an easy space to conquer.
Could stackage be used for testing new GHC versions like this? By building all packages and running their tests the coverage should be quite good..?
Many of the packages don't build currently. https://github.com/fpco/stackage/issues/378
Hmm... I don't want to spoil the party, but... Correct me if I'm wrong, but both attoparsec and cereal benchmarks are evaluating parsing result (a list) to WHNF. Isn't it cheating? Shouldn't we evaluate whole result using deepseq or something?
Does anyone else get a Configuring cabal-install-1.22.0.1... Setup: At least the following dependencies are missing: filepath &gt;=1.0 &amp;&amp; &lt;1.4 with this version when trying to bootstrap cabal-install 1.22.0.0 or 1.22.0.1 in a sandbox?
All the mingw releases give me a 403 forbidden error.
It is. https://github.com/haskell/cabal/commits/master/cabal-install/cabal-install.cabal - Allow filepath 1.4. https://github.com/haskell/cabal/issues/2461
The first project (Generalize cabal to work with collections of packages instead of having a single package focus) seems related to a utility I have made for my own purpose: http://github.com/wavewave/metapackage This utility creates a symlink farm of a collection of packages and treats it as a 'meta' cabal package. I often used it for developing hoodle which consists of many packages. metapackage is not very mature, but it may be worth to check out. Thanks for sharing your ideas!
If you have anything to share, please do!
Write a command-line client to a website (like Twitter/Reddit/Github)
Once you let in infinite loops it becomes an inconsistant logic. Also Bottom is a value not a type. Void is the type you want.
For the meantime: `cabal install cabal-install --allow-newer=filepath`
In Agda programming language if there is some pattern matching cluase that makes no sense and compiler sees it will never occur you don't have to define return value. For example: data Nat : Set where zero : Nat succ : Nat -&gt; Nat data _==_ : Nat -&gt; Nat -&gt; Set where refl : (x : Nat) -&gt; x == x data Bottom : Set where zeroIsNotOne : (0 == succ zero) -&gt; Bottom zeroIsNotOne () Here is the function which takes proof of (0 == 1) and returns bottom. The only constructor of (0 == 1) can be `refl`, which returns equation of type `x == x`, but compiler KNOWS that 0 is not the same as one, so this constructor will not be used. And programmer simply writes `()` to denote that.
No, it's very common to use `return` as a function to wrap an element in a singleton list. The notation `[2]` is not composable. You can use `(:[])`, but many people prefer `return`. I suppose that post-AMP `pure` might gain popularity, but it's the same idea.
Of course, but that stuff not building doesn't mean that there's a bug in GHC. We still need the maintainers to check things, and then either fix things or tell the GHC devs that they messed up. Yes, of course, you can write a script that filters out failures due to say the Monad/Applicative thing, but without that stuff getting fixes it can't be used to test GHC.
&gt; And yet I can think of how to implement a function of type `P -&gt; Bottom` (I believe in Haskell, it'd be something like `const undefined`; in imperative languages, you could just enter an infinite loop or throw an exception or something), [...] So if `const undefined` has type `P -&gt; Bottom`, then what is the type of `undefined`? It's `Bottom`, isn't it? So Haskell is a language in which we can construct a value of type `Bottom`. And as you know, we can use a proof of False to prove anything, so that's why you can get a proof of a seemingly-false theorem, `P -&gt; Bottom`, using it. Now, how do we reconcile the fact that Haskell proves False with the Curry-Howard isomorphism? Well, the isomorphism doesn't say that Haskell corresponds to intuitionistic logic. Instead, the basic version says that the simply-typed lambda calculus corresponds to intuitionistic propositional logic. And then as we look at bigger and bigger languages, other versions of the isomorphism say that those bigger languages correspond to bigger and bigger logics. In theorey, every programming language corresponds to a logic; but in practice, because most programming languages allow non-terminating programs, most programming languages correspond to inconsistent logics in which you can prove False. There are some programming languages, such as Agda, which look like Haskell but have been specifically designed to avoid non-termination and thus to correspond to a consistent logic. In Agda, you cannot write a proof of `P -&gt; Bottom` because Agda does not have `undefined`. &gt; [...] but I can't see how to implement a function of type `Bottom -&gt; P` without knowing P ahead of time. Use polymorphism! In Haskell, this would look like this: absurd : Bottom -&gt; a absurd = undefined And in Agda it would look like this: absurd : forall {A} -&gt; Empty -&gt; A absurd () The Agda version is interesting because it's a valid implementation of a function which returns a value of an arbitrary type `A`, yet it does so without using something like `undefined` and without producing a concrete value of type `A`. The trick is in the `()`: in Agda, `()` does not represent the unit type like it does in Haskell, but instead represents the fact that there are no valid values to pattern-match on. So whereas a function on bool would need to pattern-match on two clauses, producing an `A` in each case: data Bool : Set where true : Bool false : Bool absurdBool : forall {A} -&gt; Bool -&gt; A absurdBool true = ... -- I don't actually have a value of type A... absurdBool false = ... -- never mind two... and a function on the unit type would require one clause: data Unit : Set where unit : Bool absurdUnit : forall {A} -&gt; Unit -&gt; A absurdUnit unit = ... -- I don't actually have a value of type A... a function on an empty type would require zero clauses: data Empty : Set where absurdEmpty : forall {A} -&gt; Empty -&gt; A absurdEmpty () -- no right-hand side needed! and thus, we can write a complete implementation without having to produce a concrete value of type A!
From the GHC User's Guide [section on the `EmptyCase` extension](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/syntax-extns.html#empty-case): data Void f :: Void -&gt; Int f x = case x of { } Note that this is conceptually a total function; every possible alternative is handled by the case statement. In a total language you would never be able to use this function (since you'd never have a value of type `Void`), but it can be defined regardless, thus the type `Void -&gt; a` is inhabited. (The choice of `Int` in the example is arbitrary, of course.) Anything using `undefined`, exceptions, non-termination, or the like is instead taking advantage of the language being non-total, and as such doesn't really say anything about the corresponding logic.
Good stuff. 
My understanding (which admittedly comes from a Scala background) is that `Bottom` is a type that has no values ([Wikipedia seems to agree](http://en.wikipedia.org/wiki/Bottom_type)), whereas `Void` is (one of many) unit types, meaning it has exactly one value (see http://en.wikipedia.org/wiki/Unit_type). Specifically, if a function has a return type of `Void`, then it will terminate, returning the sole value, and because there is only one possible value, it's often convenient to not bother naming it or explicitly representing it, since you already know what the return value will be when the function returns. In contrast, a function with a return type of `Bottom` cannot return, because it cannot produce a value of type `Bottom` (there are no such values), and so it either has to enter an infinite loop, or throw an exception, or do something other than return a value. 
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Bottom type**](https://en.wikipedia.org/wiki/Bottom%20type): [](#sfw) --- &gt; &gt;In [type theory](https://en.wikipedia.org/wiki/Type_theory), a theory within [mathematical logic](https://en.wikipedia.org/wiki/Mathematical_logic), the __bottom type__ is the type that has no values. It is also called the __zero__ or __empty__ type, and is sometimes denoted with [falsum](https://en.wikipedia.org/wiki/Falsum) (⊥). &gt;A function whose return type is bottom cannot return any value. In the [Curry–Howard correspondence](https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence), the bottom type corresponds to falsity. &gt; --- ^Interesting: [^Country ^boats ^in ^Bangladesh](https://en.wikipedia.org/wiki/Country_boats_in_Bangladesh) ^| [^Hand ^bailer](https://en.wikipedia.org/wiki/Hand_bailer) ^| [^Up ^tack](https://en.wikipedia.org/wiki/Up_tack) ^| [^Top ^type](https://en.wikipedia.org/wiki/Top_type) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cphsw8j) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cphsw8j)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Now fixed.
The AMP part of BBP made the Prelude more principled. In what way did the FTP part make it more principled? Why is changing functions to make them more polymorphic principled? The justification I have seen for it is that people who usually write in a polymorphic style felt that this change would reduce the number of qualified imports they need.
I hope you'll find a growing and thriving Haskell community there. If you don't immediately find that, don't give up. It's happening in more and more places now, and you can help make it happen in New Orleans. Good luck! A few years ago, one prominent Haskeller (whose name I won't mention here) mistyped the coordinates for his geolocation marker on [Haskellers.com](http://haskellers.com) and placed it in the water just off the coast of New Orleans. I think today you can do much better than that, though. :)
My understanding is that Void in Haskell is uninhabited, while Void in e.g. C++/Java is equivalent to Unit (an uninteresting value).
I thought the order of the highlights in this change - i.e. https://downloads.haskell.org/~ghc/7.10.1-rc3/docs/html/users_guide/release-7-10-1.html - should have the breaking changes (AMP,BPP) first and the experimental releases last, so I ended up with this patch - https://phabricator.haskell.org/D736 I'm mentioning it here because it's quite likely that I did something wrong in setting up the patch (it's quite a heavyweight process for just re-ordering several paragraphs of text in a document ;-) so would appreciate comments (or it being pointed out to the "right people" as I can imagine that it's rather hectic in ghc-headquarters right now). 
The usual nomenclature in Haskell-land is that `Void` is the type conceptually inhabited by zero values and `()` (pronounced "unit") is the type conceptually inhabited by one value. But because it's not a total language, both types are inhabited by an "extra" bottom value (which is almost but not entirely unrelated to any notion of a Bottom type). For purposes of the topics you raise, it's common to hand-wave partiality, so yeah, `Void` is an uninhabited type.
Indeed! Very impressive that /u/peargreen could come up with all of those. Of course, I hope others will start contributing soon, too.
Following the types, the basic API seems to be the same as `symbol`. You can use `intern` to get an `InternedString` and `unintern` to get the `String` back out. It also happens to work on `ByteString`, `Text`, and `IntMap`. This should probably be expanded into a more complete description and added to the top level module. If /u/octatoan is uninterested in submitting this PR, I'd be happy to do it.
From what I have seen none of the breakage was anything major. Time spent on that GHC release would probably be better spent on fixing the packages on Hackage which have no active maintainers or possibly on creating a tool which automates the more common changes. 
Why do you find the Haskell Platform unnecessary? 
Who do I talk to about inverting the whole concept, so that a new version of a package can report whether it's made breaking changes instead of every individual downstream maintainer needing to verify each new version? e.g. `filepath 1.4` would have a field `breaks: &lt;1.0`, and every dependent package would just require whatever version it was written with; the requirement is satisfied by the newest `filepath` that doesn't break their dependent version. And somehow magically not breaking everything everywhere in the transition.
This is really cool, thanks! I'm not clear about how the `Route` type works though. I'm guessing that `fragments` are actually path pieces, not fragments, and there is probably no way to specify an actual fragment. Also, that `urlParams` are query parameters in a `GET` request. But how about for other methods where the parameters might be specified in the request body? Can I also put JSON in the request body? 
The package versioning policy, if I remember correctly, specifies that changes in the first two digits indicate breaking changes. In other words, `foo 1.4.5.6` should imply your `breaks: &lt;1.4`. Either way, the breakage is usually partial: only users of the precise corner of the API that's changed are actually affected.
I've always seen Bottom defined as a value which could not be constructed, of type 0 or Void.
Code written for pre-7.10 will almost never compile without change on 7.10, due to BBP. Some software will be released with new versions, but some won't. Commercial software shops will need to support code that only runs on 7.8 for years to come. With the current GHC policies, previous versions of GHC become impossible to run on supported OSes very quickly - within two or three years. I guess they could keep releasing new versions of 7.8 that run on modern platforms for years to come, but that sounds a lot harder than just releasing one version of 7.n that can be bootstrapped with 7.(n+2). Then you are guaranteed to be able to get any older GHC to run on any future platform, forever. You don't even need a full release - just a source tarball. It's time to recognize that GHC is used for real work nowadays. I program in Haskell at work, and I am a big advocate of that. GHC is an excellent, very high quality general purpose compiler, fitting to be used that way. Except for this one point, which is actually a major showstopper. There aren't too many industries in which you can afford to write code that will be unsupportable in two years. And it's really simple to fix. So, no, I don't agree that GHC HQ's time is better spent on those other things you mention. EDIT: s/n+1/n+2/
Hopefully it's better than Arkansas' FP community, which is non-existent according to my searches thus far. I literally have never met another Haskell programmer face-to-face, and have only met only a handful of Scala programmers (and that was during an interview).
To paragraph one: when, in the creation and submission of a package to Hackage, is someone forced to *read* that policy, much less obey it? A gentleman's agreement works for a close-knit academic community, but Haskell is moving away from that (and I think everyone *wants* it to). I'm with Snoyman, this has to be solved with tools. To paragraph two: how does the current system perform any better in this case? In both cases you either need to make a code change, or just update the required version. The difference with my suggestion is that this only happens when a break is *known* to exist, not forced every time one *might* (with upper bounds set downstream); and there's no danger of things breaking despite dependencies passing (as currently with no upper bounds).
Thanks for the reply! &gt; Actual hash fragments shouldn't be getting sent to the server anyway, should they? Agreed, they don't make sense in this context. But they are a part of URI syntax, so it makes the field name `fragments` confusing. &gt; The way that Routes get turned into Requests is handled entirely by the Sendable typeclass… It should be pretty simple to add a new instance of Sendable The use case I am thinking of is that sometimes REST APIs require parameters to be sent in the request body as JSON, or some other format, rather than the usual POST-style url-encoded format. So, would you use the `ByteString` instance for that? Or are you saying that the way to do that is to create your own `newtype` wrapper for JSON and write a `Sendable` instance for it?
/join #yi
Yes, unfortunately. In my opinion, they are a wart. They give the symbol `&lt;-` a new meaning which is superficially slightly similar, but in fact deeply different, than its meaning in every other context. That is very confusing. And it's completely unneeded - anything you can do with PatternGuards you can also do just as easily with `Maybe`, or other simple and standard techniques, with at most a tiny constant-factor increase in code size. The original paper which triumphantly proclaims the critical importance of PatternGuards is just flat out wrong.
The compiler is only a small piece in getting old code to compile, you also need libraries that still work and have security fixes applied. Those tend to be unavailable after about 5 years at most. So I do tend to disagree with the whole concept of long term support by keeping all versions old. At best you get someone who isn't the original author to backport security fixes and we all saw where that can lead with that whole Debian key fiasco a few years ago. You are much better off fixing the things that keep your old code from compiling once and keep using current (or close to current) versions of the compiler.
I think that's where the confusion comes from. In formal language theory, when starting from the lambda calculus and adding side effects you have to model the C++ void return type of methods (i.e. no answer) with unit because functions cannot have no answer. That way you can chain method-functions with (;) :: Unit -&gt; a -&gt; a and you don't have to change anything in your operational semantics. 
I've used PureScript to [quickcheck JavaScript code](https://github.com/purescript/purescript/wiki/Test-your-Javascript-with-QuickCheck)
&gt; previous versions of GHC become impossible to run on supported OSes very quickly - within two or three years. Really? I'm running a recent fedora version and I've got every major version of GHC back to 6.4.3 installed. It's true that GHC 6.2.2 stopped working a little while ago, but still that's about 10 years worth of GHC versions that still work. Incidentally, my company would be very happy to provide a "GHC LTS" support service if anyone needs it.
I dunno. Every time I have enabled `IncohorentInstances` in an attempt to solve a problem I have come to regret it. I've been using `UndecidableInstances` for 10 years and I don't ever recall regretting it. Though, I am not even sure when I am using `UndecidableInstances`, or what it does. I just enable it when the compiler says to ;) 
Using the `LambdaCase` extension as well: f :: Void -&gt; a f = \case
Correct. The situation is: 1. Each GHC binary tarball is created on whatever version of the OS that was current at the time. Other than for Windows, that means that it is not usable in practice for releases of the OS that come out several years later. 1. Each major version series of GHC (x.y.\*) can only be compiled from source on any given platform by bootstrapping from an existing working installation of GHC from that *same* version series on that platform. So compiling from source isn't an option, either. 1. To support a customer, we must have a working GHC that will compile the version of our code that the customer is using, on the platform that the customer is using. For Ubuntu, Herbert has been generously providing a PPA with binary ports of older versions of GHC to modern Ubuntu releases. When it exists, that is a great solution for Ubuntu, and possibly other deb-based Linux distributions. But that doesn't help for other platforms. And it's not officially part of a GHC release, just something Herbert has been doing recently, so it's hard to base our promise of support on that. As much as I love the convenience of Herbert's PPA, and as much as I appreciate the time and effort he invests maintaining it, I would really rather have him spend just a little of that time creating an official future-proof source-only GHC release. Then I'll know that we're covered, no matter what.
That seems smart. I'll take a look at them later today. Thanks!
Example of the impact of AMP: Preprocessing library ansi-wl-pprint-0.6.7.1... [1 of 1] Compiling Text.PrettyPrint.ANSI.Leijen ( Text/PrettyPrint/ANSI/Leijen.hs, dist/dist-sandbox-bd2d75c7/build/Text/PrettyPrint/ANSI/Leijen.o ) Text/PrettyPrint/ANSI/Leijen.hs:312:24: Ambiguous occurrence ‘&lt;$&gt;’ It could refer to either ‘Text.PrettyPrint.ANSI.Leijen.&lt;$&gt;’, defined at Text/PrettyPrint/ANSI/Leijen.hs:372:3 or ‘Prelude.&lt;$&gt;’, imported from ‘Prelude’ at Text/PrettyPrint/ANSI/Leijen.hs:76:8-35 (and originally defined in ‘Data.Functor’) ... tasty-0.10.1 depends on ansi-wl-pprint-0.6.7.1 which failed to install. 
1. With absolute certainty? You can't, of course. But you have *far* more implicit trust. First, there's only point of failure: all the multitudes of dependent packages have a single fixed version for their dependencies, i.e. no moving parts; they won't go stale just because of too-conservative upper bounds, nor will they suddenly break because of too-liberal (or nonexistent) one. Second, the person who controls that one moving part is the author of the thing that's *actually changing*, i.e. the person best equipped to know whether a break might occur. And by putting it in a self-explanatory field in the package description, it *doesn't matter* if they never read the PVP; rather than hoping a standard is followed, breaks are explicitly documented *relative to* whatever numbering the author may have used. Standards are still nice for communication between people, but the tooling no longer breaks just because someone used a different numbering system. 2. (Should be clear at this point, but just to be comprehensive...) You know because you're the author/maintainer who made the breaking change; or, downstream, because the author told you via a new version `breaks:`ing your dependency.
I used QuickCheck and the FFI to check a C memory suballocator!
&gt; but not what you are offering They're offering a job. If you want to negotiate salary I have no doubt it is competitive.
That is one way of dealing with non-termination and error conditions, and it's the way those things are dealt with in SML's definition, which is given through the operational semantics of the language. Because it is a strict language, those errors and non-terminating values can't 'hide' in runtime variables, so it is easy to not consider them as values at all The Haskell Report, on the other hand, is given in terms of a denotational semantics, in which all expressions map to some value in the mathematical structure that we use to express program meanings. Because the normal mathematical language of types as sets can't describe some features of programming languages, we 'lift' the sets to a mathematical object called a Scott domain that includes an ordering of 'definedness' and a least-defined value, which we know in Haskell as bottom or undefined.
Not everyone had used Erlang?
Some feedback. 1) overall, all my code compiles and works after small adjustments according to the [migration guide](https://ghc.haskell.org/trac/ghc/wiki/Migration/7.10#GHC7.10.xMigrationGuide). I added `FlexibleContexts` in one module and `import Prelude` in a few to get rid of redundant imports warnings. 2) The biggest "problem" I had was that I had `import Data.Foldable (foldMap)` and because of that, adding `import Prelude` didn't help ridding the redundant imports warning. So, I just removed the `(foldMap)` part and GHC 7.10 was happy. However, GHC 7.8.3 threw at me a bunch of unambiguous name errors for `mapM_`, `maximum`, `find`, `elem`, `notElem`, `foldr`, `any`, and `concat`. The only solution other than resorting to CPP was to use hiding like this: `import Data.Foldable hiding (mapM_, maximum, find, elem, notElem, foldr, any, concat)` and together with `import Prelude` this makes both 7.8 and 7.10 happy with no warnings. I think the migration guide should suggest using the trick with `hiding`. 3) I got smaller binaries. For my two executables, I got (respectively): * 17Mb and 19.2Mb for GHC 7.8 * 15.6Mb and 17.6Mb for GHC 7.10 on Ubuntu14.2. That's very nice but can anybody explain why? Since I am using `lens`, GHC 7.8 would always link many packages during build: Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. ... (60 or so) more ... GHC 7.10 no longer does it, which is great! Is that the reason the binary sizes are smaller? Can someone please explain why GHC 7.10 does not have to link them anymore?
The single value that unit types, by definition, have. And since there is only one such value, the value is often not given a name, and has no interesting properties other than existing.
The point is that your breaks suggestion would not be any different from upper bounds. It would still require updates to packages which would compile fine because they don't use the part of the API that changed. 
&gt; Indeed! Very impressive that /u/peargreen could come up with all of those. Also, OP waited a whole month before (s)he announced the subreddit. One month dedicated work, 30 posts, that's actually a great start for a niche subreddit.
I'm pretty sure I got it working after I fixed a bug where GHC would always select the later version of the ghc library when there were two versions available. I guess I'll check it out again.
It is draft-y and haphazard, but I think there are some good bits there anyway, so I thought I should post it after all.
Are you sure you are not confusing with its use in guards, using its standard definition as `True`? That's different from actually binding it, which would break its use in guards in nested patterns...
&gt; Why would libraries not be available after 5 years? Will someone pull them off Hackage at that time? I am saying even in environments with longer support (e.g. Debian Stable, Ubuntu LTS, RHEL,...) you won't get even fixes for critical security issues for much longer than about 5 years. Despite what some people seem to believe old does not automatically mean stable. The world is changing around you, there are very, very few use cases where you truly never need to update a library because you are so isolated from the world around you. There might be security fixes, new regulations and laws, fixes for critical bugs (e.g. one that might delete files by accident or some similar issue you can't just live with),... Providing support does not mean that you never update your code or toolchain, providing support means your customer won't have to change the systems around yours (people or other software) to accomodate your updates.
I've checked: changing whnf to nf doesn't change results much.
I think `DiffArray` is pretty close to deprecated because its theoretical speedup rarely works in practice. (I recall from way back that something like `MVar` locking was a killer.) Don Stewart said [in this Stackoverflow answer](http://stackoverflow.com/a/14344521/1088108) that it is obsolete and suggests `vector` instead.
Presumably it has something to do with the fact that instance signatures are useless (from GHC's perspective) if they specify the same type as the class but GHC can't know which specialization of the signature you had in mind.
https://github.com/batterseapower/ansi-wl-pprint/pull/11
Relevant: https://github.com/gelisam/frp-zoo
&gt; 3) I got smaller binaries This is probably just due to random patches over the course of the release that brought binary sizes down a tad... it probably can't be attributed to anything in particular. The reason why 7.10 doesn't show the 'Loading ...' messages anymore is simply a verbosity change (it was a 1 character patch to fix) - if you use `ghc -v2` it starts spitting it out again. We simply decided to suppress it in the common case since it mostly just makes your scrollback really large.
 * ghc-6.4.2 -- ghc-6.10.4 need an old libreadline.so.5 (later ghc does not use readline) * ghc-6.4.2 -- ghc-7.4.2 need an old libgmp.so.3 I've got those two .so files in /usr/local/lib64, they're just the versions from an older fedora release. Other than that it's just the binary tarballs from the ghc download site. I bet any of the old ghc versions could be rebuilt to compile and link against the newer gmp/readline versions. So I don't think it's necessary to have a ghc-x.n that builds with ghc-x.n+1.
I think I remember what the problem was with your patch (and why it wasn't picked up rightaway): you needed tweaks to boot packages such as `transformers` would cause the GHC source-tarball to diverge from the officially released upstream packages. One solution would be to rather provide an out-of-tree patch a user can apply manually, rather than making an official GHC 7.8.5 release with unofficial modifications to upstream packages (which would also require some administration overhead on the Git level as upstream packages are automatically mirrored Git submodules).
As I have explained in another comment: Haskell's unit type is called `()`, not `void`. Perhaps confusingly, it is our bottom type which is called `Void`. I don't know why.
I see. But to compile from source, you need to bootstrap from an existing GHC. How can I do that if the old binary tarball doesn't work on my newer platform, and bootstrapping doesn't work for any newer version of GHC? For the binary tarball, it's more than just gmp and readline. For example, the installer relies on some pre-compiled binary utilities that might require an older glibc. I don't know exactly what they do, and I wouldn't know where to find their source code. There are probably more things you need to know. These kinds of details are trivial for you, but not for the average end user.
Agreed with all of that. Yet, the bottom line is that to support our commercially released software, and before we can apply any patches, upgrade any dependency versions, etc., we must first be able to compile the exact code we released, however long ago that was, on the customers' current OS platform.
Johan posted a proposal for [Cabal PVP compliance checker](http://blog.johantibell.com/2015/03/google-summer-of-code-2015-project-ideas.html). Vote for this! 
Shouldn't you take care of the necessary updates to compile the software on your customer's new OS when the customer updates their OS (a.k.a. the live environment you support)? Doing so only when some issue arises would mean that the customer is actually running the software in a binary version meant for an older OS release until the first issue arises?
Yes, I think people underestimate how important it is. Just have a look at the Mirage project and what they were able to do thanks to the flexible module system. Amazing! 
It's not my only problem but I still like the solution. I'll give it a go.
In a condition system after the handler is done it can optionally allow the code to continue to execute where it left off when the problem occurred, providing some value to that code to tell it what to do. It is closer to an on error callback (potentially with return values to provide data or instructions to the code calling that callback) than to a special return value.
Even the simplest thing like adding the link to `README.md`. Today, there's a link to `CHANGES.md` but why no link to `README.md`?
That was exactly my thought. It shouldn't be too hard to do. The servant api is pretty much decoupled from routing stuff in servant-server. 
It's not, I forgot to remove that import. :0
mtl's `catch` lets you: 1. Handle an error from an arbitrary location down the call chain. At the point that `throwError` was called, the call stack up to that point is unwound. 2. Return an alternative value for the expression denoted by `catch`. conditions's `handler` lets you: 1. Handle an error from an arbitrary location down the call chain. At this point the call stack is left as-is and `signal` directly calls the handler. 2. Do a different action for the expression that called `signal`. This lets you continue the computation down below. You can see this in the types now: catchError :: MonadError e m =&gt; m a -&gt; (e -&gt; m a) -&gt; m a handler :: (Handlers,Condition c r) =&gt; (Handlers =&gt; c -&gt; r) -&gt; (Handlers =&gt; a) -&gt; a Whether the list of handlers is put in a reader monad or implicit params is probably just a matter of judgment rather than technical difference. Maybe something like this would also work: handler :: MonadCondition c r m =&gt; (c -&gt; m r) -&gt; m a -&gt; m a I chose implicit params because I was discussing this with my Common Lisper buddies and the conditions system in CL is seemingly implemented with dynamically scoped variables.
You can download the RC3 right now and try it out ([link](http://downloads.haskell.org/~ghc/7.10.1-rc3/)). If you're not using anything fancy, there may be two primary breaking points, each relatively easily addressed: - Monads without Functor/Applicative superclasses, or custom values shadowing pure/&lt;*&gt;. GHC 7.8 warns about this already. - Undecidable type inference problems when folding structures, similar to how `show . read` is a type error. You'll need an explicit type signature. Both of these will be static errors, and are very easy to fix.
It's similar to `Cont` in the shape of the `(c -&gt; r) -&gt; a -&gt; a`. We're passing in a continuation here, indeed. The beauty (I thought) of CL's error handling is that it doesn't require full continuations a la Scheme or any kind of CPS transform; because it's not about jumping to arbitrary places back and forth, there is no callCC here. Rather, just setting up a handler which is called *directly* instead of unwinding the stack first. This means it can be efficiently implemented. This is partly what I was trying to demonstrate with this Haskell library; the simplicity and efficiency of such a system. In CL you use [`HANDLER-BIND`](http://www.ai.mit.edu/projects/iiip/doc/CommonLISP/HyperSpec/Body/mac_handler-bind.html#handler-bind) which does not unwind the stack. [`HANDLER-CASE`](http://www.ai.mit.edu/projects/iiip/doc/CommonLISP/HyperSpec/Body/mac_handler-case.html) does unwind the stack, but is implemented in terms of `HANDLER-BIND`, with an explicit `RETURN-FROM` call to do the unwinding. In Haskell I achieved this via the already present unwinding support, i.e. `throw`. CL's whole standard library and user libraries are implemented like this. Often when an error occurs, instead of just seeing an exception printed, you are often presented with a choice. See just [the preview picture for this video](https://vimeo.com/77004324) at the top right; retry load, load something else, skip loading, abort, etc. These are called [restarts](http://www.gigamonkeys.com/book/beyond-exception-handling-conditions-and-restarts.html), but it's just a way of including in the condition `c` some extra choices: "by the way, you don't have access to my current state down here, so I'm going to give you some options to call instead, if you like." But that's more of a extension to the essence, I believe. And it is an interesting idea I'd like to explore more in Haskell; how to expose a condition with a list of restart choices.
the existing answers are good, but there's a very clear and straightforward ML implementation by Oleg here: http://okmij.org/ftp/ML/resumable.ml which might be enlightening.
Herbert emailed me about this earlier today actually, so I've [written a blog post](http://www.reddit.com/r/haskell/comments/2zh8we/stackage_and_ghc_710_update/) on that status.
Btw. as for verbose style: https://existentialtype.wordpress.com/2014/03/20/old-neglected-theorems-are-still-theorems/
&gt; 1) Is there any reason for a code that works now just stop working with the new Prelude? Isn't it just more general? There are several possible failures: * Since some types are now more general, you might get ambiguity errors that you have to resolve with an explicit type signature. * You're likely to get tons of warnings about thing like `import Data.Monoid (Monoid)`, as these identifiers are now (also) exported from the Prelude.
A bunch of libraries aren't building just yet with RC3, so it may be wise to hold off on that (unless you want to just for fun). Most of it has to do with restrictive upper bounds on dependencies, and it will all be sorted out by the time 7.10 is released. FWIW, when 7.8 came out, I moved a medium sized business application to it from 7.6, and it required almost no work. I say almost because GHC 7.8.2 can hang during compilation while performing some optimizations, so I had to add a compiler flag to prevent optimizing (-O0).
Neat! I built some elements of conditions once in a project of mine, and anticipated building out further support. But it turned out that the really hard thing is giving the user "enough" control in a pure environment and without a full repl such that they can actually do anything interesting once we hit a break. Furthermore, I might have pushed through that had the project tended to be run supervised -- as it was, all this stuff was part of an automated scheduled chain, and so what was important was anticipating and reacting to possible errors _ahead of time_ at which point typical exception mechanisms seemed a better fit. I think the general conditions approach has great promise though, and the CSV and IO examples are pretty excellent use cases for how this sort of flow control makes sense :-)
At this time, `plugins` is pretty much defunct. Changes in default cabal settings (`library-for-ghci`) breaks plugins by default. A lot of `eval` results are buggy. From a session reported 2014 April: GHCi &gt; :m + System.Eval.Haskell &gt; a &lt;- eval "(3+4)::Int" [] :: IO (Maybe Int) &gt; b &lt;- eval "fst (5,6) :: Int" [] :: IO (Maybe Int) &gt; c &lt;- eval "(6 * 7) :: Int" [] :: IO (Maybe Int) &gt; a Just 7 &gt; b Just 7 &gt; c Just 7 The maintainer, Jeremy Shaw, pointed me towards [plugins-ng](https://github.com/Happstack/plugins-ng). Which is also going nowhere. There are a number of other known bugs with `plugins`, e.g. regarding memory leaks when trying to unload plugins. Basically, if we want plugins working again, somebody will need to step up and become an active maintainer, or (perhaps wiser) take over the `plugins-ng` work - because starting from scratch is not a bad idea.
OK I restarted the thread. Thanks!
Awesome, thanks! I believe I can! Each would have a minimum purchase of 25, so if you could get a bunch of people interested in a color, that would work too. The nice part is we'd only have to pay for the mold 1x so it'll end up being cheaper in the long run. What color schemes would you recommend?
Do you plan to add &lt;restart&gt;s?
No they don't.
What ever happened with FTP? Did we decide to go for it in 7.10?
[Here](https://github.com/nickspinale/brainf--k)'s one using attoparsec
Really good? Or just really prolific?
Can we just boycott windows? Edit: calm down wintards, any more downvotes and I will be shadow banned by reddit's dissenting opinion killer. Lets just agree that windows is utter crap and we only support it cause we have to. I have no sympathy for an OS and company that have been systematically killing competing technologies and stinging innovation for decades (ex OpenGL).
[Comment elsewhere](http://www.reddit.com/r/haskell/comments/2zglpo/toy_project_conditions/cpive9x) suggests yes. &gt; And it is an interesting idea I'd like to explore more in Haskell; how to expose a condition with a list of restart choices.
Except for the part where Microsoft is a huge supporter of Haskell, including hiring SPJ as a researcher... 
Very cool project! I wrote a [post on Haskell-cafe at some point](https://mail.haskell.org/pipermail/haskell-cafe/2014-February/112709.html) about my own little experiment with Lisp-style restarts. By the way, I also think restarts are underappreciated. I've been interested in using conditions instead of function calls even in non-exceptional situations, in order to get a kind of inversion of control...
Yes. https://mail.haskell.org/pipermail/libraries/2015-February/025009.html
That's a great blog post, thanks for the find!
Thank you for the link.
Not supporting the most popular OS in the world is basically asking the language to never be used in industry.
Did you find the [other](http://conway.rutgers.edu/~ccshan/wiki/blog/posts/WordNumbers2/) [3](http://conway.rutgers.edu/~ccshan/wiki/blog/posts/WordNumbers3/) [parts](http://conway.rutgers.edu/~ccshan/wiki/blog/posts/WordNumbers4/)?
As a meta-observation (also, changes in `Win32` won't affect me): I wonder if there would be such a seemingly high level of breaking-change consensus if this was the `unix` package we're talking about. I suspect there would be a large outcry for creating new proper functions and leaving the old ones in place deprecated, to avoid breaking vast amount of existing code and packages relying on the old broken API.
Common Lisp. It's my #2 favorite after Haskell. 
Good thing Windows is only the most popular OS in a rather limited niche then (not that I disagree with the general sentiment that we should support Windows).
They did specify “American”, which I presumed to refer to the short scale.
Fantastic, excellent idea, well worth the wait. This is one I will use. Thanks!
It's only the most popular for desktop applications. It's not most popular on embedded devices, mobile devices, or servers.
Thank you. 2 hours are long when It should only take 5mn (or even 1) and it's bed time and the deadline is passed ;-)
One of my libraries, [`linguistic-ordinals`](http://hackage.haskell.org/package/linguistic-ordinals), will get you from Cardinal to Ordinal. [The code](https://github.com/argiopetech/linguistic-ordinals/blob/master/Text/Ordinal.hs) may give help with intuition on cleaning up your code.
I thought this was one of the wildest talks at Compose. It was great seeing program verification techniques being applied to vastly different ideas, like cellular structure and interaction, and it was awesome to see how things that we consider "not up to snuff" for some of our hard analysis problems can still really help advance the state of the art in other fields. It's also really nice to get an "angle" from my background to start to see what sort of problems people work on in vastly different fields, and to be reminded that there are really great important questions that may _use_ computers in passing, but whose answers aren't supposed to be _about_ computers :-)
I enjoyed this talk. It won my award for Most Horrifying Idea You've Never Stopped to Consider for the example of the problem of skin homeostasis.
It's a graphical game that shows graphical objects on screen. An object-oriented language would be more appropriate. Is it a technical feat, sure. But is this anything more than an impractical academic exercise?
Especially given that they used reactive programming, a tool designed to avoid callback hell from OO, this is more than academic. 
Calling someone obnoxious while threatening them with 'last warnings' to show what a powerful being you are. Cute.
Well it is a dissenting opinion obviously, by definition. But that is not what I was referring to when I mentioned the shadow ban. Reddit automatically shadow bans users that get a lot of downvotes quickly so anything that is against established opinion will quickly be buried and hidden and the user will be shadow banned for like 2 days or sometimes permanently. The reason I have -25 is because 25 more people downvoted me than upvoted me. Just to clarify for you. You think I have nothing of value to say? Fine that's your right, I went through your recent comment history and also found nothing of value. But that's really not a good reason to be trying to hide and delete things you don't agree with or prevent them from being said. So you can take you 'last warnings' and stick em up your ass for all I care.
Just deconstruct in your lambda argument. Prelude&gt; (\(a,b) -&gt; a+b) (1,2) 3 Prelude&gt; (\(x:xs) -&gt; xs) [1,2,3] [2,3] Note that it's not really pattern matching since there's no branching on whether it matched. 
They aren't truly first-class, but it does allow custom patterns: https://ghc.haskell.org/trac/ghc/wiki/PatternSynonyms
&gt; You just look like a silly little child with your sad empty threats Unlike the guy creating wasting his time creating new accounts just to prove the meanie wrong
I don't know of a standard solution, but I'd probably use `Data.List.mapAccumL` to map each `x` to `f x + acc`, replacing the accumulator at each step with `g x`. Approximately this: staggered3c :: (Num a, Num b) =&gt; (a -&gt; b) -&gt; (a -&gt; b) -&gt; [a] -&gt; [b] staggered3c f g = snd . mapAccumL (\a x -&gt; (g x, a + f x)) 0 This will, however, force sequential execution, at least inasmuch as sequential creation of thunks. I believe that full evaluation of the values can still happen in any order. If you want to change how much memory the function has, you can use e.g. an n-tuple instead of just a single value.
Now that I think of it, I can't really come up with a good reason for non-sequential execution of staggered map/zip for general f,g,h,x. Also, I just thought of another raw recursion solution, without an accumulator. Push the second element back into the list for the next step: staggered4 f g h (x:xs) = (f x) : inner (x:xs) where inner [y] = g y : [] inner (y1:ys@(y2:_)) = h (f y2) (g y1) : inner ys Any thoughts on this version? I think that one of its advantages is that it doesn't require an identity element for h. What about efficiency vs the accumulator version? 
Very cool. This is my first exposure to arrows. So, to the result of your staggered, I would need to prepend (f x) and append (g x). Do I understand your code correctly? With a sequence, attaching the edge cases wouldn't be a problem. Is there a good way to do this with just a list?
Thank you, that's a very helpful explanation, I'll read further into it. I knew my description was based on a little wishful thinking and unicorns. 
That is basically what is argued for in the [GHC User's Guide](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/syntax-extns.html#empty-case), it's not immoral enough to stop the revolution :)
Is level 6 stupid hard? It took me ages! Then the next few levels seemed easy again. Maybe I just missed something obvious :/
You may also want to see this paper, which provides some justification for the wishful thinking and unicorns: http://www.cse.chalmers.se/~nad/publications/danielsson-et-al-popl2006.html It gets pretty technical, and you'd have to be familiar with how denotational semantics and Scott domains work to follow the reasoning in the proofs, but you can probably get the gist of it from the discussion at the beginning.
Take a look at view patterns? https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/syntax-extns.html#view-patterns
[Apologies for my lack of proof-reading/coherent narrative on this. I'm up late, sick with a stomach flu, and I'm using your question to distract me.] A "free functor" F is left adjoint to a "forgetful functor" U. We write the fact that F is left adjoint to U, using the notation `F -| U`. This says there is an adjunction between F and U, or equivalently that F is left adjoint to U, or equivalently U is right adjoint to F. To talk about freeness you need to talk about what the other side of the adjunction there is forgetting. But to get there you need to know what a forgetful functor is, and what an adjunction is. Let's start with forgetful functors: A forgetful functor is a functor that just drops some stuff on the floor. _Forgetting a monoid:_ We have a category `Set`, where the objects are sets and the arrows are functions. We have a category `Mon` where the objects are monoids, given by a set `M`, an associative binary operation `mappend :: M -&gt; M -&gt; M`, and a unit for that operation `mempty :: M`, so our objects look like `(M, mappend,mempty)` and the arrows are monoid homomorphisms. A monoid homomorphism between monoids M and N is a function `phi :: M -&gt; N` such that phi mempty = mempty phi (mappend a b) = mappend (phi a) (phi b) Now `Mon` is a lot like set, except our objects have more structure. The arrows here are functions, you can just say more about what them. We could define a forgetful functor U : Mon -&gt; Set that maps an object (M,mappend,mempty) to its underlying set M, and which takes a monoid homomorphism `phi :: M -&gt; N` and just remembers that it is a function giving you the function between M and N. Just forget the extra laws. We lost information along the way, so clearly there isn't a real inverse to this functor. But how close can we come? To get there we need adjunctions. Let's put this example on hold for a minute and take a trip to see what an adjunction is. An adjunction `F -| G` between two functors `F : D -&gt; C` and `G : C -&gt; D` is a "natural isomorphism" between arrows `F a -&gt; b` in `C` and arrows `(a -&gt; G b)` in `D`. That is a mouthful, but if we fix both C and D to be the category of haskell data types "Hask", which acts a lot like set, we can find an example: Saying `Identity -| Identity` is making the claim that `Identity a -&gt; b` is isomorphic to `a -&gt; Identity b`. Saying `(_ , e) -| (e -&gt; _)` is making the claim that `(a, e) -&gt; b` is isomorphic to `a -&gt; (e -&gt; b)`. Indeed it is. We have witnesses `curry` and `uncurry` to go back and forth between these two representations. Now to return to the example of our forgetful functor `U : Mon -&gt; Set`. What must _any_ functor `F : Set -&gt; Mon` do? Well, it has to take objects to objects and arrows to arrows. On objects: Given a set `a` there needs to be a monoid `F a` that it maps it to. We also need to figure out how F works on arrows: It has to take any function on sets `a -&gt; b` and build a monoid homomorphism between `F a` and `F b`, such that `F f . F g = F (f . g)` and `F id = id` So far, there are many such functors we could pick for `F`, but the one we're interested in is the one such that `F -| U`. For _that_ to hold: given any Set `a`, and Monoid `m`, we need to be able to construct that natural isomorphism we talked about earlier. That is to say every monoid homomorphism from `F a` to `m` must be in 1-to-1 correspondence with a function from `a` to `U m`. Therefore, any function from `a` to the carrier set of some monoid, must induce a monoid homomorphism from `F a` to that monoid, and conversely every monoid homomorphism from `F a` to any monoid `m` must be determined entirely by such a function! With some finagling you can show that, in Set, the functor List is left adjoint to U. "List is the free monoid" because `List -| U` To do this right, we'd need to define how list acts on objects, and arrows, then show the above property holds. So what does List do? * List acts on objects in the category of sets `Set`, each of which is a set, by taking a set `a` to the monoid `List a` of lists of elements of that set. * List acts on arrows in the category of sets by taking any arrow from set from `a` to `b` to the monoid homomorphism between `List a` and `List b` given by mapping that function over each element of the list in turn. * for `List -| U` to hold, we'd need to know that every monoid homomorphisms from `List a` to `m` are in 1-1 correspondence with functions from `a` to the carrier set of the monoid `m`. In the reverse direction, it says given a function `(a -&gt; U m)`, I'll give you a monoid homomorphism from `List a -&gt; m`. If we only had finite cases co consider, then in Haskell terms `(a -&gt; U m)` is just a function from `(a -&gt; m)`, we lift the constraint that m is a monoid out into a dictionary, so you get: foldMap :: Monoid m =&gt; (a -&gt; m) -&gt; List a -&gt; m such that for every function f, `foldMap f` is a monoid homomorphism, and in the opposite direction the 1-1 correspondence says that _every_ monoid homomorphism `phi :: List a -&gt; m` no matter how exotic is completely determined by some function from `a` to the carrier set of `m`. *Free Monads* We can build another category of monads, where the arrows between them are monad homomorphisms. In haskell terms a monad homomorphism phi :: M -&gt; N, which is a natural transformation from M to N phi :: forall a. M a -&gt; N a that satisfies: phi (return a) = return a phi m &gt;&gt;= phi . f = phi (m &gt;&gt;= f) A monad is after all a lot like a monoid above, where a monoid was a set equipped with mappend and mempty and some laws, a monad is a functor M equipped with `return` and `join`. We could build a functor `U : Monads -&gt; Endofunctors` that forgets `return` and `join` and just remembers that our monad M is a functor. In the category of endofunctors over `Set` our objects are functors from Set -&gt; Set and our arrows are natural transformations. Then we proceed as before: U : Monads -&gt; Endofunctors is a forgetful functor. On objects `U` takes `(M, return, join)` to the functor `M`. On arrows `U` takes a monad homomorphism phi and just remembers that phi is a natural transformation. (In the category of endofunctors our arrows are natural transformations). Then we need to find a definition for F : Endofunctors -&gt; Monad Any functor F from the category of endofunctors to the category of monads would take an an object in "endofunctors" (which is a functor in its own right) and give back a monad. and given a natural transformation `eta :: f ~&gt; g`, it needs to be capable of giving us a monad homomorphism between `F f` and `F g`. But the particular F we're looking for is left adjoint to our `U : Monads -&gt; Endofunctors`. For lack of a better name, lets' call F `Free`. So, since `Free -| U`, we have: The monad homomorphisms from `Free f` to any monad `m` have to be in one to one correspondence with the natural transformations from `f ~&gt; U m`. As before U m just "forgot" that `m` was a monad, and we're basically asking for the existence of foldFree :: (Functor m, Monad m) =&gt; (forall x . f x -&gt; m x) -&gt; Free f a -&gt; m a such that for any natural transformation `f : F -&gt; M`, `foldFree f` is a monad homomorphism from `Free F` to `M`, and given any monad homomorphism `g` from `Free F` to `M`, it is entirely determined by some natural transformation `f` from `F` to `M` such that `foldFree f ~ g`. So we're asking for the existence of `foldFree` and making very very strong claims about the properties it must have. But what if i forgot even more? After all, anything free is relative to what you forget! You can forget more about `f`, not even remembering that you can `fmap` over it, just remembering that it has kind `(* -&gt; *)` and look for a left adjoint to that forgetful functor. When you do you find Heinrich Apfelmus' "operational monad," which is the free monad given a type constructor of kind `* -&gt; *`. We even have a good candidate for such a definition! What if we take the definition of Free to _be_ that we can run `foldFree` on it (without even knowing `f` is a Functor). foldFree :: Monad m =&gt; (forall x . f x -&gt; m x) -&gt; Free f a -&gt; m a foldFree f m = runFree m f newtype Free f a = Free { runFree :: Monad m =&gt; (forall x. f x -&gt; m x) -&gt; m a } All we did was flip the definition of foldFree and get 'how to run' `Free f a`, the only exercise that remains is to show that every monad homomorphism from `Free f` to `m` can be generated by calling runFree with an appropriate argument. As an aside: For a sufficiently general view of "monoid" that permits examples that don't feel like the Set-examples we learn when we first hear what a monoid is, you can show that monads are just monoid objects in a particular monoidal category, and you can get a t-shirt-quality saying out of it.
Note that although `(***)` is from `Control.Arrow`, it's instantiated here just for normal function space. So the type simply becomes: (***) :: (a -&gt; b) -&gt; (c -&gt; d) -&gt; (a, c) -&gt; (b, d)
Is there any possibility to get the course material?
- "There are no graphical games made with FP because object oriented is better suited for the task." - "A graphical game was made in an FP language - why didn't they make it in an OO language? That would be more appropriate." You can't win.
its really great this is possible. i wish you guys luck with the sales. i'd love it if somebody made a tutorial for a simple android game in haskell. 
Ha what timing :) Just done essentially the same for Python: https://blog.wearewizards.io/using-haskells-quickcheck-for-python
*cough* reddit gold... *cough* gilding posts... *cough* :-)
A free monoid is a list of values. A free monad is a "list" of functors.
Counterpoints: * Prisms fail to give you exhaustive matches for insufficiently polymorphic data types (e.g. `data Color = Red | Green | Blue`), forcing you to fall back to normal pattern matching. * first-class-patterns require a locally point-free style, decoupling the naming of bindings from their pattern match site; this is still usable, but nowhere as nice as actual first-class cases. * Generics don't even count, both because you're pattern matching on something different and because you're still relying on normal pattern matching. Of course, none of this has anything to do with what OP seemed to want.
[Link](https://bondi.it.uts.edu.au/) for ~~the lazy~~ people who had difficulty finding it. Also related is [MLPolyR](http://arxiv.org/abs/0910.2654), though it goes nowhere near as far as Bondi.
I think it would be hard to find a general solution for the edge cases, because it depends on h. In your example, h was (+) so it was natural to insert a couple 0s. What about other functions? Hard to say. What if it was (*)? So, yeah, you'd have to append and prepend some items. Maybe like: staggered f g h z xs = let extraXs = z : xs ++ [z] in map ((uncurry h) . (f *** g)) . pairs $ extraXs Then you could call `staggered f g (+) 0 xs` to run your example, or `staggered f g (*) 1 xs` to handle a different h with a different identity. p.s. Arrows are a bit weird... I like /u/chrisdoner's suggestion of bimap. :)
That's awesome. It's so quick, too! :)
/r/mildlyinfuriating -- that case expression
Nice! I like these ickle apps.
Not really. Writing mobile games and apps and shit is really not important at all. It doesn't matter what you write them in because any language is adequate for these simple and irrelevant tasks. I see haskell as a great tool for solving complex real-world problems, I don't see the need to use haskell or rave about the use of haskell for meaningless irrelevant shit.
You stated the issue much more eloquently than I did. http://haskelle.blogspot.com/2015/02/is-bottom-haskell-red-headed-stepchild.html
Brent Yorgey is moving out there to take a tenure track position at Hendrix in Conway, AR this summer, so you should have at least one peer in the state soon. I think Zachary Slade is also playing around with Haskell down there -- in Fayetteville if I recall correctly from a conversation we had one evening at ICFP 2014.
Message back here once you made up your mind or have something sorted. This is a great tool, thanks and well done. 
If this would only show intermediate steps... But very great tool! 
&gt; Usually, types in Haskell are rigid. Except, with AMP and FTP/BBP types are already becoming less rigid, across base. In many ways, that's a good thing; by having a more abstract type, you further constrain the implementation while simultaneously increasing valid call sites and thereby code reuse. Sometimes it's a little harder to understand because the graph of knowledge dependencies is deeper, but ofttimes the individuals nodes in that graph get smaller AND more connected. I always write the rigid version first, and then generalize as need/utility arises. FWIW, /u/edwardkmett has stated that lens is exactly as general as people have needed it, although some of the generalization comes directly from his needs -- i.e. none of the general types (constraints vs. concrete types) are there simply for the sake of generalization.
Great idea! Fortunately the pointfree package makes this easy. I created [a pull request](https://github.com/tfausak/blunt/pull/2) to address this. Edit: Blunt now shows the intermediate steps! 
&gt; Backward composition of lenses. [Lenses compose normally.](http://www.reddit.com/r/haskell/comments/23x3f3/lenses_dont_compose_backwards/) You are doing yourself and your readers a disservice by repeating the claim that they compose backward.
Optics are generally defined like type Lens s t a b = forall f. Functor f =&gt; (a -&gt; f b) -&gt; s -&gt; f t or type Prism s t a b = forall p f. (Choice p, Applicative f) =&gt; p a (f b) -&gt; p s (f t) But if you define a function that then takes an optic: below :: Traversable f =&gt; Prism s a -&gt; Prism' (f s) (f a) that function now has a rank-2 type: below :: (Traversable f, Choice q, Applicative h) =&gt; (forall p g. (Choice p, Applicative f) =&gt; p a (g a) -&gt; p s (g s)) -&gt; q (f a) (h (f a)) -&gt; q (f s) (h (f s)) This means that type inference goes out the window. APrism, ALens, etc. are a trick. They are about finding a concrete choice of `f` and `p` in the types above that capture all the information about the Lens or Prism in question. type APrism s t a b = Market a b a (Identity b) -&gt; Market a b s (Identity t) data Market a b s t = Market (b -&gt; t) (s -&gt; Either t a) Now, below expands to a rank-1 type! below :: (Traversable f, Choice q, Applicative h) =&gt; (Market a a a (Identity a) -&gt; Market a a s (Identity s) -&gt; q (f a) (h (f a)) -&gt; q (f s) (h (f s)) And we can reconstitute a prism from APrism: cloneLens :: ALens s t a b -&gt; Lens s t a b closeTraversal :: ATraversal s t a b -&gt; Traversal s t a b clonePrism :: APrism s t a b -&gt; Prism s t a b etc. A first order consequence of this trick, is that it reduces the combinator to rank-1, _vastly_ improving type inference. In other places in `lens` we generalize this trick. Instead of asking for a choice of `p` or `f` that fully characterizes all lenses or traversals, we ask for a particular choice that is _just enough_ for what we need. This lets the combinators in `lens` do deeply fancy things, like work with no constraints when given a lens, but ask you for a Monoid instance when given a Traversal. This maximizes the number of situations we can use the same combinator set in, while simultaneously ensuring that inference "just works". It achieves this by leaking a little bit of information into the type signature you'd think we wouldn't want to leak, and in exchange you the lens combinator "judo flips" you into asking yourself for a class constraint, while remaining delightfully simple. Is this idiomatic Haskell? Not really. It never set out to be: I wanted a reusable vocabulary for getting into and manipulating structures that I'd still be using a couple of years out. Here were are a couple of years later, and the vocabulary is still being used, and new uses for the parts are still being discovered, so that part of the mission appears to be successful. ;)
To perform the simpler task without an identity at both ends, you could write: staggered1 f g h xs = zipWith (\a b -&gt; f (g a) (h b)) xs (tail xs) If you need an identity element to be appended to the lists, you will need to define the function with an additional argument. Of course, if you know that `f` is a monoidal operation, as is the case with `(+)` and `0`, you could define it like this: staggered2 :: (Monoid b) =&gt; (a -&gt; b) -&gt; (a -&gt; b) -&gt; [a] -&gt; [b] staggered2 g h xs = zipWith mappend as bs where as = map g xs ++ [mempty] bs = mempty : map h (tail xs) Then your original addition example can be written like this: staggeredSum :: (Num b) =&gt; (a -&gt; b) -&gt; (a -&gt; b) -&gt; [a] -&gt; [b] staggeredSum g h = map getSum . staggered2 (Sum . g) (Sum . h) I know, it isn't very pretty.
I really don't get this criticism. Lens was born when we discovered that we could do something with Haskell that nobody previously had realized was possible. Even SPJ in his [lens talk](https://skillsmatter.com/skillscasts/4251-lenses-compositional-data-access-and-manipulation) awhile back said, "I don't think we had any clue when we were first designing haskell that you could do *this* kind of thing" (emphasis his). IMO it was a pretty big discovery--one that allowed us to unify a whole bunch of seemingly disparate ideas. So *of course* it's unidiomatic Haskell. And that's not a criticism at all. Using that as an argument for not using lens is analogous to arguing that we shouldn't have switched from horses to cars because the latter was an unidiomatic mode of transport.
`id x = x` -- recursion!
One way of looking at it is that they don't fit in nicely with the rest of Haskell, so there's no graceful transition and the barrier to entry is much higher. Those are real downsides even if the benefits are worth it.
&gt; they don't fit in nicely with the rest of Haskell I'm not so sure I agree. Exactly what about lens do you think doesn't fit? Just because something is hard to learn doesn't mean that it "doesn't fit nicely with the rest of Haskell". I think /u/pinealservo discusses this nicely in [his comment](http://www.reddit.com/r/haskell/comments/2zpuih/lens_is_unidiomatic_haskell/cpl91rt)
&gt;Even SPJ in his lens talk awhile back said, "I don't think we had any clue when we were first designing haskell that you could do *this* kind of thing" (emphasis his). Witheringly minor point: I was at that talk and he was talking about using newtypes and typeclasses together to give different behaviours to the "same" data, rather than lenses. 
I just listened to it again and it seemed pretty clear to me that "this kind of thing" was referring more to lens as a whole. Anyway, whether he's referring to lens as a whole or that specific technique that is used by lens, I think the point still stands.
The beauty of lens is how well it fits into the rest of haskell. The reason for all those incredibly general types in the lens internals is so that lenses (and especially their combinators) work with nearly every haskell type out there. The only thing I haven't had lens work out of the box with was indexed state monads - even then all I had to do was modify the non-indexed lens state combinators slightly (so even they fit in the lens model). 
Yep. APrism expands to "A Prism"
Haskell on Heroku?
Yes, it is running on Heroku. I'm not using any buildpacks though; I'm deploying a binary. Check out [the deploy section of the readme](https://github.com/tfausak/blunt/blob/v0.0.9/README.md#deploy) for specifics. 
Ah, I was asking specifically about this https://github.com/mietek/haskell-on-heroku but clearly you're not using it.
&gt;and a free monad is like tree. But only sort of, when you unroll it? because the structure is still linear as it exists, right? 
Yep, I almost PMed you, but figured you'd see anyway. Your approach is really interesting and anyone who hasn't seem it yet should read that blog post. I'm scrapping the whole internet in search of an implementation of this algorithm. I can't make sense of both LambdaScope and Bohm - they appear not to provide a net→term translation, so it is not useful to get the actual normal forms. If anyone is aware of a simpler implementation - doesn't need to be efficient, just minimal and understandable - please let me know.
Perhaps the paper "Sequential and Parallel Abstract Machines for Optimal Reduction" form this year's TFP is relevant for you.
Do you have a copy of the paper? The links are all broken.
Isn't that one of the main points of newtypes?
ISTR a later paper showing that the optimality came at an cost elsewhere and, in fact, there are two competing metrics and it's impossible to optimize both. But, I can't find that paper, so maybe that was just a nightmare.
I think I've seen that presentation before, but this is the first time I grokked how view is implemented with a van Laarhoven lens. That is truly elegant.
[12tfp2014_submission_30](http://foswiki.cs.uu.nl/foswiki/TFP2014/PresentationSchedule): http://www.cs.uu.nl/people/jur/preproceedingstfp2014.pdf
On a related note, you may also be interested in ["lazy specialization"](https://lukepalmer.wordpress.com/2009/07/07/emphasizing-specialization/). [Thyer's dissertation](http://thyer.name/phd-thesis/) on the topic has always struck me as appealing and just slightly out of tune with what it would need to be something truly extraordinary. 
Nice gradual progression there. Big O notion, smoothly divided into 3 easy parts, then bam, Coyoneda. =) I approve. [Yes, I realize it is a bag of topics, not a course outline, but it still grabbed me.]
Excellent presentations. Very good visual and sound quality. Topics were well thought out. The entry graphics were also excellent! You will quickly acquire a following.
Nice videos :) In your isomorphism at the start of 'Coyoneda', I think you mean to use "exists", not "forall", right?
This is cool! I'm not very experienced with pointfree, and while I was playing around with this I found something surprisingly complicated: f x y = (x + 1) + (y + 2) + 3 is pointfree like f = flip flip 3 . ((+) .) . (. (2 +)) . (+) . (1 +)
Thanks! The results from `pointfree` can get a little ridiculous. [The wiki page](https://wiki.haskell.org/Pointfree) has some good examples of that. For this example in particular, you can get some better results by shuffling things around. f x y = 3 + (1 + x) + (2 + y) f = (. (2 +)) . (+) . (4 +) And while I recognize that you were just playing around, some algebra drastically imrpvoes the result. f x y = 6 + x + y f = (+) . (6 +)
Thank you! F-algebras finally clicked for me. There is something strange with video 3 on F-algebras, though: the blackboard part is played twice.
I'm really curious what `ignus` and `nordom` will be.
I have no plans for those names, yet, but I will hint that the next project in this series is named Sigil.
Open GVIM, select Tools from the tool bar at the top, select SeT Compiler, look for Haskell and its not there. That might be where you want to start first, by adding Haskell to the SeT Compiler list or determining why it has not already been added. There must be reasons it is not there. If it were easy it would already have been done. Now I am talking GVIM and not a Vim-ish text editor written in Haskell; however, the point I am trying to make is you might have a very large project ahead of you. Good Luck!
With the first part showing ugly green flashes, definitely bad for epileptics! Probably the first part was just forgotten when editing.
Looks great! I met you guys in Workshop Coffee in SF and you mentioned you were working on these then. It's not listed as one of the future topics, but I'd love to see a video on categorical semantics of dependent type theory.
To clarify, is it correct that the claim `Coyoneda f ~ f` relies on some kind of abstraction principle? E.g., that `Coyoneda f ~ f` assuming we identify Haskell values up to relatedness in the Wadler free theorems sense?
WeChat was very private IM, not like to join into one WeChat group.
Hey all! Taylor from Loop School here. Not quite sure what happened in the final exporting of the F-Algebras video, but we'll work on it and update the video soon. Thanks for the feedback! 
Thanks for bringing fun into my life, I love it!
Are you going to post other sample videos on the page before the launch? I'm quite interested in a few of the other "in the works" videos.
Free monads are actually quite analogous to free monoids, except instead of categories of monoids and sets, we use categories of monads and endofunctors.
No, there isn't just one implementation. My package [LambdaINet](https://hackage.haskell.org/package/LambdaINet) pre-dates Jan Rochel's package. We exchanged a few emails before he began writing his own. Optimal evaluation is notoriously difficult to understand. 10 years after I first read the [Lambdascope paper by van Oostrom, van de Looij and Zwitserlood](http://www.phil.uu.nl/~oostrom/publication/pdf/lambdascope.pdf) and having implemented it myself, I'm still wrangling with the details from this rather compact paper. Actually it wasn't rigorously proved in the paper that the Lambdascope reduction strategy really achieves levy's optimality. It is known that we can use Interaction Nets to encode optimal evaluation (as well as the usual call-by-value and call-by-need). See [Ian Mackie](http://dblp.uni-trier.de/pers/hd/m/Mackie:Ian.html), [J S Pinto](https://scholar.google.com/citations?user=45QPIOIAAAAJ&amp;hl=en&amp;oi=sra), van Oostrom, and others' work. However, the optimality is only in terms of beta reduction, but not in terms of number of interactions. There have been a number of attempts to optimize these implementations, but none reached the status of being able to rival the conventional call-by-value or call-by-need implementations. Related is a concept called *complete laziness*. It was an idea first coined by [Holst, C. K. and C. K. Gomard](https://scholar.google.com/scholar?hl=en&amp;q=Partial+evaluation+is+fuller+laziness&amp;btnG=&amp;as_sdt=1%2C38&amp;as_sdtp=) but their proposal (of implementing it) was known to be wrong. Both [Thyer's dissertation](http://thyer.name/phd-thesis/) (as /u/edwardkmett mentioned in his reply) and [F R Sinot's natural semantics for complete laziness](https://scholar.google.com/scholar?cluster=12602728027269289432&amp;hl=en&amp;as_sdt=0,38&amp;as_vis=1) touched upon this topic and further discussed it. However, Thyer's and Sinot's work do not agree with each other! After much scrutiny, I tend to think Thyer's work rightly implements complete laziness, while Sinot's work is flawed. However, Thyer's implementation uses memo-table to recover the loss of sharing (in beta reduction), which is deeply unsatisfying from a theoretical point of view. 
Author here. I intended this article to be a starting point for a discussion what we, as a community, should do to improve upon the current state (if at all). I would volunteer for creating a safe-entropy package, but that would leave us with three different ways to generate random data in Haskell; I much rather improve upon the existing packages.
Thanks for putting all this effort in, Michael. I've reached out to one of the maintainers of a dependency that keeps me from supporting GHC 7.10 
I think the worst that happens with `UndecidableInstances` is that you throw the compiler into an infinite loop, but GHC has a cutoff so that's not possible. OTOH, `IncoherentInstances` can cause subtle changes in semantics because different instances may be used for the same underlying type.
From a social/human behaviour point of view, my experience is that trying to do anything multilaterally is *very* hard. I think your best bet is to produce a minimum viable set of cryptographic libraries that you believe meet the required standard and then aggressively promote them. Aiming to reach a consensus, although worthy in spirit, is likely to be a counterproductive goal.
You are most likely correct, although I silently hope that this article might provoke a reaction from the maintainers of the two Haskell entropy packages. If nothing good comes out of this, I will most likely fork the `entropy` package, patch it and keep it interface-compatible with the existing library and then aggressively promote it as you say.
Brilliant answer, thank you very much. So I finally know (or at least : where to read about) what is an `adjunct` functor.
So basically a Free thing is done by doing nothing but just replace any function by a constructor, a bit like thunks works. Ultimately one could define `freeAppend` = undefined `freeEmpty` = undefined and a `freeAppend` b `freeAppend` c is a valid expression which - until being evaluated stays - in memory as : &lt;a `freeAppend` b `freeAppend` c&gt; If we could inspect thunks in realtime, we could get a *list* of things to *append* : `a, b, c`. So *list* is the free functor corresponding to *append* (Monoid). That's my intiution anyway, is it (sort of ) right ? 
You can think of `Free f a` as a tree with an f-shaped branching structure. data Bin a = Bin a a Now `Free Bin a` is a binary trees with answers in the leaves. The grafting happens on the paths from the root to the leaves. You graft sub-trees onto the leaves of the tree and erase the seams. Monads are about "substitution followed by renormalization". You take a monad M with leaves decorated with `a`, and substitute with `a -&gt; M b` for the leaves, giving you a tree of trees `M (M b)`. Then the 'join' / renormalization step erases that seam and enforces any extra invariants in the process. Free monads have no extra invariants beyond erasing the seam between `Free f (Free f b)` to get `Free f b`. If you look at any individual path from the root, it looks linear like a list. But the tree itself is made up of f-shaped branching. This intuition is a little weird when function spaces enter the picture, but still works. Free ((-&gt;) x) a can be viewed as having a 'branch' for every possible 'x' at each internal node, as after all `x -&gt; a` is a^x.
I think that making an API compatible fork is the best route. It will give the existing users of the old package an easy migration path. edit: and an easy way for the original package maintainer to incorporate your changes if/when he chooses so.
I Thijs these are al very valid concerns in the post-snowden era (imo). Too bad I know next to none about crypto. I usually just use crypto like "oh everybody uses gpg. Gpg is probably safe" so I'm very 'vulnerable' to these kind of blind faith attacks Decided to follow a crypto course next semester to broaden my interest . If I knew a bit more I'd like to help
From my perspective a bunch of valid concerns and a small bit of tinfoil hattery. Since I agree, that in cryptography tinfoil hattery is a valid argument, this leaves us with a bunch and a bit of valid concerns. I would like to hear the stance of the library authors, though. Edit: Note: I know hardly anything about cryptography, except for the part that the standards are extremely high, which is why tinfoil hattery constitutes a valid argument. Also I just saw a /r/Dogecoin ad in the sidebar. It says "wow. such crypto" which is an amusing coincidence.
For GHCJS it's probably easiest to modify the try-purescript example. Result here: http://hdiff.luite.com/tmp/pointsfree/ Modified files here: https://gist.github.com/luite/a60cced132d6a3eb7b18 Original example: https://github.com/ghcjs/ghcjs-examples/tree/master/try-purescript also the `blunt` package needs to be added to the dependencies and the `Pointsfree` module needs to be changed to an exposed module for this code to work. (I did not use the snazzy ghcjs-ffiqq quasiquoter in the modified example since I'm working on some TH improvements, which breaks it in my current install. Not using TH just made it easier to test for me) 
&gt; securemem: abstraction to an auto scrubbing and const time eq, memory chunk. Constant time equality (as in, not short-circuiting on the first non-equal byte) is a useful but insufficient property. [Cache timing attacks (pdf)](http://cr.yp.to/antiforgery/cachetiming-20050414.pdf) are one example that is not covered by that.
Yeah I talked about that in the article, about NaCL, libsodium and its Haskell FFI 'saltine'. Saltine, unfortunately, but very wisely, discourages using it in production code.
Like I said in my opening paragraph of the article, I know next to nothing about cryptography either; I just know how to put on a tinfoil hat and act like a hacker anthropologist. I would love to hear a response from the library authors; all the author of the `entropy` package has to do is simply disable the RdRand flag by default and all my concerns would be gone.
I believe the paper you're talking about is Lawall-Mairson ICFP 1996 "Optimality and inefficiency: what isn't a cost model of the lambda calculus?" which 10 years later received the ICFP most influential paper award for that year. The paper is here (not free unless you're at a university, but it has the abstract): http://dl.acm.org/citation.cfm?doid=232627.232639 and a summary of its importance is here: http://www.sigplan.org/Awards/ICFP/#2006 The tl;dr is that "optimal reduction" doesn't mean "efficient"
OK, I'll need to dig in deeper later then. Thanks!
Well, I [raised an issue](https://github.com/vincenthz/hs-cipher-aes/issues/31) about it back in December... a little bit tongue-in-cheeck, but the author didn't respond.
In this case, it not only does memory scrubbing, but also offer constant time comparison operations.
libsodium provides high level APIs for generating random data: http://doc.libsodium.org/generating_random_data/README.html The problem with wrapping libsodium/NaCl is that it makes compilation on Windows quite cumbersome.
For RSA/DSA there's also Vincent's crypto-pubkey package, that seems to be missing from the article. Also his blog contains some information about the motivation for start the tls package and all the primitives it uses, including some comments about reimplementing the wheel: http://tab.snarc.org/posts/haskell/2013-10-25-haskell-crypto-platform.html
Unlimited recursion is easy to do in the lambda calculus, but impossible in simply typed lambda calculus. 
Exactly!
thank mr skeltal
This feels a bit like pure profunctor lenses. In particular, you can get lenses and prisms out of types like type Lensy p s t a b = p a b -&gt; p s t for `p` instantiating various subclasses of `Profunctor`. Your CPS'd lens type ought to be a cover for this. type Iso s t a b = forall p . Profunctor p =&gt; Lensy p s t a b type Lens s t a b = forall p . Strong p =&gt; Lensy p s t a b type Prism s t a b = forall p . Choice p =&gt; Lensy p s t a b My impression is that folds and traversals work into this scheme somehow as well but I've never learned quite how. iso :: (s -&gt; a) -&gt; (b -&gt; t) -&gt; Iso s t a b iso = dimap lensIso :: (s -&gt; a) -&gt; (s -&gt; b -&gt; t) -&gt; Iso s t (a, s) (b, s) lensIso gt st = iso (\s -&gt; (gt s, s)) (\(b, s) -&gt; st s b) lens :: (s -&gt; a) -&gt; (s -&gt; b -&gt; t) -&gt; Lens s t a b lens gt st = lensIso gt st . first _1 :: Lens (a, b) (x, b) a x _1 = lens fst (\(_, b) x -&gt; (x, b)) prismIso :: (b -&gt; t) -&gt; (s -&gt; Either t a) -&gt; Iso s t (Either t a) (Either t b) prismIso review peel = iso peel (either id review) prism :: (b -&gt; t) -&gt; (s -&gt; Either t a) -&gt; Prism s t a b prism review peel = prismIso review peel . right _Left :: Prism (Either a b) (Either x b) a x _Left = prism Left (either Right (Left . Right)) over :: Lensy (-&gt;) s t a b -&gt; (a -&gt; b) -&gt; s -&gt; t over l = l set :: Lensy (-&gt;) s t a b -&gt; b -&gt; s -&gt; t set l s = l (const s) Pure profunctor lenses are really pretty and drive home the symmetry of lenses and prisms. The only remaining tricky part is making `view` and `preview` work, but for this we just need an profunctor like `a -&gt; Constant a b` newtype Forget r a b = Forget { runForget :: a -&gt; r } instance Profunctor (Forget r) where dimap f _ (Forget q) = Forget (q . f) instance Strong (Forget r) where first (Forget f) = Forget (\(a, _) -&gt; f a) second (Forget f) = Forget (\(_, a) -&gt; f a) instance Monoid r =&gt; Choice (Forget r) where left (Forget f) = Forget (either f (const mempty)) right (Forget f) = Forget (either (const mempty) f) and then view :: Lensy (Forget a) s t a b -&gt; s -&gt; a view l = runForget (l (Forget id)) preview :: Lensy (Forget (Maybe a)) s t a b -&gt; s -&gt; Maybe a preview l = runForget (l (Forget Just)) 
I'd love to see `saltine` expanded into a trustworthy binding. It unfortunately fizzled out as something that I needed to maintain for work purposes, so I never got anywhere I really liked with it. Austin Seipp also has a binding called [`salt`](http://thoughtpolice.github.io/salt).
Galois specializes in making objectively verifiable software. It is pretty open about this stuff. Trusted systems is basically Galois game. They're also a great bunch of folks and I hear it is a great place to work. There is no inherent shame in doing work for the DoD. A ton of it is out in the open and benefits everybody. Btw I realize I made a new Reddit account and this is my first post. I'm not some Galois bot ;-).
Yeah, I think it's okay to be wary of a defense contractor, but it would be self-defeating to rule out their contributions altogether given the way technology development works.
It sounds to me like you want something like simulated annealing or genetic algorithms, where the model is a set of polygons or alpha shapes, and the fitness measure is based on the total area covered, # of points covered, and the complexity of the shape information. I don't think you're going to find a computationally simple method to directly calculate a covering shape that meets your needs.
If you're going to mention *that* it functions as an existential, you should also mention *why* it does so.
It's just too easy these days. [OP's PPA](https://launchpad.net/~hvr/+archive/ubuntu/ghc) makes getting whatever version of GHC/Cabal a sinch and sandboxes make experimenting with unfamiliar packages low-risk. A small shell script sets you up quickly and completely with more than enough for a standard install. sudo add-apt-repository -y ppa:hvr/ghc sudo apt-get update sudo apt-get install -y ghc-7.8.4 cabal-install-1.20 cat &lt;&lt;TOZSHRC &gt;&gt; ~/.zshrc # Adds PPA-installed GHC and Cabal to path export PATH="$PATH:/opt/ghc/7.8.4/bin:/opt/cabal/1.20/bin:.cabal-sandbox/bin:$HOME/.cabal/bin" TOZSHRC export PATH="$PATH:/opt/ghc/7.8.4/bin:/opt/cabal/1.20/bin:.cabal-sandbox/bin:$HOME/.cabal/bin" cabal update &amp;&amp; \ cabal install alex doctest ghc-mod happy 'hi==0.0.8.2' hlint hoogle pointfree pointful stylish-haskell &amp;&amp; \ hoogle data
The Platform was never useful to me. When I was a newcomer to Haskell, the Platform just ended up confusing me even more. Only some packages were in it, and when I tried to get other packages, Cabal would spew out errors. I was new so I didn't understand most of it. When I got more experience and I learned some things (e.g. use root to install GHC, leave that install alone, and just install packages in the user account) then I didn't need Platform anymore. Then at some point Platform got really old so there was even less reason to look at it. The only good justification I've heard for Platform is Windows, which I can't speak to since I don't use Windows. It would be good if there were agreement on how best to install GHC and get started. I think the GHC downloads page still encourages Platform use, which I doubt helps newcomers and was actively bad advice when Platform was ancient. Even if the answer looked something like Arch Linux--here's a long sheet of instructions to follow--it would be better than a Platform which is old a lot of the time, along with a random smattering of other instructions from different people which just leaves the newcomer to wonder "ok, so what's the best way to do this?" (edit) GHC's [download page](https://www.haskell.org/ghc/download_ghc_7_8_4) recommends distributor packages if they are available, not Platform.
You probably want to google "free monads". - Coyoneda is a free functor (i.e. turns anything into functor), - [Free](https://hackage.haskell.org/package/free-4.11/docs/Control-Monad-Free.html) a free monad, such so it turns any functor into monad, - [Codensity](https://www.stackage.org/haddock/nightly-2015-03-21/kan-extensions-4.2.1/Control-Monad-Codensity.html) is also free monad, but it turns anything info monad; kind of `Free ∘ Coyoneda`, but better.
I have to point out though that my ppa covers only Ubuntu...
I got a little distracted in the Coyoneda video by the pronunciation (Yoneda is a Japanese name, so it's closer to "yo-nay-da" than "yo-nee-da"), but that's a pretty minor complaint. Good videos so far! Edit: Another minor point in the Coyoneda video: you describe `fmap` as changing the function from `a -&gt; b` to `b -&gt; c` to `c -&gt; d` when it should be `a -&gt; b` to `a -&gt; c` to `a -&gt; d`.
Why not compute the Voronoi diagram of all points and then combine all cells with the same class into one polygon. Edit: ok now having read the whole post I see you already suggested that, what is the difficulty?
I do not use Haskell Platform. It was convenient when I was new but eventually became more of a barrier.
How is this better or worse than lens? 
"Why would Intel want to be the exclusive source?" Non-paranoid possible answer: Power usage and performance. Intel has a strong interest in enhancing the performance characteristics of its hardware offerings on all software platforms. Every little feature that makes Intel chips work better is another selling point for them. They are going to push to see these features used in FOSS circles as well as everywhere else.
The lens type you defined is actually a special case of the standard lens type. Just take the standard type: type Lens s t a b = forall f . Functor f =&gt; (a -&gt; f b) -&gt; (s -&gt; f b) Then specialize "f" to "Cont r", which implements "Functor": (a -&gt; Cont r b) -&gt; (s -&gt; Cont r t) ... and that is isomorphic to (i.e. Interconvertible with): (a -&gt; (b -&gt; r) -&gt; r) -&gt; (s -&gt; (t -&gt; r) -&gt; r) So you don't need to make any changes to the lens library to implement your desired utilities.
Yeah, I'm not even sure C is up to the task. I suspect you could do a bit better than in Haskell, but I really don't think "better" is good enough here.
FWIW, my improved version of salt is called `hs-nacl`: https://github.com/thoughtpolice/hs-nacl It is much, much different than `salt` in some key ways, and I learned a lot from the first version. It is also much more complete and portable. I'll probably get it right by the third time I rewrite it all (which, at this rate, will happen sometime in 2018, given I started working on this thing over 3(!) years ago).
In fact there is! `Yoneda` is effectively a form of "enforced fmap fusion". `Codensity` is a form of "enforced (&gt;&gt;=) fusion". newtype Codensity m a = Codensity { runCodensity :: forall r. (a -&gt; m r) -&gt; m r } Fun exercises: instance Functor (Codensity m) instance Monad (Codensity m) instance MonadTrans Codensity Now there is a problem with using it for this usecase. =( When you go to build a probability monad using `Codensity` you basically get stuck evaluating all sorts of branches without collapsing probabilities. This means even modest branching factors quickly spiral out of control.
The platform was very confusing to me as a beginner, and only now that I know what I'm doing do I see what it was trying to accomplish. I'm depending on the awesome PPA, hopefully GHC can be made easier to get running without such a thing on other platforms.
The DOD also [funds](http://homotopytypetheory.org/2014/04/29/hott-awarded-a-muri) homotopy type theory!
as a newborn Haskeller, I agree. Platform (OSX) was a huge pain, uninstalled it, learned cabal sandboxes, sometimes nuke them, everything works. only cabal pain I've felt (on a personal project, that is) is something about transformers with doctest, installing ghcjs, updating cabal to 1.22, and 20min build times for new sandboxes. and I think only the latter two are in any way cabal related, and neither is "cabal hell". I'm sure as my project increases in complexity I'll feel the pain, but I haven't yet. I'd tell my fellow amateurs to, after doing enough tutorials that you won't give up on haskell, just learn cabal and reap the awesomeness. which isn't to say I'm anti-platform, the contributors seems to put in a lot of hard work into it, and it's cool that fpcomplete's online editor gives you access to it (I think?). but I wanted to put out my anecdote as a beginner, the platform made things worse for me, not better, versus cabal sandboxes.
Hum, you could maybe perform a Delaunay triangulation, and keep only edges shared by triangles that are inside and outside the shape.. Edit: wait, that gives the convex hull...
&gt; A good metaphor would be the locations of houses with people who speak a certain language, on a planet where countries do not exist yet, and now we have to decide where to draw the country borders between those groups of people. You might want to look into political redistricting algorithms, which sound appropriate and have gotten a lot of work recently. Two examples are [BDistricting](http://bdistricting.com/about.html), which emphasizes compactness (gives rounded, slightly noisy borders), and [Splitline](http://rangevoting.org/SplitLR.html) which emphasizes straight borders (results look more like Voronoi).
Not so much any specific difficulty, I often type questions as my thoughts on the subject pass by, so the structure of the post starts making a bit less sense when I half-answer my own question :P But the feedback here is super valuable, and I think merged voronoi cells are indeed the way to go. Still looking around how to do that in haskell though!
I appreciate a lot of the points of this post, but a lot of it came off as what sounds like several personal attacks against an individual. if this was not your intent, then perhaps maybe you might be able to rephrase some of the wording? that you respect the person in question and have valid reasons to disagree with a particular opinion? instead of saying that it's unfortunate this person maintains a lot of packages, how about saying that this policy concerns you because it is applied to so many packages? 
why not (\(a, b) -&gt; a + b) (1, 2) ?
It's true that by default, crypto-random uses the RdRand backend to get as many bytes of entropy as it can before trying the other sources. But that's trivial to change. You can re-order the priority, or omit RdRand altogether. Or alternate every certain number of bytes. Or whatever. Please look more closely at the API. But yeah, if what you really want is to mix entropy from RdRand with entropy from urandom or CryptoAPI in some non-trivial way, Vincent does not provide a backend like that out of the box. If you have something in particular in mind, why not submit it as a PR?
True. But you don't have to be paranoid to recognize the published fact that NSA does have their fingers in RdRand.
In a good way or a bad way or just a way? I have not kept up on the RdRand story. At all.
Most languages have a standard library. I want the platform to be that. What's in the standard library varies from language to language, but ti's generally bigger than GHC + Cabal but smaller than the current platform. I love QuickCheck and HUnit, but I'm not sure they need to be part of the platform. I'm also not sure OpenGL(Raw?) and GLU(T|Raw) deserve a spot there. I don't know if any additions need to be made. I usually install Haskell on a new (Debian) box by grabbing haskell-platform-prof from whatever release I'm on. I'm pretty comfortable with pulling stuff from hackage, sandboxes, etc., but there's plenty of times I just want to show off / do something in the REPL and having the full platform rather than just the GHC libraries is nice.
&gt; I'm also not sure OpenGL(Raw?) and GLU(T|Raw) deserve a spot there. They most definitely deserve a spot there, if for no other reason, then because they are (very?) hard to install on Windows (used to be basically impossible, I think it's a bit better these days). Back around 2006 they were bundled with GHC itself, and it was very important for me personally that I was able to use OpenGL with Haskell (and on Windows). &gt; Most languages have a standard library. I want the platform to be that. Well that's a bit strange, because as far as I know, the platform never intended to be a standard library - more like a "batteries included" solution, which also alleviates the user from the pain of going through the mazes of cabal hell, at least for some widely used packages. We already have a standard library, it's called "base", though it's probably not big enough (especially after containers/etc were split off). So why not have both (well, all three): a minimal base, a standard library *and* the platform? I don't think it's a good idea to force the latter two to be the same. &gt; I love QuickCheck and HUnit, but I'm not sure they need to be part of the platform. From the above viewpoint, QuickCheck (and I assume HUnit too, I don't know much about it) is a very natural part of the platform.
[Galois](http://galois.com) creates methods for systematically building software when correctness matters: Computer security and safety critical systems are two of our focus areas. We like to build systems where you can verify their correctness. Sometimes this is accomplished through mathematical proofs, sometimes through high-level language design, and sometimes through open source. Our design philosophy grew out of the type system of Haskell itself, and we've been staunch supporters of Haskell for our entire history, since 1999. John Launchbury founded Galois, is one of the original authors of the Haskell98 report, and currently works at DARPA. He just announced his big project, [a research program that "aims to ensure online privacy through technology"](http://www.darpa.mil/NewsEvents/Releases/2015/03/11.aspx). It's named for the supreme court justice that advocated the concept of a "right to privacy". Galois employees [contribute to lots of packages](https://github.com/GaloisInc) on GitHub. I'm a 10 year Galois employee and one of the original authors of [Cabal](https://www.haskell.org/cabal/). I recently wrote [an article about incorrect use of crypto libraries](http://tozny.com/blog/encrypting-strings-in-android-lets-make-better-mistakes/), which got a lot of great feedback, and our little Android library is getting some good use. Over the years, Galois has hosted or maintained a lot of community infrastructure. Galois created [Cryptol](http://cryptol.net/) and [Ivory](http://ivorylang.org/ivory-introduction.html), which are programming languages to help produce correct implementations of cryptographic and embedded software. [Thomas DuBuisson](https://hackage.haskell.org/user/ThomasDuBuisson), who gets called out in the article, works hard to write and maintain a great deal of cryptographic software that's offered freely to the community. He's very knowledgeable on crypto. There are probably hundreds of examples of community contributions from pretty much every single employee. Galois is also an amazing place to work. "Joy at work" is one of our [core principles](https://galois.com/about/). We have a distributed authority model where there's no single, fixed hierarchy, but whoever is an expert in an area gets decision authority over that area. Galois even supports employees in [starting spin-off companies](http://tozny.com) for products created within Galois. peace, Isaac (edit: link format)
I'm really enjoying the videos, but they have some usability problems on my iPad mini. I like to go do other things when watching videos, like hit the treadmill, or wander about, but I can't get them to play for more than 5 minutes without crashing out and resetting the website. The big frame around the videos - which looks cool on my 19" screen - makes the text really hard to see on an iPad, too, especially in Vim demos, where all of the text is well within the top left quarter of the screen. Everything is so tiny. Also, if I accidentally tap or click in the frame, the whole video goes away, which was super frustrating a few times. The controls were really iffy, too. I couldn't get the slider to work, so I couldn't replay sections, and I need a lot of replaying to digest this stuff. I kind of wish I could just swipe left and right on videos for shuttle control, but that's a larger, more general usability issue on devices. Anyway, just some thoughts. Keep it up! I'm excited.
I find the platform to be a huge burden to beginners on OS X. It just leads to cabal hell. Anything except the minimum to build cabal-install in the global sandbox == super bad. 
I'm not clear on what alteration you're hoping from wrt `entropy`. As I said last time this came up, I'd happy accept a patch adding a cabal flag that disables use of RDRAND, just like cipher-aes128 has a flag that disables use of AESNI, if that would be useful to people.
is this the first published haskell game using FRP? I thought I remembered some game developers (Niki and the robots or something?) saying that they attempted but rejected an FRP solution.
Parts of OpenSSL and similar libraries are written in asm to avoid timing attacks. And even then they suffer from timing attacks from parts of lookup tables being cached. 
I think the exclusive Stackage snapshot could provide this, although maybe those are too big?
Sure, but will it provide the same guarantees? The model it is checking against may be tied to the hardware behavior.
It's more of a Snowden thing, in that the NSA has the motive, means, and opportunity to sabotage crypto in silicon. Furthermore, RdRand is closed source. In cryptography, all cryptographic primitives are guilty until proven innocent. Edit: The NSA could probably engineer a virus that modifies how RdRand functions by doing a microcode update.
Thank you very much! (Disregard my last message, I figured I was abusing of your goodwill.)
For me (I am using linux), the main reason I like the platform is because it allows me to avoid having to compile packages like `text`, `bytestring`, `time`, `unix`, etc again and again. Also, I use `ghci` all the time as a calculator, so I run it outside a cabal project, and I like that it has access to a basic set of packages.
The platform, in my 2 or so years experience with Haskell on Windows, is really the only sane way to start on Windows. And when I need to setup Haskell on Windows, it's still the place I start to get a working cabal/ghc to then upgrade whatever components.
What about https://github.com/fpco/minghc ?
It'd work, but I'd be a bit sad to have to go back to 32-bit. If it were on the Haskell website it would be easier to find.
I got the impression that was only 64-bit MSYS, as a few lines down in the 'Possible future enhancements' section it says 'Add support for 64-bit GHC.'.
Haskell's reduced susceptibility to buffer overflows is countered by Haskell's increased susceptibility to various side-channel attacks (memory zeroing, timing attacks, etc). My utopian view of the cryptography world is that there is one very stable, peer-reviewed crypto library out there that does everything right, and all languages wrap around that library. Unfortunately, that's not a practical world view. :)
Won't that be extremely inefficient compared to calculating the exact voronoi diagram the conventional way, and then merging cells with the same class? Depending on the algorithm used for computing the voronoi diagram you could even speed it up by merging the classes during execution because the structure internal to a class does not matter.
My idea was that import System.Locale (defaultTimeLocale) s = formatTime defaultTimeLocale ... would work in 1.4 and 1.5, and import Data.Time.Format (defaultTimeLocale) s = formatTime defaultTimeLocale ... would work in 1.5 and 1.6. Unless someone knows how I can use `defaultTimeLocale` in a way that's compatible with 1.4 and 1.5, that's smoother than what we have now.
I'm aware of that and I use it in my libraries but thanks :) I was more thinking that quite often the semantics are clear when I read "rng" or "bits" so we wouldn't need line-wrapping of the type signatures to add docs. Also: It's free! You have to name your arguments anyway. Exposing them publicly might even encourage fewer 1-letter names?
Otoh, one can argue that the variable names for the formal parameters are an implementation detail (i.e. part of the function body) and therefore shouldn't leak into the documentation by default)
I didn't know it existed! Both you and my respondent on haskell-cafe only mentioned CPP as the solution. `time-locale-compat` is indeed what I was looking for.
It just so happens that the author of that package (/u/tom-md, whose post you are answering) is extremely knowledgeable about how to generate random data, being a well-known world-class professional in that field. I cannot comment about whether that is more or less than the authors of libsodium (nor can you, probably). And anyway, even the libsodium README that you linked states: &gt; If none of these options can safely be used, custom implementations can easily be hooked. which I understand to mean: "We chose a default behavior, but we are not claiming that it is right for you. Find out what you need, and use it to replace what we did if needed." The fact that Tom chose a different default is not in any way evidence of weakness in Haskell crypto. This discussion is convincing me that the crypto situation in Haskell is actually quite good, though there is room for improvement. Which is what I would have expected to find.
I wouldn't say Ubuntu is the only sensible choice for Haskell on Linux. At LumiGuide we're using the [NixOS](http://nixos.org/) linux distribution for our development work stations and production (physical, EC2 and Hetzner) machines. We write a lot of Haskell both for the frontends (GHCJS) and backends of our system and NixOS is proving to be a great choice of OS for this: * NixOS is configured and extended in the Nix language which, like Haskell, is a purely functional language which provides many of the same benefits as Haskell: easy to reason about and good for abstraction due to higher order functions. Unfortunately is doesn't have a static type system. * NixOS uses `nixpkgs` which is a collection of packages defined in Nix. `nixpkgs` [contains almost any package on Hackage](https://raw.githubusercontent.com/NixOS/nixpkgs/master/pkgs/development/haskell-modules/hackage-packages.nix). * `nixpkgs` contains support for [many versions of GHC](https://github.com/NixOS/nixpkgs/blob/master/pkgs/top-level/haskell-ng.nix) which makes it easy to test your libraries against older versions of GHC. * `nixpkgs` is continuously build on a CI-service called [Hydra](http://nixos.org/hydra/) which means that bugs are detected early on and the successfully build packages are used as a cache which means that installing a package usually just involves downloading a binary.
&gt; Naturality is implied by parametricity. As Bird notes in The Algebra of Programming, all polymorphic functions are natural transformations. Yes. I just wasn't 100% about the opposite direction, hence my "almost" :)
&gt; Bindings to tighter pieces of code is much easier… These seem to be the most productive point in the space for the limited amount of time any one person is willing to expend. Bindings tend to be problematic for general use over time. They create serious deployment complexities for products, and their maintenance requires constant vigilance for subtle upstream changes that can cause unexpected breakage in ways that are difficult to detect. This is often true even for small bindings to tight libraries.
That's a nice idea. I think it's still better if this trick is encoded in a single place though, i.e. a compat package. 
I don't follow how Haskell's data dependant execution flow helps to protect against timing attacks unless you're just suggesting it's going to raise the noise floor. Surely, the more data affects the flow of execution then the greater chance it will leak information via direct timing and indirect effects on caches, etc?
Stackage provides absolutely no guarantees about the quality of packages or their stability. Or their compatibility with each other, except for the fact that they all compile with each other. It's an amazing feat that it achieves simultaneous compilation compatibility with such a wide selection of packages, and that is very useful, but it serves a very different purpose than the Haskell Platform. The Platform should be built *on top* of Stackage. EDIT: Clarified based on reply of /u/bss03
Those days are long past. The packages Haskell Platform provides in the global db provide good defaults, and with modern cabal they are easy to override. (There is nothing special about OS X here.)
Once you realize that you should use sandboxes, there is no disadvantage to the Haskell Platform, only advantages. EDIT: OK, OK, no disadvantages assuming the Platform keeps up. But currently it has fallen way behind in several ways, so there are indeed disadvantages. But I don't think those are the kinds of disadvantages that /u/sambocyn was referring to.
The model was Python's concept of "batteries included". It doesn't mean everything that anyone would ever want to do. But there isn't much useful you can do with Python without its libraries, and the same is true of Haskell. The libraries included in the Haskell 2010 Report are not enough. The additional libraries included with GHC were not chosen based on what you'll need, they are there because they are "boot libraries" needed by GHC itself. We need the Haskell Platform to provide a basic default working set. You can then easily use whatever other libraries you want.
See [my other comment](http://www.reddit.com/r/haskell/comments/2zts44/wither_the_platform/cpmvdcp) above on how I imagine this could work (basically, I would like to have a smaller "standard library" and a large "batteries included" platform)
Hash algorithms rely on data dependencies as one of the most fundamental ways of increasing complexity to make calculations more difficult to reverse engineer. The same should be true for timing. In fact, in Haskell you could in principal even introduce non-determinism in the timing based on data dependencies and non-strictness.
See also [halcyon](https://halcyon.sh).
That's quite tricky. The haddock-algorithm would still be simple, it just can't print argument names for the last k arguments that are point-free. Not sure how to how display that to the user though. Maybe say "point-free argument"?
Good point, though I don't see any library in the platform which requires a GUI (apart from OpenGL and friends, of course). Did I miss something?
One could argue that argument names aren't really "information" in the same sense that types are; they are only meaningful to the compiler on the inside of the function, not on the outside, and giving them meaning on the outside without telling the compiler (which is what making haddock aware of argument names would amount to) is slightly dishonest. In other words, we are throwing the information away anyway, so it might be "better" to reflect this fact in the documentation. An approach that doesn't throw away as much information would be to lift the arguments' meanings into the type system, at the very least using type synonyms: type BitCount = Int type Generator = Integer generateParams :: CPRG g =&gt; g -&gt; BitCount -&gt; Generator -&gt; (Params, g) ...but ideally, using newtypes - however, the additional ceremony may or may not be worth it. Other than that, a common style I've seen is this: -- | 'foo bar baz' performs a fooing operation using 'bar' elements from the 'baz' list, returning the best-fooed item it encountered. foo :: Fooable a =&gt; Int -&gt; [a] -&gt; a ...iow, provide an example in the documentation that allows you to refer to the arguments by name.
Sadly this seems contrary to the experience of a reasonable number of people I've tried to introduce to haskell - even doing something as simple as a `cabal install cabal-install` with a slightly too old platform leads to extreme pain. Imo, encouraging people to use anything in the global package repository and not a sandbox is a huge mistake with the current behaviour of cabal. Newbies eventually just give up. I hope this will be solved when someone builds the nix-style environments idea for cabal*. I mentioned OS X because generally the only reason I've heard to use the platform is to get a reasonable subset of packages that are guaranteed to work on Windows. \* I would love to do this if I had the time/no job. 
If you put examples in your comments, I encourage you to check out [`doctest`](https://github.com/sol/doctest/blob/master/README.markdown), which checks that all your examples work as described. It helps your comments from drifting out of date as your API shifts.
Thank you! I'm still thinking about the domain name problem. Check out [this thread](https://www.reddit.com/r/haskell/comments/2zpd2e/i_created_a_web_app_front_end_to_the_pointfree/cpl3la5) for some ideas. 
Actually there is a disadvantage with the HP, as I pointed out already in an email-response, as the extra HP packages get registered into the global package db, and therefore pollute the sandbox environment with those extra packages. 
Ah, it works in a sandbox, I meant people doing it in the global scope. But if you're doing all your stuff in a sandbox maybe we don't need a global db at all. Totally agree we should have a require sandbox option defaulted to on - new haskellers usually don't know they are supposed to use a sandbox.
&gt; Other than that, a common style I've seen is this… That's exactly the point. I could write *all* of the API documentation manually in my comments like that, and not use Haddock at all. But the purpose of Haddock is to automate. There is important human-readable information in the both the function names and the argument names. We're not throwing away that information in the function names. Why throw it away in the argument names? Backing off a little - perhaps this could be an option, settable with some kind of pragma or syntax. Authors who rely more on meaningful parameter names could allow them to come through like the did before, and authors who avoid that could omit them.
The current ecosystem is reminiscent of the world prior to the HP. Lots of platform-specific, or dev-style-specific recipes for getting a useful Haskell environment on each OS or package system. Recipes have many steps. Each additional step makes it slightly harder/less likely to get a successful install. A different paths for different distros and OSs leads to further complications. But the new tools have clear technical advantages of the old style monolithic, global package set in the current HP model. Perhaps it is time to "reboot" the HP as a unifying umbrella/installer/toolset that combines the best of the current crop of tools and systems into a community-recommended, cross-platform, single (or simple)-click installation process.
Indeed. Loads of docs are like this, but Haddock doesn't support it explicitly. E.g. in `Data.List` we have: &gt; `map :: (a -&gt; b) -&gt; [a] -&gt; [b]` &gt; &gt; map f xs is the list obtained by applying f to each element of xs In Emacs Lisp the docs entry for `mapcar` is: &gt; `(mapcar FUNCTION SEQUENCE)` &gt; &gt; Apply `FUNCTION` to each element of `SEQUENCE`, and make a list of the results. &gt; &gt; The result is a list just as long as `SEQUENCE`. &gt; &gt; `SEQUENCE` may be a list, a vector, a bool-vector, or a string. [this part isn't necessary in Haskell, obviously] So there're clear advantages and precedent for using the names while describing the properties of a function. A nice advantage one could have with Haddock would be to actually check that the argument names match up so that they're always consistent. Perhaps in Haddock could just be displayed like the map example: &gt; map :: (a -&gt; b) -&gt; [a] -&gt; [b] &gt; map f xs &gt; &gt; The list obtained by applying `f` to each element of `xs`. Alternatively there is pattern signatures: map (f :: (a -&gt; b)) (xs :: [a]) :: [b] Which doesn't look as horrible as I'd imagined.
&gt; There is important human-readable information in the both the function names and the argument names. We're not throwing away that information in the function names. Why throw it away in the argument names? Indeed; what's so great about information hiding? &gt; Authors who rely more on meaningful parameter names could allow them to come through like the did before, and authors who avoid that could omit them. I think it doesn't really matter whether the names are “meaningful” or not, just that they are there. 
&gt; most often I don't think so. Any data to back up that?
You're right that pattern matching complicates what haddock would have to do; it's irrelevant if this is most often or not. My first thought would be some kind of metavariable like *pat*.
It's a special case on the construction side in that all standard lenses are convertible to this continuation type, but a major part of the mechanics of `lens` is to be able to treat the constructed lenses at many elimination types. There's reason to believe this is still possible ("cont is the mother of all monads") but it'll still be interesting to see if it's true. Offhand, `Fold`s seem obviously impossible to express at this type, but Prisms aren't. I can't get them to `review`, though.
&gt; As Bird notes in The Algebra of Programming, all polymorphic functions are natural transformations. That's only the case when the source and target are functors. Higher-order polymorphic functions like `fix :: (a -&gt; a) -&gt; a` cannot be modeled by natural transformations because the source (`a -&gt; a`) has mixed variance.
Isn't the solution , a nice docker image including Haskell, Nix and a nix-store preloaded with usefull library ?
This is awesome. I need more. 
Oh but Haddock does actually provide you a source-link if you want to look at the gritty implementation details (and I wouldn't want to miss that ability...). You have to draw the line somewhere between what constitutes the API-documentation part and what belongs to the implementation, and for me argument names (if they are named at all -- c.f. pattern matching &amp; points-free style) fall into the implementation-details side, as they are subject to change w/o affecting the user.
Which ones? We're taking votes right now.
Or better yet ;-) {-# LANGUAGE TypeOperators #-} type a :. b = b div :: numerator :. Int -&gt; denominator :. Int -&gt; result :. Int
I've actually voted already, but those would be: Bifunctor/Profunctor, Fold/Unfold and Monad Transformers
I don't follow this logic. Data dependent branching is the core property of flawed crypto primitives open to timing attacks. Having the output of a function be dependent of the input (good) has nothing to do with data dependent branching (bad).
Yes. Install it --global if you want it available everywhere.
&gt; what's so great about information hiding? It encourages loose coupling and thereby modularity and code reuse. If you write your code against my implementation instead of my API, you may suffer breakages when I make relatively minor changes; changes that do not affect the API. That said, documentation is *supposed* to bridge the gap between API and implementation, or at least more well-define the API than automated analysis. On top of that, exposing the argument names (if they exist) carries very little risk; I can't imagine a case where acting on the knowledge of how I named my arguments could cause a tighter coupling (in Haskell; Python is different).
The correct names for those are Dividend, Divisor, and Quotient (and Remainder). And, when you use the correct ones it does make things more clear. (Technically, that's for `quotRem`, but `divMod` is so closely related it uses the same ones mostly.)
In Haskell 2010, which doesn't have flexible contexts, does this fail to compile? foo x = show . fmap (== x) 
I'm conflicted about type synonyms, but I recently used them in a library because I wanted to be able to understand the meaning of the arguments when inspecting the function's type in ghci. Yes, looking at the haddocks solves the problem, but sometimes its nice to not have two panes/windows open, and understand as much as I can from interacting with the REPL; type synonyms were the only way I could think of adding information in that context. Also I'm not aware we have the ability to print docstrings from the REPL like in other languages, which would've been a valid solution here
I had a similar issue today, trying to install network-2.6.0.2, with ExitFailure (-11) as well. Increasing the verbosity with `-v` didn't seem to help at first, because I saw more commands but not more error messages, but actually it did reveal the source of the problem because the last command was a large `hsc2hs` command which segfaulted when I tried to execute it by hand. I compiled the latest version of `hsc2hs` (which was a bit [tricky](https://github.com/gelisam/hawk/pull/140) in itself), and that solved the issue.
By default they are the first versions of those packages that cabal will try. If it can't find a build plan, it will then try other versions. If you want cabal to ignore a particular Platform version for this sandbox and instead try the newest version first for that package, add an entry like this to the `constraints` field in the `cabal.config` file in that sandbox: constraints: attoparsec source Multiple entries are separated by commas. Usually you want to do this in `cabal.config` like that so you can save it in version control for later, but for a one-shot attempt you can also say directly on the command line `cabal install --constraint="attoparsec source"`. With that syntax, each package is in its own `--constraint` option. EDIT: quotes for the command-line option
Interesting, I'll try to see if I can find anything like that on my -v3, thanks. Edit. No luck so far. Can't find the slightest error. Not really any commands I can try running myself either. I tried looking for hsc2hs commands as well but found none.
The first question seems kind of bad. It asks you to return the last element of a list, without specifying what to do with an empty list. You can infer from the type signature that you need to raise an error for that case, but it seems bad to encourage writing partial functions, as well.
Hm. Is there a way to stop the text cursor from blinking?
Yes, click outside of the editor to put it out of focus.
&gt; I think it's weird that ghc is surfacing this issue, rather than letting it automagically work as in the past. I think it makes sense. GHC should not suddenly require additional language extensions when you choose to write down the exact same type it inferred before manually. That is much more counter-intuitive behaviour than we see now in the new version.
Nice idea. Any idea how I can find this for really difficult problems? Because its an ease for me to do all the small practice, but I usually stuck on bigger problems. Often because I can't handle complexity well enough or simply can't find the right way to solve it. Would be nice to have some exercise resources like this for Monad Transformer and FRP. 
&gt; tried to upgrade usingbrew which made stuff even worst. I had stuff installed everywhere, didn't know which things correspond to which installation and spend 3 days to restore a decent environment. To be fair that seems to be generally the way setting up non-Apple-blessed development environments works on OS X, not just for Haskell.
Something tells me people would be less excited about: foo ([] :: [a]) :: [b] = ... foo ((x :: a :xs :: [a])) :: [b] = ... ...and you see how much worse this could get with more complex patterns.
A recent paper pointed out that `fix` and many other similar functions _do_ obey the properties of strong dinaturality (http://www.cs.nott.ac.uk/~jph/progs4cheap.pdf) :-)
Thanks for the feedback, did you try other problems? Here's it looks like to me when I run that code: https://i.imgur.com/UHOysXl.png
Be creative. ;) myLength :: [a] -&gt; Int myLength = foldl (const . succ) 0 
Well, yes, none of this is an excuse to throw good taste and common sense out the window.
That's why I said: &gt; ...but ideally, using newtypes - however, the additional ceremony may or may not be worth it.
the exercise is really for your own benefit, so you can always make the choice to learn or not learn :P the only person you're harming is yourself. 
Older versions of Haddock used to display also the argument names. Then that wasn't necessary. There were many problems with that, especially in the context of multiple function definitions, curried functions, complex pattern matching, etc. I understand the sentiment of cutting the complexity by just omitting the parameters altogether, which is not completely nonsensical on the surface. But by now we have had more than a long enough testing period, and I think it has become pretty clear that it was a wrong decision. 
I understand, but I'm referring to the isomorphism below that.
You could just install those packages globally once and then set installed constraints in your cabal config file to use them in all sandboxes. No need to use the platform for that.
I don't know, but since my copy of ghc 7.10 accepts it without the FlexibleContexts flag, it's probably not a very good example. I don't know what are the exact requirements of FlexibleContexts; let's figure it out! Your example has the following inferred type, which contains the non-type-variable `f Bool` in the constraint, yet it doesn't require FlexibleContexts. foo :: (Eq a, Functor f, Show (f Bool)) =&gt; a -&gt; f a -&gt; String foo x = show . fmap (== x) Maybe abstract type constructors are okay? Let's simplify as much as possible. This requires FlexibleContexts: bar :: Show [Int] =&gt; [Int] -&gt; String bar = show While this does not: baz :: Show (f Int) =&gt; f Int -&gt; String baz = show I don't know why. But knowing this, I can try to construct a [steel man](http://wiki.lesswrong.com/wiki/Steel_man) version of your question: in Haskell 2010, does this fail to compile? foo' x = show . map (== x) But that doesn't work either: the compiler knows that it can get an instance of `Show [a]` from an instance of `Show a`, so the derived type is only foo' :: Eq a =&gt; a -&gt; [a] -&gt; String and not foo' :: Eq [a] =&gt; a -&gt; [a] -&gt; String as I was trying to get. Okay, so let's create our own type class `Show'`, one for which we cannot get a `Show' [a]` from a `Show' a`. class Show' a where show' :: a -&gt; String foo'' :: (Show' [a], Eq a) =&gt; a -&gt; [a] -&gt; String foo'' x = show' . map (== x) Now this definitely requires FlexibleContexts, and should require it even if we don't specify the type signature, right? Well, without the type signature, I do get a compilation error, but it has nothing to do with flexible contexts: Could not deduce (Show' [Bool]) arising from a use of ‘show'’ from the context (Eq a) Yeah, that sounds about right, that's the kind of error you would get in a language without flexible contexts. But why is ghc sometimes suggesting FlexibleContexts and sometimes complaining that it can't find an instance? Answer: because ghc is trying to be as helpful as possible! Suppose there is, in fact, an instance of `Show' [a]` for some `a`, in another module. {-# LANGUAGE FlexibleInstances #-} module Foo where class Show' a where show' :: a -&gt; String instance Show' [Int] where show' = show This time, when we try to type check the exact same function `foo''`, ghc does suggest us to use FlexibleContexts! module Main where import Foo foo'' x = show' . map (== x) Because ghc sees that you have imported a module which is using extensions to provide non-standard instances, so it's guessing that you want to use them, and it's suggesting you to enable an extension in order to do so. If you only import Haskell 2010 modules, ghc guesses instead that you made a mistake, and gives you a Haskell-2010-style error message.
Cursor positioning wasn't working right -- seemed to be considering each character as about one pixel narrower than it actually was -- until after the first time I hit Run Code, after which it's been fine. Only on problem 3 right now, but I'm rather disturbed that every one so far has been partial. Can a brother get a `Maybe`?
Hmm, what is [this button](http://i.imgur.com/hNhqE8w.png) supposed to do, because for me it seems to be doing nothing at all.
For the non-newcomer, hackage or stackage plus Cabal is sufficient. The platform is the single entry point for new users.
Assuming you always use sandboxes - which is the only reasonable way to install cabal packages nowadays in my opinion - and that you know about some of the simple but powerful features added to cabal, the problems people used to have with installing things have pretty much disappeared now. Those problems were caused by limitations of cabal, not by the Haskell Platform. The main purpose of the Haskell Platform is to provide you with a good set of basic default packages to use. But if you want to use something else that's easy to do. That's what I mean by saying that the Platform can only help.
Check out exercism.io for good alternative material.
This is not a fight between those for and against lenses. It's the opposite - it's pointing out not that lens is worse than traditional Haskell, or better than traditional Haskell, but just that it is a very different style of programming, almost a different language. We can agree to be different, and respect each other.
Makes it fullscreen, works on my end. What browser are you using? There might be a security pop-up you need to confirm.
I think that reasoning is circular. In my opinion, Foldable/Traversable was pushed more into the forefront of Haskell mainly because it felt more comfortable that way for people who use lenses. Which is fine, that's what was decided. But it's not evidence that lens-style programming is not different in spirit from traditional Haskell programming.
It supports 64bit and there are 64bit binaries - I think the Readme just needs some janitorial love.
Arrows are weird, but `first`, `second`, `(***)`, `(&amp;&amp;&amp;)`, and `(&gt;&gt;&gt;)` are not. I wish I could use them without scaring people by importing them from `Control.Arrow`. EDIT: But I also like bimaps :)
i
For a while there were 2-3 releases every year. And the ecosystem moved a little more slowly back then, too. So no one felt that the platform was old.
So I just wanted to update you that `annah` now encodes numbers as binary numerals so that arithmetic is now possible for very large numbers. It's obviously still way slower than machine arithmetic but it still completes in a reasonable amount of time. You can test this by checking out the repository, compiling the executable, then navigating to the `Prelude` subdirectory of the repository. Then you can run: $ annah #times 1000000000 1000000000 &lt;Ctrl-D&gt; ∀(Bin : *) → Bin → ((∀(Bin_ : *) → Bin_ → (Bin_ → Bin_) → (Bin_ → Bin_) → Bin_) → Bin) → Bin 1000000000000000000 It will take a few seconds to complete, though. The most pathological 64-bit computation I tried took 30 seconds, but there is still room for more optimization. For example, storing the `times` function in its normalized form instead of the (somewhat) human-readable form shaves off 10 seconds from the running time. I also have an implementation of `plus` in there, too, which usually completes in less than a second.
No need to apologize, wasn't gonna use it anyways ;P (already solved this stuff months/years ago). I just tested it because I find that it is a nice idea :)
Even in that case, the fault won't lie on crypto-random, but on the library that makes a trust choice for the user instead of exposing the alternatives and letting the user decide which to trust.
Funny because I have been using the Haskell ecosystem since roughly one or two GHC versions longer than the platform has been around and to me it always felt outdated on average over the lifetime of one HP release. GHC 6.12.1 was released in December 2009, the next platform Beta release was in March 2010 and the next actual release in July 2010. So the platform contained the outdated GHC 6.10 for the better part of a year after the new GHC version was released. GHC 7.0.1 was released in November 2010, again we had to wait until March for a matching platform release. GHC 7.2.1 was release in August 2011 but was marked as a technology preview and completely ignored by the platform. GHC 7.4.1 was released in February 2012, the platform release including it was in June, coinciding almost exactly with the GHC 7.4.2 release which was then ignored by the platform until November, after even GHC 7.6.1 had been released in September 2012 already. To get GHC 7.6 in a platform release we had to wait until May 2013 when the reasonably current (release in April 2013) GHC 7.6.3 was included in the only platform release of that year. In April 2014 we got GHC 7.8.1 and in August 2014 we got a platform release for it which included 7.8.3, the current GHC release at the time and until GHC 7.8.4 was released in December. We still do not have a platform release including 7.8.4 now, 3 months after its release and when GHC 7.10.1 is about to be released. (all dates from the official release pages, GHC versions included in HP releases from its Wikipedia page) While I can certainly understand that it takes time to stabilize a new platform release and the limitations of volunteer work in a small community at the pace it had for all of its life time it is simply outdated by the time of its release, much less the time of the next release. Not to mention that the HP is not even handled like a regular stable distribution, e.g. it doesn't even update patch levels of packages or compilers. 
This is awesome... I love it. One thing I that would make me love it even more is some sort of feedback when you click the run button, since it takes a while sometimes.
Template Haskell have constructors for all syntactic haskell constructions. They are documented in the "The algebraic data types" section of [https://hackage.haskell.org/package/template-haskell/docs/Language-Haskell-TH.html](https://hackage.haskell.org/package/template-haskell/docs/Language-Haskell-TH.html) The simplest way to implement your example is: mkNat :: Integer -&gt; Q Exp mkNat n = sigE (conE 'Proxy) (appT (conT ''Proxy) (litT (numTyLit n))) -- equivalent : mkNat n = return $ SigE (ConE 'Proxy) (AppT (ConT ''Proxy) (LitT (NumTyLit n)))
I see your point, but if somebody were to put a microcode-based piece of malware on your machine, wouldn't you be compromised regardless of whether or not RdRand was your goto instruction?
OP you need to do some sample questions at codility.com They are real good about testing all possible edge cases and extreme cases. That would be a perfect guide for your thing.
That thought crossed my mind, but I ended up agreeing with [mstksg's philosophy](https://www.reddit.com/r/haskell/comments/2zxekg/while_learning_haskell_i_made_a_tool_for_learning/cpn6rh3). This is a training resource, people can do what they want. It's not like I'm giving out prizes for finishing ;) ^but ^^that's ^^^an ^^^^idea
Having even less haskell experience than you, i find it a little odd that the fix for record syntax is using a pretty big library, TH and still having to give fields unique names.
Thanks! I've made it disable the run button while it's running, so that gives a bit of feedback (though it would be nicer with some loading image). I've also removed a line I'd accidentally left for debugging purposes, which should speed things up as the program's no longer outputting everything haha.
To be perfectly honest, I've actually found myself using lenses less and less since I discovered the RecordWildCards extension. This [24 days of GHC extensions post](https://ocharles.org.uk/blog/posts/2014-12-04-record-wildcards.html) was excellent. But sometimes they produce a lot of code. I'm use to how terse Haskell is, but there's something so simple about `object.setValue(value);` compared to what you have to do in Haskell. Once you introduce lenses it massively simplifies the code. For example see how much simpler lenses are in this scenario: foo :: State SomeData () foo = do modify $ \SomeData{..} -&gt; SomeData{getter = "bar", ..} modify $ \SomeData{..} -&gt; SomeData{getter2 = getter2 + 1, ..} vs foo :: State SomeData () foo = do getter1 .= "foobar" getter2 += 1 If it takes a giant library to give me that Syntax, then I'll take the dependency hit whenever I need it.
Btw, Yoneda being a direct transliteration of a Japanese name, the e is pronounced more or less like the e in met. [Audio](http://hearnames.com/pronunciations/japanese-names/japanese-surnames/yoneda)
Looks like Safari might be encoding the results... I'm not sure why. Will look into it, thanks.
Yeah, I understand that sentiment. But I ran a small programming contest awhile back and I got a surprising number of solutions that looked like the above elementAt solution. A number of people seemed like they weren't able to see how their solution wasn't the general one and would break the instant I tested it with more complex input. That's why I think this quickcheck thing might be useful.
New reply so you and /u/yitz get ping'd. 99 Haskell's now decoding results. Could you please try again?
Ah I actually mentioned it as a fun coincidence, should have added a smiley face to seem less disagreeing :-)
&gt; It would be better to use arc4random when available, such as libsodium does. The actual code in there *was* taken from libsodium. Just much longer ago (the actual code dates back several years when I originally started working on it.) Adopting arc4random for BSD or getrandom(2) on particular versions of Linux wouldn't be difficult; most of the code could be copied. &gt; Also, a little note that says "when you take a snapshot of the virtual machine, and/or copy it, the same random data will be re-produced" would be important, imho (almost all urandom implementations work by seeding once, and saving the state of urandom when rebooting). Sure, I'll get around to it if I remember to. (Also, IIRC, systems like cloud-init for VPS providers can actually trigger to reseed upon cloning machines/reboots I thought? I don't know if this is true or a default. It doesn't matter much anyway...) &gt; So, this is just a rabbit hole that, in my humble opinion, shouldn't be solved again and again and again by each library. Just a simple look at libsodium[3] gives me the impression this is terribly difficult to get right. There really aren't many cases here as far as I can tell... there's a CSPRNG based Salsa20 solution, and the alternative is a uniform abstraction over /dev/urandom and arc4random(2)/getrandom(2) interfaces. The only real things actually missing are arc4random(2) and getrandom(2) interfaces IMHO. Although I did miss using `FD_CLOEXEC`, which is a legitimate bug (which I find rather surprising on my part, CLOEXEC is a necessary :) Aside from that, I still greatly value the portability of the current approach which does not impair Windows users (which I consider an important goal), and if the bulk of what needs to be done is copying libsodium's RNG code, I consider this relatively good - I mostly haven't changed the current code because It Works and making it uglier in this way is low on my priority list, all things considered
Well, shit, lol... If anyone with Safari/OS X would like to help out, I've opened [an issue on GitHub](https://github.com/bramgg/99haskell/issues/1). Running your own instance of 99 Haskell takes 5 minutes. Sorry about that ;_;
This looks like a job for `mplus`... d :: a h :: a -&gt; a -&gt; a x :: Maybe a y :: Maybe a h' :: Maybe a -&gt; Maybe a -&gt; a h' x y = fromMaybe d $ (h &lt;$&gt; x &lt;*&gt; y) `mplus` x `mplus` y 
You might also consider these sorts of infelicities as teachable moments. For example, after accepting a version of`myLast` that works on non-empty lists, you could prompt the programmer to consider what would happen if his function were given an empty list. After asking for naive implementations of a couple of other partial functions, introduce the concept of `Maybe`. It's likewise natural to introduce several of Haskell's other basic data structures by showing how they solve simple problems.
Right?
Haskell is one of the only languages that compiles down to native code without having much (any?) #ifdefs in your code. That alone is worth a lot.
Personally, I prefer to pretend lenses don't exist
I'd love to see if someone can implement the same thing in Haskell with GHC extensions.
The applicative context should handle it without the mplus.. h' x y = fromMaybe d (h &lt;$&gt; x &lt;*&gt; y) This is what applicative is primarily for, applying pure functions of arbitrity inside an applicative context. Sort of the multi-arity generalization of fmap.
What does dependently typed mean?
Types that depend on values.
That [`with`](https://gist.github.com/david-christiansen/3660d5d45e9287c25a5e#file-fizzbuzzc-idr-L36) keyword looks handy.
Damn
It would be cool to see a translation of this into Agda, for comparison. (because I'm not fully up to speed on Agda yet, either).
OK. "Feeling outdated" is obviously subjective, and I didn't feel that way until things started getting a lot further behind more recently. But in any case, it is clear that the Haskell Platform must be much more up to date. Which is why I feel that unless we get a flood of people volunteering a huge amount of hours on an ongoing basis to work on the Platform, we should forgo the idea of HP providing its own installer. Instead, we should focus on the core value of HP: providing an up-to-date set of recommended high quality stable default libraries.
&gt; It should be fairly straightforward to use quickcheck and the existing library functions to make more robust answer checking. This is actually what [Ask-Elle](http://www.open.ou.nl/bhr/AskElleDemo.html) is about. Also, it is not fairly straightforward. [Slides](http://foswiki.cs.uu.nl/foswiki/pub/IFIP21/Rome/JJRome.pdf). Concerning QuickCheck: the Ask-Elle programming tutor only provides simple counterexamples, but some students wrote an extension that can [pinpoint where in the code that the counterexample is generated](http://dspace.library.uu.nl/handle/1874/294073). Fun stuff.
I [reimplemented this module in Agda](http://liamoc.net/fizzbuzz/fizzbuzz.html), if anyone is interested. I did away with the silly DoubleDec thing. Also, Agda makes dealing with absurdity quite a bit nicer than it seems in idris. Also, I find it more pleasant to read, but YMMV.
I have [done this](http://liamoc.net/fizzbuzz/fizzbuzz.html).
As far as I know the installer is not really the thing that requires the most work, it is actually testing all those libraries and getting them to install on platforms like Windows and to a lesser degree OS X which proves difficult (and a set of stable libraries doesn't do you any good if you can't install them).
Much appreciated, thanks! Works great, apart from the stage restriction, which I assume there is no way around sadly.
I tend to leave them in with nested `with` as Agda's parser has a habit of getting confused and causing me to get confused with it.
Sure, technically they do if you look at them as function transformers. But seriously, who does that? People don't claim they compose backwards because they are idiots, but because that's the intuition they have after using normal record selector composition and other lens libraries. Maybe Van Laarhoven lenses don't compose backwards, but they certainly compose in a counter intuitive direction. Skipping spaces around the period is not going to help with that.
Here's the implementation: #if __GLASGOW_HASKELL__ &lt; 700 data Void = Void !Void #else newtype Void = Void Void #endif absurd :: Void -&gt; a absurd a = a `seq` spin a where spin (Void b) = spin b 
It's basically `case ... of { ... }`, but with dependently typed implications, akin to pattern matching on a GADT.
Hopefully this may interest /r/haskell readers who are based in the South of England. Key points: * The event is on the afternoon of the 30th April at Wolfson College, Cambridge. * Attendance is free. We welcome everybody with an interest in programming language theory, no matter which particular niche interests you, though we would appreciate prior notification if you are wishing to attend so we can get a rough idea of the numbers for catering purposes. * Our invited speaker is Conor McBride. * We are seeking four more volunteered talks of around 30 minutes in duration (including questions) to fill out the schedule.
&gt; ... that's part of the motivation for why I would like such an optimizer. Glad to hear about `annah`... I have been exploring an encoding of pure type systems proper in `morte`. The comments below about "optimal reduction" and "efficiency" are indeed relevant, as these systems in many ways characterize various complexity classes — something I am very interested in making more precise. Anyhow, I wanted to point out a result connecting parametricity and realizability that you may or may not be aware of ... &gt; Theorem 7. If P is strongly normalizing, so is P². ... and the corresponding footnote ... &gt; The proof may also be carried out constructively: the idea is to reuse the normalization procedure of terms in P to normalize terms in P² . More precisely, given a well-typed A, one can use the normalization procedure of ⌊A⌋ to normalize the 2nd level structure, and normalize the 1st level sub-terms independently. The separation properties guarantee that the interactions between first and second level structure only adds a finite number of β-reductions. in ["Realizability and Parametricty in Pure Type Systems", Bernardy, Lasson.](http://publications.lib.chalmers.se/records/fulltext/local_127466.pdf) &gt; Our work points the way towards the transportation of every parametricity theory into a corresponding realizability theory, and _vice versa_. "Realizing Parametric Futumura Projections?" is a paper I hope to read or eventually get around to writing at some point in my life, but I digress...
Your problem doesn't seem to be the actual format sent back and forth, but the synchronization of the server and client side. It sounds like you have JS on your front end already, so can't dive into Haste / GHCJS or similar, which is what I'm experimenting with to solve a very similar problem. You could probably write a DSL in your haskell code that is the official definition of what the data structures look like, which then creates functions suitable for the `toJSON` and `fromJSON` aeson functions, and also generates javascript classes suitable to export right into your front end code. 
This is pretty sweet. Nice job.
&gt; "trusting kernel developers over chipset manufacturers is ridiculous" What are you quoting here? In the [github issue](https://github.com/TomMD/entropy/issues/14), /u/tom-md says, "Joeyh reasoned, or I infer that his reasoning is, that he trusts his OS more than the chip manufacturer. I respect this reasoning but it might be true for some users an not others." And Tom then goes on to explain his own reasoning. You said you wrote this piece as a starting point for discussion, but the article itself admits to ad hominem attacks against Tom, and this comment apparently goes so far as to manufacture quotes from him to make his position look unreasoned and extreme. I think his reasoning is basically that it's a portable Haskell library, so the behavior of /dev/random is unreliable. He's said repeatedly that he's happy to provide a flag to disable RdRand to accommodate operating systems that already mix it in or that don't want to use it.
Here's the LH version. https://gist.github.com/ranjitjhala/b9cfc06e6915722aac2e (or if you want to play with it) http://goto.ucsd.edu:8090/index.html#?demo=permalink%2F1427132944.hs
Not sure about your sandboxes but mine have hundreds of packages these days while the global package database has about ten (those that come with GHC). So I really don't see in what sense sandboxes are smaller. You raise a good point when you say the ability to reinstall is not guaranteed but we already do have an option to break them (force reinstalls), why not at least attempt to fix them? Eventually cabal-install will need a way to update packages for things like security updates anyway. If there is no way to install the whole world package set consistently wouldn't it be better to at least detect that fact and allow the install for those situations where it is possible? 
Yes, there's a lot of JS on the client, which I don't want to touch. However, there's a part which deals with my data that I want to replace with JS generated from Fay.
It sounds like you just want a Haskell-to-Js compiler. That'll let you compile a blob of Haskell which implements the, say, binary serialization protocol and call it from JavaScript. Haste is supposedly good today and GHCjs is on its way.
But another install plan I did want in the past did need them. I installed A, then B broke A by reinstalling something. Why not have an optional parameter to let cabal-install fix that by reinstalling A (or the parts of its dependencies that broke) too? I am thinking of a behaviour that is similar to the various tools available for this purpose on source-based Linux distributions like Gentoo (perl-cleaner, python-updater, haskell-updater,revdep-rebuild,...).
So I just want to caution that Annah is not designed to be stable and timeless, but Morte is intended to be stable, so keep that in mind when designing your system. I totally get where you are coming from because I encountered the exact same issue a while ago when I was trying to write a compiler from an existing statically typed language to Python (to make it easy to migrate Python code bases onto a statically typed language). I was so discouraged by all the existing internal representations for statically typed languages (like Haskell, Idris, Agda, Purescript, etc.) because they were all heavily tied to the idiosyncratic choices that language made. Moreover, their internal syntax trees were complex, which made the translation really tedious and difficult. That's how I got interested in lambda calculus, because it's so easy to write a backend for it because the syntax tree is so minimalistic. All the hard part is in the front end of the language which translates things to lambda calculus (i.e. what `annah` is essentially doing). One of the goals I had for Morte to was to make it as simple and universal as possible so that it could be used as a sort of "pandoc" for languages (i.e. a shared intermediate language with readers and writers for multiple input and output languages), so that's one of the reasons Morte strives to be "timeless". There are some limitations of lambda calculus, mainly around resource management (i.e. linear logic). There is no good way to express in lambda calculus that a given value is no longer needed; you can duplicate values but you can't "delete" them. However, I decided that I was willing to settle on lambda calculus as a reasonable sweet spot for the current state of computer science. I expect that maybe a few decades or a century from now that sweet spot may eventually change. I also had the same issue about what type system to make the default, however, I want to note that the type system you use is totally independent from the shape of the syntax tree. Morte is actually designed as a "pure type system" (see the Henk paper linked in Morte's doc to learn more about pure type systems), meaning that you can encode any system in the lambda cube by just changing the `axiom` or `rule` functions. You can share the same syntax tree freely between multiple type systems Also, normalization is completely independent of the type system you choose. If you want to use the untyped lambda calculus, just use `Const Star` for the type of everything and just don't type-check the syntax tree.
God that is a lot of boilerplate. Programming with proofs will not become practical until all this proof-bloat gets automated.
&gt; source-based Linux distributions like Gentoo Of course the difference there is it does have a guarantee that rebuilding will work, because the versions have been carefully selected and tested to make that true.
It was just a toy example for having multiple top-level declarations of a function for pattern matching. Currently, they can all share a type signature. With inline signatures on patterns, not only is there extra typing, but the shared signature can get obfuscated -- both of these just take a list, but the bindings in the second pattern include one element on its own. It's directly at odds with too many core language features to really work out in Haskell, I think.
Actually no, the versions have not been carefully selected at all in source based distributions. You are thinking of binary distributions. Of course some work goes into fixing compile errors by patching packages that won't compile but Gentoo does not limit versions very much at all. They simply use dependencies, just like cabal does.
My reasoning is that next time you need A, cabal will fix it when you say `cabal install A`. There's no need to fix it *now* as you are not working on A and it will just waste your time.
But cabal already has that information. It is stored in the world-file. Why would it require me to specify things I installed in the past again? Why not use that information we have stored there (and possibly providing a command to remove entries from the world file if something previously installed is truly not wanted anymore)?
Ohhh, I see what you're getting at now. Yes, I agree.
Original original: http://www.reddit.com/r/haskell/comments/24k2wz/thinking_in_types/
We're going round in circles now. It doesn't do that because there's no particular reason to think that it'd work, because though things are recorded in the world file they don't have to have consistent deps, and so asking to reinstall them with consistent deps is likely to fail.
Cryptol developer here to say that Cryptol is absolutely for software! Well, perhaps I should say Cryptol plus our related Software Assurance Workbench (SAW) project: https://galois.com/project/software-analysis-workbench/. SAW lets you, for example, build formal models of a Cryptol spec and a C implementation and prove their functional equivalence, or load a Cryptol spec and output an equivalent program in a loop-free subset of C. Cryptol 1.* had both software and hardware synthesis and verification tools built in. Cryptol 2.* is open source and focuses more on being an executable specification language interpreter backed by formal methods tools for reasoning about properties of that specification. We're currently working on open sourcing much of the complementary SAW tools that provide ways to reason about Cryptol alongside LLVM (C), Java, Verilog and others, but if you'd like to play around with them before then, drop me a line.
I didn't know he came on reddit! Krishnaswami is one of my personal heroes. He's a great communicator, his research is interesting, and I like his pragmatism.
I haven't read all the comments yet, but has anyone else been thinking that cryptography is EXACTLY where dependent typing is going to come in handy. Yes, there are a lot of home-grown crypto-libraries out there, and it's very hard to know which ones are safe, but that's why we need something like dependent typing to help us know just how tinfoil-hattery-ish a library is.
Thanks, subscribed, and look forward to seeing you there.
A more telling example might be collision detection. Here you'd need a way to reason about the things collectively so you could somehow send back information about what collided. A bullet may be absorbed into a wall, while if it hit a player they'd need to take damage.
That's not necessarily dependently typed. Regular old Haskell can do that. module Even (Even, asEvenMay, half) where newtype Even = Even Int asEvenMay :: Int -&gt; Maybe Even asEvenMay i | i `rem` 2 == 0 = Just (Even i) | otherwise = Nothing half :: Even -&gt; Int half (Even i) = i `div` 2 
I'm not sure of the technical details -- but the concept seems like it's on the right track. The author of the linked article is in the same boat as me -- I really can't say I understand cryptography enough to know how or when it works correctly -- though I use it every day and allow my lively-hood to be dependent on it.
I have an example of types defined on the server and used in the client here: https://github.com/boothead/ohm-examples See the ohm-chat-* projects. It works quite nicely - although there's obviously some overhead of serializing to and from JSON.
On mobile, so this won't be long. By "primitives" I meant anything that can't be written in Haskell without foreign imports that correct cryptography wants. Maybe that is inline assembly or more control over the gc or something else.
I haven't tested it or run it, but wouldn't this work? test [] legends = [] test (m:ms) legends = if worthy m legends then m : test ms (m:legends) else test ms legends 
 worthy :: Person -&gt; People -&gt; Bool worthy m = (/= 10) . length . take 10 $ filter (`judges` m) This is a type error.
Thanks! I just found out that [fayResponse from Happstack-Fay](https://hackage.haskell.org/package/happstack-fay-0.2.0/docs/src/Happstack-Fay.html#fayResponse) also serializes to JSON.
But inside that module I could write `half (Even 1)`. With a dependently typed language you can make an Even type where you cannot pass in an odd integer anywhere (as that's how the type is defined). 
Actually yes. Your misunderstanding of Gentoo is quite insulting to the people who spend so much time and effort on it.
Right, it was too "point-free". Correction: worthy m ls = (/= 10) . length . take 10 $ filter (`judges` m) ls
I'd personally say it's a little bit much to say that ASTs and ABTs have "nothing" to do with one another. What you suggest is essentially correct. Technically introduction of variable binding and scoping radically changes the kind of values you're dealing with. For instance, it introduces and handles the notion of alpha equivalence which is difficult to talk about (plain, pure) ASTs as having. But mechanically you can definitely see ASTs and ABTs as being super similar.
I'm not sure how relevant the ExtendedGame example at the end is to the expression problem - that talks about adding new cases/variants to a datatype without recompiling rather than whole new data types (which is already easy in haskell). The alternative approach I gave above is also finally tagless - in this case the Renderable type is just `IO ()`. In fact most of these object oriented approaches end up like this (and so become implicitly extensible). 
&gt; I'm not sure how relevant the ExtendedGame example at the end is to the expression problem - that talks about adding new cases/variants to a datatype without recompiling rather than whole new data types (which is already easy in haskell). Using a type class to add separately-compiled data variants (and operations) was the motivating example from the paper. But although it reminded me of the Finally Tagless approach, it's not really. The key to the Finally Tagless approach is to make the type classes parameterized by the representation type, i.e. the result type of the 'constructors' that form the type class methods. The approach from the blog parameterizes the type class on an input to the render method, which is not really anything out of the ordinary. Anyway, there's certainly nothing wrong with your suggestion or Tekmo's, but I don't think the suggestion in the article falls under the 'existential typeclass antipattern' either. If he'd gone ahead and made an existential wrapper to force things into a list, then it probably would have been, but as it is it's just a normal type class a la Show. 
No copying in (most) Haskell implementations. Both the accumulator and the output will (be) point(ers) to the same immutable value.
Really... hmm How do you know? Can you point me to a source? Cause it seems to me that it builds two different lists. Sure, they end up being the same, but is the compiler smart enough to pick up on that?
it's not the compiler, it's the data structure itself. you can look up "sharing"... okasaki's functional data structures book has a good expo too. A list is a pointer to a value and a next list, so consing something to that list with a pointer to a new value and a pointer to the existing list. the existing list isn't modified at all so multiple cells can all point to the same list
I never said that there was not much work put into it. I just said that they don't generally do limit the versions you can use together. In fact you can usually update a Gentoo ebuild for a new version that had not been packaged in the main repositories and it tends to work. I believe that is mostly because the "carefully select versions" approach does not scale for a rolling release distro. You simply can not test if every package works with any other package, you have to limit yourself to those packages specified in a given package's dependencies and those listing that package as dependencies. 
I guess [Code Review on Stack Exchange](http://codereview.stackexchange.com) or [/r/haskellquestions](http://www.reddit.com/r/haskellquestions) is the right place for that. [/r/Haskell](http://www.reddit.com/r/haskell/) is for *other* stuff ;) . See description on the side.
&gt;I just said that they don't generally do limit the versions you can use together. They pick specific versions that work together, exactly what you claimed they do not do. Saying "but I can change the version number and sometimes it will work" is implicitly acknowledging this fact even.
Nacl was made because of wrong defaults in other libraries. Lots of security issues in products using openssl has been found as a result of useless defaults. Having wrong defaults in crypto libraries is directly harmful.
Maybe we have different definitions of specific versions but compared to e.g. the Haskell platform, with exactly one specific version, a distribution like Gentoo is very liberal on the versions you can use together. Of course they do not allow you to use just any historically existing version, so I suppose compared to something like Hackage they are specific versions.
Instead of your parseSquare write an instance of Read for your Square type.
Yes. You just call `render` at the point where you insert each object into the list. Note that applying the `render` function to the object does not actually render it on the spot. All you are doing is building up a list of rendering subroutines that have not been run yet, so it's not like you are doing all the rendering work up front. Just think of `render` as a wrapper for objects so that they can all be placed within the same list.
I haven't studied interaction nets yet so I can't comment much on them at the moment. So pretty much all type systems that I know of in the literature validate the program in two distinct non-interleaved phases: * The first phase is the parser, which validates the syntax of the program using a simple grammar * The second phase is the type-checker which specifies which syntax trees are well-formed In the untyped lambda calculus, all syntax trees are well-formed so all you have to do is just parse the tree but not type-check it.
Great, see you there!
As someone who used to be a Gentoo developer in charge of the Haskell packages, I can assure you that the versions are carefully picked, tested to work together and patched when necessary. It's slightly more flexible than a binary distro but the basics of package curation are really the same.
- You should not use `error` for expected errors. They should only be evaluated when there is a bug. - could`filter (`elem` rs)` be moved in tour'? - could you find better names for tour and tour'?
Wow, nice work. That's actually a very serious GHC bug. The code that gets run after the error should have been thrown could also be `launchMissiles`.
The only problem is that this is boilerplate.
Glad you enjoyed it. There are three reasons Shake throws exceptions there: 1. In Shake, my library takes library user supplied actions to build a file. Even if my code might never throw exceptions, that code might, and I have to deal with that possibility, and move exceptions around the place properly (there's a lot of code and tests to check for that). As a result, if I get an erroneous condition, I might as well throw an exception. 2. In Pure code exceptions aren't a good way of doing stuff. In IO code, exceptions aren't so bad. I call writeFile, createDirectory etc, any of which might throw exceptions. Having partial IO functions is in a way threading the exception through IO (semantically, if not in practice). 3. Adding an extra Maybe/Either everywhere would slow things down. Shake is highly optimised, and that cost would be unfortunate.
Yes, I was thinking more about if this will hurt performance by creating a lot of extra objects like cons cells and thunks. I find it hard to reason about these things in Haskell because of lazy evaluation and all the advanced optimizations GHC do. The advantage of an interface solution similar to what you would write in Java for example is that no extra objects are created during rendering.
Ah yeah that makes sense. But can't I define the Functor instance then to be like mapScale? ie. rename mapScale to fmap in your suggestion? I ask this because I was wondering whether it's a bad thing to lose the Functor instance.
Would be cool indeed. I haven't really looked into music libs in Haskell, any suggestions? By the way; this was my first time I dared to post any of my code on Reddit, I am glad the feedback is all constructive :-)
To do it this way, you have to give up the Functor instance because the whole point of a Functor (this is a very shaky explanation, but since you seem new to Haskell, it will be the only way to have it make sense) is that it contains a value of an arbitrary type inside of some context. Notice the mention of "arbitrary type". The functors you're probably familiar with are: Maybe, [], Writer w, etc. You'll notice that they are all missing a type variable on the end. There's nothing of type `Maybe` (because it's kinded `* -&gt; *`), but there are things of type `Maybe Int` (like `Just 4`, `Just 68`, and `Nothing`). So, when we look at the type signature of `fmap`, we can see that it makes sense for `Maybe`: fmap :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b -- And now the version specialized to Maybe mapMaybe :: (a -&gt; b) -&gt; Maybe a -&gt; Maybe b So in the functor instance for `Maybe`, `fmap = mapMaybe`. Now, let's try to do that for `Scale` and see why it fails: fmap :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b mapScale :: (Note -&gt; Note) -&gt; Scale -&gt; Scale This won't work because `fmap` expects to be able to receive a function going from any type to any type as it's first argument. But mapScale is too restrictive. It only works with `Note`s. So, no functor instance. Now, whether or not it's a bad thing really depends on what you're doing. If you frequently need to change the contents of the container (i.e. changing the `Note`s to `Int`s), then you should definitely just use a list instead. If you are repeatedly writing functions that operate specifically on a list of `Note`s, then having the `Scale` newtype is probably going to be nice. Also, newtypes tend to make type errors easier to read (this matters a little less after you get experienced with understanding GHC's type errors).
You're right. Thanks for pointing it out. I was just wondering if you can fmap over tuples and tried it out. I didn't notice that it only applies to the second argument. [Here](http://stackoverflow.com/questions/13442544/why-does-the-2-tuple-functor-instance-only-apply-the-function-to-the-second-elem) is a discussion why it doesn't work on both arguments. Mainly because both arguments might have different type.
use `bimap` from bifunctors
Oh, the misunderstanding was probably between you talking about the Haskell Gentoo packages and me talking about Gentoo packages in general. I agree that the Haskell packages are probably picked very carefully.
It's a pretty straightforward optimization since the list is immutable. Conceptually, if you prepend something to a linked list, you preserve the original list, so all pointers to the original list will have the same data as before. This is why a singly linked list works out nicely in an immutable setting. Either way, sharing is always performed with a named variable in GHC (excluding, usually, variables with type class polymorphic types). There is some information about it here https://wiki.haskell.org/Sharing It's also worth pointing out that a list is just a normal data structure like any you might define yourself. Its definition is essentially data [a] = [] | a : [a] Also, the non-tail recursive version will be more efficient since it never needs to reverse.
As discussed in [this](http://blogs.scientificamerican.com/roots-of-unity/2014/11/30/the-saddest-thing-i-know-about-the-integers/) wonderful article, 12 tones are a leaky abstraction. For a library aimed at simple descriptions (like generating fingerings for guitars in arbitrary tunings), it suffices, but if you wanted to make sound, things could get a little more complicated.
Haskell's performance model is very different from an object oriented language. Creating and garbage collecting tons of transient values is cheap in Haskell and keeping a thunk around is also cheap. Haskell is tuned and optimized for lots of short-lived immutable objects
Yes, that's exactly correct. `IO` order is determined completely by the order in the `main` syntax tree and is completely independent of evaluation order. Other programming languages conflate side effect order with evaluation order, but Haskell does not. You can think of Haskell as a two-phase system: in the first phase it evaluates the entire program and builds up a big `IO` syntax tree to run and in the second phase it actually executes the `IO` syntax tree.
Uhm that's not what I meant but OK :)
You may be interested in my pet project, [Bang](https://github.com/5outh/Bang). It is designed specifically for composing drum parts, but the code may give you some ideas and I think it's fun :) There's also [euterpea](http://haskell.cs.yale.edu/euterpea/), which is focused on music theory in Haskell, which was created by a group at Yale. You may also enjoy that! 
When you calculate a diatonic scale, if the root of the relative major scale is on the right side of the circle of fifths, then use flats. You can remove sharps/flats by representing pitches as the number of half steps from a relative pitch like C0. You can have 2 looping lists of strings to implement show: the chromatic scale using sharps and the chromatic scale using flats.
I'm shooting from the hip here (that is, not testing the code at all), but one way to go might be: data Note = A | B | C | D | E | F | G | Sharp Note | Flat Note Then you could do something like: wholeStep :: Note -&gt; Note wholeStep A = B wholeStep B = Sharp C ... wholeStep (Sharp n) = Sharp (wholeStep n) halfStep :: Note -&gt; Note ... Then you could represent a scales as, say, lists of functions: major = [wholeStep, wholeStep, halfStep, wholeStep, wholeStep, wholeStep, halfStep] From there, modes: nextMode mode = tail mode ++ [head mode] ionian = major dorian = nextMode ionian phrygian = nextMode dorian lydian = nextMode phrygian ... The strategy over all here is to parameterize over the tonic until you actually need the notes spelled out.
I just updated my comment.
I would probably build things around the notion of the intervals within an octave, define scales and chords as sequences of intervals, etc. Keep everything abstract from any particular tonic and finally treat chord and scale spellings as separate concerns.
But there is no sharing here. The code snippet above prepends m to the output of the recursive call, and it also prepends m to the accumulator `legends`, and at the base of the recursion, the accumulator is thrown away. There are two distinct calls to `(:)`, so the result list and the accumulator list are different lists (with the same contents). I think that is what he meant by "one copy for the accumulator, and the other copy to output".
From the get-go I decided (in the article you linked to) that I wouldn't care about the sharp/flat thing. So now you got me wondering: why is it actually important? What is the purpose, isn't it just a convention? Or is there more reasoning behind that? As said i'm a bit of a novice in both musical theory and haskell :-)
oh wow what a silly mistake. Thank you!
Well, you don't have E# either, so you can't represent the C# major scale either. In fact, I don't think this representation of Note is what you want. I think that you want something like data Note = Note NoteName Transformer where NoteName would be A, B, C, ..., G and Transformer would be Sharp, Flat, or Natural. Then, to display a scale, you'd start from the root, add a whole step, see if you end up on the next NoteName or not. If so, you're happy, if not, you can figure out if you need to flatten the next NoteName or sharpen the previous one. And so on. Extending this approach, you should event be able to handle double sharps, double flats, and so on, should you want to display C flat minor.
btw, why not (x == y) || mysearch y xs ?
When working on a problem, I find it helpful to write out some or all of the type signatures before considering implementations. Initially quite abstractly: `Problem -&gt; Solution` isn't helpful, but supposing we're solving a logic puzzle `(Board, Constraints) -&gt; Board` can be, even though that's still very ambiguous. It immediately suggests `(Board, Constraints) -&gt; (Board, Constraints) -&gt; Maybe (Board, Constraints)` for example. It now occurs to me that having the Board in that second parameter may not make sense. That's something I hadn't really considered but organizing my thoughts a little made it jump out at me.
Thanks for pointing out that D# != Eb. I always thought that it was the exact same pitch, and therefore always wondered why they even made a distinction like Eb vs. D#.
It all depends on which temperament ir tuning you are using. r/musictheory can tell you *all* about this :-)
And what if you use a fold instead of manual recursion? The suggestion I made was just getting rid of some "if-then-else" syntax sugar.
Thank you for actual criticism :) I'm happy to finally be able to put this in a space where my delusions that I've been crafting by myself for almost a year in my head will be vetted out. I probably should put down a table of comparisons between different packages; I mentioned a few in the README. I'll leave some here temporarily before moving them to somewhere permanent. 1. pipes and conduit primarily occupy the space of streaming resource management and processing. while it is possible to build games, gui's, etc. with pipes, it's not quite their recommended use case. pipes, etc. encourage "source" pipes, where the input comes from a source that isn't the proper "input" but an underlying monad like IO, making this style more like the processing of effectful streams and sources. 2. netwire focuses on delivering FRP, which provides an abstraction for working with "building blocks" of continuous-time behaviors. auto is strictly focus on discrete-time streams of values. netwire is not quite designed for situations with no continuous-time component, and auto is poorly suited for stating meaningful things about continuous-time behaviors. for a turn based game, or a gui with no real-time aspects, auto provides a more relevant abstraction. 3. Machines at this point is more of a proof-of-concept than an actual library for usage. Indeed much of the mechanics of the `Auto` type can be implemented in machines, but machines doesn't offer the semantic model or platform that this library has. The sum type nature of `Auto` is actually an implementation detail and might be mitigated in the future. Abstracting away, an Auto describes a *causal* stream transformation; a transformation from x[n] -&gt; y[n] over the domain of natural numbers n where y[n_0] cannot depend on any x[n] where n &gt; n_0. The type is meant to be a black box describing such a relationship. all of the actual `Auto`s provided by the library describe such a relationship....some more mathematical than others. this relationship is separate from the operational nature of `Auto`, and you should be able to reason with composition (`.`, `liftA2`, etc.) of `Auto`s by only reasoning about the composition of the systems they describe, instead of the actual underlying mechanisms that facilitate the stream transformation. If you import Control.Auto.Core and use constructors, then you sort of throw a lot of this out of the window, though.
GHC bugs aside, I think "only throw in IO code" is the way to go. In pure code, I think exceptions should be reserved exclusively for branches which the type system fails to prove are unreachable, but the programmer knows they will never be reached.
The reversal was declarative of my intention to preserve the original ordering in the input. Of course, I wouldnt actually reverse in the implementation ;)
I'll find it eventually. I'll make sure to let you know when I do.
José Pedro Magalhães Has done quite a lot of work on this. Here's one paper where he describes his approach. http://dreixel.net/research/pdf/fmmh.pdf
Just some more random observations: `Proxy` and `Machine` can both encode termination explicitly, with `Pure` and `Stop` respectively. `Wire` can't; there's `WConst`, but that's something else; nontermination, perhaps. `Auto` is in the `Wire` camp, perhaps a bit more strongly: an `Auto` is constructively infinite. To expand on my serialization observation, it seems to me that the critical difference from the above libraries is the existential `s` and corresponding ability to serialize and deserialize it. The folding libraries, `folds` and `foldl`, contain such constructions but don't support serialization. There's an encapsulation benefit to this approach--you can compose different stateful widgets without worrying about merging their states. Of course, on the other hand it's less explicit about what internal state is in play, and resuming a computation with the deserialization mechanism is a bit magical.
OK, I think I figured out how to build it as co-data: primes = ps where ps = 2 : 3 : filter' 2 indivisible [4..] filter' _ _ [] = [] filter' m pred (n:ns) -- a filter that keeps track of how many it passed | pred m n = n : filter' (m+1) pred ns | otherwise = filter' m pred ns indivisible m n = ([] ==) -- there are no $ filter (\p -&gt; 0 == n `mod` p) -- factors of n, NOTE: I'm using the normal filter here $ take m ps -- among the first m primes The trick was in making the predicate take a count as an additional argument, which `filter'` maintains.
Another way of stating it is that you should never catch exceptions thrown purely, and you don't want your program to crash with an uncaught exception. I spent half of my PhD working towards that: http://community.haskell.org/~ndm/catch/. 
If they're independently generated streams and they never touch, then there really isn't any point to combine them conceptually; you can run them on separate channels/queues/worlds. If you want to recombine their outputs eventually, you can use `ArrowChoice`'s `|||`: (|||) :: Auto' a c -&gt; Auto' b c -&gt; Auto' (Either a b) c So you can now run both `Auto`s on the same input stream, where `Left`s go to the first one and `Right`s go to the second one...and then recombine them. So a1 ||| a2 will run both `a1` and `a2` on the same input streams, but the `Left`s will go to `a1` and the `Right`s will go to `a2`.
Yes, this is a nice way to streamline my `primes` definition, if the problem is to get the primes. Remember, though, that my domain isn't integers. So, the next element to consider isn't 1 plus the current number, it's the next element in the list. Hence, my peculiar way of writing `primes`. I'm trying to keep it general enough to be able to apply the pattern to things other than numbers. For example, the list of `mortals`, at the top, was a list of arbitrary `Person`. 
thanks :D and thanks for the support along the way too :)
Are you using windows? (I ask since your usage of "command prompt" makes me think "yes")
Thank you :)
How have I not seen WinGHCI before?
&gt; using Ints which represent "half-tones above (below if negative) the root A torsor!
&gt; After a week or so start reading a bit about ambda Calculus (this article is interesting). "ambda Calculus".
Ah. Yes, you're right... in the context of IO, State, etc., "stream" already has a meaning. I'll have to wrestle with how to really describe this in a way that doesn't overlap with accepted vocabulary in this context... 
Huh, that's a pretty snazzy technique. I didn't really consider it because I thought it might be overkill for what I wanted. I am thinking of porting this to Idris... If I do I will certainly give it a shot.
Nice domain name!
Is it possible to edit existing comments with intolerable/reddit? edit: What exactly does flattenComments do? What's the best way to make sure I get all replies to a given comment, even if there are hundreds of them? Is [this](https://github.com/intolerable/reddit/blob/master/src/Reddit/Actions/Voting.hs#L38) intentional? It looks like that should be -1 and unvote should be 0.
Since you are on Windows, you can actually load scripts into GHCi by just dragging the .hs file to the ghci.exe file and releasing it on that. This tells Windows, "open ghci.exe with this .hs file, please".
This talk has got me thinking that React and D3 are essentially the same thing, except that instead of a virtual DOM, D3 uses `.data`. Does that make sense?
Could you please also contrast `Auto` with the [Model](https://hackage.haskell.org/package/mvc-1.0.3/docs/MVC.html#t:Model) type from the [mvc](https://hackage.haskell.org/package/mvc) package? Thank you.
This is my first proper post on /r/haskell. I wanted to try and bring together stuff I've learned from GL tutorials scattered around the Internet in various programming language to make a single quickstart blog post that can help people get up and running with gl in haskell as easily as possible! Feedback appreciated :-)
7.10 out today?
This sounds similar to the approach I took when I implemented a similar thing, extended to chords as well. See my other reply: http://www.reddit.com/r/haskell/comments/305ein/music_note_datatype_help_ie_how_do_i_not_put_an_a/cpq208b
You can always convert back and forth between lazy and strict `ByteString`s by using `Data.ByteString.Lazy.{toStrict,fromStrict}`. One way to handle `Either` is just to do: case decode o of Left l -&gt; empty &lt;?&gt; "Your error message here" Right r -&gt; return r
I see that someone downvoted this, so could someone point out the mistake? Because I also thought this. It would allow for simpler abstraction away from the triplicate code in the main.
You can use minimumMay from https://hackage.haskell.org/package/safe-0.3.8/docs/Safe.html to turn [(length (surrounding next rs), next) | next &lt;- nexts] into a Maybe (Int, Square) without first checking for whether the list is empty because minimumMay does that, then fmap snd across the result; then you can inline nexts.
&gt; The beauty of lens is how well it fits into the rest of haskell. But you are using the word "fit" in a different way. They fit in the sense that lenses are *usable* with "the rest of haskell", due to the massive (and very clever) polymorphism. But they do *not* fit with the traditional *style* of Haskell, and hence there is a lot of intellectual dissonance with much of the Haskell code out there.
Really, really, really nice and thorough article! It was a pleasure to read, and I'll certainly refer to it when working on a GL app of my own.
&gt; You can use any editor or IDE to edit the script and save it And just to make it clear - you can save your script anywhere you'd like. As for which IDE or editor: the most commonly used apps to create and edit Haskell programs are Emacs and Vim. However, those take time to learn how to use, especially for someone who has not programmed before. Surprisingly, a few of our developers here have found that on Windows you can use Notepad++ quite comfortably for Haskell development. It is very simple and intuitive for people used to Windows, it does have some built-in Haskell support, and even some fairly powerful features if you look for them. So I could recommend that to start with. In the meantime, though, I do suggest that you pick one of Emacs or Vim, install it, and start learning it in your spare time. In the long run, you'll be glad that you did.
What about regular expressions?
Hmm. What sort of thing did you have in mind? 
I will be hanged while being burned for saying that. The negative votes will make this post reach the center of the Earth, but I never found quickcheck useful. I never found any case in which I really needed quickCheck because I ever found that the thing being checked was either trivially true or else, I feel that it needed to be redesigned as a combination of things that are trivially true. You can only test things that you know that are right. If so, why make the CPU hotter?. In the other side, if you suspect that are wrong, invest the time in fixing them until they are trivially right instead of wasting the time creating tests. Our prophet Djistra said that a testing shows the presence, not the absence of bugs. And "How do we convince people that in programming simplicity and clarity —in short: what mathematicians call "elegance"— are not a dispensable luxury, but a crucial matter that decides between success and failure?" Today Djistra would have been stoned under a myriad of inutile unit tests that people perform for a sense of false security, in a sort of superstitious sacrificial ritual, a waste of time to convince himself and others that his software is right. That is encouraged by the companies, that display the tests coverage as a form of quality assurance. But it is not. But there is a secret pleasure in burning CPU cycles for testing trivialities. And the satisfaction produced by these gorgeous messages: testing x+ x= 2 *x, 1000 test OK (this would add enough negative votes to put this message in orbit) I do not say that testing is useless. I mean that elegance is far far far more important and it is the best quality assurance criteria.
I don't mean to be a dick about this, but have you considered you might just have been doing it wrong? I mean, I agree that a *lot* of quickcheck tests are pretty useless. But at the same time the majority of times I've brought a quickcheck like tool to bear on an already well tested project I've found bugs in it, and there's literally an entire company that makes a business out of using Quickcheck to find bugs in other peoples' software.
Offtopic, but it's strangely comforting to read a Web page that's been typeset in [Computer Modern](https://en.wikipedia.org/wiki/Computer_Modern). I don't think I've ever seen that done before!
Maybe I'm being dim, but I really can't see what a property for a regex would look like that wasn't just reproducing the same logic as the regex. Could you give me a concrete example?
What exactly are you trying to do? Get a fully-expanded tree of comments underneath a post?
Got to get this set up next: http://www.bramstein.com/projects/typeset/
If you agree that a lot of tests are pretty useless, then we are 90% in agreement. In the other 10% the proliferation of errors for which quickcheck is necessary could be due IMHO to a lack of simplicity/elegance. for the other few, yes, some test are inherently necessary.
You could also use lens (I know, I know, not something to throw at a beginner), right? Something about `each` and `Traversal`? You wouldn't happen to know of any examples of how to get mono-traversable behavior in lens, would you?
Well, matrices get serialized in the wrong order for one, row-major in `linear`, but OpenGL expects `column-major`, so you need to transpose everything. There are some options for supplying matrices in row-major order that were added for compatibility with D3D, but sadly if you go to use them in OpenGL ES they just bomb. Alignments are the big killer here. In STD140 they are rounded up to an integral multiple of the base element size (at least 4) in a way that makes it so that `Mat43` serializes wrong. Consider the subtle issues caused by alignments, such as gaps in the structure due to alignment-restrictions on `Vec2`'s and `Vec3`'s such as: instance Block Mat3x4 where alignment140 _ = 16 -- 3 rows, but vec3 rounds up to vec4 sizeOf140 _ = 64 -- 4 columns, each rounded up to 16 bytes each alignment430 _ = 16 -- 3 rows, but vec3 rounds up to vec4 sizeOf430 _ = 64 -- 4 columns, each rounded up to 16 bytes each STD430 relaxes some of these restrictions, fixing the Vec2 problem, instance Block Mat2 where alignment140 _ = 16 -- per vec4, despite being vec2 sizeOf140 _ = 32 -- 2 columns, each rounded up to vec4 size alignment430 _ = 8 -- vec2 sizeOf430 _ = 16 -- 2 columns, each vec2 in size but Mat3x4 would still serialize wrong as `Serializable` above. Then we get to structures. There are a number of arcane rules about how to align arrays: https://github.com/ekmett/quine/blob/master/src/Quine/GL/Block.hs#L914 structures contained in structures: https://github.com/ekmett/quine/blob/master/src/Quine/GL/Block.hs#L172 how to calculate the alignment of a struct with multiple fields: https://github.com/ekmett/quine/blob/master/src/Quine/GL/Block.hs#L145 All of these semantics differ from `Storable`.
Basically, I think if you require your code to be simple and elegant in order to believe it's correct I think you're going to spend a lot of time writing code that doesn't do what you want and is still wrong. I trust tested code much more than I trust code that is "obviously" right, and a lot of complexity is irreducible.
&gt; This is pretty much the standard existential typeclass antipattern example Just because one Haskeller wrote a blog post 5 years ago calling this an antipattern doesn't make it an antipattern. 
Try `getPostSubComments`, if you give it a PostID and the CommentID for the sub-tree of comments you want, you should be able to get a list of CommentReferences. If you use `getMoreChildren` if there's any References (as opposed to Actuals), you should be able to get concrete comments for every reply to the specific comment. Also, I don't suppose you use skype or something similar? Doing this on reddit is kinda clunky.
Modeling a poker game, using quick check to show that the total number of chips is always the same after random number of random moves. Likewise that no cards disappear.
For lack of a better place to ask questions about the library, I ask here. Is there any way to have an output which is longer than the input? In particular, something capable of doing this: &gt;&gt;&gt; streamAuto' magic [1,2,5,6,9] [Left 1, Left 2, Right 3, Right 4, Left 5, Left 6, Right 7, Right 8, Left 9]
&gt; "How do we convince people that in programming simplicity and clarity —in short: what mathematicians call "elegance"— are not a dispensable luxury, but a crucial matter that decides between success and failure?" While I agree with Dijkstra that we should always be striving for elegance. "We'll do it better, instead" is probably the worst argument for removing a QA process I've ever heard. It's rather encode my QC properties in my types and have them be verified during compilation / type checking. But, that's not always possible, and when it is the effort required to do so is 3x-10x as much as writing the property. Even *if* our code is elegant, we *still* need QA -- maybe not QC, but something -- to, at the very least to check the the model the developers have built for the computers to check against is consistent with the requirements/features/stories. A model will only arise directly from those items in relatively few fields.
Read should complement Show, at least that's what one would expect.
I think so yes. My memory is a little hazy but if I remember correctly when we update a closure (an STG closure, not the ones in my compiler) we set it to a special blackhole ptr to indicate "Hey, this is in the process of being updated so if you need it right now you're hosed". This is precisely what the blackhole attribute in my code does. The big difference is that something like `inc` checks blackholes in my rts and thus `fix x in suc x` will fail the blackhole check, but in an STG machine something like `inc` would return a closure and it's OK to close over blackhole'd pointers. This is just the difference between eager and lazy evaluation in a nutshell. 
&gt; I had quickcheck compare an online variance algorithm (copied from Knuth, but it's still possible to have bugs in translation) to the dead simple standard non-online version. People always give me the weirdest looks when I propose this type of thing as a property / unit test. I think they see having two implementations are being twice the trouble, but I see it as a way to allow both incremental improvement or wholesale replacement of the code while retaining the "original" implementation *as* an automated specification to ensure there are no regressions.
I've always found the reversing a list business not compelling (if useful for describing the internals), so I created a simple example of using quick check to check for (toy) "compiler optimizations" (search for "Constant Folding"): http://cseweb.ucsd.edu/classes/wi15/cse230-a/lectures/lec-quickcheck.html
I don't know how appealing this would be in your documentation, but I found the [example](http://www.haskellforall.com/2013/11/test-stream-programming-using-haskells.html) of using Quickcheck to test Pipes quite compelling. It is, however, writing a library, as you warn against.
Well, type checking is worst case exponential, so it's easy to construct very slow examples. 
Well, that doesn't explain why 7.8 is slower than 7.6. Is it because of the new type system extensions? 
 update :: DemoState → GLfloat → DemoState update s dt = ... Uh-oh, never do that, please! If you do this, the result becomes framerate-dependent. Instead, query the time elapsed (I'm pretty sure even without looking that GLFW can give you that), and compute the state as a pure function of time. If you absolutely need this type of state update, compute how many frames passed at a *fixed framerate* (say 100hz), and do the necessary number of updates. Then interpolate between the two closest frames. 
Out of curiosity, what are the advantages (for your case) of SmallCheck over QuickCheck?