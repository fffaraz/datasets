I'm not familiar with combinator calculus, but I don't understand how you can define a function from the natural numbers without any recursion?
FRP is not solving the problem I am talking about, which is customized widgets/layouts. Currently almost all native GUI frameworks use subtyping/interfaces to do things like "I want to sublcass this ComboBox widget but change its default height". FRP doesn't solve this at all, its solving an orthogonal issue. ELM targets Javascript, where your widgets live in CSS/HTML (CSS also exhibits this subtyping type behaviour, at least if you want to model it in a typed manner)
&gt; I don't care what every mainstream library does today. When someone finds a nice solution things can move very quickly so it's better to keep focusing on what's right even to the exclusion of what's "right now", IMO. Then don't complain about having shitty support for GUI programming in Haskell. If your language is unable to cleanly model the problems that you deal with in GUI programming (i.e. inheriting a currently existing widget and overriding some of its behaviour, i.e. the height of the widget) then you have to deal with having "stringly typed behaviour" when working with existing GUI frameworks.
Its personal because his issue is not rational. All of the other languages he mentioned (barring Haskell) are even **worse** than Scala in the common complaints he was giving (i.e. purity, side effects, referential transparency). If he said all of these other languages (along with Scala) are "bug ridden" or "crap" then I would say otherwise, but this isn't the case.
&gt; The story is more complicated by the fact GHC does support these features with LLVM (but nobody added support to the ordinary backend which sucks). I just want to emphasize how unfortunate this is. I really wish I could use the SIMD stuff from `ghc-prim`, but the LLVM-backend restriction is pretty crippling when it comes to releasing code to hackage.
The [tools section](https://pvp.haskell.org/#tools) of the PVP spec lists a few options: - [ghc-pkg-apidump](http://code.haskell.org/gtk2hs/tools/apidiff/ghc-pkg-apidump.sh): A tool for comparing the exported API of different version of the same GHC library package. - [precis](https://hackage.haskell.org/package/precis-0.5.0): Summarizes API differences between revisions of Cabal packages. There's also [hackage-diff](https://hackage.haskell.org/package/hackage-diff-0.1.0.1), which compares the public API of different versions of a Hackage library. And for what it's worth, Elm's package manager (`elm-package`) has a `diff` subcommand that does exactly this: https://github.com/elm-lang/elm-package/blob/1a364bc/README.md#publishing-updates
Thanks. My question was really a roundabout way of asking how to formalize the idea that "the initiality of [zero, succ] implies the induction principle of natural numbers" (and what to add if it initiality was insufficient). gdijkstra's answer is good. The second part of my comment was a handwavy attempt at viewing induction categorically and imagining how a proof of that statement might look like.
If you're wanting to write the frontend in elm and the backend in haskell, you may want to build a REST-like client on the backend. For simple applications and beginner friendliness, [scotty](https://hackage.haskell.org/package/scotty) is a good choice for this type of thing. If you change your mind and want to work with a haskell framework (which is probably more straightforward), you might want to work through the [yesod book (free online)](https://www.yesodweb.com/book/introduction) and go from there.
I just meant that you can define `even` and `times2` without direct recursion or a fixed point combinator (Y) because Church numerals encoded as functions already give you iteration in the form of a fold: given a number `n`, `n s z` applies `s` to `z` `n` times (by definition). So for example `plus` is defined as `λmnsz.ms(nsz)`—“m times, increment (n times, increment zero)”. With a data type, here’s another starting point: foldNat :: (a -&gt; a) -&gt; a -&gt; Nat -&gt; a foldNat s z = go where go n = case n of Zero -&gt; z Succ m -&gt; s (go m) Then `plus` is `foldNat Succ`, `times2` is `join plus`, and `even` is `foldNat not True`. 
Thanks! Do you have any thoughts on security for the REST backend? What are the go-to libraries for service security in haskell?
Oh I see what you mean now, thanks. Indeed, I have yet to a reasonable widget framework for GUI using FP.
Great, thanks! 
 class Magnify (OpT w o r m) (Opt w o' r m) o o' where magnify lens (OpT m) = OpT (m . fmap (view lens))
No thought has been put into making Tardis performant, so yes, there may be time or space leaks. If anyone has done benchmarks to measure such things about the tardis, I'd love to see them.
Well stay tuned, better docs are on their way. Thanks for the input.
Thanks for explaining this in such detail! I really appreciate it. Figuring out the status quo from the outside is hard. I've done my best to remove confused bits from my post, and have linked here instead. It sounds like automatically vectorising loops like mine is a hard thing, but there is a lot of room for library improvement using SSE vector operations directly? Is that a fair interpretation? Being able to map over a bunch of `Word8X16#` or something to transform a ByteString would be quite nice, if it can't be free.
&gt; Then don't complain about having shitty support for GUI programming in Haskell. I don't because it's not. There are interfaces to native GUI frameworks that work just fine. There's no reason to be compatible with *Java* (for GUI programming) which, itself, has "shitty support for GUI programming".
I would say it's an acceptable way to do it... *if* you already have a pretty good idea of the way GHC uses package-db stuff. Not a bad way to go if you've got a custom build pipeline and/or custom sandboxing. But most devs will appreciate the convenience of cabal or stack, and I would definitely recommend these friendlier tools to newcomers.
IMO the answer is to work toward LLVM being the default backend. There's much more potential there than in the custom native code gen.
https://stackoverflow.com/questions/12518149/with-haskell-and-gtk2hs-how-would-i-create-a-new-widget-and-associated-events Yeah, no thanks.. The interfaces work for very basic stuff (when you want to use an already existing widget), but the moment that you actually have to implement your own widgets by subclassing another widget (this is very common when doing serious UI work, not toy applications) then things are quite ugly
I've wondered about that before. To my knowledge, rust (which, admittedly, is much newer than haskell) only targets LLVM. I don't know why GHC targets two backends because it seems like for maintainers that must be extra work. Maybe the NCG offers faster compilation or something like that.
&gt; Maybe the NCG offers faster compilation or something like that That is certainly true today, though I don't think that is a necessary limitation of the LLVM backend. I think there are some Core/STG/Cmm level optimizations that GHC does which are redundant with the LLVM backend, so we could save time there. The main reason it's slower is that you have to run the LLVM optimizer in addition to the GHC one for each module, so I also think ThinLTO would go a long way toward reducing that problem.
Sent you a DM about potentially collaborating on an Elm project.
Wow. This work on adding linear types continues to be the most exciting addition to the world of Haskell in years. Very envious of the folks who are working on it!
I'm building a game. The main loop takes and "modifies" a record, the game state. It looks something like: mainLoop gameState = do ... gameState' = movePlayerA gameState gameState'' = movePlayerB gameState' gameState''' = detectCollision gameState'' ... mainLoop gameState'''' I *think* I can simplify this as `[GameState -&gt; GameState]` and fold over that. But I was wondering if this is something that would be simplified using the State monad.
A really simple CLI-tool: [writing a simple cli tool in haskell](https://maex.me/tech/2016/06/25/writing-a-simple-cli-tool-in-haskell.html)
Clickbait typo :)
As I said in the other little comment tree thing, you can _try_ to do some of that, but sometimes it's just not as helpful as you'd hoped. Particularly, GHC only exposes the `Word#` and `Int#` types for unboxed numbers, you can't pick a specific bit width, which means that you can't write certain kinds of RNG for example (unless you want to do the masking yourself). And, because library code expects boxed values, if you're calling into library code a lot your values get reboxed temporarily. You can hope that GHC will notice things and prevent the re-boxing, but it won't always do that. It's just a rough situation compared to Rust where things are big piles of (well typed) strict, unboxed primitives by default.
Yes, their [FFI](https://blog.eta-lang.org/the-best-of-both-the-worlds-eta-and-java-part-3-4100a1e4b96d) looks neat! I'm really curious to see how they handle calling overloaded functions from the Eta side.
Well haskell is exciting, amiright?
Indeed GUIs are a big problem in Haskell. Furthermore there are a number of OO libraries that implement really hairy (and evolving) protocols like [DICOM](http://dicom.offis.de/dcmtk) that basically drive tech adoption in industries that use them ( in this case medical imaging ). Reusing these reference implementations is such a pain that it effectively keeps Haskell out of these domains.
Now, under Bash for Windows. No errors *initially*... me:~$ git clone https://github.com/ghcjs/ghcjs.git Cloning into 'ghcjs'... remote: Counting objects: 8765, done. remote: Compressing objects: 100% (2/2), done. remote: Total 8765 (delta 1), reused 0 (delta 0), pack-reused 8763 Receiving objects: 100% (8765/8765), 4.82 MiB | 1.37 MiB/s, done. Resolving deltas: 100% (4438/4438), done. Checking connectivity... done. Checking out files: 100% (1612/1612), done. me:~$ cd ghcjs me:~/ghcjs$ cabal install Resolving dependencies... Configuring ghcjs-0.2.0... Building ghcjs-0.2.0... Installed ghcjs-0.2.0 me:~/ghcjs$ ghcjs-boot fatal: program node is required but could not be found at node Now, I still get that last error even after installing nodejs and then rebuilding ghcjs. 
Yes, you can and probably should do so. Just change the type of `movePlayerA` from `GameState -&gt; GameState` to `State GameState ()`. You're likely to want to switch to an `mtl` version using `MonadState`, but it's hard to suggest precisely how to do it for your game without more context.
Has its documentation gotten any less awful? That was its big weakness when I used it a couple of years ago.
Fantastic! Just don't write the following in the code editor: let x = show x in do putStrLn x
We are still organically growing... just an FYI if you're interested. ;) EDIT: of course, it would also be great to see other contributors apart from us push this forward as well! (in addition to all the existing research / academic efforts)
I've heard here and there that GHC's default prelude is "bad". I understand most of the points made, but I have trouble choosing an appropriate alternative. For information, I'm writing a toy compiler. What are good prelude alternatives for a beginner haskeller?
Definitely! Have a look at the monad pages in "Learn you a haskell for great good" book, there are some nice examples there.
Nice.
This is a nice use of linear types. I'm looking forward to seeing more details.
Do you have node-js installed on your path? 
But what is it? Is it Haskell on the JVM?
Thanks for your write up. It was very informative. It reminded of this paper: http://www.leafpetersen.com/leaf/publications/hs2013/hrc-paper.pdf I think the source was released recently.
So the other day I was writing a simple RPN calculator, and thought it might be nice to be able to retrieve the last answer by entering "h". I'm pretty sure I would use StateT in order to achieve this, but I'm unsure how I would go about using it in the main function. Basically I had something like this main = forever $ putStr "&gt; " &gt;&gt; getLine &gt;&gt;= \c -&gt; print $ process c Where process does some work of some kind.
Another main reason the LLVM backend is slow is that the compiler uses Haskell `String`s to generate llvm. That's right: not using FFI to construct C++ objects for LLVM IR, not even using bytestring to generate bytes, but a linked list of characters. 
GHC's support of llvm always seemed like an afterthought to me. A while ago someone posted here talking about how ghc should've generated llvm IR from STG, instead of going to C-- then LLVM IR. The return-a-function-pointer way of doing things also prevented llvm from doing certain optimizations. The horrible llvm mangler is also a source of slowness, hackiness, and bugs (see #9439 for a hilarious bug related to the mangler). IMO, I really like the idea of making LLVM the only supported backend, and just dispense with the other two (NCG, via C). Requiring llvm might mean building GHC itself is more complicated, but in the end I think it might be less of a maintenance burden. Any devs on the GHC want to chime in on this? Maybe /u/bgamari or /u/angerman can provide more insight?
Thanks for the neat writeup! Minor comment: you say "if we know (or postulate) that g_2 ... is increasing..." but I'm not sure this is really what you mean to say. If g_2 is increasing, then the only thing it can possibly do is send the first B numbers to 1, then the next B numbers to 2, and so forth. And technically that is nondecreasing rather than increasing. Perhaps you mean to say that g_2 is increasing *on each A_s*, which I think is all that is needed. I might be missing something though, I have never seen this stuff before.
Offtopic, but the website is unreadable on my Android tablet using the Reddit is Fun app. The menu takes up the entire screen leaving only about 1.5cm of screen space to try to read the article. Every tweag article that's been posted here has had the same problem :(
Doesn't that allow duplication? In which case it would be Multiset.
I imagine it's mostly a matter of opinion, but I have attained great pleasure from using [`protolude`](https://hackage.haskell.org/package/protolude). Here is the blog post explaining the author's design decisions: [Building a Better Custom Haskell Prelude -- Stephen Diehl](http://www.stephendiehl.com/posts/protolude.html).
For me it's not the menu but the scrollable header, when it is half-way scrolled up there is a sudden jump when the body jumps up and this header becomes transparent except for the text in that header which is then fixed in position. (Also Android + RIF, but on the smartphone)
GHCJS isn't easy to install and sometimes the install runs without any errors, but still doesn't install correctly. Install ALL the dependencies in the GHCJS README and use a recent version of nodejs.
Agree that Scotty is a great choice to start from. Both Yesod and Servant make use of fairly advanced Haskell and generate difficult type errors.
&gt; Sadly collecting all error messages is impossible for monadic parsers. You don't need _all_ of them just multiple.`megaparsec` lets you achieve this without too much difficulty. For example: if you fail to parse an array element you could skip ahead and attempt to parse the second one. If the JSON is syntactically malformed you won't be able to but in the more common "you forgot a key" situation you could output multiple helpful errors! It should definitely be possible to get mutliple error messages for something like the example posted in the OP.
I love this
Fair point, let me rephrase that: If you build the parser with monadic bind (`&gt;&gt;=` or do notation) then you can't collect multiple error messages. If you use Applicative (`&lt;*&gt;`) or Alternative (`&lt;|&gt;`) you can. Since Applicative is a super class of Monad `ap` should be equivalent to `&lt;*&gt;` so adding the error collection to the applicative instance only is a bit awkward although you could argue that they are still morally equivalent.
Someone contacted me about possibly working on this. Maybe your dreams will come true.
Where are you guys located? Remote work possible?
I disagree. There is great value in GHC having a code generator of its own. It is far easier to work on, hack, and debug/grok as opposed to LLVM. LLVM and GHC are already extremely complex projects, and I've always been wary of tying them to each other and making dependence mandatory. (When things go wrong, they *really* go wrong and it's not fun to debug 600,000 LoC of C++). There is a maintenance aspect of having to maintain it, but it isn't that big. Just as an example of that metric, there are singular directories -- at one point, single *files* -- in LLVM with more code than *all* of GHC's code generator backends combined. All the backends combined are like 15,000 LoC. Furthermore, part of the problem is just that it's *old* and not fundamentally complicated. The instruction selector could probably be a fraction of the size with some metaprogramming. You can explain most of what it does in a few hours (instruction selection &amp; register allocation). I imagine you could reduce the size of the current backend quite a bit, and make it easier to use to boot. Finally LLVM doesn't even do most of the actual hard problems anyway. It does not have a proper way to model the kind of CPS-style code GHC generates, and most of our optimizations are done beforehand. We just use it as a dumb glorified register allocator, essentially. And the fact LLVM supports multiple platforms is less rosey than you think -- many of the backends have various types of problems or inconsistencies to consider (e.g. aggregates behave *way* different between multiple backends, have various bugs, on some archs certain features outright do not work, etc. X86 is extremely well supported, though). It's very good at register allocation and certain optimizations, though (GVN, LICM, vectorization, etc). There are also a raft of other problems (like LLVM being far slower than the NCG for big chunks of code, because LLVM is just a big slow optimizer, and does many redundant passes it probably does not need to) but those are minor details that could be fixed by tuning. All that said I'm in a minority here, clearly. And I'm not listing the advantages LLVM would give us in some other ways, e.g. better methods of choosing between code size and speed, or things like the helpful tools such as XRay. (And personally, I find the amount of "brain drain" induced by LLVM rather disappointing in terms of solid compiler design and implementation for bespoke purposes, but I understand why it's succeeded at this...)
Say I wanted to contribute to this (part time and for fun - not as a job), would there be anything for me to do? My understanding of the current situation is that tweag.io is busily working on their fork of GHC. Is there anything else that needs to be done?
Actually, most of the optimizations GHC does cannot easily be performed by LLVM. It essentially feels LLVM "post-CPS" code from the C-- code generator, which completely destroys its ability to do things like generalized code motion, or in some cases even 'simple' things like constant propagation (e.g. it might not know if it's safe to propagate across certain kinds of control flow). It does not have the right model for what registers can alias, etc. All of these are substantially easier prior to LLVM codegen. (I speculate even "Advanced" optimizations like GVN would be more profitable for GHC to implement, than LLVM, because it can do them earlier.) Just as an example, the example in the OP's post does not magically become faster with LLVM, in fact it completely fails to do anything other than what GHC does already. The only actual difference is the compiler literally is slower. It's more than just having the optimizations written somewhere -- you have to have the information to exploit them. Prior work on adding vectorization to Haskell compilers is in fact *based* on exploiting this information at a high level, in *Core*, well before LLVM could ever hope to understand it. And it's how e.g. Intel's compiler got such incredible results with a relatively simple vectorizer. It does very well on straight loop bodies with floating point operations, though. Even then that's mostly GHC doing the hard work of loop-ifying the body in the first place...
&gt; Why Applicative? Why not Functor? Constructors with more than one sub-expression can not be traversed by non-applicative functors. In particular, how would you write traverseExpD f d (App x y) = App &lt;$&gt; f d x &lt;*&gt; f d y without `&lt;*&gt;`? The recursion schemes work is based on considering the data type as a fixed point of a functor. In the case of the lambda calculus from the article, this would be data ExpF a = Var !Int | Lam a | App a a | Global String deriving (Functor, Foldable, Traversable) with fixed point newtype Fix f = In { out :: f (Fix f) } type Exp = Fix ExpF Then `traverseExp` could be implemented as traverseExp :: Applicative f =&gt; (Exp -&gt; f Exp) -&gt; Exp -&gt; f Exp traverseExp f = In . traverse f . out The main practical reason not to do this is that you would end up with `In` constructors all over the place, although nowadays that can be solved with pattern synonyms. I also don't immediately see how to track binders. Also, we are letting Ghc do the deriving here, if we had to write a `Traversable` instance for `ExpF` ourselves, it really wouldn't save any effort.
What's the standard/best command line option library at the moment? I've never used one before. [This](https://wiki.haskell.org/Command_line_option_parsers) seems to indicate there are a fair few, but optparse-applicative is the only one I've heard of before.
GHCJS is notoriously difficult to install; most people that use it in production end up using Nix as well.
I'm not saying the GHC optimizer isn't still doing the bulk of the work. It definitely is. But architecturally, I think it's much much better to push the work of codegen off to a project that's more tailored to doing so, and that makes targeting new platforms is a massive amount easier.
Agreed, GHC is doing the important work. But I'd be surprised if there were literally zero work that we could remove from GHC if LLVM were the only backend; that was my only point about saving time on redundant optimizations. Anyway, the ThinLTO point was the more important one. ThinLTO can do a much better job at optimizing LLVM optimization time.
OK, thanks—I'll have to dig in and figure it out. One thing I forgot to mention is that each of those `GameState -&gt; GameState` functions can actually take in more data, like keyboard input. So more like, `movePlayerA gameState keypresses`.
Yes, it is.
This may sound like a crazy idea... but given you have already identified the places where `optparse-applicative`'s documentation is lacking... have you considered helping with improving the docs?
Via-C is needed for bootstrapping GHC on new platforms. It would be a shame to limit GHC only to platforms already supported by LLVM. Especially since probably in reality we will only support platforms with mature and complete LLVM support, which is very few. In truth, bootstrapping support has fallen into somewhat of a state of neglect. I would much rather see that fixed than giving up on it completely.
Ok, so maybe a few things need to happen here, either they could put a very clear "Why do we ask you to login" section on that page, or they could use cookies to manage where you were up to and ask "would you like to go back to your previous spot?" Cause if it's just a tour, progress can't really be so essential to the UX that it absolutely needs to be persisted, a cookie would be good enough in my view, right?
Sets are actually: `Set x = x -&gt; bool = 2ˣ` i.e. all the ways to pick elements from a set to form a new set. To avoid logarithms, one possible expansion for that is: `Set x = sum(binomial(x,n),n=0...∞)` However, that interpretation isn't so simple to actually take the derivative of, because the terms of that look like this: n | binomial(x,n) | Expanded 0 | 1/0! | 1 / 1 1 | x/1! | x / 1 2 | x(x-1)/2! | ( -x + x²) / 2 3 | x(x-1)(x-2)/3! | ( 2x - 3x² + x³) / 6 4 | x(x-1)(x-2)(x-3)/4! | (-6x + 11x² - 6x³ + x⁴) / 24 5 | x(x-1)(x-2)(x-3)(x-4)/5! | (24x - 50x² + 35x³ - 10x⁴ + x⁵)/120 You get the idea. Based on the second column `binomial(x,n)`, this is saying: * There is one way to pick no element from a set of cardinality `x`. - The empty set. * There are `x` ways to pick one element of a set of cardinality `x`. - Each element. * There are `x(x-1)` ways to pick two elements of a set of cardinality `x`. The order does not matter, so divide by `2! = 2`. - Since you already picked one element first, you're left with one less element. Double-picking is not allowed: Each choice "deletes" that choice from the source set. Note this also works if you try it with, say, `()` which corresponds to `x = 1`. In that case, the expression becomes 0 - there is no way to get two distinct elements from the unit type. etc. But anyway, the reason it's a little hard to perform the derivative here is because each higher term provides extra terms for all the lower levels except the constant, changing the coefficients throughout. Furthermore, in the expanded form, weird terms pop up. Negative values as well as strange fractions. In that form it's not at all clear what's going on, at least to me. Though the derivative for a particular `n` you ultimately get is: `d/dx binomial(x,n) = binomial(x, n) (H(x) - H(x-n))` where `H(k)` is the `k`th Harmonic number. Or you could just expand the factored form manually like this: n | binomial'(x,n) 0 | 0/0! 1 | 1/1! 2 | 1/2! ((x-1) + x) 3 | 1/3! ((x-1)(x-2) + x(x-2) + x(x-1)) 4 | 1/4! ((x-1)(x-2)(x-3) + x(x-2)(x-3) + x(x-1)(x-3) + x(x-1)(x-2)) 5 | 1/5! ((x-1)(x-2)(x-3)(x-4) + x(x-2)(x-3)(x-4) + x(x-1)(x-3)(x-4) + x(x-1)(x-2)(x-4) + x(x-1)(x-2)(x-3)) This probably best illustrates what's going on in the derivative. As always, the order does not matter, explaining the factorial up front. Otherwise you have n choices for a target set of size n: * There is no way to get a set of cardinality 0 with one hole. * There is one way to get a set of cardinality 1 with one hole. - Replace your one value by a hole! * Either your first or your second element could be a hole in a set of cardinality 2. * Either of your three elements could be a hole in a set of cardinality 3. etc. And finally, for a target set of arbitrary size, you just add up all those possibilities. In the limit, you of course get `Set' x = 2ˣ log(2)`but I have no clue how you'd interpret *that*. There is just one issue which I don't know how to resolve: I'm pretty sure none of these give an integer for any finite positive integer `x` once `n&gt;1`. It's always some fraction. Not sure what it means that there apparently are 5/2 ways to pick 1 element and 1 hole from a set of cardinality `x=3`. For the hole-less case, you get 6/2 = 3 ways, namely, given a set {1,2,3}, either of {1,2}, {1,3} and {2,3}. One last thing. The above-mentioned series is actually a special case of the more general `aˣ = sum(binomial(x,n) (a-1)ⁿ, n=0...∞)` so, assuming that fraction problem can be resolved, the same analysis could be done for a general function `x -&gt; a`. In fact, the only thing that changes is that you get an extra factor `(a-1)ⁿ` throughout. (For `a=2` that was trivially always `==1`)
Also note there is [elm-bridge](https://hackage.haskell.org/package/elm-bridge) that can generate Elm code with your Haskell types, along with the JSON encoders and decoders.
At the moment, offices in Paris and Cyprus. Remote work possible, but sometimes we hire exclusively for local projects.
Actually, the first part of the post hints at one of the few exceptions to the rule in the last part of the post: When you fork a process from inside a warp request handler thread, you want the parent thread to end promptly and send out the response. You can't afford to wait for the child thread to finish. So you need to use `forkIO`, not `async`.
I personally mostly copy code around - e.g. for a normal option. Little tweaking of the settings, perhaps some `many` or `optional` dropped at places, `fmap` on the parsed values, etc. I find all the combinators to be reasonably named.
A big one would be elaborating new use cases, and then talking about it. The more use cases, the more likely it is linear types will make it through the GHC proposals committee. We talk about a few in the paper, and /u/ehubinette has been looking at how to enforce safe use of first-class streams, but we've seen plenty of other ideas floating around. It's also a good way to find bugs in the implementation (which insofar as they are specific to the linear-types branch, should be reported on the [GitHub issue tracker](https://github.com/tweag/ghc/issues)).
That sounds reasonable - even opening a GitHub issue with additional questions or request for clarifying a section would be helpful I imagine.
What does "amenable to network communication" mean and how does that reduce the copying ? Also in the initial case, I only see four copying (instead of five): * Initialize serialization at host * Deserialization at remote size * Serialization at remote side * Deserialization at original host So which place am I missing the copying ?
If the NCG is easier to work on, why did all the SIMD stuff only get added to the LLVM backend? Is there something specific about these operations that made it easier to implement them for the LLVM backend, or is the difficulty comparable for both backends?
That would be awesome.
In situations like that I've started to write to a bounded channel instead of just forking, so that you can apply back pressure if the actions start piling up for whatever reason.
&gt; Via-C is needed for bootstrapping GHC on new platforms Could you elaborate? I would have assumed that the process for bootstrapping to new platforms would be just: 1) add a native code generator for the platform and hope it works 2) do a normal cross compilation build
Even if you do have node-js installed, unfortunately GHCJS is hardcoded to find it at /usr/local/bin/node
Out now: https://mail.haskell.org/pipermail/haskell-cafe/2017-August/127789.html
Adding one to each node of an immuatable tree copies all the links.
Would defining a `Traversal'` work with linear types? leaves :: Traversal' (Packed '[Tree]) Int so that `add1` can be defined add1 :: Packed '[Tree] -&gt; Packed '[Tree] add1 = over leaves (+ 1)
Whoa, TIL!
There were already a ton of changes needed to vector for all that to work (bundling, etc), as well as adding primops, extending/benchmarking things to use it, etc. As far as I remember, that was really the vast majority of the work -- and it was a publication (i.e. research), so the natural answer is probably "lack of time". Also upstreaming is always a pain. It's *easier* in a sense to do this with LLVM: you just use the built in vector intrinsics. You don't really do much. It's "just" about using the right types and built-in operations. But this is different from the NCG being "easier to understand and hack" which was my main claim! Just because LLVM makes it easy to use does not mean it is simple. There's a sort of spectrum here where, realistically, when something is only like 1k to, say, 20k lines of code -- it really *is* easier to hack, mold, and use in a sense -- even if something else is "simpler to use", if that "simpler" thing is 10x the size and complexity. You can probably just read literally all the source code in a few days, it's not really an exaggeration. There's ample resources (literally hundreds of papers and tutorials) about the design of these components. So when I say "easier to hack" and "simpler" I'm basically making a claim about the relative "scope" of the two components. LLVM is not needlessly bloated though, just to be clear, and it has advantages: it has much more well defined processor scheduling models, meaning it doesn't *just* generate the code, but tries to generate code that's good for the processor's instruction level parallelism and microprocessor design. LLVM has a lot of knowledge about this (depending on the platform) and GHC does not. So not only will LLVM use the right data types, it'll also select well tuned instruction sequences and patterns. But none of that is really important if the operations are unusable in the first place. I don't think it's a substantial amount of work, probably a week or two, perhaps. There would probably be a couple rough spots. But unless you have free time and effort, it will probably *seem* daunting. Even simple things can drag on for days, days to weeks, when you only have an hour or two a day, and IMO that's psychologically daunting for people. "Work on compiler" is unfortunately not an extremely economically efficient decision for most people...
Optparse-Generic is amazing.
The bindings to [Ziptastic](https://www.getziptastic.com/) use servant: https://hackage.haskell.org/package/ziptastic-client
Developing a full NCG from scratch for a new platform is a major endeavor. The task is even more daunting if every step of your iterative development work must go via cross-compilation. This raises a significant barrier to getting a working Haskell compiler on any new platform. Classic GHC allowed the following bootstrapping process for new platforms: First, you do an "unregisterised build" of GHC via-C on any existing platform, resulting in a working but very slow GHC compiler in portable C. Then you compile that using a C compiler on the target platform. From here on, no more cross-compilation is needed. Next you add some basic hints about the architecture of your new platform and do a "registerised build" of GHC. This results in a GHC which, although still slower than a regular GHC, is already fast enough to be somewhat usable. You can use it for general Haskell programming for systems whose source code isn't too large and where speed isn't too important. And you can use it to work on NCG natively on the target platform, iteratively improving until you get a GHC on the new platform that is truly performant.
Well sure, there are various ways to manage the threads if you need that. But the point is that the original fork from the warp worker thread can't be an `async`, it has to be a `forkIO`, unless you can afford to wait for your thread to finish before you send a response to the browser.
Yes, for some of them. Probably for all of them once dependent types and linear types make it into mainstream Haskell. How about Scala?
But how do you excite a Haskell process?
Can you give an example of such properties that is guaranteed to hold in Haskell? Except the obvious one about pure vs impure.
Well it is /r/haskell, not /r/scala or /r/functionalprogramming.
IME, laziness by default is much less often an actual problem than a potential problem (which is still a real dev time sink). Consider: I'm writing Haskell and I run into a blocking performance problem (e.g. OOM, slowness, etc.); laziness is a potential source of that problem (e.g. via a thunk leak); even if it doesn't turn out to be the cause, that's one more case I had to consider while debugging.
+1 for `servant`
Works fine with Relay.
&gt; You would need to add a lot to the standard, and I'm not sure how much it would really matter since ultimately, it's mostly GHC being used at this point. If it doesn't matter, why is there a standard in the first place?
While I do work on an alternative LLVM backend, it tries mostly to do away with warts in the existing llvm backend (like excessive alias generation). It still takes off from Cmm, and if this will ever end up working, I'm not sure I would invest the time to move it up to STG. However, the mangler is, IMO one of the worst parts and needs to die as quickly as possible. This won't happen in the near future though, as it's going to be extended first. The real solution here is to teach LLVM about the needs GHC has, instead of grafting them onto of assembly that LLVM produces. My biggest issue with the Mangler is that it lowers the produced code from LLVMs IR into assembly. This completely prevents GHC from emitting bitcode. The LLVM backend used to suffer from a rather unstable textual IR, this has interestingly been stable from LLVM 3.9 to LLVM 4 to apparently LLVM 5 as well.
I'd be careful to mention that CNFs probably aren't really safe on their own as a transport/message abstraction? They require the exact same binary because of info table pointers directly in their layout. That's brittle for things like dynamic network services with e.g. rolling upgrades, but more importantly it means untrusted input is very vulnerable if it's fed unsafe data. I suppose in cases like this you're probably looking at systems like Cloud Haskell/HPC scenarios? In a sense most HPC style scenarios probably do things like this anyway (RDMA, etc) so maybe it's par for the course for scenarios like this to just trust their DMZ'd nodes or whatever. I haven't used CNFs at all so I haven't thought about this, but in the end you can probably get pretty far doing something like using CBOR or store to serialize a safer representation for a network. But use the compacted form in-process for all the other benefits. I wonder how much this would cost.
Is this similar to [`vcache`](https://hackage.haskell.org/package/vcache)?
I would love to see what the result of applying the linear types extension to some of the work on session types. [This](http://homepages.inf.ed.ac.uk/slindley/papers/gvhs.pdf) would be a good start, but [this](http://homepages.inf.ed.ac.uk/slindley/papers/fst.pdf) looks particularly exciting.
Thank you for coordinating the release!
Well, authenticate, secure, and handshake versions before you start throwing CNFs around, and it can still be a time save. Connection setup is usually not a significant overhead of well-used distrbuted networks, even when the underlying connections are somewhat flawed.
Exciting = EXCeption exITING
The first principles book is hands down the best resource to try and learn Haskell. If you don't want to "drop out" of the process, it will be your best bet to start with.
with [Ryan Gosling](https://haskellryangosling.tumblr.com/)
I agree
HaskellBook.com does exactly this. Buy it came out after that article.
Yes, I'm super thankful for that, but it's a book, not a course. 
Thanks, everybody! The stockyard algorithm is beautiful. I managed to avoid using it, massaging the problem into Megaparsec.Expr. Here's the code[1], and here it is in action: &gt; parseTest expr "the children #(seem to) behave better ##when they #have eaten #within the last hour" RelX (EO True 2) (RelX (EO True 1) (Leaf "the children") (Joint "seem to") [] -- relationships with arity &gt; 2 use this field; see below (Leaf "behave better")) (Joint "when") [] (RelX (EO True 1) (Leaf "they") (Joint "have") [(Leaf "eaten",Joint "within")] (Leaf "the last hour")) The reason the outer leaves are broken out while (if they exist) the inner leaves are in that list field is because I might extend the model to where the outer members can be optional, as in "#maybe I am a dragon" or "#(for the love of money) we #make faces". For that I would have to modify Megaparsec.Expr -- which I would enjoy, but it should wait. [1] https://github.com/JeffreyBenjaminBrown/digraphs-with-text/blob/master/src/Dwt/Parse.hs
&gt; We know it's easier &amp; ideal to start functional No we don't. We know one or two people have had that experience. I'd love to believe this, but it takes a lot more evidence that alternatives are measurably worse; more than just assertions that someone *thinks* they learned better this way, even though they can't test the alternatives on themselves anymore. Measuring what makes people learn better is a notoriously difficult problem in general, not just programming.
To get the best inference, your best bet is likely to be to add more constraints: merge :: (asbs ~ (as ++ bs) , bs ~ Drop (Length as) asbs , as ~ DropRight (Length bs) asbs) =&gt; table as -&gt; table bs -&gt; table asbs This may not be so good for trying to do higher-order or proofy things, but inference will be great.
It's been a couple years, so I really don't remember a lot. At the time, I was too frantically busy to be able to help.
I have enjoyed using `protolude` as well; I can often swap it into a project, delete a few imports, and have everything working as it did before. That plus compiler flags in the .cabal file like including OverloadedStrings by default helps everything feel cleaner. However, I more often see [classy-prelude](https://hackage.haskell.org/package/classy-prelude) in the wild.
Not quite a question, but I've just learned a ton from scrolling through this thread. Would have loved to have this as a weekly item when I was just getting started. Keep it up!
You could also write it using the pattern where you purely fold updates onto the state. Same pattern used in elm, reflex and elsewhere to consolidate state changes. Something along the lines of `calc mstate ops = foldl (fromMaybe emptyState mstate) ops` where `data Operation = Add Int | Subtract Int | AddLast`. Can be done in a reader monad as well using `local` to "carry state forward". Same fold pattern used in time travelling debugger, which is basically just keeping `state` as list of states, folding onto last.
there is also [elm-export](https://github.com/krisajenkins/elm-export) (which servant-elm is using) Overall using servant with elm is quite nice - IMO just using servant to write a RESTful backend is not too hard for a beginner at least if you don't need authentication if you do I would look for spock or yesod instead
Still forces you to write the entire parser by hand, doesn't it?
&gt; For speeding up type-checking on GHCi, you can give :set -fobject-code. It'll make GHCi only recompile changed files and it will be much faster when you're editing files other than those giant ones. This is a great tip, but do you know if there's any way to stop GHCi from recompiling everything on startup? I have ":set -fobject-code" in my "~/.ghci" file, and while it does cause GHCi to use compilation instead of the interpreter, it still recompiles all modules every time I start it up. (The project I'm working on has over 250 modules, so it takes quite a while.)
First of all, the badness of GHC's default prelude is greatly exaggerated. The vast majority of Haskell code uses the default prelude, and it does just fine. But GHC makes experimentation easy, and Haskell programmers tend to be purists by nature; so there is no shortage of attempts to improve upon it. Common complaints about the standard prelude include: * non-total functions * type classes with insufficient theoretical basis or laws * functions that are not as polymorphic as they could be * too little functionality in scope by default If none of these things have ever bothered you, then there's no need to switch to a custom prelude. If they have, then the choice depends on which complaints you have. As someone else said, `classy-prelude` is probably the most popular, and it's focused on removing partial functions and achieving maximum polymorphism.
That's fine. Most of the parameters can remain as parameters. The State monad substitutes for the state parameter, and the updated state return value. So if you previously had `movePlayerA :: GameState -&gt; Key -&gt; Weather -&gt; (GameState, Bool)`, that would change to `Key -&gt; Weather -&gt; State GameState Bool`. The extra parameters and extra return values remain.
I assume the claimed "consensus" (though it's not that, really) is that type classes without laws are useless, and the purported reason is that without laws, there are no guarantees about the meaning of code written using the type class, and therefore no way to be sure it's correct. It's absolutely true that equational reasoning depends on type class laws. Of course, it's quite possible to write code that behaves as you intend without laws; it's just far more difficult to *prove* that it does so. (In particular, the proofs are not compositional, since they require that you find all instances of the type class in your program, and proceed by case analysis. And, therefore, they must fail unless you have all the code, since Haskell's type classes are open, and new instances may be written anywhere.) The optimistic solution to decide what the laws should be. Presumably, if you bothered to write the code, you must have *some* expectation of how it should behave. In principle, you ought to be able to document this, and often can do so with equational laws attached to the class. Whether that's a good use of your time is a different question, but the presumption of much of the Haskell community is that it is likely to be.
Why? 
No idea.
Not really related to the specific issue: but you shouldn't just assume a ByteString is ASCII unless you actually know where it came from. It could just as easily be a JPEG image or EBCDIC-37.
This blog post is by one of the authors of that book :).
The traditional pattern for passing around state is to keep it in a function parameter and have the function call itself with different parameters. Example: main = loop "" where loop old = do putStr "&gt; " c &lt;- getLine let new = case c of "h" -&gt; old _ -&gt; c print $ process new loop new Here, the `loop` function carries the last command around in a parameter. It's not possible to use `forever` with this pattern, so some high-levelness is lost, but this is the first thing one should reach for when dealing with state. Often, this method does not feel like one is dealing with state anymore, since the parameter feels natural somehow (e.g. in the definition of `foldl`). 
Here they tried to teach Haskell to freshmen and the results were promising: https://www.researchgate.net/publication/220676568_The_risks_and_benefits_of_teaching_purely_functional_programming_in_first_year
Yes, you are right! My bad.
Yes, like the other comment said, I actually described multisets. Sorry for that. What is nice, is that in your expanded binomial, you subtract exactly the multisets (for example ( -x + x^2 )/2 is two elements where the order does not matter and removing the possibility of having duplicates). Thanks for writing this up! Maybe the fractions show up, because it's actually unclear whether the hole is equal to any of the elements in the set? Like a uncertainty? (EDIT: formatting)
Clarification - the linked announcement states: &gt; we do not have Win32 builds in this release That sounds like 8.2.1 does not support Windows at all, because every Windows build of GHC is built on the Win32 library, including 64-bit builds. But that is not the case; only 32-bit Windows is not supported, as is made more clear on the [platform home page](https://www.haskell.org/platform/): &gt; The 8.2.1 platform release is for 64 bit Windows only, due to issues with GHC 8.2.1 on 32 bit Windows. Future releases are anticipated to resume 32 bit support.
We tried it once at our University. One of our functional programming profs put it together. If I recall correctly we lost all but two of an entire class of sixty students before the end of the course. We won't be doing that again anytime soon.
I do want authentication though. I've read a bit about yesod but it seems a bit overwhelming. Spock might be a better choice?
The records in the extensible package are represented using a HList, and thus the performance for reading record fields is O(n) not O(1). But it's a great package anyhow :-)
&gt; We tried it once at our University. One of our functional programming profs put it together. If I recall correctly we lost all but two of an entire class of sixty students before the end of the course. Thanks for sharing your experience. Do you have any info that you can share regarding why the students ultimately decided to leave the course? One of the first-year courses for the Computer Science undergrad curriculum at the University of Cambridge (UK) is entirely ML-based. Based on my experience during that year, it seemed to be a popular (and fun) course. From what I can gather (by talking to students that attended in the years after me) it still seems to be popular: https://www.cl.cam.ac.uk/teaching/1617/FoundsCS/ To get an idea of the difficulty, here is a selection of past exam questions relating to the course: http://www.cl.cam.ac.uk/teaching/exams/pastpapers/t-FoundationsofComputerScience.html
If you just wanted to make lenses linear you could do this with some slight api changes. For instance, you could only `get` with isos and split stuff into `s ⊸ (a, b ⊸ t)` otherwise. That's because s contains more data than just a if it isn't an iso so we store the rest in the `b ⊸ t` closure. Sorry, this example got a bit out of hand: -- technically this shouldn't typecheck since (+) isn't linear yet but a bug helps us out test :: Num a =&gt; [a] ⊸ [a] test = over each (+1) type ASetter s t a b = (a ⊸ Identity b) ⊸ s ⊸ Identity t type Traversal s t a b = forall f. LApplicative f =&gt; (a ⊸ f b) -&gt; s ⊸ f t type Lens s t a b = forall f. LFunctor f =&gt; (a ⊸ f b) ⊸ s ⊸ f t each :: Traversal [a] [b] a b each f (x:xs) = (:) &lt;$&gt; f x &lt;*&gt; each f xs each f [] = pure [] fst_ :: Lens (a, v) (b, v) a b fst_ f (x, y) = fmapl (\x' -&gt; (x', y)) (f x) over :: ASetter s t a b ⊸ (a ⊸ b) ⊸ s ⊸ t over l f s = getIdentity (l (\a -&gt; Identity (f a)) s) split :: Lens s t a b ⊸ s ⊸ Store a b t split l s = l sell s class LFunctor f where fmapl :: (a ⊸ b) ⊸ f a ⊸ f b class LFunctor f =&gt; LApplicative f where pure :: a ⊸ f a (&lt;*&gt;) :: f (a ⊸ b) ⊸ (f a) ⊸ f b (&lt;$&gt;) :: LFunctor f =&gt; (a ⊸ b) ⊸ f a ⊸ f b (&lt;$&gt;) = fmapl newtype Identity a = Identity a deriving Show instance LFunctor Identity where fmapl f (Identity a) = Identity (f a) instance LApplicative Identity where pure a = Identity a Identity f &lt;*&gt; Identity a = Identity (f a) getIdentity :: Identity a ⊸ a getIdentity (Identity a) = a data Store a b t = Store a (b ⊸ t) instance LFunctor (Store a b) where fmapl f (Store a e) = Store a (\a' -&gt; f (e a')) instance (MonoidL m, Comonoid n) =&gt; LApplicative (Store m n) where pure a = Store memptyl (\n -&gt; withUnit (del n) a) (&lt;*&gt;) :: forall a b. Store m n (a ⊸ b) ⊸ Store m n a ⊸ Store m n b Store x l &lt;*&gt; Store y r = Store (mappendl x y) (\b -&gt; inner (dup b)) where inner :: (n, n) ⊸ b inner (n1, n2) = l n1 (r n2) class Comonoid a where del :: a ⊸ () dup :: a ⊸ (a, a) class MonoidL m where memptyl :: m mappendl :: m ⊸ m ⊸ m sell :: a ⊸ Store a b b sell a = Store a (\x -&gt; x) However this doesn't unify with the current lenses so you would have to duplicate all operators which isn't really viable. To abstract over that we would need multiplicity polymorphism which isn't implemented yet. We could try to emulate it but types become a bit gross: data Multiplicity = One | Omega -- this would be better but a bug prevents it from compiling for now: -- type family Arr (p :: Multiplicity) a b = result | result -&gt; p a b where type family Arr (p :: Multiplicity) a b = result | result -&gt; a b where Arr 'One a b = a ⊸ b Arr 'Omega a b = a -&gt; b class LFunctor f where type MemberCount f :: Multiplicity type MemberMult f :: Multiplicity fmapl :: Arr (MemberCount f) (Arr (MemberMult f) a b) (f a ⊸ f b) If we add a third associated type we could make LFunctor instances for normal functors, not sure how useful that would be. 
I'm not certain, but I _think_ u/how_gauche is pointing out that you needn't use a fork of any form in the worker threads. You can instead do something like: workQueue &lt;- atomically newTQueue race (forever $ join $ atomically $ readTQueue workQueue) $ run 3000 $ \req send -&gt; do atomically $ writeTQueue workQueue $ putStrLn "Hello World!" send $ responseLBS status200 [] "I queued some work" Then you can do fancier things as well, such as using a bounded queue, having multiple worker threads grabbing values off the queue, and so on. This ensures that exceptions will be handled, _may_ be more efficient by avoiding thread forking overhead\*, and leverages just the `concurrently`/`race` API. \* It may also be slower due to STM overhead, you'd have to benchmark it
What are the use cases for the `IxX` classes? From the class functions it seems a mix between functors and categories, but I can't imagine examples of it.
unless something terrible happened, the first programming course at imperial college is in Haskell. many people have never programmed before that course (i had maybe 5k lines of horrible PHP and that's it) and the course is quite well liked. i'm personally grateful that i learnt to program seriously in haskell rather than something else.
Regardless of whether its better or not it doesn't make sense to teach a more rare/obscure language as someone's first language. That's why they teach *Java / C / Python* etc in universities. Your first language is often the one you are most familiar with and most likely your first career.
They should teach Assembler first. Then move up the Abstraction Ladder. I don't believe teaching Haskell first is a good idea.
Conor's [*Functional Pearl*: Kleisli arrows of outrageous fortune](https://personal.cis.strath.ac.uk/conor.mcbride/Kleisli.pdf) delves deeper into them, gives some uses but I haven't used them much. I started this from the observation that replacing the `Codensity` constructor with a right Kan extension gave this interesting type signature (&lt;*&gt;) :: Codensity m (a -&gt; b) -&gt; Codensity m a -&gt; Codensity m b Codensity f &lt;*&gt; Codensity g = Codensity (\bfr -&gt; f (\ab -&gt; g (bfr . ab))) app :: Ran j i (a -&gt; b) -&gt; Ran k j a -&gt; Ran k i b Ran f `app` Ran g = Ran (\bfr -&gt; f (\ab -&gt; g (bfr . ab))) which looks tantalisingly similar to [`IxApplicative`](https://hackage.haskell.org/package/indexed-0.1.3/docs/Control-Monad-Indexed.html#t:IxApplicative) class IxPointed m =&gt; IxApplicative m where iap :: m i j (a -&gt; b) -&gt; m j k a -&gt; m i k b So it seems like right (and left) Kan extensions are some kind of swapped indexed structure, is that interesting or useful? You tell me /r/haskell! A cool application of this is letting us write `Codensity` and `Density` as newtypes and deriving all sorts of instances for them
can confirm it's still in haskell, and well liked
Great work.
Here's an issue at hackage-diff that would allow tooling to suggest a new version: https://github.com/blitzcode/hackage-diff/issues/8
[Here](https://gist.github.com/gelisam/9845116) is an example I wrote a while ago. Basically, the `i` and `j` indices can be used to encode the current state (as in "state diagram", not as in "State monad"), an action of type `forall i. M i i Int` doesn't change the state and can be run in any state, an action of type `M Active Inactive ()` has the side-effect of transitioning from state `Active` to state `Inactive`, and an action of type `M Active Active Int` can only be executed when the state is Active. You can use this to guarantee, e.g. that an action which reads from some global file can only be executed between an action which opens the file and an action which closes it, or that an action which modifies a piece of shared mutable state can only be executed while some global lock is acquired.
SICP was my intro to CS and I turned out fine. That used to be pretty common before some mix of Java / Python took over.
That's a good tip. Thanks.
Cool stuff! Somewhat reminds me of Visser's "A Generic Approach to Datatype Persistency in Haskell", and [Baardskeerder](https://github.com/Incubaid/baardskeerder) (disclaimer: one of the authors). Also: groeten uit Gent ;-)
Is the swapping essential? With a regular Applicative, the fact that the left argument is the function is not super important, if you want to execute the effects of the `f a` before those of the `f (a -&gt; b)` you can write rap :: Applicative f =&gt; f a -&gt; f (a -&gt; b) -&gt; f b rap fx ff = (&amp;) &lt;$&gt; fx &lt;*&gt; ff Using this idea, I can turn your IsApplicativeSwap instance into an IxApplicative: instance IxApplicative Ran where iap :: Ran i j (a -&gt; b) -&gt; Ran j k a -&gt; Ran i k b iap ranF ranX = ((&amp;) `imap` ranX) `iapSwap` ranF 
Congratulations!
Is there a good reason the type arguments of `Ran` and `Lan` are in this order?
Okay, I've seen "Codensity" and "Kan extension" enough times, it's time to roll up my sleeves and finally figure out what they mean! Let's start with something similar but more familiar: newtype ContT r m a = ContT { runContT :: (a -&gt; m r) -&gt; m r } The obvious, non-crazy implementation of such a type is that the ContT is holding an `m a`, to which it applies the continuation via `(&gt;&gt;=)`. But we know that we can also do crazier things than that, and that this is what makes ContT interesting! We can ignore the continuation, short-circuiting the computation by returning our own `r`. We can call the continuation, and then perform some extra `m` actions, and maybe return a different `r`. We can call the continuation multiple times with different `a`s until we're happy with the `r` it returns, or with some side-effect-based condition obtained via `m` actions. Crazy stuff, and that's with a single continuation! instance Monad m =&gt; Monad (ContT r m) where return x = ContT $ \cc -&gt; cc x step1 &gt;&gt;= f = ContT $ \cc -&gt; do runContT step1 $ \x -&gt; do runContT (f x) $ \y -&gt; do cc y If we write a program made of ContT actions, the non-crazy steps compute the `a` needed by the next step, while the crazier steps manipulate a continuation called the "current continuation". This continuation consists of all the later steps, plus the final continuation given to `runContT`, typically `return`. We can now do things like capturing the current continuation in order to take a snapshot of the current execution stack, and call this continuation later on when we want to backtrack. import Data.Void captureCurrentContT :: Monad m =&gt; ContT r m (Either a (a -&gt; ContT r m Void)) captureCurrentContT = ContT $ \cc -&gt; do cc $ Right $ \x -&gt; ContT $ \_ -&gt; do cc (Left x) ***** Codensity is a lot less familiar to me, but its definition looks suspiciously familiar to ContT's. The only difference is that the `r` is universally quantified. newtype Codensity m a = Codensity { runCodensity :: forall r. (a -&gt; m r) -&gt; m r } The obvious implementation of such a type is again that Codensity is holding an `m a`, to which it applies the continuation via `(&gt;&gt;=)`. In my previous attempts to understand Codensity, I stopped there, thinking that this was an overly-complicated way to do something simple, and that I preferred the simpler way of using an `m a` directly instead. But now that I have the ContT to compare with, I realize that [I was a fool](https://ro-che.info/ccc/12)! It's the crazier, non-obvious implementations which must make this representation interesting. So, let's see, what crazy things can we do now? We can't shortcut the computation because we don't know which `r` will be requested. We can call the continuation and then perform some extra `m` actions, but again, we can't return a different `r` because we don't know what `r` is yet. Finally, we can call the continuation multiple times with different `a`s until we're happy with with some side-effect-based condition obtained via `m` actions. instance Monad m =&gt; Monad (Codensity m) where return x = Codensity $ \cc -&gt; cc x step1 &gt;&gt;= f = Codensity $ \cc -&gt; do runCodensity step1 $ \x -&gt; do runCodensity (f x) $ \y -&gt; do cc y The Monad instance for Codensity is exactly the same as ContT's, so once again, if we have a program made of Codensity actions, each of them has access to a "current continuation" consisting of all later steps plus the final continuation given to `runCodensity`, typically `return`. Since one of the things we can do with this current continuation is to add some extra `m` actions to the end, I now understand this [recent tweet](https://twitter.com/ProgrammerDude/status/899961144718348288): &gt; Why would you ever use kan extensions? To keep track of resources and automatically close them of course! &gt; &gt; managedActions :: Codensity IO () &gt; managedActions = do &gt; input &lt;- Codensity $ withFile "in.txt" ReadMode &gt; output &lt;- Codensity $ withFile "out.txt" WriteMode &gt; contents &lt;- liftIO $ hGetContents input &gt; liftIO $ hPutStr output contents `withFile` receives a continuation and wraps it between calls which open and close the file (plus some `bracket` magic). So the open call occurs immediately, then the continuation is ran with the file handle, then once the entire computation completes, the file is closed. The "entire computation" includes another call to `withFile`, so that second file will be closed first; resources are closed in the opposite order in which they were acquired, like in C++. Clearly, it's not Codensity which automatically closes the file, it's `withFile`. What Codensity provides is an alternative API for `withFile`, in which we don't need to provide a block delimiting the region during which the file should be kept open, and instead the file is closed at the very end of the Codensity computation. ***** All right, let's look at Ran next! Its definition looks suspiciously familiar to Codensity's, the only difference is that the `m` in the continuation is not the same as the `n` we must return using it. newtype Ran m n a = Ran { runRan :: forall r. (a -&gt; m r) -&gt; n r } This time I can't see an obvious implementation for the non-crazy case, but this discussion about IxMonad clues me in: the non-crazy case is the one which doesn't change the indices, so it's the same as Codensity. That is, `Ran m m a` holds an `m a`, it receive a continuation with effects of type `m`, which it applies using `(&gt;&gt;=)`. Once again, it's the crazy implementations which are interesting. We can do everything Codensity could do with its continuation, in which case the action will have type `Ran m m a`. But we can also do more crazy things which Codensity couldn't, by changing the type of the underlying monad. For example, we could run the rest of the computation inside an `atomically` block, in which case the rest of the code would need to be in the STM monad instead of IO: import Control.Concurrent.STM switchToSTM :: Ran STM IO () switchToSTM = Ran $ \cc -&gt; do atomically $ do cc () After looking at that example, I find it annoying that the indices are written in that order. The operation which transitions from IO to STM has type `Ran STM IO ()` instead of `Ran IO STM ()`! I think the indices are in the wrong order. Does anybody prefer the current order? import Control.Monad.Indexed newtype Ran' m n a = Ran' { runRan' :: forall r. (a -&gt; n r) -&gt; m r } instance IxMonad Ran' where ibind f step1 = Ran' $ \cc -&gt; do runRan' step1 $ \x -&gt; do runRan' (f x) $ \y -&gt; do cc y The IxMonad instance for Ran' (but not the one for Ran) is exactly the same as the Monad instance for ContT and Codensity, so once again, if we have a program made of Ran actions, each of them has access to a "current continuation" consisting of all later steps plus the final continuation given to `runRan`, typically `return`. As the computation progresses, the underlying monad changes, and once the computation is complete, the underlying monad rolls back to what it was before as each cleanup action completes. As with Codensity and automatically-closing resources, it's not Ran itself which changes the underlying monad, it's functions like `atomically`. What Ran provides is a different API for functions of that form: instead of specifying a block of code within which the monad is now STM instead of IO, the monad is permanently changed to STM, and the block of STM code is automatically closed at the end of the computation. 
I haven't personally used `vcache`, but it seems to me `vcache` is a layer above LMDB, providing nearly-transparant persistent memory for Haskell applications. We do not provide such a layer, and we are more comparable to LMDB itself. Haskey could replace the LMDB layer in `vcache`.
&gt; Your first language is often the one you are most familiar with and most likely your first career. Nah. Hardly anyone I've worked with in the past decade or so had originally started with the language we were using. My first language was Pascal, followed by C, but I have never used either professionally really. Colleagues had come from C, Perl, JavaScript, Delphi (!), BASIC, and the odd Java, but with the exception of JavaScript, none of these was among the working set required for the job. The truth is that your first programming language doesn't matter, unless it's the only language you'll ever learn. Teaching someone a language that will land them their first job can actually be a bad thing, because it makes them less likely to explore other languages.
That makes sense, thank you for providing a comparison.
Point missed entirely. The idea, which you are of course free to disagree with on it's own merit, is that functional programming teaches reasoning skills of the sort that are more fundamental to producing quality code. Whether or not programmers need to learn fundamental reasoning skills in a compsci course, is, I hope, not really up for debate, and likewise, I would hope that learning reasoning is more important than your choice of language.
This is a valid concept, but personally I think learning how to build a lisp compiler is probably better in practice. Assembly is fundamental, which is good, but you can learn what you need to know about principle without being fully literate, which seems like kind of a waste unless that's the area you're interested in studying
I don't really know. I am more in the industry side of the field, and I don't really understand the benefit to standardizing a language. (I do understand the benefit to standardizing other things, like instruction sets). There are probably others who understand the value of this more than I do.
I made a [ticket](https://github.com/ekmett/kan-extensions/issues/41) just in case, this is the order used in mathematics but apart from that I don't think there is an obvious reason for the current ordering
Precis's author here... For some reason it never worked well and I found using a graphical diff that displays folders like DiffMerge a better experience (even though diff knows nothing about Haskell source). I expect it is quite bit-rotted. 
This is a bit stream-of-consciousness: 1) Choice of language: I teach at a community college in California. We have CIT (Computer &amp; Information Technology), which is Career/Technical Education and CS (Computer Science), which is classified as Academic. The languages we teach are C++, Java, and Python. Why? Because, especially in CIT, that’s where the jobs are. (cf. [Willie Sutton](http://www.snopes.com/quotes/sutton.asp) ). Perhaps similarly, [Stanford is using JavaScript in an introductory course](http://www.stanforddaily.com/2017/02/28/cs-department-updates-introductory-courses/). 2) Focus: For me, the most important quote from the article is &gt; many of these materials try to teach functional programming concepts by analogy to imperative language features This is spot on. I have never seen an imperative language book that teaches while loops in terms of “this is a short way of writing the following GOTO sequence.” You have to go all in and write as if the functional approach is the only thing out there. 3) Teaching materials: I'm wearing my CIT hat here. Too many books that I have seen get enamored of the theory, and I still feel as if I need a PhD in category theory to do anything useful with Haskell. [Real World Haskell](http://book.realworldhaskell.org/) is a refreshing change from that approach. I write programs to *get stuff done*, and you shouldn’t forget that in a first course. Also, I very much like the approach used in the materials from [Bootstrap](http://www.bootstrapworld.org/), though they are not at a university/college level.
Counterpoint: since you probably have to learn Java or something eventually, an intro course is an ideal time to learn a different language. Learn a language that expands your understanding of programming as a whole, then use the common industry language for other courses. I learned Ada in as my intro language and I think it was one of the best parts of the curriculum. I did almost 100% of my coursework afterwards in Java. Graduated as a competent Java programmer, but was also aware of a lot of other concepts outside of Java thanks to the Ada course.
you forgot machine language. well transistors and logic gates. Actually better start with quantum mechanics...
I think Haskell is a good language for the working programmer. It's fairly big and offers a lot of stuff to make it easy to get things done. However I think a first time learner needs a more minimal and specialized language such as [BSL](http://www.ccs.neu.edu/home/matthias/HtDP2e/i1-2.html) and an ecosystem that supports teaching.
Or a warning
I know you are being facetious, but the *most* of the programming in the real world is actually done in a very highly imperative manner, and despite what people say, efficiency matters. The people of my generation learnt Assembler in university, and it has affected the way we code even in very high level languages. Jump in without understanding how a computer works, and you have abominations like the Atom editor.
Why not listenBrainz :: ListenBrainzAPICall a -&gt; m a 
&gt; The truth is that your first programming language doesn't matter, unless it's the only language you'll ever learn. The university I went to strongly suggested many other EMS majors to take the first intro to programming class. My old man who graduated in the early 80s was required to take a BASIC course to graduate. For many people it will be the only language they ever learn, might as well make it a language they could potentially find a job in.
See my other reply, same reason - I want at least something I can reason about.
For what it's worth, first year first semester at my uni (of Edinburgh) taught Haskell, both as a way to get it in your head that you're studying CS not software engineering, and also to somewhat level the playing field between those that had programmed before and those that had not
Assembler is a bit of a lie, though, and is only going to become a bigger and bigger lie as we move on. Does anyone really think computers behave in a way that even vaguely resembles a Von-Neumann architecture anymore? Between CPUs, GPUs, embedded GPU on a CPU, 3-5+ levels of memory access, out of order execution, vectorization, multi-cores, distributed computing, parallel computing, ... Assembly is just a lie. It gives you a cute, yet incorrect, mental model of a computer that's only really useful for programming in C (which behaves as if that lie is true). If you're not programming in imperative C, learning that lie isn't really going to help. Lambda calculus, on the other hand, is not a lie because it never pretends to be a real computer. Most of the programming in the real world involves an assumption that people want invariants and consistent behavior. Why not model that with laws and equational reasoning? What's the necessity of believing in the lie that computers behave like turing machines or have a Von-Neumann architecture? At the same time, one could argue that LC/Haskell is a lie because you don't evaluate on a LC-reducing computer, you evaluate on 'real hardware' and a real machine which tends to be abstracted out of concern in functional languages. That being said, I don't see either lie as being worse than the other. If anything, the LC lie is more truthful because it never pretends to be accurate with respect to the machine you run the language on.
I wouldn't go that far, but I do think people should have a better understanding of how computers actually operate when they start learning to program. Otherwise, it's technical sorcery - know the right invocation and *magic* happens. Not to say that everyone (or even anyone) should have full knowledge of every single layer of technology down to the circuit level, but I think students should at least have an idea of what happens when a variable is declared, a function is called, etc.
I just invited you to collaborate on the library if you're interested :)
Well Robert harper is at CMU and there is a Phd program called "pure and applied logic" which i deem as the most theoretical one with strong emphasis on Homotopy type theory . Also there are some summer schools about that,OPLSS maybe the most well-known one and i just come back from it,people there are extremely friendly and i have got lots of help. http://logic.cmu.edu/ https://www.cs.uoregon.edu/research/summerschool/summer17/
Sorry for the triple post; accidentally submitted the previous un-updated version instead :)
Many UK universities teach Haskell as a first language.
I use optparse-applicative, what's the advantage of Generic?
Glad you like it! Groetjes terug! ;-)
test
How come?
The library is [able to generate the parser automatically](https://hackage.haskell.org/package/optparse-generic-1.2.2/docs/Options-Generic.html) from the datatype declaration, using higher-kinded types from `Data.Monoid` and type operators. It saves quite a bit of typing! But perhaps only works for simpler optparsing.
By the way, I love this a lot.
Just as more data, I pretty much learned Haskell "first" (I had played around with programming before, but I had no CS knowledge; I didn't even know that data structures other than arrays existed). I did have a pretty good handle on variables and functions, though, from my primary/secondary school math education, and I had taught myself some calculus so I was familiar with variable binding. On that basis I'd say that most undergraduate freshmen probably _could_ be learning Haskell, as long as they've properly internalized the usual K-12 math.
Really nice blog post! It's interesting and exciting to read this. Also it's quite understandable (well, if you know «some basics»). I really want to use something similar in my projects. The only issues I had before with such more type-safe approaches is that's it's hard sometimes to move from static values to runtime values and vice versa. Sometimes you want to have a list of type-safe `Vector` and you ended up using `HVect` which is not convenient to use... Sometimes you want to have `Vector` as a field of some complex data type but you don't want to put `n` — size of `Vector` — into types. Sometimes you want to serialize/deserialize such vectors. It's just not much people using such patterns at the moment. Best practices are not studied well enough. I saw some post from WellTyped about serializing GADT but that's it. I understand that you should pay some cost for more compile-time guarantees. So I'm just looking forward to improvements of similar approaches! :) Maybe Dependent Haskell will help a lot.
Thank you! For the case of an unsized vector, I do try to explain the post a way to move back and forth between sized and unsized vectors, and that they can be converted between each other in a (hopefully) straightforward way. For the case of serialization, I actually have that as a concrete example in my [dependently typed haskell][serial] post, if you want to look at that for a reference :) [serial]: https://blog.jle.im/entry/practical-dependent-types-in-haskell-2.html
I use [type-combinators][] for this. [type-combinators]: http://hackage.haskell.org/package/type-combinators
How on earth can a lazy language be truly fast on the JVM? Don't you need your runtime to know and operate on graph reduction directly?
Cooool. I'm guessing I usually need the more flexible option, but I'll keep this in mind, thanks!
If you don't have the mental capacities to learn additional languages, you're not going to be a programmer either.
Please also checkout [Spock](https://spock.li) :-)
Fair warning to all: I've spent a total of like 1 weekend 3 years ago - and a couple days writing this - learning how to use Coq. So if the code's a mess, it's because I don't actually know what I'm doing here. Fun stuff, though. If you have a bad video game habit, try proving a few theorems - they're just as addictive.
Hello, I'm using Scotty for a backend in Haskell. The other option I considered is Servant, which uses more type level programming features. I've seen both frameworks used in production Haskell environments, and tend to go with the more simple conceptually solution, which in my opinion is Scotty, to build the API.
Is there an existing data type like so: data MyProxy k (t :: k) This is subtly different to Data.Proxy in that it has the kind explicitly as a parameter. I could easily make it myself but if it already exists I figure I might as well use it.
Gratuitous references to porn like this should not be tolerated in our community. It contributes to the creation of an atmosphere where many people will not feel welcome.
Not really, I got back to R programming language and STAN all the time. I would use Haskell 1000% of the time if 1) 1000% of my time was a figure that made sense and 2) I was able to do the super messy task of exploratory data analysis and subsequent predictive modeling in Haskell. Yes, there are things like Frames, and HaskellR, but its hard to justify using these in a work setting. Check out DataHaskell, for a working group of folks trying to improve the state of data science in Haskell! I don't thinking Haskell is a well kept secret, I just think it takes a big investment to become productive.
[@ProgrammerDude's latest tweet](http://i.imgur.com/VVSPb0W.jpg) [@ProgrammerDude on Twitter](https://twitter.com/ProgrammerDude) - ^i ^am ^a ^bot ^| ^[feedback](https://www.reddit.com/message/compose/?to=twinkiac)
I have read the post diagonally and I don't see any references to porn except for the title and first paragraph. Maybe I missed something but it doesn't look like there is anything in there that would make anyone feel uncomfortable. The author could very well say it's about cat pictures and it would not make a difference.
Note that by doing so, you will break the monad laws relating their operation to the Applicative. This may or may not pain you, but it is worth knowing. This is why the `Validation` and `Concurrently` types do not have a `Monad` instance and you need to switch to `Either` and `Async` to `(&gt;&gt;=)`.
You must find the internet incredibly frustrating.
&gt; I have read the post diagonally and I don't see any references to porn except for the title and first paragraph. Yes, as I said, it's entirely gratuitous. That only highlights the fact that *it shouldn't be in there*. &gt; Maybe I missed something but it doesn't look like there is anything in there that would make anyone feel uncomfortable. That's not for you to decide. If stuff like this doesn't get nipped in the bud that's how we'd end up with [sexually objectifying presentations](http://geekfeminism.wikia.com/wiki/Sexualized_presentation). 
&gt; Maybe I missed something but it doesn't look like there is anything in there that would make anyone feel uncomfortable. Look again: &gt; It’s an unspoken truth that most large image databases people have are actually porn collections, so we’ll be honest with ourselves and call it a porn browser. I think it would be difficult to find a more clear example of exclusionary brogramming jargon somewhere else on the Internet even if you tried, this has no place in the Haskell community.
&gt; We’re going to make an image browser. It’s an unspoken truth that most large image databases people have are actually porn collections, so we’ll be honest with ourselves and call it a porn browser. Cheekiness aside (ha ha), I'm going to lean towards /u/sacundim's attitude here and argue that you try keeping it classy. Haskell and sexuality are two worlds I really try to keep separate. (If anyone wants to come forward with their latent lens fetish, that's your prerogative.) A clickbaity title and introduction like these only serve to detract from the content you're trying to share.
Obviously, since women and teenagers old enough to code don't look at porn. Terrible of this guy to use a couple of click bait words in this topic.And besides, everyone streams porn these days, who has a local database, anyway? /s
&gt; latent lens fetish Monads and gonads?
On the one hand, I can see why this comment has been downvoted: this is Reddit and people have the freedom to post what they want to post. On the other hand... like... I don't even know, you know? For me, it's not that "this should not be tolerated in our community." It's... I mean. Look: * There are a million jokes that could have been made but instead the author introduces the subject of porn as a novelty and then does nothing with it. The missed opportunities. * Gross. Haskell is not sexy fun time. Haskell is brainy cool fun time. Don't engage sexy fun time when we're discussing theorems. "But I'm tired of hearing about Free Monads," you say. "Fine. We will move on to natural transformations," I concede. * In all seriousness, I was hoping we could have just responded with awkward silence but you got downvoted, so now I want to come forward and agree with you. It's honestly just weird, you know? Come on, Michael. MY LARGE COLLECTION OF IMAGES CONTAINS MEMES! MEMES, NOT PORN. JUST GO WITH MEMES NEXT TIME.
&gt; On the other hand... like... I don't even know, you know? Yes, it's a mystery! Men are otherwise always so good at handling being called out for toxic masculinity, remember for example how Sarkeesian's critique in the male-dominated gaming community was welcomed with open arms.
I was only partially being facetious. Efficiency does matter, but what kind of efficiency depends on the project. You have programmer efficiency: reducing developer cost and time; security efficiency: having programs that run (almost) bugfree and therefore improve user productivity; machine efficiency: reducing running time and space of a program. In the end it is usually a balance of those three, depending on the project. Usually the most speedup can be achieved by choosing the right algorithms. Only when that is done, assembly should be considered. Spending a few hours to write non-portable assembly for a function that improves the running time from 1ms to 0.3ms, only to be swamped by disk access, isn't going to make any boss happy. On the other hand, improving the inner loop of a high-perfomance numerical library that's going to be used by thousands of people, does make a lot of sense. There is rarely (say less than 0.1%) a need for knowledge about assembly, but it does come handy for those times. I'd say knowing assembly and optimization is a good bonus, as it can help you understanding the output of compilers, and give you that extra speed when you need it.
You know I agree that it's gratuitous, unnecessary, and -- worse -- neither funny or original, but the stance that it *should not be tolerated* as opposed to *should be discouraged* is a goose-step too far towards exactly the sort of hyperbolic, conversation pre-empting, infantilizing, false-empathogarchy the straw-manning (straw-personing?) of which effectively elected our not so benevolent orange antichrist. It's an insensitive title for the minority that find porn truly destructive, it's at best a stupid title for the majority who find it possibly the easiest possible access point to understanding just how incredibly varied is the panopticon of human sexuality... it's enough to point out the inherent insensitivity, it's not necessary to proscribe it as anathema.
What do you think the word "gratuitous" means? I call strawman. Anyway, whatever merit you might find to your argument (that I won't respond to), *this is not the venue for it*.
I'm not trying to be rude or anything, but if you want the average person to actually read and understand your posts, you might want to cut down on the highbrow words. "Empathogarchy" isn't even a word.
I opened a new thread for this week, you probably best post your question there. 
The age limit is on the coding, and not the porn watching? :D
No, it's not, it's the prerogative of humans to invent new words. Like all of these ones, and the ones before them. In this case it's a portmanteau of an extremely obvious prefix and suffix, and the meaning is patently obvious to anyone who knows both.
It means "uncalled for" or "given free of charge", depending on context. In neither case are you using the term strawman correctly. And refusing to respond to an argument you question the merit of -- which was itself a critique of a statement which was lacking merit -- isn't a particularly valid response in a forum that you've already assumed was appropriate to this discussion by voicing your opinion.
Sure, and that's fine -- What I'm saying is just that if you want it to be accessible to as many people as possible, you should avoid it. If you don't care about that, that's alright.
wtf I love Coq now
If I wanted my thoughts to be accessible to as many people as possible I'd be expressing them in Mandarin, Cantonese, and Urdu. I used the term advisedly; these sort of blanket prohibitions on freedom of thought -- as opposed to blanket expressions of horror and shame at expressions of unacceptable thought -- are exactly what allows the "alt-right" to co-opt the terms "social justice warrior" and "anti-fascist" to *somehow* be pejoratives. Those I oppose manipulate the language to point out the flaws in the thinking of those I agree with, I'll do the same. Also, this is a forum for Haskell users. Average hasn't got anything to do with my audience.
[removed]
&gt; Average hasn't got anything to do with my audience. No, but there *are* probably lots of people here for whom English isn't their first language. But that might not apply to sacundim, of course, so if you wanted to address your comment only to him, feel free to ignore my advice.
I found the typeclass documentation for Coq to be very unhelpful, and had a lot of help from the IRC #coq channel to write this proof. This features typeclasses hierarchies, and proofs for transformers. I could only find single-typeclass proofs for simple monads on the internet. Hopefully this will help another Haskeller getting started proving his code is correct.
It doesn't appear to apply to anyone who is policing the use of OP's use of -- non-witty -- wordplay in English. But yes, absolutely, my response is directly at one user's argument, it's not for general consumption. I appreciate what you're saying, but I hit "reply" to put it in that context. 
I'd like to bring back the context of the whole thread - as a first language for teaching CS students.
Thanks for reference! I couldn't imagine that blog post about Neural networks would contain info about serialization :) Neural networks are not my area of interest. But I will definitely read this post!
I have been playing around with linear types and what they could be used for for the last couple days and got stuck at two things. First the linear bindIOL signature from the paper is bindIOL :: IO p a -o Arr p a (IO q b) -o (IO q b) but exceptions break the linearity guaranty on the continuation, right? I guess we could do something codensity-like to ensure resources are always freed but I don't know what this would do to runtime overheads. -- no need for the higher rank type but it clears up a bit of clutter I guess data IOCodensity a = IOC (forall b. (a -o IO b) -o IO b) runIOCodensity :: IOCodensity (Unrestricted a) -o IO a runIOCodensity (IOC f) = f $ \(Unrestricted a) -&gt; return a) bracketIOC :: IO a -o (a -&gt; IO Bool) -&gt; (a -&gt; IO ()) -&gt; IOCodensity a bracketIOC alloc isFreed free = IOC $ \inner -&gt; mask $ \restore -&gt; do alloc Prelude.&gt;&gt;= \a -&gt; restore (inner a) `onException` unless (isFreed a) (free a) The second is mostly some confusion about what the most general types for type classes are. For instance a linear list monad can't have a linear type on the continuation so we'd get bindListL :: List p a -o (Arr p a (List q b)) -&gt; List q b which would have to instance of superclass of bindIOL. Then there are variants that are completely incompatible , like varying the linearity of the continuation bindContL :: Cont m1 a -o Arr m1 (a -o (Cont m2 b)) (Cont (Multiply m1 m2) b) Streams are an usage example of this, it would allow for prompt upstream resource freeing without making it impossible to capture values with linear context for other operations. data Step o m a r where Yield :: forall t. t -o (t -o m (o, r)) -&gt; (t -o m ()) -&gt; Step o m a r I am presumably not the first to stumble across these issues, do you happen to know if there is anything written about them? Anyway, thanks a ton for your work on this!
[removed]
Ok, let me try to understand. First my reasoning. Let's play with `SimpleIO`, we can only `simpleReadFile` and `simpleWriteFile`. If we start with MTL-like type-class we have: class Monad m =&gt; MonadSimpleIO m where simpleReadFile :: FilePath -&gt; m ByteString simpleWriteFile :: FilePath -&gt; ByteString -&gt; m () Next step is to bundle everything into one type: data SimpleIOAction a where SimpleReadFile :: FilePath -&gt; SimpleIOAction ByteString SimpleWriteFile :: FilePath -&gt; ByteString -&gt; SimpleIOAction () simpleActionIO :: SimpleIOAction a -&gt; IO a simpleActionIO (SimpleReadFile fp) = readFile fp simpleActionIO (SimpleWriteFile fp bs) = writeFile fp bs IMHO, with these building blocks you can do everything else: -- mtl class Monad m =&gt; MonadSimpleIO m where simpleAction :: SimpleIOAction a -&gt; m a instance MonadSimpleIO IO where simpleAction = simpleActionIO -- extensible-effects simpleActionEff :: Member SimpleIOAction r =&gt; SimpleIOAction a -&gt; Eff r a simpleActionEff action = send (inj action) runSimpleIO :: Eff (SimpleIOAction :&gt; r) a -&gt; IO (Eff r a) runSimpleIO = ... -- RIO instance MonadSimpleIO (RIO env) where simpleAction = liftIO . simpleActionIO --- Side comment: &gt; We require a HTTP Manager in our environment, and commit to using this. This &gt; has all the problems of providing a concrete monad transformer stack - we &gt; are committing to an interpretation. That's true, for `ListenBrainz`-like HTP-api, you need either concrete HTTP client data (i.e. `Manager) or abstract over lower-level HTTP-effect. However, note that `RIO` pattern is for *applications*, where you can (actually must) make the choices. --- In `https://ocharles.org.uk/blog/posts/2016-01-26-transformers-free-monads-mtl-laws.html` you mention that `MonadIO` and `MonadReader` have laws: -- liftIO . return = return -- liftIO (f &gt;&gt;= g) = liftIO f &gt;&gt;= liftIO . g class MonadIO m where liftIO :: IO a -&gt; m a -- reader . return = return -- reader (f &gt;&gt;= g) = reader f &gt;&gt;= reader . g class Monad m =&gt; MonadReader r m | m -&gt; r where reader :: (r -&gt; a) -&gt; m a -- not liftAsk ;) because, `liftIO` and `reader` are *monad homomorphisms*. That gives you tools to reason about the program. You can *inject* `IO` or `Reader` computations into bigger `m`. Thus your approach would be to use type Freer f = Free (Coyoneda f) -- or 'Program' for 'operational'. class Monad m =&gt; OliverMonadSimpleIO m where liftSimpleIO :: Freer SimpleIOAction a -&gt; m a liftFreer :: f a -&gt; Freer f a liftFreer = liftF . liftCoyoneda -- IIRC Then often `Freer SimpleIOAction a` would be of the form `liftFreer action`, i.e no bind: liftSimpleIO (liftFreer action1) &gt;&gt;= \x -&gt; liftSimpleIO (liftFreer action2) which simplifies to simpleActionIO action1 &gt;&gt;= \x -&gt; simpleActionIO action2 If there are binds: liftSimpleIO (liftFreer action1 &gt;&gt;= k1) &gt;&gt;= k2 liftSimpleIO (liftFreer action1) &gt;&gt;= (\y -&gt; k1 y &gt;&gt;= k2) simpleAction action1 &gt;&gt;= (\y -&gt; k1 y &gt;&gt;= k2) and so-one, you could recover bound `simpleAction` chain. --- Another note: remember `forall void. void` vs. `Void` discussion: https://www.reddit.com/r/haskell/comments/6qmcsd/to_void_or_to_void/. - `Void` is `Freer SimpleIOAction a` - `forall void. void` is `forall m. MonadSimpleIO m =&gt; m a` If we replace `Freer` with higher-rank type: class Monad m =&gt; WeirdMonadSimpleIO m where liftSimpleIO :: (forall n. MonadSimpleIO n =&gt; n a) -&gt; m a That screams "why you want such thing?", isn't using `MonadSimplIO` directly simpler. Another way to see it, is that following actions are isomorphic: oliverAction :: Freer SimpleActionIO a olegAction :: MonadSimpleIO m =&gt; m a And you can inject them into bigger `m`. The difference is that for `oliverAction` the homomorphism is `liftSimpleIO`, and for `olegAction` it's implicit. --- `liftSimpleIO` is monad homomorphism. I don't know what `simpleAction` means in abstact non-sense, but it's the same as `liftF . liftCoyoneda`.
It's the only valid response to incoherent pseudo-intellectual derailing ramblings. Some things are not worth discussing because the answer is so basic and obvious. "Misogynistic brogramming culture is toxic and must not be tolerated" is one of these questions. Any "discussion" questioning this is just detailing. You should ask yourself how the ideas you are reproducing in these posts help increase the representation of underrepresented groups in the Haskell community.
How do I sign up for this? How do we communicate? IRC? Why "Real World Haskell"? The book is now 9 years old. Meanwhile, "Haskell Programming from first principles" was just released.
What is this weird site, why not link the company site directly? I don't care about recruiters...
I've edited the post to say we'll communicate via a Discord server and that you sign up by saying you want to join, down here in the comments. You interested, Ysangkok? A lot of people have a high opinion of RWH, in particular compared to Learn You A Haskell. I haven't read the new book, but on one site I looked at, it's labeled as being in the Early Access state, which to me means unfinished and likely having plenty of typos. https://gumroad.com/l/haskellbook# Maybe it is a better book, and maybe it's totally finished. I just didn't have any complaint about RWH, to go looking for something improve upon it. The new book costs $60 and doesn't have a free version that I saw. Whereas, RWH is offered for free online for people who can't or don't want to buy it. That's a big difference.
If I were to start again from scratch, I'd probably start with [racket](http://racket-lang.org/), and work my way through [SICP](https://mitpress.mit.edu/sicp/). It contains all the fundamentals, and then makes learning assembler, C, C++, haskell, etc... much easier later.
It's almost 1.0: https://twitter.com/bitemyapp/status/899350024739577856 Anyway, I am interested even if you go with RWH. But I don't plan on making notes, I am more interested in just lurking in your forums.
BTW, did you see this study group which starts in september? https://www.meetup.com/Berlin-Functional-Programming-Group/events/242559370/ They have a github account too: https://github.com/sjsyrek/berlin-functional-programming-group/tree/master/haskell-study-group
Yeah, I saw it posted on reddit here, but since their meetups are in Berlin, and I'm in America, it wasn't relevant to me. If their meetups are online, I misunderstood their situation. Thanks for sharing anyway.
I would prefer if this didn't reference porn in the title of the article.
Interested.
Thanks for the interest. (However...) I've been witness to too many study groups that fizzled out due to apathy or became a lecture (always the same person doing the presenting), so I'm trying something different. I am of the belief it's better to have a small, active group, where people are contributing in not just one but multiple ways, rather than a larger group of more dilute interest. I take no joy in enforcing the policy about participation, but what's even worse is what happens if nobody enforces that policy -- the atmosphere just loses all its momentum and the group falls apart. Every study group is a social experiment of sorts. I'm not saying I know best. I do know a lot about what doesn't work, and about what other people have done wrong when they've run study groups that failed. A book that is "almost" 1.0 doesn't inspire confidence. Many typos typically are found in the first edition of books, and are fixed in the second edition. Going with a pre-release copy is even worse. I don't know if you've ever had the frustration of trying to learn programming from a book with errors in its code, thinking that it's surely right and that you must be doing something wrong. The oldness of RWG would be important if it were obsolete, but as far as I know, its material is still valid. From its publication date, Haskell has stayed the same more than it's changed. Keep in mind that Haskell was birthed in 1990 and the younger it was, the more it was changing.
&gt; The oldness of RWG would be important if it were obsolete, but as far as I know, its material is still valid. http://book.realworldhaskell.org/read/error-handling.html uses fail, which is slated for removal: https://prime.haskell.org/wiki/Libraries/Proposals/MonadFail HDBC is also used. While maybe not obsolete, it is also not getting any features. Persistent is recommended to me in the comments of the answer in following link. Esqueleto provides a type-safe EDSL for SQL queries to avoid the inline SQL strings used in RWH. Other issues listed here: https://stackoverflow.com/a/23733494/309483
Where? 
Please don't. 
https://www.reddit.com/r/haskell/comments/6w4kml/weekly_beginner_saturday_hask_anything_1/
Apologies. Post should now be visible. 😊
If the book we were using were HPFP (Haskell Programming from First Principles) instead of RWH, would you be willing to put in the effort to make notes, a few little programs, and diagrams, for each chapter, and share them with the group as we go? I'm expecting only maybe 1-3 people to be seriously interested in joining the group, due to the planned slow pace alienating many people and the required participation alienating more. If, of the few people who might be interested in the group, switching to HPFP is what they want, that's fine with me. I'm not dead set against it. I could buy a copy and study from that. Unfortunately I cannot change the title of a post once it's set, so the title would no longer be accurate, but whatever. Good finds, about the shortcomings of RWH. The stackoverflow link broke down the complaints by chapter, which makes it easy to consult the relevant part of that document based on what chapter of RWH I'd be beginning, factor their changes in (where possible), and even use them as points of discussion. It's kind of like how, with other books, you have to factor in the errata as you go. Some things from RWH would only need to be tweaked, while other things might have to be skipped entirely. Still, I'm fine with giving it a shot and dealing with problems as they arise. 
https://github.com/rainbyte/haskell-ide-chart seems accurate. Spoiler: Use Haskero, don't forget to `stack install intero`.
The best way to increase the representation of underrepresented groups in the Haskell community, by far, would be to translate all of the documentation and tutorials for it into languages other than English. Happy to do that, just need to learn some extra languages first. Removing the word "porn" from the lexicon of acceptable English terms in English posts -- especially when the body of the post had zero relationship to the word -- won't accomplish any increase in representation of any group whatsoever. It *might*, at an extreme stretch, keep one or two people from auto-decreasing themselves from the *community* due to what could only be described as a feverish over-reaction to a misguided and unfunny title of a post on Reddit by an *individual*. That individual was correctly chastised for the unwise title, including -- you might note if you got off your high horse -- by me. Everything I've said has been to draw a line between "should not be tolerated" and "should be discouraged"; the former is an extreme reaction, the latter a measured one. To conflate that with "misogynistic brogrammer culture" is similarly hyperbolic, as is calling something "incoherent" that you clearly found coherent enough to attempt to retort. No, I'll bend my pseudo-intellect to more important tasks, thanks. Like, on the one hand, getting people to understand why their ham-fisted approach to rhetoric is moving the world further from their political goals -- goals which I share and agree with -- and not closer. And, on the other hand, continuing to advocate for, promote, and hire the underrepresented into positions of leadership so they can fight the valid and important battles instead of undermining themselves fighting the invalid and meaningless ones.
Fuck, I'm turned on now..... What have you done!?
I'm a newbie developer to functional programming but I'm working to create a stack atm with servant (haskell) + elm + mongoDB with the goal to have something similar to meteorjs that can be used for rapid prototyping and something that people can use who wants to create webapps in functional programming but don't know where to start. I also use [Steel overseer](https://github.com/schell/steeloverseer) to quickly reload. Once it's done I will put up a github repo that people can use. You also have spock and yeseod as alternatives.
Oh, and to make it coherent to you: "Donald Trump and those he empowers were given the capacity to wage thermonuclear war because those who oppose him are too divided over trivialities, while those who support him are right at times to point out that you cannot become a fully inclusive society by forcefully eliminating all inappropriate expressions of thought before teaching the people who compose that society and who naturally have those thoughts to understand and empathize with why they might be inappropriate, or at the very least inappropriate to express in a given forum." Or, short version, we as a species have a surprisingly high likelihood of not surviving the next century because we cannot stop conflating feeling safe with being safe.
How do I create my own Monad? Suppose I want a stack of ReaderT and ExceptT
Is the view pattern extension commonly used in Haskell projects?
By far and away, my favorite option is GHCJS + [Reflex](https://github.com/reflex-frp/reflex-platform). Reflex is an absolute dream to work with.
I have started a project with `stack new my-project simple`. Where do I add `-Wall` so that my entire project builds with it, it is "on" when I use `stack ghci`, etc. 
This is exciting to see more ties from the world of formal-verification to Haskell! Thanks for this!
Did you consider using dependent types to define the image database in such a way that it's impossible to create one that's not InternallyConsistent? This approach can often make proofs shorter/simpler.
Those are GADTs, that I suspect do not exist in F#. The `MSet` type has 4 constructors: * `MSet` turns a list of `a` into an `MSet a` * `U` turns two `MSet a` into a single `MSet a` * `X` is probably the cartesian product. It creates `MSet (a,b)` from an `MSet a` and an `MSet b`. * Finally, `list` turns an `MSet a` into a list of `a` (and I had no clue you could do that with GADTs? What does that even mean?) The thing to understand is that the `a` that is given on the first line (with `data`) and the `a`s you see on the other lines are not related.
&gt; and I had no clue you could do that with GADTs? What does that even mean? Reading the paper, you should notice that `list` is not indented. It is a regular function.
Just `stack upgrade`, yes? (for those with `stack`, of course)
Most of the world's programming is done imperatively, but with almost no regard for performance. Look at even good web framework work from the big 4. I'm of the opinion that good algorithmic improvements and superior abstractions beneft those groups more than any advice about mechanical sympathy. Most folks can't even explain what the big O numbers their interview prep courses had them memorize actually mean.
In your `.cabal` file you would have `ghc-options: -Wall` inside the `library` section. You can find more details here: https://www.haskell.org/cabal/users-guide/developing-packages.html#build-information
Thank you!
[removed]
Don't fetish-shame me, OP, or the 36 people that upvoted the post! I refuse to tolerate your intolerance.
Sorry, but I am not going to read anything that comes from a feminist website. I had enough of their sick ideology at work.
Not that I've seen. It's used a lot in our codebase at work, but that might be the only project I've seen it in.
The simplest option is: {-# LANGUAGE GeneralizedNewtypeDeriving #-} newtype MyMonad a = MyMonad (ReaderT MyState IO a) deriving (Functor, Applicative, Monad, MonadReader MyState) And you can change the inner part as you want.
&gt; I don't actually know what I'm doing here. I mean, you're writing a browser in it. There are different definitions of not having an idea.
[removed]
&gt; this is not the venue for it. What venue is the appropriate place for that argument, then? That trite little phrase is a great way of shutting down discussion, and when the discussion is about what a community should be allowed to discuss, to say that your opponent is not allowed to advance their argument is the worst sort of censorship.
Do you mean how to make it using monad transformers? If so, that's as simple as type MyCoolMonad a = ReaderT SomeEnvironment (Except SomeError) a A monad transformer is some type that's parameterized by a monad and, when applied, is another monad the combines the effects of the two. Here's the Haddock for [ReaderT](https://www.stackage.org/haddock/nightly-2017-08-25/mtl-2.2.1/Control-Monad-Reader.html#t:ReaderT) and [ExceptT](https://www.stackage.org/haddock/nightly-2017-08-25/mtl-2.2.1/Control-Monad-Reader.html#t:ReaderT). Making a concrete monad is a little more complicated. You have to write a bunch of instances yourself. If we want the same capabilities as above, we could do something like this: data MyCoolMonad a = MyCoolMonad (SomeEnvironment -&gt; Either SomeError a) runMyCoolMonad :: SomeEnvironment -&gt; MyCoolMonad a -&gt; Either SomeError a runMyCoolMonad env (MyCoolMonad f) = f env instance Functor MyCoolMonad where fmap f (MyCoolMonad g) = MyCoolMonad $ \env -&gt; case g env of Left err -&gt; Left err Right res -&gt; f res instance Applicative MyCoolMonad where pure x = MyCoolMonad $ const $ Right x (MyCoolMonad f) &lt;*&gt; (MyCoolMonad g) = MyCoolMonad $ \env -&gt; case f env of Left err -&gt; Left err Right f' -&gt; case g env of Left err -&gt; Left err Right x -&gt; Right $ f' x instance Monad MyCoolMonad where (MyCoolMonad f) &gt;&gt;= g = MyCoolMonad $ \env -&gt; case f env of Left err -&gt; Left err Right x -&gt; runMyCoolMonad env (g x) I haven't typechecked any of that, so no promises. You can simplify a lot of that code using the Either functor, but I wrote it out for clarity.
At work, we do this: - We have a monolithic repo with our backend, frontend, assets pipeline, build system, SQL, migrations, misc services, ops, etc. - We use Shake to build everything in parallel and specify dependencies. Shake has been absolutely awesome so far both for speeding up local builds and for intelligent cached building in CI. - Codeship for CI, but any CI system based on Docker is probably sufficient (in fact, we are looking at switching to AWS CodeBuild for cost savings). - Everything gets put into its own Docker container, including services and also cron jobs. - Everything gets deployed to an AWS ECS cluster via CloudFormation. We use a [Haskell library](https://github.com/frontrowed/stratosphere) that wraps CloudFormation templates to make this a lot more palatable. - Since we are on AWS, we use an Application Load Balancer for HTTP load balancing, and a Classic Load Balancer for any tcp load balancing we need (for example, using pgbouncer). Service discovery is done through Route53 (the AWS DNS service) plus load balancers. - We are toning down our use of Ansible since moving much of our deployment to Docker, but we still use Ansible a bit. It is wrapped in a Haskell program that generates inventories and deployment variables, validates deployments, fetches any metadata we need from Github or CI, etc. Basically, we try to use Haskell for all of our actual configuration, and we treat Ansible simply as a convenient way to execute commands idempotently on remote machines. - Static scaling is pretty straightforward, just bump up some integer for the number of hosts in our Haskell config. We haven't looked into dynamic scaling via CloudWatch or anything yet because we haven't needed to. Our load is extremely predictable; we are in ed-tech and we get a smooth rise and fall of traffic throughout the US school day, with almost no usage outside of school hours. - We use Datadog for monitoring and LogDNA + CloudWatch for storing logs. Logs get put into CloudWatch via ECS, and a premade Lambda function from LogDNA gets run to forward the logs to LogDNA. Basically, we try to offload as much as we can to external services and tools without reinventing the wheel, and we aren't tied to using tools written in Haskell just because they are written in Haskell. For ops, Haskell has been awesome as a *wrapper* around lots of these tools. We use Haskell as glue to type check and apply configurations for these external services. You've asked a pretty open-ended question and I'm not sure if I've answered it, so I'm happy to provide more details :)
`stack upgrade` upgrades Stack itself. The Haskell Platform isn't normally used with Stack, although the Platform includes a `stack` executable. If you want to use GHC 8.2.1 with Stack, you need to either use a [recent nightly Stackage snapshot](https://www.stackage.org/nightly), wait for `lts-10.0`, or set `resolver: ghc-8.2.1` in `stack.yaml` and include any dependencies that don't ship with GHC in `extra-deps`.
It's getting more popular now that the `PatternSynonyms` extension has matured somewhat. In combination, you can define a pattern synonym that performs arbitrary computation. See the `:&lt;|` and `:|&gt;` pattern synonyms in `Data.Sequence`, for example.
I have never used Haskell beyond pet project, so Heroku is enough for me. There's a community buildpack to deploy Haskell on Heroku. Pretty easy to setup. I blogged a bit about it here in case you want to try http://eckyputrady.com/2017/02/18/Haskell-Heroku-Mailgun-Redis/
I don't see the `pure f &lt;*&gt; x = fmap f x` law. Is that provable from the ones you give? I don't see how, but maybe I'm missing something. If you add it, though, you can drop the `fmap f (pure x) = pure (f x)` law.
it's a perfect answer thanks! I'm just starting to get to the point where I'm making these decisions in my own work, and wanted the question to be open ended enough that it applied to anyone, a way to try and distill "best practices" from the community :) regarding more detail, if you have anything where you feel like your process is a bit better than alternatives, I'd love more details!
No ad-hominem attacks, please. 
This is no ad-hominem attack, I'm fed up with this pseudo pc crap. It has no place in a programming subreddit.
Wait ... I thought everyone mixed together math and fun times with their SO. Is it just me?
Heh, I'm usually looking at what is *worse* so we can make it better. Here are some pros and cons though. Pros: - Immutable deployments are pretty nifty. Deploying a whole system is rarely atomic, but it's nice when deploying pieces of it are fairly atomic. Just add some fully-baked containers behind the load balancer, take of the old ones, and bam our new software is deployed. - I love being able to download all of the same Docker container versions that are on prod so I can run them locally to debug any issues. For example, one of our Yesod services was returning a 404 for some CSS and it only happened in prod. I downloaded that container, ran it locally, and figured out that CI wasn't putting the CSS in the right spot. Easy fix. - Deploying a new service or cron job is a low-overhead operation because the process is now so similar between all of our services/jobs. - Using simple Haskell data types for our configuration is way nicer than any config language I've worked with. Type errors catch almost all configuration problems. The only time they don't is usually in some string parameter that we are too lazy to make a legit sum type for :) - Wrapping the Docker build in Shake has been a pretty big win. When we have so many moving pieces to put in some containers, it is tempting to scatter Dockerfiles all over the codebase. We have all of our Dockerfiles and static dependencies in a `docker/` directory, and Shake handles moving all of the pieces around to actually produce images. Cons: - Although I'm happy with our architecture, it can be a ton to wrap your head around. I mentioned that making a new service or job is simple because they are so similar, but for new ops folks making their *first* service/job can be a little daunting. Overall though, I still think it is less overhead than our old approach of configuring this stuff in-place with Ansible on long-living EC2 instances. - We are trying to move to continuous delivery (or even continuous deployment), but we aren't there yet. Deploying is as simple as running one command on your laptop, but it can still be scary for folks to deploy to prod, and especially scary to run migrations. We are hoping to move to some fully-automated deployment system (at least to some staging env when merged to master), to reduce this friction to zero. Again, most of these pros/cons aren't related to Haskell directly. However, using basic Haskell data types for configuration, using Haskell for orchestrating deployments, and using Shake for the build *are* Haskell-specific and I think they bring a ton of value to our ops system.
I use the Nix ecosystem as much as possible both at work (Awake Security) and for personal use. The main reasons I prefer Nix over other deployment tools is: * the uniform language and toolchain integrates every abstraction layer * the massive Nix ecosystem (much larger than any other ecosystem) To go into more detail: ## Building Haskell packages: Nix I wrote a guide on how to use Nix for Haskell development here: https://github.com/Gabriel439/haskell-nix ## Operating System: NixOS At work we use NixOS as the base operating system for the machines that we deploy, although I use a macbook for my laptop (mainly because I joined the company before we switched to Nix and I didn't feel like changing my laptop) At home I use NixOS for my development laptop ## Deployment: `nixops` or `terraform` For personal projects I use NixOps to deploy systems. For example, you can use NixOps to deploy a mirror of the Dhall Prelude: https://github.com/Gabriel439/Haskell-Dhall-Library/tree/master/ipfs ... or to deploy a server that runs my thesis project: https://github.com/Gabriel439/suns-search At work we use Terraform instead of NixOps to provision systems, mainly because NixOps does not have a good story for secure and shared deployment state management (last time I checked). If NixOps fixed this we would abandon Terraform in a heartbeat because it's extremely confusing and doesn't integrate with Nix anywhere near as well as NixOps does. ## Development Environments: `nix-shell` You can use `nix-shell` to create a transient virtual environment for any project that has all the dependencies you need for development. It's like Python's `virtualenv` except it works for any project in any language. You can also use `nix-shell` to provision an arbitrary set of developer tools, too. At work we don't have to "install" anything for any workflow. We just open up a `nix-shell` tailored to a specific workflow and we have everything that we need and once we close the `nix-shell` we return back to our pristine original environment. ## Continuous Integration: `hydra` You can use Hydra to build and cache Haskell projects, command line tools, development environments, and systems. With [hail](https://github.com/TaktInc/hail) you can deploy those built systems directly from CI. We haven't used `hail` at work yet but we're interested in doing so. ## Containers: Slowly migrating off of `docker` We originally adopted `docker` before Nix, so we have several Nix integrations for `docker` that we've open sourced here: https://github.com/awakesecurity/hocker ... and we'll blog about them more shortly However, we're also slowly migrating off of `docker` now that we're using Nix as Nix accomplishes much of what `docker` does but much better. Specifically, Nix derivations are composable whereas `docker` containers are not, and if we use Nix exclusively we get single-step pushbutton deploys whereas `docker`-based workflow require complex multi-stage development pipelines. Using Nix exclusively is also significantly leaner with disk/CPU/RAM than `docker` (even with our advanced `docker` integrations, which do a lot to reduce the footprint of containers) ## Configuration: NixOS The cool thing about NixOS is that you can configure literally everything about your system using a single standard NixOS configuration format, including: * the boot loader * `systemd` services * installed packages * firewall settings * containers (including `docker` and `systemd-nspawn`) * a long list of service-specific integrations (such as `kafka`/`zookeeper`, etc.) You can see the full list of options here: https://nixos.org/nixos/manual/options.html ... although it's much easier to browse them through here: https://nixos.org/nixos/options.html ... and you can easily define and use custom options of your own. The operating system configuration is fully programmable (using Nix)
And it's your place to decide that crap you're personally fed up with has no place in this subreddit? You can't express your frustration without name-calling? 
Actually, you don't have to write the instances yourself, if you do this: {-# LANGUAGE GeneralizedNewtypeDeriving #-} import Control.Monad.Reader (ReaderT, MonadReader, runReaderT) import Control.Monad.Except (Except, MonadError, runExcept) newtype MyCoolMonad a = MyCoolMonad { unMyCoolMonad :: ReaderT SomeEnvironment (Except SomeError a) } deriving ( Functor , Applicative , Monad , MonadError SomeError , MonadReader SomeEnvironment ) runMyCoolMonad :: SomeEnvironment -&gt; Either SomeError a runMyCoolMonad env m = runExcept (runReaderT (unMyCoolMonad m) env) ... and maybe turn on some other extensions that I've forgotten
I feel like this comment shouldn't be downvoted and that this is an important discussion to be having, whether or not you agree with it.
It's very much my place to speak against this, yes. Have a nice day.
Truly, what happens in privacy between mutually consenting adults is of no concern to me. Be safe, be healthy, be consensual!
&gt; *[41:20]* It seems like pretty much anything with just a completely fixed behavior, that doesn't have any nondeterministic input or random behavior or dependent on the outside world.. those all seem really boring. The interesting ones are the ones that have some notion of "it could come out one way or it could come out another way". Here is a fun `Alterantive` instance (for `ZipList` of all things) included in the latest version of GHC ([`#13520`](https://ghc.haskell.org/trac/ghc/ticket/13520)) instance Alternative ZipList where empty :: ZipList a empty = ZL [] (&lt;|&gt;) :: ZipList a -&gt; ZipList a -&gt; ZipList a ZL xs &lt;|&gt; ZL ys = ZL (xs ++ drop (length xs) ys) In use asumMap :: Alternative f =&gt; (a -&gt; f b) -&gt; ([a] -&gt; f b) asumMap f xs = asum (map f xs) &gt;&gt; asumMap (ZipList . concat . \n -&gt; replicate n (show n)) [0..15] ZipList {getZipList = "123456789010101010101112131415"}
&gt; I found the typeclass documentation for Coq to be very unhelpful Completely agree. Coq typeclasses are awesome, but we desperately need documentation and libraries.
Thanks, I was interested in both.
Thanks, I'll try that.
I will add it, thanks!
(the /r/purescript discussion is [here](https://www.reddit.com/r/purescript/comments/6w79zd/why_do_we_write_type_signatures_lefttoright_when/))
John runs the LambdaConf functional programming conference, which experienced a lot of controversy around a speaker two years ago. [This is a really well written and moderate piece on the topic](https://medium.com/@codepaintsleep/lambdaconf-2016-controversy-2d4b13c338cf), with a lot of other people being more vitriolic on either side of the equation. Discussion of the controversy has been done to death, so I imagine that the subreddit would prefer that the conversation not reappear here (again).
I thought I'd misread the word and had to re-read "Porn" a few times. Then I chuckled and clicked it.
The phrase "contributes to an atmosphere where many people will not feel welcome" is an instant downvote from me for culture-war reasons. I also think the porn reference is distasteful, but I strongly disagree with this reasoning.
x/post of /r/programming thread. Maybe I'm too much biased about my knowledge of axiom and theorem in mathematical sense. The hard part is to specify what is formally verified. And in which axiomatic system. There are two theorem but they are not even explained in plain english. The axioms are all but trivial. For example: Axiom size_nochange : forall (db : ImageDb) (img : ImageId) (cat : CategoryId), num_images (tag_image db img cat) = num_images db /\ num_images (untag_image db img cat) = num_images db. How the fact the size of the database before and after tagging don't change is an axiom. If you set complex enough axiom, the proof of a theorem should be a piece of cake.
Thanks for documenting this, but this is really more appropriate for the [GHC issue tracker](https://ghc.haskell.org/trac/ghc/), not the haskell subreddit.
&gt; It means "uncalled for" or "given free of charge", depending on context. Good! Let's try syllogism now. Is this a valid syllogism? Gratuitous X should not be tolerated Y is an X ------------------------------------------ Y should not be tolerated &gt; And refusing to respond to an argument you question the merit of -- which was itself a critique of a statement which was lacking merit -- isn't a particularly valid response in a forum that you've already assumed was appropriate to this discussion by voicing your opinion. I have no interest in debating sexual norms with you, much less in /r/haskell, where it is *completely off topic*. You're the one who's trying to abuse the forum to force such a discussion.
Out of interest is the mere mention of porn offensive? I assume the Venn diagram of other-than-male programmers and porn browsing has an intersection. Is it simply offensive to you? Are you protecting a wider audience outside of that intersection? Do they care? These are answers I'm genuinely interested in.
&gt; What venue is the appropriate place for that argument, then? Not my problem. You go find it. I come to /r/haskell to share ideas about Haskell with other people who are interested in Haskell, and I don't appreciate potential participants being driven away by gratuitous porn references, or arguments about "how incredibly varied is the panopticon of human sexuality" or how feminists are the same as Donald Trump. It's **off-topic**.
Sure, it's valid if your proposed tautology is accepted as an actual one -- it is not. And then you're employing a red herring. I am in no way debating or even addressing sexual norms, nor even coming within a country mile of the topic... I chastise you for going to "should not be tolerated" instead of "should be discouraged". The first attempts to apply *your* moral code to an entire community, the second attempts to get the community to consider its moral code. My issue with your approach is that, in this particular case, you bring a hammer to a situation that needs, at most, a gentle redirection.
I had a stab at addressing this [in a Reddit comment a while ago](https://www.reddit.com/r/haskell/comments/6p29rv/haskell_is_the_most_readable_language/dkn5fkm/). The short answer is that if `f :: Result &lt;- Int &lt;- String` then `f "String" int` looks really odd.
I've been learning about Smalltalk recently, which uses whitespace as the "invoke method" operator. So the expression someObject phraseBook at: 3 put: 'hello world'. is essentially the same as Haskell's IntMap.insert 3 "hello world" (phraseBook someObject) It's made me curious what a functional language would look like with the general function application being reversed. It makes currying weird, for sure. 
just to nitpick, there's no 'g' in 'empatho' (unlike in 'oligo').
Is there some consensus on what solution to use in modern times to overcome duplicate record fields problem? If I confront this error (two record data types with a same field name) as a novice for the first time, how should I resolve it? I've seen so many "solution" articles ranging from compiler extensions to custom libraries and now I'm a bit lost...
[How about having whitespace as a composition operator instead](http://evincarofautumn.blogspot.co.il/2012/02/why-concatenative-programming-matters.html), like in [kitten](http://kittenlang.org)?
Well nitpicked, though I was trying to work in oligo- as well, ie the fake empathy of the few.
Why and when is the `newtype` with record field syntax used? I read a lot of articles which use this pattern assuming that the reader knows it, but I failed to find a good explanation of its origins. I can follow the type of a typical `run` function used in this pattern and I can understand it from there, but I'm not really sure how and why it came to be this way. Example: ``` newtype State s a = State { runState :: s -&gt; (s, a) } ```
or, shorter, Trump's win is an anti-PC-dictate backlash. Just because people don't like to be dictated. Proponents of PC thought the discussion was over, and went into enforcement mode -- too soon, evidently. It is always too soon, because discussion is only truly over when there's no need for enforcement anymore.
Which, by the way, will be about forty five minutes after the last of us has perished and we're damned sure no one is pulling a Jon Snow. Although I do find it funny that the response to not wanting to be dictated to was to elect a dictator.
It depends on in what context it is discussed, there's a big difference between "gratuitous references" to cement one's bro-status in brogramming culture and e.g. [critically examine the phenomena](https://plato.stanford.edu/entries/feminist-sex-markets/).
that would just be empatholigarchy, perhaps.
/r/iamverysmart
Hmmm... oligopathotocracy? Yeah, I'm not sold on any of them.
there's no more discussion about virtues of cannibalism. or murder. or slavery. no need to enforce the winning opinion on these (as an opinion; enforcing behaviors is another matter). as for the survival of the species, unless someone detonates a very large cobalt bomb, or lets some deadly virus loose (much scarier prospect than a nuclear war actually), the species will be just fine. nuclear winter too is a hoax, according to Wikipedia (IIRC). :)
I think I may have misunderstood you. When you said "this is not the venue for it", I thought that you were applying that to making arguments for a softer approach to minimizing off topic discussion.
Thanks for reporting this, I've raised a ticket for this [`#14160`](https://ghc.haskell.org/trac/ghc/ticket/14160) 
No group is good at handling being called out for its faults.
This is a consequence of the historical fact that mathematics chose to write `f(x)` and `X -&gt; Y`, that are incompatible. You have some articles in category theory that write `(x)f`, but I think this is a lost battle .
Are /r/haskell users supposed to see citation of that sub as a critique? I'm sure just mentioning that you use Haskell to the average programmer will make them mentally put you in the iamverysmart bucket.
`stack build` and good old scp(1) :D
My fears for the species hinge primarily on us becoming too stupid to live, like by being able to convince oneself that a minuscule risk of a substantial vaccine injury trumps a 100% certainty of a forthcoming pandemic, or that voting for neither of two evils -- one of whom is certain to win -- is somehow better than voting for the lesser of them ... or that Wikipedia can save us from the first empirical test of nuclear physics in the large.
&gt; consensual What about its dual, nsensual? Is it like 1-, 2-, n-sensual denoting the number of senses involved? I am just a beginner, but if I learned something cool from these category theory blogs, it is the dual is usually worth inspecting a bit closer :)
Can confirm! On reddit, pick the safe route... "Functional programming is cool right? Just look at clojure! Soo clean and fun! Let's agree function are cool, okay? Now think about what you can do when you add types, just like in Java, but in better... Oh look at this innocent looking code, that is a function and its type signature. Awesome, that lack of boilerplate, isn't it? You like that? Uh. There is more!..." Then after they wrote their first p0rn browser you can tell them they wrote the haskell, and it did not even let their brains explode:)
&gt; Or, short version, we as a species have a surprisingly high likelihood of not surviving the next century because we cannot stop conflating feeling safe with being safe. Which, in a twisted way, gives me comfort whenever I am afraid about some out-of-control neuronal network with access to proper hardware. If there is a dystopy to come, our overlords won't be cold, emotionless machines... it will be us, the good old humans again.
Follow up question to: How do I create my own Monad. What is the actual difference between: `ReaderT SomeEnv (ExceptT SomeError m) a` -- Base monad is Reader or Except ? `ExceptT SomeError (ReaderT SomeEnv m) a` In both I can use features of Reader And Except. Does the order of the stack matter only from the outside? Also which monad would be the base of the stack? The out most?
I think [this post](https://ro-che.info/articles/2012-01-02-composing-monads) answers the question pretty well: some monad transformers commute, others don't (typically error and state: one nesting throws away the state when it errors whilst the other one keeps it around for you to inspect).
I think it's more quotes like these &gt; No, it's not, it's the prerogative of humans to invent new words. Like all of these ones, and the ones before them. In this case it's a portmanteau of an extremely obvious prefix and suffix, and the meaning is patently obvious to anyone who knows both. . &gt; If I wanted my thoughts to be accessible to as many people as possible I'd be expressing them in Mandarin, Cantonese, and Urdu. . &gt; Also, this is a forum for Haskell users. Average hasn't got anything to do with my audience. as well as just the way everything is phrased. Nothing to do with Haskell.
Using `newtype T' = T' T` guarantees type `T'` has the same memory representation as `T`, whereas `Data T' = T' T` wraps `T` in another layer. Why use `newType`? You know the performance and behavior of the code will be exactly the same as if you used `s -&gt; (s,a)`. Why use `data`? You're creating a product/sum type in where newType isn't available OR you want to lazily represent `s -&gt; (s,a)`. EDIT: The same differences apply whether you're using record field syntax or not. Record field syntax just automatically creates the destructors. Just known if you see `newtype A = A { getB :: B }`, type `A` will behave exactly the same behaviour as `B` and `A :: B -&gt; A` forms an isomorphism with `getB :: A -&gt; B`. If GHC allowed you to turn off the type checker, you could use them interchangeably. newType State1 s a = State1 { runState1 :: s -&gt; (s, a) } data State2 s a = State2 { runState2 :: s -&gt; (s, a) } runState1' = unsafeCoerce :: State1 s a -&gt; s -&gt; (s, a) runState2' = unsafeCoerce :: State2 s a -&gt; s -&gt; (s, a) get1 = State1 $ \s -&gt; (s,s) get2 = State2 $ \s -&gt; (s,s) -- Prints "WORKS" snd $ runState1' get1 "WORKS" -- Crashes without message (segfaults) snd $ runState2' get2 "WORKS" --- Crashes with message seq (State1 undefined) "WORKS" --- Prints "WORKS" seq (State2 undefined) "WORKS" --- Crashes with message case State1 undefined of State1 _ -&gt; "WORKS" --- Prints "WORKS" case State2 undefined of State2 _ -&gt; "WORKS" 
Thank you! But why exactly people are using a record syntax for this? Rather than defining a `runState` function separately. Is this just a convenient shortcut? Or is there something else here, some other useful properties gained by doing it in this way? This always puzzled me because as a beginner I used to think of records as a C-style "structs" which have data fields. But this construct seems to have a data field which is a function. I get it that in Haskell functions are data too, but still feels itchy :)
Thanks it helped! So to answer my own question - The base monad is the inner most monad. 
Alas, this is expected behavior. These examples which used to work in GHC 7.10 and earlier relied on a bug in the typechecker that incorrectly allowed impredicative type signatures to be inferred. Fixing this bug necessarily breaks your code, since they rely on instantiating unification variables with impredicative types. See [my answer](https://ghc.haskell.org/trac/ghc/ticket/14160#comment:2) on the GHC Trac ticket.
Thanks! I will keep this in mind too
&gt; Is this just a convenient shortcut? Sorry if I missed the point of your question, but yes. You hit the nail on the head. &gt; But this construct seems to have a data field which is a function. Haskell doesn't have `x.y` syntax for product types like C, so the only way to access the field without pattern matching would be to have a function from the product to the field you want. That's fine for most cases, but haskell comes from the world of lambda calculus and category theory where pattern matching doesn't exist, so it makes sense it would automatically provide these functions for us if we want. If you want more justification look at the definition of a [Categorical Product](https://en.wikipedia.org/wiki/Product_(category_theory\)).
Fantastic summary. It's not amazing, but I use a simple wrapper script for NixOps that allows it to integrate with git-crypt: https://github.com/grafted-in/nixops-manager. This allows all your server state to be stored encrypted *in* the repository. Pros: * Centralized place for configuration and state * Simple setup requiring only 1 extra tool: git-crypt * PGP-based authentication for encryption Cons: * Server state is basically opaque and does not work well with version control concepts like merging/branching. Conceptually, server state should never be used it if is not the most up-to-date. * Git-crypt doesn't have a great story for revoking access to a given PGP identity. There are unofficial scripts that do it for you, but it's kinda hacky. * NixOps does not have a good story for cycling keys. * NixOps server state is a little bit funky and this fact is exposed by version control: * In a binary-to-binary comparison, NixOps state data changes almost every time you use it. The changes are usually not meaningful and can actually be lost without hurting your deployments. * Even *most* meaningful changes to the NixOps state are superfluous and losing the changes don't hurt your deployments. * Some changes *are* important (like IP address) and losing this is very bad. So unless you know what you're doing, you generally want to be safe and commit changes as often as possible.
I use Haskell Syntax Highlighting because it's simple and doesn't get in my way while focusing on being just enough. https://marketplace.visualstudio.com/items?itemName=justusadam.language-haskell
Is it wrong to talk like that? It just seems like /r/iamverysmart is a cheap way to detract from someone when there's no argument against their actual ideas.
Hello OP, your post's title, and the comments it has gathered, have reminded me of a couple of blog posts by Terence Tao on the subject of writing professionally: [Write professionally](https://terrytao.wordpress.com/advice-on-writing-papers/write-professionally/) &gt; Overly philosophical, witty, obscure or otherwise “clever” comments should generally be avoided; they may not seem so clever to you ten years from now, and can sometimes irritate the very readers you want to communicate your result to. [Be professional in your work](https://terrytao.wordpress.com/career-advice/be-professional-in-your-work/) &gt; Take your duties and responsibilities seriously; being frivolous is fine with friends, but can be annoying for your colleagues, especially those who are busy with similar responsibilities. &gt; One’s writing should also be taken seriously; your work is going to appear in permanently available journals, and what may seem witty or clever today may be incredibly embarrassing for you a decade from now. By the way, if I had tried to read your post at my old job, that title would've probably not flown with the content filters. That said, thanks for sharing your blog posts. I'm hardly the ideal target audience, as most of the content for this one went way over my head, but it seems cool to me to see a language like Coq being used in worldly contexts. I had the idea that languages like Coq or Agda are mostly (or even only) for language research.
The right one for me is 'ghcid'. On large projects 'intero' occasionally succeeds to provide a type signature but mostly just heats the laptop up continuously using 100% CPU while providing "loading..." for its type signatures.
I followed the full stack and nanocoin installation instructions but got the following error: user@computer:~$ stack install nanocoin The following target packages were not found: nanocoin Any idea why stack is not finding nanocoin?
&gt; -- Can build get / set using `flip const ()` and `const x` respectively &gt; overTrip :: CoordT -&gt; (a -&gt; a) -&gt; Trip a -&gt; Trip a I think you forgot a functor if you wanted to do `get`, since `flip const ()` is just a fancy name for `id`. 
Great post! Illustrates a lot of the power of phantom types with data kinds and dependent types :) I know you didn't ask for this but I took a look over the final code to see where you could have some code-reuse and make things a bit simpler with the singletons library: https://gist.github.com/mstksg/a46c27a3ce091d5ad43bb7f4c3669a29 Some notes -- 1. I wouldn't refer to Coord as a proxy -- call it for what it is, a singleton :) 2. Recognizing the Coord is a singleton, we can have it be automatically generated for us using the singletons library. `coordVal` is just `fromSing`. 3. `BoardRep` is just a list, so there really isn't any reason, I think to re-define a custom list? This isn't something you would do in regular haskell, so I don't think you would do it in type level haskell, either. type BoardRep = [(CoordT, CoordT, PieceT)] Like I said, you probably wouldn't redefine your own monomorphic lists in haskell, so I'd think the same wisdom applies here. 4. Since `BoardRep` is just a list, it can be seen that `Played` is just `any` (from Prelude), so might as well re-use it. We can define `Played` using `any` and `==`. 5. `playX` and `playO` are really the same function, so you can just write `play` and have it be parameterized by `X` or `O`. Passing in a player singleton should allow you to pick whether you want to play as `X` or `O`. 6. To prevent something silly like passing in `SN` (playing as a "None" player) because of `play` being polymorphic, you can just have pieces be `X` or `O`, and have a board be full of Maybe's. Just a couple of suggestions. But, again, great post, and it's exciting to see more demonstrations of the power of phantom types + datakinds + dependent types +singletons out there :)
I'm interested as well.
&gt; f3 uses undefined for implementation. Reminder: [`undefined` is not always a valid dummy implementation](https://www.reddit.com/r/haskelltil/comments/4q0oni/undefined_isnt_always_a_valid_dummy_implementation/)
Quite a lot of people use `x.f()`, so there's a precedent for putting the argument first. I think there might still be room for an extra battle :) I should use `(&amp;)` more often, to see how it feels!
you would need to clone the repository first: ``` $ git clone https://github.com/tdietert/nanocoin.git ``` this will clone the nanocoin git project and put it in the `user@computer/nanocoin` directory. Then you need to go into that directory and then run the build commands.
It actually feels very good. I usually make use of the functions in the [compose-ltr](https://hackage.haskell.org/package/compose-ltr) package.
I've heard of the (&amp;) function before, but couldn't find it on hoogle. Is it in some popular package?
If anyone wants to discuss the implementation, or hop on the project and help me create an experimental cryptocurrency! I'm keen on developing this project more with some help, just for fun.
It's in a package called [base](https://www.stackage.org/haddock/lts-9.1/base-4.9.1.0/Data-Function.html#v:-38-), it's quite a popular package you should try it sometimes :)
It can be [both](https://github.com/gelisam/adicity#readme)!
At work, we use Stack/Docker/AWS, more or less. This is what happens on CircleCI every commit to master: 1) build executables and tests 2) run tests 3) run a Python script to create a minimal Docker image 4) push the image to the docker repo 5) deploy to AWS Elastic Beanstalk. This has worked well for the past couple years for our ~15K line app.
As of not *terribly* long ago, it's in `Data.Function`, which is indeed in the `base` package that came with your copy of GHC.
I've gone through HaskellBook before but wouldn't mind a slow stroll through it again. I'm a fan of discord. If you have a server for it that's cool, and there's also a general functional programming server that would probably be willing to set you up a channel if you'd like that as a way to connect with more people who could answer questions.
Thanks! Any idea why it doesn't show up in search?
Because you used [haskell.org/hoogle](https://www.haskell.org/hoogle/?hoogle=%28%26%29), which is [out of date]( http://neilmitchell.blogspot.ca/2015/02/why-is-hoogle-index-so-out-of-date.html) but supports type search, instead of [hoogle.haskell.org](http://hoogle.haskell.org/?hoogle=%28%26%29&amp;scope=set%3Astackage), which is up to date but doesn't support type search yet. There is also [stackage's hoogle](https://www.stackage.org/lts-9.1/hoogle?q=%28%26%29), which searches through a specific lts package set.
I take it you have no interest in bulldozing through Real World Haskell? It's Haskelbook/HPFP or nothing?
Take a term-level expression including a function... f "hello" 2 Do direct substitutions, translating terms to their types and the invisible application operator to a type-level application operator... f_type &lt;- String &lt;- Int Use that to declare the type of function `f`. f :: f_type &lt;- String &lt;- Int The function `f` has type such that `f` can have a string applied, then the result can have an integer applied. No ordering has changed, even if you add in the implicit parentheses... (f "hello") 2 (f_type &lt;- String) &lt;- Int f :: (f_type &lt;- String) &lt;- Int Of course we don't actually write the `f_type &lt;-` in the function type because we already know we're defining the type of function `f` and the language wouldn't understand that identifier anyway. But even so - my basic point is that the function type expression describes the order of application of arguments, and does so exactly the same way as the term-level expression. **EDIT** - Obviously as well as adding a placeholder for the function type, I was conveniently forgetting the result type. Even so, point is that the type of a function describes arguments that are *applied* (not composed) to the function. 
We could, but i found it less helpful when i tried to read through it. Only read like 6 chapters but i suspect the rest of the book would be lite on the concept teaching as well. HaskellBook has homework problems at the end of each chapter that really help the concepts sink in as you grapple with them.
Is it possible to use nix ecosystem properly within macOS or you have to move away to Linux for that?
If you're going to write such turgid prose for your tone policing, perhaps you should spare yourself some embarrassment and look up what words like "panopticon" mean before applying them. Grow up.
The main things you can do from OS X are: * You can use Nix as a package manager for your developer tools * You can deploy and orchestrate NixOS systems from OS X (i.e. using `nixops`, for example) Note that in order to deploy NixOS systems you need to be able to build Linux binaries, which you can't do locally on OS X. Fortunately, we can either: * Build them ahead of time in CI (i.e. `hydra`), or: * Delegate builds to Linux build machines for things that are not built by CI The latter approach takes advantage of Nix's support for distributed builds. We can initiate a Linux build from a developer's MacBook and then the build actually takes place on a shared Linux build slave. You can also use distributed builds to accelerate the build as well (since the build slave can have many more cores than your development machine) This is an example of how Nix's uniform toolchain pays off. Every abstraction layer (i.e. packages, containers, operating systems, integration tests) is built using Nix, so you can use distributed builds to accelerate all of these layers for free. Contrast that with other ecosystems where (A) new tools you add to your stack might not even have support for distributed builds, (B) the way they support this might differ from tool to tool, and (C) you'd have difficult getting each tool's support for distributed builds to work efficiently with each other
Good feedback. You said you've gone through HPFP before. How long ago was that?
Hmm, around a year ago perhaps was the last time i was looking at it on a regular basis. I got into the early release around halfway done, and then followed along as each new chapter came out until they were at the 99% done phase, version 0.12, which was quite a while ago. Version 1.0 dropped just within the past 2 weeks or so. The biggest delay was finding a publisher that wanted to print a book so large as a single volume, but i guess they did.
&gt; IMO, I really like the idea of making LLVM the only supported backend, and just dispense with the other two (NCG, via C). Requiring llvm might mean building GHC itself is more complicated, but in the end I think it might be less of a maintenance burden. This is the sort of thing that sounds nice in principle but in practice the story isn't nearly as rosy as one would think. Believe it or not, GHC's native code generator is one of the least maintenance-intensive parts of the compiler. Other than the (partially finished, sadly) hoopl rewrite around 6 years ago it has had very little attention. LLVM, on the other hand, has required a constant trickle of attention to keep up with upstream. It may be that /u/angerman's bitcode work will reduce the maintenance burden a bit, but this is far from certain. More fundamentally, there are some rather serious impedance mismatches between LLVM and GHC. Make no mistake: this is the fault of neither compiler; LLVM is a great compiler for imperative languages with C-like compilation and execution models. It just so happens that Haskell isn't one of those languages. Consequently we are forced to commit terrible hacks like aliasing symbols to satisfy LLVM's type system, mangling assembler to fix up stack alignment and section types, and splitting proc points (and consequently giving up optimizations) so we can insert info tables in continuations (although Kavon Farvardin has done a bit of work on fixing this). In the past things were even worse: GHC had other hacks which we have gradually phased our as we have worked with upstream to get needed features merged. This is a long process though as LLVM upstream is understandably cautious about what they want to merge; it requires reflection and discussion and is consequently the upper bound on the rate of change is significantly lower than what is possible in the NCG. This is one major reason why I am quite skeptical of the prospect of moving exclusively to LLVM. Another consideration is the effect on compilation time. Today the LLVM backend is around a factor of three slower than the NCG. Admittedly this is in part because we ask LLVM to do more optimization than is likely necessary. While we [may be able to fix this](https://ghc.haskell.org/trac/ghc/ticket/11295), this will require time and understanding of the interactions between LLVM's optimization passes, both things that few GHC developers have in abundance. Understanding GHC's NCG, on the other hand, only requires reading a handful of fairly well-documented modules; there aren't many optimization passes but the effect of each is clear and we do enough to produce quite quick code. This difference in learning curves is one reason why even [relatively basic aliasing](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/Backends/LLVM/Alias) optimizations have yet to be performed in the LLVM backend. In addition to all of this, we have a variety of smaller issues, * Debug information: While LLVM now has a somewhat usable debug metadata scheme, it isn't nearly expressive enough to represent the [rich information](https://downloads.haskell.org/~ghc/master/users-guide//debug-info.html#implementor-s-notes-dwarf-annotations) that GHC now provides. * The packaging story is a bit of a mess, meaning we would essentially have no choice but to ship LLVM in our bindists, growing the distribution size by a factor of two. * We would lose the ability to easily explore changes in our calling convention and low-level optimizations On the whole I think it would be a serious mistake to drop the NCG. Yes, it's simple and at times a bit silly; on the other hand, it's fast, easy to understand and modify, trivial to package, and, most importantly, it "just works". Of course, LLVM also has its place. While I'd love to see [SIMD support in the NCG](https://ghc.haskell.org/trac/ghc/ticket/7741) this will require that someone do the work and this will remain LLVM's exclusive domain until this happens. I doubt we will ever see an NCG written for ARM and AArch64. I also have no doubt that LLVM will always be able to beat us on tight inner loops. All of this being said, for compiling your `servant` application on x86-64 the NCG is just fine.
The easiest way to understand the difference between the two is to replace each monad transformer with its underlying implementation (i.e. unwrap the `ReaderT` and `ExceptT` newtypes) That means replacing every occurrence of: ReaderT i m r ... with: i -&gt; m r ... and every occurrence of: ExceptT e m r ... with: m (Either e r) If you do that, then you get the following equivalent types: ReaderT SomeEnv (ExceptT SomeError m) a ~ SomeEnv -&gt; ExceptT SomeError m a ~ SomeEnv -&gt; m (Either SomeError a) ExceptT SomeError (ReaderT SomeEnv m) a ~ ReaderT SomeEnv m (Either SomeError a) ~ SomeEnv -&gt; m (Either SomeErorr a) So you can see that in this specific case the two transformer stacks are equivalent to each other. We often say two monad transformers "commute" if their order with respect to each other does not matter However, not all monad transformers "commute". For example, consider these two monad transformer stacks: StateT s (ExceptT e m) r ExceptT s (StateT s m) r If you unwrap the newtypes for each stack, you get: StateT s (ExceptT e m) r ~ s -&gt; ExceptT e m (r, s) ~ s -&gt; m (Either e (r, s)) ExceptT e (StateT s m) r ~ StateT s m (Either e r) ~ s -&gt; m (Either e r, s) Those two types are not equivalent: the first type only lets you retrieve the final state if the computation "succeeds" (i.e. returns a `Right`), whereas the second type always lets you retrieve the final state regardless of whether the computation succeeds This leads to the interesting issue where you can write some `mtl` code like this: example0 :: (MonadError String m, MonadState Int m) =&gt; m () example0 = do put 1 example1 `catchError` (\_ -&gt; return ()) example0 :: (MonadError String m, MonadState Int m) =&gt; m () example1 = do put 2 throwError "Urk!" ... and the final state will differ depending on what type constructor you instantiate `m` to. For example, if you instantiate `m` to `StateT Int (ExceptT String m)`, then the final state will be `1`, but if you instantiate `m` to `ExceptT String (StateT Int m)` then the final state will be `2`! This is an example of how two monad transformers can sometimes not commute
That's because a `newtype` defined using record syntax can still technically be used like a record. For example, `get1` could have been written like this instead: get1 = State { runState = \s -&gt; (s, s) } However, the real reason why people use record syntax for newtypes is because it automatically gives you the unwrapping function for free
Indeed, this would be great. For what it's worth, did a bit of sketching this direction a couple months ago. The patch is [here](https://github.com/bgamari/ghc/commits/wip/ncg-simd); perhaps it will be helpful.
Here is a [post](https://jship.github.io/posts/2017-08-27-monad-transformer-commutativity.html) that can serve as a supplement to the one /u/gallais linked. Hope it helps! This was a confusing area for me too.
I also would love for SIMD to be more usable.
When I was a grad student doing category theory, I would always draw my arrows from right to left. There's a couple good reasons. Composition like OP mentions. It matches the ordering of `y = f(x)`. Also cardinality, `|A &lt;- B| = |A|^|B|`. One time I was giving a lecture and explained that I would use this convention and a prof in the room would not accept it. We spent like 20 minutes arguing about it, lol.
For reference, I was just pointed [here](https://softwarefoundations.cis.upenn.edu/draft/qc-current/Typeclasses.html), which at least seems to explain what the backtick operator does.
Great! Maybe you could open one or two issue describing bug to fix or some enhancements you already planned. It helps newcomers to feel a bit welcome. Writing some contribution guidelines can also be a good idea.
spock is very similar to scotty and is on of the easier frameworks out there yes just give it a try - if you have trouble you can always ask
I use ghcid combined with VS Code - http://neilmitchell.blogspot.co.uk/2017/08/ghcid-and-vs-code.html
Oh, I'm sorry, you think a means or position from which all behaviors can be observed doesn't fit the definition of a [panopticon](https://en.m.wikipedia.org/wiki/Panopticon)? Fascinating! Bentham, of course, would have recognized my meaning the very moment we explained the Internet, Big Data, and "porn" in the modern sense to him. No embarrassment, by the way, but love how you've misplaced your throw as much as you've misplaced that k.
Non-Mobile link: https://en.wikipedia.org/wiki/Panopticon *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^105656
True. Right bastards with less capacity for empathy than an algorithm could ever lack.
You don't like dense phrasing, but yet you like Haskell? Pointing out to someone that **a** isn't a word when **a** is a loose concatenation of two words is tedious, so I respond with tedium. Pointing out that I'm not appealing to the average when I'm in a forum for a language that has as much appeal to the average as Dwarf Fortress does is ridiculous, so I respond with ridicule. I'm sorry -- to an extent -- if my tone offends anyone. Swift offended me when I first read him. So did Austen when I first read her. And Garcia-Marquez. Who were these pretentious *bastards* that used such dense language and couldn't say anything without frying it in fat and butter first? Oh ... turns out they were the ones with the best understanding of *people* and the institutions and jargon they built up around their frailties. They're all *worlds* smarter than I could ever hope to be -- especially Marquez -- I mean light years ahead of me ... they leave me sitting where Einstein left the baboon. But I can *aspire*, and I can learn how to use a language *densely* with the hope that, one day, someone can feel empowered enough to come along and try to make me look stupid for aspiring not to be.
Bravo.
Actually, people still do discuss the merits of slavery. I know because at least one of them spoke at LambdaConf
&gt; feminists are the same as Donald Trump ??? Can you even *read*? You heard it here people, "**N** should not be *tolerated*", where **N** is whatever [u/sacundim](http://www.abbreviations.com/SACUNDIM) finds gratuitous/offensive/intolerable, is now **on-topic**, while any retort/question/response/critique/casual observation about "**N** should not be *tolerated*" is henceforth **off-topic**. Your forum, der commissar.
No, because even in that metaphor, the idea that pornography encompasses "all behavior" in "human sexuality" is as preposterous as it is offensive.
Sounds great. Would love to join and read the whole thing
I have been using GHCJS + [React-hs](https://github.com/liqula/react-hs). Works very nicely, great if you are familiar with react.js.
I think you're taking this too personally. I was just playing devil's advocate, and trying to objectively point out why one might try to put you in the /r/iamverysmart bucket. I'm neither offended nor do I care all that much. Just to parallel the original comment that spawned this, I'd just say that the denseness can, to some, be gratuitous, but considering this is /r/haskell, like you did earlier, you can make the argument that the audience is sufficiently advanced to warrant the denseness. Anyways, bottom line, I get where you're coming from.
Awesome! Love the unwrapping and substitute explanation! Small typo, second function signature should be `example1 :: ...`
Thanks! I'll definitely try the exercises :)
&gt; A major plus for me, is the option to write the "model" only once, and use it in both front and back end! I really want this feature. [Miso](https://github.com/haskell-miso/miso) gives you this. 
Mathematical tradition.
Thank you for continuing to try and reduce to an absurdity that which is in no way absurd. Let me spell it out for you: First, I'm probably quite a bit looser than you on the definition of pornography, as I include the written and drawn varieties... Twilight fanfic as well as woodcuts and witticisms carved on bathroom walls. The important thing is that it's recorded and/or 3rd-party observable utterances of sexual behavior or expression. But, by whatever definition, you should note that my key interest in it is in understanding the perspective of others, especially perspectives that do not come naturally to me. Through the lens of an individual's choices in pornography -- including their choices to abhor or ignore it -- you can gain insight and understanding into their individual take on human sexuality. Or, in other words, it's damned near impossible to empathize with someone else's sexuality if you don't understand or cannot see what works (and/or does not work) for that individual. Through the lens of a culture's use -- or abhorrence or lack -- of pornography, you can understand key aspects of a culture's worldview as it regards sexuality. Or, in other words, it's damned near impossible to empathize with a culture if you don't understand or cannot see how that culture views its sexuality. And I'm in no way limiting the term culture here to ethno-geographic size, I mean everything down to the subbest of subcultures. Now, to the Panopticon: the point of the metaphor (and the prison) is the person sitting at the center of it, i.e. the person viewing all the many and varied behaviors of all the others ... the Panoptes itself. It's the Watcher who can at least *observe* (and possibly *understand*) all going on around it ... THAT is what must be most flexible and varied in relationship to human sexuality. From that position, a position that YES modern, internet-accessible pornography (inclusively defined as above) enables, one can become the universal acceptor of *all* -- here I'll again explicitly qualify this for you to mean *all 3rd-party observable* -- human sexuality. Empathy is hard, I get it. It's particularly hard -- in fact I believe impossible -- to obtain if you're not willing to see everything that can be seen. To be the Watcher. I'll leave who watches the Watcher to later.
So how do you do this in practice? If I wanted to prove various properties, doesn't it make sense to start with fairly complex axioms as a sort of "top-down" approach where the software is actually possible to write? Then later, refactor into better and smaller axioms?
The thing is that we declare functions in Haskell as `f x1 x2 = ...`. This can be made more consistent by using lambda notation, but then pattern-matching suffers. Another package that helps directional composition and application is `flow`, by defining: (|&gt;) :: a -&gt; (a -&gt; b) -&gt; b (.&gt;) :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; (a -&gt; c) and their opposites, which I think is very consistent notation.
Don't worry, not taking it personally at all, though I can certainly see how the "you" in my bit above could be taken to mean you in the specific only, not you as in anyone reading the words. This is the balance I'm always trying to strike, whether or not it is better or worse to add more parentheses and sub-clauses. It *can* be gratuitous... I strive really hard to never *actually* be gratuitous. The tyranny of Twitter has everyone thinking that "brevity is the soul of wit" was intended to mean "&lt;140 :-D, &gt;140 :-(". Unfortunately I cannot do anything about the people who *actually* only read the tl;dr bit.
*Why do we write type signatures left-to-right, when we normally compose functions right-to-left?* Because we write declarations left-to-right too: add : Int -&gt; Int -&gt; Int add x y = x + y Rather than: add :: Int &lt;- Int &lt;- Int add = x + y withargs x y
Thank you!
I see, thanks!
You're welcome!
From a practicle point of view I never made a formal proof. The way you explain is certainly correct. Never having doing it, I just totally missed that this blog was describing only a step of a formal proof. Which is explained by op answer to my initial post. Here the initial comment https://www.reddit.com/r/programming/comments/6w77ol/comment/dm5vg4g
Slow learning is exactly what I'd like as I'm a slow learner. I can go with Real world haskell, as I cannot afford $60 on the haskellbook.
It's a pity that impredacivity leads to so many problems: it's something that would be really useful if we could figure out how it's supposed to work!
Der,ee ,Da,, ist es und und
I'd very much like to read more on "Using Liquid Haskell to add lightweight proofs to your existing Haskell code" - i.e. what can I do with liquid haskell vs what can I do with coq.
I think you mean good with this, but I'll have to downvote this. There's a total of 8 on-topic responses to OP, while there are 78 posts discussing "porn". That's just unreasonable and makes is impossible to have a conversation on this topic. OTOH, if the top posts are downvoted, it will be possible to those that have an interest in that part of the discussion to engage by opening them.
Much more discussion here than there!
Dunno... Right now, it looks like its gonna be right people, Less frequently is used to be people from the left. In any case, people not machines ;)
Hah. I meant right in the British sense of "proper", not in the political sense of "idiot". ;-)
As a non-native speaker: What a queer usage of that word!
While it is classed as being early access, it is the best book by a good margin. I read it in the state it was in around a year ago and it read like a finished book for the most part.
It's the tragedy of being an American-borne Canadian who lives in the UK. I'm a native speaker to three versions of my own language, none of which agree! I honestly can't imagine how people for whom English isn't their first language navigate the idiocy of it. It actually comes from the root, btw ... right from *rectus*, meaning straight, correct, or proper, vs left which -- via a long chain -- goes through the Germanic word for "weak" and ultimately ends up at the Latin word *sinister*. Which literally just means "left". Go figure.
What tools do you use for writing code? I use Emacs with Intero, but in the code that compiles with GHCJS Intero doesn't work and coding with no on-the-fly type checks cripples dev speed. Have you solved this problem? 
Without getting into the records replacement sort of libraries, the simplest options are: 1. Manual name prefixes: `data Cat = Cat {catName :: String}` 2. Import namespacing: `import Cat as C` or `import qualified Cat` 3. `-XDuplicateRecordFields` used by explicitly specifying either the record field type or record type (`name :: Cat -&gt; String` or `name (cat :: Cat)`), or automatically inferred in certain situations (the [GHC wiki]( https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields) has more info) Of these manual name prefixing seems to be the most common convention, though I personally like `-XDuplicateRecordFields`. &amp;nbsp; `-XOverloadedRecordFields` is coming eventually, which will be like `-XDuplicateRecordFields` but smarter about inferring the types so things should just magically work without having to think about it. [`HasField`](https://downloads.haskell.org/~ghc/master/users-guide/glasgow_exts.html#record-field-selector-polymorphism) is new in GHC 8.2.1 and part of the groundwork for this so there's progress actively being made.
Thanks for the feedback; you spent more time with it than I have. I downloaded the few free chapters they offer, and I agree it looks like nice material.
This is an option too. By any chance do you speak a language that is written right to left? I think this feels less unnatural in this case.
Neat! Reminds me of an experiment I did in getting concatenative-style stack polymorphism in Haskell, where functions are polymorphic in the tail of the chain that they don’t examine, rather than constraining it to an empty stack `()`. Built-in type-level lists made it a bit easier to get started: type family Tupled (ts :: [*]) z :: * type instance Tupled (t ': ts) z = (t, Tupled ts z) type instance Tupled '[] z = z newtype Fun as bs = Fun (forall z. Tupled as z -&gt; Tupled bs z) unfun :: forall as bs z. Fun as bs -&gt; Tupled as z -&gt; Tupled bs z unfun (Fun f) as = f @z as -- Various fizzy lifting functions fun_2_1 :: (a -&gt; b -&gt; c) -&gt; Fun '[a, b] '[c] fun_2_1 f = Fun (\ (a, (b, s)) -&gt; (f a b, s)) unfun (fun_2_1 (+)) (1, (2, ())) == (3, ()) Dunno how far this approach goes, though. 
Sounds great. Would love to join and read the whole thing
I'm not sure what this gains. "Ensures that exceptions will be handled" isn't good enough by itself. What I need is "ensures that exceptions will be handled in a place that makes sense." In a yesod application, for example, exceptions that are handled at the very top level, outside both the app and warp, are not much use. Yesod's `forkHandler` function uses `forkIO`, and I think that makes sense. When I fork a thread inside a Handler that will outlive the request, it makes sense for it to be my responsibility to handle exceptions within my child thread. Of course, any further threads I fork within that top-level long-running thread can be asyncs.
Could you employ these techniques to get the same safety for runtime input using an existential wrapper (like `SomeNat`)? &gt; In Haskell type functions are called Type Families, but really they're just functions on types There are some things you can do with one but not the other, so this is just true enough to get you into trouble. ;) 
I made Flow. Thanks for the shout out! Here's a link to the documentation: https://www.stackage.org/haddock/nightly-2017-08-25/flow-1.0.8/Flow.html
Why do we compose functions right-to-left? import Control.Category showInt :: String -&gt; Int showInt = toNumber &gt;&gt;&gt; toString
I meant advocating for having it *today*, not in history. ultimately that's the only question: what do we consider to be beyond the pale, not worthy of discussion but rather only of our violence. if large portions of a society start seeing each other that way, that's the definition of a civil war. we must be cognizant of that, I think. the whole Voltairian stance is, basically, trying to put off the civil war as much as possible by talking, by trying to convince one another. 
we never know the consequences of our actions. only hindsight is 20/20; unless you're a seer, in which case you can't act.
As someone with 20/15 in the left eye, 20/10 in the right, I've long wondered about that axiom.
So elm is both a language and a framework all in one. It also has no sense of abstraction in the way of classes and instances like Haskell does. I think it's fine as a stepping stone, and might even make you truely appreciate some of the more advanced Haskell features when you approach it. There are a couple operator differences, but otherwise the two are very similar. If you want something even closer to Haskell, maybe check out PureScript. That being said elm seems to have a larger community than PureScript, so it depends what your goals are.
&gt; I wouldn't want learning Elm to make Haskell any harder. I doubt it will. Starting with Elm is fine. A fair possibility is that once you get comfortable with Elm, you'll start to be frustrated by certain things which really feel like they should be automated but which instead require a lot of mindless effort from you (like writing JSON parsers, specifying explicitly whether you meant `Maybe.map` or `List.map`, etc.). Haskell can make this much less painful.
I have learned Elm after already knowing Haskell so I'm only assuming the following to be true: I think Elm is a great way to get started with functional programming and knowing it will not make it more difficult to learn Haskell afterwards. Many fundamental concepts are the same and a solid basis to build upon. As far as I know Elm is close to being a subset of Haskell with only a few differences. Elm is strict (like most languages you probably already know) which should be easier to reason about to start with. Elm has extensible records. Elm tries to force the user to write total functions (function defined for all possible inputs) which is something you should strive for in Haskell even though it's trivial to write partial functions. Going to Haskell after Elm you might miss the extensible records but many of Haskell's features (such as typeclasses) will make up for that. Good luck on your functional programming journey! :-) Don't hesitate to ask about some of the things I've mentioned, I realise I left a few concepts mostly unexplained.
I recommend learning Elm before Haskell. I haven't used this route myself, because Elm didn't exist back then, but now that it does it looks like a much easier path. Unlike Haskell, it's a language designed to be easy to learn, and yet it will teach you the benefits of purity and algebraic datatypes, without overburdening you by also teaching type classes, monads, and all the extra fancy types which Haskellers like me love to talk about. You'll know you're ready to switch to Haskell once you start to find Elm code a bit repetitive: one of the ways in which Elm makes itself easier to learn is by intentionally limiting the forms of abstraction it supports, and as a consequence Elm code can be a lot more repetitive than the corresponding Haskell code. Except for its record system, which is more advanced than Haskell's, Elm is pretty much a subset of Haskell, and so learning it definitely won't make it harder for you to learn Haskell later on.
Thank you so much for this post. I feel like I just went from intermediate Haskeller to a somewhat better intermediate Haskeller. I feel more comfortable using type families in my code now, which seemed kind of scary to me before (particularly `AmbiguousTypes`). I found myself between the value level and type level not knowing how to bridge the two properly, and I think this might be the solution. The syntax is pretty ugly, though (especially `KnownSymbol`), but if it’s sufficiently useful this could be eradicated by writing a parser from a more elegant syntax, which auto-inserts `KnownSymbol` and whatever else is needed.
Great, thank you for this summary!
Are you guys aware that [Hocker](http://www.dict.cc/deutsch-englisch/Hocker.html) is a German word? Was that chosen on purpose?
Thanks for the suggestion-- I have added several issues outlining additions I'd like to make to the project.
My experience was that I took a few half-hearted stabs at learning Haskell over the years, and found it very difficult to get comfortable with. Meanwhile I came across Elm and found that learning Elm was not only much more straightforward, but it also made concepts from Haskell much more clear to me. For example, I had learned about the Applicative typeclass in Haskell pretty early on, but never really understood it until the experience of working with JSON decoding in Elm made me arrive at the concept from a very concrete, practical angle. That same kind of experience has played out a number of times in different ways. Building a GraphQL query DSL in Elm made me understand what a Profunctor was in Haskell and why it might be useful. In the end, going back and forth between Haskell and Elm has been a wonderful way to deepen my understanding of both languages. On the Haskell side of things, I eventually found the excellent book Haskell Programming from First Principles which has been one of the best and most rewarding experiences I've ever had learning a programming language. I would learn concepts in one way from doing the exercises in that book, and then learn them in a different and complementary way by building abstractions for things in Elm and stumbling on the same core patterns (maybe months later). Each language has informed my understanding of the other in a wonderful way. In a lot of ways it's been like learning programming for the first time all over again, and it's been great.
Nobody is advocating for violence here. Quite the opposite: the whole purpose of cracking down on speech promoting slavery is to prevent violence and suffering. After all, slavery is what caused the civil war in the first place, not political correctness.
No axioms are violated. Haskell morphisms have side effects (such as non-termination), and this reasoning seems to work fine for them... [This paper](https://personal.cis.strath.ac.uk/conor.mcbride/Kleisli.pdf) discusses the link between Kleisli categories and Hoare logic, which is more-or-less a program logic built on the category axioms.
aren't computers fast becoming a thing of the past though? a programmer just wants to specify their computational process. let the cloud/smart_dust implementors worry about how to run it. that is/will be a separate profession.
When working with `IO` code, I often find it necessary to validate/parse the values, ending up with something like `IO (Either String Value)`. Is there a nice way how to work with these values? I sense that some monad transformers might be the answer.
People do know how it's supposed to work. The issue is that you tend to lose one of inference, impredicativity, or System F types no matter what. Inference is a no-go because these types get very complex. Losing System F types will make everything unfamiliar to programmers today and break a lot of the formal infrastructure. That said, learning a bit about these non-System-F types can be really informative for grokking why impredicativity is hard. The name of the system I'm referring to is "MLF" and there are a fair number of papers out there. Be warned that the inference algorithms get quick complex, though.
Generic: - We have a CI docker image that can also be used for local builds. This is similar to how Nix could be used. - We use gitlab's docker support so CI builds run in our swarm, but the gitlab cloud UI is used for browsing results. - We use a single repo for all packages as well as production setup. Docker swarm keys ++ are encrypted files in the repo that are decrypted by CI. - `stack image container` is used to create containers. We have our own base image. Creating this image, and the CI base image are the two things that are not done by CI. Those are manual processes, but of course scripted. - We use docker stacks extensively. What you get with docker swarm and stacks is the overlay network that you will not get with Nix/NixOS. I won't say this is without problems, there are some issues with the overlay network + load balancer still. - Front-end load balancer is HAProxy with dynamic registration through docker event listeners. - We don't have any production setup on AWS. AWS is an order of magnitude more expensive than the competition, and docker makes it possible to avoid lock-in. - StatusCake for monitoring. - ELK for logs processing from the swarm cluster. These run in docker. Docker Hub: - We use docker hub extensively. We push all production setup to docker hub first, and then pull at the sites. Docker hub is amazing value. Only downside is that they do strong traffic shaping, also when you are a paying customer. For deep learning, the layers can be multi-GB in size, and in those cases the push/pulls can become extremely slow. - We have a deep learning pipeline also based on docker where the data is uploaded to docker hub and we spin up an AWS p2.xlarge (spot typically) instance that does learning, do a `docker commit`, push the finished models back to docker hub, and kill the instance. This, and Route53 are the only AWS infrastructure we use. - The permission system at docker hub isn't very advanced, but since it's a standard protocol, it's possible to switch to any provider in the future. Database: - We use postgres - Postgres backup to OVH's object storage. We use pghoard for this. OVH is cheaper than S3 for pure storage, and an _order of magnitude cheaper for retrieval_ when you don't use AWS (the business model for AWS is lock-in). See http://gaul.org/object-store-comparison/ CI: - As mentioned we use gitlab. All non-master branches share a single environment. CI on this non-master branch replaces this environment. - CI pushes apps to Google Play (and App Store). We have several apps in Google Play - typically one app for the `master` branch, and one app for non-master PRs. The latter is only used by developers/testers. Thus not too long after a push you can play with the new app using the non-production environment. - I've stopped using CircleCI as I ran out of memory with the typical ghc/ghcjs memory usage. A docker-based setup with private workers makes it possible to have much faster turnaround time, as we can use much bigger machines. CircleCI has a better interface than Gitlab, but Gitlab gave us back the control we needed.
That sounds way too specialized! In how many circumstances would you need this exact sequence of steps? Even if it was more common than I imagine, this doesn't sound very composable. Haskell libraries tend to provide a number of small components which can be assembled into many different shapes. So instead of a rigid framework which only works on files, only on files which are structured into consecutive blocks of N lines, and can only run a validation function on those blocks, I would split this functionality into several smaller pieces: something which streams a file line by line, something which groups consecutive elements from a stream into groups of N elements, and something which aborts a stream if an element failing a user-provided predicate is encountered. This way, your users can stream data from other sources (such as stdin), they can perform transformations (such as removing comments) on the stream before splitting it into chunks, they can perform further transformations afterwards (such as splitting some of the N lines into even smaller groups of M words), and they can do other things with the result than validation (such as writing the elements which pass the predicate into a different file). In which case: yes, streaming libraries already exist, the two most popular ones are [Conduit](https://hackage.haskell.org/package/conduit) and [Pipes](https://hackage.haskell.org/package/pipes). [Here](https://www.stackage.org/haddock/lts-7.16/conduit-combinators-1.0.8.3/Data-Conduit-Combinators.html#v:sourceFile) is a Conduit component for reading chunks from a file, [here](https://www.stackage.org/haddock/lts-7.16/conduit-combinators-1.0.8.3/Data-Conduit-Combinators.html#v:linesUnbounded) is one which splits those chunks into lines, [here](https://hackage.haskell.org/package/conduit-1.2.11/docs/Data-Conduit-List.html#v:chunksOf) is one which groups consecutive elements into groups of size N, and you can probably use [`guard`](https://www.stackage.org/haddock/lts-8.19/base-4.9.1.0/Control-Monad.html#v:guard) to validate that each element satisfies your predicate.
Thank you, I'll read that essay.
I definitely recommend this route, as someone who learnt Elm before Haskell. It won't damage your ability to learn Haskell _at all_; you can honestly convert Elm code to Haskell really trivially as it's pretty much a limited version of Haskell. I found it a lot easier to pick up Elm than Haskell because there were fewer choices for me to make; it's a framework and you have to do things the Elm way - and a huge amount of effort has gone into making the language as simple as possible. After learning Elm and using it for a while on more complex projects you will undoubtedly get to the point where you feel like you have too much boilerplate and things just aren't quite as elegant as you feel they ought to be. This is when you should learn Haskell :)
In which sense [cassava](https://hackage.haskell.org/package/cassava-0.4.4.0/docs/Data-Csv.html) decoders of unnamed records (see Index-based record conversion) are unadequate? You can define precise shapes by defining records, and impose sizes on the decoder definition.
cracking down on speech *is* violence, by definition. Can't be done without it. "with good intentions" etc., you know. Totalitarianism is a terrible thing, believe me, I lived through it. It is much, much, MUCH better to tolerate few disgusting idiots on the fringe, IFF they are not violent. MUCH better. Take my word for it, please. I implore you. It doesn't mean a specific community can't shun certain types of speech and ostracize their promoters. It surely can (I e.g. was offended by the "porn" thing). The offend/ing/ed parties will just move elsewhere. Not so on the scale of the whole country. _That_ is the recipe for bad, bad things.
Good stuff! I'll look at working these in there! I'm intentionally avoiding singletons for simplicity (and I think it's helpful to learn the machinery behind it) but I should probably at least mention that it exists haha.
I speak a little Hebrew. Maybe that helps me be notationally ambidextrous, but I don't really think that's relevant here. You can still read left-to-right with your arrows pointing right-to-left, just read it as "from" instead of "to". I have another theory that left-to-right thinking is imperative and right-to-left is declarative but I'll save that rant for another day :-)
Make sure to post this rant here when it comes to life, I'm interested!
_Make sure to post this_ _Rant here when it comes to life,_ _I'm interested!_ &amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ^- ^guaraqe ------------------------------ ^^I'm ^^a ^^bot ^^made ^^by ^^/u/Eight1911. ^^I ^^detect ^^haiku.
Wow, this is great advice! Thank you.
is `vcache` still alive? I've looked at things like that, and `acid-state`, but this niche seems rather under developed. Are there viable alternatives yet for having a quick-to-develop, refactor, etc. persistence solution in an environment that doesn't need something as heavy as postgres?
Elm is definitely a great intro to Haskell. Case study: my little brother started out with zero CS knowledge and has learned enough Elm to create a decent webapp (with my help guiding him). Now he is starting on Haskell (via http://haskellbook.com/) so he can finish out the backend server to complement the webapp. He actually dropped out of college last year and has only been programming for about 10 months now. I also came to know Haskell by way of Elm, and I'm mostly productive in both languages by now, though I don't consider myself an expert (I use neither language at my day job unfortunately). The great thing about Elm and Haskell as first languages is that they present CS concepts in their purest form. You don't have to worry about the details of prototypical inheritance as you do with JavaScript, or OO design patterns as you do with Ruby/Python. With Elm/Haskell you get directly to the abstract concepts behind the object metaphor. The only other learning route I recommend is SICP with Scheme, but the distinction between Scheme/Haskell is largely aesthetic IMO (conceptually they are more similar than they are different). BTW I do not recommend Haskell tutorials, they tend to focus on a few aspects of the language instead of teaching it to you from the ground up. Pick up a book like http://haskellbook.com/ and work through it so you have a nice base knowledge to start from.
Both [streaming](http://hackage.haskell.org/package/streaming-0.1.4.5/docs/Streaming-Prelude.html#v:slidingWindow) and [conduit](http://hackage.haskell.org/package/conduit-combinators-1.1.1/docs/Data-Conduit-Combinators.html#v:slidingWindow) have a `slidingWindow` combinator.
ExceptT/EitherT is what you're looking for. 
/u/ChrisPenner &amp; /u/mstksg Amazing posts, both. I'm wondering if I can play on that `game` board at run time now, but am struggling to create a function `move` that makes this work: -- in type families mode move "X" = X' move "O" = O' main :: IO () main = do m &lt;- getLine putStrLn . show $ play (move m) (C', C') game -- in singletons-mode move "X" = SX move "O" = SO main :: IO () main = do m &lt;- getLine putStrLn . show $ play (move m) (SC, SC) game what is the sum type that `move` needs to return (something with an `X`-like and `O`-like sum)? 
I'm still a bit new to type theory, is this due to... decidability? IE TypeFamilies needs to be decidable, but the runtime code does not? Could you provide any examples?
I'm not advocating for government to crack down on speech. I'm advocating for private citizens to do so
If all you need is a static website, you can try `hakyll`. It has good templating capabilitiesm and you can write the pages in markdown or other markup language.
Thanks. I need to unpack the explanation and see how could translate to oCalm.
So does the singletons library also generate the Played constraint or did I just miss that? If so that would be pretty impressive!
Sounds like just HTML/CSS will do what you want, but otherwise maybe hakyll or search "static site generator Haskell"
indeed I need a static website. I will check it out, thanks! Small noob question, if I will need to insert some references into web page, something like bib file, and I will parse it into haskell data type, can I then contract a web page based on haskell datatype? I mean I always can write a pretty printer for markdown format, but maybe this thing is already thought of in hakyll?
&gt;Sounds like just HTML/CSS will do what you want It does, but I do not want to write it by hand anymore :) &gt;but otherwise maybe hakyll or search "static site generator Haskell" yes, hakyll sounds like a fastest solution, I will probably start with that and my migrate to something more interesting once I need it.
Not that much experience with either language, but I doubt learning Elm will make Haskell harder. If anything, it'll probably make you excited once you learn about certain features. Haskell being mythically hard to learn is more a meme than anything else, so don't give random comments about its difficulty too much thought. With that said, everyone in the Elm ecosystem has gone to great lengths to make it very approachable and user-friendly, so it's a great first choice! 
When developing the front end side of a web application, would you choose PureScript, Elm, or something else and why?
Your submission was automatically removed because you linked to the mobile version of a website. Please submit a non-mobile link instead. *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/haskell) if you have any questions or concerns.*
It integrates well with pandoc, which is able to deal with references.
I'd likely choose ghcjs, if possible. When size is a real issue, I'd go with PureScript instead. The biggest reason why I'd rather use ghcjs is laziness, closely followed by my familiarity with the Haskell ecosystem.
Also check out Purescript, It's mostly same as Haskell, but easier as it doesn't have language extensions and doesn't contain legacy functions which were confusing when I started haskell. 
I've been a guest on a podcast talking about this: https://www.functionalgeekery.com/episode-88-justin-woo/ Overall, I thought Elm was eye-opening to the world of programming using useful types, but the removal of its reactive programming aspects and the fact that some things felt hard (like trying to use Sets for user-defined types) made it a lot less readily usable to me. 6 months after I first tried Elm, I started learning Purescript and found it was readily usable for web and some small Node things I normally did, and even now I mostly just use Purescript and only some amount of Haskell. Also our IDE integrations are way nicer.
I'd say Scotty + blaze isn't a bad approach.
As a general thought, I'm not sure if Haskell is a good language do implement crypto in. It seems that with lazyness and the way the compiler magically optimises your code away, you have basically no control or ability to reason about how the calculations are done. This leads to side channel hell. Also, can you control that intermediate states of your calculations do not reside in memory? Am I missing something? Is there a good way to reliably reason about execution times, memory usage etc. that may leak information? 
you are actually right :) Googling around, it make me thing so. And for starters I do not even need Scotty, just create my static web site with blaze. Thanks. So now I am thinking hakyll vs blaze.
Are you not going to even have multiple URLs? I guess it's probably possible to just use blaze to dump a bunch of HTML into files and have the directory structure be the URL structure. But I mean at some point you will need a web server to know how to handle requests and give back responses, so were you thinking nginx or something like that? HTML files don't quite make a website by themselves. 
Isn't it possible to have a well defined subset of impredicative types be inferred? Situations where it is unambiguous what the resulting type should be like this one. It would be nice to allow `undefined` to be a universal stub, and it would be nice to have `$` not need to be specially handled.
it does! here: $(singletons [d| played :: CoordT -&gt; CoordT -&gt; [(CoordT, CoordT, a)] -&gt; Bool played x y = any_ (\(x', y', _) -&gt; x == x' &amp;&amp; y == y') |]) λ&gt; :i Played type family Played (a1 :: CoordT) (a2 :: CoordT) (a3 :: [(CoordT, CoordT, a0)]) :: Bool where [k, (x0 :: CoordT), (y0 :: CoordT), (b0 :: [(CoordT, CoordT, k)])] Played k x0 y0 b0 = Apply (Apply Any_Sym0 (Apply (Apply (Apply Lambda_1627615686Sym0 x0) y0) b0)) b0 -- Defined at stictactoe.hs:68:3 for learning, I also converted the `Turn`, same idea: $(singletons [d| turn :: [(CoordT, CoordT, PieceT)] -&gt; PieceT turn [] = X turn ((_,_,X):_) = O turn ((_,_,O):_) = X |])
Yep. Have used blaze with a few different servers before and haven't heard of a static 'compile' option. It's probably possible to do with JS though
I'm on the fence with this one. One benefit of the less classy title is to perhaps make the Haskell community look more approachable and friendly. One of the things many dislike about the Haskell community is we often seem very intense and academic and not about fun or practicality. I doubt many other programming subs would take much issue with this title.
There might be heuristics, but I haven't seen it. Generally, the issue is that there is ambiguity about where type variables ought to be bound. A general system cannot tolerate a situation where two possible solutions exist. The standard example is to define `left : a -&gt; a -&gt; a` with `left x y = x` and then consider `left id`. In an impredicative system we could type it as `left id : forall a . (a -&gt; a) -&gt; (a -&gt; a)` OR `(forall a . a -&gt; a) -&gt; (forall a . a -&gt; a)`.
[removed]
I mean it's not a great way to talk if you want to have friends, nor if you want to be understood, nor if you want to be liked. Besides that I guess it's fine. 
You can like Haskell and hell even be a fantastic and incredibly smart Haskell researcher and still not talk like that. For example Edward Kmett. Haskell is already hard enough for people get into without people talking like that.
Yes, I would have multiple urls :) I was thinking about damping multiple files as you said. As I understand "server" is a program which runs and accepts requests, etc. So it is not a static html page, right? The university "hosting" i use does not even support php, so I am not sure I will be allowed to run my server. But I do not know how all things is handled... so maybe I can do it :) I will probably try. 
&gt; Elm tries to force the user to write total functions (function defined for all possible inputs) which is something you should strive for in Haskell even though it's trivial to write partial functions. Could you elaborate on this? I don't know much Elm, what are the features that separate the 2 langs here?
What's wrong with learn you a Haskell? 
Yeah, type families, like type synonyms, can’t be partially applied because IIRC type-level lambdas make typechecking undecidable, and also make higher-kinded types require higher-order unification instead of the simple implementation we have now (where `m a ~ n b` implies `(m ~ n, a ~ b)`). And I think there are some other restrictions. So for example you couldn’t cleverly say “I’ll make monadic combinators work for pure functions without `Identity` by sneaking them past the typechecker”: type family Only a where Only a = a -- Nope instance Monad Only where return = id (&gt;&gt;=) = flip ($) main = do mapM_ print [1, 2, 3] print $ mapM (+ 1) [1, 2, 3] -- Nope But you can do things you couldn’t do at the value level, like define a fully polymorphic equality test: eq :: a -&gt; a -&gt; Bool eq = {-…uh…-} type family Eq (a :: k) (b :: k) :: Bool where Eq a a = 'True Eq a b = 'False Plus other things I’m probably forgetting. So there’s a significant amount of overlap, but they’re not quite the same. 
well of course, good collaboration is the best :) 
Excellent question. First, the good. The author of Learn You a Haskell isn't confusing; he writes clearly and entertainingly. If you study your way through it, you will learn a ton about manipulating code in different ways. Now, the bad. The myriad subtopics are taught in a way that is too narrowly focused. When you've finished the book, you can feel clueless about how to begin to do anything with what you've learned. "Well, now what?" It doesn't gel together. You're not demonstrably better off after the book than before it, if it was your only teacher about Haskell. It's incredible that a book can somehow be that way, but there you have it. It's fine as a supplement to other material that gives you the bigger picture.
I use Elm, mostly because it's dead simple and is pretty fun (I think r/haskell gets what I mean there -- standard non-buildy-parts-on-rails experience). Aside from e.g. no typeclasses, I think Elm is pretty much what I'd make if I was trying to make NotHaskell, the 2017 Haskell that doesn't need compatibility. Except it's only client-side -- just as well, I'm not super sold on sharing front and back end languages.
Boy people don't talk about this. I've seen "Senior" Ruby devs write test code with so many things mocked that no non-test code ever gets called. Utter failure to test usefully is easier than people assume.
yeah, you can use `SomeSing` as your sum type, or skip it altogether using `withSomeSing`. toSing :: CoordT -&gt; SomeSing CoordT withSomeSing :: CoordT -&gt; (forall c. Coord c -&gt; r) -&gt; r `SomeSing CoordT` is basically a sum between `Coord 'A`, `Coord 'B`, and `Cood 'C`, so you can pattern match on it and go from there.
i don't get why spock doesn't get more love. it's pretty nice too (just a bit behind the gate compared to scotty)...
Yes, you can. But also, just as a note, singletons provides `SomeSing PieceT` and `SomeSing CoordT`, which does that for you :)
Since you mention some Racket experience, you might want to look at [ClojureScript](https://clojurescript.org/). You can use [Lumo](https://github.com/anmonteiro/lumo), a standalone CLJS environment for node.js (no Java or JVM Clojure requirement) to work with the language locally in a REPL, get a feel for it and the Javascript interop, and then put the knowledge to use by using it to compile front-end clojurescript. Clojurescript makes an interesting functional alternative to JS for a few reasons. You get lisp macros, it's *strongly* typed (unlike JS) while still being dynamically typed (like JS), and you can do some awesome [interactive programming](http://rigsomelight.com/2014/05/01/interactive-programming-flappy-bird-clojurescript.html), where your code changes update the running browser code on-the-fly. Plus, either by using clojurescript+node or Clojure on the JVM, you have the option to use the same language on both front- and back-end, without needing that language to be javascript. The negative to it is that most ClojureScript information focused on the cljs part, and less on the javascript itself. It isn't necessarily a blocker (I learned Clojure with no Java knowledge, for example) but means you'd need to do a bit of extra legwork to figure out the JS side when needed. Still, it's not like JS is a particularly complex language. It just has a lot of dumb gotchas, most of which are hopefully avoided by using something like clojurescript or purescript. I'd say check out purescript and clojurescript, spend a little time with them, and see if you can figure out if either one suits you. Clojurescript might be slightly easier to pick up this way than with purescript (due to static vs. dynamic typing), but either one could be a good fit. They seem like the best matches for getting a decent functional language that's usable in place of writing javascript directly, even if there's a bit more learning curve involved due to lacking JS knowledge. 
Heh interesting criticism. I read that book a few years ago and I guess didn't really get it. I think you just out into words what my problem was then. I'll have to check out those other two books mentioned in this thread 
Web stuff is usually just something that needs to be done for me so I'd use Elm.
Partial functions are those which do not have to return a value at all (not even `()`). For instance, there is `head :: [a] -&gt; a; head (x:_)=x` in the standard library. `head []` does not return a value (technically, I think it returns bottom/`undefined`). Contrast this with `drop1 :: [a] -&gt; a; drop1 [] = []; drop1 (x:xs)=xs` (I forget if `tail` has this behavior, and am too lazy to check). Now, `drop1 []` *does* return a value. This is an important distinction. If we want to forbid `head []`, we should have done one of three things: (a) return `Maybe a`, (b) thrown an actually useful error, or (c) used a `NonEmptyList` type. The advantage of (a) and (c) should be clear: we cannot run into an error if the code compiles. The advantage of (b) is in that we have explicitly written that we do not want to consider `head []` (rather than forgetting it) and that we can use this when `head []` should not be callable at all (i.e. for when it is too hard/impossible to define a `NonEmptyList` type or to construct a suitable member for the situation at hand). This difference between Elm and Haskell is essentially that Elm *requires* you to take one of the approaches (a)-(c) and will fail to compile code that leaves any case out when pattern-matching.
If I'm understanding you correctly, the difference is that if you do not handle all cases in a pattern match, then in Haskell you will get a warning, while in Elm you will get a compile error?
&gt; Comparing Scala to more proper FP languages, I think that the comment is spot on. What about the fact that Scala is better suited for advanced FP than any other popular languages, besides Haskell? By that, I mean that it combines higher-kinded types and type classes (via implicit parameters). [This allows](https://github.com/scalaz/scalaz) all the [same patterns](https://github.com/typelevel/cats) that make Haskell so general and powerful. I don't know of any similar capabilities in F# or OCaml (at least until it lands implicits – and when/if it does, it's going to take a long time to build a comparable ecosystem), let alone Clojure. Discarding Scala because it's not as elegant as ML-based languages and because its flexibility can be abused seems really disingenuous.
&gt; If you'll allow it, I'll move the goal posts. Scala has no redeeming features in: &gt; * FP. The points in the video stand (plus more I could give) Allow me to quote my [above comment](https://www.reddit.com/r/haskell/comments/6uyzwe/interesting_managementlevel_insights_into_fp_and/dm7uffu/): &gt; What about the fact that Scala is better suited for advanced FP than any other popular languages, besides Haskell? &gt; By that, I mean that it combines higher-kinded types and type classes (via implicit parameters). This allows all the same patterns that make Haskell so general and powerful. &gt; I don't know of any similar capabilities in F# or OCaml (at least until it lands implicits – and when/if it does, it's going to take a long time to build a comparable ecosystem), let alone Clojure. &gt; Discarding Scala because it's not as elegant as ML-based languages and because its flexibility can be abused seems really disingenuous.
&gt; And yes, I'd rather write in java than in scala. As someone who had to write equivalent code in both language, I find this claim coming from a functional programmer really hard to believe. Are you really saying that you'd rather program in a language without ADTs/pattern-matching, without any sort of useful type inference and without standard immutable collections? – heck, Java 9 does not even have _**tuples**_. You have to write out types everywhere. You are forced to use a statement-based imperative style riddled with return statements and try/catch blocks, and lambdas are a pain to use. I know Scala can be intimidating, but it's much easier and nicer to use than it may seem. Plus, it has all you need to carry over your FP practices.
If you just want to get the job done, use jekyll. It's not based on Haskell, but it has more features (compared to Hakyll) and easier to use. 
 The primitives are written in C. So whatever timing safety you get for C, you get for Haskell. There are other areas where you can use Haskell's typesafety to your advantage. (See http://hackage.haskell.org/package/raaz-0.2.0/docs/Raaz-Core-Types.html) We also use some theory (semi-direct products) to refactor some pointer tasks. https://cse.iitk.ac.in/users/ppk/research/publication/Conference/2016-09-22-How-to-twist-pointers.pdf So I believe, by careful use of theory and practice a lot can be done in a Language like Haskell. But of course the proof is in the pudding and this pudding is only half cooked. 
I forgot to mention about memory locking. Raaz has a rather simple way of handling locked memory. That too is described in the paper I quoted. For more details you can check the haddock documentation. https://hackage.haskell.org/package/raaz-0.2.0/docs/Raaz-Core-Memory.html And see its use in say the interface for CSPRG https://hackage.haskell.org/package/raaz-0.2.0/docs/Raaz-Random.html 
Strange, I'm more likely to want to be friends who talk like that, and I don't seem to have much trouble understanding that sort of speech. I suppose it's simply a cultural mismatch for other people.
and is there a tutorials for ClojureScript which assume no knowledge of JS? I do not want to spend to much time on the web staff and learn JS first? :) - I will google now, but maybe you just know off the bat
Yes. You can still easily write partial functions, but they're partial because of Turing Completeness rather than because of some trivial omission.
This looks cool! Alas it would seem I have no idea how to get multicast working on Windows, I'll have to try it on my Linux box sometime later.
&gt; I'm wondering if I can play on that game board at run time now Been wondering the same, especially how one will satisfy 'Played' constraint to be False, considering that each move is performed at runtime (after gathering input) so typechecker doesn't have the possibility to build that `Played` type statically. For me, bridging that gap between the type-level guarantees and then allowing to use it at runtime (which most 'real-world' programs do), is something that's often tricky for people learning advanced type techniques, and it's something that completes the picture. After all, it would make sense to encode some domain/business requirement at type level, and then exposing some UI, whereas compiler will guide the one implementing the UI to satisfy all constraints, to perform all needed validation runtime. Is it something that's doable with current state of dependent typing in Haskell? After all, the `Played` constraint does not look like something I can explicitly fullfill by passing runtime value (produced by some validation check). Any thoughts, /u/ChrisPenner &amp; /u/mstksg ?
I mostly use Clojure, so I can't think of anything cljs-specific to suggest. Any generic Clojure tutorial or book, such as www.braveclojure.com, will get you started with the language itself to the point that you can do stuff in the REPL or make basic scripts and the like. For webdev-specific work, I know there are libraries with that in mind, but it's not how I use Clojure so I'm not sure what to suggest. I know that on the JVM side there's stuff related to hosting content, and for clojurescript there are wrapper libraries that take JS tech like React (a virtual DOM) and lispify it so that you don't have to interact with JS directly much (if at all), I just don't know what to suggest for reading material. You could try asking in /r/clojure, some of the regulars there work primarily with clojurescript. 
I opened an issue on GitHub for another missing law.
Don't drive a nail with a jackhammer. If it doesn't need anything extra, stick with raw HTML and CSS. You will spend a lot of time fixing problems that don't need to exist.
I mean blaze HTML just outputs a Text value I think so I guess there is no reason you couldn't do that. But it's probably not the best approach most of the time. 
Too right. Though that often doesn't stop JavaScript from trying :/
I know java, I do not know scala. I'd rather write java when i'm forced and haskell when I can (which is 100% of the time anyway) than ever waste my time learning yet another over engineered language that will be dead in 10 years. I already learned a lot of dead languages over more than 25 years of my career (delphi, powerbuilder, foxpro, php, visualbasic and many more) 
This is actually a pretty "solved" thing. You'd have to prove `Played` at runtime, which is pretty straightforward to do using singletons: sPlayed :: Sing x -&gt; Sing y -&gt; Sing bs -&gt; Sing (Played x y bs) is generated automatically. Then you can just run it on x/y/board you want, and then pattern match on the result. If it's `STrue`, then the `Played x y bs` is `'True`, and if it's `SFalse`, then `Played x y bs` is `'False`.
I use GHCJS. My main worry is that people say it can never get into mainline.
Yes the server is the program which runs and accepts requests. So there is no such thing as a true static HTML page, in the sense that there is always a server. It just might be a server already set up for you that you ignore. Because HTML pages don't know how to become http responses without some help. But yeah there is a chance they already have a very minimal server that only allows static files as input. Hopefully there is a way to run custom code and bind to a relevant port; so definitely look into that. 
So I guess my (pretty naive) approach was to always infer the most predicative type possible. And never pull impredicativity out of thin air. So you need to start out with some explicit impredicative signature, but from there you could carry that signature around in the form of storing it in a type variable automatically. This (if possible) would solve the `$` / runST special case and would also make undefined work universally as a stub. 
I haven't tried it out. Have heard people do use it though. In what way is it behind the gate, I'd love a quick summary of the differences between the two. 
I mean it generally makes you come off as a bit full of yourself and generally doesn't bode well for having a good sense of humor or being able to laugh at yourself. I value those kind of things personally. 
or simpler ... you can still write infinite loops, so this is perfectly fine in Elm: stupidHead : List a -&gt; a stupidHead xs = case xs of [] -&gt; stupidHead xs x :: _ -&gt; x What you get with elm is (as mentioned above) that you have to write full case expressions and that the core/base libraries don't include stupid things like `stupidHead` - the functions there work with `Maybe a` instead (see [List.head](http://package.elm-lang.org/packages/elm-lang/core/5.1.1/List#head))
oh it has monads (just like every other language supporting generic types) - it just don't give you any syntactic sugar or a way to express the concept in code. but `andThen` is present almost everywhere ;)
I am a huge fan of this podcast, but this was the weakest ep - better listen to earlier ones, if you haven't yet, they are really great : )
What do you mean by that? GHCJS is used plenty in prod. 
GHCJS and either reflex or [react-hs](https://github.com/liqula/react-hs). 
I mean all programming is stuff that needs to be done. If you spend more than a small portion of your time doing it may as well do it right. So to me that isn't a great reason to use Elm, now there may of course be other reasons to use Elm.
I personally find writing even raw HTML via blaze-html more productive than writing it directly. I mean you can build reusable, powerful and flexible components very easily.
I'm one of the maintainers of network-multicast, drop me a line if you manage to reproduce the error :)
Spock and scotyu are quite similar but you get more with spock out of the box. For example session management, type safe routing, database connection management, csrf protection, etc. It's a bit weak on the tutorials side, but if you are persistent enough it is manageable.
Cool, this is starting to give me an intuition for that whole singletons thing. This, and also the great answer on SO: https://stackoverflow.com/questions/45235710/haskell-singletons-what-do-we-gain-with-snat/45236774#45236774
Im afraid of the complexity in Scala that stems from its multiparadigmness. In my experience, it a long allows bad code to be written, it will at some point end up in the codebase. All languages allow bad code to be written in them, but Scala makes it very easy. You say only Haskell is same/more powerful/generic than Scala. But I see PureScript, Frege and Idris as other languages that provide this kind of power.
OK, thank you, good to hear.
You can let the compiler do all the hard work, if you give your stacks a polymorphic tail. Then your `&lt;.&lt;` is just regular function composition! https://gist.github.com/sjoerdvisscher/ae6c334a6df921fdf20f807332be1137
It's mostly syntactic sugar, and it's used widely enough that most haskellers would be able to read it easily. I've used it several times where it has made things simpler/cleaner. It's not quite a language feature (and is just syntactic sugar) so it's kind of a light-weight addition to any project and can be used internally without much trouble.
Just throwing this in here: zipWith3 validate lines (tail lines) (tail (tail lines)) Edit: aaaaand... That was supposed to go to the top level
&gt; Im afraid of the complexity in Scala that stems from its multiparadigmness That's a very valid concern. It's all about tradeoffs. OCaml chooses to have a completely distinct (but quite alien and underused) OO subset, and Scala chooses to reduce the number of concepts and merge both FP and OO, which makes it that much harder for the compiler (because it enlarges the valid design space) but also makes it somewhat more natural and elegant, in a way. &gt; In my experience, ~~it a long~~ [if a language?] allows bad code to be written, it will at some point end up in the codebase Another valid concern. But note that the original question was about how Scala is worse than other FP languages such as OCaml and Clojure. Both of those languages are at least as lenient as Scala with respect to allowing untracked mutation, so singling Scala out for this does not make sense. The fact is that Scala actually makes it easier than OCaml to adopt pure FP because of the aforementioned capabilities for type classes and HKTs. &gt; I see PureScript, Frege and Idris as other languages that provide this kind of power These are all promising, but I don't think any of them is remotely comparable to Scala or Haskell in terms of industry adoption (and Scala itself is a relatively niche language, compared to mainstream languages). That's what I meant by "popular languages," although I admit that this formulation is subject to interpretation.
&gt; I'd rather write java [than learn Scala] That's your prerogative. But the statement you made above ("I'd rather write in java than in scala") is misleading because it makes it look like you have an informed opinion on the matter, which you do not. 
Just a note: `chunksOf` does something different than what op seems to want. Specifically, it will generate something like `[A,B,C],[D,E,F],[G]`.
`drop 1 lines` and `drop 2 lines` will avoid spurious exceptions.
This would certainly be true for many uses of `tail`, but I don't think it avoids any exceptions here. Try `zipWith3 undefined [] undefined undefined`.
I stand corrected!
&gt; Would it make sense to write tests using single-cell 'channels', i.e. a simple MVar, where the 'send' function is implemented like Yeah, I think that would work. You could get richer interference by having each `MVar` store a list of messages, and introducing threads which drop, re-order, or duplicate messages when scheduled. That's effectively what I did when I was playing around with it. &gt; Anyway, if I get the lib up and running, would you be interested to collaborate on finding what can be done wrt DejaFu-or-similar testing of it? Sure, that sounds great. Ping me when you do.
Give a try to CleverCloud, you get a 20€ coupon at signup. It's a heroku like that supports a vast set of platforms including Haskell natively.
We build our Yesod apps with `stack build --docker` and then include the binary in an Docker image which has all required runtime dependencies baked in.
We are using it in production at fretlink.com (and we are hiring ^^).
This is a great question. A while ago I was writing a webapp and trying to decide whether to use my favourite Clojure, or to try to do it with Haskell as a learning exercise. Yesod was an obvious choice but the ease of writing &lt;h1&gt;hello world&lt;/h1&gt; in Clojure for Heroku made the decision for me. Clojure's been superb for what I wanted, but if there's a Heroku-like service for Haskell I will give it a go for the next one. Certainly functional programming works very well for webapps.
I'm from the Clever Cloud team and I'd be happy to help you. Don't hesitate to email me clement.delafargue@clever-cloud.com if you want more information or need help :-) I've put up a small video where I deploy a scotty web app, but it works the same with yesod
Ah, I misread the example, sorry.
Nice!
I understand that stereotype. I think it really comes down to whether a person can "switch it off".
I've also found that deploying haskell applications with docker is by far the simplest method.
Thank you, it's a great platform! I'll shoot you an E-Mail, because I there seems to be something wrong with my build. :)
`String` and `Text` are different types. You can convert betwern them with `Data.Text.pack` and `Data.Text.unpack`, and very often, these problems can be solved with the `OverloadedStrings` pragma.
&gt; The operation which transitions from IO to STM has type Ran STM IO () instead of Ran IO STM ()! Hunh, I read `Ran STM IO a` as lifting from `STM a` to `IO a`. Am I off here? The order makes perfect sense to me, as `Ran` is a sort of a profunctor: newtype Ran m n a = Ran { runRan :: forall r. (a -&gt; m r) -&gt; n r } class Profunctor1 p where dimap1 :: (forall x. m' x -&gt; m x) -&gt; (forall y. n y -&gt; n' y) -&gt; p m n z -&gt; p m' n' z instance Profunctor1 Ran where dimap1 f g (Ran h) = Ran $ \i -&gt; g (h (f . i)) 
For the record, I added some steps in the Arch Linux wiki to get your own cabal-install that allows you to link statically as usual: https://wiki.archlinux.org/index.php/Haskell#Using_Cabal_with_static_linking It's pretty straight forward after all and it's better than nothing :-/
Hey guys, I just added these steps in the Arch Linux wiki explaining how to get your own copy of cabal-install that allows to use static linking (the usual mode). It's pretty straightforward. I hope it helps for other people that was frustrated with this situation in Arch, specially when you develop in Haskell. Edit: new link as I changed the title https://wiki.archlinux.org/index.php/Haskell#Building_statically_linked_packages_with_Cabal_.28without_using_shared_libraries.29
Although, in this case, the `OverloadedStrings` extension wouldn't be sufficient because they are trying to treat the same binding as two different types.
Indeed it seems like the issue in this actual post is a result of a common misunderstanding of the capabilities of OverloadedStrings (/u/bss03)
Say you've read the lines into a list, `xs`, then you can get the sliding window with `zip3`: λ let xs = ['A'..'F'] λ zip3 xs (tail xs) (tail (tail xs)) [('A','B','C'),('B','C','D'),('C','D','E'),('D','E','F')] λ map (uncurry3 isValid) it [True,False,True,False] Or you can compute the validities directly with `zipWith3` λ zipWith3 isValid xs (tail xs) (tail (tail xs)) [True,False,True,False] For higher `n`, you can use the `ZipList` newtype wrapper for lists in `Control.Applicative` to craft a sliding window function: λ let isPeak = \a b c d e -&gt; a &lt; b &amp;&amp; b &lt; c &amp;&amp; c &gt; d &amp;&amp; d &gt; e λ getZipList $ isPeak &lt;$&gt; ZipList (drop 0 xs) &lt;*&gt; ZipList (drop 1 xs) &lt;*&gt; ZipList (drop 2 xs) &lt;*&gt; ZipList (drop 3 xs) &lt;*&gt; ZipList (drop 4 xs) [False,False] At this point, however, it might be easier to use a custom sliding window function, and map over the result of that λ import Data.List (tails) λ :{ consecutives :: Int -&gt; [a] -&gt; [[a]] consecutives n = map (take n) . dropRight n . tails dropRight :: Int -&gt; [a] -&gt; [a] dropRight n xs = zipWith const xs (drop n xs) :} λ consecutives xs ["ABCDE","BCDEF"] λ map (\[a,b,c,d,e] -&gt; a &lt; b &amp;&amp; b &lt; c &amp;&amp; c &gt; d &amp;&amp; d &gt; e) it [False,False] 
Also interested in participation. Can u count me in?
Out of curiosity, have you tried using the `stack image container` command for building the images?
I think Elm is a great intro to the pure FP languages in general. I am mostly a backend developer. After coding in Elm for few weeks I cannot stand Java/JS/what-else any more, to the point where I've started looking for Elm-like for the backend. I am studying Haskell now I am trying to figure out how to smuggle it into the system I work on (just like I did with Elm :-).
You can make intero work for front end code as long as you use something like `ghcjs-base-stub`
No. What does it do?
how to resolve it? 
I'll approach your question from a slightly different angle: instead of simplest I'll go with cheapest/most fairly priced that I've used so far: https://nearlyfreespeech.net I love their pricing model that you only have to pay for what resources you actually use: CPU, memory, storage, bandwidth. This way, for some of my low-traffic Yesod apps I only pay as little as around 0.50 USD/month/site. Yes, you read that right, half a dollar per month. That's about an order of magnitude less expensive than most other popular hosting providers usually starting around 5 usd/mo for their "nano"/"pico" offerings. "How can they stay in business this way?", you may ask. Simple: their prices actually reflect their expenses to a great degree + a small margin: no matter how little or a lot you use their services they are not losing money, but they actually are turning a profit. So I have great hope that they'll stay in business even longer than they already have (15 years so far, since 2002). Next question coming up in your mind may be: "Why am I typing up all this advertisement for them?" Well, I do wonder partly about that myself too. Because I generally have quite a disdain for advertisement myself. [It's not due to affiliation][1], that's for sure. I guess there are multiple motivations of mine at play: - I like their service and philosophy. - I haven't seen them being mentioned in haskell circles often, and more usage of their services from haskell users means more resilience for my websites too in multiple ways: - if something were to go wrong that is haskell related I could ask more people if they ran into similar issues and more likely get a response if more people use it with haskell. - And preemptive resilience too: more likely to find answers online searching for "nfsn" + "haskell"/"yesod"/"servant"/etc... - I enjoy their pricing model so much that I want others to know that such a business model is possible, and maybe inspire them to have a similar business model as well. Some of us are entrepreneurs and build businesses on the web for a living, I presume. Now that the 'sales pitch' is over, I actually want to mention some thing which I consider to be the biggest downside so far: - For the moment, they only support FreeBSD. I made this work by setting up a small headless (no window manager) virtualbox FreeBSD instance locally where I can push some code and have `stack build` it inside for me. Then rsync the binary + artifacts out, then onto the NFSN server. I've mostly automated this by now with a little shell script, so generally all I need to invoke is `./bsd-build.sh v0.9.2` and wait about 10 minutes for the new version to be live. There are so many things that could be improved on this front, so maybe others could help? E.g. as a start I'd love if I could transmit only the difference of what has actually changed in the 30MB yesod executable, since rsync re-transmits the whole file even if only a single bit is different. Anyone any ideas? Also, I'm developing on Linux, so this discrepancy has caused the most friction for me so far with their service. I greatly hope that sometime they may start offering Linux hosts as well. I'd love if I could just build locally without a VM. For which there may be some hope, since among their "realms", `Ubuntu1604lts - General-purpose Ubuntu (Beta/Current)` popped up some time ago. I tried selecting it, but once I do I'm unable to SSH into my server. I get: &gt; $ ssh accountname_sitename@ssh.phx.nearlyfreespeech.net &gt; An error prevented your access at this time. If this error persists, please contact support for assistance. &gt; Connection to ssh.phx.nearlyfreespeech.net closed. (Or maybe cross-compilation could be also an option?) So, this may be a great example of an area where we could benefit from information sharing if more of us are using it. I see no reason why we couldn't push down this to about 1min/deploy, or perhaps even faster. And with all that said, if the best is the status-quo so far, and someone requests it, I'm happy to share the hacked-together shell script too. **TLDR**: At NFSN, with the current status quo, you can host a low-traffic Yesod server for about 0.5 USD/month, if you are willing to wait about 10 minutes for a deployment and do it in a locally set up FreeBSD VM. And maybe this situation will improve--that's my hope at least. [1]: https://www.nearlyfreespeech.net/about/faq#Affiliate 
No, this is just stone age and wrong for my task. It has too much repetitive work without any clue where I made a mistake! for example I have a I have a bib file with ~100 references, so you want me to make it into html, by hand? and sort it to?
.+ for that. and as far as I can tell, blaze-html is not a hummer. It is very obvious what html will be generated + it gurantees some error will not happen, like closing a tag in wrong order or something. And of course you can remove repetitive parts with Haskell :) Moreover, there is a program which generates blaze-html haskell from existing html.... that is awesome!
I am wondering if there is a runtime cost associated with the usage of `fix`: ticking pg = unlessM (Progress.isComplete pg) $ do threadDelay $ 1000 * 1000 Progress.tickN pg 1 ticking pg -- With fix ticking pg = fix $ \go -&gt; unlessM (Progress.isComplete pg) $ do threadDelay $ 1000 * 1000 Progress.tickN pg 1 go The second form has the advantage to flag explicitly the function as recursive (which if apply as a convention would arguably improve the readibility of the code)
No, I've been thinking of changing my current workflow to use multi-stage builds instead of `stack --docker`. Would you recommend against that?
Based on some configuration values in the `stack.yaml` file, it will use `stack build` to build your executables, and then create a runtime Docker image that includes those executables, as well as things like static files. You can see an example of this in the yesodweb.com repo: https://github.com/yesodweb/yesodweb.com/blob/master/stack.yaml
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [yesodweb/yesodweb.com/.../**stack.yaml** (master → a87e394)](https://github.com/yesodweb/yesodweb.com/blob/a87e39414cc5a00ff35e9d51dcd3c8a1336da645/stack.yaml) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dm8ok0i.)^.
Depends; what does your current workflow look like? There are a few common workflows: 1. Build your code inside your `Dockerfile`, which results in a runtime image that includes your dev toolchain. This is necessary for languages like Python or Ruby, but is suboptimal for compiled languages like Haskell 2. Build your code on your host OS, and then package those executables into a Docker image (either via a `Dockerfile` or using `stack image container`). This works _if_ you're using a compatible version of Linux as your host OS 3. Build your code inside a Docker container, and then grab the resulting executables and stick them into a new Docker image I personally lean towards option 3, which can be handled with (IIRC) `stack image container --docker`. There are lots of similar ways to achieve this, and if you need some special stuff done inside your Docker image, it may make more sense to use a `Dockerfile` (which is more flexible than `stack image container`).
Someone who read all of HPFP told me today that it's focused on theory and doesn't introduce I/O until almost the end of the book. As for myself, I'd rather a book show me how to make some interactive programs sooner than later, because that kind of tinkering is more interesting. Give me the theory after I'm already playing around with the basics.
I'm also using option 3. There is a fourth option that I would like to explore: - Use a multi-stage docker file. I would look something like this: https://gist.github.com/lorenzo/087d73d38f8fe4088e60dd8e88ed0122 Haven't had the time to try it, but I'm using this technique with other languages such as golang
Thanks Max! This level of presentation was helpful for me, and relating (har) it to parametricity was too.
It doesn't explain how to build a project statically :x
&gt; One benefit of the less classy title is to perhaps make the Haskell community look more approachable and friendly. Uh, to whom, precisely? There are lots of people (myself included!) who don't find this "approachable" at all.
I would advise [[Lucid]](http://hackage.haskell.org/package/lucid) over Blaze. Lucid is basically an (IMO succeful) attempt to improve Blaze.
OK, I was totally unaware of this feature in Docker. That looks really nice. I probably won't switch away from my current `stack image container --docker` build approach since it works for me already, but next time I have a more complicated workflow, I'll definitely give it a shot. Thank you for the info!
This heroku buildpack works flawlessly for me: # create a Heroku application with custom Buildpack heroku create --buildpack https://github.com/jackarthurm/heroku-buildpack-stack.git git push heroku master 
That cabal-installed that you get following those steps will build all libraries and executables statically (which is the default with Cabal). And just to clarify, when I mean statically, I mean from the perspective of Haskell code. It won't build pure ELF static binaries, it just means it won't use dynamic Haskell libraries, which is something that it was enabled in Arch Linux and some people don't like, like me, specially when you use cabal for you own code. So, we are just replacing the packed cabal-install in the repos. I hope that explanation helps.
&gt; A class of types that are *functors*, essentially types that provide a mapping or “piercing” operation. The `map` function can be viewed in different ways: [...] "piercing"?
Recently in my personal code, I use `(&gt;)` and I **like it** &gt; *braces for impact*
I should replace “piercing” with “lifting” there. (The original reason I used “piercing” is because I like the metaphor of pushing a function “through” the `f` layer, but the terminology is non-standard and more confusing than helpful.)
Purescript. [Elm is not a very well-designed language](http://reasonablypolymorphic.com/blog/elm-is-wrong). GHCJS, well... All this talk about a single maintainer, hardcoded directory paths that don't work in the Windows version, having to deal with Nix, etc, makes me think it's not worth the trouble. Look, I know there are people who like hacking at the computer's internals, building their house of cards *just right* so that basic functionality works, but ain't nobody got time for that. Purescript Just Works. Now, with Purescript you're going to have to deal with NodeJS and the tooling clusterfuck that implies. In particular, the Purescript library ecosystem relies on Bower, a tool [**whose own developers are urging people to abandon**](https://github.com/bower/bower/issues/2298). So YMMV. 
Let me know if you encounter any issues with using the Heroku buildpack! https://github.com/mfine/heroku-buildpack-stack#usage
I'm currently using sqlite as on-disk key-value store. How is the performance of Haskey compared to sqlite?
I create a static Haskell executable and then deploy it with [Hapistrano](https://github.com/stackbuilders/hapistrano). I added the non compile feature to Hapistrano to specifically support this workflow.
In a non-strict programming language, composing functions from right-to-left makes more sense because that is the evaluation order. 
You can't write an abstraction that works for all monadic structures
[There was a thread from a couple of days ago on a similar subject that might be worth looking at](https://www.reddit.com/r/haskell/comments/6w5y2d/what_are_your_deployment_workflowstools/)
How is the Purescript community managing the Bower issue? Are they migrating to something else?
Nice! If you don't mind sharing: How do you deploy services on machines after setting them up with terraform?
My favorite thing about `flow` is that it goes in the same direction as `&gt;&gt;=`, so you can write stuff in ghci from left to right. I've written [quite a bit](https://github.com/awakesecurity/language-ninja/blob/master/library/Language/Ninja/Compile.hs) of code with pervasive use of `flow`, and I think it gives a pretty big improvement in readability.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [awakesecurity/language-ninja/.../**Compile.hs** (master → 6aaffea)](https://github.com/awakesecurity/language-ninja/blob/6aaffea06bb61efeb60aafa8fb383a6f70614d51/library/Language/Ninja/Compile.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dm8zrfc.)^.
I hate to see that Elm bashing article. It boils down to: "Elm doesn't have type classes." That's a valid complaint, but Elm can be a good choice in spite of (or even because of!) its lack of type classes.
I tried adding that to my ghcjs projects cabal file and building but nothing changed. I did not get any parse errors either. Do you have any example of a working config? I would be so happy if I could get it to work! 
To people more intimidated by intense academia and seriousness than they are by porn?
What are the advantages?
It's not Stone age to write HTML. You said yourself you're a novice. Learning a fancy framework before learning how webdev works is not good practice. Learn JS. This is very simple JS. Make a function that returns an item from a json array. Use the actual Dom API to do it. Don't think you're doing yourself a favor by skipping how to do this stuff and learning a framework first, or some fancy 'compiles to JS' language. It's going to be a good while before web assembly is really good enough to be an arbitrary solution to any webdev problem. Learning the Dom, and specifically what problems frameworks were and were not invented to solve, is important when choosing which tools to use.
You can encode GADTs using object algebras in a variety of languages. That don't otherwise explicitly support them. See [here](https://www.cs.utexas.edu/~wcook/Drafts/2012/ecoop2012.pdf) 
Thank you so much for your help guys! I went with clever-cloud in the end and it worked great. It's good to see that there are so many "plug and play" solutions available for yesod and haskell. 
For other people reading this, I sent him/her an invite link. I'm doing much of the communication via private messages, and unfortunately that might give off the impression that I'm ignoring the questions posted here.
When it's not getting into mainline there's always the possibility that it gets stale and unsupported.
For context, I joined the team over a year ago, as the Haskell project was just getting started. I've enjoyed it immensely. Working in Haskell is a lot of fun, I've learned *a lot* about supply chains (it's fascinating—more than I expected) and working on a strong multidisciplinary team is great.
The point of my question was not that *nobody* might find it more approachable, but that (a) I think far fewer than you expect, and (b) by making the Haskell community seem more approachable to people who dislike dry academic discussion but are perfectly down with discussing the practicalities of navigating their collection of pornographic images with their professional peers you rather clearly run the risk of making the Haskell community seem less approachable overall. Specifically, the first answer I would come up with to my question is "brogrammers", a term/set of elective affinities I don't associate with being welcoming.
What is mainline?
Other than being written in Haskell, how does this system compare to supply-chain optimization products such as [Oracle Supply Chain Planning](https://www.oracle.com/applications/supply-chain-management/solutions/supply-chain-planning/supply-distribution-planning.html)? Why did Target decide to roll their own? EDIT: Fixed Oracle link.
I mean at the system level, not the Haskell package (I rank my sysadmin skill level at a 0 on a 1-10 scale), but thank you for the offer all the same!
You are correct the final one is **not** valid. It is a separate code example, defining a function (bold to emphasize): &gt; We furthermore assume we have a **function** that multiplies out products and flattens unions into a list representation of a multiset: &gt; &gt; list :: MSet a → [a] **Edit**: Already pointed out by /u/bartavelle 
Man, seriously, pick up some haskell in a more controlled personal project setting. I helped you quite a bit with this already. If you're actually interested in learning haskell, I'll be happy to answer questions for you, but I'm not going to author your commit for you, and neither will any of the rest of these super helpful people.
They don't need to be the same kind in the `Eq` family. In fact... type family EqK (a :: x) (b :: y) :: Bool where EqK (a :: x) (b :: x) = 'True EqK (a :: x) (b :: y) = 'False Kind equality family!
Possibly their recent bad experience using out of house supply chain software in... all of Canada.
I think you'll be happy with mapAccumL, which basically maps and folds across a list at the same time, which is exactly what you want... With some stubbing... addLeaf, addBranch :: Add -&gt; Model -&gt; (Add,Model) addDone :: Add -&gt; Model -&gt; Model addLeaf g l@(Leaf _) = ... addLeaf g i = (g,i) addDone g d@(Done _) = ... addBranch g d@(Branch _) = ... -- remember you can use addMulti here addBranch g i = (g,i) addMulti g [] = g addMulti g lst = final where (g2, branchFree) = mapAccumL addBranch g lst (g3, allDone) = mapAccumL addLeaf g branchFree final = foldl' addDone g3 allDones 
This is super cool. Thanks for sharing! I'll be checking their website periodically to see if they ever add Debian.
I'm probably not much more qualified than you are to look at it, but one 'secure cookie store' for Haskell I've seen already made the mistake to confuse encryption for authentication, which I only stumbled across as a problem because of a humorous story. In short, encryption ensures you cannot *read* a text, not that you cannot *change the data in a useful* (read: adversarial) *way*. In general, secure cookies are nothing more than a random token (or selection of tokens) to tell you who this is.
I also use NFS with Haskell. Your issue with the Ubuntu1604 SSH login is probably that you aren't using bash as your login shell (not all login shells are installed on the Ubuntu realm, no idea why). I intend to write a blog post soon with more info on NFS and Haskell. Also bear in mind that the Ubuntu realm has some issues/quirks but seems mostly ok.
Neat! No idea what I’d use it for, but one to add to the mental toolbox.
Wow, really? Is it really that simple? This is exactly the kind of info sharing I was looking for! I'm off to trying your suggestion!
That's pretty much what GHC has today, you just have to box the types when you explicitly declare impredicativity.
Well, I've created a few sites with Ubuntu just now, and I can't seem to ssh into any one of them: https://www.screencast.com/t/k9QheCFeB An error prevented your access at this time. If this error persists, please contact support for assistance. Connection to ssh.phx.nearlyfreespeech.net closed. I've tried from within `bash`, and also changing my login shell from zsh to `/bin/bash`, without any difference. here is the `ssh -v` verbose output, if this means anything to you or anyone else: http://lpaste.net/3263122530080129024 And at any rate, I'm eagerly awaiting your blog post on the subject. Would you reply to me or send a private message once you do publish it so I can read it?
Back then I encountered an issue that makes building stack project failed on Heroku (I forget what exactly) ... but one of the forks has the fix 🙂
I'm going to disagree with you there. You cannot automatically carry around impredicative types, even ones initially declared explicitly. If it were truly implemented without boxing and such then the `$` hack wouldn't exist and `undefined` would be a universal stub. 
Jesus Mike what doesn't stack do :D
I don't see why you would even bring up Turing completeness. Haskell's case just puts an implicit `Debug.crash` for each missing case.
Oh, sorry to hear that - if you encounter the problem again please open an issue!
I can't give up the common comparison operators even if I compose more than I compare. It's just too weird. I wish I could use `(&gt;&gt;)` but that's taken by syntax. 
It's possible that `fix` is faster, but I'm willing to bet that it doesn't matter at all for most use cases. Also I rarely see code using `fix` "in the wild". Explicit recursion is far more common in my experience. 
If you like Elm but miss Haskell there is always the best of both words: https://haskell-miso.org
Thanks for digging into this stuff folks! I'm learning a lot myself! I'll hopefully have some more time to fiddle with this stuff soon!
yup :P whoops
I think spock fills in the gap of "the low barrier to entry of scotty without the 'toy' framework limitations". By "behind the gate", I mean it came out a bit after scotty and didn't quite have the momentum / userbase that scotty picked up, even though it seemed technologically superior (in terms of extensibility and performance at the time, I haven't followed either in a while though). personally i've moved on to servant but I liked spock as a beginner. it's true there aren't quite as many tutorials, but it is really lightweight/intuitive to pick up.
What do you mean by toy framework limitations? Scotty seems just fine for anything you want to do. 
That's just my impression at this point since I haven't kept up-to-date with Scotty/Spock functionality in a while now. these presentations may be useful for a bit more depth: https://www.youtube.com/watch?v=-b-Oz6y-n_Y https://www.youtube.com/watch?v=kNqsOBrCbLo
No, no, I meant *with* boxing.
But that's not a "just have to..." in my view. That's a pretty darn big annoyance, since it means undefined isn't a universal stub and `$` is a huge hack. 
Tikhon, perhaps it's worth mentioning that Target only hires within the US? 
We've just recently figured out how to work with people outside the US and India. I don't know the details—I understand it's some complex setup involving a contracting relationship through some intermediary, or something like that. All I know about it is that it took a team of people in HR and legal to set everything up :/. I don't know what kind of restrictions there are, but we definitely want to talk to international candidates.
I'm not too familiar with other offerings, so I can't give a detailed reply. I think it's a combination of two things: the people who *did* have experience with third-party tools didn't like them and we want a lot of control over the system. This is a recurring theme: we also prioritize open source tools over commercial alternatives for the same reason (control). Ultimately, it means the company is willing to invest time, effort and money up-front to build exactly what we need as a business and, crucially, to build up deep expertise in-house. I'm pretty confident that we can build a powerful tool that works better *for us*. Target's also had a lot of trouble in the past integrating systems from different vendors. We're working closely with the operations teams and the teams building our new IT infrastructure (order management, warehouse management... etc) so that everything is perfectly integrated from the getgo.
Because trying to avoid partial functions in a TC language is impossible... just because you match all cases of a sum type does not mean your function can't be partial. We do have languages that properly avoid partial functions, so it's not a moot point.
&gt;Elm is said to be purely functional, and easier than Haskell.. and web development is one of my niches. Elm doesn't really have the level of abstraction or the libraries to do traditional FP. It's definitely "functional-inspired", but it goes beyond traditional FP in some ways. &gt; I wouldn't want learning Elm to make Haskell any harder. Maybe at some point I could gradually get some Haskell in on the back end as I start to understand it more. I don't think it would. Haskell and Elm are both good in very different areas - while programming in Elm can be unnecessarily constrained, it never encouraged me to do the wrong thing. &gt; One of the problems I have with this Haskell journey is that the tutorials are very general, or if it is specialized, your fair share of Haskell knowledge should probably already be there. I'd recommend taking a proactive approach to learning Haskell. Move away from tutorials and consider books or reading example code. Unfortunately, the community is small enough that tutorials are not as forthcoming as they are for, e.g. Python. In my experience, the Elm community is in a similar position. 
&gt; Elm is strict (like most languages you probably already know) which should be easier to reason about to start with. I've never heard a solid reason behind this. I always found lazy languages easier to reason about, and I assumed it was just a question of familiarity. Does strictness enable better static analysis? 
I'm still making my own coffee every morning. But look out for version 1.8.
Of course you can. If it works on free monads, it will work on all monads. 
Miso is brilliant, but it's not "the best of both worlds". Elm's totality checker is basically mandatory, while Haskell's needs to be enabled in every project. And setting up projects using GHCJS projects is also harder than Elm's packaging. 
Eh, I think the criticism goes deeper than that. Not only does Elm not have typeclasses, there isn't anything on the horizon to fix the problems typeclasses fix. 
It's not painless, but I added the google closure compiler to my build system and the JavaScript shrunk immensely. Went from 2MB to 8kB for a browser version of a language interpreter. 
Well, Elm's design choices are subtler than this comment implies. I happen to still think they're wrong, but they do have positive sides as well (e.g. library APIs are simpler without typeclasses) &gt; having to deal with Nix, etc, makes me think it's not worth the trouble. Nix is really not that bad. You have to learn it, but that's the only real downside. 
I think in this context they mean that strictness makes it easier to reason about runtime performance whereas I suspect you are talking about it being easier to reason about eg. the correctness of the code.
For me the main difficulty with Haskell's laziness is knowing when not to use it (to control speed and memory usage). I think that from a correctness point of view strict and lazy languages have similar static analysis capabilities. The added power of laziness is the ability to represent infinitely recursing data types. I really like this but I don't always need it. Maybe I have used the terms "reason about" a bit loosely as it usually refers to mathematical reasoning, whereas I intended it more as "thinking it through". By the way, Elm does have some opt in laziness by using specific libraries such as [lazy-list](http://package.elm-lang.org/packages/elm-community/lazy-list/1.0.0/Lazy-List).
The GHC repository's `master` branch.
What office loc is this based out of?
Oh ok. Have never seen that term before. I wouldn't be so worried about that, it's used enough in prod that it really can't be abandoned. Too many people rely on it.
Impressive work and such a cool project! Can't wait for the documentation about macros.
That was incredibly hard. The [result](https://github.com/JeffreyBenjaminBrown/digraphs-with-text/blob/6ff1b4813f4cf35c0c76c4e5846a9783173a0b22/infreq/plans/nested-edit-commands.hs) is indeed prettier, and I believe more maintainable, than it would have been. Thanks! Some of the best documentation I found on mapAccum was in the Haskell wiki, on a page named, appropriately, "Blow your mind".
Yes, that's why I sign the cookies, too :). I learnt a lot about what different cryptographic techniques actually do; I suppose that's one benefit of having to use cryptographic primitives instead of higher level APIs. Currently, if it can't verify that a cookie's signature is legit, it silently drops it. That might not be the best way to do it, but it's enough. Any suggestions apart from “log it” would be useful. If this was a library for getting and setting cookies (not just a middleware to transparently encrypt cookies set by *any* library), I could make the cookie getters return a wrapped result, with one type value meaning “could not verify” or something, but I'd much prefer it was a transparent middleware that you could plug in and it works with any high level cookie API you like.
The team's pretty distributed. We have people working out of offices in SF, Sunnyvale, Minneapolis and Bangalore, along with people who are entirely remote.
Just check the core and see if the two approaches generate different core. Pass `-ddump-simpl` to GHC.
Removing type-classes just complicates things in another way. Now your data structures like Map and equality functions (`equal :: a -&gt; a -&gt; Bool`) are voodoo magic akin to dynamically typed languages. Type-classes clarify and statically check the things you're going to implicitly model anyway. I'm saying this as someone who spent a few extra weekends implementing type class resolution in [Duet](http://chrisdone.com/toys/duet-delta/). It was a buggerance but well worth it. I think that the way GHC and its imitators have implemented type-classes as a user interface is just one way, and there can be other, more newbie-friendly alternatives devised.
The old Golang deployment method eh? :) It is both a blessing and a curse that we can just slap together a full binary with barely any runtime dependencies and run it wherever. I'm guilty of using this method to bootstrap new projects, but I do try to pivot them onto a more principled system as soon as I can.
Good answer, thank you. Duplicating type class hierarchies has me worried but everything else looks great
As a side note, classic ML languages use a variant of `(x)f` for types, e.g. `int option`, which is identical to `Maybe Int` in Haskell.
I think a lot of your complexity may come from the way you are ordering things. You said that order matters, but you did not specify in which order the insertions are done. This is most naturally a right fold, and it's simple if you do it that way. It means that the insertions from a `Branch` will go from right to left. But that is anyway likely to be the best way to organize your program. add (Done _) m = m add (Leaf x) m = insertLeaf x m add (Branch xs) m = foldr add m xs
When you need to drop the cookie, provide a different signed cookie or a signed header reporting that in a verifiable way. Otherwise, an attacker could make it look like the cookie was invalid when really it was valid, possibly triggering wrong behavior that could exploited by the attacker.
I'm interested in joining!
Who knows, now that `Applicative` is a superclass of `Monad` we can co-opt `(&gt;&gt;)`
Could you give us more details about what you want to do and what you can do on the machine "you have no control over"? I assume you have an Haskell source code in a `Main.hs` file, which depends on `Data.Vector` (i.e. the `vector` package). a) Can you build your Haskell program on your local machine and just send the resulting binary to the remote machine? Most of the time it will work. b) Can you install [stack](https://docs.haskellstack.org/en/stable/README/) on the remote? AFAIK you can install it in a custom directory. Then you'll be able to build a complete haskell toolchain for your program and compile / execute it with its dependencies.
Thanks. It's probably worth mentioning the [AUR package](https://aur.archlinux.org/packages/stack-static) for statically-linked stack as an alternative to installing from upstream.
Changed as the recommended way of installing Stack, thanks!
You should probably discuss that with your master thesis tutor, or if you're not doing one, your professors in the topics you're interested in.
Details added. People have asked them on the forums to add Data.Vector but they don't seem interested. On the bright side, I just noticed they have Data.Array available, so maybe I'll just have to make do with that.
You can find the source code on hackage. Might have to modify it a bit to make it work within your Main module, but it shouldn't be too hard. 
Have you seen how much code is in Vector, and the number of modules involved? It's way over the top and will definitely be more trouble than it's worth.
It'd be a little bit difficult. The names in vector shadow many names of prelude, so you can't really use them without changing them. I'm guessing there's no magical automatic way to construct such a module either, considering this is a rather niche requirement (basically only useful for course work and online programming contests).
https://github.com/nomeata/hs-all-in-one might work, if `vector` is all pure Haskell.
Nikita Volkov's record library is really great: https://nikita-volkov.github.io/record/
Well, yes. That's why people don't do it. But it is technically possible.
Looking at the [example code](https://github.com/lexi-lambda/hackett/blob/master/hackett-demo/hackett/demo/web-server.rkt#L113), it looks like macros are written using Racket's existing [syntax-parse](https://github.com/lexi-lambda/hackett/blob/master/hackett-demo/hackett/demo/web-server.rkt#L9), so you can look at [the documentation for that](http://docs.racket-lang.org/syntax/stxparse.html). There was also a [presentation about syntax-parse](https://www.youtube.com/watch?v=L1vlGeP-6rE) at the FP meetup I co-organize in Montreal.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [lexi-lambda/hackett/.../**web-server.rkt#L9** (master → f472859)](https://github.com/lexi-lambda/hackett/blob/f472859cfc03086d39563e5c0eb81dcb2ceb49dc/hackett-demo/hackett/demo/web-server.rkt#L9) * [lexi-lambda/hackett/.../**web-server.rkt#L113** (master → f472859)](https://github.com/lexi-lambda/hackett/blob/f472859cfc03086d39563e5c0eb81dcb2ceb49dc/hackett-demo/hackett/demo/web-server.rkt#L113) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dma1xne.)^.
&gt; rsync re-transmits the whole file even if only a single bit is different Eh? `rsync` only sends the differences. Maybe I misunderstood your description.
I'm also not familiar with the details of the commercial offerings. But here's what I do know: * I know someone who works at Oracle Demantra, a unit that develops just one of a number of components that make up Oracle's supply chain offering. This unit alone fills an entire large building. They employ a significant staff of really top mathematicians and research computer scientists, besides their engineering staff. And from what I hear, it's an efficient and well run operation; its size is not because of waste. * I once went to a conference sponsored by Microsoft's Cortana unit, which is powered by the same kind of mathematics that is behind modern supply chain. The speakers were mostly from academia, but also a few from inside Microsoft. The mathematics is really cool, and the predictive power of this stuff is more than amazing, it's... creepy. But it definitely takes a solid team of mathematicians to be able to apply this stuff. I know Target is a big company, but it just doesn't seem like it would make sense for them to reinvent this huge wheel themselves. And if they're still living at the turn of the century and not doing this stuff, it's clearly an error of possibly existential proportions. If what you're doing is just the Target-specific business logic wrappers for existing work, that makes sense. Of course the third-party vendors will also offer bespoke solutions, but Target is big enough that it might make sense to do that part themselves. But if you're not working with one of these vendors for the underlying engine, something sounds wrong here.
Great thanks!
Let me get this straight, you're saying that: 1. You can run Haskell code on that box 2. That you have no control over the box That's what they call a pair of logically inconsistent statements. Now I won't entice you to root that thing, but heck a shell should be doable.
For the record: this is a terrible idea, and OP should not do it, but it would be a lot of fun. &gt; Can you install stack on the remote You can theoretically insert some Template Haskell code into a script which will download Stack, write your source code to a temporary file (either by downloading it from somewhere else, or by embedding it in your script), then running `stack tmpfile.hs`. Depending on how the resulting executable is intended to be run, you may need to futz around with building a binary and copying it to the right location. This sounds like it could be a really fun challenge. That said: using the array package instead seems like the right call.
Good to hear that. I certainly rely on it.
Not every research university will have someone doing FP (at least in the US).
I'd assume there is a reason they do not expose the `vector` package if this is a Coursera course. The exercise must be possible to do without it.
The reason I try not to do "fire and forget" anymore, in any language: presumably you're doing some asynchronous action when you'd fork the handler (logging to kafka, writing to a DB, deleting a file from disk, etc). If, for some reason your forked action were to become slow (or ingress rate exceeded processing capacity for some other reason), then the forkIO'd threads can pile up, using unbounded amounts of RAM. If you use a bounded queue, you can make a policy decision about what to do when the producer races ahead of the consumer: you can shed load (drop logging requests on the floor), block (delaying incoming requests until the logjam clears), or push back on the producer so that they back off. Snap currently just `forkIO`s when it accepts new connections, and I've been thinking about changing that lately. Bounding the ingress is an important part of keeping distributed systems stable.
`String` is a type alias. `IsString` is a type class. `[Char]` is a type. What exactly are you hiding?
That's not really true. You can't run arbitrary code, you can run what the compiler will accept. GHC can be locked down pretty hard, especially when you are only allowed to provide Main.hs. It's trivial for the compiler to add a custom Prelude and for a pre-processor strip out any imports and language pragmas that aren't whitelisted. 
On the other hand, I don't think using the Trojan Stack exploit will work if the submission machine is sandboxed properly to avoid network access.
How does this compare to https://github.com/input-output-hk/stack2nix?
Utrecht University and Radboud Nijmegen are very big on Haskell.
Consider [whileM_](http://hackage.haskell.org/package/monad-loops-0.4.3/docs/Control-Monad-Loops.html#v:whileM_).
So basically, if I'm reading this paper right, a closure is equivalent to: data Closure a = forall u. Closure u (u -&gt; a) where the function in the second argument isn't allowed to have any free variables? So it might be manipulated (in a hypothetical Haskell-like language without real closures) like this? cFmapFun :: (u, u -&gt; a, a -&gt; b) -&gt; b cFmapFun (u, ua, ab) = ab (ua u) instance Functor Closure where fmap ab (Closure u ua) = Closure (u, ua, ab) cFmapFun a &lt;$ _ = Closure a id cApFun :: (u, v, u -&gt; a -&gt; b, v -&gt; a) -&gt; b cApFun (u, v, uab, va) = uab u (va v) instance Applicative Closure where pure a = Closure a id Closure u uab &lt;*&gt; Closure v va = Closure (u, v, uab, va) cApFun _ *&gt; b = b a &lt;* _ = a cBindFun :: (u, u -&gt; a, a -&gt; Closure b) -&gt; b cBindFun (u, ua, acb) = case acb (ua u) of Closure v vb -&gt; vb v instance Monad Closure where _ &gt;&gt; b = b fail s = Closure s error Closure u ua &gt;&gt;= acb = Closure (u, ua, acb) cBindFun
There is no "bower issue" that needs immediate fixing. It doesn't work in the JS ecosystem, because they require nested dependencies to work. PureScript needs flat dependency resolution and as such bower is perfectly suitable. It hasn't grown features for years, but that's fine because it does the thing we need it to do. That being said, there are alternatives like psc-package (using a model more like stackage) or using nix as a PS package manager.
I had a roughly similar thought when it came to type safety for moves, but implemented differently. data Player = First | Second data PlayerS (p :: Player) where FirstS :: PlayerS 'First SecondS :: PlayerS 'Second class ValidPlayer (p :: Player) where type Next p :: Player playerS :: PlayerS p nextS :: k p -&gt; PlayerS (Next p) instance ValidPlayer 'First where type Next 'First = 'Second playerS = FirstS nextS _ = SecondS instance ValidPlayer 'Second where type Next 'Second = 'First playerS = SecondS nextS _ = FirstS class Game (g :: Player -&gt; k -&gt; *) where data family Move g :: Player -&gt; k -&gt; * getMoves :: ValidPlayer p =&gt; g p s -&gt; [Move g p s] makeMove :: ValidPlayer p =&gt; g p s -&gt; Move g p s -&gt; (forall t. g (Next p) t -&gt; r) -&gt; r This basically uses the same trick as the `ST` monad to ensure you can't use moves where they would be invalid.
As always, comments and questions welcome. AMA and all that. This post is intended to be more of a general reflection of how things have gone so far from an inexperienced developer perspective.
You could get in touch with a Haskell/FP professor like me.
&gt; Otherwise, an attacker could make it look like the cookie was invalid when really it was valid, possibly triggering wrong behavior that could exploited by the attacker. I'm not sure I quite understand what you mean. If an attacker tampers with the cookie to deliberately have the cookie dropped, why is that a bug? Couldn't they just remove the cookie even without tampering with it? Isn't it possible to drop cookies anyway? What attacks become possible with dropped cookies? Isn't it a user's right to remove cookies? Are you thinking of a situation where a man in the middle tampers with the cookie to have it removed? Even in that case, couldn't he just remove the cookie entirely without needing to tamper with it? Isn't this a problem that should be solved by using HTTPS? If I decide to solve this cookie-side, is your suggestion a sufficient mitigation/solution, since a man in the middle could just remove the cookie directly, meaning it never gets to my middleware to even be able to read the tampered result? Security, man… it's hard.
I'm definitely down for that. Do you know what might be a good one to cut my teeth on, or- a nice resource? 
You can use dist/maintainer clean instead of validate to reset the tree without running tests. Doesn't buy you much since you still have to rebuild it all though.
Yup. I think you could *almost* (ab)use this to roll your own unboxed closures: data UnboxedIntClosure a = forall (c :: TYPE IntRep). UnboxedIntClosure c (c -&gt; a) runUnboxedIntClosure (UnboxedIntClosure c f) = f c let f x = I# (x +# 1#) in runUnboxedIntClosure (UnboxedIntClosure 1# f) Edit: this seems brittle; I can cause a segfault by making the fields of `UnboxedIntClosure` strict. Dunno if that’s a bug or if I’m inadvertently doing something Deeply Evil. But `UnboxedTupleRep` doesn’t work (“The type `c` is not an unboxed tuple, and yet its kind suggests that it has the representation of an unboxed tuple. This is not allowed.”) and I don’t know if/how you can make them representation-polymorphic, because “a representation-polymorphic type is not allowed here”: data UnboxedClosure a = forall (rep :: RuntimeRep) (c :: TYPE rep). UnboxedClosure c (c -&gt; a) 
This looks fascinating. Can you please give some links that will make it possible for someone who knows Haskell and some math but not Coq to be able to read and understand your post? Thanks!
&gt; Nixpkgs repository already maintains some version of the Hackage snapshot predefined, but we need packages of particular versions from Stackage. So the logical outcome would be to create Stackage snapshot for Nixpkgs. IIRC, the Haskell package set in nixpkgs is derived from Stackage. Though there's no way to say "give me the nixpkgs for lts-xxx." But I suspect these Stack -&gt; Nix tools would be better if we just used that existing machinery for deriving `haskellPackages` from Stackage to build a repo that contains a generated `haskellPackages` for every LTS. No need for per-project generation if you have the entire package set defined somewhere.
I'm just saying that a request that never had a cookie to begin with and a request that had an invalid cookie is two different cases. If the middleware silently drops invalid cookies, the application can no longer distinguish between those two cases. We are not specifying any details of how the middleware is being used - it is supposed to be general. It is not hard to come up with scenarios where that loss of information will make it difficult for the application to behave properly.
Agreed. And it would be quite cool if one could write something like `haskell.packages.stackage.lts-9.1.ghcWithPackages (p: with p; [lens text mtl ...])`
I definitely don't think that should be in nixpkgs proper, but it'd be good to have that in a Nix library somewhere else
And how would one go about importing it if it was in a library? I don't think I've had experience with that part of nix yet.
oh, right! I noticed it too and wanted to try it once. If I'll ever get something serious written in Haskell, I'll try it. One last practical topic I'd like to grok is monad transformers...
oh, right! I noticed it too and wanted to try it once. If I'll ever get something serious written in Haskell, I'll try it. One last practical topic I'd like to grok is monad transformers...
We are a small research team in Newcastle University, where we use FP in hardware design. Happy to help with any questions -- just drop me a message.
It actually used to work like this, where you could write haskell.packages.lts-6_0.ghcWithPackages but support for it was [removed](https://github.com/NixOS/nixpkgs/issues/14897#issuecomment-224564140) when lts-7.0 came out because it was bloating the nixpkgs set. 
 import (nixpkgs.fetchFromGitHub { ... })
No idea then. Maybe post in the NFS forum, the guy who runs the service might reply. Don't hold out on my blog post ie don't expect it to be any good or contain much meaningful info. I'm not that knowledgeable.
I don't think you can store kind-polymorphic data in an ADT, because Int#, Float#, and Double# are two different sizes, maybe three, and they may also be a different size from pointers to boxed values.
Posting to the forum sounds like a great idea! Thanks for reminding me of that.
I recommend O'Rielys Real World Haskell for a solid, down to earth introduction to the language. However, I strongly recommend getting started by installing stack, and using their new project template to get a sandbox set up to play around with O'Reilys lessons. Unfortunately, RWH is a bit dated, so you will find some stuff after the initial syntax referring to code that may no longer compile quite right, but by the time you get there you'll probably have enough experience to know what's failing and how to research a solution. Learn you a Haskell also makes a solid tour of basic syntax and language features, but it's not as practical. The recently released Eta tour also does an excellent job introducing a newcomer to the language. The freenode irc channel and the Haskell questions subreddit are excellent resources if you get stuck. I recommend not trying to set up anything like an IDE while learning the basics, currently the ecosystem is a bit convoluted and you'll spend a lot of time fighting your editor and being super confused. Once you've built a few toy programs, feel free to start branching out. Please do feel free to DM me with any questions - I would like to help out, I just think you got in over your head with this particular choice of project and you will get more bang for your buck starting small.
I'm using this buildpack in conjunction with Elm as well and it works just fine, my code is actually open source if anyone wants to take a look at how its configured: https://github.com/bsima/upsrunner
Yeah, it would be a dynamically (or at least variably) sized type, which is possible to implement, it’s just that GHC doesn’t.
Or something resembling a GADT, along the lines of: data Closure a where ClosureL :: u -&gt; (u -&gt; a) -&gt; Closure a ClosureI :: Int# -&gt; (Int# -&gt; a) -&gt; Closure a ClosureF :: Float# -&gt; (Float# -&gt; a) -&gt; Closure a -- and so on for every unboxed type
&gt;shouldn't be too hard &gt;technically possible Pick one.
A `TypeRep` is *not* a "type level value of `a`", it's a *value-level* representation of `a`'s type. It's a fingerprint of sorts, such that given a `(TypeRep a, a)` you can safely do value level checks on the `TypeRep` to figure out the type of the `a` value. Since `TypeRep a` only describes the type of `a`, it doesn't have any value information present in it. There is no "value of type `a` inside a `TypeRep a`." There are some special cases where you can recover a value from the type. These are `singletons`: types with a single value. If you know what type of `Nat` you have, then you know what value `SNat` you must have.
I mean converting a type level value to a value is pretty much exactly what `reflect` does: a :: Proxy 5 a = Proxy reflect a -- 5 But as /u/ephrion said you aren't going to be using `TypeRep` to do what you want to do. You are going to be using `Proxy` / type applications. If you want to do it for your own type: data Color = Red | Green | Blue deriving Show instance Reifies Red Color where reflect _ = Red instance Reifies Green Color where reflect _ = Green instance Reifies Blue Color where reflect _ = Blue a :: Proxy Red a = Proxy reflect a -- Red This seems like something that could be done automatically with template haskell. As far as I am aware there is no auto deriving mechanism.
How does this work? And could you give a few *concrete, simple* examples on when such reflection is useful?
If you format your code blocks by indenting them with four spaces, you will help everyone be able to read your example code.
If you look again I have edited my comment to include how to do it for your own type, hopefully that will make it more clear how this works. The only thing missing is the `class` definition for you to be able to implement it yourself from scratch. Here is the class definition: class Reifies (s :: k) a | s -&gt; a where reflect :: forall (proxy :: k -&gt; *). proxy s -&gt; a Basically the actually value is carried around in the typeclass dictionary of the type, similar to how you can grab `""` out of `String` via `mempty`. This is necessary since the type is erased at compile time, so the only way to go from a type to a value is via typeclass dictionaries which are kept around at run time (when its not possible/safe to optimize them away that is). I guess for practical uses one situation where it may come up is if you have a data structure that has a lot of compile time information (e.g heterogenous list / dict, statically sized list / vector, number with unit). So I guess for a simple example, writing the `Show` instance of a number with type level unit: data Unit = ... instance Show Unit where show = ... newtype NumU (u :: Unit) a = NumU a instance Reifies ... Unit where reflect _ = ... instance (Reifies u Unit, Show a) =&gt; Show (NumU u a) where show (NumU x) = show x &lt;&gt; show (reflect (Proxy @u)) Note that without `reflect` or equivalent we wouldn't have a way to convert the unit type into an actual `String` that can be displayed to the user.
S-expressions are "just" a serialisation format for graphs (usually trees); the parentheses are part of that serialisation format, rather than the language they're serialising. There are [many other, equivalent, serialisation formats](http://chriswarbo.net/blog/2017-08-29-s_expressions.html) out there :)
Good to know! I'll update the post with that. Since I'm working on the test-suite it doesn't help me any, unfortunately; I'll need to run all of the tests anyway.
Everyone not using the Reddit mobile app, that is. I’ve been waiting for support for *newlines* for ages now.
As an senior in CS with an interest in Haskell and FP, what sort of research would one being doing if they were to pursue this?
Those don't clean entirely thoroughly. For that minty fresh taste, copy `mk/build.mk` (and anything else you don't want to lose) somewhere else, and run git clean -ffdx git submodule foreach git clean -ffdx By the way, if you want a really *gentle* clean, try deleting `compiler/stage2`. That will start your broken stage 2 build over from scratch without messing up your stage 1 build or library build. It can save a lot of time.
A couple chapters of Benjamin Pierce's book should be enough to get the ideas. I'll add a link to it in the post: https://softwarefoundations.cis.upenn.edu/current/toc.html To actually write this thing, the only references I used were: * Certified Programming with Dependent Types: http://adam.chlipala.net/cpdt/ * The standard library: https://coq.inria.fr/library/index.html * The standard library's source: https://github.com/coq/coq/tree/master/theories * Some scraps of code I found on Github and Stack Overflow
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [coq/coq/.../**theories** (master → 2146219)](https://github.com/coq/coq/tree/21462199ad335f44aa74f3746f805b2694eba650/theories) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dmb3of1.)^.
Thanks, edited. I meant TypeRep (a::k) -&gt; k not TypeRep a -&gt; a
Right, but actually I want something like Typeable (x::Color) =&gt; Reifies x Color which again can be done using some ugly boilerplate, but it'd be nice to get it automagically. Especially since the read/show hack works, which means that the complier has most of the information it needs.
&gt; There was no high level documentation really detailing how the codebase works from the perspective of on-boarding someone to work on it. When encountering a codebase for the first time, I always try to keep a 'diary' of everything I do and think, a sort of stream of consciousness map of my experience. Sometimes I don't do anything with it, but other times it can be a useful starting point for actually documenting the project for newcomers! It makes those first couple of weeks feel much more productive if, at the end of it, you can add a detailed readme to the project that means the next person only needs to take a few days.
~~`TypeRep` has kind `*`, so `TypeRep a` doesn't make much sense.~~ EDIT: Nvm, different `TypeRep`. 
~~I mean `TypeRep` has kind `*`, so `TypeRep a` and `TypeRep (a :: k)` both don't work.~~ EDIT: Nvm, different `TypeRep`.
Some departments advertise in the haskell communities and activity report, fwiw. It might be nice if someone created a wiki page or other resource listing graduate depts with FP research possibilities, as this tends to come up a fair bit...
So the fact that the read and show hack works (~~btw you have to change the type signature to `Read k =&gt; TypeRep -&gt; k`~~ nvm, different `TypeRep`) is somewhat of a coincidence that unfortunately won't lead to a more principled solution, without GHC explicitly adding such a thing. As said above `Typeable` converts gets a sort of blueprint or signature of a type. Even though the `Red` type constructor looks the same as the `'Red` type, they are not the same thing, so you do need some sort of boilerplate / template haskell or explicit compiler support for converting from a type to the respective value. I think similar to how we have `Coercible` and `Typeable` that GHC handles itself, it seems reasonable to have `Reifies` be possible to derive for any types created with `DataKinds`. So: data Color = Red | Green | Blue deriving Reifies It is admittedly a little inconsistent with the usual `deriving` since we aren't actually deriving `Reifies Color`, we are deriving `Reifies Red`, `Reifies Green`, `Reifies Blue`. I would personally prefer if such a compiler implemented `Reifies` used `TypeFamilies` instead of `FunctionalDependencies`/`MultiParamTypeClasses`: class Reifies (a :: k) where type Reflect :: * -&gt; * reflect :: forall (proxy :: k -&gt; *). proxy a -&gt; Reflect a
That's a good tip! Unfortunately I didn't start doing that until more than halfway through, but I'll definitely keep that in mind for future projects :)
I'm not sure what exactly the question is. You can use gold with 8.2 in the way I described in the original reddit link. But you can't give `ld-options` to stack on the command line, you have to put it into your .cabal file.
Yes, but if I set that in the cabal file, it will only link the topmost package (to which the cabal file belongs) with `gold`, right? I was curious if I can link all dependencies with `gold` (perhaps by writing something in `stack.yaml`) when building a project with a few hundred dependencies. So I can maybe speed it up this way, since that can take hours. I'm sorry if the above comment of mine wasn't very clear; is this comment perhaps clearer for you?
Could you elaborate on your use of Nix? E.g. Are you using NixOS, NixOps, etc?
Ah, yes, that clarifies it. Unfortunately you can't currently do that, according to my knowledge. 8.2 is your best bet here. Though, I imagine that it won't even give you that much of an improvement: Bulding 100s of dependencies is a task you do rarely (e.g. only when you change the Stackage LTS version, so probably every couple weeks), and library dependencies don't typically link executables (there are some library links but those are typically a small fraction of the time, even with the slow `ld.bfd`). Compared to that, your own project is linked (with full executable links) on every build, probably 100s of times per day, so I imagine that's where the real payoff lies.
&gt; library dependencies don't typically link executables &gt; there are some library links but those are typically a small fraction of the time, even with the slow ld.bfd That's interesting. And a bit confusing. I'll admit that I don't know much about how low level linking works, this guess of mine came from the fact that when I happened to find myself in a `stack`-related "unregistering-loop", and spent days having to recompile many dependencies, I looked at the process tree and I saw something like: - stack - ghc - gcc - ghc - ld - ghc - ghc - ld So it seemed like stack was running `ld` for something. Is that some different kind of linking that wouldn't benefit from `gold`? Or whatever happens there is not even linking? And at any rate, do you second my observation that even for non-executable dependencies the `ld` binary is invoked?
I trust you're not merely joking with the ADHD quips, and you're realise that you (most likely) do actually have it? This article reads more like "How to program effectively when you have ADD." Pretty solid advice! 
What about `DataKinds`? The "type" in this case maps directly to a term-level value constructor. Although, this *really* is a job for singletons (or wait until Haskell gets `π` types). 
&gt; the actual value is carried around in the typeclass dictionary of the type And with that, I now understand how half of Kmett's libraries are able to do their magic! GHC's implementation of typeclasses "leaks" type information at run-time! 
We're using Nix to manage our dependencies and build our Haskell code. We're running it on macOS (for development) and CentOS (for testing and production) and not using NixOS or NixOps anywhere.
I have quite severe ADHD and am fairly useless without my medication, yes :) Part of my problem is that I was diagnosed senior year of high school so, while I can now focus, I don't have the decade+ of coping mechanisms, focus strategies, and such that people normally build up while in grade school. I've had to spend the last 5 years or so catching up in that area. I'm doing way better than I used to, though. &gt; Pretty solid advice Thanks!
I think you're looking for [singletons](https://hackage.haskell.org/package/singletons) (and in particular, [fromSing](http://hackage.haskell.org/package/singletons-2.3.1/docs/Data-Singletons.html#v:fromSing)).
The new Typeable machinery does in fact have [an indexed TypeRep](https://hackage.haskell.org/package/base-4.10.0.0/docs/Type-Reflection.html#t:TypeRep). It's quite cool :)
&gt; (btw you have to change the type signature to Read k =&gt; TypeRep -&gt; k) I was using GHC 8.2.1's Type.Reflection, where you use TypeRep (a::k) instead of just TypeRep. But thanks for the pointers.
There are now two things named `TypeRep` in `base` `Data.Typeable.TypeRep` has kind `*`, whereas `Type.Reflection.TypeRep` has kind `(a :: k) -&gt; *` https://hackage.haskell.org/package/base-4.10.0.0/docs/Data-Typeable.html#t:TypeRep vs https://hackage.haskell.org/package/base-4.10.0.0/docs/Type-Reflection.html#t:TypeRep 
Haha I mean that is pretty much the only way to implement typeclasses. But yes that is essentially it!
In GHC 8.2.1 they've reworked `Typeable`. So in GHC 8.2.1's Type.Reflection `TypeRep` has kind `k-&gt;*`.
Sorry, I should have explained what mapAccumL does more fully.
But dictionaries are an implementation detail, no? Not a fundamental property of typeclasses. But at an abstract level, simple (multiparameter-)typeclasses map (sets of) types to concrete, run-time/term-level functions. Since the constant function is also a function, it follows that you can use typeclasses -- no matter how they're implemented -- to map types to run-time constants. Combine this fact with `DataKinds` and, voila! You can obviate the need for singletons (in a sense). The trick is that, *in order to work* typeclasses have to somehow carry around type information at run-time -- they're an innocuous way to circumvent type erasure. I'm just thinking out loud here. I didn't realise any of this stuff until a couple hours ago. 
Oh weird. They should really give it a different name. 
That is rather confusing. Perhaps an `I` suffix for indexed or something would be nice?
Is the old typerep deprecated? If not that's a rather confusing thing to add with the same name. 
I mean the fact that it is a dictionary of sorts specifically is an implementation detail. But particularly since things like polymorphic recursion are permitted (and very useful) and thus we don't always know the exact type at runtime; there isn't really an alternative to either using dictionaries of sorts or something equivalent. And to clarify typeclasses map types to term level values, there is nothing requiring them to be functions, e.g `mempty`.
Do F# and OCaml have powerful enough type systems to actually implement such compile type verified GADTs via that approach?
I don't think OCaml has GADTs either. I could be wrong though. 
Haha funny to find this post now. I did another rewrite of the library. This time with dependent types (HList) to get typesafe signature descriptions and an object algebra to abstract the syntax a bit more: https://github.com/aka-bash0r/flatline It's in a really raw state right now, though.
I'm pretty sure it's not deprecated. And yeah, it is kind of confusing that they gave it the same name.
&gt; And to clarify typeclasses map types to term level values, there is nothing requiring them to be functions, e.g `mempty`. Ah yes! I was confused by the fact that class Clazz a where q :: a is allowed, but class Clazz a where q :: b is not allowed, without the help of `AllowAmbiguousTypes` (and `TypeApplications`).
Why do you need to rerun all the tests? Can't you just rerun a subset? especially if you didn't change any code. I'm quite surprised you need to do a clean rebuild so often. The build system is quite good at doing partial rebuilds. In fact I usually only rebuild stage2 after an update. I only do a clean rebuild for a validate.
Just a general suggestion for git: It's perfectly fine to work on multiple things on the same branch. In fact that's how I usually work these days. especially if the changes depend on each other. Git's commit granularity is at the hunk level, which means you can commit only part of a file as well, or revert only part of a file. my workflow usually is: 1. make everything in one branch 2. squash the commits 3. reset head soft to undo the commit but leave all the changed `git reset --soft HEAD~1 &amp;&amp; git reset HEAD *` 4. create different commits for each change using `git add -p` and `git commit -m`, with the occasional amend if I screw up. you can also stash something partially `git stash -p` or undo changes partially `git checkout -p`. once I have everything split up I use interactive rebasing to modify the commits. e.g. `git rebase -i master` and the commits I want to edit I set to the `edit` instructions. Git will then stop at those commits, you can edit them and continue. to submit pathes or update one on phab, again, interactive rebase with edit on the commits I want then when it stops there simply `arc diff HEAD~1 --update D&lt;number&gt;` which will send the contents of only that commit. This simplified commits with dependencies A lot, as using branches gets unwieldy very quickly. 
Nixpkgs only maintains some version of Hackage packages. This one is focused on generating the Stackage LTS packages set with additional packages from Stack config.
That depends on both you and the candidate supervisor. Maybe you have an idea, a problem or an area you'd like to work on. Then you can bring that up and/or you look for a matching supervisor. Academic researchers typically have a webpage with their publications and topics of interest (e.g., [1]). There you can see what they are working on. Another good source of inspiration are the webpages of academic conferences like the Haskell Symposium [2] and the ICFP conference [3]. They give you a wider overview of active research topics and who is doing what. [1] https://people.cs.kuleuven.be/~tom.schrijvers/portfolio.html [2] https://www.haskell.org/haskell-symposium/2017/ [3] http://icfp17.sigplan.org/track/icfp-2017-papers#event-overview 
&gt; Nixpkgs only maintains some version of Hackage packages. `stack2nix` does not use the haskell package set from `nixpkgs`. My understanding is that it generates a nix expression describing all the dependencies required from the stackage snapshot. Not sure how it handles `extra-deps` or packages from git.
As a fellow first-time-GHC-hacker for my master's thesis, I hear you :) Documentation for the [build system](https://ghc.haskell.org/trac/ghc/wiki/Building/Using) and the [test suite](https://ghc.haskell.org/trac/ghc/wiki/Building/RunningTests/Running) can be found on its Wiki pages. The different flags for the testsuite might be most interesting to you, `TEST="Txxxx"` and `THREADS=8` in particular. I totally understand your gripes with the build system: I had my problems with it, too. Sometimes things were completely broken for whatever reason, so that I needed to do a complet fresh check out. /u/davidfeuer's suggestions might have helped, though (which probably amounts to a fresh checkout). Things were even weirder for me since I develop on Windows (I know, right?!), where you measure success in the diff of tests broken, because a clean build already breaks some tests. I'm hoping for [hadrian](https://github.com/snowleopard/hadrian) to make things more bearable.
That seems to be exactly what `stack2nix` does, e.g. here is the code for generating nix expressions based on your `stack.yaml` by calling `cabal2nix` https://github.com/input-output-hk/stack2nix/blob/afb187bbe1921dc6e57a50c0f98546d40e1431b5/src/Stack2nix/External/Stack.hs#L52
&gt; because a clean build already breaks some tests. This isn't an issue with the build system. This is purely an issue that people don't test on Windows before they commit. Those with commit rights tend to push their patches before the Windows bot even finishes, because of the unpredictable run order of harbormaster. The current status is that Harbormaster builds and runs the tests but doesn't report back errors. This was because of one failing tests during the release window of 8.2, which was likely a configuration issue on the bot. It was disabled with the intention of re-enabling it after 8.2 shipped, the 8.2 deadline slipped and things went from 1 intermittent failing test to a whole bunch of failing tests again. This was never looked into again and so new issues pop up now because commits aren't rejected unless they actually break the build. Windows had 0 test failures at one point. But I've given up trying to keep it at 0 without CI support on phab. No, Hadrian won't fix this. replacing harbormaster might. Also I am genuinely surprised you need to do full rebuilds so often. If you're working on stage2 you should be able to pin stage2 and builds should be faster. Aside from validation, there is very little need to do a full bootstrapping build usually. There are a few exceptions, for one submodule updates tend to require a larger rebuild (unavoidable). Or if the system gets confused with interface file changes (also unavoidable depending on what you're working on). but in 90% of the cases `make re2` should be fine.
Of course, you never _need_ `vector` to do anything. It's usually just _better_ to do it with vector.
Great post! Stuff like this should be on Trac, and linked from someplace that newcomers will be sure to notice it. Add a bit of context at the top: date, GHC version, what you were working on, so when people read it years later they'll know which grains of salt to add.
&gt; 8.2 deadline slipped Ah, that makes sense. &gt; No, Hadrian won't fix this. replacing harbormaster might. Right, I was more hoping to get hadrian to replace the makefile-based system. Also because it's awfully slow on windows due to the mingw emulation layer (yeah, there's `make -C ghc -j8 fast` or something along those lines, but that's quite a mouthful). &gt; full rebuilds Mostly, I don't. But it's the 3-5% of cases where I somehow broke my build and incrementally have to figure out why. E.g., start with `clean`, `distclean`, `maintainerclean` and finally (very rare) delete the folder and checkout again. As you also mentioned, interface files were an issue for me, because I fiddled around with something similar to strictness signatures. &gt; `make re2` Cool, never heard of that. Should've read the Wiki page more carefully! Is `make 2 -j8` like `make -C ghc -j8 fast`?
I am learning Haskell, and from time to time I will wirte some code to test, but it seems I can not define function in ghci directly Prelude&gt; foo :: Int -&gt; Int &lt;interactive&gt;:1:1: error: Variable not in scope: foo :: Int -&gt; Int currently I define the function in a file and load them in ghci, I am wondering if there is some better REPL approach
Nixos lets you manage your entire OS declaratively using the nix language. Configuration management is built in. This means you can reproduce your entire install from a file (along with all the benefits that entails). For instance, you can manage things like network config, docker installation, users, systemd services in nixos but not in nix. Nix I've found really handy when, for one reason or other, I've had to work on stable (read old) linux distros and want software from this decade. It's still a useful tool for most user programs.
Mingw isn't an emulation layer, it's native that's why we use it to build ghc. Likely you mean MSYS2. But the problem isn't that, any fork/exec heavy build system on windows will be slow due to the overhead of creating processes on windows. Shake would definitely help there, but we'll still be behind as bfd on Windows is still very slow. Much slower than on Linux. But hopefully that changes soon. No make re2 or make 2 instructs it to only rebuild the stage 2 compiler. Make - C ghc will instruct it to rebuild the compiler starting from stage 1. So unless you have stage2 pinned you'll do more work. It'll rebuild stage 1, and only then rebuild stage 2. This would be a lot slower. Also I build and publish nightly test results for ghc on windows https://github.com/Mistuke/GhcWindowsBuild so if you have a failing test you can just look it up there. Just searching the repo works quite well there are the file names contain the build dates. 
Thanks for the clarifications! Actually I think I mostly did stage2 builds only, but the fact that I always forget the specific incantations is annoying. `make 2 -j8` seems like a good shortcut, not sure why I haven't been using it.
AFAIK in NixOS you can declare your **whole system** configuration. That’s pretty mind blowing. Nix isn’t just the package manager in nixos, it’s how the whole OS is defined 
Is this GHCJS or? That's pretty impressive
You can freeze a particular stage by adding `stage=2` to your build.mk then make - j8 would be sufficient and you'll never accidentally build too much. Just remember to remove it when you want a full rebuild. Seehttps://ghc.haskell.org/trac/ghc/wiki/Building/Using which is full of tips on how to not waste too much time! 
In the repl, I'm pretty sure you can only declare lambdas. However, you could declare your function in an external file and load it with :l
That's what I was doing until I discovered the more 'pure' way of doing it at the command line and wondered why it was so much harder. It's probably much less that the build system doesn't work than that I find it unwieldy to use. Also, IIRC `stage=2` stopped working for me at some point on Windows, but I might have mixed things up in my head...
Hmmm I would be interested In when you do something and it doesn't work to know what it was and what you expected. I think build systems for large bootstrapping Compilers are always a bit complicated, but it should just be a matter of getting a workflow that works for you. There's not a one way of doing things because everyone works differently so you end up with a lot of options eat with caveats. So maybe a more detailed beginner guide would be useful. But I'm not sure it's worth the investment right now if we're changing build systems anyway. 
Nix on NixOS is definitely the best-supported configuration, but Nix on other Linux distros works decently well, and Nix on MacOS is serviceable. NixOS is by far my favorite distro, but I think that to really appreciate it you probably have to go through the Arch/Gentoo wringer for a few years. The more devops experience you have, the more obviously beneficial the features of NixOS will seem.
Just tried the new commands on a week old `HEAD` build (fully built). `make 2` doesn't work: make[1]: *** No rule to make target '2'. Stop. make: *** [Makefile:127: 2] Error 2 `make stage=2` works. `make stage=2 fast` does not, however (no rule `fast`), but seems like a semantically incorrect specification anyway, judging from its expansion. So no actual surprises apart from `make 2` not working, but I'll open a ticket next time I can reproduce something strange.
I believe make 2 and make re2 only work inside the ghc folder. They're not defined at the top level. 
I've used object algebras in F#, there's some roughness to to using the interface inherentance mechanism but it works. Can't say about ocaml. 
I didn't use to like Nix but NixOs sounds awesome. Thanks for the review
You can define function in ghci directly using the let keyword. ghci&gt; let foo x = 4 * x ghci&gt; foo 3 12 ghci&gt; :t foo foo :: Num a =&gt; a -&gt; a But it will fail for anything that needs more than one pattern match, as everytime you use the let keyword any previous definitions will be overwritten: ghci&gt; let myLength [] = 0 ghci&gt; let myLength (x:xs) = 1 + myLength xs ghci&gt; myLength "foobar" *** Exception: &lt;interactive&gt;:8:5-37: Non-exhaustive patterns in function myLength This fails because we have overwritten the previous myLength 0 pattern with the new one, which has no clue where to stop. But if you manage to get that function into a oneliner you are just fine: ghci&gt; let myLength = sum . map (\_ -&gt; 1) ghci&gt; myLength "foobar" 6 ghci&gt; myLength [] 0 ghci&gt; :t myLength myLength :: Num c =&gt; [a] -&gt; c
... and so I re-enact all the experiences I made some months ago. I probably did `make -C ghc 2`, which skips the stage0 and stage1 checks or w/e (each ~5s) compared to `make stage=2`. Thanks, that should help stick these details in my head for some longer.
Yes, there are 2 places during a library compilation where the linker is invoked (often through `gcc`): * When you use TemplateHaskell: When you see this in a normal `ghc --make` invocation (stack typically hides this output for you): ``` [2 of 2] Compiling PeeklerTest ( PeeklerTest.hs, interpreted ) Loading package pretty-1.1.1.0 ... linking ... done. Loading package array-0.4.0.1 ... linking ... done. Loading package deepseq-1.3.0.1 ... linking ... done. ``` * When the final `libHSyourlibrary.so` is linked (e.g. `libHSlens....so`) So your observation is totally correct. However, those links typically take a fraction of the CPU time it takes to compile the actual Haskell code (unless you use nix and suffer from [this bug](https://github.com/NixOS/nixpkgs/issues/27609)); my rough guess is that here linking takes somewhere between 10%-25%. If you just want that overhead to go away, I wouldn't put much effort into trying to replace ld in those places; instead I would wait or 8.2 to become available to you, which should automatically solve it. Especially because these two cabal bugs ([one](https://github.com/haskell/cabal/issues/4435), [two](https://github.com/haskell/cabal/issues/4439)) will probably need to be fixed first. If however you want to help fixing the problem in general, so that you can properly switch the linker instead of having to rely on GHC 8.2 defaulting to `gold` when possible now (there are big merits of being able to switch the linker at run time, such as me being able to experiment and write reddit threads like this), then helping to fix the cabal bugs is the way to go.
It is possible to define a function with multiple pattern matches, by using ;, like this: ghci&gt; let myLength [] = 0; myLength (x:xs) = 1 + myLength xs You can explicitly define its type the same way ghci&gt; let myLength :: [a] -&gt; Int; myLength [] = 0; myLength (x:xs) = 1 + myLength xs Imagine that by using ";", you're having each declaration be part of the same "let" block.
`docker build --tag alpine-ghc` will build and tag your image, such that the above stack.yaml will work correctly
Ok, I did not know that Haskell takes semicolon seperation, however that still means you have to fit your function on one line. So I will you give you the points for the less obfuscated function and keep my point for better readability.
Yeah, I thought about that, but then I remembered that there was some kind of easier multi-line feature in ghci, and with some research, I found you could use {: and :}, like so: ghci&gt; :{ ghci| let ghci| myLength :: [a] -&gt; Int ghci| myLength [] = 0 ghci| myLength (x:xs) = 1 + myLength xs ghci| :} ghci&gt; However, this is pretty clumsy, as you have to indent by typing spaces manually. Pressing tab triggers ghci's autocomplete feature, after all.
We use Propellor for automation and Debian packages for the actual deployment. In addition to that we also use the following: * LXD (without wishing to start a flame war there's no way I using Docker!) * Packer for building images * Terraform (soon, though I have used it in the past) for infrastructure. * Consul, for general config stuff that needs to be stored somewhere. * Vault, secrets and CA. In short you can do a lot worse than using Hashicorp stuff. It's very high quality and well thought through stuff. I like the idea of Nix but I've found NixOS as a distro somewhat lacking but I'd happily be proved wrong. The one major issue I have with it (again, I've no wish to start a flame war) is that is uses systemd and I'm just not going there. And yes I am a crusty old Linux sysadmin ;)
Holy cow. So it definitely is more practical to write it in a file than using multiline ghci (Which was my intuitive guess anyway since it allows for easier rewriting/editing of the code). But it's good to know for times when there is no other option than ghci for testing.
True. I was assuming that u/pwnita got interested to FP by following a course on it in his University.
If you don't like Nix you're not gonna enjoy NixOS.
I used NixOS for a few days before going back to Arch. As mentioned, NixOS lets you declare pretty much everything instead of configuring through commands. For example, if you wanted to start an OpenSSH server, you'd edit your `configuration.nix` file and add `services.sshd.enable = true`, then run `nixos-rebuild` and it would install the relevant packages for you and enable the service. If you remove it from your config and rebuild, it disables the service and removes the packages. Another example is editing sudoers. In Arch and other distros you'd typically use `visudo` to change your sudoers file but in NixOS you'd change the `security.sudo` options (you can see a list of them [here](https://nixos.org/nixos/options.html#sudo); you can also click on them to see descriptions and example values). It's very nice to use but the lack of documentation made me switch back. [Here](https://nixos.org/nixpkgs/manual/#sec-cross-usage) is the documentation for cross-building packages, which is very unclear to me (I think this is part of nixpkgs so it probably applies to Arch+Nix setup as well; I don't use Nix, just Arch). If I had more time to figure stuff like that out I'd definitely switch. I went through [this](https://github.com/Gabriel439/haskell-nix) to learn to use Haskell with Nix.
I believe the most widely used solution still is `acid-state`. I haven't used or researched any other library, although I do know of the existence of `TCache`.
For the life of me, I cannot figure out why Reddit hasn't been able to make their mobile app render markdown correctly. I have problems reading inline code (surrounded by back ticks), and I feel like they must have the resources to fix this.
Currently, the performance of Haskey is the worst. Lots of internal functions are not optimized at all and have a less than desirable algorithmic complexity. However, this wouldn't be such a problem, if the disk usage wasn't that bad! Currently, page reuse is not optimal yet, which leads to databases with a large free tree (the collection of free pages), as can be seen from [this analysis](https://github.com/haskell-haskey/haskey-util/blob/1449e0740fbe966fca9cfd6ab900451498e65dfc/.graphics/analysis-example.svg) which shows the types of the pages of the database after inserting 10 key-value pairs. As you can see, only one page is used for actual data, while the others are used for bookkeeping. However, in the future Haskey will outperform SQLite and other RDBMs as a key-value store, both in time and space requirements. First of all, most RDBMSs will use separate indexes for fast retrieval and querying, essentially doubling the disk requirements, while a key-value store only uses a single tree for retrieval and data storage. Furthermore, Haskey will be better integrated with Haskell, especially its concurrency infrastructure, which would eventually allow us to provide OCC instead of MVCC and user-defined collision resolutions. So for now, I'd advise you to stick to SQLite or an other key-value store, until the space management issues are resolved (which we are working on right now). But, when the space management issues are resolved, Haskey will be a viable alternative! (assuming you don't have stringent read/write latency requirements, then you'll have to wait a bit longer) **EDIT:** To show we are not messing around: we have just fixed the space issues in [#64](https://github.com/haskell-haskey/haskey/pull/64). A [new analysis](https://github.com/haskell-haskey/haskey-util/blob/52a1765d5187ea59772d37f09e1d5f75dfb8958f/.graphics/analysis-example.svg) of 10000 insertions shows off the space efficiency.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [haskell-haskey/haskey-util/.../**analysis-example.svg** (master → 1449e07)](https://github.com/haskell-haskey/haskey-util/blob/1449e0740fbe966fca9cfd6ab900451498e65dfc/.graphics/analysis-example.svg) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dmbpbmt.)^.
Apart from the wow factor what does it bring which compensate for the drawbacks it can bring. I remember trying it a few ago and giving up because configuring email was an nightmare (compare to Ubuntu where it comes out of the box). Also I could bconfogure PHP the way I wanted because some options I needed to configure weren't not exposed to mix configuration.
I tried nixos for a while and liked the idea but don't think it is a viable main os for me. My main problem was that you can decide between the stable channel (with quite a few packages out of date) and unstable channel (with quite a few packages broken). It doesn't have an equivalent of apt add repository so if a package you need would require frequent updates you have to do it manually each time. For me personally the boons of nixos weren't worth the extra work so I am back to using nix with stack where necessary.
Excellent, thanks!
Please write a blog post or something about this.
I am the author. Please ask any questions about the content here. This is my first post so suggestions are also very welcome.
My problem with nix was that it seemed to want me to use it as my shell and I really like my current shell. Hopefully nixOS is shell agnostic.
We have a script that automates `nix-build` (to build the system closure) + `nix-copy-closure` (to copy the closure to the target machine) + `nix-env --profile ... --set ...` (to install the closure)
Sounds good indeed. However,how do you do when you need something not in nix packages?
I would guess that they have vague (at least) intention of deprecating eventually, and we're "just" left with some awkwardness in the meanwhile. Should take a while...
You can safely roll back any change you make at the boot menu. There have been a lot of times at work that we bricked a system due to an update and fixed it by rolling back Upgrades are atomic, meaning that if you cancel an upgrade halfway your system is exactly in the same state that you started from. Contrast this with, say, Debian/Ubuntu where canceling or losing power during an upgrade (especially a distribution upgrade) can leave you with an unrecoverable system The system is also more secure: everything is compiled with security hardening flags, the Nix store is mounted read-only, and all dependencies are pinned to specific hashed paths so attackers cannot mutate your dependencies You never have undocumented cruft lying around on your system because your system configuration is centralized and declarative. This means that you don't need to periodically repave the system to get it back to a clean state. Similarly, there is no difference between a system upgraded from an older stable release and one installed fresh from the lastest stable release It's also just way easier to customize the system because of the standard configuration format. With other operating systems I have to read a new tutorial every time I make a new change to the system, but with NixOS I can easily search for the relevant option and set it and I'm done
Thansk you guys !
&gt; Nix on MacOS is serviceable Just an FYI, since Sierra came out, Haskell on macOS is pretty much broken. I believe small codebases still work, but once your app gets beyond a certain size it ceases to be buildable. This is the case whether you're building with Stack or Cabal, with or without Nix. We're working on a fix for this problem with Nix, but that work is not ready yet. New releases of macOS have had a pretty consistent pattern of breaking the Haskell ecosystem for years now (although it seems to be much worse with Sierra). The cost of keeping things working on macOS has gotten to be large enough that some of us at Takt have started to contemplate the possibility of dropping macOS as a company-supported developer environment and going Linux-only. :/ The jury is still out though...
In Arch? I guess I didn't use NixOS enough to come across something not in Arch's packages or the AUR.
[removed]
What do you mean by this? Nix asks you to source a script in your profile (`~/.nix-profile/etc/profile.d/nix.sh`), but does not require you to change shells. The script can be sourced directly by `bash` or `zsh`. Other shells provide compatibility tools for sourcing scripts written for `bash` - for example I have used `bass` to make Nix work in `fish` shell sessions. Or you could port the script to your shell of choice - the script is quite short.
All packages are just functions in nix's expression language. You can submit pull requests on GitHub easily enough to the main package repository, or keep your own local expressions for your own custom packages. Packing something new isn't terribly hard, but perhaps (depending on where you're coming from) takes a little getting used to.
In Nix.
Plus you can create system packages to provision multiple machines. It is like Puppet, but with better reproducibility.
Follow up question: is nixOS hard to install on a typical consumer laptop? In a scale 0-10 where Ubuntu is 0, Arch is around 5 and LFS is 10, how hard is it to get it up and running in a typical notebook?
I'm curious how you've been working around it. That's become a problem for our much smaller code base 
I'd say it's comparable to Arch, but of course, a little different. 4-5?
It wasn't part of your question, but to be clear, NixOS doesn't afford you anything with regards to the nix haskell infrastructure that can not be done on another distribution + nix. It is best to distinguish between nix (a collection of command-line tools / package manager), the nix language (a pure, lazy programming language), nixpkgs (a collection of code written in the nix language (some bash, too ..), describing mostly build instructions for packages but also containing the code needed to build, configure and run the NixOS distribution and services on top of it). I use NixOS on my personal computers. The appeal to me is that I can have a local fork of nixpkgs and treat it is as extended configuration, which together with my local configuration describes my whole system in a reproducible, deterministic way. I can change anything about my system anytime and test and deploy it often within the matter of minutes. And if I don't like it rollback safely. Or upstream it if it makes sense. I don't think I could have such a workflow with any other distribution. However I've found that I have to take advantage of this rapid DIY workflow in order to make it worthwhile. Otherwise the shortcomings (some of which will get better over time, but others are fundamental) would just annoy me. But to have this workflow one has to know at least the nix language, the idioms which are used extensively in nixpkgs and how to read nix code in nixpkgs as a substitute for documentation which is often sparse and outdated. Basically I think at this stage of the NixOS development, it makes only sense to run it if you are able to (at least theoretically) contribute and willing to fix things yourself. As a stepping stone towards using NixOS I would always recommend becoming familiar with the nix package manager and using it for while first if your current OS supports it. Because you would use it the same way if you switch over to NixOS to manage your user packages. You can develop the aforementioned skills in that environment and if you're familiar with the ordinary packages the step towards the NixOS code is a small one, it already lives in the same repository. I also wouldn't bother with the stable channels for personal use.
In general: * Pick the project that interests you the most * Breakup the problem into small pieces (e.g. for a media streaming server, find out how you can load an mp3, find out how you can send something over a network, etc) * If you know how most of the small pieces work, assemble them into bigger and bigger pieces until you have the software you want
If possible work on something you can imagine using at some point. That way it's more likely that you will see it through to the end.
Have you tried docker ?
Oh. My understanding is you'd write your own `nix` package file, similar to PKGBUILDs, I never got far enough to try it though. It seems well documented in the [Nixpkgs manual](https://nixos.org/nixpkgs/manual/#chap-quick-start), and there's plenty of examples in the [nixpkgs repo](https://github.com/NixOS/nixpkgs/).
Yes! This is exactly why I wanted to write the media streaming server. I used subsonic all the time, but only stopped because the maintainer makes you pay to let the server use mobile clients.
I use Nix+OS X on my desktop and NixOS on my servers. Tis great! I can even run `nixops deploy` on my OS X machine to deploy new builds to the NixOS servers.
Linux. Some combination of either switching to it, dual booting, or running it in some kind of VM or docker container depending on the person. Some people are also avoiding the problem by not upgrading to Sierra.
This is helpful. I guess I need to figure out how to get a Haskell program to read and play mp3s!
I did it a few months ago by following this: https://chris-martin.org/2015/installing-nixos It was more or less seamless on a Lenovo T460s
Different people in our organization have tried a variety of container/virtualization approaches. But they're not ideal...
That would be a great first step! And if that or any subsequent steps seems daunting/too big/you don't know where to start, try breaking it up in smaller problems.
Ampache has a [Subsonic API](https://github.com/ampache/ampache/wiki/API#subsonic-api) which I use for streaming, I haven't actually tried it with a mobile app though, only Clementine on desktop. There's also Libresonic which is a fork, and Groovebasin which is a decent-looking alternative.
I think the easiest way to play media with haskell is using a binding to gstreamer. Either [gstreamer](http://hackage.haskell.org/package/gstreamer) or [gi-gst](http://hackage.haskell.org/package/gi-gst). [I have written an example program for the latter](https://github.com/haskell-gi/haskell-gi/blob/a2df93f8ee03af0491f03cd8557e4c9fb60a4deb/examples/GstHelloWorld.hs). Otherwise you might want to take a look at how [hmp3](http://hackage.haskell.org/package/hmp3) works, but that is pretty old and a lot lower level.
Thanks for this great explanation. I don't really need nix in fact. I m trying to setup my laptop to be able to use Linux either as dual boot or with Virtual Box (on windows). At the moment, I only windows+virtual box+Ubuntu. One way to get the dual boot + VB working is to install the two systems and have all the shared data on a separate partition. As I want the two systems to be identical I thought about using NixOS, si I can replicate easily from the VM to the dual boot version. Then I thought the two installs could even share the same nix/store.
I want to join.
&gt;Write a media streaming server like Subsonic in Haskell and client in GHCJS or Elm or something GHCJS can be a bit painful. Not that I don't enjoy using Haskell on the frontend, but you have to *want* to use Haskell on the frontend. &gt;Write a D&amp;D 5e combat simulator/Dungeon Master helper in Haskell This sounds like it could be frustrating. Haskell makes stateful computation more difficult than other languages, while it makes things like parsers far easier. Of course, if that doesn't bother you, go ahead and push what's possible in Haskell :)
Sharing the /nix/store store should be doable. I don't think you would want to boot the same literal configuration for the VM and the native OS, as it includes e.g. hardware configuration, but you could certainly factor out the common configuration and share it. I'm not sure what you mean with you don't need nix. If you use NixOS you will probably have to use nix (at least practically speaking, I'm not aware of any attempt to make a different package manager work on top of NixOS). Unless your scope is very limited to what you want to do within the OS, I think what I said above applies. Also keep in mind that not everything is managed by NixOS, you can still have mutable configuration in you home folder (most people do). So you would want to set up some sharing scheme there, too. I heard putting dot files under version control and symlinking them works well in practice.
That segfault sounds like a GHC bug, unless you're using unsafe operations you're not showing. Does it happen in GHC 8.2.1? If so (and maybe even if not), please file a ticket.
Indeed that looks cleaner. Thanks
I would appreciate if you could stop simultaneously posting your questions on both Reddit and Stackoverflow. Give people of the respective communities some time to answer your question before you post it elsewhere. Our goal should be to maximize the number of questions that can be answered not to maximize the number of people working on a single question so you can get your answer faster.
Nice. That's my proposed solution as well.
I need to rerun all of the tests to make sure that I didn't break the test system. I'm sure I could've figured out a "complete subset" of tests but the way it's setup is flexible enough that there's a ridiculous amount of "specially designed" tests that work slightly different here and there. It's just easier to run all of the tests to make sure the test system didn't break. I've only done a clean rebuild 2-3 times so far. More often I just rebuilt stage 2 and even more often I only bothered to do that after tests randomly started failing and I couldn't diagnose the issue. I'm quite aware a lot of what I did wasn't the most efficient way to go about things. Part of why I wrote this was to put that out there so I could get feedback on how to do it better and then write that up into some documentation in a more obvious way.
Not at all. I understood mapAccumL pretty quickly, at least for mapping over a list. I can't even say what the difficulty was, really. It feels obvious in retrospect.
Unless this breaks the laziness (which I don't *think* it does), you should probably be able to get slightly better performance by swapping the arguments to `zipWith`: zipWith (+) (replicate x 0 ++ res) xs This probably allows rewrite rules to fuse `zipWith` with the (fused) `replicate` and `++`. I believe you'll get something like the hand-written go x xs where go 0 ys = zipWith (+) res ys go _ [] = [] go k (y : ys) = y : go (k - 1) ys I haven't actually tried this though.
Actually, you can intermingle channels as much as you want, if you get a little dirty with your Nix expressions. # configuration.nix { config, pkgs, ... }: let unstable = import (pkgs.fetchFromGitHub { owner = "NixOS"; repo = "nixpkgs-channels"; rev = "..."; sha256 = "..."; }) { inherit (config.nixpkgs) config; }; in { environment.systemPackages = with pkgs; [ firefox unstable.chromium ]; } This gives you the stable version of firefox, and the unstable version of chromium. Nix is sandboxed enough that you can have any number of different `nixpkgs` versions defining the packages you use. And of course, if you don't like this, you can always hand roll custom Nix expressions for any update you want.
Actually the order within a Branch can run in either direction. What I meant by order-sensitivity is that a Branch's contents have to be dealt with before it is. If you think [the result](https://github.com/JeffreyBenjaminBrown/digraphs-with-text/blob/6ff1b4813f4cf35c0c76c4e5846a9783173a0b22/infreq/plans/nested-edit-commands.hs) can be simplified, I'd love to hear how. It looks irreducible to me.
I agree.
&gt; I'm not sure what you mean with you don't need nix. What I mean, is I don't use nor really need to use nix. My goal is more to get this double dual-boot, VirtualBox things, which will involve reinstall some stuff, some being dependencies to build some haskell project. For example to use the `persist-mysql`, one need the appropriate `libmysqlclient` package installed. This leads to using NixOS (or another os), and whilst I'm there using nix (or carrying on like before) for my haskell projects. So at the moment, I'm just gathering information to decide what to do. Of course, If I use NixOS, I'll probably use Nix as well. &gt; you can still have mutable configuration in you home folder How do you do that ? &gt; I heard putting dot files under version control and symlinking them works well in practice. That's sort of what I already do. How does this relate to Nix ? Can you create symlink , download a git repo for a user in the nix configuration file ? That will be brilliant! 
I see, thank you for taking your time to write up your insight.
I think the rewrite rules fire either way, because I don't see any difference in the results of my benchmark.
They definitely don't fire the way you have it (in GHC 7.10 or later), but they may not fire the other way, or they may just not make a big enough difference to matter. Now I'm curious enough to check.
&gt; GHCJS can be a bit painful. Not that I don't enjoy using Haskell on the frontend, but you have to want to use Haskell on the frontend. It's not so bad if you use Nix, especially `reflex-platform`'s GHCJS infrastructure. If you sit on the shoulders of these giants, it actually makes the whole thing really easy, and Reflex is absolutely worth it IMO.
Oh interesting. And you get the full power of GADTs and it is all verified and erased at compile time?
A slightly different way to write the same: coins :: [Int] -&gt; [Integer] coins [] = 1 : repeat 0 coins (k:ks) = xs where xs = zipWith (+) (coins ks) (replicate k 0 ++ xs) One intuition behind this algorithm is that it is the power series expansion of `1 / ( (1-x^k1) * (1-x^k2) * ... * (1-x^kn) )` where `[k1,k2,...,kn]` are your coins. Btw this (among others) is part of the [combinat](http://hackage.haskell.org/package/combinat) library: ["coin" series](http://hackage.haskell.org/package/combinat-0.2.8.2/docs/Math-Combinat-Numbers-Series.html#g:8)
if we're micro-optimizing, they'd probably want to switch to using `foldl'` too
&gt; How do you do that ? Nothing unusual. My emacs config lives in /home/foo/.emacs.d, for example. That has nothing to to with nix or NixOS. That is just how a lot of programs are configured on unix-like distributions. There is interest in [nixifying](https://github.com/NixOS/nixpkgs/pull/9250) that, too. But for the time being that is just the way it is. &gt; That's sort of what I already do. How does this relate to Nix ? Can you create symlink , download a git repo for a user in the nix configuration file ? That will be brilliant! You can depend on arbitrary git repositories within nix expressions as long as you pin them with hash and commit. I don't think you can create symlinks inside your home folder with some configuration options (not sure, though). There is software that addresses this use case with or without relation to nix: https://github.com/NorfairKing/super-user-spark https://github.com/rycee/home-manager https://github.com/sheenobu/nix-home
Cool! First thing that pops out to me is that `solve` could cache the result of the `foldr` to create a reusable list: -- change -- solve coins target = foldr cons nil coins !! target -- to solve coins = (foldr cons nil coins !!) Now if you do `let f = solve [8,3,1,2]`, the first call to `f 1` will do more work than a subsequent `f 1`. To solve the "which coins were used problem", we can just tweak your solution to use some `Num a =&gt; a`, not `Integer`s, and pass it an embedding from `Int` to `a`: solve :: Num a =&gt; (Int -&gt; a) -&gt; [Int] -&gt; Int -&gt; a solve f coins = (foldr cons nil coins !!) where cons i as = let res = zipWith (\a a' -&gt; a + f i * a') as (replicate i 0 ++ res) in res nil = 1 : repeat 0 We can recreate the original solution using the embedding `const 1 :: Int -&gt; Integer`: λ solve (const 1) [8,3,1,2] 3 3 But if we get a little creative with what can be a `Num`: newtype Combos a = Combos [a] deriving (Show, Eq) instance Monoid a =&gt; Num (Combos a) where Combos as + Combos bs = Combos (as ++ bs) Combos as * Combos bs = Combos [ mappend a b | a &lt;- as, b &lt;- bs ] fromInteger i = Combos (genericReplicate i mempty) (-) = undefined negate = undefined abs = undefined signum = undefined We can also get the various coin combinations used with the embedding `\i -&gt; Combos [[i]] :: Int -&gt; Combos`: λ solve (\i -&gt; Combos [[i]]) [8,3,1,2] 3 Combos [[1,2],[1,1,1],[3]] 
I have now changed cons to: cons' x xs = res where res = go x xs go 0 ys = zipWith (+) res ys go _ [] = [] go k (y : ys) = y : go (k - 1) ys my benchmark is: main :: IO () main = defaultMain [ bench "solve 250" $ nf (uncurry solve ) ([1..250],250) , bench "solve 10000" $ nf (uncurry solve ) ([1..50],10000) , bench "solve' 250" $ nf (uncurry solve') ([1..250],250) , bench "solve' 10000" $ nf (uncurry solve') ([1..50],10000) ] The results: benchmarking solve 250 time 7.765 ms (7.667 ms .. 7.872 ms) 0.999 R² (0.999 R² .. 1.000 R²) mean 7.736 ms (7.678 ms .. 7.788 ms) std dev 152.5 μs (116.1 μs .. 213.3 μs) benchmarking solve 10000 time 91.54 ms (90.67 ms .. 92.05 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 91.23 ms (90.58 ms .. 91.62 ms) std dev 784.9 μs (380.0 μs .. 1.150 ms) benchmarking solve' 250 time 4.456 ms (4.422 ms .. 4.496 ms) 1.000 R² (0.999 R² .. 1.000 R²) mean 4.443 ms (4.426 ms .. 4.468 ms) std dev 62.52 μs (45.07 μs .. 95.03 μs) benchmarking solve' 10000 time 99.60 ms (98.38 ms .. 100.4 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 99.17 ms (98.76 ms .. 99.61 ms) std dev 660.1 μs (442.1 μs .. 984.4 μs)
In my experience I've been able to run small Haskell projects on RPi flawlessly but compiling the projects itself has been a nightmare because: - Building a project is quite memory intensive so you might run out of it - On ARM GHC uses LLVM which is quite heavy and takes A LOT of time I was more successfully using always these two flags: ``` -j1 -O0 ``` Run a single compilation task, otherwise you might need more than 1GB of RAM and disable optimization passes as they are quite CPU heavy. But yeah, you'll see GHC finishes quickly and LLVM takes forever to finishes generating the final binary.
The caching was already happening, it screwed with the results of my benchmarks when I did `nf (solve [1..50]) 10000` with criterion. The Num trick is very cool.
If you're in need of a GUI (in particular for the D&amp;D Dungeon Master helper), my [Threepenny-GUI][1] project may be helpful in getting you started very quickly. It works best on a local machine. [1]: https://wiki.haskell.org/Threepenny-gui#Gallery
Also, I'd love GHC to have some kind of bytecode representation that would allow to run on interpreted mode so that you could just install GHC on the RPi and run (slowly) your project without having to build the whole thing. Meaning, I'd like to try to avoid cross-compiling it.
Last I knew `nixops` didn't do very well building Linux binaries for NixOS servers on macOS. I've been using [this script](https://github.com/LnL7/nix-docker/blob/5757413132cb6ead13578c64be466fbcb77dd49f/start-docker-nix-build-slave) to create and use Docker as a build slave for `nixops`.
GHCJS wasn't painful at all for me. Pretty much just follow the instructions and start developing. I would strongly disagree with your second statement. That project would be a very doable Haskell beginner project and the state would not be in any way difficult to handle. It definitely wouldn't be "pushing what's possible" or even close to that. Haskell is perfectly good for handling state. 
That is probably a smarter way to do it -- I just have a VirtualBox instance I use as a build slave.
One option is to fork the nixpkgs repository and add to it, you can then install packages from your fork. Alternatively you can add overlays of packages to your system configuration. Or you can install packages directly from an individual .nix file that you have created.
I got started with FP in my free time during my Bachelor. I then went on and took some programming language/FP related theory courses in my master program. We don't have a FP research group here, only a group that uses term-rewriting for program analysis, but that's not quite what I have in mind. 
Nope, just doing what you see there plus: {-# LANGUAGE DataKinds, ExistentialQuantification, KindSignatures, MagicHash, TypeInType #-} import GHC.Prim import GHC.Types It manifests in 8.0.2 in GHCi but not when compiling a source file, and not in 8.2.1 in either case. I guess I’ll still open a ticket when I have a moment.
I've updated the blog post to address the questions about the differences with [input-output-hk/stack2nix](https://github.com/input-output-hk/stack2nix). Here's the excerpt: `stackage2nix` is focused on the creation of Stackage LTS packages set from [lts-x.y.yaml](https://github.com/fpco/lts-haskell) config, that can be used as a replacement for the default Haskell packages in Nixpkgs. Finally, it can override Stackage LTS with the extra-deps from `stack.yaml` Stack config file. `stack2nix` produces only final derivation. Also, it relies on external tools in runtime. It utilizes *stack* to obtain the build plan from `stack.yaml` Stack config, and *cabal2nix* to generate derivation from it.
I don't understand. What does interpreted mode have to do with that? If you want pre-built "bytecode" that you can run, why isn't that just a regular build? If you don't want to do the build on the pi, why isn't that just cross compiling?
u/catscatscat This is the blog post I was talking about.
Thanks for pinging me. I'm going to give it a read.
If you're not tied to the web from the frontend my lib [fltkhs](https://hackage.haskell.org/package/fltkhs) is pretty [easy](https://github.com/deech/fltkhs-hello-world) to get started with.
I will add my 5 cents. I am using NixOs on my personal laptop (actually it is dual boot with arch, but I use nixos 99% of the time) and arch without nix on my work machine. And I really want to try arch+nix. Probably I just do not have time to play right now that is why I have not tried it yet. So here it goes, NixOs looks like perfect os for me, but I still need to invest a little bit more time into it... If you need to add some simple package - it is simple. nix-shell is also awesome. Something which is sported by nixos devs is also awesome (like python, haskell) But I cannot make my julia packages work properly... I do not have time to solve it by myself and there seems to be no people around who would use julia under nixos :) so I use arch at work basically to use julia haha. There are some hacks... but it is not very convenient. Other than that nixos is perfect for me... EDIT: hopefully things will be better once julia have their Pkg3 
Thanks for the links
How do you go about testing if a type safe library / application (e.g. using phantom types, gadts,... to make certain states impossible) works as intended? Currently I try to enter 'invalid' expressions manually in ghci and see if they fail to typecheck but probably there is a better way? 
That makes sense, thanks for providing a comparison for the two.
difficult to tell what's going on, but I'd start with something like the below to hide the `Model` which seems irrelevant add :: Add -&gt; Model -&gt; Model add a = execState (_add a) _add :: Add -&gt; State Model Add _add l@(Leaf {}) = {{ do a Leaf thing }} _add b@(Branch as) = if {{ all Done }} then do mapM_ {{do Done things }} return (Done {{ what goes here? }} ) else {{ your f }} _add (Done _) = {{ this case was missing }} Honestly the `Add` type feels wrong (or at least not precise enough about the invariants that you want) - it seems like `f` would benefit from a newtype for retired `Add`s (they're always `Done`, right? So make a `newtype AddDone = AddDone _`) which might want their own kind of `Branch` node?
On the contrary, I find iterative algorithms using the State monad from `mtl` a pleasure to write.
[removed]
Generally, you check that by proving it correct rather than having a set of tests. If you can fully and formally specify what is valid and what is invalid, it is usually pretty simple to translate that into Haskell types which you can prove to be accurate representations of the specification.
Hmm. Maybe it was just the tutorial on nix that I found. Worth another try then.
Yeah, cross-compiling is the answers, but my impression is that it's a bit complex to set up. So I just meant some kind of CPU neutral bytecode representation I can build but *from my desktop* and then copy into the RPi the bytecode program so that ghci can run on interpreted mode without having to pull any dependency. Something like a fat .jar :P
I can't seem to reproduce it, even in GHCi 8.0.2. Please be sure to provide complete instructions in the ticket.
Right, I understand that being unable to distinguish those two cases is a bug (even though I said, “If an attacker tampers with the cookie to deliberately have the cookie dropped, why is that a bug?”, bad wording on my part), but how is it a security risk? Or how is it a security risk that we can fix, since even if we were to fix the bug itself, a MITM still has the power to drop cookies regardless?
Unfortunately there is no such thing as CPU-neutral GHC, and we're quite a long ways away from that being a possibility.
&gt; if there's a Heroku-like service for Haskell I will give it a go for the next one. I'm currently working on this. Would you be interested to receive an email when I'm done?
For `UnboxedIntClosure`, you can get somewhat more confidence from the module system than from an existential type. Anyone with a type of kind `TYPE IntRep` knows they can `unsafeCoerce#` to or from an `Int#`. And they may just do that.
I know I know. To be honest I unhappy with LLVM on ARM, not with GHC.
SICP all the way.
This, along with some idea that they'd like to add some of their own syntax extensions, seems to be the reason for calling it ETA. I'm excited about the technology, but think that the separation from GHC will just spilt the community. Even mainline GHC Haskell is not an enormous community to maintain a good package coverage; I think ETA would have been better served with a more GHCJS-like model.
You might be able to host it on [SDF](https://sdf.org/) if you're already dealing with user-land restrictions (no sudo and such).
This is really cool! I'm really curious about how macros will work in Hackett. Can you annotate your macros with types? Would this even work since macros are at compile time and so is type checking? I imagine there would be some use for being able to add type signatures to your macros. Or would it always be Syntax -&gt; Syntax and thus not be that useful?
What do you mean? `unsafeCoerce#` the `UnboxedIntClosure` and offset past the `data` tag to get to the closure? I’m assuming that the constructor wouldn’t be exported. Anyway, I don’t think it’s possible (yet) in Haskell to do what I really want from unboxed closures—namely, avoid heap allocation entirely unless you need to store closures in a container or something. Maybe some of the machinery that ships with linear types will help. This is something I’m working on in Kitten; there, I CPS-transform the call sites of functions that *return* closures: &gt; Translate a function *returning* a packed closure into one *accepting* a continuation that can accept the unpacked closure. &gt; &gt; [A → ∃c. c × (c → B)] ⇒ [∀r. A × (∀c. c → (c → B) → r) → r] Reduce functions that *accept* closures to Skolem normal form: &gt; Translate a function accepting a packed closure into one accepting an unpacked closure. &gt; &gt; [(∃c. c × (c → A)) → B] ⇒ [∀c. c × (c → A) → B] And for higher-rank polymorphism, generate specialisations for each runtime representation that’s used. So typical closures (like those passed to `map`, `filter`, etc.) won’t need to allocate anything.
Haskell runs very well on the RPi3, but as you found out yourself compiling is a nightmare. Here are the notes for one of my projects: https://github.com/blitzcode/hue-dashboard/#raspberry-pi It's probably all a bit dated now, but maybe you find some interesting bits. In the end I got a complex project building &amp; running with ~100 dependencies, all build with optimization.
BTW, you might find it useful to know that `TypeInType` implies `DataKinds` and `PolyKinds`, and that `PolyKinds` in turn implies `KindSignatures`. So you don't need any of those three extensions if you're using `TypeInType`. 
Any good tutorials/examples on using mime-mail?
Nice write up! (Although some paragraphs look somewhat crunched together on mobile?)I haven't heard of NearlyFreeSpeech, but I have a small application on a $5/mo Digital Ocean droplet. It's three containers: postgres, Nginx, servant app. I think it probably averages about 1 view per month so no need of worrying about scaling it, but it's pretty close to what you averaged per month?
Is anyone running nix on Arch? I did a bit of research and it seems like it's currently broken on Arch Linux
How do you send an email in Haskell?
I have been trying to get people to acknowledge this cost for years. Glad to hear a company is considering pushing osx out of the supported category.
Thanks, I did know that, just being explicit in this case to document what the code actually uses.
I ran into insufficient memory issues building even relatively small Haskell projects on the 5$ droplet. Did you have any tricks to doing this, or maybe you just compiled elsewhere?
Oh, I didn't think of that. I build locally (on OSX) in a docker container and scp the binary up to the machine to deploy. Then it goes into a container on the target machine. Perhaps sort of byzantine? I put all my work in here: https://github.com/erewok/simpleservantblog While I rarely work on this project, I have a ton of stuff I'd like to do with it.
This comment: -- This is currently ignored and a null IV used in its place, because we -- need a deterministic output for cookies to be removed or changed, and -- a random IV breaks that. looks worrying. Though I'm decidedly *not* a cryptographer, I believe the idea is that you *must* always use a random IV (for algorithms that want a _random_ IV, see below), but the IV is *not* secret so you transmit it along with the encrypted data. My understanding is that it's there to make known ciphertext attacks much more difficult and to defeat rainbow attacks and the like. See [Initialization Vector](https://en.wikipedia.org/wiki/Initialization_vector) on the wiki, in particular: &gt; Depending on whether the IV for a cryptographic scheme must be random or only unique the scheme is either called randomized or stateful. While randomized schemes always require the IV chosen by a sender to be forwarded to receivers, stateful schemes allow sender and receiver to share a common IV state, which is updated in a predefined way at both sides. 
See a related [SO question](https://stackoverflow.com/questions/33065965/dependent-types-can-prove-your-code-is-correct-up-to-a-specification-but-how-do). A lot of it comes down to being as clear as possible and documenting everything as well as possible. You will sometimes make mistakes.
/u/gelisam [already partially answered this question in reply to](https://www.reddit.com/r/haskell/comments/6wk474/hackett_progress_report_documentation_quality_of/dma1x2k/) /u/polux2001, but I’ll try and give a somewhat more in depth answer here. The short answer is that I don’t yet know for sure how most users’ macros will work in Hackett. I can make some guesses, though. Let me try to do so, but understand this comment is mostly just a projection of what *might* happen, not what necessarily will. **Currently**, Hackett macros are written in Racket, not Hackett. This is because Hackett code is built out of syntax objects, the same thing Racket code is built out of, and all the Racket syntax-processing tools work in Hackett, including the ever-wonderful [`syntax/parse`](http://docs.racket-lang.org/syntax/stxparse.html). For many macros, this works fine—while the macro code is written in a dynamically typed language, macros’ *expansions* are still typechecked. Many macros are so simple that this is sufficient, since there’s basically no actual code, merely syntax patterns and syntax templates (two small embedded DSLs) that are highly declarative and rather powerful. However, this is less than ideal. It means that macros that need access to type information need to call into the Hackett typechecker’s guts. Even if those guts were exposed through a public API, they would likely be a little unpleasant to use. Therefore, we have three main problems: 1. We want to be able to write our macros using Hackett, not Racket. We should be able to use our existing Hackett code at compile-time to make macro-authoring easier. 2. Macros should be able to cooperate with the typechecker with relative ease. This means it should be simple to expand macro subforms with type assumptions, and it should be possible to easily access the “expected type” of the macro currently being expanded. This is theoretically solvable with a nice API for Hackett’s typechecker, but we have a macro system. A [Turnstile](http://docs.racket-lang.org/turnstile/index.html)-style DSL would be even nicer. 3. Existing macros are operations on syntax objects, so even macros written in Hackett code would still be functions of type `{Syntax -&gt; Syntax}`. This is, of course, essentially how Template Haskell *already* works—Template Haskell `Exp`s are untyped. The newer `TExp` API provides more type safety, but is generally difficult (or even impossible) to actually use. Getting all of these things working together is, bluntly, the domain of new research. For that reason, I am eager to discuss some of them with Stephen Chang and some other Racket folks when I am at RacketCon in October. I have ideas for solutions to many of these things in my head, but they are just mental sketches, and some of them are not trivial. Hackett’s typechecker is (I believe) more advanced than anything built with Turnstile, and it has many more complications in order to get these things right. My gut tells me that this is not a problem that is perfectly solvable in general, since macros can (1) perform arbitrary code execution, (2) want access to type information of their surrounding expressions and subexpressions, and (3) want to be able to *influence* type information in some situations. This is hard, because you don’t always *know* all the type information at a particular point in the type inference process. For example, consider the following code block: (let ([x (f a)] [y (mac x)]) (x y)) What if `mac` is a macro, and it wants to control its expansion based on the type of its single subform, in this case `x`? Now, imagine `f` is the method of a typeclass that is return-type polymorphic, so the instance can only be determined by observing how `x` is used. However, `x` is applied to `y`, so we need to expand `(mac x)` to know what `y`’s type is! This is pathological, since we have a circular dependency: `mac` needs to know the type of `x` to expand, but we need to know the type of `mac`’s expansion to determine the type of `y`. If we give `mac` complete freedom over what it may do, this is simply not solvable. Macros are functions, and functions are opaque to the typechecker, so we cannot inspect the function body to determine what possibilities might occur. Similarly, we cannot make any assumptions about the type of `(mac x)`’s expansion, since it could produce *completely* different types depending on the type of `x`. However, it’s likely that we can solve specific situations in different ways. - Perhaps `mac` promises to expand to an expression with a particular type (which may even be or contain a fresh solver variable). This would allow the typechecker to continue typechecking the code with the assumption that `y` has a particular type. Once `x`’s type can be determined, the typechecker can *actually* invoke `mac` (likely as part of constraint solving), then check to make sure its expansion matches the type it promised. - Incidentally, this is likely highly related to another thing that would be very nice, which is an extensible constraint domain. That’s something I only have a vague notion about at the moment, but I’m hoping I can flesh it out with some conversations with some Racketeers who know all the dirty details of the macroexpander. - Alternatively, maybe `mac`’s expansion is restricted in some way so that the typechecker *can* inspect the potential result *before* it is actually invoked. This may mean adding some fancier types to macros, or it may involve restricting certain macros to an applicative interface rather than a monadic one. - Additionally, many macros are essentially “expression-like”, mostly just serving as abbreviations for certain expressions, and they can be semantically assigned a type. These macros are probably the 80% case, and both of the above proposed solutions would work for them, plus probably some even simpler things. These macros are already technically within reach, there’s just implementation effort to get there. This is all a little hand-wavey at the moment because many of these ideas require knowledge that only exists in my head, and it is late for me, and this comment is already growing quite long. The summary, though, is that I don’t have a precise answer yet, but I have lots of ideas, and I am confident that I will be able to find solutions that are both powerful enough to support many desirable use cases and pleasant enough for ordinary users to work with.
I read that as "Composing Coffee Interpreters"
Nice! What does the { inherit (config.nixpkgs) config; } bit do here?
The `config` parameter that is passed to your configuration.nix files is a fix-style self-reference. It is the final merging of all the `imports` in your OS config, which Nix uses as the total definition of your system. Therefore, `config.nixpkgs.config` is just the `nixpkgs.config` value that you've defined elsewhere in your config. And that field in NixOS is just the parameter used to instantiate `nixpkgs` to configure packages inside. So we give it to the unstable `nixpkgs` in order to get the same configuration in that `nixpkgs`.
In `betterLength`, when you recurse, are you still calling the old `listLength`? That is, `x:xs -&gt; 1 + listLength` instead of `x:xs -&gt; 1 + betterLength` Regarding your last question, a returned polymorphic value of type `Integral a =&gt; a` must be valid *for any such `a`*. If I have a function `foo :: Integral a =&gt; Char -&gt; a`, and I want to give the result of `foo` to a function `bar :: Int -&gt; Bool`, then `foo` had better not be lying and actually give me an `Integer` instead. Any value of type `Integral a =&gt; a` *must* be able to be specified further to be any type that has an `instance Integral`. If this isn't clear, please ask more questions.
betterLength :: Integral b =&gt; [a] -&gt; b; betterLength [] = 0; betterLength x:xs = 1 + betterLength xs; integerLength :: [a] -&gt; Integer; integerLength x = betterLength x :t integerLength [1..10] integerLength [1..10] :: Integer :t betterLength [1..10] betterLength [1..10] :: Integral b =&gt; b
I use plain and simple binary copying and have local script that copies the binary and calls remote script that does all the rest. I have ec2 instance with nginx Here is my makefile https://github.com/haskell-serbia/haskell-serbia/blob/master/Makefile and the remote script https://github.com/haskell-serbia/haskell-serbia/blob/master/install.sh 
Wow that's exactly what I'm doing. Great work identifying that even without the code haha, thanks so much! OK so what you're saying is that you wouldn't be able to use the result of Foo since Bar is expecting an Int while Integral can be other types, not just Int, is that right? Cheers! 
Thanks for the suggestion. I've tried it before but didn't realise you could host on it.
The theme is odd. Where it is bunched together it is actually a list. For some reason it doesn't show the bullet points.
(Not about Nix but about your problem.) Have you investigated the possibility to just share the filesystem(s) on which your Linux is installed between the VM and the dual boot, effectively booting the same system? If you have a kernel which can boot in both environments and enough things auto-configured on boot to paper over the 'hardware' differences between the two, then this should be doable. The big advantage would be, of course, that nothing would need to be duplicated and synced, and you could use any distro you want.
You have your answer, so I may add something about your current code, there is some redundancy in the patterns `x:[]` can be handled by `x:xs`. So you can write something like: listLength :: [a] -&gt; Integer listLength list = case list of [] -&gt; 0 x:xs -&gt; 1 + listLength xs And thinking about it, you can pattern match in the argument list: listLength :: [a] -&gt; Integer listLength [] = 0 listLength (x:xs) = 1 + listLength xs
Honestly I'm still on Yosemite (so I can still build things) and eventually I'll be forced to upgrade either the OS or the machine, at which point I'll probably go back to Linux and install NixOS, either on VM or as the base operating system. OS X has been good for video editing/screencasting software and other quality media tools, but the developer experience has been a pain at every turn. Apparently it just keeps worsening.
&gt; Have you investigated the possibility to just share the filesystem(s) on which your Linux is installed between the VM and the dual boot, effectively booting the same system? To be fair, booting and partitioning is the bits I never really understood in OS. I haven't really investigated, but from what I read, it's not recommended to boot a VM from the same partition. I'm not sure why. However,I'll welcome any help on this topic.
Maybe try to implement the Server first with a open streaming protocol so that you can use another player as client. This would reduce the work to get something useable.
Nice read! thanks!
Thanks for pointing this out. Is there any advantage over using pattern matching vs the case statement I've used?
I think it ends up as the same code (though don't quote me) but it's certainly more idiomatic Haskell. Think of it like "listLength with an empty list means 0, listLength with a list means 1 plus listLength of the rest of the list". Also, I would change the list case definition to: listLength (_:xs) = 1 + listLength xs Because the element is never examined.
Well, I haven't tried it either, so can't recommend it, though I don't currently see a reason why this should be impossible. If you want to learn how to install a Linux system the manual way (partitioning, installing the boot loader, generating a kernel and initramfs, etc.), the [Gentoo handbook](https://wiki.gentoo.org/wiki/Handbook:AMD64) is a pretty fantastic step-by-step guide. If you follow it closely, it's not too difficult, but it does take a fair amount of time. Also, be prepared for some shenanigans regarding EFI booting. I'll gladly answer any questions you might come across.
Basically, yeah. I don't think my original comment is very clear, but I think you've got the gist of it. :p Perhaps someone more well-spoken than I could bring some more clarity.
I don't think you need much going into it with the right learning materials. Haskell Programming From First Principles (http://haskellbook.com/) does a great job of starting from scratch and even letting people totally new to programming learn Haskell.
You don't need any knowledge to begin with, but if you learn as you go you should show your code occasionally to other Haskellers. The feedback will help you to get better.* But it also depends on your project, to be honest. ^((* Yeah, that's not exclusive to Haskell, I know.)^)
There's a nice online course on FutureLearn (free) on FP with Haskell that would help you learn the basics. The HaskellBook is quite complete and starts with Lamba calculus, which is important. Aside from that, try reading other people's code (beginners, because intermediate users sometimes don't hesitate to write clever code, which is painful to read)
Yea considering that parsing is just a special case of stateful programming, I'm not sure how he was able to claim Haskell is bad at one and good at the other...
Good spot. This was extracted from an app where I was using randomly generated user IDs and storing session data in Redis, anyway, but I should have fixed that in the extraction. I'm still not sure how this will be done, though. When scotty-cookie (for example), sets a cookie, the middleware encrypts it and if scotty-cookie then wants to remove or change that cookie, it doesn't know that the cookie exists under an encrypted name and creates a whole new cookie which is re-encrypted (and so needs to have the same IV, at least for the name). A mitigation might be to use deterministic cookie names, but randomised IVs for values. Alternatively, I could decrypt *all* cookies before encrypting *any* cookie, and iff one exists with the same name as the one I'm creating/changing/removing, use the same IV, but only for the name.
Exactly.
A TreeSort would be one alternative too.
&gt; Without any deeper thought, I guessed that the current type level insertion sort (which is O(n^2) in worse case complexity) was at fault. This turned out to be wrong. Any findings to share on the compilation slowdowns?
Broken nix on MacOS Sierra was incredibly painful for me on a project a while ago. I eventually managed to alleviate some of that pain with Docker, by doing everything inside a lightweight container. That configuration is here in case it helps some other poor dev in a similar situation - https://github.com/ajnsit/reflex-nix-docker.
How are you installing Nix on Arch?
&gt; A Turnstile-style DSL would be even nicer. [...] which may even be or contain a fresh solver variable [...] I have lots of ideas I too have an idea, which I would like to discuss. Since my last PR, in an attempt to become more useful to the project, I have spent a lot of time learning about and playing with syntax-parse. So far I have implement a simply-typed lambda calculus by manually transforming the context, and for my next step I'm hesitating between (1) removing the lambda argument's type annotation and implementing unification, or (2) learning Turnstile. After watching the [Turnstile video](https://www.youtube.com/watch?v=j5Hauz6cewM), it is clear that Turnstile's version of the STLC is much shorter than my syntax-parse version, but I'm getting the impression that Turnstile is limited to languages like the STLC in which the types of all variables are known, and that I couldn't implement a unification-based language using Turnstile. If so, that's unfortunate, because Hackett's type system clearly needs unification. Anyway, long story short, here's my idea: Hackett macros would use Turnstile's syntax, except that the types are Hackett types, and any pattern variable in a type expression becomes a fresh solver variable. If unification fails later on, we *backtrack*, reverting all newly-gained knowledge about which type variable unifies with what, and we try the next clause. In your example, let's say `mac` wants to negate its input if it has type `Bool`, and return `nothing` otherwise. I'd write it as (define-typed-syntax mac [(_ e) ≫ [⊢ [e ≫ e- ⇐ Bool]] ------- [⊢ [_ ≫ (not e-) ⇒ Bool]]] [(_ e) ≫ ------- [⊢ [_ ≫ nothing ⇒ (Maybe a)]]]) Let's run my proposed algorithm on your example. To make things a bit more concrete, let's say that `f a` is `pure unit`. I'll put solver variables in angle brackets. First we look at `x`'s definition and we add some new equalities to our unification set. (let ([x (pure unit)] -- x : &lt;tX&gt;, &lt;tX&gt; ~ &lt;f&gt; Unit, Applicative &lt;f&gt; [y (mac x)]) (x y)) Then we look at `y`. Since we encounter `mac`, we try its first clause. The `(_ e)` syntax matches, we can use `[e ≫ e- ⇐ Bool]` and `[_ ≫ (not e-) ⇒ Bool]` to add some more equalities to our unification set. (let ([x (pure unit)] -- x : &lt;tX&gt;, &lt;tX&gt; ~ &lt;f&gt; Unit, Applicative &lt;f&gt; [y (mac x)]) -- y : &lt;tY&gt;, &lt;tX&gt; ~ Bool, &lt;tY&gt; ~ Bool (x y)) At this point `Bool ~ &lt;f&gt; Unit` causes unification to fail, so we forget everything we learned in the previous step and we try `mac`'s second clause instead. The `(_ e)` syntax still matches, and we can use `[_ ≫ nothing ⇒ (Maybe a)]` to add more equalities to our unification set. (let ([x (pure unit)] -- x : &lt;tX&gt;, &lt;tX&gt; ~ &lt;f&gt; Unit, Applicative &lt;f&gt; [y (mac x)]) -- y : &lt;tY&gt;, &lt;tY&gt; ~ Maybe &lt;a&gt; (x y)) Finally, we look at the application. (let ([x (pure unit)] -- x : &lt;tX&gt;, &lt;tX&gt; ~ &lt;f&gt; Unit, Applicative &lt;f&gt; [y (mac x)]) -- y : &lt;tY&gt;, &lt;tY&gt; ~ Maybe &lt;a&gt; (x y)) -- &lt;tX&gt; ~ &lt;tArg&gt; -&gt; &lt;tRes&gt;, &lt;tArg&gt; ~ &lt;tY&gt; And we conclude that `&lt;tX&gt;` is `Maybe &lt;a&gt; -&gt; Unit` and `&lt;tY&gt;` is `Maybe &lt;a&gt;`, for some still unknown solver variable `&lt;a&gt;`. There's obviously some room for improvement (what if the unification failure has nothing to do with the macro, do we still mindlessly enumerate all the clauses in vain? do error messages always blame the last clause of the first macro in the program?), but it worked on your pathological example, so maybe there's something worthy to discuss in there?
No, not yet :-( Still stuck with &gt; !!! Renamer/typechecker [Main]: finished in 131658.48 milliseconds, allocated 283493.694 megabytes
Great tool. Could haskell-indexer be used to do a topological sort of a module?
Well, wrt to GADTs, I believe OA's are more general so you can encode more than you can with a GADT. But they're less constrained so you can also make a mess better than you can with a GADT. If by verified you mean correct according to the compiler, yes, they are verified. I'm not sure if F# actually erases all the interface typing at compile time; it might, but then given CLR interop, I would guess not. 
I think once you can use IO you can get stuck in to some kind of project (no need to worry about understanding monads and all that if that's in any way intimidating - just how to use IO). To begin with I found doing a couple of project Euler problems with files loaded directly into GHCi useful. Perhaps that, or similar, would do well while you get to the point of feeling like taking on a project? Of course, there are a bunch of variations on project Euler that might better suite your tastes.
I switched away from OS X 18months ago and I don't miss anything from it. I use Ubuntu on VirtualBox on Windows, with shared foled and it's sort of the best of both world. I can use XMonad, and Office. What really bothered me with OS X was, first, with time, everything got really really slow - like 30 second to open a terminal. (I had to MBP in the last 6 years and have had the same problem with both of them. I'm the one having this problem apparently). Second, (at that time) you couldn't share folder between OS x and a VM guest without duplicating the files. If (like me ) you wanted to run everything on the guest (to use XMonad for example) you ended up with only being able to use 50% of the Disk spaces. From what I've seen from Apple advert, It doesn't seem Apple is targetting powerusers anymore ...
Was going to ask the same question, figured I'd include a solution : ) u/jtcwang just follow these [instructions](https://nixos.org/nix/download.html). It's not recommended to use the system package manager to install nix on a rolling release distro. I'm running nix on Arch and I like it even though the UX of the nix-* commands is atrocious.
As cool as this is, I can't wait until we stop having to do stuff like this. Essentially blog posts of this nature just boil down to "I've implemented a fairly rudimentary comp. sci. algorithm using a heavily constrained programming language". That said, we're at least getting more tools as time goes on.
You should know that it's really fun. I started learning Haskell when I was learning JavaScript. I was trying to understand higher order functions and closure in JS, so I ended up trying Haskell to deepen those concepts. Before I knew it, I was more interested in writing Haskell and learning more about it. Regardless whether or not you stick with Haskell, I think learning it will help you with your other two languages. As far as minimum knowledge needed, I'd say there is none. Just jump into the language and have fun. Besides my tongue-in-cheek comment in the beginning, what you should know is that, just because a concept isn't crystal clear at first, you shouldn't let that discourage you from learning more about the language. Haskell is a very deep language. There's always more to learn, and the community is always striving to push the language further. 
Just dive in my dude!
I saw the email this morning and thought this sounded like a really useful thing, but I wanted to point out for anyone else who didn't follow the link that it's actually executed very nicely, too! The site is very responsive, providing a nifty way to flit around between uses and definitions.
Pretty sure it sends the entirety of any files which have differences. I believe it hashes the file on both sides and compares hashes to see if it's identical on both sides and can be skipped. For single large files that have changed rsync is basically just a slightly slower/more complex scp.
Why do you use `tryTakeMVar` instead of `takeMVar`? The `putMVar counter` after `tryTakeMVar counter` in `readBoundedChan` is especially worrying, as this could block forever if the MVar is full and no other thread leaves it open for you. *edit*: wait, after the `tryTakeMVar`, the `MVar` should now be empty, so you can only block forever if some other thread manages to perform a `putMVar` in between your `tryTakeMVar` and your `putMVar`. It's a race condition, but it's probably not the cause of the exception you're seeing, because it wouldn't happen every time. *edit*: ah! I didn't understand why you were using nested `MVar`s and replacing one of them by an empty MVar, now I do. You want the writing thread to block on this empty `MVar`, and to awaken once the read thread puts a 1 in that `MVar`. Seems legit.
- The syntax - How to run ghc
What breakage are you encountered? The only breakage I know of is [#12479](https://ghc.haskell.org/trac/ghc/ticket/12479), which was fixed in 8.0.3.
Cool! I can't wait to read this! As someone who is just starting to do mathematical work in the area of control theory -- a bit divorced I think from the background for category theory -- and just started learning Haskell, I really wanted to have a good place to get the background for Category theory and how it relates to Haskell. This looks like the perfect entry point for me. I particularly like the occasional pointers to C++, which is my home language. Is the author planning to release (or has) a print/ebook version of this?
Anyone know if a PDF edition will be available? I'd love to read this on my kindle.