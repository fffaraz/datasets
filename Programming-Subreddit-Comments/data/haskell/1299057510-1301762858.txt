&gt; You have obviously never worked in Java. foo.returnsNull().doStuff() throws a RunTimeException. null certainly doesn't propagate automatically. If you use only static methods, which is bad form, null might or might not propagate. that's the problem with null in java - if people would throw nullpointerexception directly, instead of returning null it would work just fine. &gt; the idea is to separate the pure and effectful code. I have plenty of comments on this approach, but I think it's too big topic for reddit comments.
so a lot of that code could work in that monad (with its semantics) but without using any state, exceptions or IO, is this correct?
&gt; I use the monad functions all the time so do I, because I have to, not because I want to or find it a nice solution. &gt; you haven't quite got a hold of exploiting the higher levels of abstraction monads can offer yet. like what?
I hate settling for "very nearly".
I meant situation with no nulls, but f throwing NullPointerException.
what does value restriction have to do with laziness?
The code could work in IO+State+Exception, but I would not want that. I want careful control of my effects. And I view any code in the IO monad with great suspicion. Avoiding the IO monad is probably my biggest concern. But even just coding in State+Exception is unpleasant. Btw, which State+Excrption? There are two ways of combining them and I have used both. 
I like the way Haskell does it, where you know the effects. You could get something similar with OCaml and effect inference. I've never seen a full fledged design of such a language. Well, DDC maybe. But I want to see effects in my types. 
Or PTS. 
Laziness gives you better semantic composability. I'm undecided which way to go after having used both non-strict and strict Haskell. Strict Haskell really needs to have some fundamental parts of the language redesigned because some standard non-strict Haskell idioms no longer work. 
I also desire careful control of effects, and I believe monads are too cumbersome for that - e.g. creating a monad that only allows a single IO action (print/trace) is a lot of work: creating monad, writing wrappers, and using some lift operator. not to mention combining such monad with a different restricted IO monad. &gt; Btw, which State+Excrption? There are two ways of combining them and I have used both. the more natural one - without backtracking. I find it easier, more natural (that's the way every other language works), faster (I've made parsec2 20% faster by switching to this combination) and I prefer more explicit handling of backtracking.
No thanks. 
I fully agree, types that only contain shapes of values and no effects are lies.
If we had all expressions automatically be monadic I would no want to be stuck in some predefined monad of your choice. I would want each expression to be in the appropriate monad for what I'm doing. And sometimes I would want to control it. Like picking the combination on state+exception that I want (just as a counterpoint, I got a massive speedup in djinn by switching to the one you don't think is natural). 
I'd love to have, e.g., Haskell-src-exts parametrized over the module implementing strings. 
What would flipping the associativity imply?
Many people have this problem, which shows that Haskell is too hard for broad adoption. 
&gt;that's the problem with null in java - if people would throw nullpointerexception directly, instead of returning null it would work just fine. You might be right about that, but I'm not sure. Object foo = bar.baz(); // might return null is a lot simpler than Object foo = null; try { foo = bar.baz(); } catch (NullPointerException ignored) { } Check out the Queue interface -- there are null-returning and exception-throwing versions of pretty much every method. If Java had higher order functions, turning one into the other would be trivial.
Put the existing suggestions up on a board and throw a dart at it, for all I care. Literally any one of them is better than what we have.
I'm not sure that 20 type classes is a bad thing. Especially not if we have an easy way of producing aliases for common combinations (e.g. "Num" could still exist as a shorthand for all of them at once).
Lack of laziness doesn't imply a trivial evaluation order and effects. You could still allow local definitions to appear in any order, for example, and just compute statically the evaluation order required to execute the function (still skipping the evaluation of unused branches etc.).
We have a tentative date of April 1st to 3rd, and we are waiting for a confirmation from the host.
Use something other than "." for function composition.
That's really part of the [class alias](http://haskell.org/haskellwiki/Comparing_class_alias_proposals) family of proposals. I have added a link to that wiki page. I agree that one of these really needs to be part of Haskell.
if/then/else is a nice syntax, wouldn't that proposal for rebindable if satisfy you?
It struck me as odd too, but after thinking a bit about it, it starts to make sense. With the current associativity, multiple $'s can be replaced by a pipeline of functions: f $ g $ h $ x = f (g (h x)) = f . g . h $ x With left association, you can use $ as a low-visual-impact separator of arguments: f $ g x $ h y = (f $ g x) $ h y = f (g x) (h y) At least that's my take on it anyway.
effigies mentioned [constraint synonyms](http://tomschrijvers.blogspot.com/2009/11/haskell-type-constraints-unleashed.html) in an earlier post. That is yet another variation on the class aliases proposal, but it was not mentioned on the wiki page. I added a link there.
&gt; `layer2' :: Bytestring -&gt; Bytestring` &gt; Then these form a family of functions with the same type signature, which you can then stuff in a list. &gt; Then just use sublist extaction and fold over composition. The idea is to avoid serializing/deserializing at each step. That could incur a major cost in performance. &gt; Alternately, you could use a Thrist Yes, a few people have suggested that.
or "darcs init" :)
Thanks for all the nice solutions, here and on the cafe. My conclusion is: GADTs, higher-rank types, and generics *must* be added to the Haskell 2012 standard. It's inconceivable that such a simple and common situation should not be possible to handle in any reasonable way using standard Haskell, even if it is easy using extensions provided by the most popular compiler.
Ok, just my 2¢, but why not start using ¢ for backwards-compatibility? infixl 0 ¢ (¢) :: (a -&gt; b) -&gt; a -&gt; b (¢) = ($) It's no more keystrokes on my keyboard (mac option+4, I'm not sure about other keyboards though). In RL, ¢ is like $, but different, so it hints at the meaning of the symbol in code. Another possibility would be £, with the hint being drive on the right in the US ($) , and drive on the left in Britain (£).
Or something other than "." as Module name separator?
I would overhaul the prelude and standard libraries. This would involve a number of things. Here are some of the ones that come to mind right now: 1. Ditch the current String, and make ByteString (or Text) the new String. 2. Make unsafe functions less convenient and provide better alternatives using Neil Mitchell's safe package as inspiration. 3. Fill in missing functionality that people frequently ask for such as string splitting, searching, etc--and probably while we're at it more most of Data.List into the prelude to avoid the problem of always forgetting where common functions are defined. 4. Probably revisit core hierarchy of type classes like Edward Kmett mentioned. 5. Change a lot of core functions to be of type (MonadIO m =&gt; m a) instead of IO a. I've thought seriously about actually ditching the standard prelude and migrating to my own "safeprelude" that solves some of these issues, but convenience has thus far dissuaded me.
It would mean that `f $ a $ b` means `f a b`, whereas now it means `f (a b)`. There are arguments in both directions; I personally find the current behavior more desirable, just from the practical standpoint that I very rarely feel like I need `($)` in the former case, while I frequently use it now in GHCi for tacking functions onto the beginning of existing expressions. Having *both* the right associativity and very low precedence in one operator is very useful there.
fromInteger can go to its own class for sure. We can get rid of Eq and Show constraints with no problem. Then (-), negate, abs, and signum can go to another class. That leaves us with just (+) and (*) which can be turned into a monoid and a "distributive monoid". Should solve 90% of the issues... That gives us only three classes, only one of which doesn't already exist. Except of course monoid should really be built on top of semigroup, but that's neither here nor there :-)
This originally ended up as a reply to the wrong comment, so here it is again. I'm curious if you have a proposal for "fix Num" that doesn't double the number of built-in numeric type classes. It's certainly not theoretically sound in any way, but I've always found it to be a strong argument for the current system that everyone who sets out to design an alternative ends up with about 20 type classes just to describe numeric types.
I'm fairly sure 20 type classes is a bad thing to expose in the Prelude just for handling numeric types. Maybe there's enough justification to just deal with it; but it's certainly in the disadvantages column. Don't get me wrong; I am an algebraist, and I understand the desire to do this "right". But I also know there's a reason why we don't introduce that whole catalog of algebraic structures until late undergraduate education for mathematics students. This is a case where I'm solidly on the side of providing a practical and useful simplification in the Prelude, and letting mathematical programming use its own libraries, such as the existing numeric prelude.
Neither of these symbols are on U.S. keyboards, though. Hopefully widely used operators can be kept to those things that everyone can type without a Unicode reference chart.
Yeah, on top of this, you can make ($!) correspondingly left associative, and it suddenly becomes convenient to use ($!) for strict application of more than one argument.
I'm interested in hearing more. Drop me an email at `concat ["Jake", ".", "McArthur", "@", "gmail", ".", "com"]`.
Yep, function application is meant to be left associative. What really matters about ($) is its low precedence. People occasionally abuse the fact that it's right associative now, but every case of that can be replaced by uses of function composition as yatima's comment shows. This also helps to induce a better programming style overall. Making this seemingly small syntactic change from function application to function composition ends up helping one think at the level of composing functions rather than at the level of acting again and again on a value. This tends to be a better mindset in general for functional programming.
Very true. The r6rs letrec semantics allow just this for that very reason. It just isn't a very common default. That said, laziness is important here. When you tie the knot strictly, you need to initialize everything to some default value, so you pick up a default #f value that is sometimes visible in user code, or you pick up some kind of restriction on recursive definitions.
The combination of laziness and purity conspire to eliminate it. One might view the restrictions on where unboxed variables can occur inside haskell expressions as Haskell's equivalent of the value restriction, and it only occurs because they are strict.
It is a bit of both, see my response to ssylvan. Laziness lets you avoid using the strict null-then-update idiom to tie the knot when you have cycles in your where/let clause, which can leak a visible #f value in end user code, or you have some complicated set of restrictions you need to place on mutual recursion between values, usually indicated syntactically a la ML, which kind of destroys the whole utility of a 'where' clause.
You aren't even using the Email monad. Amateur.
You can get heredocs with quasiquotation. There is even a package or two that provides them.
I would understand the purity reason. but please do explain that unboxed variables thing.
Good point!
Sure. It is considerably less galling than 'fail' =) 
I sent you an email, but then googled and found out that you live quite far away from California. I'm afraid it will be quite a hard sell to a management (unless you are willing to relocate). 
Finally... a chance to fix that Kierkegaardian Either type from Right and Left to Right and Wrong.
Umm ... people move all the time?
The definitive guide to getting started with category theory is Saunders Mac Lane's Categories for the Working Mathematician. Most of category-extras arose from working through that book and adapting the mathematics to be more suitable for manipulating Haskell types. Most of the definitions you highlighted above can be found directly on wikipedia as well. &gt; Secondly, how is this package [...] useful? category-extras is being deprecated. It has been factored out into a number of smaller packages, which export largely the same functionality or more, while remaining as often as possible Haskell 98: Haskell 98: * [semigroups](http://hackage.haskell.org/package/semigroups) -- monoids without mempty * [comonad](http://hackage.haskell.org/package/comonad), dual of monads * [contravariant](http://hackage.haskell.org/package/contravariant), contravariant functors, where contramap :: Contravariant f =&gt; (a -&gt; b) -&gt; f b -&gt; f a * [void](http://hackage.haskell.org/package/void) -- a conceptually uninhabited type. * [semigroupoids](http://hackage.haskell.org/package/semigroupoids) -- categories without id, monads without return, comonads without extract, etc. * [comonad-transformers](http://hackage.haskell.org/package/comonad-transformers), comonad transformers * [bifunctors](http://hackage.haskell.org/package/bifunctors), bifunctors * [free](http://hackage.haskell.org/package/free) -- (co)free (co)monads * [distributive](http://hackage.haskell.org/package/distributive) -- the dual of traversable * [pointed](http://hackage.haskell.org/package/pointed) -- pointed data types, retained by popular demand, but largely deprecated. separated from the functor constraint because the only law was a free theorem. Haskell 2010: * [streams](http://hackage.haskell.org/package/streams) a number of different stream comonads Requiring Extensions: * [semigroupoid-extras](http://hackage.haskell.org/package/semigroupoid-extras) -- a number of non-haskell 98 semigroupoids * [comonads-fd](http://hackage.haskell.org/package/comonads-fd) -- like the mtl/monads-fd, provides MPTCs for dealing with comonad transformers. * [comonad-extras](http://hackage.haskell.org/package/comonad-extras) -- more exotic comonads. * [keys](http://hackage.haskell.org/package/keys) -- 'keyed' functors and containers, provides a classes for working with trie-like and map-lie containers. * [recursion-schemes](http://hackage.haskell.org/package/recursion-schemes) -- generalized bananas, lenses and barbed wire. * [representable-functors](http://hackage.haskell.org/package/representable-functors) -- representable functors * [adjunctions](http://hackage.haskell.org/package/adjunctions) -- adjunctions on Hask, monads from comonads, etc. every right adjoint is a representable functor. * [representable-tries](http://hackage.haskell.org/package/representable-tries) -- enumerably representable functors make great tries * [kan-extensions](http://hackage.haskell.org/package/kan-extensions) -- Yoneda's lemma, (co)density, and other Kan extensions. * [categories](http://hackage.haskell.org/package/categories) -- the Category subclasses from category-extras I haven't yet ported any of the bifunctor recursion schemes, the indexed (co)monads, or a few other things &gt; Secondly, how is [...] stuff like bifunctors and comonads useful? Bifunctors provide a more principled version of Control.Arrow's first/second/*** and left/right/+++. They tend to make nice pointfree expressions. They become more interesting when you add all the plumbing from the categories package above, and have more operations available. In the end, a cartesian closed category is "where the lambda calculus lives" and you have a product bifunctor involved in the definition, so these pop up everywhere more as background noise than as an end to themselves. Comonads are useful for a number of scenarios that largely have nothing to do with the scenarios where monads are useful. They make a good model for the semantics of data flow programming, they can be used to encode simple automata, image filters, functional references, in other type systems the (!) modality in a linear type system is a comonad, where it can be used to reason about resource utilization. 
Tell me about it :) I moved to US from the other side of the world for the job offer. 
Irellevant, But just curious what sort of money a good haskell programmer makes?
I've never seen a haskell programmer in my life. So i cannot answer that question :) I myself do not use haskell in my work (but hope to do so one day) 
The Email monoid is trivially turned into a monad by sticking it in Writer. ;)
Well, I'll just give you a call and we'll talk about it.
 data Which a b = This a | That b which whereIts kinda getwith = case getwith of This at -&gt; whereIts at That phat -&gt; kinda phat
Note: Mac Lane is too hard for most of us. I think the only way to start with it is in conjunction with a book not directed at mathematicians (e.g. Pierce or Awodey).
One way to view the value restriction is that it governs the interaction of side-effects with polymorphism, so in that sense you are right about how purity is the nominal solution to the value restriction. But in ML you work around the value restriction with eta-expansion. In a lazy language, eta-expansion/contraction doesn't do anything (modulo seq/undefined), you can view everything as already eta-expanded, so the value-restriction does not apply. But in Haskell you can't assign an unboxed variable in top level scope, because the right hand side would be a computation that would generate that value. When should it happen? When the module is loaded? At compile time? We don't have all of the ML-like plumbing to order effects, and no precedent for evaluating something in source order.
liftM/liftA are still useful default definition of fmap in that case. They let you define fmap without having to provide it yourself.
Mac Lane makes a good wall to beat your head against. I don't know anyone who successfully read it straight through on the first go and _really_ grokked everything. You instead need to approach each paragraph with a great deal of suspicion, about what is being said, and what is not being said. Working through the examples and exercises is key. If you use that as a guide to go look through other sources, wikipedia, Awodey, Pierce, n-catlab, etc. then Mac Lane is great! If you just try to plow ahead and skip a paragraph here and there that just seemed too obscure, you'll be lost.
Yes, sorry. For some reason I thought it was just `liftA = fmap`, which would be less useful. It would have a more specific type, I suppose, but still.
What language is the codebase currently in? If it were possible to do this work on a consultant basis, I would be interested. 
Sorry, full time local job only (with relocation).
I think I'm like a lot of Haskell programmers in that I'm self taught. I'm reluctant to apply for jobs like this because I'm acutely aware of the holes in my skillset and I feel that this will disqualify me. E.g., I'm pretty good at writing monads but I'm still working to grok monad transformers and arrows. Is that enough for a junior or intermediate position? What do I need to work on to be useful? I think a set of self-evaluation questions, endorsed by employers, would help expand the pool of candidates while improving quality. It would also provide a quick way to weed out any liars (e.g., someone skilled in writing monads should have no problem recreating the Maybe monad.)
We can discuss it. Let me know how to contact you. 
n+k patterns back in the language. I understand that they were a kludge, but removing them broke the symmetry between recursive definitions and inductive proofs for functions on natural numbers, which is more important in my view. And removing them also broke code in tens of thousands of Haskell textbooks. 
Clearly, the answer is that haskell courses that want to teach that correspondence should use inductively defined naturals to begin with :-) Edit: now that I think about it actually, can't you get 90% of the way back to what you want by abusing view patterns?
I'm not so sure about the need for generics, but I do think the design space for GADTs is pretty well understood at this point, in the absence of MPTCs at least.
Most of the employed Haskellers I know, including myself, work in finance, if that helps you establish a baseline.
Unfortunately, the Paamayim Nekudotayim is already taken.
I feel very much the same. With that said, I would still be interested in more details. show $ mkEmail "c" ["functionx", "org"] (Sorry, a monad seems like overkill, but a type seems appropriate.)
We've already got one. http://hackage.haskell.org/package/pony
Nope. I find the special form to be a wart -- it can't be partially applied and I don't think plays well with the rest of the syntax. AND it eats up three perfectly good names.
If I may enquire, why are you posting this job offer in such an odd fashion? I.e., not producing an official posting (withholding the company identity still if necessary) to -cafe, or at least providing a means of contact outside of a reddit thread? Speaking of which, reddit does allow direct out-of-thread messages/mail between users for anyone that might want to respond, but doesn't feel like recording their interest to the general browsing public for the historic record from now until the intertubes clog for good.
Actually, the records-as-fixpoints construction seems like overkill to me, especially in the absence of mutation.
I am simply not aware about places where i can find haskell programmers looking for a job. r/haskell looks like a good place to me. Thanks for the idea of posting it to haskell-cafe. I am not subscribed to it, but this would be a good reason to do so. 
if/then/else doesn't stop you from using that bool/if/ifM combinators. your objection to making if,then,else keywords makes sense, perhaps syntax should also be importable (like notations mechanism in coq)
I'd redefine foldl and foldr in this more symmetric, more point free way: foldr :: (a -&gt; (b -&gt; b)) -&gt; [a] -&gt; b -&gt; b foldr f [] = id foldr f (a : as) = f a . foldr f as foldl :: (a -&gt; (b -&gt; b)) -&gt; [a] -&gt; b -&gt; b foldl f [] = id foldl f (a : as) = foldl f as . f a usage: foldr (+) [1..100] 0 returns 5050. [edit] Yes, it's silly to make everyone change their folds... but I have *unlimited* wealth and influence, so I'm pushing this *to the wall*! 
You may want to try http://haskellers.com and http://functionaljobs.com
I use it solely to not type as many parenthesis. How can f $ a $ b be useful if it meant f a b? 
And don't forget: http://cufp.org/jobs
Congrats :-)
Well, if `$` were left associative, then you could write, for instance, `h $ f x $ g x` instead of `h (f x) (g x)`. So you'd still get the low precedence, and left associativity would work well for curried multi-parameter functions, the same way it does for plain function application. My argument in response, though, is that if you have multiple arguments to a curried function, all but one is typically pretty much trivial. If not -- and you have complex expressions for multiple arguments -- the result ends up being somewhat difficult to read no matter how you handle the parentheses and precedence, and you'd be better off defining a few names to simplify. Hence, when we're talking about delimiting complex expressions, the natural left associativity of function application is less important. But the uses of right-associative `$` aren't much affected by this; people use it all the time and it's quite readable.
Yes, that would be sufficient. Though liftM and liftA are still useful to have around, they can serve as default implementations of fmap for lazy implementors; they just shouldn't be used directly.
Except that Mac Lane is only good if you are, indeed, a working mathematician. A good introduction is [Pierce](http://www.amazon.com/Category-Computer-Scientists-Foundations-Computing/dp/0262660717), though it doesn't get into gritty details. For some of the grittier details, and for those on a budget, [Adámek](http://katmat.math.uni-bremen.de/acc/) is freely available and is a good reference. However it definitely requires active reading and working through the examples, not just sitting down with a hot beverage. Once you have the basics down, Google is your friend. A lot of folks in the Haskell community have blogs talking about various things.
If you have the `if&lt;-` construct, then `ifM`, `whenM`, and `unlessM` are all obviated.
&gt; like notations mechanism in coq Ugh. Only if we can make it usable and not break the grammar the way theirs does. Alas, notations: nice idea; wretched implementation.
Because if `($)` is left associative then that means `($!)` can be too, which then allows you to say things like `f $! a $ b` instead of `(f $! a) b`. 
Yay :-) To be clear: this is in addition to the full-time job we've been advertising (deadline closed yesterday and we are currently reviewing applications).
I agree that I wouldn't tackle Mac Lane unassisted, but it does have the benefit of having remarkably few holes in its coverage. When going through Mac Lane, I turned to all of those resources, but I did so with an eye towards making it through his book. The first two or three chapters of Mac Lane are actually quite readable by a non-mathematician, and serve as a nice motivation for the study of category theory. It is only once it hits its stride that it becomes difficult.
Full disclosure: I haven't had the chance to check out Mac Lane, so my opinion is formed mainly by the opinions of some mathematicians and mathematically-inclined computer scientists I respect. It sounds like Mac Lane is at a rather comparable level of difficulty to Adámek, though he slips in some automata theory and stuff that computer scientists would be more familiar with; not sure whether Mac Lane does the same or is more focused on the topology and abstract algebra side of things.
I do not really care what "most people" want in a language; I care about a good language. Anyway I don't believe Haskell is hard. Coming from imperative languages, sure.
Response by way of sarcasm: I guess I'll go tell the abstract algebraists that they just have *way too many structures*. :)
&gt;like what? If I could articulate it well, I would be a teacher; I am not.
That was made for me! When dons was mocking me for wanting too much when I was first getting started :P
Not sure you should worry too much about arrows. It turns out they are much less interesting/useful than once thought (Category + Applicative gives you equivalent power to the Arrow class, in a much nicer way). I wrote a small Monad Transformer tutorial (Basically a path towards implementing MaybeT, and then generalizing). EDIT: Oops, forgot link: http://www.haskell.org/haskellwiki/Monad_Transformers_Tutorial
Thanks for that, very useful answer! The reason I wanted a book rather than use wikipedia is I felt wikipedia didn't help when I didn't know the "order" to learn the information - I felt I was always looking up a definition, then having 3 more concepts I would have to learn to understand that page. Then I would hit loops, where a subconcept referenced the initial concept I was trying to understand. Having a structured book should be somewhat easier. From your descriptions, those packages all look useful, however I'll have to do some learning first before using them. "generalized bananas, lenses and barbed wire" sounds fascinating, but I have absolutely no idea what that sentence means :P 
Mac Lane provides a good table of contents and order to learn stuff in if nothing else, but as you can see from the other comments, it is a bit of a controversial choice. The 'generalized bananas lenses and barbed wire' is a somewhat tongue-in-cheek reference to the paper that popularized those terms, which is a chapter out of [Erik Meijer](http://en.wikipedia.org/wiki/Erik_Meijer_(computer_scientist))'s dissertation -- he is one of the inventor's of LINQ in the .NET world and has more 'patent cubes' at Microsoft than anyone I've ever heard of. The paper can be found here: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.125 I wrote a short [knol](http://knol.google.com/k/catamorphisms) on the topic of bananas (aka [catamorphisms](http://en.wikipedia.org/wiki/Catamorphism)) (aka folds) some time back that might serve as a gentle introduction with a few more code examples than the paper and which provides references to the earlier papers. But in general the idea of a recursion scheme is to factor out the recursive part of an algorithm and instead stylistically update some seed value as you traverse or generate each item or layer in a data structure.
Thank you. I sent you my email in a PM; please let me know if you have any problems.
Definitely I agree would be politically useful... but even more so than any other language, including F#, Haskell might just turn out to be a very very bad fit for the JVM. Haskell does things very differently from the standard stock of strict languages. If the difference between C# and F# was a bridge Microsoft had to cross, the difference between Java and Haskell is the Pacific Ocean. Furthermore, performance on the the JVM depends very heavily on heuristic-based optimization of common behavior for imperative code. Keep in mind that Haskell performance has taken huge leaps and bounds from stuff like pointer tagging, optimizing the layout of data structures in memory, keeping certain values in registers in the calling convention, tricks for lock-free updates to thunks, and so on. None of that is possible on the JVM. I fear that while it might be only somewhat difficult to get something running, but then impossible to make it perform acceptably beside other JVM-based languages. Haskell compiled through STG may well be an order of magnitude slower than Java on the JVM, with no obvious means of improvement. While I can see the argument that in some cases that might not matter; I also wouldn't want that to be the default way to use Haskell on an entire platform like Android.
How about we just leave ($) alone and define (!) separately as a left associative strict apply. The current "tack things on the front" ($) operator is really useful. There's no need for strict apply to be tied to ($) though. The only current use of (!) is for array indexing, and it's not even in the prelude (unlike list (!!) indexing).
Certainly, where ever you decide to post the announcement, providing direct contact information rather than asking people to respond publicly would be an improvement!
But only for type Nat (and perhaps Int), not all Num types (Single and Double are not inductive!). Yes, we need a type Nat!
Right, but of course algebraists want to make all of those distinctions when they are thinking directly about the structures and their properties. And they can; there are libraries like the numeric prelude that define things that way. But this doesn't add anything to the question of where the right tradeoff is between simplicity and expressiveness for the Haskell Prelude. To be clear, I'm not saying that the present hierarchy is perfect. It may well be worth moving abs and signum, and it's an obvious improvement to drop the Show and Eq superclasses for Num, for example. My point is just that there's a benefit to keeping the Prelude simple enough that it doesn't necessarily require an advanced undergraduate level education in abstract algebra to understand the definition of the absolute value function.
can patches be sent without joining github?
Well, I have no concrete proposal myself. But any typeclass that doesn't even permit *naturals* to be numbers is broken. Complicated typeclass stacks are a pain in Haskell right now; that's another improvement I'd like to see made -- the ML module system has something similar to typeclasses, actually, so perhaps some integration could be done.
Yes; but if money were no object and I were doing the language over, I'd be sure to integrate `HEREDOC`s in to the syntax from the very beginning.
Thanks, now I understand.
I've no problems with that. Personally I don't care much about the associativity if `($)`. The only thing that's bothered me is the need to write `(f $! a) b` which is quite gross, but only loosely related. But there are plenty of other fish to fry before that'd make it on my short list :) If we did add `(!)` to mean strict application, I'd put it at a precedence so that we can write `foo . bar ! x . baz` to mean `foo . (bar $! x) . baz`. Of course, I'd also want to see `(f .! g) x = f $! g x` make it into the Prelude too (same associativity and precedence as `(.)`, natch).
I'd get rid of all list processing functions and replace them with something like parallel chunk processing or some abstraction that scales from GPGPU processors to distributed systems. Then I'd put warning text on all libraries on hackage where the abstraction level doesn't carry its own weight. Data should be provided by a group of average comp.sci students. If they can't understand the abstraction after a week, add a warning text ("warning: messes with your mind"). I'd then define a subset of haskell where normal programmers can be productive and push this as an industry accepted set of libraries/tools. Further, I'd add instrumentation in cabal and on the online documentation pages for libraries. The instrumentation would measure the time from a programmer reads the documentation of a library, and until the programmer produces something useful using that library. Third, I'd update the run-time-system to scale from GPGPUs to distributed systems. One feature I'd want is an auto-optimized way of dealing with whether values should be re-computed or synchronized. In parallel architectures the cost of synchronization to get to an already computed value can be as expensive as recomputing it. The run-time system should figure this out. Examples where this is true: GPGPU and distributed systems. I'd pay to get someone to write a fully parallel and concurrent garbage collector. I'd pay to get someone to develop a production quality toolbox for distributed haskell.
what's wrong with it? I really liked it, I used to force it to accept random junk (like "__{{{", and make emacs display that garbage with e.g. subscripts, or italic. my source code looked better than most pdfs!
&gt; I'm not so sure about the need for generics, but I do think the design space for GADTs is pretty well understood at this point I agree. Given the choice, I would take just GADTs. Note that glguy has provided a [solution to the puzzle](http://www.haskell.org/pipermail/haskell-cafe/2011-March/089802.html) using only GADTs, with no higher-rank types.
Thanks for this list. It would be great if you could write a blog post about what each of these are, what they're useful for, and how to use them -- I assume you (and maybe others) have already written about many/most of them, so even just this same list amended with links to already-written posts related to each package/concept would be very helpful. In particular, comonad transformers, distributive, representable functors, and adjunctions seem intriguing and vaguely-magical, and I don't recall having read or seen very much about them before.
I actually started collecting a 'wishlist of things I would like to see in Haskell and/or GHC' a few days ago, just for my own benefit. That's on a different computer than where I'm at right now, though; perhaps I'll write some of them up later. For starters, alongside reorganization of the Num, Monoid, and Functor/Monad hierarchies, I'd strive very hard to give things human-comprehensible names. We shouldn't be naming things for the benefit of mathematicians. There aren't that many of them, and they're probably smart enough to recognize that "hey, this is the same thing as concept $x". There's also no obstacle to mentioning the correspondence in the documentation. In particular, I'd institute a strict rule that for any given thing, either it must itself be familiar/intuitive/easily understandable, or its name must be. (Ideally, both, but obviously that's not always possible.) I didn't have much trouble understanding Monoid and Functor, despite them having mathematical names, because the concepts are relatively simple and intuitive -- but Monads, of course, are a different matter. Monad would become something like Sequential or Chainable, maybe even Imperative or Procedural. (I'm open to better suggestions; I'm not sure if there's a good name which makes it clear that it's the operations which are sequential/chainable/whatever and not the datatype itself). I don't have any better ideas for Applicative, in part due to the fact that I haven't even fully comprehended the concept yet. Functor would become either Map or Mappable. Relatedly, I'd figure out a rule for when classes should have the same name as their primary method and when they should be 'Somethingable', and apply it consistently. (For classes with only a single method (that anyone ever uses), like Show/show, this feels more convenient, so Functor/fmap would likely become Map/map... but would this also apply to Foldable and Traversable, for instance?)
He's using `OverloadedStrings`, and it's the email Monoid. `concat` was a typo, he meant `mconcat`.
Congratulations to both Eric and Well-Typed!
Reordering lets is sensible for strict Haskell since that's what ordinary Haskell does. 
We need a Natural type corresponding to Integer; we do already have an unsigned Word type which corresponds to Int.
I'm working with Brent Yorgey on a new typeclassopaedia that will attempt to cover the bulk of them, but have yet to write an article on the topic.
Can anybody give a slightly longer description of the improvements to the type checker? Is it just the design and implementation that's cleaned up, or is there also some noticeable change (programs that didn't typecheck before and do so now, for instance - or the other way around)?
It would work quite nicely if there was a dynCompose.
non standard functions and types, without a definition of them! highly ambiguous! org.functionx@c??? ;)
If I recall correctly, it works better with some of the newer features like GADTs and type families. If you're using pure Haskell 98 you're unlikely to notice any real differences. On the other hand, as a consequence of the changes it no longer generalises the types of things declared in let/where clauses if you're using some of those newer features. So if you enable GADTs, for example, you may need to use more explicit typing.
Of course you would have to choose a particular kind of PTS: pure type systems are a *family* of type systems parametrized by the different function spaces you allow. The Calculus of Constructions is one of the more powerful ones (it allows functions from terms to terms, types to types, terms to types and types to terms).
Of all people, I'm pretty sure, Lennart knows that. ;) That said, you can build a PTS that is parameterized on the set of sorts/axioms/rules you admit. The Calculus of Constructions is just the most powerful one within Berendregt's original lambda cube, so named because it is parameterized on 3 of the 4 rules that you can admit between the 2 sorts involved. However, moving to the calculus of _inductive_ constructions takes you out of the scope of the cube. Once you add more sorts, or a Martin-Loef-style universe tower, the calculus of constructions looks a little small. ;) Simple sort/rule-parametric injective PTS's include Jan Zwanenburg's [Yarrow](http://www.cs.ru.nl/~janz/yarrow/) and Jan-Willem Roorda's [Henk 2000](http://people.cs.uu.nl/johanj/MSc/jwroorda/). 
I don't know that I buy the 'all but one is usually trivial' argument. With things like bracket, etc. you can have 2-3 fairly complicated arguments. The main difference is that all but the last right-associative $ can be replaced with .'s, so flipping its associativity strictly increases the utility of the combinator set. 
fyi, I poked at this some more and it seems that nhc98 doesn't build for 64 bit architectures. after stumbling around a tiny bit, I found some options that got me a working build from scratch. Here's the command line: ./configure --ccoption=-m32 --ldoption=-L/usr/lib --buildopts=-m32 --buildwith=gcc make make install note that /usr/lib is the 32 bit libs on my distro --yours may be different. 
Are they specific to monads, or are they equivalent to lambda-case? And if the latter, didn't someone implement it for GHC? I could have sworn I saw that somewhere.
Importantly, I don't want all expressions to automatically be monadic. That strikes me as a large problem with Disciple. Knowing that an expression _isn't_ monadic (or, is specific to the identity monad) is noteworthy information, because it means I can do rearrangements according to equations that may not hold for arbitrary monads. For instance, there is one way to write `map` in Haskell: map f [ ] = [] map f (x:xs) = f x : map f xs There are two ways to write `mapM`: mapM f [ ] = return [] mapM f (x:xs) = (:) `liftM` f x `ap` mapM f xs mapM' f [ ] = return [] mapM' f (x:xs) = flip (:) `liftM` mapM f xs `ap` f x And if everything is implicitly monadic, then `map` above turns into one of these, but which may depend on how exactly I write it, and I cannot change the way I write it if it changes which choice is made, if the choice matters anywhere else. So even parameterizing all 'pure' code by an arbitrary monad to run in can kill the nice equational properties of writing pure functions.
Moving to the calculus of inductive constructions takes you beyond the scope of pure type systems, as well. Unless someone gets a lot more creative with the constants/axioms/rules than I've seen previously.
Very true.
Thank you for your thoughtful replies, I've learned a good deal. I don't think that the only reason to use a dynamically typed language is to avoid writing manual annotations. For one thing, Haskell already requires very few annotations, as long as you avoid GADTs and other advanced features. If you are writing well-typed code anyway, there is little downside to type-checking it. Although, it may be that it feels easier/less frustrating to get runtime errors than compile-time errors. If the program runs for a while, then crashes, you at least feel "almost there"; if the compiler tells you "can't unify Boolean with a -&gt; a", it can feel more daunting. I think this emotional reaction is utterly misleading, as the compile-time checks save huge effort over time, so it's not a compelling reason to me. To me, the more compelling reason for dynamic types is that they allow you to have programs which would otherwise be ill-typed. The fundamental example here is `eval`. The only way for `eval` to fit into a static type system is as a `String -&gt; Dynamic`. I'm not being facetious when I say that Haskell already has Dynamic - I think it's an important tool, and I wish that it had much more support for dynamic extension of the language environment (dynamic definition of functions, e.g.). However, I don't see much to emulate in the 'dynamic languages', and I think that building up support for, say, string functions which fail at runtime is a waste of time.
Given that the greater part of "what you want" here is for the syntax to look natural and obvious, I'd say no. Of course it's trivial to work around not having n+k patterns in a number of ways, once you abandon the idea that the syntax should be obvious. I'm not taking a definite side here, but having a standard Natural type to go along with Word, a la illissius's response, and n+k patterns on those types, would appeal to me.
I'm mostly teasing here, but that might be a good example of what's wrong with it. Hope no one has to work with your code that doesn't have emacs set up the same way.
woo, looking forward to the new HP, hopefully one that doesn't require admin access to install... -- http://trac.haskell.org/haskell-platform/ticket/62
Alas, `.` has the wrong precedence.
If they are complicated enough that it's not reasonable to just put parentheses around them, is it really any clearer with `$`? The *last* argument being a complex expression delimited by `$` is very useful (it may frequently even be a do block, or a lambda) but I shudder to think of doing something like that, then another $, and more complex expressions.
I'd have to look at the spec again, but I'm pretty sure `if&lt;- a then b else c` is exactly `a &gt;&gt;= \ _a -&gt; if _a then b else c` where `_a` does not appear free in `b` or `c`. So it's specifically for handling monadic booleans. The `case&lt;-` version is similar, though for case analysis on any monadic ADT. I think [she](http://personal.cis.strath.ac.uk/~conor/pub/she/) may have implemented it recently.
The problem with it is that: (a) it's extremely easy to accidentally break your grammar; (b) it's very hard to not accidentally break your grammar. As an example of this, I've never been able to implement Haskell's do-notation (with semicolons instead of whitespace sensitivity, and not dealing with pattern matching or `fail`) in a way that doesn't have at least one egregious bug making it unusable. The bugs range from needing far too many unnecessary parentheses, to not supporting unbound results (i.e., what should desugar into `(&gt;&gt;)`), to not allowing you to get out of the do-notation (e.g., to say `do { ... ; foo (\ bar -&gt; ...) }`), to not letting you nest do-notation (e.g., `do { ... ; foo (\bar -&gt; do ...) }`), and so on. Really, this sort of thing should be quite easy to implement correctly. Also, the way Coq does it allows you to disassociate the syntax for a term and the (official) name of the term in ways that remind me far too much of unhygenic macros and of using `eval` everywhere when shell scripting. If you're going to allow overloaded syntax then you need to do it right, and an untyped string-based preprocessing system is not "right" in any context. The syntax and the name should be interconvertable if not the same; to do differently is just begging for bugs and user confusion. Haskell's infix operators get this right where few other languages do. I'd much rather have the kind of flexible notation that Agda does.
Help Mikhail with the Windows installer.
I seem unlikely to convince you at this point, but the idea of something $! like that lets me use it for multiple arguments is really appealing to me. It really does not lead to a very natural or readable programming style at present. Fixing $ to follow suit just seems to be a logical next step.
My sense of natural and obvious has obviously been destroyed by prolonged exposure to dangerously simple ideas, because `S (S Z)` seems a much more intuitive description to me than the weird swooping hook adjoined to a straight line that we call `2`. Certainly, if I were going to beam a message into space, I know which one I'd pick :-)
Interestingly, we'll have a kind for Nat shortly... thanks to Iavor.
All I can really say is divide an conquer and keep on trying. Break up your problems into really small parts that are almost trivial to code and then go from there. I myself had a hard time starting with Haskell because the way in which you glue code together is so fundamentally different. In procedural code 'gluing code together' is more like "now using this library -&gt; now using this library -&gt; now using this library" until you reach the solution and you tend to think in 'actions'. Whereas, in Haskell, it it more about the transformation and flow of data, "so now I have this data set and I use these operations from this library to convert it into a different, but related, set of data, all of which follows my types." I find that, when writing my Haskell programs, it really helps if I just focus on the data transformations that need to occur to get from point A to point B. I think the only other issue, that I have to regularly think about, is where I can draw the line between code that is, and is not, in IO land. So in summary, try and think about the flow and transformation of data rather than the 'actions' you need to perform in your program. The rest just comes with practise, make something small, watch it work and it will all just come to you. If you want a starting library that is really easy to plug and play into programs then I would start with cmdargs or haskeline. They are simple but cool and useful examples. I hope this helps.
You don't say enough for me to be able to understand your issue with IO. I guess a lot of it is lack of familiarity. I don't think it's a beast. My approach to it (I'm not an expert haskeller) is to work out maximal pure functionality, and encode that in pure code, so the functionality of the IO part is as small as possible. Another way of saying that is that the inputs from the environment (file contents, pressed keys, the time, etc) should be inputs to pure code immediately after they've been read in. Likewise the outputs to the environment should be outputs of pure code. The IO part just handles I and O, with no data processing.
Today i sat and just playing bit by bit glued some code with Yesod web framework. Sessions, formlets, accessing ms sql server, everything went quite well. It was exactly the same as i would do in clojure. Except i liked it more. Static typing was catching me falling many times. And usually once compiled code just worked. I think your problem is not that haskell is harder at gluing than dynamic languages. It's that you still do not understand types. For a long time i understood only simple types. But anything with Monads would throw me off (it still does, but much less). And of course documentation for many libraries is still rudimentary/non existent. But it's getting better. Just hang in there and have patience. You'll break through eventually. Everyone does :) 
And I'm guessing you are 'jeffz'....
as far as virtualenv goes, cabal-dev is what you're looking for in the haskell world.
Oh, excellent :)
well, it's coq - it's not like someone will have to work with that code. and I was in all emacs team.
of course it's hard! you're changing the language itself - it will never be easy. but for something like do-notation, sooner or later it will be implemented correctly in a library (just like for ocaml).
Any idea when a new Haskell Platform release candidate will come out which has GHC 7.0.2?
Why a haskell-specific reproducible build environment? Why not use something that will work with any dependencies like the nix package manager? Don recently visited Amazon to talk about Haskell. In a company like that, with lots of software that lives for a very long time in projects with mixed languages/runtimes, you can't trust cabal-dev, virtualenv, rvm, maven, sbt, rubygems, cpan, pkgconfig, plt planet, etc to model your dependencies in a reproducible way because you can't guarantee that a rebuildable/redeployable environment will be pure Haskell or pure Python forever. I don't mean to sound combative, because cabal-dev looks really interesting. I just wonder if it's solving the wrong problem.
If if it just for build, doesn't a vm provide what you want? Or if you manage a lot a machines, it should be straightforward to setup and teardown a new machine to ensure everything is clean.
This release may be doing something fancier, but I think the "sandboxing" is a matter of making sure your code doesn't import Haskell modules not explicitly declared among your package dependencies. As for why nix works with any dependencies, the key words are "package manager". A tiny bit of support from GHC is enough to ensure that only explicitly mentioned Haskell packages are in scope while compiling Haskell files in your project. Recording which versions were actually picked up and which version of ghc was used seems to be enough to make a build reproducible (up to external dependencies). Cabal sandboxes your Haskell dependencies because that makes it a more reliable Haskell build manager. Nix relies on it's position as a package manager in two ways to do something similar for the whole environment. One, because it is the package manager it has unambiguous names and version for every package in the system. I don''t know how cabal would explicitly name what version of e.g. libz a build used, if it might compile on debian or FreeBSD or windows. Second, nix knows exactly where packages are supposed to be installed (and perhaps what additions to the library search path and so on), so it knows how to make every file and only the files from requested dependencies inside the clean chroot environment where it builds a package.
Right, that's a good point. My inability to "find the right library" may be as simple as me not knowing how to approach my program structure so I *can* use the library.
I don't have much of an issue with figuring out how to use IO (I think) so that's why I didn't write much about it. I guess I meant that it was a library/monad that I was managing to use correctly. Though I think a common thread among these responses is structuring your program so IO is off in it's own place. That's something I haven't really paid attention to, and may help significantly with my programs.
Yea, I saw an reddit comment about it a while back, and I just saw dons post. Will be investigating...
Yea, I think I may have written this post in a bit of frustration. I always thought of myself as someone who has taken all the classes to be able to understand Haskell theoretically. And then when I try to use it practically, I feel functionally retarded. Get it? Functionally? Anyways, I plan on trying to convert some small python programs I wrote to haskell again this weekend. Hopefully I can identify the issues/frustrations I'm having more concretely so I can knock them out one by one.
The haskell platform is supposed to provide an equiv of Python's core distribution, but it's not quite as complete yet. I can't recommend stackoverflow enough as a resource -- not just for asking new questions, but for looking up recommendations for libraries. For example, both tag searches for "[haskell] [json]" and "[haskell] [xml]" bring up questions on library recommendations with fairly comprehensive answers on the first page. The Haskell wiki is not really organized/overseen in any coordinated way outside of the front page and a few adopted sections. Nonetheless, its an excellent resource as well. The web category, for example, is very well maintained at the moment: http://www.haskell.org/haskellwiki/Web 
I don't write glue code, or at least I don't think that way. For me, it's all embedded domain specific languages. Need to parse something? Use parser combinators. Need to traverse a tree? Use a tree-monad for that, à la XML parsing. Need records? Use data-accessor. But the point is probably that in every case I'm not afraid of writing my own DSL from scratch. For example, it's nice that data-accessor exists, but if it were not available, I would just cobble together my own miniature version of it. This is possible because the main abstraction can be written in just a few lines of Haskell. I have to know the abstraction by heart, of course, but piling abstractions onto each other is a powerful way of doing great things with minimal code. 
It's kind of funny that CIC can refer to different things. Benjamin Werner's thesis, for example, makes no references to universes. Most people think of Coq's metatheory however, which does. On the other hand, i've never seen universes be used to type a term with a pure programming intent. Anyone have an example of that?
Can you tell more about how did your company come to decision to develop a customer facing web site with haskell ? Is the management happy of decision ? Or are they blissfully unaware about underlying technologies as long as it all works ? How many haskell developers do you guys have and how did you find them ? Or did you train your developers on the job ? 
This *is* something newer than I was thinking of. It seems this builds and installs your package without adding it or its dependencies to your global package database (which might affect any builds that use automatically selected dependencies). It seems build results might still be affected by packages in the system database unless the .cabal file specifies exact dependencies, but that can be managed. (make a core-packages-x.y.z with exact dependencies on everything you want to allow from the system database, etc).
I really don't like £ for this, but I simply *must* upvote you for the drive on the right v. drive on the left bit. Awesome.
I wish the module system was less strict about ambiguous symbol definitions for modules that are imported without the 'qualified' keyword. I would like to be able to import a module in an unqualified way (perhaps because it contains infix symbols), and redefine some symbols in the current module (often I would just like to 'lift' them). In this case, the current module's definition should be preferred. For example: module A (foo) where foo = 1 ----- module B where import A -- not import qualified foo = 2 bar = foo -- should prefer the current module's foo Currently the above gives the error: Ambiguous occurrence 'foo'. However, if I want A's foo, I could say A.foo. The only options seem to be either * qualify all symbols from A (import qualified A), * qualify all uses of foo (either A.foo or B.foo... although B is often quite a long name which is ugly), or * choose some other symbol than 'foo'.
I am curious- are you actually using nix? Every time I have tried I end up with build failures and have to ditch it. Cabal does more things specific to haskell than nix. Perhaps you could come up with a design which uses cabal on top of nix and satisfies everyone.
I've been thinking that the class system would be greatly improved if instance dictionaries were records (from one of the various record proposals). Your package problem would be mostly solved by having packages export "instance" records, since records don't depend on the class definition. I say mostly because defining some instances may require types that are defined elsewhere. Contexts would just be shorthand for long record types, so modifying the class hierarchy without affecting the instances is possible as long as the types still check. It would also make it much easier to implement a scheme to import and export instances in modules and to get rid of the annoying flood of -By functions.
[Release timetable](http://trac.haskell.org/haskell-platform/wiki/ReleaseTimetable): Third major release of 2011.2.0.0 (stable): Mar 5, 2011. Fourth major release of 2011.4.0.0: July 2011. 
And before you guys ask – that GHCi bug only occurs when you try to load a "then group by ..." statement in GHCi. You can compile and run it just fine with GHC, and its even possible to define such a statemant by hand in GHCi. After all, its still an open source project! So if you think you can help me to fix that one last bug, please contact me. It's my first major contribution to GHC, and I hope you guys like it. :)
Whooohooo!
I credit the letter writing and [tshirt campaign](http://haskell.spreadshirt.com/bring-back-monad-comprehensions-A6499530)
What about dynamic libraries on mac os x?
I don't have a mac, but I guess as long as they're compiled you should be fine.
BUT I CAN'T PRODUCE THEM!!!!!!
Woohoo!!
What?
i don't even know
I don't understand what you're trying to tell me? :)
i don't even know
&gt; Have utter and complete confidence in your build. If you have the slightest suspicion that your build didn’t precisely pick up on a change you made, make a new build tree and build from scratch. A lot of mysterious problems go away when you do a clean build. I think it is sad in this day and age that we still cannot trust our build systems with our eyes closed. Surely the technology required to absolutely *guarantee* that the build picked up any change you might have done is reasonable to implement?
This. If I can just tell my boss I'm using haskell for an in-house project and he says okay, it's caught on.
doesn't that already work?
I've used internal systems that resemble nix and had read about it before, but after writing the comment yesterday I decided to start trying it out. Using it as a package manager to install prebuilt packages with side-by-side dependencies works as expected, now I have to try using it as a build environment.
I do like Agda's mixfix a little better for what it does. However, one thing it used to not do is binders, which is a big omission. And now the experimental support for it is kind of lackluster if you ask me. It's more like Coq's notations, but the sugar has to be in one-to-one correspondence with desugared terms. So you can make sugar: Σ[ x : A ] B But you cannot make an `∃` sugar with the same underlying representation. That's kind of a bummer.
This doesn't work very well and solves a different but slightly overlapping usecase. If you look at scala it provides much this solution by letting you pass explicitly named "implicits" around, but with implicits you lose the confluence guarantees that make type classes so useful. You can make dictionaries for how to manipulate sets with regards to an ordering for instance, but unless you know that every two sets that are indexed by Ints have the same choice of ordering, it isn't safe to use hedge unions or intersections, and so you must revert to the asymptotically slower process of inserting every element from one into the other. This is a very big thing to give up, especially since I can emulate the behavior you want with the 'reflection' package, but I can't emulate confluence safely with implicits.
I'm still a bit queasy about the lack of a decent meta-theory about what operations it can solve, however.
any idea when scion will catch up to ghc7?
Do those t-shirts now have nostalgia value? Or are they just an excuse for old timers to say "I remember when Haskell had monad comprehensions the first time, kid".
Some of us remember Haskell from before monads. 
Ooh, I think I just got schooled :-)
Why not just use Text? You can specify what encoding anything read in is in (I believe, or at least read it in as a ByteString and then convert it to Text, specifying the encoding)
Chars are simply Unicode code points. There's no encoding involved until you read or write them somewhere.
haskell chars are unicode codepoints (which is maximum 0x10FFFF and are not stored on 16 bits, but at least 32 bits). this is not utf-16, nor utf-8. if you call ord on a char it returns the integer of this unicode codepoint. To encode/decode to/from, you should have a look at the encoding module in text, and then from a text type you can convert to string using unpack.
Where did you see documentation that Chars are 16 bit values? If that's documentation referring to Haskell then it's wrong (and we need to correct it!). As everyone else has said, Chars are simply specified to be Unicode code points.
The Char is a 4-byte-wide representation of 0x0 to 0x10FFFF unicode code points. For working with UTF-8 look for "utf" on the list of libraries at http://hackage.haskell.org/packages/archive/pkg-list.html I recommend http://hackage.haskell.org/package/utf8-string-0.3.5 which has http://hackage.haskell.org/packages/archive/utf8-string/0.3.5/doc/html/Data-ByteString-Lazy-UTF8.html to handle the lazy ByteString format.
That's not exactly true. Your computer can't store "Unicode code points", only numbers. It's just that in this case the encoding is simply the direct enumerated mapping of every code point to a number between 0x0 and 0x10FFFF, as defined by the Unicode standard, stored in machine integers large enough to hold those values without any issue. There's _always_ an encoding somewhere. To see it clearly, compare the ASCII encoding to the EBCDIC encoding. The number 65 is not a capital A; it is the ASCII encoding of the capital A. The EBCDIC encoding of the capital A is 193.
"Somewhere" includes RAM.
Of course. But the internal representation is implementation-specific, no? 
I'm sure that you could do it that way, but using cabal-dev is much easier than setting up new virtual machines and coordinating them. With cabal-dev you sandbox just the Haskell package database, rather than getting a brand new file system, OS told, etc.
I'm not really sure what you are saying here... but Unicode code points are numbers... 65 is not only the ASCII representation of A, but also the Unicode code point for A. Of course the computer needs to store these numbers in memory somehow, but that's not specified... a Char is a Unicode code point, and that's it. Contrast this, for example, with Java, where char is a 16 bit value in a UTF-16 encoding of a string.
Yes, it is easier for Haskell specific things. The OP's concern was in a broader setting.
It's March 5 today. Hm.
It depends on the semantics you want. You don't always need to keep ErrorT on top of the monad stack. &gt; ErrorT String (Writer [String]) () Runs as (Either String (), [String]) This will always return the [String] from the Writer monad, but wont return the () if there is an error. &gt; WriterT [String] (Error String) () Runs as (Either String ((), [String])) This wont return the [String] or the () if there is an error.
Isn't that the problem that `pass` shouldn't be part of the `MonadWriter` interface? Yet another reason to remove the `mtl` from general use.
I think it came from [here](http://zvon.org/other/haskell/Outputprelude/Char_d.html) but I also believe I saw it somewhere else. If I find any other sources stating this I'll link back here.
Does it handle the weird encoding system Java uses? The JVM class file format does not use normal UTF-8. It uses a 2 byte null character that while it tranforms correctly is not the correct encoding because it isn't the simplest representation of that codepoint. TBH if I can convert between a real UTF-8 lazy bytestring and a string then I can manually mangle that particular character as needed.
If you want to nit-pick computers store bits, not numbers. Numbers are an abstraction on top of bits. Unicode code points is another abstraction on top of bits. Both are equally valid or invalid.
If you want to nit-pick computers store bits, not numbers. Numbers are an abstraction on top of bits. Unicode code points is another abstraction on top of bits. Both are equally valid or invalid.
If you want to nit-pick computers store bits, not numbers. Numbers are an abstraction on top of bits. Unicode code points is another abstraction on top of bits. Both are equally valid or invalid.
I was in error. After consulting the standard, a Unicode code point is indeed the number; I thought the code point was actually what the [standard](http://www.unicode.org/versions/Unicode6.0.0/ch01.pdf) calls the "character identified by a Unicode code point". You can't store "Latin A", but you can store a Unicode code point in the natural encoding of an int.
Use `Data.Text` with UTF-8. Or if the exact encoding you need is in `iconv`, `Data.Text` is designed to work well with the `iconv` binding. If you think it would be a good idea to have an efficient native Haskell binding for this encoding, submit a patch. I'll bet Bryan would agree with you.
Please test this version: http://dissocial.st:8000/HaskellPlatform-2011.2.0.0-rc2-setup.exe "Just unpack" should work with limited accounts on Vista/7. Standard install will still result in error (can't write to HKLM).
I doubt it is worth having it in a library. JVM class files are the only place I've seen it used.
Thanks for this. Data.Bytestring.Lazy.UTF8 seems like it should be useful. Just need to handle the JVM mangling for the null character myself.
Does anyone else get a virus warning when visiting Conal's site? I get an "additional plugins necessary" popup and then ESET says "..../tetris.jar" is a virus and it closes Firefox as a precaution. Last time I visited it, I got the same popup and it crashed Firefox. Weird!
Ah, and that page has a big red link at the top to an updated version. The blog post it links to says: &gt; !!! I have been notified that the Haskell language significantly changed from the time the first version of this reference has been published (especially system of modules). I am working on an update!!! And unfortunately the bit about Char is one of those changes. The old docs say: &gt; The character type Char is an enumeration and consists of 16 bit values, conforming to the Unicode standard. So Haskell Char has always been specified to be a Unicode code point, it's just that Unicode has changed. Back in the day, a code point was only in the range 0 .. 2^16-1, but for ages now the code point range has been extended so that it no longer fits in 16bits. That's why the multi-byte UTF-16 encoding had to be introduced in the first place (and Java got completely caught out by the transition).
Fine here, on both Firefox and Chrome.
Thanks! Tested it on a limited user account under Vista, installed to my Documents folder was only prompted three times with the can't write to HKLM error. Everything unpacked successfully.
to my shame, i unthinkingly clicked 'ok' on the java permission applet that came up, and a few hours later my system had been owned. i am still fixing it... serves me right, i suppose
Fuck man, sorry to hear that. I hope Conal reads this and investigates. Over a year ago someone hacked the web hosting company I use and put in sneaky redirects to viruses (by editng all the .htaccess files on the servers I think). I didn't find out until people on reddit accused me of doing it on purpose. It was embarrassing, but I was glad to track it down (not that the company was helpful, even though it was their fault).
&gt; I'd then define a subset of haskell where normal programmers can be productive Why do you think "normal programmers" can't be productive in the full language, given that most of them haven't even tried? Low expectations breed low performance. The Java designers dumbed down their language to meet the lowest common denominator, and the result is a mess that wastes everyone's time. A lot of those "normal" Java programmers have jumped ship to C# and discovered that it's well within their power to effectively use lambda (sorry, "delegates") and monad comprehensions (sorry, "LINQ"). 
If you are still looking for someone, I am interested.
This is talking about a different "append".
I keep thinking about trying a Haskell web framework, but I'm never sure which of the various options is best. Can anyone give an unbiased summary of the relative advantages and disadvantages of Yesod vs Happstack vs ....?
I tried all 3. Happstack was too complicated for me, but that was long time ago. Now they probably have a much better documentation. Snap is awesome, very simple, easy to pickup, very good documentation. Yesod is awesome. Not so simple like Snap, much more involved, but also provides much more out of the box: sessions, forms, very sophisticated routing with variables and typed urls. One thing i liked in yesods templating system (hamlet) in comparison to snaps template system (heist) is variable interpolations. In heist you have to repeat yourself: Define variables in template, and then again define (supply) them with Splice. In hamlet you just use whatever variables are defined in source code. I chose yesod. But i liked snap too. 
The following snippet made me curious: &gt; Now that cabal can build test suites for a package, a big obstacle has been removed. ...is this a new cabal feature?
I get that too, and I'm blocking Java in Firefox to protect against malicious or useless applets. Most sites are fine without having to run whatever Java applet they try to startup.
It would be nice if the release timetable was updated a bit more often (even to "unknown" if they are waiting on something else). It still said early February until recently, even though that date passed without a release.
Don Stewart [wrote](http://comments.gmane.org/gmane.comp.lang.haskell.cafe/86965) yesterday: &gt; We're currently testing the installers, with a view to announcing the release early in the week. [2011.2.0.0 RC](http://code.galois.com/darcs/haskell-platform/download-website/)
I took a look a couple of weeks ago, and like you decided that either Snap or Yesod would be the right choice now. I looked at Snap in more detail, and talked to the devs for a while, but it seems to me (sadly) that it is way to early stages to be used as a serious framework. Is it your feeling that this will change, and Snap will at some point overtake Yesod, or has Yesod gained enough momentum to hold its own?
Yes, developed during the last Google Summer of Code. Using the latest cabal-install (0.10) you can run cabal configure --enable-tests cabal build cabal test Check the user guide: http://www.haskell.org/cabal/release/cabal-1.10.1.0/doc/users-guide/
This is a very useful overview and should become part of the GHC documentation!
It's not too often that we find people that have undertaken similar significantly large tasks in both languages. I felt like some of the more important questions were not asked. - Which language felt more natural for the job? - If you were to pick one tomorrow for a similar task, which one would you pick and why? - Did the type system deliver some of its premises of catching errors early (compared to your experience when using dynamic typing), resulting in a more robust final implementation? - Are there notable differences in runtime efficiency? - What were the major issues you've had in each? Instead, the interview reads more like a summary of both languages individually. Too bad. 
They are all in early stages. But i need something now. So i have to make a choice. 
Did you use the "Just unpack" option? It should not attempt to write to registry or do anything fancy. edit: s/should/should not/
So do I. The thing is going with something non-Haskell is also a choice.
Dang. I've heard sporadic reports of virus warnings, and I've never seen it myself. I'm baffled, and I'd really appreciate any specifics about the circumstances under which this warning appears *and* under which it does not appear. OS, browser, blog page. I'd also appreciate suggestions for how to track down and fix the cause(s) in the case that I am not able to reproduce the symptoms. Do virus warning ever appear on pages on http://conal.net *other* than in the blog? Thanks much for the alert.
ouch! sorry about that. :(
&gt; to be used as a serious framework btw, sometimes you don't want a full-blown "framework" which forces you to structure your code the way the framework needs it, but maybe you only a toolkit-like library which allows you to pick those components you need/like right now, and combine them with other components from different toolkits...
Can anyone comment on the state of haskell IDEs ? I tried leksah yesterday, and it was not easy to use. It does not support conditionals in cabal, which is a deal breaker for me. Right now the most comfortable and easy to use for me is still emacs. 
recent discussion on [cafe](http://www.haskell.org/pipermail/haskell-cafe/2011-March/089790.html). EclipseFP does support conditionals in cabal, and you can set the flag values you want to use to build your project.
I have on many occasions looked for data structures for two dimensional grids in Haskell, but I am never fully satisfied with the solution I end up using. Maybe the solution to this question could turn up a setup that will solve that problem for me once and for all. 
Haskell is a RTS and RPG now?
Is it just me or does ezyang consistently put out excellent and informative blog posts?
Absolutely. But that's not what Snap is going for (it seems to me.)
For a while I did use EclipseFP. The problem I had with the last version (before this release) is that Eclipse would get slower and slower until it started coming up with memory errors. This didn't happen with Eclipse alone, just when I started using EclipseFP to write Haskell code. I tried tweaking the jvm settings as some posts on forums suggested, but it just delayed the time until the errors started appearing. I'm assuming the problem was some kind of memory leak, although I didn't look at the source code. Does this new release fix that problem?
I should say that now I just use gedit, since it responds instantly (unlike Eclipse did even before EclipseFP started slowing it down) and I decided I value smooth typing over instant highlighting of syntax and type errors.
No such option is presented.
Er, nobody else ever reported that problem, so it wasn't explicitly fixed. I use EclipseFP myself, of course, for all my Haskell development and never run into it. If you can think of more details, please post them on the EclipseFP forum or mailing list. Thanks!
What are your criteria for these grids?
Well, it had an RTS for a while now :-)
Just happened again. It's only ever happened to me on the blog. Like I said, the same thing happened to me a year ago on Media Temple. Some nogoodnik edited the .htaccess files on the server(s). Look for a line "RedirectRules ..." and if it's redirecting anywhere other than your server, that's probably it. I'm not really server savvy so I called tech support and made them go line by line with me. It's a "smart" exploit because it only happens once a day or so. Once I close the page and go back it's fine. &gt; Thanks much for the alert. No problem. Everyone knows it's not your fault. If I see a blog post about functionally reactive viruses, I'm going to be suspicious. :)
Zoinks.
...is GHC7 supported yet btw?
It sounded like to me the implementer didn't really change the way he implemented the protocol on either. He simply recreated each other's environments, and built the same implementation on top of each. It would have been more interesting if he had embraced each paradigm a bit more.
Can anyone give a high-level summary of what is going on?
You can use conditionals in your .cabal file in Leksah (leksah.cabal does). You just can't use the GUI cabal editor on the file and must edit the cabal file manually (File -&gt; Open). To make this clear the dev version of Leksah now does File -&gt; Open if you try Package -&gt; Edit on a .cabal file with conditionals. To set the conditionals go Package -&gt; Edit Flags and put -f options into the "Config Flags" box.
Does the don't-interleave-cabal-and-distro-package-installs advice also apply to Arch Linux?
No, haven't tried it yet. When there's a Haskell Platform with GHC 7.0 we will work at supporting it.
Lisp-like macros?
Seconded, a bit more context would be nice.
After typechecking GHC translates the valid Haskell code into an explicitly typed intermediate language called Core. This is where the biggest amount of optimisation is performed. Now, of course we want optimisations to be correctness-preserving, and preferably also make our programs faster. Currently we have a guarantee for neither. There is something called "CoreLint" which is essentially a type checker for Core. If the output of a transformation passes CoreLint you can be a bit more confident in its correctness, but still not 100% sure. Coq is a theorem prover and a fully dependently typed programming language (with all the drawbacks this entails). The OP describes a framework where you can formalise Core optimisation passes in Coq, therefore getting 100% certainty of correctness for various problems. The different variants correspond roughly to ideas of using an ADT vs. a GADT. That is, you can either write your optimisation and then, separately, prove some properties about it. Or, you embed certain properties (such as well-typedness) into your program terms directly. Thus if your program typechecks, your transformation is type preserving and thus correct for all the properties enforced by the type system. It also opens up the possibility for proving further properties about your program transformation such as improved performance (although that can be very difficult).
How easy is it to port a simple cabal project into this? Currently I'm using haskell-mode for emacs and the CLI.
&gt; therefore getting 100% certainty of correctness for various problems With respect to the specs. Just saying. :-) EDIT: When you proof correctness of a program (in this case a compiler pass), you proof it correct w.r.t. to a specification, which you also write. Coq helps you proof correctness of your program, but does not help you proof that the specification, which you wrote, says what you intend it to say. I wasn't saying anything about the correctness of Coq. I was only pointing out that "100% certainty of correctness" should come with a disclaimer.
Awesome!
#haskell is certainly a MUSH. And the type system is a MMOG!
Yes, that's why I qualified it with "for various problems". There's actually no 100% certainty. The Coq proof checker could have a bug, or the underlying mechanism may be wrong, or a cosmic ray could have flipped a bit and somehow made the proof go through. All of this is much more reliable than testing, though. ;)
&gt; There is no hope trying to mix distro installation with cabal installation. The distro installer assumes it has the monopoly; cabal assumes there is no monopoly. They are fundamentally in contradiction. In NixOS, mixing between cabal and nix installation is an even bigger nightmare than on Debian, but I'm glad to see that the problems are not entirely nix related.
[Here is a picture of what I was referring to.](http://i.imgur.com/Kedg4.png) Guess I need to make this option more explicit.
Eclipse has the concept of workspaces, which contain several projects. You can import projects in a workspace. In your case, the easiest would be to create a new Haskell Project and point its location to your current project. EclipseFP will not overwrite your cabal file but will reuse it instead. 
So effectively if I do a "create project from existing code" it will use the Setup.hs and .cabal file I already have? It won't trample over my directory structure or try and change where things go? //edit - I assume there is OSX support as well?//
 lock :: ModelTransition lock (Unlocked n) = Locked n unlock :: ModelTransition unlock (Locked n) = Unlocked n This seems to be setting up for a run-time error when an unexpected state is found. Would there be a way of organizing the types to avoid this at compile time? e.g. data Locked = Locked Int data Unlocked = Unlocked Int lock :: Unlocked -&gt; Locked lock (Unlocked x) = Locked x unlock :: Locked -&gt; Unlocked unlock (Locked x) = Unlocked x modify :: Unlocked -&gt; Unlocked ... etc. I'm not sure how much this would complicate the rest of the code however. For example the view code should be able to operate on a Locked or Unlocked, so I guess you'd need a type class and a Value method or something.
No, EclipseFP uses the Cabal file has the source for information about how your project is set up, where are your sources, etc, so it won't try to do things any other way. Yes, it should work under OSX.
What I don't understand is with respect to which semantics you can prove correctness of the optimizations. Have the operational semantics of Haskell been formalized in Coq?
Just, I think, the type correctness of Haskell Core.
That's still pretty impressive...
What ?
I just downloaded the source and installed it. Everything seems to be working OK... just spent an hour or so adding typing signatures to fix my GADT-using program due to the lack of let/where generalisation.
Slides: http://blog.johantibell.com/2011/02/slides-from-my-hashing-based-containers.html
Great! I could hear the first third to helf-ish of it, and then it became very quiet. I'll look forward to watching it in its entirety when I get home and can plug in speakers.
Impressive! I look forward to seeing the impact this will have on the compile times of large systems. "Where's Fred?" "Oh, he's compiling, so he took a two week vacation."
You trust that Fermat's Last Theorem is true, right? Because of Wiles' proof? The soundness of Coq's core logic and the correctness of its implementation is much less to accept on faith.
There is a small portion around the half-way mark when the sound fades out. It comes back after a few moments though.
Especially given the existence of [coq-in-coq](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.56.7894) providing the self-consistency of most of the ideas employed in that core.
Okay, I'll try to take another look then. Does it work with zero Haskell (i.e. no haskell implementation on the system)? Well, I better figure out how to uninstall GHC without deleting the package on my distro... Currently what I'm doing is I'm packaging Hugs with the program, and putting up the Haskell source in `$(pkgdatadir)`. As it is internal to the company I don't see any licensing issues with that (and Hugs98 seems to have a 3-clause BSD anyway). Still, it may work faster if compiled, although using only the very limited data I am currently debugging it with, time is not yet an issue.
If hugs works fine, no reason to push ahead too aggressively with a switch :-) In any case, nhc works with no Haskell on the system just fine. The `--buildwith=` flag above tells it to do all the building via gcc instead of, e.g., ghc. If it doesn't detect a haskell compiler on the system, it builds with gcc automatically. So you don't need to touch GHC at all to play with a fresh nhc install.
Impressive work! 
Around 44:00, there is a question about investigating sizes of constructors, and someone mentions a tool that supposedly draws diagrams of constructors' memory use like the presenter was drawing. Did anyone catch the name of that tool? It was inaudible to me.
They were talking about [Vacuum-Cairo](http://hackage.haskell.org/package/vacuum-cairo).
Char itself is guaranteed to be wide enough to handle a single unicode codepoint. Unlike Java you should _not_ break plane 1 characters into surrogate pairs.
So let me answer here rather in the interview :) I like your points in general and you are right: It ought to be stressed that we are looking not at a comparison between the languages, but at specific implementations and designs of the same system within these languages. And from that we are trying to derive some general answers. Put the same task on the shoulders of e.g., Don Stewart and you will almost surely see a different design and a different outcome. I'll attempt to give my (subjective) view here for what it is worth: * Erlang is naturally suited for this job. The protocol fits very well in a concurrent system. For Haskell it takes more work since you either have to raise up a framework in which to work the concurrency (like I did), use STM, or use event-driven (continuation-based) programming more in style with most C and C++ clients. * Picking one tomorrow: Erlang, hands down - for this particular task it is very hard to beat IMO. For writing a compiler, I would be looking at Haskell though. * Did the type system deliver: Yes, very much. It does catch a large amount of errors before they occur. I don't know if it results in a more robust implementation though. Again it depends. Erlang copes *extremely* well when we can accept an error here and there now and then. This happens in BitTorrent because if there is an error in the handling of a single peer, we can just go grab another one - problem solved. Also note, that while Erlang does not provide static typing, it *does* provide decoupling in the sense that one (erlang-)process can't go rumaging around in the memory space of another process. This somewhat guards the danger of dynamic typing, by enforcing API's into your code base with encapsulation. In other words, robustness in e.g., python might be worse. * But there are problems, where you can't even accept a single error at all. In that case you may want to look into Haskell, whose type system I generally think will be better at delivering here than Erlang. * Runtime efficiency: The Erlang client beats the Haskell client by a significant amount. Very preliminary tests with the new Haskell IO manager shows the same performance as GHC 6.12, but I need to look more into that. Performance-wise the Erlang client can compete with the C clients. Memory efficiency is a bit different though, with Haskell coming out on top. If you load an insane amount of torrent files, it may change though. * Major Issues: For Haskell, it is efficiency and laziness. You want stuff to be *strict* as much as possible. About half of the time was spent on getting the code to run somewhat adequately. Naturally, my relative lack of skill when it comes to optimizing lazy evaluation is a problem here. * For Erlang, the major problem is that I wrote the client without having a deep knowledge of the OTP libraries. These help you a lot when harnessing the concurrency, by defining some generic solutions. However, to gain the benefit, one must structure the program in a certain way however - and I initially didn't, prompting a rewrite of major parts. Finally, the blog post that prompted the whole ordeal is here: http://jlouisramblings.blogspot.com/2010/04/haskell-vs-erlang-for-bittorent-clients.html Feel free to ask further questions if you want.
This is correct. I had a clear assumption that a message-passing model of concurrency was a premise - mainly because I don't believe that any other model is sufficient in the long run. There are other models, Event-oriented stuff as in C/C++ and Node.js comes to mind (using what is essentially delimited continuations). Haskell may or may not fare better if you embrace this. I ended up with a `forkIO` + `STM` model though implementing this, mainly because I feel it is the right way to address the concurrency behemoth. The alternative you could try doing in Haskell is to base the code around memory transactions on shared memory via `STM`. Someone else have to do that though, since I am no believer in anything shared :) Using anything else than Message Passing in Erlang is nearly impossible. You could use a tuple-space model (look up LINDA), but it wouldn't be pretty, nor fast. 
Font-lock (code coloring) on Emacs is mostly language independent. Here's a [pic](http://i.imgur.com/QGTea.png) of mine with the [code](https://github.com/jschaf/dotemacs/blob/master/colors.el). You can also browse the [Emacs color themes](http://code.google.com/p/gnuemacscolorthemetest/) and pick your favorite.
[zenburn](http://emacs-fu.blogspot.com/2010/04/zenburn-color-theme.html) 
For the sake of balance (and fanning the flames) - which colour scheme's do people use in vim?
Anyone have the color scheme for emacs that was used in the examples in the book "[Learn You a Haskell for Great Good!](http://learnyouahaskell.com/making-our-own-types-and-typeclasses#recursive-data-structures)"?
Right now, [sorcerer](http://jeetworks.org/sorcerer)
I like zenburn, but found the yellow/gold a bit bright, and so had to tweak it to something a bit more modest.
I made for myself dark and light haskell color themes. http://paste.lisp.org/display/120380 
Here's [my solution](http://stackoverflow.com/questions/5188286/idiomatic-efficient-haskell-append/5262928#5262928).
The interfaces to MVar and IORef should be more similar (to minimize cognitive overhead). Specifically, from IORef eliminate: modifyIORef :: IORef a -&gt; (a -&gt; a) -&gt; IO () and add: modifyIORef_ :: IORef a -&gt; (a -&gt; IO a) -&gt; IO () modifyIORef :: IORef a -&gt; (a -&gt; IO (a, b)) -&gt; IO b withIORef :: IORef a -&gt; (a -&gt; IO b) -&gt; IO b Additionally, strict versions of the modify functions are needed to avoid space leaks in some cases (e.g. 'modifyMVar_ succ' leaks). Add: modifyIORef_' :: IORef a -&gt; (a -&gt; IO a) -&gt; IO () modifyIORef' :: IORef a -&gt; (a -&gt; IO (a, b)) -&gt; IO b modifyMVar_' :: MVar a -&gt; (a -&gt; IO a) -&gt; IO () modifyMVar' :: MVar a -&gt; (a -&gt; IO (a, b)) -&gt; IO b 
I believe it was Eric Meijer who said: &gt; Recursion is the goto of functional programming
If you load the page and see: &gt; [...] Instead, observe the following: &gt; &lt;html&gt; &lt;head&gt;&lt;/head&gt; &lt;body bgcolor="white"&gt; &lt;center&gt;&lt;/p&gt; &gt; &lt;h1&gt;301 Moved Permanently&lt;/h1&gt; &lt;p&gt;&lt;/center&gt;&lt;/p&gt; &lt;hr /&gt; &gt; &lt;p&gt;&lt;center&gt;nginx/0.7.67&lt;/center&gt; &lt;/body&gt; &lt;/html&gt; &gt; As a nice bonus, the definitions^3 above are point-free. ...YSK it seems that this blog post needs javascript permitted for his domain *and*, of all things, gist.github.com -- and it doesn't degrade gracefully in the presence of NoScript or similar.
It does seem to me that it'd be a relatively straightforward code transformation to make either one do the work of the other.
If you can disregard the effectiveness of the code and treat a computer as an ideal environment that carries any computation instantly, then yes, by all means replace recursion with higher order functions. But what about normal (not ideal) computers ? Lets take a fold. It will process the whole list even if the answer is already found at the beginning and no need to proceed further. With recursion you can short-circuit computation. With fold you can't. Or you will have to create a new fold like function with additional parameter that determines when to get out. EDIT: I stand corrected. I've been [enlightened](http://www.reddit.com/r/haskell/comments/g1ajf/recursion_is_a_lowlevel_operation/c1k8h1e) by haskell gurus. 
You must be new at Haskell. Try it and see.
Sure, why don't you try it: fast: foldr (\x a -&gt; if a &gt; 100 then a else a + x) 0 (take 10 (repeat 51)) slow: foldr (\x a -&gt; if a &gt; 100 then a else a + x) 0 (take 1000000 (repeat 51)) 
If recursion is the goto of functional programming, then what are continuations? :)
iirc, he meant general recursion
Is there a torrent available?
Not yet.. soon. Though the new machine is holding up nicely.
I wondered what that was about. I thought it was some wit that I was failing to comprehend...
Actually you can short-cut computation with foldr. What your computation does is sum the last few elements of a list until you are over 100. Consider this example instead: foldr (&amp;&amp;) True (True:False:undefined) The issue is that (&amp;&amp;) doesn't necessarily force the evaluation of it's second argument, whereas your function does. However, it is true that you can't short-cut the Prelude's foldl; you are guaranteed to examine the spine of the entire list, although you might not examine the elements of said list. I don't entirely buy the thesis of the post, I think it's a little overly reactionary, but in Haskell there is often real benefit to using foldr. It's often just as efficient as explicit recursion, and it can even be more efficient because foldr is compatible with GHC's list fusion rules.
Does it come with SDL, OpenGL, GTK and Glade? Last I tried to get those working with GHC in windows it was a nightmare.
OpenGL. To get the others, use "cabal install"
you could compile your language to ocaml (unless your language has something really non-trivial)
You really don't need an IOref for anything here. Just a set of semantics for what it means to call a function with an argument, and what that does to some stack or environment. While not directly relevant, you might want to look at something like this very simple scheme-&gt;C compiler from matt might: http://matt.might.net/articles/compiling-scheme-to-c/ Along the way he introduces the idea of closure conversion, and a number of other constructions that will be useful if you go to make an interpreter that does more than track a list of environment variables and a context.
me too, at first, heh.
&gt;The issue is that (&amp;&amp;) doesn't necessarily force the evaluation of it's second argument, whereas your function does. Not true. You can do the same thing with my example: foldl (\a x -&gt; if a &gt; 100 then a else a + x) 0 ((take 10 (repeat 51)) ++ [undefined]) But it still will traverse the entire list (without error), even though no values from it are evaluated. Why the &amp;&amp; works i have no clue. Could be that GHC has a specific optimization for it. But then again that's incidental. We can't always be so lucky as to have functions passed to fold with magical properties like &amp;&amp;. Thus in practical situations recursion still is a viable choice over fold. 
Like lpsmith said, your code above isn't aborting when the answer is found at the beginning of your list; your code is aborting when the answer is found at the end of the list. You want to write something more like find (&gt; 100) (scanl (+) 0 (repeat 51)) or if you insist on using `foldr` find (&gt; 100) (foldr (\x k e -&gt; e : k (e + x)) (:[]) (repeat 51) 0) (Note: my function doesn't have exactly the same behaviour as your function. I could make a function that behaves like your function above, using `foldr`, but your function has an awkward specification. If I saw your code in real life I would expect it to be wrong in some way). Edit: If you want you can even fuse the `find` with the `foldr` by hand. foldr (\x k e -&gt; if e &gt; 100 then Just e else k (e + x)) (const Nothing) (repeat 51) 0
Ah thanks. That's a neat trick. Both with scanl and with fold chaining infinite list of intermediate results. I learned something today. And i concede. 
On the "contents" page http://hackage.haskell.org/platform/contents.html , the link to the "text" package is wrong. It's currently linking to "time".
That background is horrifically compressed :\ Edit: ah, it's being stretched. Can we get rid of `background-size:cover`?
Thanks, fixed.
Thanks!
You might want to take a look at [PLAI](http://www.cs.brown.edu/~sk/Publications/Books/ProgLangs/2007-04-26/) Chapter 10, "Implementing Recursion" (and preceding chapters for context). The book uses Scheme boxes, apparently in a similar way to how you plan on using IORefs.
I just realized your original function is not has hard to implement as I thought it would be. foldr (\x k e -&gt; if e &gt; 100 then e else k (e + x)) id (repeat 51) 0 
If somebody (ekmett, louis wasserman, etc.) wants to jump in with a good explanation of replicateA and sequenceTree, or a further citation, that would be great!
Thanks for the extra info. I just checked the blog's very short .htaccess, and I can't see any sign of change. And *none* of the .htaccess files in the domain have been changed in two years, according to their modify dates. My httpd.conf also looks good. I suppose there could be a smart exploit that moves my .htaccess and/or httpd.conf aside and substitutes a harmful replacement.
&gt; ...if you go to make an interpreter that does more than track a list of environment variables and a context. Actually, the point is to make an interpreter that *just* tracks a list of environment variables and a context. Is supposed to be a very, very lightweight language (i.e. one I'm not going to spend more than 24 hours implementing). Specifically it is the compile-time level of a [two-level language](http://www.reddit.com/r/haskell/comments/fgn42/research_on_twolevel_fp_languages/). Since it is specifically designed to be the compile-time level, I don't intend on doing serious compilation on that language. I suppose I could build 
Yes, precisely. But is there a more "Haskellish" way of doing something similar?
(reddit is acting weird to me...) I suppose I could build a bytecode interpreter to compile to, but really, it's supposed to be something simple to implement.
Right, you changed from `foldr` to `foldl`, which has different strictness properties. By dropping the `[ ]` around `undefined` in your latest example, you will get an error: foldl (\a x -&gt; if a &gt; 100 then a else a + x) 0 ((take 10 (repeat 51)) ++ undefined) This is because the spine of your list is fully defined, whereas the spine of my variation has an error. foldl always looks at the spine, although it might not look at the elements. Also foldr will return an error either way. This has nothing to do with short circuits, but rather because you are summing different elements: foldl (\a x -&gt; if a &gt; 10 then a else a + x) 0 [1..10] foldr (\x a -&gt; if a &gt; 10 then a else a + x) 0 [1..10] Finally, there is no mysterious optimizations with respect to (&amp;&amp;). You don't need undefined in my original example, this works too: foldr (&amp;&amp;) True (False : repeat True) You can also try this with foldr mappend EQ (EQ : LT : undefined) foldr mappend EQ (EQ : LT : undefined : []) foldl mappend EQ (EQ : LT : undefined) foldl mappend EQ (EQ : LT : undefined : [])
Yes, i saw the light in this [explanation](http://www.reddit.com/r/haskell/comments/g1ajf/recursion_is_a_lowlevel_operation/c1k8h1e)
Done.
I once implemented recursion in a simple interpreter by tying the knot, representing the environment by creating a cyclic data structure. But in a simple environment-passing interpreter, you don't even need this.
Any hope of getting OSX 10.5 support in there?
Yah, that didn't work with the penultimate release on Win 7, lots of build errors ensued.
Thanks for all the hard work! For Ubuntu, is the recommended method to build from source or is there some other option I am missing like a PPA?
Have you looked at [Write Yourself a Scheme in 48 Hours](http://jonathan.tang.name/files/scheme_in_48/tutorial/overview.html)? You may be able to glean something from the way he handles lambdas and lets in Lisp.
Corresponding Arch Linux packages moved out of testing to main repositories. pacman -S haskell-platform # to get the whole shebang once your mirror is up-to-date
Thank you dons and all the others who are involved for this new HP!
comefrom?
Best thing on the internet!
Having interrecursive functions I would go with lambda lifting. This is a simple preprocessing step and you won't need to change the interpreter code itself. If you don't want to add lambda lifting, you may want to collect all local symbols before function application and add them to your context on function application.
What I need is something like Scheme `letrec` or Common Lisp `labels`. I can't find anything there that seems to handle something similar
It's still horrifically compressed, though — lots of artifacts.
don't have the time to check now, but i believe we've met before here: http://stackoverflow.com/questions/3381741/haskell-or-ocaml-with-opengl-and-sdl-precompiled-distribution-for-windows is this issue fixed?
The main problem seems to be that GHC 7.0.2 no longer supports 10.5, [contrary to what is written on the GHC download page](http://www.haskell.org/ghc/download_ghc_7_0_2). Concretely, I only experienced two real issues getting the Haskell platform working on Leopard. First, [the precompiled GHC cannot link executables on Leopard](http://hackage.haskell.org/trac/ghc/ticket/4996), and second, the [unlit application](http://darcs.haskell.org/ghc/utils/unlit/unlit.c) needs to be recompiled as the compiled version uses linker features not available on Leopard.
Thanks for catching that. I have a misbehaving Wordpress plugin that needs attention. 
Anyone else having trouble getting the new OS X 64-bit HP to compile FFI-enabled librares after installing Xcode 4? It's reporting trouble finding standard headers (stdint.h, float.h, etc.) Also, when trying to go back and install the 32-bit version, the installer is failing.
Laziness does let foldr avoid traversing an entire list, but foldl is still a problem. Eventually it's probably quicker to write a recursive function than to instantiate the proper [recursion scheme](http://comonad.com/reader/2009/recursion-schemes/), but foldl-with-early-exit isn't quite there. For example, you could use foldExit : (a -&gt; b -&gt; Either a a) -&gt; a -&gt; [b] -&gt; a foldExit _ a [] = a foldExit f a (b:bs) = case f a b of Left done -&gt; done Right a' -&gt; foldExit f a' bs Oleg's [Iteratees](http://okmij.org/ftp/Streams.html) use a similar type for client functions - if the driver function is getting values by reading from disk or the network, you really want to be sure it doesn't read more than the client needs.
My understanding is that it due to GHC's use of dtrace, a built of GHC will be pinned to the version of the OS it is compiled on. So a GHC 7.0.2 compiled on 10.6 will only run on 10.6, and one compiled on 10.5 will only run on 10.5. The binary distribution of 7.0.2 from GHC central was compiled on 10.6, and Haskell Platform 2012.2.0.0 installers were based on that.
allright. getting SDL to work with Haskell on Windows 7. Took me several hours to get right. [this](http://www.animal-machine.com/blog/2010/04/a-haskell-adventure-in-windows/) is a guide on getting them to work, the only guide I found that did actually work. [here's](http://www.sendspace.com/file/3uk3av) the sourcecode for the sdl libraries and haskell sdl libraries. basic SDL (the "glue" ;) ), sdl-ttf (for font rendering), sdl-image (for images/textures), sdl-gfx(for primitives like triangles, rectangles and so on, this was a particular pain in the ass to get to work). the ones with an underline are the SDL packages (the headers/libs of which you need to reference in your project), the ones with a hyphen in their name are the cabal packages, already modified to work out of the box. Extract this to C:\temp\ (if you put them in another path, you'll have to change the path in the following commands as well as in the cabal files of the haskell packages), then install all the cabal packages (the ones with hyphens) by cd'ing into the corresponding directory and running: &gt; runghc Setup.lhs configure &gt; runghc Setup.lhs build &gt; runghc Setup.lhs install this will install the cabal packages. Next, put all the dll's [here](http://www.sendspace.com/file/dianrm) into your project folder (where your .cabal file resides) and add (or change existing entries) this to your cabal file: &gt; build-depends: SDL -any, SDL-gfx -any, SDL-image -any, SDL-ttf -any &gt; extra-libraries: SDL\_gfx SDL\_image SDL\_ttf &gt; extra-lib-dirs: C:\temp\SDL\_gfx-2.0.22\lib C:\temp\SDL\_image-1.2.10\lib C:\temp\SDL\_ttf-2.0.9\lib and then import whichever module you need. now you'll get most of those files from the post I linked to at the beginning, except sdl-gfx. I had to search for the gfx files on multiple sources and the headers and dll and so on are from different places, but it works flawlessly for me. I couldn't get sdl_gfx to compile in mingw, so that's the only solution that worked. Hope this helps :) [edit] oh and does anyone else get the "You broke reddit" page when visiting [eatflamingdeath](http://www.reddit.com/user/eatflamingdeath)'s profile?
I wouldn't move to a new version for X-Code! What makes you think I'll move for GHC!? */me orders snow leopard.*
For all the arguing about foldl and foldr in the specific world of Haskell, this is the _real_ answer, applicable to any decent functional language. If you need something like this, go ahead and write something like what skew has, the point is that you shouldn't _manually_ write this over and over with the individual "f" inlined every time. Pull it out into a combinator. You are not "obligated" to use only provided combinators; quite the contrary!
I'll play around with it.
Did you get this working on 32 or 64bit win7. I've never managed to succeed using 64bit.
64bit Win7 (which might be the reason I couldn't get it to compile in mingw). But then again, it shouldn't really make a difference as it's 32bit either way (no 64bit Haskell on Windows)
woot! that's cheap! also I'd like to say that to everyone who emailed me recently and I haven't wrote them back, that's because I'm a big poopy-head and I have a backlog of unanswered email but I will reply to them all!
Indeed, it is good to write your own combinators. However there is some advantage to writing these new combinators in terms of old ones foldExit f x l = foldr (\b k a -&gt; either id k (f a b)) id l x Now, assuming `foldExit` is inlined, which it can be since it is no longer recursive, it can be subject to GHC's `foldr` fusion optimizations. Whether it is worth defining `foldExit` this way, I don't know. But it isn't without merit. BTW, not every recursive function can be written efficiently with `foldr`. It just so happens you guys haven't picked any such examples yet. :D
Agreed in Haskell. In other languages that are less slick I'd write the equivalent of skew's post directly. For a putatively functional language, Erlang makes working with this sort of thing oddly annoying, for instance. It can be done, but the syntax quirks are actively annoying and combining lots of combinators together gets even more annoying. Possibly because I know enough Haskell to see the BS syntactic hoops I'm jumping through.
What about using combination of find with scanl, like roconnor presents [here](http://www.reddit.com/r/haskell/comments/g1ajf/recursion_is_a_lowlevel_operation/c1k8h1e) find (&gt; 100) (scanl (+) 0 (repeat 51)) This construction avoids traversing the entire list. 
 find pred (scanl step x0 (repeat v)) is equivalent to foldExit (\a b -&gt; let a' = step a b in if pred a' then Left a' else Right a') x0 (repeat v) If the test involves the value from the list as well, it won't work. It's also not right if the fold might reach the end without taking the early exit. An example that doesn't work with just a find like that is foldExit (\sum x -&gt; if sum == x then Left sum else Right (sum + x)) 0
Why oh why are those graphs drawn with an inverted x-axis?
Common for updating time logs.
Ohhh, it's inverted. I was like "It makes bandwidth go... down?"
For anyone still wondering: Yes, we must build HP from source.
desert -- Built in scheme ':colorscheme desert'
&gt; The main problem seems to be that GHC 7.0.2 no longer supports 10.5, contrary to what is written on the GHC download page. ::sigh:: I thought we all talked about this on the list...
Yesod gives an error on ghc 7.0.2: http://hackage.haskell.org/trac/ghc/ticket/5004 I have to revert to previous ghc. 
This is caused by the monomorphism restriction with does stupid things to definitions that have no parameters. You can either eta expend your definition, give a type signature to your function, or add a `NoMonomorphismRestriction` pragma to your module. I suggest the second.
Looks to me like you might be running into ghc's extended defaulting rules; I forget the details, but rle :: Eq c =&gt; [c] -&gt; [(c, Int)] rle = map (head &amp;&amp;&amp; length) . group should work.
Thanks! Hate having to specify my own types though.
Thanks. Is there any reason *to* have the monomorphism restriction in that case? And is this something I can get in ghci?
You can lift the monomorphism restriction in ghci by either invoking as `ghci -XNoMonomorphismRestriction` or by issuing `:set -XNoMonomorphismRestriction` at the repl. As for why the restriction exists, it's a bit complicated. Rather than trying to summarize, I'll just point you to the Report: [4.5.5 The Monomorphism Restriction](http://www.haskell.org/onlinereport/haskell2010/haskellch4.html#x10-930004.5.5). In particular note the "Motivation" bit.
It's not actually extended defaulting here, it's the monomorphism restriction. Lifting the restriction makes ghci infer the correct (generalized) type.
I suppose I'll have to trust SPJ et al on the decision, I read the motivation and it seems to be benefitting the computer over the programmer. Thanks for the answer.
uh-oh, I consider 10.5 support pretty much a blocker... 
The reason behind the monomorphism restriction is that if you write magic_constant :: (Num n) =&gt; n magic_constant = expensive_computation then for most (all existing?) implementations of Haskell, `magic_constant` will re-run the computation every time you want to use it. This is because, basically, `magic_constant` is normally implemented as a function magic_constant :: {dictionary of all the Num operations for type n, such as (+)} -&gt; n Normally, if you omit a type signature, the most general type (like the one above) will be inferred. So very early on the Haskell committee was afraid that Haskell novices would find this confusing and frustrating, so it was decided that if you wrote such a definition and omitted the type signature, the type system would infer a single concrete type, such as `Integer`, so that constants really would be constants.
It's a combination of implementation concerns and worry about unpredictable operational behavior, which is a programmer concern. All of the practical Haskell implementations support lifting the restriction, but none do so by default (for compatibility with the Report). Now that you're clued in to its existence, you should be able to recognize when you're hitting it and either add a type annotation or use -XNoMonomorphismRestriction. Finding a clean way to phase out the restriction is a perennial topic of conversation in Haskelland.
You can blame John Hughes for the monomorphisn restriction. IMO, it has never been a good idea. A warning would have been better. 
Yea, not supporting Leopard is a big mistake. 
Why? It took me a while to get into it, but now I write out my types _before_ I write my functions, and it stops me from making certain mistakes. You write what you _want_ to do before you write _how_ to do it, so it's a way of checking your work as you go.
I generally love specifying my types. The type for a function is usually pretty obvious. The implementation -- not as much. So, I write down my type first (which is likely to be correct) and then hack my code until the type-checker is happy. And then it usually works. But that is just personal taste of course.
Also, if it's a general-ish function, you can hoogle the type signature to see if something like it has already been done.
It's all good, when you know what you want. ;)
Does quora have an rss feed for Haskell like /r/haskell does, or like the SO Haskell tag?
The only confusing thing is that the labels use left-to-right text direction. A language written from right to left (better even: in columns from right to left, like classical chinese) would be clearly superior here.
I write the type signature if I'm trying to figure out how to build up a function, if I already know then I don't like to bother. This will almost certainly change as I write more complex software.
[Yes.](http://www.quora.com/Haskell/rss)
&gt;my wife walked for four hours with our son and finally got together yesterday. woah
Well it is good to know that 住井先生(Sumii-sensei) is safe. I am a foreign exchange student who is a member of the 小林・住井(Kobayashi-Sumii) Lab and from what I hear from most anyone I can contact (most peoples phones are now dead) is that the university was spared for the most part. One person I had contacted was in their lab (this was earlier on the 12th). One question I have is was 住井先生 in the lab or was he elsewhere during the quake (building subway in front of lab = large drill taller than building...)
Yeah, I'd been hearing people walked 5 hours to get across the city. The scale of Tokyo boggles the mind.
Yep, it also depends on whether they walked 5 hours in the right direction (i.e. walking a while down a road following train tracks just to find out they were the wrong ones by the police box.... that was fun)
My heart goes out to everyone in Japan who was affected by this cataclysm.
Thank you!
My orangered has been broken for the last week. No clue why. D: I had no idea how wretched that would be, until now.
Fuck yea Tokyo-ites are tough, Fuck this quake we're having a study meeting and programming competition anyway.
In case of a public transport failure, the same would happen in any city. My university town has only 400,000 people and it would still take about 2-3 hours to walk from one end to the other.
Well, indeed "Haskell's not quite the perfect EDSL host", however, for Eq/Ord/etc one possible solution is to use NoImplicitPrelude, redefine all the standard type class hierarchy, then define instances of the new type classes for both the standard Haskell types and the new DSL types. Something like: class IsBool b =&gt; Eq a b | a-&gt;b where (==) :: a -&gt; a -&gt; b Of course, there will be issues with overlapping/etc instances and all the usual annoyance. Very often I wonder if it wasn't easier to simply write a full-blown compiler for the DSL...
I'm using Conal's Boolean package now.
I am Kazutaka Matsuda, also a member of Kobayashi-Sumii Lab. As far as I know, Prof. Sumii and Prof. Kobayashi were not in Sendai during the earthquake. 
Apart from the widespread transport and mobile phone network breakdowns, Tokyo is largely unaffected. Outside, it looks like any other Sunday, and I expect pretty much eveyone will be going back to work tomorrow.
Beautiful proofs :) About all I can suggest is that you pick a column width, say 80 or 100 characters, and try to avoid going past it. That'll make your code a bit more readable (a lot of us hate side scrolling). I've been thinking about picking up a Haskell book lately. Is this one that you would recommend for someone mathematically inclined?
Perhaps you noticed already that this can be circumvented by disabling wai-handler-devel. e.g. if you use cabal: cabal install yesod -fproduction Of course, you'll miss some of the dev functionality, but I've understood that the author is going to scrap that anyway in the next version. Meanwhile, I'm checking whether a simple disabling of binary stripping during build will fix this.
[Here](http://ompldr.org/vN3NpZg) are the results of HLint on your code. ;) Of course, HLint sometimes doesn’t get the point: &gt; ./chapter2.hs:125:1: Error: Use map &gt; &gt; Found &gt; &gt; myMap f [] = [] &gt; myMap f (x : xs) = f x : myMap f xs &gt; &gt; Why not &gt; &gt; myMap f xs = map f xs
Very nice :) You did all this since the afternoon of March 11th ? I usually just end up on reddit, and don't get much done :(
It's also amusing that hlint would reject its own generated code (it would suggest `myMap = map`, of course).
I use [Wombat](http://dengmao.wordpress.com/2007/01/22/vim-color-scheme-wombat/).
What I was trying to say that at the cost of redefining half of the standard library, you can replace the prelude with this (and more, similar) code. So that you can have the old symbols. Whether it worths it depends on how much usual Haskell you want to mix with your DSL.
Fixed-points to the rescue!
Thank you, I'll definitely be keeping the column width in mind. I'm aware of four books that talk about Haskell from a more mathematical perspective. This one and "Haskell Road to Mathematics" use Haskell to teach discrete mathematics concepts. "Algorithms a Functional Approach" uses Haskell to teach algorithms. Finally, Richard Bird and Philip Wadler's "Introduction to Functional Programming using Haskell" teach haskell from a "program derivation via specification" approach. I previously worked through about 6 chapters of Bird and Wadler before I couldn't handle it. I've found that this book is a gentler way of teaching some of the same things that Bird and Wadler, although Bird/Wadler have more advanced topics.
Thank you for doing this. I'm going to set up HLint with my emacs right away :). (((I have too many unnecessary parens in my code )))
Thank you :). This was done over the afternoon of March 11th and some more on March 12th. The hardest part was just starting, but once I did, it was hard to stop... and before you know it you've worked through 3 chapters of a book :D :D.
[See FAQ](http://community.haskell.org/~ndm/darcs/hlint/hlint.htm#faq) .
He is not going to scrap it. Just decouple it from yesod package. And developing web applications by restarting them every few minutes is tedious. So yes, wai-handler-devel functionality is important to me. Especially considering that the fix is a simple recompiling of ghc. 
In this case, the primary concern is mixing with existing Haskell code, not creating a standalone language.
Interestingly, simply disabling stripping did not seem to help with this issue. *edit:* scratch that, it does help. My mistake.
enjoyed the read, especially the last point about refactoring.
Doesn't lazy evaluation take care of this? I thought magic_constant was not evaluated until needed and then all uses of it refer to the same data.
tl;dr Haskell is good at: * DSLs * giving names to ideas * refactoring
OP's definition of "hard problems" is very narrow. It is not a secret that any non trivial software system over the time grows so much that its maintenance becomes a "hard problem" itself. Thus you might say that any big enough software fits the definition of the "hard problem". Haskell with its purity, referential transparency and very powerful type system, forces programmers write highly decoupled, modular code. And thus should affect long term maintenance of big projects in a positive manner. 
this looks pretty exciting and useful!
or: myMap' f xs = [f x | x &lt;- xs]
A reasonable analysis actually; I find that I agree with the ideas presented and I think that point three was the most powerful. Getting bitten in the rear by a refactoring change that is an obvious mistake is, from experience, very painful. That Haskell greatly reduces that problem is a godsend. Even Java, though a strictly typed language itself, does not do as well on that front IMO. The type system is much better in Haskell.
As the author, I actually thought the third point was the weakest. Go figure.
Those power point slides are barely useful Here is a [much more detailed publication](http://padsproj.org/papers/Forest2010.pdf) This is [An overview of the PADS project]( http://padsproj.org/papers/icdt11.pdf) which this is under.
Huh, and here I thought that you ordered them that way so as to end with a 'finishing punch' so to speak. I still feel that point three is the best but maybe it shows that I lack a better appreciation of points one and two which are both about naming.
Re-implement standard POSIX utilities e.g. cat, find etc.
This is actually quite fun. I reimplemented tee recently using iteratees and it worked extremely well.
There are a number of libraries in other languages that try to extract the main content from a web page, stripping out banners, links, footers, ads, and so on. The most well-known is Readability, a javascript library with clones in ruby, perl, PHP, and python. As far as I know, there's nothing like this in Haskell -- it might make a good project. 
I think what you called "naming things" may be better expressed by "abstracting things". Other languages can give a name to Monoid (in some meaningless form, at least), but they cannot abstract the notion of a Monoid.
Well, the thing is that the first two points are kind of hand-wavy and require lots of effort and thought to be usable. The type system is there with you always.
&gt; I thought magic_constant was not evaluated until needed True &gt; all uses of it refer to the same data. False: since it is (likely) implemented as a function of the type-class dictionary, it does not share the result. Also, it isn't necessarily the same data at all, as `magic_constant :: Integer` is a different computation from `magic_constant :: Complex Double`.
Programming in the large is verbose in haskell since there is no context/type dependent name lookup like in oop. Programming in the small can be very terse and abstract though. 
&gt; Programming in the large is verbose in haskell since there is no context/type dependent name lookup like in oop. Record lookup is not that common of an operation in Haskell, so it is unlikely to add a lot of verbosity. Also, the popular convention is to prefix records with a very short acronym of the record, so that extra verbosity is pretty low. There are many Haskell features that greatly reduce verbosity at all project sizes. Every Java/C++ piece of code I've converted to imperative Haskell code tends to end up *less verbose*, despite imperative Haskell not being Haskell's main focus. I've not written a very large project with Haskell (it is indeed still hard to convince many workplaces to take on a large project with Haskell), but on medium-sized projects, Haskell has been far more concise than most OOP languages. Roughly the size of Python code, without the unnecessary dynamism and with far better performance.
Yes, but I meant that a specific instantiation like magic_constant :: Integer is evaluated only once and not multiple times.
GHC doesn't memoize the result of functions, not even functions of implicitly passed dictionaries. So even (magic_constant :: Integer) will be recomputed every time it is used.
I really don't think you gain a whole lot from implementing something else that someone thinks you should implement. You need something that you care in the least bit about in order to give you the drive and passion to really learn the language, and ensure you get to the best solution you are capable of. I've recently started keeping a list of project ideas around - in case I ever have a weekend free and want to try something new. At the moment I have: a few websites I'd like to try coding, I'd like to try and write a relational database engine (sans SQL), a little recipe/food logging application, a MUD server. I doubt they're anything that will ever really be good enough to release, but maybe in working on them I'll say "hey, I could really do with a library for x, that seems like it would be good to share as well" (case study - I'm working on some Haskell bindings to the Xapian search library). I know it's not exactly what you're looking for, but I think if you slow down and wait for your own idea to come to you, you'll have more luck.
&gt; despite imperative Haskell not being Haskell's main focus. Whatever happened to Haskell being the finest imperative language in existence?
Writing type signatures causes me so much work! All that moaning from the compiler when my code is broken. Without it I can just infer a broken type!
Take any library that doesn't build on the new GHC7, and update/fix it. For example: http://hackage.haskell.org/package/Vec http://hackage.haskell.org/package/blas 
Oleg - doing stuff you would never have imagined possible in ways you can't understand since 1995. 
very right. We should be little bit egoists and write the code for our own usage. At least for me it is very hard to create anything I'm not interested in or is not directly useful for me. 
I blogged a response: http://mtnviewmark.wordpress.com/2011/03/14/programming-niches/
I have an interesting task, but I'm not sure whether it's suitable for Haskell beginners. I'm writing a small program and [FRP library](https://github.com/HeinrichApfelmus/Haskell-BlackBoard) (functional reactive programming), but it's not finished yet. In particular, I'm looking for someone who uses the FRP library to write a small GUI example with wxHaskell. Drop me a line if that sounds interesting.
Implement a personal URL shortener. Oh, I have [one on github](https://github.com/noteed/hortened) that needs some care before I can upload it to hackage
I'm glad somebody understands.
That's tongue in cheek. There is no such thing as "finest imperative language" as it of course depends on your specific purpose. Haskell is a pretty great imperative language, despite it not being its main focus. Most (all?) Java code, for example, when translated directly to imperative Haskell, will be shorter in Haskell.
Include some screenshots on the web page plz.
Implement mathematical special functions (e.g. Airy, Bessel, Spherical Harmonics, etc.) You've got gsl as a reference, Abramowitz and Stegun as a guide, and each one is a pretty bite-sized piece of code. If you feel like a little more code, do interpolation and differential equation solvers.
just uploaded two examples, more to come...
Thanks!!
Kind of disappointed. The subject seems really interesting, but the author decided to spend most of the article on a tangent about parallel programming.
See also [/r/haskell_proposals](http://haskell_proposals.reddit.com).
Actually, this one looked pretty understandable to me. Here's my attempt at a tl;dr, so that you can try using this yourself next time you wish for it. 1. Express your complicated invariant in the type system. His "UNUM" class is a bog-standard type-level natural. The "HeightC" class expresses the fact that "lheight" and "rheight" are no more than one apart. 2. Use existentials to reify your type-level data as value-level data, and write a function that checks the same invariant as before but at the value level. This is the "uval" function, along with "check" in the BU instance of HeightC. 3. Write your datatype, but using separate types for each "constructor" that you would normally write, so that you can use the type-level invariant you cooked up. The instances of this invariant you wrote in step 1 give you the static version; the instance of this invariant written using the existential from step 2 give you the dynamic version. His "Leaf" and "Node" types, along with the "BBTree" class, are the meat of this step. That's pretty much it. He did some extra work so that his tree could have values with different types at each node, but none of that was complicated (and in fact distracted from the main interesting idea of the post, in my opinion).
I would recommend Points being type synonyms for `Complex Double` rather than `(Double, Double)`, because: 1. You can use `(+)` for translation, `(*)` for scaling and rotation, etc. 2. The components of a `Complex Double` are strict, meaning better code generation on GHC. You might be interested in the relevant modifications I made to [HPDF](https://github.com/lpsmith/HasPDF/blob/master/Graphics/PDF/Coordinates.hs), along with some of the helper functions (especially `dot`) 
Thanks for the feedback, at the moment i'm using vector-space to do additions and I have to say that because the underlying structure is a vector space and not the complex plane, this feels more appropriate (and the operator takes only two characters more (^+^)). On the other hand strictness might be something helpful in this case. I will think about it.
It's great, please keep up the effort! Having a really-platform-independent graphics library with the ability to have *non-cairo* backends is very important (another example is the wumpus library). Gtk and cairo still don't work too well on Windows (and are not exactly effortless to set up for development even under *nixes).
Do you mean you've put screenshots on the webpage? I can't find any and I haven't got craftwerk to build either so it's hard to get an idea of what the output is like.
My favourite is the partiality monad as seen [in Agda](http://www.cse.chalmers.se/~nad/listings/lib-0.5/Category.Monad.Partiality.html). Sometimes I port it over to Haskell, though admittedly it’s less useful if the compiler doesn’t enforce totality, like using an IO monad in Scala, or whatever. I still like it though. :)
Oh, I realize that punning complex numbers with vector spaces might not make a mathematician very happy, but I've found it to be a useful one that isn't too problematic in practice. I wasn't really aware of vector-space though; thanks for pointing that out!
&gt; using IORef on the value slots of the variables closure structures, but is there a better way? I'm not 100% sure what question you're asking here, but if the question is "how do you build the closure in an interpreter, where in other languages you would use mutation?" then laziness (/circular programs) works well enough... newEnv = addClosures functions newEnv oldEnv We can just refer the the newly constructed environment while we're still creating it.
The screenshots are not directly on the linked page, but a link is present under the section [Examples](http://mahrz.github.com/cw_screens.html).
thx, I just wanted to put that link in here :)
Right, i think having a language with such mathematical concepts, attracts programmers who rather use a conceptually more fitting data structure than a fast one ;) I'm going to evaluate the speed gains when using strict fields for points.
good link (with funny comments). IMO this treatment of monads is a bit better (and more current) http://www.cs.nott.ac.uk/~gmh/monads
[Part 1](http://bartoszmilewski.wordpress.com/2011/01/09/monads-for-the-curious-programmer-part-1/) is also interesting.
For some reason, I get an infinite loop if I try something like this in Hugs. Since I'm [restricted to using Hugs](http://www.reddit.com/r/haskell/comments/fe9wp/looking_for_a_haskelltoc_compiler_that_is_easy_to/), I can't use this technique even if it works on e.g. GHC. Of course, it's entirely likely that I'm being overly generous with strict annotations (I seem to be using strict annotations on all structure slots, for example) so that is more likely the reason for why it fails though, not necessarily Hugs. I may try this again in the future if I have time (maybe I just need to remove strictness on the environment slot of the functions), although for now I am using a really crazy Y-combinator-like thing, wherein the function carries the code of its siblings, and upon invocation of the function I inject it to its stored environment, at the same time that I inject its arguments. EDIT: Ha, took &lt;10 minutes to switch over to lazy-circularity. In any case, it seems that 2 things made my program enter an infinite loop: 1. I needed to allocate unique `Integer` ids to each function (since I need to be able to compare functions, and give at least an arbitrary ordering), and this allocation cannot be anything but strict; 2. I did put strictness on the scope (environment) slot. Now I allocate ids in a separate step in my evaluation monad, then build the new environment in a `let` declaration. Something like: eval vmap (WithExpr withbindings subexpr) = do ids &lt;- mapM (\_ -&gt; do getUniq) withbindings let vmap' = Map.fromList (map mkKappa $ zip ids withbindings) `Map.union` vmap mkKappa (id, WithBinding name args rettype expr) = (name, ValueKappa id vmap' args expr) in eval vmap' subexpr 
I have to say this is the best introduction of Monads I have read. I've been trying to "get it" for a long time. Actually going into the maths is just the right thing IMO. There should be two types of monad tutorial: 1. The kind that doesn't really touch monads at all but shows how to use a particular implementation. 2. The kind that actually deals with monads properly. This is the best type 2 monad blog entry I've seen.
This paper looks really promising. I'd been doing `compos` by hand for some time, and was recently swayed to Uniplate when I realized that it gave me, among other things, an easier-to-define compos, albeit absent the GADT generalization. I'd been pretty excited about multiplate before, and now I'm happy that there's a paper trying to put all these techniques together in a coherent way. 
So now I know two competing definitions of lenses: * lenses—well-behaved bidirectional transformations (http://www.seas.upenn.edu/~harmony/) * A lens, also known as a function reference (this paper) I believe that the former takes precedence; this paper asserts the second definition as known, but I can't find any prior use with this meaning... 
As far as I understand, the two definitions of the lens data type are isomorphic. There are a few choices for what the laws should be. The laws used in this paper are Pierce's laws of a very-well behaved lens.
Nice, I didn't know that lenses are precisely the coalgebras for the store comonad. EDIT: Wow, having skimmed the whole paper now, this is really fascinating stuff. The key point is that lenses are isomorphic to type Lens a b = forall k. Functor k =&gt; (b -&gt; k b) -&gt; (a -&gt; k a) while biplates are isomorphic to type Biplate a b = forall k. Applicative k =&gt; (b -&gt; k b) -&gt; (a -&gt; k a) Once you know the latter, it is straightforward to generalize this to multiplates type Multiplate p = forall k. Applicative k =&gt; p k -&gt; p k where the `p` is a record that captures the data types involved data P k = { coalg1 :: A -&gt; k A, coalg2 :: B -&gt; k B } The generic transformations follow easily from that. What I would like to see is an intuitive explanation why the `CartesianStore b` is an applicative functor. 
The applicative instance for the CartesianStore b is effectively the cartesian product of stores. CartesianStore b (x -&gt; y) &lt;*&gt; CartesianStore b x &lt;=&gt; b^n * (b^n -&gt; x -&gt; y) &lt;*&gt; b^m * (b^m -&gt; x) &lt;=&gt; b^(n+m) * (b^(n+m) -&gt; y) pure a is the CartesianStore b a "point" of zero dimensions.
"... more portable than both of them. ... only requires rank 3 polymorphism." That doesn't seem accurate - Uniplate/Biplate both work on Hugs, which has MPTC but no rank 3 polymorphism. I don't know a compiler that has rank 3 but no MPTC, so I'd say Biplate was strictly more portable. I also couldn't easily see any examples of actually using Multiplate to do something (like any of the examples in the Uniplate paper) - which made it harder to compare the ease of use. Can you define transform or universe in Multiplate? If not, what are your equivalents, and how to the examples from the Uniplate paper look when written with your equivalents. Either way, it looks like a very interesting exploration of the theoretical foundations of Biplate - which is really interesting. I always assumed there would be things like fusion laws for Biplate/transform/transformBi, but I never got anyway (I didn't really look too hard). I suspect your work might provide a much better basis for moving that forward. (As a side note there are some fusion properties for transformBi, which have been implemented in transformBis in the latest version of Uniplate, but they are not very principled.) And finally a typo - it seems to be using French bibtex as all your author lists have "et" in them rather than "and" :-)
&gt; That doesn't seem accurate - Uniplate/Biplate both work on Hugs, which has MPTC but no rank 3 polymorphism. I don't know a compiler that has rank 3 but no MPTC, so I'd say Biplate was strictly more portable. Thanks for this comment. From my dependently typed background I find rank 3 polymorphism more natural than MTPC and simply assumed it was more portable. &gt; Can you define transform or universe in Multiplate? If not, what are your equivalents, and how to the examples from the Uniplate paper look when written with your equivalents. `transform` corresponds to `mapFamily` and `universe` roughly corresponds to `preorderFold`, but this correspondence is much more loose than `transform`'s correspondence. There are some worked examples on the [Multiplate homepage](http://haskell.org/haskellwiki/Multiplate), but they could use more work. &gt; And finally a typo - it seems to be using French bibtex as all your author lists have "et" in them rather than "and" :-) Latin actually. Edit: Turns out ndmitchell was right: It was French, and the use of "et" instead of "and" is a bug in TeXmacs 1.0.7.9.
A very interesting article. I have some ideas about lens-like types for other classes besides Functor and Applicative, but this comment box is too small to contain them :) (and besides, I have to think them through first) Another thing I wonder about is whether the `Plate` type could be made more generic with a `HList` approach. Something like data NilPlate f = NilPlate data ConsPlate a p f = ConsPlate (a -&gt; f a) (p f) Or maybe you could use a Lens to access the members of a Plate... :)
Here's some code that implements universe and transform in Multiplate: https://gist.github.com/873548 The multiplate function is a kind of applicative descend.
Do we have any idea how these numbers correlate to uniques? I am sure I have installed a lot of these packages over 10 times in the last year if they got version bumps. Actually, probably way more than that from using cabal-dev if it doesn't reuse local caches.
It could be computed, but we don't know yet. I have one data point: 700k HP installs corresponded to 130k unique IPs.
* Define a new class `ArbitraryOrd` which gives an arbitrary non-meaningful total ordering of the type. * Define the single obvious `instance Ord a =&gt; ArbitraryOrd a`. * Make order-balanced data structures like `Data.Map` and `Data.Set` use `ArbitraryOrd` instead of `Ord`. This way, we can use order-balanced data structures for numerous types which may not have a **meaningful** total ordering, but which permit defining an arbitrary total ordering. For example, unique IDs representing interned objects. Since they're just a newtype of Int (or Word, Integer,...) we can give a total ordering based on the representation, but that ordering is guaranteed not to be meaningful and may even vary for different runs of the program (since it takes IO to update the intern table, but this is all fine if the unique IDs are opaque types).
I'm supportive, though personally I'd go for `foo'_` instead of `foo_'`.
Technically "CIC" is impredicative and thus has no use for universes, exactly like early versions of Coq. However, these days Coq's metatheory is based on "pCIC": the predicative calculus of inductive constructions. It's the predicativity that leads to the universes.
http://hackage.haskell.org/trac/ghc/ticket/5011
Any chance of seeing these numbers on a monthly level as well?
[blackboard](http://blog.jdhuntington.com/2008/11/emacs-color-theme-blackboard.html)'s nice.
A small matter of scripting and time.
What do we do if we've installed the Haskell Platform and then installed XCode 4? Do we have to build ghc from source? Will a fixed-up Haskell Platform version be available soon?
Cool! I suggest any Multiple library contains those two functions at least - they tend to represent 90% of real world uses of Uniplate.
Neat, I guess final issue is performance - have you done any comparisons? A rank-3 type could make it faster than the Uniplate structure, although these things are hard to tell in advance.
Yes, soon enough, see [http://haskell.1045720.n5.nabble.com/7-0-3-td3710427.html](http://haskell.1045720.n5.nabble.com/7-0-3-td3710427.html)
Ah! Basically, you apply the functions `x -&gt; y` to every element in the store for `x` and collect everything in a big store. Thanks!
Just add Google Analytics to the site. It's free and will compute all this for you, more accurately than one can do on one's own.
I haven't looked at performance, but that is something that ought to be done. Measuring performance isn't something that I'm skilled at yet. I'm more of a theory kinda guy. :) 
Google Analytics won't be able to track cabal installs, will it?
sjoerd's code is more or less the same as my half completed Uniplate compatibility module. I suppose I should complete it and stick it into the Multiplate package. The problem is that `universe` doesn't extend very well to mutually recursive data types. To effectively build a `universe`-like function it you have to create a third data type which is the disjoint union of all the data types in your mutual recursion: data Ceramic = { ceramicA :: A, ceramicB :: B, ceramicC :: C} ceramicPlate :: Plate (Const [Ceramic]) ceramicPlate = Plate (Const . return . ceramicA) (Const . return . ceramicB) (Const . return . ceramicC) universe :: Plate (Const [Ceramic]) universe = preorderFold cermanicPlate I speculate the most uses of `universe` for mutually recursive data types is to extract only one type of child. This is easy to build on the fly. childrenB :: Plate (Const [B]) childrenB = purePlate { fieldB = \x -&gt; Const [x] } universeB :: Plate (Const [B]) universeB = preorderFold childrenB I'm hopeful that users will actually be interested in directly building some monoid `o` instead of using `universe` to build a free monoid and then filtering that away to what they want.
Great! I managed to get things working by compiling a recent GHC snapshot from scratch, but a new platform release would be much better.
Oops, replied too soon. The link only relates to the GHC release. Will the HP be modified to contain the new release?
I guessed from the "only rank 3 polymorphism" :-) You have produced something rather nice, blending theory and practice nicely, so well done!
As a workaround, you can link /Developer/SDKs/MacOSX10.5.sdk to /Developer-old/SDKs/MacOSX10.5.sdk provided you had xcode 3 whatever and upgraded to 4. seems to work for me anyway.
Thanks! I was looking for a way to do that.
It seems likely. [dons said](http://www.haskell.org/pipermail/glasgow-haskell-users/2011-March/020187.html) that &gt; We may do a minor rev of the HP at that point too, to set us up for 2011.
It occurs to me that there may be/should be a close connection between lens/biplate/multiplate and memocombinators, but I haven't quite worked it out yet?
Yes, you're probably right. I couldn't find the original quote so I'm bound to have mis-remembered it.
Nope, but it will show you a better idea of uniques visiting the hackage pages than just uniq'ing the IPs will.
Can't you do the same potentially with the new sdk?
I have not yet looked at the benchmark code, but I suspect two probable causes: * haskell libraries didn't exist for some things that did exist in ruby, and this bulked up the haskell code unnecessarily. * the ruby benchmark code is smaller because it is overly naive in implementation, which is why it's concise. Just curious about other people's input. Not trying to start a flamewar; I love and use both languages regularly.
That code is scary. I really wish there was a version of the shootout that required clean, idiomatic code, rather than the current version where many of the programs are about 90% ugly micro-optimizations.
The issue is some parts of the ~~ghc system~~ haskell platform are specifically looking for resources in the /Developer/SDKs/MacOSX10.5.sdk path. if the path exists, you're good. if you build your own ghc, you can use 10.6 sdk (i think). this is a second hand from haskel-cafe, so ... take with salt, eh?
&gt; sp &lt;- mallocBytes ssz :(
A lot of the benchmarks could be rewritten to use Data.Vector. Submit!!!
I agree. I have long wondered if it would be a good idea for the language shootout to showcase other qualities then speed, by having separate rankings, for example, by memory use, code size, speed divided by code size, and memory divided by code size. Maybe even allow for programs to specialize in one such ranking.
Vector can be used for the benchmark? I think a big deterrent for anyone is that the rules are unfathomably fuzzy.
The rules are pretty clear, but you can also negotiate that "standard" packages that are easy to install be included (e.g. regex-pcre, or vector). I am happy to negotiate on the part of anyone who wishes to submit vector-based entries.
My point was that you can symlink the new sdk to where the old one should be and cross your fingers.
But who cares who and how many hackage page visits occur, don't you care more about package downloads?
the anonymous one is beautiful! (though, using some advanced google skillz I managed to find the original, buy hey, everything is a remix anyway!)
Oh! That's kinda ... hmm. I'd worry about the linker, and if 6 is really truly backwards compatible. That's a pretty awesome and somewhat scary idea! 
It takes balls of steel to be a Haskell programmer :p.
I don't know why you don't provide an official torrent for it on the download page. That should save some bandwidth.
http://imgur.com/DRd6T I spend to much time on reddit. 
The shootout does what it does, anything else would be a different measure.
Smaller is not exactly LOC. From [the help page](http://shootout.alioth.debian.org/help.php): "How did you measure Code-used? We started with the source-code markup you can see, removed comments, removed duplicate whitespace characters, and then applied minimum GZip compression. The Code-used measurement is the size in bytes of that GZip compressed source-code file." but perhaps we need Haskell.dons and Haskell.simple :)
also, ruby can't come anywhere near to C's speed, so they don't try: it's much more impressive to leave it clean and concise. it's at least possible with Haskell, even if your code ends up looking rather C-like.
The best ideas always threaten death and destruction if they turn out to be wrong.
In the table at the end, you list "uniform Int32" twice. Mistake?
Bryan, I appreciate your eye for speedy Haskell libraries!
That's always bugged me. It gives languages with lots of redundant syntax a free ride (since most of the goes away when compressing). It may be that two languages can do the same thing in roughly the same amount of "unique pieces", but if one of them takes 3x more typing/reading to do it due to verbose syntax, I think that should show up in the "code size" metric, rather than be compressed away. There's no optimal metric for size, but IMO non-blank/comment LOC is better than gzipped bytes.
Both are interesting stats. If one wants an idea of number of unique users of Hackage, then G.A. will do a better job of that - since I think it is safe to assume that anyone who installs packages visits the site in a browser with some frequency.
There are list comprehensions, but not monoid comprehensions. It's the list comps that give universe it's power - you might prefer the users used monoid, but it's nearly always simpler to use a list.
Is there any performance gain to be had from using Vector? It seems that in most benchmarks user IOUArray, or even Foreign.Ptr. Surely Data.Vector.Mutable.Unboxed can't offer many (any) performance advantages over Data.Array.IO? 
&gt; **It turns out** that GHC's inliner was being too aggressive with the table-related code … I bet there's a lot of interesting detail hiding in that phrase.
Fixed, thanks. Should have been "normal" the second time.
Maybe it takes less code to write slower programs? Maybe slow Haskell programs can be written using less code than faster Haskell programs? Maybe slow Ruby programs can be written using less code than faster Ruby programs?
When you can't get the name of the website correct, I don't have a lot of confidence that you actually know what "many of the programs are about".
[by memory use](http://shootout.alioth.debian.org/u64q/performance.php?test=nbody&amp;sort=kb) [by code size](http://shootout.alioth.debian.org/u64q/performance.php?test=meteor&amp;sort=gz) [by speed and code size](http://shootout.alioth.debian.org/u64q/code-used-time-used-shapes.php) [by speed and code size](http://shootout.alioth.debian.org/u64q/which-language-is-best.php?calc=chart&amp;python3=on&amp;yarv=on&amp;hipe=on&amp;ghc=on&amp;csharp=on&amp;java=on&amp;sbcl=on&amp;gpp=on&amp;xfullcpu=1&amp;xmem=0&amp;xloc=1&amp;nbody=1&amp;fannkuchredux=1&amp;meteor=0&amp;fasta=1&amp;spectralnorm=1&amp;revcomp=1&amp;mandelbrot=1&amp;knucleotide=1&amp;regexdna=1&amp;pidigits=1&amp;chameneosredux=0&amp;threadring=0&amp;binarytrees=1) [by memory and code size](http://shootout.alioth.debian.org/u64q/which-language-is-best.php?calc=chart&amp;python3=on&amp;yarv=on&amp;hipe=on&amp;ghc=on&amp;csharp=on&amp;java=on&amp;sbcl=on&amp;gpp=on&amp;xfullcpu=0&amp;xmem=1&amp;xloc=1&amp;nbody=1&amp;fannkuchredux=1&amp;meteor=0&amp;fasta=1&amp;spectralnorm=1&amp;revcomp=1&amp;mandelbrot=1&amp;knucleotide=1&amp;regexdna=1&amp;pidigits=1&amp;chameneosredux=0&amp;threadring=0&amp;binarytrees=1)
&gt; I really wish there was a version of the shootout that ... Stop wishing - start doing! Use the [measurement scripts](http://shootout.alioth.debian.org/help.php#languagex) on whichever programs you sanctify and publish your results.
&gt; I have not yet looked at the benchmark code It's so much less effort to *suspect* :-)
what would be ideal is if one could get deforestation/stream fusion and avoid ever actually creating the boxed types themselves. that'd be great because it'd be useful for performance, while retaining the elegance of haskell. n.b.: i have not read up on either of these very much, and i don't understand the problems on the shootout, so they very well may be entirely irrelevant here.
&gt; IMO non-blank/comment LOC is better A metric so brittle that we can mess with it simply by formating to different line lengths :-)
Priorities, sir. I have a reasonably informed opinion based on what I know of the language, but not enough time to look at the code at the moment. As a result, I do not look at it, but at least I am forthright about the fact that I have not, and what I say is nothing more than a suspicion. Also? I was hoping I might be lucky enough someone who happens to already know more about stream fusion/deforestation could tell me why it is relevant/irrelevant, as I can't look into it now, but am curious. (But eventually I'll look into it myself. Just not a priority now.)
Sense. Your reply makes none.
What is the name of the website? What name did you use? [It's been nearly 4 years now.](http://groups.google.com/group/haskell-cafe/msg/61e427146c8d7ab4?hl=en&amp;pli=1)
GLUT has a few rather annoying issues, but it's still the most portable and easy solution. Something along the lines, but more modern would be great. I had the impression that GLFW wants to be just that, however last time I tried, on OSX it only worked in the form of an application bundle, so I returned to GLUT.
URL starts with?
I have memoizing versions of the store comonad floating around on hackage.
Pierce's lenses are a special case of the functional reference use of the word lens. He borrowed the word from his references, and didn't mint it fresh. Section 3.2 of http://www.cis.upenn.edu/~bcpierce/papers/newlenses-popl.pdf gives an isomorphic definition to the one Russell gives here, calling it a "total, very well-behaved lens".
I wouldn't be surprised if there was also an unsafePerformIO hiding somewhere inside too...
1) Do you wish to insist that the URL *is* the name of the website? (I have come across people who think that Debian authorize and conduct the measurements, just because the project is hosted with alioth.debian.org) 2) Or do you simply wish to suggest that you mistakenly failed to notice the banner text on every web page?
Eh, you're the one who's being a mindless pedant. I'm done.
I would think that Squiggol would be a much more practical language to choose.
I'm the one who thinks programming language comparisons have nothing to do with the shootouts shown on nightly TV news. Even Racket programmers can figure out the difference between the Computer Language Benchmarks Game and the URL. Well, [they *are really smart* Racket programmers](http://www.ccs.neu.edu/scheme/pubs/pldi11-thacff.pdf) (pdf) and they have some experience with name changes. Use the measurement scripts on whichever programs you sanctify and publish your results and call your website whatever you like.
No, actually, it's [simply an unfold](https://bitbucket.org/bos/mwc-random/src/80ed6b2ae042/System/Random/MWC.hs#cl-546).
Then try GLFW-b. As I mention in the post, that binding works on windows, osx, and linux without any third external libs. Just cabal install and start using it. By default it will be statically linked.
Actually, compressed bytes is fine, but they should show the ratio bytes: compressed bytes, which gives something like the syntactic overhead of the language.
Better link: http://hackage.haskell.org/trac/summer-of-code/wiki/Soc2011
I think the issue was with the original GLFW library, not the binding; though it is certainly possible that they fixed it since. Anyway, GLFW-b looks rather nice; I will switch to it from GLUT if the OSX issue is fixed and everything else works like advertised.
Unless it just shows which programmers are willing to reduce variable and function labels to a single character.
Ok, so I made a small test program, which works fine, except that when calling "terminate" it crashes hard (with the threaded runtime), or the GHC runtime throws an exception about unsafely re-entered stuff (non-threaded runtime). This is on WinXP. Do you have any idea what's going on? edit: it also crashes on "closeWindow"
Oh, that sounds bad indeed. I'll see if I can reproduce it. Maybe the GLFW-b author has suggestions. Does the package GLFW-b-demo install and run correctly on your system? edit: I can do the following on the latest version of the Haskell Platform (GHC 7.0.2), using the version of bash that comes with git, and the latest release of GLFW-b: $ cabal-dev ghci Warning: cannot determine version of c:\Program Files (x86)\Haskell Platform\2011.2.0.0\bin\ghci.exe : "WARNING: GHCi invoked via 'ghci.exe' in *nix-like shells (cygwin-bash, in particular)\n doesn't handle Ctrl-C well; use the 'ghcii.sh' shell wrapper instead\n7.0.2\n" WARNING: GHCi invoked via 'ghci.exe' in *nix-like shells (cygwin-bash, in partic ular) doesn't handle Ctrl-C well; use the 'ghcii.sh' shell wrapper instead GHCi, version 7.0.2: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. Loading package ffi-1.0 ... linking ... done. Prelude&gt; :m + Graphics.UI.GLFW Prelude Graphics.UI.GLFW&gt; initialize Loading package GLFW-b-0.0.2.3 ... linking ... done. True Prelude Graphics.UI.GLFW&gt; openWindow defaultDisplayOptions True Prelude Graphics.UI.GLFW&gt; closeWindow Prelude Graphics.UI.GLFW&gt; terminate Prelude Graphics.UI.GLFW&gt; Could you send me an example that demonstrates your crash? I'm unable to reproduce it.
It seems that the issue is caused by the keyboard callback still being called after closeWindow/terminate. I set an MVar in the keyboard callback on down-esc, and then check the state of the MVar in the mainloop. I exit, but the callback is probably also called for the up-esc. If I don't set the keyboard callback, the program does not crash. edit: indeed, if I set the MVar on up-esc, it does not crash. GLFW-b-demo works fine, because it doesn't use the keyboard callback, but checks the state in the mainloop (which is bad design btw, because it is possible to skip a keypress if the frame is rendered slowly). Here is teh code: import Graphics.UI.GLFW import Graphics.Rendering.OpenGL import Data.Vect.Double import Data.Vect.Double.OpenGL import Control.Monad import Control.Concurrent import Control.Concurrent.MVar import Data.Int -------------------------------------------------------------------------------- ffrac :: Double -&gt; Double ffrac x = x - fromIntegral (floor x :: Int64) -------------------------------------------------------------------------------- display = do (xsiz,ysiz) &lt;- getWindowDimensions let asp = fromIntegral xsiz / fromIntegral ysiz viewport $= (Position 0 0, Size (fromIntegral xsiz) (fromIntegral ysiz)) matrixMode $=! Projection loadIdentity if asp&gt;=1 then ortho (-asp) asp (-1) 1 (-1) 1 else ortho (-1) 1 (-1/asp) (1/asp) (-1) 1 matrixMode $=! Modelview 0 loadIdentity clear [ DepthBuffer , ColorBuffer ] color (Vec3 1 0 0) renderPrimitive Lines $ do forM_ [1..32] $ \i -&gt; do t &lt;- getTime let a = pi*i/32 + ffrac (t * 0.1) vertex ( sinCos a) vertex (neg $ sinCos a) swapBuffers -------------------------------------------------------------------------------- keyboard exit key down = do case key of KeyEsc -&gt; exit _ -&gt; return () mainLoop exitCond = do display threadDelay 10000 -- b &lt;- keyIsPressed KeyEsc b &lt;- readMVar exitCond if b then do print "1" closeWindow print "2" terminate print "3" else mainLoop exitCond main = do b &lt;- initialize unless b $ error "GLFW initialiazation failed" b &lt;- openWindow $ defaultDisplayOptions unless b $ error "openWindow failed" setWindowTitle "glfw rulez-e?" exitCond &lt;- newMVar False let exit = takeMVar exitCond &gt;&gt; putMVar exitCond True setKeyCallback (keyboard exit) mainLoop exitCond 
Thanks for the code sample. I think it will be fixed in the next release. I found a fix and sent the author a pull request: https://github.com/bsl/GLFW-b/pull/6
That was fast, thanks. I was afraid that the problem is with underlying library, in which case the fix would be probably slower, but Hackage is rather dynamic. I realized that I have another question: Is there any chance to support the new OpenGL &gt;= 3.0 contexts, too? (to be frank, I have no idea if the C library supports them or not; at first sight, the issue is not even mentioned on their homepage)
is there a link to the slides used in the presentation?
The author of GLFW-b has been very responsive (and nice to work with!) in my experience, so hopefully the fix will be on hackage in a few days. As near as I can tell the OpenGL bindings work fine with OpenGL 3.2. You can see further evidence from the hackage page: http://hackage.haskell.org/package/OpenGL-2.4.0.1 A Haskell binding for the OpenGL graphics system (GL, version 3.2) and its accompanying utility library (GLU, version 1.3). The latest version of OpenGL is 4.1 and was released last summer. I'm not really an expert in how OpenGL bindings interact with drivers. I do know that very few graphics cards support the new extensions in 4.1. The next edition of the redbook will cover 4.1 but it's not due out till August of this year. My vague idea is that some how opengl implementations advertise their extensions and then you can call them if they are available. If you have concerns about the Haskell OpenGL binding not supporting something, try OpenGLRaw. I actually prefer it as it's a bit easier to tell which Haskell functions and constants correspond to the C api for OpenGL.
Near as I can figure: * Download the [archive](https://github.com/hamishmack/yesod-slides/zipball/master) * Unzip cd to hamishmack-yesod-slides $ cabal install yesod-slides.cabal $ ./dist/build/yesod-slides-test/yesod-slides-test *(could take a while if you don't already have Yesod)* * Point browser to 127.0.0.1:3000 * Click to advance
Very strange. Can't manage to try this. Cabal (with ghc6.12.1) fails to install it due to "incorrect indentation" in the first quasiquote (parseRoutes). So by curiosity I try the quasiquote example on the wiki and it compiles (after a minor correction in the Quasiquoter constructor), but the example fails to run. … EDIT: Ok, so there was a change in quasiquote syntax between ghc 6 and 7 removing "$" signs. Adding thems I got little progress: it compiles but fails to link. I'll have to retry with a more recent haskell platform than in my distro…
Working fine for me, with Haskell Platform 2011.2.0.0
On haskell-platform 2010.1.0.0.1 it fails without much information though everything seems ok with dependencies. New syntax and some other template haskell problems.
I have added a note about GHC 6 to the README and a comment to where I think it would fail in the source. I was tempted to fix it using CPP, but decided I would rather keep it clean.
I have added some more instructions to the README. If you run the yesod-slides-devel version while in the src directory, you can make modifications to the code while it is running. Yesod uses hint to reload them at runtime.
Anyway to download the video? Vimeo says it's "private". (I stream long lectures/presentations like this to my PS3 to watch on my tv so I can take notes/relax while it's playing, and not be tempted to browse away when it's a presentation I really want to watch. Flash sucks monkey butt on PS3 currently.) ***I WANT HD SIMON*** **EDIT**: [look at nlogax's link](http://www.reddit.com/r/haskell/comments/g7ajj/simon_peyton_jones_managing_parallelism_embrace/c1lhnlo)
Vimeo says it is a private video?
I don't have anything against idle curiosity - but even idle curiosity deserves a firm base, we can always limit how far we push out from that base. Without looking at the code we don't get that instant appreciation that these are tiny tiny programs - so tiny that it's grandiose to talk about them being "programming in the small".
Vimeo says private video. Also for me, on the website I hear only the sound and no video.
Indeed, I tried representing states as distinct types, but then ran into issues when trying to store them in a transactional variable. Related StackOverflow question: http://stackoverflow.com/questions/5293469/haskell-modelling-online-game-states-for-gui
Thanks, that's an interesting response.
Is it possible to download the slides somewhere?
I've [contacted](http://twitter.com/#!/jaspervdj/status/49100756883746816) the organizer, but no reply so far. It's weekend, I guess...
[Here you go](http://jaspervdj.be/files/2011-functionalpx-blaze-html.pdf)
I don't think `Monoid`s are a "design pattern", because they are abstracted by a library. If we re-implemented Monoid and its functions every time we wanted to use one, it would be a "pattern".
I was able to download the video using the [download helper](https://addons.mozilla.org/en-US/firefox/addon/video-downloadhelper/) firefox addon.
If you're using firefox on linux, start the video playing then look in `/proc/$(pidof plugin-container)/fd`. Should find a file symlinked to /tmp/Flashsomethingorother, copy it somewhere else to save it (doesn't matter if the link is broken).
My understanding is that to use OpenGL &gt;= 3.0, you have to specifically ask a 3.0 context from your OS (since they deprecated a lot a of stuff, for example, my above code wouldn't work on a strictly 3.0 implementation, since there is no glBegin/glEnd.). This is the territory of GLFW/GLUT/etc, not the OpenGL binding. Yesterday's quick google search indicates that NVidia choose to give you a 3.0 context automatically, but they also support all the deprecated stuff. Other vendors may choose not to do this.
Are these videos available in a larger format? At least the HTML5 version is a bit difficult to watch.
Thanks. Does it automate string encoding based on context, like the [Google Template System](http://google-ctemplate.googlecode.com/svn/trunk/doc/auto_escape.html)?
TIL about the [groom](http://hackage.haskell.org/package/groom) package, which tries to make valid Show instances prettier by indenting them. (And since I already hijacked the subject, GHCi feature request: let me specify a different type class than Show to be used for printing the results on the screen, with fallback to Show if there is no instance)
Oh, it's open for discussion, but I think they do classify as a design pattern. To give another example, in Ruby, a singleton is definitely a design pattern, and it's also abstracted by a library: you can just `include Singleton`.
Sort of, but not quite. It will escape content by default, and `&lt;script&gt;` and `&lt;style&gt;` have slightly different rules.
Here's [a link for us flash-less](http://dl.dropbox.com/u/2275898/SPJ%20-%20Managing%20Parallelism.mp4).
To yield better performance than sequential code, parallelism needs *predictable* execution order. Because of lazy evaluation, it is non trivial to do in Haskell. In other words, lazy evaluation helps purity by deferring computations. But deferred computations have a side effect on parallelism performance. For this reason, I think strict functional languages with a bend towards purity (in other words, Clojure) have a better chance at tackling parallelism. EDIT: Of course, I may be wrong on the subject. I am merely stating my opinion.
Why do you say it is definitely a design pattern in Ruby? What repeats besides a use of a library?
I guess it all depends on our definitions of "design pattern" — we're probably both right.
I think it's better to say that monoids are an algebraic structure that are, in Haskell, modeled by a typeclass.
1. Define "design pattern". 2. Read off from the definition whether a "Monoid" is a design pattern. It depends on your definition. A case can be made either way. Is a design pattern truly something that requires you to copy/paste a skeleton because the language is irreducibly incapable of abstracting that way, or is a common solution to a common problem that sometimes your language can abstract out and sometimes it can't? Is the body of code in the "design patterns" fundamental, or an accident as a result of the original languages chosen, and indeed the original paradigm of OO? Really, any of the stated answers make sense and produce a useful definition.
Great presentation! This one will have a much greater impression than normal because it has an example project for each type of concurrency/parallelism- showing these aren't just academic ideas.
I agree. I prefer to use the definition of "design pattern" that emphasizes the *pattern* part, which implies code that repeatedly implements some concept.
Still rocking the Comic Sans.
Predictable execution *costs* are not necessary to get good use from a few cores, if you have good load balancing and overdecompose the problem. Talking about evaluation *order* is just nonsense. It hardly matters in a pure language, and in a parallel strict language you would very much like to reserve the option to evaluate the function arguments (and the function itself) in parallel. Strictness vs. laziness doesn't seem very important once you have purity, except that doing implicit parallelism like parMap requires at least a little bit of laziness - though not more than it's reasonable to make explicit in a mostly-strict language. A lightweight task for work-stealing is may not be done by the time the parent thread needs it, so it has to be implemented much like a thunk.
Mooi werk! I was trying my hand at making something like this; now I can just play around with it :) 
Did I make a mistake allowing regex-pcre when that package doesn't seem to be part of The Haskell Platform? :-) Similarly Vector doesn't seem to be part of The Haskell Platform?
yesodweb.com has been really slow for a day or two. We're talking 1/plaid speed here. Like minutes. Problem on my end? Been using the Google cache of the book. "Waiting for docs.yesodweb.com..." The benchmark finally showed up while I was typing this.
How about an on-topic comment: See the following link for another reason to benchmark only on extra-large instances: http://perfcap.blogspot.com/2011/03/understanding-and-using-amazon-ebs.html
This thread has become pointless. I'm out.
happy to see the benchmark #'s. I'm curious where tomcat and jetty fall, I might have to set them up to compare. On my dual core machine I was able to get 22k requests per second using warp/pong. That's quite good imho. 
I'm trying to use Coral Cache. If this works, other people may be able to see the page at http://docs.yesodweb.com.nyud.net/blog/preliminary-warp-cross-language-benchmarks
&gt; However, as seen above, the automatic transformation to parallel code suddenly inserts strictness! No, your choice of translations inserts strictness. And it likely does so because you know that computing `temp` and `temp2` in parallel prior to `fun3 temp temp2` is beneficial. We could naively reduce all three in parallel if we wanted: f a b = let temp = fun1 a b temp2 = fun2 a b in temp `par` (temp2 `par` fun3 temp temp2) Although if `fun3` demands `temp2` soon enough, it may just evaluate it without waiting for an extra spark. The problem with all automatically-inferred parallelism, is that there are too many trivial things that are parallelizable, such that the overhead makes the program slower overall. This is especially true (in general) of reducing `f a b` in parallel with both `a` and `b`, but it's true of things like `(w + x) + (y + z)`, too. Anyhow, the bottom line is that there's no necessity to use the specific translation you give. The runtime could evaluate `temp` and `temp2` in parallel for a while, delaying the `fun3` call, but ultimately evaluate `fun3` without depending on what the `temp` and `temp2` evaluation does. A non-parallel version of this is called 'optimistic evaluation,' and it is still non-strict.
I believe the true automagical parallization of your original function should be: function a b = let temp1 = fun1 a b temp2 = fun2 a b in temp1 `par` temp2 `par` fun3 temp1 temp2 Which says that the compiler should spark off the calls to fun1 and fun2 in parallel with fun3. You don't need, nor particularly want, the `seq` in there. If anything, you want `pseq` instead.
Not quite on topic :) The benchmark doesn't access disk, which is the main concern according to that article. Also, the benchmarks were ran multiple times with consistent results. Thanks, though, I am very interested in that article otherwise.
not a proof, but then again you started with an anecdote too, so I assume you'll forgive me. I think yo can still have both, but you picked the wrong point to fork - you forked too early. Pass thunks to fun3 as you usually would in a regular haskell evaluator. However, when an operator in your evaluator would normally force the thunk to a value, a parallel haskell would asynchronously evaluate it on another thread, and go ahead and kick off evaluation of everything else that would normally be forced, and then join on the asynch operations before computing the result. Take your case here... suppose `fun3 a b = f (a * a) + b`. First, a new thunk is passed to f... suppose f is jus the identity fun... now two thunks are passed to "+". two threads are kicked off. one for "a*a", another for "b". The first thread invokes "*" which forces "a" twice .. the second force is a noop, but the interesting point is that if it were different, we would now have 4 threads, 3 running doing work. Anyway, in this case we now have the original thread doing a sum is blocked on threads 2 and 3, 2 blocks on 4, 4 is computing "a", 3 is running computing "b". And argument expressions we're never evaluated before they were used. Parallel, but not strict.
It's entirely possible that it's my VPS acting up again. It's been giving me lots of problems over the past few months, and unfortunately I haven't had a chance to migrate it yet. I'm planning a switch to EC2. Actually, part of the motivation of running all these benchmarks was that I'd become more familiar with EC2 in the process :).
Wrong. :)
Just for comparison, running the Warp benchmark on a quad-core desktop (instead of EC2 where I ran the actual benchmark) resulted in 121k req/sec. And Matt Brown got around 190k req/sec, though IIRC he has a much more powerful system than I do.
Some of the more traditional webservers might be good indeed. 
Living in a Java ecosystem, but liking Haskell, I've had lots of fun with CAL/Open Quark -- it's a high quality system. I've written some modules for running CAL on Google AppEngine: https://launchpad.net/cal-on-gae among other experiments.
So the perfect language wouldn't have them? Or if you do the same thing repeatedly and are able to factor out the common behavior into code, it stops being a design pattern the moment you abstract over it? I prefer to drop the "design" from the term and just use the word "pattern", with fewer connotations. Monadic operations form a pattern that we happen to have named and written a library around.
&gt; For example, we wanted to use longer, more descriptive, less mathematical names, as is common in Java. The Haskell functions null and nub are isEmpty and removeDuplicates in CAL. Awesome, I think those are indeed much better names. &gt; The Haskell type class Monoid is Appendable in CAL. Horrible! Monoid is *not* appendable. Also: public class Appendable a where public empty :: a; public isEmpty :: a -&gt; Boolean; public append :: a -&gt; a -&gt; a; public concat :: [a] -&gt; a default concatDefault; ; Having isEmpty in there rules out useful Monoid instances like the one for functions (or the general one for all Applicatives, actually): instance Monoid b =&gt; Monoid (a -&gt; b) where mempty = pure mempty mappend = liftA2 mappend Haskell is an *extremely*-well thought-out of language, in a deep sense. Many names suck. Some of the syntax sucks. But to change something as deep as the Monoid type-class? It's unlikely to be an improvement (Unless it involves removing "fail" from Monad... :-) ) Their Monad also extends Functor, but not Applicative? come on! 
Yeah, the perfect language probably wouldn't have them (even imperfect languages go a long way towards not having them, for example via macros). I think we should be able to differentiate the repetition of a use of an idea from the repetition of its implementation.
The applicable bit was about multi-tenancy: "If you ever see public benchmarks of AWS that only use m1.small, they are useless, it shows that the people running the benchmark either didn't know what they were doing or are deliberately trying to make some other system look better. You cannot expect to get consistent measurements of a system that has a very high probability of multi-tenant interference." It is about AWS, but they over subscribe CPU as well. Okay, it's a stretch. :)
*I do not believe that Goliath , Tornado or node.js are capable of scaling up to four cores* How about yaws (erlang). How about more flamewars http://news.ycombinator.com/item?id=1893332 http://timyang.net/programming/c-erlang-java-performance/ http://lionet.livejournal.com/42016.html 
To be fair, I believe that in ruby/python environments they run one server process per core then use nginx or something to load balance between them.
The difference between "non-strict" and "lazy" covers exactly this sort of case. "lazy" tends to imply that unused arguments will never be evaluated. That *is* incompatible with speculative parallelism, where idle cores might start working on them anyway. "non-strict" means if an unused argument is an infinite loop (or error, etc), the function application will still finish, but doesn't promise anything about evaluation order. In particular, "non-strict" is perfectly compatible with some other cores starting to evaluate the arguments as soon as the application is reached. 
It looks like nginx and Jetty are pretty fast. If you know how to set them up The last link is dead (EDIT: it has since been edit out). I think it was something like the author showing that a single-threaded server built around epoll could handle a competitive connection rate on a "PONG" style benchmark, compared to things which can actually run on multiple cores. I don't remember which server, thought. Then they went on to be deliberately obtuse when people suggested that using multiple cores might be helpful if each request actually required a bit of CPU time.
One neat point of reference: Eager Haskell, which also, despite the name and default evaluation strategy, provides non-strict semantics: http://csg.csail.mit.edu/pubs/haskell.html
Think of it this way: the Monoid design pattern is "use a Monoid constraint instead of fixing a single container type whenever possible."
What word can we use for the notion of reimplementing concepts because the language is too weak to abstract over them?
I think both are validly called design patterns. If the language *can* do the abstraction, the pattern is "use the abstraction, not a specific case," whereas if the language can't do the abstraction, the pattern is "fake the abstraction rather than using a specific case." In both cases, there is a development pattern in action, but in languages that support the needed abstraction, the syntactic signature of the pattern can be quite small.
I think arrows are very badly structured: * `arr` shouldn't have been there * Some can implement ArrowChoice, but not Arrow, but the subclassing prohibits it Since (Category+Applicative) are equivalent in power to (Arrow), what use do Arrows have? Here's proof of equivalence (implementing both classes in terms of each other): autoArr :: (Category c, Applicative (c a)) =&gt; (a -&gt; b) -&gt; c a b autoArr = (&lt;$&gt; id) autoStars :: (Category c, Applicative (c i1), Applicative (c i2), Applicative (c (i1, i2))) =&gt; c i1 o1 -&gt; c i2 o2 -&gt; c (i1, i2) (o1, o2) autoStars cio1 cio2 = liftA2 (,) (cio1 . autoArr fst) (cio2 . autoArr snd) autoPure :: (Arrow arr) =&gt; a -&gt; arr i a autoPure = arr . const autoAp :: (Arrow arr) =&gt; arr i (a -&gt; b) -&gt; arr i a -&gt; arr i b autoAp aToB a = (aToB &amp;&amp;&amp; a) &gt;&gt;&gt; arr (uncurry ($)) Maybe we can finally put arrows to rest?
I cannot install it, I get an error cabal.exe: cannot configure ghc-syb-0.2.0.0. It requires ghc &gt;=6.10 &amp;&amp; &lt;6.14 Do you have a work around? Thanks 
Of course those servers *can* (and do) scale to 4 cores, but you need to start 4 instances (4x RAM) and stick them behind a load balancer. Thanks. Those are interesting. I would really like to benchmark against nginx eventually- would be easy to steal the hello module from the first benchmark. 
That sounds more like you might be running an older version of cabal. You might have an older one on your path.
How do you run it ? scion on hackage has an error compiling with ghc 7.0.2. Should we get scion some other way ? from github or something ? 
I don't even see this post on r/haskell. wierd. Thanks for replying though. It's exciting to see a haskell web server break 100k rps.
there's overhead to this type of model though - the extra hop from the event based webserver to the ruby/python process will cost you. Granted for php/ruby/python/perl this is often the best way to run.
no archive of that but this thread http://news.ycombinator.com/item?id=1448463
Unfortunately, it seems that the project is mostly dead. The wiki is dead, the forums are inactive, and there is little activity on Github.
You get scion bundled with EclipseFP, and the docs say explicitly that you can't use the version on hackage, since we add and change things for EclipseFP. So just install EclipseFP, it will unzip and install scion for you.
You need to run cabal update, probably, since ghc-syb-0.2.1.0 is fairly recent.
...what is JSON?
Great projects! I'd love to see how Data.Text behaves when using UTF-8 internally. We generally store several gigabytes of Text in-memory, all of it originating form UTF-8 encoded sources. A reduction of the memory footprint (and conversion overhead) would most probably speed up our backend app a lot. Too bad I'm not a student with a free summer. :-( But, when needed, count me in for early stage testing!
If you're serious... check here: http://en.wikipedia.org/wiki/JSON
I'm a graduate and knowing Haskell certainly improved my ability to get my first job. 
Looks as if it might be an interesting article but it's impossible to read the text.
By running HLint on it. ;) [Here](http://ompldr.org/vN3hiNg) are the results.
Also, use `where` clauses and conventionally, a prime suffix (`'`) is used instead of an `_` prefix to denote related functions. That is: shuntingYard :: String -&gt; String shuntingYard stmt = shuntingYard' stmt [] where shuntingYard' [] stack = ... ... However, it is sometimes useful to move `where` scoped declarations to the top level for testing and whatnot. EDIT: Didn't realize `_shuntingYard` was called by `operatorActions` and `stackOperations`. Ignore me.
[Here](http://pastebin.com/5aVugVUv) is the result of me having a little fun with your program. It is mostly small changes, pushing out guards to pattern matches and removing some unnecessary checks.
Why does HLint seem to promote infix functions?
Beautiful, thanks. Only issue was a missing pattern in operatorActions, but I fixed it. :)
Maybe it is specifically for `elem`? I think some functions, for example also `mod`, are meant to be used infix. By the way, in mathematical notation, lots of common functions and operations are not only infix (arguments go in both horizontal sides of the function) but also have arguments above and below. It seems that the central and most important part of the computation (typically the function being applied) is emphasized by being at the center, and makes it clearer which arguments are being applied, particularly when the arguments have positional significance.
As a haskell beginner I mostly follow this subreddit because I never know what the fuck the titles mean and have to read the links to find out. :)
I'm no haskell expert, but personally I would have made an operator datatype instead of having chars which represent them everywhere you need to refer to them... something like this: import Data.Char import Data.Maybe data Assoc = AssocL | AssocR deriving (Eq) data Operator = PlusOp | MinusOp | MultOp | GTOp . . . deriving (Eq) getOperator :: Char -&gt; Maybe Operator getOperator x = case x of '+' -&gt; Just PlusOp '-' -&gt; Just MinusOp '*' -&gt; Just MultOp '&gt;' -&gt; Just GTOp . . . otherwise -&gt; Nothing isOperator :: Char -&gt; Bool isOperator = isJust . getOperator associativityOf :: Operator -&gt; Assoc associativityOf x = case x of PlusOp -&gt; AssocL MinusOp -&gt; AssocL MultOp -&gt; AssocL GTOp -&gt; AssocR . . . precedenceOf :: Operator -&gt; Int precedenceOf x = case x of GTOp -&gt; 1 PlusOp -&gt; 2 MinusOp -&gt; 2 MultOp -&gt; 3 . . . That way pattern matching on them will be a little cleaner, and if you decide you want to parse the expressions into an AST, your type signatures will be a bit more readable. The other benefit is that you can be sure you got all of the cases when you pattern match against them because the compiler will complain if you don't. With Chars you end up doing a lot of catch-alls with the otherwises, which could be potential bugs.
The main thing I noticed is that operatorActions and stackActions have some calls to _shuntingYard which are will end up just calling back into those functions again. Because of the line _shuntingYard [] stack = stackOperations stack in _shuntingYard, the line | otherwise = x : _shuntingYard [] xs in stackOperations is the same as | otherwise = x : stackOperations xs Then you have a recursive function stackOperations :: [Char] -&gt; [Char] stackOperations [] = [] stackOperations stack@(x:xs) | x == '(' = error "Unbalanced parens" | otherwise = x : stackOperations [] xs which is similar to stackOperations stack | elem '(' stack = error "Unbalanced parens" | otherwise = stack and at that point it's simple enough you might as well inline it in the main function. Also, some of your functions are only defined for nonempty lists. It might be better if they took two arguments, so you can't give them an empty list and get errors.
Good outline if the problem but where are the suggested solutions? 
Can't really intelligently comment on your question (yet), but earlier today I was thinking about trying to use Haskell for exactly this. You are much farther along it seems. Thanks for the links!
If your language doesn't have closures, I don't know why you are calling it "functional"... Anyway, without closures, monads won't work, even if you have a compile-time meta-language. The crucial point is that an action may depend on results from previous actions, and that's not something you can do in the meta-language. However, applicative functors are available in the meta-language; they are usually a good substitute. What you could do is introduce monads as a special syntactic construct, i.e. a semicolon, and figure out whether it's possible to let the programmer define new monads without closures. My [operational monad tutorial][1] and assorted package could help with that. Basically, you would allow one recursive data type. The RWS monads should be possible that way, but not lists, because the latter use recursion for `mplus`. [1]: http://apfelmus.nfshost.com/articles/operational-monad.html 
Maybe there is relevant work in this paper by William Harrison and colleagues? They are defunctionalizing monads to generate synthesizable VHDL: http://hthreads.csce.uark.edu/mediawiki/images/4/4a/CheapDefunc.pdf
Are the enhancements/changes to the scion-server protocol wrt to the vanilla scion server documented somewhere? I've been able to compile it from the GitHub checkout, but it doesn't work 100% as drop-in replacement with my current `scion.el` Emacs mode.
&gt; If your language doesn't have closures, I don't know why you are calling it "functional"... Immutability-by-default (admittedly it does have some "auto-locked" mutable locations, which are targeted to actual hardware flip-flops), first-class functions (just free ones though, not closures), tail recursion optimization, algebraic data types, very simple pattern matching, all code is in expressions, and other trappings of "functional" languages. The compile-time meta-language has no such restrictions and can in principle support closures/partial applications and recursive user data types (although I haven't gotten around to building a syntax and semantics for those). In principle I might be able to provide partial application, if the partially applied arguments are not themselves functions, i.e. suppose I have: fun f(x :: Int[0, 3], y :: Int[0, 3]) -&gt; Int[0,6]: x + y fun g(fn :: (Int[0, 3], Int[0, 3]) -&gt; Int[0,6]) -&gt; (Int[0, 3]) -&gt; Int[0,6]: partial fn(1) initial: let : tmp = g(f) output ! tmp(2) (or drop the "`partial`" keyword and make currying automatic). Each actual function type used can then be converted to a non-recursive data type: type LambdaInt03Int03toInt06 = fLambda() type LambdaInt03toInt06 = fPartial1(x :: Int[0, 3]) fun f(x :: Int[0, 3], y :: Int[0, 3]) -&gt; Int[0,6]: x + y fun g(fn :: LambdaInt03Int03toInt06) -&gt; LambdaInt03toInt06: let : x = 1 match(fn) : fLambda() = fPartial1(x) endmatch initial: let : tmp = g(fLambda()) let : y = 2 match(tmp) : fPartial(x) = f(x, y) endmatch &gt; Basically, you would allow one recursive data type. Can't, not even one - that would require a von Neumann bottleneck and lose all the benefits of parallelization that actual digital hardware has. However, would it be enough to define a family of data types? type Level1T = Level1C(x :: CoreT) type Level2T = Level2C(x :: Level1T) type Level3T = Level3C(x :: Level2T) -- and so on, as generated by the compile-time meta-language RWS is some sort of combination of MonadReader, MonadWriter, and MonadState? 
That's all nice and appreciated, but if you can't implement function composition `(.)`, then it's hard to call it a functional language. :-) Perhaps "pure language" is more fitting. &gt; &gt; Basically, you would allow one recursive data type. &gt; Can't, not even one Well, then you're screwed. That said, you *do* allow recursion; namely tail recursion. You'd need some condition like "tail recursive monad". I have no idea how to bake that into the syntax, though. &gt; RWS is some sort of combination of MonadReader, MonadWriter, and MonadState? Yes. In my case, I wanted to use it as an abbreviation for the collection of the three monads and combinations thereof.
&gt; but earlier today I was thinking about trying to use Haskell for exactly this Exactly what? Hardware synthesis? There's currently a hardware synthesis language named Lava which does structural synthesis (and has a very elegant syntax, care of Haskell, for specifying interconnections between hardware blocks), but my focus is more on behavioural synthesis. Originally I had conceived of a language that was essentially "RTL written in Haskell-like lang" but when I read the linked paper, I was blown away: this was truly higher-level than mere RTL, this was true behavioural synthesis encapsulating away most of the details of control signalling. Unfortunately the paper uses an SML-like language with its attendant syntax, and I didn't have enough imagination to figure out how to make it look more like Haskell (the fact that currying is not easy to support in hardware makes it difficult to allow types like `Foo -&gt; Bar -&gt; Nitz`, you have to use `(Foo, Bar) -&gt; Nitz`, for example, so I decided to stick with the SML-like syntax). Anyway one reason for having a nice second-level compile-time metalanguage was because I wanted to also have a Lava-like framework: create basic blocks as behavioural functions in the object language, then connect them together structurally in a Lava-like metalanguage.
&gt; That's all nice and appreciated, but if you can't implement function composition (.), then it's hard to call it a functional language. :-) Perhaps "pure language" is more fitting. Actually, it can, with the assistance of the meta-language: gen compose[f :: Fun, g :: Fun] =&gt; Fun: gencheck : $funArgs[f] == 1 "first argument of compose must be a function with 1 argument" : $funArgs[g] == 1 "second argument of compose must be a function with 1 argument" : $funArgType[f,1] == $funRetType[g] "return type of second function in compose must match input argument of first function" let : g_in = $funArgType[g, 1] : f_out = $funRetType[f] declare : fun rv(x :: g_in) -&gt; f_out: f(g(x)) rv That's why I'm wondering if the metalanguage can help me make a "reasonable approximation" of monads.
You could also do something like: data Op = Op { associativityOf :: Assoc, precedenceOf :: Int } plus = Op AssocL 2 and get the associativityOf and precedenceOf functions for free.
No, sorry, the changes are not documented further than what you can see from the git comments (a diff of Commands.hs should give you a good start though). More often that not, we have added things and not really change things that were there, but that's not guaranteed, as you found out.
Formatting-wise or writing-wise?
They are unknown. The point was that while Arrow-based signal functions were introduced primarily to prevent time-leaks, they aren't perfect at accomplishing that. For this particular (and very contrived) example, changing 'arr fst' to 'arr (\\(x, y) -&gt; y seq x)' in the 'alt' SF should take care of the problem, but that only works because Double (Time) is a non-composite datatype. A general "solution" would probably have to use deepseq. In any case my only intent was to outline the problem. FRP is in general not a solved problem.
I'm rather surprised that http://hackage.haskell.org/package/HTTP-4000.1.1 doesn't seem to have any support for `https://`... that's not quite the "batteries included"-way to go... :-/
I meant formatting-wise but it might be better when I get home and use a PC rather than my iPad.
Is the blog text impossible to read? Or is it the code sample? I do admit to being very lazy and just running hscolour -html on my code, then copy-pasting the content of the "body" tag into my blog editor. I'll find a friend that has an iPad and see what the formatting looks like.
That's how I learned too. Keep at it!
I'm not sure if I fully understand you questions, but if I were you I'd be looking heavily at things like [SKI calculus](http://en.wikipedia.org/wiki/SKI_combinator_calculus) and [BKCW](http://en.wikipedia.org/wiki/B,C,K,W_system) Wikipedia says they can be useful in modeling [hardware](http://en.wikipedia.org/wiki/Combinatory_logic#Combinatory_logic_in_computing), so they may be particularly suited for addressing your restriction #1. for example, i think partial application can be implemented as follows: &gt;curry f x = S (C f) (K x) I'm not sure if this is helpful or not, since you would still have to implement each of these combinators as primitives for your language.
So it goes. HTTPS seems to involve quite a bit (a hierarchy of certificates, for starters), thus maybe one should not be surprised at the absence. http://hackage.haskell.org/package/http-enumerator claims to support HTTPS, but looking, I don't see much else (besides a wget binding).
It's the whole page, but I've tried it using iPad Safari and it's OK in that. The application I couldn't read it in was Alien Blue (a dedicated Reddit client). Alien Blue seems to have a problem with black backgrounds. Btw, I appreciate you taking the trouble to follow this up.
&gt; For this particular (and very contrived) example, changing 'arr fst' to 'arr (\(x, y) -&gt; y seq x)' in the 'alt' SF should take care of the problem, but that only works because Double (Time) is a non-composite datatype. A general "solution" would probably have to use deepseq. `deepSeq` is almost never the answer. I would argue that in the general case, you should just use an appropriately strict data type (being a composite data type isn't a problem if all its fields are strict anyway). This doesn't seem like a problem with Arrow-base FRP as it does with programming with laziness in general.
I might be missing something obvious, but couldn't you simply make the closured-over environment explicit? e.g. -- If your language has existential types... data Closure a b = forall e. Closure e ((e,a) -&gt; b) apply :: (Closure a b, a) -&gt; b apply ((Closure e f), a) = f e a -- Example closure adder :: Int -&gt; Closure Int Int adder x = Closure x uncurried_plus -- Without existentials, we'd need to thread the 'e' parameter around, I think data Closure' e a b = Closure' e ((e,a) -&gt; b) 
But `compose` is not a member of the object language, only of the meta language, right? I don't want to take the fun out of your languages, but I would recommend against calling the object language "functional". :-) You can make monads in the meta language, of course, but the results may not flow from object to meta language. You can probably "fake" them in the object language, too, but only in a way such that the control flow graph is known at meta time.
geezusfreeek is right, not a problem with AFRP, but laziness in general.
&gt; first-class functions (just free ones though, not closures) So you mean C, maybe with some sugar for anonymous functions. Few people would call C a "functional language". It is the ability to make closures that we mean by the term "first-class functions". If functions are first class then you can do *everything* with them that you could do with non-function values. In particular, you could define them based on whatever names happen to be in scope at the point of definition. All you have here are higher-order functions. Which is a nice start, but certainly isn't first-class.
&gt; but only in a way such that the control flow graph is known at meta time. Assuming s/meta/compile/, that's exactly the same restriction which gives rise to applicative functors.
Here's another version. Not everything is a good idea (helper function type signatures were just removed to better fit a post), but pulling out the 'shunt' function seems pretty nice. import Char import Data.List import Data.Ord ops = ["=&lt;&gt;","+-","*/%","^!"] isLeftAssoc x = elem x "+-*/%" isOperator x = elem x (concat ops) precedenceOf x = maybe 0 (1+) (findIndex (elem x) ops) shunt p e k [] = e shunt p e k stack@(y:ys) | p y = y : shunt p e k ys | otherwise = k y ys shuntingYard :: String -&gt; String shuntingYard stmt = shuntingYard' stmt [] where shuntingYard' [] = shunt (/='(') [] (\_ _ -&gt; error "Unbalanced parens") shuntingYard' stmt@(x:xs) | isDigit x = (x:) . shuntingYard' xs | x == '(' = shuntingYard' xs . ('(':) | x == ')' = shunt (/='(') (error "Unbalanced parens.") (\_ ys -&gt; shuntingYard' xs ys) | isOperator x = shunt (\y -&gt; (comparing precedenceOf y x, isLeftAssoc x) &gt; (EQ,False)) (shuntingYard' xs [x]) (\y ys -&gt; shuntingYard' xs (x:y:ys)) | otherwise = shuntingYard' xs
Wow, that was fast. Somebody must really like Snappy.
Nice, it doesn't suffer from the problems in the zlib library of using lazy bytestrings.
Doi, Bryan wrote it :)
Eight levels deep...wow. I I'll bet unraveling that stack was a little bit annoying. 
Is there any good reason that a composition of ReaderT, WriterT, StateT is not optimized to essentially the same as RWS?
&gt; but I would recommend against calling the object language "functional". :-) Well, yeah, I noticed that too, that's why I'm taping on a truly functional meta language (with closures and everything) on top - the research I linked to vaguely describes this in a higher-numbered chapter, but is otherwise focused just on the object language, which *in principle* could have true function composition, but will suddenly bash you over the head with the "statically allocatable restriction", because otherwise it can't allocate a fixed number of hardware wires. It is possible in principle to have a sufficiently advanced compiler that will "magically" shift things from the object to the meta language if the object language can't handle it: fun compose(f :: (b) -&gt; c, g :: (a) -&gt; b) -&gt; (a) -&gt; c: with : fun rv(x :: a) -&gt; c: f(g(x)) rv == magic compiler optimization ==&gt; gen compose[f :: Fun, g :: Fun] =&gt; Fun: gencheck : $funArgs[f] == 1 "first argument of compose must be a function with 1 argument" : $funArgs[g] == 1 "second argument of compose must be a function with 1 argument" : $funArgType[f,1] == $funRetType[g] "return type of second function in compose must match input argument of first function" let : g_in = $funArgType[g, 1] : f_out = $funRetType[f] declare : fun rv(x :: g_in) -&gt; f_out: f(g(x)) rv but even with a magic compiler optimization we would have some trouble with particular kinds of code: fun foo(x :: ()) -&gt; (): () fun divergent() -&gt; (()) -&gt; (): compose(foo, divergent()) The above is almost sure not to make sense, but it will (should!) make the compiler attempt to allocate an infinite number of wires for the type `(()) -&gt; ()`. ---- You know, I think honestly it may be possible for me to force partial application to be possible in a limited context: my only "real" restriction is that types can't be recursive (the lack of non-tail recursion grows from this when you translate to CPS, then translate continuation closures to objects). I can possibly just try to translate all function types and partial applications to plain ADT's, and if a loop forms in the ADT, I just bomb out. A clear error message to report will be difficult to express however, which is why I've resisted putting in closures. Sigh. "Partial application caused loop in equivalent type" - then what line numbers do I report? Sigh. "Partial application violates statically-allocated restriction" might as well be "floop-doop violates herp-derp"
Here's the [commit](https://github.com/snoyberg/yesod-core/commit/70eba502de2ae0e36c66cad45e0cbbd5750c7774) where the change occurred. I hadn't seen this Monoid + RWS technique, and found it a nice little example of how to use newtyped monoid bits to update part of your state: setTitle x = GWidget $ tell $ GWData mempty (Last $ Just $ Title x) mempty mempty mempty mempty mempty 
Yes, I do plan to have a compiler pass which translates function types to their own data type. However I have a "statically-allocated restriction", which is composed of this limitation #1: 1. Data types cannot be recursive. In hardware, all data types are simply composed of the concatenation of their parts as wires. `Closure a b` becomes recursive if `e` ever becomes `Closure a b`, or becomes a type which (directly or indirectly) contains `Closure a b`. So it's not `forall e`, just `forall e. e != Closure a b`. This means that I can't do something like: {- foo :: (a -&gt; b) -&gt; (a -&gt; b) foo f = apply_me where apply_me a = f a -} foo :: Closure a b -&gt; Closure a b foo f = Closure f (apply_me) where apply_me (f, a) = apply f a or more to the point: {- bind :: (RealWorld -&gt; (RealWorld, a)) -&gt; (a -&gt; (RealWorld -&gt; (RealWorld, b))) -&gt; (RealWorld -&gt; (RealWorld, b)) bind f ga = fg where fg w = let (w', a) = f w g = ga a in g w -} bind :: Closure RealWorld (RealWorld, a) -&gt; Closure a (Closure RealWorld (RealWorld, b)) -&gt; Closure RealWorld (RealWorld, b) bind f ga = Closure (f, ga) fg -- I refer to this Closure constructor in discussion later where fg (f, ga) w = let (w', a) = apply f w g = apply ga a in g w in the above, the `Closure (f, ga) fg` constructor has type `forall a b. (Closure RealWorld (RealWorld, a), Closure a (Closure RealWorld (RealWorld, b)) -&gt; (((Closure RealWorld (RealWorld, a), Closure a (Closure RealWorld (RealWorld, b)), RealWorld) -&gt; (RealWorld, b)) -&gt; Closure RealWorld (RealWorld, b)` (okay, I shouldn't have expanded that, that was 15 minutes of my life). What this means is that if `a` == `b`, then the `e` argument contains `Closure RealWorld (RealWorld, a)` (the left side of the e argument, specifically), which is the exact type that the constructor returns. This means we have a recursive data type. This means we've violated the statically allocatable restriction, and we can't make this into hardware. Honestly though, I was wondering if there are other methods of threading a "`RealWorld`" object. So I suppose my questions is: is there a way I can thread state in a pure language but hide it most of the time, the way Monads do? Alternatives to Monads that are not as well-explored, but might themselves appreciate a `do` notation?
&gt; Wikipedia says they can be useful in modeling hardware, so they may be particularly suited for addressing your restriction #1. Uhm, the graph reduction machines looks like it has a von Neumann bottleneck (i.e. a bus with an "address" and "data" - I believe the transputer network (IIRC) addresses individual transputers via addresses), which is why restriction #1 exists in the first place - since a piece of data I am transmitting contains all the data in "raw form" (no pointers) I can send it as a set of wires without worrying that shared access to some shared memory will be done... Okay, can you please explain more how it can address restriction #1? I can't remove it, because removing it means I'll devolve down to the von Neumann bottleneck. How can I address it, given that SKI can have trees of indeterminate breadth?
I haven't seen this - I'll take a look at it and see how its approach meshes with the one I'm doing now.
Haha wow! And of course monoids compose, so there's always a completely natural flattening transformation between any number of WriterTs (and boy was that a lot!) and a single WriterT. But that's just rubbing it in :-) More helpfully, there are a number of writeups of why monads (and stacks of monads especially) can be inefficient, and there are any number of issues including laziness, etc. I don't have a complete set of references on hand, but the "Asymptotic Improvement of Computations over Free Monads" [1] paper provides some examples of a particular sort of slowdown (as well as a nifty technique to fix it). As I recall, the more deeply you nest monads, the more dramatic this slowdown gets. [1] http://www.iai.uni-bonn.de/~jv/mpc08.pdf
It's not really a time leak - each instant only needs the value from the previous instant, it's just that the value the program is explicitly written to accumulate gets large. If you wanted to log a list of all input values, that would get large too, and also not be the fault of the FRP framework. The solution here is the usual solution when you find a number is accumulated as a lazy expression - add a bit of strictness to force it while you go.
Think of a sequence of actions like thing1 &gt;&gt;= \x -&gt; thing2 &gt;&gt;= \y -&gt; thing3 y &gt;&gt;= \_ -&gt; return (x+y) If you need to use a value bound at any but the most recent &gt;&gt;=, the later lambdas have to close over the earlier value. But these closures are not necessarily recursive, so it might work anyway.
Heh, I was looking for the documentation but as soon as I saw his name as the maintainer I realized I was really just "double-checking" at that point.
Okay, I've done a quick browse through it. From what I read their technique allows only exactly two monads anyway: K and R, with R defined in terms of K. In the paper I'm using, they have a IO-impure language which, I think, is roughly equivalent to the K and R monads (i.e. in terms of purity, in the language I'm implementing everything runs in the R monad). Even the "tail recursion only" and "non-recursive ADT's" restrictions are identical. So far, it seems that the technique they present is roughly equivalent to the one in the paper I'm using. (except they do a CPS transform, which the paper I'm using doesn't do, since the paper I'm using implicitly "locks" entire functions - from what little I've gathered, their advantage is that it gets them more parallelism, my advantage is that each function is exactly one hardware block that doesn't get magically replicated (that's why I have a meta-language - the meta-language can replicate functions, the object language can't, this allows me to control when I replicate and when I share blocks)). &gt; Due to the fact that functions are not true first-class values, partial applica- tion is not allowed, Yes, exactly my restriction. Heck, they even refer to the language I'm based on: &gt; SAFL [32, 41] is a functional language designed for efficient compilation to hardware circuits. SAFL provides a first-order language, restricted to allow static allocation of resources, in which programs are described behaviorally and then compiled to Verilog netlists for simulation and synthesis. An extension, called SAFL+, adds mutable references and first-class channels for concurrency. CT provides similar capabilities [23], especially in regards to hardware compilation. However, language extensions facilitating stateful and concurrent effects are con- structed using the composition of monad transformers to add these orthogonal notions of computation. So yes, it looks exactly like my language, but with fancier terms, based on a better syntax (Haskell!!), and replication-by-default instead of share-by-default. What I want is to be able to let the designer define a Monad *other than* the "special" R monad. Do they have any research which supports adding monads other than R? I can't seem to find any yet, but I'll try to take a look.
Remember to clearly mark the code written while you are sober so you can identify code written while drunk.
maybe their (&gt;&gt;=) implementation isn't inlined by default?
Could someone explain what the advantages would be for using a monad stack over the monoid + rws technique. To my (naive) eye it seems like an inferior pattern.
&gt; Alternatives to Monads that are not as well-explored, but might themselves appreciate a do notation? That sounds like "arrows", introduced by John Hughes and by now pretty well explored in the Haskell community. See Control.Arrow. There's a do-like notation which isn't standard Haskell. There are various flavors of arrow that are less powerful (for the user) than monads, and which thereby give the implementation more flexibility. For example, an arrow-based parser can pre-compute static tables. I expect they could pre-compute hardware as well. Arrows can thread a "RealWorld" object. However, you can't thread a monad (like Haskell's IO) in an Arrow unless it's enhanced to be an ArrowApply. And with that enhancement it gains all the benefits and drawbacks of a monad.
The difference is certainly minor, but an implementation of the monad based on my [operational][1] package would probably be a tad simpler. It would be tricky to optimize, though. But if successful, *all* monads would benefit from the optimization. [1]: http://hackage.haskell.org/package/operational
It seems to me that, if this and similar techniques become popular, it might be worth providing a direct implementation of IVars? I'm not sure, but I'd imagine that IVars should be able to have somewhat lower overheads than IORefs.
Obligatory reference to "inception"
That is basically the conclusion of TFA.
No need to mark the code, just encapsulate it using the Inebriated monad.
Given your comment I was expecting a continuation interface so it could be used to compress/decompress multiple blocks, but actually it's one single strict bytestring as input and output. That's not better than using lazy bytestrings: using the zlib interface if you already have all your input as a single strict bytestring and you have a good guess at the output size, then you can use the current API to do the compression/decompression all in a single block. Having looked at the underlying C API, it's clear why Bryan has done the binding this way. Snappy is not a stream based compressor/decompressor it just takes a single input and output block. So a stream is done on top just by compressing in multiple independent blocks. You get less compression this way, but that appears to be part of the design trade-off that snappy makes: less compression, more speed.
Snappy supports a source/sink API, which should allow streaming. However, it requires callbacks.
I don't think so. IVars need to interact with the scheduler, because reading an empty IVar must block the current "thread", and we're not using real threads here. We could have IVars at the IO level, like MVars, but they wouldn't help implement the Par monad.
That's neat, I really like that.
Fair enough... but that said, we could write a `Par` monad with real threads and the standard scheduler as well, no? At which point, an IVar could potentially be helpful? Which is not to say that I don't appreciate the benefits having pluggable schedulers, an et cet as otherwise spelled out in the paper.
Sure, my point is that fundamentally snappy is a block based codec (like bzip), not a stream based one like zlib. With a block based codec you can seek to a block boundary and decode from there, with a stream based one you have to start at the beginning of the stream. The state of the compressor/decompressor depends on all the data earlier in the stream. I'm not saying one is better than the other (that's just how the codec works), my point was that whether the codec is block or stream based affects the API you use, and so complaining about whether the interface uses strict or lazy bytestrings is somewhat misplaced. (OTOH, complaining that stream based codes should use strict bytestrings plus continuations or similar is perfectly valid.)
Yes you could, but it would probably be much slower. One of the schedulers for CnC worked exactly like that (using MVars to implement IVars), and it was slower than the other schedulers.
You can also easily add more operators like '%' ,'&gt;&gt;=' and '&lt;+*&gt;' without having to extend your datatype. Try writing a parser for Haskell expressions and you'll see why I advocated this approach!
Hmm, according to [the performance test readme](https://github.com/SKoschnicke/performance-test), the code where the bug was found is still 3x slower than the Ruby comparison even after the fix. I assume this is because of the immature PostgreSQL Haskell bindings?
PostgreSQL bindings appear to be the problem: http://www.haskell.org/pipermail/web-devel/2011/001062.html &gt;it still seems like there's a slowdown from the database code, but I think the widget performance bug was the big issue.
Okay I'm laughing. Plus ten. One is not enough. Must constrain inebriated side effects. 
What is RWS? Is it just like the name (read-write-state transformer) suggests, a transformer stack with ReadT, WriteT and StateT?
&gt; If your language doesn't have closures, I don't know why you are calling it "functional"... There are first-order functional languages. A lack of closures does not immediately exclude a language from being functional.
I didn't have a look at who the authors were before skimming though, and something jumped out at me. I noticed that there is a heavy influence from the Concurrent Collections programming model on this paper, which made me think: this has to be a Ryan Newton paper.... and sure enough. I love being able to do that sometimes.
If you're willing to restrict yourself a bit, it shouldn't be hard to either bake in reader, writer, state, exception which covers most of your bases, or maybe even just something restricted to any free monad.
Yeah, I use that too. It's great :-)
http://okmij.org/ftp/Haskell/types.html#partial-sigs
Yeah, but then plus == minus (assuming the Eq instance is defined in some sane way), so you'd still need an operator datatype to do pattern matching.
Not sure exactly what you're saying here but if it's that the compiler will optimize away the False guard this won't happen because of the trace function around it.
Yes, I've looked at the implementation now, and while there is a streaming façade, it's block-based internally.
that was more of the 'Oleg already did it' kind of comment.
I can't help but wondering if this can be summarized as "forkIO without IO + MVar without M". 
http://haskell.spreadshirt.com/oleg-already-did-it-A6499531
I find this often makes debug statements a little simpler import Debug.Trace debug :: (Show a) =&gt; a -&gt; a debug a = trace ("DEBUG: " ++ show a) a 
It's not quite the same thing though, although it's certainly in the same spirit. :-)
Someone wanna buy me that shirt?
This. Mine is slightly different though to help me distinguish between different occurrences: traceMe :: Show a =&gt; String -&gt; a -&gt; a traceMe msg x = trace (msg ++ ": " ++ show x) x 
Then of course you could add a bit of code for un-parsing to it, which I believe nullifies the plus == minus problem: data Op = Op { associativityOf :: Assoc, precedenceOf :: Int, stringRep :: [Char] } plus = Op AssocL 2 "+" What I wonder is how you would take care of that kind of problem if your datatype logically didn't admit such a solution. How would you define the Eq instance to take care of this?
Simon mentions that he does not know how to scale semi-explicit parallelism to 1000s of cores and I wonder why that is. Couldn't data parallelism be implemented using semi-explicit parallelism using the chunking Simon mentions? Did I misunderstand his comment and he just meant: not *all* semi-explicitly parallel programs do, but *some* may well, scale to 1000s of cores?
Yes: [7 MB PDF from Simon's homepage](http://research.microsoft.com/en-us/um/people/simonpj/papers/parallel/Parallel-Haskell.pdf) 
Thanks!
If you use the c preprocessor and its stringizer, you can even have it spit out the variable name, file, and line!
I always wonder what symbols are left for use in golfing. That means no symbols in Prelude can be used, like $ &gt; &lt; + - / . ^ : And lots of langauge symbols are also ruled out, like @ { } [ ] ( ) , ` ' ; | = \ So that leaves us with % &amp; ! ? # Are there any others?
OK so reasoning through that comment and looking for myself yielded 5 symbols, which is probably ample for golfing purposes. Still, any more?
Nice trick, thanks! Was about to suggest it be added to the wiki, but I see that it's [already there](http://www.haskell.org/haskellwiki/Debugging). But I might never have read that so thanks for pointing it out.
Those are the only five in the ASCII range. Of course, there are practically an unlimited number a Unicode symbols to use: ⊕ and ⊗ for example make perfectly good single character symbols for code-golfing!
THIS. THIS is optimization.
@simonmar There's a typo on p. 2: &gt; ...further capitalise on lazy-evaluation-for-parallelism on by building...
 import Data.Char main = putStrLn (filter (\x -&gt; isSymbol x || isPunctuation x) [minBound..maxBound])
Is there going to be a new release of the Haskell Platform incorporating this update?
The part you probably care about is [the release notes](http://www.haskell.org/ghc/docs/7.0.3/html/users_guide/release-7-0-3.html). Looks like some good bug fixes.
I believe dons has said on IRC that there's very likely to be a new release very soon.
Doesn't build: [2 of 2] Compiling Main ( lesson11.hs, dist/build/lesson11/lesson11-tmp/Main.o ) lesson11.hs:89:29: No instance for (VertexComponent Float) arising from a use of `vertex' Possible fix: add an instance declaration for (VertexComponent Float) In the first argument of `(=&lt;&lt;)', namely `vertex' In the expression: (vertex =&lt;&lt; liftM3 Vertex3 (readArray points (x + 1, y, 0)) (readArray points (x + 1, y, 1)) (readArray points ((x' + 1) `mod` 45, y, 2))) In the expression: do { let x' = (x + offset) `mod` 45; let fx = ...; let fy = ...; let fxb = ...; .... }
a haskell version of [this modern OpenGL tutorial](http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html) would probably be a good starting point when it comes to modern OpenGL.
I made just such a port when those tutorials hit reddit but haven't had the opportunity to polish and publish them. I may try to get that done within the next month or so, but if anyone else is interested, I'd be happy to collaborate on getting them out there.
That seems to be an old version. Make sure you're building the latest version on hackage. Should be nehe-tuts-0.2.0. Try running 'cabal update' and then trying again.
I just glanced over the tutorial and it looks like we'd need a tga file loader (maybe the tutorial provides sample code we could port to Haskell), and possibly a binding to GLEW. Although, I think GLEW is just used to abort if the version of OpenGL is not new enough. So, yes it should be fairly easy and making a GLEW binding is something I should put on my todo list.
Are your sources on github or any other publicly accessible place?
glfw has a tga loader (I use it in one of my projects), I notice it's not included in glfw-b though.
&gt; If your language doesn't have closures, I don't know why you are calling it "functional"... Read up on [Backus's FP](http://www.stanford.edu/class/cs242/readings/backus.pdf).
It's a Monad with reader/writer/state capabilities but should be more optimal/efficient than a Monad transformer stack of each one composed.
&gt; These tutorial need further updates in order to match the latest in OpenGL specifications. The tutorials are still written using OpenGL's immediate mode, but that was deprecated in OpenGL 3.x so it's possible that in the future these examples won't be supported by your graphics card. I doubt that OpenGL drivers will drop support for older version, far too many programs depend on e.g. immediate mode for that to happen. However, afaik, OpenGL ES doesn't support immediate mode at all, so for people wanting to learn OpenGL for embedded devices, these tutorials will need to be updated. Going even further, newer versions of OpenGL have deprecated the entire fixed-function pipeline. You have to do everything using shaders. This is why NeHe's tutorials are bad if you want to learn *modern* OpenGL. (I'm not sure if newer version of OpenGL ES have dropped the fixed-function pipeline too?)
OpenGL ES 2.0 drops most of the fixed-function pipeline and WebGL is based on OpenGL ES 2.0.
If I understand that correctly, Backus restricts himself to a predefined set of higher-order functions? I.e. the higher-order functions themselves cannot be defined in the system.
Sounds like the only thing that matters to Linux users is * Some bad floating point results on x86 have been fixed.
I've motivated myself to do so. I have some other deadlines today, but I'll put code on github up tonight or tomorrow. Included are helpers for buffer objects, textures, and a simple TGA loader. EDIT: Code and partial port of the tutorial are now [up](http://www.arcadianvisions.com/blog/?p=224)
I made [Moio](https://github.com/cobbpg/moio) for the same purpose. I didn’t add a behaviour abstraction, but your *apply* operation can be easily implemented using the dynamic application operator (&lt;&lt;- and -&gt;&gt;). Your *before* and *after* is missing from it though.
I had an issue with FreeBSD an binary (I believe) having some illegal suffix to a movq operation on an obviously 64bit FreeBSD. I wonder if a newer gcc needs to be used with that? Been playing with hscassandra and the low level thrift bindings. 
Curses! I just got done rebuilding everything for 7.0.2!
Hi, GLFW-b maintainer here. I didn't bind to those functions because they're [deprecated](http://www.glfw.org/faq.html#2_3).
GHCi&gt; Exception: &lt;stdout&gt;: hPutChar: invalid argument (character is not in the code page) Prelude Data.Char&gt; minBound :: Char '\NUL' Prelude Data.Char&gt; maxBound :: Char '\1114111' Oops!
The new world file support in Cabal might help you rebuild more easily.
Doesn't compile with ghc 7.0.2 (it comes with base 4.3.1.0), Relaxing the base version requirement to (&gt;= 4.2 &amp;&amp; &lt; 4.4) fixed it.
Sounds like your terminal isn't configured to handle Unicode. You should fix that, it's the future.
I may miss something here, but I don't see any point in using GLEW at all. Why would we need to bind a C++ library which provides the same functionality as glExtensions :: GettableStateVar [String] (and nothing else, unless I completely misunderstood something)?
I don't see that anywhere in the OpenGLRaw package index: http://hackage.haskell.org/packages/archive/OpenGLRaw/1.1.0.1/doc/html/doc-index-All.html Where does that function come from?
[from here](http://hackage.haskell.org/packages/archive/OpenGL/2.4.0.1/doc/html/Graphics-Rendering-OpenGL-GL-StringQueries.html). It has been part of the OpenGL binding since ages, way before the OpenGLRaw reorganization stuff.
In that case, I think what you're missing is that I'm intentionally using the OpenGLRaw bindings as they are superior in several ways. I bet the extension stuff can be split into a separate package. That would be a lot easier than making a GLEW binding, so thank you for pointing it out.
I just noticed this sentence: &gt; This release represents an effort to modernize the haskell version of these tutorials. *I have stopped using the OpenGL package and now instead use the easier/cleaner OpenGLRaw bindings* Let me point out that the point of OpenGLRaw is **not** to provide an "easier/cleaner" binding, but to provide a mostly complete, (most probably) auto-generated, low-level binding as a foundation of the higher-level OpenGL binding. It was separated out to its own package for two reasons: One is hygiene, and the second is that the higher-level bindings are a little bit behind the cutting edge in functionality (arguably, it takes more thought to create a new high-level API than to blindly bind the raw C functions), and so the OpenGLRaw package is exposed for the very few users who want functionality which is not yet in the higher-level OpenGL library (but they can still use the high-level binding for the rest).
Please read my other post (directly posted as a reply to your OP) about OpenGLRaw and OpenGL. I completely disagree about OpenGLRaw being superior. More functionality (at the moment), yes, superior, no way. Also, if you would just hit the "source" link, you would see that (just like anything else in the recent versions of the OpenGL binding) it is built on the top of OpenGLRaw.
I have yet to talk to someone about the OpenGL bindings who is satisfied with the higher level API. Everyone I've talked to complains about one of the following: the arbitrary (and often confusing) renaming of api functions, the existence of argument wrappers like Vertex3 (and then being forced to add type annotations whenever you use them), as you point out lack of support for some parts of the API, and the existence of the weird get/set (and Settable) api for IORefs. So it may not have been a point of OpenGLRaw to achieve a more straightforward API, but in my opinion it seems to have succeeded there.
Ok, now you talk with someone who would blindly choose the OpenGL bindings over the OpenGLRaw one. I mean, we are not coding in C for a reason... (`gl_ELEMENT_ARRAY_BUFFER_BINDING :: GLenum`, seriously??) (to be honest, I never used the raw bindings, but it is a 1:1 mirror of the C API, and I used the latter) Of course there are a few annoying stuff or bad design design decisions, but any serious user will hide those behind an abstraction layer anyway (for example I use [vect-opengl](http://hackage.haskell.org/package/vect-opengl) instead of complaining about the `Vertex3` stuff, which is indeed a bit too typed for my taste, too). I honestly think Sven did a very good work with the API, even if it is not perfect (nothing is), and instead of going back to low-level programming, we should think about how a better high-level API would look like.
Correct. This restriction was later lifted in FL however while remaining point-free.
Great code commenting! Though, being a noob with great interest in FRP, I'm hoping you put together a video explaining how to solve a simple problem with your library (:
I'm not too sure about the ins and outs of github. I'm going to move the repository to darcs as soon as the semester ends, though, so it's just a temporary thing! I also have rewritten the data reading to use monads; much prettier now. Just need to snag a bit of free time to implement the writing in the same way.
I like the Pure, Heavens, "Functional" bit early on in the video. Cute.
I do think Sven has done a ton of amazing work to deliver a stable and complete opengl binding. I just dislike the OpenGL package for very specific reasons. I think that a slightly higher level api would be nice as an overlay over OpenGLRaw but I wouldn't change the API as much as the OpenGL package does. I would probably make a phantom handle type like, data GLHandle a = GLHandle GLuint, and then add data Texture; data VertexBufferObject, etc. Then offer an api where you get a GLHandle Texture instead of a GLuint for you texture id. I've been toying with it, but I haven't fixed on exactly the right thing yet. By the way, I've been trying to contact Sven because there are some important performance patches and cabal file updates that I want to submit, but he's not watching the opengl mailing list or responding to my direct emails after a couple weeks. Do you happen to know how to contact him?
Do you have any particular example in mind that you would like to see?
Thanks, will fix. I always constrain library versions to adhere to the versioning policy.
Thanks for making repa available for GHC 7! I hope to test it some time soon. I have a problem trying to get repa-examples to build using cabal (Win 7, Haskell Platform 2011.2.0.0): bash-3.2$ cabal install --reinstall -p -v repa-examples [...] Linking dist\build\repa-blur\repa-blur.exe ... examples\Sobel\src-repa\Main.hs:13:8: Could not find module `Solver': Use -v to see a list of the files searched for. cabal.exe: Error: some packages failed to install: repa-examples-2.0.0.1 failed during the building phase. The exception was: ExitFailure 1 Any ideas?
sigh. Yes, Solver.hs wasn't in the package description, but cabal didn't warn me when I made the distro. I'll upload a new version. 
I remember having given a similar problem in high school: calculating some rate of leakage in a gas container in outer space (this was for some high school competition). Gas dynamics has never been something I was good at, and so I had absolutely no idea where to even start. What I did have, though, was a book containing tables of chemical and physical data. So I looked up the size of an oxygen molecule, and calculated how many would fit inside the hole on the wall of the container. Then I looked at the average speed of molecules at the given temperature and initial pressure, and calculated how long it would take a solid oxygen cylinder to travel enough distance through the hole so that its volume of the outside contained enough oxygen to decrease the pressure inside to the desired level. Needless to say, this was complete BS. My calculations resulted in a fragment of a second needed for the pressure to decrease; the correct answer, as my high school physics teacher later told me laughingly, was on the magnitude of years. 
This is awesome. Thanks!
It's an easy mistake to make - that's why I wrote a tool to detect and warn about it: http://neilmitchell.blogspot.com/2010/10/enhanced-cabal-sdist.html
Keeping the errors all on one thread, I get (ghc 7.0.3 x86_64, binary Linux build from HQ, repa-examples-2.0.0.2) /usr/local/bin/ghc --make -o dist/build/repa-mmult/repa-mmult -hide-all-packages -fbuilding-cabal-package -package-conf dist/package.conf.inplace -i -idist/build/repa-mmult/repa-mmult-tmp -iexamples/MMult/src-repa -i. -idist/build/autogen -Idist/build/autogen -Idist/build/repa-mmult/repa-mmult-tmp -optP-include -optPdist/build/autogen/cabal_macros.h -odir dist/build/repa-mmult/repa-mmult-tmp -hidir dist/build/repa-mmult/repa-mmult-tmp -stubdir dist/build/repa-mmult/repa-mmult-tmp -package-id base-4.3.1.0-1554f26e1cc1c87f47464e927dddbd20 -package-id random-1.0.0.3-15ae8b1458485ee9647f74174e442c33 -package-id repa-2.0.0.2-e414c245bda72fa59113d185489fe0f2 -package-id repa-algorithms-2.0.0.2-271bbc3f0e1fd623b35068fdd7d99aaf -package-id repa-io-2.0.0.2-6e4e39101872ba963841f5b23bad3ce4 -package-id template-haskell-2.5.0.0-f076ce2e9a11168d1eecec6bf38f1d23 -package-id vector-0.7.0.1-3a18a3c369c7e19a25c6ed8559ed6128 -O -threaded -rtsopts -Odph -fllvm -optlo-O3 -fno-liberate-case -funfolding-use-threshold30 ./examples/MMult/src-repa/Main.hs [1 of 1] Compiling Main ( examples/MMult/src-repa/Main.hs, dist/build/repa-mmult/repa-mmult-tmp/Main.o ) opt: Bitcode stream should be a multiple of 4 bytes in length 
Now it works, thanks! (Though cabal generated the .exes over and over in a loop on the first try. Second try went fine.)
&gt; Do you happen to know how to contact him? Unfortunately, no (actually, we already talked about this in email)
Ordered!
What is the swag? Edit: not that it matter...buying the book anyway.
Hopefully 'Land of Lisp'-style stickers ;).
[Seven months later...](http://www.reddit.com/r/programming/comments/d519u/learn_you_a_haskell_intro_to_monads/c0xm8bp) :p Looks awesome by the way, I think I'll buy a copy for a friend...
Why use negate instead of unary - ?
pet snails with leashes!
I love it when these things happen. Reddit never forgets.
Haha, the only reason it appears like my account's been so inactive is I've relocated to tmhrtly :p
Ordered!
Purchased :D
hey! it's you!! I have you in free_copies.txt!! anyway, can you send me your name &amp; address to bonus at learnyouahaskell dot com? there's a copy with your name on it (and mine also, but that's because i wrote it)
could be great, but I have a few questions: 1. is it possible to have different type of vars in a single term? I'm currently using this: data Type = Unit | Nat | Bool | Var (Id Type) | Ref (Id Region) Type | Func Type (Id Effect) Type 2. can substitutions fail/is it possible to implement failing substitutions? 3. does it use unsafePerformIO in any form?
1. Yes, this should work fine. 2. As written the subst function never fails. Are you saying this is something you want? Can you give an example of what you mean? 3. No. (Well, full disclosure, there are still a few stray occurrences of unsafePerformIO in some quick-and-dirty tests thrown in the ends of a couple modules, but they have nothing whatsoever to do with the functioning of the library, and I intend to get rid of them soon.)
there are substitutions in Talpin's effect discipline, that aren't allowed. it's not really that important, you can always do substitution anyway (during unification) and validate it immediately afterwards, failing in unification monad, which is usually possible. &gt; .3. No. bless your pure heart!
Clearly you forgot to allow time for all the empty space between the molecules to leave as well!
I'm not sure how I feel about this title.
The free eBook is much appreciated, since I frequently end up having to wait 2-3 months for orders like these to arrive.
Because in 2006 unary - was syntax specific to Num and not a redefinable operator. Perhaps this has since changed?
If I ask for an autograph and a quip about mutation in the "Special Instructions" section, will you deliver? Pretty please!
Awesome! :-) Edit: ordered! Can't wait for the snails!
Ordered!! Largely for proselytizing purposes, but I'll certainly enjoy looking through it again.
I think this is a bug in older versions of cabal. I've seen it copy the binaries to ~/.cabal/bin several times. I'm using cabal library 1.9.0 and cabal-install 0.9.0 and it seems fine now.
That looks like an LLVM and/or GHC bug. "opt" is the LLVM optimiser. I just built it on Linux x86_64 with GHC 7.0.3 and LLVM 2.8. What version of LLVM do you have?
Ordered, and already loaded on the iPad -- now when guests come over I can ask "Coffee? Tea? Introduction in Haskell?"
typo near end: sub**s**tract fmap (substract 1)
You're probably right - it seems I'm running 2.6 (debian stable package), didn't consider that. I'll pursue a new version!
just ordered mine
D'oh!
Kudos on that library! Does it offer some static garantees like "fresh names don't escape" (using parametricity for instance)? I'll be reading the paper tonight anyway but still, I'd like to know now :)
This is much faster than over amazon+delivery... since there are formats for my ebook reader! thanks
How can I get this in the UK? I've always loved this book and it wasn't until the monads chapter came out that I actually understood them (and I've had lectures on them by Wadler) so I'd very much like to own the damn thing.
Ah, that's probably still true. :(
Canceled my pre-order from Amazon and reordered from No Starch Press for secret bonus swag! I already got the ebook from O'Reilly during their help Japan ebook sale, so I guess you can count this one as a double sale (ok, maybe not double, more like a sale and a half).
[recent paper by Russell O'Connor, expanding on this observation](http://front.math.ucdavis.edu/1103.2841)
I agree with the sad face. Forgetting to use negate instead of unary - is the single most common mistake I make, and it is annoying since everything else is so transparent IMHO. Of course, it is caught immediately with a "No instance for (Num ...)" compile error.
It does offer some static guarantees, for example, the library internals will never leak, substitution behaves as you would expect, and so on. As for "fresh names don't escape", if you mean that in the sense of Poulliard and Pottier's "A Fresh Look at Names and Binders" (ICFP '10), no, it does not guarantee that, although it's something we want to explore for future versions of the library.
The website let me order mine to Switzerland (at a cost of course).
Have a look at this: http://haskell.cs.yale.edu/?page_id=103 Euterpea is a domain-specific language, embedded in the functional language Haskell, for computer music development.
is GHC 7 really necessary? and if yes, why? (I would guess TH, but earlier versions of RepLib worked on 6.12)
No screenshots though :( Whenever I see a title for an article related to something graphical, I expect to see a screenshot to determine if the code is worth taking and tweaking.
Hiya, i've emailed you from an email address ending @me.com with the MD5 hash d394a23ff1108cca46809a730a0c3a2d (for verification?) :p Thanks!
See the linked tutorial that does include a screenshot! I didn't really explain what any of the OpenGL bits are, either, but instead just targeted someone who came across the original tutorial, then felt discouraged by the surface area of the Haskell OpenGL API when trying to translate the code.
Just tried GHC 7.0.3 on Ubuntu Natty: Building bytestring-mmap-0.2.1... [1 of 3] Compiling System.IO.Posix.MMap.Internal ( System/IO/Posix/MMap/Internal.hs, dist/build/System/IO/Posix/MMap/Internal.o ) /tmp/ghc4083_0/ghc4083_0.s: Assembler messages: /tmp/ghc4083_0/ghc4083_0.s:859:0: Error: .size expression for __stginit_bytestringzmmmapzm0zi2zi1_SystemziIOziPosixziMMapziInternal does not evaluate to a constant cabal: Error: some packages failed to install: bytestring-mmap-0.2.1 failed during the building phase. The exception was: ExitFailure 1 
Wow. Seems like PM'ing him instead of email would have been easier. =P
Sure, pictures are nice, but for a tutorial on the basics of the API, there really won't be much to look at. It's all about drawing a few triangles. The topic needs to be a lot more effects-focused to have interesting pictures.
[According to the GHC documentation](http://www.haskell.org/ghc/docs/latest/html/users_guide/syntax-extns.html#rebindable-syntax), since GHC 7, the new RebindableSyntax extension implies that unary minus translates to whatever `negate` is in scope. However, it does not work here with 6.12 and NoImplicitPrelude.
I think Agda can generate some reasonably good Haskell code as well via similar means. Can anyone verify this? I think Coq might actually extract better into an ML language than Haskell.
Why ask for a ``Maybe Bool`` when you can just get a ``Bool``? ;)
&gt; If you want to do formal verification, `sed` should not be a mandatory part of your toolchain. Indeed! Some of that is pretty ugly.
I'm fairly curious why Wouter didn't decide to try and investigate using Agda for this rather than Coq. I can think of a couple reasons but I wonder made him decide to go with it.
I have trouble reading it as well, as with most white-on-black texts. Some blue output from hscolour doesn't help.
&gt; If you wanted to log a list of all input values, that would get large too, and also not be the fault of the FRP framework. In other words, no FRP implementation can make it impossible to write programs with a space or time leak. The actual question is whether there are *unexpected* time leaks, in the sense that the implementation needs values from more than just the previous instant. 
Proving theorems in Agda is *painful!*
Code extraction from CoC is ridiculously hard. There is no compositional way that you can extract nice code. (We're working on code extraction from Matita's internal language into O'Caml and Haskell, here. It's taking us ages.)
How does this compare to Cheney's FreshLib?
I like the following quote: &gt; Never trust ‘formally veriﬁed code’ that hasn’t been tested.
Great tutorial! Bookmarked. Did you implement the gl bindings for buffer objects? I had the impression only immediate mode was supported earlier.
Why doesn't code extraction just go straight to C? Why go to fancy languages like Haskell or ML? Awkward types with coercions all over the place is good C.
Agda can generate Haskell code - but this is still very experimental. Each subexpression has an unsafeCoerce wrapped around it to make sure the GHC type checker doesn't complain. I don't think this is usable for any serious development. I agree that extraction to Caml is better supported. A lot of the problems I ran into have to do with Haskell specific idiosyncrasies (type classes or certain Data.List functions that I needed to reimplement).
I couldn't get the compiled version to run with the error: main.exe: user error (unknown GLUT entry glutInitDisplayMode)
You could probably find a way to hack around it, but there was a bug in 6.x (I forget the precise details) that affected the ability to derive the generic instances we needed for the type-indexed names. RepLib itself still works on 6.12 as far as I know (although I could be wrong).
FreshLib was definitely one of the big inspirations for this work, and the implementations, using type classes and generic programming, are similar. The biggest difference is that unbound has a much richer set of type combinators for specifying binding structure, and the combinators come with a formal semantics and provable guarantees. On the other hand, one thing FreshLib has that unbound doesn't is the ability to work with arbitrary user-supplied name types.
What's the most pleasant way of dealing with this compilation error: lesson3b.lhs:195:26: Ambiguous occurrence `$=' It could refer to either `Graphics.Rendering.OpenGL.$=', imported from Graphics.Rendering.OpenGL at lesson3b.lhs:19:3-3 4 or `Graphics.UI.GLUT.$=', imported from Graphics.UI.GLUT at lesson3b.lhs:20:3-25 
Dons, isn't `focusLeft` "wrong"? The following calls `reverse` just at the zip's beginning, making it amortized O(1) instead of O(N): focusLeft (Zipper (l : ls) x rs) = Zipper ls l (x : rs) focusLeft (Zipper [] x rs) = Zipper ls l [] where l : ls = reverse (x : rs) *Edit:* proof, in OCaml notation with explicit heads and tails instead of bindings and `prev` ≣ `focusLeft`, `next` ≣ `focusRight`. First branch: next (prev (Z (y :: ls, x, rs))) == reverse (prev (reverse (prev (Z (y :: ls, x, rs))))) == reverse (prev (reverse (Z (ls, y, x :: rs)))) == reverse (prev (Z (x :: rs, y, ls))) == reverse (Z (rs, x, y :: ls)) == Z (y :: ls, x, rs) Second branch: next (prev (Z ([], x, rs))) == reverse (prev (reverse (prev (Z ([], x, rs))))) == { x :: rs is nonempty } reverse (prev (reverse (Z (tl (rev (x :: rs)), hd (rev (x :: rs)), [])))) == reverse (prev (Z ([], hd (rev (x :: rs)), tl (rev (x :: rs))))) == reverse (Z (tl (rev ( hd (rev (x :: rs)) :: tl (rev (x :: rs)) )), hd (rev ( hd (rev (x :: rs)) :: tl (rev (x :: rs)) )), [])) == { x is nonempty ==&gt; hd x :: tl x == x } reverse (Z (tl (rev (rev (x :: rs))), hd (rev (rev (x :: rs))), [])) == { rev . rev == id } reverse (Z (tl (x :: rs), hd (x :: rs), [])) == { x :: rs is nonempty } reverse (Z (rs, x, [])) == Z ([], x, rs) *Edit 2:* BTW, the fact that `focusLeft` is total is a corollary.
Physics teaches us that there is no such thing as simultaneity.
Also see [Rethinking Time in Distributed Systems](http://www.youtube.com/watch?v=VKkGqNRlUJM) with Paul Borrill ([on reddit](http://www.reddit.com/r/programming/comments/b7ql8/rethinking_time_in_distributed_systems_vid/)). Very interesting talk and I would be very eager to see any comments on how this talk relates to FRP.
Nice try, EZYang.
One solution (which I find very natural) is to make state machines a primitive combinator: mealy_ :: (s -&gt; a -&gt; (s,b)) -&gt; s -&gt; Event a -&gt; Event b Furthermore, you can simply disallow such cycles (that is, cycles without explicit breaks with infinitesimal or finite delays), to prevent the user from such surprises.
That would be a version of mapAccum :: (acc -&gt; x -&gt; (acc, y)) -&gt; acc -&gt; Event x -&gt; (Behavior acc, Event y) The only drawback is that I would have to filter all events that do not correspond to decrements afterwards. I don't think it's possible to disallow cycles with the type system, but it might be possible to catch them during the initial setup (as long as there are no dynamic events).
&gt; The only drawback is that I would have to filter all events that do not correspond to decrements afterwards. mealyMaybe_ :: (s -&gt; a -&gt; (s, Maybe b)) -&gt; s -&gt; Event a -&gt; Event b &gt; I don't think it's possible to disallow cycles with the type system, but it might be possible to catch them during the initial setup (as long as there are no dynamic events). Yes, that was my thinking exactly (and I simply forgot about dynamic events).
 mealyMaybe_ f s e = fmap fromJust . filter isJust $ mealy_ f s e :-) EDIT: $
of course (except maybe you mixed `(.)` with `($)`?), but having convenient non-primitive combinators is... well, convenient :) (also `mealy_ f s0 = fmap snd . mealy f s0`)
No, they are provided by the OpenGL library. I did make use of a (linked) library to make the creation of buffer objects easier.
Hm, I'm not sure. If the Haskell code compiles and links, then dies on startup, I would guess there's a problem with the GLUT installation. Can you get anything using GLUT to run?
They should be the same. You could try importing Graphics.UI.GLUT qualified, then prefix most of the identifiers in the main function. Alternately, you could move `main` into another module and import the necessary identifiers from the one I posted. If it's a problem with the Haskell OpenGL and GLUT packages being out of sync, `cabal install --reinstall GLUT` might help.
That focusLeft is wrong in the presentation. It does not exist in the XMonad code, which does have focusUp' (Stack t (l:ls) rs) = Stack l ls (t:rs) focusUp' (Stack t [] rs) = Stack x xs [] where (x:xs) = reverse (t:rs) 
&gt; Physics teaches us that there is no such thing as simultaneity. "Event" in FRP has a precise, compositional semantics, which may differ from what you've assumed. With that semantics, it's easy to express two events that have (some or all) simultaneous occurrences.
This is a reimplementation of software written in Haskell. Coq has a functional language at it's core (similar in many aspects to Haskell), so it makes sense to try to use it to reimplement some "real world functional program". This offers the possibility of verifying code within Coq and then extracting verified code.
Ah, OK. Where did Wouter get that one from, then?
There are multiple implementations of the ideas of FRP, and they do not always agree. The problem here is the interaction of events scheduled to occur at the same time, leading to a lack of well-defined interaction. Physics gives *locality*, turning the total order of past, present, future into the partial order of absolute past, incomparable (outside the light cone), same event, and absolute future. Events should not be able to interact with the same resource in a well-defined way unless they are totally ordered. 
Hello World, (apologies in advance for the chopped-up post - Reddit is weirding on me lately) I've been looking at the research on type inference. My main limitation is that I lack formal academic training in this topic, so I'd like to verify my understanding of the notation. A horizontal line is a "proof" something something. So basically, if I were to translate this to a function, the definition of the function (including pattern matching) would be below the horizontal line, and any needed guards are above the line. Usually, expressions beyond the right end of the line are `where` definitions. 
Γ is a "polymorphic type environment", usually of type `Map ValueVariable GeneralType`. `ValueVariable` is generally a `String`, and `GeneralType` is generally `(Forall, Type)`, where `Forall` is simply a list of type variables that are to be generalized. In the Chitil-Erdi compositional type inference, the `GeneralType` is really a `MonomorphicTyping`, for reasons that I don't comprehend yet. A `MonomorphicTyping` is `(MonomorphicTypeEnv, Type)`. I'm not sure if it's "equivalent", in some definition of "equivalent", to the `(Forall, Type)` thing. In the Chitil-Erdi scheme, Δ is a `MonomorphicTypeEnv`, which is a `Map ValueVariable Type`. It seems to be **returned** by the Algorithm C. 
In the Chitil-Erdi scheme, Δ is a `MonomorphicTypeEnv`, which is a `Map ValueVariable Type`. It seems to be **returned** by the Algorithm C. 
ψ or σ refer to a "substitution function", which seems to be a function which substitute type variables in a type. It is usually implemented as a `Map TypeVariable Type`. Depending on the paper, the syntax for "apply a substitution function on 'x'" is either ψx or xσ. Σ is a set of type equations, it is usually the input to the unification functions. It is generally implemented as `[(Type, Type)]`. `mgu` is a function that takes in one type equation, then outputs either a substitution function (to be composed with other substitution functions) or a further set of equations, or errors. ---- There doesn't seem to be any decent algorithm for handling subtyping. For example, I might want to define integers in my language to have a very specific range (e.g. `Int 0 31` for a 5-bit binary vector). Of course, `Int 0 31` is a subtype of `Int 0 63`, as it is possible to put any `Int 0 31` into an `Int 0 63`, but not vice versa (at least without losing data). However, this seems to be largely very impossible to infer. Or is my understanding wrong/outdated?
What do I win for identifying [the paper](http://dmwit.com/papers/201009SL.pdf) being viewed in the screenshots?
No, when I bought it (and before that) I thought I would, but I don't. It really ain't comfortable, unless you've a DX of course. The thing is: you cant transform a paper to mobi/epub/whatever, because the layout is important (for propositions, proofs, code, whatever). You have to leave it as pdf then, and doing so, the text will appear quite small on the kindle, unless you zoom (which is a zoom like in conventional pdf reader on pc, and isn't really smooth). So, if you're considering buying a kindle in that single aim, I would advise you not to. However, it really is a nice device for well… any book/novel/whatever you'd wanna read, and can still be handy for reading such « academics paper » pdfs, even if it won't be the "perfect experience" (but again, considere this as a bonus, not as the main goal of your kindle). I hope my answer has helped you, good night. EDIT1: (ok so this wasn't a question but a link to an article, didn't saw this, i'm very tired, excuse my mistake) (good night anyway) EDIT2: So the soft is nice, but it *won't* do the trick, the problem ain't in most cases the two-columns mode.
Yes, a horizontal line usually indicates that if the things above the line are true, then the things below the line are true. However, be careful: the relation between things below and above the line may not be functional. For example, consider these two rules: a b ------ a /\ b not (not a) ----------- a Now, imagine trying to conclude `a /\ b`. There are two choices: either show both `a` and `b`, or show `not (not a /\ b)`. It's not a function.
Γ is just a metavariable. It's *often* used when some kind of environment mapping is meant, but there's definitely no way you can generalize that to saying that it's always a map from variable names to types. You'll have to suss out what it is based on how it's used in relations and such.
You might get a more focused audience if you ask on http://types.reddit.com .
I do, on a Kindle DX. Thanks to the larger screen most PDFs are quite readable (including two-column papers with small font with), if not in portrait then in landscape mode. Still, this does looks like an improvement. Good stuff!
 &gt; The problem here is the interaction of events scheduled to occur at the same time, leading to a lack of well-defined interaction. The precise, compositional semantics I mentioned above is *complete*, thanks to the general denotational approach. Consequently, *all* (well-typed) combinations have "well-defined interaction" (including simultaneous events). Check out, e.g., [*Functional reactive animation*](http://conal.net/papers/icfp97/) and [*Push-pull functional reactive programming*](http://conal.net/papers/push-pull-frp/). 
Well, in landscape mode, I find that this software *does* do the trick -- for me, at least. YMMV, as usual. =)
A smile. Here you go: =)
I have a DX. Personally, I find standard page-sized PDFs almost too small to read, even on the DX. Also, I get incredibly frustrated with any sort of reading that involves lots of random access. For linear reading such as fiction, where I just page through as I read it, it is fine. But for reference works, or scholarly papers where I have to refer back to previously-introduced figures or equations, the slow page-turn time of the Kindle is maddening. It feels like swimming through molasses.
I would on a Kindle DX until I got Good Reader (http://www.goodiware.com/goodreader.html) for the iPad. While the screen isn't as nice the PDF viewing experience is, overall, better: Good Reader supports cropping that removes the margins ; The speed of the PDF rendering is a lot higher enabling me to switch pages faster.
Cool, another good program like this. I've been using [cut2col](http://www.cp.eng.chula.ac.th/~somchai/cut2col/index.htm), which I find works really well because it seems to be pretty smart about how it does the cutting. I also tried [pdfscissors](http://www.pdfscissors.com/) which also does the job but seems to require a bit more manual work. However cut2col doesn't come with source, so it's great to see another option. One thing is that cut2col has a pretty annoying GUI for selecting files; I'd rather have a command-line program, but I can't modify it since I don't have the source. So I'll definitely be trying this one. (I have an eSlick, and yes I use it to read two-column papers all the time. Very useful! Nice for reviewing instead of printing everything out.)
I had the same experience at a smaller scale: left my Sony PRS-505 for GoodReader on an iPod Touch. GoodReader is so convenient it makes up for the difference in screensize. Still, whenever I can get source for a paper I reformat it more appropriately for the small iPod screen.
Thanks a ton for this!
I'm fairly certain that there's a semantic domain on which `x = 2 - x` is well defined, but I haven't yet figured out quite what it is. Lazy naturals don't work at all. Improving values might capture it, but I'm not sure how. The problem is what we can choose that would guarantee the `-` operator is productive.
 &gt; There are multiple implementations of the ideas of FRP, and they do not always agree. I'm talking about *semantics*, not implementations. I've been surprised at the variety of things called "functional reactive programming". Perhaps people combine the associations they attach to each of these three words. From the beginning (before it was called "FRP"), the work had two primary properties: denotative and continuous time. For anyone who's interested, I've written a response elsewhere to the question "[What is (functional) reactive programming?](http://stackoverflow.com/questions/1028250/what-is-functional-reactive-programming/1030631#1030631)", which many people have found helpful. 
Thanks, I'm browsing it now. I've begun to understand just a little bit what the horizontal bar means from browsing there. However, it seems that the topics are a bit too advanced for me.
Benjamin Pierce's _Types and Programming Languages_ and Franklyn Turbak and David Gifford's _Design Concepts in Programming Languages_ are both friendly and comprehensive introductions to the concepts you'll see in research papers. Pierce has several chapters on subtyping.
Cool, thanks for additional links. I did look around for such a tool, but didn't find cut2col or pdfscissors. I'll look at both -- I might want to install (and/or hack in good bits from) both. =) ...argh, mold is making this difficult.
this is good, but a video game involving shuffling pipes around would be more fun :/
Very cool. I have a DX and I'll definitely be trying out this tool!
Are you, by any chance, working yourself through [my thesis](http://gergo.erdi.hu/projects/tandoori/) or its implementation?
This is cracking stuff. I bought myself a Kindle 3 recently and have been catching up on the Lambda papers and various functional pearls on my daily commute. This will be a great addition.
Probably on next year's April 1st.
Yes, I am. As an aside, I managed to get a copy of your thesis paper and there seems to be a misprint in the algorithm for let (section 5.5.5 on [this copy](http://docs.google.com/viewer?a=v&amp;q=cache:IwKTd0HcKNYJ:gergo.erdi.hu/elte/thesis.pdf+compositional+type+checking&amp;hl=en&amp;pid=bl&amp;srcid=ADGEEShMo5azu_8QkYUDNmewvfBZp2EGNGXBMk7FME8SrSy7MnXPkGx9Bj3-UfA5-dzvLkayWvKkjCIJyfRffxSRbq0irAg3FK9wO9MYdFHm_qsFpDZQKpa_LOvFVnhlqgHBXi93de6c&amp;sig=AHIEtbSTDBjeiKouXcjCQcWmAZhMNssUew)): delta0 requires psi, but psi requires delta0. I suppose that delta0 should really depend on psi0 instead. I've managed to implement a small section of it for the [language I've been going on about](http://www.reddit.com/r/haskell/comments/fgn42/research_on_twolevel_fp_languages/), and the error reporting is quite nice so far, but verbose. I've also needed to reorganize the type system since I didn't exactly plan for decent type inference. I've also started to read through Chitil's work, and the "directed debugging" seems to be attractive - i.e. I WANT! It doesn't seem to be explicitly specified, but algorithm C seems to be: algoC :: PolyTypeEnv -&gt; Expr -&gt; (MonoTypeEnv, Type) is this correct? By my analysis, it then seems to me that the equivalent to algorithm M would be something like: algoCm :: PolyTypeEnv -&gt; Expr -&gt; (MonoTypeEnv, Type) -&gt; SubstitutionFunction Right now I'm trying to shoehorn the fact that in my language I want to force integers to have user-defined limits in the type - basically, the type for integers is really an interval. This means that not only do I need type equations, I also need actual integer equations.
I find mobi and a regular size kindle works for many papers, or landscape mode for pdfs where figures get messed up (I keep one of each on the device and read mobi if it works, because of reflow). For 2 column the kindle software really needs some love from Amazon, like allowing finer grain zoom control, but this pdf split looks like a good work-around, thanks!
Well spotted! LordGreyhawk is right - it's a typo in the slide.
Comparison of performance (original Rails app vs. Yesod app resulting from conversion): [presentation](http://www.youtube.com/watch?feature=player_detailpage&amp;v=CD2LRROpph0#t=45s)
The [Papers](http://mekentosj.com) app is also quite nice, although unless you use Papers for the Mac too, I don't think the iPad version is worth the expense.
This looks awesome, and I'll give it a try. But reading pdfs on a Kindle is still sub-optimal. Most papers have an HTML version too these days, and what I want is an easy way to reformat the HTML version into a .mobi file, including (and this is the tricky part) decent-resolution figures. Anyone know if there is a script that can do this easily?
From the video: "The code that deals with side effects really stands out of the common body of code […] Is like having a bathroom in a house […], everybody who's in the house has to go to the toilet from time to time, but you know that when you need it there's bathroom, and you really shouldn't do the same business anywhere else in the house because it will just be a mess. That's why in Haskell, if you need to go to the bathroom you'll use monads."
In short the line is an implication a -- b is the same as a =&gt; b *edited to remove stupidity*
Great reading! The final image, the one under the heading "(Co-)limits as adjunctions", is missing. As an aside, I found the use of "to equal" as transitive in "so both must equal k" a bit confusing.
I use [trim-to-djvu](https://bitbucket.org/jetxee/pdfscripts/src/tip/pdf-trim-to-djvu) shell script to prepare papers for an e-Ink reader. It can take a PDF document and: * trim white margins * cut multi-column layouts in several smaller pages * create a color/grayscale/white-and-black DjVu on output (which renders faster and better than PDF in my e-reader) It will not work for Kindle, because Kindles do not support DjVu format as far as I know, but the script might be useful for users of other readers which do. Otherwise, if you feel like creating a PDF on output, it should not be difficult to edit the script. It depends on ImageMagick and djvulibre-bin utils.
Some inference rules can be read directly as an algorithm, some can't. The rules of the Hindley-Milner type system can't, because e.g. the rule for λ-abstraction involves a choice for the type for the variable, which you can't locally come up with. This is why W and M are more than this translation of the inference rules.
A pair (MonoEnv, Ty) is what is called a typing, yes. I am happy to explain details, but first please have a look at 5.3 which I hope is a reasonable explanation of the relationship between quantified types like Forall a.Ty and a typing (MonoEnv, Ty). The basic idea is that forall-quantified type variables are those not occuring in the MonoEnv. 
&gt; I managed to get a copy of your thesis paper and there seems to be a misprint in the algorithm for let (section 5.5.5 on this copy): delta0 requires psi, but psi requires delta0. I suppose that delta0 should really depend on psi0 instead. Good catch! Delta_0 should indeed be defined by the Psi_0-substituted Deltas. The type signature of algoC is correct (with the usual caveats that you may need to maintain some other state to create e.g. fresh type variables). I cannot (yet) comment on what the equivalent of an M-like upside-down algorithm would be for the compositional type system.
I am very interested in helping you with further problems that might come up. Don't hesistate to contact me directly if that's more convenient.
Don't you mean the other direction?
Yeah. Just being stupid for a moment.
Nice. But if it relies on ImageMagick... it doesn't convert the whole pdf to an image does it?
`haskell-platform-2009.2.0`? 01 April.
BTW, I adopted the term "denotative" from Peter Landin (grandfather of ML &amp; Haskell), who recommended it as a replacement for "functional". See [this comment](http://conal.net/blog/posts/is-haskell-a-purely-functional-language/#comment-35882) for pointers.
x is the fixed point of function f that is defined by f (x) = 2 - x. In the case - is strict, the fixed point is bottom; in the case that - is lazy, x is 1. In haskell, all arithmetic operations are strict. 
Operations are strict on Ints and Doubles. Who says they have to be strict on everything? The puzzle is to figure out what the proper semantic domain is (i.e. the proper instance of Num) so that the haskell expression `let x = 2 - x in x` *is* well defined in Haskell.
I have no proof that it's impossible, but consider the definition `x = 1 - x`. The point is that this slightly different definition has no integer solutions, but subtraction maps integers to integers. The proper semantic domain would have to perform division by 2, which is not obvious from the use of subtraction at all.
&gt; So basically, if I were to translate this to a function, the definition of the function (including pattern matching) would be below the horizontal line, and any needed guards are above the line. Unfortunately it is not always the case that you can directly translate the typing judgments into a functional implementation that way. We *like* for that to be the case, and so it is considered to be good form to let typing judgments be 'syntax directed', so that the choice of rule to apply follows from a straightforward translation of this sort, but it doesn't always work. A good example of where that approach breaks down is with subtyping. Γ |- a &lt;: b, Γ |- b &lt;: c ---------------------- Γ |- a &lt;: c That is to say that if a is a subtype of b in some environment, and b is a subtype of c in that same environment then a is a subtype of c. Here you can't just look at the goal a &lt;: c and apply this rule functionally, because you have no idea what to choose for b! There are ways to work around this for certain cases that come up in practice, but it just goes to show you can't always read off judgments like you want to. &gt; A horizontal line is a "proof" something something. The horizontal line in A - B is saying that given A, then B holds. When translating this into an algorithm, you usually read the step backwards to try to take apart some construct into smaller pieces. -------------- Γ, x : t |- x : t is a claim that no matter what Γ is, you can always extract a claim that x has type t from the environment. It is from this rule that your "Γ is a map" intuition comes from. Γ, x : s |- M : t --------------------- Γ |- (λx: s. M): s -&gt; t says that if you have an environment Γ, that when it is extended with an additional variable x with type s, in which the expression M has type t, then you can bundle that up into a lambda expression, which has type s -&gt; t. When 'read backwards' this helps you build your type inferencer. Similarly, Γ |- M : s -&gt; t Γ |- N : s --------------------------- Γ |- M N : t says that if in some environment gamma, the expressions M and N have types s -&gt; t and s respectively, then the application of M to N has type t. Each one of these rules isn't a proof, but they are used as step in a typing judgment. A typing judgment would be formed by composing those steps, by building towers of these by chaining them together. Ideally, in a nicely defined type system, there is only one such step that you can apply to any given context, but as with the subtyping example, this isn't always the case, so sometimes you have to be clever to transpose a type system into an algorithm. &gt; Usually, expressions beyond the right end of the line are where definitions. The statements beyond the right are 'side conditions' for that judgment to be sound, like certain variables being free, etc.
In addition to my more verbose reply, you may want to read up on sequent calculi, which is where these notations originated: http://en.wikipedia.org/wiki/Sequent_calculus
I p3rsonally couldn't draw too much from it that appli3s to FRP.
So that's what zygohistomorphic prepromorphisms are used for!
for what it's worth, it seems to me that the M-like top-down algorithm will be less efficient, not more. It does have the advantage of passing in a delta - the variable lookup can then look it up there rather than mock up a type variable that'll be unified into the delta built by lambda anyway. Also, it'll be easier to compute numeric equations that way (which I'll need if I define the integer type as intervals and have +-*/ work on intervals). Basically, for a function application algoCm(gamma, delta, `f g`, t): 1. get a = new 2. get psi_f = algoCm(gamma, delta, f, a -&gt; t); psi_g = algoCm(gamma, delta, g, a) 3. let delta_f = psi_f(delta); delta_g = psi_g(delta) ; a_f = psi_f(a) ; a_g = psi_g(a) ; t_f = psi_f(t) 4. Unify([delta_f, delta_g],[a_f ~ a_g, t_f ~ t]) ---- I'm not yet certain I want type classes in my language yet - I probably do, and I'll probably bash my head again when I change types so that I can handle type classes - but I have to balance my time between actually implementing my language or reading more type theory ;).
algoCm(gamma, delta, `x`, t) | x \`find\` gamma: 1. get (delta', t') = x \`lookup\` gamma 2. Unify([delta, delta'], [t' ~ t]) algoCm(gamma, delta, `x`, t) | x \`find\` delta: 1. Unify([t ~ (x \`lookup\` delta)]) 
Not sure what you are referring to, but the post is no joke: the software works as promised. (And in my time zone it is April 2 already so you can trust this comment ;) )
:(
:)
Yes, on a Kindle DX. Some text can be a little small, but the overall reading experience is good enough that I don't need to print 9 out of 10 papers I read.
Yes, that was I thought (in a sorta hand-wavey way) it would be, or something like that. However I got lost in `Tandoori.Typing.Instantiate`. By my understanding, a PolyTypeVar rule triggering would call the `instantiateTyping` function to get the Typing to be returned - but while descending into `instantiateTyping` I got lost trying to disentangle the monad... I'm almost sure my implementation of the PolyTypeVar rule is wrong (really, I haven't actually managed to wrap my head completely around `forall` for that matter...), although strangely it still typechecks very simple expressions correctly ;) So quick question - when instantiating a MonoTyping should I instantiate all the type variables? So I have { x :: a -&gt; t} |- t, I need to make { x :: a' -&gt; t' } |- t' when instantiating? Or just the t, like so: { x :: a -&gt; t' } |- t' ?
It also strikes me that since this is a *compositional* type system, it may be useful to have stuff like combinators. So we'd put gamma into the monad and allow execution with new gammas via: withGamma :: PolyTypeEnv -&gt; Infer a -&gt; Infer a We unify using: (&lt;===&gt;) :: Unifiable a =&gt; Infer a -&gt; Infer a -&gt; Infer a instance Unifiable MonoTypeEnv where ... instance Unifiable MonoTyping where -- this is actually more similar to Chitils' unify (MonoTyping deltax ty) &lt;===&gt; (MonoTyping deltay ty) = do psi &lt;- unify [deltax, deltay] [(tx, ty)] let delta = applySubst psi deltax `union` applySubst psi deltay t = applySubst psi tx in (MonoTyping delta t) We round off with some other utilities: -- attach a message to show when a unification error occurs -- similar to Parsec's &lt;?&gt; (&lt;?&gt;) :: Infer a -&gt; String -&gt; Infer a -- construct a function type (--&gt;&gt;) :: Type -&gt; Type -&gt; Type -- empty MonoTypeEnv zeroDelta :: MonoTypeEnv Then application is: typingOf :: Expr -&gt; Infer MonoTyping typingOf (EApp f g) = do a &lt;- newTv (MonoTyping delta_g type_g) &lt;- (typingOf g &lt;===&gt; (MonoTyping zeroDelta t)) (MonoTyping delta type_f) &lt;- (typingOf f &lt;===&gt; (MonoTyping delta_g (type_g --&gt;&gt; a)) &lt;?&gt; "applied value must be a function on the value to apply" return $ MonoTyping delta (retTypeOf type_f) A better example may be if-then-else: typingOf (EIf x true false) = do (MonoTyping delta_x _) &lt;- (typingOf x &lt;===&gt; (MonoTyping zeroDelta typeBool)) &lt;?&gt; "conditional of an 'if' statement must return 'Bool'" a &lt;- newTv (typingOf true) &lt;===&gt; (typingOf false) &lt;===&gt; (MonoTyping delta_x a) &lt;?&gt; "return types of 'if' clauses must be the same"
Does anyone realise how weird English sentences sound for Danish/Swedish/Norwegian people when diacritics or slashes are added to the o's? :)
GHC migratiuern ter git cermplehteh?
[never forget :)](http://www.haskell.org/pipermail/haskell/2005-November/017007.html)
Is there a web interface to ghc repositories?
I now have a proof that it's *impossible*. The key observation is that partial values can be represented by sets of total values. For instance, bottom corresponds to the whole set _|_ :: Nat = {0,1,2,3,…} Put differently, a partial value is represented by all total values that it can eventually be extended to. Functions can be applied to sets and monotonicity means that the sets are always made smaller. Strict addition would operate like this: 1 + _|_ = 1 + {0,1,2,…} = {0,1,2,…} = _|_ while lazy addition would operate like this 1 + _|_ = 1 + {0,1,2,…} = {1,2,3,…} making the set of possible values slightly smaller. The point of this is that we know how subtraction works on total values, which we can use to gain information about partial values. Namely, we have the lower bound f {x : p x} ⊇ {f x : p x} Alright, then, let's calculate. Denote `f x = 2 - x`. Consider a semantic domain that contains at least the numbers 0 and 2 and consider the set of total values S := f _|_ = 2 - _|_ Since `f` maps the values 0,2 to the values 2,0, we have S ⊇ {0,2}. But by the same reason, the set `f S` must also contain the values 0 and 2. This repeats ad nauseam; applying the function will never yield enough information to distinguish between 0 and 2. Hence, the least fixed point of `f` can never be a total value. 
dons wasn't allowed to use certain letters an 1 Apr due to reddit mold.
Yes: http://hackage.haskell.org/trac/ghc/browser
Hopefully there'll be an up-to-date GitHub mirror too soon.
Pandoc can convert to (at least) epub
They grow up so fast!
I make a point out of pronouncing [Motörhead](http://en.wikipedia.org/wiki/Mot%C3%B6rhead) and all those other Metal bands like they're written. Confuses the heck out of people.
I was getting the impression that darcs had improved a lot. Does is still have problems for large repositories?
my main problem with Kindle DX is that they decided to include 3G but not WiFi :-(
In the U.S. we can take it out for its first (legal) beer!
And already a father!
&gt; Comic Sans MS Stay classy, /r/haskell.
Yep. I'm hoping for a DX version of the Kindle 3 WiFi. With WiFi and the Kindle 3's full-featured web browser, it would be a killer reader, handling not only academic papers but also Reddit, Google Reader, and my other favorite online reads. (With my wife's Kindle 3 WiFi, the screen's a bit too small for Google Reader, and it's way too small for academic papers.)