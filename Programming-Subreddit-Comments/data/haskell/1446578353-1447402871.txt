Does anyone have a link to this library? I searched for "treersec" and it turned up nothing.
me and the team are happy to try to answer any questions here :) 
hylo :: Functor f =&gt; (f b -&gt; b) -&gt; (a -&gt; f a) -&gt; a -&gt; b
I think you're making a lot of sense. From an abstract perspective these are identical, though, or, rather, differences in implementation which could impact access rates. The nice part about lenses is that they let you treat abstract types as "bags of properties" both product-like and sum-like. This is similar to how abstract types work via allowing getters and setters (or course!) but more flexible. A type becomes "essentially concrete" when we not only know that some lenses exist but also that only these lenses exist and therefore the abstract type is isomorphic to some product (although it could be implemented differently still).
I'll be uploading it to hackage pretty soon, I wanted to fix a few outstanding bugs first.
It depends. If the data is only passed as is through your application without actually inspecting it, sure no need to be specific. After all we also model names as strings even if "trftiunrffwwww" is no reasonable name by most standards. If, on the other hand, your application logic depends on the distinction between different kinds of say positions, modeling this explicitly is exactly what you should do. If you add a position you might have to update your logic to handle the new case. Programs don't specify themselves (yet). If the compiler can tell you that and additionally where the problems are that's a great benefit and big part of what's supposed to be type-safety. Also you can always open up your type by specifying a general catch-the-rest constructor if you need that. data Position = Boss | Henchman | Other String 
Cool! Looking forward to seeing it. Enjoyed your talk.
Hi, team lead here, thanks for your question! The answer is: Yes. (EDIT Answer is serious, there is no template for the engagement. We've had devs pitching ideas to senior management alongside more traditional reqs-gathering from a LOB proposal.)
I noticed that too. It's not too hard to find his name from his post history.
Poor education is not an argument against quality. It took a while to learn to write, should we also ignore writing because some people can't?
This doesn't include training, it's just execution of a network that already exists. I don't think training can be done in one line.
Any plans to integrate this as a backend to emacs to mimic SHM or Paredit?
Thank you :) That's part of this field, though! Google is the best tool around. For the record, there is actually an isomorphism. And if you ain't know what that term is, you best be Goog-lin'.
Akka is the punchline to the Scala joke. Try to do it worse.
One thing I would add to this great answer is it's important to recognize that while most uses of arrays in imperative programs are best translated into haskell code using lists, haskell lists are not at all arrays; they are nested structures with this kind of shape: (1 (2 (3 (4 ())))) ...like a linked list in C (although haskell's lists are also not at all linked lists). Note also that the standard library also provides arrays (pure and various flavors of mutable) for when you need O(1) random access.
I haven't used Meta-Haskell, but to me that sounds like a cleaner solution. Wouldn't it?
That's a half-baked solution though. What if the communists take over? You have to have to edit and recompile to remove Boss. Closed sums are lovely for fixed lexicons, but for data modelling (which is my specific charge against them) they are a poor solution.
&gt; ap zip tail zip &lt;*&gt; tail also works :)
Reaktor compiles code natively yes, that's necessary to avoid the performance wall you describe. The programs you mention are solid tools that work great for lots of people, just like traditional analogue synthesizers work great for lots of people. For more experimental use, the lack of feedback is a problem, because it enables organic and often interesting behaviour. One popular Reaktor creation called Prism has a feedback loop around the whole synth, so it's not only about primitives. You could implement an entire synth as a primitive, but then there wouldn't be much point in languages like SuperCollider. :)
Akka is a huge pain in the tail, with a model that looks simple but is maladapted for almost everything. If you want to do http on Scala use Twitter's stuff. You'll stay sane and you'll actually get the perf that spray promises and stack traces when things blow up instead 'ask pattern timed out' and no clue where it came from.
The work sounds interesting but banks are generally abysmal work places.
Nonsense, if you're dealing with responses to calls to actors then you have to transform instances of Future. In Haskell you'd be doing the same with IO, except without the extra actor code, configuration of actor system and wrapping of database libraries as anything like that on Haskell will return IO instances. On top of that, typed actors are more work, without them, there's the risk of sending messages to an actor that doesn't handle that type. Actor systems solve certain problems well. But IO in Haskell is more equivalent to Future in Scala.
The deterministic finite automaton: check (s0, δ, isFinal) = isFinal . foldl δ s0
This is a response i secretly hoped for, except that this is not always convenient to hide law breaking operations.
&gt; You specifically disparaged code beauty You confuse mathematical jargon with code beauty. Very symptomatic about where haskell is going. &gt;Events and behaviors are pure functions of time. There's no better independence and "scoping" than this, nor better composability than what pure functions give. I talk about problems in reactive applications with which a software developer should be concerned. You answer with abstractions. Also symptomatic, unfortunately; Events happens withing spatial coordinates, in the scope of some object in the screen which is managed by some code in the program. Where these characteristics of reactive systems in displays are reflected in the FRP abstraction? Nowhere. How that spatiality is managed? no answer. That is the reason of the failure, of the FRP model for creating complex GUIs. And i mean failure. I repeat failure because I know you know we all know the failure of the FRP model for creating complex GUIs. Not to mention composable, efficient, dynamic GUIs. Phooey. I don´t know who made it. What matters is what Phooey does and does not. It improves the poor standard Haskell FRP kind of solution but has serious drawbacks: I see better the context where liftA2 is used. It has more problems than what I envisioned. It shares the problems of the FRP model but solves some of them. liftA2 does not compose widgets, it composes signals. Therefore it can not aggregate widgets to create bigger units of the same kind and so on. rendering is generated by the monad, but it is static. once created it can not changed. That means that no dynamic effects are possible, since the composition does not consider rendering as part of the result of the widget. Only signal results are managed. computation in the monad is reduced to signal operations. I don´t like to manage my todo list within a monad made for handling signals. Apart from that is an step forward. Specially for the de-inversion of control that makes it more intuitive and more composable. 
I tried something similar to what i understand your virtual field approach is (care to give a more comprehensive example? Would be great!), my feeling is that: - This is cumbersome when records become a bit complex (how do virtual fields *compose*?) - The substructures holding the virtual field are hard to reuse, and most often *you* are the one reusing them - Having your actual structures plain old data is sometimes a huge win (serialization is easier, refactorings are too, etc) As for a practical example, you can think of an html document you manipulate with a jquery like interface, in which you want to maintain some homogeneity between the body background color and the colors of some divs in a table element. An analogue would be: is it sensible to define a **documentColor** lens ? 
&gt; One does not simply open a text editor and start banging at the keyboard to write a Haskell program. Maybe it's just me, but of all the programming languages I know, I find Haskell to be the easiest one for prototyping. I think one of the reasons for it is that Haskell lets you write programs in terms of *what* it has to do, instead of *how*, and "what" is oftentimes simpler than "how". So yes, if I'm writing Haskell code, then in 90% of cases it means opening vim and "banging" at the keyboard until it compiles, ahem, works. Contrast it to JavaScript, which is widely touted as the perfect language for quick prototyping. That might be true for simple things like popping up a dialog box in a web page, but in my experience as the code base grows bigger, every refactoring becomes a potential source of numerous bugs. And while getting Haskell code to compile can be a pain from time to time, I just spent 6 hours weeding out typos from a single change in my JS code. Haskell never even gets close to being that bad.
Wait they're not linked lists?
Agda and idris and lean/coq are better technological foundations for smart contracts than ethereum. That and some of the lovely work by folks such as Conor McBride, Frank Pfenning and Jesse Tov. Further details requires joining the team or waiting for the eventual open source release of the language in question. 
I'll be in Manhattan for thanksgiving, wanna grab a drink? :P
From what I understood, the plan is not to operate on text (e.g. emacs buffers) directly, hence all the discussion about needing to do "version-control" with this system. It needs a LL(1) grammar defined up front, and this gives you a "UI" to modify the AST. The only way to get the text out would be to pretty-print it, but it didn't seem like there was an intention to go "the other way" from text to AST.
Indeed. Our mitigating factors are flat team structure, wide latitude to pursue non-standard-bank practices and technology (did we mention we're a Haskell group?), Brooklyn location and dress code, Macs, and the ability to avoid or ditch projects mired in legacy technology or turf wars. It's as close to a startup vibe as you'll find in this world.
Yep. We have an in-house ethereum VM that we're going to open-source soon ...
&gt; The problem is that "Declarations of this kind are disallowed" is not true. Do you mean the fact that these two pieces of code will work by "is not true"? One: someFunction = showsA A -- Commented out: &amp;&amp; showsA B where showsA = (== "A") . show Two: someFunction = showsA A &amp;&amp; showsA B where showsA x = ((== "A") . show) x This code will work but when you paste the second piece at lpaste (http://lpaste.net/4319417045872541696) lpaste says: 4:5: Error: Eta reduce Found: showsA x = ((== "A") . show) x Why not: showsA = ((== "A") . show) So following lpaste's advice leads to code not compiling with a nearly incomprehensible error message. It can also happen when trying to eliminate duplicate code: some :: Something a =&gt; SomeType -&gt; a -&gt; Bool some = ... checkSomething :: A -&gt; B -&gt; C -&gt; Bool checkSomething a b c = some (complex expression) a &amp;&amp; some (complex expression) b &amp;&amp; some (complex expression) c Obvious "fix" that won't compile: checkSomething :: A -&gt; B -&gt; C -&gt; Bool checkSomething a b c = exp a &amp;&amp; exp b &amp;&amp; exp c where exp = some (complex expression)
&gt; Unlike Haskell, if you ask your coworkers to learn Go over the weekend, everyone will ~~come back with a little app they built~~ come back refreshed from a couple days of leisure and mindfulness away from machines, ready to do this sort of work on Monday. FTFY
WTF JPMorgan, the Illuminati is real apparently. Are they trying to hack bitcoin for nefarious purposes. 
&gt; but in my experience as the code base grows bigger, every refactoring becomes a potential source of numerous bugs The general solution to this is to throw away the prototype, people hate to do it but just discarding the prototype avoids the refactoring pain quite effectively.
Why mysql-simple rather than persistent, hasql, or something similar?
I almost made this point in my own response. It doesn't hurt the rest of the model, and I wanted to focus on that without too many confusing details; glad someone brought it up tho.
We can do it as a series of one liners... fittest f = maximumBy (compare `on` f) search fit best rnd (current,local) = let c = (current - best) * rnd in (c, fittest fit [local,c]) pso fit rnds (best, candidates) = let new = zipWith (search fit best) rnds candidates in (fittest fit $ map snd new, new) evolve fit base = foldr (pso fit) (fittest fit base, zipWith (,) base base) This is a basic form of [Particle Swarm Optimization](https://en.wikipedia.org/wiki/Particle_swarm_optimization) All that remains is to make your chosen datatype (e.g. a [[[Double]]]) a Num and feed it a source of randomness, which I do not consider interesting enough to do now. Lifting `*` for example, is just a matter of `zipWith . zipWith . zipWith $ (*)` and the random is mostly just a bunch of replicating.
Uh... no. Banks are just interested in bitcoin as a tool and naturally would eventually spec out teams for it.
Here's a little pointfullization fix (liftM2 (:) head . liftM2 (filter . ((/= 0) .) . flip mod) head . (. tail)) $ [2..] focus on liftM2 (:) head . liftM2 (filter . ((/= 0) .) . flip mod) head . (. tail) liftM2 (:) head . liftM2 (filter . ((/= 0) .) . flip mod) head . (\f -&gt; f . tail) \recur -&gt; liftM2 (:) head . liftM2 (filter . ((/= 0) .) . flip mod) head . (\f -&gt; f . tail) $ recur \recur -&gt; liftM2 (:) head $ liftM2 (filter . ((/= 0) .) . flip mod) head (recur . tail) \recur -&gt; liftM2 (:) head $ liftM2 (\n -&gt; filter $ (/= 0) . flip mod n) head (recur . tail) \recur -&gt; liftM2 (:) head $ liftM2 (\n -&gt; filter (\x -&gt; x `mod` n /= 0)) head (recur . tail) Now we'll decode the `liftM2`s with liftM2 :: (a -&gt; b -&gt; c) -&gt; (m a -&gt; m b -&gt; m c) liftM2 f a b = do xa &lt;- a xb &lt;- b return (f xa xb) since `head :: [a] -&gt; a` we know that we're in the `([a] -&gt; _)` monad and `liftM2 f a b` is `\x -&gt; f (a x) (b x)` \recur -&gt; liftM2 (:) head $ liftM2 (\n -&gt; filter (\x -&gt; x `mod` n /= 0)) head (recur . tail) \recur a -&gt; head a : liftM2 (\n -&gt; filter (\x -&gt; x `mod` n /= 0)) head (recur . tail) a \recur a -&gt; head a : (\b -&gt; (\n -&gt; filter (\x -&gt; x `mod` n /= 0)) (head b) (recur (tail b))) a \recur a -&gt; head a : (\n -&gt; filter (\x -&gt; x `mod` n /= 0)) (head a) (recur (tail a))) \recur a -&gt; head a : filter (\x -&gt; x `mod` head a /= 0) (recur (tail a)) So now we have (guessing at the type of `go`) loop [2..] where loop :: [Int] -&gt; [Int] loop = fix go go :: ([Int] -&gt; [Int]) -&gt; ([Int] -&gt; [Int]) go recur a = head a : filter (\x -&gt; x `mod` head a /= 0) (recur (tail a)) Since `fix f = f (fix f)` we have loop = fix go loop = go (fix go) loop = go loop loop = \a -&gt; head a : filter (\x -&gt; x `mod` head a /= 0) (loop (tail a)) so loop :: [Int] -&gt; [Int] loop a = head a : filter (\x -&gt; x `mod` head a /= 0) (loop (tail a)) is just a regular prime sieve—essentially the same one that's on the https://www.haskell.org/ page. It's not even shorter to write it in the super pointless style fix (liftM2 (:) head . liftM2 (filter . ((/= 0) .) . flip mod) head . (. tail)) $ [2..] let loop a = head a : filter (\x -&gt; x `mod` head a /= 0) (loop (tail a)); loop [2..] But that's how you can laboriously repointilize pointless code.
Yes!
I've been using a workflow recently where I keep a cabal-sandbox in ~/temp/haskellplay and a shell script `hs`: #!/bin/sh mkdir -p ~/temp/haskellplay cd ~/temp/haskellplay exec vim Module_$(date +"%s")_$$.hs Then I can just jump into a file and load it with GHCI when I'm playing around with an idea. The sandbox allows for library dependencies to be reused between experiments. Since GHCI spins up so quickly, I don't bother to keep it running anywhere, and just bind a vim mapping to load the current file. This also works well for listing by date, grepping, etc. Works for me! 
Haskell expertise isn't the key bit, it's ability to learn new things quickly and engineer well. And communicate with at least other engineers effectively. 
Very neat! So many interesting things that I am not yet qualified for :) Approximate salary range? Minimum experience requirements?
Yeah, that's not surprising, though Ethereum is going the way of formal verifictation as it is. The subject of CASPER as far as I know is being first done in Pi-Calc, and it would solve more or less solve the problem of scaling. Needless to say, it's a bit disingenuous to say it'd be better done in those languages, Ethereum at base is a set of VM primitives that access a network (afaik, it's besides the point?), the system being mostly agnostic and specification driven, and the Foundation seems pretty open to responses that make the core system better. I'm pretty interested what sorts of changes will happen to core/head come time of the Ice Age, though... Any idea what to look out for when it comes to this planned new language or system by McBridge et al.? /u/cartazio ?
 f = ("a" == ) . show mono.hs:3:17: No instance for (Show a0) arising from a use of `show' The type variable `a0' is ambiguous Possible cause: the monomorphism restriction applied to the following: f :: a0 -&gt; Bool (bound at mono.hs:3:1) Probable fix: give these definition(s) an explicit type signature or use -XNoMonomorphismRestriction Note: there are several potential instances: instance Show Double -- Defined in `GHC.Float' instance Show Float -- Defined in `GHC.Float' instance (Integral a, Show a) =&gt; Show (GHC.Real.Ratio a) -- Defined in `GHC.Real' ...plus 23 others In the second argument of `(.)', namely `show' In the expression: ("A" ==) . show In an equation for `f': f = ("A" ==) . show f :: Show a =&gt; a -&gt; Bool ... If you add type signature then it will compile. If you separate definition, it will produce error message you want. I remember myself puzzled by this kind of error. I came to conclusion that in case of some confusing error it is good to annotate expressions with type signatures or separate them.
Did you consider using AWS containers and deploying the image you build in? That'll avoid any issues with data used by dependencies.
&gt; I normally have one editor window (vi) where I edit stuff, and SBT shell where it continuously run tests / main or whatever I specify I think you're going to enjoy my workflow then! I have two terminal windows open, one with my editor (vim), and one which continuously runs tests / main or whatever I specify. That second window runs [ghcid](http://hackage.haskell.org/package/ghcid), which recompiles your project every time you change a file. To set it up, you need to create a `.cabal` file describing your project structure and dependencies, similar to `build.sbt`. In addition to the fields generated by `cabal init`, I like to add `ghc-options: -W -Wall` so that `ghcid` prints warnings about my code. I also add [`doctest`](https://hackage.haskell.org/package/doctest) to `build-depends`, along with any other library I'll need for this project. Unlike `sbt`, `ghcid` does not install the dependencies, so I then run `cabal sandbox init` and `cabal install` to install them locally to my project. In `src/Main.hs`, I then write a failing doc test to make sure everything is working, and I write a function which runs all the doc tests in this file. import Test.DocTest -- | -- &gt;&gt;&gt; 2 + 2 -- 3 main :: IO () main = doctest ["src/Main.hs"] Finally, in the other window, I launch `ghcid` and I tell it to run my tests every time I change a file. $ ghcid --test=main I then confirm that it's doing its job by making the test pass. This behaves similarly to `sbt '~ test'`. I haven't yet figured out a way to emulate `sbt '~ re-start'` (for Haskellers reading this: [`re-start`](https://github.com/spray/sbt-revolver#usage) kills the executable and starts it again, useful for testing servers and other long-running programs).
There's a limit to how much I can layout the problem space and how the design for the right language / vm for this domain is fundamentally different form anything that's currently out there. Those who wish to know more need to either a) have coffee with me sometime. b) join the team c) wait for the open source release and/or for a paper on the language design to be published at some PL conference in the next year. We do some seriously interesting stuff in terms of language semantics / types to make it easy to model our problem domain. And getting there took a lot of thought, and it's still rapidly evolving. 
I don't see any reason editing the AST via the pretty printed output is precluded. This is what SHM and paredit attempt to do. 
Original parent asked if folks graduating in the spring should also reach out. The boss says yes!
What I meant by it not being true is that the declaration is allowed. It's the use with two different type instantiations that isn't allowed. So the error belongs with the use, not the declaration. It also looks like, from a different reply, the clearer error message I agreed would be good is already implemented, but gets lost somehow when the declaration is local. So this looks like just a bug.
I don't suppose there's any chance of you open sourcing whatever consensus algorithm you end up using?
So is this officially part of JP Morgan (they have a building in Brooklyn? any other JPM teams there) or is some kind of a team that's contracted out?
Tabs. So much. It's the only warning that I turn off, rather than on.
Well i guess thats true, yes
We had coffee with your NYC team mate earlier this week :) The external language need not be that rich, but the core / ir , for certain interesting modelling requirements, MUST be if you wish to sanely build a composable modelling toolkit. 
I work in tmux, so I tend to `&lt;C-b&gt;o` back and forth between a vim split on the left, and a GHCi split on the right. I also occasionally make a couple of helpful mappings. This one reloads the file in GHCi: nnoremap &lt;CR&gt; :silent !tmux send-keys -t :2.1 :r Enter&lt;CR&gt;&lt;C-l&gt; The first `&lt;CR&gt;` is the Enter key - I like to just hit Enter in Vim to reload GHCi in the other pane. `:` from normal mode puts you in command mode. `silent` keeps Vim from reporting on the shell's output. The `!` is how you call out to the shell. the `-t :2.1` is how you say "window 2, pane 1" - look at your tmux bar for the window (asterisk next to it), and if unsure, do `&lt;C-b&gt;q` to get a 2-second overlay of the pane numbers. I happen to be in :2.1 right now, so I used it as my example. Be sure you get this number right; I had it wrong once, and spent several minutes spamming nonsense to #haskell in a different window that I couldn't see. Anyway, the `:r` is the command to reload in `GHCi`, and the Enter is interpreted by tmux's sendkeys as hitting enter (the space allows it to parse - without it, it literally sends the keys in the word "Enter"). The second `&lt;CR&gt;` is interpreted by Vim to actually run the command (i.e. finally hitting enter on the command line) and exiting back to normal mode. The final `&lt;C-l&gt;` (that's an L) is "redraw" in Vim from normal mode, which I have to add, because "silent" makes my Vim display go all wonky (YMMV). I also like to clear the screen in GHCi (also `&lt;C-l&gt;`), especially to more easily a small pile of compiler errors, so I sometimes make this mapping, too. nnoremap g&lt;CR&gt; :silent !tmux send-keys -t :2.1 C-l&lt;CR&gt;&lt;C-l&gt; `vim-slime` (and maybe others) promprt you for the window and pane to send to, but it's more about sending bits of code, i.e. selection, and it wraps things to try to make multiline into one-liners, and I've found that it gets things wrong many times, and was just annoying. There's literally no way to get around some of its choices, so there are important things you simply can't send. I tried hacking it, and got a little of the way there, but it was messy. These two mappings go a long way to giving me the little bit extra I want.
Yes, that's a possibility I have been recently exploring, although I do prefer, at the end of the day, to have a real binary rather than a container. EDIT: I'm quite surprised by the downvote, as I'm essentially saying, perhaps explaining myself poorly, exactly what /u/Taladar is saying.
At least two windows: ghci, vim (or whatever text editor, it really doesn't matter as long as you configure it to convert tabs to spaces for you). Whenever you save your file :r in ghci will reload the currently loaded file and you can try out your new definitions.
If it took 4 years to learn, it means you didn't have a good mentor you could ask questions. I've personally seen how people learning Haskell *with* mentors around learn it very very quickly. They become proficient beginners in a matter of a few days. Intermediate Haskellers in a month or two.
Thank you! I still have to understand a couple of parts but your explanation is perfect. It's fix that confuses me the most.
Why flip? That you need flip to express this type signature "subtly points in the direction" that the type signaure should be the other way round. ...or that foldl' should have its second and third argument swapped.
Wow, looks really cool. Too bad I'm just a FP novice.
The theorem states you can't do this *in general*. You admit this yourself by having a "can't tell" return value. For a sufficiently general decision procedure, almost all programs will return "can't tell". Note that even inspecting the source code and deciding if two programs are equivalent in behaviour (program A can be transformed into B by *some* process which preserves semantics) is itself undecidable.
&gt; I think the major takeaway for me has been to carefully evaluate whether or not Haskell is the correct vehicle for me to develop this critical new piece of infrastructure. I'm working on a Haskell project now in a Java shop. I started with a Haskell tool to generate test cases, more like a script to get the job done. Now I have a small project where I generate protocol client/server code, similar to protobuffer or thrift. Using Haskell at a small spot where nobody notices and going further with the knowledge was a strategy that worked for me. 
For me it's the other way around. This has nothing to do with these programming languages, it's mostly personal familiarity that matters.
Why would you deploy the image you build in? It is much larger than necessary and contains many tools that should not be in a production environment.
This is the way we have been using it, but it turned out to be a nightmare after a while. cabal's add-source behavior is weird. When a dependency of a project changes, it tries to rebuild all the dependent projects as well. Confusing and slow.
Not even close. Don't bother commenting if you don't know what you're talking about.
Agree, I was looking at `linear` and it uses generics for its internals. I was just trying to understand how to write statically-typed vectors..
Cool, I didn't know stack has such capability. We are using cabal for now, but that's another good reason to move to stack.
I have never tried nix, but it seems to be a reasonable choice for a functional programming lovers. I am not sure what the transition cost is, anyways. Considering the fact that we already have all of environments setup using cabal, stack might be an easier option to move to.?
&gt; One does not simply open a text editor and start banging at the keyboard to write a Haskell program TIL I've been doing Haskell wrong the whole time.
I can reconcile the type with the definition now, but I'm having trouble outlining a proof for its uniqueness. Not exactly sure what I'm missing.
I'd say it's not really that hard once you get through the rather hairy interface. Still, I've found it much easier to use [vinyl](http://hackage.haskell.org/package/vinyl) ([HList](http://hackage.haskell.org/package/HList) is fine too I guess), and just loop over the data types, using typeclasses if necessary. 
I think if you look at the original Greek, it's written with one of those "rough breathing" marks that get transliterated as "h". Not sure why it doesn't show up in English for "eureka", though it does for, for example, "history".
Here's some [more words on that subject](http://blog.crabmusket.net/2015-07-10/development-versus-deployment-docker-and-haskell/) (shameless self-promotion).
Bear in mind that stack allows you to pinpoint a git revision by commit, so ideally, if that matches an Hackage release, should be pretty stable.
You know about Kontiki, right? ;-)
Well played :-)
Some of it would port over - the selection and input algorithms in particular. The pretty-printing would be much less flexible, because the output would have to correctly parse as plain-text. Placeholders could be represented with a special character or string, configurable per grammar. If a user modified the file manually so that it didn't parse, the mode would stop working until the error was corrected. I'm not personally interested in pursuing it. Many of the features I want to implement are pretty particular to a 2d-graphics world, rather than character buffers; for example, live editing of a page layout or TeX-style formula, or an image.
In JuicyPixels, the bit reading is done with a [state monad transformer](https://github.com/Twinside/Juicy.Pixels/blob/4ce78b82bd35b7f3b8160461fa974063dbb72787/src/Codec/Picture/BitWriter.hs#L57) over ST (ST is used to write the underlying image). -- | Current bit index, current value, string data BoolState = BoolState {-# UNPACK #-} !Int {-# UNPACK #-} !Word8 !B.ByteString -- | Type used to read bits type BoolReader s a = S.StateT BoolState (ST s) a It think a ReaderT with some STRef could relieve the pressure on the GC some more, but as it's not the bottleneck of the current implementation, I didn't gave it much though. And the readBits functions are clearly not optimized in the current implementation.
Private dependencies that aren't uploaded to hackage?
[Binary-bits package](http://hackage.haskell.org/package/binary-bits) does something similar.
 This is really awesome! I like minimalistic gems like this. What I always wonder is how the complexity is kept at bay while supporting more features (e.g. full http)
TIL what a strong monad is. This was a strangely educational read. I'm also hungry now.
your post seems to imply that this isn't common among haskell packages... however, just take a look at the `lens` package, which is cluttered with examples... 
Lens is a great example of the kind of docs I love. However, standard Haskell libs seem to be somewhat lacking. For example, take a look at Haskell's Data.List: https://hackage.haskell.org/package/base-4.8.1.0/docs/Data-List.html . For people familiar with FP, all of these definitions might be straightforward and understandable, but for people coming from an OO background, examples would explain a lot. If you'd like to see a good example of this, I'd suggest you to take a look at Elixir's list docs: http://elixir-lang.org/docs/stable/elixir/List.html . This is exactly what makes Elixir so accessible IMO, and makes it harder for Haskell to get accepted by the masses. People read the explanation, don't understand the concepts mentioned, and need to start googling. A simple example sometimes makes way more sense.
Thanks all of you guys. I think this is the one I am looking for.
If switching from Vim, [Spacemacs](https://github.com/syl20bnr/spacemacs) is a nice pre-built emacs config. I actually switched to it from my custom emacs config without evil mode. It has a really good Haskell layer too 
&gt; I'm probably writing a paper on this in the near future, maybe you would like to be a proof-reader at some point? That would be nice :) Yes, certainly, I can have a look. :-)
This is a very broad question. To be honest I find the reader monad mostly excessive in use cases like this. It's easy enough to just pass around the context or environment by hand.
Yeah. I'd really rather avoid that, though. This needs to be a standard environment, for testing purposes.
That is a very cool use of pattern guards.
I'd like to see code repositories carefully maintained to tell a story that goes from idea, to grokkable prototype, to full-featured thing full of incidental environmental complexity. Instead of viewing the repository log as a timeline of random work, you'd view it as a structural spine of the project as an evolving whole. This might be too much work for most projects. Successful implementation of this might require some kind of meta-repository, because right now it's very difficult to collaborate in the presence of rebasing.
I learned way more from this than from years of casually browsing random Wikipedia pages about category theory. My hunch is that mathematicians talk kind of like this among themselves at lunch, but then obfuscate everything when they sit down to write the PDFs so everything will sound really serious and impressive.
Email myself (william &lt;dot&gt; j &lt;dot&gt; martino &lt;at&gt; jpmchase &lt;dot&gt; com) or carter with your info, your background definitely sounds solid enough for me to be interested. If you look up carter on twitter (@cartazio) you'll get all the relevant papers you could ever want.
And as an extra benefit, you can check your examples with [doctest](https://hackage.haskell.org/package/doctest)
Asking about generalization is a fair question, of course. I think that the full Servant code base is not horribly complicated once you understand the basic setup (which the blog post and if necessary, the paper should help you to do). If I find the time, I may very well end up writing a "part 2" of this one though, trying to add a number of extensions/generalizations to TinyServant and see how this affects the implementation. After all, one of my claims in the post is that TinyServant is useful for exploration and experimentation of features, so I might try to do just that in the future ...
There's talk of a London branch opening up...
Partly it's to reduce confusion between server and client code. If you can have a shared base of types and logic, that can eliminate a whole class of bugs. This is also a big reason to use JavaScript as your server language, if you don't want to use "transpilation." That reason isn't unique to Haskell. It can also explain ParenScript (for Common Lisp) and ClojureScript (for Clojure) and Opal (for Ruby) and so on and so on. Then of course there's also the fact taken for granted by Haskell programmers that compiler type checking is helpful for developing correct software. If we didn't believe that, we wouldn't use Haskell at all. Related is the belief that Haskell's features make it especially well-suited for particular kinds of programs. For example, if you want your web app client to do some tricky kind of parsing, tree manipulation, compiling, or whatever it is you find Haskell excels at, then it makes sense to write that in Haskell and use JavaScript as the output language. Some people choose to view JavaScript as a kind of assembly language for the web. You could even compile Haskell to "asm.js", the assembly-like subset of JavaScript designed for efficient execution while maintaining backwards compatibility.
What are the single and double dot operators in the monad laws?
You could ask yourself a question, why people use all these languages only to convert their code to assembler. Why not code directly in assembler? Surely anything that one of these compilers can make could be written in pure assembler,or am I wrong? Now the comparison is not exactly fair, because javascript is a very high level language (compared to assembler). But it is still not very helpful for programmers. It is a dynamic language with a very bad error reporting and non existent module system. That's why the new version of javascript is coming (JS6) with added module system. That's why Microsoft made Typescript to add (optional) static typing. A lot of people are not happy with current javascript. Not only those who want to use haskell. But practically everyone. Even people who code in javascript do not code in javascript (they use babel to compile not released js6 into current js5) So to answer your question. Javascript is horrible. And everyone who is using it is trying to move to some other language that compiles to javascript. But different people have a different opinion what language to chose to replace javascript. (there are hundreds of languages that compile to javascript). And amongst those people there are those who think haskell is a good replacement.
I understand that many haskell libraries are a underdocumented (although I think that has changed a lot lately), but as to your example of Data.List, reading it, I can't think of how to make it any better. The only knowledge it presupposes is knowing what a list is and the most basic of haskell syntax. What do you believe is missing?
&gt; Also, a "lax monoidal endofunctor" is totally the categorical name for "Applicative"! It's one name for Applicative, but it's not my favorite. "Closed functor" is more direct. &gt; I learned that the categorical jargon for this is "T is a strong functor". In Haskell, all functors are strong: strength (a, fb) = fmap (a,) fb This is because (platonic) Hask is a cartesian closed category. Note that it is possible to be both a "strong functor" and a "strong monoidal functor" (or, for that matter, a "lax monoidal functor"). For this reason, this notion of strength is often pronounced *with a strength*. This makes an Applicative a *lax monoidal functor with a strength*. The `Strong` type class represents a particular kind of strength: "profunctor strength with respect to the product structure of Hask." The linked paper goes into quite some detail about this, but it is indeed based on this notion of functorial strength. &gt; "Note that the tortilla endofunctor is, in general, not monoidal. Given tortilla-wrapped rice and beans, it is usually not possible to recover tortilla-wrapped rice and tortilla-wrapped beans" This would prevent it from being a *strong* monoidal functor, but it could still be a lax monoidal functor. &gt; "Nor is it always possible to produce a tortilla-wrapped nothing out of nothing" *This* is what actually prevents it from being a monoidal functor. A monoidal functor must have a morphism ϵ : ID → F(ID), where ID is the monoidal unit, in this case ε (nothing). On the other hand, I don't see why you couldn't fold up an empty tortilla and call that T(ε) as a matter of lore. &gt; you can probably have a recipe from (T Nothing, a) to T a. Yes, assuming that you can have T Nothing, this is given by strength and the monoidal properties of Nothing and T.
What's new: * incremental output thanks to Haskell-Pipes (both with default human readable output and with JSON). It feels faster and actually uses less memory. * Added the `--cabal-file` option to read default extensions from Cabal. * Added the `--cabal-macros` option to read Cabal-generated CPP macros (like MIN_VERSION_base...). Argon cannot find the file automatically, you will have to supply the right path. If you're using Stack, it's very easy, the path is `$(stack path --dist-dir)/build/autogen/cabal_macros.h`. This file is generated during the building stage, so it's up-to-date as long as you've recently built your package. * finally, solved many problems with GHC 7.8.4 (GHC 7.10.2 is always recommended). What's being worked now for a future relase (suggestions/critiques and general feedback very welcomed): * widen the interpretation of cyclomatic complexity to consider also the number of local "variables" in a function body. This will allow more relevant results. If Argon fails to parse some code, please file a bug on Github or post it here so that I can fix it. I've analyzed a lot of popular Haskell packages, but it's very likely that I missed on some obscure/rare GHC extension.
TypeScript and Flow did a good job of introducing static typing into the JavaScript community. But what JS, and many languages which compile down to JS, including TypeScript and Flow, lack is good support for ADTs and pattern matching. Haskell seems nice at first, and GHCJS is getting better every week. But the compiled code does not integrate well with plain JavaScript code, especially when you try to call into Haskell from JavaScript. Due to how GHCJS implements weak references, you have to explicitly export Haskell heap objects if you want to pass them to JavaScript functions, and dereference them when you're done. This amounts to, essentially, manual memory management. So while I would love to use Haskell, I had to resort to PureScript, which integrates much better with JavaScript, while retaining many of the nice features you know from Haskell (ADTs, pattern matching, purity). What I miss is TH (for generating lenses), good documentation generator (a la Haddock), and some of the language constructs.
QQ: Why do you need 'data Proxy a = Proxy'. Why can't you simply pass in the layout directly and pass in as value (undefined :: MyAPI)?
ES6, not JS6. And not even that is true anymore, it's called ES2015 now.
All I can say for certain is that arrows are a big part of this library, Kleisli arrows in particular. Using an arbitrary arrow in place of (-&gt;) is what makes the datatype F interesting.
One of my personal project ideas is a Github repository viewer that easily allows you to step through the history. The primary use case that I have in mind *is* a sort of time-travelling blog post that goes into incremental development of some idea. Ah! To have free time again!
This is *so* cool. I'm tempted to hack on Servant now!
I answered this [here on StackOverflow](https://stackoverflow.com/questions/31636431/data-proxy-in-servants-public-api-why-proxy-with-scopedtypevariables-doesnt-w/31637456#31637456). Note that explicit type application (an upcoming extension to GHC) makes the need for `Proxy` go away, but we'll keep `Proxy` until we don't want to support 7.8/7.10 anymore *and* if we decide that we want to force people to use explicit type applications.
&gt; The only knowledge it presupposes is knowing what a list is ...which is actually a pretty common point of confusion, isn't it? "Lists are not arrays" is one of the most common responses I see to questions from new users about performance or unexpected behavior. There's no documentation anywhere stating `data [a] = a : [a] | []`. There can't be, because it's special syntax; mixfix like `[a]` isn't otherwise legal. IMO an introductory paragraph or two describing the structure of a list in `Data.List` (and/or `Prelude`?) is probably a good idea.
Pure Javascript is not statically typed, and most people who use Haskell enjoy Haskell's type system. Haskell-to-Javascript compilers let them author code in Haskell, catch a lot of bugs using the type system, and then generate Javascript code from that.
JavaScript is designed to be super easy to get started with and write very small programs. The inventor of the language said that 50 lines would be a really big JavaScript program. As a result, it was designed to "guess" what you wanted, and preferred doing something weird or wrong rather than throw an error. It doesn't really let you talk about what things are, and building structured data is tedious and error prone. Haskell, in contrast, was designed as a research language to push the boundaries of programming language design. The type system makes it really easy to talk about what things are, and the compiler is able to catch many errors at compile time. Those same errors in JavaScript would result in runtime errors, or worse, incorrect behavior. When building something of decent complexity, Haskell is much nicer and safer to use than JavaScript. Haskell is more difficult to learn at first, but once you know how it works, it's much easier to use. On the other hand, JavaScript is really easy to learn at first, but using it requires much more attention to detail and care to prevent errors, since the language isn't helping you much.
that's a great idea, but I think a tutorial that snapshots and explains design decisions is better than some curated or messy history. I guess my point is that the "teaching" and "documenting history" parts are too hard to automate, and must be done manually. 
A /u/ephrion says, you in principle can, but why would you want to? There are only disadvantages to that solution. You cannot see from the type signature that you're supposed to pass in `undefined`. And working with `undefined` never feels nice. It always leaves you exposed to the possibility that someone might accidentally end up forcing it and that your program will crash due to that. On the other hand, proxies are there to solve exactly this problem. Rather than abusing `undefined`, you pass in a well-defined term. And by looking at the type signature of the function, you can immediately see what is expected.
The single dot is composition of morphisms, as usual. The double dot appears to be [composition of natural transformations](https://en.wikipedia.org/wiki/Natural_transformation#Operations_with_natural_transformations), as usually notated by juxtaposition. Their second law is usually written μ ∘ Tμ = μ ∘ μT, familiar to us in Haskell as `join . fmap join = fmap join . join`. They may also be using flipped composition.
Because Haskell brings a few features to the table that are absolute killers if you know how to use them. The type system is famously expressive and sophisticated, and when Haskell programmers say "if it compiles, then it's bug free", they're actually being half serious - that's because the type system can capture so many subtleties that several classes of errors that are very common in JavaScript programs simply don't exist in Haskell. But there's more: full-blown functional programming features without any concessions, an extremely concise and powerful syntax, purity, controlled and checked effects (through the type system, again), pattern matching, lazy evaluation, etc. In practice this translates to extremely robust code, lots of compile-time guarantees, many of them to be had without writing any explicit tests, safe ruthless refactoring, and pretty damn good performance for a high-level language. On top of that, Haskell had a bit more thought put into its design, and so it doesn't share the quirks and inconsistencies that plague JavaScript's nuts and bolts - there's a full range of numeric types, operators behave consistently and predictably, there's no object weirdness to work around - anything that people complain about in JS is blissfully absent from Haskell.
uBlock Origin doesn't like that website. Apparently it's on http://www.malwaredomains.com/ I suggest using a better domain.
I really like that a blog post from 6 years ago is "recent" in this neighborhood.
Being Phooey the best that the FRP model can do, which is far better that many newer ones, yes there are things that can not do . First, because the composition of widgets is trough the monad. Not bad, but its rendering runs once, so it can not change. And for other, the signal results are meaningless out of sliders and counters. For example, anyone that change the rendering of the second widget depending of the result of the first widget can not be done: For example: city &lt;- boxtext "please tell me the city" if city == "NY" then message "hey I´m from NY we can meet" else do street &lt;- accessTheDatabaseGetStreetsAndchooseStreetInComboboxFor city ..... Another thing: a widget may receive different events. How I choose the kind of event? keypress? keyUp? click? when enter is pressed? How? What means receiving a signal in time whit the content of a city in your famous FRP model? I want to access databases, manage things like streets, combo boxes, keyboard and mouse events. These things are not your signals. I want to make a program for my grandma, not to write an academic paper. A string can be a signal? yes and a car can be a Bool. So much signal complications but at last All the examples use the last value of the last time. That is the one that matter in ALL the examples. Why not get rid of the signal concept then, which is inherently leaky and almost useless? Because multiplying signals like numbers may be useless but it is cool and because many people, from time ago, made a living from it. 
Sad as it is, this post does crystallise how I've been feeling about these languages for a while. While I _like_ Haskell more, I'm finding it hard to justify pushing it at the startup I work at, simply because there are other team members who'd have to learn it. Go essentially removes all concerns that you won't have the right team - it puts a hard upper bound on the difficulty of learning the language, or understanding any particular line. We have yet to see whether this will lower software quality (it's easy for anyone to come on board with the project... and quickly start making mistakes).
&gt; Github repository viewer You mean git history viewer? Like git log (or fancy graphical versions thereof)?
It still isn't common. lens a few others are an exception.
It almost seems like a shorthand for expressions of the former `Proxy :: Proxy X` would be as useful as explicit type application in some respects.
You likely need lots of Windows dependencies. Not sure what gcc requires but if they are C/C++ they might be available in a Visual Studio package. That being said you'll likely have an easier time building from Windows. So if you can, I'd go that route.
Wanting... or needing. Some of the cool GHCJS projects I've seen, and the hopefully-cool one I'm building, are providing tools for Haskell programming, so a different language isn't even on the table. It simply wouldn't fit the task at hand.
&gt; it seems uncharitable to include it in what you called a list of "mistakes and omissions". Hmm, I guess I should have been more careful with my wording! I was trying to list what I've learned and the sentences which led me to learn it, not list mistakes.
&gt; there are probably language features that would allow for more sophisticated rhyming features Well, I'm kind of inclined to thinking that's not particularly true. Either way the algorithms come out similarly. It's mostly about the algorithms you implement. Part of this is that you're already using Python, BTW. For example, Python already "borrowed" list comprehensions and generator comprehensions from Haskell a very long time ago, two features you might find very useful. If you said "I have this code in C, would Haskell be better?" my answer to your question would come out very differently. But Python isn't a bad choice for this task. &gt; Is Haskell often used for parsing or processing text? Ah, now there's a clear difference. I would consider Haskell much better at parsing text. In fact I'd call Haskell the clear leader of the field [1]. But... that's parsing in the "true" sense of "parsing", where you're building an abstract syntax tree from a non-trivial grammar, even if it's just the "order-of-operation-respecting calculator" example that features in the examples for almost every parser ever. If you're just loading a simple file up and the job can be done with a couple of regular expressions, it's not really "parsing". [1]: Maybe someday Perl 6 will take that crown when it comes to parsing text, but at the moment I'm not ready to move it out of the "joke" category of languages. It's still got some cooking to do even if it has gone "officially 1.0".
Ok, `Future HTTPResponse` is not edible, touche, but if we're in a category, that entails that for all objects in the category I have a function `identity : o -&gt; o`. Your `Burrito WeddingCake` may be tasty, but can you tell me how your `Burrito (WeddingCake -&gt; WeddingCake)` tastes? EDIT: Ok, I read up a bit just to make sure I wasn't completely misunderstanding everything, and after reading, I'm even more convinced that my original example is correct. Functor is a mapping *between categories*. To contend that I can't construct `Burrito HTTPResponse` because `HttpResponse` not in the category you want is silly. Functors don't have any control over the category that their children reside in. EDIT: I have been enlightened. `Functors` don't. functors do.
Nowhere do we say that the category is Cartesian (i.e. has exponentials). That is to say, even though the morphism exists, it doesn't necessarily exist as an object.
I have had reasonable success installing the haskell platform via wine and building using that, though I had to go into `winecfg` and muck about with the emulated windows versions to get anything to build. I think XP and 2000 worked well. The reason I didn't install minghc or stack is because anything involving running an autoconf `configure` script would cause `sh.exe` to blow up, and that was stopping me from installing a project dependency.
this is great! you could post some of the output for some of those packages in the README. as proof that it "really will work".
It is not cross-compilation, but here is the [Dockerfile](https://github.com/leksah/leksah/blob/master/Dockerfile) used to build the Win64 version of Leksah on Fedora using Wine. Build instructions are [here](https://github.com/leksah/leksah#building-leksah-for-windows-using-docker).
Remind me then. But not via Reddit :)
&gt; While I like Haskell more, I'm finding it hard to justify pushing it at the startup I work at, simply because there are other team members who'd have to learn it. Go essentially removes all concerns that you won't have the right team +1, though I'd rather try pushing for Rust instead (I've had no success in "selling" either Haskell or Rust, though).
&gt; it is truly nonsense to contend that you're able to eat the morphism WeddingCake ~&gt; WeddingCake. Luckily, this isn't contended by the paper that you still obviously haven't actually read. I won't waste any more of my time if you aren't even going to read the paper you are trying to criticize.
The worst things about JavaScript is that its errors can live unnoticed very long, since usually people don't put some top-level error-catching code which would report it through services like, say, sentry. There are also a lot of situations where you would get an undefined value instead of exception, and pass it through logic until it's hard to understand where it came from at all. If GHCJS is an option for you (due to its runtime) – benefit of using a better language (Haskell, of course) becomes even higher than on back-end, where all errors are under "strict observation" and fixed environment.
Ah sorry, no idea why it is, as it's just some web space on my cable provider... I'll take a look.
Can you point us at existing publications from your team? Even if they are a different topic that would be interesting.
Examples that somehow show how the function works. This implies that, even if you don't know the used terminology, you could still see what it's useful for. In elixir you have examples for even the most simple cases like comparing 2 values...
A functor is a mapping from objects and arrows in one category to objects and arrows respectively in another category such that if the objects and arrows are connected in the first you can sketch out the same connections in the target category. This concept (functor with a lowercase `f`) not to be confused with the `Functor` with an uppercase `F` that we have access to in Haskell is a very very general thing that implies almost none of the "common sense" reasoning you are trying to do here. `Functor` in Haskell carries baggage, it maps one particular category, informally known as Hask, back to Hask in one particular way by using a type constructor in an injective manner. There are _lots_ of functors that are not `Functor` instances in this sense failing either because the category has the gall to be something other than the "category of Haskell data types" or because it simply isn't a trivial injective mapping. Hask is a particularly "nice" category in which my objects are types and my arrows are functions. &gt; that entails that for all objects in the category I have a function identity : o -&gt; o. Indeed there is an identity *arrow* that takes every item of food and gives it right back unmolested. Calling it a function betrays that your intuition arises from the category that `Functor` with a capital `F` maps, not the general notion of a functor from category theory from which it derives. However. `identity` is an *arrow* in the category, not an object. In Haskell arrows themselves are both arrows and can be seen as objects in the category because we're in a "closed category", which means that we have "exponential" objects. That is to say that arrows themselves act like objects in our category. The category of Haskell data types is self-enriched, so exponentials feel like everything else in that one case, but this isn't a property that every category has! Nothing in the article requires FÜD to be self-enriched. `WeddingCake` may be rich, but that doesn't mean that there is an exponential object in FÜD with an eval morphism, and all the other things you need for FÜD to be a closed category. The article doesn't even argue that FÜD is monoidal, though one could construct a fairly simple tupling scheme based on table settings. Without FÜD being FÜD-enriched (having exponentials) arrows don't exist as (exponential) objects in the category. Hask is Hask-enriched. We can pass around exponentials as objects in the category so well we often lose sight of the distinction. &gt; Functor is a mapping between categories. Indeed it is. To borrow a turn of phrase, "to contend that a Burrito must be able to map things that aren't even in that category is silly."
Why don't you read the paper and tell me.
If by "paxos derived" you don't actually mean Raft derived then I will be very disappointed with you, Carter.
To some extent the `SignatureSections` extension (https://ghc.haskell.org/trac/ghc/wiki/SignatureSections), based on an idea from /u/augustss, provides this. The problem is that it, like proxies in general, works only if the author of the function expects a type argument to be necessary at a particular point. The advantage of explicit type application is that you can decide at the call site, rather than the definition site, whether a type argument should be supplied.
Well, it's certainly not meant as a toy/PoC implementation... The thing you linked to indeed doesn't look like a WAL *implementation*. It's the interface the FSM needs to access the WAL (well, read stuff from it, writing to it is handled differently). The idea is to leave the implementation of the WAL to whichever application uses the (model-only) FSM implementation. Does that make sense? Update: to elaborate on this, the 'Command a' type (https://github.com/NicolasT/kontiki/blob/c7093d933419ca035313febbe5efd6cc6a6bd76c/src/Network/Kontiki/Types.hs#L379) lists all 'output' side-effects of a state transition, where 'CLogEntries' is the side-effect of appending things to the WAL, which should then be performed by the application. In a next iteration, these freshly-appended entries should be observable through the log access interface you linked to before. Any management of the log (splitting in multiple files, compressing,...) is left to the application. Log pruning as part of the Raft protocol is currently not implemented. The reason Kontiki doesn't implement a WAL log itself is because the requirements for such log can vary per application, and some applications already have some other kind of WAL log. I used to work on a Multi-Paxos implementation where at one point in time we dabbled with merging the WAL and the key-value store (which was what the system exposed) in a single system (https://github.com/Incubaid/baardskeerder)
I am uncharitable towards a lot of things. If this weren't the Haskell forum, I'd probably be whining about how Haskellers make everything so fancy and complicated.
&gt; Is it because its easier to write good code in haskell then convert it? It certainly is. The oft-heard argument that "a good programmer can write good code in any language" is vacuous. First of all, it ignores the cost of doing so: the good programmer could write in machine code too, but then 90% of that good programmer's time would be spent on looking up instruction codes and manually hacking together function calls via jumps. More perniciously, that argument denies that there is human error. While it might soothe some people's ego to think that they won't make mistakes, the typical program is characterized by a plethora of bugs and errors. Now let's relate this general argument to JS. The differences between JS and Haskell are threefold: 1. What you can't do. Crucially, Haskell doesn't have the following three things: state, side-effects, null. The absence of this unholy trinity prevents a slew of bugs; if a function has type `Int -&gt; Int`, you can be sure that it'll only compute a number based on another number, not check the current date, fill up a 14GB log file, and return null at the end. 2. What you can do. Lazy evaluation (coupled with immutability) allows you to to write your own control structures and easily fuse functions together. Specifically, this means that you can implement map, fold, &amp; friends, and won't have to write 99% of loops anymore, which are the most tedious and error-prone construct in mainstream languages. You can emulate LE in strict languages with closures, but, in practice, this is seldom done. 3. Languages design. Haskell is a much more simple languages than JS. While people joke about the zygohistomorphic prepromorphisms, the language is very clean and consistent, unlike JS with its arcane scoping and casting rules that trip up even long-time users. In summary: Haskell allows you to express what you want, it stops you from being an idiot, and it doesn't have weird, surprising semantics.
I might be missing something, but why is env &lt;- ask needed in the eval (LetE x e1 e2) case? Edit: removing it and running the code still works fine, so I guess it's just a typo/leftover code?
I'd it's somewhere in the "middle". On one hand, Haskell gives you the tools that let you express your ideas more clearly directly in the source code (the "what" vs "how" part), lending to a better starting point. At the same time, I think that Haskell is much more forgiving if your initial model isn't quite right, due to the ability to abstract out almost everything, and the type system catching many mistakes you might make on the way.
I've been interested in Rust for embedded work, but I don't know if I'd bother for web apps. I think either OCaml or Nim would be ideal, but I have ecosystem worries about the latter, and the former is still a bit of a stretch for team members whose bread-and-butter is PHP.
I've been trying Reflex lately and, by itself, it can be the main reason to use Haskell in the front-end.
&gt; `filter odd :: [a] -&gt; [a]` Isn't that essentially what currying does? Other than that, I think both C++ and JavaScript fit the bill. In both you can pass functions around as arguments (either as templated arguments or `std::function` in C++), and return them from other functions, but neither language support currying out of the box. Of course, that doesn't stop you from writing a function that does it (example in JavaScript, C++ is more elaborate): function curry (fn) { var args = Array.prototype.slice.call (arguments, 1); return (function curried_fn (/* ... */) { return fn.apply (this, args.concat (Array.prototype.slice.call (arguments))); }); } (There is also `Function.prototype.bind` in ES5.1, and STL `bind` and argument placeholders that do it for you.)
John Skeet wrote a [nice article](http://codeblog.jonskeet.uk/2012/01/30/currying-vs-partial-function-application/) on this for C♯. The syntax is not nearly as beautiful, but it is practical enough for certain real-world uses. Edit: John Resig also wrote a [nice article](http://ejohn.org/blog/partial-functions-in-javascript/) to do this in JavaScript. I've never imagined a real-world scenario where this would be a good idea for JavaScript. Seems like you would have to put lots of `typeof` checks on your output, which could become scary and ugly.
I wasn't aware of that: I thought it was just a cunning use of operator precedence and evaluation order. I learned something today!
All programming languages support currying, but some require awful hacks to do so. The ease or difficulty of writing *apply* function *f* to an arbitrary *args-list* is the ease or difficulty or porting a basic QuickCheck library to a language.
"Defaults currying" or "syntactically privileges currying" is probably a better way of thinking about it. For another example, every language ultimately supports mutable values and immutable values; the question is, which is privileged by the syntax and the semantics? In some languages, immutability is so neglected that it is only implementable by convention and very careful programming, but technically nothing stops you from using immutable values. When something isn't privileged, even when it's possible, sometimes it's so messy it's practically useless. "Practically" in this case being not merely the generic English amplification adjective, but as in "in practice, in real engineering". Anything that can return first-class functions, which is nearly every language in common use today and probably every language in common use in the future (the feature has "won" and I don't expect that to change), can "curry", but without syntactic privilege, it almost always so messy as to be practically useless. And currying gets complicated in the face of keyword arguments, defaults, and other such things, which makes the generic "curry" function complicated, and then makes the result harder to use, too.
Thanks! That's great to hear!
If you're asking why `filter odd` doesn't have type `[a] -&gt; [a]` ... it actually *does* have that type (technically, `Integral a =&gt; [a] -&gt; [a]`) If you're not asking that, I'm not really sure what you're asking.
Lots of people use currying in JavaScript. [Ramda](http://ramdajs.com/docs) for instance is a library in which all functions are curried by default. The invocation syntax in JavaScript means that this is not a seamless as in Haskell but it still has practical benefit.
Scheme supports currying.
Why do you think it doesn't? If you partially apply a function, you get a new function back.
I'm not an expert at all. My thinking though is that you're asking about point-free function composition. Of course any language with functions supports function composition. Some made up calls in Java for example: List&lt;String&gt; foo = filter(odd, map(inc, xs)) Is that not function composition? Only point-free is lost without currying and partial function application, right?
&gt;If you partially apply a function, you get a new function back. If the function have type a -&gt; b -&gt; c but not when it has type (a,b) -&gt; c
Yes, I am talking about point-free. I don't think you even need currying so much if you don't use point-free style.
Emacs /ducks 
I think my vimrc looks like this: set tabstop=8 set softtabstop=0 set expandtab set shiftwidth=4 set smarttab and I indent myself.
It's not a bad answer. I'm tempted, but my keyboard requires me to move my thumb in awkward ways to reach the arrow keys which is giving me pain. I've even tried getting used to ctrl-p,n,f,b for cursor movement but it isn't for me. Installing Viper or equivalent just feels weird. 
There's [evil mode](http://emacswiki.org/emacs/Evil) that emulates vim in emacs. You get the best of both worlds. 
I think this question is better phrased as "is there a language without curried functions as default that still supports automatic partial application".
1) Because that is specifically the type of a function that's *not* in a position to be partially applied. If you *can* partially apply it, then the type signature is lying (and that's bad), because you did not in fact pass it a tuple when it clearly states that it takes a tuple. 2) Because if you can return a function (and I don't think literally any of us would like it if you *couldn't* return a function in Haskell) then it is trivial to write the `curry` function, giving you `curry filter :: (a -&gt; Bool) -&gt; [a] -&gt; [a]`. Since this form is just so much more useful, that's simply what's in the base libraries.
I'm happy with neovimhaskell/haskell-vim. I really like the syntax highlighting. The indent isn't perfect, but it's good enough for me.
Yes, but that's not what you're asking. Any language supporting lambdas and first-class functions supports function composition of single-argument functions, because even if the language doesn't provide `(.)` in its standard library, you can still define function composition yourself: compose :: (b -&gt; c, a -&gt; b) -&gt; (a -&gt; c) compose (f, g) = \x -&gt; f (g x) From the examples you give, however, I can see that you're instead asking something like "if a language doesn't encourage functions to be defined in curried style, can it still partially apply those functions?" Hmm, but if the language allows you to write a function `f :: (a, b) -&gt; c` and interprets `f(x)` as a function of type `b -&gt; c`, like in your example, in which way does `f` differ from a curried function of type `a -&gt; b -&gt; c`? It seems to me that the only difference is in the syntax, that such a language writes `(a, b) -&gt; c` to mean "a function accepting an `a` and returning a function `b -&gt; c`". Or perhaps the difference is in the runtime behavior, because calling `f(x,y)` would execute as a single function call while `f(x)(y)` would execute as two function calls? That makes intuitive sense, except this is a *model* of what happens a runtime, not necessarily what really happens in the presence of optimisations. For example, the compiler could replace `f(x)(y)` by the faster `f(x,y)`, and in fact given a function `g :: a -&gt; b -&gt; c`, GHC does produce a specialized versions `g2 :: (a, b) -&gt; c` and replaces `g x y` with `g2 (x, y)`. When thinking about the behavior of a language in which `f(x)` has type `b -&gt; c`, we can choose to imagine that a temporary function `\y -&gt; f(x, y)` is created implicitly by the compiler, and when thinking about Haskell, we can choose to think of `g x` as `\y -&gt; g2 x y`. But since applying a single value to `f` or to `g` produces a function, I find it simpler to think of `f` and `g` as being functions which return functions, and so I prefer the language whose syntax matches my mental model, i.e. if `f :: (a, b) -&gt; c` and `g :: a -&gt; b -&gt; c` mean the same thing anyway, I prefer the syntax of `g`.
&gt; It seems to me that the only difference is in the syntax I don't think it's only syntax. For example you don't need ```curry``` ```uncurry``` functions. When you use arrow's operators like ```&amp;&amp;&amp;``` you shift to the "tuple world" but all haskell function in the "curry world" so you need to use ```curry``` ```uncurry```. It's a little annoying.
I've stopped relying on Vim indentation to be perfect, and instead switched to using `hindent`. Basically, one of the above modes will produce syntactically valid code, and then I have a two-character keybinding (Space-r in my case) that reformats the current declaration with `hindent`. So as I write code I habitually spam Space-r to reformat whatever I'm working on. It also works well for me because when it starts looking ridiculous it's clear to me that it's time to refactor out some subexpressions (because `hindent` tries to respect the maximum line length). Certainly not the workflow that everyone desires but it works very well for me.
Well, that's not why they're usually total, but it's a fortunate side effect of them being total.
Wat.
&gt; I think if a language did not support currying, what that would mean is that it is not possible for a function to return another function most lisps don't support currying, but they all have first class functions.
For this reason, I'm considering a move to a different service. I hear medium.com is nice, but haven't checked if they omitted code snippets too.
Let's use a concrete example: baz :: ((a, b), c) -&gt; IO () baz ((x, y), z) = putStrLn "Hello, world!" ... now what if I apply `baz` to: baz ((1, 2), 3) Do I treat `((1, 2), 3)` as the first argument to `baz` (i.e. the `(a, b)`) or the entire argument to baz (i.e. the `((a, b), c)`? Keep in mind that the type parameter `a` could itself be tuple, so both choices would type-check: * `x = 1`, `y = 2`, `z = 3` * `x = (1, 2)`, `y= 3`
Because JavaScript is a terrible language (in almost all aspects) and is the only way for client-side scripting.
I have never needed to call the `uncurry` function. Why does it annoy you? Why are you ever using it? 
If they're not totally, you can write a function of type (a -&gt; a) -&gt; a, which let's you prove literally anything, making the type system unsound and removing any guarantees of safety the types give. EDIT: inconsistent, not unsound. And you don't lose all guarantees, but it lets you encode proofs of things that aren't true, which is something that is probably okay in Haskell, but very not okay in a theorem-prover.
This is an interesting idea. If `hindent` is unavailable, then manual indentation is available. Otherwise, automated indentation with a keypress. Thanks. 
```(+ 1) &amp;&amp;&amp; (* 10)``` the output is a tuple and you cannot use it as an input to the haskell functions, so you need to uncurry them ```(+ ) &amp;&amp;&amp; (* 10) &gt;&gt;&gt; uncurry mod```
&gt; It's so ungrateful. I am so hurt by all of this. I care for you guys. But you're not willing to bother with the mental effort to think deeply. And that means around the loser, self-defeating paradigms you're ego investing in. I don't take "care" from people who spit in my face and tell me my perspective is some elitist diversion, rather than a legit way of expressing computation that I, among many others, learned alongside OOP and procedural programming in school. I just happen to find myself preferring FP when expressing myself as a programmer. If you're going to criticize FP, bring out some of the actual criticisms. I don't consider myself very smart and I don't consider myself an expert in FP, but even I can point out some pretty clear flaws in the paradigm (and Haskell in particular) that I profess to love so much. For instance, you could have brought up the following with just Haskell alone: * Haskell doesn't have any killer apps or frameworks that provide a clearly justifiable reason to use the language in a professional setting, or at least get it through the door. If you're using Haskell in a business, it's because you *really* wanted to find an excuse to use it, not because you *had* to use it. At the end of the day, Haskell isn't providing any unique engineering solutions to problems that other languages aren't already providing in some way/shape/form. I find its selling point is that it's just a wiser choice for long-term projects from a maintenance perspective and that it does a lot of everyday things better than what I've used prior to Haskell. * Functional Programming (and Haskell by extension) is fundamentally at odds with the Von Neumann architecture. This is a problem for any engineer who's job is to draft a solution that takes advantage of the resources (hardware) that's actually available. Functional programming is often not the most efficient way to utilize todays hardware. If your job is to make a AAA 3D game engine for everyone else to build their game upon, then FP is almost certainly an unprofessional choice. This sucks. * On a related note, FP is kind of locked out of systems programming given we rely heavily on memory being managed automatically for us. * Embedded and Real-time systems are kind of out of the picture too. * Haskell deliberately abstracts over the runtime characteristics of a program, making it hard to reason about the performance simply by staring at the code itself - you pretty much *have* to benchmark and profile. This is a pretty big pitfall given the whole strength of Haskell *has* been that it lets us reason about our code better and is more "safe", yet none of that is true the moment you start talking about runtime behavior on actual hardware. In Ruby and Python, I used to curse because I had to constantly test. In Haskell, I curse because I have to constantly monitor my resource usage. Did Haskell really improve anything for me here? * You can't write the sexy syntax you see in toy examples when working on real-world projects. And often in order to write code that will give you the performance you seek, you have to leave behind a lot of the cool things about FP and write code with the conscious goal of triggering optimizations like stream fusion. This is unfortunate. FP is a leaky abstraction when it comes to performance. All beginners who start using Strings and Lists everywhere finds this out the hard way. * GHC sacrifices compilation times in order to give us type-level features as well as optimizations that can make Haskell competitive with other imperative garbage collected languages. * Haskell has fragmented over the years such that some libraries that everyone uses (containers, text, bytestring, vector, time, etc.) are not in base. Also, our tools and resources are fragmented (Hoogle vs. Hayoo, wiki.haskell.org vs. haskell.org, cabal vs. stack, hackage vs. stackage, git vs. darcs). And then there's the fact that our learning resources are unhelpfully distributed among countless blogs and books, many of which are going out of date. * Dependency hygiene in Haskell is hell. I never seem to be able to author a module using *just* what is in base. * Like C++, Haskell is a huge beast of a language. Everyone has their subset of language extensions and libraries that they understand and use, this basically turns Haskell into a personal DSL that's inaccessible to fresh eyes to the code base. * We're horribly unhelpful with names. Type Classes, Currying, Applicative, Monad, etc. We create confusion with these. Also, we have an annoying tendency to name things by just appending/substituting an 'h' to the front of a word. It's cute at first, but it wore off on me quick. I'm sure all the Clojure, Erlang, Scala, F#, Idris, etc. people could tear their language a new one in a heart beat if you asked them. There's issues with everything, and I just wish you'd focus on some of them instead of acting as if this 60ish year old field has actually figured out all the answers to how one should approach computing and we just got our head up our respective asses. I don't think anyone has those answers yet, in the same way an early field like psychology doesn't have any master models for approaching the mind, just a bag of limited perspectives. If there's any points above that you feel you *did* express and I'm just repeating, it's probably because I was focusing more on how you expressed those ideas rather than what you actually said. The fact is, you can't just walk into somebody's house and tell them their taste in interior design sucks and not expect them to get defensive and irrational. It's important not to be belligerent when starting up a discussion based on criticism, otherwise people are just going to return the favor. I know we're all supposed to be professional and purely rational here, but presentation matters at the end of the day.
Suppose, in pseudo-code, that I have the following in a source file: import SomeLibrary(f) Import OtherLibrary(g) f' = f "hello" :: IO () g' = g @"hello" :: IO () -- f' == g' `f'` is a regular-old function that is curried on a string value-literal. `g'` is a function that is curried on a string type-literal. It seems to me that `xmonad` from `Xmonad.Main` uses the `f'` approach for configuration, while `Servant` uses the `g'` approach for configuration. What are the trade-offs between the two approaches?
What an entertaining example :) This kind of reminds me of [this comment](https://www.reddit.com/r/haskell/comments/3r8x5m/best_way_to_model_a_deck_of_cards/cwml7nx) on the [best way to model a deck of cards](https://www.reddit.com/r/haskell/comments/3r8x5m/best_way_to_model_a_deck_of_cards/). Spoiler alert: it only takes 230 lines... :)
For someone who knows so much about programming, you seem to be struggling really hard with trivial tasks like copying preformatted text into an HTML website. Or writing correct English. Some nerve you have there.
I think all these problems could be solved, maybe with some restriction.
As someone who used vim for 6 years, switched to emacs for 3, then wanted vim but with the extensibility of emacs, but struggled to set up evil... Spacemacs was and is awesome. However hopefully they start pinning package versions soon.
&gt;We can't throw away business code and 3 decades of libraries to rewrite it all. Can't you do math? You don't have to throw it away *and* you can use Haskell on your new projects. &gt;Do you realize that life has side effects? Can you consider you're not in a vacuum. It's not just you? Haskell also had side effects. Ever use writeFile? 
unless you work all day at job and then all night at (permissively-licensed) os, you aren't passionate enough.
This was addressed in prior comments are articles. It makes it look like you exhibit the symptoms brought up in my comments and our blog. Looking for ways to nitpick, not seeing the forest for the trees, etc.
If there isn't any type restriction, full application has most priority. ```f (1, "foo") :: Int```
you mean elm? that is not composition. that is aggregation within something that can not be composed. I mean this: widget -&gt; widget -&gt; widget preferably with standard haskell operators: &lt;&gt; &lt;|&gt; &lt;*&gt; etc. or monadic if you wish (very important for dynamic interfaces) widget a -&gt; (a -&gt; widget b) -&gt; widget b where widget is a self contained element with rendering, event handling and logic, that return something to a computation. To summarize, a monad/applicative/alternative/monoid where widgets are first class.
Point taken... but there's a reason you don't see people proving theorems in Haskell the same way you do in a DT system. Still good for providing static guarantees of safety, but not for general theorem proving, which is really the main benefit that DTs provide.
Without totality there's little hope for substantial erasure. Consider a partial language with a spurious non-terminating "proof" that two types are equal. We can use it to coerce values from one type to the other, and we can only preserve type safety if we always run the "proof" runtime, so that its non-termination prevents the program from ever reaching the part with nonsense coercions. Since any proof might be spurious, they can never be erased from runtime. This is roughly the current state of affairs in Haskell with singletons. Haskell only erases `*` and the lifted kinds. Compare e. g. Idris, which can erase lots of superfluous runtime data. The termination of type checking is not quite as interesting a feature, in comparison. It's easy to write expressions in total languages that take billion years to evaluate, after all. The point here is that if the compiler or the program only needs to know that a value of a particular type *exists*, but don't need to actually compute with the value, then we can skip the billion-year evaluation altogether (in a total language).
It doesn't imply unsoundness, only logical inconsitency. Logical inconsitency: every type has a value. Type soundness: if `a` has type `t`, then `a` is either divergent or `a` is in fact a canonical value of `t`. For example, in a type sound language we never have a value with type `Bool` that is actually a runtime `Int` or something else. Haskell is inconsistent, but has realistic aspirations for type soundess (with usual caveats about `unsafeCoerce`, etc...). 
This is not the case. Pervasive "totality" is a design decision, not a requirement for a useful type theory or proof assistant; Nuprl is based on a partial type theory, and is of course both sound and consistent. When you use a partial type theory, you simply make totality part of the meaning of any type which you intend to be total; so, for instance, something is not an inhabitant of the Pi type unless it is a total function. However, you can base the whole apparatus on a Turing Complete computation system, as we do in Nuprl and JonPRL. The dual misconception is that something like Agda or Epigram is not Turing Complete; this is true in one sense, but false in a more important one. The reality is that you can write any program you want, but you will have to be honest when you try and execute it. Agda and Epigram are based on formal type theory, and therefore do not have a built-in computation system, so it is difficult to compare to something like Nuprl, but I think that the falsehoods of "You can't write general recursive programs in Agda" and "Partial type theory is inconsistent" are very much related.
Something like [hindent](https://github.com/chrisdone/hindent)?
Let's separate the question of a decision procedure for erasure (which is an implementation detail) from erasure itself. If you deal with erasure in the type structure (as is done in the Nuprl family of type theories), you can erase anything you like without committing to totality. It is not the case that somehow you have to run a proof to make sure it's not bottom—since if the application was well-typed, then we *know* that it wasn't bottom! You can compute lazily with no problem. Proofs of irrelevant stuff like equality, etc., are trivially erased in this kind of type theory—they never appear in the computational witness, and this falls directly out of the type structure. Of course, this isn't the case in a language like Haskell or Dependent Haskell. But it is manifestly the case in Nuprl, which is based on a partial type theory. The reason it works out is that type membership is a semantic property, and you have to prove that it holds: and you just can't prove that bottom inhabits a "total" type (i.e. a type whose inhabitants are generated from its values).
What happened to you? What makes you go on an angry rant like that? What are you trying to accomplish? People who frequent /r/haskell tend to be friendly, smart, and helpful, and I have never seen anyone here, or anywhere in the Haskell community really, try to force FP onto anyone. We've found a way of doing this tremendously difficult thing called programming in a way that is productive, satisfying, and theoretically sound; many of us use it to build breathtakingly complex systems making or saving impressive sums of money. If that way isn't yours, no harm done, do whatever works for you - but please don't waltz in here, waste our time, spew nonsensical insults and demand that we address your absurd Russellian teapot non-arguments. This is not cool, especially not in what I consider one of the most welcoming and open-minded programming communities in the world.
I got through the "grindy"-feeling part of learning Haskell by running through Project Euler problems (in order, since the difficulty level ramps up in a nice way for learning things). Haskell's (unsurprisingly) pretty well-suited to mathematical reasoning, and I noticed a big difference in productivity in more ambitious projects before and after going through the first page or so of them.
Haskell turns a few expectations on their head when it comes to what's hard and what's not. If you feel like you're still learning the *language*, I'd actually start with more mathematical problems, and /u/babblingbree's suggestion to try Project Euler is a great one. Once you feel like you have the idea of the language down strongly, then I think something like learning the Yesod ecosystem (which is a reasonably isolated subuniverse of Haskell) might be of some value. But it would depend on your goals, really.
I love the Haskell community -- even a blatant troll article receives a carefully constructed thousand-word response essay.
My impression is that Yesod is a good choice for web things, yes. Although it does use a few fancy type-system tricks, and quite a few type classes and type synonyms, which might make type errors difficult to understand if you're still learning. As a learner you might be better off with Scotty, which is much simpler (and also very well designed, in my opinion).
Awesome. Thanks. This line is busted for me in the tutorial: stack ghc -- -O2 -threaded example.hs Error: ghc: unrecognised flag: -02 Edit: Took out the flag and it worked, but not sure if it's necessary and wanted to let you know.
Thanks. I'll try this before Yesod.
Ah. O2 like oxygen. Ok.
Yeah - I tried all of those too. I've settled on Hacksyn because at least the highlighting is nice and it doesn't lag Vim too hard... but the indentation is awful. I currently just indent everything manually and it's not fun. 
Yes, every newbie following a tutorial. 
That is helpful. Thanks.
I personally disagree with the idea that you should do Project Euler problems in order to learn; it's what I did for a long time but I constantly felt like I couldn't do anything actually *useful* in Haskell. Sure, you might get great at thinking recursively, but you'll never learn how to write a shell tool, display some graphics on the screen, build a GUI, write a web server, etc. If it isn't already obvious, I think you'd be much better off trying to do one of the above things as early as possible. Which one you choose should just depend on your interests, but once you have chosen you might like to ask a more directed question (such as *Which web framework is easiest to use for beginners?*).
Well, my setup is pretty straighforward. I'm using: * http://ghcformacosx.github.io (7.10.2) * stack (for building projects/installing tooling) * spacemacs (for editing - there's a haskell layer) The only issue I had was exactly installing ghc-mod through stack. Back then, I followed [this thread](https://www.reddit.com/r/haskell/comments/3cf5yd/stack_ghcmod_work_in_progress/) and was able to get it done!
....dafuk?
As an alternative (or supplement) to Euler, I highly recommend [exercism.io](http://exercism.io/). Simpler problems for the most part -- more like [codewars](http://www.codewars.com/) katas (and they also have Haskell!) -- but the community interaction model is fantastic. You not only get feedback on your code, but you're encouraged to review and respond to other peoples' solutions, and iterate/refactor your own older solutions as you learn. **Edit:** Oh, re: web frameworks -- I just finished a project using [`servant`](https://github.com/haskell-servant/servant) which parses product info from a supplied target URL (via [`wreq`](http://www.serpentine.com/wreq/)). This is the first end-to-end web project I've ever done and though it's quite simple to begin with (one path with one query parameter), `servant` and `wreq`'s excellent tutorials made it just about effortless.
Did you run into [this problem](https://github.com/haskell/cabal/issues/2653)? I believe I did because I tried using `cabal run`. My project had already been set up with cabal. Maybe I'll try recreating my project with stack, and seeing if that works.
Side benefit: you can compare your solution to others' solutions in Haskell, C++, Java, Python, etc. It gives you a feel for Haskell's strengths, and how it influences your solution.
exercism.io looks neat, thanks for sharing.
I did not. But maybe you could try [this](https://github.com/ghcformacosx/ghc-dot-app/issues/41).
Thanks, yep, I'm chasing this down.
&gt;which let's you prove literally anything, True &gt; making the type system unsound False &gt; and removing any guarantees of safety the types give. False
Both. Builds on Cabal-the-library, replaces cabal-install-the-binary (for the most part).
For starting out, you'd want a simple web framework without to much magic. Scotty or if you're feeling more adventurous, Happstack (which I know had an excellent crash course which is slowly turning into a book). However, a very awesome very simple library you may want to look into is `gloss`. Gloss is direct2d made so easy that it's trivial. There are no monads, no Template Haskell, there isn't even parameterized types (except the return type which you don't have to even think about, just assign the whole thing to `main` and call it a day). The cornerstone of an interactive gloss project is the play function. It takes a window description, a background colour, a starting world state, a function to update that state when time steps forward, a function to update that state when there is user input, and a function to draw the world state.. and I think that's it I think. You have total flexibility over how you actually make them. The function to draw the world state is a function from whatever your world state is to the Picture type, which has something like fifteen builder functions (such terribly complicated things as Line, Bitmap, Scale, and Text) and that's basically it. And at that it's the most complicated of the functions you have to provide. It has an extremely low cognitive overhead.
We're just making some simplifying assumptions about the properties of tortillas :).
I was confused too but I surmised the same as you.
If you feel the need for some exrta adversity in your life, you should check out [RamdaJS](http://ramdajs.com). It's a pretty slick functional javascript library like lodash/underscore that lets you do point free composition and all that jazz. So you can basically code JS in the spirit of Haskell with all the same ideas, just in a very verbose fashion.
Won't be long before someone integrates this into emacs ;)
I don't remember having to install any special Hakell plugin, and I don't have anything special in my `~/.vim`, so I must be using whatever comes with vim. I found a file called `/usr/share/vim/vim73/syntax/haskell.vim` file which begins with those lines, does your vim installation have that as well? " Vim syntax file " Language: Haskell " Maintainer: Haskell Cafe mailinglist &lt;haskell-cafe@haskell.org&gt; " Last Change: 2008 Dec 15 " Original Author: John Williams &lt;jrw@pobox.com&gt; 
I think `zoom` might do the trick: https://hackage.haskell.org/package/lens-4.13/docs/Control-Lens-Zoom.html#v:zoom Paging /u/edwardkmett
c...can we not use lens? :( I mean, this function is 4 lines. all you need are two (ideally invariant) projections, not a sense of largeness or smallness
Usually when I use a combinator like this it is to run an action designed for a smaller state within a larger state, so the combinator you gave is just a little too weak for that situation. The `lens` version is further complicated by the fact that it supports swapping out the state even deep within a transformer stack, traversals that visit multiple parts of the state and let me run the action at each focus point, and glue together the results, etc. doing all of those things at the same time was rather tricky. You can of course roll all of this by hand, for special cases, like you've done here.
Consider the following: apply1 :: (a -&gt; b) -&gt; (a, x) -&gt; b apply1 f (a, _) = f a g :: (Int, Int) -&gt; Int g (x, y) = x + y wrap a = apply1 g (a, 0) Now `wrap (1, 2)` is `apply1 g ((1, 2), 0)`, which is `g (1, 2)`, which is `3`. And `wrap (1, '2')` would be `apply1 g ((1, '2'), 0)`, which does not typecheck. But you can also interpret it as `wrap 1 '2'`, which is `apply1 g (1, 0) '2'`, which can be interpreted as `apply1 g ((1, 0), '2')`, which is `g (1, 0)` and thus `1`. So `wrap` is now a function which, given a pair `(x, y)` of type `(Int, b)`, can magically decide whether `b ~ Int`. In this case, it evaluates to `x + y`, otherwise it evaluates to `x`.
Looks nice. How does this compare to http://hackage.haskell.org/package/JsonGrammar ?
There's also a [lens-family-core](https://hackage.haskell.org/package/lens-family-1.2.0/docs/Lens-Family2-State-Lazy.html) version, that can only zoom the outermost `StateT` layer, and has to be manually `hoist`ed if it's needed deeper in the transformer stack.
it's pretty useful if you are already using lens in your project :) it's nice because you get free transformers from any lens you might have lying around... everything fits together so well! 
&gt; First, a demonstration: we will compute the fibonacci numbers by starting with them and mapping them back to the empty list. Hahahaha, yeah, that was it.
That specific example is a weird thing to write. Why not: mod &lt;$&gt; (+ 1) &lt;*&gt; (* 10)
I might write a plugin for Neovim myself!
This is awesome, I'll create a PR documenting this for ghcid's README unless someone else beats me to it.
stack
Noob question: is the any hope for an (optional) totality checker for (some subset of) Dependent Haskell, or is it completely by `Type :: Type` and the like? 
https://github.com/rsdn/nemerle/wiki/Quick-guide#Anonymous_Functions_and_Partial_Applications This is nicer than the story in Haskell/F#/OCaml
- I'm using the default highlighting within vim installation. - For indentation I screwed up my own vimscript to automatically align / indent the next line based on the non-space column of the previous line. (the same for LaTeX)
I particularly enjoyed going through the Haskell katas at http://codewars.com. The best part is that it automatically scales up the difficulty based on your skill level from absolute beginners to advanced category theory stuff.
I'd like Haskell to be that way - but [it isn't true](https://storify.com/realtalktech/functional-programmers-react-to-criticism). While I found this response rare, extraordinary and detailed, it didn't address the points in the articles.
Yeah man, community-wide introspection is the only reasonable response to this incoherent article written by someone with no apparent academic or industrial bonafides.
So, one of my goto recommendation projects to do in Haskell is: Write a compiler. It's substantially simpler than in imperative languages, doesn't force you to rely on a complex ecosystem of a billion library and suffer through finding a set of dependencies that compiles. All you really need is a parser library (parsec, attoparsec, anything will do). Maybe LLVM if you want to generate actual code instead of interpret, but that's it. Some links: https://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours http://www.stephendiehl.com/llvm/ http://dev.stephendiehl.com/fun/ EDIT: Personally, I find it really easy to write concurrent networking stuff too. Just fire up one thread per client and write straight IO code to do whatever.
 brew install haskell-stack stack setup stack ghci I have no system GHC, to get around the cabal-helper issue follow the instructions on the box and add this to your global stack.yaml in the place it says to add it: Recommended action: try adding the following to your extra-deps in /Users/you/.stack/global/stack.yaml - cabal-helper-0.6.1.0 I have managed to get stack into a sad, bad place once where I got lots of linker errors about missing .so files, I sorted it out with `rm -rf ~/.stack/snapshots`.
&gt; I use atom. Start atom from the command line for best results. I got it to start properly from the Dock with a lot of trouble it's best to stick with starting atom from the command line. Whenever you specify tools, just give the absolute path. For Haskell tools: - `haskell-ghc-mod` should have your `stack` binary directory set in "Additional Path Directories", `/Users/you/.local/bin` by default. - It should also have the `ghc-mod` path explicitly include the path, such as `/Users/you/.local/bin/ghc-mod`, and don't use `ghc-modi`. - `ide-haskell` only needs the full path for `stylish-haskell`, the `cabal` path is unnecessary. The editor post you linked is a little out of date, current versions of `ghc-mod` support stack just fine (so long as you don't have a `dist/` directory in your project root).
I started with the list of [99 questions](https://wiki.haskell.org/99_questions) to get the very basics - syntax, library facilities, types. I did that before actually reading a book, only used the syntax reference on the wiki, but I did take up quite a few books in the meantime. Meanwhile, I started writing whatever tool I needed for my day job in Haskell. I figure, the more hands-on experience you get with something, the better and more informed opinion about it you're able to form. Then you can better judge whether it's the right tool for a job. Haskell's incredibly fun so far, that's for sure. Haven't had that thought for a language since I first learned Python.
Yep, there are some older indentation modes that are rather fragile, especially when it comes to syntactic extensions. Using `haskell-indentation-mode` from `haskell-mode` should "just work". `haskell-mode` is very actively developed, so if anything breaks it should be fixed quickly. In fact, as of the most recent version of `haskell-mode`, [`haskell-indentation-mode` is enabled by default](https://github.com/haskell/haskell-mode/wiki/Month-in-Haskell-Mode-October-2015).
That is an extraordinarily good criticism. I mostly agree. But let me explain my bottom line: Functional programming is just programming. programming is about algoritms. algorithms are about sequences of statements. And these things must be codified imperative/monadically. Always. Nothing can be done without a sequence of efectful statements. What is called "functional programming" is nothing but a new set of tricks and tools added to normal, traditional imperative programming, no matter if you like it or not. The first day when functional programming added something to imperative programming was when FORTRAN was created, because it managed mathematical equations. In the other side, no single useful FP program can be made without an imperative sequence. So that separation does not make sense. I'm happy telling that Haskell is the finest imperative language, because it is. The acceptation of this is what could make people less belligerent and less self-deceived in his approach to programming.
Perhaps I'm missing something. But what would be the advantage of a singletons-based Servant?
I just started building something. It never became very useful and there are some early mistakes in the design that I've lost the energy to overcome but I learned a tremendous amount from it. I kept iterating and refactoring - I probably wrote the same 4000 lines like 8 times. You can see the entire sordid history, starting from when I barely knew beans and was just playing with concurrency primitives in the [repo](https://github.com/jeremyjh/free-agent/graphs/contributors).
Towards simplicity and coherence trough complication. The monster that GHC Haskell is, now is going to grow with a little more breaking thing, to confuse more people, to trigger a chain of problems and complications trough all the tools and documentations for NO additional functionality whatsoever. Only to satisfy some zealots. Avoid success at all costs. ADDED: The above motto is outdated and was for the advancement of the language. It is not radical enough. The new motto should be something like "clean my favorite category theoretical playground from any bit of reality"
Spock is similar to Scotty in being not-the-full-monty (as Yesod is). It doees a little more out of the box though (auth'ing comes to mind). Yesod also has some mini-app scaffold; which is a nice place to start as well.
I'd tweak it a bit to be non-symmetric, so that the outer state wouldn't have to be completely recoverable from the inner state: withState :: Functor m =&gt; (s -&gt; t) -&gt; (t -&gt; s -&gt; s) -&gt; StateT t m a -&gt; StateT s m a withState to from x = StateT $ \s -&gt; second (`from` s) &lt;$&gt; runStateT x (to s) Equivalently, using `do`-notation: withState :: Monad m =&gt; (s -&gt; t) -&gt; (t -&gt; s -&gt; s) -&gt; StateT t m a -&gt; StateT s m a withState to from x = do t &lt;- gets to (a, t) &lt;- lift (runStateT x t) modify (from t) return a
I'm using neovimhaskell/haskell-vim. Syntax highlighting is very good, and it doesn't force an indentation style on you. By that I mean that I find most of the defaults reasonable and you can deviate from them when you choose so.
I just use the ghc bin dist (or my personal build). Has to be 7.10.2 mind you. Then I wget the cabal-install release from hackage. Then I open the tar ball and type ./bootstrap to build / install that. Add ~/.cabal/bin to your path and BAM. time to do some computer science ! Then I have an El Capitan friendly cabal binary and ghc and all is good. No crazy tooling needed and it's easy to share!
Can you explain how type checking guaranteeing values aren't bottom is different from totality?
Probably, but it depends. What is the project?
Adam, do you happen to know if type application will also be different from an optimization standpoint? We have a lot of functions that need to use term-level information (mostly scaling factors) derived from the type at which the function is invoked and accessed through `Proxy Something -&gt; KindStarVersionOfSomething` functions. AIUI GHC can't see (and there's no way to prod it) that when `Something` is monomorphic for a given inlined call it would be nice to evaluate that bit at compilation time. This can lead to doing multiplications by `1` that are pointless, which is sometimes expensive because there's no chance to notice that you are fmap'ing `id`.
You are not making any sense...
Useful first projects include some kind of parser, creating useful data types to allow manipulation and/or some sort of verification of the data, then writing that data back to disk. This is my go-to practice scenario for any new language. It gets you comfortable with important aspects of IO, sorting, types, and file output.
Alright, thanks to this post I got a lot further. I also found some vim user docs in the spacemacs repo that has cleared a lot up. I will experiment with this. Thanks!
I think you meant String with Text.
&gt; Yes, type-level computation typically occurs at compile time. This is why dependently-typed languages are usually total since if they weren't you could make the compiler hang due to non-termination in the type-checking process. What is 'dependently-typed language'? What is 'total'? Also, what is the likelihood of someone who has a question what type-level programing is already knowing what the above terms mean? People often wonder why Haskell has a bad rep for approachability; your answer goes a long way in showing why at least some of it is well-deserved.
I use [structured-haskell-mode](https://github.com/chrisdone/structured-haskell-mode) which has skeletons which I get a lot of mileage out of. Example [here.](https://camo.githubusercontent.com/116beffff218dd0f6dd502caf51fa36d661942af/687474703a2f2f6368726973646f6e652e636f6d2f737472756374757265642d6861736b656c6c2d6d6f64652f676966732d6e6f63616368652f736b656c65746f6e732e676966) In essence it's just writing `do SPC` or `if SPC` or `case SPC` and it automatically filling out something useful with `undefined` holes. To get `if x then y else z` you write `if SPC x TAB y TAB z`. The proper indentation is automatic. I don't use a tab cycle of any kind. The current node [makes it unambiguous where to indent next](https://camo.githubusercontent.com/fe552846904ae431c07f180b626d35191e5a0b0a/687474703a2f2f6368726973646f6e652e636f6d2f737472756374757265642d6861736b656c6c2d6d6f64652f676966732d6e6f63616368652f6e65776c696e652d696e64656e742e676966)
To be fair, at some point the discussion stopped being about MRP, and MRP became rather the scapegoat proxy for a larger still unresolved issue/conflict at hand that has been smouldering already since the *real* ["Burning Bridges Proposal"](https://mail.haskell.org/pipermail/libraries/2013-May/020046.html) was published back in 2013.
Oh I'm sorry I caused some confusion here, my question mainly concerned the indentation of `then` and `else` in relation to `if`. But from what I understand your saying that example 1 would be fine if the `else` was on the same level as `then` ?
Wow this looks really good, is it compatible with haskell-mode out of the box? Or is it more like a replacement. Edit: Nevermind, this is explained on the Github Page.
In the kind of system I'm talking about, there is no type checking, there is type "proving". That's because there is no decision procedure for *semantic* membership or equality in a type. So, when you get around to proving that a program `M` has type `Nat` (for natural number), you will be proving that if you run `M`, you either get `Zero` or `Succ(N)` such that `N` is a program of type `Nat`. So, your proof excludes the possibility `M` is bottom. However, in this framework, there is nothing to prevent having a type which is in fact partial. In fact, Nuprl has a type constructor that does precisely this; `Partial(A)` contains the inhabitants of `A` as well as bottom, for instance. This is, to be clear, different from `Maybe(A)`, since whereas you can always find out whether something is `Just` or `Nothing`, it's only *semidecidable* whether something is bottom (that is, you can run it and wait, but you may be waiting forever!). **So, to actually answer your question**, whether the type system guarantees that a program is not bottom is decided on a type-by-type basis. So many types will of course be total ("value types")! For instance, `Empty` will have to be such a type. But we don't have to commit to pervasive totality; we can have partial types where they are useful. There is also nothing preventing us from using general recursion in order to *implement* a total program; and, unlike in Agda and Epigram, where general recursion is also perfectly possible, here we do not need to encode the graph of a program as a datatype. In Nuprl, you can work directly with the computation system and reflect upon its operational semantics in order to prove things about untyped code. But the techniques are analogous.
Just to note, I upgraded my work yosemite laptop to el capitan and everything kept working in nix. Of course everything recompiled after upgrade but until it was done it still worked well enough to use emacs etc...
I can second the exercism.io recommendation
Yes, hopefully it will be useful by itself, that's the motivation for getting it in first. It doesn't let you do fancy polymorphic things, but it does solve the basic problem of not being able to use the same field name twice. I don't know much about GHC.Generics, but a bit of quick hacking suggests that DRF doesn't play nicely. But there's still time before GHC 8.0, so thanks for pointing this out!
Do you have an example? I tried this and it works just fine for me. I'm using GHC 7.10.2. main :: IO () main = do t &lt;- if True then return True else return False if t then print True else print False 
You're welcome! The scenario I had in mind was generating aeson's FromJSON and ToJSON instances for records using Generics, in presence of duplicate record fields. Ideally the field names in the generated JSON should be the original ones, not the "mangled" versions used internally.
Great work! Looks like this might become a standard Haskell tool :-) Some people debate the actual usefulness of cyclomatic complexity, are you planning any additional metrics? They should be easy to add with most heavy lifting already done.
You're right that it works. Works in 7.8.4 as well. I do remember thinking that indentation is necessary and I found this in the wikibooks. https://en.wikibooks.org/wiki/Haskell/Indentation#if_within_do
Where did racism, sexism come from? How do you expect a subreddit for a niche topic not to have prejudice. How do you know that I'm not a HFA that got red pilled? &gt; Even when you do make a point, it's thinly veiled insult. You can dish out a lot of insults out of nowhere. There's nothing wrong with affording contradictory viewpoints in your worldview and life philosophy. It's considered a paradigm for a reason. https://en.wikipedia.org/wiki/Integrative_complexity This is something you need to increase. You have to understand how there are many external variables, factors and on going side effects which play a role in the imperative gift that is life.
I think you should use Text for haskell-ide-engine. Performance quickly becomes a bugbear for ide tools.
As a lens-newbie, I am not sure I would be confident in the zoom approach. Although it's brilliant, I'm just not sure I would find myself drinking the lens kool-aid _just_ to have this combinator - it would add many more semantic guarantees than I need, and add lens' typeclasses to my project, which I'm unsure if they would change or if I would be able to understand them at a later time, not being a lens connoisseur. What are the chances we could have this utility function added to mtl?
Yes. I use nix exclusively. Do you remember what sort of problems you ran into?
indeed. I'd keep using lens, I don't know how useful the proposal is to my current development. but, I already put spaces between ALL operators. in a silly hope for Agda-style non-alphanumeric identifiers and mixfix alphanumeric operators. -- proofs, as would be written on paper f.[g.h]=[f.g].h :: f -&gt; g -&gt; h -&gt; (f . g) . h = f . (g . h) f.[g.h]=[f.g].h f g h = ... or: -- Python style _if_else_ :: a -&gt; Bool -&gt; a -&gt; a _if_else_ x True y = x _if_else_ x False y = y check x = 0 if (x&lt;0) else x -- SQL SELECT_FROM_ SELECT_FROM_WHERE_ etc. I can't stop hoping. it's OCD.
My vim auto-highlights literals, too. Syntastic is the only other thing I use.
In Haskell98 you have to indent `then` and `else` farther than `if` inside `do` blocks. However, [there is an extension called `DoAndIfThenElse`](https://prime.haskell.org/wiki/DoAndIfThenElse) which is default enabled in Haskell2010 and [in GHC for a while](https://downloads.haskell.org/~ghc/7.0-latest/docs/html/users_guide/release-7-0-1.html) which eliminates this requirement. One caveat is that `DoAndIfThenElse` is off by default in Cabal, since Cabal uses Haskell98 by default. To make `Haskell2010` the default in Cabal you can add `default-language: Haskell2010` to your targets in your `&lt;package&gt;.cabal` files. More info on `default-langauge:` [here](https://www.vex.net/~trebla/haskell/cabal-cabal.xhtml#haskell2010).
Basically, you are telling people that out there in the real word, programming language must be imperative, fair enough. What they are answering you is : out there in the real world (in this subreddit at least) there are rules which are, do not insult people if you are expecting a constructive answer. Now, someone has to do the first step and try to understand the other side world. However, you are the one which apparently is looking for answer, so do the first step. Rewrite your blog without insulting everybody and I'm sure people will be happy to answer your points. Also, you claim &gt;functional programmers failed to defend themselves - all they have done as of yet is criticize spelling and toss aside things as "unintelligent", "intelligence of a brick". Maybe but, can you prove it's only FP programmers ? Rewrite your post and replace functional programming by any mainstream languages and post to the corresponding subredit and see if imperative programmer react better to all those insults ? 
I have to slightly disagree with that. You might need a way to get data in and out but that doesn't mean that the language itself needs to deal (and be aware) of side-effect. Do you consider a calculator or an excel spreadsheet having side-effect ? My first programs in haskell were not using IO or even monad. I still manage to write some algorithms (e.g sort) without any sequences of statements, just expressions. I used then to load the file in ghci and evaluate what I needed from it. Yes ghci had side effect, but outside haskell itself. I could have taken a pen and paper and evaluate the expression by hand, would you call that side-effect ? Of course, when I say side-effect, I'm talking about referencial transparency AND things are relevant to the user and can be modelized in a monadic way. I'm not talking about the bloody heat generated by the CPU .
That's not a problem because the `if` statement is the expression being bound to `t`. It matters when the `if` is at the top level of the `do` block as in: main :: IO () main = do if True then putStrLn "True" else putStrLn "False"
# THE TYPES MUST FLOW
A few weeks ago I tried to optimize a heavily text oriented program and I got a bit of experience that might be worth sharing. * Don't forget to enable the `OverloadedStrings` in GHC, since Text is an instance of `IsString`, or otherwise you will have a hard time using Texts. * Generally, I prefer `Text` over `String`, but there are certain cases that `String` works better. `Text` vs. `String` is just like `Array` vs. `LinkedList`. Are arrays better than linked lists? For most use cases yes, because they take less memory and have a faster access time. But for certain operations, like `cons`, Strings are faster, because you don't need to copy the second one to the new memory. * Try to understand why [`Text.length`](https://hackage.haskell.org/package/text-1.2.1.3/docs/Data-Text.html) takes `O(n)` to run (while length of arrays can be obtained in constant time) and it helps you a lot. That's because `Text` internally keeps an array of bytes with UTF-8 encoding, in which some of the chars are 1 byte and some others are multi-byte. It has to go over all the chars to detect which one is which. (**Edit:** It seems that `Text` encoding is UTF-16. See the comments below, but anyways, it doesn't change my point.) * `Text.concat [t1, t2, ..., tn]` is much faster than `t1 &lt;&gt; t2 &lt;&gt; ... &lt;&gt; tn` (`O(n)` vs. `O(n^2)`). Use Lazy Text and Text Builder if you want to do a lot of appends. You can use `&lt;&gt;` and `IsString` instance with both of them as well. * **Edit 2:** I forgot to mention this one: `String` does not check for validity of code points. You may end up with a string that contains invalid UTF code points. But `Text` is stricter about that. It may not be so important in most applications. But you may see some weird behaviour when comparing Strings with invalid chars: they may looks the same but `==` returns `False`.
My example code tested both the case of an `if` expression bound to a variable and a top-level `if` expression.
Haskell's List and String type aren't linked lists but stacks. Edit: I definitely made a big mistake. Thanks for all being constructive and patient for my ignorance.
Thanks! Yes, cyclomatic complexity in Haskell is not as useful as in imperative languages. For the next release, I was planning to implement kamatsu's suggestion about counting the number of variables ([here](https://www.reddit.com/r/haskell/comments/3qlgwu/ann_argon_0310_replaced_haskellsrcexts_with_ghc/cwi0at2)). It does not seem very simple, so before tackling this problem I was finishing other things. Right now I'm integrating pipes-files so that I add the filesystem traversal to the pipeline. I'm also increasing test coverage. Do you have specific metrics in mind? It shouldn't be too difficult to add them, but not too easy either.
There are some things about TH that are just plain amazing, like Yesod's templating, even though the types can be rather weird.
Thanks for all the good comments. I think I need to clarify my question slightly. Should I use Text to the point where String is effectively deprecated? Is this a way of future-proofing my code against coming changes? This is not so much about efficiency or optimisation, more a question around if I want to do a thing with "something in quotes" in it, what should my "without thinking" choice be? Carefully trying to phrase that without the loaded words 'string' or 'text'. 
After having implemented part 2&amp;3 (and soon 1) of ORF and having played with it a little I must say that the type error messages get very bad. The #foo construct has a too general type. But perhaps it can be engineered to look better. Time will tell.
`String` is also human readable Unicode text, just stored in a less efficient way that is sometimes more convenient to use in pattern matching and similar situations.
Only for convenience.
Truth be told, if we ever get quantum computing off the ground, imperative probably won't stick around for long because the destruction of information inherent to it *doesn't exist* at the quantum level. The universe is not only more functional in nature, but the computations have to be *reversable*.
That they write "foo.bar" instead of "foo . bar".
Maybe we could use @? But then we'd overload that as well since it is used in as patterns
Not any more, it was moved to a separate "Dot as Postfix Function Apply" proposal, but that's what the original email was about.
I'm not a Unicode expert, but I think the issue with using a list-like type such as `String` to represent Unicode text is that it promotes a few common mistakes. For example, a programmer might naively think that if they want to uppercase a `String` they just need to uppercase each character (i.e. `map toUpper`), but that's not correct: upper-casing the string might change the length of the string. Now, you can probably provide an `uppercase` function that works on entire `String`s at a time instead of working character-wise, but it won't be immediately apparent to the user that they even need to seek out such a function. They will much more likely do the obvious but wrong thing because they won't know any better.
Well, technically if you are talking about operations on the types I am not sure if `Text` is fully Unicode-compliant there either, e.g. with the requirement to treat different representations of certain strings as equal. Does the `Eq Text` instance do that?
I'm not advocating that you should incur a dependency on `lens` for 4 lines of code, just addressing the "why doesn't this exist" part of the topic with a location in which something that subsumes it does exist. If it were to be added it'd go in `transformers`, not the `mtl`. It uses none of the classes, just the StateT data type, which lives own in `transformers`. I'd personally argue against _this_ combinator, favoring one where the part you look at doesn't have to be isomorphic to the whole, but Ross maintains `transformers`, not me. withState :: (s1 -&gt; s2) -&gt; (s1 -&gt; s2 -&gt; s1) -&gt; StateT s2 m a -&gt; StateT s1 m a subsumes the usecase you have for withState' :: (s1 -&gt; s2) -&gt; (s2 -&gt; s1) -&gt; StateT s2 m a -&gt; StateT s1 m a but also admits many more interesting scenarios.
I never said we ARE going to adopt the Monad(return) proposal. I said that IF WE DID, it would have to be done carefully. A careful migration plan is a necessary but not sufficient condition for its adoption. I personally do not see any plan in which Monad(return) happens in the next few years. How is acknowledging that it is possible that it might be worth doing, or worth shaping the report in such a way that it could allow for the eventual removal of Monad's vestigial tail someday zealotry?
microlens-mtl also has minimal dependencies.
Yeah, I verified that it doesn't: $ ghci &gt; import Data.Text &gt; :set -XOverloadedStrings &gt; let text1 = "\x006E\x0303" :: Text &gt; let text2 = "\x00F1" :: Text &gt; text1 == text2 False
Yeah, the onsite requirement does not play well with scarce haskell talent out there. Trust me, we tried to find local haskell dev in Los Angeles area for several years until management finally agreed to remote hire. 
Don't just blindly replace `++` with `&lt;&gt;` unless you want serious slowness.
what horror has god brought upon us
In Mu. I've found other problems with ORF as well. I'll try to write an email or blog post. 
because of polymorphism? or rewrite rules not firing? 
The reporter misheard the name of Prof Curry's organization, which was the State College Bird-Wadler Club. This of course was a reading group of computer scientists and graduate students at Penn State. The three-foot pile of papers was the accumulated output of Drs Richard Bird and Philip Wadler, evidently printed doubled-sided on tissue. The group met in the park to emphasize the fact that computer science has nothing at all to do with computers, the membership's incidence of astenopic myopia notwithstanding (the preponderance of corrective lenses of unusual thickness, understandably mistaken for binoculars of another sort, perhaps further mislead our intrepid reporter). Yes, exclamations resounded of the rarity and beauty of Bird's idiosyncrasies; the predilection for tweed among the Bird-Wadlers was merely coincidental. An altogether understandable error, given the deplorable state of journalism, then, now, and generally.
cool thanks!
Yes, it does!
I think mainly extensibility. The set of Haskell types is open and the set of type class instances is also open, so in some cases programming at the type level (in Haskell specifically) permits easier extension. The disadvantage is worse type errors, more extensions, and greater difficulty equationally reasoning about how it works. I'm actually not totally convinced that `servant`'s APIs needed to be specified at the type level, but I believe that was the stated rationale for doing things at the type level instead of the term level.
We would absolutely accept an experienced engineer with little Haskell experience as well - provided that they could demonstrate a strong interest in the language and were willing to learn. Nothing that we're doing in this first project could be considered particularly difficult - we are parsing files and persisting what we find to a database. 
[Here!](http://i.imgur.com/QgnDgAN.png)
Throwaway account, for obvious reasons: OP posted "Conversant is hiring", but has a CJ (Commission Junction) email address. This is important, because Conversant engineering outside of CJ is a clusterfuck (I run several engineering teams there, so I know) but CJ is almost a completely separate business within Conversant. So if you're looking at all the negativity on Glassdoor be aware that it's almost certainly referring to non-CJ Conversant. So: don't be put off by things Google might turn up! From all I've heard CJ is a well-lead, forward-looking business, with a strong engineering culture.
 extractFromJust = Safe.fromJustNote "Invalid Operator" Just an FYI. I can see why one wouldn't pull in a new library for a single function. 
Some things to keep in mind when using Text: * Be careful with `Data.Text.Encoding.decodeUtf8` on user-provided byte strings, as it throws exceptions on invalid user data. There's a lenient way to call it, but it's not the default. * From reasons already mentioned, taking an indexed sub-string via `Data.Text.drop x $ Data.Text.take y` is quite inefficient, even when the input string is large, and when y is small.
Never knew that, thanks.
Wat... I thought you supported MRP? What made you change your mind so suddenly? Why are u giving in to the dictatorship of a rude change-is-bad-handwavin foot-voting minority?! Don't you see the irony? ಠ_ಠ
This is what I do too, it's very easy to get up and running if you're already using `brew` You can also add `~/.stack/programs/x86_64-osx/ghc-7.10.2/bin` to your path if you want to use the installed ghc directly. (`stack path` comes in useful here). I've also added ``` local-bin-path: &lt;HOME&gt;/.stack/bin ``` to my `~/.stack/config.yaml`, so that `stack install` installs there rather than `~/.local/bin`. You can install some useful tools with: ``` stack install hasktags stylish-haskell hlint hoogle hindent ``` Finally, if you want to install `ghc-mod` with `stack`: ``` stack install ghc-mod --resolver nightly ``` 
`fromMaybe (error "Invalid operator")` also works, but speaking sensibly a pure library that implicitly throws exceptions is essentially satanic.
It's because frameworks and infrastructure for logging, monitoring, etc. are already in place, so we want to hook into those rather than adopt Haskell-specific solutions.
Is there anything worst listening in this video ? I 've given up at the monad/semicolon bit. It sounded like if you know about Haskell you don't learn anything and if you don't know Haskell you don't understand anything either ....
Out of curiosity, was it interoperability features that made you choose PureScript over Haste, and if so, which features?
I've just read JPMorgan is searching for Haskell programmers—I don't know if JPMorgan is a NASDAQ company, but anyway… I think you should embrace the fact that all the functional programmers are not purely functional programmers, even when they are not aware of this. In Haskell monads exist for a reason. (Odd name apart, think of a monad as something that makes possible to do stuffs with side-effects.)
btw, everything you complain about could be said about the AMP as well
1. Yes, Scala is not purely functional 2. Purely functional languages (i.e. Haskell, not Scala) can have side effects, without defeating the purpose of being purely functional The goal of purely functional programming is not to eliminate side effects, but rather to model them in such a way that they don't interfere with [equational reasoning](http://www.haskellforall.com/2013/12/equational-reasoning.html). The thing that differentiates Haskell is that side effects are decoupled from evaluation order. This means that your program's behavior is (with some caveats about bottom and unsafe functions) insensitive to evaluation order. This in turn implies that you can treat `IO` as an ordinary, inert, data type and a Haskell program as just a pure computation that [builds up a larger and larger `IO` value](http://chris-taylor.github.io/blog/2013/02/09/io-is-not-a-side-effect/).
&gt; We can write sum . filter even . map (^2) and GHC is capable of turning that into a single tight loop. And also that is not IMHO qualitatively different from the way FORTRAN or any other imperative languages convert a numerical expression into machine code. Maybe also creating some loops to calculate numerical series or managing interruptions to interface with the math co-processor. It is the same business of achieving more and more abstraction to program at a higher level. 
And the String interface is way more powerful than a stack interface, albeit at a performance cost.
I work in MonoDevelop in C# every day, and I want to claw my eyes out at how horrible the experience is. I run screaming back to Vim every night, and then feel fantastic for the duration of my time there.
Nothing. They're just saying `&lt;&gt;` is slow compared to using e.g. a Builder if you're mappending a lot of things.
Curious, why onsite? What is it about remote work that is bad? ... I honestly don't know, I have an onsite job, but work from home once or twice a week and can see with the right culture that remote work is completely possible and sometimes I prefer it because no one runs up to my desk every 5 minutes. Just curious why the preference .. I'd apply but I'm not moving as my wife wouldn't let me. ;) Edit: for those wondering the onsite seems to be firm. At least that is what they told me.
A better aproach, already in a library: safeFromJust :: Maybe a -&gt; Maybe a http://hackage.haskell.org/package/acme-safe-0.1.0.0/docs/Acme-Safe.html ;)
yes, that's right.
Why would it waste memory like that? UTF-8 is better pretty much [always](http://utf8everywhere.org/).
what is this I can't even
All good and whatever. Still, what I've said is that you can't do anything “real” with a purely functional language and in fact every functional language must think about something (no matter how you call it in your preferred functional language or how you can describe that abstraction) to escape the purity — since this is a dirty impure world and no matter the cool abstractions you use, their caveats, advantages and disadvantages: beneath there's an impure hardware which many find easier to tune and tame fully with other kind of abstractions and programming paradigms. Which may look less cool to functional guys, but are proved to be very *effective*. I think the path from imperative languages to imperative languages borrowing functional ideas is a winner; while “perfectly” designed and engineered purely functional languages, that are forced to recover whatever their purity had taken away by the mean of other abstractions hiding the truth under the bed and making hard to control the actual “states” of the hardware, will stay in those niches where such things as “the real world current hardware” do not matter so much. Things may be different in a future where common hardware will change a lot. 
There's `decodeUtf8'` which returns `Either UnicodeException Text` which sounds like a better option.
I think you're still missing the point. I'm trying to say that Haskell is **better** at handling side effects than imperative languages because Haskell treats side effects as ordinary values. That means that I can do things stick side effects in data structures and access them in whatever order I want and it will "just work". Here's an example: main = do let effects = [print 1, do x &lt;- getLine; putStrLn x, fail "Urk!"] effects !! 1 effects !! 0 The above program will run the second side effect in the list (read a line from the user, then print it) and then run the first side effect in the list (print a 1). It won't trigger the third side effect (throwing an exception), even if we were to strictly evaluate the entire list. You can prove this to yourself by strictly evaluating the entire list ahead of time.
Don't ask me.
[reifyInstances is probably what you want](https://hackage.haskell.org/package/template-haskell-2.10.0.0/docs/Language-Haskell-TH.html#v:reifyInstances) &lt;3 TH
Your words start proving somebody else point… Are you talking about **syntax** then?! If C had first-class functions… Should I give you the example in C++11? No, you'd say a lambda *wraps* the code since your language has a different syntax and allows you to put crude expressions or sequences here and there, without a **syntactically evident container**. You are still trying to prove (or worse, to explain!) your point through a inappropriate example. And you seem unable to get it. Aren't you *controlling carefully* evaluation order by those `effects !! 1` and `effects !! 0`? Or you are just pretending that it's not the case since you put your “side-effects” nude (not wrapped in anything, **according to you**) in the list/array and **you**, as Haskell programmer, expected them to be evaluated… and since it doesn't happen (except when you decide to **call** —it's the same— them with `effects !! N`), then you claim you explained the point and proved Haskell handles better the “side-effects”?!
I have no idea if Rust is "lower-level" than Go, but I admit I was going to use it for writing plugins for LabView - until I've realized that compiling Rust to a standalone (as in: load and forget, without any initialization that might be possible in LabView, but I don't know how, as I'm rather new to it) `.dll` isn't as straightforward as I thought, and I've decided it's better to give up and accept C rather than take the responsibility of picking a language that led to failure of the whole project...
That's still not safe enough, it returns `undefined` if you give it `undefined`. I prefer: safeFromJust :: Maybe a -&gt; Maybe a safeFromJust x = Nothing Guaranteed not to give bottom for _all_ inputs
Terrible Purporse?
My instinct agree with you, however I really like the idea of having an operator with a fixity superior to function application : that will remove lots of parentheses and make some code readable. It took me ages to train my eyes to parse `a.b c.d` as `a . (b c) . d` and once your eyes are train you just write `a . b c .b`. Also, it could make record/module like easier. I mean you can modelize an (ML?) module as a record of functions like data Logger = Logger { debug, warning, error :: Show a -&gt; IO ()} and then do things like import SimpleLogger (logger) logger.debug a+1 which look like a namespace. Last but not the least, `.` is already a special operator (used with qualified name). 
No need to rely on fancy type syntax mainly, but also the code of servant wouldn't have to live at the type level, it could be written using Haskell functions, which would make it arguably easier to understand and maintain.
http://stackoverflow.com/questions/23765903/why-is-text-utf-16-as-opposed-to-utf-8#answer-23766378
Notice how the example Haskell side effects weren't functions. In Haskell there is a clear distinction between functions and subroutine, where imperative languages conflate the two concepts. That's why the only way you can introduce a subroutine in C or any other imperative language is through some sort of function. That's why even if you have anonymous functions or lambdas you still have the issue of conflating subroutines with functions. For example, even if you changed your example to a language that does allow first-class inline functions (like Javascript, Scala, ML, whatever) my criticism still holds: you have to wrap your side effect in a function to protect it against accidental evaluation. I actually wasn't carefully controlling evaluation order. Here's an example to prove it: main = do let x = print 1 evaluate $! x print 2 That only prints 2, even though the `print 1` statement is fully evaluated first. This is because functions and evaluation order have nothing to do with side effects in Haskell. So you might wonder: "Why does it even matter whether or not side effects are tied to evaluation order?" The reason this is important is that the process of equationally reasoning about code (i.e. substituting equals for equals) changes evaluation order. [This small article on the Haskell wiki](https://wiki.haskell.org/Referential_transparency) illustrates the issue quite succinctly. In Haskell, if I write: (putChar 'h' &gt;&gt; putChar 'a') &gt;&gt; (putChar 'h' &gt;&gt; putChar 'a') ... I can refactor the repetition by writing: let ha = putChar 'h' &gt;&gt; putChar 'a' in ha &gt;&gt; ha ... but the same transformation is not correct in ML. If I take this ML program: print "h"; print "a"; print "h"; print "a" ... and refactor it as: let val ha = (print "h"; print "a") in ha; ha end ... that will only print `ha` once instead of twice. You have to guard the side effect behind a function call in order to properly control the effect: let fun ha () = (print "h"; print "a") in ha (); ha () end This is a small and focused example, but the objection isn't theoretical. I actually see this problem all the time in real code at Twitter using Scala (a high-level language with first-class functions and side effects tied to evaluation order) where people make code transformations analogous to the above example that break the code because the evaluation order changed. It's much easier to reason about and transform code when you no longer have to even think about evaluation order. It greatly simplifies the mental model of how the program works.
Also, nobody prevents you from putting that property into your type class. At least for purely equational properties, this can be made independent of QuickCheck so that your package doesn't expose a dependency. Making it independent would also be useful when testing with other mechanisms, e.g. smallcheck.
I also never said we aren't going to adopt the Monad(return) proposal. ;) Personally I think any migration of Monad(return) is best deferred several versions and possibly addressed by careful wording inside of a Haskell Report, in which we can make it clear in the library portion that certain classes are allowed to have extra members in different implementations for efficiency or historical reasons. This might mean that return would move to top level in a report, but stay there in GHC in practice either for a while or indefinitely. I'm not wedded to any one particular solution here. There are reasonable arguments on all sides. At the meeting of the core libraries committee the other day we resolved to push any decision on Monad(return) to a future committee, because it gets easier to do over time as AMP fades into the background and the default `return = pure` definition gets a chance to get used. Doing it sooner is just more pain, doing it at all can be their decision.
That works but I would like to reduce the boilerplate code. quickCheckAll, does a bit more than quickCheck, it accumulates all the properties in a file and tests them one after the other. The output also identifies which property is being tested. I can envision some TemplateHaskell code that enumerates all the instances of an axiomatic class but I was hoping someone had thought of it before me and that a framework exists somewhere.. Ideally, I would add one line in every file that declares instances of an axiomatic class. something like: test_axioms = $(quickCheckAllAxioms ''MyAxiomaticClass) and then I could call test_axioms in my test suite.
they were talking about ++ versus polymorphic &lt;&gt; 
what does it not represent? 
Good to know. I'm not sure I'm the perfect candidate, but would be willing to have them tell me no if remote was an option. Are there any offices in Utah?
I think indeed that I seems a good idea but I haven't tried it and might realise using that it's a bad idea. However if you have arguments about why it s a bad idea, please let me know.
needlessly, but when there's a need?
I tend to put composition pipelines on multiple lines = a . b c . d is both concise and readable
This wouldn't be an operator, it is special syntax. But wanting that doesn't mean it needs to be ".", which is already overloaded to be multiple things and does not need to be made even more ambiguous and confusing.
we need open functions and open types: http://www.andres-loeh.de/OpenDatatypes.pdf (pinging /u/kosmikus) 
I'm guessing either you evaluate the return value or you don't, and if you do that is going to evaluate the argument regardless? Sneaky.
Tekmo's tutorials are the most approachable tutorials in the Haskell community. But, you raise a good point. Answers that define any new (i.e. not in the question) terms are better. Still, jargon is better than nothing, as it provides the questioner with keywords to google. 
I honestly don't mind the compiling part. I had home-brew set to always compile. Never liked how home-brew wanted to take over /usr/local and with el capitan I said screw it, time to just use nix on osx. Can't say I regret it to be honest. But to each their own.
Hey! Thanks! This just saved me a bunch of time! Arguably I should have thought to myself, "It can't find GHC. Is there a GHC (of the correct version) on your path?" But I didn't, and you did. Thanks.
I've not looked at it in any detail, but I expect there can be a small overhead to using `Proxy` - you are passing an extra argument, after all. I wouldn't expect it to make much difference in most cases, because it may be inlined away or the optimizer may notice that the argument it unused. There's also the option of using `Proxy#`, which has a zero-bit representation, unlike `Proxy`. I think the problem you're describing might be slightly different, though. I don't completely follow, but it sounds like you want GHC to do some extra evaluation for certain specializations of polymorphic functions? I wouldn't expect that to depend on whether the type is determined using a proxy or explicit type application. Can you give more of an example?
`@` is slated for use in explicit type application, for consistency with Core and with papers. Although who knows when explicit type application might actually be implemented...
A perfectly unbalanced tree is essentially the same thing as a singly linked list. And anyway, you're incorrect, because in Haskell the type [] is a singly-linked list, and String is implemented like so: type String = [Char] https://hackage.haskell.org/package/base-4.8.1.0/docs/src/GHC.Base.html#String
I was getting to that stage in my career as a first year postdoc. A group of us were trying to apply for funding. The process was incredibly hard to get started in, because you have no established profile or funding sources you have to make your research proposal fit the grant or piggy-back the work of well established PIs. My friends who were successful happened to be working in hot fields. Some people are lucky with their connections, so it is a bit of a lottery. 
How do you "test that going from `a` to `[a]` does not break the invariant"? The obvious answer is to pick a few base cases for `a` and show that either both the `a` and the `[a]` instances satisfy the invariant or both fail. But that is demonstrably not enough; for example, `\x -&gt; elem (head (show x)) "'\""` is a simple property for which passing from `Char`, `Int`, or `Bool` to `[Char]`, `[Int]`, or `[Bool]` does not change the outcome; but passing from `[Char]` to `[[Char]]` *does*. To generalize that complaint: consider the (stupid, pathological... but also educational) code below: class Level a where level :: a -&gt; Integer instance Level Bool where level _ = 0 instance Level a =&gt; Level [a] where level xs = level (head xs) + 1 For any `n`, I can write the property `\x -&gt; level x &lt; n`, and that property is true for the first `n` applications of the instance and false afterwards. In other words: you can't know a priori at which level the invariant is going to break, if ever.
Hasochism and hasoschism would be completely different things.
This is hilarious.
&gt; So it's more like "Functor is not really the type of functors but more the type of endofunctors in Hask". Yeah, that's what I've thought. I'm still not sure if I understand the concept of "strength", though. Wikipedia pages aren't really helpful, to be honest (these only talk about "strong monads" in a language much more obscure than OP's joke burrito article, unfortunately) 
I totally agree with you about side-effect, but is your example not more about lazzyness rather than side-effect ?
Haskell's treatment of side effects is orthogonal to laziness. Everything I said would also be true in a 100% strict dialect of Haskell.
I can only imagine this to be true if you use MonoDevelop exclusively as a text editor and nothing more.
I fill in as much as I can in the terminal with CLI tools. Everything in C# is mutable state. All of our methods are impure - almost none even take arguments, and there are side effects in every one, of course. There's no explicit importing, so every module - of several thousand - uses things that just exist in the world, from all other modules combined, so every new thing is a potential problem for every old thing. Thus, I have no sense of what exists, and I have to follow the trail of "Find References" and "Go to Definition" for every single thing, usually to 5 or more levels deep, and keep a huge amount of state in my head to have any hope of writing new code, and I don't think I've ever fixed anything yet. Everything is done through coroutines with IEnumerators and delegates/actions, so there are lots of race conditions and unknowability, and bugs. It's the worst dev experience I've had in 25 years.
Isn't the fact that head [print "a", print "b"] doesn't evaluate `print "b"` due to laziness ?
There is no difference between `Text` and `String` in that regard. Both are implementations of Unicode text. The difference is that the underlying representation is different, resulting in different APIs and different performance characteristics.
You can easily verify that this is not due to laziness by strictly evaluating the entire list (Try it!). Here is an even simpler example: main = do let x = print 1 seq x (print 2) The above example only prints 2 even though the first print command is strictly evaluated.
That's the program that the OP's referring to.
Hmm. I would agree if it could truly live on the term level (as in a dependently typed language). But given the current overheads of the singletons library, I personally consider the direct type-level encoding easier. I will reevaluate once the types/kind merge has happened, but this will probably have positive impact on Servant no matter what. 
Right, but if it says "Functional Programming Lab", that sounds like it's being described as an IDE. It it was displayed as just "Haskell" then I would agree with the OP.
Some comments: * `popAll` returns a tuple, but the second component is never used; you can change it to return just the first element `popAll x y = reverse y ++ x`, and then you don't have to use `fst` in `toRPN`. * I would remove "end" as a sentinel value, and write `toRPN x = popAll (foldl ...)` * You might consider having a datatype `data Token = Number Complex | Function String | Operator String | OpenParen | CloseParen` and use pattern matching (this way the compiler can see you covered all the cases)
Exactly. 
&gt; Notice how the example Haskell side effects weren't functions. So what? It doesn't matter how Haskell calls them and how they look in syntax: be sure that behind the scenes there are “functions” (or, more exactly, closures). The implementation can't leave without them. In your first example we have these functions: `print 1`, (do)`x&lt;-getLine; putStrLn x` and `fail "Urk!"`. The fact that in C (or whatever) you need to write explicitly a function/closure is **syntax**, and not something that proves *Haskell handles side-effects better*. Let me anticipate this: no matter how many great concepts and abstractions you can list of your language: **you can't prove a sentence like that**. &gt; clear distinction […] where imperative languages conflate the two concepts Fortran has both the keyword `function` and `subroutine`. Is it conflating the two concepts? But, heck, **which concepts**? &gt; That's why the only way you can introduce a subroutine in C or any other imperative language is through some sort of function Again you are indeed talking about **syntax**. To be extreme, every language is syntactic sugar for the machine code… C `void something(void)` **is a subroutine**, if by “subroutine” you accept the wikipedia definition *a sequence of program instructions that perform a specific task, packaged as a unit*. Which is widely generic so that functions are indeed also subroutines. Moreover, *in different programming languages, a subroutine may be called a procedure, a function, a routine, a method, or a subprogram* (quoting [wikipedia](https://en.wikipedia.org/wiki/Subroutine)). In fact, in C you call `void something(void)` a function, even if it doesn't return a value. Fortran discriminates them this way: a *function* returns a value, a *subroutine* does not; so a function can be used in expressions like `calc(x)+1`, subroutine can be executed with `call subr`. Was you thinking about something else? How this speech is useful in “proving” that Haskell handles side-effects better than imperative programming? &gt; That's why even if you have anonymous functions or lambdas you still have the issue of conflating […] If you call it “an issue”, it does not mean it is, nor that you are proved that it is. In fact, I can't see any issue in this **supposed** conflation. In Fortran you can prefix `function` with `pure` to inform the compiler that the function has no side effects… Otherwise, they are allowed to. Nonetheless, I bet Fortran “has the problem”, too. So, it can't be about *the conflation*. &gt; you have to wrap your side effect in a function to protect it against accidental evaluation. **Accidental evaluation**?! Are we doing stochastic programming? If I know my language, I know when something is evaluated/executed… I wrap it into a subroutine (`void name(void)`) since the syntax does not allow array of function/subroutine to be written as `print 1`, `do …` or `fail …`. It allows only array of homogeneous types. Among these types there are not closures, but pointers, e.g. to functions/subroutines. **I must do so even if there are no side-effects at all**. E.g. void do_nothing(void) {return;} So: **I am not wrapping stuff in functions (subroutines) to avoid “accidental evaluation”**, but since **syntax** does not allow me to do otherwise. Does “accidental evaluation” happen in Haskell? &gt; I actually wasn't carefully controlling evaluation order You did. `effects!!1` triggered the result expected by evaluating 2nd element of the list, and `effects!!0` triggered the result expected by evaluating the 1st. If you add `effects !! 2` between, it makes the program halt before `effects!!0` is executed. That is definitively bound to the order you write `effects !! N`. The example fails to show anything interesting and it's not an argument for your “proof”. Already said this. &gt; That only prints 2, even though the print 1 statement is fully evaluated first. So the `print 1`, even when evaluated, do nothing… Evaluation is execution? … I'm entitled to see **syntax**, again. By `let x = print 1` you **define** a function/subroutine. Definition doesn't imply execution, of course. Then, you never call the function `x`: you just pass it as argument to a `evaluate` function which *seems* not to execute `x`. Or maybe it executes `x`, but also redirects the I/O output to a sink. Hence I won't see the `1`. If I write main = do let x = print 1 x print 2 Everything works as expected. I see a line with `2` coming after a line with `1`. If I write dontdo f = return () main = do let x = print 1 dontdo $! x print 2 I obtain the exact same result. I could interpret it even like this: the evaluation (execution) occurs but the result of side-effect is discarded. main = do let x = print 1 y &lt;- evaluate $! x y print 2 This shows that the result is **wrapped inside a closure**, and if I “call” it, here we see the line with `1`! Is this the trick? I could deceptively translate it in GNU/Smalltalk: |x| x := [1 displayNl]. x. 2 displayNl. If I run this code, I only see 2! I.e. your examples explain little. Looking at `evaluate`, things become funnier. `Evaluate` *forces its argument to be evaluated to weak head normal form* **when the resultant IO action is executed**. So the question should be: when `let x = print 1` is evaluated, is `print 1` executed? Of course no, in the strict meaning; otherwise we would see `1`. When `evaluate $! x` is evaluated, is `x` really executed? Again the answer must be… no. This is the **real** reason why we don't see `1`: the execution does not happen “for real”; if it is done, it is done inside an (implicitly created) “container”. Try `ghc -O -S` with your code and then try removing also the `evaluate` line; you obtain — surprise — the same code. If you remove `-O`, the code differs. And what you see, as I've already someway deduced above, is that the execution “lives” inside a closure. Basically, **it's wrapped into a function**! You need to do it explicitly in ML. Hence, I talk again of **syntax**. The doc continues: *It can be used to order evaluation with respect to other IO operations*. So you need to order evaluation… The doc doesn't say *carefully* but… Anyway, it must be something really new in the computer world! Maybe superscalar CPU and pipelines made everything a mess. &gt; … illustrates the issue … You again talk about an issue. Which one? It seems like there exists an issue in Haskell, but outside the perception is quite different. &gt; the same transformation is not correct in ML […] Differences in **syntax** is everything I can spot: the “same transformation” in ML produces something which, even if still syntactically correct, has a different meaning, hence a different behaviour. The same meaning in ML is expressed through a different **syntax**. You should imagine the implicit creation of closures in Haskell. In your example with `ha`, we can think that `let` works like a literal substitution to honor the *equational reasoning* in which, by the way, I can't see elements to say “this surely proves that Haskell handles side-effects better!”… If we imagine `let` working as literal substitution, then we can translate your example in C using `#define` — indeed it would be nicer with `inline` functions, but then you'd say I am using a function, even if indeed, syntax apart, they are just a definition of a piece of code that can be put elsewhere also through copy-paste, if syntax would allow it… &gt; This is a small and focused example The link explains: *The reason is that print's side effect is realized when ha gets bound*. `let val` means that the ML compiler must evaluate — **execute** — the expression to produce an actual **val**ue to be bound to `ha`. If you want to *defer* the execution to when you want, and also you want to **execute** the code everytime you need, you **define a function** (`let fun` in ML). At this point it seems like Haskell is trying to change terminology to redefine the matter so that it seems that there's an issue in other languages… and this is shown misusing/misinterpreting what other languages offer. The “issue” does not exist at all, nor in ML nor elsewhere. &gt; the objection isn't theoretical […] I actually see this problem all the time in real code at Twitter using Scala Not theoretical…? Just silly! and there's no *“this problem”*, unless you really are stating that the problem has to do with being forced to write `let fun ha () =…` instead of `let val ha = …` and `ha ()` instead of `ha`! Or whatever the syntax is in Scala. Undisciplined, distracted or poor programmers break code while coding or while refactoring. Does this prove that Haskell handles side-effects better? How does Haskell succeed in fixing the programmers through a different syntax and things happening behind the scenes? &gt; It's much easier to reason about and transform code […] Likely. But you haven't “proved” Haskell has the better solution to evaluation-order easiness. (Something we hardly was worried about until we needed to refactor Scala code at Twitter…) &gt; It greatly simplifies the mental model of how the program works. Rather subjective (mental models stay in the mind of a thinker, and not all thinkers think the same way) and related to the exact nature of the program, how the program was conceived, organized, how it has grown… … … ## Finally I stick to my original sentence. A *pure functional language* can't do anything real. In order to make things happen, you have to abandon purity. Then, you can invent all the abstraction and cool concepts you want, but the reality will be still there and you have to cope with it, measuring advanteges and drawbacks. 
That seems like a bit of a disingenuous argument to me. It's not "Functional Programming Lab for Haskell". Consider this title : "Microsoft Office - Enterprise Productivity Suite" You don't read that and think this happens to be one of many "enterprise productivity suites" that happens to support "microsoft office" when you read that. Also, the the web site still says "Haskell For Mac" - http://haskellformac.com/. As a new user, the reasonable conclusion is that it is the sanctioned way to get haskell for mac.
https://github.com/nh2/call-haskell-from-anything#i-cant-get-the-dependencies-to-install-with-ghc--78
A reasonable user might also read the description of the product before spending $20 and see exactly what it was. Titles are intended to get your attention. 
I don't know why exactly but this makes me sad.
I believe that the attitude towards newcomers as completely devoid of reason beings, is fundamentally incorrect. It's likely that the people who decide to learn Haskell, have not spent their whole preceding life in jungle either, which makes them capable of discerning marketing. That's why I believe you're making a fuss over nothing.
So there's no solution? Is it possible to install each component manually, without using cabal? Are you aware of any alternatives to call-haskell-from-anything? Specifically, I'm looking to call haskell executables from Python.
Which frameworks do you use for backend, for frontend and for db access? 
`Illegal polymorphic or qualified type` is because the type synonym `Lens'` has that universal quantification in it: type Lens' s a = Lens s s a a type Lens s t a b = forall f. Functor f =&gt; (a -&gt; f b) -&gt; s -&gt; f t So you can't define the `instance` directly like that; you could wrap them in `newtype`s and write the instances on those: -- | Basically this is @'ReifiedLens' s s a a@ newtype Lens'Wrapper s a = Lens'Wrapper { _unwrapLens :: forall f. (Functor f) =&gt; (a -&gt; f a) -&gt; s -&gt; f s } class SomeLens l s a where getter :: l s a -&gt; s -&gt; a setter :: l s a -&gt; a -&gt; s -&gt; s instance SomeLens DataLens.Lens s a where getter = DataLens.getL setter = DataLens.setL instance SomeLens Lens'Wrapper s a where getter = Lens.view . _unwrapLens setter = Lens.set . _unwrapLens If you only have a `Traversal'` and not a `Lens'`, well, your `SomeLens` is still undemanding enough to let you define this too (the `Monoid` constraint is necessary because then you're using the `Traversal'` as a `Fold`: newtype Traversal'Wrapper s a = Traversal'Wrapper { _unwrapTraversal :: forall f. (Applicative f) =&gt; (a -&gt; f a) -&gt; s -&gt; f s } instance (Monoid a) =&gt; SomeLens Traversal'Wrapper s a where getter = Lens.view . _unwrapTraversal setter = Lens.set . _unwrapTraversal
The solution is a fork.
like map changes the length? good to know.
This is a [bug](https://ghc.haskell.org/trac/ghc/ticket/10920)
 &gt;Is this a reference to the "Real *x* have *y*" meme? "Real programmers do/don't [blah]" is a meme that's older than www. It was started by an essay "real programmers don't use Pascal", in reference to the recently-released book "real men don't eat quiche". 
Is it really correct ? For me it looks more like a problem with `seq` (which is a compiler trick anyway, in a pure world `seq` is equivalent to `const`) rather than the state monad.
`State` is only the victim here of Haskell's idea that it should be lazy and have `seq` at the same time. The confusion will be caused by `⊥ :: a -&gt; b` vs. `(λ _ -&gt; ⊥) :: a -&gt; b` because these two are the same in many semantic models (in particular the domain-theoretic ones which supposedly Haskell is modelled after) but Haskell will naturally distinguish them (as one cannot *observe* that a function always returns bottom). One way out of the conundrum is to state the monad laws as *inequalities*, e.g., `x ≤ (x &gt;&gt;= return)`. One probably gets this way some notion of a [2-monad](http://ncatlab.org/nlab/show/2-monad), but I doubt the Haskell world really wants to dive into 2-categories and 3-categories. Haskellites live in a fantasy world of "fast and lose reasoning", while at the same time they spout false propaganda about purity and equational reasoning. That is what they get for not having a reasonable definition of the language.
I'm not saying otherwise.
An important enough question that I wrote a web page about it to remind myself[1]. The answer is &gt; In Section 10.3 of “A History of Haskell: Being Lazy with Class”[2] it is explained that seq originally required a typeclass. This requirement was removed when it required too many changes to type signatures when speculative seqs were introduced when debugging. which, now that we have the benefit of hindsight, is a particulary poor justification, in my opinion. [1] http://h2.jaguarpaw.co.uk/posts/seq/ [2] http://research.microsoft.com/en-us/um/people/simonpj/Papers/history-of-haskell/history.pdf
That it was too inconvenient. If you add a seq to a polymorphic function you might have to change its type signature. 
&gt; Haskellites live in a fantasy world of "fast and lose reasoning", while at the same time they spout false propaganda about purity and equational reasoning. This is not really fair. The Haskellers who expound about purity and equational reasoning highly value those concepts, whilst at the same time realising that Haskell doesn't quite meet the standard. Still, Haskell's the best, or close to the best, that we've got. One doesn't mention the unfortunate edge cases when proselytising to Python programmers, much like one doesn't mention that [processed meat causes cancer](http://www.bbc.co.uk/news/health-34615621) when feeding bacon to a starving man. Of course, when talking to a type theorist (or organic-vegetarian-gluten-free eater) like yourself, one has to be -- and can afford to be -- a bit more precise. 
But the cases when you really know that something is Just are rare. It happens only in your internal code. And in those cases it is ok to use partial functions (IMHO). The trouble with partial functions starts when facing external APIs. Which happens more frequently imho. 
I'm curious. Could this be prevented if we didn't have type erasure at runtime? It appears to me that that's what the paragraph about eta-equality is saying but I'm not sure, if I understood it correctly. // Edit: Clarification.
"I like this" is not the same as "this is required".
One solution is to use safe unpacking with the pattern matching and the extra complexity. The other (much less verbose) option is to use safe *packing*. `NonEmpty.tail` has type NonEmpty a -&gt; a, and will ensure you're not passing it a potentially empty list. So, when you've just prepended something to a list and are passing it to something that will take a tail, you can just use `(:|)` instead of `(:)` and there's no extra pattern matching or anything. Meanwhile, you've conveyed what you know (that you just prepended something to the list) to the compiler, and now it can help you more.
While we're at it, let's also have Apple rename Photos and Numbers lest someone think those *are* photos and numbers.
The State monad is kind of a red herring here IMO; the real defect, if you want to call it that, is that Haskell chose to be slightly incorrect when it comes to bottoms, for reasons of practicality. I haven't done the math, but I believe that as long as no bottoms are involved, State does follow the Monad laws. The authors note that having both `seq` and lazy evaluation in the same language is the problem, but here again bottom is the culprit. One might say that bottoms are Haskell's null pointers...
I like the simple approach they use in Ocaml, where they give a pretty name to the total function and an ugly name to the partial function. So List.head has type `[a] -&gt; Maybe a` and List.head_exn has type `[a] -&gt; a` but raises an exception on empty lists.
I agree that this is a bit annoying. It could also show an example use like some of the other functions do in the same module: http://hackage.haskell.org/package/base-4.8.1.0/docs/Data-List.html#v:drop
(When did we not have runtime type erasure?) 
Thanks for your reply. I would like to avoid a newtype. Requiring a newtype is the same as not taking advantage of adhoc polymorphism and implementing data SomeLens s a = SomeLens { getter :: s -&gt; a, setter :: a -&gt; s -&gt; s } lens'Wrapper lens = SomeLens (Lens.view lens) (Lens.set lens) My real question is why can't GHC make instances for these kind of types? It makes types like Lens' less desirable. 
Yes this is rather annoying. If they kept the extra lines, a simple filter over the output to drop empty lines for those desiring it would be easy.
Looks good!
I would have probably done the same, considering also the portability to Windows. Just documentation would be good here. Looks like `extra` is worth poring over. I only knew about `MissingH`. Is there a list somewhere for the wide-aiming complementary packages of this kind?
Those cases are dwindling with every new extension to the type system. Now, they can never be fully eliminated while maintaining TCness... though if your invariant is really so complicated that you can't express it with the type options becoming available, you might want to consider a redesign because odds are you can't manage it mentally either.
at line 1052 you have -- &gt; lines "a\nb" == "a\nb\n" did you mean -- &gt; lines "a\nb" == lines "a\nb\n" ?
Do you still get the problem with `-XNoImplicitPrelude`? If I add `import Prelude` to test.hs then I can do $ ghci -XNoImplicitPrelude test.hs GHCi, version 7.10.2: http://www.haskell.org/ghc/ :? for help [1 of 2] Compiling Prelude ( Prelude.hs, interpreted ) [2 of 2] Compiling Main ( test.hs, interpreted ) Ok, modules loaded: Prelude, Main. *Main&gt; main "hi" *Main&gt; The behavior is familiar if you work with `X.Y.Prelude` modules, like `Pipes.Prelude` and so on. 
Thanks, this is a viable workaround, but it still seems to me that there is a bug in ghci. What I want is a custom, implicit Prelude (to work around differences in base versions), and ghc allows this. Shouldn't ghci behave the same way? Note for anyone facing this problem: in addition to adding `import Prelude` to every source file, you should add `ghc-options: -XNoImplicitPrelude` to your cabal file. Then both `cabal repl` and `stack ghci` will work properly, without the user needing to add `-XNoImplicitPrelude` explicitly. Adding the `NoImplicitPrelude` pragma to each source file is enough for ghc, but it's not enough for ghci to work properly. 
I'm more frustrated by their complete lack of responsive web design...
Good catch, thanks. It's fixed now.
Let's figure it out! The first thing which surprises me with your example is that `./test.hs` has access to `./Prelude.hs` even though it did not import it. I knew it was possible to do this: {-# LANGUAGE NoImplicitPrelude #-} import My.Custom.Prelude main = hi in order to use a custom prelude instead of the one from `base`, but I didn't know that just putting a file named "Prelude.hs" was sufficient for it to get imported. Is this behaviour documented anywhere? Or maybe it's a bug? The second surprising thing is that it seems to me that with a custom module called "Prelude", it is now ambiguous whether to import the one from `base` or the one you defined. Adding a local file called `./Data/List.hs` and importing `Data.List` from `test.hs`, I conclude that local modules take precedence over library modules of the same name. Okay. Next, you compare `ghc --make test.hs` and `ghci test.hs`. But the missing `--make` argument is very important! With just `ghc test.hs`, only `test.hs` is being compiled, not its dependencies. And yet, the third surprising thing is that with your example, `ghc test.hs` does compile `./Prelude.hs`! Taking a closer look at [the documentation for `ghc --make`](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/modes.html#make-mode), however, it looks like `--make` is implied by `ghc test.hs` because "no other mode is specified", so this is actually the expected behaviour. `ghci test.hs`, however, is equivalent to `ghc --interactive test.hs`, so a mode *is* specified, and so `--make` should *not* be implied, right? Yet if I delete `./Prelude.hs` and run `ghci test.hs`, my `./Data/List.hs` does get loaded. How? Taking a closer look at [the documentation for `ghci`](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ghci-invocation.html), it's not `--make`, but `ghci` does load all the dependencies of the specified file. Okay. What if I don't specify `test.hs` on the command line, but `:load` it interactively instead? $ ghci GHCi, version 7.10.2: http://www.haskell.org/ghc/ :? for help &lt;interactive&gt;:1:1: attempting to use module ‘Prelude’ (./Prelude.hs) which is not loaded But `ghci`, I didn't even specify `test.hs`, why are you loading its dependencies? I don't know, but this `./Prelude.hs` file is preventing me from playing with the interactive mode, so let me delete it and try again. $ ghci GHCi, version 7.10.2: http://www.haskell.org/ghc/ :? for help Prelude&gt; Right! `ghci` imports the Prelude so I can type expressions using its functions at the command line. So for the same reason `test.hs`'s implicit import of the Prelude causes it to import `./Prelude.hs` instead, `ghci`'s implicit import of the Prelude causes it to import `./Prelude.hs` instead. Okay, so why isn't that import working? Let me try to import `./Data/List.hs` using an explicit import: &gt; import Data.List &lt;interactive&gt;:1:1: attempting to use module ‘Data.List’ (./Data/List.hs) which is not loaded Aha! &gt; :load ./Data/List.hs [1 of 1] Compiling Data.List ( Data/List.hs, interpreted ) Ok, modules loaded: Data.List. &gt; import Data.List &gt; Okay, I think I have a pretty good idea of what's happening now. In `ghci`, you need to `:load` files before importing them. Doing so loads all the dependencies of this file as well. Running `ghci test.hs`, as the aforementioned documentation says, is equivalent to running `ghci` followed by `:load test.hs`. Doing so would load `./Prelude.hs` and `./test.hs`, and everything would work fine. However, *before* the `:load test.hs` is even attempted, `ghci` does the `import Prelude` it does every time you launch it, and that fails because `./Prelude.hs` is not loaded yet! Okay, so how do we work around this problem? We must somehow allow the initial `import Prelude` to import the Prelude from `base`, and only then run `:load test.hs`. One way I found is to move all the source files to a subfolder, which I named `src`: $ mkdir src $ mv Prelude.hs test.hs src/ $ ghci GHCi, version 7.10.2: http://www.haskell.org/ghc/ :? for help Prelude&gt; :set -isrc Prelude&gt; :load ./src/test.hs [1 of 3] Compiling Prelude ( src/Prelude.hs, interpreted ) [2 of 3] Compiling Data.List ( src/Data/List.hs, interpreted ) [3 of 3] Compiling Main ( src/test.hs, interpreted ) Ok, modules loaded: Prelude, Data.List, Main. You can also put those two commands in `./.ghci` and they will be executed automatically every time you run `ghci`.
Yesod and Servant for backend. Yesod for traditional websites, Servant for SPAs. I use Persistent for database access. I really like PureScript with the halogen framework for front end development, though I need to try some of the ghcjs solutions.
Or a takeover, as documented [here](https://wiki.haskell.org/Taking_over_a_package).
I thought bottom was just a loop ...
Infinite loop is bottom; undefined and error are also bottom.
You are aware that Turing completeness requires bottom though, right?
It is perhaps worth noting that one of the reasons `Lens` is just a type synonym for `forall f. (Functor f) =&gt; (a -&gt; f a) -&gt; s -&gt; f s` is to allow *lens*, *lens-family*, *microlens*, etc. to use the same type, so that in the simpler use cases switching libraries only takes changing the import lists, as opposed to introducing further abstraction such as a `SomeLens` class.
If you're a pro with Haskell this may not be a great option, but if you're learning and playing around with it [Haskell for Mac](http://haskellformac.com) is pretty awesome. It's not quite perfect but I've been able to learn far more Haskell using this than anything else. Launch the app and start typing. If you look through all the other comments you see stuff about learning nix, playing around with Homebrew, etc. Not that there's anything wrong with that stuff. Homebrew is awesome. But if you're just trying to get a grasp on Haskell I'd check out this program. When I started taking programming classes we were using C++ in a nice IDE and using dynamic memory. We weren't connecting to the UNIX server, writing code in vim, and compiling an output to run, etc. Yes it's powerful and you sometimes you need to do stuff that way. But if you're trying to learn a language, especially if something that's so alien, sometimes it's best to just focus on the language. Again, if you're a pro with Haskell/functional languages, etc. it's probably a waste of money. If you're n00bish like me I'd check it out.
Strictness still causes different equational-reasoning related problems when bottom is involved. The only way to really avoid it is to go total.
Thank you! That's what I did. So far as I can see, it works really well. Unlike the TemplateHaskell code in Test.QuickCheck.All, this approaches gives me access to all visible instances. That can be a strength or a weakness. With Test.QuickCheck.All, every module can have a feature call checkProps that can be called from the test suite. Doing that with my quickCheckClasses risks including instances more than once unless one is careful to call quickCheckClasses only once per class and to make sure to have all instances visible when one does so. It's more precaution than I would like to but it is manageable.
I don't get it. The `State` type is defined with a newtype, not a synonym. Thus, `&gt;&gt;=` in general has to pattern match on the first argument. Pattern matching on bottom is bottom. Unless someone is going out of their way to write something that behaves in a bizarre way, like `State (error "boom")`... Not to even mention that no one in their sane mind calls `seq` on ` State`, or throws bottoms around when expected to return a `State` (when you already are in a monad there are exceptions for this. And if the bottom is due to nontermination rather than calling `error`, then well, a miswritten program that constructs a `State` only to `seq` it and do nothing else, might work instead of crashing. Doesn't seem like a huge deal)
totalDiscount takes two ints as arguments, but you're passing it a floating point number and a list of integers. What do you want the function to do? Because right now it returns a percentage of a to b.
This would be interesting to try to fix if it does not cause too much of a riot. Here by the way is something that I was considering, but I think is probably impossible? Suppose we had `smartSeq` with the same type as `seq` but with the following behavior: On functions it would _never_ force them, but on all values that were not functions it would do what it did today. From a denotational standpoint it seems this would be totally fine? Like even though it seems weird, it would be properly lawful and not introduce the problems `seq` does today. But from a practical standpoint I suspect this would be very hard to implement because we would need to be able to know _without inspecting thunks for any sort of tag_ which polymorphic values were instantiated at arrow types and which were not. So at first glance at least this seems like quite a bit of work. (Although perhaps still less work than /u/dolio 's proposal? https://mail.haskell.org/pipermail/haskell-cafe/2014-April/113378.html)
That isn't what that paper says at all. It gives the result that we can build a "partial language" and an embedded "total language" inside it, and in such a circumstance "if two closed terms have the same semantics in the total language, then they have related semantics in the partial language." So all that means is that if you reason in the absence of bottoms, then, _assuming no bottoms_ your results hold even in the total language. I think it's a great paper, but mainly refer to it as a good compendium on all the ways in which things _don't_ work as nicely in a partial setting as a total one. Furthermore the problem illustrated here is worse than the usual issues with partiality, because it has to do with haskell, thanks to its polymorphic `seq` not even obeying the usual domain-theoretic semantics you would want in a lazy language.
Still, there's a difference between being able to express a property as a type, and writing the proof term that will convince the type checker that the property holds. Proving that a list is not empty at some point of some program can involve an arbitrarily complex proof that not everyone can afford to write in a machine checkable form. (If that was easy, we could be programming all our programs in Agda or Coq.)
Your code is valid and type-checks, but you are using the function incorrectly. Try this: &gt;&gt;&gt; totalDiscount 4 5 80.0 
yes I got. thanks for helping but how can I get it like this totaldiscount 0.1 [4,5] and giving me 0.9 ?
there is way to got like what I mention ?
Just as an additional data point - it is well known that the same exact argument "proves" that IO is not a monad, either.
I believe the first one to do something like this in Haskell - it was for π, but using a different series - was Jerszy Karczmarczuk in this [1998 paper](https://karczmarczuk.users.greyc.fr/arpap/lazypi.pdf). He also gives some history of similar such calculations from before Haskell.
It's a surreal discussion. After you gave wrong examples, it ends up you were talking of literal transformations of code and that C, an imperative language, has this feature, through its preprocessor… and hence it's *purely functional*, sort of (not read the link yet). According to this, other languages, no matter the paradigm they foster, can become *purely functional* by using a preprocessor suited form them. (It's also interesting to note that the *preprocessor* is considered a dangerous and sometimes overused tool in C…) Since we are dealing with text transformations, nothing is really evaluated/executed: I would have used somewhere the word *parsing*, just to make clear the stage the compiler was at; even *compile time evaluation* does **not** make clear the compiler is not dealing with the **values** of the expressions, but only with the **textual** form of them. So we find at last **metaprogramming**. But you haven't used this obvious word. These features (“macros” and metaprogramming) anyway are not interwined with functional languages: imperative languages can have them as well, and they can be added as an extra layer, with variable amount of effort, if someone feels the need. Of course, while transforming the source, nothing is *evaluated* nor executed, so it makes no sense to worry about side-effects of the *actual* code. About “nothing is evaluated”: you can say the compiler is evaluating the source code, but in doing so, you must make clear what I've already said: it's evaluating at a level where the “meaning” is totally ignored, and only the “form” (**syntax**) is taken into account. All this happens according to certain rules which you need to know and **understand** in order to 1) write correct code, and 2) forecast what your source code will actually be, if you want to know. If you know that ML `let val` **evaluates** (in the common meaning) the expression you're fine: you know **exactly** when that will happen in your running program. Otherwise you use `let fun`. The rules of the language, its grammar, explain this and make it possible to write code and, of course, to refactor it correctly. &gt; I don't have to understand evaluation order in order to understand […] Textual manipulation does not *evaluate* anything at all, so there's no order to be understood there. But don't you need to **understand** how that happens? which constructs behave like *macros* and which not? everything behaves like that and hence actually nothing is executed?! When all substitutions are done, you need to understand what will be going on after that. You still need **to understand Haskell code**, which is less trivial than many imperative programming languages. &gt; It's one less thing I have to think about when I program in Haskell compared to other programming languages. This is not even an argument of any kind and the other languages strongly disagree. I can say: it's one less thing you have to think about, after you have added ten new things to think about, or made simple things ten time harder than usual. &gt; this is an implementation detail that is not necessary In a lot of real world applications certain details, as the underlying implementation behaviours, matter more than abstraction and the finest concepts. Hiding and abstracting is usually fine, but not necessarily always. In a discussion like this, what's under the hood made me discover the boxed thing, which you disregarded in your previous speech. Anyway… At the end of your game **you obtain a lambda** which **must** execute code with side-effects once “used”. After a triple somersault, you are at the starting point: **a piece of impure code**. You have not written it explicitly, in fact **you used tons of syntactic sugar** to obtain it. My only initial point still holds: strictly pure functional language can't do anything: they need impurity. Your examples proves this is true and also shows that you can hide the impurity through **sophisticated syntactic sugar**. Therefore, at last, you're talking **only** about maintenability and safety, sort of. Tons of languages must deal with these issues, among others, and they do it in their way. I can't see anything in your speech that proves that Haskell solves any issue better than others and avoid programmers' flaws that no other compilers disallow. I say a language like Ada reaches certain goals better, and without the drawbacks of Haskell's executables. &gt; The difference is not purely syntactic Everything you are talking about it's **syntax**. Certain implementation details aren't necessary… &gt; The difference is that I have a compile-time protection […] that the evaluation order of my program has no effect on its behavior *With some caveats […] but they are small*, as it is small the chance that anyone has to worry about this kind of things. If there are some sort of extra special requirements, they exist other solutions to cope with those. Surely the solution wouldn't be to translate all the code into Haskell. &gt; Other languages do not offer the same protection You are implicitly claiming that this sort of “protection” is something necessarily desiderable to the point that, say, Scala should be abandoned in favor of Haskell, or, more generally, that we should use Haskell to benefit of the “protection”. &gt; The point of Haskell's protection is not about what you can do, but rather what you cannot mistakenly do This is a possible definition of “protection” — it seems now we are talking explicitly about that — i.e. to make someone to avoid mistakes. Nothing special about Haskell, generally speaking. But of course Haskell does it better. &gt; you can stick side effects in list or containers […] but you don't have the same protection **because** *you have to have a very precise mental model of the order in which things get evaluated* in order to make sure that side effects are triggered in the correct order (Emphasis added.) Then what a luck/miracle that the vast majority of side effects were triggered in the correct order until now! Likely it is because average programmers of imperative languages have a “very precise mental model of the order in which things get evaluated”, and this can happen since imperative languages make it possible to have that “precise mental model” very easily. Things get trickier when concurrency and parallelism are in the game, I agree. Isn't so in Haskell, too? Sorry but you wrote gibberish and poor reasoning. Weird, to say the least. &gt; In contrast, Haskell doesn't let me break my program if I screw with evaluation order Why should you screw with evaluation order? Mindless refactoring? Show me how you screw with evaluation order in other imperative languages. And… If you move `effects !! 0` before `effects !! 1`, altering *de facto* the output… are you screwing evaluation order? How does Haskell protect you from doing so? (I know you were talking of something else; but now I am asking you how Haskell protect you by screwing with those two lines.) &gt; there's no way that I can trigger that semantically significant evaluation. The compiler forbids me from triggering those closures. If you can't trigger them, why they exist in first place? … Anyway, you are saying that things do not happen accidentally. &gt; the key thing that Haskell forbids is tying execution order to evaluation order First, define properly “evaluation order”. Are you still talking about how the compiler parses the source code? Then make a real example that proves that it is a really important thing **in practice**. If you need it to build your mental model, it doesn't count: other people does not need it to achieve the same purpose. &gt; completely eliminates the entire evaluation order axis from my mental model of the program. I've never had an “entire evaluation order axis” in my mental model of a program, and I do mostly imperative programming. What I care about, is if `effects !! 0` must be before or after `effects !! 1`, and this is something you need, too. I can understand it because I know the the grammar of my language of choice. If it would be, say, ML, I would know that `let val` is “executed” right there, in order to obtain a **val**ue. &gt; But that's the point! […] There does **not** exist anything like **accidental** evaluation. You need to know at least the basic of your language; otherwise everything will look magic and you can break it easily. This is true for Haskell as well. &gt; It's a productivity enhancement. For the way you build your mental models… Not necessarily for everyone and for all the programming challenges. &gt; No, you had it right in your previous sentence So your example was about a way to avoid execution of code, and you presented it as something incredible. I usually use comments for that. &gt; The perfectly disciplined programmer is like the sufficiently smart compiler: a myth In fact I was talking about undisciplinated, distracted or poor programmers. No way your Haskell features can save the code from them. &gt; If the compiler can statically eliminate an entire class of mistakes […] Sure. But which class of mistakes exactly does the feature you're talking about eliminate? It makes your mental model easier. Ok. The fact that you need to use `let fun`, or an explicit closure, or whatever, it's **syntax** — the fact that indeed your are defining a closure can be thought indeed as a *detail of the implementation*… &gt; it's literally impossible to change execution order using evaluation order unless you cheat There's a contraddiction: *literally impossible* and *unless you cheat*. So, it **permits** to cheat. And, you know, the “Undisciplined, distracted or poor programmer” is still out there, ready to break Haskell code with the same easiness other programmers break imperative code. 
I can't really help you with how to write your function unless you clearly explain what the function is supposed to do. Just telling me what inputs it should give for certain outputs is not enough information. After all, I could just write this, which is legal Haskell code: totaldiscount 0.1 [4,5] = 0.9 ... but I'm guessing that is not what you want.
&gt; According to this, other languages, no matter the paradigm they foster, can become purely functional by using a preprocessor suited form them. (It's also interesting to note that the preprocessor is considered a dangerous and sometimes overused tool in C…) The key point wasn't the use of a text preprocessor, but rather the two-phase distinction. So the answer to your question is: yes, any programming language can become purely functional by having some sort of two-phase distinction (not necessarily a text preprocessor). In Haskell there are essentially two phases: * Phase #1: Pure evaluation (exposed to the programmer) * Phase #2: Impure evaluation (hidden from the programmer; evaluate the side-effect closures hidden inside the `IO` values) So, under the hood there is definitely impure evaluation, but the programmer can't access it because of the phase distinction. Haskell forbids you from triggering side effects via evaluation due to the phase distinction. &gt; I can't see anything in your speech that proves that Haskell solves any issue better than others and avoid programmers' flaws that no other compilers disallow. The key thing that Haskell does different than other programming languages is that Haskell does not let you tie execution order to side effect order. It's literally impossible (with caveats, as always, but it's basically true) because it's a compile-time guarantee. &gt; This is a possible definition of “protection” — it seems now we are talking explicitly about that — i.e. to make someone to avoid mistakes. Nothing special about Haskell, generally speaking. But of course Haskell does it better. Again, the thing Haskell does is statically forbid you from tying execution order to evaluation order. Static guarantees are much more effective than programmer discipline at preventing these sorts of mistakes. &gt; Then what a luck/miracle that the vast majority of side effects were triggered in the correct order until now! Likely it is because average programmers of imperative languages have a “very precise mental model of the order in which things get evaluated”, and this can happen since imperative languages make it possible to have that “precise mental model” very easily. Imperative languages make it easy to understand evaluation order, so I spend a small but non-zero amount of mental effort in understanding evaluation order. In Haskel I don't have to understand evaluation order at all in order to understand execution order, so I spend even less mental effort in haskell than in imperative languages. &gt; Why should you screw with evaluation order? Mindless refactoring? Here's a simple example. Consider these two expressions: flip f a b f b a In a strict, imperative language, the `a` and `b` sub-expressions are evaluated in a different order depending on which form you choose and could trigger side effects in a different order, so those two expressions are not equivalent in general. In Haskell, they are always equivalent, no exceptions. Here's another example. Consider these two expressions: const a b b In a strict imperative language those two expressions are not necessarily equal. In Haskell they are always equal, no exceptions. &gt; And… If you move effects !! 0 before effects !! 1, altering de facto the output… are you screwing evaluation order? How does Haskell protect you from doing so? It doesn't, and I never argued that it did. The order of lines in a `do` block has absolutely nothing to do with evaluation order. Google "reverse state monad" if you don't believe me. &gt; You need to know at least the basic of your language; otherwise everything will look magic and you can break it easily. This is true for Haskell as well. I agree with that. But I'm saying that if we evaluate languages purely along the axis of how much I need to understand evaluation order, Haskell ranks better along on that axis because I need to understand even less about evaluation order than in other languages. &gt; In fact I was talking about undisciplinated, distracted or poor programmers. No way your Haskell features can save the code from them. These are precisely the programmers that static guarantees benefit the most! They have the least time to carefully pay attention to evaluation details, so they are the most likely to mess it up. Haskell is a language designed for lazy, error-prone programmers like me who make lots of mistakes. Expertly disciplined programmers don't need Haskell: they can program in a lower level unsafe language because they won't make as many mistakes. &gt; Sure. But which class of mistakes exactly does the feature you're talking about eliminate? The most common mistakes that I see at work (in Scala) that would be prevented by not tying side effects to execution order are: * Premature triggering of effects in the wrong context using the collections API (i.e. `map`, `filter`). This is a very common mistake in Scala * Large-scale refactors that should preserve behavior but don't * Accessing mutable or lazily-initialized values before they are completely initialized You also can't easily prove anything about languages that are purely functional. Equational reasoning doesn't really work that well in imperative languages. You can prove things on a small scale within an imperative language, but if you want to scale to larger proofs then equational reasoning is a must. So purely functional languages are good if you care about proving program correctness. &gt; There's a contraddiction: literally impossible and unless you cheat. So, it permits to cheat. And, you know, the “Undisciplined, distracted or poor programmer” is still out there, ready to break Haskell code with the same easiness other programmers break imperative code. I should clarify that the purpose of Haskell's static assurances is to prevent *unintentional* mistakes. It's not even possible to devise a programming language that prevents *intentional* cheating since a sufficiently determined programmer can always just choose to switch to an unsafe language.
Okay, so what would `totaldiscount` return if given this input: totaldiscount 0.2 [3, 4]
Good question. I can see three possibilities: 1. Pretend that `seq` is not there. Who put it there in the first place and for what reason? 2. Use inequalities instead of equalities and then establish strictness separately. 3. Use ML. Am I allowed to troll on reddit?
Would give -&gt; 0.14
In my current smallish project I've got something that roughly boils down to this: func :: Maybe Thing -&gt; EitherT ErrorType IO Result func thing = case cheapTest &lt;$&gt; thing of Just True -&gt; expensiveIO $ fromJust thing _ -&gt; left someError The real code is a Servant `Server`, so I don't think there's a clean way of changing the call point to avoid an incoming `Maybe`. How would you handle this? I haven't thought of a way that doesn't involve duplicating the error state, would you just go ahead and do that? --- **Edit:** With the aid of some brainstorming downthread -- func :: Maybe Thing -&gt; EitherT ErrorType IO Result func thing = case mfilter cheapTest thing of Just thing' -&gt; expensiveIO thing' _ -&gt; left someError Gorgeous!
Or, my favorite, remove polymorphic seq. It should never have been introduced in the first place. 
&gt; that Haskell chose to be slightly incorrect when it comes to bottoms I think "slightly" is an understatement. It's not just that `(λ _ -&gt; ⊥) /= ⊥`. It's also simultaneously believing `(⊥,⊥) /= ⊥` along with `(_,⊥) /= ⊥`, and `(⊥,_) /= ⊥`; Haskell's pairs are neither cartesian products nor smash products. Considering that functions and pairs are what gives us a CCC, the fact that *both* of them are wrong means that basically anything built under the assumption that we're working in a CCC is busted.
I like purity. it's not required. 
That is probably more extensible but looks like it might not be quite as type-safe. At least for simple cases where performance is not a concern I like the approach in aeson-applicative better.
Ok? You said "." was required. I pointed out that it is not required. Your response was "but I like it". That is not an argument that it is required.
if you're constraint is "method chaining, with any symbol" it's not required. if it's "method chaining, with the dot symbol", it is. I don't want the former, only the latter. or nothing, lens is fine.
&gt;if you're constraint is "method chaining, with any symbol" it's not required Please stop, good lord. Go read what you posted.
you mean it could be injective but partial?
(I just want to say, whenever a post of yours starts with "let's figure it out", I know that I'm always going to learn a lot, about both results and process)
 totalWithDiscount :: Float -&gt; [Float] -&gt; Float totalWithDiscount x y = x + sum y so how about this ? what's wrong with it I got totalWithDiscount 0.1 [4,5] 8.9 rather than totalWithDiscount 0.1 [4,5] 8.1 Can some one explain it to me please there is any way to list discountedItems ? such as discountedItems 0.1 [4,5] [3.6,4.5] 
This seems like something more appropriate for Stack Overflow
&gt;I need to Write a recursive function getUserName that takes an email address and returns words (before the @). filter (\c -&gt; elem (toLower c) ['a'..'z']) . takeWhile (/= '@') $ "target12345~~@host.com
Stop asking us to do your homework for you, or at least provide some evidence that you've thought about your problem before begging.
you're reframing what you don't want as what's unrequired. but yeah, I'm done.
this is not homework I am not in school I need to learn Haskell to be in job soon In munich ,germany 
I recently heard of [located-base](https://hackage.haskell.org/package/located-base). Would it help in your case? It turns partial functions into partial functions *with location info*.
&gt; filter (\c -&gt; elem (toLower c) ['a'..'z']) . takeWhile (/= '@') $ "target12345~~@host.com Can explain what you did ? I need to know it not just answer 
There are only 4 functions here: [takeWhile] (http://zvon.org/other/haskell/Outputprelude/takeWhile_f.html), [elem](http://zvon.org/other/haskell/Outputprelude/elem_f.html), [toLower](http://zvon.org/other/haskell/Outputchar/toLower_f.html), and [filter](http://zvon.org/other/haskell/Outputprelude/filter_f.html). If you are still having trouble, feel free to use hoogle to lookup the function definition and navigate to it's source code and decipher what it does that way. For example, that process would look like this with filter: 1. Go to https://www.haskell.org/hoogle/ 2. Type in filter 3. hit search 4. Click on the first orange link that reads "filter" 5. Read the description, then click "source" on the far right. 6. If you don't understand how filter works, join #haskell-beginners on http://webchat.freenode.net/ 7. repeat this with every function you want to understand. 
I have a question about your technical remarks - how is it possible to *not* permit `seq`, given that it can be defined as: seq a b = case a of _ -&gt; b If that doesn't force the `a` to WHNF, then `case` needs to have a more complicated operational semantics, or something... I'm not totally sure what that would mean though, do you? Or does this definition somehow magically get a more complicated type than `a -&gt; b -&gt; b`? If it does, that seems to imply that evaluation is tracked in the types, which would be an extremely far-reaching change to the language and type system. (Which could be very interesting, but also a rather different point in the design space.)
I think they meant it could be injective but not surjective. A function is bijective if and only if it is both injective and surjective.
Yes. From my GHCi: &gt; :t getter (x :: Lens.ALens' Foo Int) getter (x :: Lens.ALens' Foo Int) :: Foo -&gt; Int `makeLenses` generates `x` with a polymorphic type, but instance selection requires a more monomorphic type. I was able to introduce an auxiliary function and get something that doesn't require types to be mentioned explicitly: &gt; let { monoLens :: Lens.Lens' s a -&gt; Lens.ALens' s a; monoLens x = x } &gt; :t getter $ monoLens x getter $ monoLens x :: Foo -&gt; Int NB: you should use `storing` and `^#` instead of `view` and `set` when you know you have `ALens` instead of `Lens`. I wonder if the type equality instance context trick could guide inference the way we want. Yes, I think so. You'll have to turn on either `GADTs` or `TypeFamilies`, import Control.Lens.Internal.Context (as `Context`) and then change your instance to: instance (f ~ Context.Pretext' (-&gt;) a) =&gt; SomeLens (Lens.LensLike' f s a) s a where getter = flip (Lens.^#) setter = Lens.storing and then you'll be able to do: &gt; :t x x :: Functor f =&gt; (Int -&gt; f Int) -&gt; Foo -&gt; f Foo &gt; :t getter x getter x :: Foo -&gt; Int &gt; :t setter x setter x :: Int -&gt; Foo -&gt; Foo and also still have: &gt; :t getter (x :: Lens.ALens' Foo Int) getter (x :: Lens.ALens' Foo Int) :: Foo -&gt; Int &gt; :t setter (x :: Lens.ALens' Foo Int) setter (x :: Lens.ALens' Foo Int) :: Int -&gt; Foo -&gt; Foo HTH. EDIT: Using Pretext' instead of Pretext. Also, I considered switching to the Context functor instead of the Pretext functor, but that makes things more complicated and doesn't seem to still allow ALens values to be passed directly. It makes me wonder why Context isn't just a (type-restricted) alias for Pretext...
Perhaps the reasoning is: if we have some function called (whatever), and some other function called un-(whatever), and the types match up, then it is reasonable to expect these to be inverses.
https://github.com/begriffs/haskell-vim-now
Heh /u/ranjitjhala, you're famous
That definition of seq does not evaluate the first argument. The semantics of case is, of course, lazy. It will evaluate the scrutinee as much as is needed to find the first pattern that matches. Matching against _ requires no evaluation. (It's spelled out in the report if you want more details.)
Ah, `mfilter`! I knew there had to be a more general form, I just didn't know what it was :)
Please elaborate why we can't get rid of bottoms (in practical applications)
So init just does the classic x:xs1:xs2:...:xsn:[]. Js looks like magic to me
I do not know how and where JsonGrammar would not be type-safe. But ok, I will have a look and compare myself. 
The short version is I'd suggest not doing this. The a medium length version is something like: -- force anything that looks like `Lens s t a b` to refine to `ALens' s a`. instance (f ~ Pretext (-&gt;) a b, a ~ b, s ~ t) =&gt; SomeLens ((a -&gt; f b) -&gt; (s -&gt; f t) where getter = flip (^#) setter = storing Getting better inference still requires using some type families to pick apart the shape of the (a -&gt; f b) -&gt; (s -&gt; f t) into `a2fb -&gt; s2ft` and various type families to access parts of `a2fb` and `s2ft`. It can be done. I don't recommend it. 
lenses are a type alias. The OP is asking to make an instance for a complex type alias that has a rank-2 type in it of the general shape `(a -&gt; f b) -&gt; s -&gt; f t`. That is going to be a mess no matter how you do it.
If you take `seq` into account, **any** `Monad` violates the monad laws. Even `Identity`. See http://stackoverflow.com/a/12620418.
The `case` construct is an elimination form. It makes sense for sum types (and inductively defined types whose outer constructor is a sum) where we can actually test which alternative we have got. The correct elimination form for function types is application. Thus `case` at a function type is just as problematic as `seq` at a function type. I am sorry I was not clear enough, what I wanted to say was that `seq` at function types is problematic; I think others complain about the same thing when they say that `seq` should not be polymorphic.
Interesting. I'm running engineering at another News startup that I co-founded a couple of years ago: http://borde.rs We run everything on Haskell (and NixOS). We should probably talk.
I think the main reason for lifting the API to the type level in Servant is that the API actually *is* a type, in the sense that the information contained in the API is information we want to have available statically. Interpretations of the API in Servant compute types from the API type. This is what enables us to require the types of the handlers (for the server interpretation) or the client functions (for the client interpretation) to exactly match the specified API. It's quite possible that there are slightly different ways in Haskell to achieve more or less the same thing. For example, we might be able to build the API by constructing a GADT, which would be on the value level. The index would still need to reflect the current API type, but one would not need to write it down manually. Interpretations could also make use of GADTs with a matching index, then one would not need type families. These other approaches may have certain advantages and disadvantages. So far, I think the design decisions Servant work rather well in practice. Extensibility is a reason for not using promoted datatypes (datakinds), which are closed by default, but to define the constructs of the API language as separate Haskell datatype. This tradeoff is discussed in the paper. If we had wanted to just have term-level extensibility, we might have tried to go with a finally tagless encoding.
You may be able to get rid of bottoms in practical applications if you think hard about your code. That doesn't mean we can get rid of bottoms from the language definition though. Unrestricted recursion demands them.
&gt; There's 2 JS libraries that make pattern matching: one creates by string (like "x:xs"..seriously!), other via macros (need to install some stuff in JS that changes synthax, crazy!) That is truly ridiculous. When I see people using small precompilers over JavaScript, like the library you mention, or, e.g., React, I doubt their sanity. They've already made the decision to abandon the vanilla JS, so why not fix the problem altogether and use a better language, which compiles to it?
Hey /u/ranjitjhala, Why does no one want to use it?
I've had an idea for a similar project some time ago, but then I dropped it, because I couldn't find a sane way of dealing with government-controlled agencies, throwing at your service crowds of people, whose job is to spread lies. I mean, it's hardly a secret for anyone that Russia, USA, China, Ukraine have those, as I'm pretty sure a lot of other countries do to a certain degree. I wonder how your project plans to deal with that..?
 &gt; case undefined of _ -&gt; () () There's no reason I can see that this polymorphic `case` should even be syntactically allowed. It doesn't do anything at all. Why not restrict `case` to only thoses cases[1] where you know the type of the scrutinee (because it can be deduced from the patterns)? [1] :)
Could you explain? I thought this proof depended very much on `\_ -&gt; _|_` as the wrapped monadic value. Are you suggesting something similar based on `IO = State RealWorld`? Or were you talking about this: https://www.reddit.com/r/haskell/comments/3rym37/haskell_has_no_state_monad/cwtuxjj
I think they are doing our community a great service by providing this product, lowering the barrier to entry. Seeing the occasional screenshot makes me wish I was still on mac. For now they are a worthy flag bearer in the IDE space. Users will surely read the description on their home/product page and understand what the product is.
There is the problem that strictness annotations on constructors would require datatype contexts, which have since been removed.
Yes, it's something we have to think about. We've had members of ISIS come through and try to vandalize. There's no silver bullet for combating bad actors but we can deal with most situations through a combination of things: 1) we have a log of every single action/edit on the site and make reverting simple 2) we have hundreds of people sitting in our newsroom who immediately see the changes made and can correct mistakes or intentional misinformation 3) hellbanning Bad actors usually look for easy targets. If you give the good people (the majority) powerful tools, the vandals usually go away. 
Can you please send me an email? I'd love to chat and hear about your experience using Haskell. levi at grasswire dot com
Well, excluded middle either holds or it doesn't hold. If it holds then I am trolling right now, but if it doesn't hold then I am only not not trolling.
"l'enfer, c'est Javascript" - J. P. Sartre
"On ne naît pas programmeur JS : on le devient." - S. de Beauvoir
This is very nice. It reduces boilerplate in Javascript for small functions. Don't care about these dismissive haskell supremacists ;)
Seems like you accept remote applicants. Kudos for that! I hope that this will bring you awesome applicants and that more companies follow your example!
Because with JS you can earn a living but with haskell don't? If Haskell is clearly superior, obviously we the haskellers are doing something wrong. Let's try to find it and then fix it. First stage: be more humble.
Posted to the wrong subreddit, go to /r/cringe with that
really? Can I apply for a job too? :D (no kidding: what company/startup in Munich is using Haskell and willing to employ/teach people with none or small experience - this is **not** to offend you - it's just that usually you only find jobs for people with expert-level haskell knowledge and I would love to find a job where you don't have to show your Phd to get it ... see: I have none :( )
Ah. I was under the mistaken impression that case must evaluate the scrutinee to at least whnf, even if no pattern requires it.
Actually it seems this code is a noop. According to Lennart, case only evaluates the scrutinee enough to pick a branch. In this case the branch doesn't demand anything so no evaluation would take place.
Actually it seems this code is a noop. According to Lennart, case only evaluates the scrutinee enough to pick a branch. In this case the branch doesn't demand anything so no evaluation would take place.
As Lennart just schooled me, case is lazier than I thought, so my definition of seq doesn't even work. So while you can pattern match on a function type, this won't actually do anything since there is no pattern you could write for a function type that would demand its evaluation (unless you apply the function to an argument, but then it's evaluation would be expected).
Or Purescript!
Correct. I'm just wondering why such a case statement is even allowed. It's very misleading, as you've discovered.
Yes. 'Z' is hugely overloaded. See also [the Z notation](https://en.wikipedia.org/wiki/Z_notation).
I wasn't thrashing the OP, I was agreeing with him.
Something similar to me happend, I quote : &gt; Haskell ? why not assembler ? ;-) 
\n is only ever a "separator" in broken data. It has *always* been a terminator. That said, of course we won't change the semantics of functions in Prelude... hopefully ever...
The treatment of bottoms as values in Haskell has nothing to do with Turing completeness, it's purely a Haskell quirk. You can't put bottom in a register in assembly language, or pass bottom to a method in Java, even though both are Turing complete.
All the time. &gt; What programming language do you use at work? &gt; &gt; Haskell. &gt; &gt; Pascal? &gt; &gt; No, Haskell. It's a different, functional programming language. &gt; &gt; Is it any relation to Pascal? &gt; &gt; No. &gt; &gt; Oh, okay. 
I agree concerning the humbleness and I don't understand the downvotes on your comment. It really takes patience when you see something obviously stupid not to call it so out loud. However I also think that it hardly makes a problem for Haskell adoption. For instance, I doubt the JS community isn't rude. We need to push the language into production. E.g., I spend a lot of effort on pushing the technologies like Haskell or Elm at the company I work for and sometimes I even succeed. Once I'll make the majority realise how much that changes, I'll win. My point was that once you start to use a precompiler on JS, you're not doing JS anymore either, you're doing some weird temporary language, which probably won't even be maintained in a year or two. That makes "making a living" part quite more questionable there as well.
Not very relevant to the discussion, but at least on hackage it seems uploading non-opensource libraries is a breach of their rules. &gt; The code and other material you upload and distribute via this site must be under an open source license. This is a service operated for the benefit of the community and that is our policy. It is also so that we can operate the service in compliance with copyright laws. -- from https://hackage.haskell.org/upload This is probably the reason why there are so few non-opensource packages there.
I was very puzzled when a new programmer from the other dept immediately understood what I was talking about when mentioning haskell. It was sort of weird, because he was nodding along but I was dropping names that I wasn't an expert in either. It took me about a week to understand why. *Clarification:* why = what he misunderstood. So it took me a week to figure out that he mistook pascal for haskell.
Isn't `undefined` a computation that never completes successfully?
Every deviation you make from "standard/vanilla JS" narrows the candidate pool and makes interop harder. If you can just hire any ol' JS developer, then you have an absurd amount of candidates to pick from. If you want to specify that you want React developers, well, that's starting to use a preprocessor for JSX, and it's a bit of a specific design philosophy, but it's JS at heart, and you could probably get a vanilla JS dev up to speed on React fairly quickly. It's a little trickier getting eg jQuery plugins to work with React, but not a huge deal. Same with ES6. It's JS at heart, but with a bit of additional stuff. A good JS dev can be productive while learning ES6, and compiling your own code from ES6 doesn't make it any harder to use regular JavaScript plugins or libraries. Going to Facebook's Flow type annotations is a similar "small step", but it does require more learning and onboarding. Going to TypeScript is basically JS but with mandatory types, and that's even further off. PureScript, Elm, and GHCjs are no longer JavaScript in any sense. Interop with existing JavaScript is much more painful, and there are far less tools and libraries available. The advantages that the languages have, while very significant, aren't the whole story. React with Webpack and hot loading/swapping of components is honestly a pretty good development setup. ImmutableJS, ES6, and Flow type annotations are enough to bring JavaScript from "terrible but usable" to "mediocre," and "mediocre language + good tooling" is more than enough to create good stuff.
You might want to work on your enunciation.
Pronouncing it the European way helps. Rhymes with rascal, unlike Pascal.
Yup
gotcha
Given that it's in general impossible to test for `return`, as equality between functions is undecidable, then yes, `a &gt;&gt;= return == a` stated like that seems wrong to me (not the law itself, of course). But that wasn't my point. I just think that bottoms are only used as an "emergency brake" (as in `error`) or arise in miswritten programs (as in infinite loops), so there's nothing wrong in ignoring them most of the time. Now, evaluation order is important when "tying the knot" or using infinite data structures, but that techniques are only available in lazy languages, so the extra work and documentation definitely isn't wasted.
Biggest problem I have is googling Haskell and getting the England Rugby player!
OK, I'm going to have another stab at what you're going for, here. This time, you're asking for the total price, after the discount has been applied, yes? So totalWithDiscount = total - totalDiscount Thus totalDiscount :: Float -&gt; [Float] -&gt; Float totalDiscount x y = x * sum y totalDiscount 0.1 [4,5] == 0.9 and **totalWithDiscount :: Float -&gt; [Float] -&gt; Float** **totalWithDiscount x y = (sum y) - (totalDiscount x y)** So that totalWithDiscount 0.1 [4,5] == 8.1 As for listing the items, try this (not the most efficient way, but it's what came to mind first, and is two steps so you can see what's happening): itemDiscounts :: Float -&gt; [Float] -&gt; [Float] itemDiscounts = map .(*) discountedItems :: Float -&gt; [Float] -&gt; [Float] discountedItems x y = zipWith (-) y (itemDiscounts x y) By the way, your English is considerably better than my German, but you're not being fully clear about what your overall objective is.
It's elegant in a sense to have `\x -&gt; case x of y -&gt; y` to mirror `\x -&gt; let y = x in y`
&gt; Sometimes it's not about being sane or insane. It's about tackling a problem that interests you for fun. I learned a lot about writing an efficient pattern-matching engine (sparkler.js compiles to very efficient JS with excellent sharing). Kudos for learning. But, I expect you will admit that such projects are not practical. The basic issue is that one cannot stack multiple precompilers on top of each other, because it's likely that the syntax that one precompiler expects will make the other precompiler fail during parsing, which renders precompilers unscalable. E.g., will your precompiler survive the JSX input? I doubt it will. I also doubt it would work the other way around. Precompilers lately becoming a common practice in the JS world is what I'd actually call the insanity, as well as a decision of making a project depend on such unreliable technologies.
Yeah, those all rhyme for me too.
As a non-native speaker, I firmly believe that English phonetics is a work of the devil.
&gt; Kudos for learning. But, I expect you will admit that such projects are not practical. Well, frankly, who cares? I'm literally the only person that should care about the practicality of my own project. I can say the same thing about anonymous records in Haskell. The fact that its a preprocessor guarantees I'll probably never try it. I don't think you've lost anything by me not using it. BTW, in a sense I *have* moved on to other languages. I like to stay involved in the PureScript community. It's a Haskell-like language with anonymous records. You might like it.
All the freakin' time. Made worse since I'm South African and we have a dozen official languages and half a dozen distinct English accents, so it's reasonable to assume that if someone rhymes it with "rascal" they may just be pronouncing "Pascal" in a funny manner. And yes, I've heard first language English speakers pronounce "Pascal" with stress on the first syllable (like "rascal"). My wife's colleagues were having a random conversation at work, humble-bragging about all the computer languages which they had to tackle in university or technikon (vocational colleges), when she mentioned that I'm learning Haskell (at the time). They laughed at her heartily since they thought she said Pascal, and being a non-programmer she couldn't be sure if she could correct them. When she told me about her embarrassment later that day it turned into an opportunity to teach her a little bit of the history of programming languages. :-) 
To your other points. The entire reason sweet.js exists is because a powerful hygienic macro system solves the problem of composable compilers. My pattern-matching engine can be combined with any number of other syntax extensions that people have written. Many of which are extremely non-trivial. Obviously you need community buy-in or its just another precompiler. To do that you need libraries written to take advantage of it. Whether sweet.js will ever meet those goals in the long run is unknown.
&gt; I've heard first language English speakers pronounce "Pascal" with stress on the first syllable (like "rascal"). All the time. This used to annoy me, but at this point I figure it's just the way English speakers pronounce it.
It could be worse. At a meetup I frequent I introduced myself to someone and said I was a "Haskell developer". About a week later he messaged me and asked me to clarify because he thought he heard me say I was an "asshole developer." Enunciation is important! 
Right, this is why I mentioned "without inspecting thunks for any sort of tag" as the difficult part :-)
Indeed, as epitomized by ["The Chaos"](http://ncf.idallen.com/english.html) poem. Penned by a non-native speaker, I'm pretty sure.
OK, forget I mentioned tags. Are you asking for a version of `seq` where seq (undefined :: Int -&gt; Int) () = () and seq (undefined :: Int) () = undefined ?
At FP Complete, clients use Stack's Docker integration and we distribute closed libraries via custom Docker images.
This is why I keep digging up and playing w/ designs for a "Turbo Haskell" compiler. If it is going to be heard that way anyways I might as well maximize confusion!
It usually goes something like "Pascal??... Wow. People still use that?". Especially from people who learned programming at some point in their education but don't really use it. The irony is that Haskell isn't that new itself (though not as old as Pascal either) :-)
Haskell Curry was a native English speaker. My assumption is that he pronounced his name that way.
Here is my "proof" that IO is not a monad: https://mail.haskell.org/pipermail/haskell-cafe/2007-January/021536.html See also the fun and interesting discussion that follows. :)
Assembly is a very unnatural word in my native tongue, so we call it "assembler language" (or assembler for short) instead. It's the language you feed into an assembler, and the word assembler sounds reasonably native. I suspect this might be the case with German too, which accounts for a large number of speakers.
Didn't know about that. That certainly changes the perspective to a certain degree.
&gt; BTW, in a sense I have moved on to other languages. I like to stay involved in the PureScript community. It's a Haskell-like language with anonymous records. You might like it. That was my original point. That precompiling JS is about enough of an indication of the time to switch to another language. PureScript is a great project, but I like Elm better. &gt; Well, frankly, who cares? I'm literally the only person that should care about the practicality of my own project. I disagree, but it's up to you of course. &gt; I can say the same thing about anonymous records in Haskell. The fact that its a preprocessor guarantees I'll probably never try it. That was exactly the perspective I was talking about the precompilers to you from. The sole purpose of the project however was in research and to prove the usefulness of the concept so that it gets implemented in GHC. Also Haskell is far from a piece of crap of a language, which JS is, so it hardly ever needs preprocessors, which is why the likelyhood of the user ever needing to stack multiple preprocessors is way lower. That project was literally the first time I ever thought about implementing or using one in my experience. However, if the preprocessor is what scares you, all the versions prior to 0.4 were based on TemplateHaskell instead.
&gt; The sole purpose of the project however was in research and to prove the usefulness of the concept so that it gets implemented in GHC. FYI, the first POC implementation of ES2016 async/await was implemented with sweet.js. Pattern matching ideas have floated around in JS-land for years, but no one had ever made a reasonable attempt at an honest-to-god implementation until sparkler, so no one could actually see and try it out in the context of a real program.
That's just how the English pronounce it. Most of us anyway!
The Haskell Lamb!
Must... Not... Make... Projection... Joke...
*successfully* is the key word there. A computation that raises Exception is defined as being unsuccessful.
As a non-native speaker... this thread does not make any sense. Al least without IPA.
We really do need pronunciation reform.
I'd use slightly different terminology here: "proxy" usually refers to an argument that is used solely to determine a type, whereas I think what you have is a "singleton" because it actually represents a value that is passed at both the type and term levels. Unfortunately I'm not aware of a way to make the conversion of a singleton into the corresponding term happen at compile time. Hopefully as GHC gains better support for dependent types this kind of thing may improve, though.
I once told someone I was interested in roles focussed on functional programming. Their response: &gt; Oh you want to be a Product Manager! 
My systems programming professor actually claimed that C is a functional language, because "all code is inside a function."
Just applied, and had a phone chat with Levi. Very cool dude! Good luck to all candidates.
Yeah, nulls usually happen by someone's stupid decision, while bottoms usually happen by accident. I'm not sure if that makes one of them more acceptable than the other. The time bomb behavior is the same. IMO it's best to have a language that disallows both, and ensures that an integer is always an honest integer. SML or Rust are good examples.
He was an accent instructor, IIRC. The poem is meant to be didactic.
Of ES6 and JS? Preferably neither :)
I might be using it without being aware of it : what is the "top-down" solver ?
Happened to me once.
From the blog posts I have read TCO made it into the es6 or now es2015(I think it recently switched). Here is the [specfication](http://www.ecma-international.org/ecma-262/6.0/#sec-tail-position-calls). The transpires like babel do not fully implement it so it will be pretty cool once it is offered natively in V8 and other browser engines.
They don't even have to be government-controlled.
To be fair, the "function", "functional" etc have been used WRT functionality in various software related design terms for a very long time. Actually, "functional design" in that sense probably predates programmable electronic computers. I'm not sure, but I thought that although LISP was around quite a bit earlier, the term "functional programming" referring to referentially transparent functions didn't really arise until Peter Landin in the mid-60s. Actually, I vaguely recall in college a lecturer asking "does anyone know about functional programming" and I said something along the lines of "everything in C is a function" (meaning as opposed to Pascal, where there are procedures (different keyword to functions) that can't return a value). That was a prelude to a few hours learning the basics of Miranda. 
A time bomb isn't just a diverging program, it's something that blows up later than the actual point of failure. Time has passed and the stack trace is totally different. "Why am I getting a null pointer exception here? Where did this null originally come from?" Nulls and bottoms can lead to that kind of problem, but vanilla non-termination usually doesn't.
sorry - I might have missed the subtle meaning here ...
I understand your opinion but I'm aiming people that only use pure js, thanks
- I'm planing to ship a List object to work with recursion and mutability in the library too, any other suggestion is welcome! - as our friend said, tail call will be implemented by Javascript interpreters soon Thanks!
For the confused: * &lt;Pascal&gt; /pæˈskɑl/ * &lt;rascal&gt; /ˈræskəl/ * &lt;Haskell&gt; /hæˈskɑl/ or /ˈhæskəl/ The main difference here being the location of the stress: ha-SKAL or HASK-ell.
Hi, I just discovery pun today. But I'll stick with z, since the synthax is nicer in ES6, and I think that's more friendly to people that doesn't know functional programming and just want to discover pattern matching goodies at first Thanks!
`case` seems strict in core https://www.davidterei.com/talks/2014-05-cs240h/ghc.html
&gt;minElem :: [Int] -&gt; Int is the function basicly
What do you mean "use it with Integer and Double"? Do you want to produce functions of the form `[Integer] -&gt; Integer` and `[Double] -&gt; Double`?
i want to use my function for [Int] -&gt; Int with Integer and Double, is it just enough to write [Integer] -&gt; Integer at the beginning? sorry i'm a noob 
This is what I do.
Funny, when I mention it, people hear Beaver.
Worked for me. I copy and pasted results from my ghci (7.6.3). Moving the equality constraint out allows it to propagate outward (or otherwise be checked after instance selection) instead of being used when matching the type against the "instance head".
alright thank you very much
I was talking with a guy about functional programming and he told me that he learned modula-2. ...So, kind of yes.
Its a shame that GHC can't make instances (of any class) for universally quantified types like Lens'. It makes these types less useful. Does anyone see GHC fixing this in the future? I'm sure Ed considered this drawback in his design of 'lens', but alternative designs were probably worse for other reasons. One alternative would have been to create Lens, Prism, Traversal, Fold, etc. `data` types, and ALen, APrism, ATraversal, AFold, etc. classes arranged in the optic hierarchy, with appropriate instances defined for each data type. How much worse would this design have been? 
You're a student at Bielefeld University, first semester, right? Well, in any case, you probably want a [typeclass](http://learnyouahaskell.com/types-and-typeclasses#typeclasses-101) constraint to Ord (as in "minElem :: Ord a =&gt; [a] -&gt; a"). You probably won't even have to change much of your code (since Int is already in Ord). If you are still having problems, ask your tutor or swing by my [additional tutorium](https://ekvv.uni-bielefeld.de/kvv_publ/publ/vd?id=65192486), on mondays. We always tackle stuff anybody is stuck on. ;-)
No problem
For your consideration: http://nuenglish.net/articles.htm As a side note, I believe that English is going the way of French, with swathes of silent letters and difficult-to-pronounce vowels. If you don't believe me, take a look at the U.S. pronunciation of the following words: &gt; "Internet" (innöh-nehd) &gt; "water" (wahda) &gt; "lady" (la'e) &gt; "butter" (baddah) This in addition to already heavily elided, older words like &gt; "night" (nite) &gt; "weight" (wayh) &gt; "pointy-tooty, pull-and-shooty" (gun) This isn't meant as a negative judgment; I'm just describing my observations.
Here in Brazil we have billboards spreading the name "Haskell". Unfortunately, they always refer to [a local cosmetic company](http://www.haskellcosmeticos.com.br/en).
Possibly no-one. I can't recall ever hearing &lt;Haskell&gt; stressed on the second vowel, but /u/jaybee implies that as a potential variation. Pronouncing &lt;Pascal&gt; with /ɑ/ is definitely a thing here in the UK, though.
[diagrams](http://projects.haskell.org/diagrams/gallery.html) is probably a good start.
Indirectly related: http://functional-art.org/
A good but somewhat outdated book is the late Paul Hudak's [Haskell School of Expression](http://www.cs.yale.edu/homes/hudak/SOE/). You can [preview some of the text on Google Books](https://books.google.com/books?id=db42AAAAQBAJ). The only issue as a beginner is that some of the libraries it uses have not been updated in a while and so you might need to set up a virtual machine to install old dependencies. That being said, do check out [diagrams](http://projects.haskell.org/diagrams/gallery.html) as /u/maxigit mentioned and maybe [the elm language](http://elm-lang.org/) (Haskell-like, but potentially helpful).
indeed, but core is not haskell
You might like [this blog](http://mathr.co.uk/blog/). Some (but not all) of the artwork there is generated from Haskell code.
Does not work in GHC-7.10.2 with lens-4.12.3. The types I get are more general &gt; :t x x :: (Functor f, Lens.Profunctor p) =&gt; p Int (f Int) -&gt; p Foo (f Foo) &gt; :t getter x getter x :: (Functor f, Lens.Profunctor p, SomeLens (p Int (f Int) -&gt; p Foo (f Foo)) s a) =&gt; s -&gt; a &gt; getter x (Foo 0) &lt;interactive&gt;:20:1: Could not deduce (Functor f0) from the context (Functor f, Lens.Profunctor p, SomeLens (p Int (f Int) -&gt; p Foo (f Foo)) Foo a) bound by the inferred type for ‘it’: (Functor f, Lens.Profunctor p, SomeLens (p Int (f Int) -&gt; p Foo (f Foo)) Foo a) =&gt; a at &lt;interactive&gt;:20:1-16 The type variable ‘f0’ is ambiguous When checking that ‘it’ has the inferred type it :: forall a (p :: * -&gt; * -&gt; *) (f :: * -&gt; *). (Functor f, Lens.Profunctor p, SomeLens (p Int (f Int) -&gt; p Foo (f Foo)) Foo a) =&gt; a Probable cause: the inferred type is ambiguous 
Ah, the Profunctor generalization is screwing it up. &gt; :t (x :: Lens.Lens' Foo Int) (x :: Lens.Lens' Foo Int) :: Functor f =&gt; (Int -&gt; f Int) -&gt; Foo -&gt; f Foo &gt; :t getter (x :: Lens.Lens' Foo Int) getter (x :: Lens.Lens' Foo Int) :: Foo -&gt; Int &gt; getter (x :: Lens.Lens' Foo Int) (Foo 0) 0 `makeLenses` does not really create a `Lens` type it makes something more general, an `Optic`. So the following instance works! instance (p ~ (-&gt;), f ~ Lens.Pretext' (-&gt;) a) =&gt; SomeLens (Lens.Optic' p f s a) s a where getter = flip (Lens.^#) setter = Lens.storing &gt; :t getter x getter x :: Foo -&gt; Int &gt; getter x (Foo 0) 0 Thanks so much for your help! 
U.S. pronunciation is not that uniform. 
We found a solution! {-# LANGUAGE FunctionalDependencies, FlexibleInstances #-} {-# LANGUAGE TypeSynonymInstances, RankNTypes, TypeFamilies, TemplateHaskell #-} import qualified Control.Lens as Lens -- from package 'lens' import qualified Control.Lens.Internal.Context as Lens (Pretext') class SomeLens x s a | x -&gt; s a where getter :: x -&gt; s -&gt; a setter :: x -&gt; a -&gt; s -&gt; s instance (p ~ (-&gt;), f ~ Lens.Pretext' (-&gt;) a) =&gt; SomeLens (Lens.Optic' p f s a) s a where getter = flip (Lens.^#) setter = Lens.storing data Foo = Foo {_x :: Int} Lens.makeLenses ''Foo getX :: Foo -&gt; Int getX = getter x 
On the other hand, it has everything to do with the *definition* of functions, which other programming languages revised when they stole the word from math.
It's possible to load object at runtime by [plugins](https://hackage.haskell.org/package/plugins-1.5.5.0/docs/System-Plugins-Load.html). The downside is you have to run loaded functions and modules in IO monad.
To correct this problem say: 'Hask' *breath* 'ell' Make sure that the 'e' in 'ell' is a hard 'e' so that the phoneme 'ell' sounds like the pronunciation of the letter 'L'. The difference in cadence along with the difference between Haskell's hard 'e' and Pascal's soft 'a' seems to remove the phonetic confusion from most listeners.
Start with "Do you know Monad?" and show them how monad transformers are composable with aspects. It would rather look novel and interesting. 
As a native speaker, I'm in the same boat.
Even Haskell functions aren't really functions, in the categorical sense of the word. At best they're partial functions, or pointed functions, since they can return "undefined". EDIT: And also, as /u/adribar pointed out, Haskell functions are non-strict, which makes them slightly different from ordinary partial functions in that `f ⊥` may not be bottom.
Fair enough, my pronunciation of German words is no doubt also rather odd :-)
Anyway you can do nothing with pure functional. What you want to mean is that Haskell is a better imperative language. Yes it is
"Going the way of French"? English has been this way since the great vowel shift in the middle ages and we stopped pronouncing knight as ker-nicht-eh and all that. 
It's a playground, not a module. And that's just marketing speak. It probably does have the most advanced type _inference_ system on the planet ;)
(right, I meant it might be the source of their mistake.)
Ehh... `undefined` is named for how the programmer should use it. It still evaluates to a defined value, ⊥ (bottom). It's perhaps a mathematical *oddity* that all types in Haskell are defined to contain ⊥, but does that really change anything else?
FWIW (not much!) here's a cafe post from someone pronouncing it with the stress on the second syllable, if I interpret it correctly: https://mail.haskell.org/pipermail/haskell-cafe/2008-January/038757.html
I take your point. Dadda-base is one that gets on my wick :-)
&gt; buder No, but you could write it as "buddah". Maybe you could then go on to found the religion of Buddhism, preferably in Wisconsin. :)^please^don't^hit^me
The relation is purely phonetic.
Esperanto's where it's at.
Ask an Australian to count to twenty and listen to Ts and Ds.
*grumble grumble* If and when it gets evaluated then. In any case, non-strictness is just another way Haskell functions are not like mathematical ones.
I don't get it... &gt; Pass the data that shows the invariant holds. So I'd pass `take 1 xs` or `listToMaybe xs` rather than using `head xs`. ? &gt; This means that instead of e.g. running `isJust` and then `fromJust`, you pattern match and pass the inner value. Is that what it means? And how does pattern matching help? If you don't consider ask cases you're partial. I am sure I am missing something...
please teach me not just answer
Well, a program in any language is just a state function. The question of which functions are allowed seems pretty crucial to the definition of the language.
[Haskell formally resolves to gain more autonomy from federal government](http://www2.ljworld.com/news/2015/oct/08/haskell-formally-resolves-gain-more-autonomy-feder/?city_local).
A lot, actually, but I usually find they mistake has-KELL for Pascal more than HAS-kell.
I saw you post another homework-style question recently. Are you in an introductory Haskell class?
I can't sell you on Haskell, there might be some DSLs/libraries in Haskell that do this well (perhaps even just with the monad over `[]`) but if you're looking for a language that deals well with this kind of problem, I'd poke my nose in with the logic programming paradigm. You're effectively trying to constrain a set of results based on how good of a rhyme it is. EDIT: You might still want to poke at Haskell some time to see if it is a "better Python" for you. It is for some people.
Ah, that's a fair point. (Hehe. Pointed.) Haskell functions are at least accurately "morphisms in the category Hask," tho, right? So... Morphismic Programming? :)
Have you read any books on Haskell? There are a few serious errors in the code you posted. I know you might feel pressured to learn quickly because of your job, but it is really hard to learn something complicated like a programming language without starting from the basics. The essence of learning to program (or anything technical, really) is building up pieces of reusable knowledge. You have to have a firm grasp on the basics before you can do anything complex.
Check out the sidebar on the right for some links. The canonical example is of course [Learn You a Haskell](http://learnyouahaskell.com/).
Another option is to use the `Applicative` style with your first `formatMessage` definition: farewell = do formatted &lt;- formatMessage &lt;$&gt; asks farewellText &lt;*&gt; asks screenWidth liftIO $ forM_ formatted putStrLn
I would argue that [equational reasoning](http://www.haskellforall.com/2013/12/equational-reasoning.html) is the unifying principle behind Haskell. Once you understand equational reasoning a lot more things will make sense like: * why Haskell uses the `=` sign to define functions * why we minimize shared mutable state and instead program in a purely functional style * why side effects are decoupled from evaluation order * why highly generic functions are a good thing * why mathematical interfaces are better than ad-hoc interfaces
Multiple-paradigm languages certainly don't, but the good ones do. Even if they don't have a single unifying principle, they have multiple principles that cohere, and are consistently applied throughout the language. Lisp and Smalltalk are this way, and I think that Haskell is too. I just haven't found the right abstraction(s) to describe it.
Here are two fundamental principles. 1. **Functions are first class**. That's the core principle of functional programming in general. Rather than "everything is an object" you have "functions are values." Whatever you want to construct something, you can think of how you build it out of functions. So you build things out of verbs, not nouns. This principle applies to scheme as well, mind you, and I first learned it from SICP. 2. **Types can induce values**. This is a principle that you can find in other ways based in fancier type theories, but in Haskell it really flows from type classes. Consider the `Read` class -- depending on the flow of type inference, the same `read` can parse an `Int` or a `Float` or a `Maybe Bool`. This lends itself to a style of very general, polymorphic programming where functions are no more specific than they actually need to be. This style of programming is very convenient in the Haskell-family of languages, and different than the way people might approach things in an untyped functional language or even something like ML or OCaml, where different abstraction mechanisms (such as modules) would be used instead.
While my use of Haskell is limited to small side projects, the power of Haskell, like some others have mentioned is the purity aspect and the fact that it is strongly typed, with a powerful inference mechanism. It makes reasoning about the behavior of functions much easier. Also, one of the big pluses that's always been touted about OOP is the reuseability of classes. I think anyone that's programmed in the OO paradigm long enough will feel like this promise falls kind of short. Once you start programming in a FP like Haskell, esp. a pure one, you'll truly understand reusablility better, since unlike objects, functions compose. This leads to naturally structuring complex computations from simple, digestible pieces. This is huge: since Haskell puts a wall, so to speak between the effectful world and the pure world, once you know your smaller, pure functions work as you intend, composing them together leads to a bigger function that also "just works". They're not going ti do any funny stuff on the side, like increment a global variable, or launch missiles, or anything. It's a great feeling when it all just works like it should. Not to mention this kind of behavior both removes certain classes of bugs entirely and obviates the need for unit tests. And QuickCheck! There's really nothing like that in the OOP world that measures up. The type system also guides programming intuition in a really natural way. To paraphrase Ben Pierce, "let the types guide you". When you have a programming hole to fill, you try to find the function with the type that matches the type definition you need and fill it in, and it usually does what you need. And while I'm evangelizing, state is the enemy in any large scale project. Languages like Python and Smalltalk have exploit it, Haskell tames it and forces you to deal with in a deliberate and principled way. Ok, that's the end of my rant /s
Well that, and we could have a `thc` that's legal to use in more U.S. states than just Washington and Colorado.
You got the French to thank for that mess. 
What is the sanctioned way?
why do you call this `BigNumbers` where it obviously a sorting algorithm? Also `[4,5,800]` and `[4,5,8]` seems to be sorted - where the `200` and the `Boolean` should come from I have no clue --- also I still am curious on what company in Munich is looking for Haskell beginners or willing to get them started on the job - ofc you can give me a hint via PM if you don't want to go public (I really would appreciate the hint)
the devil? one single devil? More the work of a commitee of devils, each pushing his own agenda. Furthermore, they never released new versions of the standard even when pronunciation changed and they were out of date.
what have the french have to do with this? we're not talking the pronunciation of "lingerie" here. Just look at the change in the way things are pronounced going from the south of the UK up to Scotland. Just pure English words, "like", you know, "lake". And very little French influence here as well (the website is in the .fr domain just to confuse the issue): http://pauillac.inria.fr/~xleroy/stuff/english-pronunciation.html
I agree, which is why I also tried to be informative :)
I have to stop being so grumpy old man-ish.
Most common among whom? I've never heard it that way from any source, except these threads talking about how rampant the confusion is! Where are you from?
Calling it an unifying principle might be a bit much, but one thing that is, I think, unique to Haskell, is treating complex objects as just dumb data with some attached meaning. For example, if I have a function `a -&gt; b -&gt; c` I can easily map it across an `IO a` and an `IO b` to get an `IO c`, or `Parser`s, or some `Foo` as long s it implements the `Applicative` type class, and let the implementations handle what does it mean to combine two (or more) values like that. There's of course much more than that, as described in the [Typeclassopedia](https://wiki.haskell.org/Typeclassopedia).
Part of that does have to do with the Normans invading Briton in 1066. Before that English was far more Germanic. Just hearing Middle English from an episode of Sleepy Hollow sounded more German than it did present English. I think part of what you are referring to is more common of accents than pronounciation as a whole, but I don't get out much these days.
Monads are about the composition of kleisli arrows. Dan Piponi sums this up nicely in a [blog post](http://blog.sigfpe.com/2006/06/monads-kleisli-arrows-comonads-and.html?m=1). There is also a video on youtube called "don't fear the monad" where the same point is made. It's just so hard to see because the bind operator does all this wrapping and unwrapping an is asymmetric. The kleisli composition (&gt;=&gt;) is much more intuitive. 
&gt; The closest answer I've found is referential transparency, but it just doesn't seem as powerful an abstraction as the object model. Save this post and check back on this statement in a year or two. You will look back and laugh at it. Once you have programmed in Haskell for a while, you become hyperaware of all state you use in other languages. While you are rarely concerned about the particular ordering of statements inside a method, what you don't realize is how often you end up using state as a signaling mechanism. This makes *writing* code easier in the short term, but as you scale, these hidden dependencies become the primary source of uncertainty in how your program works. It's hard to come up with simple examples of this, since it happens when your program exceeds a certain level of complexity. However, any C++ programmer should be fully reverent of the threat of memory leaks, and even in the presence of garbage collection, it should be clear that those same kinds of issues can arise through more sophisticated resources (sockets, memory mappings, database handles). And any programmer who has ever created a UI for an application knows the frustration of the user's controls entering an unintended state. While referential transparency does not prevent these kinds of problems, it does give you a stricter default. If you are going to write effectful code, you have to be explicit about it. The culture helps push you away from using these tools unless you are in dire need. And in the absence of effects, you can apply equational reasoning to refactor code without the uncertainty that you have when working in other languages. &gt; The unifying principle of Smalltalk is object-orientation. Everything is an object, these objects have state, and they are nouns, not verbs. I would argue that this is not even really true. It is a story that is taught to young programmers. For one thing, what is an "object". A better name might be an "actor" or "agent". But even in functional programming, everything is still an "object". Even better than having an internal state, everything in Haskell is simply a value. It has no moving parts inside. To tie this point to the last point, actually, think about hardware for a second. It is generally agreed that solid state hard drives are much more reliable than spinning disks. The reason is the spinning disk has moving parts inside. More moving parts means the thing is easier to break. The advantage of OOP, though, is it is often a better model for building systems. The idea that distinct actors can pass messages between each other to perform tasks is very useful when you want to, say, design an operating system. The printer, the network, the graphical interface, the disk all act more-or-less independently of each other, but need to cooperate to get the job done. The advantage Functional programming, in contrast, works best when designing software that is isolated from outside interference. Functional programming languages are undoubtedly the best choice for compiler design. Text goes in, an executable comes out. Everything else that happens in between, the outside world need not worry about. And to the compiler, data is available, unmolested by the concerns of what the user is doing in his web browser for the 1-300 seconds it takes for it to chug away at optimizations. But there are lots of good take-aways from Haskell. Beginners are often amazed at the power of higher-order functions (which is less impressive now than it was 10 years ago thanks to their acceptance into the mainstream languages). Another big eye-opener is pattern-matching and algebraic datatypes. Still more big ideas are buried away in various libraries: * STM in Haskell seems miraculous to a C++ programmer who is used to working with locks. * Free monads offer an excellent technique for writing DSLs, with the option for multiple interpreters (so you can switch between production mode, logging, mocking, etc). * Parser combinators make you feel embarrassed of yourself that you ever thought regular expressions were a good idea. (And you realize how easy it is to write lightweight parsers for your problem domain). And beyond this, Haskell is a gateway to more advanced functional concepts. There is a lot of interesting research going on in language and library design which simply isn't possible when you work in Java. In Idris, for instance, there is research into protocol typing. This is where you give a specification for a given protocol (say, a TCP/IP handshake), and then the type checker guarantees that whatever code you write is in compliance with the protocol, rejecting programs which might violate it. Another language, Ur/Web, is designed to be a functional language whose compiler is able to perform very sophisticated typechecking and optimizations for web applications. It is able to, for instance, validate your HTML and SQL queries are well-formed and well-typed at compile time, eliminating injection attacks without programmer intervention.
BTW, I (the author) am not a category theory expert. If anyone has any corrections, I'd like to hear about it.
Wow - that's the first time anyone not on the same uni course as I was knew what modula2 was, _ever_. 
That is not what people mean when they say functional. People tend to call languages like Clojure functional too.
To be fair, C is procedural, which almost sounds like functional since C programmers calls procedures "functions".
&gt; The idea that distinct actors can pass messages between each other (...) I don't want to sound like some functional zealot here, but I don't think OO actually fulfills this promise. OO messages are nothing like the ones in, let's say, Erlang, but it's just a fancy name for a method call - with all hell breaking loose when you actually want to have independent actors working concurrently. I'd say that Haskell, with streaming libraries like `pipes` or `conduit`, with STM (that happens to be more composable than message passing anyway), `async`, or [transient](https://hackage.haskell.org/package/transient), is much better suited for modeling concurrent systems than OO.
Aside: I'd argue that the unifying principle of Smalltalk is **messaging** *(which not the same as method invocation, and that's where Java screwed it up)*. Smalltalk's objects exist to empower the message-passing paradigm, and not vice versa! I'd go so far as to say objects in Smalltalk are analogous to Haskell's `IO` monad: they're the real-world necessity which lets you actually *do* something with that beautiful underlying model.
Niklaus Wirth is a) not a native English speaker and b) I would expect him to have a hyper-correct, French-like pronunciation of his own language's name. More relevant is ordinary non-expert English speakers' pronunciation. But it does seem to vary regionally. This is obviously only a problem for people who have my pronunciation.
The closest you can get here are these three points: * The language should be lazy. This is why Haskell remained purely functional despite embarrassing early implementations of it while REBOL dropped that early in development. If you're not purely functional, lazy evaluation is hard to reason about. * The implementation should break things. Backwards compatibility should not introduce strange quirks. As far as possible, get things right the first time. This is what the maxim "avoid success at all costs" is about. The third though, is closest to a "general unifying principle": Haskell was meant to consolidate the fragmented space of lazy functional languages into a single one available to researchers for them to tinker with. The most prominent effect of that is probably the existence of `LANGUAGE` pragmas, which allow various variations on the language to be rapidly introduced and dropped as needed without changing the standard itself. But this isn't so different from other languages. Most of these unifying principles are vague. For C++ you could point to the notion that code that doesn't perform well should be syntactically heavy (not that C++ is great at this, but the idea is there). For Smalltalk, perhaps it is more that all code should be hot-swappable and fail gracefully when it has to (message passing is just a way to achieve this). As far as functional programming itself is concerned, we could consider Haskell as a functional language because it is built to make it straightforward to translate "functional programming" in the specification/architecture sense to code, more so than the likes of FORTRAN or ALGOL. That could be seen as another unifying theme, albeit one that predates Haskell considerably. EDIT: What you are asking about is more about what is the most common, and important abstraction. In Haskell, this would definitely be the pure function. It's no less "powerful" than objects and messages, you can talk about these things in terms of function application and evaluation.
This isn't good style. You should only use it when you actually need access to it, not just in case you could need it. The type signature should tell you what is going on as much as possible.
&gt; but I don't think OO actually fulfills this promise. Really, I agree. The promise made by OOP really describes Erlang better than it describes Java or even Smalltalk. Smalltalk can be forgiven, at least, because of the approach it took. In the Smalltalk environment, you were able to invoke methods on any object in what was effectively your own little operating system. I think typed languages like Haskell have a conflict of interest when it comes to systems-level concurrency. Namely, a type system is most effective when it accurately captures the rules of the system. But the larger the system, the fewer hard rules exist. An operating system needs to be able to handle hardware failure, buggy software, and malicious actors. There are certainly type systems which can model these features, but they tend to devour your complexity budget. The traditional approach is to simply toss out the baby and bathwater together and adopt Python. I'm not saying I agree with that. But you have to give credit that that kind of naive "Just fucking do it" mentality has some advantages over spending time writing typesafe code which fails to meet the dynamic, culturally-driven demands put on the system.
Bjarne, Larry, Matz, Anders, and Guido probably all believe that they have good unifying philosophies behind their babies. Offering a limited set of abstractions isn't the only way to do that.
Without even having read the paper, this is amazing.
&gt; Java's is that "everything is an object". If that's Java's unifying metaphor, it's doing a pretty bad job at it.
That looks really interesting, since the paper is not linked on the website you can find it [here](http://compilers.cs.ucla.edu/popl16/popl16-full.pdf).
Exactly, and in this case it would tell you that the function might have a different effect with different configuration.
Aha! I was looking for that link earlier. :)
And if you're willing to go further up the rabbit hole, there are languages like agda and idris which can encode the problem into the types of your program. It can make basically make your program a proof of its correctness.
I would argue that English speaker pronunciation is irrelevant for a language designed by Niklaus Wirth who is from Switzerland named after Blaise Pascal, who was French.
No, because we are talking about people (English speakers) getting confused by the similarity of the word "Haskell" to *their* pronunciation of "Pascal". It doesn't make sense to tell someone "you aren't actually confused, because Pascal is pronounced PasCAL."
IMHO The unifying principle is: Everything is a mathematical term Everything in reality is mathematical so it can be captured in a math expression. Since it is a term in an algebraic expression, in practical terms, everything can be first class and composable in an EDSL that captures the algebra that your problem obey. If it is not composable (i.e no algebra where the elements of your problem are first class) is because you have not solved such problem functionally. This is beyond what many haskellers use to consider "equational reasoning" , as combinations of pure functions. On the contrary, it includes efectful "non pure" , non properly functional, non commutative algebras, where order is important. For example: basic ones like monadic, applicative alternative ones, that in general any problem could obey. www.cs.ox.ac.uk/jeremy.gibbons/publications/mr.pdf
But it **is** pronounced PasCAL. I coded in it for many years (it was the 1980s!) and never met anyone who didn't pronounce it that way.
Yes, I also think that the distinction is artificial
Thanks! I've been trying to find the pdf of this paper for a week now! I'm going to try to work through it now, but this seems like an amazing result. It destroys a folk lore / conventional wisdom that we've long held about language embeddings. (Or at least spells out and validates an ouroborous of soundness that we've all wanted but haven't been sure exists!)
I think that register sized data types are the unifying concept in C. It's just that we forget this because they're so common in other languages too. Since that isn't very interesting in itself, it pulls in a very spartan memory model and calling convention, both of which also tie in nicely with the register sized primitives. 
You have inspired me! [All my friends are Monads](http://i.imgur.com/gxcPXtJ.png) (a joke based on [this](http://www.amazon.com/All-My-Friends-Are-Dead/dp/0811874559))
So all of us who have the problem this thread is about are just imagining it? If you don't have this problem, LUCKY YOU.
We do know *exactly why* some transformers don't compose and we can write those reasons down and *prove* them. That's still a lot better than what OO can give you
I don't understand how using Python or Java or C++ do *not* suffer from those same insecurities. Most programmers only focus on happy program flow. State just makes it really easy to slip into unhappy program flows *on top of* memory exhaustion, network cables being severed, etc. I mean, doesn't Haskell have exceptions in place that can catch a stack overlow exception, or a memory exhausted exception?
Can you link me to someplace I can learn about free monads?
Static typing is used to prove properties of programs. In powerful type systems, you can prove an interpreter is correct with respect to its specification. A language L generally can't prove correct interpreters for any language more or as complex as L without resorting to manual proofs, due to Goedel, ie. straightforward self-interpreters are impossible. Some tricks have been proposed over the years to circumvent some of these limitations, and this may be another one.
&gt; One thing that jumped out at me, /u/kamatsu, was that I expected patches to be type-indexed (I may be misusing the term), capturing both the source and the target of the arrow, forcing composition to be tail-to-head. That is certainly possible, using some phantom type to represent the documents perhaps, but one of my goals with this project was to write code that was simple enough to be expressible in basic Haskell with minimal extensions. I also wanted it to be simple enough to demonstrate to students in a course we have at UNSW which involves, in part, lightweight specifications of software with QuickCheck, and it's harder to write QC properties as types get more complicated. &gt; Is it even meaningful to ask what rebases (as opposed to merges) look like? In this patch theory there's no notion of a history DAG like in Git. It's just a linear sequence of patches. So, in a way, all merges are rebases. We can move patches around using transform anyway, so we don't even keep track of which version each patch came from, it's just not a necessary piece of information. &gt; Are there associativity and commutativity properties that fall of for merges due to the pushout construction? The commutativity of the transform function depends on the commutativity of the merge resolution operator, as I mentioned in the article. Associativity just comes from the standard composition associativity: You can prove that two subsequent transforms of a patch q against a divergent patch p1 then p2 is the same as the transform of q against the composed patch p1;p2. So as it respects composition two merges can be done in either order and you'll get the same result. 
Are you in touch with darcs folks about this? Looks like Ganesh and Florent would be very interested, and you did mention pijul in the references (I'm afraid I haven't had the time to read this properly, and may not have the background to understand it, except for the darcs side of it, maybe. I just remember "pushout" a lot when Florent presented pijul during the Paris sprint in September.) Are you in any sort of position to come talk to us in Seville in January?
I've only ever had the briefest of discussions with the darcs folks a few years ago. I first learnt about patch theory from Alexandré Mah, who was one of the people behind the patch theory and operational transformation used at Google for Wave (and presumably now Docs). I discovered this particular categorical analogue of my patch theory after I had implemented it, and some folks on twitter pointed me to pijul, which appears to use a theory that is, if not exactly the same, extremely similar. As a poor PhD student in Sydney on non-patch-theory-related topics, I fear Seville would be out of my reach ;)
IMO, this is the most accurate explanation in this thread of why Haskell was designed the way it was.
Thanks.
I find it very amusing. What is wrong with it?
Some languages, like Smalltalk or Scheme, are like small shining jewels that express one idea perfectly. Haskell is not like that. It's a pretty big language. I think of it as the result of three independent choices: 1) Lambda calculus with a Hindley-Milner type system and algebraic data types. That's an amazing combination of ideas that are pretty much meant to be together. All ML family languages, including Haskell, share this common core. 2) Laziness. Haskell was originally designed for research on whether adding (2) to (1) is a good idea. That question is still controversial, with weekly flamewars on /r/haskell alone :-) 3) Type classes. Another controversial idea that could've just as easily gone the other way. Our resident advocate for them is /u/edwardkmett, a very prolific programmer.
Oh sure I agree. I myself do the same. But the problem with "total programming" is how do you address partiality. E.g. what do you do if you know there is no else. For a moment it seemed like you had some way retrieve values out of some partial monad, in a total manner, via some witness proving totality. But that seemed to dependently typed for Haskell.
The two go hand in hand, but I'd say more of a prerequisite than a consequence. In order to be able to do lazy evaluation effectively, you have to drastically limit where side effects can appear in your code. Otherwise you'll have a very hard time getting your side effects to happen when you want them to happen.
Why, thank you. I know what I'm reading tomorrow :)
&gt; OP, can you say something about conflict handling strategies? For my intended use cases, I don't much care how conflicts are resolved. My conflict-handling strategies do indeed basically consist of throwing my hands in the air and letting the user handle the conflict manually. Keep in mind, I'm not looking to further the state of the art here. I wrote this library with a specific goal in mind, and I've achieved that goal. If anyone wants to improve on my conflict resolution, _patches_ are welcome :)
Yes, it happens to me often. It is even worse when I am in Japan, the letter "HA" and the letter "PA" are the same, except "PA" has a little circle next to it. So transliterating both Haskell and Pascal in Japanese are identical except for a little circle, and except for the first syllable, are pronounced identically.
Links please!
To /u/dream-spark and other newcomers: You can learn a lot by reading through /u/Tekmo's blog (linked above) and raising any questions you have here on Reddit.
Coming from a background in dynamic languages (Python, Javascript), the concept of the *total function* completely blew me away once I grasped it. In other words, the idea that I could completely define a function for all possible inputs (and *impossible inputs* would merely be disallowed by the compiler) and that it was also possible to prove that my function was correct using this method of equational reasoning, provided each smaller piece followed the same rules. Once I realized that this was possible, it became obvious that I should wish to write a majority of my code in a purely functional style using as many total functions as possible, which meant that when I later considered bugs in my code, I would have potentially reduced my problem space by a great deal because I can mostly ignore all that purely functional code. This seems huge to me. It's a contract that states, "Write as much of your code like this as possible, and you can be pretty sure it works (assuming you have eliminated semantic bugs)." I haven't encountered anything like this in other languages. I don't know if I did a very good job of describing it, but the two concepts together strike me as a pretty powerful. I'm still working to catch up to a lot of the theoretical stuff in Haskell, so maybe the *total function* is implied somehow in one of these larger concepts or maybe, being a neophyte, I'm naive (and overly optimistic) about the power of total functions?
Is there *any* formal definition of what a merge actually is in patch theory? I might be mistaken, but I seem to recall that all the papers I've read on the topic basically throw their hands in the air at this point and say, "well, it's something that does a vaguely defined right thing."
Let me see if I can make some of this notation more accessible to Haskellers (or at least Template Haskellers): `bar(-)` means that `bar` is a function with a single parameter "metafunction" means that it's not a thing existing in System F-omega itself, rather it's "outside", in whatever "metalanguage" we are using to work with / talk about / manipulate / prove things about System F-omega and the things in it (whether that's Haskell, Agda, HoTT, set theory, ...) `tau` is a conventional name for a type variable (the type-theoretic analogue to Haskell's `a`) If I'm understanding things correctly (correct me if I'm not!), `quote(tau)` is the type of ASTs which, if interpreted, would evaluate to a result of type `tau` -- much like Template Haskell's [`TExp`](http://hackage.haskell.org/package/template-haskell-2.10.0.0/docs/Language-Haskell-TH.html#t:TExp). `unquote`, then, is like a Template Haskell splice `$()`, and `requote` is like a [`lift`](http://hackage.haskell.org/package/template-haskell-2.10.0.0/docs/Language-Haskell-TH-Syntax.html#t:Lift) that returns `TExp t` instead of `Exp` (or is it like a simple typed TH quote `[|| ||]`? I can't always keep these straight...) Finally `|-&gt;` and `|-&gt;*` are variants on "evaluates to", the first is "after a single step", the second is "after fully evaluating it"
Painfully true
The pseudocommutation operation in [Homotopical Patch Theory](https://www.cs.cmu.edu/~rwh/papers/htpt/paper.pdf) is the closest thing I've seen to a formal definition of merge. It is fairly clear that some "extra" structure is needed in order to define merge. To me, merging feels analogous to parallel transport on a vector bundle which requires (and is equivalent to) defining a connection on the vector bundle. That is the extra structure. Two patches commute only if they are "flat". Of course the space of patches doesn't form a smooth manifold, so the analogy doesn't go very far.
you can also see it as a consequence if you look from a different angle. If your language uses strict evaluation, the temptation to add side effects is just too high :)
I'm pretty sure it'd couldn't be for all representations but I can't immediately prove why so *shrug*
Pancito, Pan [https://wiki.haskell.org/Applications_and_libraries/Graphics] (https://wiki.haskell.org/Applications_and_libraries/Graphics) 
Oh, I agree completely with that.
See the last paragraph. Also, here is [an article in the Register](http://www.theregister.co.uk/2015/11/05/wiegley_new_emacs_maintainer/), where it is mentioned that John was appointed to the position by RMS while "in Boston for a Haskell meet-up." Congratulations, John!
Another big part of it is that the original Haskell language committee when faced with the choice of two ways to do things with reasonable trade-offs between them, that didn't complicated the internal behind the scenes theory took both options. * We have both let bindings and where clauses. The former is an expression, the latter can scope over pattern guards. * We have both pattern matching for multiple arguments and explicit case statements. * More recently, we now have both functional dependencies and type families. Each of them desugar to the same kinds of things in the back end, but embrace a bit of syntax complexity. Haskell was designed to build a standard language for exploring laziness in the first place. Haskell without laziness is removing the very reason why the language was created in the first place. There are pragmatic reasons for such a language (e.g. Purescript) to exist, but they aren't Haskell.
"Haskell actually uses finger trees for their sequence data structure, the workhorse of the language." Well. Thanks for the reference to Haskell. `Data.Sequence` is indeed important, and it indeed uses finger trees. But I wouldn't exactly call it "the workhorse of the language".
don't cosmic rays flipping bits "ruin" referential transparency too?
Best one I've seen is How To Design Programs. http://www.ccs.neu.edu/home/matthias/HtDP2e/
I get giddy whenever I see a post by /u/gelisam that starts with "Let's figure it out!".
I would suggest the Typed Lambda Calculus (aka "system F"). After type checking, the first step in compiling Haskell is actually to "desugar" it into an intermediate form that is basically System F.
However if you have significant state then totality becomes much harder to achieve. You have to treat the entire program state as a hidden parameter to each function and demonstrate totality under all possible states as well as all possible arguments.
Congratulations!!
Puts me in mind of a manager I once talked to back in the 90s. He knew I was into OO, so he said "Our project is fairly object oriented. We have 3 objects..."
I first ran into haskell whilst raiding the jwiegley dotemacs settings, and shortly after remember swapping notes about the difficulties of getting changes through the emacs-devel. I gave up a long time ago but John obviously went for the long-haul, tenacious approach.
&gt; English phonetics That's not really the issue. Pascal is not pronounced by English rules because it's French.
[removed]
That seems fair. My experience with static typing is entirely in C, which definitely feels quite a bit different from Haskell. Plus, as you said, the idea that "typically exceptions are used to indicate failure," seems like a dramatically different style of programming, one that provides far less comfort than the equivalent total function in Haskell. Still, you are probably right that moving from mostly working in dynamic languages has me perhaps too impressed by this aspect of the language. This is why I was hoping for some feedback on this point, actually, especially when I didn't see it mentioned anywhere else in this thread.
Haskell school of expression is a very well written intro book. If you are into that kind of stuff (animation, music, etc), then this is the best book for sure. The libraries are fine (just cabal sandbox it -- I did it one or two month ago on Debian 8). You need to go to the author's webpage to see instructions on installing library. I played with most of the examples. They work without issues. However, in the very beginning (maybe even the first example), the example is to show a graph and press a key to exit... there is one bug that prevents the program from exiting. You can stackoverflow it -- it's an easy-to-fix bug. Other than this exmaple, I haven't noticed any other bug. 
I really need to know what you needed this function for. Also, you might want to dig into recursion schemes if you haven't already. While I don't know if any of them directly provide that operation, it's the sort of style they embody.
Well done !! do share the code please !
I think it communicates intent more clearly. If you have a value that's never going to change, throw it into a reader. It gives the next developer a little bit of a hint as to what you intend to be "mutable".
The idea behind Reader and Writer is to model a separate "channel" for configuration and output information, respectively. On the other hand, State might be anything that should not be visible to the user.
It's in the video description and I just posted it below/above. http://lpaste.net/144907
I don't think any of these are quite relevant. I'm not looking for an intro to programming, especially not from precise specs like AoP outlines. I'm looking more for something discussing how to go from spec-less, informal examples of the intended behavior down through calculation to see what options exist for the solution. i'm not even sure that any sort of spec could exist for the example problem, since the function-to-be-defined isn't itself readily identifiable at the outside. just knowing `magic'` is the "goal" isn't sufficient, because `magic'` really is just a particular instantiation of the more general `magic`, which is the real solution, even tho the shape of the solution is constrained by its instantiation as `magic'`
Yes I did, thanks!
The closest I've been able to come up with is "compiler-oriented programming" or "parser-oriented programming". The go-to technique is always to separate your program into a domain-specific language and a parser for that language. And referential transparency makes that much easier!
I wasn't really recommending them as an introduction to programming, only because they contain examples of how to do more or less what you're asking. It has been a while though - if I get a chance tonight I'll have a flip through them and double check I'm not thinking of something else.
I think this is one case of a more general principle, which says that you should aim to use the abstraction with the least power (which is still capable of expressing what you want to express). This will often make it easier to reason about your code, which makes refactoring easier, for example. It might also make the implementation simpler or safer. Say you want some state, but your action only needs to read from it. If you choose to use Reader, then you don't need to worry about that state being modified. If you use State, however, you might need to worry about this.
I agree that there should be a type class for `zip` and `zipWith` (and variants which don't trim to the shortest list), but I don't think Traversable is the right type class for them. How would you define it for pairs, for example? zipPair :: (e, a) -&gt; (e, b) -&gt; (e, (a, b)) zipPair (e1, x) (e2, y) = (?, (x,y)) It seems we'd need a `Monoid e` constraint, which the Traversable instance for `(e,)` does not have. Its Applicative instance does, however, and `liftA2 (,)` even has the right type, but it doesn't have the right semantics. I think we'd need a new type class, and I wouldn't be surprised if there was already one out there.
I wish! And https://github.com/knupfer/haskell-emacs seems like a good place to start. I feel like /u/chrisdoner would have an informed opinion.
Sounds interesting. I'll look into it (although performance was not a priority when I wrote it).
&gt; Frankly, why is (,) even traversable? Because a lawful implementation of `traverse :: Applicative f =&gt; (a -&gt; f b) -&gt; (e, a) -&gt; f (e, b)` exists?
seems like the laws are flawed!
If you're interested in variants that don't trim to the shortest list, there is sort of an implementation here: https://hackage.haskell.org/package/these-0.6.1.0/docs/Data-Align.html However, in a GitHub issue, the package author told me that those typeclasses were guided by his/her intuition, not backed by anything that exists in math. But still, pretty cool.
&gt; Have you done N-body simulations before? Nope, learnt it from scratch, though it took several iterations over a couple of months, Nvidia has some CUDA code snippets that really helped, I don't even have any formal training, use to write python scripts for a hobby. I would like to think it's within 10x of the speed of C, though I have no comparison, it's doing 4 steps per frame, and its using verlet velocity as well for integration (little more complex than euler), I don't think it's using any SIMD extensions because I can't get LLVM to work on windows.
What are you after from your expression language? I tend to use Free for DSLs that are more statement-oriented, and GADTs (or ADTs and smart constructors) for expressions. Partly because I treat the the type parameter in the DSL case as "and then do this" which doesn't always fit when I'm putting together a type for expressions, and partly because I really like having the option to use`bound` in the latter case :) I'm currently a bit pushed for time, but I managed to knock together a version of your expression that puts the square peg in the not-necessarily-square hole [here](https://gist.github.com/dalaing/6e0744987098b261286e). 
Thanks! That's a much nicer formulation than I had. My current project with them is a DSL to express basic propositional logic. Once I have that together, I want to be able to extend the base DSL to account for other forms of logic. Right now, I've got this worked up: data AndF k = forall f. (AndF :&lt;: f) =&gt; And (Free f Bool) (Free f Bool) (Bool -&gt; k) data OrF k = forall f. (OrF :&lt;: f) =&gt; Or (Free f Bool) (Free f Bool) (Bool -&gt; k) with the following test: test :: (AndF :&lt;: f, OrF :&lt;: f) =&gt; Free f Bool test = do asdf &lt;- true .|| false wat &lt;- pure asdf .&amp;&amp; false .&amp;&amp; true pure asdf .|| pure wat (.||) :: (OrF :&lt;: f) =&gt; Free f Bool -&gt; Free f Bool -&gt; Free f Bool a .|| b = Free . inj $ Or a b Pure (.&amp;&amp;) :: (AndF :&lt;: f) =&gt; Free f Bool -&gt; Free f Bool -&gt; Free f Bool a .&amp;&amp; b = Free . inj $ And a b Pure true :: Functor f =&gt; Free f Bool true = Pure True false :: Functor f =&gt; Free f Bool false = Pure False 
I didn't know this particular piece, but it looks like I accurately guessed the artist at least. :)
You're welcome! Any questions, let me know: it goes off the deep end pretty quickly but the idea of creating a DSL for a hand-rolled language with a least-fixpoint construction is presented before the Wasteland. :)
That doesn't look like the correct translation for transforming your `Expr` type to `Free`. This is the mechanical translation I was expecting: data ExprF a x = Lit a | Add x x type Expr a = Free (ExprF a) Void
Frankly, why is (,) even applicative?
to be honest I never heard of this before, a on a (very short) first glance it looks like the basis used for the [SPD1x Systematic Program Design](https://courses.edx.org/courses/course-v1:UBCx+SPD1x+1T2016/info) course on edX - so maybe this could be an alternative
You're confusing groupoid with connected groupoid. You can have a trivial groupoid where every object is only isomorphic to itself, for example. Now, it does turn out that this particular category does collapse, because there are paths from every object to any other. But just being a groupoid is far from enough for that to happen.
impressive demo. love it!
It was posted, although it could have afforded to be more prominent. https://www.reddit.com/r/haskell/comments/3s93w1/a_selfinterpreter_for_fomega/cwv5u0m 
Well, my impression (from some old email threads) was that data Foo a = Foo !a needs to `seq` the `a` values, so the data type would need the `Eval` context for this. But are you saying that strictness annotations compile to code not using `seq` at all? If that's true, then my comment above is wrong. From a brief look at the haskell report, I can't see `seq` mentioned in relation to strictness annotations, so maybe you are right.
SPD is excellent. Note that it's a CS101 level class, so the pacing is quite slow.
If we're allowed to brainstorm, it would be neat, given 204 students answers, to compile them all to Core and then generate a similarity rating for all of them (with alpha-equivalence) which lets you see the standard deviation from the "norm" result, detect possible cheating, and find outliers early. You could adjust the result with weights by looking at a sample from the norm and seeing whether it's correct, if it is, then programs similar to that are likely to be correct or nearly there. In the same vein, some of the outliers might be correct but done in a novel way, and marking that as correct might adjust the result to show that other ones like it might be right or near right. Probably for simple exercises the generated core (sans naming) for a lot would be precisely the same so you could mark them all correct (or wrong) in one batch. From this you could also see which exercises cause the most diverse answers, which ones are so obvious that all the answers are identical. You just need to write a unifier for Core, which is [a small AST.](https://www.haskell.org/platform/old/doc/2014.2.0.0/ghc/libraries/ghc-7.8.3/CoreSyn.html#g:1) Here's a [trivial, trivial](https://github.com/chrisdone/corebot/blob/master/src/Main.hs) example of compiling to core. It was intended as an IRC bot that would spit out the Core of an expression (because people always argue that X expression is different to Y expression when in fact the generated Core is often the same). Perhaps "needs to compile" is too harsh a demand on students, that would depend on your experience. How often do student answers actually compile? If not very often then perhaps an HSE fallback could be in order. If they don't even parse then the student has different problems. Fallback to edit distance? At 200 people I'd also be considering as much automation as possible, like you.
I don't think the French pronounce it in a way that can be confused with "Haskell".
The paper's result is between the useless trivial case (quote terms as themselves, unquote with identity function) and the highly useful, but impossible-in-total-language case (decide whether untyped representation can be unquoted at given type). I think it's a pretty cool result, it gives us access to term structure so we can do pretty printing or extract untyped lambda terms from quoted terms, etc.
Something that tripped me up a few times when I was first learning Haskell, so I figure I should point it out here... `state` and `action` are lowercase, that means they're type *variables*. You're probably used to seeing `a` and `b` used for that, but like any other binding, they can be long descriptive names too. So, this structure will work as `Game Int Int` if that's what you're comfortable starting out with; but it doesn't need to be changed at all when you move to more complex types of `state` and `action`.
You define the relevant functions for tic-tac-toe: fillSquare :: Grid -&gt; Square -&gt; Grid fillSquare grid square = -- takes a 3×3 grid and -- which square to play -- in, returns a new grid -- with a symbol in that -- square emptySquares :: Grid -&gt; [Square] emptySquares grid = -- returns all empty -- squares in the grid and then you put them into the data structure: tictactoe :: Game Grid Square tictactoe = Game fillSquare emptySquares
I wanted to compile your code, i found that it depends on `repa, mtl, random, lens, linear, vector, GPipe, GLFW`, but i still do not know in which package to find `Graphics.GPipe.Context.GLFW`
In [GPipe-GLFW](https://hackage.haskell.org/package/GPipe-GLFW-1.1/docs/Graphics-GPipe-Context-GLFW.html).
Good question. I don't know if this sort of style has a name. The `Game` datatype is often described as a "dictionary of operations".
Yes I agree - I did find it wonderful of how they tell you to basically write type-signatures into comments ;) ... have to admit I did not finish (yet) ... got bored around LQ 17
OK, I see. So it's a representation that can be converted to a Godel number, but cannot be obtained from a Godel number by total functions within the language. If such representations are allowed, doesn't the result become trivial by defining the "representation" of lambda term X as the pair (Godel number of X, normal form of X)? Or am I missing something?
If you did want to make it concrete, with a specific `State` and `Action` types, there would not have been a need to use type variables in the data and there it would have been: data Game = Game { nextState :: State -&gt; Action -&gt; State , getPossibleActionsFromState :: State -&gt; [Action] } That said, I like how tome's version would have worked with any state or action types, as you explained, and therefore could be used for any other "Game" with state and actions.
Still, in the less abstract world, it often makes sense to distinguish between the outer product and `intersectionWith`/`zipWith`. The difference becomes much more clear when you have a container with keys. map :: (a -&gt; b) -&gt; f a -&gt; f b outerWith :: (a -&gt; b -&gt; c) -&gt; f a -&gt; f b -&gt; f c intersectionWith :: (a -&gt; b -&gt; c) -&gt; f a -&gt; f b -&gt; f c mapWithKey :: (k -&gt; a -&gt; b) -&gt; f k a -&gt; f k b outerWithKey :: (k1 -&gt; k2 -&gt; a -&gt; b -&gt; c) -&gt; f k1 a -&gt; f k2 b -&gt; f (k1,k2) c intersectionWithKey :: (k -&gt; a -&gt; b -&gt; c) -&gt; f k a -&gt; f k b -&gt; f k c For lists, `outerWith = ap`, `intersectionWith = zipWith`.
I do not have a GPU. Can I run this code? 
This has already been covered in other comments, but I'd like to second the thought. Use a data type, not a type class. Additionally, I would recommend Soul-Burn's advice about [making it monomorphic](https://www.reddit.com/r/haskell/comments/3se3d9/question_design_guidelines_in_haskell/cwwg8rr), since you'll probably be dealing with a very specific type of state.
With `(GödelNum(term), nf(term))`, there's no internal evidence that the Gödel number is the actually the Gödel number of the term in the second tuple field. We could create fake representations runtime. In contrast, the paper's representation is like a term and additional information about its structure baked together. We can't create fake representations, because the types fully determine representations as "representation of some term with a given type". (To speak in nutshell about the paper's representation, terms are Church-encoded as folds over their structure, and their types are simultaneously encoded as `* -&gt; *` eliminations of the original type. Thus, quoted terms are functions expecting an `* -&gt; *` eliminator and functions for each term constructor, and have return types determined by the eliminator applied to the type of the unquoted term. We can compute with quoted terms by simply applying them to arguments) 
That's a more difficult question, since there is no lawful implementation of `pure :: a -&gt; (e, a)` nor `(&lt;*&gt;) :: (e, a -&gt; b) -&gt; (e, a) -&gt; (e, b)`. There are, however, implementations of those once we add the `Monoid e` constraint, but I don't know what justifies this addition other than the fact that adding it makes it possible to write lawful implementations. For some reason this reminds me of the fact that I don't like `Map k a`'s Monoid instance, because in case of conflict they pick the right argument's value instead of requiring `Monoid a` or `Semigroup a` and merging the values with `(&lt;&gt;)`. Anyway, whether `(e,)` is Applicative or not is irrelevant to whether it is also Traversable, because the `Applicative f` constraint in `traverse`'s signature is on the computation which is applied to the `a`, not on `(e,)`.
Equational reasoning works on everything you mentioned, not just pure code. See the post I linked above which uses a completely impure running example.
Isn't that the same as eval under quote, which is impossible?
I am assuming you already tried this. But if you have so many students shouldn't the university assign a TA to help you?
If you find yourself about to say "non-participating minorities", "some people hate their users and ..." or anything like that, please reconsider. All it does is put people in a more hostile mindset. There are extremely respectable, highly productive people on both sides of all the debates we've had recently.
Indeed. It was buried enough that I did not spot it!
I really like this idea a lot, but I'm sure somebody will find a reason to veto this...
Core splicing idea is very alluring. GHC + TH would be only reasonable syntax shy of becoming a do-it-yourself functional language a'la Lisp on steroids :)
I like this idea in theory, but in practice GHC's internal API is extremely large, contains ton of detail, and changes quite a bit from release to release. `ide-backend` has code to walk over the AST to collect information, and the changes between [7.4](https://github.com/fpco/ide-backend/blob/master/ide-backend-server/GhcShim/GhcShim742.hs), [7.8](https://github.com/fpco/ide-backend/blob/master/ide-backend-server/GhcShim/GhcShim78.hs), and [7.10](https://github.com/fpco/ide-backend/blob/master/ide-backend-server/GhcShim/GhcShim710.hs) (we skipped 7.6) were non-trivial. The AST representation is very low-level; OK (just about) for a library like `ide-backend`, but exposing this without a higher abstraction layer directly to all libraries, instead of TH, would be rather painful I suspect.
I'd like to find some kind of good methodology for this, so that we could pin down what's going on, but I think it'd need some good example cases. maybe I'll puzzle over how this works for some common functions. if there IS something we could extract, it might be really useful as a teaching aid, and also as a tool for developers (assuming it's mechanical enough)
Apparently `GeneralizedNewtypeDeriving` is confused by it.
fair or helpful, heh 
why can't they be updated? the world keeps changing, why can't we update learning materials?
Since the change is so minor, both conceptually and in number of bits that need to be shuffled, the documentation is hardly a big issue. Actually shuffling the bits on all existing Haskell programs is the engineering effort.
FWIW- We had a meeting of the core libraries committee last week, and one resolution was explicitly not to do anything on the `Monad(return)` front at this time. We're not saying it'll never happen, but it'll be the job of a future committee to decide when and if the existing default definition of `return = pure` is old enough that we can officially move `return` to a top level definition with sufficient backwards compatibility. (Meeting minutes will be forthcoming shortly.)
Some of the materials will be updated, of course. I think the issue is with books that are already printed, blog posts that aren't maintained, that sort of things. I don't think this should stop us from making the right decision though!
But if the complaint is about written literature still referring to `return`, shouldn't there at least be some kind of advisory to consider `return` deprecated in new code/books/tutorials/etc? Otherwise `return` will keep being mentioned, and it will never become forgotten enough to be moved away.
As of GHC 7.10 `return = pure` is a default definition in the class. You do not have to define `return` today. So if you have documentation you are supplying going forward you can teach how to write a valid `Applicative`, and then how to add `Monad` with `(&gt;&gt;=)`, and the code would remain compliant even if a future committee eventually removed `return` from `Monad`. (Even if they did so, it would remain as a top level definition, so _calling_ `return` is just fine.)
Meh, Rails has been doing that all the time and it hasn't stopped it from getting popular. Same is true for JavaScript: you probably won't even recognize modern JS libraries based on resources that are even a few years old. Living languages change, old literature becomes old and it's all fine. Sure it means that language-specific literature doesn't get to be timeless like math or CS books, but that's what you get for diving into the arbitrary, everyday details of a specific tool. This is a problem that's woefully overstated. We shouldn't value backwards compatibility at all costs, and we certainly shouldn't value *backwards compatibility with books* all that highly. (Especially when the actual change involved is pretty minor.)
Looks like emacs is getting a [C FFI](https://github.com/aaptel/emacs-dynamic-module/tree/dynamic-modules-clean-1) in the next release... so maybe not as far off as you think!
Don't you remember the worldwide catastrophe when 3 decades worth of literature about imperative programming were made obsolete in one strike? Conservatism at its finest. Next we'll hear about protecting the kids from pedophiles or something.
you're saying to normalize the Patch, then count up the Inserts versus Deletes, and then allocate an output vector of length (insertCount - deleteCount) once. and then write to it (perhaps even dropping bounds checking?) that's sounds cool. I gotta learn more "mutable haskell". 
&gt; You do not have to define `return` today. Does everybody know that? Or will people just perpetuate the old pre-AMP wisdom of having to define both, `pure` *and* `return`?
I reckon this misses two the essential "cool" thing about what the general public like about spreadsheets, which is: 1. Immediate feedback (live reloading). 2. Coupling of code with results. But I have [a horse](https://docs.google.com/presentation/d/1lh9_QlLKtW4L5WsFkgRggyIFORXAkTUu6Y470VkM4uI/edit#slide=id.p) in this race. &gt;_&gt;
Stop bragging :)
[I've updated my comment with build instructions](https://www.reddit.com/r/haskell/comments/3sc8aw/my_first_bigger_project_a_realtime_60fps_800_body/cwvwvgp)
[I've updated my comment with build instructions](https://www.reddit.com/r/haskell/comments/3sc8aw/my_first_bigger_project_a_realtime_60fps_800_body/cwvwvgp)
what difference did you notice between O3 and O2. I thought I read somewhere that there's rarely a noticeable different. but I guess, even if true, graphics might be a case where it counts.
Want to know a secret? -O3 doesn't exist. Infact I think -Odph is meant to be the best option for this kind of work load. https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/options-optimise.html
I'm curious for the dependencies... Any FRP in the mix?
Grasswire is rad. Now only if I knew Haskell... 
This is Paul's talk from the Oct 21 [Boston Haskell](http://www.meetup.com/Boston-Haskell/events/224740727/) meeting. The sound quality in this video is my fault, not his :) 
oh right, nm
But even raw TH has the problem of being too big for some things (which motivated [`th-desugar`](https://hackage.haskell.org/package/th-desugar)).
Also some notes and refs from the talk posted here: https://gist.github.com/pchiusano/25c6a595d5ad7b0888f0
I'm using Linux, and it is not working here :/ Maybe a version mismatch? I have installed: linear-1.20.2 gpipe-2.1.3 gpipe-glfw-1.1 repa-3.4.0.2
And I'm confused by `GeneralizedNewtypeDeriving`.
It did not cause a split. A many-years-long effort to politicise scala open-source libraries led to an attempt to steal scalaz, which failed. Many people were subsequently duped. There is no split; there never was.
Allow me to explain myself - I didn't mean phonetics as of itself, but rather it being shoehorned into the Latin alphabet with dozens of idiosyncrasies. I actually think English would look way better with some diacritics, perhaps resurrecting old letters like þ, etc. :) Btw. what do you think is screwed up about English grammar? From an outside perspective, I've found it rather regular and straightforward. Especially compared to my native Polish, where a lot of grammatical information is provided by word formation with rules that are sometimes about a Lovecraftian as English spelling...
That is what I said. That equational reasoning extend beyond pure functions
Oh, then I misunderstood. I guess we agree :)
I have that "All my friends are dead" cover on a T-shirt. I'd definitely wear a Haskell version, but... I don't get the joke. What's the relationship between monads and dinosaurs?
Everybody knows, and everybody will stop.
&gt; It's right bias all the way down. When `(,) a b /~ (,) b a` you have to pick one bias, and whichever one you pick all of the instances will lean that way -- you have to use newtypes (or something similar) to reverse or eliminate the bias.
For the basic type, you don't have to pick a bias at all, and then make newtypes that are biased. It's same as there is no `Monoid` instance for `Num`. Instead you have `Product` and `Sum` that have a `Monoid` instance with the correct `mzero` and `mappend`.
you need gpipe-glfw 1.2 from their github.
Ok one: the author of this post is clearly targeting psychopaths, two: Facebook is going pretty heavy on Haskell adoption and the people there seem to be enjoying themselves, three: fuck you, who gives a shit about getting rich? Four: your shitty website doesn't actually serve a working link to the next page of the article you broke up into pages to artificially increase pageviews.
&gt; Or haskell is already unreadable, so he had no idea where to place indentation, and/or it wasn't immediately obvious after he pasted it in? It's already indented in the page he links to, so I'm gonna go with active malice.
Please ignore this. This account is a huge troll. Do not try to engage in meaningful discourse with them. It's clear from the storify profile page ([screenshot](http://i.imgur.com/Aaw31Dz.png)) that they do not expect to be taken seriously. All submissions by this account are similar attack articles. I'm not clear on the motivation. Possibly clickbait to get page views from the community they are attacking.
I am thinking that it might be time to apply the following rule from the Community Guidelines regarding the submitter: &gt; 1) Obvious trolls are obvious, and also useless, so they will be removed with extreme prejudice.
&gt; three: fuck you, who gives a shit about getting rich? Four: your shitty website doesn't actually serve a working link to the next page of the article you broke up into pages to artificially increase pageviews. Why are you so hateful and violent? Also, it's storify. Not our own software. The pageview thing is just an invention by you as an attempt to suppress ideas that you disagree away as illegitimate. If you even read the blog post - you're confirming the stereotype. Functional programmers can't handle the truth - so they create lies instead of improving themselves. RealTalk's Law of Functional Programmers: &gt; A functional programmer takes all criticism as a conspiracy to eliminate the whole language. He therefore will attempt to strike at your legitimacy. Corollary: &gt; The functional programmer will nitpick you over a small detail instead of accepting criticism. If nothing exists, he will purposely misconstrue minutiae for his own purpose;
Everything that needs to be said about this article [has already been said](https://www.reddit.com/r/programming/comments/w6rbp/functional_programming_a_step_backward/).
This is one of the more grown up and respectful things I've read. Have an upvote :)
The Erlang example is quite readable, don't you think? :)
[hmft](http://hackage.haskell.org/package/hfmt) was just created a week ago!
I saw that before posting this. Did you look at what it does? It doesn't appear to do any work itself, it's pulling together hindent and stylish-haskell which doesn't solve the problem I was talking about. That being: hindent choking hard when you want to just format an entire file, fire-and-forget style.
I did, but I thought it was relevant to the topic and hadn't already been linked.
No... we can't... that would invalidate literature!
Anyone who can't get indentation copy and pasting working in a blog post is not likely qualified to discuss real world implications of programming languages. 
I agree -- this project is neat, but really is more like a "reactive formlet". The admixture of code cells and data cells and the chaining of dependencies are my take on why people are keen on spreadsheets, but those largely fall under your point 2 :-)
It has the thinking-feel of "solid state" programming; there are no "moving parts." But then again, if you describe things that way to people, they just think you're on drugs.
how does that even work? what are the mechanics of that?
Want. It repeatedly amazes me that perl has a better tidy/formatter than Haskell -- a language with a community so well versed in static analysis, parsing, and pretty printing. One of the very last things I want to do with my time is type the space or delete key.
I think that's almost the kind of thing that triggered his emotional backlash. The startup he works at used haskell somewhere and he found it hard and is now throwing his toys all over the place. It doesn't sound like he got fired yet, so maybe he's behaving at work and venting all his feelings on the Internet. 
Not benchmark but if I remember [http://hackage.haskell.org/package/ekg](http://hackage.haskell.org/package/ekg) allows you to monitor various statistics during runtime.
Nice. A suggestion after just glancing at your code for a few seconds, write explicit type signatures for all your top level definitions! It makes reading the code so much easier.
OK. &gt; But for some reason, Haskell is held to a higher standard for a much smaller user-base or rather for the prospect of a yet to materialise large user-base. Is this a sign of delusion? I don't think Haskell was ever meant to be popular. And that's a good thing, in my opinion.
Yes indeed, thanks, though the scope/goal is quite different :-)
Yes please! The indentation rules and the lack of convention always felt weird to me. Something like go fmt would be ideal. Next best thing would be something like Python's pep8.
&gt; I wish you all the best, and I hope Haskell won't go the way of the Dodo. It's not like anyone would still use Haskell if Idris could leverage GHC and Haskell libraries, had real type classes, didn't insist on strictness by default, etc. &lt;hides&gt;
I just put an hour aside and made a very hacky but working version. I've been avoiding publishing any kind of prototype in defense against premature judgments. I've also been learning Elm as I wanted to try it out as my UI language. But here is a very, very simple and stupid prototype using only JavaScript and the existing tryhaskell.org evaluator: http://chrisdone.com/haskell-spreadsheet-prototype This is just supporting listing out some declarations. It's a shadow of what I have on paper, but that's what you get in an hour's work. I doubt I have to explain the UI, it should be self-explanatory. It lacks some obvious things from my slides: * Parsing in a Haskell module (requires writing some Haskell and deploying a new server) * Generating a Haskell source module to share with friends and colleagues * Immediate feedback (this uses the slow mueval evaluator, not exactly immediate) * Access to external resources * A nice code editor (CodeMirror or Ace or whatnot) * Displaying of data structures properly (e.g. a list or vector should be displayed as a list of cells, a matrix as a grid, etc. I'll steal your idea of displaying a boolean as a checkbox, that's nice, a Diagrams diagram as an actual picture, etc.) * Editing data structures and writing that back to code * Decent error messages (not just GHC's regular stuff) * Imports? * If you make circular references it just fails. But I thought a bit of dependency tracking could yield more helpful messages. * Also I wanted the ability to move the declarations around visually on the screen. There's no need for them to be listed in any particular order. The user should be able to group things together how they like without it affecting their formulae. * I can't do it on this demo as the overhead is too much, but displaying results as you type rather than after you hit Edit would also be nice.
Additionally, right from the very start people at /r/rust have been working on [rustfmt](https://github.com/rust-lang-nursery/rustfmt) ... Would want it for Haskell as well!
I'm not quite sure what you mean. I'm talking about things like: size :: Children a -&gt; Size size (N4 size _ _) = size size (N16 size _ _) = size size (N48 size _ _) = size size (N256 size _) = size I have some more interesting/useful examples somewhere, but can't find them right now. This particular one could be rewritten by futzing with the data type a bit, but you get the point.
&gt; This by might be a naive question, but is this possible or am I limited to working with jpeg, tiff and png images? If by "possible" you mean "there's already a library on Hackage that reads raws" then it might be possible that you're indeed limited to the formats you've mentioned, as this is what [JuicyPixels](https://hackage.haskell.org/package/JuicyPixels), the most popular image saving/loading library, handles. Though if I were you I'd search Hackage before giving up. On the other hand, rolling your own binary file parser in Haskell is relatively straightforward with libraries like [binary](https://hackage.haskell.org/package/binary) or [cereal](https://hackage.haskell.org/package/cereal). 
hfmt is based on haskell-src-exts and therefore I won't ever use it. We really need to sync our tools with the compiler or they're incredibly annoying (for example, haskell indentation in Emacs routinely fails to parse my code and _prevents me from typing_ because it's not using the same parser as my compiler).
Perl was designed to process (in and output) text files, of all languages it makes sense that they have the best formatting libraries.
Users come and go... what's the problem with a few leaving? Those quitters basically give up their place at the table and won't be involved in any further decisions. If you want to have a say, you stay. It makes little sense to accomodate users that have already left...
It's not so much the disagreement per se (which is perfectly fine), as rather the questionable means that were used to express that disagreement as they ran out of arguments
Yes, I would consider that too. I love lambda case! But it doesn't make a huge difference in this situation, in my opinion. Yanking out the boilerplateyness is definitely the way to go.
if-then-else and do are semantically redundant. I think that a coherent category theoretical academic language should suppress such nonsense. It makes haskell not a serious playground for category-theoretical bikeshedding. "a whole different kettle of fish" seems to me not an argument but a metaphor taken from real life that is inappropriate for a serious discussion that should be carried on with mathematical arguments. The fact that this change break almost everything in exchange for no additional functionality is not an argument.
If you would have to maintain the same amount of code than these high profile haskellers, then you might understand their perspective. I'm not saying that their view is absolutely correct, but reality is complex enough that they have a point. I think you need the right amount of idealistic and pragmatic people to get the best possible results, that they appreciate each other, that the other side isn't per definition wrong. 
Perhaps if maintaining a lot of code is hard we should find tooling or community processes to make it easier or have a discussion on how to lessen their workload, not halt all progress to the language. One thing I disliked about a part of the recent discussions was the hand-waving going on when people were asked what exactly caused the large amount of work and what the suggestion was as an alternative to the current process that did not halt all language changes forever. As far as I have seen all the suggested changes to the process that were realistic (I do not consider never changing a single bit of the language forever realistic) came from the camp already in favour of some or all of the current set of breaking changes.
I'm not saying their view is wrong either, I'm just saying their behaviour was very disappointing.
&gt; what do they program in I guess it's SML: https://existentialtype.wordpress.com
Woah man, this is cool! Nice RTree optimization. I'm doing a NCurses game myself, it's nice to see you followed a similar approach :D Oh and what about type signatures? I can hardly follow the code without them ... Edit: love this piece: checkBetween = do -- monads!
&gt; (type-theorists, category theorists?) ... what do they use to solve real-life programming problems I think you have your answer
I believe a lot of people who find Haskell impractical use languages like OCaml or Scala, which provide some of the benefits, but which also have escape hatches like OO or imperative mechanisms.
&gt; ...type-theorists, category theorists... &gt; ...solve real-life programming problems Ha, good one ;) Toning down the snark a bit: many theorists (not all, but some/many) are not all that concerned with solving practical problems, so there's a whole bunch of practical stuff they don't need to concern themselves with. In that frame of mind, it's really easy to tear down a tool that maybe makes different trade-offs for the sake of practicality. Consider taking their criticism of haskell less seriously :) 
&gt; when their arguments have been responded to at length "responded to" is not the same as "given good reason for"... 
&gt; For the basic type, you don't have to pick a bias at all. You do. It follows from how you choose to partially apply types, and what you allow as an "instance head" when doing type classes. There *is* no valid Functor instance for "(,)" other than the one provided, because "(,) a" is roughly equivalent to "\x -&gt; (,) a x" and not to "\x -&gt; (,) x a" and because type-level lambdas (including type families) are not allowed as an instance head. Changing the first property is crazy and make type level partial application look positively bizzare compared to term level partial application. Changing the second property would, as I understand it, make instance selection intractable and not just in "takes too long to be practicable" but in the "doesn't always/ever have a unique solution" way.
Reasons have been given. For some, they might not compare favorably to the downsides of the changes, and that is understandable. But what is the point of keeping on acting dejected, calling names or going all dramatic with bus top analogies ? The AMP and FTP changes were clearly appealing to lot of people. They are there now. If one think of a better solution, he should just propose (and implement) something. If he thinks those changes are unwarranted and nefarious, he should just keep on using the version of `base` that works for him.
Great. I was doing exactly the same thing [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/) -&gt; Haskell but was lazy to finish it.
dont you get it? the end justifies the means: the committee doesn't dare to go near MRP anymore ... mission accomplished ୧( ಠ Д ಠ )୨
Bob Harper, who has problems with Haskell on both theoretical and practical grounds, advocates ML languages, see answer above: https://www.reddit.com/r/haskell/comments/3sj0zz/if_haskell_is_such_a_terrible_language_what_do/cwxmefz.
&gt; I see many people, (type-theorists, category theorists?) bashing Haskell to the ground for its flaws. I think that's a big strong. Both Agda and Idris are written in Haskell. As for why Agda/Idris aren't used much in practice, well... I predict they (or languages like them) will be as popular and successful as math in the future. Interpret that as you will.
As long as it uses tabs, as is proper for indentation. &gt;:)
&gt; And for the world to realize styling should be a function of the tools you use to view the code Ah, another fan of tab characters for indentation!
That's right
Type theorists will often bash Haskell because it's the language they're using to get their programming done. It's only natural to complain about your tools when your job is difficult :)
I feel that way. It's not really a language I want to invest my time in learning because it has too much surface area and too many surprises.
type roles don't make anybody really happy, except they close the `unsafeCoerce` gap in GeneralizedNewtypeDeriving. If we'd done AMP before roles we might be in the awkward situation of choosing to remove `join` or move it to the top level, but I don't think the roles story would have gone away.
Sadly, in the presence of type roles, you can't have that either. `join` as a member fails `GeneralizedNewtypeDeriving` because it doesn't know that the argument must be representational to pass the laws. We need smarter role machinery first.
Yup. SML in its pure state (that is, without extensions) is completely mathematically proven.
Haskell has never required that there "be only one way to do something". The language has long adopted a stance that a rich surface language syntax is okay, especially if it yields a simple core. We have both case and definitional style, let and where, fundeps and type families. Nobody is coming for your if-then-else. In the matter of `return` _being in the class_ these arguments don't hold. It adds no power there, it isn't a linguistic construct, it is a function provably redundant with one we have one class up now. The game has changed and the assumptions that held before are somewhat invalid now. Nothing is changing today or in the near future around `return`. Everything that would have to happen for the "Monad of No Return" proposal around `return` in the short term was done in 7.10, `return = pure` is already the default definition. Even if in the distant future it does change nobody is suggesting removing `return` from the language, merely from a class, moving it to a top level where the consistency of return and pure can be safely enforced.
Oh I'm sold on Idris - personally I like a lot about it and am hoping to experiment with it more once Edwin finishes his book on it (since I'm also pretty busy playing around with Elm anyway).
MRP is considerably less popular than FTP was.
I just want to say that this kind of problem can be perfectly modeled via Haskell type system. My suggestion is to focus on higher level abstractions, before figuring the alleged way the program should manipulate these. 
I don't have to reference the previous matches, but I have to know where the players end up. The problem with your code is there is no information regarding the tournament structure. Where does the highest seeded player start? Where does the winner of round 1 match 1 end up? A normal single-elimination bracket (think World Cup playoffs) can be represented by a binary tree with the root at the finals, but the above double-elimination can't. Does that help?
What he said!
Ok, thanks, but that's what I've been doing. I wanted to know how to do something like that, or at least if it's possible, which you claim it is.
FWIW the author is presumably /u/hagda (cf. other recent posts here which are now unattributed).
&lt;nitpick&gt; 140 characters, IIRC (I don't use Twitter). &lt;/nitpick&gt;
&gt; I don't have to reference the previous matches, but I have to know where the players end up. If your Player strings are unique, then this can be computed from a list of Rounds. &gt; Where does the highest seeded player start? You could modify `Round` like this: type Seed = Int -- Is this appropriate? I'm not a sports guy haha type Seeds = Map Player Seed data Round = Round [Match] Seeds Then, you can compute the player's seed in a given round (is this even how seeds work? I think seeds are only for the first round. I don't know...) &gt; Where does the winner of round 1 match 1 end up? Again, this can be computed given a set of Rounds. Make sure your matches in a round are ordered in the list, so the first match in the list can be "match 1". I try to keep my data structures dumb and simple, and move all the heavy lifting and business logic to functions. I aim to find the simplest encoding of my problem domain in types, while still being able to compute all the info I need.
It is a bit simplistic to try to paint the parties in the current discussion with such labels such as "idealistic" and "pragmatic".
The committee wasn't terribly enthusiastic about MRP to begin with. =P We may well go towards the Monad((&gt;&gt;)) part of it, but there isn't anything we'd even be able to do on the return side in the near future, even if we were super-excited by the prospect.
I appreciate the help, but I don't think you've fully understood my problem (which is of course my fault). &gt; If your Player strings are unique, then this can be computed from a list of Rounds. Yes, but I want to have the full tournament structure before I populate it with players and scores. Could you try to model the example structure that I wrote in the OP?
Would something like this do the trick? The finals are still the root, players are the leaves, and everything in between is described by the winner or loser of other matches. data Player = Player { playerName :: Text } data PlayerSource = SourcePlayer Player | WinnerOf Match | LoserOf Match data MatchResult = Player1Wins | Player2Wins | NotYetCompleted data Match = Match PlayerSource PlayerSource MatchResult If I'm not mistaken, this could represent a double elimination tournament tree, including games that have not yet been completed.
`hindent` definitely *does* work on whole files! At least, with the `gibiansky` style :P I've been using `hindent` as a `go fmt` replacement for IHaskell for probably almost a year now. Travis CI verifies that all files look exactly like they do if they were passed through `hindent`. This includes CPP, extensions, and a whole ton of other stuff. It's not perfect, and sometimes it generates formatting that I wouldn't manually use, but on the other hand, reformatting a haskell declaration is just two keystrokes in my editor configuration (see /u/expipiplus1's comment), so I just spam that configuration whenever I've written a syntactically valid expression / declaration.
I've looked into that, and I think it's not trivial. The architecture of `hindent` and `ghc-exactprint`, as far as I can tell, look very different. I'm sure it's possible, just not easy; better `ghc-exactprint` documentation would also help. (It's a little bit impenetrable – just hasn't been around long enough to develop tutorials, guides, etc, yet... Still usable though!)
&gt;I see many people, (type-theorists, category theorists?) bashing Haskell to the ground for its flaws Are you sure about that? There's a couple of well known trolls, but that's about it. Who are these "many people"?
Not quite, simple example (sub-case of the example in the op): p1 --&gt; match -- w ------------------&gt; match -- w --&gt; 1st p2 --&gt; 1 -- l -+ +-&gt; 3 -- l --&gt; 2nd +-&gt; match - w -+ p3 ------------------&gt; 2 - l ------------------&gt; 3rd match1 = Match (SourcePlayer "p1") (SourcePlayer "p2") NotYetCompleted match2 = Match (LoserOf match1) (SourcePlayer "p3") NotYetCompleted match3 = Match (WinnerOf match1) (WinnerOf match2) NotYetCompleted Now `match3` contains a different copy of `match1` than `match2`. If I evaluate the result in one, how do I find the second output? Also, it's possible to create invalid tournaments: m = Match (WinnerOf match1) (WinnerOf match1) NotYetCompleted 
&gt; The committee wasn't terribly enthusiastic about MRP to begin with. =P [*cough*](https://www.reddit.com/r/haskell/comments/3mb8lb/monad_of_no_return_proposal_mrp/cvdnc12)
They are only unique because of the contrived limitation for the partial application. It is *not* a mathematical uniqueness. Specifically /u/ekmett's lens library is one that overcomes this bias in many functions such as `over`.
&gt; contrived limitation I don't think it is particularly contrived. In any case, *that's* already been decided, and I doubt it will change any time soon. Let me know when you have a proposal for altering instance heads and/or partial type-level application, and we can revisit their uniqueness and the utility of providing the instances.
That's also possible when you parameterise over player type, but adds the complication that you have partially filled trees. If you want partially filled trees, do that, I went with a design where you could be sure the trees were either empty or full for simplicity.
Well, people on lambda-the-ultimate seem to like expressing their dislike quite often, then there's practically every non-Haskell functional programming forum and blog where people like to bash Haskell in particular. I don't know, is that maybe because it's one of the most popular among them?
&gt; It has never not gotten stuck formatting a module. &gt; I haven't reported it
I removed it because it seemed a bit unfair, but since you've replied I'll restate it: I haven't had good luck reporting issues with Done's tooling projects (ghci-ng, etc.) in the past, so I gave up. I talked to others (including at Hac-Phi) that have had a similar experience. I don't have a lot of patience for tools that require constant tracking of `HEAD` in order to keep them in working order.
[This is my last month of public-only work on my github](http://i.imgur.com/jIZWIcj.png), what's yours? I'm not obligated to report issues prolifically for every single tool I try out. As a general rule, I report issues a _lot_ more frequently than most people. I also encourage others to report issues they have with their tools/libraries. Edit: there, I reported it: https://github.com/chrisdone/hindent/issues/161
The things you've marked: 1) `Monoid` instance for `ScoreRecord` - if you used `Sum Integer` (from `Data.Monoid` instead of a plain `Integer` you could just use `mappend` and `mempty` inside the definition. On the other hand, this can make other places in your code more awkward. 2) `unwrapVisibility` - what about making `Visibility` just a tag: data Visibility = Shown | Hidden then using `Writer (Last Visiblity) a` whenever you use your current `Visiblity a`? `Last` is again from `Data.Monoid`. This would make sure that the visiblity annotation is always replaced with the last value, like your code currently does, without the clutter of writing your own instances. `unwrapVisiblity` becomes `fst . runWriter`. 3) You didn't mark it, but that `AI` type class looks sketchy to me. There's only one implementation, so why don't make `play` a standalone function of type: play :: AIType -&gt; Deck -&gt; Hand -&gt; (Deck, Hand) 4) `drawCard` - if you want to be sure that a list is infinite, you can always define a new data type like: data InfiniteList a = InfiniteList a (InfiniteList a) 5) `blackjack` - "XXX: I really don't think composing fmap is the right way to do this..." - I'm not sure if I understand what you mean, but `fmap a . fmap b` is guaranteed to be equal to `fmap (a.b)` (unless someone who wrote a `Functor` instance was feeling evil that day), so you can use this. 6) `startingHand` - "XXX --I feel like I'm using the State monad completely wrong" - indeed you are. Try something like: drawCard :: State Deck Card drawCard = state $ \(x:xs) -&gt; (x, xs) startingHand :: State Deck Hand startingHand = do firstCard &lt;- drawCard firstDeck secondCard &lt;- drawCard secondDeck return [Hidden firstCard, Shown secondCard] Alternatively: startingHand :: State Deck Hand startingHand = (\a b -&gt; [a,b]) &lt;$&gt; firstCard &lt;*&gt; secondCard Minor things: 1) You can derive a `Functor` instance if you add a `{-# LANGUAGE DeriveFunctor #-} pragma to the top of you program. 2) I hesitate to suggest this, but the first thing I've noticed is that `addResult` looks like a perfect use case for lenses: {-# LANGUAGE RankNTypes, TemplateHaskell #-} import Control.Lens data ScoreRecord = ScoreRecord { _wins :: Integer , _ties :: Integer , _losses :: Integer } makeLenses ''ScoreRecord data Result = Win | Tie | Lose result :: Result -&gt; Lens' ScoreRecord Integer result r = case r of Win -&gt; wins Tie -&gt; ties Lose -&gt; losses addResult :: Result -&gt; ScoreRecord -&gt; ScoreRecord addResult r = over (result r) (+1) Make of this what you will.
&gt;overstating Are you serious? Running hindent and having to pkill and restart Emacs because I forgot which files in my project will cause it to get stuck is absurd.
Agreed, but it's simply never happened to me. So, I interpreted your complaint as treating the pathological case as the common one.
I was able to reproduce it by running it on one of the simplest files in the `hindent` source code. It fails on roughly half of the modules in our work projects (quick sampling). You are very fortunate to write only code that hindent likes. I work on a lot of other peoples' code as well (github, helping new people), not much luck with hindent there either. It doesn't really even need to happen half the time to be a serious problem (although I'm not getting a batting average much better than that), it just needs to be frequent enough that I cringe at the possibility of my Emacs getting stuck again.
&gt;then there's practically every non-Haskell functional programming forum and blog where people like to bash Haskell in particular Those aren't "(type-theorists, category theorists?)", they are the people you specifically said you already understood. People using impure languages.
&gt; typeclasses are most helpful and convenient when you have "laws" As an aside, is there any particular reason we couldn't have laws we expect to hold between members of a record? Or wouldn't expect it to be of much value?
`hindent` is not perfect. I think both I and Chris will agree on that – we've put a lot of work into it and it works more-or-less for us, but neither one of us will claim that it's really ready to be the `go fmt` for the entire community. With that said, I do think that `hindent` is the closest we've gotten so far, and that it's close. It needs a bunch more testing and some more work, but it can get there. Also, if anyone is curious, you can take a look [here](https://github.com/gibiansky/IHaskell/blob/master/verify_formatting.py) at a script I use to check the entire [IHaskell](http://github.com/gibiansky/IHaskell) codebase for compliance with `hindent` formatting. We all have full-time jobs but nonetheless I try fairly hard to be responsive on `hindent` issues that I can help with that are relevant to the overall system or the `gibiansky` style. We can't help unless we get issue reports! :)
This script is [here](https://github.com/gibiansky/IHaskell/blob/master/verify_formatting.py)
I mean, functional programming forums and blogs where the primary language is not Haskell. Such as Elm etc.
There is a rather ridiculous amount of code out there that losing GND on the Monad class would break. As in it would break more code than AMP + FTP together. Definitions like newtype M a = M { runM :: ReaderT MyEnv (StateT MyState (EitherT MyError IO)) a } deriving (Functor,Applicative,Monad,MonadReader MyEnv, MonadState MyState, MonadIO, Alternative, MonadPlus) litter application code in the wild.
`Double` is not a type class, it is a concrete type, you can't use it that way !
I might be wrong, but it seems this is using the `id` trick. It works but I don't think it a satisfying solution (as nothing can guaranty than a given id references a existing object). Using `STRef` is probably better.
I'm torn. I do enjoy go fmt, but it works in conjunction with grammar-level features (like trailing commas on lists, hint hint) to give the best experience. But I do wonder what characteristics of the autoformatter would encourage changes in writing style when using it. For example, if it can't adequately infer operator fixity, maybe there'll be a trend away from complex expressions that rely on fixity to make sense.
Well, I don't have to, but it would be nice to use a common structure to model an ongoing tournament and display it on, say, a website while on the other hand use it to simulate a tournament.
Think of it as the same as `liftM2 (,) [1..10] [1..10]` and understand that first, via playing with the list monad. Then work through the types in your example and see why things come out the same in this case.
&gt; practical grounds For some value of practical, that is. I do find his blog (and PFPL) interesting, and he does make important points about [reasoning about parallel programs](https://existentialtype.wordpress.com/2012/08/26/yet-another-reason-not-to-be-lazy-or-imperative), for example, but on the other hand, some of his criticisms of Haskell seem to be like (not his) "Haskell has no `State` monad" argument recently posted on this list - technically true, but irrelevant to any Haskell program ever written. I mean, I do feel a little uneasy that it's possible to write a legal Haskell program that crashes and burns, but if the only way to make it happen is to do something hopelessly braindead, like spoofing a `Typeable` instance, or making a program depend on whether some value can be evaluated with `seq` without ever needing anything contained inside it, then, at the end of the day, I really can live with that.
&gt; :t ($) &gt; ($) :: (a -&gt; b) -&gt; a -&gt; b &gt; &gt; :t (&lt;*&gt;) &gt; (&lt;*&gt;) :: Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b This is not a full answer, but notice that, if you squint a little, &lt;*&gt; is very much like function application, but one in which both the function and the argument lie in an "special context", which in this case is []. And **f &lt;$&gt; x** is a synonym for **pure f &lt;*&gt; x**. Where **pure** is a function of type **Applicative f =&gt; a -&gt; f a** that can put any pure value into the "special context". In the case of lists, it just makes a singleton list out of the argument. Your example could be written as &gt; pure (,) &lt;\*&gt; [1..10] &lt;\*&gt; [1..10] or &gt; [(,)] &lt;\*&gt; [1..10] &lt;\*&gt; [1..10] 
Here you go: https://gist.github.com/Prillan/f92ecc406f0e8bac54e5 That code never meant to be seen by the public. It's a simulation of last week's DreamHack Cluj-Napoca CS:GO tournament. Basically all of the commented out code are matches that now have finished. The `predict` function takes two teams ids and returns the probability that the first team wins against the second.
Seems so. It doesn't look very extensible and hasn't been worked on for 3 years.
But that's half the fun of using Haskell!
I'd like to know too. I'm over my haskell honeymoon phase and back in the everything sucks phase. I'm not sure monad transformers and typed IO are worth their weight. End users seem really turned off by the enormous binaries GHC produces and the size of GHC itself if they're going to build something from source. I like lots about Go (easy cross compiling, native code, good tooling, easy to understand), but it's too simple (no generics, no sum types), rust is nice but I don't know if the borrow checking complexity worth the nice parts given I can usually afford a GC. A less ugly ocaml with proper multicore support would probably do it. I'm told F# fits what I want, but fuck microsoft and I hear Mono has some potential patent violations in it should MS ever decide ot destroy them. 
`(*) &lt;$&gt; Just 2 &lt;*&gt; Just 3` -&gt; Just 6 might be an easier example to understand, i always find the list monad unintuitive
Thanks for your help. Unfortunately, this doesn't seem to be able to describe the example structure I gave in the OP.
Thanks for sharing this; I'll take the code apart later, but after skimming it, I have a nitpick to share: I'm not so happy with this "cookbook" approach to machine learning that's becoming popular these past few years, light on formulas and heavy on folklore. On the plus side, the author chose to not depend on any external packages (OTOH, there's a jungle of zips and maps and folds to keep track of), which I'm sure will attract newcomers of both disciplines.
But the class inclusions are, from general to specific: Functor &gt; Applicative &gt; Monad; in this case the results don't depend on the call order, so Monad might be overkill .. 
There was some jitter running only 6 threads, upping it to 7 seemed to fix that, so probably not a garbage collector issue. That is a 100% realtime recording, using OBS. I really haven't seen anything that seemed to be a GC issue, Repa though seems to handle things pretty well.
If I remember correctly, it's a template-haskell-like system for Idris that splices Idris core instead of Idris surface language. In the context of Idris, this allows users to essentially use Idris as Idris's tactic language. Very very cool stuff, and I wanted one of my own right away. /u/davidchristiansen, care to comment?
I'll try to give you an intuition of how it works for lists (,) &lt;$&gt; [1..10] &lt;*&gt; [1..10] (,) --&gt; two argument function &lt;$&gt; --&gt; apply the function to [1..10] --&gt; list of first arguments &lt;*&gt; --&gt; then for each result [1..10] --&gt; list of second arguments you are basically doing the same as this: `(,) 1 1` but using an applicative functor instead of a simple function, because of the applicative (in this case specifically a list applicative), your parameters act as a list of possible values, and each new parameter produces a new cross-product with the previous list of parameters. For example: `(,)` is a function with two parameters X and Y that produces (X,Y) `(,) &lt;$&gt; [1,2]` here [1,2] takes the place of X producing a list with two functions, which are already applied to each value in the first parameter: [(1,Y), (2,Y)] `(,) &lt;$&gt; [1,2] &lt;*&gt; [3,4,5]` here [3,4,5] takes the place of the second parameter Y in each of the functions of the previous list producing: [(1,3),(1,4),(1,5), (2,3),(2,4),(2,5)] This cross product of parameters is specific to the list applicative, other applicative instances behave in different ways, what they share in common is basically the syntax to apply a multi-parameter function to applicative types (Maybe, [], (-&gt;) e, ...)
Well, my apologies, but its' really not very clear what exactly you want. If you want to reconstruct exactly what you have drawn there in types, you can use more type magic. data MatchKind = TMatch | TWinners | TLosers | TDecider data Match (k :: MatchKind) a b where Match :: a -&gt; b -&gt; Match 'TMatch a b Winners :: Match 'TMatch a b -&gt; Match 'TMatch c d -&gt; Match 'TWinners a c Losers :: Match 'TMatch a b -&gt; Match 'TMatch c d -&gt; Match 'TLosers b d Decider :: Match 'TWinners a b -&gt; Match 'TLosers c d -&gt; Match 'TDecider b c data Player (s :: Symbol) = Player alignFirst :: Player p -&gt; Match k (Player p) b -&gt; Player p alignFirst _ _ = Player printWinner :: KnownSymbol p =&gt; Match k (Player p) b -&gt; String printWinner = symbolVal . alignTypes Player `Match k a b` represents a match of kind k between a and b where a wins. I didn't completely explore the entire space of possible outcomes with the constructors because that would clutter up the example, but you can fill that in pretty much mechanically, in the style of `Match2 :: a -&gt; b -&gt; Match 'Match b a` representing a match where b won. But I advise against this unless you are comfortable with everything going on here without further explanation.
Just remember that `&lt;*&gt;` does Cartesian join; &gt; (*) &lt;$&gt; [2] &lt;*&gt; [3] [6] &gt; (*) &lt;$&gt; [1, 2] &lt;*&gt; [3, 1] [3, 1, 6, 2]
I figured out a similar &lt;$&gt; and &lt;*&gt; combination myself not long ago. Answer to every question I asked myself can be found in your post, in a clear, type guided way. If you write a Haskell book in the style of this post, I'll definitely buy it. 
Here is a PDF link to the paper [Applicative Programming with Effects](http://staff.city.ac.uk/~ross/papers/Applicative.pdf). (Yes, I'm going to be that guy.) I didn't understand, _really_ understand, `Applicative` until I sat down with the paper for a good couple of hours, desugared the syntax by hand and worked through the examples on paper. I consider it time well spent.
What have you tried so far? This might be a good place to start: https://en.wikipedia.org/wiki/Rubik's_Cube_group
It's a reference to [Mark Lentczner's post](https://mail.haskell.org/pipermail/ghc-devs/2015-October/010068.html).
If you want to prove stuff about your programs, like Harper does, then tiny problems aren't so tiny.
For example, with `Maybe` the order of application will never matter. With `IO` it always will. The difference between `Monad` and a mere `Applicative` is that with the former the *shape* (or *effects*, or whatever makes sense for a particular type) can depend on the values obtained from monadic computations.
The goal is to be able to compile *.hs files out of the box, unless they use more advanced GHC extensions. But I guess it will take another year, or so. If anyone is interested to help in this regard, here are some slides that list some tasks https://docs.google.com/presentation/d/1wdvP70ziDRaGZiKI3Zqb0ciLir4_axOH04APxaRtIsY/edit?usp=sharing There are quite some tasks (libraries, standard type classes, etc.) that can be done without touching the compiler. 
Best explanation ever for beginners: [Functors, Applicatives, And Monads in pictures](http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html)
Don't waste your time reading this.... really, don't....
If there isn't some significant engineering tradeoff involved (the `Typeable` one, for example), there's really no excuse to keep the behavior around. In the `Typeable` example, it did get fixed, making your own instances of that typeclass is now illegal Haskell.
Forget containers and think of these things in terms of contexts. That's where the cool intuitions hide. The maybe context says that a value may exist, or it may not, but you get to apply your function to your value anyway. If the value isn't there, it'll be Nothing, and your result will be, too. In the function context, the function, and the value you want to apply it to are *both* behind pay-walls. You need to give each a value so each can give you back the thing you want. Then you can apply the function to the value. The context of the function functor/applicative/monad is that the function and value are hidden on the other side of computations, and you have to do a little transaction to get to each. E.g., for the function applicative: (*) &lt;*&gt; (+3) The `(*)` needs two values, ultimately, but the `(+3)` only needs one, and both take the same type (importantly, the function side of things takes the same type as its final input), so we can `&lt;*&gt;` them together (which is function application), and get a new thing in the same applicative type - i.e. another function. When we give *that* a value - let's say `5` - it gets fed to both sides; the left side becomes `(5*)`, a function, and the right becomes `8`, a value. Now we can do the application, which is what all of these abstractions are about, and get 40. ((*) &lt;*&gt; (+3)) 5 40 We didn't have to use sections. We just need to fulfill the types. This also works (we just throw away the eventual input to the function side via const): const reverse &lt;*&gt; tail -- feed this a list, and get the reverse of the tail The list context is interesting. In all the other functors/applicatives/monads we work on a single value with a single function. Lists can have 0 or more values, though. You can gain a lot of insight if you think of a list as still containing a single value, but with many faces. Think of a many-worlds scenario. [3,6,2] is one value in this context, but it exists in a kind of superposition as 3 possibilities. If we apply our single function, say [id, const 5] (a function in superposition with 2 possible interpretations), we multiply their indecisiveness, and end up with an even less definitive result: [3,5,6,5,2,5], i.e. all possible worlds. And yet, it just works, and now we still have what we started with - a value, with many possible interpretations, ready to have another multi-faceted function applied to it. That's the context. It works for the functor, too; you're still applying a function to a value, but the application has to shatter across multiple realities in the process.
If you're worried about patents in the F# compiler, Mono doesn't have some reimplementation of it that might be taken down for patent violations because the F# compiler is open source and builds fine on Linux already. EDIT: Also .NET is open source now, and is to be (is already?) officially supported on OS X and Linux. It was already officially supported on FreeBSD in the past. From what I remember, Microsoft has (intentionally?) put themselves in a position that if they sue over these patents, they open up counter-suits because the patents are intended to be deterrents against being sued for infringement themselves and their FOSS licenses are explicitly intended to be used in conjunction with this MAD scheme.
You could try the ST monad.
If you find `[]` hard, do some beginning Prolog. That gives you a clear analogy (indeed, `LogicT` is basically intended to be a better `[]`).
Agreed. Polymorphic Variants and the module system are about the only ocaml features I miss in Haskell. Most of the ocaml coders I work with also complain a lot about Haskell's laziness and how it makes it harder to reason about memory usage. 
Why don't you explain in more detail what the program should do? A good rule of thumb when doing things in Haskell is that you don't literally translate the algorithm you would write in Python. There's probably a perfectly concise way to write what you're doing without requiring references of any sort.
That sounds really interesting. But... How much can it really do? Here's an example of some hand-tuned formatting: numbers = strings "ten eleven twelve" + (strings "thir four" + prefixes) * string "teen" + (strings "twen thir for" + prefixes) * string "ty" * (one + tens) Notice how * The first "strings" has an extra space so it aligns to the other "strings" while still allowing the "+"s to align with the "=" * The "+ prefixes" are aligned inside nested expressions * The "*" in the last line is shifted to form a new column after the strings * But the "+ prefixes" are *not* shifted although the strings could form a separate column, too. I can imagine that an automated tool can do some of that, but some parts might be impossible in general. How far are we?
Don't forget CUDA!
Compare Conal Elliott's work on [Tangible Functional Programming (2007)](http://conal.net/papers/Eros/). "Reactive formlets" and "Tangible values" are so similar at first glance.
I think you have a typo - it should say CoFreeScript - to be invented when OO recognizes it's comonadic foundations.
Thanks. I did have thought of this but I actually have several different types: declarations, expressions, patterns, and types. After reading your comment I got a new idea, for some related types, namely declarations and expressions, I would use your approach, but for the rest I could try to pas the annotation directly to allow me to reconstruct them, like data ExprF fe info = ELam (Ann Pat info) fe, which seems a bit more natural considering my future recursive operations.
So you want to define two mutually-recursive types via non-recursive FooF definitions and a Fix combinator which implements type-level recursion. How about using two Fix-like combinators implementing mutual recursion instead? data ExprF e p = EInt Int | ELam p e deriving (Eq, Show) data PatF e p = PAs String e deriving (Eq, Show) data Fix1 f1 f2 = In1 { out1 :: f1 (Fix1 f1 f2) (Fix2 f1 f2) } data Fix2 f1 f2 = In2 { out2 :: f2 (Fix1 f1 f2) (Fix2 f1 f2) } instance Eq (f1 (Fix1 f1 f2) (Fix2 f1 f2)) =&gt; Eq (Fix1 f1 f2) where In1 x == In1 y = x == y instance Eq (f2 (Fix1 f1 f2) (Fix2 f1 f2)) =&gt; Eq (Fix2 f1 f2) where In2 x == In2 y = x == y I'm quite confident that this general idea should work, but beware any typos in the code, I'm on my phone and haven't had the chance to typecheck it yet. **edit**: /u/paf31 had the same idea and typed it faster than me, and so deserves the upvotes. But hey, no typos! I find it amazing that just by using Haskell a lot, without special training for the offline case, my brain just picked up the ability to typecheck code for me in the background. I remember back in the university, I was very impressed to see that my type theory teacher, [Stefan Monnier](http://www.iro.umontreal.ca/~monnier/), could typecheck Agda code I wrote on the blackboard in his head. I guess I have this superpower too now, and I didn't even have to do anything special to get it! I also find it amazing that /u/paf31 and I both came up with the exact same solution, not because we both remembered the same solution from some paper we both read (I don't remember encountering this problem or solution anywhere before), but just by thinking about the problem and writing down the only reasonable solution. Well, one of two solutions, since /u/AndrasKovacs's solution is also quite good and completely different than ours. I suspected that there was another solution in that general vicinity, but there's no way my brain could have typechecked that one :)
As far as I know, Agda was never meant to be practical, More so used as a replacement for Coq(a proof assistant). Idris however is supposed to be a practical implementation of dependent types. We might even see Dependent types making their way into Haskell in the coming years. 
Black magic. Seriously though, as a beginner I'm still amazed at how cryptic functional syntax can be. I love glancing at some new code I just wrote and pretend it's my first time seeing functional code. Mind blown each time. I know &lt;$&gt; is fmap, which is a way to "unwrap" values from their functor. So (+2) &lt;$&gt; Just 1 will be Just 3 because fmap "unwrapped" the Just to reveal its value, applied addition to 2, then wrapped it back up with a Just. I don't know enough about &lt;*&gt; to simplify it in a useful way, but unwrapping values from context really helped me understand applicatives and functors.
I posted my (working) solution here: http://lpaste.net/145154 It may be of help.
I wish that this comment weren't so reductive. 
Typechecking for intensional dependent types with subtyping and linear typing and codata isn't undecidable. Type inference is.
For me the list is type classes, linear types, anonymous variants (at higher kinds), and all the stuff gadts drag in. I think we all have such a list, even if it's not written down.
What are your choices for first two on the list of best FP languages?
&gt; You come to realize that any fixed language limits what your mind can imagine. This is the most insightful comment in the entire thread. Spot on.
They did in the past, also, C# would benefit too.
"Unwrapped" is one way to see it, but you can also imagine the function being lifted up into the functor: fmap (*2) [1, 2, 3] = [1*2, 2*2, 3*2] = [2,4,6] fmap (*2) (Just 3) = Just (2*3) = Just 6 fmap (*2) (+3) $ 5 = ((*2) . (+3)) 5 = 16 Looking at the type: fmap :: (a -&gt; b) -&gt; f a -&gt; f b ...and remembering that the function constructor (the -&gt;) is right associative, and thus this is the same thing: fmap :: (a -&gt; b) -&gt; (f a -&gt; f b) ...you can also imagine that fmap is a thing that transforms a regular a -&gt; b function into a context-respecting f a -&gt; f b function.
Couldn't agree more
Is lift the proper term for this? Thanks.
I'm sorry, I don't have any. (But I doubt I'd like it!)
Would be interesting to also know how ghcjs would fare.
It's possible to add this to hindent, in one of the styles, but nobody has seen it worth doing yet. I don't personally rate infix alignment as a useful embellishment. hindent formats this example (with a paren added) as numbers = (strings "ten eleven twelve") + (strings "thir four" + prefixes) * string "teen" + (strings "twen thir for" + prefixes) * string "ty" * (one + tens) I'd call that done and move onto something else. 
To put simple: (,) &lt;$&gt; [1..10] &lt;*&gt; [1..10] [(1,), (2,), (3,) .. (10,)] &lt;*&gt;[1..10] [(1,1), (1,2), (1,3), .. (2,1) .. (2,2) .. (10,10)] 
I would try something like this: data ExprF p e = EInt Int | ELam p e data Pat e = PAs String e newtype Expr = Expr (ExprF (Pat Expr) Expr) or this data ExprF p e = EInt Int | ELam (p e) e data Pat e = PAs String e type Expr = Fix (ExprF Pat)
That's a really cool way of doing it, thanks!
This only describes a double elimination bracket, right?